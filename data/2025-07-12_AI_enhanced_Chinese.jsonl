{"id": "2507.07142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07142", "abs": "https://arxiv.org/abs/2507.07142", "authors": ["Quanjie Qiu", "MengCheng Lau"], "title": "g2o vs. Ceres: Optimizing Scan Matching in Cartographer SLAM", "comment": null, "summary": "This article presents a comparative analysis of g2o and Ceres solvers in\nenhancing scan matching performance within the Cartographer framework.\nCartographer, a widely-used library for Simultaneous Localization and Mapping\n(SLAM), relies on optimization algorithms to refine pose estimates and improve\nmap accuracy. The research aims to evaluate the performance, efficiency, and\naccuracy of the g2o solver in comparison to the Ceres solver, which is the\ndefault in Cartographer. In our experiments comparing Ceres and g2o within\nCartographer, Ceres outperformed g2o in terms of speed, convergence efficiency,\nand overall map clarity. Ceres required fewer iterations and less time to\nconverge, producing more accurate and well-defined maps, especially in\nreal-world mapping scenarios with the AgileX LIMO robot. However, g2o excelled\nin localized obstacle detection, highlighting its value in specific situations.", "AI": {"tldr": "\u6bd4\u8f83\u4e86g2o\u548cCeres\u5728Cartographer\u6846\u67b6\u4e2d\u7684\u6027\u80fd\uff0cCeres\u5728\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u800cg2o\u5728\u5c40\u90e8\u969c\u788d\u68c0\u6d4b\u4e2d\u66f4\u51fa\u8272\u3002", "motivation": "\u8bc4\u4f30g2o\u548cCeres\u5728SLAM\u6846\u67b6Cartographer\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u4f18\u5316\u5730\u56fe\u6784\u5efa\u548c\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u5728Cartographer\u4e2d\u5bf9\u6bd4g2o\u548cCeres\u7684\u4f18\u5316\u6548\u679c\uff0c\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u91cf\u901f\u5ea6\u3001\u6536\u655b\u6548\u7387\u548c\u5730\u56fe\u6e05\u6670\u5ea6\u3002", "result": "Ceres\u5728\u901f\u5ea6\u3001\u6536\u655b\u6548\u7387\u548c\u5730\u56fe\u6e05\u6670\u5ea6\u4e0a\u4f18\u4e8eg2o\uff0c\u4f46g2o\u5728\u5c40\u90e8\u969c\u788d\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "Ceres\u66f4\u9002\u5408\u6574\u4f53SLAM\u4efb\u52a1\uff0c\u800cg2o\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u4ecd\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.07221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07221", "abs": "https://arxiv.org/abs/2507.07221", "authors": ["Nam Gyun Kim", "William E. Heap", "Yimeng Qin", "Elvy B. Yao", "Jee-Hwan Ryu", "Allison M. Okamura"], "title": "Self-Wearing Adaptive Garments via Soft Robotic Unfurling", "comment": null, "summary": "Robotic dressing assistance has the potential to improve the quality of life\nfor individuals with limited mobility. Existing solutions predominantly rely on\nrigid robotic manipulators, which have challenges in handling deformable\ngarments and ensuring safe physical interaction with the human body. Prior\nrobotic dressing methods require excessive operation times, complex control\nstrategies, and constrained user postures, limiting their practicality and\nadaptability. This paper proposes a novel soft robotic dressing system, the\nSelf-Wearing Adaptive Garment (SWAG), which uses an unfurling and growth\nmechanism to facilitate autonomous dressing. Unlike traditional approaches,the\nSWAG conforms to the human body through an unfurling based deployment method,\neliminating skin-garment friction and enabling a safer and more efficient\ndressing process. We present the working principles of the SWAG, introduce its\ndesign and fabrication, and demonstrate its performance in dressing assistance.\nThe proposed system demonstrates effective garment application across various\ngarment configurations, presenting a promising alternative to conventional\nrobotic dressing assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f6f\u673a\u5668\u4eba\u7a7f\u8863\u7cfb\u7edfSWAG\uff0c\u901a\u8fc7\u5c55\u5f00\u548c\u751f\u957f\u673a\u5236\u5b9e\u73b0\u81ea\u4e3b\u7a7f\u8863\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u521a\u6027\u673a\u5668\u4eba\u5904\u7406\u53d8\u5f62\u8863\u7269\u548c\u786e\u4fdd\u5b89\u5168\u4e92\u52a8\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7a7f\u8863\u65b9\u6848\u4f9d\u8d56\u521a\u6027\u673a\u68b0\u81c2\uff0c\u64cd\u4f5c\u65f6\u95f4\u957f\u3001\u63a7\u5236\u590d\u6742\u4e14\u9650\u5236\u7528\u6237\u59ff\u52bf\uff0c\u5b9e\u7528\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5c55\u5f00\u7684\u90e8\u7f72\u65b9\u6cd5\uff0c\u4f7fSWAG\u8d34\u5408\u4eba\u4f53\uff0c\u51cf\u5c11\u76ae\u80a4\u4e0e\u8863\u7269\u6469\u64e6\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "result": "SWAG\u5728\u4e0d\u540c\u8863\u7269\u914d\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f20\u7edf\u673a\u5668\u4eba\u7a7f\u8863\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "SWAG\u7cfb\u7edf\u901a\u8fc7\u8f6f\u673a\u5668\u4eba\u6280\u672f\u5b9e\u73b0\u4e86\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u7684\u7a7f\u8863\u8f85\u52a9\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.07225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07225", "abs": "https://arxiv.org/abs/2507.07225", "authors": ["Yimeng Qin", "Jared Grinberg", "William Heap", "Allison M. Okamura"], "title": "3D Steering and Localization in Pipes and Burrows using an Externally Steered Soft Growing Robot", "comment": null, "summary": "Navigation and inspection in confined environments, such as tunnels and\npipes, pose significant challenges for existing robots due to limitations in\nmaneuverability and adaptability to varying geometries. Vine robots, which are\nsoft growing continuum robots that extend their length through soft material\neversion at their tip, offer unique advantages due to their ability to navigate\ntight spaces, adapt to complex paths, and minimize friction. However, existing\nvine robot designs struggle with navigation in manmade and natural passageways,\nwith branches and sharp 3D turns. In this letter, we introduce a steerable vine\nrobot specifically designed for pipe and burrow environments. The robot\nfeatures a simple tubular body and an external tip mount that steers the vine\nrobot in three degrees of freedom by changing the growth direction and, when\nnecessary, bracing against the wall of the pipe or burrow. Our external tip\nsteering approach enables: (1) active branch selection in 3D space with a\nmaximum steerable angle of 51.7{\\deg}, (2) navigation of pipe networks with\nradii as small as 2.5 cm, (3) a compliant tip enabling navigation of sharp\nturns, and (4) real-time 3D localization in GPS-denied environments using\ntip-mounted sensors and continuum body odometry. We describe the forward\nkinematics, characterize steerability, and demonstrate the system in a 3D pipe\nsystem as well as a natural animal burrow.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53ef\u8f6c\u5411\u85e4\u8513\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u5728\u72ed\u7a84\u7ba1\u9053\u548c\u6d1e\u7a74\u4e2d\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u85e4\u8513\u673a\u5668\u4eba\u5728\u590d\u6742\u8def\u5f84\u548c\u5206\u652f\u73af\u5883\u4e2d\u5bfc\u822a\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5916\u90e8\u5c16\u7aef\u5b89\u88c5\u548c\u4e09\u7ef4\u8f6c\u5411\u8bbe\u8ba1\uff0c\u7ed3\u5408\u5b9e\u65f6\u5b9a\u4f4d\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u4e8651.7\u00b0\u7684\u6700\u5927\u8f6c\u5411\u89d2\u5ea6\uff0c\u53ef\u5bfc\u822a\u534a\u5f84\u5c0f\u81f32.5\u5398\u7c73\u7684\u7ba1\u9053\uff0c\u5e76\u80fd\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\u5b9a\u4f4d\u3002", "conclusion": "\u8be5\u673a\u5668\u4eba\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u72ed\u7a84\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2507.07299", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07299", "abs": "https://arxiv.org/abs/2507.07299", "authors": ["Sonia Raychaudhuri", "Enrico Cancelli", "Tommaso Campari", "Lamberto Ballan", "Manolis Savva", "Angel X. Chang"], "title": "LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation", "comment": null, "summary": "Recent progress in large vision-language models has driven improvements in\nlanguage-based semantic navigation, where an embodied agent must reach a target\nobject described in natural language. Despite these advances, we still lack a\nclear, language-focused benchmark for testing how well such agents ground the\nwords in their instructions. We address this gap with LangNav, an open-set\ndataset specifically created to test an agent's ability to locate objects\ndescribed at different levels of detail, from broad category names to fine\nattributes and object-object relations. Every description in LangNav was\nmanually checked, yielding a lower error rate than existing lifelong- and\nsemantic-navigation datasets. On top of LangNav we build LangNavBench, a\nbenchmark that measures how well current semantic-navigation methods understand\nand act on these descriptions while moving toward their targets. LangNavBench\nallows us to systematically compare models on their handling of attributes,\nspatial and relational cues, and category hierarchies, offering the first\nthorough, language-centric evaluation of embodied navigation systems. We also\npresent Multi-Layered Feature Map (MLFM), a method that builds a queryable\nmulti-layered semantic map, particularly effective when dealing with small\nobjects or instructions involving spatial relations. MLFM outperforms\nstate-of-the-art mapping-based navigation baselines on the LangNav dataset.", "AI": {"tldr": "LangNav\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8bed\u8a00\u7406\u89e3\u7684\u8bed\u4e49\u5bfc\u822a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u667a\u80fd\u4f53\u5728\u4e0d\u540c\u8be6\u7ec6\u7a0b\u5ea6\u7684\u8bed\u8a00\u6307\u4ee4\u4e0b\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86LangNavBench\u57fa\u51c6\u548cMLFM\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u660e\u786e\u7684\u8bed\u8a00\u57fa\u51c6\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u667a\u80fd\u4f53\u5bf9\u8bed\u8a00\u6307\u4ee4\u7684\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86LangNav\u6570\u636e\u96c6\u548cLangNavBench\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86MLFM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u53ef\u67e5\u8be2\u7684\u591a\u5c42\u8bed\u4e49\u5730\u56fe\u6765\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u3002", "result": "MLFM\u5728LangNav\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u5730\u56fe\u7684\u5bfc\u822a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LangNav\u548cLangNavBench\u4e3a\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u5de5\u5177\uff0cMLFM\u65b9\u6cd5\u5728\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.07315", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07315", "abs": "https://arxiv.org/abs/2507.07315", "authors": ["Ricardo Vega", "Cameron Nowzari"], "title": "Classifying Emergence in Robot Swarms: An Observer-Dependent Approach", "comment": "25 pages, 3 tables, 8 figures", "summary": "Emergence and swarms are widely discussed topics, yet no consensus exists on\ntheir formal definitions. This lack of agreement makes it difficult not only\nfor new researchers to grasp these concepts, but also for experts who may use\nthe same terms to mean different things. Many attempts have been made to\nobjectively define 'swarm' or 'emergence,' with recent work highlighting the\nrole of the external observer. Still, several researchers argue that once an\nobserver's vantage point (e.g., scope, resolution, context) is established, the\nterms can be made objective or measured quantitatively. In this note, we\npropose a framework to discuss these ideas rigorously by separating externally\nobservable states from latent, unobservable ones. This allows us to compare and\ncontrast existing definitions of swarms and emergence on common ground. We\nargue that these concepts are ultimately subjective-shaped less by the system\nitself than by the perception and tacit knowledge of the observer.\nSpecifically, we suggest that a 'swarm' is not defined by its group behavior\nalone, but by the process generating that behavior. Our broader goal is to\nsupport the design and deployment of robotic swarm systems, highlighting the\ncritical distinction between multi-robot systems and true swarms.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u201c\u6d8c\u73b0\u201d\u548c\u201c\u7fa4\u96c6\u201d\u7f3a\u4e4f\u5171\u8bc6\u5b9a\u4e49\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u533a\u5206\u53ef\u89c2\u5bdf\u548c\u4e0d\u53ef\u89c2\u5bdf\u72b6\u6001\uff0c\u5f3a\u8c03\u8fd9\u4e9b\u6982\u5ff5\u7684\u4e3b\u89c2\u6027\u3002", "motivation": "\u7531\u4e8e\u5bf9\u201c\u6d8c\u73b0\u201d\u548c\u201c\u7fa4\u96c6\u201d\u7f3a\u4e4f\u7edf\u4e00\u7684\u5b9a\u4e49\uff0c\u65b0\u7814\u7a76\u8005\u96be\u4ee5\u7406\u89e3\uff0c\u4e13\u5bb6\u4e5f\u53ef\u80fd\u56e0\u672f\u8bed\u6b67\u4e49\u4ea7\u751f\u8bef\u89e3\u3002\u8bba\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u4e25\u8c28\u7684\u8ba8\u8bba\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u79bb\u53ef\u89c2\u5bdf\u72b6\u6001\u548c\u6f5c\u5728\u4e0d\u53ef\u89c2\u5bdf\u72b6\u6001\uff0c\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5bf9\u6bd4\u73b0\u6709\u5b9a\u4e49\uff0c\u5f3a\u8c03\u89c2\u5bdf\u8005\u7684\u89c6\u89d2\u548c\u9690\u6027\u77e5\u8bc6\u7684\u4f5c\u7528\u3002", "result": "\u8ba4\u4e3a\u201c\u7fa4\u96c6\u201d\u4e0d\u4ec5\u7531\u7fa4\u4f53\u884c\u4e3a\u5b9a\u4e49\uff0c\u8fd8\u7531\u884c\u4e3a\u751f\u6210\u7684\u8fc7\u7a0b\u5b9a\u4e49\uff0c\u6307\u51fa\u8fd9\u4e9b\u6982\u5ff5\u7684\u4e3b\u89c2\u6027\u3002", "conclusion": "\u8bba\u6587\u652f\u6301\u673a\u5668\u4eba\u7fa4\u96c6\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u90e8\u7f72\uff0c\u5f3a\u8c03\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e0e\u771f\u6b63\u7fa4\u96c6\u7684\u5173\u952e\u533a\u522b\u3002"}}
{"id": "2507.07327", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07327", "abs": "https://arxiv.org/abs/2507.07327", "authors": ["Brian B. Vuong", "Josie Davidson", "Sangheui Cheon", "Kyujin Cho", "Allison M. Okamura"], "title": "Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Previous work has shown that the addition of haptic feedback to the hands can\nimprove awareness of tool-tissue interactions and enhance performance of\nteleoperated tasks in robot-assisted minimally invasive surgery. However,\nhand-based haptic feedback occludes direct interaction with the manipulanda of\nsurgeon console in teleoperated surgical robots. We propose relocating haptic\nfeedback to the wrist using a wearable haptic device so that haptic feedback\nmechanisms do not need to be integrated into the manipulanda. However, it is\nunknown if such feedback will be effective, given that it is not co-located\nwith the finger movements used for manipulation. To test if relocated haptic\nfeedback improves force application during teleoperated tasks using da Vinci\nResearch Kit (dVRK) surgical robot, participants learned to palpate a phantom\ntissue to desired forces. A soft pneumatic wrist-worn haptic device with an\nanchoring system renders tool-tissue interaction forces to the wrist of the\nuser. Participants performed the palpation task with and without wrist-worn\nhaptic feedback and were evaluated for the accuracy of applied forces.\nParticipants demonstrated statistically significant lower force error when\nwrist-worn haptic feedback was provided. Participants also performed the\npalpation task with longer movement times when provided wrist-worn haptic\nfeedback, indicating that the haptic feedback may have caused participants to\noperate at a different point in the speed-accuracy tradeoff curve.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u5c06\u89e6\u89c9\u53cd\u9988\u4ece\u624b\u90e8\u8f6c\u79fb\u5230\u624b\u8155\uff0c\u4ee5\u907f\u514d\u5e72\u6270\u624b\u672f\u673a\u5668\u4eba\u7684\u64cd\u4f5c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u624b\u8155\u89e6\u89c9\u53cd\u9988\u5728\u63d0\u9ad8\u529b\u63a7\u5236\u51c6\u786e\u6027\u4e0a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u624b\u90e8\u89e6\u89c9\u53cd\u9988\u4f1a\u5e72\u6270\u624b\u672f\u673a\u5668\u4eba\u7684\u76f4\u63a5\u64cd\u4f5c\uff0c\u56e0\u6b64\u7814\u7a76\u63a2\u7d22\u4e86\u624b\u8155\u89e6\u89c9\u53cd\u9988\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u8f6f\u6c14\u52a8\u624b\u8155\u89e6\u89c9\u8bbe\u5907\uff0c\u53c2\u4e0e\u8005\u901a\u8fc7\u8fbe\u82ac\u5947\u7814\u7a76\u5957\u4ef6\u5b8c\u6210\u7ec4\u7ec7\u89e6\u8bca\u4efb\u52a1\uff0c\u6bd4\u8f83\u6709\u65e0\u89e6\u89c9\u53cd\u9988\u7684\u8868\u73b0\u3002", "result": "\u624b\u8155\u89e6\u89c9\u53cd\u9988\u663e\u8457\u964d\u4f4e\u4e86\u529b\u8bef\u5dee\uff0c\u4f46\u5ef6\u957f\u4e86\u4efb\u52a1\u65f6\u95f4\u3002", "conclusion": "\u624b\u8155\u89e6\u89c9\u53cd\u9988\u80fd\u6709\u6548\u63d0\u9ad8\u529b\u63a7\u5236\u51c6\u786e\u6027\uff0c\u4f46\u53ef\u80fd\u5f71\u54cd\u64cd\u4f5c\u901f\u5ea6\u3002"}}
{"id": "2507.07356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07356", "abs": "https://arxiv.org/abs/2507.07356", "authors": ["Kangning Yin", "Weishuai Zeng", "Ke Fan", "Zirui Wang", "Qiang Zhang", "Zheng Tian", "Jingbo Wang", "Jiangmiao Pang", "Weinan Zhang"], "title": "UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots", "comment": "10 pages, 5 figures", "summary": "Humanoid robots must achieve diverse, robust, and generalizable whole-body\ncontrol to operate effectively in complex, human-centric environments. However,\nexisting methods, particularly those based on teacher-student frameworks often\nsuffer from a loss of motion diversity during policy distillation and exhibit\nlimited generalization to unseen behaviors. In this work, we present\nUniTracker, a simplified yet powerful framework that integrates a Conditional\nVariational Autoencoder (CVAE) into the student policy to explicitly model the\nlatent diversity of human motion. By leveraging a learned CVAE prior, our\nmethod enables the student to retain expressive motion characteristics while\nimproving robustness and adaptability under partial observations. The result is\na single policy capable of tracking a wide spectrum of whole-body motions with\nhigh fidelity and stability. Comprehensive experiments in both simulation and\nreal-world deployments demonstrate that UniTracker significantly outperforms\nMLP-based DAgger baselines in motion quality, generalization to unseen\nreferences, and deployment robustness, offering a practical and scalable\nsolution for expressive humanoid control.", "AI": {"tldr": "UniTracker\u6846\u67b6\u901a\u8fc7CVAE\u663e\u5f0f\u5efa\u6a21\u4eba\u7c7b\u8fd0\u52a8\u591a\u6837\u6027\uff0c\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u7684\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5e08\u751f\u6846\u67b6\u7684\u65b9\u6cd5\u5728\u7b56\u7565\u84b8\u998f\u4e2d\u4f1a\u4e22\u5931\u8fd0\u52a8\u591a\u6837\u6027\uff0c\u4e14\u5bf9\u672a\u89c1\u884c\u4e3a\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u96c6\u6210CVAE\u5230\u5b66\u751f\u7b56\u7565\u4e2d\uff0c\u5229\u7528\u5176\u6f5c\u5728\u7a7a\u95f4\u5efa\u6a21\u8fd0\u52a8\u591a\u6837\u6027\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "UniTracker\u5728\u8fd0\u52a8\u8d28\u91cf\u3001\u6cdb\u5316\u80fd\u529b\u548c\u90e8\u7f72\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8eMLP-based DAgger\u57fa\u7ebf\u3002", "conclusion": "UniTracker\u4e3a\u8868\u8fbe\u6027\u4eba\u5f62\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07370", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07370", "abs": "https://arxiv.org/abs/2507.07370", "authors": ["Zhanhong Jiang", "Dylan Shah", "Hsin-Jung Yang", "Soumik Sarkar"], "title": "Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification", "comment": "6 pages; 6 figures; accepted at the 5th Modeling, Estimation and\n  Control Conference (MECC 2025)", "summary": "Precise kinematic modeling is critical in calibration and controller design\nfor soft robots, yet remains a challenging issue due to their highly nonlinear\nand complex behaviors. To tackle the issue, numerous data-driven machine\nlearning approaches have been proposed for modeling nonlinear dynamics.\nHowever, these models suffer from prediction uncertainty that can negatively\naffect modeling accuracy, and uncertainty quantification for kinematic modeling\nin soft robots is underexplored. In this work, using limited simulation and\nreal-world data, we first investigate multiple linear and nonlinear machine\nlearning models commonly used for kinematic modeling of soft robots. The\nresults reveal that nonlinear ensemble methods exhibit the most robust\ngeneralization performance. We then develop a conformal kinematic modeling\nframework for soft robots by utilizing split conformal prediction to quantify\npredictive position uncertainty, ensuring distribution-free prediction\nintervals with a theoretical guarantee.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u590d\u6742\u884c\u4e3a\u4f7f\u5f97\u7cbe\u786e\u8fd0\u52a8\u5b66\u5efa\u6a21\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5b58\u5728\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u7814\u7a76\u591a\u79cd\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5f00\u53d1\u57fa\u4e8e\u5206\u62c6\u5171\u5f62\u9884\u6d4b\u7684\u6846\u67b6\u4ee5\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u975e\u7ebf\u6027\u96c6\u6210\u65b9\u6cd5\u8868\u73b0\u6700\u4f18\uff0c\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u80fd\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.07376", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07376", "abs": "https://arxiv.org/abs/2507.07376", "authors": ["Hengrui Liu", "Yi Feng", "Qilong Zhang"], "title": "PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments", "comment": null, "summary": "Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster\nresponse, exploration, and reconnaissance. However, dynamic and unknown\nenvironments pose significant challenges due to target unpredictability and\nenvironmental uncertainty. To tackle these issues, we propose PILOC, a\nframework that operates without global prior knowledge, leveraging local\nperception and communication. It introduces a pheromone inverse guidance\nmechanism to enable efficient coordination and dynamic target localization.\nPILOC promotes decentralized cooperation through local communication,\nsignificantly reducing reliance on global channels. Unlike conventional\nheuristics, the pheromone mechanism is embedded into the observation space of\nDeep Reinforcement Learning (DRL), supporting indirect agent coordination based\non environmental cues. We further integrate this strategy into a DRL-based\nmulti-agent architecture and conduct extensive experiments. Results show that\ncombining local communication with pheromone-based guidance significantly\nboosts search efficiency, adaptability, and system robustness. Compared to\nexisting methods, PILOC performs better under dynamic and\ncommunication-constrained scenarios, offering promising directions for future\nMASAR applications.", "AI": {"tldr": "PILOC\u6846\u67b6\u901a\u8fc7\u5c40\u90e8\u611f\u77e5\u548c\u901a\u4fe1\uff0c\u7ed3\u5408\u4fe1\u606f\u7d20\u673a\u5236\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u641c\u7d22\u6551\u63f4\u7684\u6548\u7387\u4e0e\u9002\u5e94\u6027\u3002", "motivation": "\u52a8\u6001\u548c\u672a\u77e5\u73af\u5883\u4e2d\u7684\u76ee\u6807\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u5bf9\u591a\u667a\u80fd\u4f53\u641c\u7d22\u6551\u63f4\uff08MASAR\uff09\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51faPILOC\u6846\u67b6\uff0c\u5229\u7528\u5c40\u90e8\u611f\u77e5\u548c\u901a\u4fe1\uff0c\u5f15\u5165\u4fe1\u606f\u7d20\u9006\u5411\u5f15\u5bfc\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u95f4\u63a5\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPILOC\u5728\u52a8\u6001\u548c\u901a\u4fe1\u53d7\u9650\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u641c\u7d22\u6548\u7387\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "conclusion": "PILOC\u4e3a\u672a\u6765MASAR\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5c24\u5176\u5728\u52a8\u6001\u548c\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.07444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07444", "abs": "https://arxiv.org/abs/2507.07444", "authors": ["Korbinian Moller", "Rafael Neher", "Marvin Seegert", "Johannes Betz"], "title": "Towards Safe Autonomous Driving: A Real-Time Safeguarding Concept for Motion Planning Algorithms", "comment": "7 pages, submitted to the IEEE ICVES 2025, Coventry, UK", "summary": "Ensuring the functional safety of motion planning modules in autonomous\nvehicles remains a critical challenge, especially when dealing with complex or\nlearning-based software. Online verification has emerged as a promising\napproach to monitor such systems at runtime, yet its integration into embedded\nreal-time environments remains limited. This work presents a safeguarding\nconcept for motion planning that extends prior approaches by introducing a time\nsafeguard. While existing methods focus on geometric and dynamic feasibility,\nour approach additionally monitors the temporal consistency of planning outputs\nto ensure timely system response. A prototypical implementation on a real-time\noperating system evaluates trajectory candidates using constraint-based\nfeasibility checks and cost-based plausibility metrics. Preliminary results\nshow that the safeguarding module operates within real-time bounds and\neffectively detects unsafe trajectories. However, the full integration of the\ntime safeguard logic and fallback strategies is ongoing. This study contributes\na modular and extensible framework for runtime trajectory verification and\nhighlights key aspects for deployment on automotive-grade hardware. Future work\nincludes completing the safeguarding logic and validating its effectiveness\nthrough hardware-in-the-loop simulations and vehicle-based testing. The code is\navailable at: https://github.com/TUM-AVS/motion-planning-supervisor", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u6a21\u5757\u7684\u5b89\u5168\u4fdd\u969c\u6982\u5ff5\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u76d1\u63a7\u673a\u5236\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u786e\u4fdd\u7cfb\u7edf\u54cd\u5e94\u7684\u53ca\u65f6\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6216\u57fa\u4e8e\u5b66\u4e60\u7684\u8f6f\u4ef6\u4e2d\u8fd0\u52a8\u89c4\u5212\u6a21\u5757\u7684\u529f\u80fd\u5b89\u5168\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5d4c\u5165\u5f0f\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u9a8c\u8bc1\u9700\u6c42\u3002", "method": "\u5728\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u539f\u578b\uff0c\u7ed3\u5408\u7ea6\u675f\u53ef\u884c\u6027\u68c0\u67e5\u548c\u57fa\u4e8e\u6210\u672c\u7684\u5408\u7406\u6027\u8bc4\u4f30\uff0c\u76d1\u63a7\u8f68\u8ff9\u5019\u9009\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u5b89\u5168\u4fdd\u969c\u6a21\u5757\u80fd\u5728\u5b9e\u65f6\u9650\u5236\u5185\u8fd0\u884c\u5e76\u6709\u6548\u68c0\u6d4b\u4e0d\u5b89\u5168\u8f68\u8ff9\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\u8fd0\u884c\u65f6\u8f68\u8ff9\u9a8c\u8bc1\u6846\u67b6\uff0c\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u5b8c\u5584\u903b\u8f91\u5e76\u901a\u8fc7\u786c\u4ef6\u5728\u73af\u548c\u8f66\u8f86\u6d4b\u8bd5\u9a8c\u8bc1\u6709\u6548\u6027\u3002"}}
{"id": "2507.07467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07467", "abs": "https://arxiv.org/abs/2507.07467", "authors": ["Juyeop Han", "Lukas Lao Beyer", "Guilherme V. Cavalheiro", "Sertac Karaman"], "title": "SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation", "comment": "8 pages, 7 figures, 3 tables", "summary": "Autonomous flight in GPS denied indoor spaces requires trajectories that keep\nvisual localization error tightly bounded across varied missions. Whereas\nvisual inertial odometry (VIO) accumulates drift over time, scene coordinate\nregression (SCR) yields drift-free, high accuracy absolute pose estimation. We\npresent a perception-aware framework that couples an evidential learning-based\nSCR pose estimator with a receding horizon trajectory optimizer. The optimizer\nsteers the onboard camera toward pixels whose uncertainty predicts reliable\nscene coordinates, while a fixed-lag smoother fuses the low rate SCR stream\nwith high rate IMU data to close the perception control loop in real time. In\nsimulation, our planner reduces translation (rotation) mean error by 54% / 15%\n(40% / 31%) relative to yaw fixed and forward-looking baselines, respectively.\nMoreover, hardware in the loop experiment validates the feasibility of our\nproposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u573a\u666f\u5750\u6807\u56de\u5f52\uff08SCR\uff09\u548c\u8f68\u8ff9\u4f18\u5316\u7684\u611f\u77e5\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8eGPS\u62d2\u7edd\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u98de\u884c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9a\u4f4d\u8bef\u5dee\u3002", "motivation": "\u5728GPS\u62d2\u7edd\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u4f1a\u968f\u65f6\u95f4\u7d2f\u79ef\u6f02\u79fb\uff0c\u800cSCR\u63d0\u4f9b\u65e0\u6f02\u79fb\u7684\u9ad8\u7cbe\u5ea6\u7edd\u5bf9\u59ff\u6001\u4f30\u8ba1\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u8bc1\u636e\u5b66\u4e60\u7684SCR\u59ff\u6001\u4f30\u8ba1\u5668\u548c\u540e\u9000\u6c34\u5e73\u8f68\u8ff9\u4f18\u5316\u5668\uff0c\u4f18\u5316\u5668\u5f15\u5bfc\u76f8\u673a\u671d\u5411\u4e0d\u786e\u5b9a\u6027\u4f4e\u7684\u50cf\u7d20\uff0c\u540c\u65f6\u56fa\u5b9a\u6ede\u540e\u5e73\u6ed1\u5668\u878d\u5408\u4f4e\u901f\u7387SCR\u6570\u636e\u4e0e\u9ad8\u901f\u7387IMU\u6570\u636e\u3002", "result": "\u4eff\u771f\u4e2d\uff0c\u89c4\u5212\u5668\u5c06\u5e73\u79fb\uff08\u65cb\u8f6c\uff09\u5e73\u5747\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8654%/15%\uff0840%/31%\uff09\uff0c\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u5b9a\u4f4d\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u611f\u77e5\u63a7\u5236\u95ed\u73af\u3002"}}
{"id": "2507.07661", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07661", "abs": "https://arxiv.org/abs/2507.07661", "authors": ["Daria Trinitatova", "Dzmitry Tsetserukou"], "title": "FiDTouch: A 3D Wearable Haptic Display for the Finger Pad", "comment": "Accepted to the IEEE World Haptics Conference 2025 (IEEE WHC 2025), 7\n  pages, 8 figures, 3 tables", "summary": "The applications of fingertip haptic devices have spread to various fields\nfrom revolutionizing virtual reality and medical training simulations to\nfacilitating remote robotic operations, proposing great potential for enhancing\nuser experiences, improving training outcomes, and new forms of interaction. In\nthis work, we present FiDTouch, a 3D wearable haptic device that delivers\ncutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin\nstretch, and vibrotactile feedback. The application of a tiny inverted Delta\nrobot in the mechanism design allows providing accurate contact and fast\nchanging dynamic stimuli to the finger pad surface. The performance of the\ndeveloped display was evaluated in a two-stage user study of the perception of\nstatic spatial contact stimuli and skin stretch stimuli generated on the finger\npad. The proposed display, by providing users with precise touch and force\nstimuli, can enhance user immersion and efficiency in the fields of\nhuman-computer and human-robot interactions.", "AI": {"tldr": "FiDTouch\u662f\u4e00\u79cd3D\u53ef\u7a7f\u6234\u89e6\u89c9\u8bbe\u5907\uff0c\u901a\u8fc7\u5fae\u578b\u5012\u7f6eDelta\u673a\u5668\u4eba\u63d0\u4f9b\u7cbe\u786e\u7684\u63a5\u89e6\u548c\u52a8\u6001\u523a\u6fc0\uff0c\u589e\u5f3a\u7528\u6237\u6c89\u6d78\u611f\u548c\u4ea4\u4e92\u6548\u7387\u3002", "motivation": "\u6307\u5c16\u89e6\u89c9\u8bbe\u5907\u5728\u865a\u62df\u73b0\u5b9e\u3001\u533b\u7597\u57f9\u8bad\u548c\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u66f4\u7cbe\u786e\u7684\u89e6\u89c9\u53cd\u9988\u3002", "method": "\u91c7\u7528\u5fae\u578b\u5012\u7f6eDelta\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u63a5\u89e6\u3001\u538b\u529b\u3001\u76ae\u80a4\u62c9\u4f38\u548c\u632f\u52a8\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u9759\u6001\u63a5\u89e6\u548c\u76ae\u80a4\u62c9\u4f38\u523a\u6fc0\u7684\u611f\u77e5\u3002", "result": "FiDTouch\u80fd\u591f\u63d0\u4f9b\u7cbe\u786e\u7684\u89e6\u89c9\u548c\u529b\u523a\u6fc0\uff0c\u63d0\u5347\u7528\u6237\u5728\u4eba\u673a\u4ea4\u4e92\u548c\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u6c89\u6d78\u611f\u548c\u6548\u7387\u3002", "conclusion": "FiDTouch\u901a\u8fc7\u521b\u65b0\u7684\u673a\u5236\u8bbe\u8ba1\uff0c\u4e3a\u89e6\u89c9\u53cd\u9988\u8bbe\u5907\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.07714", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07714", "abs": "https://arxiv.org/abs/2507.07714", "authors": ["Julio Garrido", "Javier Vales", "Diego Silva-Mu\u00f1iz", "Enrique Riveiro", "Pablo L\u00f3pez-Matencio", "Josu\u00e9 Rivera-Andrade"], "title": "Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots", "comment": "14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent\n  Systems", "summary": "Cable-Driven Parallel Robots (CDPRs) are increasingly used for load\nmanipulation tasks involving predefined toolpaths with intermediate stops. At\neach stop, where the platform maintains a fixed pose and the motors keep the\ncables under tension, the system must evaluate whether it is safe to proceed by\ndetecting anomalies that could compromise performance (e.g., wind gusts or\ncable impacts). This paper investigates whether anomalies can be detected using\nonly motor torque data, without additional sensors. It introduces an adaptive,\nunsupervised outlier detection algorithm based on Gaussian Mixture Models\n(GMMs) to identify anomalies from torque signals. The method starts with a\nbrief calibration period, just a few seconds, during which a GMM is fit on\nknown anomaly-free data. Real-time torque measurements are then evaluated using\nMahalanobis distance from the GMM, with statistically derived thresholds\ntriggering anomaly flags. Model parameters are periodically updated using the\nlatest segments identified as anomaly-free to adapt to changing conditions.\nValidation includes 14 long-duration test sessions simulating varied wind\nintensities. The proposed method achieves a 100% true positive rate and 95.4%\naverage true negative rate, with 1-second detection latency. Comparative\nevaluation against power threshold and non-adaptive GMM methods indicates\nhigher robustness to drift and environmental variation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u81ea\u9002\u5e94\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff0c\u4ec5\u4f7f\u7528\u7535\u673a\u626d\u77e9\u6570\u636e\u68c0\u6d4b\u7535\u7f06\u9a71\u52a8\u5e76\u8054\u673a\u5668\u4eba\u7684\u5f02\u5e38\u3002", "motivation": "\u5728\u7535\u7f06\u9a71\u52a8\u5e76\u8054\u673a\u5668\u4eba\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u989d\u5916\u4f20\u611f\u5668\u68c0\u6d4b\u5f02\u5e38\uff0c\u800c\u672c\u6587\u63a2\u7d22\u4ec5\u7528\u7535\u673a\u626d\u77e9\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u62df\u5408\u65e0\u5f02\u5e38\u6570\u636e\uff0c\u901a\u8fc7\u9a6c\u6c0f\u8ddd\u79bb\u5b9e\u65f6\u8bc4\u4f30\u626d\u77e9\u4fe1\u53f7\uff0c\u5e76\u52a8\u6001\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u4ee5\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "result": "\u572814\u6b21\u957f\u65f6\u95f4\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\u7684\u771f\u9633\u6027\u7387\u548c95.4%\u7684\u5e73\u5747\u771f\u9634\u6027\u7387\uff0c\u68c0\u6d4b\u5ef6\u8fdf\u4e3a1\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u989d\u5916\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u51fa\u5bf9\u6f02\u79fb\u548c\u73af\u5883\u53d8\u5316\u7684\u9ad8\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u9608\u503c\u548c\u975e\u81ea\u9002\u5e94\u65b9\u6cd5\u3002"}}
{"id": "2507.07718", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07718", "abs": "https://arxiv.org/abs/2507.07718", "authors": ["Alberto Rota", "Ke Fan", "Elena De Momi"], "title": "Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics", "comment": null, "summary": "The integration of high-level assistance algorithms in surgical robotics\ntraining curricula may be beneficial in establishing a more comprehensive and\nrobust skillset for aspiring surgeons, improving their clinical performance as\na consequence. This work presents the development and validation of a\nhaptic-enhanced Virtual Reality simulator for surgical robotics training,\nfeaturing 8 surgical tasks that the trainee can interact with thanks to the\nembedded physics engine. This virtual simulated environment is augmented by the\nintroduction of high-level haptic interfaces for robotic assistance that aim at\nre-directing the motion of the trainee's hands and wrists toward targets or\naway from obstacles, and providing a quantitative performance score after the\nexecution of each training exercise.An experimental study shows that the\nintroduction of enhanced robotic assistance into a surgical robotics training\ncurriculum improves performance during the training process and, crucially,\npromotes the transfer of the acquired skills to an unassisted surgical\nscenario, like the clinical one.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u5e26\u6709\u89e6\u89c9\u589e\u5f3a\u7684\u865a\u62df\u73b0\u5b9e\u624b\u672f\u673a\u5668\u4eba\u8bad\u7ec3\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u9ad8\u6c34\u5e73\u7684\u89e6\u89c9\u8f85\u52a9\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u9ad8\u6c34\u5e73\u7684\u8f85\u52a9\u7b97\u6cd5\uff0c\u63d0\u5347\u624b\u672f\u673a\u5668\u4eba\u57f9\u8bad\u8bfe\u7a0b\u7684\u5168\u9762\u6027\u548c\u7a33\u5065\u6027\uff0c\u4ece\u800c\u6539\u5584\u5b66\u5458\u7684\u4e34\u5e8a\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b8\u4e2a\u624b\u672f\u4efb\u52a1\u7684\u89e6\u89c9\u589e\u5f3a\u865a\u62df\u73b0\u5b9e\u6a21\u62df\u5668\uff0c\u5229\u7528\u7269\u7406\u5f15\u64ce\u548c\u89e6\u89c9\u63a5\u53e3\u63d0\u4f9b\u8fd0\u52a8\u5f15\u5bfc\u548c\u6027\u80fd\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u5f3a\u7684\u673a\u5668\u4eba\u8f85\u52a9\u63d0\u9ad8\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4fc3\u8fdb\u4e86\u6280\u80fd\u5411\u65e0\u8f85\u52a9\u624b\u672f\u573a\u666f\u7684\u8f6c\u79fb\u3002", "conclusion": "\u9ad8\u6c34\u5e73\u7684\u89e6\u89c9\u8f85\u52a9\u5728\u624b\u672f\u673a\u5668\u4eba\u8bad\u7ec3\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5b66\u5458\u7684\u6280\u80fd\u548c\u4e34\u5e8a\u8868\u73b0\u3002"}}
{"id": "2507.07724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07724", "abs": "https://arxiv.org/abs/2507.07724", "authors": ["Thiemen Siemensma", "Niels de Boer", "Bahar Haghighat"], "title": "Distributed Surface Inspection via Operational Modal Analysis by a Swarm of Miniaturized Vibration-Sensing Robots", "comment": null, "summary": "Robot swarms offer the potential to serve a variety of distributed sensing\napplications. An interesting real-world application that stands to benefit\nsignificantly from deployment of swarms is structural monitoring, where\ntraditional sensor networks face challenges in structural coverage due to their\nstatic nature. This paper investigates the deployment of a swarm of\nminiaturized vibration sensing robots to inspect and localize structural\ndamages on a surface section within a high-fidelity simulation environment. In\nparticular, we consider a 1 m x 1 m x 3 mm steel surface section and utilize\nfinite element analysis using Abaqus to obtain realistic structural vibration\ndata. The resulting vibration data is imported into the physics-based robotic\nsimulator Webots, where we simulate the dynamics of our surface inspecting\nrobot swarm. We employ (i) Gaussian process estimators to guide the robots'\nexploration as they collect vibration samples across the surface and (ii)\noperational modal analysis to detect structural damages by estimating and\ncomparing existing and intact structural vibration patterns. We analyze the\ninfluence of exploration radii on estimation uncertainty and assess the\neffectiveness of our method across 10 randomized scenarios, where the number,\nlocations, surface area, and depth of structural damages vary. Our simulation\nstudies validate the efficacy of our miniaturized robot swarm for\nvibration-based structural inspection.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5229\u7528\u5fae\u578b\u632f\u52a8\u4f20\u611f\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u6a21\u62df\u73af\u5883\u4e2d\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7ed3\u6784\u635f\u4f24\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u4f20\u611f\u5668\u7f51\u7edc\u5728\u7ed3\u6784\u76d1\u6d4b\u4e2d\u8986\u76d6\u4e0d\u8db3\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u53ef\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6709\u9650\u5143\u5206\u6790\u548c\u673a\u5668\u4eba\u6a21\u62df\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u4f30\u8ba1\u5668\u548c\u6a21\u6001\u5206\u6790\u68c0\u6d4b\u635f\u4f24\u3002", "result": "\u6a21\u62df\u9a8c\u8bc1\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u632f\u52a8\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5fae\u578b\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u7ed3\u6784\u76d1\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.07745", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07745", "abs": "https://arxiv.org/abs/2507.07745", "authors": ["Eleni Konstantinidou", "Nikolaos Kounalakis", "Nikolaos Efstathopoulos", "Dimitrios Papageorgiou"], "title": "On the capabilities of LLMs for classifying and segmenting time series of fruit picking motions into primitive actions", "comment": "This paper is a Late Breaking Results report and it will be presented\n  through a poster at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN), 2025 at Eindhoven, the Netherlands", "summary": "Despite their recent introduction to human society, Large Language Models\n(LLMs) have significantly affected the way we tackle mental challenges in our\neveryday lives. From optimizing our linguistic communication to assisting us in\nmaking important decisions, LLMs, such as ChatGPT, are notably reducing our\ncognitive load by gradually taking on an increasing share of our mental\nactivities. In the context of Learning by Demonstration (LbD), classifying and\nsegmenting complex motions into primitive actions, such as pushing, pulling,\ntwisting etc, is considered to be a key-step towards encoding a task. In this\nwork, we investigate the capabilities of LLMs to undertake this task,\nconsidering a finite set of predefined primitive actions found in fruit picking\noperations. By utilizing LLMs instead of simple supervised learning or analytic\nmethods, we aim at making the method easily applicable and deployable in a\nreal-life scenario. Three different fine-tuning approaches are investigated,\ncompared on datasets captured kinesthetically, using a UR10e robot, during a\nfruit-picking scenario.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b66\u4e60\u548c\u6f14\u793a\uff08LbD\uff09\u4e2d\u5206\u7c7b\u548c\u5206\u5272\u590d\u6742\u52a8\u4f5c\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6c34\u679c\u91c7\u6458\u573a\u666f\u4e2d\u3002", "motivation": "LLMs\u80fd\u591f\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5176\u5728LbD\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u76d1\u7763\u5b66\u4e60\u6216\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u5728\u4f7f\u7528UR10e\u673a\u5668\u4eba\u6355\u83b7\u7684\u6c34\u679c\u91c7\u6458\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u65e8\u5728\u9a8c\u8bc1LLMs\u5728LbD\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "LLMs\u6709\u671b\u6210\u4e3aLbD\u4efb\u52a1\u4e2d\u5206\u7c7b\u548c\u5206\u5272\u590d\u6742\u52a8\u4f5c\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.07752", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07752", "abs": "https://arxiv.org/abs/2507.07752", "authors": ["Thanh Nguyen Canh", "Bao Nguyen Quoc", "Haolan Zhang", "Bupesh Rethinam Veeraiah", "Xiem HoangVan", "Nak Young Chong"], "title": "IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End for Visual SLAM in Challenging Environments", "comment": "In the European Conference on Mobile Robots 2025", "summary": "Robust Visual SLAM (vSLAM) is essential for autonomous systems operating in\nreal-world environments, where challenges such as dynamic objects, low texture,\nand critically, varying illumination conditions often degrade performance.\nExisting feature-based SLAM systems rely on fixed front-end parameters, making\nthem vulnerable to sudden lighting changes and unstable feature tracking. To\naddress these challenges, we propose ``IRAF-SLAM'', an Illumination-Robust and\nAdaptive Feature-Culling front-end designed to enhance vSLAM resilience in\ncomplex and challenging environments. Our approach introduces: (1) an image\nenhancement scheme to preprocess and adjust image quality under varying\nlighting conditions; (2) an adaptive feature extraction mechanism that\ndynamically adjusts detection sensitivity based on image entropy, pixel\nintensity, and gradient analysis; and (3) a feature culling strategy that\nfilters out unreliable feature points using density distribution analysis and a\nlighting impact factor. Comprehensive evaluations on the TUM-VI and European\nRobotics Challenge (EuRoC) datasets demonstrate that IRAF-SLAM significantly\nreduces tracking failures and achieves superior trajectory accuracy compared to\nstate-of-the-art vSLAM methods under adverse illumination conditions. These\nresults highlight the effectiveness of adaptive front-end strategies in\nimproving vSLAM robustness without incurring significant computational\noverhead. The implementation of IRAF-SLAM is publicly available at\nhttps://thanhnguyencanh. github.io/IRAF-SLAM/.", "AI": {"tldr": "IRAF-SLAM\u662f\u4e00\u79cd\u9488\u5bf9\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u7684\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u50cf\u589e\u5f3a\u3001\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u548c\u7279\u5f81\u7b5b\u9009\u7b56\u7565\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709SLAM\u7cfb\u7edf\u5728\u52a8\u6001\u5149\u7167\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u9002\u5e94\u524d\u7aef\u6765\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u56fe\u50cf\u589e\u5f3a\u65b9\u6848\u3001\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u673a\u5236\u548c\u7279\u5f81\u7b5b\u9009\u7b56\u7565\u3002", "result": "\u5728TUM-VI\u548cEuRoC\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51cf\u5c11\u8ddf\u8e2a\u5931\u8d25\u5e76\u63d0\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\u3002", "conclusion": "\u81ea\u9002\u5e94\u524d\u7aef\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347SLAM\u9c81\u68d2\u6027\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\u3002"}}
{"id": "2507.07794", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07794", "abs": "https://arxiv.org/abs/2507.07794", "authors": ["Zhe Han", "Huanyu Tian", "Tom Vercauteren", "Da Liu", "Changsheng Li", "Xingguang Duan"], "title": "Collaborative Human-Robot Surgery for Mandibular Angle Split Osteotomy: Optical Tracking based Approach", "comment": null, "summary": "Mandibular Angle Split Osteotomy (MASO) is a significant procedure in oral\nand maxillofacial surgery. Despite advances in technique and instrumentation,\nits success still relies heavily on the surgeon's experience. In this work, a\nhuman-robot collaborative system is proposed to perform MASO according to a\npreoperative plan and under guidance of a surgeon. A task decomposition\nmethodology is used to divide the collaborative surgical procedure into three\nsubtasks: (1) positional control and (2) orientation control, both led by the\nrobot for precise alignment; and (3) force-control, managed by surgeon to\nensure safety. Additionally, to achieve patient tracking without the need for a\nskull clamp, an optical tracking system (OTS) is utilized. Movement of the\npatient mandibular is measured with an optical-based tracker mounted on a\ndental occlusal splint. A registration method and Robot-OTS calibration method\nare introduced to achieve reliable navigation within our framework. The\nexperiments of drilling were conducted on the realistic phantom model, which\ndemonstrated that the average error between the planned and actual drilling\npoints is 1.85mm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7528\u4e8e\u4e0b\u988c\u89d2\u622a\u9aa8\u672f\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u5149\u5b66\u8ddf\u8e2a\u7cfb\u7edf\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u6280\u672f\u548c\u5668\u68b0\u6709\u6240\u8fdb\u6b65\uff0c\u4e0b\u988c\u89d2\u622a\u9aa8\u672f\u7684\u6210\u529f\u4ecd\u9ad8\u5ea6\u4f9d\u8d56\u5916\u79d1\u533b\u751f\u7684\u7ecf\u9a8c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u548c\u5b89\u5168\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u624b\u672f\u8fc7\u7a0b\u5206\u89e3\u4e3a\u673a\u5668\u4eba\u4e3b\u5bfc\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u63a7\u5236\uff0c\u4ee5\u53ca\u5916\u79d1\u533b\u751f\u4e3b\u5bfc\u7684\u529b\u63a7\u5236\uff0c\u5e76\u4f7f\u7528\u5149\u5b66\u8ddf\u8e2a\u7cfb\u7edf\u5b9e\u73b0\u60a3\u8005\u8ddf\u8e2a\u3002", "result": "\u5728\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0c\u8ba1\u5212\u4e0e\u5b9e\u9645\u94bb\u5b54\u70b9\u7684\u5e73\u5747\u8bef\u5dee\u4e3a1.85mm\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u548c\u5149\u5b66\u8ddf\u8e2a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u624b\u672f\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.07825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07825", "abs": "https://arxiv.org/abs/2507.07825", "authors": ["Leixin Chang", "Yuxuan Nai", "Hua Chen", "Liangjing Yang"], "title": "Beyond Robustness: Learning Unknown Dynamic Load Adaptation for Quadruped Locomotion on Rough Terrain", "comment": "Accepted to the 2025 IEEE International Conference on Robotics &\n  Automation (ICRA). 8 pages, 8 figures", "summary": "Unknown dynamic load carrying is one important practical application for\nquadruped robots. Such a problem is non-trivial, posing three major challenges\nin quadruped locomotion control. First, how to model or represent the dynamics\nof the load in a generic manner. Second, how to make the robot capture the\ndynamics without any external sensing. Third, how to enable the robot to\ninteract with load handling the mutual effect and stabilizing the load. In this\nwork, we propose a general load modeling approach called load characteristics\nmodeling to capture the dynamics of the load. We integrate this proposed\nmodeling technique and leverage recent advances in Reinforcement Learning (RL)\nbased locomotion control to enable the robot to infer the dynamics of load\nmovement and interact with the load indirectly to stabilize it and realize the\nsim-to-real deployment to verify its effectiveness in real scenarios. We\nconduct extensive comparative simulation experiments to validate the\neffectiveness and superiority of our proposed method. Results show that our\nmethod outperforms other methods in sudden load resistance, load stabilizing\nand locomotion with heavy load on rough terrain.\n\\href{https://leixinjonaschang.github.io/leggedloadadapt.github.io/}{Project\nPage}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8d1f\u8f7d\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u63a8\u65ad\u8d1f\u8f7d\u52a8\u6001\u5e76\u7a33\u5b9a\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u90e8\u7f72\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u52a8\u6001\u8d1f\u8f7d\u4e0b\u7684\u5efa\u6a21\u3001\u611f\u77e5\u548c\u4ea4\u4e92\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u8d1f\u8f7d\u7279\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u6280\u672f\u3002", "result": "\u5728\u7a81\u53d1\u8d1f\u8f7d\u62b5\u6297\u3001\u8d1f\u8f7d\u7a33\u5b9a\u548c\u5d0e\u5c96\u5730\u5f62\u91cd\u8f7d\u8fd0\u52a8\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u65b9\u6cd5\u6709\u6548\u4e14\u4f18\u8d8a\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.07845", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07845", "abs": "https://arxiv.org/abs/2507.07845", "authors": ["David Warutumo", "Ciira wa Maina"], "title": "Perceptual Distortions and Autonomous Representation Learning in a Minimal Robotic System", "comment": "2 authors, 23 pages, 11 figures", "summary": "Autonomous agents, particularly in the field of robotics, rely on sensory\ninformation to perceive and navigate their environment. However, these sensory\ninputs are often imperfect, leading to distortions in the agent's internal\nrepresentation of the world. This paper investigates the nature of these\nperceptual distortions and how they influence autonomous representation\nlearning using a minimal robotic system. We utilize a simulated two-wheeled\nrobot equipped with distance sensors and a compass, operating within a simple\nsquare environment. Through analysis of the robot's sensor data during random\nexploration, we demonstrate how a distorted perceptual space emerges. Despite\nthese distortions, we identify emergent structures within the perceptual space\nthat correlate with the physical environment, revealing how the robot\nautonomously learns a structured representation for navigation without explicit\nspatial information. This work contributes to the understanding of embodied\ncognition, minimal agency, and the role of perception in self-generated\nnavigation strategies in artificial life.", "AI": {"tldr": "\u7814\u7a76\u81ea\u4e3b\u673a\u5668\u4eba\u611f\u77e5\u626d\u66f2\u5bf9\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u63ed\u793a\u626d\u66f2\u611f\u77e5\u7a7a\u95f4\u4e2d\u4e0e\u7269\u7406\u73af\u5883\u76f8\u5173\u7684\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u81ea\u4e3b\u673a\u5668\u4eba\u611f\u77e5\u626d\u66f2\u5982\u4f55\u5f71\u54cd\u5176\u5185\u90e8\u4e16\u754c\u8868\u793a\u7684\u5b66\u4e60\uff0c\u4ee5\u7406\u89e3\u611f\u77e5\u5728\u81ea\u4e3b\u5bfc\u822a\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u914d\u5907\u8ddd\u79bb\u4f20\u611f\u5668\u548c\u6307\u5357\u9488\u7684\u6a21\u62df\u4e24\u8f6e\u673a\u5668\u4eba\uff0c\u5728\u7b80\u5355\u65b9\u5f62\u73af\u5883\u4e2d\u968f\u673a\u63a2\u7d22\uff0c\u5206\u6790\u4f20\u611f\u5668\u6570\u636e\u3002", "result": "\u53d1\u73b0\u626d\u66f2\u611f\u77e5\u7a7a\u95f4\u4e2d\u5b58\u5728\u4e0e\u7269\u7406\u73af\u5883\u76f8\u5173\u7684\u7ed3\u6784\uff0c\u673a\u5668\u4eba\u80fd\u81ea\u4e3b\u5b66\u4e60\u5bfc\u822a\u8868\u793a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5177\u8eab\u8ba4\u77e5\u3001\u6700\u5c0f\u4ee3\u7406\u53ca\u611f\u77e5\u5728\u4eba\u5de5\u751f\u547d\u5bfc\u822a\u7b56\u7565\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.07846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07846", "abs": "https://arxiv.org/abs/2507.07846", "authors": ["Kavindie Katuwandeniya", "Samith Rajapaksha Jayasekara Widhanapathirana"], "title": "ROS Help Desk: GenAI Powered, User-Centric Framework for ROS Error Diagnosis and Debugging", "comment": null, "summary": "As the robotics systems increasingly integrate into daily life, from smart\nhome assistants to the new-wave of industrial automation systems (Industry\n4.0), there's an increasing need to bridge the gap between complex robotic\nsystems and everyday users. The Robot Operating System (ROS) is a flexible\nframework often utilised in writing robot software, providing tools and\nlibraries for building complex robotic systems. However, ROS's distributed\narchitecture and technical messaging system create barriers for understanding\nrobot status and diagnosing errors. This gap can lead to extended maintenance\ndowntimes, as users with limited ROS knowledge may struggle to quickly diagnose\nand resolve system issues. Moreover, this deficit in expertise often delays\nproactive maintenance and troubleshooting, further increasing the frequency and\nduration of system interruptions. ROS Help Desk provides intuitive error\nexplanations and debugging support, dynamically customized to users of varying\nexpertise levels. It features user-centric debugging tools that simplify error\ndiagnosis, implements proactive error detection capabilities to reduce\ndowntime, and integrates multimodal data processing for comprehensive system\nstate understanding across multi-sensor data (e.g., lidar, RGB). Testing\nqualitatively and quantitatively with artificially induced errors demonstrates\nthe system's ability to proactively and accurately diagnose problems,\nultimately reducing maintenance time and fostering more effective human-robot\ncollaboration.", "AI": {"tldr": "ROS Help Desk\u901a\u8fc7\u76f4\u89c2\u7684\u9519\u8bef\u89e3\u91ca\u548c\u8c03\u8bd5\u652f\u6301\uff0c\u51cf\u5c11\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7ef4\u62a4\u65f6\u95f4\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u7cfb\u7edf\u878d\u5165\u65e5\u5e38\u751f\u6d3b\uff0cROS\u7684\u590d\u6742\u67b6\u6784\u548c\u6280\u672f\u6d88\u606f\u7cfb\u7edf\u5bfc\u81f4\u7528\u6237\u96be\u4ee5\u7406\u89e3\u548c\u8bca\u65ad\u9519\u8bef\uff0c\u589e\u52a0\u4e86\u7ef4\u62a4\u505c\u673a\u65f6\u95f4\u3002", "method": "ROS Help Desk\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684\u8c03\u8bd5\u5de5\u5177\uff0c\u652f\u6301\u4e3b\u52a8\u9519\u8bef\u68c0\u6d4b\uff0c\u5e76\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u4ee5\u5168\u9762\u7406\u89e3\u7cfb\u7edf\u72b6\u6001\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u80fd\u4e3b\u52a8\u51c6\u786e\u8bca\u65ad\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u7ef4\u62a4\u65f6\u95f4\u3002", "conclusion": "ROS Help Desk\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u7ef4\u62a4\u6027\u548c\u4eba\u673a\u534f\u4f5c\u6548\u679c\u3002"}}
{"id": "2507.07872", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07872", "abs": "https://arxiv.org/abs/2507.07872", "authors": ["Daniel Betschinske", "Steven Peters"], "title": "Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle", "comment": "This work has been accepted for publication at the 2025 IEEE\n  International Automated Vehicle Validation Conference (IAVVC)", "summary": "The safety validation of automatic emergency braking system (AEBS) requires\naccurately distinguishing between false positive (FP) and true positive (TP)\nsystem activations. While simulations allow straightforward differentiation by\ncomparing scenarios with and without interventions, analyzing activations from\nopen-loop resimulations - such as those from field operational testing (FOT) -\nis more complex. This complexity arises from scenario parameter uncertainty and\nthe influence of driver interventions in the recorded data. Human labeling is\nfrequently used to address these challenges, relying on subjective assessments\nof intervention necessity or situational criticality, potentially introducing\nbiases and limitations. This work proposes a rule-based classification approach\nleveraging the Prediction Divergence Principle (PDP) to address those issues.\nApplied to a simplified AEBS, the proposed method reveals key strengths,\nlimitations, and system requirements for effective implementation. The findings\nsuggest that combining this approach with human labeling may enhance the\ntransparency and consistency of classification, thereby improving the overall\nvalidation process. While the rule set for classification derived in this work\nadopts a conservative approach, the paper outlines future directions for\nrefinement and broader applicability. Finally, this work highlights the\npotential of such methods to complement existing practices, paving the way for\nmore reliable and reproducible AEBS validation frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u6d4b\u5206\u6b67\u539f\u5219\uff08PDP\uff09\u6765\u533a\u5206\u81ea\u52a8\u7d27\u6025\u5236\u52a8\u7cfb\u7edf\uff08AEBS\uff09\u7684\u8bef\u62a5\u548c\u771f\u62a5\uff0c\u4ee5\u63d0\u9ad8\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e3b\u89c2\u4eba\u4e3a\u6807\u6ce8\uff0c\u53ef\u80fd\u5f15\u5165\u504f\u89c1\u548c\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408PDP\u539f\u5219\uff0c\u5206\u6790AEBS\u7684\u6fc0\u6d3b\u60c5\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86\u5173\u952e\u4f18\u52bf\u3001\u5c40\u9650\u6027\u548c\u7cfb\u7edf\u9700\u6c42\uff0c\u7ed3\u5408\u4eba\u4e3a\u6807\u6ce8\u53ef\u63d0\u5347\u5206\u7c7b\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u671b\u8865\u5145\u73b0\u6709\u5b9e\u8df5\uff0c\u4e3a\u66f4\u53ef\u9760\u548c\u53ef\u590d\u73b0\u7684AEBS\u9a8c\u8bc1\u6846\u67b6\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.07980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07980", "abs": "https://arxiv.org/abs/2507.07980", "authors": ["Wanjia Fu", "Hongyu Li", "Ivy X. He", "Stefanie Tellex", "Srinath Sridhar"], "title": "UniTac: Whole-Robot Touch Sensing Without Tactile Sensors", "comment": null, "summary": "Robots can better interact with humans and unstructured environments through\ntouch sensing. However, most commercial robots are not equipped with tactile\nskins, making it challenging to achieve even basic touch-sensing functions,\nsuch as contact localization. We present UniTac, a data-driven whole-body\ntouch-sensing approach that uses only proprioceptive joint sensors and does not\nrequire the installation of additional sensors. Our approach enables a robot\nequipped solely with joint sensors to localize contacts. Our goal is to\ndemocratize touch sensing and provide an off-the-shelf tool for HRI researchers\nto provide their robots with touch-sensing capabilities. We validate our\napproach on two platforms: the Franka robot arm and the Spot quadruped. On\nFranka, we can localize contact to within 8.0 centimeters, and on Spot, we can\nlocalize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU\nwithout adding any additional sensors to the robot. Project website:\nhttps://ivl.cs.brown.edu/research/unitac.", "AI": {"tldr": "UniTac\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u89c9\u5173\u8282\u4f20\u611f\u5668\u7684\u6570\u636e\u9a71\u52a8\u5168\u8eab\u89e6\u89c9\u611f\u77e5\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u63a5\u89e6\u5b9a\u4f4d\u3002", "motivation": "\u5927\u591a\u6570\u5546\u7528\u673a\u5668\u4eba\u672a\u914d\u5907\u89e6\u89c9\u76ae\u80a4\uff0c\u96be\u4ee5\u5b9e\u73b0\u57fa\u672c\u89e6\u89c9\u529f\u80fd\uff08\u5982\u63a5\u89e6\u5b9a\u4f4d\uff09\uff0cUniTac\u65e8\u5728\u666e\u53ca\u89e6\u89c9\u611f\u77e5\u6280\u672f\u3002", "method": "\u5229\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u5173\u8282\u4f20\u611f\u5668\u5b9e\u73b0\u63a5\u89e6\u5b9a\u4f4d\uff0c\u65e0\u9700\u5b89\u88c5\u989d\u5916\u4f20\u611f\u5668\u3002", "result": "\u5728Franka\u673a\u68b0\u81c2\u4e0a\u5b9a\u4f4d\u7cbe\u5ea6\u4e3a8.0\u5398\u7c73\uff0cSpot\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u4e3a7.2\u5398\u7c73\uff0c\u9891\u7387\u7ea62000Hz\u3002", "conclusion": "UniTac\u4e3aHRI\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u73b0\u6210\u7684\u89e6\u89c9\u611f\u77e5\u5de5\u5177\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u3002"}}
