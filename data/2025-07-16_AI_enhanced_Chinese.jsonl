{"id": "2507.10564", "categories": ["cs.LG", "cs.AI", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10564", "abs": "https://arxiv.org/abs/2507.10564", "authors": ["Sameera Bharadwaja H.", "Siddhrath Jandial", "Shashank S. Agashe", "Rajesh Kumar Reddy Moore", "Youngkwan Kim"], "title": "Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing", "comment": null, "summary": "We consider the problem of tool-to-tool matching (TTTM), also called, chamber\nmatching in the context of a semiconductor manufacturing equipment. Traditional\nTTTM approaches utilize static configuration data or depend on a golden\nreference which are difficult to obtain in a commercial manufacturing line.\nFurther, existing methods do not extend very well to a heterogeneous setting,\nwhere equipment are of different make-and-model, sourced from different\nequipment vendors. We propose novel TTTM analysis pipelines to overcome these\nissues. We hypothesize that a mismatched equipment would have higher variance\nand/or higher number of modes in the data. Our best univariate method achieves\na correlation coefficient >0.95 and >0.5 with the variance and number of modes,\nrespectively showing that the proposed methods are effective. Also, the best\nmultivariate method achieves a correlation coefficient >0.75 with the\ntop-performing univariate methods, showing its effectiveness. Finally, we\nanalyze the sensitivity of the multivariate algorithms to the algorithm\nhyper-parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5de5\u5177\u95f4\u5339\u914d\uff08TTTM\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5f02\u6784\u8bbe\u5907\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfTTTM\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u914d\u7f6e\u6216\u9ec4\u91d1\u53c2\u8003\uff0c\u96be\u4ee5\u5728\u5546\u4e1a\u751f\u4ea7\u7ebf\u4e2d\u5e94\u7528\uff0c\u4e14\u4e0d\u9002\u7528\u4e8e\u5f02\u6784\u8bbe\u5907\u73af\u5883\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6570\u636e\u65b9\u5dee\u548c\u6a21\u5f0f\u6570\u91cf\u7684\u65b0\u5206\u6790\u6d41\u7a0b\uff0c\u5305\u62ec\u5355\u53d8\u91cf\u548c\u591a\u53d8\u91cf\u65b9\u6cd5\u3002", "result": "\u6700\u4f73\u5355\u53d8\u91cf\u65b9\u6cd5\u5728\u65b9\u5dee\u548c\u6a21\u5f0f\u6570\u91cf\u4e0a\u7684\u76f8\u5173\u7cfb\u6570\u5206\u522b>0.95\u548c>0.5\uff1b\u591a\u53d8\u91cf\u65b9\u6cd5\u4e0e\u6700\u4f73\u5355\u53d8\u91cf\u65b9\u6cd5\u7684\u76f8\u5173\u7cfb\u6570>0.75\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u5f02\u6784\u8bbe\u5907\u73af\u5883\u4e2d\u8868\u73b0\u6709\u6548\uff0c\u5e76\u5bf9\u7b97\u6cd5\u8d85\u53c2\u6570\u654f\u611f\u3002"}}
{"id": "2507.10574", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.10574", "abs": "https://arxiv.org/abs/2507.10574", "authors": ["Jae Wan Shim"], "title": "Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance", "comment": "13 pages, 2 figures", "summary": "We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel\nmeasure derived from the information theory. In comparison to the standard\ncross entropy loss function, the proposed one has an additional term that\ndepends on the predicted probability of the true class. This feature serves to\nenhance the optimization process in classification tasks involving one-hot\nencoded class labels. The proposed one has been evaluated on a ResNet-based\nmodel using the CIFAR-100 dataset. Preliminary results show that the proposed\none consistently outperforms the standard cross entropy loss function in terms\nof classification accuracy. Moreover, the proposed one maintains simplicity,\nachieving practically the same efficiency to the traditional cross entropy\nloss. These findings suggest that our approach could broaden the scope for\nfuture research into loss function design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u7ebf\u6027\u81ea\u9002\u5e94\u4ea4\u53c9\u71b5\u635f\u5931\u51fd\u6570\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u3002", "motivation": "\u6807\u51c6\u4ea4\u53c9\u71b5\u635f\u5931\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u53ef\u80fd\u4f18\u5316\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u4f9d\u8d56\u4e8e\u771f\u5b9e\u7c7b\u9884\u6d4b\u6982\u7387\u7684\u989d\u5916\u9879\uff0c\u589e\u5f3a\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\uff0cResNet\u6a21\u578b\u4e2d\u4f7f\u7528\u65b0\u635f\u5931\u51fd\u6570\u5206\u7c7b\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "\u65b0\u635f\u5931\u51fd\u6570\u7b80\u5355\u9ad8\u6548\uff0c\u4e3a\u672a\u6765\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.10575", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10575", "abs": "https://arxiv.org/abs/2507.10575", "authors": ["Kieran Chai Kai Ren"], "title": "An Adaptive Volatility-based Learning Rate Scheduler", "comment": null, "summary": "Effective learning rate (LR) scheduling is crucial for training deep neural\nnetworks. However, popular pre-defined and adaptive schedulers can still lead\nto suboptimal generalization. This paper introduces VolSched, a novel adaptive\nLR scheduler inspired by the concept of volatility in stochastic processes like\nGeometric Brownian Motion to dynamically adjust the learning rate. By\ncalculating the ratio between long-term and short-term accuracy volatility,\nVolSched increases the LR to escape plateaus and decreases it to stabilize\ntraining, allowing the model to explore the loss landscape more effectively. We\nevaluate VolSched on the CIFAR-100 dataset against a strong baseline using a\nstandard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our\nscheduler delivers consistent performance gains, improving top-1 accuracy by\n1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals\nthat VolSched promotes a longer exploration phase. A quantitative analysis of\nthe Hessian shows that VolSched finds a final solution that is 38% flatter than\nthe next-best baseline, allowing the model to obtain wider minima and hence\nbetter generalization performance.", "AI": {"tldr": "VolSched\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u6cdb\u5316\u6027\u80fd\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u52a8\u6001\u8c03\u6574\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u957f\u671f\u548c\u77ed\u671f\u51c6\u786e\u7387\u6ce2\u52a8\u6bd4\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\uff0c\u7ed3\u5408\u51e0\u4f55\u5e03\u6717\u8fd0\u52a8\u6982\u5ff5\u3002", "result": "\u5728CIFAR-100\u4e0a\uff0cResNet-18\u548cResNet-34\u7684top-1\u51c6\u786e\u7387\u5206\u522b\u63d0\u53471.4\u548c1.3\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "VolSched\u901a\u8fc7\u66f4\u957f\u7684\u63a2\u7d22\u671f\u548c\u66f4\u5e73\u5766\u7684\u635f\u5931\u66f2\u9762\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10581", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10581", "abs": "https://arxiv.org/abs/2507.10581", "authors": ["Esmail Gumaan"], "title": "Universal Approximation Theorem for a Single-Layer Transformer", "comment": "7 pages, 2 figures, 1 theorem, 10 formulas", "summary": "Deep learning employs multi-layer neural networks trained via the\nbackpropagation algorithm. This approach has achieved success across many\ndomains and relies on adaptive gradient methods such as the Adam optimizer.\nSequence modeling evolved from recurrent neural networks to attention-based\nmodels, culminating in the Transformer architecture. Transformers have achieved\nstate-of-the-art performance in natural language processing (for example, BERT\nand GPT-3) and have been applied in computer vision and computational biology.\nHowever, theoretical understanding of these models remains limited. In this\npaper, we examine the mathematical foundations of deep learning and\nTransformers and present a novel theoretical result. We review key concepts\nfrom linear algebra, probability, and optimization that underpin deep learning,\nand we analyze the multi-head self-attention mechanism and the backpropagation\nalgorithm in detail. Our main contribution is a universal approximation theorem\nfor Transformers: we prove that a single-layer Transformer, comprising one\nself-attention layer followed by a position-wise feed-forward network with ReLU\nactivation, can approximate any continuous sequence-to-sequence mapping on a\ncompact domain to arbitrary precision. We provide a formal statement and a\ncomplete proof. Finally, we present case studies that demonstrate the practical\nimplications of this result. Our findings advance the theoretical understanding\nof Transformer models and help bridge the gap between theory and practice.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6df1\u5ea6\u5b66\u4e60\u548cTransformer\u7684\u6570\u5b66\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8fd1\u4f3c\u5b9a\u7406\uff0c\u8bc1\u660e\u5355\u5c42Transformer\u53ef\u4ee5\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\u4efb\u4f55\u8fde\u7eed\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u6620\u5c04\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u548cTransformer\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u5176\u7406\u8bba\u57fa\u7840\u4ecd\u7136\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u56de\u987e\u4e86\u7ebf\u6027\u4ee3\u6570\u3001\u6982\u7387\u548c\u4f18\u5316\u7b49\u5173\u952e\u6982\u5ff5\uff0c\u8be6\u7ec6\u5206\u6790\u4e86\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8fd1\u4f3c\u5b9a\u7406\u3002", "result": "\u8bc1\u660e\u4e86\u5355\u5c42Transformer\u53ef\u4ee5\u903c\u8fd1\u4efb\u4f55\u8fde\u7eed\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u6620\u5c04\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u9648\u8ff0\u548c\u5b8c\u6574\u8bc1\u660e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63a8\u52a8\u4e86Transformer\u6a21\u578b\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u5e2e\u52a9\u7f29\u5c0f\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.10602", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10602", "abs": "https://arxiv.org/abs/2507.10602", "authors": ["Maximilian St\u00f6lzle", "T. Konstantin Rusch", "Zach J. Patterson", "Rodrigo P\u00e9rez-Dattari", "Francesco Stella", "Josie Hughes", "Cosimo Della Santina", "Daniela Rus"], "title": "Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees", "comment": "73 pages", "summary": "Learning from demonstration provides a sample-efficient approach to acquiring\ncomplex behaviors, enabling robots to move robustly, compliantly, and with\nfluidity. In this context, Dynamic Motion Primitives offer built - in stability\nand robustness to disturbances but often struggle to capture complex periodic\nbehaviors. Moreover, they are limited in their ability to interpolate between\ndifferent tasks. These shortcomings substantially narrow their applicability,\nexcluding a wide class of practically meaningful tasks such as locomotion and\nrhythmic tool use. In this work, we introduce Orbitally Stable Motion\nPrimitives (OSMPs) - a framework that combines a learned diffeomorphic encoder\nwith a supercritical Hopf bifurcation in latent space, enabling the accurate\nacquisition of periodic motions from demonstrations while ensuring formal\nguarantees of orbital stability and transverse contraction. Furthermore, by\nconditioning the bijective encoder on the task, we enable a single learned\npolicy to represent multiple motion objectives, yielding consistent zero-shot\ngeneralization to unseen motion objectives within the training distribution. We\nvalidate the proposed approach through extensive simulation and real-world\nexperiments across a diverse range of robotic platforms - from collaborative\narms and soft manipulators to a bio-inspired rigid-soft turtle robot -\ndemonstrating its versatility and effectiveness in consistently outperforming\nstate-of-the-art baselines such as diffusion policies, among others.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u9053\u7a33\u5b9a\u8fd0\u52a8\u57fa\u5143\uff08OSMPs\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u5468\u671f\u6027\u8fd0\u52a8\uff0c\u5e76\u786e\u4fdd\u8f68\u9053\u7a33\u5b9a\u6027\u548c\u6a2a\u5411\u6536\u7f29\u6027\u3002", "motivation": "\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff08DMPs\uff09\u5728\u6355\u6349\u590d\u6742\u5468\u671f\u6027\u884c\u4e3a\u548c\u4efb\u52a1\u95f4\u63d2\u503c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "\u7ed3\u5408\u5b66\u4e60\u7684\u5fae\u5206\u540c\u80da\u7f16\u7801\u5668\u548c\u8d85\u4e34\u754cHopf\u5206\u5c94\uff0c\u5b9e\u73b0\u5468\u671f\u6027\u8fd0\u52a8\u7684\u51c6\u786e\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u6761\u4ef6\u5316\u7f16\u7801\u5668\u5b9e\u73b0\u591a\u4efb\u52a1\u8868\u793a\u3002", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86OSMPs\u7684\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u3002", "conclusion": "OSMPs\u5728\u5b66\u4e60\u548c\u6cdb\u5316\u5468\u671f\u6027\u8fd0\u52a8\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10689", "abs": "https://arxiv.org/abs/2507.10689", "authors": ["Tongshun Zhang", "Pingping Liu", "Yubing Lu", "Mengen Cai", "Zijian Zhang", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement", "comment": "Accepted by ICCV 2025", "summary": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on\nuniform brightness adjustment, often neglecting instance-level semantic\ninformation and the inherent characteristics of different features. To address\nthese limitations, we propose CWNet (Causal Wavelet Network), a novel\narchitecture that leverages wavelet transforms for causal reasoning.\nSpecifically, our approach comprises two key components: 1) Inspired by the\nconcept of intervention in causality, we adopt a causal reasoning perspective\nto reveal the underlying causal relationships in low-light enhancement. From a\nglobal perspective, we employ a metric learning strategy to ensure causal\nembeddings adhere to causal principles, separating them from non-causal\nconfounding factors while focusing on the invariance of causal factors. At the\nlocal level, we introduce an instance-level CLIP semantic loss to precisely\nmaintain causal factor consistency. 2) Based on our causal analysis, we present\na wavelet transform-based backbone network that effectively optimizes the\nrecovery of frequency information, ensuring precise enhancement tailored to the\nspecific attributes of wavelet transforms. Extensive experiments demonstrate\nthat CWNet significantly outperforms current state-of-the-art methods across\nmultiple datasets, showcasing its robust performance across diverse scenes.\nCode is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.", "AI": {"tldr": "CWNet\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u548c\u56e0\u679c\u63a8\u7406\u63d0\u5347\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u5168\u5c40\u5ea6\u91cf\u5b66\u4e60\u548c\u5c40\u90e8\u8bed\u4e49\u635f\u5931\u4f18\u5316\u56e0\u679c\u56e0\u7d20\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5ffd\u89c6\u5b9e\u4f8b\u7ea7\u8bed\u4e49\u4fe1\u606f\u548c\u7279\u5f81\u7279\u6027\uff0cCWNet\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u548c\u5c0f\u6ce2\u53d8\u6362\uff0c\u5305\u62ec\u5168\u5c40\u5ea6\u91cf\u5b66\u4e60\u548c\u5c40\u90e8CLIP\u8bed\u4e49\u635f\u5931\uff0c\u8bbe\u8ba1\u5c0f\u6ce2\u53d8\u6362\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "CWNet\u901a\u8fc7\u56e0\u679c\u63a8\u7406\u548c\u5c0f\u6ce2\u53d8\u6362\u6709\u6548\u63d0\u5347\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u6027\u80fd\u3002"}}
{"id": "2507.10591", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.PF", "68T01", "I.2"], "pdf": "https://arxiv.org/pdf/2507.10591", "abs": "https://arxiv.org/abs/2507.10591", "authors": ["Vanderson Rocha", "Diego Kreutz", "Gabriel Canto", "Hendrio Bragan\u00e7a", "Eduardo Feitosa"], "title": "MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation", "comment": "11 pages; 4 figures; 5 tables; submitted to JBCS", "summary": "Feature selection is vital for building effective predictive models, as it\nreduces dimensionality and emphasizes key features. However, current research\noften suffers from limited benchmarking and reliance on proprietary datasets.\nThis severely hinders reproducibility and can negatively impact overall\nperformance. To address these limitations, we introduce the MH-FSF framework, a\ncomprehensive, modular, and extensible platform designed to facilitate the\nreproduction and implementation of feature selection methods. Developed through\ncollaborative research, MH-FSF provides implementations of 17 methods (11\nclassical, 6 domain-specific) and enables systematic evaluation on 10 publicly\navailable Android malware datasets. Our results reveal performance variations\nacross both balanced and imbalanced datasets, highlighting the critical need\nfor data preprocessing and selection criteria that account for these\nasymmetries. We demonstrate the importance of a unified platform for comparing\ndiverse feature selection techniques, fostering methodological consistency and\nrigor. By providing this framework, we aim to significantly broaden the\nexisting literature and pave the way for new research directions in feature\nselection, particularly within the context of Android malware detection.", "AI": {"tldr": "MH-FSF\u6846\u67b6\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u7528\u4e8e\u5b9e\u73b0\u548c\u6bd4\u8f83\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9Android\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u3002", "motivation": "\u5f53\u524d\u7279\u5f81\u9009\u62e9\u7814\u7a76\u5b58\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0d\u8db3\u548c\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u53ef\u91cd\u590d\u6027\u548c\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86MH-FSF\u6846\u67b6\uff0c\u5305\u542b17\u79cd\u7279\u5f81\u9009\u62e9\u65b9\u6cd5\uff0811\u79cd\u7ecf\u5178\u65b9\u6cd5\uff0c6\u79cd\u9886\u57df\u7279\u5b9a\u65b9\u6cd5\uff09\uff0c\u5e76\u572810\u4e2a\u516c\u5f00\u7684Android\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u9884\u5904\u7406\u548c\u9009\u62e9\u6807\u51c6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "MH-FSF\u6846\u67b6\u4e3a\u7279\u5f81\u9009\u62e9\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u5e73\u53f0\uff0c\u4fc3\u8fdb\u4e86\u65b9\u6cd5\u4e00\u81f4\u6027\u548c\u4e25\u8c28\u6027\uff0c\u5e76\u62d3\u5bbd\u4e86\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.10672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10672", "abs": "https://arxiv.org/abs/2507.10672", "authors": ["Muhayy Ud Din", "Waseem Akram", "Lyes Saad Saoud", "Jan Rosell", "Irfan Hussain"], "title": "Vision Language Action Models in Robotic Manipulation: A Systematic Review", "comment": "submitted to annual review in control", "summary": "Vision Language Action (VLA) models represent a transformative shift in\nrobotics, with the aim of unifying visual perception, natural language\nunderstanding, and embodied control within a single learning framework. This\nreview presents a comprehensive and forward-looking synthesis of the VLA\nparadigm, with a particular emphasis on robotic manipulation and\ninstruction-driven autonomy. We comprehensively analyze 102 VLA models, 26\nfoundational datasets, and 12 simulation platforms that collectively shape the\ndevelopment and evaluation of VLAs models. These models are categorized into\nkey architectural paradigms, each reflecting distinct strategies for\nintegrating vision, language, and control in robotic systems. Foundational\ndatasets are evaluated using a novel criterion based on task complexity,\nvariety of modalities, and dataset scale, allowing a comparative analysis of\ntheir suitability for generalist policy learning. We introduce a\ntwo-dimensional characterization framework that organizes these datasets based\non semantic richness and multimodal alignment, showing underexplored regions in\nthe current data landscape. Simulation environments are evaluated for their\neffectiveness in generating large-scale data, as well as their ability to\nfacilitate transfer from simulation to real-world settings and the variety of\nsupported tasks. Using both academic and industrial contributions, we recognize\nongoing challenges and outline strategic directions such as scalable\npretraining protocols, modular architectural design, and robust multimodal\nalignment strategies. This review serves as both a technical reference and a\nconceptual roadmap for advancing embodiment and robotic control, providing\ninsights that span from dataset generation to real world deployment of\ngeneralist robotic agents.", "AI": {"tldr": "VLA\u6a21\u578b\u7edf\u4e00\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u672c\u6587\u7efc\u8ff0\u4e86102\u4e2a\u6a21\u578b\u300126\u4e2a\u6570\u636e\u96c6\u548c12\u4e2a\u4eff\u771f\u5e73\u53f0\uff0c\u63d0\u51fa\u5206\u7c7b\u6846\u67b6\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7VLA\u6a21\u578b\u7edf\u4e00\u89c6\u89c9\u611f\u77e5\u3001\u8bed\u8a00\u7406\u89e3\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u6280\u672f\u7684\u901a\u7528\u5316\u53d1\u5c55\u3002", "method": "\u5206\u7c7b\u5206\u6790VLA\u6a21\u578b\u7684\u67b6\u6784\u8303\u5f0f\uff0c\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u4eff\u771f\u5e73\u53f0\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4ef7\u6807\u51c6\u3002", "result": "\u63ed\u793a\u4e86\u5f53\u524d\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u6a21\u6001\u5bf9\u9f50\u7b49\u672a\u6765\u65b9\u5411\u3002", "conclusion": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6570\u636e\u3001\u67b6\u6784\u548c\u8fc1\u79fb\u7b49\u6311\u6218\u3002"}}
{"id": "2507.10737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10737", "abs": "https://arxiv.org/abs/2507.10737", "authors": ["Jiayuan Chen", "Thai-Hoang Pham", "Yuanlong Wang", "Ping Zhang"], "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines", "comment": "ICCV 2025", "summary": "High-throughput screening techniques, such as microscopy imaging of cellular\nresponses to genetic and chemical perturbations, play a crucial role in drug\ndiscovery and biomedical research. However, robust perturbation screening for\n\\textit{de novo} cell lines remains challenging due to the significant\nmorphological and biological heterogeneity across cell lines. To address this,\nwe propose a novel framework that integrates external biological knowledge into\nexisting pretraining strategies to enhance microscopy image profiling models.\nOur approach explicitly disentangles perturbation-specific and cell\nline-specific representations using external biological information.\nSpecifically, we construct a knowledge graph leveraging protein interaction\ndata from STRING and Hetionet databases to guide models toward\nperturbation-specific features during pretraining. Additionally, we incorporate\ntranscriptomic features from single-cell foundation models to capture cell\nline-specific representations. By learning these disentangled features, our\nmethod improves the generalization of imaging models to \\textit{de novo} cell\nlines. We evaluate our framework on the RxRx database through one-shot\nfine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from\nthe RxRx19a dataset. Experimental results demonstrate that our method enhances\nmicroscopy image profiling for \\textit{de novo} cell lines, highlighting its\neffectiveness in real-world phenotype-based drug discovery applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u5916\u90e8\u751f\u7269\u77e5\u8bc6\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u589e\u5f3a\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u6a21\u578b\u5728\u65b0\u578b\u7ec6\u80de\u7cfb\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u65b0\u578b\u7ec6\u80de\u7cfb\u5728\u6270\u52a8\u7b5b\u9009\u4e2d\u56e0\u5f62\u6001\u548c\u751f\u7269\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u6570\u636e\u548c\u8f6c\u5f55\u7ec4\u7279\u5f81\uff0c\u89e3\u8026\u6270\u52a8\u7279\u5f02\u6027\u548c\u7ec6\u80de\u7cfb\u7279\u5f02\u6027\u8868\u5f81\u3002", "result": "\u5728RxRx\u6570\u636e\u5e93\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u5728\u65b0\u578b\u7ec6\u80de\u7cfb\u4e2d\u7684\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8868\u578b\u836f\u7269\u53d1\u73b0\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.10594", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10594", "abs": "https://arxiv.org/abs/2507.10594", "authors": ["Shengda Zhuo", "Di Wu", "Yi He", "Shuqiang Huang", "Xindong Wu"], "title": "Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features", "comment": null, "summary": "Online learning, where feature spaces can change over time, offers a flexible\nlearning paradigm that has attracted considerable attention. However, it still\nfaces three significant challenges. First, the heterogeneity of real-world data\nstreams with mixed feature types presents challenges for traditional parametric\nmodeling. Second, data stream distributions can shift over time, causing an\nabrupt and substantial decline in model performance. Third, it is often\ninfeasible to label every data instance due to time and cost constraints. To\naddress these issues, we proposed OL-MDISF (Online Learning from Mix-typed,\nDrifted, and Incomplete Streaming Features), which constructs a latent\ncopula-based representation for heterogeneous features, detects drifts via\nensemble entropy and latent mismatch, and performs structure-aware\npseudo-labeling.\n  This companion paper serves as a standalone technical reference to OL-MDISF.\nIt provides a contextual discussion of related work in mixed-type modeling,\ndrift adaptation, and weak supervision, as well as a comprehensive set of\nexperiments across 14 real-world datasets under two types of drift scenarios.\nThese include CER trends, ablation studies, sensitivity analyses, and temporal\nensemble dynamics. We hope this document offers a reproducible benchmark for\nonline learning on complex, weakly supervised streaming data.", "AI": {"tldr": "OL-MDISF\u662f\u4e00\u79cd\u5728\u7ebf\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u6df7\u5408\u7c7b\u578b\u3001\u6f02\u79fb\u548c\u4e0d\u5b8c\u6574\u6570\u636e\u6d41\u7684\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u5b66\u4e60\u4e2d\u6570\u636e\u6d41\u6df7\u5408\u7c7b\u578b\u3001\u5206\u5e03\u6f02\u79fb\u548c\u6807\u7b7e\u7f3a\u5931\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u6f5c\u5728copula\u7684\u8868\u793a\uff0c\u901a\u8fc7\u96c6\u6210\u71b5\u548c\u6f5c\u5728\u4e0d\u5339\u914d\u68c0\u6d4b\u6f02\u79fb\uff0c\u5e76\u8fdb\u884c\u7ed3\u6784\u611f\u77e5\u4f2a\u6807\u8bb0\u3002", "result": "\u572814\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5305\u62ecCER\u8d8b\u52bf\u3001\u6d88\u878d\u7814\u7a76\u548c\u654f\u611f\u6027\u5206\u6790\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u3001\u5f31\u76d1\u7763\u7684\u6d41\u6570\u636e\u5728\u7ebf\u5b66\u4e60\u3002"}}
{"id": "2507.10694", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10694", "abs": "https://arxiv.org/abs/2507.10694", "authors": ["Francesco Fuentes", "Serigne Diagne", "Zachary Kingston", "Laura H. Blumenschein"], "title": "Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots", "comment": "22 pages, 21 figures, submitted to journal for potential publication", "summary": "Passive deformation due to compliance is a commonly used benefit of soft\nrobots, providing opportunities to achieve robust actuation with few active\ndegrees of freedom. Soft growing robots in particular have shown promise in\nnavigation of unstructured environments due to their passive deformation. If\ntheir collisions and subsequent deformations can be better understood, soft\nrobots could be used to understand the structure of the environment from direct\ntactile measurements. In this work, we propose the use of soft growing robots\nas mapping and exploration tools. We do this by first characterizing collision\nbehavior during discrete turns, then leveraging this model to develop a\ngeometry-based simulator that models robot trajectories in 2D environments.\nFinally, we demonstrate the model and simulator validity by mapping unknown\nenvironments using Monte Carlo sampling to estimate the optimal next deployment\ngiven current knowledge. Over both uniform and non-uniform environments, this\nselection method rapidly approaches ideal actions, showing the potential for\nsoft growing robots in unstructured environment exploration and mapping.", "AI": {"tldr": "\u5229\u7528\u8f6f\u4f53\u751f\u957f\u673a\u5668\u4eba\u8fdb\u884c\u73af\u5883\u63a2\u7d22\u548c\u5730\u56fe\u6784\u5efa\uff0c\u901a\u8fc7\u78b0\u649e\u884c\u4e3a\u5efa\u6a21\u548c\u51e0\u4f55\u6a21\u62df\u5668\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u56e0\u5176\u88ab\u52a8\u53d8\u5f62\u7279\u6027\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8981\u66f4\u597d\u5730\u7406\u89e3\u78b0\u649e\u548c\u53d8\u5f62\u884c\u4e3a\u4ee5\u5b9e\u73b0\u73af\u5883\u7ed3\u6784\u7684\u76f4\u63a5\u89e6\u89c9\u6d4b\u91cf\u3002", "method": "\u9996\u5148\u8868\u5f81\u79bb\u6563\u8f6c\u5411\u4e2d\u7684\u78b0\u649e\u884c\u4e3a\uff0c\u5f00\u53d1\u57fa\u4e8e\u51e0\u4f55\u7684\u6a21\u62df\u5668\u6a21\u62df2D\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u6700\u540e\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u4f30\u8ba1\u6700\u4f18\u90e8\u7f72\u7b56\u7565\u3002", "result": "\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u5feb\u901f\u903c\u8fd1\u7406\u60f3\u52a8\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u8f6f\u4f53\u751f\u957f\u673a\u5668\u4eba\u5728\u73af\u5883\u63a2\u7d22\u548c\u5730\u56fe\u6784\u5efa\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8f6f\u4f53\u751f\u957f\u673a\u5668\u4eba\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u73af\u5883\u63a2\u7d22\u548c\u5730\u56fe\u6784\u5efa\u5de5\u5177\uff0c\u5c24\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.10755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10755", "abs": "https://arxiv.org/abs/2507.10755", "authors": ["Rina Khan", "Catherine Stinson"], "title": "Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias", "comment": null, "summary": "Facial expression recognition (FER) algorithms classify facial expressions\ninto emotions such as happy, sad, or angry. An evaluative challenge facing FER\nalgorithms is the fall in performance when detecting spontaneous expressions\ncompared to posed expressions. An ethical (and evaluative) challenge facing FER\nalgorithms is that they tend to perform poorly for people of some races and\nskin colors. These challenges are linked to the data collection practices\nemployed in the creation of FER datasets. In this study, we audit two\nstate-of-the-art FER datasets. We take random samples from each dataset and\nexamine whether images are spontaneous or posed. In doing so, we propose a\nmethodology for identifying spontaneous or posed images. We discover a\nsignificant number of images that were posed in the datasets purporting to\nconsist of in-the-wild images. Since performance of FER models vary between\nspontaneous and posed images, the performance of models trained on these\ndatasets will not represent the true performance if such models were to be\ndeployed in in-the-wild applications. We also observe the skin color of\nindividuals in the samples, and test three models trained on each of the\ndatasets to predict facial expressions of people from various races and skin\ntones. We find that the FER models audited were more likely to predict people\nlabeled as not white or determined to have dark skin as showing a negative\nemotion such as anger or sadness even when they were smiling. This bias makes\nsuch models prone to perpetuate harm in real life applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u7b97\u6cd5\u5728\u68c0\u6d4b\u81ea\u53d1\u8868\u60c5\u548c\u4e0d\u540c\u79cd\u65cf\u80a4\u8272\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5ba1\u8ba1\u4e86\u4e24\u4e2aFER\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u591a\u4e3a\u6446\u62cd\u800c\u975e\u771f\u5b9e\u573a\u666f\uff0c\u4e14\u6a21\u578b\u5bf9\u975e\u767d\u4eba\u6216\u6df1\u80a4\u8272\u4eba\u7fa4\u5b58\u5728\u504f\u89c1\u3002", "motivation": "FER\u7b97\u6cd5\u5728\u68c0\u6d4b\u81ea\u53d1\u8868\u60c5\u548c\u4e0d\u540c\u79cd\u65cf\u80a4\u8272\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u5b58\u5728\u4f26\u7406\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u5ba1\u8ba1\u6570\u636e\u96c6\u548c\u6a21\u578b\u6027\u80fd\u3002", "method": "\u968f\u673a\u62bd\u6837\u4e24\u4e2aFER\u6570\u636e\u96c6\uff0c\u5206\u6790\u56fe\u50cf\u662f\u81ea\u53d1\u8fd8\u662f\u6446\u62cd\uff0c\u5e76\u6d4b\u8bd5\u6a21\u578b\u5bf9\u4e0d\u540c\u79cd\u65cf\u548c\u80a4\u8272\u4eba\u7fa4\u7684\u9884\u6d4b\u504f\u5dee\u3002", "result": "\u53d1\u73b0\u6570\u636e\u96c6\u4e2d\u8bb8\u591a\u56fe\u50cf\u4e3a\u6446\u62cd\uff0c\u4e14\u6a21\u578b\u5bf9\u975e\u767d\u4eba\u6216\u6df1\u80a4\u8272\u4eba\u7fa4\u5b58\u5728\u8d1f\u9762\u60c5\u7eea\u9884\u6d4b\u504f\u89c1\u3002", "conclusion": "\u6570\u636e\u96c6\u548c\u6a21\u578b\u5b58\u5728\u504f\u5dee\uff0c\u53ef\u80fd\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4f26\u7406\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2507.10595", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10595", "abs": "https://arxiv.org/abs/2507.10595", "authors": ["Yaowen Hu", "Wenxuan Tu", "Yue Liu", "Miaomiao Li", "Wenpeng Lu", "Zhigang Luo", "Xinwang Liu", "Ping Chen"], "title": "Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs", "comment": null, "summary": "Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised\ntask aimed at partitioning nodes with incomplete attributes into distinct\nclusters. Addressing this challenging issue is vital for practical\napplications. However, research in this area remains underexplored. Existing\nimputation methods for attribute-missing graphs often fail to account for the\nvarying amounts of information available across node neighborhoods, leading to\nunreliable results, especially for nodes with insufficient known neighborhood.\nTo address this issue, we propose a novel method named Divide-Then-Rule Graph\nCompletion (DTRGC). This method first addresses nodes with sufficient known\nneighborhood information and treats the imputed results as new knowledge to\niteratively impute more challenging nodes, while leveraging clustering\ninformation to correct imputation errors. Specifically, Dynamic Cluster-Aware\nFeature Propagation (DCFP) initializes missing node attributes by adjusting\npropagation weights based on the clustering structure. Subsequently,\nHierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing\nnodes into three groups based on the completeness of their neighborhood\nattributes. The imputation is performed hierarchically, prioritizing the groups\nwith nodes that have the most available neighborhood information. The cluster\nstructure is then used to refine the imputation and correct potential errors.\nFinally, Hop-wise Representation Enhancement (HRE) integrates information\nacross multiple hops, thereby enriching the expressiveness of node\nrepresentations. Experimental results on six widely used graph datasets show\nthat DTRGC significantly improves the clustering performance of various DGC\nmethods under attribute-missing graphs.", "AI": {"tldr": "DTRGC\u65b9\u6cd5\u901a\u8fc7\u5206\u9636\u6bb5\u8865\u5168\u7f3a\u5931\u5c5e\u6027\u5e76\u5229\u7528\u805a\u7c7b\u4fe1\u606f\u7ea0\u6b63\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c5e\u6027\u7f3a\u5931\u56fe\u4e0a\u7684\u6df1\u5ea6\u56fe\u805a\u7c7b\u6027\u80fd\u3002", "motivation": "\u5c5e\u6027\u7f3a\u5931\u56fe\u7684\u6df1\u5ea6\u56fe\u805a\u7c7b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u8282\u70b9\u90bb\u57df\u4fe1\u606f\u91cf\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faDTRGC\u65b9\u6cd5\uff0c\u5305\u62ec\u52a8\u6001\u805a\u7c7b\u611f\u77e5\u7279\u5f81\u4f20\u64ad\uff08DCFP\uff09\u3001\u5206\u5c42\u90bb\u57df\u611f\u77e5\u8865\u5168\uff08HNAI\uff09\u548c\u8df3\u6570\u8868\u793a\u589e\u5f3a\uff08HRE\uff09\u3002", "result": "\u5728\u516d\u4e2a\u5e38\u7528\u56fe\u6570\u636e\u96c6\u4e0a\uff0cDTRGC\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u6df1\u5ea6\u56fe\u805a\u7c7b\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "DTRGC\u901a\u8fc7\u5206\u9636\u6bb5\u8865\u5168\u548c\u805a\u7c7b\u4fe1\u606f\u6821\u6b63\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c5e\u6027\u7f3a\u5931\u56fe\u7684\u805a\u7c7b\u95ee\u9898\u3002"}}
{"id": "2507.10749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10749", "abs": "https://arxiv.org/abs/2507.10749", "authors": ["Benjamin Stoler", "Juliet Yang", "Jonathan Francis", "Jean Oh"], "title": "RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding", "comment": null, "summary": "Safety-critical scenarios are essential for training and evaluating\nautonomous driving (AD) systems, yet remain extremely rare in real-world\ndriving datasets. To address this, we propose Real-world Crash Grounding (RCG),\na scenario generation framework that integrates crash-informed semantics into\nadversarial perturbation pipelines. We construct a safety-aware behavior\nrepresentation through contrastive pre-training on large-scale driving logs,\nfollowed by fine-tuning on a small, crash-rich dataset with approximate\ntrajectory annotations extracted from video. This embedding captures semantic\nstructure aligned with real-world accident behaviors and supports selection of\nadversary trajectories that are both high-risk and behaviorally realistic. We\nincorporate the resulting selection mechanism into two prior scenario\ngeneration pipelines, replacing their handcrafted scoring objectives with an\nembedding-based criterion. Experimental results show that ego agents trained\nagainst these generated scenarios achieve consistently higher downstream\nsuccess rates, with an average improvement of 9.2% across seven evaluation\nsettings. Qualitative and quantitative analyses further demonstrate that our\napproach produces more plausible and nuanced adversary behaviors, enabling more\neffective and realistic stress testing of AD systems. Code and tools will be\nreleased publicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRCG\u7684\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u78b0\u649e\u4fe1\u606f\u878d\u5165\u5bf9\u6297\u6270\u52a8\u6d41\u7a0b\uff0c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u9ad8\u98ce\u9669\u9a7e\u9a76\u573a\u666f\u3002", "motivation": "\u73b0\u5b9e\u9a7e\u9a76\u6570\u636e\u4e2d\u5b89\u5168\u5173\u952e\u573a\u666f\u7a00\u7f3a\uff0c\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u7ed3\u5408\u5bf9\u6bd4\u9884\u8bad\u7ec3\u548c\u5c11\u91cf\u78b0\u649e\u6570\u636e\u5fae\u8c03\uff0c\u6784\u5efa\u5b89\u5168\u611f\u77e5\u7684\u884c\u4e3a\u8868\u793a\uff0c\u5e76\u5d4c\u5165\u5230\u73b0\u6709\u573a\u666f\u751f\u6210\u6d41\u7a0b\u4e2d\u3002", "result": "\u751f\u6210\u7684\u573a\u666f\u4f7f\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u5e73\u5747\u63d0\u53479.2%\uff0c\u4e14\u5bf9\u6297\u884c\u4e3a\u66f4\u771f\u5b9e\u3002", "conclusion": "RCG\u6846\u67b6\u80fd\u751f\u6210\u66f4\u6709\u6548\u7684\u538b\u529b\u6d4b\u8bd5\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u8bc4\u4f30\u6548\u679c\u3002"}}
{"id": "2507.10770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10770", "abs": "https://arxiv.org/abs/2507.10770", "authors": ["Ionu\u0163 Grigore", "C\u0103lin-Adrian Popa", "Claudiu Leoveanu-Condrei"], "title": "FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching", "comment": null, "summary": "The extraction and matching of interest points are fundamental to many\ngeometric computer vision tasks. Traditionally, matching is performed by\nassigning descriptors to interest points and identifying correspondences based\non descriptor similarity. This work introduces a technique where interest\npoints are inherently associated during detection, eliminating the need for\ncomputing, storing, transmitting, or matching descriptors. Although the\nmatching accuracy is marginally lower than that of conventional approaches, our\nmethod completely eliminates the need for descriptors, leading to a drastic\nreduction in memory usage for localization systems. We assess its effectiveness\nby comparing it against both classical handcrafted methods and modern learned\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u63cf\u8ff0\u7b26\u7684\u5174\u8da3\u70b9\u5339\u914d\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u8ba1\u7b97\u548c\u5339\u914d\u63cf\u8ff0\u7b26\uff0c\u5185\u5b58\u5f00\u9500\u5927\u3002", "method": "\u5728\u68c0\u6d4b\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5173\u8054\u5174\u8da3\u70b9\uff0c\u7701\u53bb\u63cf\u8ff0\u7b26\u6b65\u9aa4\u3002", "result": "\u5339\u914d\u7cbe\u5ea6\u7565\u4f4e\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4f46\u5185\u5b58\u4f7f\u7528\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5185\u5b58\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u548c\u73b0\u4ee3\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.10605", "categories": ["cs.LG", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.10605", "abs": "https://arxiv.org/abs/2507.10605", "authors": ["Fei Zhao", "Chonggang Lu", "Yue Wang", "Zheyong Xie", "Ziyan Liu", "Haofu Qian", "JianZhao Huang", "Fangcheng Shi", "Zijie Meng", "Hongcheng Guo", "Mingqian He", "Xinze Lyu", "Yiming Lu", "Ziyang Xiang", "Zheyu Ye", "Chengqiang Lu", "Zhe Xu", "Yi Wu", "Yao Hu", "Yan Gao", "Jun Fan", "Xiaolong Jiang", "Weiting Liu", "Boyang Wang", "Shaosheng Cao"], "title": "RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services", "comment": null, "summary": "As a primary medium for modern information dissemination, social networking\nservices (SNS) have experienced rapid growth, which has proposed significant\nchallenges for platform content management and interaction quality improvement.\nRecently, the development of large language models (LLMs) has offered potential\nsolutions but existing studies focus on isolated tasks, which not only\nencounter diminishing benefit from the data scaling within individual scenarios\nbut also fail to flexibly adapt to diverse real-world context. To address these\nchallenges, we introduce RedOne, a domain-specific LLM designed to break the\nperformance bottleneck of single-task baselines and establish a comprehensive\nfoundation for the SNS. RedOne was developed through a three-stage training\nstrategy consisting of continue pretraining, supervised fine-tuning, and\npreference optimization, using a large-scale real-world dataset. Through\nextensive experiments, RedOne maintains strong general capabilities, and\nachieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%\nin SNS bilingual evaluation benchmark, compared with base models. Furthermore,\nthrough online testing, RedOne reduced the exposure rate in harmful content\ndetection by 11.23% and improved the click page rate in post-view search by\n14.95% compared with single-tasks finetuned baseline models. These results\nestablish RedOne as a robust domain-specific LLM for SNS, demonstrating\nexcellent generalization across various tasks and promising applicability in\nreal-world scenarios.", "AI": {"tldr": "RedOne\u662f\u4e00\u4e2a\u9488\u5bf9\u793e\u4ea4\u7f51\u7edc\u670d\u52a1\uff08SNS\uff09\u7684\u9886\u57df\u7279\u5b9a\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5728SNS\u5185\u5bb9\u7ba1\u7406\u548c\u4ea4\u4e92\u8d28\u91cf\u63d0\u5347\u4e2d\u9762\u4e34\u7684\u5355\u4efb\u52a1\u5c40\u9650\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u504f\u597d\u4f18\u5316\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u3002", "result": "\u57288\u4e2a\u4e3b\u8981SNS\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u534714.02%\uff0c\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u66dd\u5149\u7387\u964d\u4f4e11.23%\uff0c\u5e16\u5b50\u641c\u7d22\u70b9\u51fb\u7387\u63d0\u9ad814.95%\u3002", "conclusion": "RedOne\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u9886\u57df\u7279\u5b9aLLM\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10776", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10776", "abs": "https://arxiv.org/abs/2507.10776", "authors": ["Howard H. Qian", "Yiting Chen", "Gaotian Wang", "Podshara Chanrungmaneekul", "Kaiyu Hang"], "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding", "comment": "8 pages, IROS 2025, Interactive Perception, Segmentation, Robotics,\n  Computer Vision", "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u4ea4\u4e92\u611f\u77e5\u6846\u67b6rt-RISeg\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u4ea4\u4e92\u548c\u8bbe\u8ba1\u7684\u4f53\u5e27\u4e0d\u53d8\u7279\u5f81\uff08BFIF\uff09\u8fde\u7eed\u5206\u5272\u672a\u89c1\u7269\u4f53\uff0c\u65e0\u9700\u5b66\u4e60\u5206\u5272\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u672a\u89c1\u7269\u4f53\u5b9e\u4f8b\u5206\u5272\uff08UOIS\uff09\u65b9\u6cd5\u56e0\u4f9d\u8d56\u9759\u6001\u89c6\u89c9\u7279\u5f81\u800c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u673a\u5668\u4eba\u4ea4\u4e92\u4ea7\u751f\u7684\u76f8\u5bf9\u65cb\u8f6c\u548c\u7ebf\u6027\u901f\u5ea6\u8bc6\u522b\u7269\u4f53\uff0c\u5b9e\u65f6\u751f\u6210\u548c\u66f4\u65b0\u5206\u5272\u63a9\u7801\u3002", "result": "\u5e73\u5747\u5206\u5272\u51c6\u786e\u7387\u6bd4\u73b0\u6709UOIS\u65b9\u6cd5\u9ad827.5%\uff0c\u4e14\u5206\u5272\u63a9\u7801\u53ef\u4f5c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "rt-RISeg\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u7269\u4f53\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10775", "abs": "https://arxiv.org/abs/2507.10775", "authors": ["Jeffrey Joan Sam", "Janhavi Sathe", "Nikhil Chigali", "Naman Gupta", "Radhey Ruparel", "Yicheng Jiang", "Janmajay Singh", "James W. Berck", "Arko Barman"], "title": "A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers", "comment": null, "summary": "Spacecraft deployed in outer space are routinely subjected to various forms\nof damage due to exposure to hazardous environments. In addition, there are\nsignificant risks to the subsequent process of in-space repairs through human\nextravehicular activity or robotic manipulation, incurring substantial\noperational costs. Recent developments in image segmentation could enable the\ndevelopment of reliable and cost-effective autonomous inspection systems. While\nthese models often require large amounts of training data to achieve\nsatisfactory results, publicly available annotated spacecraft segmentation data\nare very scarce. Here, we present a new dataset of nearly 64k annotated\nspacecraft images that was created using real spacecraft models, superimposed\non a mixture of real and synthetic backgrounds generated using NASA's TTALOS\npipeline. To mimic camera distortions and noise in real-world image\nacquisition, we also added different types of noise and distortion to the\nimages. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to\ngenerate performance benchmarks for the dataset under well-defined hardware and\ninference time constraints to mimic real-world image segmentation challenges\nfor real-time onboard applications in space on NASA's inspector spacecraft. The\nresulting models, when tested under these constraints, achieved a Dice score of\n0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.\nThe dataset and models for performance benchmark are available at\nhttps://github.com/RiceD2KLab/SWiM.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u6570\u636e\u96c6\u548c\u6a21\u578b\u7528\u4e8e\u822a\u5929\u5668\u56fe\u50cf\u5206\u5272\uff0c\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u822a\u5929\u5668\u5728\u592a\u7a7a\u6613\u53d7\u635f\uff0c\u4eba\u5de5\u6216\u673a\u5668\u4eba\u7ef4\u4fee\u6210\u672c\u9ad8\uff0c\u9700\u53ef\u9760\u81ea\u4e3b\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u521b\u5efa64k\u6807\u6ce8\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u771f\u5b9e\u4e0e\u5408\u6210\u80cc\u666f\uff0c\u6dfb\u52a0\u566a\u58f0\u548c\u5931\u771f\uff0c\u5fae\u8c03YOLOv8\u548cYOLOv11\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u7ea6\u675f\u6761\u4ef6\u4e0bDice\u5f97\u52060.92\uff0cHausdorff\u8ddd\u79bb0.69\uff0c\u63a8\u7406\u65f6\u95f4\u7ea60.5\u79d2\u3002", "conclusion": "\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u5b9e\u65f6\u822a\u5929\u5668\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u6709\u6548\u57fa\u51c6\u3002"}}
{"id": "2507.10606", "categories": ["cs.LG", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.10606", "abs": "https://arxiv.org/abs/2507.10606", "authors": ["Bing-Yue Wu", "Vidya A. Chhabria"], "title": "DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design", "comment": "Under review at Asia and South Pacific Design Automation Conference\n  (ASP-DAC'26)", "summary": "Machine learning (ML) has demonstrated significant promise in various\nphysical design (PD) tasks. However, model generalizability remains limited by\nthe availability of high-quality, large-scale training datasets. Creating such\ndatasets is often computationally expensive and constrained by IP. While very\nfew public datasets are available, they are typically static, slow to generate,\nand require frequent updates. To address these limitations, we present DALI-PD,\na scalable framework for generating synthetic layout heatmaps to accelerate ML\nin PD research. DALI-PD uses a diffusion model to generate diverse layout\nheatmaps via fast inference in seconds. The heatmaps include power, IR drop,\ncongestion, macro placement, and cell density maps. Using DALI-PD, we created a\ndataset comprising over 20,000 layout configurations with varying macro counts\nand placements. These heatmaps closely resemble real layouts and improve ML\naccuracy on downstream ML tasks such as IR drop or congestion prediction.", "AI": {"tldr": "DALI-PD\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u5408\u6210\u5e03\u5c40\u70ed\u56fe\uff0c\u4ee5\u52a0\u901f\u7269\u7406\u8bbe\u8ba1\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u3001\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u96c6\u5728\u7269\u7406\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u96be\u4ee5\u83b7\u53d6\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5feb\u901f\u751f\u6210\u591a\u6837\u5316\u7684\u5e03\u5c40\u70ed\u56fe\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b20,000\u591a\u79cd\u5e03\u5c40\u914d\u7f6e\u7684\u6570\u636e\u96c6\uff0c\u63d0\u5347\u4e86\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "DALI-PD\u80fd\u591f\u9ad8\u6548\u751f\u6210\u903c\u771f\u7684\u5e03\u5c40\u70ed\u56fe\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u5b66\u4e60\u5728\u7269\u7406\u8bbe\u8ba1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.10814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10814", "abs": "https://arxiv.org/abs/2507.10814", "authors": ["Huiyi Wang", "Fahim Shahriar", "Alireza Azimi", "Gautham Vasan", "Rupam Mahmood", "Colin Bellinger"], "title": "Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection", "comment": "8 pages, 4 figures, 3 tables", "summary": "General-purpose robotic manipulation, including reach and grasp, is essential\nfor deployment into households and workspaces involving diverse and evolving\ntasks. Recent advances propose using large pre-trained models, such as Large\nLanguage Models and object detectors, to boost robotic perception in\nreinforcement learning. These models, trained on large datasets via\nself-supervised learning, can process text prompts and identify diverse objects\nin scenes, an invaluable skill in RL where learning object interaction is\nresource-intensive. This study demonstrates how to integrate such models into\nGoal-Conditioned Reinforcement Learning to enable general and versatile robotic\nreach and grasp capabilities. We use a pre-trained object detection model to\nenable the agent to identify the object from a text prompt and generate a mask\nfor goal conditioning. Mask-based goal conditioning provides object-agnostic\ncues, improving feature sharing and generalization. The effectiveness of the\nproposed framework is demonstrated in a simulated reach-and-grasp task, where\nthe mask-based goal conditioning consistently maintains a $\\sim$90\\% success\nrate in grasping both in and out-of-distribution objects, while also ensuring\nfaster convergence to higher returns.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u548c\u63a9\u7801\u76ee\u6807\u6761\u4ef6\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u901a\u7528\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\uff08\u5982\u6293\u53d6\uff09\u5728\u5bb6\u5ead\u548c\u5de5\u4f5c\u573a\u666f\u4e2d\u9700\u6c42\u5e7f\u6cdb\uff0c\u4f46\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5bf9\u8c61\u4ea4\u4e92\u5b66\u4e60\u4e0a\u8d44\u6e90\u6d88\u8017\u5927\u3002\u5229\u7528\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff09\u53ef\u4ee5\u63d0\u5347\u611f\u77e5\u80fd\u529b\u3002", "method": "\u5c06\u9884\u8bad\u7ec3\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u96c6\u6210\u5230\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u751f\u6210\u5bf9\u8c61\u63a9\u7801\u4f5c\u4e3a\u76ee\u6807\u6761\u4ef6\uff0c\u5b9e\u73b0\u5bf9\u8c61\u65e0\u5173\u7684\u6293\u53d6\u4efb\u52a1\u3002", "result": "\u5728\u6a21\u62df\u6293\u53d6\u4efb\u52a1\u4e2d\uff0c\u63a9\u7801\u76ee\u6807\u6761\u4ef6\u65b9\u6cd5\u5728\u5206\u5e03\u5185\u5916\u5bf9\u8c61\u4e0a\u5747\u4fdd\u6301\u7ea690%\u7684\u6210\u529f\u7387\uff0c\u4e14\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\u63a9\u7801\u76ee\u6807\u6761\u4ef6\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.10778", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10778", "abs": "https://arxiv.org/abs/2507.10778", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "title": "Warehouse Spatial Question Answering with LLM Agent", "comment": "1st Place Solution of the 9th AI City Challenge Track 3", "summary": "Spatial understanding has been a challenging task for existing Multi-modal\nLarge Language Models~(MLLMs). Previous methods leverage large-scale MLLM\nfinetuning to enhance MLLM's spatial understanding ability. In this paper, we\npresent a data-efficient approach. We propose a LLM agent system with strong\nand advanced spatial reasoning ability, which can be used to solve the\nchallenging spatial question answering task in complex indoor warehouse\nscenarios. Our system integrates multiple tools that allow the LLM agent to\nconduct spatial reasoning and API tools interaction to answer the given\ncomplicated spatial question. Extensive evaluations on the 2025 AI City\nChallenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that\nour system achieves high accuracy and efficiency in tasks such as object\nretrieval, counting, and distance estimation. The code is available at:\nhttps://github.com/hsiangwei0903/SpatialAgent", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u7cfb\u7edf\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u590d\u6742\u5ba4\u5185\u4ed3\u5e93\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u95ee\u7b54\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5927\u89c4\u6a21\u5fae\u8c03\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2aLLM\u4ee3\u7406\u7cfb\u7edf\uff0c\u96c6\u6210\u591a\u79cd\u5de5\u5177\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406\u548cAPI\u4ea4\u4e92\u3002", "result": "\u5728AI City Challenge\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u7269\u4f53\u68c0\u7d22\u3001\u8ba1\u6570\u548c\u8ddd\u79bb\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u7a7a\u95f4\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.10609", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10609", "abs": "https://arxiv.org/abs/2507.10609", "authors": ["Obumneme Nwafor", "Chioma Nwafor", "Amro Zakaria", "Nkechi Nwankwo"], "title": "A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights", "comment": null, "summary": "The United Arab Emirates (UAE) relies heavily on seawater desalination to\nmeet over 90% of its drinking water needs. Desalination processes are highly\nenergy intensive and account for approximately 15% of the UAE's electricity\nconsumption, contributing to over 22% of the country's energy-related CO2\nemissions. Moreover, these processes face significant sustainability challenges\nin the face of climate uncertainties such as rising seawater temperatures,\nsalinity, and aerosol optical depth (AOD). AOD greatly affects the operational\nand economic performance of solar-powered desalination systems through\nphotovoltaic soiling, membrane fouling, and water turbidity cycles.\n  This study proposes a novel pipelined two-stage predictive modelling\narchitecture: the first stage forecasts AOD using satellite-derived time series\nand meteorological data; the second stage uses the predicted AOD and other\nmeteorological factors to predict desalination performance efficiency losses.\nThe framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)\nwas used to reveal key drivers of system degradation. Furthermore, this study\nproposes a dust-aware rule-based control logic for desalination systems based\non predicted values of AOD and solar efficiency. This control logic is used to\nadjust the desalination plant feed water pressure, adapt maintenance\nscheduling, and regulate energy source switching.\n  To enhance the practical utility of the research findings, the predictive\nmodels and rule-based controls were packaged into an interactive dashboard for\nscenario and predictive analytics. This provides a management decision-support\nsystem for climate-adaptive planning.", "AI": {"tldr": "\u963f\u8054\u914b\u4f9d\u8d56\u6d77\u6c34\u6de1\u5316\u6ee1\u8db390%\u4ee5\u4e0a\u996e\u7528\u6c34\u9700\u6c42\uff0c\u4f46\u8be5\u8fc7\u7a0b\u80fd\u8017\u9ad8\u4e14\u9762\u4e34\u6c14\u5019\u4e0d\u786e\u5b9a\u6027\u6311\u6218\u3002\u7814\u7a76\u63d0\u51fa\u4e24\u9636\u6bb5\u9884\u6d4b\u6a21\u578b\u548c\u667a\u80fd\u63a7\u5236\u903b\u8f91\uff0c\u63d0\u5347\u7cfb\u7edf\u6548\u7387\u3002", "motivation": "\u6d77\u6c34\u6de1\u5316\u5728\u963f\u8054\u914b\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\u4e2d\u5360\u6bd4\u9ad8\uff0c\u4e14\u53d7\u6c14\u5019\u56e0\u7d20\uff08\u5982AOD\uff09\u5f71\u54cd\u663e\u8457\uff0c\u9700\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e24\u9636\u6bb5\u9884\u6d4b\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u9884\u6d4bAOD\uff0c\u7b2c\u4e8c\u9636\u6bb5\u9884\u6d4b\u6548\u7387\u635f\u5931\uff1b\u7ed3\u5408SHAP\u5206\u6790\u548c\u89c4\u5219\u63a7\u5236\u903b\u8f91\u3002", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8fbe98%\uff0c\u5e76\u5f00\u53d1\u4ea4\u4e92\u5f0f\u4eea\u8868\u76d8\u652f\u6301\u51b3\u7b56\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u6c14\u5019\u9002\u5e94\u6027\u89c4\u5212\u5de5\u5177\uff0c\u663e\u8457\u63d0\u5347\u6d77\u6c34\u6de1\u5316\u7cfb\u7edf\u53ef\u6301\u7eed\u6027\u3002"}}
{"id": "2507.10878", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10878", "abs": "https://arxiv.org/abs/2507.10878", "authors": ["Savva Morozov", "Tobia Marcucci", "Bernhard Paus Graesdal", "Alexandre Amice", "Pablo A. Parrilo", "Russ Tedrake"], "title": "Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets", "comment": "10 pages", "summary": "We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A\nGCS is a graph where each vertex is paired with a convex program, and each edge\ncouples adjacent programs via additional costs and constraints. A walk in a GCS\nis a sequence of vertices connected by edges, where vertices may be repeated.\nThe length of a walk is given by the cumulative optimal value of the\ncorresponding convex programs. To solve the SWP in GCS, we first synthesize a\npiecewise-quadratic lower bound on the problem's cost-to-go function using\nsemidefinite programming. Then we use this lower bound to guide an\nincremental-search algorithm that yields an approximate shortest walk. We show\nthat the SWP in GCS is a natural language for many mixed discrete-continuous\nplanning problems in robotics, unifying problems that typically require\nspecialized solutions while delivering high performance and computational\nefficiency. We demonstrate this through experiments in collision-free motion\nplanning, skill chaining, and optimal control of hybrid systems.", "AI": {"tldr": "\u7814\u7a76\u5728\u51f8\u96c6\u56fe\uff08GCS\uff09\u4e2d\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff08SWP\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534a\u5b9a\u89c4\u5212\u548c\u589e\u91cf\u641c\u7d22\u7684\u8fd1\u4f3c\u89e3\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u51f8\u96c6\u56fe\u4e2d\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\u80fd\u7edf\u4e00\u5904\u7406\u673a\u5668\u4eba\u9886\u57df\u4e2d\u7684\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u89c4\u5212\u95ee\u9898\uff0c\u907f\u514d\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u95ee\u9898\u8bbe\u8ba1\u4e13\u95e8\u89e3\u6cd5\u3002", "method": "\u901a\u8fc7\u534a\u5b9a\u89c4\u5212\u5408\u6210\u95ee\u9898\u7684\u6210\u672c\u51fd\u6570\u7684\u5206\u6bb5\u4e8c\u6b21\u4e0b\u754c\uff0c\u5e76\u7528\u5176\u6307\u5bfc\u589e\u91cf\u641c\u7d22\u7b97\u6cd5\u4ee5\u8fd1\u4f3c\u6c42\u89e3\u6700\u77ed\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u78b0\u649e\u907f\u514d\u8fd0\u52a8\u89c4\u5212\u3001\u6280\u80fd\u94fe\u548c\u6df7\u5408\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u4e2d\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "SWP\u5728GCS\u4e2d\u4e3a\u673a\u5668\u4eba\u6df7\u5408\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10800", "abs": "https://arxiv.org/abs/2507.10800", "authors": ["Ali Hojjat", "Janek Haberer", "Soren Pirk", "Olaf Landsiedel"], "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference", "comment": "Under Review", "summary": "Vision Transformers deliver state-of-the-art performance, yet their fixed\ncomputational budget prevents scalable deployment across heterogeneous\nhardware. Recent nested Transformer architectures mitigate this by embedding\nnested subnetworks within a single model to enable scalable inference. However,\nthese models allocate the same amount of compute to all inputs, regardless of\ntheir complexity, which leads to inefficiencies. To address this, we introduce\nThinkingViT, a nested ViT architecture that employs progressive thinking stages\nto dynamically adjust inference computation based on input difficulty.\nThinkingViT initiates inference by activating a small subset of the most\nimportant attention heads and terminates early if predictions reach sufficient\ncertainty. Otherwise, it activates additional attention heads and re-evaluates\nthe input. At the core of ThinkingViT is our Token Recycling mechanism, which\nconditions each subsequent inference stage on the embeddings from the previous\nstage, enabling progressive improvement. Due to its backbone-preserving design,\nThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show\nthat ThinkingViT surpasses nested baselines by up to 2.0 percentage points\n(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs\non ImageNet-1K. The source code is available at\nhttps://github.com/ds-kiel/ThinkingViT.", "AI": {"tldr": "ThinkingViT\u662f\u4e00\u79cd\u5d4c\u5957ViT\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u548c\u8f93\u5165\u590d\u6742\u5ea6\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u601d\u8003\u9636\u6bb5\u548cToken Recycling\u673a\u5236\u3002", "result": "\u5728ImageNet-1K\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ThinkingViT\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u3002"}}
{"id": "2507.10611", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10611", "abs": "https://arxiv.org/abs/2507.10611", "authors": ["Mengwen Ye", "Yingzi Huangfu", "Shujian Gao", "Wei Ren", "Weifan Liu", "Zekuan Yu"], "title": "FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise", "comment": null, "summary": "Federated Learning (FL) emerged as a solution for collaborative medical image\nclassification while preserving data privacy. However, label noise, which\narises from inter-institutional data variability, can cause training\ninstability and degrade model performance. Existing FL methods struggle with\nnoise heterogeneity and the imbalance in medical data. Motivated by these\nchallenges, we propose FedGSCA, a novel framework for enhancing robustness in\nnoisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates\nnoise knowledge from all clients, effectively addressing noise heterogeneity\nand improving global model stability. Furthermore, we develop a Client Adaptive\nAdjustment (CAA) mechanism that combines adaptive threshold pseudo-label\ngeneration and Robust Credal Labeling Loss. CAA dynamically adjusts to class\ndistributions, ensuring the inclusion of minority samples and carefully\nmanaging noisy labels by considering multiple plausible labels. This dual\napproach mitigates the impact of noisy data and prevents overfitting during\nlocal training, which improves the generalizability of the model. We evaluate\nFedGSCA on one real-world colon slides dataset and two synthetic medical\ndatasets under various noise conditions, including symmetric, asymmetric,\nextreme, and heterogeneous types. The results show that FedGSCA outperforms the\nstate-of-the-art methods, excelling in extreme and heterogeneous noise\nscenarios. Moreover, FedGSCA demonstrates significant advantages in improving\nmodel stability and handling complex noise, making it well-suited for\nreal-world medical federated learning scenarios.", "AI": {"tldr": "FedGSCA\u662f\u4e00\u4e2a\u9488\u5bf9\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u6807\u7b7e\u566a\u58f0\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u6837\u672c\u9009\u62e9\u5668\u548c\u5ba2\u6237\u7aef\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u7597\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u9700\u6c42\u4e0e\u6807\u7b7e\u566a\u58f0\uff08\u7531\u673a\u6784\u95f4\u6570\u636e\u5dee\u5f02\u5f15\u8d77\uff09\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51faFedGSCA\u6846\u67b6\uff0c\u5305\u542b\u5168\u5c40\u6837\u672c\u9009\u62e9\u5668\uff08\u805a\u5408\u566a\u58f0\u77e5\u8bc6\uff09\u548c\u5ba2\u6237\u7aef\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\uff08\u7ed3\u5408\u81ea\u9002\u5e94\u9608\u503c\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u9c81\u68d2\u6807\u7b7e\u635f\u5931\uff09\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u533b\u7597\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cFedGSCA\u5728\u6781\u7aef\u548c\u5f02\u6784\u566a\u58f0\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u566a\u58f0\u5904\u7406\u80fd\u529b\u3002", "conclusion": "FedGSCA\u9002\u7528\u4e8e\u73b0\u5b9e\u533b\u7597\u8054\u90a6\u5b66\u4e60\u573a\u666f\uff0c\u6709\u6548\u89e3\u51b3\u566a\u58f0\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10899", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10899", "abs": "https://arxiv.org/abs/2507.10899", "authors": ["Wang Zhicheng", "Satoshi Yagi", "Satoshi Yamamori", "Jun Morimoto"], "title": "Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning", "comment": null, "summary": "Imitation learning for mobile manipulation is a key challenge in the field of\nrobotic manipulation. However, current mobile manipulation frameworks typically\ndecouple navigation and manipulation, executing manipulation only after\nreaching a certain location. This can lead to performance degradation when\nnavigation is imprecise, especially due to misalignment in approach angles. To\nenable a mobile manipulator to perform the same task from diverse orientations,\nan essential capability for building general-purpose robotic models, we propose\nan object-centric method based on SAM2, a foundation model towards solving\npromptable visual segmentation in images, which incorporates manipulation\norientation information into our model. Our approach enables consistent\nunderstanding of the same task from different orientations. We deploy the model\non a custom-built mobile manipulator and evaluate it on a pick-and-place task\nunder varied orientation angles. Compared to Action Chunking Transformer, our\nmodel maintains superior generalization when trained with demonstrations from\nvaried approach angles. This work significantly enhances the generalization and\nrobustness of imitation learning-based mobile manipulation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSAM2\u7684\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u7528\u4e8e\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u4ece\u4e0d\u540c\u65b9\u5411\u6267\u884c\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u64cd\u4f5c\u6846\u67b6\u901a\u5e38\u5c06\u5bfc\u822a\u548c\u64cd\u4f5c\u89e3\u8026\uff0c\u5bfc\u81f4\u5bfc\u822a\u4e0d\u7cbe\u786e\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u89d2\u5ea6\u4e0d\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528SAM2\u57fa\u7840\u6a21\u578b\uff0c\u5c06\u64cd\u4f5c\u65b9\u5411\u4fe1\u606f\u878d\u5165\u6a21\u578b\uff0c\u5b9e\u73b0\u4ece\u4e0d\u540c\u65b9\u5411\u5bf9\u540c\u4e00\u4efb\u52a1\u7684\u4e00\u81f4\u7406\u89e3\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u79fb\u52a8\u64cd\u4f5c\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u89d2\u5ea6\u4e0b\u8fdb\u884c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u8bc4\u4f30\uff0c\u8868\u73b0\u4f18\u4e8eAction Chunking Transformer\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10844", "abs": "https://arxiv.org/abs/2507.10844", "authors": ["Furkan Mumcu", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "LLM-Guided Agentic Object Detection for Open-World Understanding", "comment": null, "summary": "Object detection traditionally relies on fixed category sets, requiring\ncostly re-training to handle novel objects. While Open-World and\nOpen-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD\nlacks semantic labels for unknowns, and OVOD depends on user prompts, limiting\nautonomy. We propose an LLM-guided agentic object detection (LAOD) framework\nthat enables fully label-free, zero-shot detection by prompting a Large\nLanguage Model (LLM) to generate scene-specific object names. These are passed\nto an open-vocabulary detector for localization, allowing the system to adapt\nits goals dynamically. We introduce two new metrics, Class-Agnostic Average\nPrecision (CAAP) and Semantic Naming Average Precision (SNAP), to separately\nevaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD\nvalidate our approach, showing strong performance in detecting and naming novel\nobjects. Our method offers enhanced autonomy and adaptability for open-world\nunderstanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u4e3b\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff08LAOD\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u573a\u666f\u7279\u5b9a\u5bf9\u8c61\u540d\u79f0\u5b9e\u73b0\u96f6\u6837\u672c\u68c0\u6d4b\uff0c\u65e0\u9700\u6807\u7b7e\u3002", "motivation": "\u4f20\u7edf\u76ee\u6807\u68c0\u6d4b\u4f9d\u8d56\u56fa\u5b9a\u7c7b\u522b\u96c6\uff0c\u7075\u6d3b\u6027\u4e0d\u8db3\uff1b\u73b0\u6709\u5f00\u653e\u4e16\u754c\uff08OWOD\uff09\u548c\u5f00\u653e\u8bcd\u6c47\uff08OVOD\uff09\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u6807\u7b7e\u7f3a\u5931\u6216\u4f9d\u8d56\u7528\u6237\u63d0\u793a\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528LLM\u751f\u6210\u573a\u666f\u7279\u5b9a\u5bf9\u8c61\u540d\u79f0\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u8fdb\u884c\u5b9a\u4f4d\uff0c\u52a8\u6001\u8c03\u6574\u76ee\u6807\u3002", "result": "\u5728LVIS\u3001COCO\u548cCOCO-OOD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u9ad8\u6548\u68c0\u6d4b\u548c\u547d\u540d\u65b0\u5bf9\u8c61\u3002", "conclusion": "LAOD\u6846\u67b6\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u7406\u89e3\u7684\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.10613", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10613", "abs": "https://arxiv.org/abs/2507.10613", "authors": ["Zhengyu Chen", "Siqi Wang", "Teng Xiao", "Yudong Wang", "Shiqi Chen", "Xunliang Cai", "Junxian He", "Jingang Wang"], "title": "Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs", "comment": null, "summary": "Traditional scaling laws in natural language processing suggest that\nincreasing model size and training data enhances performance. However, recent\nstudies reveal deviations, particularly in large language models, where\nperformance improvements decelerate, which is a phenomenon known as\nsub-scaling. This paper revisits these scaling laws by examining the impact of\ndata quality and training strategies on model performance. Through extensive\nempirical analysis of over 400 models, we identify high data density and\nnon-optimal resource allocation as key factors contributing to sub-scaling.\nHigh data density leads to diminishing returns due to redundant information,\nwhile optimal resource allocation is crucial for sustained performance\nimprovements. We propose a sub-optimal scaling law that better predicts\nperformance in sub-scaling regimes, highlighting the importance of data quality\nand diversity.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6269\u5c55\u5b9a\u5f8b\uff0c\u53d1\u73b0\u6570\u636e\u8d28\u91cf\u548c\u8bad\u7ec3\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u6b21\u4f18\u6269\u5c55\u7684\u9884\u6d4b\u5b9a\u5f8b\u3002", "motivation": "\u4f20\u7edf\u6269\u5c55\u5b9a\u5f8b\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u51fa\u73b0\u6027\u80fd\u63d0\u5347\u51cf\u7f13\u7684\u73b0\u8c61\uff08\u6b21\u4f18\u6269\u5c55\uff09\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5176\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5206\u6790400\u591a\u4e2a\u6a21\u578b\u7684\u5b9e\u8bc1\u6570\u636e\uff0c\u7814\u7a76\u4e86\u6570\u636e\u5bc6\u5ea6\u548c\u8d44\u6e90\u5206\u914d\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u9ad8\u6570\u636e\u5bc6\u5ea6\u548c\u6b21\u4f18\u8d44\u6e90\u5206\u914d\u662f\u5bfc\u81f4\u6b21\u4f18\u6269\u5c55\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u9884\u6d4b\u5b9a\u5f8b\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u548c\u591a\u6837\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u65b0\u7684\u6269\u5c55\u5b9a\u5f8b\u80fd\u66f4\u51c6\u786e\u9884\u6d4b\u6b21\u4f18\u6269\u5c55\u60c5\u51b5\u3002"}}
{"id": "2507.10914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10914", "abs": "https://arxiv.org/abs/2507.10914", "authors": ["James A. Preiss", "Fengze Xie", "Yiheng Lin", "Adam Wierman", "Yisong Yue"], "title": "Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization", "comment": "11 pages, 9 figures", "summary": "We study online algorithms to tune the parameters of a robot controller in a\nsetting where the dynamics, policy class, and optimality objective are all\ntime-varying. The system follows a single trajectory without episodes or state\nresets, and the time-varying information is not known in advance. Focusing on\nnonlinear geometric quadrotor controllers as a test case, we propose a\npractical implementation of a single-trajectory model-based online policy\noptimization algorithm, M-GAPS,along with reparameterizations of the quadrotor\nstate space and policy class to improve the optimization landscape. In hardware\nexperiments,we compare to model-based and model-free baselines that impose\nartificial episodes. We show that M-GAPS finds near-optimal parameters more\nquickly, especially when the episode length is not favorable. We also show that\nM-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and\nachieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our\nresults demonstrate the hardware practicality of this emerging class of online\npolicy optimization that offers significantly more flexibility than classic\nadaptive control, while being more stable and data-efficient than model-free\nreinforcement learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-GAPS\u7684\u5355\u8f68\u8ff9\u6a21\u578b\u5728\u7ebf\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u63a7\u5236\u5668\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u51e0\u4f55\u56db\u65cb\u7ffc\u63a7\u5236\u5668\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u57fa\u4e8e\u6a21\u578b\u548c\u65e0\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u5728\u52a8\u6001\u3001\u7b56\u7565\u7c7b\u548c\u6700\u4f18\u76ee\u6807\u5747\u968f\u65f6\u95f4\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u5728\u7ebf\u8c03\u6574\u673a\u5668\u4eba\u63a7\u5236\u5668\u53c2\u6570\uff0c\u4e14\u65e0\u9700\u9884\u5148\u77e5\u9053\u65f6\u95f4\u53d8\u5316\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86M-GAPS\u7b97\u6cd5\uff0c\u91cd\u65b0\u53c2\u6570\u5316\u56db\u65cb\u7ffc\u72b6\u6001\u7a7a\u95f4\u548c\u7b56\u7565\u7c7b\u4ee5\u4f18\u5316\u4f18\u5316\u666f\u89c2\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u4e0e\u57fa\u4e8e\u6a21\u578b\u548c\u65e0\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "M-GAPS\u80fd\u66f4\u5feb\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u53c2\u6570\uff0c\u5c24\u5176\u5728\u4e0d\u5229\u7684\u7247\u6bb5\u957f\u5ea6\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u80fd\u5feb\u901f\u9002\u5e94\u672a\u5efa\u6a21\u7684\u98ce\u548c\u8d1f\u8f7d\u5e72\u6270\u3002", "conclusion": "M-GAPS\u5c55\u793a\u4e86\u5728\u7ebf\u7b56\u7565\u4f18\u5316\u5728\u786c\u4ef6\u4e0a\u7684\u5b9e\u7528\u6027\uff0c\u6bd4\u7ecf\u5178\u81ea\u9002\u5e94\u63a7\u5236\u66f4\u7075\u6d3b\uff0c\u6bd4\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u66f4\u7a33\u5b9a\u548c\u6570\u636e\u9ad8\u6548\u3002"}}
{"id": "2507.10846", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10846", "abs": "https://arxiv.org/abs/2507.10846", "authors": ["Casey Wall", "Longwei Wang", "Rodrigue Rizk", "KC Santosh"], "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization", "comment": "15 pages, 10 figures, 7 tables. Submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "summary": "Interpreting the decision-making process of Convolutional Neural Networks\n(CNNs) is critical for deploying models in high-stakes domains.\nGradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method\nfor visual explanations, yet it typically focuses on the final convolutional\nlayer or na\\\"ively averages across layers, strategies that can obscure\nimportant semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a\nnovel, human-tunable extension of Grad-CAM that generates robust and coherent\nsaliency maps by aggregating information across all convolutional layers. To\nmitigate the influence of noisy or extreme attribution values, Winsor-CAM\napplies Winsorization, a percentile-based outlier attenuation technique. A\nuser-controllable threshold allows for semantic-level tuning, enabling flexible\nexploration of model behavior across representational hierarchies. Evaluations\non standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the\nPASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable\nheatmaps and achieves superior performance in localization metrics, including\nintersection-over-union and center-of-mass alignment, when compared to Grad-CAM\nand uniform layer-averaging baselines. Winsor-CAM advances the goal of\ntrustworthy AI by offering interpretable, multi-layer insights with\nhuman-in-the-loop control.", "AI": {"tldr": "Winsor-CAM\u662f\u4e00\u79cd\u6539\u8fdb\u7684Grad-CAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u5c42\u805a\u5408\u4fe1\u606f\u548cWinsorization\u6280\u672f\u751f\u6210\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u663e\u8457\u6027\u56fe\u3002", "motivation": "\u73b0\u6709Grad-CAM\u65b9\u6cd5\u901a\u5e38\u4ec5\u5173\u6ce8\u6700\u540e\u4e00\u5c42\u5377\u79ef\u5c42\u6216\u7b80\u5355\u5e73\u5747\u591a\u5c42\u4fe1\u606f\uff0c\u53ef\u80fd\u63a9\u76d6\u91cd\u8981\u8bed\u4e49\u7ebf\u7d22\u6216\u653e\u5927\u566a\u58f0\u3002", "method": "\u63d0\u51faWinsor-CAM\uff0c\u7ed3\u5408Winsorization\u6280\u672f\uff08\u57fa\u4e8e\u767e\u5206\u6bd4\u7684\u5f02\u5e38\u503c\u8870\u51cf\uff09\u548c\u7528\u6237\u53ef\u8c03\u9608\u503c\uff0c\u8de8\u5c42\u805a\u5408\u4fe1\u606f\u3002", "result": "\u5728PASCAL VOC 2012\u6570\u636e\u96c6\u4e0a\uff0cWinsor-CAM\u5728\u5b9a\u4f4d\u6307\u6807\uff08\u5982IoU\u548c\u8d28\u5fc3\u5bf9\u9f50\uff09\u4e0a\u4f18\u4e8eGrad-CAM\u548c\u5747\u5300\u5c42\u5e73\u5747\u57fa\u7ebf\u3002", "conclusion": "Winsor-CAM\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u591a\u5c42\u6d1e\u5bdf\u548c\u4eba\u5de5\u63a7\u5236\uff0c\u63a8\u52a8\u4e86\u53ef\u4fe1AI\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.10614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10614", "abs": "https://arxiv.org/abs/2507.10614", "authors": ["Fei Liu", "Rui Zhang", "Xi Lin", "Zhichao Lu", "Qingfu Zhang"], "title": "Fine-tuning Large Language Model for Automated Algorithm Design", "comment": null, "summary": "The integration of large language models (LLMs) into automated algorithm\ndesign has shown promising potential. A prevalent approach embeds LLMs within\nsearch routines to iteratively generate and refine candidate algorithms.\nHowever, most existing methods rely on off-the-shelf LLMs trained for general\ncoding tasks,leaving a key question open: Do we need LLMs specifically tailored\nfor algorithm design? If so, how can such LLMs be effectively obtained and how\nwell can they generalize across different algorithm design tasks? In this\npaper, we take a first step toward answering these questions by exploring\nfine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank\nbased (DAR) sampling strategy to balance training data diversity and quality,\nthen we leverage direct preference optimization to efficiently align LLM\noutputs with task objectives. Our experiments, conducted on\nLlama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm\ndesign tasks. Results suggest that finetuned LLMs can significantly outperform\ntheir off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and\nmatch the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,\nwe observe promising generalization: LLMs finetuned on specific algorithm\ndesign tasks also improve performance on related tasks with varying settings.\nThese findings highlight the value of task-specific adaptation for LLMs in\nalgorithm design and open new avenues for future research.", "AI": {"tldr": "\u63a2\u7d22\u4e3a\u7b97\u6cd5\u8bbe\u8ba1\u5fae\u8c03LLMs\u7684\u6548\u679c\uff0c\u63d0\u51faDAR\u91c7\u6837\u7b56\u7565\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u5fae\u8c03\u540e\u7684LLMs\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u901a\u7528LLMs\uff0c\u4f46\u7b97\u6cd5\u8bbe\u8ba1\u662f\u5426\u9700\u8981\u4e13\u95e8\u5b9a\u5236\u7684LLMs\uff1f\u5982\u4f55\u9ad8\u6548\u83b7\u53d6\u5e76\u8bc4\u4f30\u5176\u6cdb\u5316\u80fd\u529b\uff1f", "method": "\u5f15\u5165DAR\u91c7\u6837\u7b56\u7565\u5e73\u8861\u6570\u636e\u591a\u6837\u6027\u548c\u8d28\u91cf\uff0c\u7ed3\u5408\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5fae\u8c03LLMs\u3002", "result": "\u5fae\u8c03\u540e\u7684LLMs\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u4e14\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u5bf9LLMs\u5728\u7b97\u6cd5\u8bbe\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10950", "abs": "https://arxiv.org/abs/2507.10950", "authors": ["Zhiwei Wu", "Jiahao Luo", "Siyi Wei", "Jinhui Zhang"], "title": "Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances", "comment": null, "summary": "This paper presents a unified modeling and optimization framework to enhance\nthe kinematic performance of multi-magnet embedded soft continuum robots\n(MeSCRs). To this end, we establish a differentiable system formulation based\non an extended pseudo-rigid-body model. This formulation enables analysis of\nthe equilibrium well-posedness and the geometry of the induced configuration\nunder magnetic actuation. In particular, we show that the maximum controllable\ndegrees of freedom of a MeSCR equal twice the number of embedded magnets. We\nsubsequently develop a structural optimization framework based on differential\ngeometry that links classical kinematic measures (e.g., manipulability and\ndexterity) to the configuration of embedded magnets. The resulting optimization\ncondition reveals that improving local performance requires structurally\nmodulating the spectrum of the configuration space metric to counteract its\ndistortion. Closed-form solutions for optimal magnet configurations are derived\nunder representative conditions, and a gradient-based numerical method is\nproposed for general design scenarios. Simulation studies validate the\neffectiveness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5efa\u6a21\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u78c1\u4f53\u5d4c\u5165\u5f0f\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\uff08MeSCRs\uff09\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u78c1\u4f53\u914d\u7f6e\u6765\u589e\u5f3a\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u4f2a\u521a\u4f53\u6a21\u578b\u5efa\u7acb\u53ef\u5fae\u7cfb\u7edf\uff0c\u7ed3\u5408\u5fae\u5206\u51e0\u4f55\u8fdb\u884c\u7ed3\u6784\u4f18\u5316\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u5927\u53ef\u63a7\u81ea\u7531\u5ea6\u662f\u5d4c\u5165\u78c1\u4f53\u6570\u91cf\u7684\u4e24\u500d\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u6761\u4ef6\u548c\u6570\u503c\u65b9\u6cd5\u3002", "conclusion": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.10855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10855", "abs": "https://arxiv.org/abs/2507.10855", "authors": ["Wei Chen", "Jingxi Yu", "Zichen Miao", "Qiang Qiu"], "title": "Sparse Fine-Tuning of Transformers for Generative Tasks", "comment": "Accepted by International Conference on Computer Vision 2025", "summary": "Large pre-trained transformers have revolutionized artificial intelligence\nacross various domains, and fine-tuning remains the dominant approach for\nadapting these models to downstream tasks due to the cost of training from\nscratch. However, in existing fine-tuning methods, the updated representations\nare formed as a dense combination of modified parameters, making it challenging\nto interpret their contributions and understand how the model adapts to new\ntasks. In this work, we introduce a fine-tuning framework inspired by sparse\ncoding, where fine-tuned features are represented as a sparse combination of\nbasic elements, i.e., feature dictionary atoms. The feature dictionary atoms\nfunction as fundamental building blocks of the representation, and tuning atoms\nallows for seamless adaptation to downstream tasks. Sparse coefficients then\nserve as indicators of atom importance, identifying the contribution of each\natom to the updated representation. Leveraging the atom selection capability of\nsparse coefficients, we first demonstrate that our method enhances image\nediting performance by improving text alignment through the removal of\nunimportant feature dictionary atoms. Additionally, we validate the\neffectiveness of our approach in the text-to-image concept customization task,\nwhere our method efficiently constructs the target concept using a sparse\ncombination of feature dictionary atoms, outperforming various baseline\nfine-tuning methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u7f16\u7801\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u7ec4\u5408\u7279\u5f81\u5b57\u5178\u539f\u5b50\u6765\u6539\u8fdb\u6a21\u578b\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u89e3\u91ca\u53c2\u6570\u66f4\u65b0\u7684\u8d21\u732e\uff0c\u7a00\u758f\u7f16\u7801\u6846\u67b6\u80fd\u66f4\u597d\u5730\u7406\u89e3\u6a21\u578b\u5982\u4f55\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u7f16\u7801\u8868\u793a\u5fae\u8c03\u7279\u5f81\uff0c\u7279\u5f81\u5b57\u5178\u539f\u5b50\u4f5c\u4e3a\u57fa\u672c\u6784\u5efa\u5757\uff0c\u7a00\u758f\u7cfb\u6570\u6307\u793a\u539f\u5b50\u91cd\u8981\u6027\u3002", "result": "\u5728\u56fe\u50cf\u7f16\u8f91\u548c\u6587\u672c\u5230\u56fe\u50cf\u6982\u5ff5\u5b9a\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7a00\u758f\u7f16\u7801\u6846\u67b6\u63d0\u9ad8\u4e86\u6a21\u578b\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.10616", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10616", "abs": "https://arxiv.org/abs/2507.10616", "authors": ["Neel Rajani", "Aryo Pradipta Gema", "Seraphina Goldfarb-Tarrant", "Ivan Titov"], "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them", "comment": null, "summary": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0RL\u5728\u6570\u5b66\u9886\u57df\u6709\u5fae\u5c0f\u63d0\u5347\uff0c\u800cSFT\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u5dee\u3002", "motivation": "\u7406\u89e3RL\u548cSFT\u5728LLM\u540e\u8bad\u7ec3\u4e2d\u7684\u52a8\u6001\u5dee\u5f02\u53ca\u5176\u5bf9\u6a21\u578b\u80fd\u529b\u7684\u5f71\u54cd\u3002", "method": "\u5728\u540c\u4e00\u6a21\u578b\u548c\u8d85\u53c2\u6570\u4e0b\uff0c\u5bf9RL\u548cSFT\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff0c\u5e76\u89c2\u5bdf\u6a21\u578b\u53c2\u6570\u53d8\u5316\u3002", "result": "RL\u5728\u6570\u5b66\u9886\u57df\u6709\u8f7b\u5fae\u63d0\u5347\uff0cSFT\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u5dee\uff1bSFT\u5bf9\u6a21\u578b\u53c2\u6570\u7684\u4fee\u6539\u66f4\u663e\u8457\u3002", "conclusion": "RL\u53ef\u80fd\u589e\u5f3a\u73b0\u6709\u80fd\u529b\uff0c\u800cSFT\u53ef\u80fd\u66ff\u6362\u65e7\u6280\u80fd\uff1b\u51bb\u7ed3\u90e8\u5206\u6a21\u578b\u53c2\u6570\u7684\u6548\u679c\u5c1a\u4e0d\u660e\u786e\u3002"}}
{"id": "2507.10960", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10960", "abs": "https://arxiv.org/abs/2507.10960", "authors": ["He Zhu", "Ryo Miyoshi", "Yuki Okafuji"], "title": "Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction", "comment": null, "summary": "Prior human-robot interaction (HRI) research has primarily focused on\nsingle-user interactions, where robots do not need to consider the timing or\nrecipient of their responses. However, in multi-party interactions, such as at\nmalls and hospitals, social robots must understand the context and decide both\nwhen and to whom they should respond. In this paper, we propose a\nTransformer-based multi-task learning framework to improve the decision-making\nprocess of social robots, particularly in multi-user environments. Considering\nthe characteristics of HRI, we propose two novel loss functions: one that\nenforces constraints on active speakers to improve scene modeling, and another\nthat guides response selection towards utterances specifically directed at the\nrobot. Additionally, we construct a novel multi-party HRI dataset that captures\nreal-world complexities, such as gaze misalignment. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in respond\ndecisions, outperforming existing heuristic-based and single-task approaches.\nOur findings contribute to the development of socially intelligent social\nrobots capable of engaging in natural and context-aware multi-party\ninteractions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5728\u591a\u7528\u6237\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u79cd\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\u548c\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u54cd\u5e94\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u7528\u6237\u573a\u666f\uff0c\u800c\u591a\u7528\u6237\u573a\u666f\uff08\u5982\u5546\u573a\u3001\u533b\u9662\uff09\u4e2d\u673a\u5668\u4eba\u9700\u8981\u66f4\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u548c\u54cd\u5e94\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u91c7\u7528Transformer\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e24\u79cd\u635f\u5931\u51fd\u6570\uff1a\u4e00\u79cd\u7ea6\u675f\u4e3b\u52a8\u8bf4\u8bdd\u8005\u4ee5\u6539\u8fdb\u573a\u666f\u5efa\u6a21\uff0c\u53e6\u4e00\u79cd\u5f15\u5bfc\u673a\u5668\u4eba\u9009\u62e9\u9488\u5bf9\u5176\u7684\u54cd\u5e94\u3002\u6784\u5efa\u4e86\u65b0\u7684\u591a\u7528\u6237HRI\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u54cd\u5e94\u51b3\u7b56\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u548c\u5355\u4efb\u52a1\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u5177\u6709\u793e\u4ea4\u667a\u80fd\u7684\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u4f7f\u5176\u80fd\u591f\u53c2\u4e0e\u81ea\u7136\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u7528\u6237\u4ea4\u4e92\u3002"}}
{"id": "2507.10864", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10864", "abs": "https://arxiv.org/abs/2507.10864", "authors": ["Saadat Behzadi", "Danial Sharifrazi", "Bita Mesbahzadeh", "Javad Hassannataj Joloudarid", "Roohallah Alizadehsani"], "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n", "comment": null, "summary": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LOF\u7b97\u6cd5\u548cYOLO-v11n\u7684\u8f7b\u91cf\u7ea7\u7ed3\u76f4\u80a0\u606f\u8089\u68c0\u6d4b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u7ed3\u76f4\u80a0\u764c\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\u4e4b\u4e00\uff0c\u53ca\u65f6\u51c6\u786e\u7684\u606f\u8089\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u9884\u9632\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528LOF\u7b97\u6cd5\u53bb\u9664\u566a\u58f0\u6570\u636e\uff0c\u7ed3\u5408YOLO-v11n\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u589e\u5f3a\u7b56\u7565\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7cbe\u5ea6\u8fbe95.83%\uff0c\u53ec\u56de\u738791.85%\uff0cF1\u5206\u657093.48%\uff0cmAP@0.5\u4e3a96.48%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u4e34\u5e8a\u5b9e\u65f6\u7ed3\u80a0\u955c\u68c0\u67e5\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u9884\u5904\u7406\u548c\u6a21\u578b\u6548\u7387\u5728\u533b\u5b66\u5f71\u50cfAI\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.10618", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10618", "abs": "https://arxiv.org/abs/2507.10618", "authors": ["Peter Barnett"], "title": "Compute Requirements for Algorithmic Innovation in Frontier AI Models", "comment": null, "summary": "Algorithmic innovation in the pretraining of large language models has driven\na massive reduction in the total compute required to reach a given level of\ncapability. In this paper we empirically investigate the compute requirements\nfor developing algorithmic innovations. We catalog 36 pre-training algorithmic\ninnovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate\nboth the total FLOP used in development and the FLOP/s of the hardware\nutilized. Innovations using significant resources double in their requirements\neach year. We then use this dataset to investigate the effect of compute caps\non innovation. Our analysis suggests that compute caps alone are unlikely to\ndramatically slow AI algorithmic progress. Even stringent compute caps -- such\nas capping total operations to the compute used to train GPT-2 or capping\nhardware capacity to 8 H100 GPUs -- could still have allowed for half of the\ncataloged innovations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u7b97\u6cd5\u521b\u65b0\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\uff0c\u53d1\u73b0\u5373\u4f7f\u4e25\u683c\u7684\u8ba1\u7b97\u9650\u5236\u4e5f\u4e0d\u4f1a\u663e\u8457\u51cf\u7f13AI\u7b97\u6cd5\u8fdb\u6b65\u3002", "motivation": "\u63a2\u7d22\u7b97\u6cd5\u521b\u65b0\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u4ee5\u8bc4\u4f30\u8ba1\u7b97\u9650\u5236\u5bf9AI\u7b97\u6cd5\u53d1\u5c55\u7684\u5f71\u54cd\u3002", "method": "\u6536\u96c6\u4e8636\u79cd\u9884\u8bad\u7ec3\u7b97\u6cd5\u521b\u65b0\u6570\u636e\uff0c\u4f30\u7b97\u5176\u5f00\u53d1\u6240\u9700\u7684FLOP\u548c\u786c\u4ef6\u6027\u80fd\uff0c\u5e76\u5206\u6790\u8ba1\u7b97\u9650\u5236\u5bf9\u521b\u65b0\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u4e25\u683c\u7684\u8ba1\u7b97\u9650\u5236\uff08\u5982\u9650\u5236\u5230GPT-2\u7684\u8bad\u7ec3\u8ba1\u7b97\u91cf\u62168\u4e2aH100 GPU\uff09\u4ecd\u80fd\u652f\u6301\u534a\u6570\u521b\u65b0\u3002", "conclusion": "\u8ba1\u7b97\u9650\u5236\u4e0d\u592a\u53ef\u80fd\u663e\u8457\u51cf\u7f13AI\u7b97\u6cd5\u8fdb\u6b65\uff0c\u56e0\u4e3a\u8bb8\u591a\u521b\u65b0\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u3002"}}
{"id": "2507.10961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10961", "abs": "https://arxiv.org/abs/2507.10961", "authors": ["Joohwan Seo", "Arvind Kruthiventy", "Soomi Lee", "Megan Teng", "Xiang Zhang", "Seoyeon Choi", "Jongeun Choi", "Roberto Horowitz"], "title": "EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks", "comment": "Submitted to RA-L", "summary": "This paper presents a framework for learning vision-based robotic policies\nfor contact-rich manipulation tasks that generalize spatially across task\nconfigurations. We focus on achieving robust spatial generalization of the\npolicy for the peg-in-hole (PiH) task trained from a small number of\ndemonstrations. We propose EquiContact, a hierarchical policy composed of a\nhigh-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)\nand a novel low-level compliant visuomotor policy (Geometric Compliant ACT,\nG-CompACT). G-CompACT operates using only localized observations (geometrically\nconsistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB\nimages) and produces actions defined in the end-effector frame. Through these\ndesign choices, we show that the entire EquiContact pipeline is\nSE(3)-equivariant, from perception to force control. We also outline three key\ncomponents for spatially generalizable contact-rich policies: compliance,\nlocalized policies, and induced equivariance. Real-world experiments on PiH\ntasks demonstrate a near-perfect success rate and robust generalization to\nunseen spatial configurations, validating the proposed framework and\nprinciples. The experimental videos can be found on the project website:\nhttps://sites.google.com/berkeley.edu/equicontact", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEquiContact\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u89c6\u89c9\u9a71\u52a8\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a7a\u95f4\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff08\u5982peg-in-hole\uff09\u5728\u7a7a\u95f4\u914d\u7f6e\u4e0a\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5c24\u5176\u662f\u4ece\u5c11\u91cf\u6f14\u793a\u4e2d\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\uff0c\u5305\u62ec\u9ad8\u5c42\u89c6\u89c9\u89c4\u5212\u5668\uff08Diff-EDF\uff09\u548c\u4f4e\u5c42\u987a\u5e94\u6027\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff08G-CompACT\uff09\uff0c\u5229\u7528\u5c40\u90e8\u89c2\u6d4b\u548cSE(3)-\u7b49\u53d8\u6027\u8bbe\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684peg-in-hole\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6210\u529f\u7387\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7a7a\u95f4\u914d\u7f6e\u3002", "conclusion": "EquiContact\u6846\u67b6\u901a\u8fc7\u987a\u5e94\u6027\u3001\u5c40\u90e8\u7b56\u7565\u548c\u7b49\u53d8\u6027\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u7684\u7a7a\u95f4\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2507.10881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10881", "abs": "https://arxiv.org/abs/2507.10881", "authors": ["Roman Naeem", "David Hagerman", "Jennifer Alv\u00e9n", "Lennart Svensson", "Fredrik Kahl"], "title": "Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes", "comment": "Submitted Version. Accepted at MICCAI 2025", "summary": "Tubular tree structures, such as blood vessels and airways, are essential in\nhuman anatomy and accurately tracking them while preserving their topology is\ncrucial for various downstream tasks. Trexplorer is a recurrent model designed\nfor centerline tracking in 3D medical images but it struggles with predicting\nduplicate branches and terminating tracking prematurely. To address these\nissues, we present Trexplorer Super, an enhanced version that notably improves\nperformance through novel advancements. However, evaluating centerline tracking\nmodels is challenging due to the lack of public datasets. To enable thorough\nevaluation, we develop three centerline datasets, one synthetic and two real,\neach with increasing difficulty. Using these datasets, we conduct a\ncomprehensive evaluation of existing state-of-the-art (SOTA) models and compare\nthem with our approach. Trexplorer Super outperforms previous SOTA models on\nevery dataset. Our results also highlight that strong performance on synthetic\ndata does not necessarily translate to real datasets. The code and datasets are\navailable at https://github.com/RomStriker/Trexplorer-Super.", "AI": {"tldr": "Trexplorer Super\u662f\u4e00\u79cd\u6539\u8fdb\u76843D\u533b\u5b66\u56fe\u50cf\u4e2d\u5fc3\u7ebf\u8ffd\u8e2a\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u91cd\u590d\u5206\u652f\u548c\u8fc7\u65e9\u7ec8\u6b62\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u51c6\u786e\u8ffd\u8e2a\u7ba1\u72b6\u6811\u7ed3\u6784\uff08\u5982\u8840\u7ba1\u548c\u6c14\u9053\uff09\u5bf9\u533b\u5b66\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u5b58\u5728\u91cd\u590d\u5206\u652f\u548c\u8fc7\u65e9\u7ec8\u6b62\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faTrexplorer Super\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u65b9\u6cd5\u6539\u8fdb\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u4e86\u4e09\u4e2a\u6570\u636e\u96c6\uff08\u4e00\u4e2a\u5408\u6210\u548c\u4e24\u4e2a\u771f\u5b9e\uff09\u7528\u4e8e\u8bc4\u4f30\u3002", "result": "Trexplorer Super\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\uff0c\u4e14\u5408\u6210\u6570\u636e\u8868\u73b0\u4e0d\u4e00\u5b9a\u9002\u7528\u4e8e\u771f\u5b9e\u6570\u636e\u3002", "conclusion": "Trexplorer Super\u663e\u8457\u63d0\u5347\u4e86\u4e2d\u5fc3\u7ebf\u8ffd\u8e2a\u6027\u80fd\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.10619", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10619", "abs": "https://arxiv.org/abs/2507.10619", "authors": ["Oluwaseyi Giwa", "Tobi Awodunmila", "Muhammad Ahmed Mohsin", "Ahsan Bilal", "Muhammad Ali Jamshed"], "title": "Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks", "comment": "5 pages, 6 figures, under review at IEEE Wireless Communications\n  Letters", "summary": "The dynamic allocation of spectrum in 5G / 6G networks is critical to\nefficient resource utilization. However, applying traditional deep\nreinforcement learning (DRL) is often infeasible due to its immense sample\ncomplexity and the safety risks associated with unguided exploration, which can\ncause severe network interference. To address these challenges, we propose a\nmeta-learning framework that enables agents to learn a robust initial policy\nand rapidly adapt to new wireless scenarios with minimal data. We implement\nthree meta-learning architectures, model-agnostic meta-learning (MAML),\nrecurrent neural network (RNN), and an attention-enhanced RNN, and evaluate\nthem against a non-meta-learning DRL algorithm, proximal policy optimization\n(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)\nenvironment. Our results show a clear performance gap. The attention-based\nmeta-learning agent reaches a peak mean network throughput of 48 Mbps, while\nthe PPO baseline decreased drastically to 10 Mbps. Furthermore, our method\nreduces SINR and latency violations by more than 50% compared to PPO. It also\nshows quick adaptation, with a fairness index 0.7, showing better resource\nallocation. This work proves that meta-learning is a very effective and safer\noption for intelligent control in complex wireless systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e5G/6G\u7f51\u7edc\u4e2d\u52a8\u6001\u9891\u8c31\u5206\u914d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u590d\u6742\u6027\u548c\u5b89\u5168\u98ce\u9669\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u52a8\u6001\u9891\u8c31\u5206\u914d\u4e2d\u6837\u672c\u590d\u6742\u4e14\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u5b89\u5168\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u5143\u5b66\u4e60\u67b6\u6784\uff08MAML\u3001RNN\u548c\u6ce8\u610f\u529b\u589e\u5f3aRNN\uff09\uff0c\u5728\u52a8\u6001IAB\u6a21\u62df\u73af\u5883\u4e2d\u4e0ePPO\u57fa\u7ebf\u5bf9\u6bd4\u3002", "result": "\u6ce8\u610f\u529b\u5143\u5b66\u4e60\u4ee3\u7406\u5cf0\u503c\u541e\u5410\u91cf\u8fbe48 Mbps\uff0c\u4f18\u4e8ePPO\u768410 Mbps\uff0c\u4e14SINR\u548c\u5ef6\u8fdf\u8fdd\u89c4\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "\u5143\u5b66\u4e60\u662f\u590d\u6742\u65e0\u7ebf\u7cfb\u7edf\u4e2d\u667a\u80fd\u63a7\u5236\u7684\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u9009\u62e9\u3002"}}
{"id": "2507.10968", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10968", "abs": "https://arxiv.org/abs/2507.10968", "authors": ["Toktam Mohammadnejad", "Jovin D'sa", "Behdad Chalaki", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi-Pari"], "title": "SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging", "comment": "Accepted at IEEE ITSC 2025", "summary": "Merging onto a highway is a complex driving task that requires identifying a\nsafe gap, adjusting speed, often interactions to create a merging gap, and\ncompleting the merge maneuver within a limited time window while maintaining\nsafety and driving comfort. In this paper, we introduce a Safe Merging and\nReal-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed\nto facilitate safe and comfortable forced merging. By deliberately adapting\ncost terms to the unique challenges of forced merging and introducing a desired\nspeed heuristic, SMART-Merge planner enables the ego vehicle to merge\nsuccessfully while minimizing the merge time. We verify the efficiency and\neffectiveness of the proposed merge planner through high-fidelity CarMaker\nsimulations on hundreds of highway merge scenarios. Our proposed planner\nachieves the success rate of 100% as well as completes the merge maneuver in\nthe shortest amount of time compared with the baselines, demonstrating our\nplanner's capability to handle complex forced merge tasks and provide a\nreliable and robust solution for autonomous highway merge. The simulation\nresult videos are available at\nhttps://sites.google.com/view/smart-merge-planner/home.", "AI": {"tldr": "SMART-Merge\u662f\u4e00\u79cd\u57fa\u4e8e\u683c\u6805\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u5b9e\u73b0\u5b89\u5168\u8212\u9002\u7684\u9ad8\u901f\u516c\u8def\u5f3a\u5236\u5e76\u9053\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u5e76\u9053\u662f\u4e00\u9879\u590d\u6742\u7684\u9a7e\u9a76\u4efb\u52a1\uff0c\u6d89\u53ca\u8bc6\u522b\u5b89\u5168\u95f4\u9699\u3001\u8c03\u6574\u901f\u5ea6\u3001\u4ea4\u4e92\u4ee5\u521b\u5efa\u5e76\u9053\u95f4\u9699\uff0c\u5e76\u5728\u6709\u9650\u65f6\u95f4\u5185\u5b8c\u6210\u5e76\u9053\uff0c\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u548c\u8212\u9002\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u6210\u672c\u9879\u4ee5\u9002\u5e94\u5f3a\u5236\u5e76\u9053\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u5f15\u5165\u671f\u671b\u901f\u5ea6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0cSMART-Merge\u89c4\u5212\u5668\u4f7f\u8f66\u8f86\u80fd\u591f\u6210\u529f\u5e76\u9053\u5e76\u6700\u5c0f\u5316\u5e76\u9053\u65f6\u95f4\u3002", "result": "\u5728\u9ad8\u4fdd\u771fCarMaker\u6a21\u62df\u4e2d\uff0cSMART-Merge\u5728\u6570\u767e\u79cd\u9ad8\u901f\u516c\u8def\u5e76\u9053\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\uff0c\u5e76\u5728\u6700\u77ed\u65f6\u95f4\u5185\u5b8c\u6210\u5e76\u9053\u3002", "conclusion": "SMART-Merge\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u5f3a\u5236\u5e76\u9053\u4efb\u52a1\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9ad8\u901f\u516c\u8def\u5e76\u9053\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10893", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.10893", "abs": "https://arxiv.org/abs/2507.10893", "authors": ["Minjong Cheon", "Eunhan Goo", "Su-Hyeon Shin", "Muhammad Ahmed", "Hyungjun Kim"], "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency", "comment": "26pages, 9 Figures", "summary": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u8f7b\u91cf\u7ea7\u5168\u7403\u5929\u6c14\u9884\u62a5\u6a21\u578bKAI-a\uff0c\u6027\u80fd\u5ab2\u7f8e\u73b0\u6709Transformer\u6a21\u578b\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709AI\u5929\u6c14\u9884\u62a5\u6a21\u578b\u591a\u57fa\u4e8eTransformer\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u8d44\u6e90\u9700\u6c42\u5927\u3002", "method": "\u91c7\u7528\u73b0\u4ee3CNN\u67b6\u6784\uff0c\u7ed3\u5408\u5c3a\u5ea6\u4e0d\u53d8\u8bbe\u8ba1\u548cInceptionNext\u5757\uff0c\u9488\u5bf9\u5730\u7403\u7cfb\u7edf\u6570\u636e\u4f18\u5316\u3002", "result": "KAI-a\u5728\u4e2d\u7b49\u8303\u56f4\u9884\u62a5\u4e2d\u6027\u80fd\u4e0eSOTA\u76f8\u5f53\uff0c\u53c2\u6570\u4ec5700\u4e07\uff0c\u8bad\u7ec3\u65f6\u95f412\u5c0f\u65f6\u3002", "conclusion": "KAI-a\u5728\u6781\u7aef\u4e8b\u4ef6\uff08\u5982\u70ed\u6d6a\u3001\u5b63\u98ce\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u7528\u6027\u5f3a\u3002"}}
{"id": "2507.10620", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10620", "abs": "https://arxiv.org/abs/2507.10620", "authors": ["Chenxi Liu", "Hao Miao", "Cheng Long", "Yan Zhao", "Ziyue Li", "Panos Kalnis"], "title": "LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions", "comment": "Accepted at SSTD 2025 (Tutorial). arXiv admin note: text overlap with\n  arXiv:2505.02583", "summary": "Large Language Models (LLMs) have emerged as a promising paradigm for time\nseries analytics, leveraging their massive parameters and the shared sequential\nnature of textual and time series data. However, a cross-modality gap exists\nbetween time series and textual data, as LLMs are pre-trained on textual\ncorpora and are not inherently optimized for time series. In this tutorial, we\nprovide an up-to-date overview of LLM-based cross-modal time series analytics.\nWe introduce a taxonomy that classifies existing approaches into three groups\nbased on cross-modal modeling strategies, e.g., conversion, alignment, and\nfusion, and then discuss their applications across a range of downstream tasks.\nIn addition, we summarize several open challenges. This tutorial aims to expand\nthe practical application of LLMs in solving real-world problems in cross-modal\ntime series analytics while balancing effectiveness and efficiency.\nParticipants will gain a thorough understanding of current advancements,\nmethodologies, and future research directions in cross-modal time series\nanalytics.", "AI": {"tldr": "LLMs\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\u53ca\u8de8\u6a21\u6001\u6311\u6218\u6982\u8ff0\u3002", "motivation": "LLMs\u5728\u6587\u672c\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u4e0e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5b58\u5728\u8de8\u6a21\u6001\u5dee\u8ddd\uff0c\u9700\u4f18\u5316\u5176\u5e94\u7528\u3002", "method": "\u5206\u7c7b\u73b0\u6709\u65b9\u6cd5\u4e3a\u8f6c\u6362\u3001\u5bf9\u9f50\u548c\u878d\u5408\u4e09\u7c7b\uff0c\u5e76\u8ba8\u8bba\u5176\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u603b\u7ed3\u4e86\u5f53\u524d\u8fdb\u5c55\u3001\u65b9\u6cd5\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u65e8\u5728\u6269\u5c55LLMs\u5728\u8de8\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5e73\u8861\u6548\u679c\u4e0e\u6548\u7387\u3002"}}
{"id": "2507.10991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10991", "abs": "https://arxiv.org/abs/2507.10991", "authors": ["Abhimanyu Bhowmik", "Mohit Singh", "Madhushree Sannigrahi", "Martin Ludvigsen", "Kostas Alexis"], "title": "Uncertainty Aware Mapping for Vision-Based Underwater Robots", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Vision-based underwater robots can be useful in inspecting and exploring\nconfined spaces where traditional sensors and preplanned paths cannot be\nfollowed. Sensor noise and situational change can cause significant uncertainty\nin environmental representation. Thus, this paper explores how to represent\nmapping inconsistency in vision-based sensing and incorporate depth estimation\nconfidence into the mapping framework. The scene depth and the confidence are\nestimated using the RAFT-Stereo model and are integrated into a voxel-based\nmapping framework, Voxblox. Improvements in the existing Voxblox weight\ncalculation and update mechanism are also proposed. Finally, a qualitative\nanalysis of the proposed method is performed in a confined pool and in a pier\nin the Trondheim fjord. Experiments using an underwater robot demonstrated the\nchange in uncertainty in the visualization.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u89c6\u89c9\u6c34\u4e0b\u673a\u5668\u4eba\u4e2d\u8868\u793a\u5730\u56fe\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5c06\u6df1\u5ea6\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\u878d\u5165\u4f53\u7d20\u5730\u56fe\u6846\u67b6\u3002", "motivation": "\u4f20\u7edf\u4f20\u611f\u5668\u548c\u9884\u89c4\u5212\u8def\u5f84\u5728\u53d7\u9650\u7a7a\u95f4\u4e2d\u96be\u4ee5\u9002\u7528\uff0c\u4f20\u611f\u5668\u566a\u58f0\u548c\u73af\u5883\u53d8\u5316\u5bfc\u81f4\u73af\u5883\u8868\u793a\u4e0d\u786e\u5b9a\u6027\u9ad8\u3002", "method": "\u4f7f\u7528RAFT-Stereo\u6a21\u578b\u4f30\u8ba1\u573a\u666f\u6df1\u5ea6\u548c\u7f6e\u4fe1\u5ea6\uff0c\u6539\u8fdbVoxblox\u7684\u6743\u91cd\u8ba1\u7b97\u548c\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728\u53d7\u9650\u6c34\u6c60\u548c\u7279\u9686\u8d6b\u59c6\u5ce1\u6e7e\u7801\u5934\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u4e0d\u786e\u5b9a\u6027\u5728\u53ef\u89c6\u5316\u4e2d\u7684\u53d8\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u89c6\u89c9\u6c34\u4e0b\u673a\u5668\u4eba\u73af\u5883\u8868\u793a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.10895", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10895", "abs": "https://arxiv.org/abs/2507.10895", "authors": ["Xiaocong Zeng", "Craig Michoski", "Yan Pang", "Dongyang Kuang"], "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition", "comment": null, "summary": "In this work, we address the often-overlooked issue of Timescale Dependent\nLabel Inconsistency (TsDLI) in training neural network models for EEG-based\nhuman emotion recognition. To mitigate TsDLI and enhance model generalization\nand explainability, we propose two novel regularization strategies: Local\nVariation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods\nincorporate classical mathematical principles--specifically, functions of\nbounded variation and commute-time distances--within a graph theoretic\nframework. Complementing our regularizers, we introduce a suite of new\nevaluation metrics that better capture the alignment between temporally local\npredictions and their associated global emotion labels. We validate our\napproach through comprehensive experiments on two widely used EEG emotion\ndatasets, DREAMER and DEAP, across a range of neural architectures including\nLSTM and transformer-based models. Performance is assessed using five distinct\nmetrics encompassing both quantitative accuracy and qualitative consistency.\nResults consistently show that our proposed methods outperform state-of-the-art\nbaselines, delivering superior aggregate performance and offering a principled\ntrade-off between interpretability and predictive power under label\ninconsistency. Notably, LVL achieves the best aggregate rank across all\nbenchmarked backbones and metrics, while LGCL frequently ranks the second,\nhighlighting the effectiveness of our framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6b63\u5219\u5316\u7b56\u7565\uff08LVL\u548cLGCL\uff09\u89e3\u51b3EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u65f6\u95f4\u5c3a\u5ea6\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u65f6\u95f4\u5c3a\u5ea6\u4f9d\u8d56\u6807\u7b7e\u4e0d\u4e00\u81f4\uff08TsDLI\uff09\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u53d8\u5316\u635f\u5931\uff08LVL\uff09\u548c\u5c40\u90e8-\u5168\u5c40\u4e00\u81f4\u6027\u635f\u5931\uff08LGCL\uff09\uff0c\u7ed3\u5408\u6709\u754c\u53d8\u5dee\u51fd\u6570\u548c\u901a\u52e4\u65f6\u95f4\u8ddd\u79bb\u7684\u6570\u5b66\u539f\u7406\u3002", "result": "\u5728DREAMER\u548cDEAP\u6570\u636e\u96c6\u4e0a\uff0cLVL\u548cLGCL\u5728\u591a\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cLVL\u7efc\u5408\u6392\u540d\u6700\u4f73\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5e73\u8861\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.10623", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10623", "abs": "https://arxiv.org/abs/2507.10623", "authors": ["Daniel Saragih", "Deyu Cao", "Tejas Balaji"], "title": "Flows and Diffusions on the Neural Manifold", "comment": "40 pages, 6 figures, 13 tables", "summary": "Diffusion and flow-based generative models have achieved remarkable success\nin domains such as image synthesis, video generation, and natural language\nmodeling. In this work, we extend these advances to weight space learning by\nleveraging recent techniques to incorporate structural priors derived from\noptimization dynamics. Central to our approach is modeling the trajectory\ninduced by gradient descent as a trajectory inference problem. We unify several\ntrajectory inference techniques under the framework of gradient flow matching,\nproviding a theoretical framework for treating optimization paths as inductive\nbias. We further explore architectural and algorithmic choices, including\nreward fine-tuning by adjoint matching, the use of autoencoders for latent\nweight representation, conditioning on task-specific context data, and adopting\ninformative source distributions such as Kaiming uniform. Experiments\ndemonstrate that our method matches or surpasses baselines in generating\nin-distribution weights, improves initialization for downstream training, and\nsupports fine-tuning to enhance performance. Finally, we illustrate a practical\napplication in safety-critical systems: detecting harmful covariate shifts,\nwhere our method outperforms the closest comparable baseline.", "AI": {"tldr": "\u5c06\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u6269\u5c55\u5230\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\uff0c\u5229\u7528\u68af\u5ea6\u6d41\u5339\u914d\u6846\u67b6\u7edf\u4e00\u8f68\u8ff9\u63a8\u65ad\u6280\u672f\uff0c\u63d0\u5347\u6743\u91cd\u751f\u6210\u548c\u521d\u59cb\u5316\u6548\u679c\u3002", "motivation": "\u6269\u5c55\u6269\u6563\u548c\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u5728\u6743\u91cd\u7a7a\u95f4\u5b66\u4e60\u7684\u5e94\u7528\uff0c\u5229\u7528\u4f18\u5316\u52a8\u6001\u7684\u7ed3\u6784\u5148\u9a8c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5c06\u68af\u5ea6\u4e0b\u964d\u8f68\u8ff9\u5efa\u6a21\u4e3a\u8f68\u8ff9\u63a8\u65ad\u95ee\u9898\uff0c\u91c7\u7528\u68af\u5ea6\u6d41\u5339\u914d\u6846\u67b6\uff0c\u7ed3\u5408\u5956\u52b1\u5fae\u8c03\u3001\u81ea\u7f16\u7801\u5668\u3001\u4efb\u52a1\u4e0a\u4e0b\u6587\u6570\u636e\u7b49\u3002", "result": "\u5728\u751f\u6210\u5206\u5e03\u5185\u6743\u91cd\u3001\u4e0b\u6e38\u8bad\u7ec3\u521d\u59cb\u5316\u548c\u5fae\u8c03\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5e76\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u68c0\u6d4b\u6709\u5bb3\u534f\u53d8\u91cf\u504f\u79fb\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u68af\u5ea6\u6d41\u5339\u914d\u6846\u67b6\u4e3a\u4f18\u5316\u8def\u5f84\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u6743\u91cd\u751f\u6210\u548c\u521d\u59cb\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11000", "abs": "https://arxiv.org/abs/2507.11000", "authors": ["Minwoo Cho", "Jaehwi Jang", "Daehyung Park"], "title": "ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations", "comment": "8 pages, 6 figures", "summary": "We aim to solve the problem of temporal-constraint learning from\ndemonstrations to reproduce demonstration-like logic-constrained behaviors.\nLearning logic constraints is challenging due to the combinatorially large\nspace of possible specifications and the ill-posed nature of non-Markovian\nconstraints. To figure it out, we introduce a novel temporal-constraint\nlearning method, which we call inverse logic-constraint learning (ILCL). Our\nmethod frames ICL as a two-player zero-sum game between 1) a genetic\nalgorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained\nreinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax\ntrees for parameterized truncated linear temporal logic (TLTL) without\npredefined templates. Subsequently, Logic-CRL finds a policy that maximizes\ntask rewards under the constructed TLTL constraints via a novel constraint\nredistribution scheme. Our evaluations show ILCL outperforms state-of-the-art\nbaselines in learning and transferring TL constraints on four temporally\nconstrained tasks. We also demonstrate successful transfer to real-world\npeg-in-shallow-hole tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65f6\u95f4\u7ea6\u675f\u5b66\u4e60\u65b9\u6cd5\uff08ILCL\uff09\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u548c\u903b\u8f91\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u535a\u5f08\uff0c\u9ad8\u6548\u5b66\u4e60\u5e76\u8f6c\u79fb\u65f6\u95f4\u903b\u8f91\u7ea6\u675f\u3002", "motivation": "\u89e3\u51b3\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u65f6\u95f4\u7ea6\u675f\u4ee5\u590d\u73b0\u7c7b\u4f3c\u903b\u8f91\u7ea6\u675f\u884c\u4e3a\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u7ec4\u5408\u7a7a\u95f4\u5927\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u7ea6\u675f\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "method": "ILCL\u65b9\u6cd5\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u7684\u65f6\u95f4\u903b\u8f91\u6316\u6398\uff08GA-TL-Mining\uff09\u548c\u903b\u8f91\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff08Logic-CRL\uff09\uff0c\u901a\u8fc7\u96f6\u548c\u535a\u5f08\u6846\u67b6\u5b66\u4e60TLTL\u7ea6\u675f\u3002", "result": "\u5728\u56db\u4e2a\u65f6\u95f4\u7ea6\u675f\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u7684\u6d45\u5b54\u63d2\u6869\u4efb\u52a1\u3002", "conclusion": "ILCL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u7ea6\u675f\u5b66\u4e60\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10935", "abs": "https://arxiv.org/abs/2507.10935", "authors": ["Shaowen Tong", "Zimin Xia", "Alexandre Alahi", "Xuming He", "Yujiao Shi"], "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization", "comment": "accepted by ICCV2025", "summary": "Cross-view localization, the task of estimating a camera's\n3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with\nsatellite images, is crucial for large-scale outdoor applications like\nautonomous navigation and augmented reality. Existing methods often rely on\nfully supervised learning, which requires costly ground-truth pose annotations.\nIn this work, we propose GeoDistill, a Geometry guided weakly supervised self\ndistillation framework that uses teacher-student learning with Field-of-View\n(FoV)-based masking to enhance local feature learning for robust cross-view\nlocalization. In GeoDistill, the teacher model localizes a panoramic image,\nwhile the student model predicts locations from a limited FoV counterpart\ncreated by FoV-based masking. By aligning the student's predictions with those\nof the teacher, the student focuses on key features like lane lines and ignores\ntextureless regions, such as roads. This results in more accurate predictions\nand reduced uncertainty, regardless of whether the query images are panoramas\nor limited FoV images. Our experiments show that GeoDistill significantly\nimproves localization performance across different frameworks. Additionally, we\nintroduce a novel orientation estimation network that predicts relative\norientation without requiring precise planar position ground truth. GeoDistill\nprovides a scalable and efficient solution for real-world cross-view\nlocalization challenges. Code and model can be found at\nhttps://github.com/tongshw/GeoDistill.", "AI": {"tldr": "GeoDistill\u662f\u4e00\u79cd\u51e0\u4f55\u5f15\u5bfc\u7684\u5f31\u76d1\u7763\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u5b66\u4e60\u548c\u57fa\u4e8e\u89c6\u573a\u7684\u63a9\u7801\u63d0\u5347\u8de8\u89c6\u56fe\u5b9a\u4f4d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5168\u76d1\u7763\u5b66\u4e60\uff0c\u9700\u8981\u6602\u8d35\u7684\u771f\u5b9e\u59ff\u6001\u6807\u6ce8\uff0cGeoDistill\u65e8\u5728\u51cf\u5c11\u8fd9\u79cd\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u6559\u5e08\u6a21\u578b\u5b9a\u4f4d\u5168\u666f\u56fe\u50cf\uff0c\u5b66\u751f\u6a21\u578b\u9884\u6d4b\u6709\u9650\u89c6\u573a\u56fe\u50cf\u7684\u4f4d\u7f6e\uff0c\u901a\u8fc7\u5bf9\u9f50\u9884\u6d4b\u7ed3\u679c\u63d0\u5347\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGeoDistill\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u65e0\u9700\u7cbe\u786e\u5e73\u9762\u4f4d\u7f6e\u7684\u65b0\u65b9\u5411\u4f30\u8ba1\u7f51\u7edc\u3002", "conclusion": "GeoDistill\u4e3a\u8de8\u89c6\u56fe\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10626", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10626", "abs": "https://arxiv.org/abs/2507.10626", "authors": ["Lintao Wang", "Shiwen Xu", "Michael Horton", "Joachim Gudmundsson", "Zhiyong Wang"], "title": "Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction", "comment": null, "summary": "Predicting soccer match outcomes is a challenging task due to the inherently\nunpredictable nature of the game and the numerous dynamic factors influencing\nresults. While it conventionally relies on meticulous feature engineering, deep\nlearning techniques have recently shown a great promise in learning effective\nplayer and team representations directly for soccer outcome prediction.\nHowever, existing methods often overlook the heterogeneous nature of\ninteractions among players and teams, which is crucial for accurately modeling\nmatch dynamics. To address this gap, we propose HIGFormer (Heterogeneous\nInteraction Graph Transformer), a novel graph-augmented transformer-based deep\nlearning model for soccer outcome prediction. HIGFormer introduces a\nmulti-level interaction framework that captures both fine-grained player\ndynamics and high-level team interactions. Specifically, it comprises (1) a\nPlayer Interaction Network, which encodes player performance through\nheterogeneous interaction graphs, combining local graph convolutions with a\nglobal graph-augmented transformer; (2) a Team Interaction Network, which\nconstructs interaction graphs from a team-to-team perspective to model\nhistorical match relationships; and (3) a Match Comparison Transformer, which\njointly analyzes both team and player-level information to predict match\noutcomes. Extensive experiments on the WyScout Open Access Dataset, a\nlarge-scale real-world soccer dataset, demonstrate that HIGFormer significantly\noutperforms existing methods in prediction accuracy. Furthermore, we provide\nvaluable insights into leveraging our model for player performance evaluation,\noffering a new perspective on talent scouting and team strategy analysis.", "AI": {"tldr": "HIGFormer\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u589e\u5f3aTransformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u8db3\u7403\u6bd4\u8d5b\u7ed3\u679c\uff0c\u901a\u8fc7\u591a\u7ea7\u4ea4\u4e92\u6846\u67b6\u6355\u6349\u7403\u5458\u548c\u56e2\u961f\u7684\u52a8\u6001\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5ffd\u89c6\u4e86\u7403\u5458\u548c\u56e2\u961f\u4e4b\u95f4\u5f02\u8d28\u6027\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u800c\u8fd9\u4e9b\u5bf9\u51c6\u786e\u5efa\u6a21\u6bd4\u8d5b\u52a8\u6001\u81f3\u5173\u91cd\u8981\u3002", "method": "HIGFormer\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aPlayer Interaction Network\uff08\u7f16\u7801\u7403\u5458\u8868\u73b0\uff09\u3001Team Interaction Network\uff08\u5efa\u6a21\u56e2\u961f\u5386\u53f2\u5173\u7cfb\uff09\u548cMatch Comparison Transformer\uff08\u7efc\u5408\u5206\u6790\u9884\u6d4b\u7ed3\u679c\uff09\u3002", "result": "\u5728WyScout\u6570\u636e\u96c6\u4e0a\uff0cHIGFormer\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "HIGFormer\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u4e3a\u7403\u5458\u8bc4\u4f30\u548c\u56e2\u961f\u7b56\u7565\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.11001", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11001", "abs": "https://arxiv.org/abs/2507.11001", "authors": ["Yanbo Wang", "Zipeng Fang", "Lei Zhao", "Weidong Chen"], "title": "Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation", "comment": null, "summary": "Service robots are increasingly deployed in diverse and dynamic environments,\nwhere both physical layouts and social contexts change over time and across\nlocations. In these unstructured settings, conventional navigation systems that\nrely on fixed parameters often fail to generalize across scenarios, resulting\nin degraded performance and reduced social acceptance. Although recent\napproaches have leveraged reinforcement learning to enhance traditional\nplanners, these methods often fail in real-world deployments due to poor\ngeneralization and limited simulation diversity, which hampers effective\nsim-to-real transfer. To tackle these issues, we present LE-Nav, an\ninterpretable and scene-aware navigation framework that leverages multi-modal\nlarge language model reasoning and conditional variational autoencoders to\nadaptively tune planner hyperparameters. To achieve zero-shot scene\nunderstanding, we utilize one-shot exemplars and chain-of-thought prompting\nstrategies. Additionally, a conditional variational autoencoder captures the\nmapping between natural language instructions and navigation hyperparameters,\nenabling expert-level tuning. Experiments show that LE-Nav can generate\nhyperparameters achieving human-level tuning across diverse planners and\nscenarios. Real-world navigation trials and a user study on a smart wheelchair\nplatform demonstrate that it outperforms state-of-the-art methods on\nquantitative metrics such as success rate, efficiency, safety, and comfort,\nwhile receiving higher subjective scores for perceived safety and social\nacceptance. Code is available at https://github.com/Cavendish518/LE-Nav.", "AI": {"tldr": "LE-Nav\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u89c4\u5212\u5668\u8d85\u53c2\u6570\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u4f9d\u8d56\u56fa\u5b9a\u53c2\u6570\uff0c\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u548c\u591a\u6837\u5316\u73af\u5883\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u548c\u793e\u4f1a\u63a5\u53d7\u5ea6\u964d\u4f4e\u3002\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u4eff\u771f\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u4e2d\u5e94\u7528\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5355\u6837\u672c\u793a\u4f8b\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7b56\u7565\u5b9e\u73b0\u96f6\u6837\u672c\u573a\u666f\u7406\u89e3\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u89c4\u5212\u5668\u8d85\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLE-Nav\u80fd\u751f\u6210\u4eba\u7c7b\u6c34\u5e73\u7684\u8d85\u53c2\u6570\uff0c\u5728\u771f\u5b9e\u5bfc\u822a\u8bd5\u9a8c\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0c\u5176\u5728\u6210\u529f\u7387\u3001\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LE-Nav\u901a\u8fc7\u573a\u666f\u611f\u77e5\u548c\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u548c\u793e\u4f1a\u63a5\u53d7\u5ea6\u3002"}}
{"id": "2507.10938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10938", "abs": "https://arxiv.org/abs/2507.10938", "authors": ["Zhengyi Xu", "Haoran Wu", "Wen Jiang", "Jie Geng"], "title": "Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing", "comment": null, "summary": "Semantic change detection (SCD) extends the binary change detection task to\nprovide not only the change locations but also the detailed \"from-to\"\ncategories in multi-temporal remote sensing data. Such detailed semantic\ninsights into changes offer considerable advantages for a wide array of\napplications. However, since SCD involves the simultaneous optimization of\nmultiple tasks, the model is prone to negative transfer due to task-specific\nlearning difficulties and conflicting gradient flows. To address this issue, we\npropose Graph Aggregation Prototype Learning for Semantic Change Detection in\nremote sensing(GAPL-SCD). In this framework, a multi-task joint optimization\nmethod is designed to optimize the primary task of semantic segmentation and\nchange detection, along with the auxiliary task of graph aggregation prototype\nlearning. Adaptive weight allocation and gradient rotation methods are used to\nalleviate the conflict between training tasks and improve multi-task learning\ncapabilities. Specifically, the graph aggregation prototype learning module\nconstructs an interaction graph using high-level features. Prototypes serve as\nclass proxies, enabling category-level domain alignment across time points and\nreducing interference from irrelevant changes. Additionally, the proposed\nself-query multi-level feature interaction and bi-temporal feature fusion\nmodules further enhance multi-scale feature representation, improving\nperformance in complex scenes. Experimental results on the SECOND and\nLandsat-SCD datasets demonstrate that our method achieves state-of-the-art\nperformance, with significant improvements in accuracy and robustness for SCD\ntask.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAPL-SCD\u7684\u56fe\u805a\u5408\u539f\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u8054\u5408\u4f18\u5316\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\uff08SCD\uff09\u9700\u8981\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u4efb\u52a1\uff0c\u5bb9\u6613\u56e0\u4efb\u52a1\u95f4\u51b2\u7a81\u5bfc\u81f4\u8d1f\u8fc1\u79fb\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u4efb\u52a1\u8054\u5408\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u805a\u5408\u539f\u578b\u5b66\u4e60\u6a21\u5757\u3001\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u548c\u68af\u5ea6\u65cb\u8f6c\u65b9\u6cd5\u3002", "result": "\u5728SECOND\u548cLandsat-SCD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86SCD\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GAPL-SCD\u901a\u8fc7\u591a\u4efb\u52a1\u4f18\u5316\u548c\u7279\u5f81\u4ea4\u4e92\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bed\u4e49\u53d8\u5316\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10628", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10628", "abs": "https://arxiv.org/abs/2507.10628", "authors": ["Ziru Liu", "Cheng Gong", "Xinyu Fu", "Yaofang Liu", "Ran Chen", "Shoubo Hu", "Suiyun Zhang", "Rui Liu", "Qingfu Zhang", "Dandan Tu"], "title": "GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for facilitating the self-improvement of large language\nmodels (LLMs), particularly in the domain of complex reasoning tasks. However,\nprevailing on-policy RL methods often contend with significant training\ninstability and inefficiency. This is primarily due to a capacity-difficulty\nmismatch, where the complexity of training data frequently outpaces the model's\ncurrent capabilities, leading to critically sparse reward signals and stalled\nlearning progress. This challenge is particularly acute for smaller, more\nresource-efficient LLMs. To overcome this, we introduce the Guided Hybrid\nPolicy Optimization (GHPO), a novel difficulty-aware reinforcement learning\nframework. GHPO dynamically calibrates task difficulty by employing adaptive\nprompt refinement to provide targeted guidance. This unique approach adaptively\nbalances direct imitation learning for problems currently beyond the model's\nreach with exploration-based reinforcement learning for more manageable tasks,\neffectively creating a smooth and optimized learning curriculum. Extensive\nexperiments demonstrate that GHPO achieves an average performance gain of\napproximately 5% across six challenging mathematics benchmarks, consistently\noutperforming strong on-policy reinforcement learning and curriculum learning\nbaselines. Further analysis confirms that our framework significantly enhances\nboth training stability and final reasoning performance, thus offering a\nscalable and efficient solution for developing powerful and robust reasoning\nmodels.", "AI": {"tldr": "GHPO\u662f\u4e00\u79cd\u65b0\u9896\u7684\u96be\u5ea6\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u96be\u5ea6\u548c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u4e0e\u63a2\u7d22\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5c0f\u578bLLMs\u9762\u4e34\u80fd\u529b\u4e0e\u4efb\u52a1\u96be\u5ea6\u4e0d\u5339\u914d\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faGHPO\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63d0\u793a\u7ec6\u5316\u52a8\u6001\u6821\u51c6\u4efb\u52a1\u96be\u5ea6\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\uff08\u89e3\u51b3\u8d85\u51fa\u80fd\u529b\u7684\u95ee\u9898\uff09\u548c\u63a2\u7d22\u5b66\u4e60\uff08\u5904\u7406\u53ef\u7ba1\u7406\u4efb\u52a1\uff09\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u5347\u7ea65%\uff0c\u4f18\u4e8e\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u548c\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "GHPO\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u5f3a\u5927\u4e14\u7a33\u5065\u7684\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11006", "abs": "https://arxiv.org/abs/2507.11006", "authors": ["Ashutosh Mishra", "Shreya Santra", "Hazal Gozbasi", "Kentaro Uno", "Kazuya Yoshida"], "title": "Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments", "comment": "6 pages, 7 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)", "summary": "This study presents an advanced approach to enhance robotic manipulation in\nuncertain and challenging environments, with a focus on autonomous operations\naugmented by human-in-the-loop (HITL) control for lunar missions. By\nintegrating human decision-making with autonomous robotic functions, the\nresearch improves task reliability and efficiency for space applications. The\nkey task addressed is the autonomous deployment of flexible solar panels using\nan extendable ladder-like structure and a robotic manipulator with real-time\nfeedback for precision. The manipulator relays position and force-torque data,\nenabling dynamic error detection and adaptive control during deployment. To\nmitigate the effects of sinkage, variable payload, and low-lighting conditions,\nefficient motion planning strategies are employed, supplemented by human\ncontrol that allows operators to intervene in ambiguous scenarios. Digital twin\nsimulation enhances system robustness by enabling continuous feedback,\niterative task refinement, and seamless integration with the deployment\npipeline. The system has been tested to validate its performance in simulated\nlunar conditions and ensure reliability in extreme lighting, variable terrain,\nchanging payloads, and sensor limitations.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u4e3b\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u63a7\u5236\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u6708\u7403\u4efb\u52a1\u4e2d\u7684\u67d4\u6027\u592a\u9633\u80fd\u677f\u90e8\u7f72\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5728\u4e0d\u786e\u5b9a\u548c\u6311\u6218\u6027\u7684\u6708\u7403\u73af\u5883\u4e2d\uff0c\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u51b3\u7b56\u4e0e\u81ea\u4e3b\u529f\u80fd\u3002", "method": "\u91c7\u7528\u53ef\u6269\u5c55\u7684\u68af\u72b6\u7ed3\u6784\u548c\u673a\u5668\u4eba\u64cd\u7eb5\u5668\uff0c\u7ed3\u5408\u5b9e\u65f6\u53cd\u9988\u3001\u52a8\u6001\u8bef\u5dee\u68c0\u6d4b\u548c\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u8f85\u4ee5\u4eba\u7c7b\u5e72\u9884\u548c\u6570\u5b57\u5b6a\u751f\u4eff\u771f\u3002", "result": "\u7cfb\u7edf\u5728\u6a21\u62df\u6708\u7403\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u6027\u80fd\uff0c\u80fd\u591f\u5e94\u5bf9\u6781\u7aef\u5149\u7167\u3001\u591a\u53d8\u5730\u5f62\u3001\u53d8\u5316\u8f7d\u8377\u548c\u4f20\u611f\u5668\u9650\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6708\u7403\u4efb\u52a1\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u6765\u7a7a\u95f4\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10943", "abs": "https://arxiv.org/abs/2507.10943", "authors": ["Yushun Fang", "Lu Liu", "Xiang Gao", "Qiang Hu", "Ning Cao", "Jianghe Cui", "Gang Chen", "Xiaoyun Zhang"], "title": "Robust ID-Specific Face Restoration via Alignment Learning", "comment": "17 pages, 8 figures", "summary": "The latest developments in Face Restoration have yielded significant\nadvancements in visual quality through the utilization of diverse diffusion\npriors. Nevertheless, the uncertainty of face identity introduced by\nidentity-obscure inputs and stochastic generative processes remains unresolved.\nTo address this challenge, we present Robust ID-Specific Face Restoration\n(RIDFR), a novel ID-specific face restoration framework based on diffusion\nmodels. Specifically, RIDFR leverages a pre-trained diffusion model in\nconjunction with two parallel conditioning modules. The Content Injection\nModule inputs the severely degraded image, while the Identity Injection Module\nintegrates the specific identity from a given image. Subsequently, RIDFR\nincorporates Alignment Learning, which aligns the restoration results from\nmultiple references with the same identity in order to suppress the\ninterference of ID-irrelevant face semantics (e.g. pose, expression, make-up,\nhair style). Experiments demonstrate that our framework outperforms the\nstate-of-the-art methods, reconstructing high-quality ID-specific results with\nhigh identity fidelity and demonstrating strong robustness.", "AI": {"tldr": "RIDFR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u578bID\u7279\u5b9a\u4eba\u8138\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u6761\u4ef6\u6a21\u5757\u548c\u8eab\u4efd\u5bf9\u9f50\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u8eab\u4efd\u6a21\u7cca\u8f93\u5165\u548c\u968f\u673a\u751f\u6210\u8fc7\u7a0b\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u4eba\u8138\u4fee\u590d\u6280\u672f\u867d\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u8eab\u4efd\u4e0d\u786e\u5b9a\u6027\u4ecd\u672a\u89e3\u51b3\uff0cRIDFR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "RIDFR\u7ed3\u5408\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u548c\u4e24\u4e2a\u5e76\u884c\u6761\u4ef6\u6a21\u5757\uff08\u5185\u5bb9\u6ce8\u5165\u548c\u8eab\u4efd\u6ce8\u5165\uff09\uff0c\u5e76\u901a\u8fc7\u8eab\u4efd\u5bf9\u9f50\u5b66\u4e60\u6291\u5236\u65e0\u5173\u8bed\u4e49\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRIDFR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u91cd\u5efa\u9ad8\u8d28\u91cf\u4e14\u8eab\u4efd\u4fdd\u771f\u7684\u7ed3\u679c\u3002", "conclusion": "RIDFR\u5728\u8eab\u4efd\u7279\u5b9a\u4eba\u8138\u4fee\u590d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u4fdd\u771f\u6027\u548c\u5f3a\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10632", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10632", "abs": "https://arxiv.org/abs/2507.10632", "authors": ["Issei Saito", "Masatoshi Nagano", "Tomoaki Nakamura", "Daichi Mochihashi", "Koki Mimura"], "title": "Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process", "comment": null, "summary": "In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series\nsegmentation method that incorporates random Fourier features (RFF) to address\nthe high computational cost of the Gaussian process hidden semi-Markov model\n(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring\ninversion of an N times N kernel matrix during training, where N is the number\nof data points. As the scale of the data increases, matrix inversion incurs a\nsignificant computational cost. To address this, the proposed method\napproximates the Gaussian process with linear regression using RFF, preserving\nexpressive power while eliminating the need for inversion of the kernel matrix.\nExperiments on the Carnegie Mellon University (CMU) motion-capture dataset\ndemonstrate that the proposed method achieves segmentation performance\ncomparable to that of conventional methods, with approximately 278 times faster\nsegmentation on time-series data comprising 39,200 frames.", "AI": {"tldr": "RFF-GP-HSMM\u662f\u4e00\u79cd\u5feb\u901f\u65e0\u76d1\u7763\u65f6\u95f4\u5e8f\u5217\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\uff08RFF\uff09\u964d\u4f4eGP-HSMM\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u9690\u534a\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08GP-HSMM\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u65f6\uff0c\u77e9\u9635\u6c42\u9006\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5229\u7528RFF\u8fd1\u4f3c\u9ad8\u65af\u8fc7\u7a0b\uff0c\u8f6c\u5316\u4e3a\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff0c\u907f\u514d\u6838\u77e9\u9635\u6c42\u9006\u3002", "result": "\u5728CMU\u8fd0\u52a8\u6355\u6349\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u5272\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u901f\u5ea6\u63d0\u5347\u7ea6278\u500d\u3002", "conclusion": "RFF-GP-HSMM\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.11069", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11069", "abs": "https://arxiv.org/abs/2507.11069", "authors": ["Jeongyun Kim", "Seunghoon Jeong", "Giseop Kim", "Myung-Hwan Jeon", "Eunji Jun", "Ayoung Kim"], "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update", "comment": null, "summary": "Understanding the 3D geometry of transparent objects from RGB images is\nchallenging due to their inherent physical properties, such as reflection and\nrefraction. To address these difficulties, especially in scenarios with sparse\nviews and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian\nSplatting-based depth reconstruction method for transparent objects. Our key\ninsight lies in separating transparent objects from the background, enabling\nfocused optimization of Gaussians corresponding to the object. We mitigate\nartifacts with an object-aware loss that places Gaussians in obscured regions,\nensuring coverage of invisible surfaces while reducing overfitting.\nFurthermore, we incorporate a physics-based simulation that refines the\nreconstruction in just a few seconds, effectively handling object removal and\nchain-reaction movement of remaining objects without the need for rescanning.\nTRAN-D is evaluated on both synthetic and real-world sequences, and it\nconsistently demonstrated robust improvements over existing GS-based\nstate-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean\nabsolute error by over 39% for the synthetic TRansPose sequences. Furthermore,\ndespite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm\naccuracy of 48.46%, over 1.5 times that of baselines, which uses six images.\nCode and more results are available at https://jeongyun0609.github.io/TRAN-D/.", "AI": {"tldr": "TRAN-D\u662f\u4e00\u79cd\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u900f\u660e\u7269\u4f53\u6df1\u5ea6\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u900f\u660e\u7269\u4f53\u4e0e\u80cc\u666f\u3001\u4f18\u5316\u9ad8\u65af\u5206\u5e03\u548c\u7269\u7406\u6a21\u62df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u4e0b\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u900f\u660e\u7269\u4f53\u76843D\u51e0\u4f55\u91cd\u5efa\u56e0\u53cd\u5c04\u548c\u6298\u5c04\u7b49\u7269\u7406\u7279\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u4e2d\u3002", "method": "TRAN-D\u901a\u8fc7\u5206\u79bb\u900f\u660e\u7269\u4f53\u4e0e\u80cc\u666f\uff0c\u4f18\u5316\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u7269\u4f53\u611f\u77e5\u635f\u5931\u548c\u7269\u7406\u6a21\u62df\u6765\u51cf\u5c11\u4f2a\u5f71\u548c\u8fc7\u62df\u5408\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5e8f\u5217\u4e2d\uff0cTRAN-D\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e\u4e8639%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff0c\u5e76\u5728\u4ec5\u4f7f\u7528\u4e00\u5f20\u56fe\u50cf\u65f6\u8fbe\u523048.46%\u7684\u7cbe\u5ea6\u3002", "conclusion": "TRAN-D\u5728\u900f\u660e\u7269\u4f53\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2507.10969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10969", "abs": "https://arxiv.org/abs/2507.10969", "authors": ["Palash Ray", "Mahuya Sasmal", "Asish Bera"], "title": "Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data", "comment": null, "summary": "Sports action classification representing complex body postures and\nplayer-object interactions is an emerging area in image-based sports analysis.\nSome works have contributed to automated sports action recognition using\nmachine learning techniques over the past decades. However, sufficient image\ndatasets representing women sports actions with enough intra- and inter-class\nvariations are not available to the researchers. To overcome this limitation,\nthis work presents a new dataset named WomenSports for women sports\nclassification using small-scale training data. This dataset includes a variety\nof sports activities, covering wide variations in movements, environments, and\ninteractions among players. In addition, this study proposes a convolutional\nneural network (CNN) for deep feature extraction. A channel attention scheme\nupon local contextual regions is applied to refine and enhance feature\nrepresentation. The experiments are carried out on three different sports\ndatasets and one dance dataset for generalizing the proposed algorithm, and the\nperformances on these datasets are noteworthy. The deep learning method\nachieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed\nWomenSports dataset, which is publicly available for research at Mendeley Data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u5973\u6027\u4f53\u80b2\u52a8\u4f5c\u6570\u636e\u96c6WomenSports\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u7684CNN\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e8689.15%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5973\u6027\u4f53\u80b2\u52a8\u4f5c\u7684\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u76f8\u5173\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e00\u4e2aCNN\u6a21\u578b\uff0c\u7ed3\u5408\u901a\u9053\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u7279\u5f81\u8868\u793a\u3002", "result": "\u5728WomenSports\u6570\u636e\u96c6\u4e0a\u8fbe\u523089.15%\u7684Top-1\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "WomenSports\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5973\u6027\u4f53\u80b2\u52a8\u4f5c\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.10636", "categories": ["cs.LG", "cs.AI", "cs.NE", "cs.RO", "90B06", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.10636", "abs": "https://arxiv.org/abs/2507.10636", "authors": ["Jianing Zhi", "Xinghua Li", "Zidong Chen"], "title": "GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem", "comment": "12 Pages, 5 Figures", "summary": "The rapid development of urban low-altitude unmanned aerial vehicle (UAV)\neconomy poses new challenges for dynamic site selection of UAV landing points\nand supply stations. Traditional deep reinforcement learning methods face\ncomputational complexity bottlenecks, particularly with standard attention\nmechanisms, when handling large-scale urban-level location problems. This paper\nproposes GeoHopNet, a Hopfield-augmented sparse spatial attention network\nspecifically designed for dynamic UAV site location problems. Our approach\nintroduces four core innovations: (1) distance-biased multi-head attention\nmechanism that explicitly encodes spatial geometric information; (2) K-nearest\nneighbor sparse attention that reduces computational complexity from $O(N^2)$\nto $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory\nregularization strategy. Experimental results demonstrate that GeoHopNet\nextends the boundary of solvable problem sizes. For large-scale instances with\n1,000 nodes, where standard attention models become prohibitively slow (over 3\nseconds per instance) and traditional solvers fail, GeoHopNet finds\nhigh-quality solutions (0.22\\% optimality gap) in under 0.1 seconds. Compared\nto the state-of-the-art ADNet baseline on 100-node instances, our method\nimproves solution quality by 22.2\\% and is 1.8$\\times$ faster.", "AI": {"tldr": "GeoHopNet\u662f\u4e00\u79cd\u7528\u4e8e\u65e0\u4eba\u673a\u52a8\u6001\u9009\u5740\u7684Hopfield\u589e\u5f3a\u7a00\u758f\u7a7a\u95f4\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u901a\u8fc7\u56db\u79cd\u521b\u65b0\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u57ce\u5e02\u7ea7\u65e0\u4eba\u673a\u9009\u5740\u95ee\u9898\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u8ddd\u79bb\u504f\u7f6e\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u3001K\u8fd1\u90bb\u7a00\u758f\u6ce8\u610f\u529b\u3001\u73b0\u4ee3Hopfield\u5916\u90e8\u8bb0\u5fc6\u6a21\u5757\u548c\u8bb0\u5fc6\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u57281000\u8282\u70b9\u7684\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\uff0cGeoHopNet\u57280.1\u79d2\u5185\u627e\u5230\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\uff080.22%\u6700\u4f18\u6027\u5dee\u8ddd\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GeoHopNet\u6269\u5c55\u4e86\u53ef\u89e3\u51b3\u95ee\u9898\u7684\u89c4\u6a21\u8fb9\u754c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002"}}
{"id": "2507.11076", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.11076", "abs": "https://arxiv.org/abs/2507.11076", "authors": ["Andreas Mueller", "Shivesh Kumar"], "title": "Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems", "comment": null, "summary": "Derivatives of equations of motion(EOM) describing the dynamics of rigid body\nsystems are becoming increasingly relevant for the robotics community and find\nmany applications in design and control of robotic systems. Controlling robots,\nand multibody systems comprising elastic components in particular, not only\nrequires smooth trajectories but also the time derivatives of the control\nforces/torques, hence of the EOM. This paper presents the time derivatives of\nthe EOM in closed form up to second-order as an alternative formulation to the\nexisting recursive algorithms for this purpose, which provides a direct insight\ninto the structure of the derivatives. The Lie group formulation for rigid body\nsystems is used giving rise to very compact and easily parameterized equations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u521a\u4f53\u7cfb\u7edf\u8fd0\u52a8\u65b9\u7a0b\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u95ed\u5f0f\u89e3\uff0c\u66ff\u4ee3\u73b0\u6709\u9012\u5f52\u7b97\u6cd5\uff0c\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u7ed3\u6784\u6d1e\u5bdf\u3002", "motivation": "\u673a\u5668\u4eba\u63a7\u5236\u9700\u8981\u5e73\u6ed1\u8f68\u8ff9\u53ca\u63a7\u5236\u529b/\u529b\u77e9\u7684\u65f6\u95f4\u5bfc\u6570\uff0c\u73b0\u6709\u9012\u5f52\u7b97\u6cd5\u590d\u6742\uff0c\u9700\u66f4\u76f4\u63a5\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u674e\u7fa4\u7406\u8bba\uff0c\u63a8\u5bfc\u51fa\u7d27\u51d1\u4e14\u6613\u53c2\u6570\u5316\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u95ed\u5f0f\u89e3\u3002", "result": "\u63d0\u51fa\u7684\u95ed\u5f0f\u89e3\u6bd4\u9012\u5f52\u7b97\u6cd5\u66f4\u76f4\u89c2\uff0c\u9002\u7528\u4e8e\u5f39\u6027\u591a\u4f53\u7cfb\u7edf\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u8bbe\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u9636\u5bfc\u6570\u7684\u573a\u666f\u3002"}}
{"id": "2507.10977", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10977", "abs": "https://arxiv.org/abs/2507.10977", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "Human-object interaction (HOI) detection is essential for accurately\nlocalizing and characterizing interactions between humans and objects,\nproviding a comprehensive understanding of complex visual scenes across various\ndomains. However, existing HOI detectors often struggle to deliver reliable\npredictions efficiently, relying on resource-intensive training methods and\ninefficient architectures. To address these challenges, we conceptualize a\nwavelet attention-like backbone and a novel ray-based encoder architecture\ntailored for HOI detection. Our wavelet backbone addresses the limitations of\nexpressing middle-order interactions by aggregating discriminative features\nfrom the low- and high-order interactions extracted from diverse convolutional\nfilters. Concurrently, the ray-based encoder facilitates multi-scale attention\nby optimizing the focus of the decoder on relevant regions of interest and\nmitigating computational overhead. As a result of harnessing the attenuated\nintensity of learnable ray origins, our decoder aligns query embeddings with\nemphasized regions of interest for accurate predictions. Experimental results\non benchmark datasets, including ImageNet and HICO-DET, showcase the potential\nof our proposed architecture. The code is publicly available at\n[https://github.com/henry-pay/RayEncoder].", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u6ce8\u610f\u529b\u548c\u5c04\u7ebf\u7f16\u7801\u5668\u7684\u65b0\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u65b9\u6cd5\u5728\u6548\u7387\u548c\u53ef\u9760\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u8bad\u7ec3\u548c\u4f4e\u6548\u67b6\u6784\u3002", "method": "\u8bbe\u8ba1\u4e86\u5c0f\u6ce2\u6ce8\u610f\u529b\u4e3b\u5e72\u548c\u5c04\u7ebf\u7f16\u7801\u5668\uff0c\u5206\u522b\u7528\u4e8e\u805a\u5408\u591a\u9636\u4ea4\u4e92\u7279\u5f81\u548c\u4f18\u5316\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u3002", "result": "\u5728ImageNet\u548cHICO-DET\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u67b6\u6784\u901a\u8fc7\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.10637", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10637", "abs": "https://arxiv.org/abs/2507.10637", "authors": ["\u00c9. K\u00fcnzel", "A. Jaziri", "V. Ramesh"], "title": "A Simple Baseline for Stable and Plastic Neural Networks", "comment": "11 pages, 50 figures", "summary": "Continual learning in computer vision requires that models adapt to a\ncontinuous stream of tasks without forgetting prior knowledge, yet existing\napproaches often tip the balance heavily toward either plasticity or stability.\nWe introduce RDBP, a simple, low-overhead baseline that unites two\ncomplementary mechanisms: ReLUDown, a lightweight activation modification that\npreserves feature sensitivity while preventing neuron dormancy, and Decreasing\nBackpropagation, a biologically inspired gradient-scheduling scheme that\nprogressively shields early layers from catastrophic updates. Evaluated on the\nContinual ImageNet benchmark, RDBP matches or exceeds the plasticity and\nstability of state-of-the-art methods while reducing computational cost. RDBP\nthus provides both a practical solution for real-world continual learning and a\nclear benchmark against which future continual learning strategies can be\nmeasured.", "AI": {"tldr": "RDBP\u662f\u4e00\u79cd\u7b80\u5355\u3001\u4f4e\u5f00\u9500\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86ReLUDown\u548cDecreasing Backpropagation\u4e24\u79cd\u673a\u5236\uff0c\u5728Continual ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u6a21\u578b\u5728\u9002\u5e94\u65b0\u4efb\u52a1\u65f6\u5bb9\u6613\u9057\u5fd8\u65e7\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u5e73\u8861\u53ef\u5851\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u7ed3\u5408ReLUDown\uff08\u8f7b\u91cf\u7ea7\u6fc0\u6d3b\u4fee\u6539\uff09\u548cDecreasing Backpropagation\uff08\u68af\u5ea6\u8c03\u5ea6\u65b9\u6848\uff09\uff0c\u9632\u6b62\u795e\u7ecf\u5143\u4f11\u7720\u548c\u707e\u96be\u6027\u66f4\u65b0\u3002", "result": "\u5728Continual ImageNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRDBP\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "RDBP\u4e3a\u5b9e\u9645\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u672a\u6765\u7b56\u7565\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.11133", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11133", "abs": "https://arxiv.org/abs/2507.11133", "authors": ["Luca Beber", "Edoardo Lamon", "Giacomo Moretti", "Matteo Saveriano", "Luca Fambri", "Luigi Palopoli", "Daniele Fontanelli"], "title": "Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm", "comment": null, "summary": "Diagnostic activities, such as ultrasound scans and palpation, are relatively\nlow-cost. They play a crucial role in the early detection of health problems\nand in assessing their progression. However, they are also error-prone\nactivities, which require highly skilled medical staff. The use of robotic\nsolutions can be key to decreasing the inherent subjectivity of the results and\nreducing the waiting list. For a robot to perform palpation or ultrasound\nscans, it must effectively manage physical interactions with the human body,\nwhich greatly benefits from precise estimation of the patient's tissue\nbiomechanical properties. This paper assesses the accuracy and precision of a\nrobotic system in estimating the viscoelastic parameters of various materials,\nincluding some tests on ex vivo tissues as a preliminary proof-of-concept\ndemonstration of the method's applicability to biological samples. The\nmeasurements are compared against a ground truth derived from silicone\nspecimens with different viscoelastic properties, characterised using a\nhigh-precision instrument. Experimental results show that the robotic system's\naccuracy closely matches the ground truth, increasing confidence in the\npotential use of robots for such clinical applications.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u4f30\u8ba1\u6750\u6599\u7c98\u5f39\u6027\u53c2\u6570\u65b9\u9762\u7684\u51c6\u786e\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u751f\u7269\u6837\u672c\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u8bca\u65ad\u6d3b\u52a8\uff08\u5982\u8d85\u58f0\u626b\u63cf\u548c\u89e6\u8bca\uff09\u6210\u672c\u4f4e\u4f46\u6613\u51fa\u9519\uff0c\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u53ef\u51cf\u5c11\u4e3b\u89c2\u6027\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u901a\u8fc7\u673a\u5668\u4eba\u7cfb\u7edf\u6d4b\u91cf\u4e0d\u540c\u6750\u6599\u7684\u7c98\u5f39\u6027\u53c2\u6570\uff0c\u5e76\u4e0e\u9ad8\u7cbe\u5ea6\u4eea\u5668\u6d4b\u5f97\u7684\u57fa\u51c6\u6570\u636e\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u63a5\u8fd1\u57fa\u51c6\u6570\u636e\uff0c\u652f\u6301\u5176\u5728\u4e34\u5e8a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u4f30\u8ba1\u7ec4\u7ec7\u751f\u7269\u529b\u5b66\u7279\u6027\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u6709\u671b\u7528\u4e8e\u4e34\u5e8a\u8bca\u65ad\u3002"}}
{"id": "2507.10978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10978", "abs": "https://arxiv.org/abs/2507.10978", "authors": ["Ayush Gupta", "Siyuan Huang", "Rama Chellappa"], "title": "Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction", "comment": "Accepted at IJCB 2025", "summary": "Gait is becoming popular as a method of person re-identification because of\nits ability to identify people at a distance. However, most current works in\ngait recognition do not address the practical problem of occlusions. Among\nthose which do, some require paired tuples of occluded and holistic sequences,\nwhich are impractical to collect in the real world. Further, these approaches\nwork on occlusions but fail to retain performance on holistic inputs. To\naddress these challenges, we propose RG-Gait, a method for residual correction\nfor occluded gait recognition with holistic retention. We model the problem as\na residual learning task, conceptualizing the occluded gait signature as a\nresidual deviation from the holistic gait representation. Our proposed network\nadaptively integrates the learned residual, significantly improving performance\non occluded gait sequences without compromising the holistic recognition\naccuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR\ndatasets and show that learning the residual can be an effective technique to\ntackle occluded gait recognition with holistic retention.", "AI": {"tldr": "RG-Gait\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5dee\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6b65\u6001\u8bc6\u522b\u4e2d\u7684\u906e\u6321\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u5b8c\u6574\u6b65\u6001\u7684\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u5927\u591a\u672a\u89e3\u51b3\u906e\u6321\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u548c\u5b8c\u6574\u6b65\u6001\u4e4b\u95f4\u96be\u4ee5\u517c\u987e\u3002", "method": "\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u5efa\u6a21\u906e\u6321\u6b65\u6001\u4e0e\u5b8c\u6574\u6b65\u6001\u4e4b\u95f4\u7684\u504f\u5dee\uff0c\u81ea\u9002\u5e94\u6574\u5408\u6b8b\u5dee\u3002", "result": "\u5728Gait3D\u3001GREW\u548cBRIAR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6b8b\u5dee\u5b66\u4e60\u662f\u89e3\u51b3\u906e\u6321\u6b65\u6001\u8bc6\u522b\u5e76\u4fdd\u7559\u5b8c\u6574\u6b65\u6001\u6027\u80fd\u7684\u6709\u6548\u6280\u672f\u3002"}}
{"id": "2507.10638", "categories": ["cs.LG", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.10638", "abs": "https://arxiv.org/abs/2507.10638", "authors": ["Shim Soon Yong"], "title": "ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space", "comment": null, "summary": "We introduce a novel classification framework, ZClassifier, that replaces\nconventional deterministic logits with diagonal Gaussian-distributed logits.\nOur method simultaneously addresses temperature scaling and manifold\napproximation by minimizing the Kullback-Leibler (KL) divergence between the\npredicted Gaussian distributions and a unit isotropic Gaussian. This unifies\nuncertainty calibration and latent control in a principled probabilistic\nmanner, enabling a natural interpretation of class confidence and geometric\nconsistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier\nimproves over softmax classifiers in robustness, calibration, and latent\nseparation. We also demonstrate its effectiveness for classifier-guided\ngeneration by interpreting logits as Gaussian semantic potentials.", "AI": {"tldr": "ZClassifier\u662f\u4e00\u79cd\u65b0\u578b\u5206\u7c7b\u6846\u67b6\uff0c\u7528\u9ad8\u65af\u5206\u5e03\u66ff\u4ee3\u4f20\u7edf\u786e\u5b9a\u6027logits\uff0c\u7edf\u4e00\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u6f5c\u5728\u63a7\u5236\u3002", "motivation": "\u4f20\u7edfsoftmax\u5206\u7c7b\u5668\u5728\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u6f5c\u5728\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cZClassifier\u901a\u8fc7\u9ad8\u65af\u5206\u5e03logits\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u9884\u6d4b\u9ad8\u65af\u5206\u5e03\u4e0e\u5355\u4f4d\u5404\u5411\u540c\u6027\u9ad8\u65af\u4e4b\u95f4\u7684KL\u6563\u5ea6\uff0c\u540c\u65f6\u5904\u7406\u6e29\u5ea6\u7f29\u653e\u548c\u6d41\u5f62\u8fd1\u4f3c\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u4e0a\uff0cZClassifier\u5728\u9c81\u68d2\u6027\u3001\u6821\u51c6\u548c\u6f5c\u5728\u5206\u79bb\u65b9\u9762\u4f18\u4e8esoftmax\u5206\u7c7b\u5668\u3002", "conclusion": "ZClassifier\u4e3a\u5206\u7c7b\u548c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6982\u7387\u6846\u67b6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11170", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11170", "abs": "https://arxiv.org/abs/2507.11170", "authors": ["Giulio Giacomuzzo", "Mohamed Abdelwahab", "Marco Cal\u00ec", "Alberto Dalla Libera", "Ruggero Carli"], "title": "A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty", "comment": null, "summary": "In this paper, we propose a novel learning-based robust feedback\nlinearization strategy to ensure precise trajectory tracking for an important\nfamily of Lagrangian systems. We assume a nominal knowledge of the dynamics is\ngiven but no a-priori bounds on the model mismatch are available. In our\napproach, the key ingredient is the adoption of a regression framework based on\nGaussian Processes (GPR) to estimate the model mismatch. This estimate is added\nto the outer loop of a classical feedback linearization scheme based on the\nnominal knowledge available. Then, to compensate for the residual uncertainty,\nwe robustify the controller including an additional term whose size is designed\nbased on the variance provided by the GPR framework. We proved that, with high\nprobability, the proposed scheme is able to guarantee asymptotic tracking of a\ndesired trajectory. We tested numerically our strategy on a 2 degrees of\nfreedom planar robot.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u9c81\u68d2\u53cd\u9988\u7ebf\u6027\u5316\u7b56\u7565\uff0c\u7528\u4e8e\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u7684\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u786e\u4fdd\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7cbe\u786e\u6027\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u4f30\u8ba1\u6a21\u578b\u4e0d\u5339\u914d\uff0c\u5e76\u7ed3\u5408\u7ecf\u5178\u53cd\u9988\u7ebf\u6027\u5316\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b56\u7565\u57282\u81ea\u7531\u5ea6\u5e73\u9762\u673a\u5668\u4eba\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6848\u80fd\u4ee5\u9ad8\u6982\u7387\u4fdd\u8bc1\u6e10\u8fd1\u8ddf\u8e2a\u76ee\u6807\u8f68\u8ff9\u3002"}}
{"id": "2507.10999", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10999", "abs": "https://arxiv.org/abs/2507.10999", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "The resurgence of convolutional neural networks (CNNs) in visual recognition\ntasks, exemplified by ConvNeXt, has demonstrated their capability to rival\ntransformer-based architectures through advanced training methodologies and\nViT-inspired design principles. However, both CNNs and transformers exhibit a\nsimplicity bias, favoring straightforward features over complex structural\nrepresentations. Furthermore, modern CNNs often integrate MLP-like blocks akin\nto those in transformers, but these blocks suffer from significant information\nredundancies, necessitating high expansion ratios to sustain competitive\nperformance. To address these limitations, we propose SpaRTAN, a lightweight\narchitectural design that enhances spatial and channel-wise information\nprocessing. SpaRTAN employs kernels with varying receptive fields, controlled\nby kernel size and dilation factor, to capture discriminative multi-order\nspatial features effectively. A wave-based channel aggregation module further\nmodulates and reinforces pixel interactions, mitigating channel-wise\nredundancies. Combining the two modules, the proposed network can efficiently\ngather and dynamically contextualize discriminative features. Experimental\nresults in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable\nparameter efficiency while maintaining competitive performance. In particular,\non the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M\nparameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver\nstrong performance through an efficient design. On the COCO benchmark, it\nachieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M\nparameters. The code is publicly available at\n[https://github.com/henry-pay/SpaRTAN].", "AI": {"tldr": "SpaRTAN\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u67b6\u6784\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u9636\u7a7a\u95f4\u7279\u5f81\u548c\u901a\u9053\u805a\u5408\u6a21\u5757\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u5728\u53c2\u6570\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3CNN\u548cTransformer\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u504f\u597d\u7b80\u5355\u7279\u5f81\u548c\u5b58\u5728\u4fe1\u606f\u5197\u4f59\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53ef\u53d8\u611f\u53d7\u91ce\u7684\u6838\u548c\u6ce2\u57fa\u901a\u9053\u805a\u5408\u6a21\u5757\uff0c\u4f18\u5316\u7a7a\u95f4\u548c\u901a\u9053\u4fe1\u606f\u5904\u7406\u3002", "result": "\u5728ImageNet-1k\u4e0a\u8fbe\u523077.7%\u51c6\u786e\u7387\uff083.8M\u53c2\u6570\uff09\uff0cCOCO\u4e0a50.0% AP\uff0821.5M\u53c2\u6570\uff09\u3002", "conclusion": "SpaRTAN\u901a\u8fc7\u9ad8\u6548\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u53c2\u6570\u6548\u7387\u663e\u8457\u3002"}}
{"id": "2507.10642", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10642", "abs": "https://arxiv.org/abs/2507.10642", "authors": ["Andrew Gascoyne", "Wendy Lomas"], "title": "First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network", "comment": "12 pages, 5 figures", "summary": "A growing issue within conservation bioacoustics is the task of analysing the\nvast amount of data generated from the use of passive acoustic monitoring\ndevices. In this paper, we present an alternative AI model which has the\npotential to help alleviate this problem. Our model formulation addresses the\nkey issues encountered when using current AI models for bioacoustic analysis,\nnamely the: limited training data available; environmental impact, particularly\nin energy consumption and carbon footprint of training and implementing these\nmodels; and associated hardware requirements. The model developed in this work\nuses associative memory via a transparent, explainable Hopfield neural network\nto store signals and detect similar signals which can then be used to classify\nspecies. Training is rapid ($3$\\,ms), as only one representative signal is\nrequired for each target sound within a dataset. The model is fast, taking only\n$5.4$\\,s to pre-process and classify all $10384$ publicly available bat\nrecordings, on a standard Apple MacBook Air. The model is also lightweight with\na small memory footprint of $144.09$\\,MB of RAM usage. Hence, the low\ncomputational demands make the model ideal for use on a variety of standard\npersonal devices with potential for deployment in the field via edge-processing\ndevices. It is also competitively accurate, with up to $86\\%$ precision on the\ndataset used to evaluate the model. In fact, we could not find a single case of\ndisagreement between model and manual identification via expert field guides.\nAlthough a dataset of bat echolocation calls was chosen to demo this\nfirst-of-its-kind AI model, trained on only two representative calls, the model\nis not species specific. In conclusion, we propose an equitable AI model that\nhas the potential to be a game changer for fast, lightweight, sustainable,\ntransparent, explainable and accurate bioacoustic analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHopfield\u795e\u7ecf\u7f51\u7edc\u7684\u8f7b\u91cf\u7ea7AI\u6a21\u578b\uff0c\u7528\u4e8e\u5feb\u901f\u3001\u4f4e\u80fd\u8017\u7684\u751f\u7269\u58f0\u5b66\u5206\u6790\uff0c\u9002\u7528\u4e8e\u6807\u51c6\u4e2a\u4eba\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u58f0\u5b66\u5206\u6790\u4e2d\u6570\u636e\u91cf\u5927\u3001\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3001\u80fd\u8017\u9ad8\u53ca\u786c\u4ef6\u9700\u6c42\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u900f\u660e\u4e14\u53ef\u89e3\u91ca\u7684Hopfield\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5173\u8054\u8bb0\u5fc6\u5b58\u50a8\u4fe1\u53f7\u5e76\u5206\u7c7b\u7269\u79cd\uff0c\u8bad\u7ec3\u4ec5\u9700\u4e00\u4e2a\u4ee3\u8868\u6027\u4fe1\u53f7\u3002", "result": "\u6a21\u578b\u5728\u6807\u51c6\u8bbe\u5907\u4e0a\u5feb\u901f\uff085.4\u79d2\u5904\u740610384\u4e2a\u8759\u8760\u5f55\u97f3\uff09\u4e14\u51c6\u786e\uff0886%\u7cbe\u5ea6\uff09\uff0c\u5185\u5b58\u5360\u7528\u4f4e\uff08144.09MB\uff09\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5feb\u901f\u3001\u8f7b\u91cf\u3001\u53ef\u6301\u7eed\u3001\u900f\u660e\u4e14\u51c6\u786e\u7684\u751f\u7269\u58f0\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11211", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11211", "abs": "https://arxiv.org/abs/2507.11211", "authors": ["Chen Cai", "Ernesto Dickel Saraiva", "Ya-jun Pan", "Steven Liu"], "title": "MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments", "comment": "10 pages, 5 figures, submitted to IEEE Robotics and Automation\n  Letters (RA-L)", "summary": "This letter presents a novel coarse-to-fine motion planning framework for\nrobotic manipulation in cluttered, unmodeled environments. The system\nintegrates a dual-camera perception setup with a B-spline-based model\npredictive control (MPC) scheme. Initially, the planner generates feasible\nglobal trajectories from partial and uncertain observations. As new visual data\nare incrementally fused, both the environment model and motion planning are\nprogressively refined. A vision-based cost function promotes target-driven\nexploration, while a refined kernel-perceptron collision detector enables\nefficient constraint updates for real-time planning. The framework accommodates\nclosed-chain kinematics and supports dynamic replanning. Experiments on a\nmulti-arm platform validate its robustness and adaptability under uncertainties\nand clutter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5728\u6742\u4e71\u3001\u672a\u5efa\u6a21\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u5728\u90e8\u5206\u548c\u4e0d\u786e\u5b9a\u89c2\u6d4b\u4e0b\u751f\u6210\u53ef\u884c\u8f68\u8ff9\u7684\u6311\u6218\uff0c\u5e76\u9010\u6b65\u4f18\u5316\u73af\u5883\u6a21\u578b\u548c\u8fd0\u52a8\u89c4\u5212\u3002", "method": "\u7ed3\u5408\u53cc\u6444\u50cf\u5934\u611f\u77e5\u548c\u57fa\u4e8eB\u6837\u6761\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u4f7f\u7528\u89c6\u89c9\u6210\u672c\u51fd\u6570\u548c\u76ee\u6807\u9a71\u52a8\u63a2\u7d22\uff0c\u4ee5\u53ca\u9ad8\u6548\u7684\u78b0\u649e\u68c0\u6d4b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u6742\u4e71\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u652f\u6301\u52a8\u6001\u91cd\u65b0\u89c4\u5212\uff0c\u9002\u7528\u4e8e\u591a\u81c2\u5e73\u53f0\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.11003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11003", "abs": "https://arxiv.org/abs/2507.11003", "authors": ["Yuhu Bai", "Jiangning Zhang", "Yunkang Cao", "Guangyuan Lu", "Qingdong He", "Xiangtai Li", "Guanzhong Tian"], "title": "Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection", "comment": null, "summary": "With the advent of vision-language models (e.g., CLIP) in zero- and few-shot\nsettings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in\nrecent research, where the rare classes are essential and expected in many\napplications. This study introduces \\textbf{FiSeCLIP} for ZSAD with\ntraining-free \\textbf{CLIP}, combining the feature matching with the\ncross-modal alignment. Testing with the entire dataset is impractical, while\nbatch-based testing better aligns with real industrial needs, and images within\na batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes\nother images in the same batch as reference information for the current image.\nHowever, the lack of labels for these references can introduce ambiguity, we\napply text information to \\textbf{fi}lter out noisy features. In addition, we\nfurther explore CLIP's inherent potential to restore its local\n\\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection\ntasks to enable a more accurate filtering process. Our approach exhibits\nsuperior performance for both anomaly classification and segmentation on\nanomaly detection benchmarks, building a stronger baseline for the direction,\ne.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by\n+4.6\\%$\\uparrow$/+5.7\\%$\\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.", "AI": {"tldr": "FiSeCLIP\u5229\u7528CLIP\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u7279\u5f81\u5339\u914d\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728\u6279\u6b21\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7f55\u89c1\u7c7b\u522b\u7684\u8bc6\u522b\u95ee\u9898\uff0c\u5e76\u9002\u5e94\u5de5\u4e1a\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u7279\u5f81\u5339\u914d\u4e0e\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5229\u7528\u6279\u6b21\u5185\u56fe\u50cf\u4f5c\u4e3a\u53c2\u8003\uff0c\u5e76\u901a\u8fc7\u6587\u672c\u4fe1\u606f\u8fc7\u6ee4\u566a\u58f0\u3002", "result": "\u5728MVTec-AD\u6570\u636e\u96c6\u4e0a\uff0cFiSeCLIP\u5728\u5206\u5272\u6307\u6807AU-ROC\u548cF1-max\u4e0a\u5206\u522b\u6bd4SOTA\u65b9\u6cd5AdaCLIP\u63d0\u53474.6%\u548c5.7%\u3002", "conclusion": "FiSeCLIP\u4e3a\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5f3a\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86CLIP\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10678", "categories": ["cs.LG", "cs.AI", "cs.NE", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.10678", "abs": "https://arxiv.org/abs/2507.10678", "authors": ["Cutter Dawes", "Simon Segert", "Kamesh Krishnamurthy", "Jonathan D. Cohen"], "title": "A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks", "comment": "22 pages, 6 figures", "summary": "A major challenge in the use of neural networks both for modeling human\ncognitive function and for artificial intelligence is the design of systems\nwith the capacity to efficiently learn functions that support radical\ngeneralization. At the roots of this is the capacity to discover and implement\nsymmetry functions. In this paper, we investigate a paradigmatic example of\nradical generalization through the use of symmetry: base addition. We present a\ngroup theoretic analysis of base addition, a fundamental and defining\ncharacteristic of which is the carry function -- the transfer of the remainder,\nwhen a sum exceeds the base modulus, to the next significant place. Our\nanalysis exposes a range of alternative carry functions for a given base, and\nwe introduce quantitative measures to characterize these. We then exploit\ndifferences in carry functions to probe the inductive biases of neural networks\nin symmetry learning, by training neural networks to carry out base addition\nusing different carries, and comparing efficacy and rate of learning as a\nfunction of their structure. We find that even simple neural networks can\nachieve radical generalization with the right input format and carry function,\nand that learning speed is closely correlated with carry function structure. We\nthen discuss the relevance this has for cognitive science and machine learning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u901a\u8fc7\u5bf9\u79f0\u6027\u5b9e\u73b0\u6fc0\u8fdb\u6cdb\u5316\u7684\u80fd\u529b\uff0c\u4ee5\u57fa\u6570\u52a0\u6cd5\u4e3a\u4f8b\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u7684\u8fdb\u4f4d\u51fd\u6570\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u652f\u6301\u6fc0\u8fdb\u6cdb\u5316\u7684\u7cfb\u7edf\u662f\u795e\u7ecf\u7f51\u7edc\u5728\u8ba4\u77e5\u5efa\u6a21\u548c\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5173\u952e\u5728\u4e8e\u53d1\u73b0\u548c\u5b9e\u73b0\u5bf9\u79f0\u6027\u51fd\u6570\u3002", "method": "\u901a\u8fc7\u7fa4\u8bba\u5206\u6790\u57fa\u6570\u52a0\u6cd5\uff0c\u63d0\u51fa\u4e0d\u540c\u7684\u8fdb\u4f4d\u51fd\u6570\uff0c\u5e76\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u4f7f\u7528\u4e0d\u540c\u8fdb\u4f4d\u51fd\u6570\u8fdb\u884c\u57fa\u6570\u52a0\u6cd5\uff0c\u6bd4\u8f83\u5b66\u4e60\u6548\u679c\u3002", "result": "\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u5728\u5408\u9002\u7684\u8f93\u5165\u683c\u5f0f\u548c\u8fdb\u4f4d\u51fd\u6570\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u6fc0\u8fdb\u6cdb\u5316\uff0c\u5b66\u4e60\u901f\u5ea6\u4e0e\u8fdb\u4f4d\u51fd\u6570\u7ed3\u6784\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u8ba4\u77e5\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u63ed\u793a\u4e86\u8fdb\u4f4d\u51fd\u6570\u7ed3\u6784\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.11241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11241", "abs": "https://arxiv.org/abs/2507.11241", "authors": ["Tobias Kern", "Leon Tolksdorf", "Christian Birkner"], "title": "Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors", "comment": null, "summary": "Physically reduced-scale vehicles are emerging to accelerate the development\nof advanced automated driving functions. In this paper, we investigate the\neffects of scaling on self-localization accuracy with visual and\nvisual-inertial algorithms using cameras and an inertial measurement unit\n(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms\nare selected, and datasets are chosen as a baseline for real-sized vehicles. A\ntest drive is conducted to record data of reduced-scale vehicles. We compare\nthe selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in\nterms of their pose accuracy against the ground-truth and against data from\nreal-sized vehicles. When comparing the implementation of the selected\nlocalization algorithms to real-sized vehicles, OpenVINS has the lowest average\nlocalization error. Although all selected localization algorithms have\noverlapping error ranges, OpenVINS also performs best when applied to a\nreduced-scale vehicle. When reduced-scale vehicles were compared to real-sized\nvehicles, minor differences were found in translational vehicle motion\nestimation accuracy. However, no significant differences were found when\ncomparing the estimation accuracy of rotational vehicle motion, allowing RSVRs\nto be used as testing platforms for self-localization algorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7269\u7406\u7f29\u6bd4\u8f66\u8f86\u5bf9\u89c6\u89c9\u548c\u89c6\u89c9-\u60ef\u6027\u81ea\u5b9a\u4f4d\u7b97\u6cd5\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0OpenVINS\u5728\u7f29\u6bd4\u548c\u771f\u5b9e\u5c3a\u5bf8\u8f66\u8f86\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u52a0\u901f\u9ad8\u7ea7\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u7684\u5f00\u53d1\uff0c\u901a\u8fc7\u7f29\u6bd4\u8f66\u8f86\u6d4b\u8bd5\u81ea\u5b9a\u4f4d\u7b97\u6cd5\u7684\u53ef\u884c\u6027\u3002", "method": "\u9009\u62e9ROS2\u517c\u5bb9\u7684\u89c6\u89c9\u548c\u89c6\u89c9-\u60ef\u6027\u7b97\u6cd5\uff08OpenVINS\u3001VINS-Fusion\u3001RTAB-Map\uff09\uff0c\u8bb0\u5f55\u7f29\u6bd4\u8f66\u8f86\u6570\u636e\u5e76\u4e0e\u771f\u5b9e\u5c3a\u5bf8\u8f66\u8f86\u6570\u636e\u5bf9\u6bd4\u3002", "result": "OpenVINS\u5b9a\u4f4d\u8bef\u5dee\u6700\u4f4e\uff0c\u7f29\u6bd4\u8f66\u8f86\u5728\u5e73\u79fb\u8fd0\u52a8\u4f30\u8ba1\u4e0a\u6709\u5fae\u5c0f\u5dee\u5f02\uff0c\u4f46\u65cb\u8f6c\u8fd0\u52a8\u4f30\u8ba1\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7f29\u6bd4\u8f66\u8f86\u53ef\u4f5c\u4e3a\u81ea\u5b9a\u4f4d\u7b97\u6cd5\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2507.11015", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11015", "abs": "https://arxiv.org/abs/2507.11015", "authors": ["Zeyi Hou", "Zeqiang Wei", "Ruixin Yan", "Ning Lang", "Xiuzhuang Zhou"], "title": "Semantically Informed Salient Regions Guided Radiology Report Generation", "comment": null, "summary": "Recent advances in automated radiology report generation from chest X-rays\nusing deep learning algorithms have the potential to significantly reduce the\narduous workload of radiologists. However, due to the inherent massive data\nbias in radiology images, where abnormalities are typically subtle and sparsely\ndistributed, existing methods often produce fluent yet medically inaccurate\nreports, limiting their applicability in clinical practice. To address this\nissue effectively, we propose a Semantically Informed Salient Regions-guided\n(SISRNet) report generation method. Specifically, our approach explicitly\nidentifies salient regions with medically critical characteristics using\nfine-grained cross-modal semantics. Then, SISRNet systematically focuses on\nthese high-information regions during both image modeling and report\ngeneration, effectively capturing subtle abnormal findings, mitigating the\nnegative impact of data bias, and ultimately generating clinically accurate\nreports. Compared to its peers, SISRNet demonstrates superior performance on\nwidely used IU-Xray and MIMIC-CXR datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u663e\u8457\u533a\u57df\u7684\u653e\u5c04\u62a5\u544a\u751f\u6210\u65b9\u6cd5\uff08SISRNet\uff09\uff0c\u901a\u8fc7\u5173\u6ce8\u533b\u5b66\u5173\u952e\u533a\u57df\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u504f\u5dee\u5bfc\u81f4\u7684\u62a5\u544a\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0\u653e\u5c04\u56fe\u50cf\u6570\u636e\u504f\u5dee\uff08\u5f02\u5e38\u7a00\u758f\u4e14\u7ec6\u5fae\uff09\u5e38\u751f\u6210\u6d41\u7545\u4f46\u4e0d\u51c6\u786e\u7684\u62a5\u544a\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "SISRNet\u5229\u7528\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u8bed\u4e49\u8bc6\u522b\u533b\u5b66\u5173\u952e\u663e\u8457\u533a\u57df\uff0c\u5e76\u5728\u56fe\u50cf\u5efa\u6a21\u548c\u62a5\u544a\u751f\u6210\u4e2d\u7cfb\u7edf\u5173\u6ce8\u8fd9\u4e9b\u533a\u57df\u3002", "result": "\u5728IU-Xray\u548cMIMIC-CXR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u540c\u7c7b\u65b9\u6cd5\u3002", "conclusion": "SISRNet\u80fd\u6709\u6548\u6355\u6349\u7ec6\u5fae\u5f02\u5e38\uff0c\u51cf\u5c11\u6570\u636e\u504f\u5dee\u5f71\u54cd\uff0c\u751f\u6210\u4e34\u5e8a\u51c6\u786e\u7684\u62a5\u544a\u3002"}}
{"id": "2507.10714", "categories": ["cs.LG", "q-bio.QM", "stat.ML", "68, 92", "I.6; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.10714", "abs": "https://arxiv.org/abs/2507.10714", "authors": ["Bright Kwaku Manu", "Trevor Reckell", "Beckett Sterner", "Petar Jevtic"], "title": "A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models", "comment": "12 pages, 10 figures, for all associated codes and files, see\n  https://github.com/BrightManu-lang/SPN-param-recovery.git", "summary": "Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for\nmodeling discrete-event dynamics in areas such as epidemiology and systems\nbiology, yet their parameter estimation remains challenging in general and in\nparticular when transition rates depend on external covariates and explicit\nlikelihoods are unavailable. We introduce a neural-surrogate\n(neural-network--based approximation of the posterior distribution) framework\nthat predicts the coefficients of known covariate-dependent rate functions\ndirectly from noisy, partially observed token trajectories. Our model employs a\nlightweight 1D Convolutional Residual Network trained end-to-end on\nGillespie-simulated SPN realizations, learning to invert system dynamics under\nrealistic conditions of event dropout. During inference, Monte Carlo dropout\nprovides calibrated uncertainty bounds together with point estimates. On\nsynthetic SPNs with 20% missing events, our surrogate recovers rate-function\ncoefficients with an RMSE = 0.108 and substantially runs faster than\ntraditional Bayesian approaches. These results demonstrate that data-driven,\nlikelihood-free surrogates can enable accurate, robust, and real-time parameter\nrecovery in complex, partially observed discrete-event systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u66ff\u4ee3\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u968f\u673aPetri\u7f51\uff08SPN\uff09\u4e2d\u4f9d\u8d56\u4e8e\u5916\u90e8\u534f\u53d8\u91cf\u7684\u901f\u7387\u51fd\u6570\u7cfb\u6570\uff0c\u89e3\u51b3\u4e86\u53c2\u6570\u4f30\u8ba1\u7684\u6311\u6218\u3002", "motivation": "\u968f\u673aPetri\u7f51\u5728\u5efa\u6a21\u79bb\u6563\u4e8b\u4ef6\u52a8\u6001\u65f6\u53c2\u6570\u4f30\u8ba1\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u901f\u7387\u51fd\u6570\u4f9d\u8d56\u4e8e\u5916\u90e8\u534f\u53d8\u91cf\u4e14\u663e\u5f0f\u4f3c\u7136\u4e0d\u53ef\u5f97\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea71D\u5377\u79ef\u6b8b\u5dee\u7f51\u7edc\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3Gillespie\u6a21\u62df\u7684SPN\u5b9e\u73b0\uff0c\u5b66\u4e60\u5728\u4e8b\u4ef6\u4e22\u5931\u7684\u73b0\u5b9e\u6761\u4ef6\u4e0b\u53cd\u8f6c\u7cfb\u7edf\u52a8\u6001\u3002", "result": "\u572820%\u4e8b\u4ef6\u4e22\u5931\u7684\u5408\u6210SPN\u4e0a\uff0c\u66ff\u4ee3\u6a21\u578b\u6062\u590d\u901f\u7387\u51fd\u6570\u7cfb\u6570\u7684RMSE\u4e3a0.108\uff0c\u4e14\u8fd0\u884c\u901f\u5ea6\u663e\u8457\u5feb\u4e8e\u4f20\u7edf\u8d1d\u53f6\u65af\u65b9\u6cd5\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u3001\u65e0\u4f3c\u7136\u7684\u66ff\u4ee3\u6a21\u578b\u80fd\u591f\u5728\u590d\u6742\u3001\u90e8\u5206\u89c2\u6d4b\u7684\u79bb\u6563\u4e8b\u4ef6\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u51c6\u786e\u3001\u9c81\u68d2\u548c\u5b9e\u65f6\u7684\u53c2\u6570\u6062\u590d\u3002"}}
{"id": "2507.11270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11270", "abs": "https://arxiv.org/abs/2507.11270", "authors": ["Ting-Wei Ou", "Jia-Hao Jiang", "Guan-Lin Huang", "Kuu-Young Young"], "title": "Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection", "comment": "Accepted to the IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) 2025", "summary": "The COVID-19 pandemic has severely affected public health, healthcare\nsystems, and daily life, especially amid resource shortages and limited\nworkers. This crisis has underscored the urgent need for automation in hospital\nenvironments, particularly disinfection, which is crucial to controlling virus\ntransmission and improving the safety of healthcare personnel and patients.\nUltraviolet (UV) light disinfection, known for its high efficiency, has been\nwidely adopted in hospital settings. However, most existing research focuses on\nmaximizing UV coverage while paying little attention to the impact of human\nactivity on virus distribution. To address this issue, we propose a mobile\nrobotic system for UV disinfection focusing on the virus hotspot. The system\nprioritizes disinfection in high-risk areas and employs an approach for\noptimized UV dosage to ensure that all surfaces receive an adequate level of UV\nexposure while significantly reducing disinfection time. It not only improves\ndisinfection efficiency but also minimizes unnecessary exposure in low-risk\nareas. In two representative hospital scenarios, our method achieves the same\ndisinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,\nrespectively. The video of the experiment is available at:\nhttps://youtu.be/wHcWzOcoMPM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u75c5\u6bd2\u70ed\u70b9\u533a\u57df\u7684\u79fb\u52a8\u673a\u5668\u4eba\u7d2b\u5916\u7ebf\u6d88\u6bd2\u7cfb\u7edf\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6d88\u6bd2\u65f6\u95f4\u3002", "motivation": "COVID-19\u75ab\u60c5\u51f8\u663e\u4e86\u533b\u9662\u73af\u5883\u4e2d\u81ea\u52a8\u5316\u6d88\u6bd2\u7684\u7d27\u8feb\u6027\uff0c\u5c24\u5176\u662f\u7d2b\u5916\u7ebf\u6d88\u6bd2\u7684\u9ad8\u6548\u6027\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7d2b\u5916\u7ebf\u8986\u76d6\uff0c\u800c\u5ffd\u7565\u4e86\u4eba\u7c7b\u6d3b\u52a8\u5bf9\u75c5\u6bd2\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f18\u5148\u6d88\u6bd2\u9ad8\u98ce\u9669\u533a\u57df\uff0c\u5e76\u4f18\u5316\u7d2b\u5916\u7ebf\u5242\u91cf\u5206\u914d\u3002", "result": "\u5728\u4e24\u4e2a\u5178\u578b\u533b\u9662\u573a\u666f\u4e2d\uff0c\u6d88\u6bd2\u65f6\u95f4\u5206\u522b\u51cf\u5c11\u4e8630.7%\u548c31.9%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u6d88\u6bd2\u6548\u679c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6d88\u6bd2\u6548\u7387\uff0c\u8fd8\u51cf\u5c11\u4e86\u4f4e\u98ce\u9669\u533a\u57df\u7684\u4e0d\u5fc5\u8981\u7d2b\u5916\u7ebf\u66b4\u9732\u3002"}}
{"id": "2507.11025", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11025", "abs": "https://arxiv.org/abs/2507.11025", "authors": ["Sung Ho Kang", "Hyun-Cheol Park"], "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schr\u00f6dinger Bridge with Conditional Diffusion", "comment": null, "summary": "We present a novel framework for CBCT-to-MDCT translation, grounded in the\nSchrodinger Bridge (SB) formulation, which integrates GAN-derived priors with\nhuman-guided conditional diffusion. Unlike conventional GANs or diffusion\nmodels, our approach explicitly enforces boundary consistency between CBCT\ninputs and pseudo targets, ensuring both anatomical fidelity and perceptual\ncontrollability. Binary human feedback is incorporated via classifier-free\nguidance (CFG), effectively steering the generative process toward clinically\npreferred outcomes. Through iterative refinement and tournament-based\npreference selection, the model internalizes human preferences without relying\non a reward model. Subtraction image visualizations reveal that the proposed\nmethod selectively attenuates shade artifacts in key anatomical regions while\npreserving fine structural detail. Quantitative evaluations further demonstrate\nsuperior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical\ndatasets -- outperforming prior GAN- and fine-tuning-based feedback methods --\nwhile requiring only 10 sampling steps. These findings underscore the\neffectiveness and efficiency of our framework for real-time, preference-aligned\nmedical image translation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchrodinger Bridge\u7684CBCT-to-MDCT\u8f6c\u6362\u6846\u67b6\uff0c\u7ed3\u5408GAN\u5148\u9a8c\u548c\u4eba\u7c7b\u5f15\u5bfc\u7684\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u548c\u53ef\u63a7\u7684\u533b\u5b66\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u4f20\u7edfGAN\u6216\u6269\u6563\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u8f6c\u6362\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u89e3\u5256\u5b66\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u53ef\u63a7\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u7b26\u5408\u4e34\u5e8a\u9700\u6c42\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7Schrodinger Bridge\u6574\u5408GAN\u5148\u9a8c\u548c\u4eba\u7c7b\u53cd\u9988\uff08\u57fa\u4e8eCFG\uff09\uff0c\u91c7\u7528\u8fed\u4ee3\u4f18\u5316\u548c\u9526\u6807\u8d5b\u9009\u62e9\u7b56\u7565\uff0c\u65e0\u9700\u5956\u52b1\u6a21\u578b\u5373\u53ef\u5b66\u4e60\u4eba\u7c7b\u504f\u597d\u3002", "result": "\u5728\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728RMSE\u3001SSIM\u3001LPIPS\u548cDice\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4ec5\u970010\u6b65\u91c7\u6837\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u9ad8\u6548\u4e14\u7b26\u5408\u4e34\u5e8a\u504f\u597d\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u533b\u5b66\u56fe\u50cf\u8f6c\u6362\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.10718", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10718", "abs": "https://arxiv.org/abs/2507.10718", "authors": ["Shuyao Li", "Ilias Diakonikolas", "Jelena Diakonikolas"], "title": "Distributionally Robust Optimization with Adversarial Data Contamination", "comment": null, "summary": "Distributionally Robust Optimization (DRO) provides a framework for\ndecision-making under distributional uncertainty, yet its effectiveness can be\ncompromised by outliers in the training data. This paper introduces a\nprincipled approach to simultaneously address both challenges. We focus on\noptimizing Wasserstein-1 DRO objectives for generalized linear models with\nconvex Lipschitz loss functions, where an $\\epsilon$-fraction of the training\ndata is adversarially corrupted. Our primary contribution lies in a novel\nmodeling framework that integrates robustness against training data\ncontamination with robustness against distributional shifts, alongside an\nefficient algorithm inspired by robust statistics to solve the resulting\noptimization problem. We prove that our method achieves an estimation error of\n$O(\\sqrt{\\epsilon})$ for the true DRO objective value using only the\ncontaminated data under the bounded covariance assumption. This work\nestablishes the first rigorous guarantees, supported by efficient computation,\nfor learning under the dual challenges of data contamination and distributional\nshifts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540c\u65f6\u5e94\u5bf9\u6570\u636e\u6c61\u67d3\u548c\u5206\u5e03\u504f\u79fb\u7684\u53cc\u91cd\u6311\u6218\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u9c81\u68d2\u7edf\u8ba1\u548c\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8ba1\u7b97\u548c\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff08DRO\uff09\u5728\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u5f02\u5e38\u503c\u65f6\u6548\u679c\u53d7\u9650\uff0c\u9700\u8981\u540c\u65f6\u89e3\u51b3\u6570\u636e\u6c61\u67d3\u548c\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u9488\u5bf9\u5e7f\u4e49\u7ebf\u6027\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Wasserstein-1 DRO\u548c\u9c81\u68d2\u7edf\u8ba1\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u9ad8\u6548\u7b97\u6cd5\u3002", "result": "\u5728\u6570\u636e\u88ab\u6c61\u67d3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u771f\u5b9eDRO\u76ee\u6807\u503c\u7684O(\u221a\u03b5)\u4f30\u8ba1\u8bef\u5dee\u3002", "conclusion": "\u9996\u6b21\u4e3a\u6570\u636e\u6c61\u67d3\u548c\u5206\u5e03\u504f\u79fb\u7684\u53cc\u91cd\u6311\u6218\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u9ad8\u6548\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2507.11283", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11283", "abs": "https://arxiv.org/abs/2507.11283", "authors": ["Weiyi Liu", "Jingzehua Xu", "Guanwen Xie", "Yi Li"], "title": "Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks", "comment": null, "summary": "This paper presents a diffusion-augmented reinforcement learning (RL)\napproach for robust autonomous underwater vehicle (AUV) control, addressing key\nchallenges in underwater trajectory planning and dynamic environment\nadaptation. The proposed method integrates three core innovations: (1) A\ndiffusion-based trajectory generation framework that produces physically\nfeasible multi-step trajectories, enhanced by a high-dimensional state encoding\nmechanism combining current observations with historical states and actions\nthrough a novel diffusion U-Net architecture, significantly improving\nlong-horizon planning. (2) A sample-efficient hybrid learning architecture that\nsynergizes diffusion-guided exploration with RL policy optimization, where the\ndiffusion model generates diverse candidate actions and the RL critic selects\noptimal actions, achieving higher exploration efficiency and policy stability\nin dynamic underwater environments. Extensive simulation experiments validating\nthe method's superior robustness and flexibility, outperforms conventional\ncontrol methods in challenging marine conditions, offering enhanced\nadaptability and reliability for AUV operations in the underwater tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\u7684\u9c81\u68d2\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u8f68\u8ff9\u89c4\u5212\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u590d\u6742\u591a\u53d8\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6b65\u8f68\u8ff9\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u6269\u6563U-Net\u67b6\u6784\u589e\u5f3a\u72b6\u6001\u7f16\u7801\uff0c\u63d0\u9ad8\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6d77\u6d0b\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u6269\u6563\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3a\u6c34\u4e0b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11030", "abs": "https://arxiv.org/abs/2507.11030", "authors": ["Sunghyun Park", "Jungsoo Lee", "Shubhankar Borse", "Munawar Hayat", "Sungha Choi", "Kyuwoong Hwang", "Fatih Porikli"], "title": "Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation", "comment": "Accepted to ICCV 2025; 15 pages", "summary": "While open-vocabulary semantic segmentation (OVSS) can segment an image into\nsemantic regions based on arbitrarily given text descriptions even for classes\nunseen during training, it fails to understand personal texts (e.g., `my mug\ncup') for segmenting regions of specific interest to users. This paper\naddresses challenges like recognizing `my mug cup' among `multiple mug cups'.\nTo overcome this challenge, we introduce a novel task termed\n\\textit{personalized open-vocabulary semantic segmentation} and propose a text\nprompt tuning-based plug-in method designed to recognize personal visual\nconcepts using a few pairs of images and masks, while maintaining the\nperformance of the original OVSS. Based on the observation that reducing false\npredictions is essential when applying text prompt tuning to this task, our\nproposed method employs `negative mask proposal' that captures visual concepts\nother than the personalized concept. We further improve the performance by\nenriching the representation of text prompts by injecting visual embeddings of\nthe personal concept into them. This approach enhances personalized OVSS\nwithout compromising the original OVSS performance. We demonstrate the\nsuperiority of our method on our newly established benchmarks for this task,\nincluding FSS$^\\text{per}$, CUB$^\\text{per}$, and ADE$^\\text{per}$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u8c03\u4f18\u7684\u63d2\u4ef6\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u901a\u8fc7\u8d1f\u63a9\u7801\u63d0\u6848\u548c\u89c6\u89c9\u5d4c\u5165\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u65e0\u6cd5\u7406\u89e3\u4e2a\u6027\u5316\u6587\u672c\uff08\u5982'\u6211\u7684\u676f\u5b50'\uff09\u7684\u95ee\u9898\uff0c\u8bc6\u522b\u7528\u6237\u7279\u5b9a\u5174\u8da3\u533a\u57df\u3002", "method": "\u91c7\u7528\u6587\u672c\u63d0\u793a\u8c03\u4f18\u548c\u8d1f\u63a9\u7801\u63d0\u6848\uff0c\u51cf\u5c11\u8bef\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u5d4c\u5165\u589e\u5f3a\u6587\u672c\u63d0\u793a\u8868\u793a\u3002", "result": "\u5728\u65b0\u5efa\u7acb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08FSS$^\\text{per}$\u3001CUB$^\\text{per}$\u548cADE$^\\text{per}$\uff09\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u5f71\u54cd\u539f\u59cb\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u6027\u5316\u5206\u5272\u80fd\u529b\u3002"}}
{"id": "2507.10741", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10741", "abs": "https://arxiv.org/abs/2507.10741", "authors": ["Andrew C. Li", "Toryn Q. Klassen", "Andrew Wang", "Parand A. Alamdari", "Sheila A. McIlraith"], "title": "Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language", "comment": null, "summary": "Grounding language in complex perception (e.g. pixels) and action is a key\nchallenge when building situated agents that can interact with humans via\nlanguage. In past works, this is often solved via manual design of the language\ngrounding or by curating massive datasets relating language to elements of the\nenvironment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for\ngrounding formal language from data, and eliciting behaviours by directly\ntasking RL agents through this language. By virtue of data-driven learning, our\nframework avoids the manual design of domain-specific elements like reward\nfunctions or symbol detectors. By virtue of compositional formal language\nsemantics, our framework achieves data-efficient grounding and generalization\nto arbitrary language compositions. Experiments on an image-based gridworld and\na MuJoCo robotics domain show that our approach reliably maps formal language\ninstructions to behaviours with limited data while end-to-end, data-driven\napproaches fail.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6Ground-Compose-Reinforce\uff0c\u7528\u4e8e\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u8bed\u8a00\u57fa\u7840\uff0c\u5e76\u901a\u8fc7\u8bed\u8a00\u76f4\u63a5\u6307\u5bfcRL\u4ee3\u7406\u884c\u4e3a\u3002", "motivation": "\u89e3\u51b3\u8bed\u8a00\u5728\u590d\u6742\u611f\u77e5\u548c\u52a8\u4f5c\u4e2d\u7684\u57fa\u7840\u95ee\u9898\uff0c\u907f\u514d\u624b\u52a8\u8bbe\u8ba1\u6216\u5927\u89c4\u6a21\u6570\u636e\u6807\u6ce8\u3002", "method": "\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u548c\u7ec4\u5408\u5f62\u5f0f\u8bed\u8a00\u8bed\u4e49\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8bed\u8a00\u57fa\u7840\u548c\u884c\u4e3a\u751f\u6210\u3002", "result": "\u5728\u56fe\u50cf\u7f51\u683c\u4e16\u754c\u548cMuJoCo\u673a\u5668\u4eba\u9886\u57df\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0b\u6210\u529f\u6620\u5c04\u8bed\u8a00\u6307\u4ee4\u5230\u884c\u4e3a\uff0c\u800c\u7aef\u5230\u7aef\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5931\u8d25\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ec4\u5408\u5f62\u5f0f\u8bed\u8a00\u8bed\u4e49\u548c\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bed\u8a00\u57fa\u7840\u548c\u884c\u4e3a\u751f\u6210\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.11296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11296", "abs": "https://arxiv.org/abs/2507.11296", "authors": ["Huilin Xu", "Jian Ding", "Jiakun Xu", "Ruixiang Wang", "Jun Chen", "Jinjie Mai", "Yanwei Fu", "Bernard Ghanem", "Feng Xu", "Mohamed Elhoseiny"], "title": "Diffusion-Based Imaginative Coordination for Bimanual Manipulation", "comment": "15 pages, including 10 figures and 16 tables. Accepted at ICCV 2025", "summary": "Bimanual manipulation is crucial in robotics, enabling complex tasks in\nindustrial automation and household services. However, it poses significant\nchallenges due to the high-dimensional action space and intricate coordination\nrequirements. While video prediction has been recently studied for\nrepresentation learning and control, leveraging its ability to capture rich\ndynamic and behavioral information, its potential for enhancing bimanual\ncoordination remains underexplored. To bridge this gap, we propose a unified\ndiffusion-based framework for the joint optimization of video and action\nprediction. Specifically, we propose a multi-frame latent prediction strategy\nthat encodes future states in a compressed latent space, preserving\ntask-relevant features. Furthermore, we introduce a unidirectional attention\nmechanism where video prediction is conditioned on the action, while action\nprediction remains independent of video prediction. This design allows us to\nomit video prediction during inference, significantly enhancing efficiency.\nExperiments on two simulated benchmarks and a real-world setting demonstrate a\nsignificant improvement in the success rate over the strong baseline ACT using\nour method, achieving a \\textbf{24.9\\%} increase on ALOHA, an \\textbf{11.1\\%}\nincrease on RoboTwin, and a \\textbf{32.5\\%} increase in real-world experiments.\nOur models and code are publicly available at\nhttps://github.com/return-sleep/Diffusion_based_imaginative_Coordination.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u89c6\u9891\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u534f\u8c03\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u5bb6\u5ead\u670d\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u548c\u590d\u6742\u534f\u8c03\u9700\u6c42\u5e26\u6765\u4e86\u6311\u6218\u3002\u89c6\u9891\u9884\u6d4b\u5728\u8868\u793a\u5b66\u4e60\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u591a\u5e27\u6f5c\u5728\u9884\u6d4b\u7b56\u7565\u548c\u5355\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89c6\u9891\u9884\u6d4b\u4f9d\u8d56\u4e8e\u52a8\u4f5c\uff0c\u800c\u52a8\u4f5c\u9884\u6d4b\u72ec\u7acb\u4e8e\u89c6\u9891\u9884\u6d4b\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728\u4e24\u4e2a\u6a21\u62df\u57fa\u51c6\u548c\u4e00\u4e2a\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff0cALOHA\u63d0\u9ad824.9%\uff0cRoboTwin\u63d0\u9ad811.1%\uff0c\u771f\u5b9e\u5b9e\u9a8c\u63d0\u9ad832.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u534f\u8c03\u6027\u548c\u6210\u529f\u7387\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11035", "abs": "https://arxiv.org/abs/2507.11035", "authors": ["Lirong Zheng", "Yanshan Li", "Rui Yu", "Kaihao Zhang"], "title": "Efficient Dual-domain Image Dehazing with Haze Prior Perception", "comment": "12 pages", "summary": "Transformer-based models exhibit strong global modeling capabilities in\nsingle-image dehazing, but their high computational cost limits real-time\napplicability. Existing methods predominantly rely on spatial-domain features\nto capture long-range dependencies, which are computationally expensive and\noften inadequate under complex haze conditions. While some approaches introduce\nfrequency-domain cues, the weak coupling between spatial and frequency branches\nlimits the overall performance. To overcome these limitations, we propose the\nDark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel\ndual-domain framework that performs physically guided degradation alignment\nacross spatial and frequency domains. At its core, the DGFDBlock comprises two\nkey modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a\npixel-level haze confidence map from dark channel priors to adaptively enhance\nhaze-relevant frequency components, thereby achieving global degradation-aware\nspectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which\nfuses multi-scale features through diverse convolutional kernels and hybrid\ngating mechanisms to recover fine structural details. Additionally, a Prior\nCorrection Guidance Branch (PCGB) incorporates a closed-loop feedback\nmechanism, enabling iterative refinement of the prior by intermediate dehazed\nfeatures and significantly improving haze localization accuracy, especially in\nchallenging outdoor scenes. Extensive experiments on four benchmark haze\ndatasets demonstrate that DGFDNet achieves state-of-the-art performance with\nsuperior robustness and real-time efficiency. Code is available at:\nhttps://github.com/Dilizlr/DGFDNet.", "AI": {"tldr": "DGFDNet\u662f\u4e00\u79cd\u65b0\u578b\u7684\u53cc\u57df\u53bb\u96fe\u7f51\u7edc\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u9891\u7387\u57df\u7684\u7ed3\u5408\u63d0\u5347\u53bb\u96fe\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u96fe\u51b5\u4e0b\u6027\u80fd\u4e0d\u8db3\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cDGFDNet\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6697\u901a\u9053\u5148\u9a8c\u548c\u9891\u7387\u8c03\u5236\uff0c\u901a\u8fc7HAFM\u548cMGAM\u6a21\u5757\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u9891\u7387\u589e\u5f3a\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u517c\u5177\u9c81\u68d2\u6027\u548c\u5b9e\u65f6\u6027\u3002", "conclusion": "DGFDNet\u901a\u8fc7\u53cc\u57df\u534f\u540c\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u96fe\u6548\u679c\u548c\u6548\u7387\u3002"}}
{"id": "2507.10747", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10747", "abs": "https://arxiv.org/abs/2507.10747", "authors": ["Kaustubh Tangsali", "Rishikesh Ranade", "Mohammad Amin Nabian", "Alexey Kamenev", "Peter Sharpe", "Neil Ashton", "Ram Cherukuri", "Sanjay Choudhry"], "title": "A Benchmarking Framework for AI models in Automotive Aerodynamics", "comment": null, "summary": "In this paper, we introduce a benchmarking framework within the open-source\nNVIDIA PhysicsNeMo-CFD framework designed to systematically assess the\naccuracy, performance, scalability, and generalization capabilities of AI\nmodels for automotive aerodynamics predictions. The open extensible framework\nenables incorporation of a diverse set of metrics relevant to the\nComputer-Aided Engineering (CAE) community. By providing a standardized\nmethodology for comparing AI models, the framework enhances transparency and\nconsistency in performance assessment, with the overarching goal of improving\nthe understanding and development of these models to accelerate research and\ninnovation in the field. To demonstrate its utility, the framework includes\nevaluation of both surface and volumetric flow field predictions on three AI\nmodels: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It\nalso includes guidelines for integrating additional models and datasets, making\nit extensible for physically consistent metrics. This benchmarking study aims\nto enable researchers and industry professionals in selecting, refining, and\nadvancing AI-driven aerodynamic modeling approaches, ultimately fostering the\ndevelopment of more efficient, accurate, and interpretable solutions in\nautomotive aerodynamics", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30AI\u6a21\u578b\u5728\u6c7d\u8f66\u7a7a\u6c14\u52a8\u529b\u5b66\u9884\u6d4b\u4e2d\u7684\u51c6\u786e\u6027\u3001\u6027\u80fd\u3001\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5f00\u6e90\u57fa\u51c6\u6846\u67b6\u3002", "motivation": "\u63d0\u5347AI\u6a21\u578b\u5728\u6c7d\u8f66\u7a7a\u6c14\u52a8\u529b\u5b66\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u8bc4\u4f30\uff0c\u52a0\u901f\u7814\u7a76\u548c\u521b\u65b0\u3002", "method": "\u5728NVIDIA PhysicsNeMo-CFD\u6846\u67b6\u4e2d\u6784\u5efa\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u6837\u5316\u7684CAE\u76f8\u5173\u6307\u6807\u3002", "result": "\u8bc4\u4f30\u4e86\u4e09\u79cdAI\u6a21\u578b\uff08DoMINO\u3001X-MeshGraphNet\u3001FIGConvNet\uff09\u5728DrivAerML\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u6269\u5c55\u6307\u5357\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u548c\u884c\u4e1a\u4e13\u4e1a\u4eba\u58eb\u9009\u62e9\u548c\u4f18\u5316AI\u9a71\u52a8\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u5efa\u6a21\u65b9\u6cd5\uff0c\u63a8\u52a8\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u53d1\u5c55\u3002"}}
{"id": "2507.11302", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11302", "abs": "https://arxiv.org/abs/2507.11302", "authors": ["Jesse J. Hagenaars", "Stein Stroobants", "Sander M. Bohte", "Guido C. H. E. De Croon"], "title": "All Eyes, no IMU: Learning Flight Attitude from Vision Alone", "comment": null, "summary": "Vision is an essential part of attitude control for many flying animals, some\nof which have no dedicated sense of gravity. Flying robots, on the other hand,\ntypically depend heavily on accelerometers and gyroscopes for attitude\nstabilization. In this work, we present the first vision-only approach to\nflight control for use in generic environments. We show that a quadrotor drone\nequipped with a downward-facing event camera can estimate its attitude and\nrotation rate from just the event stream, enabling flight control without\ninertial sensors. Our approach uses a small recurrent convolutional neural\nnetwork trained through supervised learning. Real-world flight tests\ndemonstrate that our combination of event camera and low-latency neural network\nis capable of replacing the inertial measurement unit in a traditional flight\ncontrol loop. Furthermore, we investigate the network's generalization across\ndifferent environments, and the impact of memory and different fields of view.\nWhile networks with memory and access to horizon-like visual cues achieve best\nperformance, variants with a narrower field of view achieve better relative\ngeneralization. Our work showcases vision-only flight control as a promising\ncandidate for enabling autonomous, insect-scale flying robots.", "AI": {"tldr": "\u9996\u4e2a\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u65e0\u4eba\u673a\u98de\u884c\u63a7\u5236\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u60ef\u6027\u4f20\u611f\u5668\u3002", "motivation": "\u98de\u884c\u673a\u5668\u4eba\u901a\u5e38\u4f9d\u8d56\u60ef\u6027\u4f20\u611f\u5668\uff0c\u800c\u8bb8\u591a\u98de\u884c\u751f\u7269\u4ec5\u4f9d\u8d56\u89c6\u89c9\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4ec5\u89c6\u89c9\u63a7\u5236\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u4e0b\u89c6\u4e8b\u4ef6\u76f8\u673a\u548c\u9012\u5f52\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f30\u8ba1\u59ff\u6001\u548c\u65cb\u8f6c\u901f\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u66ff\u4ee3\u4f20\u7edf\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff0c\u4e14\u7a84\u89c6\u91ce\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u4ec5\u89c6\u89c9\u63a7\u5236\u4e3a\u5fae\u578b\u81ea\u4e3b\u98de\u884c\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.11037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11037", "abs": "https://arxiv.org/abs/2507.11037", "authors": ["Jie-Wen Li", "Zi-Han Ye", "Qingyuan Zhou", "Jiayi Song", "Ying He", "Ben Fei", "Wen-Ming Chen"], "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion", "comment": "15 pages, 10 figures, 2 tables", "summary": "The kinematics analysis of foot-ankle complex during gait is essential for\nadvancing biomechanical research and clinical assessment. Collecting accurate\nsurface geometry data from the foot and ankle during dynamic gait conditions is\ninherently challenging due to swing foot occlusions and viewing limitations.\nThus, this paper introduces FootGait3D, a novel multi-view dataset of\nhigh-resolution ankle-foot surface point clouds captured during natural gait.\nDifferent from existing gait datasets that typically target whole-body or\nlower-limb motion, FootGait3D focuses specifically on the detailed modeling of\nthe ankle-foot region, offering a finer granularity of motion data. To address\nthis, FootGait3D consists of 8,403 point cloud frames collected from 46\nsubjects using a custom five-camera depth sensing system. Each frame includes a\ncomplete 5-view reconstruction of the foot and ankle (serving as ground truth)\nalong with partial point clouds obtained from only four, three, or two views.\nThis structured variation enables rigorous evaluation of 3D point cloud\ncompletion methods under varying occlusion levels and viewpoints. Our dataset\nis designed for shape completion tasks, facilitating the benchmarking of\nstate-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and\nmulti-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the\nchallenge of recovering the full foot geometry from occluded inputs. FootGait3D\nhas significant potential to advance research in biomechanics and multi-segment\nfoot modeling, offering a valuable testbed for clinical gait analysis,\nprosthetic design, and robotics applications requiring detailed 3D models of\nthe foot during motion. The dataset is now available at\nhttps://huggingface.co/datasets/ljw285/FootGait3D.", "AI": {"tldr": "FootGait3D\u662f\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u591a\u89c6\u89d2\u8db3\u8e1d\u70b9\u4e91\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6b65\u6001\u5206\u6790\u548c3D\u70b9\u4e91\u8865\u5168\u4efb\u52a1\u3002", "motivation": "\u6b65\u6001\u4e2d\u8db3\u8e1d\u590d\u6742\u8fd0\u52a8\u7684\u7cbe\u786e\u51e0\u4f55\u6570\u636e\u91c7\u96c6\u56e0\u906e\u6321\u548c\u89c6\u89d2\u9650\u5236\u800c\u56f0\u96be\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u96c6\u652f\u6301\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u4e94\u6444\u50cf\u5934\u6df1\u5ea6\u4f20\u611f\u7cfb\u7edf\u91c7\u96c646\u540d\u53d7\u8bd5\u8005\u76848,403\u5e27\u70b9\u4e91\u6570\u636e\uff0c\u5305\u542b\u5b8c\u6574\u548c\u90e8\u5206\u89c6\u89d2\u7684\u70b9\u4e91\u3002", "result": "\u6570\u636e\u96c6\u652f\u6301\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u70b9\u4e91\u8865\u5168\u65b9\u6cd5\u7684\u8bc4\u4f30\uff0c\u4e3a\u8db3\u90e8\u51e0\u4f55\u6062\u590d\u63d0\u4f9b\u57fa\u51c6\u3002", "conclusion": "FootGait3D\u6709\u52a9\u4e8e\u751f\u7269\u529b\u5b66\u3001\u4e34\u5e8a\u6b65\u6001\u5206\u6790\u548c\u673a\u5668\u4eba\u5e94\u7528\u7684\u7814\u7a76\uff0c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2507.10768", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10768", "abs": "https://arxiv.org/abs/2507.10768", "authors": ["Bart Pogodzinski", "Christopher Wewer", "Bernt Schiele", "Jan Eric Lenssen"], "title": "Spatial Reasoners for Continuous Variables in Any Domain", "comment": "For the project documentation see https://spatialreasoners.github.io/\n  . The SRM project website is available at\n  https://geometric-rl.mpi-inf.mpg.de/srm/ . The work was published on ICML\n  2025 CODEML workshop", "summary": "We present Spatial Reasoners, a software framework to perform spatial\nreasoning over continuous variables with generative denoising models. Denoising\ngenerative models have become the de-facto standard for image generation, due\nto their effectiveness in sampling from complex, high-dimensional\ndistributions. Recently, they have started being explored in the context of\nreasoning over multiple continuous variables. Providing infrastructure for\ngenerative reasoning with such models requires a high effort, due to a wide\nrange of different denoising formulations, samplers, and inference strategies.\nOur presented framework aims to facilitate research in this area, providing\neasy-to-use interfaces to control variable mapping from arbitrary data domains,\ngenerative model paradigms, and inference strategies. Spatial Reasoners are\nopenly available at https://spatialreasoners.github.io/", "AI": {"tldr": "Spatial Reasoners\u662f\u4e00\u4e2a\u7528\u4e8e\u8fde\u7eed\u53d8\u91cf\u7a7a\u95f4\u63a8\u7406\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u57fa\u4e8e\u751f\u6210\u53bb\u566a\u6a21\u578b\u3002", "motivation": "\u751f\u6210\u53bb\u566a\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u5df2\u6210\u4e3a\u6807\u51c6\uff0c\u4f46\u5728\u591a\u8fde\u7eed\u53d8\u91cf\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u5c1a\u9700\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u3002", "method": "\u63d0\u4f9b\u6613\u7528\u63a5\u53e3\uff0c\u652f\u6301\u53d8\u91cf\u6620\u5c04\u3001\u751f\u6210\u6a21\u578b\u8303\u5f0f\u548c\u63a8\u7406\u7b56\u7565\u3002", "result": "\u6846\u67b6\u5f00\u6e90\uff0c\u65e8\u5728\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u3002", "conclusion": "Spatial Reasoners\u4e3a\u751f\u6210\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2507.11345", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11345", "abs": "https://arxiv.org/abs/2507.11345", "authors": ["Oscar Lima", "Marc Vinci", "Sunandita Patra", "Sebastian Stock", "Joachim Hertzberg", "Martin Atzmueller", "Malik Ghallab", "Dana Nau", "Paolo Traverso"], "title": "Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM", "comment": "Accepted in ECMR 2025 conference", "summary": "Robotic task execution faces challenges due to the inconsistency between\nsymbolic planner models and the rich control structures actually running on the\nrobot. In this paper, we present the first physical deployment of an integrated\nactor-planner system that shares hierarchical operational models for both\nacting and planning, interleaving the Reactive Acting Engine (RAE) with an\nanytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile\nmanipulator in a real-world deployment for an object collection task. Our\nexperiments demonstrate robust task execution under action failures and sensor\nnoise, and provide empirical insights into the interleaved acting-and-planning\ndecision making process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53cd\u5e94\u5f0f\u6267\u884c\u5f15\u64ce\uff08RAE\uff09\u548c\u8499\u7279\u5361\u6d1b\u89c4\u5212\u5668\uff08UPOM\uff09\u7684\u96c6\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u89c4\u5212\u6a21\u578b\u4e0e\u5b9e\u9645\u673a\u5668\u4eba\u63a7\u5236\u7ed3\u6784\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408RAE\u548cUPOM\uff0c\u5171\u4eab\u5c42\u6b21\u5316\u64cd\u4f5c\u6a21\u578b\uff0c\u5b9e\u73b0\u5b9e\u65f6\u89c4\u5212\u4e0e\u6267\u884c\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u9c81\u68d2\u7684\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u80fd\u591f\u5e94\u5bf9\u52a8\u4f5c\u5931\u8d25\u548c\u4f20\u611f\u5668\u566a\u58f0\u3002", "conclusion": "\u7cfb\u7edf\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89c4\u5212\u4e0e\u6267\u884c\u96c6\u6210\u65b9\u6848\u3002"}}
{"id": "2507.11040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11040", "abs": "https://arxiv.org/abs/2507.11040", "authors": ["Nicolas Drapier", "Aladine Chetouani", "Aur\u00e9lien Chateigner"], "title": "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery", "comment": "11 pages, 9 figures", "summary": "We present GLOD, a transformer-first architecture for object detection in\nhigh-resolution satellite imagery. GLOD replaces CNN backbones with a Swin\nTransformer for end-to-end feature extraction, combined with novel UpConvMixer\nblocks for robust upsampling and Fusion Blocks for multi-scale feature\nintegration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods\nby 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a\nmulti-path head design capturing objects across scales. The architecture is\noptimized for satellite imagery challenges, leveraging spatial priors while\nmaintaining computational efficiency.", "AI": {"tldr": "GLOD\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7Swin Transformer\u548c\u65b0\u578bUpConvMixer\u5757\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd511.46%\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u4e2d\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u4f7f\u7528Swin Transformer\u66ff\u4ee3CNN\u9aa8\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408UpConvMixer\u5757\u548cFusion Blocks\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728xView\u6570\u636e\u96c6\u4e0a\u8fbe\u523032.95%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd511.46%\u3002", "conclusion": "GLOD\u901a\u8fc7\u521b\u65b0\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f\u56fe\u50cf\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.10792", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10792", "abs": "https://arxiv.org/abs/2507.10792", "authors": ["Yuchen Wang", "Hongjue Zhao", "Haohong Lin", "Enze Xu", "Lifang He", "Huajie Shao"], "title": "A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments", "comment": "8 pages, 6 figures, accepted in ICML 2025", "summary": "This work aims to address the problem of long-term dynamic forecasting in\ncomplex environments where data are noisy and irregularly sampled. While recent\nstudies have introduced some methods to improve prediction performance, these\napproaches still face a significant challenge in handling long-term\nextrapolation tasks under such complex scenarios. To overcome this challenge,\nwe propose Phy-SSM, a generalizable method that integrates partial physics\nknowledge into state space models (SSMs) for long-term dynamics forecasting in\ncomplex environments. Our motivation is that SSMs can effectively capture\nlong-range dependencies in sequential data and model continuous dynamical\nsystems, while the incorporation of physics knowledge improves generalization\nability. The key challenge lies in how to seamlessly incorporate partially\nknown physics into SSMs. To achieve this, we decompose partially known system\ndynamics into known and unknown state matrices, which are integrated into a\nPhy-SSM unit. To further enhance long-term prediction performance, we introduce\na physics state regularization term to make the estimated latent states align\nwith system dynamics. Besides, we theoretically analyze the uniqueness of the\nsolutions for our method. Extensive experiments on three real-world\napplications, including vehicle motion prediction, drone state prediction, and\nCOVID-19 epidemiology forecasting, demonstrate the superior performance of\nPhy-SSM over the baselines in both long-term interpolation and extrapolation\ntasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.", "AI": {"tldr": "\u63d0\u51faPhy-SSM\u65b9\u6cd5\uff0c\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u4e0e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e0b\u957f\u671f\u52a8\u6001\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u957f\u671f\u5916\u63a8\u4efb\u52a1\u8868\u73b0\u4e0d\u4f73\uff0c\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u53ef\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u90e8\u5206\u5df2\u77e5\u7269\u7406\u77e5\u8bc6\u5206\u89e3\u4e3a\u5df2\u77e5\u548c\u672a\u77e5\u72b6\u6001\u77e9\u9635\uff0c\u96c6\u6210\u5230Phy-SSM\u5355\u5143\uff0c\u5e76\u5f15\u5165\u7269\u7406\u72b6\u6001\u6b63\u5219\u5316\u9879\u3002", "result": "\u5728\u8f66\u8f86\u8fd0\u52a8\u3001\u65e0\u4eba\u673a\u72b6\u6001\u548cCOVID-19\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "Phy-SSM\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u9884\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.11402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11402", "abs": "https://arxiv.org/abs/2507.11402", "authors": ["Supun Dissanayaka", "Alexander Ferrein", "Till Hofmann", "Kosuke Nakajima", "Mario Sanz-Lopez", "Jesus Savage", "Daniel Swoboda", "Matteo Tschesche", "Wataru Uemura", "Tarik Viehmann", "Shohei Yasuda"], "title": "From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League", "comment": "RoboCup Symposium 2025", "summary": "The RoboCup Logistics League is a RoboCup competition in a smart factory\nscenario that has focused on task planning, job scheduling, and multi-agent\ncoordination. The focus on production logistics allowed teams to develop highly\ncompetitive strategies, but also meant that some recent developments in the\ncontext of smart manufacturing are not reflected in the competition, weakening\nits relevance over the years. In this paper, we describe the vision for the\nRoboCup Smart Manufacturing League, a new competition designed as a larger\nsmart manufacturing scenario, reflecting all the major aspects of a modern\nfactory. It will consist of several tracks that are initially independent but\ngradually combined into one smart manufacturing scenario. The new tracks will\ncover industrial robotics challenges such as assembly, human-robot\ncollaboration, and humanoid robotics, but also retain a focus on production\nlogistics. We expect the reenvisioned competition to be more attractive to\nnewcomers and well-tried teams, while also shifting the focus to current and\nfuture challenges of industrial robotics.", "AI": {"tldr": "RoboCup Logistics League\u5347\u7ea7\u4e3aRoboCup Smart Manufacturing League\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u53cd\u6620\u73b0\u4ee3\u5de5\u5382\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709RoboCup Logistics League\u672a\u80fd\u6db5\u76d6\u667a\u80fd\u5236\u9020\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u524a\u5f31\u4e86\u5176\u76f8\u5173\u6027\u3002", "method": "\u8bbe\u8ba1\u65b0\u7684\u7ade\u8d5b\uff0c\u5305\u542b\u591a\u4e2a\u72ec\u7acb\u4f46\u9010\u6b65\u7ed3\u5408\u7684\u8d5b\u9053\uff0c\u6db5\u76d6\u5de5\u4e1a\u673a\u5668\u4eba\u6311\u6218\u548c\u751f\u4ea7\u7269\u6d41\u3002", "result": "\u65b0\u7ade\u8d5b\u5c06\u66f4\u5438\u5f15\u65b0\u8001\u56e2\u961f\uff0c\u5e76\u805a\u7126\u5f53\u524d\u548c\u672a\u6765\u7684\u5de5\u4e1a\u673a\u5668\u4eba\u6311\u6218\u3002", "conclusion": "\u65b0\u7ade\u8d5b\u6709\u671b\u63d0\u5347\u76f8\u5173\u6027\u548c\u5438\u5f15\u529b\uff0c\u63a8\u52a8\u5de5\u4e1a\u673a\u5668\u4eba\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.11055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11055", "abs": "https://arxiv.org/abs/2507.11055", "authors": ["Shuchang Ye", "Usman Naseem", "Mingyuan Meng", "Jinman Kim"], "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation", "comment": "Accepted to ICCV 2025", "summary": "Medical language-guided segmentation, integrating textual clinical reports as\nauxiliary guidance to enhance image segmentation, has demonstrated significant\nimprovements over unimodal approaches. However, its inherent reliance on paired\nimage-text input, which we refer to as ``textual reliance\", presents two\nfundamental limitations: 1) many medical segmentation datasets lack paired\nreports, leaving a substantial portion of image-only data underutilized for\ntraining; and 2) inference is limited to retrospective analysis of cases with\npaired reports, limiting its applicability in most clinical scenarios where\nsegmentation typically precedes reporting. To address these limitations, we\npropose ProLearn, the first Prototype-driven Learning framework for\nlanguage-guided segmentation that fundamentally alleviates textual reliance. At\nits core, in ProLearn, we introduce a novel Prototype-driven Semantic\nApproximation (PSA) module to enable approximation of semantic guidance from\ntextual input. PSA initializes a discrete and compact prototype space by\ndistilling segmentation-relevant semantics from textual reports. Once\ninitialized, it supports a query-and-respond mechanism which approximates\nsemantic guidance for images without textual input, thereby alleviating textual\nreliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG\ndemonstrate that ProLearn outperforms state-of-the-art language-guided methods\nwhen limited text is available.", "AI": {"tldr": "ProLearn\u6846\u67b6\u901a\u8fc7\u539f\u578b\u9a71\u52a8\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u8bed\u8a00\u5f15\u5bfc\u5206\u5272\u4e2d\u7684\u6587\u672c\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65e0\u914d\u5bf9\u6587\u672c\u6570\u636e\u7684\u5229\u7528\u7387\u548c\u4e34\u5e8a\u9002\u7528\u6027\u3002", "motivation": "\u533b\u5b66\u8bed\u8a00\u5f15\u5bfc\u5206\u5272\u4f9d\u8d56\u6210\u5bf9\u7684\u56fe\u50cf-\u6587\u672c\u8f93\u5165\uff0c\u5bfc\u81f4\u8bb8\u591a\u65e0\u914d\u5bf9\u6587\u672c\u7684\u6570\u636e\u65e0\u6cd5\u5145\u5206\u5229\u7528\uff0c\u4e14\u9650\u5236\u4e86\u5728\u4e34\u5e8a\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faProLearn\u6846\u67b6\uff0c\u5f15\u5165\u539f\u578b\u9a71\u52a8\u7684\u8bed\u4e49\u8fd1\u4f3c\uff08PSA\uff09\u6a21\u5757\uff0c\u901a\u8fc7\u79bb\u6563\u539f\u578b\u7a7a\u95f4\u8fd1\u4f3c\u8bed\u4e49\u5f15\u5bfc\uff0c\u51cf\u5c11\u5bf9\u6587\u672c\u8f93\u5165\u7684\u4f9d\u8d56\u3002", "result": "\u5728QaTa-COV19\u3001MosMedData+\u548cKvasir-SEG\u6570\u636e\u96c6\u4e0a\uff0cProLearn\u5728\u6587\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ProLearn\u901a\u8fc7\u539f\u578b\u9a71\u52a8\u7684\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u5f15\u5bfc\u5206\u5272\u7684\u6587\u672c\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.10797", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10797", "abs": "https://arxiv.org/abs/2507.10797", "authors": ["Mohammad Pedramfar", "Siamak Ravanbakhsh"], "title": "Multi-Armed Sampling Problem and the End of Exploration", "comment": null, "summary": "This paper introduces the framework of multi-armed sampling, as the sampling\ncounterpart to the optimization problem of multi-arm bandits. Our primary\nmotivation is to rigorously examine the exploration-exploitation trade-off in\nthe context of sampling. We systematically define plausible notions of regret\nfor this framework and establish corresponding lower bounds. We then propose a\nsimple algorithm that achieves these optimal regret bounds. Our theoretical\nresults demonstrate that in contrast to optimization, sampling does not require\nexploration. To further connect our findings with those of multi-armed bandits,\nwe define a continuous family of problems and associated regret measures that\nsmoothly interpolates and unifies multi-armed sampling and multi-armed bandit\nproblems using a temperature parameter. We believe the multi-armed sampling\nframework, and our findings in this setting can have a foundational role in the\nstudy of sampling including recent neural samplers, akin to the role of\nmulti-armed bandits in reinforcement learning. In particular, our work sheds\nlight on the need for exploration and the convergence properties of algorithm\nfor entropy-regularized reinforcement learning, fine-tuning of pretrained\nmodels and reinforcement learning with human feedback (RLHF).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u81c2\u91c7\u6837\u6846\u67b6\uff0c\u4f5c\u4e3a\u591a\u81c2\u8001\u864e\u673a\u4f18\u5316\u95ee\u9898\u7684\u91c7\u6837\u5bf9\u5e94\uff0c\u63a2\u8ba8\u4e86\u91c7\u6837\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u6700\u4f18\u9057\u61be\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u91c7\u6837\u4e2d\u7684\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff0c\u586b\u8865\u591a\u81c2\u8001\u864e\u673a\u4e0e\u91c7\u6837\u95ee\u9898\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u5b9a\u4e49\u91c7\u6837\u6846\u67b6\u7684\u9057\u61be\u6982\u5ff5\uff0c\u63d0\u51fa\u7b80\u5355\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u6700\u4f18\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u91c7\u6837\u65e0\u9700\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u6e29\u5ea6\u53c2\u6570\u7edf\u4e00\u591a\u81c2\u91c7\u6837\u4e0e\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u3002", "conclusion": "\u591a\u81c2\u91c7\u6837\u6846\u67b6\u5bf9\u7814\u7a76\u795e\u7ecf\u91c7\u6837\u5668\u7b49\u5177\u6709\u57fa\u7840\u6027\u610f\u4e49\uff0c\u63ed\u793a\u4e86\u71b5\u6b63\u5219\u5316\u5f3a\u5316\u5b66\u4e60\u7b49\u9886\u57df\u7684\u63a2\u7d22\u9700\u6c42\u4e0e\u7b97\u6cd5\u6536\u655b\u6027\u3002"}}
{"id": "2507.11447", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11447", "abs": "https://arxiv.org/abs/2507.11447", "authors": ["Shuo Yang", "John Z. Zhang", "Ibrahima Sory Sow", "Zachary Manchester"], "title": "Multi-IMU Sensor Fusion for Legged Robots", "comment": "16 pages", "summary": "This paper presents a state-estimation solution for legged robots that uses a\nset of low-cost, compact, and lightweight sensors to achieve low-drift pose and\nvelocity estimation under challenging locomotion conditions. The key idea is to\nleverage multiple inertial measurement units on different links of the robot to\ncorrect a major error source in standard proprioceptive odometry. We fuse the\ninertial sensor information and joint encoder measurements in an extended\nKalman filter, then combine the velocity estimate from this filter with camera\ndata in a factor-graph-based sliding-window estimator to form a\nvisual-inertial-leg odometry method. We validate our state estimator through\ncomprehensive theoretical analysis and hardware experiments performed using\nreal-world robot data collected during a variety of challenging locomotion\ntasks. Our algorithm consistently achieves minimal position deviation, even in\nscenarios involving substantial ground impact, foot slippage, and sudden body\nrotations. A C++ implementation, along with a large-scale dataset, is available\nat https://github.com/ShuoYangRobotics/Cerberus2.0.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u7684\u4f4e\u6210\u672c\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u8fd0\u52a8\u6761\u4ef6\u4e0b\u7684\u4f4e\u6f02\u79fb\u4f4d\u59ff\u548c\u901f\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u6807\u51c6\u672c\u4f53\u611f\u77e5\u91cc\u7a0b\u8ba1\u4e2d\u7684\u4e3b\u8981\u8bef\u5dee\u6e90\uff0c\u63d0\u9ad8\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u8fd0\u52a8\u6761\u4ef6\u4e0b\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u878d\u5408\u60ef\u6027\u4f20\u611f\u5668\u548c\u5173\u8282\u7f16\u7801\u5668\u6570\u636e\uff0c\u518d\u7ed3\u5408\u76f8\u673a\u6570\u636e\u5728\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u6ed1\u52a8\u7a97\u53e3\u4f30\u8ba1\u5668\u4e2d\u5f62\u6210\u89c6\u89c9-\u60ef\u6027-\u817f\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u6311\u6218\u6027\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u7b97\u6cd5\u8868\u73b0\u51fa\u6700\u5c0f\u7684\u4f4d\u7f6e\u504f\u5dee\uff0c\u5c24\u5176\u5728\u5f3a\u5730\u9762\u51b2\u51fb\u3001\u8db3\u90e8\u6ed1\u52a8\u548c\u7a81\u7136\u8eab\u4f53\u65cb\u8f6c\u7b49\u573a\u666f\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2507.11061", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11061", "abs": "https://arxiv.org/abs/2507.11061", "authors": ["Hayeon Kim", "Ji Ha Jang", "Se Young Chun"], "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling", "comment": null, "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing.", "AI": {"tldr": "RoMaP\u662f\u4e00\u79cd\u65b0\u578b\u7684\u5c40\u90e83D\u9ad8\u65af\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc73D-GALP\u548c\u6b63\u5219\u5316SDS\u635f\u5931\u5b9e\u73b0\u7cbe\u786e\u7684\u90e8\u4ef6\u7ea7\u4fee\u6539\u3002", "motivation": "\u89e3\u51b3\u9ad8\u65af\u6e85\u5c04\u4e2d\u591a\u89c6\u56fe2D\u90e8\u4ef6\u5206\u5272\u4e0d\u4e00\u81f4\u548cSDS\u635f\u5931\u6a21\u7cca\u6027\u5bfc\u81f4\u7684\u5c40\u90e83D\u7f16\u8f91\u96be\u9898\u3002", "method": "\u5f15\u51653D-GALP\u6a21\u5757\u751f\u6210\u9c81\u68d2\u76843D\u63a9\u7801\uff0c\u5e76\u63d0\u51fa\u6b63\u5219\u5316SDS\u635f\u5931\uff08\u5305\u62ecL1\u951a\u5b9a\u635f\u5931\u548c\u9ad8\u65af\u5148\u9a8c\u53bb\u9664\uff09\u3002", "result": "\u5728\u91cd\u5efa\u548c\u751f\u6210\u7684\u9ad8\u65af\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5c40\u90e83D\u7f16\u8f91\u6548\u679c\u3002", "conclusion": "RoMaP\u4e3a\u90e8\u4ef6\u7ea73D\u9ad8\u65af\u7f16\u8f91\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10809", "abs": "https://arxiv.org/abs/2507.10809", "authors": ["Kazi Tasnim Zinat", "Yun Zhou", "Xiang Lyu", "Yawei Wang", "Zhicheng Liu", "Panpan Xu"], "title": "Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions", "comment": "Accepted at ICANN 2025", "summary": "Inferring causal relationships between event pairs in a temporal sequence is\napplicable in many domains such as healthcare, manufacturing, and\ntransportation. Most existing work on causal inference primarily focuses on\nevent types within the designated domain, without considering the impact of\nexogenous out-of-domain interventions. In real-world settings, these\nout-of-domain interventions can significantly alter causal dynamics. To address\nthis gap, we propose a new causal framework to define average treatment effect\n(ATE), beyond independent and identically distributed (i.i.d.) data in classic\nRubin's causal framework, to capture the causal relation shift between events\nof temporal process under out-of-domain intervention. We design an unbiased ATE\nestimator, and devise a Transformer-based neural network model to handle both\nlong-range temporal dependencies and local patterns while integrating\nout-of-domain intervention information into process modeling. Extensive\nexperiments on both simulated and real-world datasets demonstrate that our\nmethod outperforms baselines in ATE estimation and goodness-of-fit under\nout-of-domain-augmented point processes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56e0\u679c\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u5916\u57df\u5e72\u9884\u4e0b\u7684\u65f6\u5e8f\u4e8b\u4ef6\u56e0\u679c\u63a8\u65ad\uff0c\u8bbe\u8ba1\u4e86\u65e0\u504fATE\u4f30\u8ba1\u5668\u548cTransformer\u6a21\u578b\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u57df\u5185\u4e8b\u4ef6\u7c7b\u578b\uff0c\u5ffd\u7565\u4e86\u5916\u57df\u5e72\u9884\u7684\u5f71\u54cd\uff0c\u800c\u73b0\u5b9e\u4e2d\u8fd9\u4e9b\u5e72\u9884\u4f1a\u663e\u8457\u6539\u53d8\u56e0\u679c\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u65b0\u56e0\u679c\u6846\u67b6\u5b9a\u4e49ATE\uff0c\u8bbe\u8ba1\u65e0\u504fATE\u4f30\u8ba1\u5668\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6574\u5408\u5916\u57df\u5e72\u9884\u4fe1\u606f\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728ATE\u4f30\u8ba1\u548c\u62df\u5408\u4f18\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u65b0\u6846\u67b6\u6709\u6548\u6355\u6349\u5916\u57df\u5e72\u9884\u4e0b\u7684\u56e0\u679c\u5173\u7cfb\u53d8\u5316\uff0c\u4e3a\u65f6\u5e8f\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11460", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11460", "abs": "https://arxiv.org/abs/2507.11460", "authors": ["Jacinto Colan", "Ana Davila", "Yutaro Yamada", "Yasuhisa Hasegawa"], "title": "Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Human-robot collaboration in surgery represents a significant area of\nresearch, driven by the increasing capability of autonomous robotic systems to\nassist surgeons in complex procedures. This systematic review examines the\nadvancements and persistent challenges in the development of autonomous\nsurgical robotic assistants (ASARs), focusing specifically on scenarios where\nrobots provide meaningful and active support to human surgeons. Adhering to the\nPRISMA guidelines, a comprehensive literature search was conducted across the\nIEEE Xplore, Scopus, and Web of Science databases, resulting in the selection\nof 32 studies for detailed analysis. Two primary collaborative setups were\nidentified: teleoperation-based assistance and direct hands-on interaction. The\nfindings reveal a growing research emphasis on ASARs, with predominant\napplications currently in endoscope guidance, alongside emerging progress in\nautonomous tool manipulation. Several key challenges hinder wider adoption,\nincluding the alignment of robotic actions with human surgeon preferences, the\nnecessity for procedural awareness within autonomous systems, the establishment\nof seamless human-robot information exchange, and the complexities of skill\nacquisition in shared workspaces. This review synthesizes current trends,\nidentifies critical limitations, and outlines future research directions\nessential to improve the reliability, safety, and effectiveness of human-robot\ncollaboration in surgical environments.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u63a2\u8ba8\u4e86\u81ea\u4e3b\u624b\u672f\u673a\u5668\u4eba\u52a9\u624b\uff08ASARs\uff09\u7684\u53d1\u5c55\u4e0e\u6311\u6218\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u8d8b\u52bf\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u673a\u5668\u4eba\u5982\u4f55\u5728\u590d\u6742\u624b\u672f\u4e2d\u4e3a\u5916\u79d1\u533b\u751f\u63d0\u4f9b\u6709\u6548\u652f\u6301\uff0c\u63a8\u52a8\u4eba\u673a\u534f\u4f5c\u7684\u53d1\u5c55\u3002", "method": "\u9075\u5faaPRISMA\u6307\u5357\uff0c\u5bf9IEEE Xplore\u3001Scopus\u548cWeb of Science\u6570\u636e\u5e93\u4e2d\u768432\u9879\u7814\u7a76\u8fdb\u884c\u4e86\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0ASARs\u4e3b\u8981\u5e94\u7528\u4e8e\u5185\u7aa5\u955c\u5f15\u5bfc\u548c\u5de5\u5177\u64cd\u4f5c\uff0c\u4f46\u4ecd\u9762\u4e34\u52a8\u4f5c\u5bf9\u9f50\u3001\u7a0b\u5e8f\u610f\u8bc6\u3001\u4fe1\u606f\u4ea4\u6362\u548c\u6280\u80fd\u83b7\u53d6\u7b49\u6311\u6218\u3002", "conclusion": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u5f53\u524d\u8d8b\u52bf\u548c\u5173\u952e\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u9ad8\u624b\u672f\u4e2d\u4eba\u673a\u534f\u4f5c\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.11075", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2507.11075", "abs": "https://arxiv.org/abs/2507.11075", "authors": ["Chang Peng", "Yifei Zhou", "Huifeng Xi", "Shiqing Huang", "Chuangye Chen", "Jianming Yang", "Bao Yang", "Zhenyu Jiang"], "title": "Joint angle model based learning to refine kinematic human pose estimation", "comment": null, "summary": "Marker-free human pose estimation (HPE) has found increasing applications in\nvarious fields. Current HPE suffers from occasional errors in keypoint\nrecognition and random fluctuation in keypoint trajectories when analyzing\nkinematic human poses. The performance of existing deep learning-based models\nfor HPE refinement is considerably limited by inaccurate training datasets in\nwhich the keypoints are manually annotated. This paper proposed a novel method\nto overcome the difficulty through joint angle-based modeling. The key\ntechniques include: (i) A joint angle-based model of human pose, which is\nrobust to describe kinematic human poses; (ii) Approximating temporal variation\nof joint angles through high order Fourier series to get reliable \"ground\ntruth\"; (iii) A bidirectional recurrent network is designed as a\npost-processing module to refine the estimation of well-established HRNet.\nTrained with the high-quality dataset constructed using our method, the network\ndemonstrates outstanding performance to correct wrongly recognized joints and\nsmooth their spatiotemporal trajectories. Tests show that joint angle-based\nrefinement (JAR) outperforms the state-of-the-art HPE refinement network in\nchallenging cases like figure skating and breaking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5173\u8282\u89d2\u5ea6\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u65e0\u6807\u8bb0\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff08HPE\uff09\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u53cc\u5411\u5faa\u73af\u7f51\u7edc\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709HPE\u65b9\u6cd5\u5728\u5173\u952e\u70b9\u8bc6\u522b\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u4e0a\u5b58\u5728\u8bef\u5dee\uff0c\u4e14\u8bad\u7ec3\u6570\u636e\u96c6\u6807\u6ce8\u4e0d\u51c6\u786e\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u5173\u8282\u89d2\u5ea6\u7684\u4eba\u4f53\u59ff\u6001\u6a21\u578b\uff1b2. \u7528\u9ad8\u9636\u5085\u91cc\u53f6\u7ea7\u6570\u8fd1\u4f3c\u5173\u8282\u89d2\u5ea6\u53d8\u5316\u4ee5\u83b7\u53d6\u53ef\u9760\u201c\u771f\u503c\u201d\uff1b3. \u8bbe\u8ba1\u53cc\u5411\u5faa\u73af\u7f51\u7edc\u4f5c\u4e3a\u540e\u5904\u7406\u6a21\u5757\u3002", "result": "\u5728\u82b1\u6837\u6ed1\u51b0\u548c\u8857\u821e\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0cJAR\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709HPE\u4f18\u5316\u7f51\u7edc\u3002", "conclusion": "\u57fa\u4e8e\u5173\u8282\u89d2\u5ea6\u7684\u5efa\u6a21\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u8fdbHPE\u7684\u51c6\u786e\u6027\u548c\u8f68\u8ff9\u5e73\u6ed1\u6027\u3002"}}
{"id": "2507.10820", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10820", "abs": "https://arxiv.org/abs/2507.10820", "authors": ["Robert M\u00fcller"], "title": "Semantic Context for Tool Orchestration", "comment": "Workshop on Computer Use Agents @ ICML2025", "summary": "This paper demonstrates that Semantic Context (SC), leveraging descriptive\ntool information, is a foundational component for robust tool orchestration.\nOur contributions are threefold. First, we provide a theoretical foundation\nusing contextual bandits, introducing SC-LinUCB and proving it achieves lower\nregret and adapts favourably in dynamic action spaces. Second, we provide\nparallel empirical validation with Large Language Models, showing that SC is\ncritical for successful in-context learning in both static (efficient learning)\nand non-stationary (robust adaptation) settings. Third, we propose the FiReAct\npipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based\nretrieval enables an LLM to effectively orchestrate over a large action space.\nThese findings provide a comprehensive guide to building more sample-efficient,\nadaptive, and scalable orchestration agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bed\u4e49\u4e0a\u4e0b\u6587\uff08SC\uff09\u662f\u5de5\u5177\u7f16\u6392\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u7406\u8bba\u3001\u5b9e\u9a8c\u548c\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u8bed\u4e49\u4e0a\u4e0b\u6587\u5728\u5de5\u5177\u7f16\u6392\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4ee5\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3001\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "1. \u7406\u8bba\uff1a\u4f7f\u7528\u4e0a\u4e0b\u6587\u8001\u864e\u673a\u63d0\u51faSC-LinUCB\u7b97\u6cd5\uff1b2. \u5b9e\u9a8c\uff1a\u9a8c\u8bc1SC\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u91cd\u8981\u6027\uff1b3. \u5e94\u7528\uff1a\u63d0\u51faFiReAct\u6d41\u7a0b\uff0c\u57fa\u4e8eSC\u7684\u68c0\u7d22\u652f\u6301\u5927\u89c4\u6a21\u5de5\u5177\u7f16\u6392\u3002", "result": "SC-LinUCB\u964d\u4f4e\u9057\u61be\u503c\uff0cSC\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b66\u4e60\u6548\u7387\u548c\u9002\u5e94\u6027\uff0cFiReAct\u572810,000+\u5de5\u5177\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8bed\u4e49\u4e0a\u4e0b\u6587\u662f\u6784\u5efa\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u548c\u53ef\u6269\u5c55\u7f16\u6392\u4ee3\u7406\u7684\u5173\u952e\u3002"}}
{"id": "2507.11464", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11464", "abs": "https://arxiv.org/abs/2507.11464", "authors": ["Ajay Shankar", "Keisuke Okumura", "Amanda Prorok"], "title": "LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control", "comment": "9 pages; under review for IEEE Robotics & Automation - Letters (RA-L)", "summary": "We propose a multi-robot control paradigm to solve point-to-point navigation\ntasks for a team of holonomic robots with access to the full environment\ninformation. The framework invokes two processes asynchronously at high\nfrequency: (i) a centralized, discrete, and full-horizon planner for computing\ncollision- and deadlock-free paths rapidly, leveraging recent advances in\nmulti-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal\ntrajectory controllers that ensure all robots independently follow their\nassigned paths reliably. This hierarchical shift in planning representation\nfrom (i) discrete and coupled to (ii) continuous and decoupled domains enables\nthe framework to maintain long-term scalable motion synthesis. As an\ninstantiation of this idea, we present LF, which combines a fast\nstate-of-the-art MAPF solver (LaCAM), and a robust feedback control stack\n(Freyja) for executing agile robot maneuvers. LF provides a robust and\nversatile mechanism for lifelong multi-robot navigation even under asynchronous\nand partial goal updates, and adapts to dynamic workspaces simply by quick\nreplanning. We present various multirotor and ground robot demonstrations,\nincluding the deployment of 15 real multirotors with random, consecutive target\nupdates while a person walks through the operational workspace.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u8def\u5f84\u89c4\u5212\u548c\u5206\u6563\u5f0f\u8f68\u8ff9\u63a7\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u70b9\u5bf9\u70b9\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u5df2\u77e5\u73af\u5883\u4e2d\u7684\u70b9\u5bf9\u70b9\u5bfc\u822a\u95ee\u9898\uff0c\u786e\u4fdd\u65e0\u78b0\u649e\u3001\u65e0\u6b7b\u9501\uff0c\u5e76\u9002\u5e94\u52a8\u6001\u73af\u5883\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff1a1) \u96c6\u4e2d\u5f0f\u79bb\u6563\u8def\u5f84\u89c4\u5212\uff08\u57fa\u4e8eMAPF\uff09\uff1b2) \u5206\u6563\u5f0f\u8fde\u7eed\u8f68\u8ff9\u63a7\u5236\u3002", "result": "\u5c55\u793a\u4e86LF\u6846\u67b6\u572815\u4e2a\u771f\u5b9e\u591a\u65cb\u7ffc\u673a\u5668\u4eba\u4e2d\u7684\u6210\u529f\u5e94\u7528\uff0c\u652f\u6301\u52a8\u6001\u76ee\u6807\u66f4\u65b0\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u3002", "conclusion": "LF\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u573a\u666f\u3002"}}
{"id": "2507.11077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11077", "abs": "https://arxiv.org/abs/2507.11077", "authors": ["Weizhao Ma", "Dong Zhou", "Yuhui Hu", "Zipeng He"], "title": "GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft", "comment": null, "summary": "Monocular pose estimation of non-cooperative spacecraft is significant for\non-orbit service (OOS) tasks, such as satellite maintenance, space debris\nremoval, and station assembly. Considering the high demands on pose estimation\naccuracy, mainstream monocular pose estimation methods typically consist of\nkeypoint detectors and PnP solver. However, current keypoint detectors remain\nvulnerable to structural symmetry and partial occlusion of non-cooperative\nspacecraft. To this end, we propose a graph-based keypoints network for the\nmonocular pose estimation of non-cooperative spacecraft, GKNet, which leverages\nthe geometric constraint of keypoints graph. In order to better validate\nkeypoint detectors, we present a moderate-scale dataset for the spacecraft\nkeypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000\nsimulated images, and corresponding high-precise keypoint annotations.\nExtensive experiments and an ablation study have demonstrated the high accuracy\nand effectiveness of our GKNet, compared to the state-of-the-art spacecraft\nkeypoint detectors. The code for GKNet and the SKD dataset is available at\nhttps://github.com/Dongzhou-1996/GKNet.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5173\u952e\u70b9\u7f51\u7edc\uff08GKNet\uff09\uff0c\u7528\u4e8e\u975e\u5408\u4f5c\u822a\u5929\u5668\u7684\u5355\u76ee\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u53d1\u5e03\u4e86SKD\u6570\u636e\u96c6\u7528\u4e8e\u9a8c\u8bc1\u3002", "motivation": "\u975e\u5408\u4f5c\u822a\u5929\u5668\u7684\u5355\u76ee\u59ff\u6001\u4f30\u8ba1\u5728\u5728\u8f68\u670d\u52a1\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5173\u952e\u70b9\u68c0\u6d4b\u5668\u5bf9\u7ed3\u6784\u5bf9\u79f0\u6027\u548c\u90e8\u5206\u906e\u6321\u654f\u611f\u3002", "method": "\u63d0\u51faGKNet\uff0c\u5229\u7528\u5173\u952e\u70b9\u56fe\u7684\u51e0\u4f55\u7ea6\u675f\uff0c\u5e76\u6784\u5efaSKD\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGKNet\u5728\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GKNet\u548cSKD\u6570\u636e\u96c6\u4e3a\u975e\u5408\u4f5c\u822a\u5929\u5668\u7684\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10834", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10834", "abs": "https://arxiv.org/abs/2507.10834", "authors": ["Guokai Li", "Pin Gao", "Stefanus Jasin", "Zizhuo Wang"], "title": "From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems", "comment": "Conference version. The journal version will be updated soon", "summary": "Assortment optimization involves selecting a subset of substitutable products\n(subject to certain constraints) to maximize the expected revenue. It is a\nclassic problem in revenue management and finds applications across various\nindustries. However, the problem is usually NP-hard due to its combinatorial\nand non-linear nature. In this work, we explore how graph concolutional\nnetworks (GCNs) can be leveraged to efficiently solve constrained assortment\noptimization under the mixed multinomial logit choice model. We first develop a\ngraph representation of the assortment problem, then train a GCN to learn the\npatterns of optimal assortments, and lastly propose two inference policies\nbased on the GCN's output. Due to the GCN's inherent ability to generalize\nacross inputs of varying sizes, we can use a GCN trained on small-scale\ninstances to facilitate large-scale instances. Extensive numerical experiments\ndemonstrate that given a GCN trained on small-scale instances (e.g., with 20\nproducts), the proposed policies can achieve superior performance (90%+\noptimality) on large-scale instances (with up to 2,000 products) within\nseconds, which outperform existing heuristic policies in both performance and\nefficiency. Furthermore, we extend our framework to a model-free setting where\nthe underlying choice model is unknown but transaction data is available. We\nalso conduct numerical experiments to demonstrate the effectiveness and\nefficiency of our proposed policies in this setting.", "AI": {"tldr": "\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u89e3\u51b3\u7ea6\u675f\u6027\u4ea7\u54c1\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5728\u5c0f\u89c4\u6a21\u8bad\u7ec3\u540e\u80fd\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u5b9e\u4f8b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "motivation": "\u4ea7\u54c1\u7ec4\u5408\u4f18\u5316\u662f\u7ecf\u5178\u4f46NP\u96be\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u9700\u63a2\u7d22\u65b0\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4ea7\u54c1\u7ec4\u5408\u7684\u56fe\u8868\u793a\uff0c\u8bad\u7ec3GCN\u5b66\u4e60\u6700\u4f18\u7ec4\u5408\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e24\u79cd\u63a8\u65ad\u7b56\u7565\u3002", "result": "GCN\u5728\u5c0f\u89c4\u6a21\u8bad\u7ec3\u540e\uff0c\u80fd\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e2d\u5b9e\u73b090%\u4ee5\u4e0a\u7684\u6700\u4f18\u6027\uff0c\u901f\u5ea6\u5feb\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GCN\u6846\u67b6\u5728\u5df2\u77e5\u548c\u672a\u77e5\u9009\u62e9\u6a21\u578b\u4e0b\u5747\u9ad8\u6548\uff0c\u6269\u5c55\u6027\u5f3a\u3002"}}
{"id": "2507.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11498", "abs": "https://arxiv.org/abs/2507.11498", "authors": ["Asad Ali Shahid", "Francesco Braghin", "Loris Roveda"], "title": "Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming", "comment": null, "summary": "Humanoid robots have seen remarkable advances in dexterity, balance, and\nlocomotion, yet their role in expressive domains, such as music performance,\nremains largely unexplored. Musical tasks, like drumming, present unique\nchallenges, including split-second timing, rapid contacts, and multi-limb\ncoordination over pieces lasting minutes. In this paper, we introduce Robot\nDrummer, a humanoid system capable of expressive, high-precision drumming\nacross a diverse repertoire of songs. We formulate humanoid drumming as\nsequential fulfillment of timed-contacts and transform drum scores in to a\nRhythmic Contact Chain. To handle the long-horizon nature of musical\nperformance, we decompose each piece into fixed-length segments and train a\nsingle policy across all segments in parallel using reinforcement learning.\nThrough extensive experiments on over thirty popular rock, metal, and jazz\ntracks, our results demonstrate that Robot Drummer consistently achieves high\nF1 scores. The learned behaviors exhibit emergent human-like drumming\nstrategies, such as cross-arm strikes, and adaptive sticks assignments,\ndemonstrating the potential of reinforcement learning to bring humanoid robots\ninto the domain of creative musical performance. Project page:\n\\href{https://robot-drummer.github.io}{robot-drummer.github.io}", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aRobot Drummer\u7684\u4eba\u5f62\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u8868\u73b0\u529b\u7684\u9f13\u4e50\u6f14\u594f\u3002", "motivation": "\u63a2\u7d22\u4eba\u5f62\u673a\u5668\u4eba\u5728\u97f3\u4e50\u8868\u6f14\u7b49\u8868\u8fbe\u6027\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u9f13\u4e50\u6f14\u594f\u4e2d\u7684\u5feb\u901f\u53cd\u5e94\u548c\u591a\u80a2\u534f\u8c03\u6311\u6218\u3002", "method": "\u5c06\u9f13\u4e50\u6f14\u594f\u5efa\u6a21\u4e3a\u5b9a\u65f6\u63a5\u89e6\u5e8f\u5217\uff0c\u5c06\u9f13\u8c31\u8f6c\u5316\u4e3a\u8282\u594f\u63a5\u89e6\u94fe\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u5206\u6bb5\u7b56\u7565\u3002", "result": "\u5728\u4e09\u5341\u591a\u9996\u6d41\u884c\u6447\u6eda\u3001\u91d1\u5c5e\u548c\u7235\u58eb\u4e50\u66f2\u4e2d\uff0cRobot Drummer\u8868\u73b0\u51fa\u9ad8F1\u5206\u6570\uff0c\u5e76\u5c55\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9f13\u4e50\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u63a8\u52a8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u521b\u9020\u6027\u97f3\u4e50\u8868\u6f14\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2507.11081", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.11081", "abs": "https://arxiv.org/abs/2507.11081", "authors": ["Chang Peng", "Bao Yang", "Meiqi Li", "Ge Zhang", "Hui Sun", "Zhenyu Jiang"], "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification", "comment": null, "summary": "Ground penetrating radar (GPR) has become a rapid and non-destructive\nsolution for road subsurface distress (RSD) detection. However, RSD recognition\nfrom GPR images is labor-intensive and heavily relies on inspectors' expertise.\nDeep learning offers the possibility for automatic RSD recognition, but its\ncurrent performance is limited by two factors: Scarcity of high-quality dataset\nfor network training and insufficient capability of network to distinguish RSD.\nIn this study, a rigorously validated 3D GPR dataset containing 2134 samples of\ndiverse types was constructed through field scanning. Based on the finding that\nthe YOLO model trained with one of the three scans of GPR images exhibits\nvarying sensitivity to specific type of RSD, we proposed a novel\ncross-verification strategy with outstanding accuracy in RSD recognition,\nachieving recall over 98.6% in field tests. The approach, integrated into an\nonline RSD detection system, can reduce the labor of inspection by around 90%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eYOLO\u6a21\u578b\u548c\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\u7684\u81ea\u52a8\u9053\u8def\u5730\u4e0b\u75c5\u5bb3\uff08RSD\uff09\u8bc6\u522b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "motivation": "GPR\u56fe\u50cf\u4e2d\u7684RSD\u8bc6\u522b\u4f9d\u8d56\u4eba\u5de5\u4e14\u6548\u7387\u4f4e\uff0c\u6df1\u5ea6\u5b66\u4e60\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u8d28\u91cf\u548c\u7f51\u7edc\u533a\u5206\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u76843D GPR\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8eYOLO\u6a21\u578b\u7684\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\u3002", "result": "\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\u53ec\u56de\u7387\u8d85\u8fc798.6%\uff0c\u68c0\u6d4b\u7cfb\u7edf\u53ef\u51cf\u5c11\u7ea690%\u7684\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aRSD\u81ea\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.10843", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10843", "abs": "https://arxiv.org/abs/2507.10843", "authors": ["Motoki Omura", "Yusuke Mukuta", "Kazuki Ota", "Takayuki Osa", "Tatsuya Harada"], "title": "Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps", "comment": "Accepted at RLC 2025", "summary": "Offline reinforcement learning (RL) aims to learn an optimal policy from a\nstatic dataset, making it particularly valuable in scenarios where data\ncollection is costly, such as robotics. A major challenge in offline RL is\ndistributional shift, where the learned policy deviates from the dataset\ndistribution, potentially leading to unreliable out-of-distribution actions. To\nmitigate this issue, regularization techniques have been employed. While many\nexisting methods utilize density ratio-based measures, such as the\n$f$-divergence, for regularization, we propose an approach that utilizes the\nWasserstein distance, which is robust to out-of-distribution data and captures\nthe similarity between actions. Our method employs input-convex neural networks\n(ICNNs) to model optimal transport maps, enabling the computation of the\nWasserstein distance in a discriminator-free manner, thereby avoiding\nadversarial training and ensuring stable learning. Our approach demonstrates\ncomparable or superior performance to widely used existing methods on the D4RL\nbenchmark dataset. The code is available at\nhttps://github.com/motokiomura/Q-DOT .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein\u8ddd\u79bb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u5bf9\u6297\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5206\u5e03\u504f\u79fb\u662f\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8e\u5bc6\u5ea6\u6bd4\uff0c\u672c\u6587\u63d0\u51fa\u66f4\u7a33\u5065\u7684Wasserstein\u8ddd\u79bb\u3002", "method": "\u4f7f\u7528\u8f93\u5165\u51f8\u795e\u7ecf\u7f51\u7edc\uff08ICNNs\uff09\u5efa\u6a21\u6700\u4f18\u4f20\u8f93\u6620\u5c04\uff0c\u8ba1\u7b97Wasserstein\u8ddd\u79bb\uff0c\u65e0\u9700\u5bf9\u6297\u8bad\u7ec3\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "Wasserstein\u8ddd\u79bb\u548cICNNs\u7684\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.11525", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11525", "abs": "https://arxiv.org/abs/2507.11525", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "LLM-based ambiguity detection in natural language instructions for collaborative surgical robots", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Ambiguity in natural language instructions poses significant risks in\nsafety-critical human-robot interaction, particularly in domains such as\nsurgery. To address this, we propose a framework that uses Large Language\nModels (LLMs) for ambiguity detection specifically designed for collaborative\nsurgical scenarios. Our method employs an ensemble of LLM evaluators, each\nconfigured with distinct prompting techniques to identify linguistic,\ncontextual, procedural, and critical ambiguities. A chain-of-thought evaluator\nis included to systematically analyze instruction structure for potential\nissues. Individual evaluator assessments are synthesized through conformal\nprediction, which yields non-conformity scores based on comparison to a labeled\ncalibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed\nclassification accuracy exceeding 60% in differentiating ambiguous from\nunambiguous surgical instructions. Our approach improves the safety and\nreliability of human-robot collaboration in surgery by offering a mechanism to\nidentify potentially ambiguous instructions before robot action.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u624b\u672f\u573a\u666f\u4e2d\u7684\u6307\u4ee4\u6b67\u4e49\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u63d0\u793a\u6280\u672f\u548c\u5171\u5f62\u9884\u6d4b\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6b67\u4e49\u5728\u5b89\u5168\u5173\u952e\u7684\u4eba\u673a\u4ea4\u4e92\uff08\u5982\u624b\u672f\uff09\u4e2d\u5b58\u5728\u98ce\u9669\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u89e3\u51b3\u8fd9\u4e9b\u6b67\u4e49\u3002", "method": "\u4f7f\u7528\u96c6\u6210LLM\u8bc4\u4f30\u5668\uff0c\u7ed3\u5408\u4e0d\u540c\u63d0\u793a\u6280\u672f\u68c0\u6d4b\u591a\u79cd\u6b67\u4e49\u7c7b\u578b\uff0c\u5e76\u901a\u8fc7\u5171\u5f62\u9884\u6d4b\u5408\u6210\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5728Llama 3.2 11B\u548cGemma 3 12B\u4e0a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc760%\uff0c\u80fd\u591f\u533a\u5206\u624b\u672f\u6307\u4ee4\u7684\u6b67\u4e49\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u624b\u672f\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u80fd\u591f\u5728\u673a\u5668\u4eba\u884c\u52a8\u524d\u8bc6\u522b\u6f5c\u5728\u6b67\u4e49\u6307\u4ee4\u3002"}}
{"id": "2507.11085", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11085", "abs": "https://arxiv.org/abs/2507.11085", "authors": ["Tianchi Xu"], "title": "Atmos-Bench: 3D Atmospheric Structures for Climate Insight", "comment": null, "summary": "Atmospheric structure, represented by backscatter coefficients (BC) recovered\nfrom satellite LiDAR attenuated backscatter (ATB), provides a volumetric view\nof clouds, aerosols, and molecules, playing a critical role in human\nactivities, climate understanding, and extreme weather forecasting. Existing\nmethods often rely on auxiliary inputs and simplified physics-based\napproximations, and lack a standardized 3D benchmark for fair evaluation.\nHowever, such approaches may introduce additional uncertainties and\ninsufficiently capture realistic radiative transfer and atmospheric\nscattering-absorption effects. To bridge these gaps, we present Atmos-Bench:\nthe first 3D atmospheric benchmark, along with a novel FourCastX:\nFrequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)\ngenerates 921,600 image slices from 3D scattering volumes simulated at 532 nm\nand 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean\ntime steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC\nphysical constraints into the model architecture, promoting energy consistency\nduring restoration; (c) achieves consistent improvements on the Atmos-Bench\ndataset across both 355 nm and 532 nm bands, outperforming state-of-the-art\nbaseline models without relying on auxiliary inputs. Atmos-Bench establishes a\nnew standard for satellite-based 3D atmospheric structure recovery and paves\nthe way for deeper climate insight.", "AI": {"tldr": "Atmos-Bench\u662f\u9996\u4e2a3D\u5927\u6c14\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7ed3\u5408FourCastX\u7f51\u7edc\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u548c\u9ad8\u8d28\u91cf\u6a21\u62df\u6570\u636e\u63d0\u5347\u5927\u6c14\u7ed3\u6784\u6062\u590d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8f85\u52a9\u8f93\u5165\u548c\u7b80\u5316\u7269\u7406\u8fd1\u4f3c\uff0c\u7f3a\u4e4f\u6807\u51c6\u53163D\u57fa\u51c6\uff0c\u5bfc\u81f4\u4e0d\u786e\u5b9a\u6027\u548c\u5bf9\u771f\u5b9e\u8f90\u5c04\u4f20\u8f93\u7684\u6355\u6349\u4e0d\u8db3\u3002", "method": "\u63d0\u51faAtmos-Bench\u57fa\u51c6\u548cFourCastX\u7f51\u7edc\uff0c\u901a\u8fc7\u6a21\u62df3D\u6563\u5c04\u4f53\u79ef\u548c\u5d4c\u5165\u7269\u7406\u7ea6\u675f\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u6062\u590d\u3002", "result": "\u5728355 nm\u548c532 nm\u6ce2\u6bb5\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u65e0\u9700\u8f85\u52a9\u8f93\u5165\u3002", "conclusion": "Atmos-Bench\u4e3a\u536b\u661f3D\u5927\u6c14\u7ed3\u6784\u6062\u590d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u63a8\u52a8\u6c14\u5019\u7814\u7a76\u6df1\u5165\u53d1\u5c55\u3002"}}
{"id": "2507.10861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10861", "abs": "https://arxiv.org/abs/2507.10861", "authors": ["Edoardo Pinzuti", "Oliver T\u00fcscher", "Andr\u00e9 Ferreira Castro"], "title": "Visually grounded emotion regulation via diffusion models and user-driven reappraisal", "comment": null, "summary": "Cognitive reappraisal is a key strategy in emotion regulation, involving\nreinterpretation of emotionally charged stimuli to alter affective responses.\nDespite its central role in clinical and cognitive science, real-world\nreappraisal interventions remain cognitively demanding, abstract, and primarily\nverbal. This reliance on higher-order cognitive and linguistic processes is\noften impaired in individuals with trauma or depression, limiting the\neffectiveness of standard approaches. Here, we propose a novel, visually based\naugmentation of cognitive reappraisal by integrating large-scale text-to-image\ndiffusion models into the emotional regulation process. Specifically, we\nintroduce a system in which users reinterpret emotionally negative images via\nspoken reappraisals, which are transformed into supportive, emotionally\ncongruent visualizations using stable diffusion models with a fine-tuned\nIP-adapter. This generative transformation visually instantiates users'\nreappraisals while maintaining structural similarity to the original stimuli,\nexternalizing and reinforcing regulatory intent. To test this approach, we\nconducted a within-subject experiment (N = 20) using a modified cognitive\nemotion regulation (CER) task. Participants reappraised or described aversive\nimages from the International Affective Picture System (IAPS), with or without\nAI-generated visual feedback. Results show that AI-assisted reappraisal\nsignificantly reduced negative affect compared to both non-AI and control\nconditions. Further analyses reveal that sentiment alignment between\nparticipant reappraisals and generated images correlates with affective relief,\nsuggesting that multimodal coherence enhances regulatory efficacy. These\nfindings demonstrate that generative visual input can support cogitive\nreappraisal and open new directions at the intersection of generative AI,\naffective computing, and therapeutic technology.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u8ba4\u77e5\u91cd\u8bc4\u589e\u5f3a\u65b9\u6cd5\uff0c\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u652f\u6301\u6027\u89c6\u89c9\u53cd\u9988\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8d1f\u9762\u60c5\u7eea\u3002", "motivation": "\u4f20\u7edf\u7684\u8ba4\u77e5\u91cd\u8bc4\u4f9d\u8d56\u9ad8\u9636\u8ba4\u77e5\u548c\u8bed\u8a00\u8fc7\u7a0b\uff0c\u5bf9\u521b\u4f24\u6216\u6291\u90c1\u60a3\u8005\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u89c2\u7684\u5e72\u9884\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u548c\u5fae\u8c03\u7684IP\u9002\u914d\u5668\uff0c\u5c06\u7528\u6237\u7684\u53e3\u5934\u91cd\u8bc4\u8f6c\u5316\u4e3a\u60c5\u611f\u4e00\u81f4\u7684\u89c6\u89c9\u53cd\u9988\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u5bf9\u6bd4\u4e86AI\u8f85\u52a9\u4e0e\u975eAI\u6761\u4ef6\u4e0b\u7684\u6548\u679c\u3002", "result": "AI\u8f85\u52a9\u7684\u91cd\u8bc4\u663e\u8457\u964d\u4f4e\u4e86\u8d1f\u9762\u60c5\u7eea\uff0c\u4e14\u60c5\u611f\u4e00\u81f4\u6027\u9ad8\u7684\u89c6\u89c9\u53cd\u9988\u4e0e\u60c5\u7eea\u7f13\u89e3\u76f8\u5173\u3002", "conclusion": "\u751f\u6210\u5f0f\u89c6\u89c9\u8f93\u5165\u80fd\u6709\u6548\u652f\u6301\u8ba4\u77e5\u91cd\u8bc4\uff0c\u4e3a\u751f\u6210\u5f0fAI\u3001\u60c5\u611f\u8ba1\u7b97\u548c\u6cbb\u7597\u6280\u672f\u7684\u7ed3\u5408\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.11099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11099", "abs": "https://arxiv.org/abs/2507.11099", "authors": ["Qiyang Wan", "Chengzhi Gao", "Ruiping Wang", "Xilin Chen"], "title": "A Survey on Interpretability in Visual Recognition", "comment": "20 pages, 7 figures, 2 tables. Under review", "summary": "In recent years, visual recognition methods have advanced significantly,\nfinding applications across diverse fields. While researchers seek to\nunderstand the mechanisms behind the success of these models, there is also a\ngrowing impetus to deploy them in critical areas like autonomous driving and\nmedical diagnostics to better diagnose failures, which promotes the development\nof interpretability research. This paper systematically reviews existing\nresearch on the interpretability of visual recognition models and proposes a\ntaxonomy of methods from a human-centered perspective. The proposed taxonomy\ncategorizes interpretable recognition methods based on Intent, Object,\nPresentation, and Methodology, thereby establishing a systematic and coherent\nset of grouping criteria for these XAI methods. Additionally, we summarize the\nrequirements for evaluation metrics and explore new opportunities enabled by\nrecent technologies, such as large multimodal models. We aim to organize\nexisting research in this domain and inspire future investigations into the\ninterpretability of visual recognition models.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u6307\u6807\u548c\u65b0\u6280\u672f\u7684\u673a\u9047\u3002", "motivation": "\u968f\u7740\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u5728\u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7406\u89e3\u5176\u673a\u5236\u548c\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u73b0\u6709\u7814\u7a76\uff0c\u63d0\u51fa\u57fa\u4e8e\u610f\u56fe\u3001\u5bf9\u8c61\u3001\u5448\u73b0\u548c\u65b9\u6cd5\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u5efa\u7acb\u4e86\u7cfb\u7edf\u7684\u5206\u7c7b\u6807\u51c6\uff0c\u603b\u7ed3\u4e86\u8bc4\u4f30\u6307\u6807\u9700\u6c42\uff0c\u5e76\u63a2\u7d22\u4e86\u65b0\u6280\u672f\u5e26\u6765\u7684\u673a\u9047\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u7ec4\u7ec7\u73b0\u6709\u7814\u7a76\u5e76\u542f\u53d1\u672a\u6765\u5bf9\u89c6\u89c9\u8bc6\u522b\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u63a2\u7d22\u3002"}}
{"id": "2507.10871", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.10871", "abs": "https://arxiv.org/abs/2507.10871", "authors": ["Tsung Yeh Hsieh", "Yongjie Jessica Zhang"], "title": "GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport", "comment": null, "summary": "Neurons exhibit intricate geometries within their neurite networks, which\nplay a crucial role in processes such as signaling and nutrient transport.\nAccurate simulation of material transport in the networks is essential for\nunderstanding these biological phenomena but poses significant computational\nchallenges because of the complex tree-like structures involved. Traditional\napproaches are time-intensive and resource-demanding, yet the inherent\nproperties of neuron trees, which consists primarily of pipes with steady-state\nparabolic velocity profiles and bifurcations, provide opportunities for\ncomputational optimization. To address these challenges, we propose a\nGraph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is\nspecifically designed to streamline the simulation of material transport in\nneural trees. GALDS employs a graph autoencoder to encode latent\nrepresentations of the network's geometry, velocity fields, and concentration\nprofiles. These latent space representations are then assembled into a global\ngraph, which is subsequently used to predict system dynamics in the latent\nspace via a trained graph latent space system dynamic model, inspired by the\nNeural Ordinary Differential Equations (Neural ODEs) concept. The integration\nof an autoencoder allows for the use of smaller graph neural network models\nwith reduced training data requirements. Furthermore, the Neural ODE component\neffectively mitigates the issue of error accumulation commonly encountered in\nrecurrent neural networks. The effectiveness of the GALDS model is demonstrated\nthrough results on eight unseen geometries and four abnormal transport\nexamples, where our approach achieves mean relative error of 3% with maximum\nrelative error <8% and demonstrates a 10-fold speed improvement compared to\nprevious surrogate model approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u81ea\u52a8\u7f16\u7801\u5668\u7684\u6f5c\u5728\u52a8\u529b\u5b66\u66ff\u4ee3\u6a21\u578b\uff08GALDS\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u6a21\u62df\u795e\u7ecf\u6811\u4e2d\u7684\u7269\u8d28\u8fd0\u8f93\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u795e\u7ecf\u5143\u7684\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u5bf9\u7269\u8d28\u8fd0\u8f93\u6a21\u62df\u63d0\u51fa\u4e86\u8ba1\u7b97\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u8017\u65f6\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u4f7f\u7528\u56fe\u81ea\u52a8\u7f16\u7801\u5668\u7f16\u7801\u7f51\u7edc\u51e0\u4f55\u3001\u901f\u5ea6\u573a\u548c\u6d53\u5ea6\u5206\u5e03\uff0c\u7ed3\u5408\u56fe\u6f5c\u5728\u7a7a\u95f4\u7cfb\u7edf\u52a8\u6001\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u3002", "result": "\u57288\u79cd\u672a\u89c1\u51e0\u4f55\u548c4\u79cd\u5f02\u5e38\u8fd0\u8f93\u6848\u4f8b\u4e2d\uff0c\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4e3a3%\uff0c\u6700\u5927\u8bef\u5dee<8%\uff0c\u901f\u5ea6\u63d0\u534710\u500d\u3002", "conclusion": "GALDS\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u795e\u7ecf\u6811\u6a21\u62df\u3002"}}
{"id": "2507.11102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11102", "abs": "https://arxiv.org/abs/2507.11102", "authors": ["Jie Yang", "Wang Zeng", "Sheng Jin", "Lumin Xu", "Wentao Liu", "Chen Qian", "Zhen Li", "Ruimao Zhang"], "title": "KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model", "comment": "Extended Version of KptLLM. arXiv admin note: text overlap with\n  arXiv:2411.01846", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has revolutionized\nimage understanding by bridging textual and visual modalities. However, these\nmodels often struggle with capturing fine-grained semantic information, such as\nthe precise identification and analysis of object keypoints. Keypoints, as\nstructure-aware, pixel-level, and compact representations of objects,\nparticularly articulated ones, play a crucial role in applications such as\nfine-grained image analysis, object retrieval, and behavior recognition. In\nthis paper, we propose KptLLM++, a novel multimodal large language model that\nspecifically designed for generic keypoint comprehension through the\nintegration of diverse input modalities guided by user-defined instructions. By\nunifying keypoint detection across varied contexts, KptLLM++ establishes itself\nas an advanced interface, fostering more effective human-AI collaboration. The\nmodel is built upon a novel identify-then-detect paradigm, which first\ninterprets keypoint semantics and subsequently localizes their precise\npositions through a structured chain-of-thought reasoning mechanism. To push\nthe boundaries of performance, we have scaled up the training dataset to over\n500K samples, encompassing diverse objects, keypoint categories, image styles,\nand scenarios with complex occlusions. This extensive scaling enables KptLLM++\nto unlock its potential, achieving remarkable accuracy and generalization.\nComprehensive experiments on multiple keypoint detection benchmarks demonstrate\nits state-of-the-art performance, underscoring its potential as a unified\nsolution for fine-grained image understanding and its transformative\nimplications for human-AI interaction.", "AI": {"tldr": "KptLLM++\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u901a\u7528\u5173\u952e\u70b9\u7406\u89e3\uff0c\u901a\u8fc7\u7528\u6237\u6307\u4ee4\u6574\u5408\u591a\u79cd\u8f93\u5165\u6a21\u6001\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5173\u952e\u70b9\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\u4fe1\u606f\uff08\u5982\u5173\u952e\u70b9\uff09\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u800c\u5173\u952e\u70b9\u5bf9\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u6790\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u8bc6\u522b-\u68c0\u6d4b\u8303\u5f0f\uff0c\u5148\u89e3\u91ca\u5173\u952e\u70b9\u8bed\u4e49\uff0c\u518d\u901a\u8fc7\u94fe\u5f0f\u63a8\u7406\u673a\u5236\u5b9a\u4f4d\u5176\u7cbe\u786e\u4f4d\u7f6e\uff0c\u5e76\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u96c6\u81f350\u4e07\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u5173\u952e\u70b9\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u7684\u6f5c\u529b\u3002", "conclusion": "KptLLM++\u4e3a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u7406\u89e3\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10880", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10880", "abs": "https://arxiv.org/abs/2507.10880", "authors": ["Souvik Nath", "Sumit Wadhwa", "Luiz Perez"], "title": "Domain-Adaptive Small Language Models for Structured Tax Code Prediction", "comment": "10 pages, 3 figures", "summary": "Every day, multinational firms process thousands of transactions, each of\nwhich must adhere to tax regulations that vary by jurisdiction and are often\nnuanced. The determination of product and service tax codes, such as HSN or SAC\nis a major use case in Tax compliance. An accurate determination of such codes\nis imperative to avoid any tax penalties. This paper proposes a domain-adaptive\nsmall language model (SLM) with an encoder-decoder architecture for the\nenhanced prediction of product and service tax codes. In this approach, we\naddress the problem of predicting hierarchical tax code sequences using\nunstructured product and services data. We employ an SLM based upon\nencoder-decoder architecture as this enables sequential generation of tax codes\nto capture the hierarchical dependencies present within the tax codes. Our\nexperiments demonstrate that encoder-decoder SLMs can be successfully applied\nto the sequential prediction of structured tax codes, a domain that remains\ncomparatively unexplored in current NLP research. In this paper, we demonstrate\nthe superior performance of the domain-adaptive encoder-decoder SLMs over flat\nclassifiers when applied to the Harmonized System of Nomenclature (HSN), and\nachieve superior results compared to decoder-only and encoder-only\narchitectures for structured sequence generation tasks. This approach can also\nbe scaled to other government-mandated tax commodity codes, such as United\nNations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura\nComum do Mercosul (NCM).", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u9886\u57df\u81ea\u9002\u5e94\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u4ea7\u54c1\u548c\u670d\u52a1\u7684\u7a0e\u52a1\u4ee3\u7801\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u8de8\u56fd\u4f01\u4e1a\u6bcf\u5929\u5904\u7406\u5927\u91cf\u4ea4\u6613\uff0c\u9700\u9075\u5b88\u4e0d\u540c\u7a0e\u52a1\u7ba1\u8f96\u533a\u7684\u590d\u6742\u7a0e\u52a1\u89c4\u5b9a\uff0c\u51c6\u786e\u9884\u6d4b\u7a0e\u52a1\u4ee3\u7801\u4ee5\u907f\u514d\u7f5a\u6b3e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684SLM\uff0c\u901a\u8fc7\u975e\u7ed3\u6784\u5316\u6570\u636e\u9884\u6d4b\u5c42\u6b21\u5316\u7a0e\u52a1\u4ee3\u7801\u5e8f\u5217\uff0c\u6355\u6349\u4ee3\u7801\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u5c42\u6b21\u5316\u7a0e\u52a1\u4ee3\u7801\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6241\u5e73\u5206\u7c7b\u5668\u3001\u4ec5\u89e3\u7801\u5668\u548c\u4ec5\u7f16\u7801\u5668\u67b6\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u653f\u5e9c\u89c4\u5b9a\u7684\u7a0e\u52a1\u4ee3\u7801\uff0c\u5c55\u793a\u4e86SLM\u5728\u7a0e\u52a1\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.11287", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11287", "abs": "https://arxiv.org/abs/2507.11287", "authors": ["An-Lun Liu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers", "comment": "Accepted by ICCV 2025", "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u5bfc\u5411\u7684\u4eba\u4f53\u6293\u53d6\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u63a5\u89e6\u56fe\u63d0\u5347\u6293\u53d6\u8d28\u91cf\u4e0e\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6293\u53d6\u5408\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u573a\u666f\u548c\u4efb\u52a1\u7684\u8003\u8651\uff0c\u5bfc\u81f4\u6293\u53d6\u59ff\u52bf\u4e0e\u4efb\u52a1\u9700\u6c42\u4e0d\u5339\u914d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u751f\u6210\u4efb\u52a1\u611f\u77e5\u63a5\u89e6\u56fe\uff0c\u968f\u540e\u57fa\u4e8e\u8be5\u56fe\u5408\u6210\u4efb\u52a1\u5bfc\u5411\u7684\u6293\u53d6\u59ff\u52bf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6293\u53d6\u8d28\u91cf\u548c\u4efb\u52a1\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4efb\u52a1\u548c\u573a\u666f\u4fe1\u606f\u7684\u7efc\u5408\u5efa\u6a21\u5bf9\u63d0\u5347\u6293\u53d6\u5408\u6210\u7684\u6548\u679c\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.11116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11116", "abs": "https://arxiv.org/abs/2507.11116", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha", "Mostofa Kamal Nasir"], "title": "Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach", "comment": "This paper has been accepted at the IEEE QPAIN 2025. The final\n  version will be available in the IEEE Xplore Digital Library", "summary": "Jellyfish, a diverse group of gelatinous marine organisms, play a crucial\nrole in maintaining marine ecosystems but pose significant challenges for\nbiodiversity and conservation due to their rapid proliferation and ecological\nimpact. Accurate identification of jellyfish species is essential for\necological monitoring and management. In this study, we proposed a deep\nlearning framework for jellyfish species detection and classification using an\nunderwater image dataset. The framework integrates advanced feature extraction\ntechniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,\ncombined with seven traditional machine learning classifiers and three\nFeedforward Neural Network classifiers for precise species identification.\nAdditionally, we activated the softmax function to directly classify jellyfish\nspecies using the convolutional neural network models. The combination of the\nArtificial Neural Network with MobileNetV3 is our best-performing model,\nachieving an exceptional accuracy of 98%, significantly outperforming other\nfeature extractor-classifier combinations. This study demonstrates the efficacy\nof deep learning and hybrid frameworks in addressing biodiversity challenges\nand advancing species detection in marine environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u6bcd\u7269\u79cd\u68c0\u6d4b\u548c\u5206\u7c7b\uff0c\u7ed3\u5408\u591a\u79cd\u7279\u5f81\u63d0\u53d6\u6280\u672f\u548c\u5206\u7c7b\u5668\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe98%\u3002", "motivation": "\u6c34\u6bcd\u5728\u6d77\u6d0b\u751f\u6001\u7cfb\u7edf\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5176\u5feb\u901f\u589e\u6b96\u548c\u751f\u6001\u5f71\u54cd\u5bf9\u751f\u7269\u591a\u6837\u6027\u548c\u4fdd\u62a4\u6784\u6210\u6311\u6218\uff0c\u51c6\u786e\u8bc6\u522b\u6c34\u6bcd\u7269\u79cd\u5bf9\u751f\u6001\u76d1\u6d4b\u548c\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002", "method": "\u96c6\u6210\u4e86MobileNetV3\u3001ResNet50\u3001EfficientNetV2-B0\u548cVGG16\u7b49\u7279\u5f81\u63d0\u53d6\u6280\u672f\uff0c\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\uff0c\u5e76\u4f7f\u7528softmax\u51fd\u6570\u76f4\u63a5\u5206\u7c7b\u3002", "result": "\u6700\u4f73\u6a21\u578b\uff08MobileNetV3\u4e0e\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\uff09\u8fbe\u523098%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7ec4\u5408\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e0e\u6df7\u5408\u6846\u67b6\u5728\u89e3\u51b3\u751f\u7269\u591a\u6837\u6027\u6311\u6218\u548c\u63a8\u8fdb\u6d77\u6d0b\u7269\u79cd\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.10884", "categories": ["cs.LG", "math.DS", "68T07, 68T05, 70G60"], "pdf": "https://arxiv.org/pdf/2507.10884", "abs": "https://arxiv.org/abs/2507.10884", "authors": ["Hyunwoo Cho", "Hyeontae Jo", "Hyung Ju Hwang"], "title": "Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model", "comment": null, "summary": "System inference for nonlinear dynamic models, represented by ordinary\ndifferential equations (ODEs), remains a significant challenge in many fields,\nparticularly when the data are noisy, sparse, or partially observable. In this\npaper, we propose a Simulation-based Generative Model for Imperfect Data\n(SiGMoID) that enables precise and robust inference for dynamic systems. The\nproposed approach integrates two key methods: (1) physics-informed neural\nnetworks with hyper-networks that constructs an ODE solver, and (2) Wasserstein\ngenerative adversarial networks that estimates ODE parameters by effectively\ncapturing noisy data distributions. We demonstrate that SiGMoID quantifies data\nnoise, estimates system parameters, and infers unobserved system components.\nIts effectiveness is validated validated through realistic experimental\nexamples, showcasing its broad applicability in various domains, from\nscientific research to engineered systems, and enabling the discovery of full\nsystem dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u7684\u751f\u6210\u6a21\u578b\uff08SiGMoID\uff09\uff0c\u7528\u4e8e\u4ece\u566a\u58f0\u3001\u7a00\u758f\u6216\u4e0d\u5b8c\u6574\u6570\u636e\u4e2d\u7cbe\u786e\u63a8\u65ad\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u7684ODE\u53c2\u6570\u548c\u672a\u89c2\u6d4b\u7ec4\u4ef6\u3002", "motivation": "\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u7684\u7cfb\u7edf\u63a8\u65ad\u5728\u566a\u58f0\u3001\u7a00\u758f\u6216\u90e8\u5206\u53ef\u89c2\u6d4b\u6570\u636e\u4e0b\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u4e0e\u8d85\u7f51\u7edc\u6784\u5efaODE\u6c42\u89e3\u5668\uff0c\u4ee5\u53caWasserstein\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u4f30\u8ba1\u53c2\u6570\u3002", "result": "SiGMoID\u80fd\u91cf\u5316\u6570\u636e\u566a\u58f0\u3001\u4f30\u8ba1\u53c2\u6570\u5e76\u63a8\u65ad\u672a\u89c2\u6d4b\u7ec4\u4ef6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "SiGMoID\u4e3a\u79d1\u5b66\u7814\u7a76\u548c\u5de5\u7a0b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u52a8\u6001\u7cfb\u7edf\u63a8\u65ad\u5de5\u5177\u3002"}}
{"id": "2507.11119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11119", "abs": "https://arxiv.org/abs/2507.11119", "authors": ["Hankun Liu", "Yujian Zhao", "Guanglin Niu"], "title": "Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID", "comment": null, "summary": "Hard samples pose a significant challenge in person re-identification (ReID)\ntasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent\nambiguity or similarity, coupled with the lack of explicit definitions, makes\nthem a fundamental bottleneck. These issues not only limit the design of\ntargeted learning strategies but also diminish the model's robustness under\nclothing or viewpoint changes. In this paper, we propose a novel\nmultimodal-guided Hard Sample Generation and Learning (HSGL) framework, which\nis the first effort to unify textual and visual modalities to explicitly\ndefine, generate, and optimize hard samples within a unified paradigm. HSGL\ncomprises two core components: (1) Dual-Granularity Hard Sample Generation\n(DGHSG), which leverages multimodal cues to synthesize semantically consistent\nsamples, including both coarse- and fine-grained hard positives and negatives\nfor effectively increasing the hardness and diversity of the training data. (2)\nHard Sample Adaptive Learning (HSAL), which introduces a hardness-aware\noptimization strategy that adjusts feature distances based on textual semantic\nlabels, encouraging the separation of hard positives and drawing hard negatives\ncloser in the embedding space to enhance the model's discriminative capability\nand robustness to hard samples. Extensive experiments on multiple CC-ReID\nbenchmarks demonstrate the effectiveness of our approach and highlight the\npotential of multimodal-guided hard sample generation and learning for robust\nCC-ReID. Notably, HSAL significantly accelerates the convergence of the\ntargeted learning procedure and achieves state-of-the-art performance on both\nPRCC and LTCC datasets. The code is available at\nhttps://github.com/undooo/TryHarder-ACMMM25.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u786c\u6837\u672c\u751f\u6210\u4e0e\u5b66\u4e60\u6846\u67b6\uff08HSGL\uff09\uff0c\u9996\u6b21\u7edf\u4e00\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u6765\u5b9a\u4e49\u3001\u751f\u6210\u548c\u4f18\u5316\u786c\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u670d\u88c5\u53d8\u5316\u884c\u4eba\u91cd\u8bc6\u522b\uff08CC-ReID\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u786c\u6837\u672c\u5728\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5728\u670d\u88c5\u53d8\u5316\u573a\u666f\u4e0b\u3002\u5176\u6a21\u7cca\u6027\u548c\u7f3a\u4e4f\u660e\u786e\u5b9a\u4e49\u9650\u5236\u4e86\u9488\u5bf9\u6027\u5b66\u4e60\u7b56\u7565\u7684\u8bbe\u8ba1\u548c\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "HSGL\u6846\u67b6\u5305\u542b\u53cc\u7c92\u5ea6\u786c\u6837\u672c\u751f\u6210\uff08DGHSG\uff09\u548c\u786c\u6837\u672c\u81ea\u9002\u5e94\u5b66\u4e60\uff08HSAL\uff09\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7ebf\u7d22\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u6837\u672c\uff0c\u5e76\u57fa\u4e8e\u6587\u672c\u8bed\u4e49\u6807\u7b7e\u8c03\u6574\u7279\u5f81\u8ddd\u79bb\u3002", "result": "\u5728PRCC\u548cLTCC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u52a0\u901f\u4e86\u5b66\u4e60\u8fc7\u7a0b\u7684\u6536\u655b\u3002", "conclusion": "\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u786c\u6837\u672c\u751f\u6210\u4e0e\u5b66\u4e60\u4e3a\u9c81\u68d2\u7684CC-ReID\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0cHSGL\u6846\u67b6\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.10886", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10886", "abs": "https://arxiv.org/abs/2507.10886", "authors": ["Patryk Jasiorski", "Marek Klonowski", "Micha\u0142 Wo\u017aniak"], "title": "How to Protect Models against Adversarial Unlearning?", "comment": null, "summary": "AI models need to be unlearned to fulfill the requirements of legal acts such\nas the AI Act or GDPR, and also because of the need to remove toxic content,\ndebiasing, the impact of malicious instances, or changes in the data\ndistribution structure in which a model works. Unfortunately, removing\nknowledge may cause undesirable side effects, such as a deterioration in model\nperformance. In this paper, we investigate the problem of adversarial\nunlearning, where a malicious party intentionally sends unlearn requests to\ndeteriorate the model's performance maximally. We show that this phenomenon and\nthe adversary's capabilities depend on many factors, primarily on the backbone\nmodel itself and strategy/limitations in selecting data to be unlearned. The\nmain result of this work is a new method of protecting model performance from\nthese side effects, both in the case of unlearned behavior resulting from\nspontaneous processes and adversary actions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5bf9\u6297\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u62a4\u6a21\u578b\u6027\u80fd\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "AI\u6a21\u578b\u9700\u8981\u9057\u5fd8\u4ee5\u6ee1\u8db3\u6cd5\u5f8b\u8981\u6c42\uff08\u5982AI\u6cd5\u6848\u6216GDPR\uff09\uff0c\u6216\u79fb\u9664\u6709\u6bd2\u5185\u5bb9\u3001\u53bb\u504f\u89c1\u7b49\u3002\u4f46\u9057\u5fd8\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u7814\u7a76\u4e86\u5bf9\u6297\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5206\u6790\u4e86\u5f71\u54cd\u56e0\u7d20\uff08\u5982\u6a21\u578b\u7ed3\u6784\u548c\u6570\u636e\u9009\u62e9\u7b56\u7565\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u62a4\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86\u5bf9\u6297\u6027\u9057\u5fd8\u73b0\u8c61\u53ca\u5176\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9632\u62a4\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u80fd\u6709\u6548\u9632\u6b62\u56e0\u9057\u5fd8\uff08\u81ea\u53d1\u6216\u5bf9\u6297\u6027\uff09\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2507.11129", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11129", "abs": "https://arxiv.org/abs/2507.11129", "authors": ["Zhifeng Gu", "Bing Wang"], "title": "MMOne: Representing Multiple Modalities in One Scene", "comment": "Accepted to ICCV 2025", "summary": "Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.", "AI": {"tldr": "MMOne\u6846\u67b6\u901a\u8fc7\u6a21\u6001\u5efa\u6a21\u6a21\u5757\u548c\u591a\u6a21\u6001\u5206\u89e3\u673a\u5236\u89e3\u51b3\u6a21\u6001\u51b2\u7a81\uff0c\u63d0\u5347\u591a\u6a21\u6001\u573a\u666f\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u611f\u77e5\u589e\u5f3a\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\uff0c\u4f46\u6a21\u6001\u51b2\u7a81\uff08\u5c5e\u6027\u5dee\u5f02\u548c\u7c92\u5ea6\u5dee\u5f02\uff09\u5e26\u6765\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u5efa\u6a21\u6a21\u5757\u548c\u6a21\u6001\u6307\u793a\u5668\uff0c\u8bbe\u8ba1\u591a\u6a21\u6001\u5206\u89e3\u673a\u5236\u5206\u79bb\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u63d0\u5347\u5404\u6a21\u6001\u8868\u793a\u80fd\u529b\uff0c\u4e14\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u6a21\u6001\u3002", "conclusion": "MMOne\u6846\u67b6\u6709\u6548\u89e3\u51b3\u6a21\u6001\u51b2\u7a81\uff0c\u5b9e\u73b0\u7d27\u51d1\u9ad8\u6548\u7684\u591a\u6a21\u6001\u573a\u666f\u8868\u793a\u3002"}}
{"id": "2507.10890", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10890", "abs": "https://arxiv.org/abs/2507.10890", "authors": ["Riccardo Savorgnan", "Udaya Ghai", "Carson Eisenach", "Dean Foster"], "title": "Outbound Modeling for Inventory Management", "comment": "KDD - AI for Supply Chain Workshop", "summary": "We study the problem of forecasting the number of units fulfilled (or\n``drained'') from each inventory warehouse to meet customer demand, along with\nthe associated outbound shipping costs. The actual drain and shipping costs are\ndetermined by complex production systems that manage the planning and execution\nof customers' orders fulfillment, i.e. from where and how to ship a unit to be\ndelivered to a customer. Accurately modeling these processes is critical for\nregional inventory planning, especially when using Reinforcement Learning (RL)\nto develop control policies. For the RL usecase, a drain model is incorporated\ninto a simulator to produce long rollouts, which we desire to be\ndifferentiable. While simulating the calls to the internal software systems can\nbe used to recover this transition, they are non-differentiable and too slow\nand costly to run within an RL training environment. Accordingly, we frame this\nas a probabilistic forecasting problem, modeling the joint distribution of\noutbound drain and shipping costs across all warehouses at each time period,\nconditioned on inventory positions and exogenous customer demand. To ensure\nrobustness in an RL environment, the model must handle out-of-distribution\nscenarios that arise from off-policy trajectories. We propose a validation\nscheme that leverages production systems to evaluate the drain model on\ncounterfactual inventory states induced by RL policies. Preliminary results\ndemonstrate the model's accuracy within the in-distribution setting.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9884\u6d4b\u5e93\u5b58\u4ed3\u5e93\u6ee1\u8db3\u5ba2\u6237\u9700\u6c42\u7684\u5355\u4f4d\u6570\u91cf\uff08\u201cdrain\u201d\uff09\u53ca\u76f8\u5173\u51fa\u5e93\u8fd0\u8f93\u6210\u672c\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5e93\u5b58\u89c4\u5212\u3002", "motivation": "\u51c6\u786e\u5efa\u6a21\u5e93\u5b58\u548c\u8fd0\u8f93\u6210\u672c\u5bf9\u533a\u57df\u5e93\u5b58\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5f00\u53d1\u63a7\u5236\u7b56\u7565\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6a21\u62df\u5185\u90e8\u8f6f\u4ef6\u7cfb\u7edf\uff09\u4e0d\u53ef\u5fae\u5206\u4e14\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6982\u7387\u9884\u6d4b\u95ee\u9898\uff0c\u5efa\u6a21\u6240\u6709\u4ed3\u5e93\u5728\u6bcf\u4e2a\u65f6\u95f4\u6bb5\u7684\u51fa\u5e93drain\u548c\u8fd0\u8f93\u6210\u672c\u7684\u8054\u5408\u5206\u5e03\uff0c\u8003\u8651\u5e93\u5b58\u4f4d\u7f6e\u548c\u5916\u90e8\u5ba2\u6237\u9700\u6c42\u3002\u6a21\u578b\u9700\u5904\u7406\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u975e\u5206\u5e03\u573a\u666f\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\u6a21\u578b\u5728\u5206\u5e03\u5185\u8bbe\u7f6e\u4e0b\u5177\u6709\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u9884\u6d4b\u6a21\u578b\u4e3a\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u5e93\u5b58\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u65b9\u6848\u8bc4\u4f30\u4e86\u5176\u5728\u975e\u5206\u5e03\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11143", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11143", "abs": "https://arxiv.org/abs/2507.11143", "authors": ["Lam Pham", "Cam Le", "Hieu Tang", "Khang Truong", "Truong Nguyen", "Jasmin Lampert", "Alexander Schindler", "Martin Boyer", "Son Phan"], "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images", "comment": null, "summary": "In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5229\u7528\u9065\u611f\u56fe\u50cf\u81ea\u52a8\u89c2\u6d4b\u6ed1\u5761\u4e8b\u4ef6\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u7cbe\u5ea6\u7ed3\u679c\u3002", "motivation": "\u6ed1\u5761\u707e\u5bb3\u9891\u53d1\uff0c\u4f46\u4f20\u7edf\u89c2\u6d4b\u65b9\u6cd5\u96be\u4ee5\u8986\u76d6\u5927\u9762\u79ef\u548c\u590d\u6742\u5730\u5f62\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u6ed1\u5761\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\uff0c\u8f93\u5165\u4e3a\u9065\u611f\u56fe\u50cf\u3002", "result": "\u5728LandSlide4Sense\u3001Bijie\u548cNepal\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u4efb\u52a1F1\u5206\u6570\u5206\u522b\u4e3a98.23\u548c93.83\uff0c\u5206\u5272\u4efb\u52a1mIoU\u5206\u6570\u4e3a63.74\u548c76.88\u3002", "conclusion": "\u6a21\u578b\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u96c6\u6210\u5230\u6ed1\u5761\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u3002"}}
{"id": "2507.10904", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10904", "abs": "https://arxiv.org/abs/2507.10904", "authors": ["Elisa Tsai", "Haizhong Zheng", "Atul Prakash"], "title": "Class-Proportional Coreset Selection for Difficulty-Separable Data", "comment": "This paper has been accepted to the ICCV 2025 Workshop on Curated\n  Data for Efficient Learning (CDEL)", "summary": "High-quality training data is essential for building reliable and efficient\nmachine learning systems. One-shot coreset selection addresses this by pruning\nthe dataset while maintaining or even improving model performance, often\nrelying on training-dynamics-based data difficulty scores. However, most\nexisting methods implicitly assume class-wise homogeneity in data difficulty,\noverlooking variation in data difficulty across different classes.\n  In this work, we challenge this assumption by showing that, in domains such\nas network intrusion detection and medical imaging, data difficulty often\nclusters by class. We formalize this as class-difficulty separability and\nintroduce the Class Difficulty Separability Coefficient (CDSC) as a\nquantitative measure. We demonstrate that high CDSC values correlate with\nperformance degradation in class-agnostic coreset methods, which tend to\noverrepresent easy majority classes while neglecting rare but informative ones.\n  To address this, we introduce class-proportional variants of multiple\nsampling strategies. Evaluated on five diverse datasets spanning security and\nmedical domains, our methods consistently achieve state-of-the-art data\nefficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a\nclass-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows\nremarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and\nrecall 0.19%. In contrast, the class-agnostic CCS baseline, the next best\nmethod, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and\n4.11% in recall.\n  We further show that aggressive pruning enhances generalization in noisy,\nimbalanced, and large-scale datasets. Our results underscore that explicitly\nmodeling class-difficulty separability leads to more effective, robust, and\ngeneralizable data pruning, particularly in high-stakes scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u7c7b\u522b\u95f4\u6570\u636e\u96be\u5ea6\u5dee\u5f02\u7684\u6838\u96c6\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u522b\u96be\u5ea6\u53ef\u5206\u6027\u7cfb\u6570\uff08CDSC\uff09\u548c\u6539\u8fdb\u7684\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u4fee\u526a\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6838\u96c6\u9009\u62e9\u65b9\u6cd5\u5047\u8bbe\u6570\u636e\u96be\u5ea6\u5728\u7c7b\u522b\u95f4\u5747\u5300\u5206\u5e03\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u4e2d\u7c7b\u522b\u95f4\u6570\u636e\u96be\u5ea6\u7684\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u7c7b\u522b\u96be\u5ea6\u53ef\u5206\u6027\u7cfb\u6570\uff08CDSC\uff09\u8861\u91cf\u7c7b\u522b\u95f4\u6570\u636e\u96be\u5ea6\u5dee\u5f02\uff0c\u5e76\u8bbe\u8ba1\u7c7b\u522b\u6bd4\u4f8b\u8c03\u6574\u7684\u91c7\u6837\u7b56\u7565\uff08\u5982CCS-CP\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b0\u65b9\u6cd5\u5728\u6781\u7aef\u4fee\u526a\u7387\u4e0b\u6027\u80fd\u4e0b\u964d\u66f4\u5c0f\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u663e\u5f0f\u5efa\u6a21\u7c7b\u522b\u96be\u5ea6\u53ef\u5206\u6027\u53ef\u63d0\u5347\u6570\u636e\u4fee\u526a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u3002"}}
{"id": "2507.11152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11152", "abs": "https://arxiv.org/abs/2507.11152", "authors": ["Duoyou Chen", "Yunqing Chen", "Can Zhang", "Zhou Wang", "Cheng Chen", "Ruoxiu Xiao"], "title": "Latent Space Consistency for Sparse-View CT Reconstruction", "comment": "ACMMM2025 Accepted", "summary": "Computed Tomography (CT) is a widely utilized imaging modality in clinical\nsettings. Using densely acquired rotational X-ray arrays, CT can capture 3D\nspatial features. However, it is confronted with challenged such as significant\ntime consumption and high radiation exposure. CT reconstruction methods based\non sparse-view X-ray images have garnered substantial attention from\nresearchers as they present a means to mitigate costs and risks. In recent\nyears, diffusion models, particularly the Latent Diffusion Model (LDM), have\ndemonstrated promising potential in the domain of 3D CT reconstruction.\nNonetheless, due to the substantial differences between the 2D latent\nrepresentation of X-ray modalities and the 3D latent representation of CT\nmodalities, the vanilla LDM is incapable of achieving effective alignment\nwithin the latent space. To address this issue, we propose the Consistent\nLatent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature\ncontrastive learning to efficiently extract latent 3D information from 2D X-ray\nimages and achieve latent space alignment between modalities. Experimental\nresults indicate that CLS-DM outperforms classical and state-of-the-art\ngenerative models in terms of standard voxel-level metrics (PSNR, SSIM) on the\nLIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing\nthe effectiveness and economic viability of sparse X-ray reconstructed CT but\ncan also be generalized to other cross-modal transformation tasks, such as\ntext-to-image synthesis. We have made our code publicly available at\nhttps://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research\nand applications in other domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u7684\u6269\u6563\u6a21\u578b\uff08CLS-DM\uff09\uff0c\u7528\u4e8e\u4ece\u7a00\u758fX\u5c04\u7ebf\u56fe\u50cf\u91cd\u5efa3D CT\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLDM\u5728\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfCT\u91cd\u5efa\u65b9\u6cd5\u8017\u65f6\u957f\u4e14\u8f90\u5c04\u9ad8\uff0c\u7a00\u758fX\u5c04\u7ebf\u56fe\u50cf\u91cd\u5efa\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\uff08\u5982LDM\uff09\u5728\u8de8\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faCLS-DM\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u6bd4\u5b66\u4e60\u4ece2D X\u5c04\u7ebf\u56fe\u50cf\u4e2d\u63d0\u53d63D\u6f5c\u5728\u4fe1\u606f\uff0c\u5e76\u5b9e\u73b0\u6a21\u6001\u95f4\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u3002", "result": "\u5728LIDC-IDRI\u548cCTSpine1K\u6570\u636e\u96c6\u4e0a\uff0cCLS-DM\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u4f18\u4e8e\u7ecf\u5178\u548c\u6700\u65b0\u751f\u6210\u6a21\u578b\u3002", "conclusion": "CLS-DM\u63d0\u5347\u4e86\u7a00\u758fX\u5c04\u7ebf\u91cd\u5efaCT\u7684\u6548\u679c\u548c\u7ecf\u6d4e\u6027\uff0c\u5e76\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u8de8\u6a21\u6001\u4efb\u52a1\uff08\u5982\u6587\u672c\u5230\u56fe\u50cf\u5408\u6210\uff09\u3002"}}
{"id": "2507.10955", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10955", "abs": "https://arxiv.org/abs/2507.10955", "authors": ["Chi-en Amy Tai", "Alexander Wong"], "title": "Diffusion Decoding for Peptide De Novo Sequencing", "comment": null, "summary": "Peptide de novo sequencing is a method used to reconstruct amino acid\nsequences from tandem mass spectrometry data without relying on existing\nprotein sequence databases. Traditional deep learning approaches, such as\nCasanovo, mainly utilize autoregressive decoders and predict amino acids\nsequentially. Subsequently, they encounter cascading errors and fail to\nleverage high-confidence regions effectively. To address these issues, this\npaper investigates using diffusion decoders adapted for the discrete data\ndomain. These decoders provide a different approach, allowing sequence\ngeneration to start from any peptide segment, thereby enhancing prediction\naccuracy. We experiment with three different diffusion decoder designs,\nknapsack beam search, and various loss functions. We find knapsack beam search\ndid not improve performance metrics and simply replacing the transformer\ndecoder with a diffusion decoder lowered performance. Although peptide\nprecision and recall were still 0, the best diffusion decoder design with the\nDINOISER loss function obtained a statistically significant improvement in\namino acid recall by 0.373 compared to the baseline autoregressive\ndecoder-based Casanovo model. These findings highlight the potential of\ndiffusion decoders to not only enhance model sensitivity but also drive\nsignificant advancements in peptide de novo sequencing.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u80bd\u6bb5\u4ece\u5934\u6d4b\u5e8f\u4e2d\u4f7f\u7528\u6269\u6563\u89e3\u7801\u5668\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u81ea\u56de\u5f52\u89e3\u7801\u5668\uff0c\u6269\u6563\u89e3\u7801\u5668\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u81ea\u56de\u5f52\u89e3\u7801\u5668\u5b58\u5728\u7ea7\u8054\u9519\u8bef\u548c\u672a\u80fd\u6709\u6548\u5229\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u533a\u57df\u7684\u95ee\u9898\uff0c\u6269\u6563\u89e3\u7801\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u601d\u8def\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u6269\u6563\u89e3\u7801\u5668\u8bbe\u8ba1\u3001\u80cc\u5305\u675f\u641c\u7d22\u548c\u4e0d\u540c\u635f\u5931\u51fd\u6570\uff0c\u53d1\u73b0DINOISER\u635f\u5931\u51fd\u6570\u6548\u679c\u6700\u4f73\u3002", "result": "\u6700\u4f73\u6269\u6563\u89e3\u7801\u5668\u8bbe\u8ba1\u5728\u6c28\u57fa\u9178\u53ec\u56de\u7387\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e860.373\uff0c\u4f46\u80bd\u6bb5\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u4ecd\u4e3a0\u3002", "conclusion": "\u6269\u6563\u89e3\u7801\u5668\u5728\u80bd\u6bb5\u4ece\u5934\u6d4b\u5e8f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u63d0\u5347\u6a21\u578b\u7075\u654f\u5ea6\u5e76\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2507.11153", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11153", "abs": "https://arxiv.org/abs/2507.11153", "authors": ["Hongfei Ye", "Bin Chen", "Wenxi Liu", "Yu Zhang", "Zhao Li", "Dandan Ni", "Hongyang Chen"], "title": "Assessing Color Vision Test in Large Vision-language Models", "comment": null, "summary": "With the widespread adoption of large vision-language models, the capacity\nfor color vision in these models is crucial. However, the color vision\nabilities of large visual-language models have not yet been thoroughly\nexplored. To address this gap, we define a color vision testing task for large\nvision-language models and construct a dataset \\footnote{Anonymous Github\nShowing some of the data\nhttps://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers\nmultiple categories of test questions and tasks of varying difficulty levels.\nFurthermore, we analyze the types of errors made by large vision-language\nmodels and propose fine-tuning strategies to enhance their performance in color\nvision tests.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8272\u5f69\u89c6\u89c9\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u6d4b\u8bd5\u4efb\u52a1\u5e76\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u9519\u8bef\u7c7b\u578b\u5e76\u63d0\u51fa\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8272\u5f69\u89c6\u89c9\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u662f\u672c\u6587\u7684\u52a8\u673a\u3002", "method": "\u5b9a\u4e49\u4e86\u8272\u5f69\u89c6\u89c9\u6d4b\u8bd5\u4efb\u52a1\uff0c\u6784\u5efa\u4e86\u591a\u7c7b\u522b\u3001\u591a\u96be\u5ea6\u7ea7\u522b\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u7684\u9519\u8bef\u7c7b\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u9488\u5bf9\u8272\u5f69\u89c6\u89c9\u6d4b\u8bd5\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8272\u5f69\u89c6\u89c9\u80fd\u529b\u63d0\u4f9b\u4e86\u6d4b\u8bd5\u6846\u67b6\u548c\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2507.10983", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10983", "abs": "https://arxiv.org/abs/2507.10983", "authors": ["Tao Han", "Zahra Taheri", "Hyunwoong Ko"], "title": "Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review", "comment": "11 pages, 1 figure, 3 tables, IDETC-CIE 2025", "summary": "Semiconductor manufacturing relies heavily on film deposition processes, such\nas Chemical Vapor Deposition and Physical Vapor Deposition. These complex\nprocesses require precise control to achieve film uniformity, proper adhesion,\nand desired functionality. Recent advancements in Physics-Informed Neural\nNetworks (PINNs), an innovative machine learning (ML) approach, have shown\nsignificant promise in addressing challenges related to process control,\nquality assurance, and predictive modeling within semiconductor film deposition\nand other manufacturing domains. This paper provides a comprehensive review of\nML applications targeted at semiconductor film deposition processes. Through a\nthematic analysis, we identify key trends, existing limitations, and research\ngaps, offering insights into both the advantages and constraints of current\nmethodologies. Our structured analysis aims to highlight the potential\nintegration of these ML techniques to enhance interpretability, accuracy, and\nrobustness in film deposition processes. Additionally, we examine\nstate-of-the-art PINN methods, discussing strategies for embedding physical\nknowledge, governing laws, and partial differential equations into advanced\nneural network architectures tailored for semiconductor manufacturing. Based on\nthis detailed review, we propose novel research directions that integrate the\nstrengths of PINNs to significantly advance film deposition processes. The\ncontributions of this study include establishing a clear pathway for future\nresearch in integrating physics-informed ML frameworks, addressing existing\nmethodological gaps, and ultimately improving precision, scalability, and\noperational efficiency within semiconductor manufacturing.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u673a\u5668\u5b66\u4e60\u5728\u534a\u5bfc\u4f53\u8584\u819c\u6c89\u79ef\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002", "motivation": "\u534a\u5bfc\u4f53\u8584\u819c\u6c89\u79ef\u8fc7\u7a0b\u590d\u6742\uff0c\u9700\u8981\u7cbe\u786e\u63a7\u5236\uff0c\u673a\u5668\u5b66\u4e60\u5c24\u5176\u662fPINNs\u5728\u63d0\u5347\u5de5\u827a\u63a7\u5236\u548c\u8d28\u91cf\u9884\u6d4b\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u4e3b\u9898\u5206\u6790\uff0c\u603b\u7ed3\u4e86\u5f53\u524dML\u65b9\u6cd5\u7684\u8d8b\u52bf\u3001\u5c40\u9650\u6027\u548c\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63a2\u8ba8\u4e86PINNs\u5728\u5d4c\u5165\u7269\u7406\u77e5\u8bc6\u65b9\u9762\u7684\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u7ed3\u5408PINNs\u4f18\u52bf\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63d0\u5347\u8584\u819c\u6c89\u79ef\u7684\u7cbe\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u64cd\u4f5c\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u4e3a\u672a\u6765\u7269\u7406\u4fe1\u606fML\u6846\u67b6\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6e05\u6670\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u586b\u8865\u65b9\u6cd5\u5b66\u7a7a\u767d\u5e76\u63a8\u52a8\u534a\u5bfc\u4f53\u5236\u9020\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.11171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11171", "abs": "https://arxiv.org/abs/2507.11171", "authors": ["Jun Chen", "Yonghua Yu", "Weifu Li", "Yaohui Chen", "Hong Chen"], "title": "Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification", "comment": "11 pages, 5 figures", "summary": "Citrus, as one of the most economically important fruit crops globally,\nsuffers severe yield depressions due to various diseases. Accurate disease\ndetection and classification serve as critical prerequisites for implementing\ntargeted control measures. Recent advancements in artificial intelligence,\nparticularly deep learning-based computer vision algorithms, have substantially\ndecreased time and labor requirements while maintaining the accuracy of\ndetection and classification. Nevertheless, these methods predominantly rely on\nmassive, high-quality annotated training examples to attain promising\nperformance. By introducing two key designs: contrasting with cluster centroids\nand a multi-layer contrastive training (MCT) paradigm, this paper proposes a\nnovel clustering-guided self-supervised multi-layer contrastive representation\nlearning (CMCRL) algorithm. The proposed method demonstrates several advantages\nover existing counterparts: (1) optimizing with massive unannotated samples;\n(2) effective adaptation to the symptom similarity across distinct citrus\ndiseases; (3) hierarchical feature representation learning. The proposed method\nachieves state-of-the-art performance on the public citrus image set CDD,\noutperforming existing methods by 4.5\\%-30.1\\% accuracy. Remarkably, our method\nnarrows the performance gap with fully supervised counterparts (all samples are\nlabeled). Beyond classification accuracy, our method shows great performance on\nother evaluation metrics (F1 score, precision, and recall), highlighting the\nrobustness against the class imbalance challenge.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u591a\u5c42\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u7684\u67d1\u6a58\u75c5\u5bb3\u5206\u7c7b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u67d1\u6a58\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u4ea7\u91cf\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u6210\u672c\u9ad8\u3002", "method": "\u5f15\u5165\u805a\u7c7b\u4e2d\u5fc3\u5bf9\u6bd4\u548c\u591a\u5c42\u5bf9\u6bd4\u8bad\u7ec3\u8303\u5f0f\uff0c\u63d0\u51faCMCRL\u7b97\u6cd5\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6CDD\u4e0a\u51c6\u786e\u7387\u63d0\u53474.5%-30.1%\uff0c\u63a5\u8fd1\u5168\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51cf\u5c11\u6807\u6ce8\u4f9d\u8d56\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10986", "abs": "https://arxiv.org/abs/2507.10986", "authors": ["Tianyu Su", "Zhiqiang Zou", "Ali Luo", "Xiao Kong", "Qingyu Lu", "Min Li"], "title": "StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data", "comment": null, "summary": "Stellar flare forecasting, a critical research frontier in astronomy, offers\nprofound insights into stellar activity. However, the field is constrained by\nboth the sparsity of recorded flare events and the absence of domain-specific\nlarge-scale predictive models. To address these challenges, this study\nintroduces StellarF (Stellar Flare Forecasting), a novel large model that\nleverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient\nlearning for stellar flare forecasting. At its core, StellarF integrates an\nflare statistical information module with a historical flare record module,\nenabling multi-scale pattern recognition from observational data. Extensive\nexperiments on our self-constructed datasets (derived from Kepler and TESS\nlight curves) demonstrate that StellarF achieves state-of-the-art performance\ncompared to existing methods. The proposed prediction paradigm establishes a\nnovel methodological framework for advancing astrophysical research and\ncross-disciplinary applications.", "AI": {"tldr": "StellarF\u6a21\u578b\u901a\u8fc7LoRA\u548cAdapter\u6280\u672f\u9ad8\u6548\u5b66\u4e60\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u6a21\u5f0f\u8bc6\u522b\uff0c\u5728\u6052\u661f\u8000\u6591\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6052\u661f\u8000\u6591\u9884\u6d4b\u9886\u57df\u56e0\u6570\u636e\u7a00\u758f\u548c\u7f3a\u4e4f\u5927\u89c4\u6a21\u9884\u6d4b\u6a21\u578b\u800c\u53d7\u9650\uff0cStellarF\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "StellarF\u6574\u5408\u8000\u6591\u7edf\u8ba1\u4fe1\u606f\u6a21\u5757\u548c\u5386\u53f2\u8bb0\u5f55\u6a21\u5757\uff0c\u5229\u7528LoRA\u548cAdapter\u6280\u672f\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5b66\u4e60\u3002", "result": "\u5728Kepler\u548cTESS\u6570\u636e\u4e0a\uff0cStellarF\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "StellarF\u4e3a\u5929\u4f53\u7269\u7406\u7814\u7a76\u548c\u8de8\u5b66\u79d1\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u6846\u67b6\u3002"}}
{"id": "2507.11200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11200", "abs": "https://arxiv.org/abs/2507.11200", "authors": ["Che Liu", "Jiazhen Pan", "Weixiang Shen", "Wenjia Bai", "Daniel Rueckert", "Rossella Arcucci"], "title": "How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study", "comment": "Accepted by the International Conference on AI in Healthcare 2025", "summary": "Vision-Language Models (VLMs) trained on web-scale corpora excel at natural\nimage tasks and are increasingly repurposed for healthcare; however, their\ncompetence in medical tasks remains underexplored. We present a comprehensive\nevaluation of open-source general-purpose and medically specialised VLMs,\nranging from 3B to 72B parameters, across eight benchmarks: MedXpert,\nOmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model\nperformance across different aspects, we first separate it into understanding\nand reasoning components. Three salient findings emerge. First, large\ngeneral-purpose models already match or surpass medical-specific counterparts\non several benchmarks, demonstrating strong zero-shot transfer from natural to\nmedical images. Second, reasoning performance is consistently lower than\nunderstanding, highlighting a critical barrier to safe decision support. Third,\nperformance varies widely across benchmarks, reflecting differences in task\ndesign, annotation quality, and knowledge demands. No model yet reaches the\nreliability threshold for clinical deployment, underscoring the need for\nstronger multimodal alignment and more rigorous, fine-grained evaluation\nprotocols.", "AI": {"tldr": "\u8bc4\u4f30\u5f00\u6e90\u901a\u7528\u548c\u533b\u5b66\u4e13\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u901a\u7528\u5927\u6a21\u578b\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u5df2\u8d85\u8d8a\u533b\u5b66\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u4ecd\u662f\u74f6\u9888\uff0c\u4e34\u5e8a\u90e8\u7f72\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5bf93B\u81f372B\u53c2\u6570\u7684\u6a21\u578b\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u4e3a\u7406\u89e3\u548c\u63a8\u7406\u4e24\u90e8\u5206\u3002", "result": "\u901a\u7528\u5927\u6a21\u578b\u5728\u90e8\u5206\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u533b\u5b66\u4e13\u7528\u6a21\u578b\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u8f83\u5f31\uff0c\u4e14\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u95f4\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "\u9700\u52a0\u5f3a\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u63d0\u5347\u4e34\u5e8a\u90e8\u7f72\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.10990", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10990", "abs": "https://arxiv.org/abs/2507.10990", "authors": ["Rodney Lafuente-Mercado"], "title": "High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization", "comment": null, "summary": "Scaling reinforcement learning (RL) workloads often requires distributing\nenvironment simulation across compute clusters. Existing frameworks entangle\nsimulation, learning logic, and orchestration into monolithic systems, limiting\nmodularity and reusability. We present ClusterEnv, a lightweight,\nlearner-agnostic interface for distributed environment execution that mirrors\nthe Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples\nsimulation from training by offloading reset() and step() operations to remote\nworkers while keeping learning centralized. To address policy staleness in\ndistributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),\na divergence-triggered update mechanism that reduces synchronization overhead\nwithout sacrificing performance. ClusterEnv integrates cleanly into existing RL\npipelines, supports both on-policy and off-policy methods, and requires minimal\ncode changes. Experiments on discrete control tasks demonstrate that AAPS\nachieves high sample efficiency with significantly fewer weight updates. Source\ncode is available at https://github.com/rodlaf/ClusterEnv.", "AI": {"tldr": "ClusterEnv\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u4e0e\u5b66\u4e60\u5668\u65e0\u5173\u7684\u5206\u5e03\u5f0f\u73af\u5883\u6267\u884c\u63a5\u53e3\uff0c\u901a\u8fc7DETACH\u6a21\u5f0f\u89e3\u8026\u6a21\u62df\u4e0e\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165AAPS\u673a\u5236\u51cf\u5c11\u540c\u6b65\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u6846\u67b6\u5c06\u6a21\u62df\u3001\u5b66\u4e60\u903b\u8f91\u548c\u7f16\u6392\u8026\u5408\u4e3a\u5355\u4e00\u7cfb\u7edf\uff0c\u9650\u5236\u4e86\u6a21\u5757\u5316\u548c\u53ef\u91cd\u7528\u6027\u3002", "method": "ClusterEnv\u91c7\u7528DETACH\u6a21\u5f0f\uff0c\u5c06reset()\u548cstep()\u64cd\u4f5c\u5378\u8f7d\u5230\u8fdc\u7a0b\u5de5\u4f5c\u8282\u70b9\uff0c\u540c\u65f6\u4fdd\u6301\u5b66\u4e60\u96c6\u4e2d\u5316\uff1bAAPS\u673a\u5236\u901a\u8fc7\u53d1\u6563\u89e6\u53d1\u66f4\u65b0\u51cf\u5c11\u540c\u6b65\u5f00\u9500\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAAPS\u5728\u79bb\u6563\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6837\u672c\u6548\u7387\uff0c\u4e14\u6743\u91cd\u66f4\u65b0\u6b21\u6570\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "ClusterEnv\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709RL\u6d41\u7a0b\u4e2d\uff0c\u652f\u6301\u591a\u79cd\u7b56\u7565\u65b9\u6cd5\uff0c\u4ee3\u7801\u6539\u52a8\u6781\u5c0f\u3002"}}
{"id": "2507.11202", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11202", "abs": "https://arxiv.org/abs/2507.11202", "authors": ["Xinkui Zhao", "Jinsong Shu", "Yangyang Wu", "Guanjie Cheng", "Zihe Liu", "Naibo Wang", "Shuiguang Deng", "Zhongle Xie", "Jianwei Yin"], "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition", "comment": null, "summary": "Multimodal Emotion Recognition (MER) often encounters incomplete\nmultimodality in practical applications due to sensor failures or privacy\nprotection requirements. While existing methods attempt to address various\nincomplete multimodal scenarios by balancing the training of each modality\ncombination through additional gradients, these approaches face a critical\nlimitation: training gradients from different modality combinations conflict\nwith each other, ultimately degrading the performance of the final prediction\nmodel. In this paper, we propose a unimodal decoupled dynamic low-rank\nadaptation method based on modality combinations, named MCULoRA, which is a\nnovel framework for the parameter-efficient training of incomplete multimodal\nlearning models. MCULoRA consists of two key modules, modality combination\naware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The\nMCLA module effectively decouples the shared information from the distinct\ncharacteristics of individual modality combinations. The DPFT module adjusts\nthe training ratio of modality combinations based on the separability of each\nmodality's representation space, optimizing the learning efficiency across\ndifferent modality combinations. Our extensive experimental evaluation in\nmultiple benchmark datasets demonstrates that MCULoRA substantially outperforms\nprevious incomplete multimodal learning approaches in downstream task accuracy.", "AI": {"tldr": "MCULoRA\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u6001\u7ec4\u5408\u7684\u5355\u6a21\u6001\u89e3\u8026\u52a8\u6001\u4f4e\u79e9\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bad\u7ec3\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u5e38\u56e0\u4f20\u611f\u5668\u6545\u969c\u6216\u9690\u79c1\u4fdd\u62a4\u5bfc\u81f4\u6a21\u6001\u7f3a\u5931\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u6a21\u6001\u7ec4\u5408\u8bad\u7ec3\u68af\u5ea6\u51b2\u7a81\u800c\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faMCULoRA\u6846\u67b6\uff0c\u5305\u542b\u6a21\u6001\u7ec4\u5408\u611f\u77e5\u4f4e\u79e9\u9002\u5e94\uff08MCLA\uff09\u548c\u52a8\u6001\u53c2\u6570\u5fae\u8c03\uff08DPFT\uff09\u6a21\u5757\uff0c\u5206\u522b\u89e3\u8026\u6a21\u6001\u5171\u4eab\u4fe1\u606f\u5e76\u4f18\u5316\u8bad\u7ec3\u6bd4\u4f8b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMCULoRA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "MCULoRA\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u7279\u6027\u548c\u52a8\u6001\u8c03\u6574\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u5b8c\u6574\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002"}}
{"id": "2507.10995", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10995", "abs": "https://arxiv.org/abs/2507.10995", "authors": ["Henrik Marklund", "Alex Infanger", "Benjamin Van Roy"], "title": "Misalignment from Treating Means as Ends", "comment": null, "summary": "Reward functions, learned or manually specified, are rarely perfect. Instead\nof accurately expressing human goals, these reward functions are often\ndistorted by human beliefs about how best to achieve those goals. Specifically,\nthese reward functions often express a combination of the human's terminal\ngoals -- those which are ends in themselves -- and the human's instrumental\ngoals -- those which are means to an end. We formulate a simple example in\nwhich even slight conflation of instrumental and terminal goals results in\nsevere misalignment: optimizing the misspecified reward function results in\npoor performance when measured by the true reward function. This example\ndistills the essential properties of environments that make reinforcement\nlearning highly sensitive to conflation of instrumental and terminal goals. We\ndiscuss how this issue can arise with a common approach to reward learning and\nhow it can manifest in real environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5956\u52b1\u51fd\u6570\u4e2d\u7ec8\u7aef\u76ee\u6807\u4e0e\u5de5\u5177\u76ee\u6807\u7684\u6df7\u6dc6\u5982\u4f55\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e25\u91cd\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u5956\u52b1\u51fd\u6570\u901a\u5e38\u4e0d\u5b8c\u7f8e\uff0c\u53ef\u80fd\u6df7\u6dc6\u7ec8\u7aef\u76ee\u6807\u548c\u5de5\u5177\u76ee\u6807\uff0c\u5bfc\u81f4\u4f18\u5316\u7ed3\u679c\u4e0e\u771f\u5b9e\u76ee\u6807\u4e0d\u7b26\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u793a\u4f8b\u5c55\u793a\u6df7\u6dc6\u76ee\u6807\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u73af\u5883\u7279\u6027\u5982\u4f55\u52a0\u5267\u8fd9\u4e00\u95ee\u9898\u3002", "result": "\u5373\u4f7f\u8f7b\u5fae\u7684\u6df7\u6dc6\u4e5f\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4f18\u5316\u9519\u8bef\u5956\u52b1\u51fd\u6570\u4f1a\u504f\u79bb\u771f\u5b9e\u76ee\u6807\u3002", "conclusion": "\u9700\u8981\u66f4\u6e05\u6670\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u4ee5\u907f\u514d\u76ee\u6807\u6df7\u6dc6\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u3002"}}
{"id": "2507.11245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11245", "abs": "https://arxiv.org/abs/2507.11245", "authors": ["X. Feng", "H. Yu", "M. Wu", "S. Hu", "J. Chen", "C. Zhu", "J. Wu", "X. Chu", "K. Huang"], "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models", "comment": "Project Page: https://amap-ml.github.io/NarrLV-Website/", "summary": "With the rapid development of foundation video generation technologies, long\nvideo generation models have exhibited promising research potential thanks to\nexpanded content creation space. Recent studies reveal that the goal of long\nvideo generation tasks is not only to extend video duration but also to\naccurately express richer narrative content within longer videos. However, due\nto the lack of evaluation benchmarks specifically designed for long video\ngeneration models, the current assessment of these models primarily relies on\nbenchmarks with simple narrative prompts (e.g., VBench). To the best of our\nknowledge, our proposed NarrLV is the first benchmark to comprehensively\nevaluate the Narrative expression capabilities of Long Video generation models.\nInspired by film narrative theory, (i) we first introduce the basic narrative\nunit maintaining continuous visual presentation in videos as Temporal Narrative\nAtom (TNA), and use its count to quantitatively measure narrative richness.\nGuided by three key film narrative elements influencing TNA changes, we\nconstruct an automatic prompt generation pipeline capable of producing\nevaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based\non the three progressive levels of narrative content expression, we design an\neffective evaluation metric using the MLLM-based question generation and\nanswering framework. (iii) Finally, we conduct extensive evaluations on\nexisting long video generation models and the foundation generation models.\nExperimental results demonstrate that our metric aligns closely with human\njudgments. The derived evaluation outcomes reveal the detailed capability\nboundaries of current video generation models in narrative content expression.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u9762\u8bc4\u4f30\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u7684\u57fa\u51c6NarrLV\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u53d9\u4e8b\u539f\u5b50\uff08TNA\uff09\u548c\u81ea\u52a8\u63d0\u793a\u751f\u6210\u7ba1\u9053\uff0c\u7ed3\u5408\u591a\u7ea7\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u53d9\u4e8b\u8868\u8fbe\u8fb9\u754c\u3002", "motivation": "\u73b0\u6709\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u53d9\u4e8b\u8868\u8fbe\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5bfc\u81f4\u8bc4\u4f30\u4e0d\u5168\u9762\u3002", "method": "1. \u5f15\u5165TNA\u4f5c\u4e3a\u57fa\u672c\u53d9\u4e8b\u5355\u5143\uff1b2. \u6784\u5efa\u81ea\u52a8\u63d0\u793a\u751f\u6210\u7ba1\u9053\uff1b3. \u8bbe\u8ba1\u57fa\u4e8eMLLM\u7684\u591a\u7ea7\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNarrLV\u7684\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u53d9\u4e8b\u8868\u8fbe\u5c40\u9650\u6027\u3002", "conclusion": "NarrLV\u4e3a\u957f\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u53d9\u4e8b\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u8bc4\u4f30\u5de5\u5177\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.10998", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10998", "abs": "https://arxiv.org/abs/2507.10998", "authors": ["Zhipeng He", "Alexander Stevens", "Chun Ouyang", "Johannes De Smedt", "Alistair Barros", "Catarina Moreira"], "title": "Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data", "comment": "32 pages", "summary": "Adversarial attacks on tabular data present fundamental challenges distinct\nfrom image or text domains due to the heterogeneous nature of mixed categorical\nand numerical features. Unlike images where pixel perturbations maintain visual\nsimilarity, tabular data lacks intuitive similarity metrics, making it\ndifficult to define imperceptible modifications. Additionally, traditional\ngradient-based methods prioritise $\\ell_p$-norm constraints, often producing\nadversarial examples that deviate from the original data distributions, making\nthem detectable. We propose a latent space perturbation framework using a\nmixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial\nexamples. The proposed VAE integrates categorical embeddings and numerical\nfeatures into a unified latent manifold, enabling perturbations that preserve\nstatistical consistency. We specify In-Distribution Success Rate (IDSR) to\nmeasure the proportion of adversarial examples that remain statistically\nindistinguishable from the input distribution. Evaluation across six publicly\navailable datasets and three model architectures demonstrates that our method\nachieves substantially lower outlier rates and more consistent performance\ncompared to traditional input-space attacks and other VAE-based methods adapted\nfrom image domain approaches. Our comprehensive analysis includes\nhyperparameter sensitivity, sparsity control mechanisms, and generative\narchitectural comparisons, revealing that VAE-based attacks depend critically\non reconstruction quality but offer superior practical utility when sufficient\ntraining data is available. This work highlights the importance of on-manifold\nperturbations for realistic adversarial attacks on tabular data, offering a\nrobust approach for practical deployment. The source code can be accessed\nthrough https://github.com/ZhipengHe/VAE-TabAttack.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u8f93\u5165\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u6270\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u96be\u4ee5\u5bdf\u89c9\u7684\u5bf9\u6297\u6837\u672c\uff0c\u9002\u7528\u4e8e\u5f02\u6784\u8868\u683c\u6570\u636e\u3002", "motivation": "\u8868\u683c\u6570\u636e\u7684\u5f02\u6784\u6027\uff08\u6df7\u5408\u5206\u7c7b\u548c\u6570\u503c\u7279\u5f81\uff09\u4f7f\u5f97\u5bf9\u6297\u653b\u51fb\u96be\u4ee5\u5b9a\u4e49\u76f4\u89c2\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u4f20\u7edf\u68af\u5ea6\u65b9\u6cd5\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u6613\u504f\u79bb\u539f\u59cb\u6570\u636e\u5206\u5e03\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u8f93\u5165VAE\u5c06\u5206\u7c7b\u5d4c\u5165\u548c\u6570\u503c\u7279\u5f81\u6574\u5408\u5230\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u751f\u6210\u7edf\u8ba1\u4e00\u81f4\u7684\u5bf9\u6297\u6837\u672c\uff0c\u5e76\u63d0\u51faIDSR\u6307\u6807\u8861\u91cf\u7edf\u8ba1\u4e0d\u53ef\u533a\u5206\u6027\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e09\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u5f02\u5e38\u7387\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u8f93\u5165\u7a7a\u95f4\u653b\u51fb\u548c\u5176\u4ed6VAE\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eVAE\u7684\u6f5c\u5728\u7a7a\u95f4\u6270\u52a8\u5bf9\u8868\u683c\u6570\u636e\u7684\u5bf9\u6297\u653b\u51fb\u66f4\u5b9e\u7528\uff0c\u5f3a\u8c03\u4e86\u5728\u6d41\u5f62\u4e0a\u6270\u52a8\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.11247", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11247", "abs": "https://arxiv.org/abs/2507.11247", "authors": ["Veronika Shilova", "Emmanuel Malherbe", "Giovanni Palma", "Laurent Risser", "Jean-Michel Loubes"], "title": "Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone", "comment": null, "summary": "Within a legal framework, fairness in datasets and models is typically\nassessed by dividing observations into predefined groups and then computing\nfairness measures (e.g., Disparate Impact or Equality of Odds with respect to\ngender). However, when sensitive attributes such as skin color are continuous,\ndividing into default groups may overlook or obscure the discrimination\nexperienced by certain minority subpopulations. To address this limitation, we\npropose a fairness-based grouping approach for continuous (possibly\nmultidimensional) sensitive attributes. By grouping data according to observed\nlevels of discrimination, our method identifies the partition that maximizes a\nnovel criterion based on inter-group variance in discrimination, thereby\nisolating the most critical subgroups.\n  We validate the proposed approach using multiple synthetic datasets and\ndemonstrate its robustness under changing population distributions - revealing\nhow discrimination is manifested within the space of sensitive attributes.\nFurthermore, we examine a specialized setting of monotonic fairness for the\ncase of skin color. Our empirical results on both CelebA and FFHQ, leveraging\nthe skin tone as predicted by an industrial proprietary algorithm, show that\nthe proposed segmentation uncovers more nuanced patterns of discrimination than\npreviously reported, and that these findings remain stable across datasets for\na given model. Finally, we leverage our grouping model for debiasing purpose,\naiming at predicting fair scores with group-by-group post-processing. The\nresults demonstrate that our approach improves fairness while having minimal\nimpact on accuracy, thus confirming our partition method and opening the door\nfor industrial deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516c\u5e73\u6027\u7684\u5206\u7ec4\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u8fde\u7eed\u654f\u611f\u5c5e\u6027\uff0c\u4ee5\u63ed\u793a\u66f4\u7ec6\u5fae\u7684\u6b67\u89c6\u6a21\u5f0f\uff0c\u5e76\u7528\u4e8e\u53bb\u504f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fde\u7eed\u654f\u611f\u5c5e\u6027\uff08\u5982\u80a4\u8272\uff09\u65f6\uff0c\u53ef\u80fd\u5ffd\u7565\u5c11\u6570\u7fa4\u4f53\u7684\u6b67\u89c6\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u5206\u7ec4\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u6700\u5927\u5316\u7ec4\u95f4\u6b67\u89c6\u5dee\u5f02\u7684\u65b0\u51c6\u5219\uff0c\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u7ec4\uff0c\u8bc6\u522b\u5173\u952e\u5b50\u7fa4\u4f53\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08CelebA\u3001FFHQ\uff09\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u66f4\u7ec6\u5fae\u7684\u6b67\u89c6\u6a21\u5f0f\uff0c\u5e76\u5728\u53bb\u504f\u4e2d\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u7a33\u5b9a\u8bc6\u522b\u6b67\u89c6\u6a21\u5f0f\uff0c\u5e76\u5728\u53bb\u504f\u4e2d\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u9002\u5408\u5de5\u4e1a\u5e94\u7528\u3002"}}
{"id": "2507.11005", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11005", "abs": "https://arxiv.org/abs/2507.11005", "authors": ["Chongjie Si", "Debing Zhang", "Wei Shen"], "title": "AdaMuon: Adaptive Muon Optimizer", "comment": null, "summary": "We propose AdaMuon, an adaptive learning-rate framework built upon the\nrecently validated Muon optimizer, which has demonstrated substantial\nefficiency gains over AdamW in large-scale model training. AdaMuon augments\nMuon with two mutually dependent modules: (1) a per-parameter second-moment\nmodulation that captures orthogonal gradient updates to ensure update-level\nadaptivity, and (2) a RMS-aligned rescaling that regulates the overall update\nmagnitude by aligning it with the intrinsic structure of the parameter space.\nEmpirical results on multiple model scales and learning-rate regimes confirm\nthat AdaMuon consistently outperforms the original Muon, delivering higher\nacceleration in convergence while maintaining training stability. Our method\nintroduces no additional tuning burden and can be seamlessly integrated into\nexisting Muon training pipelines.", "AI": {"tldr": "AdaMuon\u662f\u4e00\u79cd\u57fa\u4e8eMuon\u4f18\u5316\u5668\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u6a21\u5757\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "Muon\u4f18\u5316\u5668\u5728\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u81ea\u9002\u5e94\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "method": "AdaMuon\u5f15\u5165\u4e24\u4e2a\u6a21\u5757\uff1a\u53c2\u6570\u7ea7\u4e8c\u9636\u77e9\u8c03\u5236\u548cRMS\u5bf9\u9f50\u7684\u91cd\u65b0\u7f29\u653e\uff0c\u4ee5\u589e\u5f3a\u81ea\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdaMuon\u5728\u591a\u79cd\u6a21\u578b\u89c4\u6a21\u548c\u5b66\u4e60\u7387\u4e0b\u5747\u4f18\u4e8e\u539f\u59cbMuon\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u7a33\u5b9a\u3002", "conclusion": "AdaMuon\u65e0\u9700\u989d\u5916\u8c03\u53c2\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709Muon\u8bad\u7ec3\u6d41\u7a0b\u4e2d\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u4f18\u5316\u5668\u3002"}}
{"id": "2507.11252", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11252", "abs": "https://arxiv.org/abs/2507.11252", "authors": ["Guanghao Wu", "Chen Xu", "Hai Song", "Chong Wang", "Qixing Zhang"], "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection", "comment": "18 pages, 11 figures", "summary": "Smoke is the first visible indicator of a wildfire.With the advancement of\ndeep learning, image-based smoke detection has become a crucial method for\ndetecting and preventing forest fires. However, the scarcity of smoke image\ndata from forest fires is one of the significant factors hindering the\ndetection of forest fire smoke. Image generation models offer a promising\nsolution for synthesizing realistic smoke images. However, current inpainting\nmodels exhibit limitations in generating high-quality smoke representations,\nparticularly manifesting as inconsistencies between synthesized smoke and\nbackground contexts. To solve these problems, we proposed a comprehensive\nframework for generating forest fire smoke images. Firstly, we employed the\npre-trained segmentation model and the multimodal model to obtain smoke masks\nand image captions.Then, to address the insufficient utilization of masks and\nmasked images by inpainting models, we introduced a network architecture guided\nby mask and masked image features. We also proposed a new loss function, the\nmask random difference loss, which enhances the consistency of the generated\neffects around the mask by randomly expanding and eroding the mask\nedges.Finally, to generate a smoke image dataset using random masks for\nsubsequent detection tasks, we incorporated smoke characteristics and use a\nmultimodal large language model as a filtering tool to select diverse and\nreasonable smoke images, thereby improving the quality of the synthetic\ndataset. Experiments showed that our generated smoke images are realistic and\ndiverse, and effectively enhance the performance of forest fire smoke detection\nmodels. Code is available at https://github.com/wghr123/MFGDiffusion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u68ee\u6797\u706b\u707e\u70df\u96fe\u56fe\u50cf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u4fee\u590d\u6a21\u578b\u548c\u5f15\u5165\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u70df\u96fe\u56fe\u50cf\uff0c\u63d0\u5347\u70df\u96fe\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4fee\u590d\u6a21\u578b\u5728\u751f\u6210\u70df\u96fe\u56fe\u50cf\u65f6\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u70df\u96fe\u4e0e\u80cc\u666f\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u83b7\u53d6\u70df\u96fe\u63a9\u7801\u548c\u56fe\u50cf\u63cf\u8ff0\uff0c\u63d0\u51fa\u57fa\u4e8e\u63a9\u7801\u548c\u63a9\u7801\u56fe\u50cf\u7279\u5f81\u7684\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u63a9\u7801\u968f\u673a\u5dee\u5f02\u635f\u5931\u51fd\u6570\u3002", "result": "\u751f\u6210\u7684\u70df\u96fe\u56fe\u50cf\u771f\u5b9e\u591a\u6837\uff0c\u6709\u6548\u63d0\u5347\u4e86\u70df\u96fe\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u70df\u96fe\u56fe\u50cf\uff0c\u4e3a\u68ee\u6797\u706b\u707e\u70df\u96fe\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002"}}
{"id": "2507.11012", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11012", "abs": "https://arxiv.org/abs/2507.11012", "authors": ["Dipak Dulal", "Joseph J. Charney", "Michael R. Gallagher", "Pitambar Acharya", "Carmeliza Navasca", "Nicholas S. Skowronski"], "title": "Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire", "comment": "arXiv admin note: text overlap with arXiv:2311.05128", "summary": "This study explores the potential for predicting turbulent kinetic energy\n(TKE) from more readily acquired temperature data using temperature profiles\nand turbulence data collected concurrently at 10 Hz during a small experimental\nprescribed burn in the New Jersey Pine Barrens. Machine learning models,\nincluding Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and\nGaussian Process Regressor, were employed to assess the potential to predict\nTKE from temperature perturbations and explore temporal and spatial dynamics of\ncorrelations. Data visualization and correlation analyses revealed patterns and\nrelationships between thermocouple temperatures and TKE, providing insight into\nthe underlying dynamics. More accurate predictions of TKE were achieved by\nemploying various machine learning models despite a weak correlation between\nthe predictors and the target variable. The results demonstrate significant\nsuccess, particularly from regression models, in accurately predicting the TKE.\nThe findings of this study demonstrate a novel numerical approach to\nidentifying new relationships between temperature and airflow processes in and\naround the fire environment. These relationships can help refine our\nunderstanding of combustion environment processes and the coupling and\ndecoupling of fire environment processes necessary for improving fire\noperations strategy and fire and smoke model predictions. The findings of this\nstudy additionally highlight the valuable role of machine learning techniques\nin analyzing the complex large datasets of the fire environments, showcasing\ntheir potential to advance fire research and management practices.", "AI": {"tldr": "\u5229\u7528\u6e29\u5ea6\u6570\u636e\u9884\u6d4b\u6e4d\u6d41\u52a8\u80fd\uff08TKE\uff09\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u706b\u707e\u73af\u5883\u4e2d\u63ed\u793a\u6e29\u5ea6\u4e0e\u6c14\u6d41\u7684\u65b0\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u4ece\u6613\u83b7\u53d6\u7684\u6e29\u5ea6\u6570\u636e\u9884\u6d4bTKE\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u6539\u8fdb\u706b\u707e\u73af\u5883\u4e2d\u7684\u71c3\u70e7\u8fc7\u7a0b\u7406\u89e3\u548c\u6a21\u578b\u9884\u6d4b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3001\u968f\u673a\u68ee\u6797\u56de\u5f52\u3001\u68af\u5ea6\u63d0\u5347\u548c\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790\u6e29\u5ea6\u6270\u52a8\u4e0eTKE\u7684\u5173\u7cfb\u3002", "result": "\u5c3d\u7ba1\u9884\u6d4b\u53d8\u91cf\u4e0e\u76ee\u6807\u53d8\u91cf\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ecd\u80fd\u8f83\u51c6\u786e\u5730\u9884\u6d4bTKE\uff0c\u56de\u5f52\u6a21\u578b\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u706b\u707e\u73af\u5883\u6570\u636e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u706b\u707e\u7814\u7a76\u548c\u7ba1\u7406\u7684\u8fdb\u6b65\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.11261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11261", "abs": "https://arxiv.org/abs/2507.11261", "authors": ["Ronggang Huang", "Haoxin Yang", "Yan Cai", "Xuemiao Xu", "Huaidong Zhang", "Shengfeng He"], "title": "ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition", "comment": "Accepted by ICCV 2025", "summary": "3D visual grounding aims to identify and localize objects in a 3D space based\non textual descriptions. However, existing methods struggle with disentangling\ntargets from anchors in complex multi-anchor queries and resolving\ninconsistencies in spatial descriptions caused by perspective variations. To\ntackle these challenges, we propose ViewSRD, a framework that formulates 3D\nvisual grounding as a structured multi-view decomposition process. First, the\nSimple Relation Decoupling (SRD) module restructures complex multi-anchor\nqueries into a set of targeted single-anchor statements, generating a\nstructured set of perspective-aware descriptions that clarify positional\nrelationships. These decomposed representations serve as the foundation for the\nMulti-view Textual-Scene Interaction (Multi-TSI) module, which integrates\ntextual and scene features across multiple viewpoints using shared, Cross-modal\nConsistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a\nTextual-Scene Reasoning module synthesizes multi-view predictions into a\nunified and robust 3D visual grounding. Experiments on 3D visual grounding\ndatasets show that ViewSRD significantly outperforms state-of-the-art methods,\nparticularly in complex queries requiring precise spatial differentiation.", "AI": {"tldr": "ViewSRD\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u89c6\u89d2\u5206\u89e3\u89e3\u51b33D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u590d\u6742\u67e5\u8be2\u548c\u89c6\u89d2\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u591a\u951a\u70b9\u67e5\u8be2\u548c\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7684\u7a7a\u95f4\u63cf\u8ff0\u4e0d\u4e00\u81f4\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faViewSRD\u6846\u67b6\uff0c\u5305\u62ecSimple Relation Decoupling\u6a21\u5757\u5206\u89e3\u590d\u6742\u67e5\u8be2\uff0cMulti-view Textual-Scene Interaction\u6a21\u5757\u6574\u5408\u591a\u89c6\u89d2\u7279\u5f81\uff0c\u4ee5\u53caTextual-Scene Reasoning\u6a21\u5757\u7edf\u4e00\u9884\u6d4b\u3002", "result": "\u57283D\u89c6\u89c9\u5b9a\u4f4d\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u7a7a\u95f4\u533a\u5206\u7684\u590d\u6742\u67e5\u8be2\u4e2d\u3002", "conclusion": "ViewSRD\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u89c6\u89d2\u5206\u89e3\u6709\u6548\u89e3\u51b3\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2507.11017", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11017", "abs": "https://arxiv.org/abs/2507.11017", "authors": ["Xingyu Zheng", "Haotong Qin", "Yuye Li", "Jiakai Wang", "Jinyang Guo", "Michele Magno", "Xianglong Liu"], "title": "First-Order Error Matters: Accurate Compensation for Quantized Large Language Models", "comment": null, "summary": "Post-training quantization (PTQ) offers an efficient approach to compressing\nlarge language models (LLMs), significantly reducing memory access and\ncomputational costs. Existing compensation-based weight calibration methods\noften rely on a second-order Taylor expansion to model quantization error,\nunder the assumption that the first-order term is negligible in well-trained\nfull-precision models. However, we reveal that the progressive compensation\nprocess introduces accumulated first-order deviations between latent weights\nand their full-precision counterparts, making this assumption fundamentally\nflawed. To address this, we propose FOEM, a novel PTQ method that explicitly\nincorporates first-order gradient terms to improve quantization error\ncompensation. FOEM approximates gradients by directly computing the difference\nbetween latent and full-precision weights, avoiding the high cost and limited\ngeneralization of backpropagation-based gradient computation. This approach\nintroduces minimal additional computational overhead. Moreover, FOEM leverages\nprecomputed Cholesky factors to efficiently recover the inverse of Hessian\nsubmatrices in real time. Extensive experiments across a wide range of models\nand benchmarks demonstrate that FOEM consistently outperforms the classical\nGPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of\nLlama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from\n51.7% to 74.9%, approaching the full-precision performance of 78.6%.\nFurthermore, FOEM can be seamlessly integrated with advanced techniques such as\nGPTAQ and SpinQuant, yielding additional improvements under the challenging\nW4A4KV4 setting, and further narrowing the accuracy gap with full-precision\nbaselines beyond what current state-of-the-art methods achieve. The code is\navailable at https://github.com/Xingyu-Zheng/FOEM.", "AI": {"tldr": "FOEM\u662f\u4e00\u79cd\u65b0\u7684PTQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5f15\u5165\u4e00\u9636\u68af\u5ea6\u9879\u6539\u8fdb\u91cf\u5316\u8bef\u5dee\u8865\u507f\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8865\u507f\u65b9\u6cd5\u5047\u8bbe\u4e00\u9636\u9879\u53ef\u5ffd\u7565\uff0c\u4f46\u5b9e\u9645\u7d2f\u79ef\u504f\u5dee\u5bfc\u81f4\u5047\u8bbe\u9519\u8bef\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u76f4\u63a5\u8ba1\u7b97\u6f5c\u5728\u6743\u91cd\u4e0e\u5168\u7cbe\u5ea6\u6743\u91cd\u5dee\u5f02\u8fd1\u4f3c\u68af\u5ea6\uff0c\u5229\u7528\u9884\u8ba1\u7b97Cholesky\u56e0\u5b50\u9ad8\u6548\u6062\u590dHessian\u5b50\u77e9\u9635\u9006\u3002", "result": "\u57283-bit\u91cf\u5316\u4e2d\uff0cFOEM\u663e\u8457\u964d\u4f4e\u56f0\u60d1\u5ea6\u5e76\u63d0\u5347\u51c6\u786e\u7387\uff0c\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6027\u80fd\u3002", "conclusion": "FOEM\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u4e0e\u5176\u4ed6\u5148\u8fdb\u6280\u672f\u65e0\u7f1d\u96c6\u6210\uff0c\u8fdb\u4e00\u6b65\u7f29\u5c0f\u4e0e\u5168\u7cbe\u5ea6\u57fa\u7ebf\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.11267", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11267", "abs": "https://arxiv.org/abs/2507.11267", "authors": ["Aon Safdar", "Usman Akram", "Waseem Anwar", "Basit Malik", "Mian Ibad Ali"], "title": "YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery", "comment": "Published in 25th Irish Machine Vision and Image Processing Conf.,\n  Galway, Ireland, Aug 30-Sep 1 2023 Also available at\n  https://doi.org/10.5281/zenodo.8264062", "summary": "Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared\n(TI) imagery in the defense and surveillance domain is a challenging computer\nvision (CV) task in comparison to the commercial autonomous vehicle perception\ndomain. Limited datasets, peculiar domain-specific and TI modality-specific\nchallenges, i.e., limited hardware, scale invariance issues due to greater\ndistances, deliberate occlusion by tactical vehicles, lower sensor resolution\nand resultant lack of structural information in targets, effects of weather,\ntemperature, and time of day variations, and varying target to clutter ratios\nall result in increased intra-class variability and higher inter-class\nsimilarity, making accurate real-time ATR a challenging CV task. Resultantly,\ncontemporary state-of-the-art (SOTA) deep learning architectures underperform\nin the ATR domain. We propose a modified anchor-based single-stage detector,\ncalled YOLOatr, based on a modified YOLOv5s, with optimal modifications to the\ndetection heads, feature fusion in the neck, and a custom augmentation profile.\nWe evaluate the performance of our proposed model on a comprehensive DSIAC MWIR\ndataset for real-time ATR over both correlated and decorrelated testing\nprotocols. The results demonstrate that our proposed model achieves\nstate-of-the-art ATR performance of up to 99.6%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5355\u9636\u6bb5\u68c0\u6d4b\u5668YOLOatr\uff0c\u7528\u4e8e\u70ed\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u6027\u80fd\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u70ed\u7ea2\u5916\u56fe\u50cf\u5728\u56fd\u9632\u548c\u76d1\u63a7\u9886\u57df\u7684\u81ea\u52a8\u76ee\u6807\u68c0\u6d4b\u4e0e\u8bc6\u522b\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u5982\u6570\u636e\u96c6\u6709\u9650\u3001\u786c\u4ef6\u9650\u5236\u3001\u5929\u6c14\u5f71\u54cd\u7b49\uff0c\u5bfc\u81f4\u73b0\u6709SOTA\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u57fa\u4e8e\u6539\u8fdb\u7684YOLOv5s\uff0c\u4f18\u5316\u4e86\u68c0\u6d4b\u5934\u3001\u7279\u5f81\u878d\u5408\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u5728DSIAC MWIR\u6570\u636e\u96c6\u4e0a\uff0cYOLOatr\u5b9e\u73b0\u4e8699.6%\u7684SOTA\u6027\u80fd\u3002", "conclusion": "YOLOatr\u5728\u70ed\u7ea2\u5916\u56fe\u50cf\u76ee\u6807\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.11019", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11019", "abs": "https://arxiv.org/abs/2507.11019", "authors": ["Claas Voelcker", "Axel Brunnbauer", "Marcel Hussing", "Michal Nauman", "Pieter Abbeel", "Eric Eaton", "Radu Grosu", "Amir-massoud Farahmand", "Igor Gilitschenski"], "title": "Relative Entropy Pathwise Policy Optimization", "comment": null, "summary": "Score-function policy gradients have delivered strong results in\ngame-playing, robotics and language-model fine-tuning. Yet its high-variance\noften undermines training stability. On the other hand, pathwise policy\ngradients alleviate the training variance, but are reliable only when driven by\nan accurate action-conditioned value function which is notoriously hard to\ntrain without relying on past off-policy data. In this paper, we discuss how to\nconstruct a value-gradient driven, on-policy algorithm that allow training\nQ-value models purely from on-policy data, unlocking the possibility of using\npathwise policy updates in the context of on-policy learning. We show how to\nbalance stochastic policies for exploration with constrained policy updates for\nstable training, and evaluate important architectural components that\nfacilitate accurate value function learning. Building on these insights, we\npropose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient\non-policy algorithm that combines the sample-efficiency of pathwise policy\ngradients with the simplicity and minimal memory footprint of standard\non-policy learning. We demonstrate that REPPO provides strong empirical\nperformance at decreased sample requirements, wall-clock time, memory footprint\nas well as high hyperparameter robustness in a set of experiments on two\nstandard GPU-parallelized benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ef7\u503c\u68af\u5ea6\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5REPPO\uff0c\u7ed3\u5408\u4e86\u8def\u5f84\u7b56\u7565\u68af\u5ea6\u7684\u6837\u672c\u6548\u7387\u548c\u6807\u51c6\u7b56\u7565\u5b66\u4e60\u7684\u7b80\u5355\u6027\uff0c\u964d\u4f4e\u4e86\u6837\u672c\u9700\u6c42\u548c\u65f6\u95f4\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e2d\u9ad8\u65b9\u5dee\u548c\u8def\u5f84\u7b56\u7565\u68af\u5ea6\u4f9d\u8d56\u51c6\u786e\u52a8\u4f5c\u6761\u4ef6\u4ef7\u503c\u51fd\u6570\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u4f7f\u7528\u8def\u5f84\u7b56\u7565\u66f4\u65b0\u3002", "method": "\u63d0\u51faREPPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u968f\u673a\u7b56\u7565\u63a2\u7d22\u548c\u7ea6\u675f\u7b56\u7565\u66f4\u65b0\uff0c\u7ed3\u5408\u4ef7\u503c\u51fd\u6570\u5b66\u4e60\u7684\u5173\u952e\u67b6\u6784\u7ec4\u4ef6\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cREPPO\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6027\u80fd\u3001\u66f4\u4f4e\u7684\u6837\u672c\u9700\u6c42\u3001\u65f6\u95f4\u6210\u672c\u548c\u5185\u5b58\u5360\u7528\uff0c\u4e14\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u9ad8\u3002", "conclusion": "REPPO\u6210\u529f\u7ed3\u5408\u4e86\u8def\u5f84\u7b56\u7565\u68af\u5ea6\u7684\u4f18\u52bf\u4e0e\u6807\u51c6\u7b56\u7565\u5b66\u4e60\u7684\u7b80\u5355\u6027\uff0c\u4e3a\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11279", "abs": "https://arxiv.org/abs/2507.11279", "authors": ["Yujie Zhang", "Sabine Struckmeyer", "Andreas Kolb", "Sven Reichardt"], "title": "Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping", "comment": null, "summary": "Observer bias and inconsistencies in traditional plant phenotyping methods\nlimit the accuracy and reproducibility of fine-grained plant analysis. To\novercome these challenges, we developed TomatoMAP, a comprehensive dataset for\nSolanum lycopersicum using an Internet of Things (IoT) based imaging system\nwith standardized data acquisition protocols. Our dataset contains 64,464 RGB\nimages that capture 12 different plant poses from four camera elevation angles.\nEach image includes manually annotated bounding boxes for seven regions of\ninterest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,\naxillary shoot, shoot and whole plant area, along with 50 fine-grained growth\nstage classifications based on the BBCH scale. Additionally, we provide 3,616\nhigh-resolution image subset with pixel-wise semantic and instance segmentation\nannotations for fine-grained phenotyping. We validated our dataset using a\ncascading model deep learning framework combining MobileNetv3 for\nclassification, YOLOv11 for object detection, and MaskRCNN for segmentation.\nThrough AI vs. Human analysis involving five domain experts, we demonstrate\nthat the models trained on our dataset achieve accuracy and speed comparable to\nthe experts. Cohen's Kappa and inter-rater agreement heatmap confirm the\nreliability of automated fine-grained phenotyping using our approach.", "AI": {"tldr": "TomatoMAP\u662f\u4e00\u4e2a\u57fa\u4e8eIoT\u7684\u756a\u8304\u8868\u578b\u6570\u636e\u96c6\uff0c\u5305\u542b64,464\u5f20RGB\u56fe\u50cf\u548c\u7cbe\u7ec6\u6807\u6ce8\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u690d\u7269\u8868\u578b\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u89c2\u5bdf\u8005\u504f\u5dee\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u5f00\u53d1\u4e86TomatoMAP\u6570\u636e\u96c6\uff0c\u4f7f\u7528IoT\u6210\u50cf\u7cfb\u7edf\u548c\u6807\u51c6\u5316\u534f\u8bae\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08MobileNetv3\u3001YOLOv11\u3001MaskRCNN\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u7cbe\u7ec6\u8868\u578b\u5206\u6790\u4e2d\u8fbe\u5230\u4e0e\u4e13\u5bb6\u76f8\u5f53\u7684\u51c6\u786e\u6027\u548c\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u53ef\u9760\u6027\u3002", "conclusion": "TomatoMAP\u4e3a\u690d\u7269\u8868\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6570\u636e\u548c\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.11053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11053", "abs": "https://arxiv.org/abs/2507.11053", "authors": ["Danish Gufran", "Sudeep Pasricha"], "title": "GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices", "comment": null, "summary": "Accurate indoor localization is crucial for enabling spatial context in smart\nenvironments and navigation systems. Wi-Fi Received Signal Strength (RSS)\nfingerprinting is a widely used indoor localization approach due to its\ncompatibility with mobile embedded devices. Deep Learning (DL) models improve\naccuracy in localization tasks by learning RSS variations across locations, but\nthey assume fingerprint vectors exist in a Euclidean space, failing to\nincorporate spatial relationships and the non-uniform distribution of\nreal-world RSS noise. This results in poor generalization across heterogeneous\nmobile devices, where variations in hardware and signal processing distort RSS\nreadings. Graph Neural Networks (GNNs) can improve upon conventional DL models\nby encoding indoor locations as nodes and modeling their spatial and signal\nrelationships as edges. However, GNNs struggle with non-Euclidean noise\ndistributions and suffer from the GNN blind spot problem, leading to degraded\naccuracy in environments with dense access points (APs). To address these\nchallenges, we propose GATE, a novel framework that constructs an adaptive\ngraph representation of fingerprint vectors while preserving an indoor\nstate-space topology, modeling the non-Euclidean structure of RSS noise to\nmitigate environmental noise and address device heterogeneity. GATE introduces\n1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a\nnovel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind\nspot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic\ngraph adaptation. Extensive real-world evaluations across multiple indoor\nspaces with varying path lengths, AP densities, and heterogeneous devices\ndemonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and\n1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor\nlocalization frameworks.", "AI": {"tldr": "GATE\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u8868\u793a\u548c\u65b0\u578b\u5411\u91cf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5ba4\u5185\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4e86\u8bbe\u5907\u5f02\u6784\u6027\u548c\u566a\u58f0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5ba4\u5185\u5b9a\u4f4d\u4e2d\u56e0\u5ffd\u7565\u975e\u6b27\u51e0\u91cc\u5f97\u566a\u58f0\u548c\u8bbe\u5907\u5f02\u6784\u6027\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faGATE\u6846\u67b6\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u8d85\u7a7a\u95f4\u5411\u91cf\u3001\u591a\u7ef4\u8d85\u7a7a\u95f4\u5411\u91cf\u548c\u5b9e\u65f6\u8fb9\u6784\u5efa\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u5ba4\u5185\u73af\u5883\u4e2d\uff0cGATE\u7684\u5b9a\u4f4d\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GATE\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5185\u5b9a\u4f4d\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.11063", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11063", "abs": "https://arxiv.org/abs/2507.11063", "authors": ["Gwen Maudet", "Gr\u00e9goire Danoy"], "title": "A Distance Metric for Mixed Integer Programming Instances", "comment": "Accepted to ECAI 2025", "summary": "Mixed-integer linear programming (MILP) is a powerful tool for addressing a\nwide range of real-world problems, but it lacks a clear structure for comparing\ninstances. A reliable similarity metric could establish meaningful\nrelationships between instances, enabling more effective evaluation of instance\nset heterogeneity and providing better guidance to solvers, particularly when\nmachine learning is involved. Existing similarity metrics often lack precision\nin identifying instance classes or rely heavily on labeled data, which limits\ntheir applicability and generalization. To bridge this gap, this paper\nintroduces the first mathematical distance metric for MILP instances, derived\ndirectly from their mathematical formulations. By discretizing right-hand\nsides, weights, and variables into classes, the proposed metric draws\ninspiration from the Earth mover's distance to quantify mismatches in\nweight-variable distributions for constraint comparisons. This approach\nnaturally extends to enable instance-level comparisons. We evaluate both an\nexact and a greedy variant of our metric under various parameter settings,\nusing the StrIPLIB dataset. Results show that all components of the metric\ncontribute to class identification, and that the greedy version achieves\naccuracy nearly identical to the exact formulation while being nearly 200 times\nfaster. Compared to state-of-the-art baselines, including feature-based,\nimage-based, and neural network models, our unsupervised method consistently\noutperforms all non-learned approaches and rivals the performance of a\nsupervised classifier on class and subclass grouping tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b66\u516c\u5f0f\u7684MILP\u5b9e\u4f8b\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u6bd4\u8f83\u5b9e\u4f8b\u76f8\u4f3c\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709MILP\u5b9e\u4f8b\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\u7f3a\u4e4f\u7cbe\u786e\u6027\u6216\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u79bb\u6563\u5316\u53f3\u4fa7\u503c\u3001\u6743\u91cd\u548c\u53d8\u91cf\uff0c\u501f\u9274Earth mover's distance\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u5728StrIPLIB\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8d2a\u5a6a\u7248\u672c\u63a5\u8fd1\u7cbe\u786e\u7248\u672c\u7684\u51c6\u786e\u6027\u4e14\u5feb200\u500d\uff0c\u4f18\u4e8e\u975e\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u60c5\u51b5\u4e0b\u4f18\u4e8e\u975e\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u4e0e\u76d1\u7763\u5206\u7c7b\u5668\u6027\u80fd\u76f8\u5f53\u3002"}}
{"id": "2507.11293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11293", "abs": "https://arxiv.org/abs/2507.11293", "authors": ["J. Senthilnath", "Chen Hao", "F. C. Wellstood"], "title": "3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images", "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "In semiconductor packaging, accurately recovering 3D information is crucial\nfor non-destructive testing (NDT) to localize circuit defects. This paper\npresents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),\nwhich leverages Magnetic Field Images (MFI) to retrieve the parameters for the\n3D current flow of a single-segment. The 3D MIR integrates a deep learning\n(DL)-based Convolutional Neural Network (CNN), spatial-physics-based\nconstraints, and optimization techniques. The method operates in three stages:\ni) The CNN model processes the MFI data to predict ($\\ell/z_o$), where $\\ell$\nis the wire length and $z_o$ is the wire's vertical depth beneath the magnetic\nsensors and classify segment type ($c$). ii) By leveraging\nspatial-physics-based constraints, the routine provides initial estimates for\nthe position ($x_o$, $y_o$, $z_o$), length ($\\ell$), current ($I$), and current\nflow direction (positive or negative) of the current segment. iii) An optimizer\nthen adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\\ell$, $I$) to\nminimize the difference between the reconstructed MFI and the actual MFI. The\nresults demonstrate that the 3D MIR method accurately recovers 3D information\nwith high precision, setting a new benchmark for magnetic image reconstruction\nin semiconductor packaging. This method highlights the potential of combining\nDL and physics-driven optimization in practical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3D MIR\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u7269\u7406\u7ea6\u675f\uff0c\u7528\u4e8e\u534a\u5bfc\u4f53\u5c01\u88c5\u4e2d\u76843D\u7535\u6d41\u53c2\u6570\u6062\u590d\u3002", "motivation": "\u5728\u534a\u5bfc\u4f53\u5c01\u88c5\u4e2d\uff0c\u51c6\u786e\u6062\u590d3D\u4fe1\u606f\u5bf9\u65e0\u635f\u68c0\u6d4b\u548c\u7535\u8def\u7f3a\u9677\u5b9a\u4f4d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u4e09\u9636\u6bb5\uff1a1) CNN\u5904\u7406\u78c1\u56fe\u50cf\u9884\u6d4b\u53c2\u6570\uff1b2) \u7269\u7406\u7ea6\u675f\u63d0\u4f9b\u521d\u59cb\u4f30\u8ba1\uff1b3) \u4f18\u5316\u5668\u8c03\u6574\u53c2\u6570\u4ee5\u6700\u5c0f\u5316\u8bef\u5dee\u3002", "result": "3D MIR\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u6062\u590d3D\u4fe1\u606f\uff0c\u4e3a\u78c1\u56fe\u50cf\u91cd\u5efa\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u7269\u7406\u9a71\u52a8\u4f18\u5316\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.11071", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11071", "abs": "https://arxiv.org/abs/2507.11071", "authors": ["Isaiah Thompson Ocansey", "Ritwik Bhattacharya", "Tanmay Sen"], "title": "LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection", "comment": null, "summary": "Log anomaly detection using traditional rule based or deep learning based\nmethods is often challenging due to the large volume and highly complex nature\nof log sequence. So effective way of detection of anomalous sequence of logs is\ncrucial for system maintenance and development. This paper proposes parameter\nefficient finetuning specifically low rank adaptation (LoRA) and adapter based\napproaches for finding contextual anomalies in sequence of logs in large log\ndata set. It compares different tiny large language models (LLMs) on the\nThunderbird dataset. The results show that LoRA based finetuning provides\nsubstantial performance improvements of 18 to 19 percentage over LogBert based\nfull finetuning approach, achieving accuracy scores between 97.76% and 98.83%\ncompared to 79.37%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLoRA\u548c\u9002\u914d\u5668\u7684\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u89c4\u6a21\u65e5\u5fd7\u6570\u636e\u4e2d\u7684\u5f02\u5e38\u5e8f\u5217\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c4\u5219\u6216\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u9762\u4e34\u6570\u636e\u91cf\u5927\u548c\u590d\u6742\u6027\u9ad8\u7684\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff08\u5982LoRA\u548c\u9002\u914d\u5668\uff09\uff0c\u5e76\u5728Thunderbird\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e0d\u540c\u5c0f\u578bLLMs\u7684\u6027\u80fd\u3002", "result": "LoRA\u5fae\u8c03\u6bd4LogBert\u5168\u5fae\u8c03\u65b9\u6cd5\u6027\u80fd\u63d0\u534718-19%\uff0c\u51c6\u786e\u7387\u8fbe\u523097.76%-98.83%\uff0c\u800c\u540e\u8005\u4ec5\u4e3a79.37%\u3002", "conclusion": "LoRA\u5fae\u8c03\u5728\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u7cfb\u7edf\u7ef4\u62a4\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11301", "abs": "https://arxiv.org/abs/2507.11301", "authors": ["Pa\u00fal Maji", "Marlon T\u00faquerres", "Stalin Valencia", "Marcela Valenzuela", "Christian Mejia-Escobar"], "title": "Detecci\u00f3n y Cuantificaci\u00f3n de Erosi\u00f3n Fluvial con Visi\u00f3n Artificial", "comment": "18 pages, in Spanish language, 13 figures, 4 tables", "summary": "Fluvial erosion is a natural process that can generate significant impacts on\nsoil stability and strategic infrastructures. The detection and monitoring of\nthis phenomenon is traditionally addressed by photogrammetric methods and\nanalysis in geographic information systems. These tasks require specific\nknowledge and intensive manual processing. This study proposes an artificial\nintelligence-based approach for automatic identification of eroded zones and\nestimation of their area. The state-of-the-art computer vision model YOLOv11,\nadjusted by fine-tuning and trained with photographs and LiDAR images, is used.\nThis combined dataset was segmented and labeled using the Roboflow platform.\nExperimental results indicate efficient detection of erosion patterns with an\naccuracy of 70%, precise identification of eroded areas and reliable\ncalculation of their extent in pixels and square meters. As a final product,\nthe EROSCAN system has been developed, an interactive web application that\nallows users to upload images and obtain automatic segmentations of fluvial\nerosion, together with the estimated area. This tool optimizes the detection\nand quantification of the phenomenon, facilitating decision making in risk\nmanagement and territorial planning.", "AI": {"tldr": "\u4f7f\u7528YOLOv11\u6a21\u578b\u548cLiDAR\u56fe\u50cf\u81ea\u52a8\u68c0\u6d4b\u6cb3\u6d41\u4fb5\u8680\u533a\u57df\uff0c\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u7f51\u7edc\u5e94\u7528EROSCAN\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u4e14\u5904\u7406\u7e41\u7410\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8c03\u6574YOLOv11\u6a21\u578b\uff0c\u7ed3\u5408\u7167\u7247\u548cLiDAR\u56fe\u50cf\u8bad\u7ec3\uff0c\u4f7f\u7528Roboflow\u5e73\u53f0\u6807\u6ce8\u6570\u636e\u3002", "result": "\u68c0\u6d4b\u51c6\u786e\u738770%\uff0c\u80fd\u7cbe\u786e\u8ba1\u7b97\u4fb5\u8680\u533a\u57df\u9762\u79ef\u3002", "conclusion": "EROSCAN\u7cfb\u7edf\u4f18\u5316\u4e86\u4fb5\u8680\u68c0\u6d4b\u548c\u91cf\u5316\uff0c\u652f\u6301\u98ce\u9669\u7ba1\u7406\u51b3\u7b56\u3002"}}
{"id": "2507.11173", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11173", "abs": "https://arxiv.org/abs/2507.11173", "authors": ["Deepak Kumar Panda", "Weisi Guo"], "title": "Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction", "comment": null, "summary": "Autonomous unmanned aerial vehicles (UAVs) rely on global navigation\nsatellite system (GNSS) pseudorange measurements for accurate real-time\nlocalization and navigation. However, this dependence exposes them to\nsophisticated spoofing threats, where adversaries manipulate pseudoranges to\ndeceive UAV receivers. Among these, drift-evasive spoofing attacks subtly\nperturb measurements, gradually diverting the UAVs trajectory without\ntriggering conventional signal-level anti-spoofing mechanisms. Traditional\ndistributional shift detection techniques often require accumulating a\nthreshold number of samples, causing delays that impede rapid detection and\ntimely response. Consequently, robust temporal-scale detection methods are\nessential to identify attack onset and enable contingency planning with\nalternative sensing modalities, improving resilience against stealthy\nadversarial manipulations. This study explores a Bayesian online change point\ndetection (BOCPD) approach that monitors temporal shifts in value estimates\nfrom a reinforcement learning (RL) critic network to detect subtle behavioural\ndeviations in UAV navigation. Experimental results show that this temporal\nvalue-based framework outperforms conventional GNSS spoofing detectors,\ntemporal semi-supervised learning frameworks, and the Page-Hinkley test,\nachieving higher detection accuracy and lower false-positive and false-negative\nrates for drift-evasive spoofing attacks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u5728\u7ebf\u53d8\u5316\u70b9\u68c0\u6d4b\uff08BOCPD\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u6d4b\u5f3a\u5316\u5b66\u4e60\u6279\u8bc4\u7f51\u7edc\u7684\u65f6\u5e8f\u53d8\u5316\u6765\u68c0\u6d4b\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u7684\u7ec6\u5fae\u884c\u4e3a\u504f\u5dee\uff0c\u4ee5\u5e94\u5bf9\u6f02\u79fb\u89c4\u907f\u6b3a\u9a97\u653b\u51fb\u3002", "motivation": "\u65e0\u4eba\u673a\u4f9d\u8d56GNSS\u4f2a\u8ddd\u6d4b\u91cf\u8fdb\u884c\u5b9a\u4f4d\u548c\u5bfc\u822a\uff0c\u4f46\u6613\u53d7\u6b3a\u9a97\u653b\u51fb\uff0c\u5c24\u5176\u662f\u6f02\u79fb\u89c4\u907f\u653b\u51fb\uff0c\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u56e0\u5ef6\u8fdf\u95ee\u9898\u96be\u4ee5\u5feb\u901f\u54cd\u5e94\u3002", "method": "\u91c7\u7528BOCPD\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6279\u8bc4\u7f51\u7edc\u7684\u65f6\u5e8f\u4ef7\u503c\u4f30\u8ba1\uff0c\u68c0\u6d4b\u653b\u51fb\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u3001\u5047\u9633\u6027\u548c\u5047\u9634\u6027\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edfGNSS\u6b3a\u9a97\u68c0\u6d4b\u5668\u548c\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u3002", "conclusion": "\u8be5\u65f6\u5e8f\u4ef7\u503c\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u65e0\u4eba\u673a\u5bf9\u6f02\u79fb\u89c4\u907f\u6b3a\u9a97\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u589e\u5f3a\u7cfb\u7edf\u97e7\u6027\u3002"}}
{"id": "2507.11321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11321", "abs": "https://arxiv.org/abs/2507.11321", "authors": ["Haoxuan Qu", "Yujun Cai", "Hossein Rahmani", "Ajay Kumar", "Junsong Yuan", "Jun Liu"], "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction", "comment": null, "summary": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface\nreconstruction. However, while 3D objects can be of complex and diverse shapes\nin the real world, existing GS-based methods only limitedly use a single type\nof splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent\nobject surfaces during their reconstruction. In this paper, we highlight that\nthis can be insufficient for object surfaces to be represented in high quality.\nThus, we propose a novel framework that, for the first time, enables Gaussian\nSplatting to incorporate multiple types of (geometrical) primitives during its\nsurface reconstruction process. Specifically, in our framework, we first\npropose a compositional splatting strategy, enabling the splatting and\nrendering of different types of primitives in the Gaussian Splatting pipeline.\nIn addition, we also design our framework with a mixed-primitive-based\ninitialization strategy and a vertex pruning mechanism to further promote its\nsurface representation learning process to be well executed leveraging\ndifferent types of primitives. Extensive experiments show the efficacy of our\nframework and its accurate surface reconstruction performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\uff0c\u9996\u6b21\u5f15\u5165\u591a\u79cd\u51e0\u4f55\u57fa\u5143\u4ee5\u63d0\u5347\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u4ec5\u4f7f\u7528\u5355\u4e00\u57fa\u5143\uff08\u692d\u5706\u6216\u692d\u7403\uff09\u8868\u793a\u590d\u6742\u7269\u4f53\u8868\u9762\uff0c\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cd\u5efa\u3002", "method": "\u63d0\u51fa\u7ec4\u5408\u6cfc\u6e85\u7b56\u7565\u3001\u6df7\u5408\u57fa\u5143\u521d\u59cb\u5316\u7b56\u7565\u548c\u9876\u70b9\u4fee\u526a\u673a\u5236\uff0c\u652f\u6301\u591a\u79cd\u57fa\u5143\u7684\u9ad8\u65af\u6cfc\u6e85\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u8868\u9762\u91cd\u5efa\u3002", "conclusion": "\u591a\u57fa\u5143\u9ad8\u65af\u6cfc\u6e85\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8868\u9762\u91cd\u5efa\u8d28\u91cf\u3002"}}
{"id": "2507.11178", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11178", "abs": "https://arxiv.org/abs/2507.11178", "authors": ["Meiliang Liu", "Huiwen Dong", "Xiaoxiao Yang", "Yunfang Xu", "Zijin Li", "Zhengye Si", "Xinyue Yang", "Zhiwen Zhao"], "title": "Gradient Regularization-based Neural Granger Causality", "comment": "9 pages,3 figures, conference", "summary": "With the advancement of deep learning technologies, various neural\nnetwork-based Granger causality models have been proposed. Although these\nmodels have demonstrated notable improvements, several limitations remain. Most\nexisting approaches adopt the component-wise architecture, necessitating the\nconstruction of a separate model for each time series, which results in\nsubstantial computational costs. In addition, imposing the sparsity-inducing\npenalty on the first-layer weights of the neural network to extract causal\nrelationships weakens the model's ability to capture complex interactions. To\naddress these limitations, we propose Gradient Regularization-based Neural\nGranger Causality (GRNGC), which requires only one time series prediction model\nand applies $L_{1}$ regularization to the gradient between model's input and\noutput to infer Granger causality. Moreover, GRNGC is not tied to a specific\ntime series forecasting model and can be implemented with diverse architectures\nsuch as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical\nsimulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC\noutperforms existing baselines and significantly reduces computational\noverhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder\nurothelial carcinoma datasets further validate the model's effectiveness in\nreconstructing gene regulatory networks.", "AI": {"tldr": "GRNGC\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u6b63\u5219\u5316\u7684\u795e\u7ecf\u683c\u5170\u6770\u56e0\u679c\u6a21\u578b\uff0c\u901a\u8fc7\u5355\u4e00\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u548cL1\u6b63\u5219\u5316\u68af\u5ea6\u6765\u63a8\u65ad\u56e0\u679c\u5173\u7cfb\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u683c\u5170\u6770\u56e0\u679c\u6a21\u578b\u9700\u8981\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u6784\u5efa\u5355\u72ec\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14\u7a00\u758f\u6027\u60e9\u7f5a\u524a\u5f31\u4e86\u6355\u6349\u590d\u6742\u4ea4\u4e92\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faGRNGC\uff0c\u4ec5\u9700\u4e00\u4e2a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u5bf9\u8f93\u5165\u8f93\u51fa\u68af\u5ea6\u65bd\u52a0L1\u6b63\u5219\u5316\uff0c\u652f\u6301\u591a\u79cd\u67b6\u6784\uff08\u5982KAN\u3001MLP\u3001LSTM\uff09\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5982DREAM\u3001fMRI BOLD\u3001\u57fa\u56e0\u6570\u636e\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "GRNGC\u9ad8\u6548\u7075\u6d3b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u5728\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u91cd\u5efa\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.11325", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11325", "abs": "https://arxiv.org/abs/2507.11325", "authors": ["Arefin Ittesafun Abian", "Ripon Kumar Debnath", "Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Md Rafiqul Islam", "Asif Karim", "Reem E. Mohamed", "Sami Azam"], "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging", "comment": "10 figures. Will be submitted to IEEE Transactions on Radiation and\n  Plasma Medical Sciences", "summary": "Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the\n3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of\n1.525 mm, and VOE of 19.71%, indicating strong generalization across different\ndatasets. These results confirm the effectiveness and robustness of HANS-Net in\nproviding anatomically consistent, accurate, and confident liver and tumor\nsegmentation.", "AI": {"tldr": "HANS-Net\u662f\u4e00\u79cd\u65b0\u578b\u5206\u5272\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u6280\u672f\u63d0\u5347\u809d\u810f\u548c\u80bf\u7624CT\u56fe\u50cf\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u8179\u90e8CT\u56fe\u50cf\u4e2d\u809d\u810f\u548c\u80bf\u7624\u7684\u51c6\u786e\u5206\u5272\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u590d\u6742\u89e3\u5256\u7ed3\u6784\u3001\u80bf\u7624\u5916\u89c2\u591a\u53d8\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u7b49\u6311\u6218\u3002", "method": "HANS-Net\u7ed3\u5408\u53cc\u66f2\u5377\u79ef\u3001\u5c0f\u6ce2\u5206\u89e3\u6a21\u5757\u3001\u7a81\u89e6\u53ef\u5851\u6027\u673a\u5236\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u6ce8\u610f\u529b\u3002", "result": "\u5728LiTS\u6570\u636e\u96c6\u4e0a\u8fbe\u523093.26% Dice\u5206\u6570\uff0c\u8de8\u6570\u636e\u96c6\u9a8c\u8bc1\u4e5f\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HANS-Net\u80fd\u63d0\u4f9b\u89e3\u5256\u4e00\u81f4\u3001\u51c6\u786e\u4e14\u53ef\u9760\u7684\u5206\u5272\u7ed3\u679c\uff0c\u8bc1\u5b9e\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.11181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11181", "abs": "https://arxiv.org/abs/2507.11181", "authors": ["Danyang Zhang", "Junhao Song", "Ziqian Bi", "Yingfang Yuan", "Tianyang Wang", "Joe Yeong", "Junfeng Hao"], "title": "Mixture of Experts in Large Language Models", "comment": null, "summary": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE)\narchitecture in large language models, highlighting its ability to\nsignificantly enhance model performance while maintaining minimal computational\noverhead. Through a systematic analysis spanning theoretical foundations, core\narchitectural designs, and large language model (LLM) applications, we examine\nexpert gating and routing mechanisms, hierarchical and sparse MoE\nconfigurations, meta-learning approaches, multimodal and multitask learning\nscenarios, real-world deployment cases, and recent advances and challenges in\ndeep learning. Our analysis identifies key advantages of MoE, including\nsuperior model capacity compared to equivalent Bayesian approaches, improved\ntask-specific performance, and the ability to scale model capacity efficiently.\nWe also underscore the importance of ensuring expert diversity, accurate\ncalibration, and reliable inference aggregation, as these are essential for\nmaximizing the effectiveness of MoE architectures. Finally, this review\noutlines current research limitations, open challenges, and promising future\ndirections, providing a foundation for continued innovation in MoE architecture\nand its applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u5176\u5728\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8MoE\u67b6\u6784\u7684\u7406\u8bba\u57fa\u7840\u3001\u6838\u5fc3\u8bbe\u8ba1\u53ca\u5b9e\u9645\u5e94\u7528\uff0c\u4ee5\u63a8\u52a8\u5176\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u4e13\u5bb6\u95e8\u63a7\u4e0e\u8def\u7531\u673a\u5236\u3001\u5206\u5c42\u4e0e\u7a00\u758fMoE\u914d\u7f6e\u3001\u5143\u5b66\u4e60\u65b9\u6cd5\u3001\u591a\u6a21\u6001\u4e0e\u591a\u4efb\u52a1\u5b66\u4e60\u573a\u666f\u7b49\u3002", "result": "MoE\u67b6\u6784\u5728\u6a21\u578b\u5bb9\u91cf\u3001\u4efb\u52a1\u6027\u80fd\u53ca\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u5173\u6ce8\u4e13\u5bb6\u591a\u6837\u6027\u3001\u6821\u51c6\u7cbe\u5ea6\u548c\u63a8\u7406\u805a\u5408\u53ef\u9760\u6027\u3002", "conclusion": "MoE\u67b6\u6784\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5f53\u524d\u7814\u7a76\u5c40\u9650\u548c\u6311\u6218\uff0c\u4e3a\u672a\u6765\u521b\u65b0\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2507.11333", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11333", "abs": "https://arxiv.org/abs/2507.11333", "authors": ["Jianfei Jiang", "Qiankun Liu", "Haochen Yu", "Hongyuan Liu", "Liyong Wang", "Jiansheng Chen", "Huimin Ma"], "title": "MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network", "comment": "Accepted by ICCV 2025", "summary": "Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for\na sequence of calibrated images to recover dense point clouds. However,\nexisting MVS methods often struggle with challenging regions, such as\ntextureless regions and reflective surfaces, where feature matching fails. In\ncontrast, monocular depth estimation inherently does not require feature\nmatching, allowing it to achieve robust relative depth estimation in these\nregions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature\nand depth guided MVS network that integrates powerful priors from a monocular\nfoundation model into multi-view geometry. Firstly, the monocular feature of\nthe reference view is integrated into source view features by the attention\nmechanism with a newly designed cross-view position encoding. Then, the\nmonocular depth of the reference view is aligned to dynamically update the\ndepth candidates for edge regions during the sampling procedure. Finally, a\nrelative consistency loss is further designed based on the monocular depth to\nsupervise the depth prediction. Extensive experiments demonstrate that\nMonoMVSNet achieves state-of-the-art performance on the DTU and\nTanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate\nand Advanced benchmarks. The source code is available at\nhttps://github.com/JianfeiJ/MonoMVSNet.", "AI": {"tldr": "MonoMVSNet\u7ed3\u5408\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e0e\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u6df1\u5ea6\u5019\u9009\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7eb9\u7406\u7f3a\u5931\u548c\u53cd\u5c04\u533a\u57df\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MVS\u65b9\u6cd5\u5728\u7eb9\u7406\u7f3a\u5931\u548c\u53cd\u5c04\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5728\u8fd9\u4e9b\u533a\u57df\u5177\u6709\u4f18\u52bf\u3002", "method": "\u63d0\u51faMonoMVSNet\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u5355\u76ee\u7279\u5f81\uff0c\u52a8\u6001\u66f4\u65b0\u6df1\u5ea6\u5019\u9009\uff0c\u5e76\u8bbe\u8ba1\u76f8\u5bf9\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728DTU\u548cTanks-and-Temples\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0c\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "MonoMVSNet\u6709\u6548\u7ed3\u5408\u5355\u76ee\u4e0e\u591a\u89c6\u89d2\u4f18\u52bf\uff0c\u663e\u8457\u63d0\u5347MVS\u6027\u80fd\u3002"}}
{"id": "2507.11183", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11183", "abs": "https://arxiv.org/abs/2507.11183", "authors": ["Dimitrios Kritsiolis", "Constantine Kotropoulos"], "title": "Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications", "comment": "In Proceedings of the 2025 IARIA Annual Congress on Frontiers in\n  Science, Technology, Services, and Applications (IARIA Congress 2025),\n  Venice, Italy, July 6-10, 2025", "summary": "Federated learning is a machine learning approach that enables multiple\ndevices (i.e., agents) to train a shared model cooperatively without exchanging\nraw data. This technique keeps data localized on user devices, ensuring privacy\nand security, while each agent trains the model on their own data and only\nshares model updates. The communication overhead is a significant challenge due\nto the frequent exchange of model updates between the agents and the central\nserver. In this paper, we propose a communication-efficient federated learning\nscheme that utilizes low-rank approximation of neural network gradients and\nquantization to significantly reduce the network load of the decentralized\nlearning process with minimal impact on the model's accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6848\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u91cf\u5316\u51cf\u5c11\u7f51\u7edc\u8d1f\u8f7d\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u9891\u7e41\u7684\u6a21\u578b\u66f4\u65b0\u4ea4\u6362\u5bfc\u81f4\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u5f71\u54cd\u6548\u7387\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7f51\u7edc\u68af\u5ea6\u7684\u4f4e\u79e9\u8fd1\u4f3c\u548c\u91cf\u5316\u6280\u672f\u3002", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u7f51\u7edc\u8d1f\u8f7d\uff0c\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u901a\u4fe1\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2507.11336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11336", "abs": "https://arxiv.org/abs/2507.11336", "authors": ["Peiran Wu", "Yunze Liu", "Zhengdong Zhu", "Enmin Zhou", "Shawn Shen"], "title": "UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks", "comment": null, "summary": "Real-world user-generated videos, especially on platforms like TikTok, often\nfeature rich and intertwined audio visual content. However, existing video\ncaptioning benchmarks and models remain predominantly visual centric,\noverlooking the crucial role of audio in conveying scene dynamics, speaker\nintent, and narrative context. This lack of omni datasets and lightweight,\ncapable models hampers progress in fine grained, multimodal video\nunderstanding. To address these challenges, we introduce UGC-VideoCap, a new\nbenchmark and model framework specifically designed for detailed omnimodal\ncaptioning of short form user-generated videos. Unlike prior datasets,\nUGC-VideoCap emphasizes balanced integration of audio and visual modalities,\nfeaturing 1000 TikTok videos annotated through a structured three stage\nhuman-in-the-loop pipeline covering audio only, visual only, and joint audio\nvisual semantics. The benchmark also includes 4000 carefully crafted QA pairs\nprobing both unimodal and cross modal understanding. Alongside the dataset, we\npropose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from\nGemini 2.5 Flash. Using a novel two-stage training strategy supervised fine\ntuning followed by Group Relative Policy Optimization (GRPO), our approach\nenables efficient adaptation from limited data while maintaining competitive\nperformance. Together, our benchmark and model offer a high-quality foundation\nand a data-efficient solution for advancing omnimodal video captioning in\nunconstrained real-world UGC settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86UGC-VideoCap\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u97f3\u9891\u548c\u89c6\u89c9\u5e73\u8861\u6574\u5408\u7684\u77ed\u89c6\u9891\u5b57\u5e55\u6570\u636e\u96c6\u548c\u6a21\u578b\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u6280\u672f\u5ffd\u89c6\u97f3\u9891\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5b57\u5e55\u6280\u672f\u8fc7\u4e8e\u89c6\u89c9\u4e2d\u5fc3\u5316\uff0c\u5ffd\u89c6\u4e86\u97f3\u9891\u5728\u4f20\u8fbe\u573a\u666f\u52a8\u6001\u548c\u53d9\u4e8b\u80cc\u666f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u4eba\u5de5\u6807\u6ce8\u6d41\u7a0b\u521b\u5efa\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a3B\u53c2\u6570\u7684\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08\u76d1\u7763\u5fae\u8c03\u548cGRPO\uff09\u3002", "result": "UGC-VideoCap\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u548c\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u6a21\u578b\uff0c\u652f\u6301\u591a\u6a21\u6001\u89c6\u9891\u5b57\u5e55\u7684\u8fdb\u6b65\u3002", "conclusion": "UGC-VideoCap\u4e3a\u771f\u5b9e\u4e16\u754c\u7528\u6237\u751f\u6210\u89c6\u9891\u7684\u591a\u6a21\u6001\u5b57\u5e55\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u57fa\u7840\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11185", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11185", "abs": "https://arxiv.org/abs/2507.11185", "authors": ["Md. Emon Akter Sourov", "Md. Sabbir Hossen", "Pabon Shaha", "Mohammad Minoar Hossain", "Md Sadiq Iqbal"], "title": "An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment", "comment": "This paper has been accepted at the IEEE QPAIN 2025. The final\n  version will be available in the IEEE Xplore Digital Library", "summary": "Heart disease remains a major global health concern, particularly in regions\nwith limited access to medical resources and diagnostic facilities. Traditional\ndiagnostic methods often fail to accurately identify and manage heart disease\nrisks, leading to adverse outcomes. Machine learning has the potential to\nsignificantly enhance the accuracy, efficiency, and speed of heart disease\ndiagnosis. In this study, we proposed a comprehensive framework that combines\nclassification models for heart disease detection and regression models for\nrisk prediction. We employed the Heart Disease dataset, which comprises 1,035\ncases. To address the issue of class imbalance, the Synthetic Minority\nOversampling Technique (SMOTE) was applied, resulting in the generation of an\nadditional 100,000 synthetic data points. Performance metrics, including\naccuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to\nevaluate the model's effectiveness. Among the classification models, Random\nForest emerged as the standout performer, achieving an accuracy of 97.2% on\nreal data and 97.6% on synthetic data. For regression tasks, Linear Regression\ndemonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic\ndatasets, respectively, with the lowest error metrics. Additionally,\nExplainable AI techniques were employed to enhance the interpretability of the\nmodels. This study highlights the potential of machine learning to\nrevolutionize heart disease diagnosis and risk prediction, thereby facilitating\nearly intervention and enhancing clinical decision-making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u7c7b\u548c\u56de\u5f52\u6a21\u578b\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5fc3\u810f\u75c5\u8bca\u65ad\u548c\u98ce\u9669\u9884\u6d4b\uff0c\u4f7f\u7528SMOTE\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u968f\u673a\u68ee\u6797\u548c\u7ebf\u6027\u56de\u5f52\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f20\u7edf\u5fc3\u810f\u75c5\u8bca\u65ad\u65b9\u6cd5\u5728\u8d44\u6e90\u6709\u9650\u5730\u533a\u6548\u679c\u4e0d\u4f73\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u63d0\u9ad8\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u901f\u5ea6\u3002", "method": "\u4f7f\u7528\u5305\u542b1,035\u4e2a\u75c5\u4f8b\u7684\u5fc3\u810f\u75c5\u6570\u636e\u96c6\uff0c\u5e94\u7528SMOTE\u751f\u621010\u4e07\u5408\u6210\u6570\u636e\u70b9\uff0c\u8bc4\u4f30\u591a\u79cd\u6027\u80fd\u6307\u6807\u3002", "result": "\u968f\u673a\u68ee\u6797\u5206\u7c7b\u51c6\u786e\u7387\u8fbe97.2%\uff08\u771f\u5b9e\u6570\u636e\uff09\u548c97.6%\uff08\u5408\u6210\u6570\u636e\uff09\uff0c\u7ebf\u6027\u56de\u5f52R2\u503c\u6700\u9ad8\uff080.992\u548c0.984\uff09\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u80fd\u663e\u8457\u6539\u5584\u5fc3\u810f\u75c5\u8bca\u65ad\u548c\u98ce\u9669\u9884\u6d4b\uff0c\u652f\u6301\u65e9\u671f\u5e72\u9884\u548c\u4e34\u5e8a\u51b3\u7b56\u3002"}}
{"id": "2507.11372", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11372", "abs": "https://arxiv.org/abs/2507.11372", "authors": ["Pierrick Leroy", "Antonio Mastropietro", "Marco Nurisso", "Francesco Vaccarino"], "title": "Attributes Shape the Embedding Space of Face Recognition Models", "comment": null, "summary": "Face Recognition (FR) tasks have made significant progress with the advent of\nDeep Neural Networks, particularly through margin-based triplet losses that\nembed facial images into high-dimensional feature spaces. During training,\nthese contrastive losses focus exclusively on identity information as labels.\nHowever, we observe a multiscale geometric structure emerging in the embedding\nspace, influenced by interpretable facial (e.g., hair color) and image\nattributes (e.g., contrast). We propose a geometric approach to describe the\ndependence or invariance of FR models to these attributes and introduce a\nphysics-inspired alignment metric. We evaluate the proposed metric on\ncontrolled, simplified models and widely used FR models fine-tuned with\nsynthetic data for targeted attribute augmentation. Our findings reveal that\nthe models exhibit varying degrees of invariance across different attributes,\nproviding insight into their strengths and weaknesses and enabling deeper\ninterpretability. Code available here:\nhttps://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\uff0c\u7528\u4e8e\u63cf\u8ff0\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u5bf9\u53ef\u89e3\u91ca\u5c5e\u6027\u7684\u4f9d\u8d56\u6027\u6216\u4e0d\u53d8\u6027\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u7269\u7406\u542f\u53d1\u7684\u5bf9\u9f50\u5ea6\u91cf\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b58\u5728\u591a\u5c3a\u5ea6\u51e0\u4f55\u7ed3\u6784\uff0c\u53d7\u53ef\u89e3\u91ca\u7684\u9762\u90e8\u548c\u56fe\u50cf\u5c5e\u6027\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u91cf\u5316\u6a21\u578b\u5bf9\u8fd9\u4e9b\u5c5e\u6027\u7684\u4f9d\u8d56\u6027\u6216\u4e0d\u53d8\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\u548c\u7269\u7406\u542f\u53d1\u7684\u5bf9\u9f50\u5ea6\u91cf\uff0c\u5e76\u5728\u53d7\u63a7\u6a21\u578b\u548c\u5e7f\u6cdb\u4f7f\u7528\u7684\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u4e0d\u540c\u5c5e\u6027\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u4e0d\u53d8\u6027\uff0c\u63ed\u793a\u4e86\u5176\u4f18\u7f3a\u70b9\uff0c\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u4eba\u8138\u8bc6\u522b\u6a21\u578b\u7684\u5c5e\u6027\u4f9d\u8d56\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u6a21\u578b\u8bbe\u8ba1\u3002"}}
{"id": "2507.11187", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11187", "abs": "https://arxiv.org/abs/2507.11187", "authors": ["Shao-Bo Lin", "Xiaotong Liu", "Yao Wang"], "title": "Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms", "comment": null, "summary": "Online collaborative medical prediction platforms offer convenience and\nreal-time feedback by leveraging massive electronic health records. However,\ngrowing concerns about privacy and low prediction quality can deter patient\nparticipation and doctor cooperation. In this paper, we first clarify the\nprivacy attacks, namely attribute attacks targeting patients and model\nextraction attacks targeting doctors, and specify the corresponding privacy\nprinciples. We then propose a privacy-preserving mechanism and integrate it\ninto a novel one-shot distributed learning framework, aiming to simultaneously\nmeet both privacy requirements and prediction performance objectives. Within\nthe framework of statistical learning theory, we theoretically demonstrate that\nthe proposed distributed learning framework can achieve the optimal prediction\nperformance under specific privacy requirements. We further validate the\ndeveloped privacy-preserving collaborative medical prediction platform through\nboth toy simulations and real-world data experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u534f\u4f5c\u533b\u7597\u9884\u6d4b\u5e73\u53f0\uff0c\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u548c\u6027\u80fd\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u9884\u6d4b\u5e73\u53f0\u4e2d\u9690\u79c1\u6cc4\u9732\u548c\u9884\u6d4b\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u4fc3\u8fdb\u60a3\u8005\u548c\u533b\u751f\u7684\u53c2\u4e0e\u3002", "method": "\u63d0\u51fa\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u5e76\u96c6\u6210\u5230\u4e00\u6b21\u6027\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6700\u4f18\u6027\u80fd\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5e73\u53f0\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u6ee1\u8db3\u9690\u79c1\u8981\u6c42\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u4f18\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.11441", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.11441", "abs": "https://arxiv.org/abs/2507.11441", "authors": ["Kaif Shaikh", "Antoni Kowalczuk", "Franziska Boenisch", "Adam Dziedzic"], "title": "Implementing Adaptations for Vision AutoRegressive Model", "comment": "Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025", "summary": "Vision AutoRegressive model (VAR) was recently introduced as an alternative\nto Diffusion Models (DMs) in image generation domain. In this work we focus on\nits adaptations, which aim to fine-tune pre-trained models to perform specific\ndownstream tasks, like medical data generation. While for DMs there exist many\ntechniques, adaptations for VAR remain underexplored. Similarly, differentially\nprivate (DP) adaptations-ones that aim to preserve privacy of the adaptation\ndata-have been extensively studied for DMs, while VAR lacks such solutions. In\nour work, we implement and benchmark many strategies for VAR, and compare them\nto state-of-the-art DM adaptation strategies. We observe that VAR outperforms\nDMs for non-DP adaptations, however, the performance of DP suffers, which\nnecessitates further research in private adaptations for VAR. Code is available\nat https://github.com/sprintml/finetuning_var_dp.", "AI": {"tldr": "VAR\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u672c\u6587\u7814\u7a76\u4e86\u5176\u9002\u5e94\u6027\u548c\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u9002\u5e94\u6027\uff0c\u53d1\u73b0VAR\u5728\u975eDP\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4f46\u5728DP\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u7814\u7a76VAR\u6a21\u578b\u7684\u9002\u5e94\u6027\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u6570\u636e\u751f\u6210\u7b49\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u586b\u8865VAR\u5728\u5dee\u5206\u9690\u79c1\u9002\u5e94\u6027\u65b9\u9762\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5b9e\u73b0\u5e76\u6bd4\u8f83\u4e86\u591a\u79cdVAR\u9002\u5e94\u7b56\u7565\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\u9002\u5e94\u7b56\u7565\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "VAR\u5728\u975eDP\u9002\u5e94\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4f46\u5728DP\u9002\u5e94\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "VAR\u5728\u975e\u9690\u79c1\u4efb\u52a1\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u9690\u79c1\u4fdd\u62a4\u4efb\u52a1\u4e2d\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11228", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.11228", "abs": "https://arxiv.org/abs/2507.11228", "authors": ["Si Yi Meng", "Baptiste Goujaud", "Antonio Orvieto", "Christopher De Sa"], "title": "Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?", "comment": null, "summary": "Gradient descent (GD) on logistic regression has many fascinating properties.\nWhen the dataset is linearly separable, it is known that the iterates converge\nin direction to the maximum-margin separator regardless of how large the step\nsize is. In the non-separable case, however, it has been shown that GD can\nexhibit a cycling behaviour even when the step sizes is still below the\nstability threshold $2/\\lambda$, where $\\lambda$ is the largest eigenvalue of\nthe Hessian at the solution. This short paper explores whether restricting the\ndata to have equal magnitude is a sufficient condition for global convergence,\nunder any step size below the stability threshold. We prove that this is true\nin a one dimensional space, but in higher dimensions cycling behaviour can\nstill occur. We hope to inspire further studies on quantifying how common these\ncycles are in realistic datasets, as well as finding sufficient conditions to\nguarantee global convergence with large step sizes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u68af\u5ea6\u4e0b\u964d\u5728\u903b\u8f91\u56de\u5f52\u4e2d\u7684\u6536\u655b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u5177\u6709\u76f8\u540c\u5e45\u5ea6\u7684\u6761\u4ef6\u4e0b\uff0c\u662f\u5426\u80fd\u5728\u7a33\u5b9a\u6027\u9608\u503c\u5185\u5b9e\u73b0\u5168\u5c40\u6536\u655b\u3002", "motivation": "\u7814\u7a76\u68af\u5ea6\u4e0b\u964d\u5728\u975e\u53ef\u5206\u6570\u636e\u96c6\u4e0a\u7684\u5faa\u73af\u884c\u4e3a\uff0c\u5e76\u63a2\u7d22\u6570\u636e\u5e45\u5ea6\u4e00\u81f4\u662f\u5426\u8db3\u4ee5\u4fdd\u8bc1\u5168\u5c40\u6536\u655b\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u5728\u4e00\u7ef4\u548c\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u9a8c\u8bc1\u6570\u636e\u5e45\u5ea6\u4e00\u81f4\u5bf9\u68af\u5ea6\u4e0b\u964d\u6536\u655b\u6027\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e00\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u6570\u636e\u5e45\u5ea6\u4e00\u81f4\u53ef\u4fdd\u8bc1\u5168\u5c40\u6536\u655b\uff1b\u4f46\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u4ecd\u53ef\u80fd\u51fa\u73b0\u5faa\u73af\u884c\u4e3a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u540e\u7eed\u91cf\u5316\u5faa\u73af\u884c\u4e3a\u7684\u666e\u904d\u6027\u53ca\u5bfb\u627e\u4fdd\u8bc1\u5168\u5c40\u6536\u655b\u7684\u5145\u5206\u6761\u4ef6\u63d0\u4f9b\u4e86\u542f\u53d1\u3002"}}
{"id": "2507.11443", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11443", "abs": "https://arxiv.org/abs/2507.11443", "authors": ["Haoran Wang", "Hanyu Pei", "Yang Lyu", "Kai Zhang", "Li Li", "Feng-Lei Fan"], "title": "COLI: A Hierarchical Efficient Compressor for Large Images", "comment": null, "summary": "The escalating adoption of high-resolution, large-field-of-view imagery\namplifies the need for efficient compression methodologies. Conventional\ntechniques frequently fail to preserve critical image details, while\ndata-driven approaches exhibit limited generalizability. Implicit Neural\nRepresentations (INRs) present a promising alternative by learning continuous\nmappings from spatial coordinates to pixel intensities for individual images,\nthereby storing network weights rather than raw pixels and avoiding the\ngeneralization problem. However, INR-based compression of large images faces\nchallenges including slow compression speed and suboptimal compression ratios.\nTo address these limitations, we introduce COLI (Compressor for Large Images),\na novel framework leveraging Neural Representations for Videos (NeRV). First,\nrecognizing that INR-based compression constitutes a training process, we\naccelerate its convergence through a pretraining-finetuning paradigm,\nmixed-precision training, and reformulation of the sequential loss into a\nparallelizable objective. Second, capitalizing on INRs' transformation of image\nstorage constraints into weight storage, we implement Hyper-Compression, a\nnovel post-training technique to substantially enhance compression ratios while\nmaintaining minimal output distortion. Evaluations across two medical imaging\ndatasets demonstrate that COLI consistently achieves competitive or superior\nPSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while\naccelerating NeRV training by up to 4 times.", "AI": {"tldr": "COLI\u6846\u67b6\u5229\u7528\u795e\u7ecf\u89c6\u9891\u8868\u793a\uff08NeRV\uff09\u6539\u8fdb\u5927\u56fe\u50cf\u538b\u7f29\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3-\u5fae\u8c03\u3001\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u5e76\u884c\u5316\u76ee\u6807\u52a0\u901f\u6536\u655b\uff0c\u5e76\u5f15\u5165\u8d85\u538b\u7f29\u6280\u672f\u63d0\u5347\u538b\u7f29\u6bd4\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u5927\u89c6\u573a\u56fe\u50cf\u7684\u538b\u7f29\u9700\u6c42\u589e\u52a0\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u7ec6\u8282\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6cdb\u5316\u6027\u5dee\uff0cINRs\u867d\u597d\u4f46\u538b\u7f29\u901f\u5ea6\u6162\u4e14\u538b\u7f29\u6bd4\u4e0d\u8db3\u3002", "method": "\u91c7\u7528NeRV\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3-\u5fae\u8c03\u3001\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u548c\u5e76\u884c\u5316\u76ee\u6807\u52a0\u901fINR\u6536\u655b\uff1b\u5f15\u5165\u8d85\u538b\u7f29\u6280\u672f\u63d0\u5347\u538b\u7f29\u6bd4\u3002", "result": "\u5728\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\uff0cCOLI\u5728PSNR\u548cSSIM\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u538b\u7f29\u901f\u5ea6\u63d0\u53474\u500d\u3002", "conclusion": "COLI\u901a\u8fc7\u6539\u8fdbINR\u538b\u7f29\u901f\u5ea6\u548c\u538b\u7f29\u6bd4\uff0c\u4e3a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11246", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11246", "abs": "https://arxiv.org/abs/2507.11246", "authors": ["Lingwei Kong", "Lu Wang", "Changping Peng", "Zhangang Lin", "Ching Law", "Jingping Shao"], "title": "Generative Click-through Rate Prediction with Applications to Search Advertising", "comment": "This work was first submitted on February 9, 2024", "summary": "Click-Through Rate (CTR) prediction models are integral to a myriad of\nindustrial settings, such as personalized search advertising. Current methods\ntypically involve feature extraction from users' historical behavior sequences\ncombined with product information, feeding into a discriminative model that is\ntrained on user feedback to estimate CTR. With the success of models such as\nGPT, the potential for generative models to enrich expressive power beyond\ndiscriminative models has become apparent. In light of this, we introduce a\nnovel model that leverages generative models to enhance the precision of CTR\npredictions in discriminative models. To reconcile the disparate data\naggregation needs of both model types, we design a two-stage training process:\n1) Generative pre-training for next-item prediction with the given item\ncategory in user behavior sequences; 2) Fine-tuning the well-trained generative\nmodel within a discriminative CTR prediction framework. Our method's efficacy\nis substantiated through extensive experiments on a new dataset, and its\nsignificant utility is further corroborated by online A/B testing results.\nCurrently, the model is deployed on one of the world's largest e-commerce\nplatforms, and we intend to release the associated code and dataset in the\nfuture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u6a21\u578b\u548c\u5224\u522b\u6a21\u578b\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u70b9\u51fb\u7387\u9884\u6d4b\u7684\u7cbe\u5ea6\u3002", "motivation": "\u751f\u6210\u6a21\u578b\uff08\u5982GPT\uff09\u5728\u8868\u8fbe\u80fd\u529b\u4e0a\u8d85\u8d8a\u5224\u522b\u6a21\u578b\uff0c\u56e0\u6b64\u63a2\u7d22\u5176\u5728\u70b9\u51fb\u7387\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u751f\u6210\u5f0f\u9884\u8bad\u7ec3\u7528\u4e8e\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u4e2d\u7684\u4e0b\u4e00\u9879\u9884\u6d4b\uff1b2) \u5728\u5224\u522b\u5f0f\u6846\u67b6\u4e2d\u5fae\u8c03\u751f\u6210\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5df2\u90e8\u7f72\u4e8e\u5927\u578b\u7535\u5546\u5e73\u53f0\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u80fd\u663e\u8457\u63d0\u5347\u70b9\u51fb\u7387\u9884\u6d4b\u7684\u7cbe\u5ea6\uff0c\u672a\u6765\u5c06\u516c\u5f00\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2507.11474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11474", "abs": "https://arxiv.org/abs/2507.11474", "authors": ["Pan Du", "Mingqi Xu", "Xiaozhi Zhu", "Jian-xun Wang"], "title": "HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing", "comment": "59 pages, 9 figures", "summary": "Accurate characterization of vascular geometry is essential for\ncardiovascular diagnosis and treatment planning. Traditional statistical shape\nmodeling (SSM) methods rely on linear assumptions, limiting their expressivity\nand scalability to complex topologies such as multi-branch vascular structures.\nWe introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular\ngeometry Synthesis, which integrates NURBS surface parameterization with\ndiffusion-based generative modeling to synthesize realistic, fine-grained\naortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates\nanatomically faithful aortas with supra-aortic branches, yielding biomarker\ndistributions that closely match those of the original dataset. HUG-VAS adopts\na hierarchical architecture comprising a denoising diffusion model that\ngenerates centerlines and a guided diffusion model that synthesizes radial\nprofiles conditioned on those centerlines, thereby capturing two layers of\nanatomical variability. Critically, the framework supports zero-shot\nconditional generation from image-derived priors, enabling practical\napplications such as interactive semi-automatic segmentation, robust\nreconstruction under degraded imaging conditions, and implantable device\noptimization. To our knowledge, HUG-VAS is the first SSM framework to bridge\nimage-derived priors with generative shape modeling via a unified integration\nof NURBS parameterization and hierarchical diffusion processes.", "AI": {"tldr": "HUG-VAS\u662f\u4e00\u79cd\u57fa\u4e8eNURBS\u548c\u6269\u6563\u6a21\u578b\u7684\u8840\u7ba1\u51e0\u4f55\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u5408\u6210\u771f\u5b9e\u7684\u591a\u5206\u652f\u4e3b\u52a8\u8109\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u5f62\u72b6\u5efa\u6a21\u65b9\u6cd5\u56e0\u7ebf\u6027\u5047\u8bbe\u9650\u5236\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u8840\u7ba1\u62d3\u6251\u7ed3\u6784\u3002", "method": "\u7ed3\u5408NURBS\u53c2\u6570\u5316\u548c\u5206\u5c42\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u4e2d\u5fc3\u7ebf\u548c\u5f84\u5411\u8f6e\u5ed3\u3002", "result": "\u751f\u6210\u7684\u4e3b\u52a8\u8109\u7ed3\u6784\u89e3\u5256\u5b66\u4e0a\u51c6\u786e\uff0c\u751f\u7269\u6807\u5fd7\u7269\u5206\u5e03\u4e0e\u539f\u59cb\u6570\u636e\u5339\u914d\u3002", "conclusion": "HUG-VAS\u9996\u6b21\u5c06\u56fe\u50cf\u5148\u9a8c\u4e0e\u751f\u6210\u5f62\u72b6\u5efa\u6a21\u7edf\u4e00\uff0c\u652f\u6301\u96f6\u6837\u672c\u6761\u4ef6\u751f\u6210\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e34\u5e8a\u5e94\u7528\u3002"}}
{"id": "2507.11262", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.11262", "abs": "https://arxiv.org/abs/2507.11262", "authors": ["Elmira Mirzabeigi", "Sepehr Rezaee", "Kourosh Parand"], "title": "LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments", "comment": null, "summary": "Training deep neural networks, particularly in computer vision tasks, often\nsuffers from noisy gradients and unstable convergence, which hinder performance\nand generalization. In this paper, we propose LyAm, a novel optimizer that\nintegrates Adam's adaptive moment estimation with Lyapunov-based stability\nmechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability\ntheory to enhance convergence robustness and mitigate training noise. We\nprovide a rigorous theoretical framework proving the convergence guarantees of\nLyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10\nand CIFAR-100 show that LyAm consistently outperforms state-of-the-art\noptimizers in terms of accuracy, convergence speed, and stability, establishing\nit as a strong candidate for robust deep learning optimization.", "AI": {"tldr": "LyAm\u662f\u4e00\u79cd\u65b0\u578b\u4f18\u5316\u5668\uff0c\u7ed3\u5408\u4e86Adam\u7684\u81ea\u9002\u5e94\u77e9\u4f30\u8ba1\u548c\u57fa\u4e8eLyapunov\u7684\u7a33\u5b9a\u6027\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5e38\u9762\u4e34\u68af\u5ea6\u566a\u58f0\u548c\u6536\u655b\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "LyAm\u901a\u8fc7Lyapunov\u7a33\u5b9a\u6027\u7406\u8bba\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u7387\uff0c\u589e\u5f3a\u6536\u655b\u9c81\u68d2\u6027\u5e76\u51cf\u5c11\u8bad\u7ec3\u566a\u58f0\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u7b49\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLyAm\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u4f18\u5316\u5668\u3002", "conclusion": "LyAm\u662f\u4e00\u79cd\u9002\u7528\u4e8e\u590d\u6742\u975e\u51f8\u573a\u666f\u7684\u9c81\u68d2\u6df1\u5ea6\u5b66\u4e60\u4f18\u5316\u5668\u3002"}}
{"id": "2507.11476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11476", "abs": "https://arxiv.org/abs/2507.11476", "authors": ["Esteban Rom\u00e1n Catafau", "Torbj\u00f6rn E. M. Nordling"], "title": "C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images", "comment": "22 pages, 16 figures", "summary": "This paper addresses the fundamental computer vision challenge of robust\ncircle detection and fitting in degraded imaging conditions. We present\nCombinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an\nalgorithm that bridges the gap between circle detection and precise parametric\nfitting by combining (1) efficient combinatorial edge pixel (edgel) sampling\nand (2) convolution-based density estimation in parameter space.\n  We evaluate 3C-FBI across three experimental frameworks: (1) real-world\nmedical data from Parkinson's disease assessments (144 frames from 36 videos),\n(2) controlled synthetic data following established circle-fitting benchmarks,\nand (3) systematic analysis across varying spatial resolutions and outlier\ncontamination levels. Results show that 3C-FBI achieves state-of-the-art\naccuracy (Jaccard index 0.896) while maintaining real-time performance (40.3\nfps), significantly outperforming classical methods like RCD (6.8 fps) on a\nstandard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost\n1.0) at high resolutions (480x480) and reliable performance (Jaccard higher\nthan 0.95) down to 160x160 with up to 20% outliers.\n  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989\nacross contamination levels, comparable to modern methods like Qi et al. (2024,\n0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and\nrobustness makes 3C-FBI ideal for medical imaging, robotics, and industrial\ninspection under challenging conditions.", "AI": {"tldr": "3C-FBI\u7b97\u6cd5\u901a\u8fc7\u7ec4\u5408\u8fb9\u7f18\u50cf\u7d20\u91c7\u6837\u548c\u5377\u79ef\u5bc6\u5ea6\u4f30\u8ba1\uff0c\u5728\u6a21\u7cca\u56fe\u50cf\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9e\u65f6\u5706\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u5728\u9000\u5316\u6210\u50cf\u6761\u4ef6\u4e0b\u9c81\u68d2\u7684\u5706\u68c0\u6d4b\u548c\u62df\u5408\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u7ec4\u5408\u8fb9\u7f18\u50cf\u7d20\u91c7\u6837\u548c\u53c2\u6570\u7a7a\u95f4\u5377\u79ef\u5bc6\u5ea6\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u79cd\u5b9e\u9a8c\u6846\u67b6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0cJaccard\u6307\u6570\u8fbe0.896\uff0c\u5b9e\u65f6\u6027\u80fd40.3 fps\u3002", "conclusion": "3C-FBI\u5728\u7cbe\u5ea6\u3001\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u4e0a\u8868\u73b0\u5353\u8d8a\uff0c\u9002\u7528\u4e8e\u533b\u7597\u5f71\u50cf\u548c\u5de5\u4e1a\u68c0\u6d4b\u3002"}}
{"id": "2507.11269", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11269", "abs": "https://arxiv.org/abs/2507.11269", "authors": ["Tal Fiskus", "Uri Shaham"], "title": "Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound", "comment": "51 pages, 16 figures", "summary": "Deep reinforcement learning (DRL) agents excel in solving complex\ndecision-making tasks across various domains. However, they often require a\nsubstantial number of training steps and a vast experience replay buffer,\nleading to significant computational and resource demands. To address these\nchallenges, we introduce a novel theoretical result that leverages the\nNeyman-Rubin potential outcomes framework into DRL. Unlike most methods that\nfocus on bounding the counterfactual loss, we establish a causal bound on the\nfactual loss, which is analogous to the on-policy loss in DRL. This bound is\ncomputed by storing past value network outputs in the experience replay buffer,\neffectively utilizing data that is usually discarded. Extensive experiments\nacross the Atari 2600 and MuJoCo domains on various agents, such as DQN and\nSAC, achieve up to 2,427% higher reward ratio, outperforming the same agents\nwithout our proposed term, and reducing the experience replay buffer size by up\nto 96%, significantly improving sample efficiency at negligible cost.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eNeyman-Rubin\u6f5c\u5728\u7ed3\u679c\u6846\u67b6\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u7684\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4e2d\u8bad\u7ec3\u6b65\u9aa4\u591a\u3001\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528Neyman-Rubin\u6f5c\u5728\u7ed3\u679c\u6846\u67b6\uff0c\u5efa\u7acb\u4e8b\u5b9e\u635f\u5931\u7684\u56e0\u679c\u754c\u9650\uff0c\u5e76\u5b58\u50a8\u5386\u53f2\u4ef7\u503c\u7f51\u7edc\u8f93\u51fa\u4ee5\u5229\u7528\u901a\u5e38\u88ab\u4e22\u5f03\u7684\u6570\u636e\u3002", "result": "\u5728Atari 2600\u548cMuJoCo\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5956\u52b1\u6bd4\u7387\u63d0\u9ad8\u4e862427%\uff0c\u7ecf\u9a8c\u56de\u653e\u7f13\u51b2\u533a\u5927\u5c0f\u51cf\u5c11\u4e8696%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\uff0c\u4e14\u6210\u672c\u51e0\u4e4e\u53ef\u4ee5\u5ffd\u7565\u3002"}}
{"id": "2507.11488", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11488", "abs": "https://arxiv.org/abs/2507.11488", "authors": ["Pakizar Shamoi", "Nuray Toganas", "Muragul Muratbekova", "Elnara Kadyrgali", "Adilet Yerkin", "Ayan Igali", "Malika Ziyada", "Ayana Adilova", "Aron Karatayev", "Yerdauit Torekhan"], "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation", "comment": "submitted to IEEE for consideration", "summary": "Colors are omnipresent in today's world and play a vital role in how humans\nperceive and interact with their surroundings. However, it is challenging for\ncomputers to imitate human color perception. This paper introduces the Human\nPerception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based\nRepresentation and Interpretation), designed to bridge the gap between\ncomputational color representations and human visual perception. The proposed\nmodel uses fuzzy sets and logic to create a framework for color categorization.\nUsing a three-phase experimental approach, the study first identifies\ndistinguishable color stimuli for hue, saturation, and intensity through\npreliminary experiments, followed by a large-scale human categorization survey\ninvolving more than 1000 human subjects. The resulting data are used to extract\nfuzzy partitions and generate membership functions that reflect real-world\nperceptual uncertainty. The model incorporates a mechanism for adaptation that\nallows refinement based on feedback and contextual changes. Comparative\nevaluations demonstrate the model's alignment with human perception compared to\ntraditional color models, such as RGB, HSV, and LAB. To the best of our\nknowledge, no previous research has documented the construction of a model for\ncolor attribute specification based on a sample of this size or a comparable\nsample of the human population (n = 2496). Our findings are significant for\nfields such as design, artificial intelligence, marketing, and human-computer\ninteraction, where perceptually relevant color representation is critical.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u611f\u77e5\u7684\u6a21\u7cca\u989c\u8272\u6a21\u578bCOLIBRI\uff0c\u901a\u8fc7\u6a21\u7cca\u96c6\u548c\u903b\u8f91\u6784\u5efa\u989c\u8272\u5206\u7c7b\u6846\u67b6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u4f20\u7edf\u989c\u8272\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u96be\u4ee5\u6a21\u4eff\u4eba\u7c7b\u989c\u8272\u611f\u77e5\u7684\u95ee\u9898\uff0c\u7f29\u5c0f\u8ba1\u7b97\u989c\u8272\u8868\u793a\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u5b9e\u9a8c\u65b9\u6cd5\uff0c\u5305\u62ec\u521d\u6b65\u5b9e\u9a8c\u786e\u5b9a\u53ef\u533a\u5206\u989c\u8272\u523a\u6fc0\uff0c\u5927\u89c4\u6a21\u4eba\u7c7b\u5206\u7c7b\u8c03\u67e5\uff0c\u63d0\u53d6\u6a21\u7cca\u5206\u533a\u548c\u751f\u6210\u96b6\u5c5e\u51fd\u6570\u3002", "result": "\u6a21\u578b\u5728\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\u4e0a\u4f18\u4e8eRGB\u3001HSV\u548cLAB\u7b49\u4f20\u7edf\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8bbe\u8ba1\u3001AI\u3001\u8425\u9500\u548c\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u3002", "conclusion": "COLIBRI\u6a21\u578b\u5728\u989c\u8272\u5c5e\u6027\u89c4\u8303\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u586b\u8865\u4e86\u5927\u89c4\u6a21\u4eba\u7c7b\u6837\u672c\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.11274", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11274", "abs": "https://arxiv.org/abs/2507.11274", "authors": ["Amit Attia", "Matan Schliserman", "Uri Sherman", "Tomer Koren"], "title": "Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime", "comment": "27 pages", "summary": "We study population convergence guarantees of stochastic gradient descent\n(SGD) for smooth convex objectives in the interpolation regime, where the noise\nat optimum is zero or near zero. The behavior of the last iterate of SGD in\nthis setting -- particularly with large (constant) stepsizes -- has received\ngrowing attention in recent years due to implications for the training of\nover-parameterized models, as well as to analyzing forgetting in continual\nlearning and to understanding the convergence of the randomized Kaczmarz method\nfor solving linear systems. We establish that after $T$ steps of SGD on\n$\\beta$-smooth convex loss functions with stepsize $\\eta \\leq 1/\\beta$, the\nlast iterate exhibits expected excess risk $\\widetilde{O}(1/(\\eta\nT^{1-\\beta\\eta/2}) + \\eta T^{\\beta\\eta/2} \\sigma_\\star^2)$, where\n$\\sigma_\\star^2$ denotes the variance of the stochastic gradients at the\noptimum. In particular, for a well-tuned stepsize we obtain a near optimal\n$\\widetilde{O}(1/T + \\sigma_\\star/\\sqrt{T})$ rate for the last iterate,\nextending the results of Varre et al. (2021) beyond least squares regression;\nand when $\\sigma_\\star=0$ we obtain a rate of $O(1/\\sqrt{T})$ with\n$\\eta=1/\\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently\nestablished by Evron et al. (2025) in the special case of realizable linear\nregression.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u63d2\u503c\u6761\u4ef6\u4e0bSGD\u7684\u6536\u655b\u6027\uff0c\u7279\u522b\u662f\u5728\u5927\u5b66\u4e60\u7387\u4e0b\u6700\u540e\u8fed\u4ee3\u7684\u8868\u73b0\uff0c\u5e76\u7ed9\u51fa\u4e86\u65b0\u7684\u6536\u655b\u901f\u7387\u3002", "motivation": "\u63a2\u7d22SGD\u5728\u63d2\u503c\u6761\u4ef6\u4e0b\u7684\u6536\u655b\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u8fc7\u53c2\u6570\u5316\u6a21\u578b\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5206\u6790SGD\u5728\u03b2\u5e73\u6ed1\u51f8\u635f\u5931\u51fd\u6570\u4e0a\u7684\u8868\u73b0\uff0c\u4f7f\u7528\u5b66\u4e60\u7387\u03b7\u22641/\u03b2\u3002", "result": "\u8bc1\u660e\u4e86\u6700\u540e\u8fed\u4ee3\u7684\u671f\u671b\u8d85\u989d\u98ce\u9669\u4e3aO~(1/(\u03b7T^(1-\u03b2\u03b7/2)) + \u03b7T^(\u03b2\u03b7/2)\u03c3_*^2)\uff0c\u5e76\u7ed9\u51fa\u4e86\u6700\u4f18\u5b66\u4e60\u7387\u4e0b\u7684\u6536\u655b\u901f\u7387\u3002", "conclusion": "\u6269\u5c55\u4e86Varre\u7b49\u4eba\u7684\u7ed3\u679c\uff0c\u5e76\u5728\u03c3_*=0\u65f6\u6539\u8fdb\u4e86Evron\u7b49\u4eba\u7684\u6536\u655b\u901f\u7387\u3002"}}
{"id": "2507.11522", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11522", "abs": "https://arxiv.org/abs/2507.11522", "authors": ["Tariq Mehmood", "Hamza Ahmad", "Muhammad Haroon Shakeel", "Murtaza Taj"], "title": "CATVis: Context-Aware Thought Visualization", "comment": "Accepted at MICCAI 2025. This is the submitted version prior to peer\n  review. The final Version of Record will appear in the MICCAI 2025\n  proceedings (Springer LNCS)", "summary": "EEG-based brain-computer interfaces (BCIs) have shown promise in various\napplications, such as motor imagery and cognitive state monitoring. However,\ndecoding visual representations from EEG signals remains a significant\nchallenge due to their complex and noisy nature. We thus propose a novel\n5-stage framework for decoding visual representations from EEG signals: (1) an\nEEG encoder for concept classification, (2) cross-modal alignment of EEG and\ntext embeddings in CLIP feature space, (3) caption refinement via re-ranking,\n(4) weighted interpolation of concept and caption embeddings for richer\nsemantics, and (5) image generation using a pre-trained Stable Diffusion model.\nWe enable context-aware EEG-to-image generation through cross-modal alignment\nand re-ranking. Experimental results demonstrate that our method generates\nhigh-quality images aligned with visual stimuli, outperforming SOTA approaches\nby 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and\nreducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic\nalignment and image quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76845\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u4eceEEG\u4fe1\u53f7\u89e3\u7801\u89c6\u89c9\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "EEG\u4fe1\u53f7\u590d\u6742\u4e14\u566a\u58f0\u591a\uff0c\u89e3\u7801\u89c6\u89c9\u8868\u793a\u5177\u6709\u6311\u6218\u6027\u3002", "method": "5\u9636\u6bb5\u6846\u67b6\uff1aEEG\u7f16\u7801\u5668\u3001\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u6807\u9898\u91cd\u6392\u3001\u52a0\u6743\u63d2\u503c\u548c\u56fe\u50cf\u751f\u6210\u3002", "result": "\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5206\u7c7b\u548c\u751f\u6210\u51c6\u786e\u7387\u5206\u522b\u63d0\u534713.43%\u548c15.21%\uff0cFID\u964d\u4f4e36.61%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.11344", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11344", "abs": "https://arxiv.org/abs/2507.11344", "authors": ["Zara Hall", "Melanie Subbiah", "Thomas P Zollo", "Kathleen McKeown", "Richard Zemel"], "title": "Guiding LLM Decision-Making with Fairness Reward Models", "comment": null, "summary": "Large language models are increasingly used to support high-stakes decisions,\npotentially influencing who is granted bail or receives a loan. Naive\nchain-of-thought sampling can improve average decision accuracy, but has also\nbeen shown to amplify unfair bias. To address this challenge and enable the\ntrustworthy use of reasoning models in high-stakes decision-making, we propose\na framework for training a generalizable Fairness Reward Model (FRM). Our model\nassigns a fairness score to LLM reasoning, enabling the system to down-weight\nbiased trajectories and favor equitable ones when aggregating decisions across\nreasoning chains. We show that a single Fairness Reward Model, trained on\nweakly supervised, LLM-annotated examples of biased versus unbiased reasoning,\ntransfers across tasks, domains, and model families without additional\nfine-tuning. Applied to real-world decision-making tasks including recidivism\nprediction and social media moderation, we show that our approach consistently\nimproves fairness while matching, or even surpassing, baseline accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u901a\u7528\u516c\u5e73\u5956\u52b1\u6a21\u578b\uff08FRM\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u53ef\u80fd\u653e\u5927\u7684\u4e0d\u516c\u5e73\u504f\u89c1\u95ee\u9898\uff0c\u786e\u4fdd\u5176\u53ef\u4fe1\u8d56\u6027\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u7684\u516c\u5e73\u5956\u52b1\u6a21\u578b\uff08FRM\uff09\uff0c\u4e3aLLM\u7684\u63a8\u7406\u5206\u914d\u516c\u5e73\u5206\u6570\uff0c\u4ece\u800c\u5728\u51b3\u7b56\u94fe\u4e2d\u51cf\u5c11\u504f\u89c1\u3002", "result": "FRM\u5728\u65e0\u9700\u989d\u5916\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u8de8\u4efb\u52a1\u3001\u9886\u57df\u548c\u6a21\u578b\u5bb6\u65cf\u8fc1\u79fb\uff0c\u5e76\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u8d85\u8fc7\u57fa\u7ebf\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684FRM\u6846\u67b6\u80fd\u591f\u6709\u6548\u51cf\u5c11\u504f\u89c1\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u7684\u516c\u5e73\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u3002"}}
{"id": "2507.11533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11533", "abs": "https://arxiv.org/abs/2507.11533", "authors": ["Mengyu Wang", "Henghui Ding", "Jianing Peng", "Yao Zhao", "Yunpeng Chen", "Yunchao Wei"], "title": "CharaConsist: Fine-Grained Consistent Character Generation", "comment": "ICCV 2025 accepted paper, project page:\n  https://murray-wang.github.io/CharaConsist/", "summary": "In text-to-image generation, producing a series of consistent contents that\npreserve the same identity is highly valuable for real-world applications.\nAlthough a few works have explored training-free methods to enhance the\nconsistency of generated subjects, we observe that they suffer from the\nfollowing problems. First, they fail to maintain consistent background details,\nwhich limits their applicability. Furthermore, when the foreground character\nundergoes large motion variations, inconsistencies in identity and clothing\ndetails become evident. To address these problems, we propose CharaConsist,\nwhich employs point-tracking attention and adaptive token merge along with\ndecoupled control of the foreground and background. CharaConsist enables\nfine-grained consistency for both foreground and background, supporting the\ngeneration of one character in continuous shots within a fixed scene or in\ndiscrete shots across different scenes. Moreover, CharaConsist is the first\nconsistent generation method tailored for text-to-image DiT model. Its ability\nto maintain fine-grained consistency, combined with the larger capacity of\nlatest base model, enables it to produce high-quality visual outputs,\nbroadening its applicability to a wider range of real-world scenarios. The\nsource code has been released at https://github.com/Murray-Wang/CharaConsist", "AI": {"tldr": "CharaConsist\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u4fdd\u6301\u524d\u540e\u4e00\u81f4\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u70b9\u8ddf\u8e2a\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94\u4ee4\u724c\u5408\u5e76\uff0c\u89e3\u51b3\u4e86\u80cc\u666f\u548c\u524d\u666f\u7ec6\u8282\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u80cc\u666f\u7ec6\u8282\u548c\u524d\u666f\u89d2\u8272\u5927\u52a8\u4f5c\u53d8\u5316\u65f6\u7684\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u70b9\u8ddf\u8e2a\u6ce8\u610f\u529b\u548c\u81ea\u9002\u5e94\u4ee4\u724c\u5408\u5e76\u6280\u672f\uff0c\u7ed3\u5408\u524d\u666f\u548c\u80cc\u666f\u7684\u89e3\u8026\u63a7\u5236\u3002", "result": "CharaConsist\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u4e00\u81f4\u7684\u89c6\u89c9\u8f93\u51fa\uff0c\u9002\u7528\u4e8e\u56fa\u5b9a\u573a\u666f\u6216\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u8fde\u7eed\u6216\u79bb\u6563\u955c\u5934\u3002", "conclusion": "CharaConsist\u662f\u9996\u4e2a\u9488\u5bf9DiT\u6a21\u578b\u7684\u6587\u672c\u5230\u56fe\u50cf\u4e00\u81f4\u6027\u751f\u6210\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.11357", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11357", "abs": "https://arxiv.org/abs/2507.11357", "authors": ["Emile van Krieken", "Pasquale Minervini", "Edoardo Ponti", "Antonio Vergari"], "title": "Neurosymbolic Reasoning Shortcuts under the Independence Assumption", "comment": "Accepted at NeSy 2025", "summary": "The ubiquitous independence assumption among symbolic concepts in\nneurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors\nuse it to speed up probabilistic reasoning. Recent works like van Krieken et\nal. (2024) and Marconato et al. (2024) argued that the independence assumption\ncan hinder learning of NeSy predictors and, more crucially, prevent them from\ncorrectly modelling uncertainty. There is, however, scepticism in the NeSy\ncommunity around the scenarios in which the independence assumption actually\nlimits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle\nthis question by formally showing that assuming independence among symbolic\nconcepts entails that a model can never represent uncertainty over certain\nconcept combinations. Thus, the model fails to be aware of reasoning shortcuts,\ni.e., the pathological behaviour of NeSy predictors that predict correct\ndownstream tasks but for the wrong reasons.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u795e\u7ecf\u7b26\u53f7\u9884\u6d4b\u5668\u4e2d\u7b26\u53f7\u6982\u5ff5\u72ec\u7acb\u6027\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8bc1\u660e\u5176\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u8868\u793a\u67d0\u4e9b\u6982\u5ff5\u7ec4\u5408\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7b26\u53f7\u9884\u6d4b\u5668\u4e2d\u7b26\u53f7\u6982\u5ff5\u72ec\u7acb\u6027\u5047\u8bbe\u5bf9\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u8bc1\u660e\u5c55\u793a\u72ec\u7acb\u6027\u5047\u8bbe\u7684\u5c40\u9650\u6027\u3002", "result": "\u72ec\u7acb\u6027\u5047\u8bbe\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u8868\u793a\u67d0\u4e9b\u6982\u5ff5\u7ec4\u5408\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u5f15\u53d1\u63a8\u7406\u6377\u5f84\u95ee\u9898\u3002", "conclusion": "\u72ec\u7acb\u6027\u5047\u8bbe\u9650\u5236\u4e86\u795e\u7ecf\u7b26\u53f7\u9884\u6d4b\u5668\u7684\u80fd\u529b\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6\u5176\u9002\u7528\u6027\u3002"}}
{"id": "2507.11539", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11539", "abs": "https://arxiv.org/abs/2507.11539", "authors": ["Dong Zhuo", "Wenzhao Zheng", "Jiahe Guo", "Yuqi Wu", "Jie Zhou", "Jiwen Lu"], "title": "Streaming 4D Visual Geometry Transformer", "comment": "Code is available at: https://github.com/wzzheng/StreamVGGT", "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f0f4D\u89c6\u89c9\u51e0\u4f55\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u5b9e\u65f6\u611f\u77e5\u548c\u91cd\u5efa4D\u65f6\u7a7a\u51e0\u4f55\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u7684\u601d\u8def\uff0c\u652f\u6301\u9ad8\u6548\u5728\u7ebf\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u89c6\u9891\u4e2d4D\u65f6\u7a7a\u51e0\u4f55\u611f\u77e5\u548c\u91cd\u5efa\u7684\u6311\u6218\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u548c\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u56e0\u679c\u53d8\u6362\u5668\u67b6\u6784\uff0c\u4f7f\u7528\u65f6\u5e8f\u56e0\u679c\u6ce8\u610f\u529b\u548c\u5386\u53f2\u952e\u503c\u7f13\u5b58\u4f5c\u4e3a\u9690\u5f0f\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6d41\u5f0f\u957f\u671f4D\u91cd\u5efa\u3002", "result": "\u5728\u591a\u4e2a4D\u51e0\u4f55\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u5728\u7ebf\u573a\u666f\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7a7a\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u548c\u4ea4\u4e92\u5f0f\u76844D\u89c6\u89c9\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11367", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11367", "abs": "https://arxiv.org/abs/2507.11367", "authors": ["Daniel Tanneberg"], "title": "Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning", "comment": "accepted at the European Conference on Artificial Intelligence (ECAI\n  2025)", "summary": "Training neural networks with reinforcement learning (RL) typically relies on\nbackpropagation (BP), necessitating storage of activations from the forward\npass for subsequent backward updates. Furthermore, backpropagating error\nsignals through multiple layers often leads to vanishing or exploding\ngradients, which can degrade learning performance and stability. We propose a\nnovel approach that trains each layer of the neural network using local signals\nduring the forward pass in RL settings. Our approach introduces local,\nlayer-wise losses leveraging the principle of matching pairwise distances from\nmulti-dimensional scaling, enhanced with optional reward-driven guidance. This\nmethod allows each hidden layer to be trained using local signals computed\nduring forward propagation, thus eliminating the need for backward passes and\nstoring intermediate activations. Our experiments, conducted with policy\ngradient methods across common RL benchmarks, demonstrate that this\nbackpropagation-free method achieves competitive performance compared to their\nclassical BP-based counterpart. Additionally, the proposed method enhances\nstability and consistency within and across runs, and improves performance\nespecially in challenging environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u4fe1\u53f7\u548c\u5c42\u95f4\u635f\u5931\u5728\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u4e2d\u8bad\u7ec3\u7f51\u7edc\uff0c\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u4e14\u66f4\u7a33\u5b9a\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f9d\u8d56\u53cd\u5411\u4f20\u64ad\u9700\u8981\u5b58\u50a8\u6fc0\u6d3b\u503c\u4e14\u6613\u51fa\u73b0\u68af\u5ea6\u6d88\u5931\u6216\u7206\u70b8\u95ee\u9898\uff0c\u5f71\u54cd\u5b66\u4e60\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u4fe1\u53f7\u548c\u5c42\u95f4\u635f\u5931\uff0c\u7ed3\u5408\u591a\u7ef4\u5c3a\u5ea6\u5339\u914d\u539f\u5219\u548c\u5956\u52b1\u9a71\u52a8\u6307\u5bfc\uff0c\u5728\u6b63\u5411\u4f20\u64ad\u4e2d\u8bad\u7ec3\u6bcf\u5c42\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5e38\u89c1\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u7a33\u5b9a\u6027\u66f4\u9ad8\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u907f\u514d\u4e86\u53cd\u5411\u4f20\u64ad\u7684\u7f3a\u70b9\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2507.11540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11540", "abs": "https://arxiv.org/abs/2507.11540", "authors": ["Zhen Xu", "Hongyu Zhou", "Sida Peng", "Haotong Lin", "Haoyu Guo", "Jiahao Shao", "Peishan Yang", "Qinglin Yang", "Sheng Miao", "Xingyi He", "Yifan Wang", "Yue Wang", "Ruizhen Hu", "Yiyi Liao", "Xiaowei Zhou", "Hujun Bao"], "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation", "comment": null, "summary": "Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u7684\u53d1\u5c55\uff0c\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5355\u76ee\u3001\u7acb\u4f53\u3001\u591a\u89c6\u56fe\u548c\u5355\u76ee\u89c6\u9891\u8bbe\u7f6e\u4e0b\u7684\u67b6\u6784\u548c\u8303\u5f0f\uff0c\u5e76\u5c55\u671b\u4e86\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u786c\u4ef6\u4f20\u611f\u5668\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u5206\u8fa8\u7387\u4f4e\u4e14\u5bf9\u73af\u5883\u654f\u611f\uff0c\u800c\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u7a33\u5b9a\u6027\u4e0a\u9762\u4e34\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u5177\u6709\u5f3a\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u63a2\u7d22\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5e94\u7528\uff0c\u4ee5\u4fc3\u8fdb\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "result": "\u63d0\u51fa\u4e86\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u67b6\u6784\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u6709\u671b\u89e3\u51b3\u73b0\u6709\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u5176\u67b6\u6784\u4f18\u5316\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5229\u7528\u3002"}}
{"id": "2507.11371", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11371", "abs": "https://arxiv.org/abs/2507.11371", "authors": ["Gabriel Bo", "Koa Chang", "Justin Gu"], "title": "Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs", "comment": "12 pages, 4 figures", "summary": "We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel\nreinforcement learning framework that teaches large language models to explore\ndiverse tool usage patterns beyond conventional high-temperature sampling.\nBuilding on recent advances in step-wise reinforcement learning, we introduce a\ndual-objective reward system that simultaneously optimizes for answer quality\nand tool diversity, training a Llama-3.1 8B model through offline PPO on\nsynthetically generated trajectories from the MMLU-Pro dataset. Our approach\nuniquely employs a rarity-first exploitation strategy where a GPT-4o judge\nscores candidate actions across eight distinct tools plus chain-of-thought\nreasoning, with the policy favoring less-frequently used but still viable tools\nto encourage systematic exploration. Empirical results demonstrate that SPaRK\nachieves competitive performance across 14 MMLU-Pro categories while exhibiting\nsignificantly higher entropy in tool selection compared to both baseline and\nsupervised fine-tuning approaches, suggesting that algorithmic exploration\nthrough explicit tool diversity can enhance reasoning capabilities without\nsacrificing accuracy.", "AI": {"tldr": "SPaRK\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u76ee\u6807\u5956\u52b1\u7cfb\u7edf\u4f18\u5316\u7b54\u6848\u8d28\u91cf\u548c\u5de5\u5177\u591a\u6837\u6027\uff0c\u8bad\u7ec3\u6a21\u578b\u5728MMLU-Pro\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u7ade\u4e89\u6027\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\u4e0a\u7684\u591a\u6837\u6027\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u9ad8\u6e29\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u9010\u6b65\u5f3a\u5316\u5b66\u4e60\uff0c\u5f15\u5165\u53cc\u76ee\u6807\u5956\u52b1\u7cfb\u7edf\uff0c\u7ed3\u5408\u79bb\u7ebfPPO\u8bad\u7ec3Llama-3.1 8B\u6a21\u578b\uff0c\u5e76\u4f7f\u7528GPT-4o\u8bc4\u5206\u5de5\u5177\u591a\u6837\u6027\u3002", "result": "\u572814\u4e2aMMLU-Pro\u7c7b\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5de5\u5177\u9009\u62e9\u71b5\u663e\u8457\u9ad8\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5de5\u5177\u591a\u6837\u6027\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002"}}
{"id": "2507.11393", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11393", "abs": "https://arxiv.org/abs/2507.11393", "authors": ["James P Jun", "Vijay Marupudi", "Raj Sanjay Shah", "Sashank Varma"], "title": "A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning", "comment": "Accepted to CogSci 2025. 7 pages, 7 figures", "summary": "Learning new information without forgetting prior knowledge is central to\nhuman intelligence. In contrast, neural network models suffer from catastrophic\nforgetting: a significant degradation in performance on previously learned\ntasks when acquiring new information. The Complementary Learning Systems (CLS)\ntheory offers an explanation for this human ability, proposing that the brain\nhas distinct systems for pattern separation (encoding distinct memories) and\npattern completion (retrieving complete memories from partial cues). To capture\nthese complementary functions, we leverage the representational generalization\ncapabilities of variational autoencoders (VAEs) and the robust memory storage\nproperties of Modern Hopfield networks (MHNs), combining them into a neurally\nplausible continual learning model. We evaluate this model on the Split-MNIST\ntask, a popular continual learning benchmark, and achieve close to\nstate-of-the-art accuracy (~90%), substantially reducing forgetting.\nRepresentational analyses empirically confirm the functional dissociation: the\nVAE underwrites pattern completion, while the MHN drives pattern separation. By\ncapturing pattern separation and completion in scalable architectures, our work\nprovides a functional template for modeling memory consolidation,\ngeneralization, and continual learning in both biological and artificial\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u73b0\u4ee3Hopfield\u7f51\u7edc\uff08MHN\uff09\u7684\u6301\u7eed\u5b66\u4e60\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u5728Split-MNIST\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u501f\u9274\u4eba\u8111\u7684\u4e92\u8865\u5b66\u4e60\u7cfb\u7edf\u7406\u8bba\u3002", "method": "\u7ed3\u5408VAE\u7684\u6a21\u5f0f\u5b8c\u6210\u80fd\u529b\u548cMHN\u7684\u6a21\u5f0f\u5206\u79bb\u80fd\u529b\uff0c\u6784\u5efa\u795e\u7ecf\u5408\u7406\u7684\u6301\u7eed\u5b66\u4e60\u6a21\u578b\u3002", "result": "\u5728Split-MNIST\u4efb\u52a1\u4e0a\u8fbe\u5230\u7ea690%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u51cf\u5c11\u9057\u5fd8\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u751f\u7269\u548c\u4eba\u5de5\u7cfb\u7edf\u4e2d\u7684\u8bb0\u5fc6\u5de9\u56fa\u3001\u6cdb\u5316\u548c\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u529f\u80fd\u6a21\u677f\u3002"}}
{"id": "2507.11411", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11411", "abs": "https://arxiv.org/abs/2507.11411", "authors": ["Seyedsaman Emami", "Gonzalo Mart\u00ednez-Mu\u00f1oz", "Daniel Hern\u00e1ndez-Lobato"], "title": "Robust-Multi-Task Gradient Boosting", "comment": null, "summary": "Multi-task learning (MTL) has shown effectiveness in exploiting shared\ninformation across tasks to improve generalization. MTL assumes tasks share\nsimilarities that can improve performance. In addition, boosting algorithms\nhave demonstrated exceptional performance across diverse learning problems,\nprimarily due to their ability to focus on hard-to-learn instances and\niteratively reduce residual errors. This makes them a promising approach for\nlearning multi-task problems. However, real-world MTL scenarios often involve\ntasks that are not well-aligned (known as outlier or adversarial tasks), which\ndo not share beneficial similarities with others and can, in fact, deteriorate\nthe performance of the overall model. To overcome this challenge, we propose\nRobust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that\nexplicitly models and adapts to task heterogeneity during training. R-MTGB\nstructures the learning process into three sequential blocks: (1) learning\nshared patterns, (2) partitioning tasks into outliers and non-outliers with\nregularized parameters, and (3) fine-tuning task-specific predictors. This\narchitecture enables R-MTGB to automatically detect and penalize outlier tasks\nwhile promoting effective knowledge transfer among related tasks. Our method\nintegrates these mechanisms seamlessly within gradient boosting, allowing\nrobust handling of noisy or adversarial tasks without sacrificing accuracy.\nExtensive experiments on both synthetic benchmarks and real-world datasets\ndemonstrate that our approach successfully isolates outliers, transfers\nknowledge, and consistently reduces prediction errors for each task\nindividually, and achieves overall performance gains across all tasks. These\nresults highlight robustness, adaptability, and reliable convergence of R-MTGB\nin challenging MTL environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u591a\u4efb\u52a1\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff08R-MTGB\uff09\uff0c\u901a\u8fc7\u5efa\u6a21\u4efb\u52a1\u5f02\u8d28\u6027\u6765\u63d0\u5347\u591a\u4efb\u52a1\u5b66\u4e60\u6027\u80fd\uff0c\u6709\u6548\u5904\u7406\u5f02\u5e38\u4efb\u52a1\u3002", "motivation": "\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u5b58\u5728\u5f02\u5e38\u4efb\u52a1\uff08\u5982\u5bf9\u6297\u6027\u4efb\u52a1\uff09\uff0c\u5b83\u4eec\u4f1a\u635f\u5bb3\u6a21\u578b\u6574\u4f53\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u5904\u7406\u3002", "method": "R-MTGB\u901a\u8fc7\u4e09\u9636\u6bb5\u5b66\u4e60\uff1a\u5b66\u4e60\u5171\u4eab\u6a21\u5f0f\u3001\u6b63\u5219\u5316\u53c2\u6570\u5212\u5206\u5f02\u5e38\u4efb\u52a1\u3001\u5fae\u8c03\u4efb\u52a1\u7279\u5b9a\u9884\u6d4b\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660eR-MTGB\u80fd\u6709\u6548\u9694\u79bb\u5f02\u5e38\u4efb\u52a1\uff0c\u4fc3\u8fdb\u77e5\u8bc6\u8fc1\u79fb\uff0c\u964d\u4f4e\u9884\u6d4b\u8bef\u5dee\uff0c\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002", "conclusion": "R-MTGB\u5728\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u548c\u53ef\u9760\u6536\u655b\u6027\u3002"}}
{"id": "2507.11436", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11436", "abs": "https://arxiv.org/abs/2507.11436", "authors": ["Behtom Adeli", "John McLinden", "Pankaj Pandey", "Ming Shao", "Yalda Shahriari"], "title": "Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures", "comment": null, "summary": "Activation functions are critical to the performance of deep neural networks,\nparticularly in domains such as functional near-infrared spectroscopy (fNIRS),\nwhere nonlinearity, low signal-to-noise ratio (SNR), and signal variability\nposes significant challenges to model accuracy. However, the impact of\nactivation functions on deep learning (DL) performance in the fNIRS domain\nremains underexplored and lacks systematic investigation in the current\nliterature. This study evaluates a range of conventional and field-specific\nactivation functions for fNIRS classification tasks using multiple deep\nlearning architectures, including the domain-specific fNIRSNet, AbsoluteNet,\nMDNN, and shallowConvNet (as the baseline), all tested on a single dataset\nrecorded during an auditory task. To ensure fair a comparison, all networks\nwere trained and tested using standardized preprocessing and consistent\ntraining parameters. The results show that symmetrical activation functions\nsuch as Tanh and the Absolute value function Abs(x) can outperform commonly\nused functions like the Rectified Linear Unit (ReLU), depending on the\narchitecture. Additionally, a focused analysis of the role of symmetry was\nconducted using a Modified Absolute Function (MAF), with results further\nsupporting the effectiveness of symmetrical activation functions on performance\ngains. These findings underscore the importance of selecting proper activation\nfunctions that align with the signal characteristics of fNIRS data.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6fc0\u6d3b\u51fd\u6570\u5728fNIRS\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5bf9\u79f0\u6fc0\u6d3b\u51fd\u6570\u5982Tanh\u548cAbs(x)\u5728\u67d0\u4e9b\u67b6\u6784\u4e2d\u4f18\u4e8eReLU\u3002", "motivation": "fNIRS\u9886\u57df\u7f3a\u4e4f\u5bf9\u6fc0\u6d3b\u51fd\u6570\u5f71\u54cd\u7684\u7cfb\u7edf\u7814\u7a76\uff0c\u4e14\u4fe1\u53f7\u7684\u975e\u7ebf\u6027\u3001\u4f4e\u4fe1\u566a\u6bd4\u548c\u53d8\u5f02\u6027\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u6784\u6210\u6311\u6218\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u4f20\u7edf\u548c\u9886\u57df\u7279\u5b9a\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u7528\u5305\u62ecfNIRSNet\u3001AbsoluteNet\u7b49\u5728\u5185\u7684\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u5e76\u5728\u5355\u4e00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u5bf9\u79f0\u6fc0\u6d3b\u51fd\u6570\uff08\u5982Tanh\u548cAbs(x\uff09\uff09\u5728\u67d0\u4e9b\u67b6\u6784\u4e2d\u8868\u73b0\u4f18\u4e8eReLU\uff0c\u4e14\u5bf9\u79f0\u6027\u5206\u6790\u8fdb\u4e00\u6b65\u652f\u6301\u5176\u6027\u80fd\u4f18\u52bf\u3002", "conclusion": "\u9009\u62e9\u4e0efNIRS\u4fe1\u53f7\u7279\u6027\u5339\u914d\u7684\u6fc0\u6d3b\u51fd\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.11439", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11439", "abs": "https://arxiv.org/abs/2507.11439", "authors": ["Hongming Tan", "Ting Chen", "Ruochong Jin", "Wai Kin Chan"], "title": "Data Augmentation in Time Series Forecasting through Inverted Framework", "comment": null, "summary": "Currently, iTransformer is one of the most popular and effective models for\nmultivariate time series (MTS) forecasting. Thanks to its inverted framework,\niTransformer effectively captures multivariate correlation. However, the\ninverted framework still has some limitations. It diminishes temporal\ninterdependency information, and introduces noise in cases of nonsignificant\nvariable correlation. To address these limitations, we introduce a novel data\naugmentation method on inverted framework, called DAIF. Unlike previous data\naugmentation methods, DAIF stands out as the first real-time augmentation\nspecifically designed for the inverted framework in MTS forecasting. We first\ndefine the structure of the inverted sequence-to-sequence framework, then\npropose two different DAIF strategies, Frequency Filtering and Cross-variation\nPatching to address the existing challenges of the inverted framework.\nExperiments across multiple datasets and inverted models have demonstrated the\neffectiveness of our DAIF.", "AI": {"tldr": "DAIF\u662f\u4e00\u79cd\u9488\u5bf9iTransformer\u6a21\u578b\u7684\u65b0\u578b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u8fc7\u6ee4\u548c\u8de8\u53d8\u91cf\u4fee\u8865\u7b56\u7565\u89e3\u51b3\u4e86\u5176\u6846\u67b6\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "iTransformer\u6846\u67b6\u5728\u6355\u6349\u591a\u5143\u76f8\u5173\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f1a\u524a\u5f31\u65f6\u95f4\u4f9d\u8d56\u6027\u4fe1\u606f\u5e76\u5f15\u5165\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cdDAIF\u7b56\u7565\uff1a\u9891\u7387\u8fc7\u6ee4\u548c\u8de8\u53d8\u91cf\u4fee\u8865\uff0c\u4ee5\u589e\u5f3aiTransformer\u6846\u67b6\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDAIF\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "DAIF\u662f\u9996\u4e2a\u4e13\u4e3aiTransformer\u6846\u67b6\u8bbe\u8ba1\u7684\u5b9e\u65f6\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u6027\u80fd\u3002"}}
{"id": "2507.11457", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11457", "abs": "https://arxiv.org/abs/2507.11457", "authors": ["Yaoxian Dong", "Yifan Gao", "Haoyue Li", "Yanfen Cui", "Xin Gao"], "title": "LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer", "comment": null, "summary": "Accurate preoperative assessment of lymph node (LN) metastasis in rectal\ncancer guides treatment decisions, yet conventional MRI evaluation based on\nmorphological criteria shows limited diagnostic performance. While some\nartificial intelligence models have been developed, they often operate as black\nboxes, lacking the interpretability needed for clinical trust. Moreover, these\nmodels typically evaluate nodes in isolation, overlooking the patient-level\ncontext. To address these limitations, we introduce LRMR, an LLM-Driven\nRelational Multi-node Ranking framework. This approach reframes the diagnostic\ntask from a direct classification problem into a structured reasoning and\nranking process. The LRMR framework operates in two stages. First, a multimodal\nlarge language model (LLM) analyzes a composite montage image of all LNs from a\npatient, generating a structured report that details ten distinct radiological\nfeatures. Second, a text-based LLM performs pairwise comparisons of these\nreports between different patients, establishing a relative risk ranking based\non the severity and number of adverse features. We evaluated our method on a\nretrospective cohort of 117 rectal cancer patients. LRMR achieved an area under\nthe curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of\ndeep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies\nconfirmed the value of our two main contributions: removing the relational\nranking stage or the structured prompting stage led to a significant\nperformance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our\nwork demonstrates that decoupling visual perception from cognitive reasoning\nthrough a two-stage LLM framework offers a powerful, interpretable, and\neffective new paradigm for assessing lymph node metastasis in rectal cancer.", "AI": {"tldr": "LRMR\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5LLM\u65b9\u6cd5\u63d0\u5347\u76f4\u80a0\u764c\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edfMRI\u8bc4\u4f30\u548c\u73b0\u6709AI\u6a21\u578b\u5728\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u8bca\u65ad\u4e2d\u8868\u73b0\u6709\u9650\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "LRMR\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u591a\u6a21\u6001LLM\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\uff0c\u6587\u672cLLM\u8fdb\u884c\u60a3\u8005\u95f4\u98ce\u9669\u6392\u5e8f\u3002", "result": "\u5728117\u4f8b\u60a3\u8005\u4e2d\uff0cLRMR\u7684AUC\u4e3a0.7917\uff0c\u4f18\u4e8eResNet50\u7b49\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u4e24\u9636\u6bb5LLM\u6846\u67b6\u4e3a\u6dcb\u5df4\u7ed3\u8f6c\u79fb\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.11471", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11471", "abs": "https://arxiv.org/abs/2507.11471", "authors": ["Harsha Varun Marisetty", "Manik Gupta", "Yogesh Simmhan"], "title": "D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data", "comment": "Preprint of paper to appear in the proceedings of IEEE INTERNATIONAL\n  CONFERENCE ON EDGE COMPUTING & COMMUNICATIONS EDGE 2025", "summary": "With advancements in computing and communication technologies, the Internet\nof Things (IoT) has seen significant growth. IoT devices typically collect data\nfrom various sensors, such as temperature, humidity, and energy meters. Much of\nthis data is temporal in nature. Traditionally, data from IoT devices is\ncentralized for analysis, but this approach introduces delays and increased\ncommunication costs. Federated learning (FL) has emerged as an effective\nalternative, allowing for model training across distributed devices without the\nneed to centralize data. In many applications, such as smart home energy and\nenvironmental monitoring, the data collected by IoT devices across different\nlocations can exhibit significant variation in trends and seasonal patterns.\nAccurately forecasting such non-stationary, non-linear time-series data is\ncrucial for applications like energy consumption estimation and weather\nforecasting. However, these data variations can severely impact prediction\naccuracy. The key contributions of this paper are: (1) Investigating how\nnon-linear, non-stationary time-series data distributions, like generalized\nextreme value (gen-extreme) and log norm distributions, affect FL performance.\n(2) Analyzing how different detrending techniques for non-linear time-series\ndata influence the forecasting model's performance in a FL setup. We generated\nseveral synthetic time-series datasets using non-linear data distributions and\ntrained an LSTM-based forecasting model using both centralized and FL\napproaches. Additionally, we evaluated the impact of detrending on real-world\ndatasets with non-linear time-series data distributions. Our experimental\nresults show that: (1) FL performs worse than centralized approaches when\ndealing with non-linear data distributions. (2) The use of appropriate\ndetrending techniques improves FL performance, reducing loss across different\ndata distributions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u975e\u7ebf\u6027\u548c\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5206\u5e03\u5bf9\u8054\u90a6\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u8ba8\u4e86\u53bb\u8d8b\u52bf\u6280\u672f\u5bf9FL\u6027\u80fd\u7684\u63d0\u5347\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u8bbe\u5907\u7684\u666e\u53ca\uff0c\u5176\u6536\u96c6\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5177\u6709\u975e\u7ebf\u6027\u548c\u975e\u5e73\u7a33\u7279\u6027\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u5ef6\u8fdf\u548c\u901a\u4fe1\u6210\u672c\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60\u6210\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u6570\u636e\u5206\u5e03\u5dee\u5f02\u4f1a\u5f71\u54cd\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\uff08\u4f7f\u7528\u975e\u7ebf\u6027\u5206\u5e03\uff09\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u8bad\u7ec3LSTM\u9884\u6d4b\u6a21\u578b\uff0c\u6bd4\u8f83\u96c6\u4e2d\u5f0f\u548c\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u53bb\u8d8b\u52bf\u6280\u672f\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u8054\u90a6\u5b66\u4e60\u5728\u975e\u7ebf\u6027\u6570\u636e\u5206\u5e03\u4e0b\u8868\u73b0\u4e0d\u5982\u96c6\u4e2d\u5f0f\u65b9\u6cd5\uff1b2\uff09\u5408\u9002\u7684\u53bb\u8d8b\u52bf\u6280\u672f\u80fd\u63d0\u5347FL\u6027\u80fd\uff0c\u51cf\u5c11\u635f\u5931\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u5728\u5904\u7406\u975e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u53bb\u8d8b\u52bf\u6280\u672f\u53ef\u4ee5\u663e\u8457\u6539\u5584\u5176\u6027\u80fd\u3002"}}
{"id": "2507.11486", "categories": ["cs.LG", "I.2.1"], "pdf": "https://arxiv.org/pdf/2507.11486", "abs": "https://arxiv.org/abs/2507.11486", "authors": ["Jeremi Levesque", "Antoine Th\u00e9berge", "Maxime Descoteaux", "Pierre-Marc Jodoin"], "title": "Exploring the robustness of TractOracle methods in RL-based tractography", "comment": "38 pages, 8 figures. Submitted to Medical Image Analysis", "summary": "Tractography algorithms leverage diffusion MRI to reconstruct the fibrous\narchitecture of the brain's white matter. Among machine learning approaches,\nreinforcement learning (RL) has emerged as a promising framework for\ntractography, outperforming traditional methods in several key aspects.\nTractOracle-RL, a recent RL-based approach, reduces false positives by\nincorporating anatomical priors into the training process via a reward-based\nmechanism. In this paper, we investigate four extensions of the original\nTractOracle-RL framework by integrating recent advances in RL, and we evaluate\ntheir performance across five diverse diffusion MRI datasets. Results\ndemonstrate that combining an oracle with the RL framework consistently leads\nto robust and reliable tractography, regardless of the specific method or\ndataset used. We also introduce a novel RL training scheme called Iterative\nReward Training (IRT), inspired by the Reinforcement Learning from Human\nFeedback (RLHF) paradigm. Instead of relying on human input, IRT leverages\nbundle filtering methods to iteratively refine the oracle's guidance throughout\ntraining. Experimental results show that RL methods trained with oracle\nfeedback significantly outperform widely used tractography techniques in terms\nof accuracy and anatomical validity.", "AI": {"tldr": "TractOracle-RL\u7684\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u548c\u65b0\u578b\u8bad\u7ec3\u65b9\u6848IRT\u63d0\u5347\u4e86\u7ea4\u7ef4\u8ffd\u8e2a\u7684\u51c6\u786e\u6027\u548c\u89e3\u5256\u5b66\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7ea4\u7ef4\u8ffd\u8e2a\u65b9\u6cd5\u5b58\u5728\u5047\u9633\u6027\u95ee\u9898\uff0cTractOracle-RL\u901a\u8fc7\u7ed3\u5408\u89e3\u5256\u5b66\u5148\u9a8c\u77e5\u8bc6\u6539\u8fdb\u4e86\u6027\u80fd\uff0c\u4f46\u4ecd\u6709\u4f18\u5316\u7a7a\u95f4\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cdTractOracle-RL\u7684\u6269\u5c55\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u578b\u8bad\u7ec3\u65b9\u6848IRT\uff0c\u5229\u7528\u675f\u8fc7\u6ee4\u65b9\u6cd5\u8fed\u4ee3\u4f18\u5316\u5956\u52b1\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408oracle\u7684RL\u65b9\u6cd5\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0cIRT\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u89e3\u5256\u5b66\u6709\u6548\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u7ed3\u5408oracle\u53cd\u9988\u662f\u7ea4\u7ef4\u8ffd\u8e2a\u7684\u6709\u6548\u65b9\u6cd5\uff0cIRT\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.11493", "categories": ["cs.LG", "cs.NE", "68T07"], "pdf": "https://arxiv.org/pdf/2507.11493", "abs": "https://arxiv.org/abs/2507.11493", "authors": ["Majid Darehmiraki"], "title": "A parametric activation function based on Wendland RBF", "comment": "11 pages, 2 figures", "summary": "This paper introduces a novel parametric activation function based on\nWendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,\nknown for their compact support, smoothness, and positive definiteness in\napproximation theory, are adapted to address limitations of traditional\nactivation functions like ReLU, sigmoid, and tanh. The proposed enhanced\nWendland activation combines a standard Wendland component with linear and\nexponential terms, offering tunable locality, improved gradient propagation,\nand enhanced stability during training. Theoretical analysis highlights its\nmathematical properties, including smoothness and adaptability, while empirical\nexperiments on synthetic tasks (e.g., sine wave approximation) and benchmark\ndatasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results\nshow that the Wendland-based activation achieves superior accuracy in certain\nscenarios, particularly in regression tasks, while maintaining computational\nefficiency. The study bridges classical RBF theory with modern deep learning,\nsuggesting that Wendland activations can mitigate overfitting and improve\ngeneralization through localized, smooth transformations. Future directions\ninclude hybrid architectures and domain-specific adaptations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWendland\u5f84\u5411\u57fa\u51fd\u6570\u7684\u65b0\u578b\u53c2\u6570\u5316\u6fc0\u6d3b\u51fd\u6570\uff0c\u7ed3\u5408\u7ebf\u6027\u4e0e\u6307\u6570\u9879\uff0c\u63d0\u5347\u4e86\u68af\u5ea6\u4f20\u64ad\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ReLU\u3001sigmoid\u3001tanh\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0cWendland RBF\u56e0\u5176\u7d27\u652f\u6491\u6027\u3001\u5e73\u6ed1\u6027\u548c\u6b63\u5b9a\u6027\u5728\u903c\u8fd1\u7406\u8bba\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6fc0\u6d3b\u51fd\u6570\u3002", "method": "\u5c06\u6807\u51c6Wendland\u7ec4\u4ef6\u4e0e\u7ebf\u6027\u53ca\u6307\u6570\u9879\u7ed3\u5408\uff0c\u8bbe\u8ba1\u51fa\u53ef\u8c03\u5c40\u90e8\u6027\u3001\u68af\u5ea6\u4f20\u64ad\u66f4\u4f18\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u5176\u6570\u5b66\u6027\u8d28\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\uff08\u5982\u6b63\u5f26\u6ce2\u903c\u8fd1\uff09\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff08MNIST\u3001Fashion-MNIST\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Wendland\u6fc0\u6d3b\u51fd\u6570\u901a\u8fc7\u5c40\u90e8\u5e73\u6ed1\u53d8\u6362\u7f13\u89e3\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u4e0e\u7ecf\u5178RBF\u7406\u8bba\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u6df7\u5408\u67b6\u6784\u548c\u9886\u57df\u9002\u914d\u3002"}}
{"id": "2507.11515", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11515", "abs": "https://arxiv.org/abs/2507.11515", "authors": ["Shiyi Yang", "Xiaoxue Yu", "Rongpeng Li", "Jianhang Zhu", "Zhifeng Zhao", "Honggang Zhang"], "title": "AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air", "comment": "11 pages, 8 figures", "summary": "Operating Large Language Models (LLMs) on edge devices is increasingly\nchallenged by limited communication bandwidth and strained computational and\nmemory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.\nNevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ\nfixed or heuristic rank configurations, and the subsequent over-the-air\ntransmission of all LoRA parameters could be rather inefficient. To address\nthis limitation, we develop AirLLM, a hierarchical diffusion policy framework\nfor communication-aware LoRA adaptation. Specifically, AirLLM models the rank\nconfiguration as a structured action vector that spans all LoRA-inserted\nprojections. To solve the underlying high-dimensional sequential\ndecision-making problem, a Proximal Policy Optimization (PPO) agent generates\ncoarse-grained decisions by jointly observing wireless states and linguistic\ncomplexity, which are then refined via Denoising Diffusion Implicit Models\n(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The\ntwo modules are optimized alternatively, with the DDIM trained under the\nClassifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.\nExperiments under varying signal-to-noise ratios demonstrate that AirLLM\nconsistently enhances fine-tuning performance while significantly reducing\ntransmission costs, highlighting the effectiveness of reinforcement-driven,\ndiffusion-refined rank adaptation for scalable and efficient remote fine-tuning\nover the air.", "AI": {"tldr": "AirLLM\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u6269\u6563\u7b56\u7565\u7684\u901a\u4fe1\u611f\u77e5LoRA\u9002\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6269\u6563\u6a21\u578b\u4f18\u5316LoRA\u7684\u79e9\u914d\u7f6e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u6210\u672c\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u901a\u4fe1\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8fdc\u7a0b\u5fae\u8c03\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408PPO\u548cDDIM\uff0c\u751f\u6210\u4efb\u52a1\u548c\u4fe1\u9053\u81ea\u9002\u5e94\u7684\u79e9\u5411\u91cf\uff0c\u901a\u8fc7\u5206\u5c42\u51b3\u7b56\u4f18\u5316LoRA\u914d\u7f6e\u3002", "result": "\u5728\u4e0d\u540c\u4fe1\u566a\u6bd4\u4e0b\uff0cAirLLM\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u8f93\u6210\u672c\u5e76\u63d0\u5347\u4e86\u5fae\u8c03\u6027\u80fd\u3002", "conclusion": "AirLLM\u5c55\u793a\u4e86\u5f3a\u5316\u9a71\u52a8\u548c\u6269\u6563\u4f18\u5316\u7684\u79e9\u9002\u914d\u5728\u8fdc\u7a0b\u5fae\u8c03\u4e2d\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.11531", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.11531", "abs": "https://arxiv.org/abs/2507.11531", "authors": ["Yue Song", "T. Anderson Keller", "Yisong Yue", "Pietro Perona", "Max Welling"], "title": "Langevin Flows for Modeling Neural Latent Dynamics", "comment": "Full version of the Cognitive Computational Neuroscience (CCN) 2025\n  poster", "summary": "Neural populations exhibit latent dynamical structures that drive\ntime-evolving spiking activities, motivating the search for models that capture\nboth intrinsic network dynamics and external unobserved influences. In this\nwork, we introduce LangevinFlow, a sequential Variational Auto-Encoder where\nthe time evolution of latent variables is governed by the underdamped Langevin\nequation. Our approach incorporates physical priors -- such as inertia,\ndamping, a learned potential function, and stochastic forces -- to represent\nboth autonomous and non-autonomous processes in neural systems. Crucially, the\npotential function is parameterized as a network of locally coupled\noscillators, biasing the model toward oscillatory and flow-like behaviors\nobserved in biological neural populations. Our model features a recurrent\nencoder, a one-layer Transformer decoder, and Langevin dynamics in the latent\nspace. Empirically, our method outperforms state-of-the-art baselines on\nsynthetic neural populations generated by a Lorenz attractor, closely matching\nground-truth firing rates. On the Neural Latents Benchmark (NLB), the model\nachieves superior held-out neuron likelihoods (bits per spike) and forward\nprediction accuracy across four challenging datasets. It also matches or\nsurpasses alternative methods in decoding behavioral metrics such as hand\nvelocity. Overall, this work introduces a flexible, physics-inspired,\nhigh-performing framework for modeling complex neural population dynamics and\ntheir unobserved influences.", "AI": {"tldr": "LangevinFlow\u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5efa\u6a21\u795e\u7ecf\u7fa4\u4f53\u7684\u52a8\u6001\u7ed3\u6784\u548c\u5916\u90e8\u672a\u89c2\u6d4b\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7fa4\u4f53\u7684\u6f5c\u5728\u52a8\u6001\u7ed3\u6784\u53ca\u5176\u5916\u90e8\u5f71\u54cd\uff0c\u9700\u8981\u80fd\u591f\u540c\u65f6\u6355\u6349\u5185\u5728\u7f51\u7edc\u52a8\u6001\u548c\u5916\u90e8\u672a\u89c2\u6d4b\u56e0\u7d20\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faLangevinFlow\uff0c\u5229\u7528\u6b20\u963b\u5c3cLangevin\u65b9\u7a0b\u63a7\u5236\u6f5c\u5728\u53d8\u91cf\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u7ed3\u5408\u7269\u7406\u5148\u9a8c\uff08\u5982\u60ef\u6027\u3001\u963b\u5c3c\u3001\u5b66\u4e60\u52bf\u51fd\u6570\u548c\u968f\u673a\u529b\uff09\u548c\u5c40\u90e8\u8026\u5408\u632f\u8361\u5668\u7f51\u7edc\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548cNLB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u7801\u884c\u4e3a\u6307\u6807\u5982\u624b\u90e8\u901f\u5ea6\u3002", "conclusion": "LangevinFlow\u662f\u4e00\u4e2a\u7075\u6d3b\u3001\u9ad8\u6027\u80fd\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u795e\u7ecf\u7fa4\u4f53\u52a8\u6001\u53ca\u5176\u672a\u89c2\u6d4b\u5f71\u54cd\u7684\u5efa\u6a21\u3002"}}
