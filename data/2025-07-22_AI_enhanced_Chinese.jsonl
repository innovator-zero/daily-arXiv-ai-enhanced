{"id": "2507.14170", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14170", "abs": "https://arxiv.org/abs/2507.14170", "authors": ["Jaeheun Jung", "Donghun Lee"], "title": "Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space", "comment": "ICML 2025 workshop HiLD 2025 (3rd workshop on High-dimensional\n  Learning Dynamics)", "summary": "Structured pruning aims to reduce the size and computational cost of deep\nneural networks by removing entire filters or channels. The traditional\nregularizers such as L1 or Group Lasso and its variants lead to\nmagnitude-biased pruning decisions, such that the filters with small magnitudes\nare likely to be pruned. Also, they often entail pruning results with almost\nzero margin around pruning decision boundary, such that tiny perturbation in a\nfilter magnitude can flip the pruning decision. In this paper, we identify the\nprecise algebraic condition under which pruning operations preserve model\nperformance, and use the condition to construct a novel regularizer defined in\nan extended parameter space via auxiliary catalyst variables. The proposed\nCatalyst regularization ensures fair pruning chance for each filters with\ntheoretically provable zero bias to their magnitude and robust pruning behavior\nachieved by wide-margin bifurcation of magnitudes between the preserved and the\npruned filters. The theoretical properties naturally lead to real-world\neffectiveness, as shown by empirical validations of Catalyst Pruning algorithm.\nPruning results on various datasets and models are superior to state-of-the-art\nfilter pruning methods, and at the same time confirm the predicted robust and\nfair pruning characteristics of Catalyst pruning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCatalyst\u7684\u65b0\u578b\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u526a\u679d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u5e45\u5ea6\u504f\u5dee\u548c\u51b3\u7b56\u8fb9\u754c\u7a84\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\uff08\u5982L1\u6216Group Lasso\uff09\u5b58\u5728\u5e45\u5ea6\u504f\u5dee\u548c\u51b3\u7b56\u8fb9\u754c\u7a84\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u526a\u679d\u7ed3\u679c\u4e0d\u7a33\u5b9a\u4e14\u4e0d\u516c\u5e73\u3002", "method": "\u901a\u8fc7\u4ee3\u6570\u6761\u4ef6\u6784\u5efa\u8f85\u52a9\u50ac\u5316\u5242\u53d8\u91cf\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u786e\u4fdd\u526a\u679d\u51b3\u7b56\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86Catalyst\u526a\u679d\u7684\u4f18\u8d8a\u6027\uff0c\u8868\u73b0\u51fa\u516c\u5e73\u4e14\u9c81\u68d2\u7684\u526a\u679d\u7279\u6027\u3002", "conclusion": "Catalyst\u6b63\u5219\u5316\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u3002"}}
{"id": "2507.14171", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14171", "abs": "https://arxiv.org/abs/2507.14171", "authors": ["Jaeheun Jung", "Jaehyuk Lee", "Yeajin Lee", "Donghun Lee"], "title": "IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning", "comment": null, "summary": "With the growth of demand on neural network compression methods, the\nstructured pruning methods including importance-based approach are actively\nstudied. The magnitude importance and many correlated modern importance\ncriteria often limit the capacity of pruning decision, since the filters with\nlarger magnitudes are not likely to be pruned if the smaller one didn't, even\nif it is redundant. In this paper, we propose a novel pruning strategy to\nchallenge this dominating effect of magnitude and provide fair chance to each\nfilter to be pruned, by placing it on projective space. After that, we observe\nthe gradient descent movement whether the filters move toward the origin or\nnot, to measure how the filter is likely to be pruned. This measurement is used\nto construct PROscore, a novel importance score for IPPRO, a novel\nimportance-based structured pruning with magnitude-indifference. Our evaluation\nresults shows that the proposed importance criteria using the projective space\nachieves near-lossless pruning by reducing the performance drop in pruning,\nwith promising performance after the finetuning. Our work debunks the\n``size-matters'' myth in pruning and expands the frontier of importance-based\npruning both theoretically and empirically.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6295\u5f71\u7a7a\u95f4\u7684\u65b0\u578b\u526a\u679d\u7b56\u7565IPPRO\uff0c\u901a\u8fc7PROscore\u8861\u91cf\u6ee4\u6ce2\u5668\u91cd\u8981\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u526a\u679d\u4e2d\u2018\u5927\u5c0f\u51b3\u5b9a\u91cd\u8981\u6027\u2019\u7684\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\u65b9\u6cd5\u9650\u5236\u4e86\u526a\u679d\u51b3\u7b56\u7684\u516c\u5e73\u6027\uff0c\u5373\u4f7f\u5197\u4f59\u7684\u6ee4\u6ce2\u5668\u4e5f\u53ef\u80fd\u56e0\u5e45\u5ea6\u8f83\u5927\u800c\u88ab\u4fdd\u7559\u3002", "method": "\u5c06\u6ee4\u6ce2\u5668\u7f6e\u4e8e\u6295\u5f71\u7a7a\u95f4\uff0c\u89c2\u5bdf\u68af\u5ea6\u4e0b\u964d\u8fd0\u52a8\u4e2d\u6ee4\u6ce2\u5668\u662f\u5426\u5411\u539f\u70b9\u79fb\u52a8\uff0c\u6784\u5efaPROscore\u4f5c\u4e3a\u91cd\u8981\u6027\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u526a\u679d\u540e\u6027\u80fd\u4e0b\u964d\u8f83\u5c0f\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u635f\u7684\u526a\u679d\u6548\u679c\u3002", "conclusion": "IPPRO\u65b9\u6cd5\u6253\u7834\u4e86\u526a\u679d\u4e2d\u2018\u5927\u5c0f\u51b3\u5b9a\u91cd\u8981\u6027\u2019\u7684\u8ff7\u601d\uff0c\u4e3a\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u526a\u679d\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2507.14172", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14172", "abs": "https://arxiv.org/abs/2507.14172", "authors": ["Julien Pourcel", "C\u00e9dric Colas", "Pierre-Yves Oudeyer"], "title": "Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI", "comment": null, "summary": "Many program synthesis tasks prove too challenging for even state-of-the-art\nlanguage models to solve in single attempts. Search-based evolutionary methods\noffer a promising alternative by exploring solution spaces iteratively, but\ntheir effectiveness remain limited by the fixed capabilities of the underlying\ngenerative model.\n  We propose SOAR, a method that learns program synthesis by integrating\nlanguage models into a self-improving evolutionary loop.\n  SOAR alternates between (1) an evolutionary search that uses an LLM to sample\nand refine candidate solutions, and (2) a hindsight learning phase that\nconverts search attempts into valid problem-solution pairs used to fine-tune\nthe LLM's sampling and refinement capabilities\\, -- \\,enabling increasingly\neffective search in subsequent iterations.\n  On the challenging ARC-AGI benchmark, SOAR achieves significant performance\ngains across model scales and iterations, leveraging positive transfer between\nthe sampling and refinement finetuning tasks. These improvements carry over to\ntest-time adaptation, enabling SOAR to solve 52\\% of the public test set. Our\ncode is open-sourced at: https://github.com/flowersteam/SOAR", "AI": {"tldr": "SOAR\u662f\u4e00\u79cd\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\u7684\u81ea\u6539\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5728\u5355\u6b21\u5c1d\u8bd5\u4e2d\u96be\u4ee5\u89e3\u51b3\u590d\u6742\u7684\u7a0b\u5e8f\u5408\u6210\u4efb\u52a1\uff0c\u800c\u8fdb\u5316\u641c\u7d22\u65b9\u6cd5\u53d7\u9650\u4e8e\u751f\u6210\u6a21\u578b\u7684\u56fa\u5b9a\u80fd\u529b\u3002", "method": "SOAR\u901a\u8fc7\u4ea4\u66ff\u8fdb\u884c\u8fdb\u5316\u641c\u7d22\u548c\u540e\u89c1\u5b66\u4e60\uff0c\u5229\u7528LLM\u91c7\u6837\u548c\u4f18\u5316\u5019\u9009\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002", "result": "\u5728ARC-AGI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSOAR\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u89e3\u51b3\u4e8652%\u7684\u516c\u5171\u6d4b\u8bd5\u96c6\u95ee\u9898\u3002", "conclusion": "SOAR\u901a\u8fc7\u81ea\u6539\u8fdb\u5faa\u73af\u6709\u6548\u63d0\u5347\u4e86\u7a0b\u5e8f\u5408\u6210\u7684\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u8fc1\u79fb\u5b66\u4e60\u548c\u9002\u5e94\u6027\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14249", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14249", "abs": "https://arxiv.org/abs/2507.14249", "authors": ["Yuejiao Xie", "Maonan Wang", "Di Zhou", "Man-On Pun", "Zhu Han"], "title": "Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach", "comment": null, "summary": "Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions\nto alleviate urban congestion, with path planning becoming a key focus area.\nUnlike ground transportation, UAM trajectory planning has to prioritize\ncommunication quality for accurate location tracking in constantly changing\nenvironments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,\nrequires adaptive planning to respond to real-time passenger requests,\nespecially in ride-sharing scenarios where passenger demands are unpredictable\nand dynamic. However, conventional trajectory planning strategies based on\npredefined routes lack the flexibility to meet varied passenger ride demands.\nTo address these challenges, this work first proposes constructing a radio map\nto evaluate the communication quality of urban airspace. Building on this, we\nintroduce a novel Multi-Source Hybrid Attention Reinforcement Learning\n(MSHA-RL) framework for the challenge of effectively focusing on passengers and\nUAM locations, which arises from the significant dimensional disparity between\nthe representations. This model first generates the alignment among diverse\ndata sources with large gap dimensions before employing hybrid attention to\nbalance global and local insights, thereby facilitating responsive, real-time\npath planning. Extensive experimental results demonstrate that the approach\nenables communication-compliant trajectory planning, reducing travel time and\nenhancing operational efficiency while prioritizing passenger safety.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6e90\u6df7\u5408\u6ce8\u610f\u529b\u5f3a\u5316\u5b66\u4e60\u7684UAM\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u4e58\u5ba2\u9700\u6c42\u548c\u901a\u4fe1\u8d28\u91cf\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7cfb\u7edf\uff08UAM\uff09\u4e2d\u56e0\u52a8\u6001\u4e58\u5ba2\u9700\u6c42\u548c\u901a\u4fe1\u8d28\u91cf\u8981\u6c42\u5bfc\u81f4\u7684\u8def\u5f84\u89c4\u5212\u7075\u6d3b\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u6784\u5efa\u65e0\u7ebf\u7535\u5730\u56fe\u8bc4\u4f30\u901a\u4fe1\u8d28\u91cf\uff0c\u5e76\u8bbe\u8ba1MSHA-RL\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u5e73\u8861\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u901a\u4fe1\u5408\u89c4\u7684\u8def\u5f84\u89c4\u5212\uff0c\u51cf\u5c11\u65c5\u884c\u65f6\u95f4\u5e76\u63d0\u5347\u6548\u7387\u3002", "conclusion": "MSHA-RL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86UAM\u8def\u5f84\u89c4\u5212\u7684\u52a8\u6001\u9700\u6c42\u4e0e\u901a\u4fe1\u8d28\u91cf\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2507.14175", "categories": ["cs.LG", "cs.AI", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.14175", "abs": "https://arxiv.org/abs/2507.14175", "authors": ["Youcef Barkat", "Dylan Hamitouche", "Deven Parekh", "Ivy Guo", "David Benrimoh"], "title": "Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data", "comment": null, "summary": "Background: Mental illnesses such as depression and anxiety require improved\nmethods for early detection and personalized intervention. Traditional\npredictive models often rely on unimodal data or early fusion strategies that\nfail to capture the complex, multimodal nature of psychiatric data. Advanced\nintegration techniques, such as intermediate (latent space) fusion, may offer\nbetter accuracy and clinical utility. Methods: Using data from the BRIGHTEN\nclinical trial, we evaluated intermediate (latent space) fusion for predicting\ndaily depressive symptoms (PHQ-2 scores). We compared early fusion implemented\nwith a Random Forest (RF) model and intermediate fusion implemented via a\nCombined Model (CM) using autoencoders and a neural network. The dataset\nincluded behavioral (smartphone-based), demographic, and clinical features.\nExperiments were conducted across multiple temporal splits and data stream\ncombinations. Performance was evaluated using mean squared error (MSE) and\ncoefficient of determination (R2). Results: The CM outperformed both RF and\nLinear Regression (LR) baselines across all setups, achieving lower MSE (0.4985\nvs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed\nsigns of overfitting, with a large gap between training and test performance,\nwhile the CM maintained consistent generalization. Performance was best when\nintegrating all data modalities in the CM (in contradistinction to RF),\nunderscoring the value of latent space fusion for capturing non-linear\ninteractions in complex psychiatric datasets. Conclusion: Latent space fusion\noffers a robust alternative to traditional fusion methods for prediction with\nmultimodal mental health data. Future work should explore model\ninterpretability and individual-level prediction for clinical deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u7684\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u6291\u90c1\u75c7\u72b6\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u4f9d\u8d56\u5355\u6a21\u6001\u6570\u636e\u6216\u65e9\u671f\u878d\u5408\u7b56\u7565\uff0c\u96be\u4ee5\u6355\u6349\u7cbe\u795e\u6570\u636e\u7684\u590d\u6742\u591a\u6a21\u6001\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6574\u5408\u6280\u672f\u3002", "method": "\u4f7f\u7528BRIGHTEN\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\uff0c\u6bd4\u8f83\u4e86\u968f\u673a\u68ee\u6797\uff08RF\uff09\u7684\u65e9\u671f\u878d\u5408\u548c\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ec4\u5408\u6a21\u578b\uff08CM\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u3002", "result": "CM\u5728\u6240\u6709\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8eRF\u548c\u7ebf\u6027\u56de\u5f52\uff08LR\uff09\uff0cMSE\u66f4\u4f4e\uff080.4985 vs. 0.5305\uff09\uff0cR2\u66f4\u9ad8\uff080.4695 vs. 0.4356\uff09\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u878d\u5408\u662f\u591a\u6a21\u6001\u5fc3\u7406\u5065\u5eb7\u6570\u636e\u9884\u6d4b\u7684\u7a33\u5065\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u63a2\u7d22\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e2a\u4f53\u5316\u9884\u6d4b\u3002"}}
{"id": "2507.14274", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.14274", "abs": "https://arxiv.org/abs/2507.14274", "authors": ["Andreas Mueller", "Shivesh Kumar", "Thomas Kordik"], "title": "A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators", "comment": null, "summary": "Series elastic actuators (SEA) were introduced for serial robotic arms. Their\nmodel-based trajectory tracking control requires the second time derivatives of\nthe inverse dynamics solution, for which algorithms were proposed. Trajectory\ncontrol of parallel kinematics manipulators (PKM) equipped with SEAs has not\nyet been pursued. Key element for this is the computationally efficient\nevaluation of the second time derivative of the inverse dynamics solution. This\nhas not been presented in the literature, and is addressed in the present paper\nfor the first time. The special topology of PKM is exploited reusing the\nrecursive algorithms for evaluating the inverse dynamics of serial robots. A\nLie group formulation is used and all relations are derived within this\nframework. Numerical results are presented for a 6-DOF Gough-Stewart platform\n(as part of an exoskeleton), and for a planar PKM when a flatness-based control\nscheme is applied.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u5e76\u884c\u8fd0\u52a8\u5b66\u673a\u68b0\u81c2\uff08PKM\uff09\u914d\u5907\u7cfb\u5217\u5f39\u6027\u6267\u884c\u5668\uff08SEA\uff09\u7684\u9006\u52a8\u529b\u5b66\u89e3\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u7684\u65b9\u6cd5\u3002", "motivation": "PKM\u914d\u5907SEA\u7684\u8f68\u8ff9\u63a7\u5236\u5c1a\u672a\u5b9e\u73b0\uff0c\u5173\u952e\u5728\u4e8e\u9ad8\u6548\u8ba1\u7b97\u9006\u52a8\u529b\u5b66\u89e3\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u3002", "method": "\u5229\u7528PKM\u7684\u7279\u6b8a\u62d3\u6251\u7ed3\u6784\uff0c\u590d\u7528\u4e32\u884c\u673a\u5668\u4eba\u7684\u9012\u5f52\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u674e\u7fa4\u6846\u67b6\u63a8\u5bfc\u6240\u6709\u5173\u7cfb\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u57286\u81ea\u7531\u5ea6Gough-Stewart\u5e73\u53f0\u548c\u5e73\u9762PKM\u4e0a\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u89e3\u51b3\u4e86PKM\u914d\u5907SEA\u7684\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.14268", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "\u6bd4\u8f83\u5206\u6790\u7528\u4e8e\u62df\u54083D\u56fe\u50cf\u6570\u636e\u7684\u9576\u5d4c\u6a21\u578b\u7b97\u6cd5\u7b56\u7565\uff0c\u8bc4\u4f30\u4e0d\u540c\u4f18\u5316\u65b9\u6cd5\u5728\u751f\u6210Voronoi\u3001Laguerre\u548cGBPDs\u65f6\u7684\u6548\u679c\u3002", "motivation": "\u5728\u6750\u6599\u79d1\u5b66\u4e2d\uff0c\u51c6\u786e\u62df\u5408\u591a\u6676\u4f53\u548c\u6ce1\u6cab\u7b49\u6750\u6599\u76843D\u56fe\u50cf\u6570\u636e\u662f\u4e00\u4e2a\u4e0d\u65ad\u53d1\u5c55\u7684\u9886\u57df\uff0c\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u4f18\u5316\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u7ebf\u6027/\u975e\u7ebf\u6027\u89c4\u5212\u3001\u968f\u673a\u4f18\u5316\uff08\u4ea4\u53c9\u71b5\u6cd5\uff09\u548c\u68af\u5ea6\u4e0b\u964d\u7b49\u65b9\u6cd5\u751f\u6210\u9576\u5d4c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4f53\u79ef\u3001\u8868\u9762\u79ef\u548c\u62d3\u6251\u5dee\u5f02\u8bc4\u4f30\u62df\u5408\u8d28\u91cf\u3002", "result": "\u7ed3\u679c\u5c55\u793a\u4e86\u6a21\u578b\u590d\u6742\u6027\u3001\u4f18\u5316\u65b9\u6cd5\u590d\u6742\u6027\u548c\u62df\u5408\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4e3a\u6839\u636e\u6570\u636e\u7279\u5f81\u548c\u5e94\u7528\u9700\u6c42\u9009\u62e9\u5408\u9002\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6750\u6599\u79d1\u5b66\u4e2d3D\u56fe\u50cf\u6570\u636e\u7684\u9576\u5d4c\u6a21\u578b\u62df\u5408\u63d0\u4f9b\u4e86\u65b9\u6cd5\u9009\u62e9\u548c\u4f18\u5316\u7b56\u7565\u7684\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2507.14176", "categories": ["cs.LG", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14176", "abs": "https://arxiv.org/abs/2507.14176", "authors": ["Andr\u00e9s Morales-Forero", "Lili J. Rueda", "Ronald Herrera", "Samuel Bassetto", "Eric Coatanea"], "title": "Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection", "comment": null, "summary": "Artificial intelligence (AI) systems increasingly inform medical\ndecision-making, yet concerns about algorithmic bias and inequitable outcomes\npersist, particularly for historically marginalized populations. This paper\nintroduces the concept of Predictive Representativity (PR), a framework of\nfairness auditing that shifts the focus from the composition of the data set to\noutcomes-level equity. Through a case study in dermatology, we evaluated\nAI-based skin cancer classifiers trained on the widely used HAM10000 dataset\nand on an independent clinical dataset (BOSQUE Test set) from Colombia. Our\nanalysis reveals substantial performance disparities by skin phototype, with\nclassifiers consistently underperforming for individuals with darker skin,\ndespite proportional sampling in the source data. We argue that\nrepresentativity must be understood not as a static feature of datasets but as\na dynamic, context-sensitive property of model predictions. PR operationalizes\nthis shift by quantifying how reliably models generalize fairness across\nsubpopulations and deployment contexts. We further propose an External\nTransportability Criterion that formalizes the thresholds for fairness\ngeneralization. Our findings highlight the ethical imperative for post-hoc\nfairness auditing, transparency in dataset documentation, and inclusive model\nvalidation pipelines. This work offers a scalable tool for diagnosing\nstructural inequities in AI systems, contributing to discussions on equity,\ninterpretability, and data justice and fostering a critical re-evaluation of\nfairness in data-driven healthcare.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPredictive Representativity\uff08PR\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u6027\u5ba1\u8ba1\uff0c\u5f3a\u8c03\u4ece\u6570\u636e\u96c6\u7ec4\u6210\u8f6c\u5411\u7ed3\u679c\u5c42\u9762\u7684\u516c\u5e73\u6027\u3002\u901a\u8fc7\u76ae\u80a4\u75c5\u5b66\u6848\u4f8b\u7814\u7a76\uff0c\u53d1\u73b0AI\u6a21\u578b\u5728\u6df1\u8272\u76ae\u80a4\u4eba\u7fa4\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u63d0\u51fa\u52a8\u6001\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "AI\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u7b97\u6cd5\u504f\u89c1\u548c\u4e0d\u516c\u5e73\u7ed3\u679c\u95ee\u9898\u7a81\u51fa\uff0c\u5c24\u5176\u662f\u5bf9\u5386\u53f2\u4e0a\u8fb9\u7f18\u5316\u7fa4\u4f53\u3002", "method": "\u4f7f\u7528HAM10000\u548cBOSQUE Test\u6570\u636e\u96c6\u8bc4\u4f30\u76ae\u80a4\u764c\u5206\u7c7b\u5668\uff0c\u63d0\u51faPR\u6846\u67b6\u548cExternal Transportability Criterion\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u6df1\u8272\u76ae\u80a4\u4eba\u7fa4\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5f3a\u8c03\u516c\u5e73\u6027\u9700\u52a8\u6001\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51faPR\u6846\u67b6\u548c\u516c\u5e73\u6027\u5ba1\u8ba1\u5de5\u5177\uff0c\u4fc3\u8fdb\u6570\u636e\u9a71\u52a8\u7684\u533b\u7597\u516c\u5e73\u6027\u91cd\u65b0\u8bc4\u4f30\u3002"}}
{"id": "2507.14412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14412", "abs": "https://arxiv.org/abs/2507.14412", "authors": ["Mengxue Fu", "Zhonghao Shi", "Minyu Huang", "Siqi Liu", "Mina Kian", "Yirui Song", "Maja J. Matari\u0107"], "title": "Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support", "comment": null, "summary": "Socially assistive robots (SARs) have shown great potential for supplementing\nwell-being support. However, prior studies have found that existing dialogue\npipelines for SARs remain limited in real-time latency, back-channeling, and\npersonalized speech dialogue. Toward addressing these limitations, we propose\nusing integrated end-to-end speech-language models (SLMs) with SARs. This work\n1) evaluated the usability of an SLM-enabled SAR dialogue system through a\nsmall user study, and 2) identified remaining limitations through study user\nfeedback to inform future improvements. We conducted a small within-participant\nuser study with university students (N = 11) whose results showed that\nparticipants perceived an SLM-enabled SAR system as capable of providing\nempathetic feedback, natural turn-taking, back-channeling, and adaptive\nresponses. We also found that participants reported the robot's nonverbal\nbehaviors as lacking variability and synchronization with conversation, and the\nSLM's verbal feedback as generic and repetitive. These findings highlighted the\nneed for real-time robot movement synchronized with conversation, improved\nprompting or fine-tuning to generate outputs better aligned with mental health\npractices, and more expressive, adaptive vocal generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u7aef\u5230\u7aef\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u6539\u8fdb\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff08SAR\uff09\u7684\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u73b0\u6709SAR\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5b9e\u65f6\u5ef6\u8fdf\u3001\u53cd\u9988\u673a\u5236\u548c\u4e2a\u6027\u5316\u5bf9\u8bdd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aefSLM\u4e0eSAR\u7ed3\u5408\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u7528\u6237\u7814\u7a76\uff08N=11\uff09\u8bc4\u4f30\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "result": "\u7528\u6237\u8ba4\u4e3aSLM-SAR\u7cfb\u7edf\u80fd\u63d0\u4f9b\u5171\u60c5\u53cd\u9988\u548c\u81ea\u7136\u5bf9\u8bdd\uff0c\u4f46\u975e\u8bed\u8a00\u884c\u4e3a\u548c\u53cd\u9988\u5185\u5bb9\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u9700\u4f18\u5316\u673a\u5668\u4eba\u52a8\u4f5c\u540c\u6b65\u3001\u53cd\u9988\u5185\u5bb9\u4e2a\u6027\u5316\u53ca\u8bed\u97f3\u751f\u6210\uff0c\u4ee5\u63d0\u5347SAR\u7cfb\u7edf\u6548\u679c\u3002"}}
{"id": "2507.14303", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u51e0\u79cd\u9ad8\u6548\u6a21\u578b\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u7684\u573a\u666f\u7406\u89e3\uff0c\u901a\u8fc7BDD100k\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u5e76\u63a2\u8ba8\u4e0d\u540c\u4e3b\u5e72\u7f51\u7edc\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\u4e3b\u5e72\u7f51\u7edc\u9009\u62e9\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u6700\u7ec8\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u5e73\u5747IoU\u548c\u635f\u5931\u51fd\u6570\u4e0a\u5747\u6709\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u5206\u5272\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u4eba\u7c7b\u4e13\u5bb6\u7684\u4f9d\u8d56\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u4f18\u5316\u51b3\u7b56\u3002", "method": "\u4f7f\u7528BDD100k\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u591a\u79cd\u9ad8\u6548\u6a21\u578b\uff0c\u5e76\u5c1d\u8bd5\u4e0d\u540c\u4e3b\u5e72\u7f51\u7edc\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u8bc4\u4f30\u5176\u5bf9\u8bed\u4e49\u5206\u5272\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e3b\u5e72\u7f51\u7edc\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u6700\u7ec8\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u5e73\u5747IoU\u548c\u635f\u5931\u51fd\u6570\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4e3b\u5e72\u7f51\u7edc\uff0c\u8bed\u4e49\u5206\u5272\u6027\u80fd\u63d0\u5347\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u7406\u89e3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2507.14177", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "68T07(Primary), 41A15(Secondary)", "I.2.6; G.1.2"], "pdf": "https://arxiv.org/pdf/2507.14177", "abs": "https://arxiv.org/abs/2507.14177", "authors": ["Changcun Huang"], "title": "Understanding Two-Layer Neural Networks with Smooth Activation Functions", "comment": null, "summary": "This paper aims to understand the training solution, which is obtained by the\nback-propagation algorithm, of two-layer neural networks whose hidden layer is\ncomposed of the units with smooth activation functions, including the usual\nsigmoid type most commonly used before the advent of ReLUs. The mechanism\ncontains four main principles: construction of Taylor series expansions, strict\npartial order of knots, smooth-spline implementation and smooth-continuity\nrestriction. The universal approximation for arbitrary input dimensionality is\nproved and experimental verification is given, through which the mystery of\n``black box'' of the solution space is largely revealed. The new proofs\nemployed also enrich approximation theory.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u89e3\uff0c\u63ed\u793a\u4e86\u5176\u89e3\u7a7a\u95f4\u7684\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u903c\u8fd1\u80fd\u529b\u3002", "motivation": "\u7406\u89e3\u4f7f\u7528\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\u7684\u4e24\u5c42\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5f97\u5230\u7684\u8bad\u7ec3\u89e3\uff0c\u63ed\u793a\u5176\u89e3\u7a7a\u95f4\u7684\u673a\u5236\u3002", "method": "\u91c7\u7528\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u3001\u4e25\u683c\u8282\u70b9\u504f\u5e8f\u3001\u5e73\u6ed1\u6837\u6761\u5b9e\u73b0\u548c\u5e73\u6ed1\u8fde\u7eed\u6027\u9650\u5236\u56db\u79cd\u4e3b\u8981\u539f\u5219\u3002", "result": "\u8bc1\u660e\u4e86\u4efb\u610f\u8f93\u5165\u7ef4\u5ea6\u7684\u901a\u7528\u903c\u8fd1\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4e30\u5bcc\u4e86\u903c\u8fd1\u7406\u8bba\u3002", "conclusion": "\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u89e3\u7a7a\u95f4\u7684\u201c\u9ed1\u7bb1\u201d\u673a\u5236\uff0c\u4e3a\u903c\u8fd1\u7406\u8bba\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc1\u660e\u65b9\u6cd5\u3002"}}
{"id": "2507.14455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14455", "abs": "https://arxiv.org/abs/2507.14455", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking", "comment": null, "summary": "Time-delay embedding is a technique that uses snapshots of state history over\ntime to build a linear state space model of a nonlinear smooth system. We\ndemonstrate that periodic non-smooth or hybrid system can also be modeled as a\nlinear state space system using this approach as long as its behavior is\nconsistent in modes and timings. We extended time-delay embeddings to generate\na linear model of two periodic hybrid systems: the bouncing pendulum and the\nsimplest walker with control inputs. This leads to a novel state history\naugmented linear quadratic regulator (LQR) which uses current and past state\nhistory for feedback control.", "AI": {"tldr": "\u5c06\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u6269\u5c55\u5230\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u72b6\u6001\u5386\u53f2\u589e\u5f3a\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u3002", "motivation": "\u63a2\u7d22\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u662f\u5426\u9002\u7528\u4e8e\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u4ee5\u6784\u5efa\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "method": "\u6269\u5c55\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u4e24\u4e2a\u5468\u671f\u6027\u6df7\u5408\u7cfb\u7edf\uff08\u5f39\u8df3\u6446\u548c\u7b80\u5355\u6b65\u884c\u5668\uff09\uff0c\u5e76\u5f00\u53d1\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u53ef\u7528\u4e8e\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u4e3a\u63a7\u5236\u8fd9\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.14312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Cl\u00e9ment Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "CLIPTTA\u662f\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u5bf9\u6bd4\u635f\u5931\u6539\u8fdbCLIP\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u71b5\u6700\u5c0f\u5316\u7684TTA\u65b9\u6cd5\u4e0eCLIP\u7684\u5bf9\u6bd4\u8bad\u7ec3\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u63d0\u51faCLIPTTA\uff0c\u4f7f\u7528\u8f6f\u5bf9\u6bd4\u635f\u5931\uff0c\u5e76\u6269\u5c55\u81f3\u5f00\u653e\u96c6\u8bbe\u7f6e\uff0c\u5f15\u5165OCE\u635f\u5931\u6539\u8fdbOOD\u68c0\u6d4b\u3002", "result": "\u572875\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u71b5\u7684\u76ee\u6807\uff0c\u5e76\u5728\u591a\u79cd\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "CLIPTTA\u662f\u4e00\u79cd\u6709\u6548\u7684TTA\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5f00\u653e\u96c6\u548c\u5206\u5e03\u504f\u79fb\u573a\u666f\u3002"}}
{"id": "2507.14178", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14178", "abs": "https://arxiv.org/abs/2507.14178", "authors": ["Yuhang Liu", "Yuefei Wu", "Bin Shi", "Bo Dong"], "title": "Feature Bank Enhancement for Distance-based Out-of-Distribution Detection", "comment": "8 pages, 5 figures", "summary": "Out-of-distribution (OOD) detection is critical to ensuring the reliability\nof deep learning applications and has attracted significant attention in recent\nyears. A rich body of literature has emerged to develop efficient score\nfunctions that assign high scores to in-distribution (ID) samples and low\nscores to OOD samples, thereby helping distinguish OOD samples. Among these\nmethods, distance-based score functions are widely used because of their\nefficiency and ease of use. However, deep learning often leads to a biased\ndistribution of data features, and extreme features are inevitable. These\nextreme features make the distance-based methods tend to assign too low scores\nto ID samples. This limits the OOD detection capabilities of such methods. To\naddress this issue, we propose a simple yet effective method, Feature Bank\nEnhancement (FBE), that uses statistical characteristics from dataset to\nidentify and constrain extreme features to the separation boundaries, therapy\nmaking the distance between samples inside and outside the distribution\nfarther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10\nrespectively, and the results show that our method achieves state-of-the-art\nperformance on both benchmark. Additionally, theoretical analysis and\nsupplementary experiments are conducted to provide more insights into our\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFBE\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6781\u7aef\u7279\u5f81\u63d0\u5347OOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u8ddd\u79bb\u57fa\u4e8e\u7684OOD\u68c0\u6d4b\u65b9\u6cd5\u56e0\u6781\u7aef\u7279\u5f81\u5bfc\u81f4ID\u6837\u672c\u5f97\u5206\u8fc7\u4f4e\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6570\u636e\u96c6\u7684\u7edf\u8ba1\u7279\u5f81\u8bc6\u522b\u5e76\u7ea6\u675f\u6781\u7aef\u7279\u5f81\uff0c\u589e\u5927\u5206\u5e03\u5185\u5916\u6837\u672c\u7684\u8ddd\u79bb\u3002", "result": "\u5728ImageNet-1k\u548cCIFAR-10\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "FBE\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.14538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14538", "abs": "https://arxiv.org/abs/2507.14538", "authors": ["Jin Chai", "Xiang Yao", "Mengfan Hou", "Yanghong Li", "Erbao Dong"], "title": "A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0", "comment": null, "summary": "CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid\ntendon-driven actuation system that combines shape memory alloys (SMAs) and DC\nmotors. The hand employs high-strength fishing line as artificial tendons and\nuses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal\nand tendon-muscle structure of the human hand. A linear motor-driven module\ncontrols finger flexion, while an SMA-based module enables finger extension and\nlateral abduction. These modules are integrated into a compact hybrid actuation\nunit mounted on a custom rear support structure. Mechanical and kinematic\nexperiments, conducted under an Arduino Mega 2560-based control system,\nvalidate the effectiveness of the design and demonstrate its biomimetic\ndexterity.", "AI": {"tldr": "CYJ Hand-0\u662f\u4e00\u79cd21\u81ea\u7531\u5ea6\u4eff\u4eba\u7075\u5de7\u624b\uff0c\u91c7\u7528\u6df7\u5408\u808c\u8171\u9a71\u52a8\u7cfb\u7edf\uff08SMAs\u548cDC\u7535\u673a\uff09\uff0c\u901a\u8fc73D\u6253\u5370\u91d1\u5c5e\u6846\u67b6\u548c\u4eba\u5de5\u808c\u8171\u6a21\u62df\u4eba\u624b\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u4eff\u751f\u7075\u5de7\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u4eff\u4eba\u7075\u5de7\u624b\uff0c\u7ed3\u5408SMAs\u548cDC\u7535\u673a\u7684\u4f18\u52bf\uff0c\u6a21\u62df\u4eba\u624b\u7684\u9aa8\u9abc\u548c\u808c\u8171\u808c\u8089\u7ed3\u6784\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7075\u6d3b\u6027\u548c\u529f\u80fd\u6027\u3002", "method": "\u4f7f\u75283D\u6253\u5370AlSi10Mg\u91d1\u5c5e\u6846\u67b6\u548c\u9ad8\u5f3a\u5ea6\u9493\u9c7c\u7ebf\u4f5c\u4e3a\u4eba\u5de5\u808c\u8171\uff0c\u7ebf\u6027\u7535\u673a\u63a7\u5236\u624b\u6307\u5c48\u66f2\uff0cSMA\u6a21\u5757\u63a7\u5236\u624b\u6307\u4f38\u5c55\u548c\u5916\u5c55\uff0c\u96c6\u6210\u5230\u7d27\u51d1\u7684\u6df7\u5408\u9a71\u52a8\u5355\u5143\u4e2d\u3002", "result": "\u673a\u68b0\u548c\u8fd0\u52a8\u5b66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4eff\u751f\u7075\u5de7\u6027\u3002", "conclusion": "CYJ Hand-0\u7684\u8bbe\u8ba1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u4eff\u4eba\u7075\u5de7\u624b\u7684\u6f5c\u529b\uff0c\u7ed3\u5408\u4e86SMAs\u548cDC\u7535\u673a\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.14315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6ce8\u610f\u529b\u805a\u7126\uff08AF\uff09\u7684\u673a\u5236\uff0c\u901a\u8fc7\u526a\u9664\u975e\u4fe1\u606f\u6027\u6807\u8bb0\u6765\u4f18\u5316\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\uff08GCD\uff09\u4e2d\u7684\u7279\u5f81\u63d0\u53d6\u3002", "motivation": "\u73b0\u6709GCD\u65b9\u6cd5\u5728\u5904\u7406\u672a\u6807\u8bb0\u6570\u636e\u65f6\u5bb9\u6613\u5206\u6563\u6ce8\u610f\u529b\uff0c\u5173\u6ce8\u4efb\u52a1\u65e0\u5173\u7684\u80cc\u666f\u533a\u57df\uff0c\u5bfc\u81f4\u7279\u5f81\u63d0\u53d6\u4e0d\u7406\u60f3\u3002", "method": "AF\u7531\u4e24\u4e2a\u7ec4\u4ef6\u7ec4\u6210\uff1a\u6807\u8bb0\u91cd\u8981\u6027\u6d4b\u91cf\uff08TIME\uff09\u548c\u591a\u5c3a\u5ea6\u6807\u8bb0\u81ea\u9002\u5e94\u526a\u679d\uff08TAP\uff09\uff0c\u901a\u8fc7\u91cf\u5316\u6807\u8bb0\u91cd\u8981\u6027\u5e76\u526a\u9664\u975e\u4fe1\u606f\u6027\u6807\u8bb0\u6765\u4f18\u5316\u6a21\u578b\u6ce8\u610f\u529b\u3002", "result": "\u5c06AF\u96c6\u6210\u5230SimGCD\u65b9\u6cd5\u4e2d\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe15.4%\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "AF\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\uff0c\u80fd\u663e\u8457\u63d0\u5347GCD\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14179", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.14179", "abs": "https://arxiv.org/abs/2507.14179", "authors": ["Nobel Dhar", "Bobin Deng", "Md Romyull Islam", "Xinyue Zhang", "Kazi Fahim Ahmad Nasif", "Kun Suo"], "title": "A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering", "comment": "To be published in Euro-Par 2025", "summary": "Large Language Models (LLMs) exhibit significant activation sparsity, where\nonly a subset of neurons are active for a given input. Although this sparsity\npresents opportunities to reduce computational cost, efficiently utilizing it\nrequires predicting activation patterns in a scalable manner. However, direct\nprediction at the neuron level is computationally expensive due to the vast\nnumber of neurons in modern LLMs. To enable efficient prediction and\nutilization of activation sparsity, we propose a clustering-based activation\npattern compression framework. Instead of treating each neuron independently,\nwe group similar activation patterns into a small set of representative\nclusters. Our method achieves up to 79.34% clustering precision, outperforming\nstandard binary clustering approaches while maintaining minimal degradation in\nperplexity (PPL) scores. With a sufficiently large number of clusters, our\napproach attains a PPL score as low as 12.49, demonstrating its effectiveness\nin preserving model quality while reducing computational overhead. By\npredicting cluster assignments rather than individual neuron states, future\nmodels can efficiently infer activation patterns from pre-computed centroids.\nWe detail the clustering algorithm, analyze its effectiveness in capturing\nmeaningful activation structures, and demonstrate its potential to improve\nsparse computation efficiency. This clustering-based formulation serves as a\nfoundation for future work on activation pattern prediction, paving the way for\nefficient inference in large-scale language models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u76f8\u4f3c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u4e3a\u5c11\u91cf\u4ee3\u8868\u6027\u805a\u7c7b\uff0c\u6709\u6548\u9884\u6d4b\u548c\u5229\u7528LLMs\u4e2d\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\u3002", "motivation": "LLMs\u4e2d\u5b58\u5728\u663e\u8457\u7684\u6fc0\u6d3b\u7a00\u758f\u6027\uff0c\u4f46\u76f4\u63a5\u9884\u6d4b\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u6fc0\u6d3b\u6a21\u5f0f\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u5229\u7528\u8fd9\u79cd\u7a00\u758f\u6027\u3002", "method": "\u91c7\u7528\u805a\u7c7b\u65b9\u6cd5\uff0c\u5c06\u76f8\u4f3c\u7684\u6fc0\u6d3b\u6a21\u5f0f\u5206\u7ec4\u4e3a\u5c11\u91cf\u4ee3\u8868\u6027\u805a\u7c7b\uff0c\u800c\u975e\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u795e\u7ecf\u5143\u3002", "result": "\u805a\u7c7b\u7cbe\u5ea6\u8fbe79.34%\uff0c\u56f0\u60d1\u5ea6\uff08PPL\uff09\u4ec5\u8f7b\u5fae\u4e0b\u964d\uff08\u6700\u4f4e12.49\uff09\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6fc0\u6d3b\u6a21\u5f0f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2507.14582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14582", "abs": "https://arxiv.org/abs/2507.14582", "authors": ["Zezhi Liu", "Shizhen Wu", "Hanqian Luo", "Deyun Qin", "Yongchun Fang"], "title": "BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives", "comment": "11 pages, 8 figures", "summary": "In the field of Learning from Demonstration (LfD), enabling robots to\ngeneralize learned manipulation skills to novel scenarios for long-horizon\ntasks remains challenging. Specifically, it is still difficult for robots to\nadapt the learned skills to new environments with different task and motion\nrequirements, especially in long-horizon, multi-stage scenarios with intricate\nconstraints. This paper proposes a novel hierarchical framework, called\nBT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and\nDynamical Movement Primitives (DMPs) to address this problem. Within this\nframework, Signal Temporal Logic (STL) is employed to formally specify complex,\nlong-horizon task requirements and constraints. These STL specifications are\nsystematically transformed to generate reactive and modular BTs for high-level\ndecision-making task structure. An STL-constrained DMP optimization method is\nproposed to optimize the DMP forcing term, allowing the learned motion\nprimitives to adapt flexibly while satisfying intricate spatiotemporal\nrequirements and, crucially, preserving the essential dynamics learned from\ndemonstrations. The framework is validated through simulations demonstrating\ngeneralization capabilities under various STL constraints and real-world\nexperiments on several long-horizon robotic manipulation tasks. The results\ndemonstrate that the proposed framework effectively bridges the symbolic-motion\ngap, enabling more reliable and generalizable autonomous manipulation for\ncomplex robotic tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBT-TL-DMPs\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u884c\u4e3a\u6811\u3001\u65f6\u5e8f\u903b\u8f91\u548c\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u6f14\u793a\u6280\u80fd\u5728\u65b0\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u5728\u6f14\u793a\u5b66\u4e60\uff08LfD\uff09\u4e2d\uff0c\u673a\u5668\u4eba\u96be\u4ee5\u5c06\u5b66\u5230\u7684\u6280\u80fd\u6cdb\u5316\u5230\u5177\u6709\u4e0d\u540c\u4efb\u52a1\u548c\u8fd0\u52a8\u9700\u6c42\u7684\u65b0\u73af\u5883\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u957f\u65f6\u7a0b\u3001\u591a\u9636\u6bb5\u7684\u590d\u6742\u7ea6\u675f\u573a\u666f\u4e2d\u3002", "method": "\u6846\u67b6\u6574\u5408\u4e86\u884c\u4e3a\u6811\uff08BT\uff09\u3001\u65f6\u5e8f\u903b\u8f91\uff08TL\uff09\u548c\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff08DMPs\uff09\uff0c\u4f7f\u7528\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\u4efb\u52a1\u9700\u6c42\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u6a21\u5757\u5316\u7684\u884c\u4e3a\u6811\uff0c\u540c\u65f6\u4f18\u5316DMP\u4ee5\u6ee1\u8db3\u65f6\u7a7a\u7ea6\u675f\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6846\u67b6\u80fd\u591f\u6709\u6548\u6cdb\u5316\u6280\u80fd\uff0c\u6ee1\u8db3\u590d\u6742\u4efb\u52a1\u9700\u6c42\uff0c\u5e76\u5f25\u5408\u7b26\u53f7\u4e0e\u8fd0\u52a8\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u64cd\u4f5c\u53ef\u9760\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "\u751f\u6210\u8d85\u5206\u8fa8\u7387\uff08GSR\uff09\u5728\u611f\u77e5\u56fe\u50cf\u8d28\u91cf\u4e0a\u9886\u5148\uff0c\u4f46\u5b58\u5728\u4e0e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff08LRI\uff09\u6216\u771f\u5b9e\u56fe\u50cf\uff08GTI\uff09\u4e0d\u5339\u914d\u7684\u4f2a\u5f71\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u201c\u5e7b\u89c9\u8bc4\u5206\u201d\uff08HS\uff09\u6765\u8861\u91cf\u548c\u7f13\u89e3\u8fd9\u4e9b\u4f2a\u5f71\u3002", "motivation": "GSR\u6a21\u578b\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u7684\u7ec6\u8282\u4e0eLRI\u6216GTI\u4e0d\u5339\u914d\u7684\u4f2a\u5f71\u95ee\u9898\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528MLLM\u6784\u5efa\u63d0\u793a\u8bc4\u4f30\u5e7b\u89c9\u89c6\u89c9\u5143\u7d20\u5e76\u751f\u6210HS\uff0c\u540c\u65f6\u63d0\u51fa\u4f7f\u7528\u4e0eHS\u5f3a\u76f8\u5173\u7684\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u4f5c\u4e3a\u53ef\u5fae\u5956\u52b1\u51fd\u6570\u6765\u5bf9\u9f50GSR\u6a21\u578b\u3002", "result": "HS\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u4e3a\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u56fe\u50cf\u6307\u6807\u63d0\u4f9b\u4e86\u8865\u5145\u89c1\u89e3\u3002", "conclusion": "\u901a\u8fc7HS\u548c\u6df1\u5ea6\u7279\u5f81\u8ddd\u79bb\u7684\u5956\u52b1\u51fd\u6570\uff0c\u53ef\u4ee5\u6709\u6548\u7f13\u89e3GSR\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002"}}
{"id": "2507.14180", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14180", "abs": "https://arxiv.org/abs/2507.14180", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "title": "Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems", "comment": null, "summary": "In line with the AI-native 6G vision, explainability and robustness are\ncrucial for building trust and ensuring reliable performance in millimeter-wave\n(mmWave) systems. Efficient beam alignment is essential for initial access, but\ndeep learning (DL) solutions face challenges, including high data collection\noverhead, hardware constraints, lack of explainability, and susceptibility to\nadversarial attacks. This paper proposes a robust and explainable DL-based beam\nalignment engine (BAE) for mmWave multiple-input multiple output (MIMO)\nsystems. The BAE uses received signal strength indicator (RSSI) measurements\nfrom wide beams to predict the best narrow beam, reducing the overhead of\nexhaustive beam sweeping. To overcome the challenge of real-world data\ncollection, this work leverages a site-specific digital twin (DT) to generate\nsynthetic channel data closely resembling real-world environments. A model\nrefinement via transfer learning is proposed to fine-tune the pre-trained model\nresiding in the DT with minimal real-world data, effectively bridging\nmismatches between the digital replica and real-world environments. To reduce\nbeam training overhead and enhance transparency, the framework uses deep\nShapley additive explanations (SHAP) to rank input features by importance,\nprioritizing key spatial directions and minimizing beam sweeping. It also\nincorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a\ncredibility metric for detecting out-of-distribution inputs and ensuring\nrobust, transparent decision-making. Experimental results show that the\nproposed framework reduces real-world data needs by 70%, beam training overhead\nby 62%, and improves outlier detection robustness by up to 8.5x, achieving\nnear-optimal spectral efficiency and transparent decision making compared to\ntraditional softmax based DL models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u6ce2\u675f\u5bf9\u9f50\u5f15\u64ce\uff08BAE\uff09\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2MIMO\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u548c\u8fc1\u79fb\u5b66\u4e60\u51cf\u5c11\u6570\u636e\u9700\u6c42\uff0c\u5e76\u5229\u7528SHAP\u548cDkNN\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u57286G\u613f\u666f\u4e0b\uff0c\u6beb\u7c73\u6ce2\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u5bf9\u5efa\u7acb\u4fe1\u4efb\u548c\u786e\u4fdd\u53ef\u9760\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709DL\u89e3\u51b3\u65b9\u6848\u9762\u4e34\u6570\u636e\u6536\u96c6\u5f00\u9500\u5927\u3001\u786c\u4ef6\u9650\u5236\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5fae\u8c03\u6a21\u578b\uff1b\u5229\u7528SHAP\u5bf9\u8f93\u5165\u7279\u5f81\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u51cf\u5c11\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\uff1b\u7ed3\u5408DkNN\u68c0\u6d4b\u5f02\u5e38\u8f93\u5165\uff0c\u786e\u4fdd\u51b3\u7b56\u900f\u660e\u548c\u9c81\u68d2\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u51cf\u5c1170%\u771f\u5b9e\u6570\u636e\u9700\u6c42\u300162%\u6ce2\u675f\u8bad\u7ec3\u5f00\u9500\uff0c\u5f02\u5e38\u68c0\u6d4b\u9c81\u68d2\u6027\u63d0\u53478.5\u500d\uff0c\u9891\u8c31\u6548\u7387\u63a5\u8fd1\u6700\u4f18\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u5728\u51cf\u5c11\u6570\u636e\u9700\u6c42\u548c\u8bad\u7ec3\u5f00\u9500\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u6beb\u7c73\u6ce2\u7cfb\u7edf\u7684DL\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14605", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14605", "abs": "https://arxiv.org/abs/2507.14605", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition", "comment": null, "summary": "Online optimal control of quadrupedal robots would enable them to plan their\nmovement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged\nas a practical approach for real-time control. In LMPC, an optimization problem\nwith a quadratic cost and linear constraints is formulated over a finite\nhorizon and solved on the fly. However, LMPC relies on linearizing the\nequations of motion (EOM), which may lead to poor solution quality. In this\npaper, we use Koopman operator theory and the Extended Dynamic Mode\nDecomposition (EDMD) to create a linear model of the system in high dimensional\nspace, thus retaining the nonlinearity of the EOM. We model the aerial phase\nand ground contact phases using different linear models. Then, using LMPC, we\ndemonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait\ntransitions in level and rough terrains. The main novelty is the use of Koopman\noperator theory to create hybrid models of a quadrupedal system and demonstrate\nthe online generation of multiple gaits and gaits transitions.", "AI": {"tldr": "\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u548cEDMD\u65b9\u6cd5\u521b\u5efa\u9ad8\u7ef4\u7ebf\u6027\u6a21\u578b\uff0c\u7ed3\u5408LMPC\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u7684\u591a\u79cd\u6b65\u6001\u5728\u7ebf\u751f\u6210\u4e0e\u8f6c\u6362\u3002", "motivation": "\u89e3\u51b3LMPC\u56e0\u7ebf\u6027\u5316\u8fd0\u52a8\u65b9\u7a0b\u5bfc\u81f4\u7684\u89e3\u8d28\u91cf\u5dee\u95ee\u9898\uff0c\u4fdd\u7559\u975e\u7ebf\u6027\u7279\u6027\u4ee5\u5b9e\u73b0\u66f4\u4f18\u63a7\u5236\u3002", "method": "\u7ed3\u5408Koopman\u7b97\u5b50\u7406\u8bba\u548cEDMD\u65b9\u6cd5\u6784\u5efa\u9ad8\u7ef4\u7ebf\u6027\u6a21\u578b\uff0c\u5206\u9636\u6bb5\u5efa\u6a21\u7a7a\u4e2d\u4e0e\u5730\u9762\u63a5\u89e6\uff0c\u5e94\u7528LMPC\u8fdb\u884c\u63a7\u5236\u3002", "result": "\u5728\u5e73\u5766\u548c\u5d0e\u5c96\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u8df3\u8dc3\u3001\u5c0f\u8dd1\u53ca\u6b65\u6001\u8f6c\u6362\u3002", "conclusion": "Koopman\u7b97\u5b50\u7406\u8bba\u80fd\u6709\u6548\u6784\u5efa\u6df7\u5408\u6a21\u578b\uff0c\u652f\u6301\u5728\u7ebf\u751f\u6210\u591a\u6b65\u6001\u53ca\u8f6c\u6362\uff0c\u63d0\u5347\u63a7\u5236\u8d28\u91cf\u3002"}}
{"id": "2507.14368", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallar\u00e8s-L\u00f3pez", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u6280\u672f\uff0c\u63d0\u4f9b\u534a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8eB\u8d85\u89c6\u9891\u4e2d\u7684\u70b9\u8ffd\u8e2a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u89e3\u5256\u7ed3\u6784\u548c\u8fd0\u52a8\u6a21\u5f0f\u3002", "motivation": "B\u8d85\u56fe\u50cf\u4e2d\u7684\u6591\u70b9\u566a\u58f0\u3001\u4f4e\u8fb9\u7f18\u5bf9\u6bd4\u5ea6\u548c\u5e73\u9762\u5916\u8fd0\u52a8\u4f7f\u5f97\u7ec4\u7ec7\u8fd0\u52a8\u8ffd\u8e2a\u56f0\u96be\uff0c\u5f71\u54cd\u4e34\u5e8a\u548c\u7814\u7a76\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u5149\u6d41\u6280\u672f\uff0c\u5f00\u53d1\u56fe\u5f62\u7528\u6237\u754c\u9762\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u91c7\u7528\u5149\u6d41\u6ee4\u6ce2\u6280\u672f\u51cf\u5c11\u5e27\u95f4\u566a\u58f0\u3002", "result": "DUSTrack\u5728\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u96f6\u6837\u672c\u8ffd\u8e2a\u5668\uff0c\u4e0e\u4e13\u4e1a\u65b9\u6cd5\u76f8\u5f53\uff0c\u9002\u7528\u4e8e\u5fc3\u810f\u58c1\u8fd0\u52a8\u3001\u808c\u8089\u53d8\u5f62\u7b49\u573a\u666f\u3002", "conclusion": "DUSTrack\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\uff0c\u4e3a\u4e34\u5e8a\u548c\u751f\u7269\u529b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7075\u6d3b\u7684\u7ec4\u7ec7\u8fd0\u52a8\u91cf\u5316\u6846\u67b6\u3002"}}
{"id": "2507.14181", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14181", "abs": "https://arxiv.org/abs/2507.14181", "authors": ["Yajiao Dai", "Jun Li", "Zhen Mei", "Yiyang Ni", "Shi Jin", "Zengxiang Li", "Sheng Guo", "Wei Xiang"], "title": "Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis", "comment": "Accepted to IEEE Internet of Things Journal, Early Access. 14 pages,\n  5 figures", "summary": "Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe\noperation of industrial machinery and improving production efficiency. However,\ntraditional supervised deep learning methods require a large amount of training\ndata and labels, which are often located in different clients. Additionally,\nthe cost of data labeling is high, making labels difficult to acquire.\nMeanwhile, differences in data distribution among clients may also hinder the\nmodel's performance. To tackle these challenges, this paper proposes a\nsemi-supervised federated learning framework, SSFL-DCSL, which integrates dual\ncontrastive loss and soft labeling to address data and label scarcity for\ndistributed clients with few labeled samples while safeguarding user privacy.\nIt enables representation learning using unlabeled data on the client side and\nfacilitates joint learning among clients through prototypes, thereby achieving\nmutual knowledge sharing and preventing local model divergence. Specifically,\nfirst, a sample weighting function based on the Laplace distribution is\ndesigned to alleviate bias caused by low confidence in pseudo labels during the\nsemi-supervised training process. Second, a dual contrastive loss is introduced\nto mitigate model divergence caused by different data distributions, comprising\nlocal contrastive loss and global contrastive loss. Third, local prototypes are\naggregated on the server with weighted averaging and updated with momentum to\nshare knowledge among clients. To evaluate the proposed SSFL-DCSL framework,\nexperiments are conducted on two publicly available datasets and a dataset\ncollected on motors from the factory. In the most challenging task, where only\n10\\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by\n1.15% to 7.85% over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6SSFL-DCSL\uff0c\u901a\u8fc7\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\u548c\u8f6f\u6807\u7b7e\u89e3\u51b3\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u6545\u969c\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u6570\u636e\u5206\u5e03\u5dee\u5f02\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0cSSFL-DCSL\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u53cc\u91cd\u5bf9\u6bd4\u635f\u5931\uff08\u5c40\u90e8\u548c\u5168\u5c40\uff09\u548c\u8f6f\u6807\u7b7e\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5206\u5e03\u7684\u6837\u672c\u52a0\u6743\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u805a\u5408\u5b9e\u73b0\u77e5\u8bc6\u5171\u4eab\u3002", "result": "\u5728\u4ec510%\u6807\u6ce8\u6570\u636e\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0cSSFL-DCSL\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u53471.15%\u81f37.85%\u3002", "conclusion": "SSFL-DCSL\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5206\u5e03\u5dee\u5f02\u548c\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14694", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14694", "abs": "https://arxiv.org/abs/2507.14694", "authors": ["Yue Ma", "Kanglei Zhou", "Fuyang Yu", "Frederick W. B. Li", "Xiaohui Liang"], "title": "Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks", "comment": null, "summary": "3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.", "AI": {"tldr": "ProbHMI\u5229\u7528\u53ef\u9006\u7f51\u7edc\u5728\u89e3\u8026\u6f5c\u5728\u7a7a\u95f4\u4e2d\u53c2\u6570\u5316\u59ff\u6001\uff0c\u5b9e\u73b0\u6982\u7387\u52a8\u529b\u5b66\u5efa\u6a21\uff0c\u6709\u6548\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5982\u4eba\u673a\u534f\u4f5c\uff09\u4e2d\uff0c\u91cf\u5316\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5f15\u5165\u53ef\u9006\u7f51\u7edc\u53c2\u6570\u5316\u59ff\u6001\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5efa\u6a21\u6982\u7387\u52a8\u529b\u5b66\uff0c\u663e\u5f0f\u9884\u6d4b\u672a\u6765\u6f5c\u5728\u5206\u5e03\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProbHMI\u9002\u7528\u4e8e\u786e\u5b9a\u6027\u53ca\u591a\u6837\u5316\u9884\u6d4b\uff0c\u652f\u6301\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u3002"}}
{"id": "2507.14426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u89e3\u91ca\u7684affordance grounding\uff0c\u901a\u8fc7\u7ed3\u5408ConceptNet\u548c\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u5e38\u8bc6\u5148\u9a8c\u4e0eCLIP\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u63d0\u5347\u573a\u666f\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u573a\u666f\u4e2d\u5bf9\u8c61\u4e0e\u52a8\u4f5c\u5173\u8054\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u573a\u666f\u7406\u89e3\u7684\u9c81\u68d2\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u96c6\u6210ConceptNet\u548c\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5316\u5e38\u8bc6\u5148\u9a8c\u4e0eCLIP\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u901a\u8fc7\u57fa\u4e8e\u80fd\u91cf\u7684\u63a8\u7406\u5faa\u73af\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u591a\u5bf9\u8c61\u3001\u65e0\u6807\u7b7e\u8bbe\u7f6e\u4e0b\uff0cCRAFT\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CRAFT\u4e3a\u9c81\u68d2\u4e14\u53ef\u4fe1\u7684\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.14182", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14182", "abs": "https://arxiv.org/abs/2507.14182", "authors": ["Xiaotong Luo", "Shengda Zhuo", "Min Chen", "Lichun Li", "Ruizhao Lu", "Wenqi Fan", "Shuqiang Huang", "Yin Tang"], "title": "From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling", "comment": null, "summary": "Financial markets exhibit highly dynamic and complex behaviors shaped by both\nhistorical price trajectories and exogenous narratives, such as news, policy\ninterpretations, and social media sentiment. The heterogeneity in these data\nand the diverse insight of investors introduce biases that complicate the\nmodeling of market dynamics. Unlike prior work, this paper explores the\npotential of bull and bear regimes in investor-driven market dynamics. Through\nempirical analysis on real-world financial datasets, we uncover a dynamic\nrelationship between bias variation and behavioral adaptation, which enhances\ntrend prediction under evolving market conditions. To model this mechanism, we\npropose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified\nframework that jointly embeds temporal price sequences and external contextual\nsignals into a shared latent space where opposing bull and bear forces\nnaturally emerge, forming the foundation for bias representation. Within this\nspace, an inertial pairing module pairs temporally adjacent samples to preserve\nmomentum, while the dual competition mechanism contrasts bullish and bearish\nembeddings to capture behavioral divergence. Together, these components allow\nB4 to model bias-driven asymmetry, behavioral inertia, and market\nheterogeneity. Experimental results on real-world financial datasets\ndemonstrate that our model not only achieves superior performance in predicting\nmarket trends but also provides interpretable insights into the interplay of\nbiases, investor behaviors, and market dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faB4\u6a21\u578b\uff0c\u901a\u8fc7\u5d4c\u5165\u4ef7\u683c\u5e8f\u5217\u548c\u5916\u90e8\u4fe1\u53f7\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u6355\u6349\u725b\u5e02\u548c\u718a\u5e02\u52a8\u6001\uff0c\u63d0\u5347\u5e02\u573a\u8d8b\u52bf\u9884\u6d4b\u3002", "motivation": "\u91d1\u878d\u5e02\u573a\u884c\u4e3a\u590d\u6742\uff0c\u53d7\u5386\u53f2\u4ef7\u683c\u548c\u5916\u90e8\u53d9\u4e8b\u5f71\u54cd\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u548c\u6295\u8d44\u8005\u504f\u89c1\u3002", "method": "\u63d0\u51faB4\u6a21\u578b\uff0c\u7ed3\u5408\u60ef\u6027\u914d\u5bf9\u6a21\u5757\u548c\u53cc\u7ade\u4e89\u673a\u5236\uff0c\u5efa\u6a21\u504f\u89c1\u9a71\u52a8\u7684\u975e\u5bf9\u79f0\u6027\u548c\u884c\u4e3a\u60ef\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aB4\u5728\u9884\u6d4b\u5e02\u573a\u8d8b\u52bf\u4e0a\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u63d0\u4f9b\u504f\u89c1\u3001\u884c\u4e3a\u548c\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "B4\u6a21\u578b\u6709\u6548\u6355\u6349\u5e02\u573a\u5f02\u8d28\u6027\uff0c\u4e3a\u6295\u8d44\u8005\u884c\u4e3a\u548c\u504f\u89c1\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.14700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14700", "abs": "https://arxiv.org/abs/2507.14700", "authors": ["Nicholas Mohammad", "Nicola Bezzo"], "title": "Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation", "comment": "To be presented in the 64th IEEE Conference on Decision and Control\n  (CDC 25)", "summary": "Safe navigation in unknown and cluttered environments remains a challenging\nproblem in robotics. Model Predictive Contour Control (MPCC) has shown promise\nfor performant obstacle avoidance by enabling precise and agile trajectory\ntracking, however, existing methods lack formal safety assurances. To address\nthis issue, we propose a general Control Lyapunov Function (CLF) and Control\nBarrier Function (CBF) enabled MPCC framework that enforces safety constraints\nderived from a free-space corridor around the planned trajectory. To enhance\nfeasibility, we dynamically adapt the CBF parameters at runtime using a Soft\nActor-Critic (SAC) policy. The approach is validated with extensive simulations\nand an experiment on mobile robot navigation in unknown cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7SAC\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\u4ee5\u63d0\u9ad8\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709MPCC\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\uff0c\u65e0\u6cd5\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\u3002", "method": "\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\uff0c\u5229\u7528SAC\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u79fb\u52a8\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u5bfc\u822a\u3002"}}
{"id": "2507.14432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u76843DGS\u89c6\u9891\u6d41\u5a92\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u548c\u5e26\u5bbd\u9002\u5e94\u3002", "motivation": "3DGS\u89c6\u9891\u7684\u6570\u636e\u91cf\u5927\u4e14\u538b\u7f29\u4f20\u8f93\u590d\u6742\uff0c\u4f20\u7edf\u6d41\u5a92\u4f53\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u3002", "method": "\u57fa\u4e8e\u9ad8\u65af\u53d8\u5f62\u573a\u6784\u5efa3DGS\u89c6\u9891\uff0c\u91c7\u7528\u6df7\u5408\u663e\u8457\u6027\u5206\u5757\u548c\u5dee\u5f02\u5316\u8d28\u91cf\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u9891\u8d28\u91cf\u3001\u538b\u7f29\u6548\u7387\u548c\u4f20\u8f93\u901f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e863DGS\u89c6\u9891\u6d41\u5a92\u4f53\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u4f20\u8f93\u6027\u80fd\u3002"}}
{"id": "2507.14204", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14204", "abs": "https://arxiv.org/abs/2507.14204", "authors": ["Dachuan Shi", "Yonggan Fu", "Xiangchi Yuan", "Zhongzhi Yu", "Haoran You", "Sixu Li", "Xin Dong", "Jan Kautz", "Pavlo Molchanov", "Yingyan", "Lin"], "title": "LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models", "comment": "ICML 2025. Code: https://github.com/GATECH-EIC/LaCache", "summary": "Recent advancements in Large Language Models (LLMs) have spurred interest in\nnumerous applications requiring robust long-range capabilities, essential for\nprocessing extensive input contexts and continuously generating extended\noutputs. As sequence lengths increase, the number of Key-Value (KV) pairs in\nLLMs escalates, creating a significant efficiency bottleneck. In this paper, we\npropose a new KV cache optimization paradigm called LaCache, a training-free\nmethod for efficient and accurate generative inference of LLMs. LaCache enables\nLLMs to simultaneously address both of the critical challenges in long-range\nmodeling: robust long-range capabilities and continuous generation without\nrunning out-of-memory (OOM). Specifically, LaCache integrates two key\ninnovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only\nsequentially (left-to-right within each layer) but also across layers (from\nshallow to deep), providing an extended span for capturing long-range\ndependencies under a fixed storage budget, thereby boosting long-range\ncapabilities; and (2) an iterative compaction mechanism that progressively\ncompresses older caches, freeing up space for new tokens within a fixed cache\nsize. This token distance-based dynamic compression enables more effective\ncontinuous generation under constrained cache budgets. Experiments across\nvarious tasks, benchmarks, and LLM models consistently validate LaCache's\neffectiveness in enhancing LLMs' long-range capabilities. Our code is available\nat https://github.com/GATECH-EIC/LaCache.", "AI": {"tldr": "LaCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u9636\u68af\u5f62KV\u7f13\u5b58\u6a21\u5f0f\u548c\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff0c\u63d0\u5347LLMs\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u80fd\u529b\u548c\u8fde\u7eed\u751f\u6210\u6548\u7387\u3002", "motivation": "\u968f\u7740\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\uff0cLLMs\u4e2d\u7684KV\u5bf9\u6570\u91cf\u6fc0\u589e\uff0c\u5bfc\u81f4\u6548\u7387\u74f6\u9888\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u540c\u65f6\u89e3\u51b3\u957f\u8ddd\u79bb\u5efa\u6a21\u548c\u5185\u5b58\u4e0d\u8db3\u95ee\u9898\u3002", "method": "LaCache\u91c7\u7528\u9636\u68af\u5f62KV\u7f13\u5b58\u6a21\u5f0f\uff08\u8de8\u5c42\u5b58\u50a8KV\u5bf9\uff09\u548c\u8fed\u4ee3\u538b\u7f29\u673a\u5236\uff08\u52a8\u6001\u538b\u7f29\u65e7\u7f13\u5b58\uff09\uff0c\u5728\u56fa\u5b9a\u7f13\u5b58\u9884\u7b97\u4e0b\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLaCache\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u957f\u8ddd\u79bb\u80fd\u529b\u3002", "conclusion": "LaCache\u4e3aLLMs\u7684\u957f\u8ddd\u79bb\u5efa\u6a21\u548c\u8fde\u7eed\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14721", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14721", "abs": "https://arxiv.org/abs/2507.14721", "authors": ["Keita Kobashi", "Masayoshi Tomizuka"], "title": "Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls", "comment": "7 pages, 7 figures", "summary": "This study addresses the problem of occluded grasping, where primary grasp\nconfigurations of an object are not available due to occlusion with\nenvironment. Simple parallel grippers often struggle with such tasks due to\nlimited dexterity and actuation constraints. Prior works have explored object\npose reorientation such as pivoting by utilizing extrinsic contacts between an\nobject and an environment feature like a wall, to make the object graspable.\nHowever, such works often assume the presence of a short wall, and this\nassumption may not always hold in real-world scenarios. If the wall available\nfor interaction is too large or too tall, the robot may still fail to grasp the\nobject even after pivoting, and the robot must combine different types of\nactions to grasp. To address this, we propose a hierarchical reinforcement\nlearning (RL) framework. We use Q-learning to train a high-level policy that\nselects the type of action expected to yield the highest reward. The selected\nlow-level skill then samples a specific robot action in continuous space. To\nguide the robot to an appropriate location for executing the selected action,\nwe adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on\nthe object point cloud and the skill ID, enabling it to infer a suitable\nlocation based on the object geometry and the selected skill. To promote\ngeneralization, we apply domain randomization during the training of low-level\nskills. The RL policy is trained entirely in simulation with a box-like object\nand deployed to six objects in real world. We conduct experiments to evaluate\nour method and demonstrate both its generalizability and robust sim-to-real\ntransfer performance with promising success rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u906e\u6321\u6293\u53d6\u95ee\u9898\uff0c\u901a\u8fc7\u9ad8\u5c42\u7b56\u7565\u9009\u62e9\u52a8\u4f5c\u7c7b\u578b\uff0c\u4f4e\u5c42\u6280\u80fd\u6267\u884c\u5177\u4f53\u52a8\u4f5c\uff0c\u5e76\u7ed3\u5408CVAE\u548c\u57df\u968f\u673a\u5316\u5b9e\u73b0\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3\u56e0\u73af\u5883\u906e\u6321\u5bfc\u81f4\u7684\u4e3b\u8981\u6293\u53d6\u914d\u7f6e\u4e0d\u53ef\u7528\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5e73\u884c\u5939\u6301\u5668\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u7684\u77ed\u5899\u6761\u4ef6\u5728\u73b0\u5b9e\u4e2d\u53ef\u80fd\u4e0d\u6210\u7acb\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9ad8\u5c42\u7b56\u7565\u4f7f\u7528Q\u5b66\u4e60\u9009\u62e9\u52a8\u4f5c\u7c7b\u578b\uff0c\u4f4e\u5c42\u6280\u80fd\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u6267\u884c\u5177\u4f53\u52a8\u4f5c\uff0c\u7ed3\u5408CVAE\u63a8\u65ad\u5408\u9002\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u57df\u968f\u673a\u5316\u8bad\u7ec3\u4f4e\u5c42\u6280\u80fd\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u540e\uff0c\u6210\u529f\u90e8\u7f72\u5230\u771f\u5b9e\u4e16\u754c\u7684\u516d\u79cd\u7269\u4f53\u4e0a\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u906e\u6321\u6293\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7269\u4f53\u548c\u73af\u5883\u6761\u4ef6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "IRGPT\u662f\u9996\u4e2a\u9488\u5bf9\u771f\u5b9e\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u7ea2\u5916-\u6587\u672c\u6570\u636e\u96c6\uff08IR-TD\uff09\uff0c\u901a\u8fc7\u53cc\u5411\u8de8\u6a21\u6001\u8bfe\u7a0b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u7ea2\u5916\u56fe\u50cf\u7684\u5c40\u9650\u6027\uff0c\u6355\u6349\u7ea2\u5916\u6a21\u6001\u7684\u72ec\u7279\u7279\u6027\u3002", "method": "\u6784\u5efaIR-TD\u6570\u636e\u96c6\uff0826\u4e07\u771f\u5b9e\u56fe\u50cf-\u6587\u672c\u5bf9\uff09\uff0c\u63d0\u51fa\u53cc\u5411\u8de8\u6a21\u6001\u8bfe\u7a0b\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u57289\u9879\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "IRGPT\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ea2\u5916\u56fe\u50cf\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.14215", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.14215", "abs": "https://arxiv.org/abs/2507.14215", "authors": ["Jiayu", "Liu"], "title": "Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired", "comment": null, "summary": "This study aims to develop a deep learning system for an accessibility device\nfor the deaf or hearing impaired. The device will accurately localize and\nidentify sound sources in real time. This study will fill an important gap in\ncurrent research by leveraging machine learning techniques to target the\nunderprivileged community. The system includes three main components. 1.\nJerryNet: A custom designed CNN architecture that determines the direction of\narrival (DoA) for nine possible directions. 2. Audio Classification: This model\nis based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model\nto identify the exact sound classes only based on audio. 3. Multimodal\nintegration model: This is an accurate sound localization model that combines\naudio, visual, and text data to locate the exact sound sources in the images.\nThe part consists of two modules, one object detection using Yolov9 to generate\nall the bounding boxes of the objects, and an audio visual localization model\nto identify the optimal bounding box using complete Intersection over Union\n(CIoU). The hardware consists of a four-microphone rectangular formation and a\ncamera mounted on glasses with a wristband for displaying necessary information\nlike direction. On a custom collected data set, JerryNet achieved a precision\nof 91. 1% for the sound direction, outperforming all the baseline models. The\nCLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,\nrespectively. The audio-visual localization model within component 3 yielded a\ncIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are\nmany future potentials to this study, paving the way to creating a new\ngeneration of accessibility devices.", "AI": {"tldr": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u4e3a\u542c\u969c\u4eba\u58eb\u63d0\u4f9b\u5b9e\u65f6\u58f0\u97f3\u5b9a\u4f4d\u4e0e\u8bc6\u522b\u8bbe\u5907\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u9488\u5bf9\u542c\u969c\u4eba\u58eb\u9700\u6c42\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u5f00\u53d1\u5b9e\u65f6\u58f0\u97f3\u5b9a\u4f4d\u4e0e\u8bc6\u522b\u8bbe\u5907\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u5305\u62ec\u4e09\u4e2a\u7ec4\u4ef6\uff1aJerryNet\uff08CNN\u67b6\u6784\u786e\u5b9a\u58f0\u97f3\u65b9\u5411\uff09\u3001\u97f3\u9891\u5206\u7c7b\uff08\u57fa\u4e8eCLAP\u6a21\u578b\uff09\u3001\u591a\u6a21\u6001\u96c6\u6210\u6a21\u578b\uff08\u7ed3\u5408\u97f3\u9891\u3001\u89c6\u89c9\u548c\u6587\u672c\u6570\u636e\uff09\u3002", "result": "JerryNet\u65b9\u5411\u7cbe\u5ea691.1%\uff0cCLAP\u6a21\u578b\u5728\u81ea\u5b9a\u4e49\u548cAudioSet\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.5%\u548c95%\u51c6\u786e\u7387\uff0c\u591a\u6a21\u6001\u6a21\u578bcIoU\u4e3a0.892\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65b0\u4e00\u4ee3\u65e0\u969c\u788d\u8bbe\u5907\u94fa\u5e73\u9053\u8def\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.14731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14731", "abs": "https://arxiv.org/abs/2507.14731", "authors": ["Haitong Wang", "Aaron Hao Tan", "Angus Fung", "Goldie Nejat"], "title": "X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots", "comment": null, "summary": "Existing navigation methods are primarily designed for specific robot\nembodiments, limiting their generalizability across diverse robot platforms. In\nthis paper, we introduce X-Nav, a novel framework for end-to-end\ncross-embodiment navigation where a single unified policy can be deployed\nacross various embodiments for both wheeled and quadrupedal robots. X-Nav\nconsists of two learning stages: 1) multiple expert policies are trained using\ndeep reinforcement learning with privileged observations on a wide range of\nrandomly generated robot embodiments; and 2) a single general policy is\ndistilled from the expert policies via navigation action chunking with\ntransformer (Nav-ACT). The general policy directly maps visual and\nproprioceptive observations to low-level control commands, enabling\ngeneralization to novel robot embodiments. Simulated experiments demonstrated\nthat X-Nav achieved zero-shot transfer to both unseen embodiments and\nphotorealistic environments. A scalability study showed that the performance of\nX-Nav improves when trained with an increasing number of randomly generated\nembodiments. An ablation study confirmed the design choices of X-Nav.\nFurthermore, real-world experiments were conducted to validate the\ngeneralizability of X-Nav in real-world environments.", "AI": {"tldr": "X-Nav\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8de8\u4f53\u73b0\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\uff08\u4e13\u5bb6\u7b56\u7565\u8bad\u7ec3\u548c\u901a\u7528\u7b56\u7565\u84b8\u998f\uff09\u5b9e\u73b0\u5355\u4e00\u7b56\u7565\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u8de8\u5e73\u53f0\u7684\u901a\u7528\u6027\u3002", "method": "1) \u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u591a\u4e2a\u4e13\u5bb6\u7b56\u7565\uff1b2) \u901a\u8fc7Nav-ACT\u84b8\u998f\u51fa\u901a\u7528\u7b56\u7565\u3002", "result": "X-Nav\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u5b9e\u73b0\u4e86\u5bf9\u65b0\u673a\u5668\u4eba\u5e73\u53f0\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "conclusion": "X-Nav\u5c55\u793a\u4e86\u8de8\u4f53\u73b0\u5bfc\u822a\u7684\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.14452", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGestalt\u539f\u5219\u7684\u5e76\u884c\u4ea4\u4e92\u7f51\u7edc\uff08GPI-Net\uff09\uff0c\u7528\u4e8e\u70b9\u4e91\u914d\u51c6\u4e2d\u7684\u9ad8\u8d28\u91cf\u5bf9\u5e94\u5173\u7cfb\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u878d\u5408\u4e2d\u7684\u5197\u4f59\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u95ee\u9898\u3002", "method": "\u5229\u7528Gestalt\u539f\u5219\u8bbe\u8ba1\u6b63\u4ea4\u51e0\u4f55\u4e00\u81f4\u6027\u7b56\u7565\uff0c\u7ed3\u5408Gestalt\u7279\u5f81\u6ce8\u610f\u529b\u5757\u548c\u53cc\u8def\u5f84\u591a\u7c92\u5ea6\u5e76\u884c\u4ea4\u4e92\u805a\u5408\u5757\u3002", "result": "\u5728\u591a\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GPI-Net\u901a\u8fc7Gestalt\u539f\u5219\u548c\u5e76\u884c\u4ea4\u4e92\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86\u70b9\u4e91\u914d\u51c6\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14217", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14217", "abs": "https://arxiv.org/abs/2507.14217", "authors": ["Tudor Matei Opran", "Samir Loudni"], "title": "Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation", "comment": null, "summary": "We address the pattern explosion problem in pattern mining by proposing an\ninteractive learning framework that combines nonlinear utility aggregation with\ngeometry-aware query selection. Our method models user preferences through a\nChoquet integral over multiple interestingness measures and exploits the\ngeometric structure of the version space to guide the selection of informative\ncomparisons. A branch-and-bound strategy with tight distance bounds enables\nefficient identification of queries near the decision boundary. Experiments on\nUCI datasets show that our approach outperforms existing methods such as\nChoquetRank, achieving better ranking accuracy with fewer user interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u975e\u7ebf\u6027\u6548\u7528\u805a\u5408\u548c\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\u7684\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u7684\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6a21\u5f0f\u6316\u6398\u4e2d\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5b66\u4e60\u6846\u67b6\u66f4\u9ad8\u6548\u5730\u5efa\u6a21\u7528\u6237\u504f\u597d\u3002", "method": "\u4f7f\u7528Choquet\u79ef\u5206\u5efa\u6a21\u7528\u6237\u504f\u597d\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u67e5\u8be2\u9009\u62e9\u548c\u5206\u652f\u5b9a\u754c\u7b56\u7565\u9ad8\u6548\u8bc6\u522b\u51b3\u7b56\u8fb9\u754c\u9644\u8fd1\u7684\u67e5\u8be2\u3002", "result": "\u5728UCI\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8eChoquetRank\u7b49\u65b9\u6cd5\uff0c\u4ee5\u66f4\u5c11\u7684\u7528\u6237\u4ea4\u4e92\u5b9e\u73b0\u66f4\u9ad8\u7684\u6392\u540d\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u5f0f\u7206\u70b8\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4ea4\u4e92\u6548\u7387\u548c\u6392\u540d\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14820", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14820", "abs": "https://arxiv.org/abs/2507.14820", "authors": ["Bingran Chen", "Baorun Li", "Jian Yang", "Yong Liu", "Guangyao Zhai"], "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning", "comment": null, "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.", "AI": {"tldr": "KGN-Pro\u662f\u4e00\u79cd\u65b0\u578b\u6293\u53d6\u7f51\u7edc\uff0c\u901a\u8fc7\u6982\u7387PnP\u5c42\u6574\u54083D\u4f18\u5316\uff0c\u63d0\u53476-DoF\u6293\u53d6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57282D\u8868\u793a\u548c3D\u4fe1\u606f\u5229\u7528\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0cKGN-Pro\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528RGB-D\u56fe\u50cf\u751f\u6210\u5173\u952e\u70b9\u56fe\u548c\u7f6e\u4fe1\u56fe\uff0c\u901a\u8fc7\u6982\u7387PnP\u5c42\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5e73\u53f0\u4e0a\uff0cKGN-Pro\u5728\u6293\u53d6\u8986\u76d6\u7387\u548c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "KGN-Pro\u901a\u8fc73D\u76d1\u7763\u548c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6027\u80fd\u3002"}}
{"id": "2507.14454", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\u3001\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u548c\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6bd4\u7279\u7387\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863DGS\u89c6\u9891\u6d41\u6027\u80fd\u3002", "motivation": "3DGS\u89c6\u9891\u6d41\u5728\u63d0\u4f9b\u6c89\u6d78\u5f0f\u4f53\u9a8c\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5206\u5757\u3001\u8d28\u91cf\u8bc4\u4f30\u548c\u6bd4\u7279\u7387\u9002\u5e94\u7b49\u57fa\u7840\u95ee\u9898\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002", "method": "\u7ed3\u5408\u663e\u8457\u6027\u5206\u6790\u7684\u81ea\u9002\u5e943DGS\u5206\u5757\u6280\u672f\u3001\u591a\u8d28\u91cf\u7248\u672c\u7f16\u7801\u3001\u7a7a\u95f4\u57df\u548c\u6e32\u67d3\u56fe\u50cf\u8d28\u91cf\u8054\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6bd4\u7279\u7387\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u901a\u8fc7\u7efc\u5408\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e863DGS\u89c6\u9891\u6d41\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.14219", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14219", "abs": "https://arxiv.org/abs/2507.14219", "authors": ["Obumneme Zimuzor Nwafor", "Mohammed Abdul Majeed Al Hooti"], "title": "Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman", "comment": null, "summary": "As nations seek sustainable alternatives to fossil fuels, green hydrogen has\nemerged as a promising strategic pathway toward decarbonisation, particularly\nin solar-rich arid regions. However, identifying optimal locations for hydrogen\nproduction requires the integration of complex environmental, atmospheric, and\ninfrastructural factors, often compounded by limited availability of direct\nhydrogen yield data. This study presents a novel Artificial Intelligence (AI)\nframework for computing green hydrogen yield and site suitability index using\nmean absolute SHAP (SHapley Additive exPlanations) values. This framework\nconsists of a multi-stage pipeline of unsupervised multi-variable clustering,\nsupervised machine learning classifier and SHAP algorithm. The pipeline trains\non an integrated meteorological, topographic and temporal dataset and the\nresults revealed distinct spatial patterns of suitability and relative\ninfluence of the variables. With model predictive accuracy of 98%, the result\nalso showed that water proximity, elevation and seasonal variation are the most\ninfluential factors determining green hydrogen site suitability in Oman with\nmean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.\nGiven limited or absence of ground-truth yield data in many countries that have\ngreen hydrogen prospects and ambitions, this study offers an objective and\nreproducible alternative to subjective expert weightings, thus allowing the\ndata to speak for itself and potentially discover novel latent groupings\nwithout pre-imposed assumptions. This study offers industry stakeholders and\npolicymakers a replicable and scalable tool for green hydrogen infrastructure\nplanning and other decision making in data-scarce regions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u7eff\u8272\u6c22\u4ea7\u91cf\u548c\u9009\u5740\u9002\u5b9c\u6027\u6307\u6570\uff0c\u7ed3\u5408\u4e86\u65e0\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7SHAP\u503c\u5206\u6790\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002", "motivation": "\u4e3a\u7f3a\u4e4f\u76f4\u63a5\u6c22\u4ea7\u91cf\u6570\u636e\u7684\u5730\u533a\u63d0\u4f9b\u5ba2\u89c2\u3001\u53ef\u590d\u5236\u7684\u9009\u5740\u5de5\u5177\uff0c\u66ff\u4ee3\u4e3b\u89c2\u4e13\u5bb6\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5AI\u6846\u67b6\uff0c\u5305\u62ec\u65e0\u76d1\u7763\u591a\u53d8\u91cf\u805a\u7c7b\u3001\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u548cSHAP\u7b97\u6cd5\uff0c\u5206\u6790\u6c14\u8c61\u3001\u5730\u5f62\u548c\u65f6\u95f4\u6570\u636e\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe98%\uff0c\u53d1\u73b0\u6c34\u8d44\u6e90\u63a5\u8fd1\u5ea6\u3001\u6d77\u62d4\u548c\u5b63\u8282\u53d8\u5316\u662f\u963f\u66fc\u7eff\u8272\u6c22\u9009\u5740\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u7a00\u7f3a\u5730\u533a\u7684\u7eff\u8272\u6c22\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2507.14903", "categories": ["cs.RO", "I.2.9; I.2.10; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.14903", "abs": "https://arxiv.org/abs/2507.14903", "authors": ["Pan Hu"], "title": "CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "Autonomous driving demands reliable and efficient solutions to closely\nrelated problems such as decision-making and motion planning. In this work,\ndecision-making refers specifically to highway lane selection, while motion\nplanning involves generating control commands (such as speed and steering) to\nreach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),\nachieving both flexible and safe lane selection alongside precise trajectory\nexecution remains a significant challenge. This paper proposes a framework\ncalled Cohesive Decision-Guided Motion Planning (CDGMP), which tightly\nintegrates decision-making and motion planning using a Mixture of Experts (MoE)\ninspired architecture combined with multi-policy reinforcement learning. By\ncoordinating multiple specialized sub-networks through a gating mechanism, the\nmethod decomposes the complex driving task into modular components. Each\nsub-network focuses on a specific aspect of driving, improving efficiency by\nactivating only the most relevant modules during inference. This design also\nenhances safety through modular specialization. CDGMP improves the adaptability\nand robustness of CAVs across diverse traffic scenarios, offering a scalable\nsolution to real-world autonomy challenges. The architectural principles behind\nCDGMP, especially the use of MoE, also provide a strong foundation for other\nhigh-dimensional decision and control tasks. Simulation results (available at\nhttps://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane\nselection and motion planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDGMP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u7d27\u5bc6\u96c6\u6210\u51b3\u7b56\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u51b3\u7b56\uff08\u5982\u8f66\u9053\u9009\u62e9\uff09\u548c\u8fd0\u52a8\u89c4\u5212\uff08\u5982\u901f\u5ea6\u548c\u8f6c\u5411\u63a7\u5236\uff09\u7684\u7d27\u5bc6\u96c6\u6210\u662f\u91cd\u8981\u6311\u6218\uff0c\u9700\u8981\u7075\u6d3b\u4e14\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u534f\u8c03\u591a\u4e2a\u5b50\u7f51\u7edc\uff0c\u5c06\u590d\u6742\u9a7e\u9a76\u4efb\u52a1\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7ec4\u4ef6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCDGMP\u5728\u8f66\u9053\u9009\u62e9\u548c\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "CDGMP\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u67b6\u6784\u539f\u5219\uff08\u5982MoE\uff09\u4e5f\u4e3a\u5176\u4ed6\u9ad8\u7ef4\u51b3\u7b56\u548c\u63a7\u5236\u4efb\u52a1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14456", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "GEMINUS\u662f\u4e00\u4e2a\u6df7\u5408\u4e13\u5bb6\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u4e13\u5bb6\u548c\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u7684\u8026\u5408\uff0c\u7ed3\u5408\u53cc\u611f\u77e5\u8def\u7531\u5668\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u6a21\u5f0f\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6837\u5316\u573a\u666f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5b66\u4e60\u591a\u6837\u5316\u9a7e\u9a76\u6280\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGEMINUS\u6846\u67b6\uff0c\u5305\u542b\u5168\u5c40\u4e13\u5bb6\u3001\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u7ec4\u548c\u53cc\u611f\u77e5\u8def\u7531\u5668\uff0c\u52a8\u6001\u6fc0\u6d3b\u4e13\u5bb6\u6a21\u5757\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a7e\u9a76\u5206\u6570\u548c\u6210\u529f\u7387\u5747\u8fbe\u5230SOTA\uff0c\u5355\u76ee\u89c6\u89c9\u8f93\u5165\u4e0b\u4ecd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "GEMINUS\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14227", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14227", "abs": "https://arxiv.org/abs/2507.14227", "authors": ["Khoi Do", "Duong Nguyen", "Nam-Khanh Le", "Quoc-Viet Pham", "Binh-Son Hua", "Won-Joo Hwang"], "title": "Domain Generalization via Pareto Optimal Gradient Matching", "comment": null, "summary": "In this study, we address the gradient-based domain generalization problem,\nwhere predictors aim for consistent gradient directions across different\ndomains. Existing methods have two main challenges. First, minimization of\ngradient empirical distance or gradient inner products (GIP) leads to gradient\nfluctuations among domains, thereby hindering straightforward learning. Second,\nthe direct application of gradient learning to the joint loss function can\nincur high computation overheads due to second-order derivative approximation.\nTo tackle these challenges, we propose a new Pareto Optimality Gradient\nMatching (POGM) method. In contrast to existing methods that add gradient\nmatching as regularization, we leverage gradient trajectories as collected data\nand apply independent training at the meta-learner. In the meta-update, we\nmaximize GIP while limiting the learned gradient from deviating too far from\nthe empirical risk minimization gradient trajectory. By doing so, the aggregate\ngradient can incorporate knowledge from all domains without suffering gradient\nfluctuation towards any particular domain. Experimental evaluations on datasets\nfrom DomainBed demonstrate competitive results yielded by POGM against other\nbaselines while achieving computational efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684POGM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u5316\u68af\u5ea6\u5185\u79ef\u5e76\u9650\u5236\u68af\u5ea6\u504f\u79bb\uff0c\u89e3\u51b3\u4e86\u68af\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u68af\u5ea6\u5339\u914d\u4e2d\u5b58\u5728\u68af\u5ea6\u6ce2\u52a8\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u68af\u5ea6\u8f68\u8ff9\u4f5c\u4e3a\u6570\u636e\uff0c\u5728\u5143\u5b66\u4e60\u5668\u4e2d\u8fdb\u884c\u72ec\u7acb\u8bad\u7ec3\uff0c\u6700\u5927\u5316\u68af\u5ea6\u5185\u79ef\u5e76\u9650\u5236\u68af\u5ea6\u504f\u79bb\u3002", "result": "\u5728DomainBed\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "POGM\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u6ce2\u52a8\u95ee\u9898\uff0c\u5e76\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.14914", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14914", "abs": "https://arxiv.org/abs/2507.14914", "authors": ["Zhexuan Xu", "Jie Wang", "Siyuan Xu", "Zijie Geng", "Mingxuan Yuan", "Feng Wu"], "title": "One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner", "comment": null, "summary": "Floorplanning determines the shapes and locations of modules on a chip canvas\nand plays a critical role in optimizing the chip's Power, Performance, and Area\n(PPA) metrics. However, existing floorplanning approaches often fail to\nintegrate with subsequent physical design stages, leading to suboptimal\nin-module component placement and excessive inter-module feedthrough. To tackle\nthis challenge, we propose Flora, a three-stage feedthrough and placement aware\nrectilinear floorplanner. In the first stage, Flora employs wiremask and\nposition mask techniques to achieve coarse-grained optimization of HPWL and\nfeedthrough. In the second stage, under the constraint of a fixed outline,\nFlora achieves a zero-whitespace layout by locally resizing module shapes,\nthereby performing fine-grained optimization of feedthrough and improving\ncomponent placement. In the third stage, Flora utilizes a fast tree\nsearch-based method to efficiently place components-including macros and\nstandard cells-within each module, subsequently adjusting module boundaries\nbased on the placement results to enable cross-stage optimization. Experimental\nresults show that Flora outperforms recent state-of-the-art floorplanning\napproaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,\n29.15% in FTmod, and a 14% improvement in component placement performance.", "AI": {"tldr": "Flora\u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u9988\u901a\u548c\u5e03\u5c40\u611f\u77e5\u7684\u77e9\u5f62\u5e73\u9762\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316HPWL\u3001\u9988\u901a\u548c\u7ec4\u4ef6\u5e03\u5c40\uff0c\u663e\u8457\u63d0\u5347\u82af\u7247PPA\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u5e73\u9762\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u4e0e\u540e\u7eed\u7269\u7406\u8bbe\u8ba1\u9636\u6bb5\u96c6\u6210\uff0c\u5bfc\u81f4\u6a21\u5757\u5185\u7ec4\u4ef6\u5e03\u5c40\u4e0d\u4f18\u548c\u6a21\u5757\u95f4\u9988\u901a\u8fc7\u591a\u3002", "method": "Flora\u5206\u4e09\u9636\u6bb5\uff1a1) \u7c97\u7c92\u5ea6\u4f18\u5316HPWL\u548c\u9988\u901a\uff1b2) \u56fa\u5b9a\u8f6e\u5ed3\u4e0b\u96f6\u7a7a\u767d\u5e03\u5c40\uff1b3) \u5feb\u901f\u6811\u641c\u7d22\u7ec4\u4ef6\u5e03\u5c40\u5e76\u8c03\u6574\u6a21\u5757\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFlora\u5e73\u5747\u51cf\u5c116% HPWL\u30015.16% FTpin\u300129.15% FTmod\uff0c\u7ec4\u4ef6\u5e03\u5c40\u6027\u80fd\u63d0\u534714%\u3002", "conclusion": "Flora\u901a\u8fc7\u8de8\u9636\u6bb5\u4f18\u5316\u663e\u8457\u63d0\u5347\u5e73\u9762\u89c4\u5212\u6548\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "VisGuard\u662f\u4e00\u4e2a\u6297\u7be1\u6539\u7684\u53ef\u89c6\u5316\u56fe\u50cf\u6570\u636e\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u5143\u6570\u636e\u94fe\u63a5\u6765\u4fdd\u62a4\u53ef\u89c6\u5316\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u4fe1\u606f\uff0c\u5373\u4f7f\u56fe\u50cf\u88ab\u7be1\u6539\u4e5f\u80fd\u6062\u590d\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u56fe\u50cf\u5728\u7ebf\u5206\u53d1\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u56e0\u88c1\u526a\u548c\u7f16\u8f91\u7b49\u7be1\u6539\u800c\u5931\u6548\uff0c\u5bfc\u81f4\u5173\u952e\u4fe1\u606f\u4e22\u5931\u3002", "method": "VisGuard\u91c7\u7528\u91cd\u590d\u6570\u636e\u5e73\u94fa\u3001\u53ef\u9006\u4fe1\u606f\u5e7f\u64ad\u548c\u57fa\u4e8e\u951a\u70b9\u7684\u88c1\u526a\u5b9a\u4f4d\u6280\u672f\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVisGuard\u5728\u6570\u636e\u68c0\u7d22\u51c6\u786e\u6027\u3001\u5d4c\u5165\u5bb9\u91cf\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VisGuard\u80fd\u6709\u6548\u4fdd\u62a4\u548c\u4fc3\u8fdb\u53ef\u89c6\u5316\u4f20\u64ad\u53ca\u4fe1\u606f\u4f20\u9012\u3002"}}
{"id": "2507.14245", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "cs.CE", "q-bio.BM", "I.6.5; J.3; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.14245", "abs": "https://arxiv.org/abs/2507.14245", "authors": ["Hengjie Yu", "Kenneth A. Dawson", "Haiyun Yang", "Shuya Liu", "Yan Yan", "Yaochu Jin"], "title": "A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions", "comment": "31 pages, 6 figures", "summary": "Unlocking the potential of nanomaterials in medicine and environmental\nscience hinges on understanding their interactions with proteins, a complex\ndecision space where AI is poised to make a transformative impact. However,\nprogress has been hindered by limited datasets and the restricted\ngeneralizability of existing models. Here, we propose NanoPro-3M, the largest\nnanomaterial-protein interaction dataset to date, comprising over 3.2 million\nsamples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,\na foundational model that predicts nanomaterial-protein affinities through\nmultimodal representation learning, demonstrating strong generalization,\nhandling missing features, and unseen nanomaterials or proteins. We show that\nmultimodal modeling significantly outperforms single-modality approaches and\nidentifies key determinants of corona formation. Furthermore, we demonstrate\nits applicability to a range of downstream tasks through zero-shot inference\nand fine-tuning. Together, this work establishes a solid foundation for\nhigh-performance and generalized prediction of nanomaterial-protein interaction\nendpoints, reducing experimental reliance and accelerating various in vitro\napplications.", "AI": {"tldr": "NanoPro-3M\u662f\u6700\u5927\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u6570\u636e\u96c6\uff0c\u7ed3\u5408NanoProFormer\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u51cf\u5c11\u5b9e\u9a8c\u4f9d\u8d56\u3002", "motivation": "\u7eb3\u7c73\u6750\u6599\u5728\u533b\u5b66\u548c\u73af\u5883\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faNanoPro-3M\u6570\u636e\u96c6\u548cNanoProFormer\u6a21\u578b\uff0c\u5229\u7528\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u9884\u6d4b\u4eb2\u548c\u529b\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u80fd\u5904\u7406\u7f3a\u5931\u7279\u5f81\u548c\u672a\u77e5\u6837\u672c\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u4e3a\u9ad8\u6027\u80fd\u548c\u6cdb\u5316\u7684\u7eb3\u7c73\u6750\u6599-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u5960\u5b9a\u57fa\u7840\uff0c\u52a0\u901f\u5e94\u7528\u7814\u7a76\u3002"}}
{"id": "2507.14929", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14929", "abs": "https://arxiv.org/abs/2507.14929", "authors": ["Tero Kaarlela", "Sami Salo", "Jose Outeiro"], "title": "Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly", "comment": null, "summary": "Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a\nsustainable transition to electric vehicles by enabling a closed-loop supply\nchain. Currently, the manual disassembly process exposes workers to hazards,\nincluding electrocution and toxic chemicals. We propose a teleoperated system\nfor the safe disassembly and sorting of EVBs. A human-in-the-loop can create\nand save disassembly sequences for unknown EVB types, enabling future\nautomation. An RGB camera aligns the physical and digital twins of the EVB, and\nthe digital twin of the robot is based on the Robot Operating System (ROS)\nmiddleware. This hybrid approach combines teleoperation and automation to\nimprove safety, adaptability, and efficiency in EVB disassembly and sorting.\nThe economic contribution is realized by reducing labor dependency and\nincreasing throughput in battery recycling. An online pilot study was set up to\nevaluate the usability of the presented approach, and the results demonstrate\nthe potential as a user-friendly solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\uff08EVB\uff09\u5b89\u5168\u62c6\u89e3\u548c\u5206\u7c7b\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u4eba\u673a\u534f\u4f5c\u548c\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u3001\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "motivation": "\u624b\u52a8\u62c6\u89e3EVB\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff08\u5982\u89e6\u7535\u548c\u6709\u6bd2\u5316\u5b66\u54c1\uff09\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u652f\u6301\u53ef\u6301\u7eed\u7684\u7535\u52a8\u6c7d\u8f66\u8f6c\u578b\u3002", "method": "\u91c7\u7528\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u4eba\u673a\u534f\u4f5c\u548cROS\u4e2d\u95f4\u4ef6\uff0c\u901a\u8fc7RGB\u6444\u50cf\u5934\u5bf9\u9f50\u7269\u7406\u548c\u6570\u5b57\u5b6a\u751f\uff0c\u5b9e\u73b0\u62c6\u89e3\u5e8f\u5217\u7684\u521b\u5efa\u4e0e\u4fdd\u5b58\u3002", "result": "\u5728\u7ebf\u8bd5\u70b9\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u7528\u6237\u53cb\u597d\u6027\u548c\u6f5c\u529b\uff0c\u53ef\u51cf\u5c11\u52b3\u52a8\u529b\u4f9d\u8d56\u5e76\u63d0\u9ad8\u7535\u6c60\u56de\u6536\u6548\u7387\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u5728EVB\u62c6\u89e3\u4e2d\u5c55\u73b0\u51fa\u5b89\u5168\u3001\u9002\u5e94\u6027\u5f3a\u548c\u9ad8\u6548\u7684\u7279\u70b9\uff0c\u4e3a\u95ed\u73af\u4f9b\u5e94\u94fe\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u8d21\u732e\u3002"}}
{"id": "2507.14477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "OptiCorNet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e8f\u5217\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u548c\u65f6\u95f4\u5dee\u5206\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u63d0\u5347\u52a8\u6001\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u52a8\u6001\u548c\u611f\u77e5\u6df7\u6dc6\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08VPR\uff09\u662f\u957f\u671f\u5b9a\u4f4d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5ffd\u7565\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea71D\u5377\u79ef\u7f16\u7801\u5668\u548c\u53ef\u5b66\u4e60\u5dee\u5206\u65f6\u95f4\u7b97\u5b50\uff08DSD\uff09\uff0c\u7ed3\u5408LSTM\u4f18\u5316\u548c\u56db\u5143\u7ec4\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOptiCorNet\u5728\u5b63\u8282\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OptiCorNet\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u5e8f\u5217\u7ea7\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5730\u70b9\u8bc6\u522b\u6548\u679c\u3002"}}
{"id": "2507.14257", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14257", "abs": "https://arxiv.org/abs/2507.14257", "authors": ["Julio Candanedo"], "title": "Linearized Diffusion Map", "comment": null, "summary": "We introduce the Linearized Diffusion Map (LDM), a novel linear\ndimensionality reduction method constructed via a linear approximation of the\ndiffusion-map kernel. LDM integrates the geometric intuition of diffusion-based\nnonlinear methods with the computational simplicity, efficiency, and\ninterpretability inherent in linear embeddings such as PCA and classical MDS.\nThrough comprehensive experiments on synthetic datasets (Swiss roll and\nhyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that\nLDM captures distinct geometric features of datasets compared to PCA, offering\ncomplementary advantages. Specifically, LDM embeddings outperform PCA in\ndatasets exhibiting explicit manifold structures, particularly in\nhigh-dimensional regimes, whereas PCA remains preferable in scenarios dominated\nby variance or noise. Furthermore, the complete positivity of LDM's kernel\nmatrix allows direct applicability of Non-negative Matrix Factorization (NMF),\nsuggesting opportunities for interpretable latent-structure discovery. Our\nanalysis positions LDM as a valuable new linear dimensionality reduction\ntechnique with promising theoretical and practical extensions.", "AI": {"tldr": "LDM\u662f\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u964d\u7ef4\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u8fd1\u4f3c\u6269\u6563\u6620\u5c04\u6838\uff0c\u7ed3\u5408\u4e86\u51e0\u4f55\u76f4\u89c9\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7ed3\u5408\u975e\u7ebf\u6027\u65b9\u6cd5\u7684\u51e0\u4f55\u76f4\u89c9\u4e0e\u7ebf\u6027\u65b9\u6cd5\uff08\u5982PCA\uff09\u7684\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u8fd1\u4f3c\u6269\u6563\u6620\u5c04\u6838\u6784\u5efaLDM\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "LDM\u5728\u5177\u6709\u660e\u786e\u6d41\u5f62\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u4f18\u4e8ePCA\uff0c\u800cPCA\u5728\u65b9\u5dee\u6216\u566a\u58f0\u4e3b\u5bfc\u7684\u573a\u666f\u4e2d\u66f4\u4f18\u3002", "conclusion": "LDM\u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u7ebf\u6027\u964d\u7ef4\u6280\u672f\uff0c\u9002\u7528\u4e8e\u7406\u8bba\u548c\u5b9e\u9645\u6269\u5c55\u3002"}}
{"id": "2507.14931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14931", "abs": "https://arxiv.org/abs/2507.14931", "authors": ["Qiaoqiao Ren", "Remko Proesmans", "Arend Pissens", "Lara Dehandschutter", "William Denecker", "Lotte Rouckhout", "Joke Carrette", "Peter Vanhopplinus", "Tony Belpaeme", "Francis wyffels"], "title": "Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry", "comment": null, "summary": "Forensic mental health care involves the treatment of individuals with severe\nmental disorders who have committed violent offences. These settings are often\ncharacterized by high levels of bureaucracy, risk avoidance, and restricted\nautonomy. Patients frequently experience a profound loss of control over their\nlives, leading to heightened psychological stress-sometimes resulting in\nisolation as a safety measure. In this study, we explore how co-design can be\nused to collaboratively develop a companion robot that helps monitor and\nregulate stress while maintaining tracking of the patients' interaction\nbehaviours for long-term intervention. We conducted four co-design workshops in\na forensic psychiatric clinic with patients, caregivers, and therapists. Our\nprocess began with the presentation of an initial speculative prototype to\ntherapists, enabling reflection on shared concerns, ethical risks, and\ndesirable features. This was followed by a creative ideation session with\npatients, a third workshop focused on defining desired functions and emotional\nresponses, and we are planning a final prototype demo to gather direct patient\nfeedback. Our findings emphasize the importance of empowering patients in the\ndesign process and adapting proposals based on their current emotional state.\nThe goal was to empower the patient in the design process and ensure each\npatient's voice was heard.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u5f00\u53d1\u4e00\u6b3e\u966a\u4f34\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u76d1\u63a7\u548c\u8c03\u8282\u6cd5\u533b\u7cbe\u795e\u75c5\u60a3\u8005\u7684\u538b\u529b\uff0c\u540c\u65f6\u8bb0\u5f55\u5176\u4e92\u52a8\u884c\u4e3a\u4ee5\u8fdb\u884c\u957f\u671f\u5e72\u9884\u3002", "motivation": "\u6cd5\u533b\u5fc3\u7406\u5065\u5eb7\u62a4\u7406\u73af\u5883\u4e2d\uff0c\u60a3\u8005\u5e38\u56e0\u9ad8\u5ea6\u5b98\u50da\u5316\u3001\u98ce\u9669\u89c4\u907f\u548c\u81ea\u4e3b\u6743\u53d7\u9650\u800c\u7ecf\u5386\u5fc3\u7406\u538b\u529b\uff0c\u751a\u81f3\u88ab\u9694\u79bb\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u6539\u5584\u8fd9\u4e00\u72b6\u51b5\u3002", "method": "\u5728\u6cd5\u533b\u7cbe\u795e\u75c5\u8bca\u6240\u8fdb\u884c\u4e86\u56db\u6b21\u5171\u540c\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u53c2\u4e0e\u8005\u5305\u62ec\u60a3\u8005\u3001\u62a4\u7406\u4eba\u5458\u548c\u6cbb\u7597\u5e08\uff0c\u4ece\u539f\u578b\u5c55\u793a\u5230\u521b\u610f\u6784\u601d\u518d\u5230\u529f\u80fd\u5b9a\u4e49\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8bbe\u8ba1\u4e2d\u8d4b\u4e88\u60a3\u8005\u6743\u529b\u5e76\u6839\u636e\u5176\u60c5\u7eea\u72b6\u6001\u8c03\u6574\u65b9\u6848\u81f3\u5173\u91cd\u8981\uff0c\u786e\u4fdd\u6bcf\u4f4d\u60a3\u8005\u7684\u58f0\u97f3\u88ab\u542c\u5230\u3002", "conclusion": "\u5171\u540c\u8bbe\u8ba1\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u60a3\u8005\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u53c2\u4e0e\u611f\uff0c\u4e3a\u5f00\u53d1\u9002\u5408\u7684\u966a\u4f34\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.14481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "DFQ-ViT\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u7684ViT\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u8fdb\u5408\u6210\u6837\u672c\u8d28\u91cf\u548c\u5f15\u5165\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5408\u6210\u6837\u672c\u65f6\u672a\u80fd\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u7279\u5f81\uff0c\u4e14\u91cf\u5316\u6a21\u578b\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5206\u5e03\u5dee\u5f02\u5927\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u6309\u96be\u5ea6\u9012\u589e\u987a\u5e8f\u5408\u6210\u6837\u672c\uff0c\u5e76\u5f15\u5165\u6fc0\u6d3b\u6821\u6b63\u77e9\u9635\u5bf9\u9f50\u4e2d\u95f4\u5c42\u6fc0\u6d3b\u5206\u5e03\u3002", "result": "DFQ-ViT\u6027\u80fd\u4f18\u4e8e\u73b0\u6709DFQ\u65b9\u6cd5\uff0c\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u91cf\u5316\u6a21\u578b\uff0c\u5982DeiT-T 3-bit\u91cf\u5316\u6027\u80fd\u63d0\u53474.29%\u3002", "conclusion": "DFQ-ViT\u65e0\u9700\u5fae\u8c03\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u548c\u90e8\u7f72\u95e8\u69db\uff0c\u7b26\u5408\u7eff\u8272\u5b66\u4e60\u539f\u5219\u3002"}}
{"id": "2507.14295", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14295", "abs": "https://arxiv.org/abs/2507.14295", "authors": ["Licheng Liu", "Zihan Wang", "Linjie Li", "Chenwei Xu", "Yiping Lu", "Han Liu", "Avirup Sil", "Manling Li"], "title": "A Simple \"Try Again\" Can Elicit Multi-Turn LLM Reasoning", "comment": null, "summary": "Multi-turn problem solving is critical yet challenging for Large Reasoning\nModels (LRMs) to reflect on their reasoning and revise from feedback. Existing\nReinforcement Learning (RL) methods train large reasoning models on a\nsingle-turn paradigm with verifiable rewards. However, we observe that models\ntrained with existing RL paradigms often lose their ability to solve problems\nacross multiple turns and struggle to revise answers based on contextual\nfeedback, leading to repetitive responses. We ask: can LRMs learn to reflect\ntheir answers in a multi-turn context? In this work, we find that training\nmodels with multi-turn RL using only unary feedback (e.g., \"Let's try again\")\nafter wrong answers can improve both single-turn performance and multi-turn\nreasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement\nlearning, which uses minimal yet common unary user feedback during iterative\nproblem solving. It can be easily applied to existing single-turn RL training\nsetups. Experimental results show that RL training with UFO keeps single-turn\nperformance and improves multi-turn reasoning accuracy by up to 14%, enabling\nlanguage models to better react to feedback in multi-turn problem solving. To\nfurther minimize the number of turns needed for a correct answer while\nencouraging diverse reasoning when mistakes occur, we design reward structures\nthat guide models to produce careful and deliberate answers in each turn. Code:\nhttps://github.com/lichengliu03/unary-feedback", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aUFO\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u8f6e\u53cd\u9988\u63d0\u5347\u591a\u8f6e\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5355\u8f6e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u591a\u8f6e\u63a8\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u6a21\u578b\u65e0\u6cd5\u6839\u636e\u53cd\u9988\u4fee\u6b63\u7b54\u6848\u3002", "method": "\u5f15\u5165UFO\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u8f6e\u53cd\u9988\uff08\u5982\u201c\u518d\u8bd5\u4e00\u6b21\u201d\uff09\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u5956\u52b1\u7ed3\u6784\u4ee5\u4f18\u5316\u591a\u8f6e\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cUFO\u65b9\u6cd5\u5728\u4fdd\u6301\u5355\u8f6e\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u591a\u8f6e\u63a8\u7406\u51c6\u786e\u7387\u63d0\u534714%\u3002", "conclusion": "UFO\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u591a\u8f6e\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u53cd\u9988\u54cd\u5e94\u80fd\u529b\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6240\u9700\u8f6e\u6b21\u3002"}}
{"id": "2507.14967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14967", "abs": "https://arxiv.org/abs/2507.14967", "authors": ["Pratik Ingle", "Kasper St\u00f8y", "Andres Fai\u00f1a"], "title": "Heterogeneous object manipulation on nonlinear soft surface through linear controller", "comment": "8 pages, 3 figures", "summary": "Manipulation surfaces indirectly control and reposition objects by actively\nmodifying their shape or properties rather than directly gripping objects.\nThese surfaces, equipped with dense actuator arrays, generate dynamic\ndeformations. However, a high-density actuator array introduces considerable\ncomplexity due to increased degrees of freedom (DOF), complicating control\ntasks. High DOF restrict the implementation and utilization of manipulation\nsurfaces in real-world applications as the maintenance and control of such\nsystems exponentially increase with array/surface size. Learning-based control\napproaches may ease the control complexity, but they require extensive training\nsamples and struggle to generalize for heterogeneous objects. In this study, we\nintroduce a simple, precise and robust PID-based linear close-loop feedback\ncontrol strategy for heterogeneous object manipulation on MANTA-RAY\n(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation\ndensity). Our approach employs a geometric transformation-driven PID\ncontroller, directly mapping tilt angle control outputs(1D/2D) to actuator\ncommands to eliminate the need for extensive black-box training. We validate\nthe proposed method through simulations and experiments on a physical system,\nsuccessfully manipulating objects with diverse geometries, weights and\ntextures, including fragile objects like eggs and apples. The outcomes\ndemonstrate that our approach is highly generalized and offers a practical and\nreliable solution for object manipulation on soft robotic manipulation,\nfacilitating real-world implementation without prohibitive training demands.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePID\u95ed\u73af\u53cd\u9988\u63a7\u5236\u7684\u7b80\u5355\u3001\u7cbe\u786e\u4e14\u9c81\u68d2\u7684\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u4f4e\u5bc6\u5ea6\u9a71\u52a8\u9635\u5217\u7684\u8f6f\u673a\u5668\u4eba\u8868\u9762\u4e0a\u64cd\u7eb5\u5f02\u8d28\u7269\u4f53\u3002", "motivation": "\u9ad8\u5bc6\u5ea6\u9a71\u52a8\u9635\u5217\u7684\u590d\u6742\u6027\u548c\u9ad8\u81ea\u7531\u5ea6\u9650\u5236\u4e86\u64cd\u7eb5\u8868\u9762\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5b66\u4e60\u578b\u63a7\u5236\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6837\u672c\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u53d8\u6362\u9a71\u52a8\u7684PID\u63a7\u5236\u5668\uff0c\u76f4\u63a5\u5c06\u503e\u659c\u89d2\u5ea6\u63a7\u5236\u8f93\u51fa\u6620\u5c04\u5230\u6267\u884c\u5668\u547d\u4ee4\uff0c\u907f\u514d\u9ed1\u76d2\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u7269\u7406\u7cfb\u7edf\u5b9e\u9a8c\u4e2d\u6210\u529f\u64cd\u7eb5\u4e86\u591a\u79cd\u51e0\u4f55\u5f62\u72b6\u3001\u91cd\u91cf\u548c\u7eb9\u7406\u7684\u7269\u4f53\uff0c\u5305\u62ec\u6613\u788e\u7269\u54c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u5ea6\u6cdb\u5316\u6027\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u64cd\u7eb5\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.14485", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u7684\u70b9\u4e91\u8865\u5168\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u68c0\u7d22\u5b66\u4e60\u7ed3\u6784\u5148\u9a8c\u4fe1\u606f\uff0c\u751f\u6210\u7ec6\u7c92\u5ea6\u70b9\u4e91\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u8f93\u5165\u7c7b\u522b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u70b9\u4e91\u8865\u5168\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5171\u4eab\u7279\u5f81\u7f16\u7801\u5668\uff08SSFE\uff09\u548c\u6e10\u8fdb\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5668\uff08PRAG\uff09\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u63a7\u5236\u95e8\u548c\u5206\u5c42\u7279\u5f81\u878d\u5408\u673a\u5236\u589e\u5f3a\u76f8\u5173\u7ed3\u6784\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u7c7b\u522b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u7ec6\u7c92\u5ea6\u70b9\u4e91\uff0c\u5e76\u5177\u5907\u5904\u7406\u7a00\u758f\u6570\u636e\u548c\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14322", "categories": ["cs.LG", "cs.CR", "cs.DC", "I.2.11; C.2.4; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.14322", "abs": "https://arxiv.org/abs/2507.14322", "authors": ["Md Rafid Haque", "Abu Raihan Mostofa Kamal", "Md. Azam Hossain"], "title": "FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning", "comment": "24 pages, 8 figures. This work is intended for a journal submission", "summary": "Federated Learning (FL) offers a paradigm for privacy-preserving\ncollaborative AI, but its decentralized nature creates significant\nvulnerabilities to model poisoning attacks. While numerous static defenses\nexist, their effectiveness is highly context-dependent, often failing against\nadaptive adversaries or in heterogeneous data environments. This paper\nintroduces FedStrategist, a novel meta-learning framework that reframes robust\naggregation as a real-time, cost-aware control problem. We design a lightweight\ncontextual bandit agent that dynamically selects the optimal aggregation rule\nfrom an arsenal of defenses based on real-time diagnostic metrics. Through\ncomprehensive experiments, we demonstrate that no single static rule is\nuniversally optimal. We show that our adaptive agent successfully learns\nsuperior policies across diverse scenarios, including a ``Krum-favorable\"\nenvironment and against a sophisticated \"stealth\" adversary designed to\nneutralize specific diagnostic signals. Critically, we analyze the paradoxical\nscenario where a non-robust baseline achieves high but compromised accuracy,\nand demonstrate that our agent learns a conservative policy to prioritize model\nintegrity. Furthermore, we prove the agent's policy is controllable via a\nsingle \"risk tolerance\" parameter, allowing practitioners to explicitly manage\nthe trade-off between performance and security. Our work provides a new,\npractical, and analyzable approach to creating resilient and intelligent\ndecentralized AI systems.", "AI": {"tldr": "FedStrategist\u662f\u4e00\u4e2a\u57fa\u4e8e\u5143\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u805a\u5408\u89c4\u5219\u6765\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u53bb\u4e2d\u5fc3\u5316\u7279\u6027\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u9759\u6001\u9632\u5fa1\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e0a\u4e0b\u6587\u5f3a\u76d7\u4ee3\u7406\uff0c\u5b9e\u65f6\u9009\u62e9\u6700\u4f18\u805a\u5408\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u7b56\u7565\u4f18\u4e8e\u9759\u6001\u89c4\u5219\uff0c\u5e76\u80fd\u901a\u8fc7\u98ce\u9669\u5bb9\u5fcd\u53c2\u6570\u63a7\u5236\u6027\u80fd\u4e0e\u5b89\u5168\u7684\u6743\u8861\u3002", "conclusion": "FedStrategist\u4e3a\u6784\u5efa\u5f39\u6027\u4e14\u667a\u80fd\u7684\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u5206\u6790\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.14975", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14975", "abs": "https://arxiv.org/abs/2507.14975", "authors": ["Yufan Song", "Jiatao Zhang", "Zeng Gu", "Qingmiao Liang", "Tuocheng Hu", "Wei Song", "Shiqiang Zhu"], "title": "FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models", "comment": "8 pages, 6 figures, IROS 2025", "summary": "Autonomous error correction is critical for domestic robots to achieve\nreliable execution of complex long-horizon tasks. Prior work has explored\nself-reflection in Large Language Models (LLMs) for task planning error\ncorrection; however, existing methods are constrained by inflexible\nself-reflection mechanisms that limit their effectiveness. Motivated by these\nlimitations and inspired by human cognitive adaptation, we propose the Flexible\nConstructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture\nthat enables LLMs to perform flexible self-reflection based on task difficulty,\nwhile constructively integrating historical valuable experience with failure\nlessons. We evaluated FCRF on diverse domestic tasks through simulation in\nAlfWorld and physical deployment in the real-world environment. Experimental\nresults demonstrate that FCRF significantly improves overall performance and\nself-reflection flexibility in complex long-horizon robotic tasks.", "AI": {"tldr": "FCRF\u6846\u67b6\u901a\u8fc7\u7075\u6d3b\u7684\u81ea\u6211\u53cd\u601d\u673a\u5236\u63d0\u5347\u5bb6\u7528\u673a\u5668\u4eba\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u81ea\u6211\u53cd\u601d\u673a\u5236\u4e0d\u591f\u7075\u6d3b\uff0c\u9650\u5236\u4e86\u9519\u8bef\u7ea0\u6b63\u6548\u679c\u3002", "method": "\u63d0\u51faMentor-Actor\u67b6\u6784\u7684FCRF\u6846\u67b6\uff0c\u7ed3\u5408\u4efb\u52a1\u96be\u5ea6\u548c\u5386\u53f2\u7ecf\u9a8c\u8fdb\u884c\u7075\u6d3b\u53cd\u601d\u3002", "result": "\u5728AlfWorld\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0cFCRF\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u53cd\u601d\u7075\u6d3b\u6027\u3002", "conclusion": "FCRF\u4e3a\u590d\u6742\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u81ea\u4e3b\u9519\u8bef\u7ea0\u6b63\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14497", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "TCP-LLaVA\u662f\u4e00\u79cd\u65b0\u578b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u4ee4\u724c\u538b\u7f29\u6280\u672f\u89e3\u51b3\u75c5\u7406\u5b66\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u75c5\u7406\u5b66\u5168\u5207\u7247\u56fe\u50cf\u7684\u9ad8\u5206\u8fa8\u7387\u548c\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u51fa\u4e86\u5de8\u5927\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u80fd\u529b\u548c\u8d44\u6e90\u6d88\u8017\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTCP-LLaVA\uff0c\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u538b\u7f29\u4ee4\u724c\u805a\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\uff0c\u4ec5\u5c06\u538b\u7f29\u540e\u7684\u4ee4\u724c\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b54\u6848\u3002", "result": "\u5728\u5341\u79cdTCGA\u80bf\u7624\u4e9a\u578b\u7684\u5b9e\u9a8c\u4e2d\uff0cTCP-LLaVA\u5728VQA\u51c6\u786e\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u3002", "conclusion": "TCP-LLaVA\u4e3a\u75c5\u7406\u5b66WSI\u7684VQA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8d44\u6e90\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14326", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14326", "abs": "https://arxiv.org/abs/2507.14326", "authors": ["Aryana Hou", "Li Lin", "Justin Li", "Shu Hu"], "title": "Rethinking Individual Fairness in Deepfake Detection", "comment": "This paper has been accepted by ACM MM 2025", "summary": "Generative AI models have substantially improved the realism of synthetic\nmedia, yet their misuse through sophisticated DeepFakes poses significant\nrisks. Despite recent advances in deepfake detection, fairness remains\ninadequately addressed, enabling deepfake markers to exploit biases against\nspecific populations. While previous studies have emphasized group-level\nfairness, individual fairness (i.e., ensuring similar predictions for similar\nindividuals) remains largely unexplored. In this work, we identify for the\nfirst time that the original principle of individual fairness fundamentally\nfails in the context of deepfake detection, revealing a critical gap previously\nunexplored in the literature. To mitigate it, we propose the first\ngeneralizable framework that can be integrated into existing deepfake detectors\nto enhance individual fairness and generalization. Extensive experiments\nconducted on leading deepfake datasets demonstrate that our approach\nsignificantly improves individual fairness while maintaining robust detection\nperformance, outperforming state-of-the-art methods. The code is available at\nhttps://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63d0\u5347\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u4e2a\u4f53\u516c\u5e73\u6027\u7684\u901a\u7528\u6846\u67b6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u6ee5\u7528\u5bf9\u7279\u5b9a\u7fa4\u4f53\u5b58\u5728\u504f\u89c1\uff0c\u800c\u4e2a\u4f53\u516c\u5e73\u6027\u5728\u68c0\u6d4b\u4e2d\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u96c6\u6210\u5230\u73b0\u6709\u68c0\u6d4b\u5668\u4e2d\u7684\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u4e2a\u4f53\u516c\u5e73\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u4f53\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u4e2a\u4f53\u516c\u5e73\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15022", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15022", "abs": "https://arxiv.org/abs/2507.15022", "authors": ["Sumeadh MS", "Kevin Dsouza", "Ravi Prakash"], "title": "CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions", "comment": "6pages, 4figures, Submitted to the prestigious Indian Control\n  Conference (ICC), 2025", "summary": "Among the promising approaches to enforce safety in control systems, learning\nControl Barrier Functions (CBFs) from expert demonstrations has emerged as an\neffective strategy. However, a critical challenge remains: verifying that the\nlearned CBFs truly enforce safety across the entire state space. This is\nespecially difficult when CBF is represented using neural networks (NCBFs).\nSeveral existing verification techniques attempt to address this problem\nincluding SMT-based solvers, mixed-integer programming (MIP), and interval or\nbound-propagation methods but these approaches often introduce loose,\nconservative bounds. To overcome these limitations, in this work we use\nCPED-NCBFs a split-conformal prediction based verification strategy to verify\nthe learned NCBF from the expert demonstrations. We further validate our method\non point mass systems and unicycle models to demonstrate the effectiveness of\nthe proposed theory.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5f62\u5171\u5f62\u9884\u6d4b\u7684\u9a8c\u8bc1\u7b56\u7565\uff08CPED-NCBFs\uff09\uff0c\u7528\u4e8e\u9a8c\u8bc1\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08NCBFs\uff09\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\uff08\u5982SMT\u6c42\u89e3\u5668\u3001\u6df7\u5408\u6574\u6570\u89c4\u5212\u7b49\uff09\u5728\u5904\u7406NCBFs\u65f6\u5b58\u5728\u8fb9\u754c\u677e\u6563\u6216\u4fdd\u5b88\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u9a8c\u8bc1\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5206\u5f62\u5171\u5f62\u9884\u6d4b\uff08split-conformal prediction\uff09\u65b9\u6cd5\u9a8c\u8bc1NCBFs\u7684\u5b89\u5168\u6027\uff0c\u5e76\u5728\u70b9\u8d28\u91cf\u7cfb\u7edf\u548c\u975e\u5b8c\u6574\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCPED-NCBFs\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9a8c\u8bc1NCBFs\u7684\u5b89\u5168\u6027\u3002", "conclusion": "CPED-NCBFs\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u9a8c\u8bc1\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.14500", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Ferm\u00fcller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u6cd5\u5411\u6d41\u7684\u8fd0\u52a8\u5206\u5272\u548c\u81ea\u8fd0\u52a8\u4f30\u8ba1\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u4f20\u611f\u5668\uff0c\u65e0\u9700\u5b8c\u6574\u5149\u6d41\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5149\u6d41\u6216\u6df1\u5ea6\u4f30\u8ba1\uff0c\u800c\u65b0\u65b9\u6cd5\u5229\u7528\u7a00\u758f\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u4e8b\u4ef6\u6570\u636e\uff0c\u7ed3\u5408\u51e0\u4f55\u7ea6\u675f\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u6d41\u7a0b\u8fdb\u884c\u4e8b\u4ef6\u8fc7\u5206\u5272\uff0c\u901a\u8fc7\u6b8b\u5dee\u5206\u6790\u9694\u79bb\u72ec\u7acb\u8fd0\u52a8\u7269\u4f53\uff0c\u5e76\u5229\u7528\u8fd0\u52a8\u76f8\u4f3c\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5c42\u6b21\u805a\u7c7b\u7ec6\u5316\u5206\u5272\u3002", "result": "\u5728EVIMO2v2\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u51c6\u786e\u7684\u5206\u5272\u548c\u5e73\u79fb\u8fd0\u52a8\u4f30\u8ba1\uff0c\u5c24\u5176\u5728\u7269\u4f53\u8fb9\u754c\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u65f6\u673a\u5668\u4eba\u548c\u5bfc\u822a\u5e94\u7528\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u548c\u6f5c\u529b\u3002"}}
{"id": "2507.14332", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14332", "abs": "https://arxiv.org/abs/2507.14332", "authors": ["Aidan Furlong", "Xingang Zhao", "Robert Salko", "Xu Wu"], "title": "Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries", "comment": "Accepted for inclusion in Transactions of the American Nuclear\n  Society for the 2025 ANS Winter Conference", "summary": "Accurate prediction of critical heat flux (CHF) is an essential component of\nsafety analysis in pressurized and boiling water reactors. To support reliable\nprediction of this quantity, several empirical correlations and lookup tables\nhave been constructed from physical experiments over the past several decades.\nWith the onset of accessible machine learning (ML) frameworks, multiple\ninitiatives have been established with the goal of predicting CHF more\naccurately than these traditional methods. While purely data-driven surrogate\nmodeling has been extensively investigated, these approaches lack\ninterpretability, lack resilience to data scarcity, and have been developed\nmostly using data from tube experiments. As a result, bias-correction hybrid\napproaches have become increasingly popular, which correct initial\n\"low-fidelity\" estimates provided by deterministic base models by using\nML-predicted residuals. This body of work has mostly considered round tube\ngeometries; annular geometry-specific ML models have not yet been deployed in\nthermal hydraulic codes. This study developed, deployed, and validated four ML\nmodels to predict CHF in annular geometries using the CTF subchannel code.\nThree empirical correlation models, Biasi, Bowring, and Katto, were used as\nbase models for comparison. The ML models were trained and tested using 577\nexperimental annulus data points from four datasets: Becker, Beus, Janssen, and\nMortimore. Baseline CHF predictions were obtained from the empirical\ncorrelations, with mean relative errors above 26%. The ML-driven models\nachieved mean relative errors below 3.5%, with no more than one point exceeding\nthe 10% error envelope. In all cases, the hybrid ML models significantly\noutperformed their empirical counterparts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u73af\u5f62\u51e0\u4f55\u4e2d\u7684\u4e34\u754c\u70ed\u901a\u91cf\uff08CHF\uff09\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7ecf\u9a8c\u6a21\u578b\u3002", "motivation": "\u51c6\u786e\u9884\u6d4bCHF\u5bf9\u53cd\u5e94\u5806\u5b89\u5168\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u7ecf\u9a8c\u76f8\u5173\u6027\u548c\u67e5\u627e\u8868\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u867d\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u5bf9\u6570\u636e\u7a00\u7f3a\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u56db\u79cdML\u6a21\u578b\uff0c\u7ed3\u5408\u4e09\u79cd\u7ecf\u9a8c\u76f8\u5173\u6027\u6a21\u578b\uff08Biasi\u3001Bowring\u3001Katto\uff09\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u7528577\u4e2a\u73af\u5f62\u51e0\u4f55\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "ML\u6a21\u578b\u7684\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4f4e\u4e8e3.5%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u768426%\u4ee5\u4e0a\u8bef\u5dee\u3002", "conclusion": "\u6df7\u5408ML\u6a21\u578b\u5728\u73af\u5f62\u51e0\u4f55\u4e2d\u9884\u6d4bCHF\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u70ed\u5de5\u6c34\u529b\u4ee3\u7801\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.15062", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15062", "abs": "https://arxiv.org/abs/2507.15062", "authors": ["Xinyue Zhu", "Binghao Huang", "Yunzhu Li"], "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper", "comment": "More videos can be found on our\n  website:https://binghao-huang.github.io/touch_in_the_wild/", "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .", "AI": {"tldr": "\u4fbf\u643a\u5f0f\u5939\u6301\u5668\u96c6\u6210\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u63d0\u51fa\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7cbe\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5939\u6301\u5668\u7f3a\u4e4f\u89e6\u89c9\u53cd\u9988\uff0c\u800c\u89e6\u89c9\u5728\u7cbe\u786e\u64cd\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4fbf\u643a\u5f0f\u89e6\u89c9\u5939\u6301\u5668\uff0c\u63d0\u51fa\u89c6\u89c9\u4e0e\u89e6\u89c9\u4fe1\u53f7\u878d\u5408\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u5728\u7cbe\u7ec6\u4efb\u52a1\uff08\u5982\u8bd5\u7ba1\u63d2\u5165\u548c\u79fb\u6db2\u64cd\u4f5c\uff09\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u96c6\u6210\u89e6\u89c9\u53cd\u9988\u7684\u8de8\u6a21\u6001\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "comment": "A project page associated with this survey is available at\n  https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer\nvision, graphics, and immersive technologies such as augmented reality (AR),\nvirtual reality (VR), and digital twins. Traditional methods rely on\ncomputationally intensive iterative optimization in a complex chain, limiting\ntheir applicability in real-world scenarios. Recent advances in feed-forward\napproaches, driven by deep learning, have revolutionized this field by enabling\nfast and generalizable 3D reconstruction and view synthesis. This survey offers\na comprehensive review of feed-forward techniques for 3D reconstruction and\nview synthesis, with a taxonomy according to the underlying representation\narchitectures including point cloud, 3D Gaussian Splatting (3DGS), Neural\nRadiance Fields (NeRF), etc. We examine key tasks such as pose-free\nreconstruction, dynamic 3D reconstruction, and 3D-aware image and video\nsynthesis, highlighting their applications in digital humans, SLAM, robotics,\nand beyond. In addition, we review commonly used datasets with detailed\nstatistics, along with evaluation protocols for various downstream tasks. We\nconclude by discussing open research challenges and promising directions for\nfuture work, emphasizing the potential of feed-forward approaches to advance\nthe state of the art in 3D vision.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u57fa\u4e8e\u524d\u9988\u65b9\u6cd5\u76843D\u91cd\u5efa\u548c\u89c6\u56fe\u5408\u6210\u6280\u672f\uff0c\u5206\u7c7b\u8ba8\u8bba\u4e86\u4e0d\u540c\u8868\u793a\u67b6\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5e94\u7528\u3001\u6570\u636e\u96c6\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u96be\u4ee5\u5b9e\u65f6\u5e94\u7528\uff0c\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u524d\u9988\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u8ba8\u8bba\u70b9\u4e91\u30013D\u9ad8\u65af\u6cfc\u6e85\u3001NeRF\u7b49\u8868\u793a\u67b6\u6784\uff0c\u5206\u6790\u5173\u952e\u4efb\u52a1\u5982\u65e0\u59ff\u6001\u91cd\u5efa\u548c\u52a8\u60013D\u91cd\u5efa\u3002", "result": "\u603b\u7ed3\u4e86\u524d\u9988\u65b9\u6cd5\u5728\u6570\u5b57\u4eba\u3001SLAM\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\u7684\u8be6\u7ec6\u7edf\u8ba1\u3002", "conclusion": "\u524d\u9988\u65b9\u6cd5\u57283D\u89c6\u89c9\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5f00\u653e\u6311\u6218\u4ee5\u63a8\u52a8\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2507.14344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14344", "abs": "https://arxiv.org/abs/2507.14344", "authors": ["Daniel Fein", "Gabriela Aranguiz-Dias"], "title": "Influence Functions for Preference Dataset Pruning", "comment": null, "summary": "Language models are commonly fine-tuned via reinforcement learning to alter\ntheir behavior or elicit new capabilities. Datasets used for these purposes,\nand particularly human preference datasets, are often noisy. The relatively\nsmall size post-training datasets, combined with parameter-efficient\nfine-tuning methods, enable the use of influence functions approximations to\ndetect and prune training examples that are harmful to performance on a\nvalidation set. In this work, we adapt the TL;DR dataset for reward model\ntraining to demonstrate how conjugate-gradient approximated influence functions\ncan be used to filter datasets. In our experiments, influence function\nfiltering yields a small retraining accuracy uplift of 1.5% after removing 10%\nof training examples. We also show that gradient similarity outperforms\ninfluence functions for detecting helpful training examples. This suggests that\nlocal curvature is important for detecting harmful training examples, but less\nso for identifying helpful examples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u8f6d\u68af\u5ea6\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fc7\u6ee4\u566a\u58f0\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4ee5\u6539\u53d8\u884c\u4e3a\u6216\u6fc0\u53d1\u65b0\u80fd\u529b\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\uff08\u5c24\u5176\u662f\u4eba\u7c7b\u504f\u597d\u6570\u636e\uff09\u5f80\u5f80\u5b58\u5728\u566a\u58f0\u3002", "method": "\u91c7\u7528\u5171\u8f6d\u68af\u5ea6\u8fd1\u4f3c\u5f71\u54cd\u51fd\u6570\u6765\u68c0\u6d4b\u548c\u5220\u9664\u5bf9\u9a8c\u8bc1\u96c6\u6027\u80fd\u6709\u5bb3\u7684\u8bad\u7ec3\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8fc7\u6ee410%\u7684\u8bad\u7ec3\u6570\u636e\u540e\uff0c\u6a21\u578b\u51c6\u786e\u7387\u63d0\u5347\u4e861.5%\u3002", "conclusion": "\u5c40\u90e8\u66f2\u7387\u5bf9\u68c0\u6d4b\u6709\u5bb3\u6837\u672c\u66f4\u91cd\u8981\uff0c\u4f46\u5bf9\u8bc6\u522b\u6709\u76ca\u6837\u672c\u6548\u679c\u8f83\u5dee\uff0c\u68af\u5ea6\u76f8\u4f3c\u6027\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2507.15088", "categories": ["cs.RO", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.15088", "abs": "https://arxiv.org/abs/2507.15088", "authors": ["Pouya Panahandeh", "Mohammad Pirani", "Baris Fidan", "Amir Khajepour"], "title": "Search-Based Autonomous Vehicle Motion Planning Using Game Theory", "comment": null, "summary": "In this paper, we propose a search-based interactive motion planning scheme\nfor autonomous vehicles (AVs), using a game-theoretic approach. In contrast to\ntraditional search-based approaches, the newly developed approach considers\nother road users (e.g. drivers and pedestrians) as intelligent agents rather\nthan static obstacles. This leads to the generation of a more realistic path\nfor the AV. Due to the low computational time, the proposed motion planning\nscheme is implementable in real-time applications. The performance of the\ndeveloped motion planning scheme is compared with existing motion planning\ntechniques and validated through experiments using WATonoBus, an electrical\nall-weather autonomous shuttle bus.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff0c\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u8003\u8651\u4e86\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u667a\u80fd\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5176\u4ed6\u9053\u8def\u7528\u6237\u89c6\u4e3a\u9759\u6001\u969c\u788d\uff0c\u800c\u65b0\u65b9\u6cd5\u5c06\u5176\u89c6\u4e3a\u667a\u80fd\u4ee3\u7406\uff0c\u4ee5\u751f\u6210\u66f4\u771f\u5b9e\u7684\u8def\u5f84\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u7ed3\u5408\u641c\u7d22\u6280\u672f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\u3002", "result": "\u8ba1\u7b97\u65f6\u95f4\u77ed\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling\nand pedestrian localization. Human modeling represents pedestrians in 3D space\nby fusing multiview information, making its quality crucial for detection\naccuracy. However, existing methods often introduce noise and have low\nprecision. While some approaches reduce noise by fitting on costly multiview 3D\nannotations, they often struggle to generalize across diverse scenes. To\neliminate reliance on human-labeled annotations and accurately model humans, we\npropose Depth-Consistent Human Modeling (DCHM), a framework designed for\nconsistent depth estimation and multiview fusion in global coordinates.\nSpecifically, our proposed pipeline with superpixel-wise Gaussian Splatting\nachieves multiview depth consistency in sparse-view, large-scaled, and crowded\nscenarios, producing precise point clouds for pedestrian localization.\nExtensive validations demonstrate that our method significantly reduces noise\nduring human modeling, outperforming previous state-of-the-art baselines.\nAdditionally, to our knowledge, DCHM is the first to reconstruct pedestrians\nand perform multiview segmentation in such a challenging setting. Code is\navailable on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDCHM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u4e00\u81f4\u6027\u548c\u591a\u89c6\u56fe\u878d\u5408\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u884c\u4eba\u68c0\u6d4b\u4e2d\u4eba\u4f53\u5efa\u6a21\u9636\u6bb5\u7684\u566a\u58f0\uff0c\u5e76\u5728\u7a00\u758f\u89c6\u56fe\u3001\u5927\u89c4\u6a21\u548c\u62e5\u6324\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u56fe\u884c\u4eba\u68c0\u6d4b\u4e2d\u4eba\u4f53\u5efa\u6a21\u9636\u6bb5\u5e38\u5f15\u5165\u566a\u58f0\u4e14\u7cbe\u5ea6\u4f4e\uff0c\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u4e00\u81f4\u6027\u4eba\u4f53\u5efa\u6a21\uff08DCHM\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u8d85\u50cf\u7d20\u7ea7\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u5168\u5c40\u5750\u6807\u7cfb\u4e0b\u7684\u591a\u89c6\u56fe\u6df1\u5ea6\u4e00\u81f4\u6027\u548c\u878d\u5408\u3002", "result": "\u663e\u8457\u51cf\u5c11\u566a\u58f0\uff0c\u751f\u6210\u7cbe\u786e\u70b9\u4e91\uff0c\u5728\u884c\u4eba\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u9996\u6b21\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u5b9e\u73b0\u884c\u4eba\u91cd\u5efa\u548c\u591a\u89c6\u56fe\u5206\u5272\u3002", "conclusion": "DCHM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u884c\u4eba\u68c0\u6d4b\u4e2d\u4eba\u4f53\u5efa\u6a21\u7684\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14353", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14353", "abs": "https://arxiv.org/abs/2507.14353", "authors": ["Harsh Nilesh Pathak", "Randy Paffenroth"], "title": "Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers", "comment": null, "summary": "Parameter efficient fine tuning (PEFT) is a versatile and extensible approach\nfor adapting a Large Language Model (LLM) for newer tasks. One of the most\nprominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on\nadjusting the attention weight matrices within individual decoder blocks of a\nGenerative Pre trained Transformer (GPT2). In contrast, we introduce Solo\nConnection a novel method that adapts the representation at the decoder-block\nlevel rather than modifying individual weight matrices. Not only does Solo\nConnection outperform LoRA on E2E natural language generation benchmarks, but\nit also reduces the number of trainable parameters by 59% relative to LoRA and\nby more than 99% compared to full fine-tuning of GPT2, an early version of\nLarge Language Models (LLMs). Solo Connection is also motivated by homotopy\ntheory: we introduce a trainable linear transformation that gradually\ninterpolates between a zero vector and the task-specific representation,\nenabling smooth and stable adaptation over time. While skip connections in the\noriginal 12 layer GPT2 are typically confined to individual decoder blocks,\nsubsequent GPT2 variants scale up to 48 layers, and even larger language models\ncan include 128 or more decoder blocks. These expanded architectures underscore\nthe need to revisit how skip connections are employed during fine-tuning. This\npaper focuses on long skip connections that link outputs of different decoder\nblocks, potentially enhancing the model's ability to adapt to new tasks while\nleveraging pre-trained knowledge.", "AI": {"tldr": "Solo Connection\u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u89e3\u7801\u5668\u5757\u7ea7\u522b\u7684\u8868\u793a\u800c\u975e\u5355\u4e2a\u6743\u91cd\u77e9\u9635\uff0c\u4f18\u4e8eLoRA\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u9700\u8981\u66f4\u9ad8\u6548\u5730\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u540c\u65f6\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "Solo Connection\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u53d8\u6362\u5728\u96f6\u5411\u91cf\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u4e4b\u95f4\u63d2\u503c\uff0c\u5b9e\u73b0\u5e73\u6ed1\u7a33\u5b9a\u7684\u9002\u5e94\u3002", "result": "Solo Connection\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8eLoRA\uff0c\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c1159%\uff08\u76f8\u6bd4LoRA\uff09\u548c99%\uff08\u76f8\u6bd4\u5168\u5fae\u8c03\uff09\u3002", "conclusion": "Solo Connection\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u591a\u5c42\u67b6\u6784\u3002"}}
{"id": "2507.15155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15155", "abs": "https://arxiv.org/abs/2507.15155", "authors": ["Majid Roshanfar", "Alex Zhang", "Changyan He", "Amir Hooshiar", "Dale J. Podolsky", "Thomas Looi", "Eric Diller"], "title": "Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions", "comment": null, "summary": "This letter introduces a novel learning-based modeling framework for a\nmagnetically steerable soft suction device designed for endoscopic endonasal\nbrain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm\ninner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,\nand integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape\nfeedback. Shape reconstruction is represented using four Bezier control points,\nenabling a compact and smooth model of the device's deformation. A data-driven\nmodel was trained on 5,097 experimental samples covering a range of magnetic\nfield magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical\ntip distances (90-100 mm), using both Neural Network (NN) and Random Forest\n(RF) architectures. The RF model outperformed the NN across all metrics,\nachieving a mean root mean square error of 0.087 mm in control point prediction\nand a mean shape reconstruction error of 0.064 mm. Feature importance analysis\nfurther revealed that magnetic field components predominantly influence distal\ncontrol points, while frequency and distance affect the base configuration.\nThis learning-based approach effectively models the complex nonlinear behavior\nof hyperelastic soft robots under magnetic actuation without relying on\nsimplified physical assumptions. By enabling sub-millimeter shape prediction\naccuracy and real-time inference, this work represents an advancement toward\nthe intelligent control of magnetically actuated soft robotic tools in\nminimally invasive neurosurgery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u78c1\u63a7\u8f6f\u5438\u5f15\u88c5\u7f6e\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u9f3b\u5185\u8111\u80bf\u7624\u5207\u9664\uff0c\u5b9e\u73b0\u4e86\u4e9a\u6beb\u7c73\u7ea7\u7684\u5f62\u72b6\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u78c1\u63a7\u8f6f\u673a\u5668\u4eba\u5728\u5fae\u521b\u795e\u7ecf\u5916\u79d1\u624b\u672f\u4e2d\u590d\u6742\u975e\u7ebf\u6027\u884c\u4e3a\u7684\u5efa\u6a21\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u7b80\u5316\u7684\u7269\u7406\u5047\u8bbe\u3002", "method": "\u4f7f\u75283D\u6253\u5370\u7684\u751f\u7269\u76f8\u5bb9\u6027\u6750\u6599\u5236\u9020\u8bbe\u5907\uff0c\u96c6\u6210FBG\u4f20\u611f\u5668\u5b9e\u65f6\u53cd\u9988\u5f62\u72b6\uff0c\u901a\u8fc7Bezier\u63a7\u5236\u70b9\u8868\u793a\u5f62\u72b6\u91cd\u5efa\uff0c\u8bad\u7ec3NN\u548cRF\u6a21\u578b\u3002", "result": "RF\u6a21\u578b\u5728\u6240\u6709\u6307\u6807\u4e0a\u4f18\u4e8eNN\uff0c\u63a7\u5236\u70b9\u9884\u6d4b\u7684\u5e73\u5747RMSE\u4e3a0.087 mm\uff0c\u5f62\u72b6\u91cd\u5efa\u8bef\u5dee\u4e3a0.064 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u78c1\u63a7\u8f6f\u673a\u5668\u4eba\u5728\u5fae\u521b\u624b\u672f\u4e2d\u7684\u667a\u80fd\u63a7\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.14533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14533", "abs": "https://arxiv.org/abs/2507.14533", "authors": ["Shuo Cao", "Nan Ma", "Jiayang Li", "Xiaohui Li", "Lihao Shao", "Kaiwen Zhu", "Yu Zhou", "Yuandong Pu", "Jiarui Wu", "Jiaquan Wang", "Bo Qu", "Wenhai Wang", "Yu Qiao", "Dajuin Yao", "Yihao Liu"], "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "comment": "43 pages, 31 figures, 13 tables", "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.", "AI": {"tldr": "\u63d0\u51faArtiMuse\u6a21\u578b\u548cArtiMuse-10K\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u591a\u6a21\u6001IAA\u65b9\u6cd5\u7684\u6a21\u6001\u504f\u5dee\u548c\u7ec6\u7c92\u5ea6\u5c5e\u6027\u5206\u89e3\u95ee\u9898\u3002", "motivation": "\u6559\u80b2\u3001\u827a\u672f\u521b\u4f5c\u548cAIGC\u6280\u672f\u7684\u53d1\u5c55\u5bf9\u56fe\u50cf\u7f8e\u5b66\u8bc4\u4f30\uff08IAA\uff09\u63d0\u51fa\u66f4\u9ad8\u8981\u6c42\uff0c\u73b0\u6709MLLM\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u504f\u5dee\u548c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "method": "\u5f00\u53d1ArtiMuse\u6a21\u578b\uff0c\u7ed3\u5408\u8bc4\u5206\u4e0e\u4e13\u5bb6\u7ea7\u7406\u89e3\u80fd\u529b\uff1b\u6784\u5efaArtiMuse-10K\u6570\u636e\u96c6\uff0c\u5305\u542b10,000\u5f20\u4e13\u4e1a\u6807\u6ce8\u56fe\u50cf\u3002", "result": "ArtiMuse\u6a21\u578b\u548c\u6570\u636e\u96c6\u516c\u5f00\uff0c\u63a8\u52a8IAA\u9886\u57df\u53d1\u5c55\u3002", "conclusion": "ArtiMuse\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7f8e\u5b66\u8bc4\u4f30\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u652f\u6301\u3002"}}
{"id": "2507.14387", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14387", "abs": "https://arxiv.org/abs/2507.14387", "authors": ["Arun Vignesh Malarkkan", "Dongjie Wang", "Haoyue Bai", "Yanjie Fu"], "title": "Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures", "comment": "12 pages, 5 figures, 3 Tables, under review in IEEE Transactions on\n  Big Data", "summary": "The escalating threat of cyberattacks on real-time critical infrastructures\nposes serious risks to public safety, demanding detection methods that\neffectively capture complex system interdependencies and adapt to evolving\nattack patterns. Traditional real-time anomaly detection techniques often\nsuffer from excessive false positives due to their statistical sensitivity to\nhigh data variance and class imbalance. To address these limitations, recent\nresearch has explored modeling causal relationships among system components.\nHowever, prior work mainly focuses on offline causal graph-based approaches\nthat require static historical data and fail to generalize to real-time\nsettings. These methods are fundamentally constrained by: (1) their inability\nto adapt to dynamic shifts in data distribution without retraining, and (2) the\nrisk of catastrophic forgetting when lacking timely supervision in live\nsystems. To overcome these challenges, we propose INCADET, a novel framework\nfor incremental causal graph learning tailored to real-time cyberattack\ndetection. INCADET dynamically captures evolving system behavior by\nincrementally updating causal graphs across streaming time windows. The\nframework comprises three modules: 1) Early Symptom Detection: Detects\ntransitions in system status using divergence in edge-weight distributions\nacross sequential causal graphs. 2) Incremental Causal Graph Learning:\nLeverages experience replay and edge reinforcement to continually refine causal\nstructures while preserving prior knowledge. 3) Causal Graph Classification:\nEmploys Graph Convolutional Networks (GCNs) to classify system status using the\nlearned causal graphs. Extensive experiments on real-world critical\ninfrastructure datasets demonstrate that INCADET achieves superior accuracy,\nrobustness, and adaptability compared to both static causal and deep temporal\nbaselines in evolving attack scenarios.", "AI": {"tldr": "INCADET\u662f\u4e00\u79cd\u9488\u5bf9\u5b9e\u65f6\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u7684\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u56e0\u679c\u56fe\u6765\u9002\u5e94\u7cfb\u7edf\u884c\u4e3a\u53d8\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u56e0\u9ad8\u6570\u636e\u65b9\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\uff0c\u4e14\u73b0\u6709\u56e0\u679c\u56fe\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u5b9e\u65f6\u52a8\u6001\u6570\u636e\u5206\u5e03\u53d8\u5316\u3002", "method": "INCADET\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u65e9\u671f\u75c7\u72b6\u68c0\u6d4b\u3001\u589e\u91cf\u56e0\u679c\u56fe\u5b66\u4e60\u548c\u56e0\u679c\u56fe\u5206\u7c7b\uff0c\u7ed3\u5408\u7ecf\u9a8c\u56de\u653e\u548cGCN\u6280\u672f\u3002", "result": "\u5728\u771f\u5b9e\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u6570\u636e\u96c6\u4e0a\uff0cINCADET\u5728\u52a8\u6001\u653b\u51fb\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "INCADET\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u89e3\u51b3\u4e86\u5b9e\u65f6\u56e0\u679c\u56fe\u66f4\u65b0\u7684\u6311\u6218\uff0c\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u7f51\u7edc\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15189", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15189", "abs": "https://arxiv.org/abs/2507.15189", "authors": ["Kevin Christiansen Marsim", "Jinwoo Jeon", "Yeeun Kim", "Myeongwoo Jeong", "Hyun Myung"], "title": "CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer", "comment": null, "summary": "Depth information which specifies the distance between objects and current\nposition of the robot is essential for many robot tasks such as navigation.\nRecently, researchers have proposed depth completion frameworks to provide\ndense depth maps that offer comprehensive information about the surrounding\nenvironment. However, existing methods show significant trade-offs between\ncomputational efficiency and accuracy during inference. The substantial memory\nand computational requirements make them unsuitable for real-time applications,\nhighlighting the need to improve the completeness and accuracy of depth\ninformation while improving processing speed to enhance robot performance in\nvarious tasks. To address these challenges, in this paper, we propose\nCHADET(cross-hierarchical-attention depth-completion transformer), a\nlightweight depth-completion network that can generate accurate dense depth\nmaps from RGB images and sparse depth points. For each pair, its feature is\nextracted from the depthwise blocks and passed to the equally lightweight\ntransformer-based decoder. In the decoder, we utilize the novel\ncross-hierarchical-attention module that refines the image features from the\ndepth information. Our approach improves the quality and reduces memory usage\nof the depth map prediction, as validated in both KITTI, NYUv2, and VOID\ndatasets.", "AI": {"tldr": "CHADET\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u8865\u5168\u7f51\u7edc\uff0c\u901a\u8fc7\u4ea4\u53c9\u5c42\u6b21\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u6df1\u5ea6\u56fe\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6743\u8861\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51faCHADET\uff0c\u7ed3\u5408\u6df1\u5ea6\u5757\u7279\u5f81\u63d0\u53d6\u548c\u8f7b\u91cf\u7ea7Transformer\u89e3\u7801\u5668\uff0c\u5229\u7528\u4ea4\u53c9\u5c42\u6b21\u6ce8\u610f\u529b\u6a21\u5757\u4f18\u5316\u7279\u5f81\u3002", "result": "\u5728KITTI\u3001NYUv2\u548cVOID\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6df1\u5ea6\u56fe\u9884\u6d4b\u8d28\u91cf\u7684\u63d0\u5347\u548c\u5185\u5b58\u4f7f\u7528\u7684\u51cf\u5c11\u3002", "conclusion": "CHADET\u5728\u4fdd\u6301\u8f7b\u91cf\u5316\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u8865\u5168\u7684\u51c6\u786e\u6027\u548c\u5904\u7406\u901f\u5ea6\u3002"}}
{"id": "2507.14543", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.14543", "abs": "https://arxiv.org/abs/2507.14543", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6d4f\u89c8\u5668\u6269\u5c55\uff0c\u7528\u4e8e\u5728\u89c6\u9891\u4f1a\u8bae\u4e2d\u5c06\u624b\u8bed\u81ea\u52a8\u7ffb\u8bd1\u4e3a\u5b57\u5e55\uff0c\u4ee5\u5e2e\u52a9\u542c\u529b\u969c\u788d\u8005\u4e0e\u666e\u901a\u4eba\u6c9f\u901a\u3002", "motivation": "\u89e3\u51b3\u542c\u529b\u969c\u788d\u8005\u5728\u89c6\u9891\u4f1a\u8bae\u4e2d\u66f4\u503e\u5411\u4e8e\u4f7f\u7528\u624b\u8bed\u800c\u975e\u6253\u5b57\u7684\u95ee\u9898\uff0c\u6d88\u9664\u6c9f\u901a\u969c\u788d\u3002", "method": "\u5229\u7528\u5305\u542b2000\u591a\u4e2a\u5355\u8bcd\u7ea7ASL\u89c6\u9891\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u6d4f\u89c8\u5668\u6269\u5c55\u8fdb\u884c\u624b\u8bed\u8bc6\u522b\u548c\u7ffb\u8bd1\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5b9e\u73b0\u624b\u8bed\u5230\u5b57\u5e55\u7684\u81ea\u52a8\u7ffb\u8bd1\u3002", "conclusion": "\u8be5\u5de5\u5177\u6709\u6f5c\u529b\u663e\u8457\u6539\u5584\u542c\u529b\u969c\u788d\u8005\u5728\u89c6\u9891\u4f1a\u8bae\u4e2d\u7684\u6c9f\u901a\u4f53\u9a8c\u3002"}}
{"id": "2507.14419", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14419", "abs": "https://arxiv.org/abs/2507.14419", "authors": ["Guojun Wu"], "title": "It's Not That Simple. An Analysis of Simple Test-Time Scaling", "comment": null, "summary": "Prior work proposed simple test-time scaling, a method for replicating this\nscaling behavior with models distilled from o1-like models by manually\ncontrolling test-time compute: either scaling down by enforcing a maximum\nlength or scaling up by iteratively appending \"Wait\" when the model is about to\nterminate its generation. This paper presents an analysis of simple test-time\nscaling and finds that the scaling behavior is largely attributed to scaling\ndown by enforcing a maximum length. In contrast, fine-tuning on long CoT data\ndistilled from o1-like models has no significant impact on scaling behavior,\nand scaling up by appending \"Wait\" leads to inconsistencies, as the model may\noscillate between solutions. A key distinction exists between scaling down by\nenforcing a maximum length and scaling up test-time compute in o1-like models,\nsuch as DeepSeek-R1\\@. These models are typically allowed to utilize as much\ncompute as needed, with the only constraint being the model's maximum supported\nlength. By learning to naturally scale up test-time compute during\nreinforcement learning, o1-like models surpass their peak performance when\nscaling up. In contrast, simple test-time scaling progressively imposes a lower\nupper limit on model performance as it scales down. While replicating the\ntest-time scaling behavior of o1 models can be straightforward by scaling down,\nit is crucial to recognize that the goal of scaling test-time compute is to\nunlock higher performance -- beyond what the model could originally achieve --\nrather than merely reproducing the appearance of scaling behavior.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u4e3b\u8981\u901a\u8fc7\u9650\u5236\u6700\u5927\u957f\u5ea6\u5b9e\u73b0\u7f29\u653e\uff0c\u800c\u901a\u8fc7\u8ffd\u52a0\u201cWait\u201d\u6216\u5fae\u8c03\u957fCoT\u6570\u636e\u5bf9\u7f29\u653e\u884c\u4e3a\u5f71\u54cd\u4e0d\u5927\u3002", "motivation": "\u7814\u7a76\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\u7684\u5b9e\u9645\u6548\u679c\uff0c\u63ed\u793a\u5176\u4e0eo1\u7c7b\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7f29\u653e\u4e0a\u7684\u5173\u952e\u533a\u522b\u3002", "method": "\u5206\u6790\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u65b9\u6cd5\uff0c\u5305\u62ec\u9650\u5236\u6700\u5927\u957f\u5ea6\u548c\u8ffd\u52a0\u201cWait\u201d\u4e24\u79cd\u7b56\u7565\uff0c\u5e76\u4e0eo1\u7c7b\u6a21\u578b\u7684\u81ea\u7136\u7f29\u653e\u884c\u4e3a\u5bf9\u6bd4\u3002", "result": "\u9650\u5236\u6700\u5927\u957f\u5ea6\u662f\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u7684\u4e3b\u8981\u673a\u5236\uff0c\u800c\u8ffd\u52a0\u201cWait\u201d\u4f1a\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\uff1bo1\u7c7b\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u7136\u7f29\u653e\uff0c\u6027\u80fd\u4e0a\u9650\u66f4\u9ad8\u3002", "conclusion": "\u7b80\u5355\u6d4b\u8bd5\u65f6\u7f29\u653e\u4ec5\u80fd\u590d\u5236\u7f29\u653e\u8868\u8c61\uff0c\u800co1\u7c7b\u6a21\u578b\u7684\u81ea\u7136\u7f29\u653e\u80fd\u89e3\u9501\u66f4\u9ad8\u6027\u80fd\uff0c\u76ee\u6807\u5e94\u805a\u7126\u4e8e\u6027\u80fd\u63d0\u5347\u800c\u975e\u8868\u8c61\u590d\u5236\u3002"}}
{"id": "2507.15266", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15266", "abs": "https://arxiv.org/abs/2507.15266", "authors": ["Haichao Liu", "Haoren Guo", "Pei Liu", "Benshan Ma", "Yuxiang Zhang", "Jun Ma", "Tong Heng Lee"], "title": "VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving", "comment": "14 pages, 12 figures", "summary": "Scene understanding and risk-aware attentions are crucial for human drivers\nto make safe and effective driving decisions. To imitate this cognitive ability\nin urban autonomous driving while ensuring the transparency and\ninterpretability, we propose a vision-language model (VLM)-enhanced unified\ndecision-making and motion control framework, named VLM-UDMC. This framework\nincorporates scene reasoning and risk-aware insights into an upper-level slow\nsystem, which dynamically reconfigures the optimal motion planning for the\ndownstream fast system. The reconfiguration is based on real-time environmental\nchanges, which are encoded through context-aware potential functions. More\nspecifically, the upper-level slow system employs a two-step reasoning policy\nwith Retrieval-Augmented Generation (RAG), leveraging foundation models to\nprocess multimodal inputs and retrieve contextual knowledge, thereby generating\nrisk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM\nprovides real-time trajectory predictions for heterogeneous traffic\nparticipants by extracting smoother trend representations for short-horizon\ntrajectory prediction. The effectiveness of the proposed VLM-UDMC framework is\nverified via both simulations and real-world experiments with a full-size\nautonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively\nleverages scene understanding and attention decomposition for rational driving\ndecisions, thus improving the overall urban driving performance. Our\nopen-source project is available at https://github.com/henryhcliu/vlmudmc.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u51b3\u7b56\u4e0e\u8fd0\u52a8\u63a7\u5236\u6846\u67b6VLM-UDMC\uff0c\u901a\u8fc7\u573a\u666f\u63a8\u7406\u548c\u98ce\u9669\u611f\u77e5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6a21\u4eff\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u573a\u666f\u7406\u89e3\u548c\u98ce\u9669\u611f\u77e5\u80fd\u529b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u51b3\u7b56\u6548\u7387\u3002", "method": "\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u591a\u6838\u5206\u89e3LSTM\uff0c\u5b9e\u73b0\u52a8\u6001\u8fd0\u52a8\u89c4\u5212\u548c\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u8f66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u57ce\u5e02\u9a7e\u9a76\u6027\u80fd\u3002", "conclusion": "VLM-UDMC\u6846\u67b6\u901a\u8fc7\u573a\u666f\u7406\u89e3\u548c\u6ce8\u610f\u529b\u5206\u89e3\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u5408\u7406\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2507.14544", "categories": ["cs.CV", "cs.AI", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8; H.3.1"], "pdf": "https://arxiv.org/pdf/2507.14544", "abs": "https://arxiv.org/abs/2507.14544", "authors": ["Sujata Gaihre", "Amir Thapa Magar", "Prasuna Pokharel", "Laxmi Tiwari"], "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025", "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings", "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8eFlorence\u6a21\u578b\u7684\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u65b9\u6cd5\uff0c\u7528\u4e8e\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u56fe\u50cf\uff0c\u901a\u8fc7\u9886\u57df\u589e\u5f3a\u548c\u5fae\u8c03\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u5728\u80c3\u80a0\u9053\u5185\u7aa5\u955c\u9886\u57df\u7684\u6311\u6218\uff0c\u63d0\u5347\u6a21\u578b\u5728\u4e34\u5e8a\u76f8\u5173\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528Florence\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\uff0c\u5e76\u5e94\u7528\u9886\u57df\u589e\u5f3a\u6280\u672f\u3002", "result": "\u5728KASVIR\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Florence\u6a21\u578b\u540e\uff0c\u5728\u5b98\u65b9\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u533b\u5b66VQA\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7ebf\u3002"}}
{"id": "2507.14446", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14446", "abs": "https://arxiv.org/abs/2507.14446", "authors": ["Feng Liu", "Ying Liu", "Carson Eisenach"], "title": "Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness", "comment": null, "summary": "In this work, we study how to efficiently apply reinforcement learning (RL)\nfor solving large-scale stochastic optimization problems by leveraging\nintervention models. The key of the proposed methodology is to better explore\nthe solution space by simulating and composing the stochastic processes using\npre-trained deep learning (DL) models. We demonstrate our approach on a\nchallenging real-world application, the multi-sourcing multi-period inventory\nmanagement problem in supply chain optimization. In particular, we employ deep\nRL models for learning and forecasting the stochastic supply chain processes\nunder a range of assumptions. Moreover, we also introduce a constraint\ncoordination mechanism, designed to forecast dual costs given the\ncross-products constraints in the inventory network. We highlight that instead\nof directly modeling the complex physical constraints into the RL optimization\nproblem and solving the stochastic problem as a whole, our approach breaks down\nthose supply chain processes into scalable and composable DL modules, leading\nto improved performance on large real-world datasets. We also outline open\nproblems for future research to further investigate the efficacy of such\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f9b\u5e94\u94fe\u5e93\u5b58\u7ba1\u7406\u4e2d\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u968f\u673a\u4f18\u5316\u95ee\u9898\u4e2d\u63a2\u7d22\u89e3\u7a7a\u95f4\u7684\u6548\u7387\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u7684\u4f9b\u5e94\u94fe\u573a\u666f\u4e2d\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6a21\u62df\u548c\u7ec4\u5408\u968f\u673a\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u7ea6\u675f\u534f\u8c03\u673a\u5236\u9884\u6d4b\u53cc\u91cd\u6210\u672c\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u65b9\u6cd5\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u7ea6\u675f\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.15293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15293", "abs": "https://arxiv.org/abs/2507.15293", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "RepILN: Reparameterized Inertial Localization Network", "comment": null, "summary": "Inertial localization is regarded as a promising positioning solution for\nconsumer-grade IoT devices due to its cost-effectiveness and independence from\nexternal infrastructure. However, data-driven inertial localization methods\noften rely on increasingly complex network architectures to improve accuracy,\nwhich challenges the limited computational resources of IoT devices. Moreover,\nthese methods frequently overlook the importance of modeling long-term\ndependencies in inertial measurements - a critical factor for accurate\ntrajectory reconstruction - thereby limiting localization performance. To\naddress these challenges, we propose a reparameterized inertial localization\nnetwork that uses a multi-branch structure during training to enhance feature\nextraction. At inference time, this structure is transformed into an equivalent\nsingle-path architecture to improve parameter efficiency. To further capture\nlong-term dependencies in motion trajectories, we introduce a temporal-scale\nsparse attention mechanism that selectively emphasizes key trajectory segments\nwhile suppressing noise. Additionally, a gated convolutional unit is\nincorporated to effectively integrate long-range dependencies with local\nfine-grained features. Extensive experiments on public benchmarks demonstrate\nthat our method achieves a favorable trade-off between accuracy and model\ncompactness. For example, on the RoNIN dataset, our approach reduces the\nAbsolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while\nreducing the number of parameters by 3.86%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u53c2\u6570\u5316\u7684\u60ef\u6027\u5b9a\u4f4d\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5206\u652f\u7ed3\u6784\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u8f6c\u6362\u4e3a\u5355\u8def\u5f84\u67b6\u6784\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u540c\u65f6\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5377\u79ef\u5355\u5143\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6a21\u578b\u7d27\u51d1\u6027\u3002", "motivation": "\u60ef\u6027\u5b9a\u4f4d\u56e0\u5176\u6210\u672c\u6548\u76ca\u548c\u72ec\u7acb\u6027\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5e38\u4f9d\u8d56\u590d\u6742\u7f51\u7edc\u67b6\u6784\uff0c\u4e14\u5ffd\u89c6\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u91c7\u7528\u591a\u5206\u652f\u8bad\u7ec3\u7ed3\u6784\u8f6c\u6362\u4e3a\u5355\u8def\u5f84\u63a8\u7406\u67b6\u6784\uff0c\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5377\u79ef\u5355\u5143\uff0c\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u548c\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u3002", "result": "\u5728RoNIN\u6570\u636e\u96c6\u4e0a\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e2.59%\uff0c\u53c2\u6570\u91cf\u51cf\u5c113.86%\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u4e0e\u6a21\u578b\u7d27\u51d1\u6027\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u60ef\u6027\u5b9a\u4f4d\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u548c\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u7269\u8054\u7f51\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14549", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14549", "abs": "https://arxiv.org/abs/2507.14549", "authors": ["Haotian Deng", "Chi Zhang", "Chen Wei", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions", "comment": "Accepted by IJCNN 2025", "summary": "A fundamental challenge in affective cognitive science is to develop models\nthat accurately capture the relationship between external emotional stimuli and\nhuman internal experiences. While ANNs have demonstrated remarkable accuracy in\nfacial expression recognition, their ability to model inter-individual\ndifferences in human perception remains underexplored. This study investigates\nthe phenomenon of high perceptual variability-where individuals exhibit\nsignificant differences in emotion categorization even when viewing the same\nstimulus. Inspired by the similarity between ANNs and human perception, we\nhypothesize that facial expression samples that are ambiguous for ANN\nclassifiers also elicit divergent perceptual judgments among human observers.\nTo examine this hypothesis, we introduce a novel perceptual boundary sampling\nmethod to generate facial expression stimuli that lie along ANN decision\nboundaries. These ambiguous samples form the basis of the varEmotion dataset,\nconstructed through large-scale human behavioral experiments. Our analysis\nreveals that these ANN-confusing stimuli also provoke heightened perceptual\nuncertainty in human participants, highlighting shared computational principles\nin emotion perception. Finally, by fine-tuning ANN representations using\nbehavioral data, we achieve alignment between ANN predictions and both\ngroup-level and individual-level human perceptual patterns. Our findings\nestablish a systematic link between ANN decision boundaries and human\nperceptual variability, offering new insights into personalized modeling of\nemotional interpretation.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u51b3\u7b56\u8fb9\u754c\u4e0e\u4eba\u7c7b\u60c5\u611f\u611f\u77e5\u53d8\u5f02\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u53d1\u73b0ANN\u96be\u4ee5\u5206\u7c7b\u7684\u6a21\u7cca\u8868\u60c5\u6837\u672c\u540c\u6837\u5f15\u53d1\u4eba\u7c7b\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u60c5\u611f\u8ba4\u77e5\u79d1\u5b66\u4e2d\u5916\u90e8\u60c5\u611f\u523a\u6fc0\u4e0e\u4eba\u7c7b\u5185\u90e8\u4f53\u9a8c\u5173\u7cfb\u5efa\u6a21\u7684\u6311\u6218\uff0c\u7279\u522b\u662fANN\u5728\u4e2a\u4f53\u611f\u77e5\u5dee\u5f02\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u611f\u77e5\u8fb9\u754c\u91c7\u6837\u65b9\u6cd5\uff0c\u751f\u6210\u4f4d\u4e8eANN\u51b3\u7b56\u8fb9\u754c\u7684\u9762\u90e8\u8868\u60c5\u523a\u6fc0\uff0c\u6784\u5efavarEmotion\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6a21\u7cca\u6837\u672c\u5f15\u53d1\u4eba\u7c7b\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u884c\u4e3a\u6570\u636e\u5fae\u8c03ANN\u8868\u5f81\uff0c\u5b9e\u73b0\u4e86ANN\u9884\u6d4b\u4e0e\u4eba\u7c7b\u7fa4\u4f53\u53ca\u4e2a\u4f53\u611f\u77e5\u6a21\u5f0f\u7684\u5bf9\u9f50\u3002", "conclusion": "\u5efa\u7acb\u4e86ANN\u51b3\u7b56\u8fb9\u754c\u4e0e\u4eba\u7c7b\u611f\u77e5\u53d8\u5f02\u6027\u7684\u7cfb\u7edf\u6027\u8054\u7cfb\uff0c\u4e3a\u60c5\u611f\u89e3\u91ca\u7684\u4e2a\u6027\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.14484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14484", "abs": "https://arxiv.org/abs/2507.14484", "authors": ["Yule Li", "Yifeng Lu", "Zhen Wang", "Zhewei Wei", "Yaliang Li", "Bolin Ding"], "title": "ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions", "comment": null, "summary": "In recent years, graph neural networks (GNN) have achieved unprecedented\nsuccesses in node classification tasks. Although GNNs inherently encode\nspecific inductive biases (e.g., acting as low-pass or high-pass filters), most\nexisting methods implicitly assume conditional independence among node labels\nin their optimization objectives. While this assumption is suitable for\ntraditional classification tasks such as image recognition, it contradicts the\nintuitive observation that node labels in graphs remain correlated, even after\nconditioning on the graph structure. To make structured predictions for node\nlabels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for\nStructured node Classification. ReDiSC estimates the joint distribution of node\nlabels using a reparameterized masked diffusion model, which is learned through\nthe variational expectation-maximization (EM) framework. Our theoretical\nanalysis shows the efficiency advantage of ReDiSC in the E-step compared to\nDPM-SNC, a state-of-the-art model that relies on a manifold-constrained\ndiffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's\nM-step objective to popular GNN and label propagation hybrid approaches.\nExtensive experiments demonstrate that ReDiSC achieves superior or highly\ncompetitive performance compared to state-of-the-art GNN, label propagation,\nand diffusion-based baselines across both homophilic and heterophilic graphs of\nvarying sizes. Notably, ReDiSC scales effectively to large-scale datasets on\nwhich previous structured diffusion methods fail due to computational\nconstraints, highlighting its significant practical advantage in structured\nnode classification tasks.", "AI": {"tldr": "ReDiSC\u662f\u4e00\u79cd\u57fa\u4e8e\u91cd\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u7ed3\u6784\u5316\u8282\u70b9\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5206EM\u6846\u67b6\u5b66\u4e60\u8282\u70b9\u6807\u7b7e\u7684\u8054\u5408\u5206\u5e03\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5047\u8bbe\u8282\u70b9\u6807\u7b7e\u6761\u4ef6\u72ec\u7acb\uff0c\u5ffd\u7565\u4e86\u56fe\u4e2d\u6807\u7b7e\u7684\u76f8\u5173\u6027\uff0cReDiSC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u91cd\u53c2\u6570\u5316\u63a9\u7801\u6269\u6563\u6a21\u578b\u4f30\u8ba1\u8282\u70b9\u6807\u7b7e\u8054\u5408\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u53d8\u5206EM\u6846\u67b6\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u56fe\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u80fd\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "conclusion": "ReDiSC\u5728\u7ed3\u6784\u5316\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u6548\u7387\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.15444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15444", "abs": "https://arxiv.org/abs/2507.15444", "authors": ["Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "comment": "17 pages", "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u60ac\u505c\u7684\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\u3002", "motivation": "\u5728\u72ed\u7a84\u7ba1\u9053\u7b49\u53d7\u9650\u7a7a\u95f4\u4e2d\uff0c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u98de\u884c\u9762\u4e34\u7531\u81ea\u8bf1\u5bfc\u6c14\u6d41\u6270\u52a8\u5e26\u6765\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6301\u7eed\u8fd0\u52a8\uff0c\u8981\u4e48\u60ac\u505c\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4f4e\u5ef6\u8fdf\u7684\u4e8b\u4ef6\u578b\u70df\u96fe\u6d4b\u901f\u6cd5\uff0c\u7ed3\u5408\u57fa\u4e8e\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u6270\u52a8\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a7\u5236\u5668\u3002", "result": "\u7cfb\u7edf\u5728\u7ba1\u9053\u6a2a\u622a\u9762\u4fa7\u5411\u79fb\u52a8\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u6709\u6548\u62b5\u6d88\u77ac\u6001\u6c14\u52a8\u6548\u5e94\uff0c\u907f\u514d\u78b0\u649e\u3002", "conclusion": "\u9996\u6b21\u5c55\u793a\u4e86\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u65e0\u4eba\u673a\u95ed\u73af\u63a7\u5236\uff0c\u4e3a\u590d\u6742\u6c14\u52a8\u73af\u5883\u4e2d\u7684\u98de\u884c\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.14553", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u76f8\u673a\u5f15\u5bfc\u7cfb\u7edf\uff0c\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u548c\u53bb\u9664\u7167\u7247\u4e2d\u7684\u6742\u4e71\uff0c\u63d0\u5347\u7167\u7247\u8d28\u91cf\u3002", "motivation": "\u4e1a\u4f59\u6444\u5f71\u5e08\u5e38\u56e0\u758f\u5ffd\u6216\u7ecf\u9a8c\u4e0d\u8db3\u5728\u7167\u7247\u4e2d\u7559\u4e0b\u6742\u4e71\uff0c\u5f71\u54cd\u60c5\u611f\u8868\u8fbe\u548c\u6545\u4e8b\u4f20\u8fbe\u3002", "method": "\u7ed3\u5408\u7f8e\u5b66\u8bc4\u4f30\u7b97\u6cd5\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u56fe\u50cf\u4fee\u590d\u6280\u672f\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u6742\u4e71\u8bc6\u522b\u548c\u53bb\u9664\u5de5\u5177\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u5e2e\u52a9\u7528\u6237\u66f4\u5feb\u8bc6\u522b\u5e72\u6270\u7269\u5e76\u62cd\u6444\u66f4\u9ad8\u8d28\u91cf\u7684\u7167\u7247\u3002", "conclusion": "\u7cfb\u7edf\u901a\u8fc7\u7075\u6d3b\u754c\u9762\u548c\u51c6\u786e\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u6444\u5f71\u8d28\u91cf\uff0c\u51cf\u5c11\u6742\u4e71\u5e72\u6270\u3002"}}
{"id": "2507.14487", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14487", "abs": "https://arxiv.org/abs/2507.14487", "authors": ["Ukjo Hwang", "Songnam Hong"], "title": "Federated Reinforcement Learning in Heterogeneous Environments", "comment": null, "summary": "We investigate a Federated Reinforcement Learning with Environment\nHeterogeneity (FRL-EH) framework, where local environments exhibit statistical\nheterogeneity. Within this framework, agents collaboratively learn a global\npolicy by aggregating their collective experiences while preserving the privacy\nof their local trajectories. To better reflect real-world scenarios, we\nintroduce a robust FRL-EH framework by presenting a novel global objective\nfunction. This function is specifically designed to optimize a global policy\nthat ensures robust performance across heterogeneous local environments and\ntheir plausible perturbations. We propose a tabular FRL algorithm named FedRQ\nand theoretically prove its asymptotic convergence to an optimal policy for the\nglobal objective function. Furthermore, we extend FedRQ to environments with\ncontinuous state space through the use of expectile loss, addressing the key\nchallenge of minimizing a value function over a continuous subset of the state\nspace. This advancement facilitates the seamless integration of the principles\nof FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive\nempirical evaluations validate the effectiveness and robustness of our FRL\nalgorithms across diverse heterogeneous environments, consistently achieving\nsuperior performance over the existing state-of-the-art FRL algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6FRL-EH\uff0c\u7528\u4e8e\u5904\u7406\u73af\u5883\u5f02\u8d28\u6027\uff0c\u5e76\u901a\u8fc7FedRQ\u7b97\u6cd5\u5b9e\u73b0\u5168\u5c40\u7b56\u7565\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u672c\u5730\u73af\u5883\u7edf\u8ba1\u5f02\u8d28\u6027\u4e0b\u7684\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4f18\u5316\u5168\u5c40\u7b56\u7565\u3002", "method": "\u63d0\u51faFedRQ\u7b97\u6cd5\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6536\u655b\u6027\uff0c\u5e76\u6269\u5c55\u81f3\u8fde\u7eed\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1FedRQ\u5728\u5f02\u8d28\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FRL-EH\u6846\u67b6\u548cFedRQ\u7b97\u6cd5\u5728\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.15469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15469", "abs": "https://arxiv.org/abs/2507.15469", "authors": ["Thanh Thi Nguyen", "Saeid Nahavandi", "Imran Razzak", "Dung Nguyen", "Nhat Truong Pham", "Quoc Viet Hung Nguyen"], "title": "The Emergence of Deep Reinforcement Learning for Path Planning", "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "summary": "The increasing demand for autonomous systems in complex and dynamic\nenvironments has driven significant research into intelligent path planning\nmethodologies. For decades, graph-based search algorithms, linear programming\ntechniques, and evolutionary computation methods have served as foundational\napproaches in this domain. Recently, deep reinforcement learning (DRL) has\nemerged as a powerful method for enabling autonomous agents to learn optimal\nnavigation strategies through interaction with their environments. This survey\nprovides a comprehensive overview of traditional approaches as well as the\nrecent advancements in DRL applied to path planning tasks, focusing on\nautonomous vehicles, drones, and robotic platforms. Key algorithms across both\nconventional and learning-based paradigms are categorized, with their\ninnovations and practical implementations highlighted. This is followed by a\nthorough discussion of their respective strengths and limitations in terms of\ncomputational efficiency, scalability, adaptability, and robustness. The survey\nconcludes by identifying key open challenges and outlining promising avenues\nfor future research. Special attention is given to hybrid approaches that\nintegrate DRL with classical planning techniques to leverage the benefits of\nboth learning-based adaptability and deterministic reliability, offering\npromising directions for robust and resilient autonomous navigation.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63a2\u8ba8\u4e86\u6df7\u5408\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "motivation": "\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u7cfb\u7edf\u7684\u9700\u6c42\u63a8\u52a8\u4e86\u667a\u80fd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7684\u7814\u7a76\uff0c\u5c24\u5176\u662fDRL\u7684\u5174\u8d77\u3002", "method": "\u5206\u7c7b\u548c\u6bd4\u8f83\u4e86\u4f20\u7edf\u56fe\u641c\u7d22\u3001\u7ebf\u6027\u89c4\u5212\u3001\u8fdb\u5316\u8ba1\u7b97\u53caDRL\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u603b\u7ed3\u4e86\u5404\u7c7b\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5f3a\u8c03\u4e86\u6df7\u5408\u65b9\u6cd5\u5728\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u7279\u522b\u662f\u7ed3\u5408DRL\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u6df7\u5408\u8def\u5f84\u89c4\u5212\u6280\u672f\u3002"}}
{"id": "2507.14555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14555", "abs": "https://arxiv.org/abs/2507.14555", "authors": ["Jintang Xue", "Ganning Zhao", "Jie-En Yao", "Hong-En Chen", "Yue Hu", "Meida Chen", "Suya You", "C. -C. Jay Kuo"], "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions", "comment": null, "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.", "AI": {"tldr": "Descrip3D\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7f16\u7801\u7269\u4f53\u5173\u7cfb\uff0c\u63d0\u53473D\u573a\u666f\u7406\u89e3\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u67093D\u573a\u666f-\u8bed\u8a00\u6a21\u578b\u5728\u7269\u4f53\u5173\u7cfb\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0cDescrip3D\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u589e\u5f3a\u5173\u7cfb\u8868\u793a\u3002", "method": "Descrip3D\u4e3a\u7269\u4f53\u6dfb\u52a0\u6587\u672c\u63cf\u8ff0\uff0c\u7ed3\u5408\u5d4c\u5165\u878d\u5408\u548c\u63d0\u793a\u7ea7\u6ce8\u5165\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u7edf\u4e00\u63a8\u7406\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDescrip3D\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u5173\u7cfb\u8868\u793a\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u5ba4\u5185\u573a\u666f\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.14492", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14492", "abs": "https://arxiv.org/abs/2507.14492", "authors": ["Satyankar Chandra", "Ashutosh Gupta", "Kaushik Mallik", "Krishna Shankaranarayanan", "Namrita Varshney"], "title": "Glitches in Decision Tree Ensemble Models", "comment": null, "summary": "Many critical decision-making tasks are now delegated to machine-learned\nmodels, and it is imperative that their decisions are trustworthy and reliable,\nand their outputs are consistent across similar inputs. We identify a new\nsource of unreliable behaviors-called glitches-which may significantly impair\nthe reliability of AI models having steep decision boundaries. Roughly\nspeaking, glitches are small neighborhoods in the input space where the model's\noutput abruptly oscillates with respect to small changes in the input. We\nprovide a formal definition of glitches, and use well-known models and datasets\nfrom the literature to demonstrate that they have widespread existence and\nargue they usually indicate potential model inconsistencies in the neighborhood\nof where they are found. We proceed to the algorithmic search of glitches for\nwidely used gradient-boosted decision tree (GBDT) models. We prove that the\nproblem of detecting glitches is NP-complete for tree ensembles, already for\ntrees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP\nencoding of the problem, and its effectiveness and computational feasibility\nare demonstrated on a set of widely used GBDT benchmarks taken from the\nliterature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u6545\u969c\u201d\u7684\u65b0\u4e0d\u53ef\u9760\u884c\u4e3a\u6765\u6e90\uff0c\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u5177\u6709\u9661\u5ced\u51b3\u7b56\u8fb9\u754c\u7684AI\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002\u4f5c\u8005\u901a\u8fc7\u5f62\u5f0f\u5316\u5b9a\u4e49\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u6545\u969c\u7684\u5e7f\u6cdb\u5b58\u5728\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u6a21\u578b\u7684\u6545\u969c\u641c\u7d22\u7b97\u6cd5\u3002", "motivation": "\u8bb8\u591a\u5173\u952e\u51b3\u7b56\u4efb\u52a1\u59d4\u6258\u7ed9\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u9700\u8981\u786e\u4fdd\u5176\u51b3\u7b56\u53ef\u4fe1\u3001\u53ef\u9760\u4e14\u8f93\u51fa\u4e00\u81f4\u3002\u4f5c\u8005\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u4e0d\u53ef\u9760\u884c\u4e3a\u2014\u2014\u6545\u969c\uff0c\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u4f5c\u8005\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u6545\u969c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5e7f\u6cdb\u5b58\u5728\u3002\u9488\u5bf9\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eMILP\u7f16\u7801\u7684\u6545\u969c\u641c\u7d22\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6545\u969c\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6a21\u578b\u4e2d\u666e\u904d\u5b58\u5728\uff0c\u4e14\u901a\u5e38\u6307\u793a\u6a21\u578b\u5728\u6545\u969c\u9644\u8fd1\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u6545\u969c\u68c0\u6d4b\u95ee\u9898\u5bf9\u6df1\u5ea6\u4e3a4\u7684\u6811\u96c6\u6210\u662fNP\u5b8c\u5168\u7684\u3002", "conclusion": "\u6545\u969c\u662f\u5f71\u54cdAI\u6a21\u578b\u53ef\u9760\u6027\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u4f5c\u8005\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u6545\u969c\uff0c\u4e3a\u6a21\u578b\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.15474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15474", "abs": "https://arxiv.org/abs/2507.15474", "authors": ["Charith Premachandra", "Achala Athukorala", "U-Xuan Tan"], "title": "All-UWB SLAM Using UWB Radar and UWB AOA", "comment": null, "summary": "There has been a growing interest in autonomous systems designed to operate\nin adverse conditions (e.g. smoke, dust), where the visible light spectrum\nfails. In this context, Ultra-wideband (UWB) radar is capable of penetrating\nthrough such challenging environmental conditions due to the lower frequency\ncomponents within its broad bandwidth. Therefore, UWB radar has emerged as a\npotential sensing technology for Simultaneous Localization and Mapping (SLAM)\nin vision-denied environments where optical sensors (e.g. LiDAR, Camera) are\nprone to failure. Existing approaches involving UWB radar as the primary\nexteroceptive sensor generally extract features in the environment, which are\nlater initialized as landmarks in a map. However, these methods are constrained\nby the number of distinguishable features in the environment. Hence, this paper\nproposes a novel method incorporating UWB Angle of Arrival (AOA) measurements\ninto UWB radar-based SLAM systems to improve the accuracy and scalability of\nSLAM in feature-deficient environments. The AOA measurements are obtained using\nUWB anchor-tag units which are dynamically deployed by the robot in featureless\nareas during mapping of the environment. This paper thoroughly discusses\nprevailing constraints associated with UWB AOA measurement units and presents\nsolutions to overcome them. Our experimental results show that integrating UWB\nAOA units with UWB radar enables SLAM in vision-denied feature-deficient\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408UWB\u96f7\u8fbe\u548cAOA\u6d4b\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u89c6\u89c9\u53d7\u9650\u4e14\u7279\u5f81\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u63d0\u5347SLAM\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5728\u6076\u52a3\u73af\u5883\uff08\u5982\u70df\u96fe\u3001\u7070\u5c18\uff09\u4e2d\uff0c\u5149\u5b66\u4f20\u611f\u5668\u6613\u5931\u6548\uff0cUWB\u96f7\u8fbe\u56e0\u5176\u7a7f\u900f\u80fd\u529b\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u73af\u5883\u7279\u5f81\u6570\u91cf\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u90e8\u7f72UWB\u951a\u70b9-\u6807\u7b7e\u5355\u5143\u83b7\u53d6AOA\u6d4b\u91cf\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230UWB\u96f7\u8fbeSLAM\u7cfb\u7edf\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408UWB AOA\u5355\u5143\u53ef\u5728\u89c6\u89c9\u53d7\u9650\u4e14\u7279\u5f81\u7a00\u7f3a\u7684\u73af\u5883\u4e2d\u5b9e\u73b0SLAM\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86UWB AOA\u6d4b\u91cf\u5355\u5143\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86SLAM\u5728\u6076\u52a3\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14559", "abs": "https://arxiv.org/abs/2507.14559", "authors": ["Zixuan Hu", "Xiaotong Li", "Shixiang Tang", "Jun Liu", "Yichun Hu", "Ling-Yu Duan"], "title": "LEAD: Exploring Logit Space Evolution for Model Selection", "comment": "Accepted by CVPR 2024", "summary": "The remarkable success of pretrain-then-finetune paradigm has led to a\nproliferation of available pre-trained models for vision tasks. This surge\npresents a significant challenge in efficiently choosing the most suitable\npre-trained models for downstream tasks. The critical aspect of this challenge\nlies in effectively predicting the model transferability by considering the\nunderlying fine-tuning dynamics. Existing methods often model fine-tuning\ndynamics in feature space with linear transformations, which do not precisely\nalign with the fine-tuning objective and fail to grasp the essential\nnonlinearity from optimization. To this end, we present LEAD, a\nfinetuning-aligned approach based on the network output of logits. LEAD\nproposes a theoretical framework to model the optimization process and derives\nan ordinary differential equation (ODE) to depict the nonlinear evolution\ntoward the final logit state. Additionally, we design a class-aware\ndecomposition method to consider the varying evolution dynamics across classes\nand further ensure practical applicability. Integrating the closely aligned\noptimization objective and nonlinear modeling capabilities derived from the\ndifferential equation, our method offers a concise solution to effectively\nbridge the optimization gap in a single step, bypassing the lengthy fine-tuning\nprocess. The comprehensive experiments on 24 supervised and self-supervised\npre-trained models across 10 downstream datasets demonstrate impressive\nperformances and showcase its broad adaptability even in low-data scenarios.", "AI": {"tldr": "LEAD\u662f\u4e00\u79cd\u57fa\u4e8e\u7f51\u7edc\u8f93\u51fa\u7684\u5fae\u8c03\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7ODE\u5efa\u6a21\u975e\u7ebf\u6027\u4f18\u5316\u8fc7\u7a0b\uff0c\u6709\u6548\u9884\u6d4b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u7ebf\u6027\u5efa\u6a21\u5fae\u8c03\u52a8\u6001\uff0c\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u975e\u7ebf\u6027\u7279\u6027\uff0c\u5bfc\u81f4\u9884\u6d4b\u6a21\u578b\u8fc1\u79fb\u6027\u80fd\u4e0d\u51c6\u786e\u3002", "method": "LEAD\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5efa\u6a21\u4f18\u5316\u8fc7\u7a0b\uff0c\u63a8\u5bfcODE\u63cf\u8ff0\u975e\u7ebf\u6027\u6f14\u5316\uff0c\u5e76\u8bbe\u8ba1\u7c7b\u611f\u77e5\u5206\u89e3\u65b9\u6cd5\u8003\u8651\u4e0d\u540c\u7c7b\u522b\u7684\u52a8\u6001\u53d8\u5316\u3002", "result": "\u572824\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u548c10\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLEAD\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u4ecd\u5177\u5e7f\u6cdb\u9002\u5e94\u6027\u3002", "conclusion": "LEAD\u901a\u8fc7\u7d27\u5bc6\u5bf9\u9f50\u4f18\u5316\u76ee\u6807\u548c\u975e\u7ebf\u6027\u5efa\u6a21\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f18\u5316\u5dee\u8ddd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u8fc1\u79fb\u6027\u80fd\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.14503", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14503", "abs": "https://arxiv.org/abs/2507.14503", "authors": ["Jiequan Cui", "Beier Zhu", "Qingshan Xu", "Xiaogang Xu", "Pengguang Chen", "Xiaojuan Qi", "Bei Yu", "Hanwang Zhang", "Richang Hong"], "title": "Generative Distribution Distillation", "comment": "Technique report", "summary": "In this paper, we formulate the knowledge distillation (KD) as a conditional\ngenerative problem and propose the \\textit{Generative Distribution Distillation\n(GenDD)} framework. A naive \\textit{GenDD} baseline encounters two major\nchallenges: the curse of high-dimensional optimization and the lack of semantic\nsupervision from labels. To address these issues, we introduce a \\textit{Split\nTokenization} strategy, achieving stable and effective unsupervised KD.\nAdditionally, we develop the \\textit{Distribution Contraction} technique to\nintegrate label supervision into the reconstruction objective. Our theoretical\nproof demonstrates that \\textit{GenDD} with \\textit{Distribution Contraction}\nserves as a gradient-level surrogate for multi-task learning, realizing\nefficient supervised training without explicit classification loss on\nmulti-step sampling image representations. To evaluate the effectiveness of our\nmethod, we conduct experiments on balanced, imbalanced, and unlabeled data.\nExperimental results show that \\textit{GenDD} performs competitively in the\nunsupervised setting, significantly surpassing KL baseline by \\textbf{16.29\\%}\non ImageNet validation set. With label supervision, our ResNet-50 achieves\n\\textbf{82.28\\%} top-1 accuracy on ImageNet in 600 epochs training,\nestablishing a new state-of-the-art.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u751f\u6210\u7684\u77e5\u8bc6\u84b8\u998f\u6846\u67b6GenDD\uff0c\u901a\u8fc7Split Tokenization\u548cDistribution Contraction\u6280\u672f\u89e3\u51b3\u4e86\u9ad8\u7ef4\u4f18\u5316\u548c\u6807\u7b7e\u8bed\u4e49\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u65e0\u76d1\u7763\u548c\u76d1\u7763\u8bbe\u7f6e\u4e0b\u5747\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u9ad8\u7ef4\u4f18\u5316\u548c\u7f3a\u4e4f\u6807\u7b7e\u8bed\u4e49\u76d1\u7763\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faGenDD\u6846\u67b6\uff0c\u7ed3\u5408Split Tokenization\u7b56\u7565\u5b9e\u73b0\u7a33\u5b9a\u65e0\u76d1\u7763\u84b8\u998f\uff0c\u5e76\u901a\u8fc7Distribution Contraction\u6280\u672f\u6574\u5408\u6807\u7b7e\u76d1\u7763\u3002", "result": "\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cGenDD\u663e\u8457\u8d85\u8d8aKL\u57fa\u7ebf16.29%\uff1b\u5728\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cResNet-50\u5728ImageNet\u4e0a\u8fbe\u523082.28%\u7684top-1\u51c6\u786e\u7387\u3002", "conclusion": "GenDD\u6846\u67b6\u5728\u77e5\u8bc6\u84b8\u998f\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u65e0\u76d1\u7763\u548c\u76d1\u7763\u573a\u666f\u4e0b\u5747\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u4f18\u5f02\u6027\u80fd\u3002"}}
{"id": "2507.15478", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15478", "abs": "https://arxiv.org/abs/2507.15478", "authors": ["Simon Kohaut", "Felix Divo", "Navid Hamid", "Benedict Flade", "Julian Eggert", "Devendra Singh Dhami", "Kristian Kersting"], "title": "The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents", "comment": null, "summary": "Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7684\u6846\u67b6CoCo\uff0c\u901a\u8fc7\u6982\u7387\u903b\u8f91\u7a0b\u5e8f\u548c\u81ea\u6000\u7591\u673a\u5236\u63d0\u5347\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u53ef\u9760\u4e14\u5408\u89c4\u884c\u4e3a\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6982\u7387\u7b26\u53f7\u63a8\u7406\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u5f15\u5165Constitutional Controller\uff08CoCo\uff09\u6846\u67b6\u548c\u81ea\u6000\u7591\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u7a7a\u4e2d\u4ea4\u901a\u7814\u7a76\u4e2d\uff0cCoCo\u80fd\u5b89\u5168\u5bfc\u822a\u590d\u6742\u73af\u5883\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u3002"}}
{"id": "2507.14575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Rav\u00ec"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image\ncontrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering\ndistinct diagnostic insights. However, acquiring all desired modalities\nincreases scan time and cost, motivating research into computational methods\nfor cross-modal synthesis. To address this, recent approaches aim to synthesize\nmissing MRI contrasts from those already acquired, reducing acquisition time\nwhile preserving diagnostic quality. Image-to-image (I2I) translation provides\na promising framework for this task. In this paper, we present a comprehensive\nbenchmark of generative models$\\unicode{x2013}$specifically, Generative\nAdversarial Networks (GANs), diffusion models, and flow matching (FM)\ntechniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All\nframeworks are implemented with comparable settings and evaluated on three\npublicly available MRI datasets of healthy adults. Our quantitative and\nqualitative analyses show that the GAN-based Pix2Pix model outperforms\ndiffusion and FM-based methods in terms of structural fidelity, image quality,\nand computational efficiency. Consistent with existing literature, these\nresults suggest that flow-based models are prone to overfitting on small\ndatasets and simpler tasks, and may require more data to match or surpass GAN\nperformance. These findings offer practical guidance for deploying I2I\ntranslation techniques in real-world MRI workflows and highlight promising\ndirections for future research in cross-modal medical image synthesis. Code and\nmodels are publicly available at\nhttps://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86GAN\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6280\u672f\u5728T1w\u5230T2w MRI\u56fe\u50cf\u8f6c\u6362\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Pix2Pix GAN\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u6700\u4f18\u3002", "motivation": "\u51cf\u5c11MRI\u626b\u63cf\u65f6\u95f4\u548c\u6210\u672c\uff0c\u901a\u8fc7\u8ba1\u7b97\u5408\u6210\u7f3a\u5931\u7684\u6a21\u6001\u3002", "method": "\u4f7f\u7528GAN\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6280\u672f\u8fdb\u884cT1w\u5230T2w\u76842D MRI\u56fe\u50cf\u8f6c\u6362\uff0c\u5e76\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "Pix2Pix GAN\u5728\u7ed3\u6784\u4fdd\u771f\u5ea6\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "GAN\u66f4\u9002\u5408\u5c0f\u6570\u636e\u96c6\u548c\u7b80\u5355\u4efb\u52a1\uff0c\u6d41\u5339\u914d\u6a21\u578b\u9700\u66f4\u591a\u6570\u636e\u3002\u7814\u7a76\u4e3a\u5b9e\u9645MRI\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.14516", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14516", "abs": "https://arxiv.org/abs/2507.14516", "authors": ["Jeyoung Lee", "Hochul Kang"], "title": "SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning", "comment": null, "summary": "We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware\nmetric function for time series self-supervised representation learning. Most\nSelf-Supervised Learning (SSL) methods for signals commonly adopt\ndistance-based objectives such as mean squared error (MSE), which are sensitive\nto amplitude, invariant to waveform polarity, and unbounded in scale. These\nproperties hinder semantic alignment and reduce interpretability. SDSC\naddresses this by quantifying structural agreement between temporal signals\nbased on the intersection of signed amplitudes, derived from the Dice\nSimilarity Coefficient (DSC).Although SDSC is defined as a structure-aware\nmetric, it can be used as a loss by subtracting from 1 and applying a\ndifferentiable approximation of the Heaviside function for gradient-based\noptimization. A hybrid loss formulation is also proposed to combine SDSC with\nMSE, improving stability and preserving amplitude where necessary. Experiments\non forecasting and classification benchmarks demonstrate that SDSC-based\npre-training achieves comparable or improved performance over MSE, particularly\nin in-domain and low-resource scenarios. The results suggest that structural\nfidelity in signal representations enhances the semantic representation\nquality, supporting the consideration of structure-aware metrics as viable\nalternatives to conventional distance-based methods.", "AI": {"tldr": "\u63d0\u51faSDSC\uff0c\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u65f6\u5e8f\u4fe1\u53f7\u81ea\u76d1\u7763\u5b66\u4e60\u5ea6\u91cf\u51fd\u6570\uff0c\u4f18\u4e8e\u4f20\u7edfMSE\u3002", "motivation": "\u4f20\u7edfMSE\u5bf9\u632f\u5e45\u654f\u611f\u3001\u5bf9\u6ce2\u5f62\u6781\u6027\u4e0d\u53d8\u4e14\u5c3a\u5ea6\u65e0\u754c\uff0c\u963b\u788d\u8bed\u4e49\u5bf9\u9f50\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u57fa\u4e8eDice\u76f8\u4f3c\u7cfb\u6570\uff0c\u91cf\u5316\u65f6\u5e8f\u4fe1\u53f7\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u8bbe\u8ba1\u53ef\u5fae\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0eMSE\u76f8\u5f53\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u5ea6\u91cf\u80fd\u63d0\u5347\u8bed\u4e49\u8868\u793a\u8d28\u91cf\uff0c\u662f\u4f20\u7edf\u8ddd\u79bb\u65b9\u6cd5\u7684\u53ef\u884c\u66ff\u4ee3\u3002"}}
{"id": "2507.15484", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15484", "abs": "https://arxiv.org/abs/2507.15484", "authors": ["Jamie Bell"], "title": "Robots for Kiwifruit Harvesting and Pollination", "comment": null, "summary": "This research was a part of a project that developed mobile robots that\nperformed targeted pollen spraying and automated harvesting in pergola\nstructured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were\ndesigned and field testing of one of the concepts showed that the mechanism\ncould reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism\nwas able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,\nwhereas the previous state of the art mechanism was only able to reach less\nthan 70 percent of the fruit. Artificial pollination was performed by detecting\nflowers and then spraying pollen in solution onto the detected flowers from a\nline of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the\nheight of the canopy was measured and the spray boom was moved up and down to\nkeep the boom close enough to the flowers for the spray to reach the flowers,\nwhile minimising collisions with the canopy. Mobile robot navigation was\nperformed using a 2D lidar in apple orchards and vineyards. Lidar navigation in\nkiwifruit orchards was more challenging because the pergola structure only\nprovides a small amount of data for the direction of rows, compared to the\namount of data from the overhead canopy, the undulating ground and other\nobjects in the orchards. Multiple methods are presented here for extracting\nstructure defining features from 3D lidar data in kiwifruit orchards. In\naddition, a 3D lidar navigation system -- which performed row following, row\nend detection and row end turns -- was tested for over 30 km of autonomous\ndriving in kiwifruit orchards. Computer vision algorithms for row detection and\nrow following were also tested. The computer vision algorithm worked as well as\nthe 3D lidar row following method in testing.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u7528\u4e8e\u7315\u7334\u6843\u56ed\u7684\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u82b1\u7c89\u55b7\u6d12\u548c\u81ea\u52a8\u5316\u91c7\u6458\uff0c\u6539\u8fdb\u4e86\u679c\u5b9e\u91c7\u6458\u673a\u5236\u548c\u5bfc\u822a\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u7315\u7334\u6843\u56ed\u4e2d\u81ea\u52a8\u5316\u91c7\u6458\u548c\u82b1\u7c89\u55b7\u6d12\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u79cd\u679c\u5b9e\u91c7\u6458\u673a\u5236\uff0c\u4f7f\u75282D\u548c3D\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u5bfc\u822a\uff0c\u5f00\u53d1\u4e86\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u8fdb\u884c\u884c\u68c0\u6d4b\u548c\u8ddf\u968f\u3002", "result": "\u91c7\u6458\u673a\u5236\u8986\u76d680%\u679c\u5b9e\uff0c\u4f18\u4e8e\u4e4b\u524d\u768470%\uff1b\u82b1\u7c89\u55b7\u6d12\u548c\u5bfc\u822a\u7cfb\u7edf\u572830\u516c\u91cc\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u79fb\u52a8\u673a\u5668\u4eba\u548c\u5bfc\u822a\u7cfb\u7edf\u5728\u7315\u7334\u6843\u56ed\u4e2d\u6709\u6548\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e3D\u6fc0\u5149\u96f7\u8fbe\u6027\u80fd\u76f8\u5f53\u3002"}}
{"id": "2507.14587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14587", "abs": "https://arxiv.org/abs/2507.14587", "authors": ["Merjem Be\u0107irovi\u0107", "Amina Kurtovi\u0107", "Nordin Smajlovi\u0107", "Medina Kapo", "Amila Akagi\u0107"], "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX", "comment": null, "summary": "Medical imaging plays a vital role in early disease diagnosis and monitoring.\nSpecifically, blood microscopy offers valuable insights into blood cell\nmorphology and the detection of hematological disorders. In recent years, deep\nlearning-based automated classification systems have demonstrated high\npotential in enhancing the accuracy and efficiency of blood image analysis.\nHowever, a detailed performance analysis of specific deep learning frameworks\nappears to be lacking. This paper compares the performance of three popular\ndeep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in\nclassifying blood cell images from the publicly available BloodMNIST dataset.\nThe study primarily focuses on inference time differences, but also\nclassification performance for different image sizes. The results reveal\nvariations in performance across frameworks, influenced by factors such as\nimage resolution and framework-specific optimizations. Classification accuracy\nfor JAX and PyTorch was comparable to current benchmarks, showcasing the\nefficiency of these frameworks for medical image classification.", "AI": {"tldr": "\u6bd4\u8f83\u4e86TensorFlow\u3001PyTorch\u548cJAX\u5728\u8840\u6db2\u7ec6\u80de\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0JAX\u548cPyTorch\u8868\u73b0\u63a5\u8fd1\u5f53\u524d\u57fa\u51c6\u3002", "motivation": "\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u8840\u6db2\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u8be6\u7ec6\u6027\u80fd\u5206\u6790\u3002", "method": "\u4f7f\u7528BloodMNIST\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e09\u79cd\u6846\u67b6\u5728\u63a8\u7406\u65f6\u95f4\u548c\u5206\u7c7b\u6027\u80fd\u4e0a\u7684\u5dee\u5f02\u3002", "result": "JAX\u548cPyTorch\u7684\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1\u5f53\u524d\u57fa\u51c6\uff0c\u6027\u80fd\u53d7\u56fe\u50cf\u5206\u8fa8\u7387\u548c\u6846\u67b6\u4f18\u5316\u5f71\u54cd\u3002", "conclusion": "JAX\u548cPyTorch\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.14528", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14528", "abs": "https://arxiv.org/abs/2507.14528", "authors": ["Ilias Tsoumas", "Dimitrios Bormpoudakis", "Vasileios Sitokonstantinou", "Athanasios Askitopoulos", "Andreas Kalogeras", "Charalampos Kontoes", "Ioannis Athanasiadis"], "title": "Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference", "comment": "Accepted at KDD 2025 Workshop on Causal Inference and Machine\n  Learning in Practice", "summary": "In causal inference, whether through randomized controlled trials or\nobservational studies, access to both treated and control units is essential\nfor estimating the effect of a treatment on an outcome of interest. When\ntreatment assignment is random, the average treatment effect (ATE) can be\nestimated directly by comparing outcomes between groups. In non-randomized\nsettings, various techniques are employed to adjust for confounding and\napproximate the counterfactual scenario to recover an unbiased ATE. A common\nchallenge, especially in observational studies, is the absence of units clearly\nlabeled as controls-that is, units known not to have received the treatment. To\naddress this, we propose positive-unlabeled (PU) learning as a framework for\nidentifying, with high confidence, control units from a pool of unlabeled ones,\nusing only the available treated (positive) units. We evaluate this approach\nusing both simulated and real-world data. We construct a causal graph with\ndiverse relationships and use it to generate synthetic data under various\nscenarios, assessing how reliably the method recovers control groups that allow\nestimates of true ATE. We also apply our approach to real-world data on optimal\nsowing and fertilizer treatments in sustainable agriculture. Our findings show\nthat PU learning can successfully identify control (negative) units from\nunlabeled data based only on treated units and, through the resulting control\ngroup, estimate an ATE that closely approximates the true value. This work has\nimportant implications for observational causal inference, especially in fields\nwhere randomized experiments are difficult or costly. In domains such as earth,\nenvironmental, and agricultural sciences, it enables a plethora of\nquasi-experiments by leveraging available earth observation and climate data,\nparticularly when treated units are available but control units are lacking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u672a\u6807\u8bb0\uff08PU\uff09\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\u8bc6\u522b\u63a7\u5236\u7ec4\uff0c\u4ece\u800c\u4f30\u8ba1\u5e73\u5747\u5904\u7406\u6548\u5e94\uff08ATE\uff09\u3002", "motivation": "\u5728\u89c2\u5bdf\u6027\u7814\u7a76\u4e2d\uff0c\u7f3a\u4e4f\u660e\u786e\u6807\u8bb0\u7684\u63a7\u5236\u7ec4\u662f\u4e00\u4e2a\u5e38\u89c1\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u968f\u673a\u5b9e\u9a8c\u96be\u4ee5\u5b9e\u65bd\u6216\u6210\u672c\u9ad8\u6602\u7684\u9886\u57df\u3002", "method": "\u4f7f\u7528PU\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u57fa\u4e8e\u5df2\u5904\u7406\u7684\uff08\u6b63\uff09\u6837\u672c\u4ece\u672a\u6807\u8bb0\u6570\u636e\u4e2d\u9ad8\u7f6e\u4fe1\u5ea6\u5730\u8bc6\u522b\u63a7\u5236\u7ec4\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u63a7\u5236\u7ec4\u5e76\u4f30\u8ba1\u63a5\u8fd1\u771f\u5b9e\u503c\u7684ATE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89c2\u5bdf\u6027\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5730\u7403\u3001\u73af\u5883\u548c\u519c\u4e1a\u79d1\u5b66\u7b49\u9886\u57df\u3002"}}
{"id": "2507.15493", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15493", "abs": "https://arxiv.org/abs/2507.15493", "authors": ["Chilam Cheang", "Sijin Chen", "Zhongren Cui", "Yingdong Hu", "Liqun Huang", "Tao Kong", "Hang Li", "Yifeng Li", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Wenxuan Ou", "Wanli Peng", "Zeyu Ren", "Haixin Shi", "Jiawen Tian", "Hongtao Wu", "Xin Xiao", "Yuyang Xiao", "Jiafeng Xu", "Yichu Yang"], "title": "GR-3 Technical Report", "comment": "Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/", "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "AI": {"tldr": "GR-3\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u6cdb\u5316\u5230\u65b0\u5bf9\u8c61\u3001\u73af\u5883\u548c\u62bd\u8c61\u6982\u5ff5\u6307\u4ee4\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5e76\u80fd\u9ad8\u6548\u5fae\u8c03\u4ee5\u9002\u5e94\u65b0\u573a\u666f\u3002", "motivation": "\u5f00\u53d1\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\uff0c\u5e2e\u52a9\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u3002", "method": "\u901a\u8fc7\u591a\u65b9\u9762\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5305\u62ec\u4e0e\u7f51\u7edc\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u7684\u5171\u540c\u8bad\u7ec3\u3001\u4eceVR\u8bbe\u5907\u6536\u96c6\u7684\u4eba\u7c7b\u8f68\u8ff9\u6570\u636e\u7684\u9ad8\u6548\u5fae\u8c03\uff0c\u4ee5\u53ca\u673a\u5668\u4eba\u8f68\u8ff9\u6570\u636e\u7684\u6709\u6548\u6a21\u4eff\u5b66\u4e60\u3002", "result": "GR-3\u5728\u591a\u79cd\u6311\u6218\u6027\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u03c0\u2080\u3002", "conclusion": "GR-3\u662f\u6784\u5efa\u80fd\u591f\u534f\u52a9\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u7684\u901a\u7528\u673a\u5668\u4eba\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Lo\u00efc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D\u662f\u4e00\u79cd\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f00\u653e\u8bcd\u6c47\u6307\u5bfc\u76843D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u65b9\u6cd5\uff0c\u5728\u573a\u666f\u548c\u7528\u6237\u67e5\u8be2\u9002\u5e94\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u76ee\u6807\u6216\u573a\u666f\u5185\u5bb9\uff0cDiSCO-3D\u65e8\u5728\u89e3\u51b3\u66f4\u5e7f\u6cdb\u76843D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u795e\u7ecf\u573a\u8868\u793a\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5206\u5272\u548c\u5f31\u5f00\u653e\u8bcd\u6c47\u6307\u5bfc\u3002", "result": "\u5728\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u5f00\u653e\u8bcd\u6c47\u548c\u65e0\u76d1\u7763\u5206\u5272\u7684\u8fb9\u7f18\u6848\u4f8b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DiSCO-3D\u662f\u9996\u4e2a\u89e3\u51b33D\u5f00\u653e\u8bcd\u6c47\u5b50\u6982\u5ff5\u53d1\u73b0\u7684\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u9002\u5e94\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.14529", "categories": ["cs.LG", "math.OC", "91A16, 68T05, 49N45, 93E20, 46E22"], "pdf": "https://arxiv.org/pdf/2507.14529", "abs": "https://arxiv.org/abs/2507.14529", "authors": ["Berkay Anahtarci", "Can Deha Kariksiz", "Naci Saldi"], "title": "Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games", "comment": null, "summary": "We consider the maximum causal entropy inverse reinforcement learning problem\nfor infinite-horizon stationary mean-field games, in which we model the unknown\nreward function within a reproducing kernel Hilbert space. This allows the\ninference of rich and potentially nonlinear reward structures directly from\nexpert demonstrations, in contrast to most existing inverse reinforcement\nlearning approaches for mean-field games that typically restrict the reward\nfunction to a linear combination of a fixed finite set of basis functions. We\nalso focus on the infinite-horizon cost structure, whereas prior studies\nprimarily rely on finite-horizon formulations. We introduce a Lagrangian\nrelaxation to this maximum causal entropy inverse reinforcement learning\nproblem that enables us to reformulate it as an unconstrained log-likelihood\nmaximization problem, and obtain a solution \\lk{via} a gradient ascent\nalgorithm. To illustrate the theoretical consistency of the algorithm, we\nestablish the smoothness of the log-likelihood objective by proving the\nFr\\'echet differentiability of the related soft Bellman operators with respect\nto the parameters in the reproducing kernel Hilbert space. We demonstrate the\neffectiveness of our method on a mean-field traffic routing game, where it\naccurately recovers expert behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u56e0\u679c\u71b5\u7684\u9006\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u9650\u65f6\u57df\u5e73\u7a33\u5e73\u5747\u573a\u535a\u5f08\uff0c\u901a\u8fc7\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5efa\u6a21\u672a\u77e5\u5956\u52b1\u51fd\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9650\u5236\u5956\u52b1\u51fd\u6570\u4e3a\u56fa\u5b9a\u57fa\u51fd\u6570\u7684\u7ebf\u6027\u7ec4\u5408\uff0c\u4e14\u591a\u57fa\u4e8e\u6709\u9650\u65f6\u57df\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5f15\u5165\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u65e0\u7ea6\u675f\u5bf9\u6570\u4f3c\u7136\u6700\u5927\u5316\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u7b97\u6cd5\u6c42\u89e3\u3002", "result": "\u8bc1\u660e\u4e86\u7b97\u6cd5\u7684\u7406\u8bba\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5e73\u5747\u573a\u4ea4\u901a\u8def\u7531\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u6062\u590d\u4e13\u5bb6\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u975e\u7ebf\u6027\u5956\u52b1\u7ed3\u6784\u7684\u63a8\u65ad\u3002"}}
{"id": "2507.15499", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15499", "abs": "https://arxiv.org/abs/2507.15499", "authors": ["Jongseok Lee", "Timo Birr", "Rudolph Triebel", "Tamim Asfour"], "title": "CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions", "comment": "8 pages. Accepted to IEEE RAL", "summary": "We propose CLEVER, an active learning system for robust semantic perception\nwith Deep Neural Networks (DNNs). For data arriving in streams, our system\nseeks human support when encountering failures and adapts DNNs online based on\nhuman instructions. In this way, CLEVER can eventually accomplish the given\nsemantic perception tasks. Our main contribution is the design of a system that\nmeets several desiderata of realizing the aforementioned capabilities. The key\nenabler herein is our Bayesian formulation that encodes domain knowledge\nthrough priors. Empirically, we not only motivate CLEVER's design but further\ndemonstrate its capabilities with a user validation study as well as\nexperiments on humanoid and deformable objects. To our knowledge, we are the\nfirst to realize stream-based active learning on a real robot, providing\nevidence that the robustness of the DNN-based semantic perception can be\nimproved in practice. The project website can be accessed at\nhttps://sites.google.com/view/thecleversystem.", "AI": {"tldr": "CLEVER\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u9002\u5e94\u4eba\u7c7b\u6307\u4ee4\u63d0\u5347\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u6d41\u4e2d\u8bed\u4e49\u611f\u77e5\u4efb\u52a1\u7684\u5931\u8d25\u95ee\u9898\uff0c\u901a\u8fc7\u4eba\u7c7b\u652f\u6301\u63d0\u5347DNN\u7684\u9002\u5e94\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7cfb\u7edf\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u516c\u5f0f\u7f16\u7801\u9886\u57df\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u9a8c\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u80fd\u529b\u3002", "result": "\u9996\u6b21\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u57fa\u4e8e\u6d41\u7684\u4e3b\u52a8\u5b66\u4e60\uff0c\u63d0\u5347\u4e86DNN\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CLEVER\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u6307\u4ee4\u548c\u5728\u7ebf\u9002\u5e94\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.14608", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14608", "abs": "https://arxiv.org/abs/2507.14608", "authors": ["Nandani Sharma", "Dinesh Singh"], "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition", "comment": null, "summary": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.", "AI": {"tldr": "Exp-Graph\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u5efa\u6a21\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9Transformer\u548c\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u5728\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u90e8\u5c5e\u6027\u7684\u7ed3\u6784\u53d8\u5316\u589e\u52a0\u4e86\u8bc6\u522b\u96be\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u7ed3\u5408\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u9762\u90e8\u5173\u952e\u70b9\u4f5c\u4e3a\u56fe\u7684\u9876\u70b9\uff0c\u57fa\u4e8e\u90bb\u8fd1\u6027\u548c\u5c40\u90e8\u5916\u89c2\u76f8\u4f3c\u6027\u786e\u5b9a\u8fb9\uff0c\u7ed3\u5408\u89c6\u89c9Transformer\u548c\u56fe\u5377\u79ef\u7f51\u7edc\u6355\u6349\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523098.09%\u300179.01%\u548c56.39%\u7684\u51c6\u786e\u7387\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Exp-Graph\u5728\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.14560", "categories": ["cs.LG", "cs.CV", "68T07, 05C50, 15A18", "I.2.6; I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.14560", "abs": "https://arxiv.org/abs/2507.14560", "authors": ["Giorgio Roffo"], "title": "The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers", "comment": "24 pages, 10 figures, submitted for review. Companion code and\n  reproducibility materials available", "summary": "The self-attention mechanism, now central to deep learning architectures such\nas Transformers, is a modern instance of a more general computational\nprinciple: learning and using pairwise affinity matrices to control how\ninformation flows through a model. This paper traces the conceptual origins of\nself-attention across multiple domains, including computer vision, natural\nlanguage processing, and graph learning, through their shared reliance on an\naffinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)\nas a foundational approach that generalizes the idea of affinity-based\nweighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS\ndefines A either through domain knowledge or by learning, and computes feature\nrelevance through multi-hop propagation over the affinity graph. From this\nperspective, self-attention can be seen as a special case of Inf-FS: it uses a\nsingle-hop affinity computation where A is dynamically built from token\nsimilarities. We argue that the underlying structure, reasoning over pairwise\nrelationships, is preserved across both approaches, and the key differences lie\nin how the affinity matrix is defined and applied. By situating self-attention\nwithin the broader paradigm of affinity-based computation, we unify several\nstrands of machine learning research and highlight a common mathematical\nfoundation that underpins diverse models and tasks.", "AI": {"tldr": "\u8bba\u6587\u5c06\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89c6\u4e3a\u66f4\u5e7f\u6cdb\u7684\u57fa\u4e8e\u4eb2\u548c\u529b\u77e9\u9635\u7684\u8ba1\u7b97\u539f\u5219\u7684\u7279\u4f8b\uff0c\u901a\u8fc7\u65e0\u9650\u7279\u5f81\u9009\u62e9\uff08Inf-FS\uff09\u7edf\u4e00\u4e86\u591a\u4e2a\u9886\u57df\u7684\u6a21\u578b\u3002", "motivation": "\u63a2\u8ba8\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u4e0d\u540c\u9886\u57df\u4e2d\u7684\u5171\u540c\u6570\u5b66\u57fa\u7840\uff0c\u63ed\u793a\u5176\u4e0eInf-FS\u7684\u5173\u8054\u3002", "method": "\u901a\u8fc7\u5206\u6790\u81ea\u6ce8\u610f\u529b\u4e0eInf-FS\u7684\u4eb2\u548c\u529b\u77e9\u9635\u5b9a\u4e49\u548c\u5e94\u7528\u65b9\u5f0f\uff0c\u6bd4\u8f83\u4e24\u8005\u7684\u5f02\u540c\u3002", "result": "\u81ea\u6ce8\u610f\u529b\u662fInf-FS\u7684\u5355\u8df3\u7279\u4f8b\uff0c\u4e24\u8005\u5171\u4eab\u57fa\u4e8e\u6210\u5bf9\u5173\u7cfb\u7684\u8ba1\u7b97\u7ed3\u6784\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u4eb2\u548c\u529b\u77e9\u9635\u7684\u7edf\u4e00\u89c6\u89d2\uff0c\u8fde\u63a5\u4e86\u4e0d\u540c\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002"}}
{"id": "2507.15604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15604", "abs": "https://arxiv.org/abs/2507.15604", "authors": ["Johannes Hartwig", "Philipp Lienhardt", "Dominik Henrich"], "title": "Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2025 (to appear)", "summary": "As the availability of cobots increases, it is essential to address the needs\nof users with little to no programming knowledge to operate such systems\nefficiently. Programming concepts often use intuitive interaction modalities,\nsuch as hand guiding, to address this. When programming in-contact motions,\nsuch frameworks require knowledge of the robot tool's payload inertial\nparameters (PIP) in addition to the demonstrated velocities and forces to\nensure effective hybrid motion-force control. This paper aims to enable\nnon-expert users to program in-contact motions more efficiently by eliminating\nthe need for a dedicated PIP calibration, thereby enabling flexible robot tool\nchanges. Since demonstrated tasks generally also contain motions with\nnon-contact, our approach uses these parts to estimate the robot's PIP using\nestablished estimation techniques. The results show that the estimation of the\npayload's mass is accurate, whereas the center of mass and the inertia tensor\nare affected by noise and a lack of excitation. Overall, these findings show\nthe feasibility of PIP estimation during hand guiding but also highlight the\nneed for sufficient payload accelerations for an accurate estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4e13\u7528\u8d1f\u8f7d\u60ef\u6027\u53c2\u6570\uff08PIP\uff09\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\u4f30\u8ba1PIP\uff0c\u4f7f\u975e\u4e13\u5bb6\u7528\u6237\u80fd\u66f4\u9ad8\u6548\u5730\u7f16\u7a0b\u63a5\u89e6\u8fd0\u52a8\u3002", "motivation": "\u968f\u7740\u534f\u4f5c\u673a\u5668\u4eba\uff08cobot\uff09\u7684\u666e\u53ca\uff0c\u9700\u8981\u8ba9\u975e\u7f16\u7a0b\u4e13\u5bb6\u4e5f\u80fd\u9ad8\u6548\u64cd\u4f5c\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56PIP\u6821\u51c6\uff0c\u9650\u5236\u4e86\u5de5\u5177\u7684\u7075\u6d3b\u6027\u3002", "method": "\u5229\u7528\u4efb\u52a1\u4e2d\u7684\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\uff0c\u901a\u8fc7\u5df2\u6709\u4f30\u8ba1\u6280\u672f\u8ba1\u7b97PIP\uff0c\u907f\u514d\u4e13\u7528\u6821\u51c6\u3002", "result": "\u8d28\u91cf\u4f30\u8ba1\u51c6\u786e\uff0c\u4f46\u8d28\u5fc3\u548c\u60ef\u6027\u5f20\u91cf\u53d7\u566a\u58f0\u548c\u6fc0\u52b1\u4e0d\u8db3\u5f71\u54cd\u3002", "conclusion": "\u65b9\u6cd5\u53ef\u884c\uff0c\u4f46\u9700\u8db3\u591f\u8d1f\u8f7d\u52a0\u901f\u5ea6\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2507.14613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14613", "abs": "https://arxiv.org/abs/2507.14613", "authors": ["Guoping Xu", "Christopher Kabat", "You Zhang"], "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2", "comment": "24 pages, 6 figures", "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.", "AI": {"tldr": "DD-SAM2\u662f\u4e00\u79cd\u9ad8\u6548\u9002\u914dSAM2\u7684\u6846\u67b6\uff0c\u901a\u8fc7Depthwise-Dilated Adapter\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u89c6\u9891\u5206\u5272\u4e0e\u8ddf\u8e2a\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u591a\u4e3a\u6a21\u6001\u7279\u5b9a\u8bbe\u8ba1\uff0c\u9002\u5e94\u6027\u5dee\uff0c\u4e14SAM2\u5728\u533b\u5b66\u89c6\u9891\u573a\u666f\u4e2d\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\u91cd\u65b0\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51faDD-SAM2\u6846\u67b6\uff0c\u5f15\u5165DD-Adapter\uff0c\u4ee5\u6700\u5c0f\u53c2\u6570\u91cf\u589e\u5f3a\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u652f\u6301\u6709\u9650\u6570\u636e\u7684\u5fae\u8c03\u3002", "result": "\u5728TrackRad2025\u548cEchoNet-Dynamic\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cDice\u5206\u6570\u5206\u522b\u8fbe\u52300.93\u548c0.97\u3002", "conclusion": "DD-SAM2\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u57fa\u4e8e\u9002\u914d\u5668\u7684SAM2\u5fae\u8c03\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u89c6\u9891\u5206\u5272\u4e0e\u8ddf\u8e2a\u3002"}}
{"id": "2507.14570", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14570", "abs": "https://arxiv.org/abs/2507.14570", "authors": ["Xu Cheng", "Liang Yao", "Feng He", "Yukuo Cen", "Yufei He", "Chenhui Zhang", "Wenzheng Feng", "Hongyun Cai", "Jie Tang"], "title": "LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges", "comment": null, "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for various graph\nmining tasks, yet existing scalable solutions often struggle to balance\nexecution efficiency with prediction accuracy. These difficulties stem from\niterative message-passing techniques, which place significant computational\ndemands and require extensive GPU memory, particularly when dealing with the\nneighbor explosion issue inherent in large-scale graphs. This paper introduces\na scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,\nwhich can perform representation learning on 100 billion graphs with a single\nGPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We\nexamine existing graph partitioning methods and design a superior graph\npartition algorithm named LPMetis. In particular, LPMetis outperforms current\nstate-of-the-art (SOTA) approaches on various evaluation metrics. In addition,\nour paper proposes a subgraph augmentation strategy to enhance the model's\npredictive performance. It exhibits excellent compatibility, allowing the\nentire framework to accommodate various GNN algorithms. Successfully deployed\non the Tencent platform, LPS-GNN has been tested on public and real-world\ndatasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in\nonline applications.", "AI": {"tldr": "LPS-GNN\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684GNN\u6846\u67b6\uff0c\u80fd\u5728\u5355GPU\u4e0a\u5904\u74061000\u4ebf\u89c4\u6a21\u7684\u56fe\u6570\u636e\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u73b0\u6709GNN\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u5c24\u5176\u662f\u5927\u89c4\u6a21\u56fe\u6570\u636e\u4e0b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u95ee\u9898\u3002", "method": "\u63d0\u51faLPS-GNN\u6846\u67b6\uff0c\u7ed3\u5408LPMetis\u56fe\u5206\u533a\u7b97\u6cd5\u548c\u5b50\u56fe\u589e\u5f3a\u7b56\u7565\uff0c\u517c\u5bb9\u591a\u79cdGNN\u7b97\u6cd5\u3002", "result": "\u5728\u817e\u8baf\u5e73\u53f0\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u63d0\u53478.24%\u81f313.89%\uff0c\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\u3002", "conclusion": "LPS-GNN\u4e3a\u5927\u89c4\u6a21\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15607", "abs": "https://arxiv.org/abs/2507.15607", "authors": ["Yanbo Chen", "Yunzhe Tan", "Yaojia Wang", "Zhengzhe Xu", "Junbo Tan", "Xueqian Wang"], "title": "A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning", "comment": "8 pages, 10 figures", "summary": "Autonomous navigation of vehicle-trailer systems is crucial in environments\nlike airports, supermarkets, and concert venues, where various types of\ntrailers are needed to navigate with different payloads and conditions.\nHowever, accurately modeling such systems remains challenging, especially for\ntrailers with castor wheels. In this work, we propose a novel universal\nvehicle-trailer navigation system that integrates a hybrid nominal kinematic\nmodel--combining classical nonholonomic constraints for vehicles and neural\nnetwork-based trailer kinematics--with a lightweight online residual learning\nmodule to correct real-time modeling discrepancies and disturbances.\nAdditionally, we develop a model predictive control framework with a weighted\nmodel combination strategy that improves long-horizon prediction accuracy and\nensures safer motion planning. Our approach is validated through extensive\nreal-world experiments involving multiple trailer types and varying payload\nconditions, demonstrating robust performance without manual tuning or\ntrailer-specific calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u901a\u7528\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df7\u5408\u8fd0\u52a8\u5b66\u6a21\u578b\u548c\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u8f66\u8f86-\u62d6\u8f66\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5efa\u6a21\u548c\u5bfc\u822a\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5e26\u811a\u8f6e\u7684\u62d6\u8f66\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u975e\u5b8c\u6574\u7ea6\u675f\u548c\u795e\u7ecf\u7f51\u7edc\u62d6\u8f66\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\uff1b\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u62d6\u8f66\u548c\u8d1f\u8f7d\u6761\u4ef6\u7684\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8868\u73b0\u7a33\u5065\u4e14\u65e0\u9700\u624b\u52a8\u8c03\u6574\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "AI": {"tldr": "BusterX++ \u662f\u4e00\u79cd\u65b0\u578b\u8de8\u6a21\u6001\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5f15\u5165 GenBuster++ \u57fa\u51c6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "motivation": "\u751f\u6210\u5f0f AI \u7684\u8fdb\u6b65\u589e\u52a0\u4e86\u865a\u5047\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u73b0\u6709\u5355\u6a21\u6001\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u591a\u6a21\u6001\u5408\u6210\u5185\u5bb9\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u7b56\u7565\uff08\u591a\u9636\u6bb5\u8bad\u7ec3\u3001\u601d\u7ef4\u5956\u52b1\u3001\u6df7\u5408\u63a8\u7406\uff09\u6d88\u9664\u51b7\u542f\u52a8\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e BusterX++ \u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "BusterX++ \u4e3a\u8de8\u6a21\u6001\u5408\u6210\u5a92\u4f53\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14592", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14592", "abs": "https://arxiv.org/abs/2507.14592", "authors": ["Haochen Liu", "Jia Bi", "Xiaomin Wang", "Xin Yang", "Ling Wang"], "title": "A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification", "comment": "13 pages, 7 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,\nlogistics, agriculture, disaster management, and military operations. Accurate\ndetection and classification of UAV flight states, such as hovering, cruising,\nascending, or transitioning, which are essential for safe and effective\noperations. However, conventional time series classification (TSC) methods\noften lack robustness and generalization for dynamic UAV environments, while\nstate of the art(SOTA) models like Transformers and LSTM based architectures\ntypically require large datasets and entail high computational costs,\nespecially with high-dimensional data streams. This paper proposes a novel\nframework that integrates a Transformer-based Generative Adversarial Network\n(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address\nthese challenges in UAV flight state classification. The Transformer encoder\ncaptures long-range temporal dependencies and complex telemetry dynamics, while\nthe GAN module augments limited datasets with realistic synthetic samples. MIL\nis incorporated to focus attention on the most discriminative input segments,\nreducing noise and computational overhead. Experimental results show that the\nproposed method achieves superior accuracy 96.5% on the DroneDetect dataset and\n98.6% on the DroneRF dataset that outperforming other SOTA approaches. The\nframework also demonstrates strong computational efficiency and robust\ngeneralization across diverse UAV platforms and flight states, highlighting its\npotential for real-time deployment in resource constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer-GAN\u548cMILET\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u98de\u884c\u72b6\u6001\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5\u5728\u52a8\u6001\u65e0\u4eba\u673a\u73af\u5883\u4e2d\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u73b0\u6709SOTA\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "method": "\u96c6\u6210\u4e86Transformer\u7f16\u7801\u5668\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\uff0cGAN\u6a21\u5757\u751f\u6210\u5408\u6210\u6570\u636e\u589e\u5f3a\u6570\u636e\u96c6\uff0cMILET\u805a\u7126\u5173\u952e\u8f93\u5165\u6bb5\u3002", "result": "\u5728DroneDetect\u548cDroneRF\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523096.5%\u548c98.6%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6SOTA\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2507.15608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15608", "abs": "https://arxiv.org/abs/2507.15608", "authors": ["Johannes Hartwig", "Fabian Viessmann", "Dominik Henrich"], "title": "Optimizing Force Signals from Human Demonstrations of In-Contact Motions", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2024 (to appear)", "summary": "For non-robot-programming experts, kinesthetic guiding can be an intuitive\ninput method, as robot programming of in-contact tasks is becoming more\nprominent. However, imprecise and noisy input signals from human demonstrations\npose problems when reproducing motions directly or using the signal as input\nfor machine learning methods. This paper explores optimizing force signals to\ncorrespond better to the human intention of the demonstrated signal. We compare\ndifferent signal filtering methods and propose a peak detection method for\ndealing with first-contact deviations in the signal. The evaluation of these\nmethods considers a specialized error criterion between the input and the\nhuman-intended signal. In addition, we analyze the critical parameters'\ninfluence on the filtering methods. The quality for an individual motion could\nbe increased by up to \\SI{20}{\\percent} concerning the error criterion. The\nproposed contribution can improve the usability of robot programming and the\ninteraction between humans and robots.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u4f18\u5316\u529b\u4fe1\u53f7\u4ee5\u66f4\u597d\u5730\u5339\u914d\u4eba\u7c7b\u610f\u56fe\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u4fe1\u53f7\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7f16\u7a0b\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u975e\u673a\u5668\u4eba\u7f16\u7a0b\u4e13\u5bb6\u901a\u8fc7\u76f4\u89c2\u7684\u8f93\u5165\u65b9\u6cd5\uff08\u5982\u529b\u5f15\u5bfc\uff09\u8fdb\u884c\u7f16\u7a0b\u65f6\uff0c\u4fe1\u53f7\u4e0d\u7cbe\u786e\u548c\u566a\u58f0\u4f1a\u5f71\u54cd\u8fd0\u52a8\u518d\u73b0\u6216\u673a\u5668\u5b66\u4e60\u8f93\u5165\u3002", "method": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u4fe1\u53f7\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\u5904\u7406\u9996\u6b21\u63a5\u89e6\u504f\u5dee\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u4fe1\u53f7\uff0c\u5355\u4e2a\u8fd0\u52a8\u7684\u8bef\u5dee\u6807\u51c6\u53ef\u964d\u4f4e20%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7f16\u7a0b\u7684\u53ef\u7528\u6027\u548c\u4eba\u673a\u4ea4\u4e92\u6548\u679c\u3002"}}
{"id": "2507.14643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14643", "abs": "https://arxiv.org/abs/2507.14643", "authors": ["Jifeng Shen", "Haibo Zhan", "Shaohua Dong", "Xin Zuo", "Wankou Yang", "Haibin Ling"], "title": "Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection", "comment": "submitted on 30/4/2025, Under Major Revision", "summary": "Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.", "AI": {"tldr": "MS2Fusion\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5149\u8c31\u7279\u5f81\u878d\u5408\u4e2d\u7684\u5c40\u90e8\u4e92\u8865\u7279\u5f81\u504f\u597d\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u5c40\u90e8\u4e92\u8865\u7279\u5f81\u800c\u5ffd\u7565\u8de8\u6a21\u6001\u5171\u4eab\u8bed\u4e49\uff0c\u4e14\u96be\u4ee5\u5e73\u8861\u611f\u53d7\u91ce\u5927\u5c0f\u4e0e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u53c2\u6570\u4ea4\u4e92\u673a\u5236\uff0c\u5206\u522b\u6316\u6398\u4e92\u8865\u4fe1\u606f\u548c\u5171\u4eab\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728FLIR\u3001M3FD\u548cLLVIP\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728RGB-T\u8bed\u4e49\u5206\u5272\u548cRGBT\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MS2Fusion\u5728\u591a\u5149\u8c31\u611f\u77e5\u4efb\u52a1\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.14631", "categories": ["cs.LG", "cs.CG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.14631", "abs": "https://arxiv.org/abs/2507.14631", "authors": ["Daniel Greenhut", "Dan Feldman"], "title": "$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation", "comment": null, "summary": "Given an integer $k\\geq1$ and a set $P$ of $n$ points in $\\REAL^d$, the\nclassic $k$-PCA (Principle Component Analysis) approximates the affine\n\\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear\nsubspace that minimizes its sum of squared Euclidean distances\n($\\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.\nThe \\emph{$k$-subspace median} is the subspace that minimizes its sum of\n(non-squared) Euclidean distances ($\\ell_{2,1}$-mixed norm), i.e., their\nmedian. The median subspace is usually more sparse and robust to noise/outliers\nthan the mean, but also much harder to approximate since, unlike the\n$\\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.\n  We provide the first polynomial-time deterministic algorithm whose both\nrunning time and approximation factor are not exponential in $k$. More\nprecisely, the multiplicative approximation factor is $\\sqrt{d}$, and the\nrunning time is polynomial in the size of the input. We expect that our\ntechnique would be useful for many other related problems, such as $\\ell_{2,z}$\nnorm of distances for $z\\not \\in \\br{1,2}$, e.g., $z=\\infty$, and handling\noutliers/sparsity.\n  Open code and experimental results on real-world datasets are also provided.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u7528\u4e8e\u8fd1\u4f3ck-\u5b50\u7a7a\u95f4\u4e2d\u4f4d\u6570\uff0c\u89e3\u51b3\u4e86\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u7a00\u758f\u6027\u3002", "motivation": "k-\u5b50\u7a7a\u95f4\u4e2d\u4f4d\u6570\u6bd4\u5747\u503c\u66f4\u9c81\u68d2\u4e14\u7a00\u758f\uff0c\u4f46\u7531\u4e8e\u975e\u51f8\u6027\u96be\u4ee5\u8fd1\u4f3c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u65f6\u95f4\u786e\u5b9a\u6027\u7b97\u6cd5\uff0c\u8fd1\u4f3c\u56e0\u5b50\u4e3a\u221ad\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u6570\u636e\u3002", "result": "\u7b97\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u4ee3\u7801\u5f00\u6e90\u3002", "conclusion": "\u8be5\u6280\u672f\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u8ddd\u79bb\u8303\u6570\u95ee\u9898\uff0c\u5982\u21132,z\u8303\u6570\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15649", "abs": "https://arxiv.org/abs/2507.15649", "authors": ["Haocheng Xu", "Haodong Zhang", "Zhenghan Chen", "Rong Xiong"], "title": "EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation", "comment": null, "summary": "To support humanoid robots in performing manipulation tasks, it is essential\nto study stable standing while accommodating upper-body motions. However, the\nlimited controllable range of humanoid robots in a standing position affects\nthe stability of the entire body. Thus we introduce a reinforcement learning\nbased framework for humanoid robots to imitate human upper-body motions while\nmaintaining overall stability. Our approach begins with designing a retargeting\nnetwork that generates a large-scale upper-body motion dataset for training the\nreinforcement learning (RL) policy, which enables the humanoid robot to track\nupper-body motion targets, employing domain randomization for enhanced\nrobustness. To avoid exceeding the robot's execution capability and ensure\nsafety and stability, we propose an Executable Motion Prior (EMP) module, which\nadjusts the input target movements based on the robot's current state. This\nadjustment improves standing stability while minimizing changes to motion\namplitude. We evaluate our framework through simulation and real-world tests,\ndemonstrating its practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u4eba\u7c7b\u4e0a\u534a\u8eab\u52a8\u4f5c\u5e76\u4fdd\u6301\u6574\u4f53\u7a33\u5b9a\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u5f62\u673a\u5668\u4eba\u5728\u7ad9\u7acb\u65f6\u5982\u4f55\u7a33\u5b9a\u6267\u884c\u4e0a\u534a\u8eab\u52a8\u4f5c\uff0c\u4ee5\u652f\u6301\u5176\u5b8c\u6210\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u4e86\u91cd\u5b9a\u5411\u7f51\u7edc\u751f\u6210\u5927\u89c4\u6a21\u4e0a\u534a\u8eab\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u9886\u57df\u968f\u673a\u5316\uff0c\u5e76\u5f15\u5165\u53ef\u6267\u884c\u8fd0\u52a8\u5148\u9a8c\uff08EMP\uff09\u6a21\u5757\u8c03\u6574\u76ee\u6807\u52a8\u4f5c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u6267\u884c\u4e0a\u534a\u8eab\u52a8\u4f5c\u65f6\u7684\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.14657", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14657", "abs": "https://arxiv.org/abs/2507.14657", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "title": "AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)", "comment": "24 pages, 9 figures", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\nFST.ai, a novel AI-powered framework designed to enhance officiating in Sport\nTaekwondo, particularly focusing on the complex task of real-time head kick\ndetection and scoring. Leveraging computer vision, deep learning, and edge\ninference, the system automates the identification and classification of key\nactions, significantly reducing decision time from minutes to seconds while\nimproving consistency and transparency. Importantly, the methodology is not\nlimited to Taekwondo. The underlying framework -- based on pose estimation,\nmotion classification, and impact analysis -- can be adapted to a wide range of\nsports requiring action detection, such as judo, karate, fencing, or even team\nsports like football and basketball, where foul recognition or performance\ntracking is critical. By addressing one of Taekwondo's most challenging\nscenarios -- head kick scoring -- we demonstrate the robustness, scalability,\nand sport-agnostic potential of FST.ai to transform officiating standards\nacross multiple disciplines.", "AI": {"tldr": "FST.ai\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u63d0\u5347\u4f53\u80b2\u88c1\u5224\u7684\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8dc6\u62f3\u9053\u5934\u90e8\u8e22\u51fb\u68c0\u6d4b\u548c\u8bc4\u5206\u4e2d\u3002", "motivation": "\u4f20\u7edf\u88c1\u5224\u7cfb\u7edf\u5b58\u5728\u5ef6\u8fdf\u3001\u4e3b\u89c2\u6027\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u516c\u5e73\u6027\u548c\u8fd0\u52a8\u5458\u4fe1\u4efb\u3002", "method": "\u5229\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u8fb9\u7f18\u63a8\u7406\u6280\u672f\uff0c\u901a\u8fc7\u59ff\u6001\u4f30\u8ba1\u3001\u52a8\u4f5c\u5206\u7c7b\u548c\u5f71\u54cd\u5206\u6790\u5b9e\u73b0\u5b9e\u65f6\u52a8\u4f5c\u68c0\u6d4b\u3002", "result": "\u7cfb\u7edf\u5c06\u51b3\u7b56\u65f6\u95f4\u4ece\u5206\u949f\u7f29\u77ed\u5230\u79d2\uff0c\u63d0\u9ad8\u4e86\u88c1\u5224\u7684\u4e00\u81f4\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "FST.ai\u6846\u67b6\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u9002\u7528\u4e8e\u591a\u79cd\u4f53\u80b2\u9879\u76ee\uff0c\u6539\u53d8\u88c1\u5224\u6807\u51c6\u3002"}}
{"id": "2507.14668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14668", "abs": "https://arxiv.org/abs/2507.14668", "authors": ["Yunfeng Li", "Junhong Liu", "Zhaohui Yang", "Guofu Liao", "Chuyun Zhang"], "title": "Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model", "comment": "15 pages, 14 figures", "summary": "Deep learning models have been widely adopted for False Data Injection Attack\n(FDIA) detection in smart grids due to their ability to capture unstructured\nand sparse features. However, the increasing system scale and data\ndimensionality introduce significant computational and memory burdens,\nparticularly in large-scale industrial datasets, limiting detection efficiency.\nTo address these issues, this paper proposes Rec-AD, a computationally\nefficient framework that integrates Tensor Train decomposition with the Deep\nLearning Recommendation Model (DLRM). Rec-AD enhances training and inference\nefficiency through embedding compression, optimized data access via index\nreordering, and a pipeline training mechanism that reduces memory communication\noverhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing\nFDIA detection systems without code modifications. Experimental results show\nthat Rec-AD significantly improves computational throughput and real-time\ndetection performance, narrowing the attack window and increasing attacker\ncost. These advancements strengthen edge computing capabilities and\nscalability, providing robust technical support for smart grid security.", "AI": {"tldr": "Rec-AD\u6846\u67b6\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u548c\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\u63d0\u5347FDIA\u68c0\u6d4b\u6548\u7387\uff0c\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\u3002", "motivation": "\u667a\u80fd\u7535\u7f51\u4e2dFDIA\u68c0\u6d4b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u8d1f\u62c5\u9650\u5236\u4e86\u6548\u7387\uff0c\u9700\u8981\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5f20\u91cf\u5206\u89e3\u548cDLRM\uff0c\u901a\u8fc7\u5d4c\u5165\u538b\u7f29\u3001\u7d22\u5f15\u91cd\u6392\u5e8f\u548c\u6d41\u6c34\u7ebf\u8bad\u7ec3\u673a\u5236\u4f18\u5316\u6548\u7387\u3002", "result": "\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u541e\u5410\u91cf\u548c\u5b9e\u65f6\u68c0\u6d4b\u6027\u80fd\uff0c\u7f29\u5c0f\u653b\u51fb\u7a97\u53e3\u5e76\u589e\u52a0\u653b\u51fb\u6210\u672c\u3002", "conclusion": "Rec-AD\u4e3a\u667a\u80fd\u7535\u7f51\u5b89\u5168\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2507.15677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15677", "abs": "https://arxiv.org/abs/2507.15677", "authors": ["Huayue Liang", "Yanbo Chen", "Hongyang Cheng", "Yanzhao Yu", "Shoujie Li", "Junbo Tan", "Xueqian Wang", "Long Zeng"], "title": "Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms", "comment": null, "summary": "Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant\nmotion. Still, the inherent properties of cables, such as resilience,\nhysteresis, and friction, often lead to particular difficulties in modeling and\ncontrol. This paper proposes a model predictive control (MPC) method that\nrelies exclusively on input-output data, without a physical model, to improve\nthe control accuracy of FCRAs. First, we develop an implicit model based on\ninput-output data and integrate it into an MPC optimization framework. Second,\na data selection algorithm (DSA) is introduced to filter the data that best\ncharacterize the system, thereby reducing the solution time per step to\napproximately 4 ms, which is an improvement of nearly 80%. Lastly, the\ninfluence of hyperparameters on tracking error is investigated through\nsimulation. The proposed method has been validated on a real FCRA platform,\nincluding five-point positioning accuracy tests, a five-point response tracking\ntest, and trajectory tracking for letter drawing. The results demonstrate that\nthe average positioning accuracy is approximately 2.070 mm. Moreover, compared\nto the PID method with an average tracking error of 1.418{\\deg}, the proposed\nmethod achieves an average tracking error of 0.541{\\deg}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\u7684\u63a7\u5236\u7cbe\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u65f6\u95f4\u5e76\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\uff08FCRAs\uff09\u7684\u7535\u7f06\u7279\u6027\uff08\u5982\u5f39\u6027\u3001\u6ede\u540e\u548c\u6469\u64e6\uff09\u5bfc\u81f4\u5efa\u6a21\u548c\u63a7\u5236\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7269\u7406\u6a21\u578b\u7684\u9ad8\u6548\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u7684\u9690\u5f0f\u6a21\u578b\uff0c\u5e76\u96c6\u6210\u5230MPC\u6846\u67b6\u4e2d\uff1b\u5f15\u5165\u6570\u636e\u9009\u62e9\u7b97\u6cd5\uff08DSA\uff09\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u771f\u5b9eFCRA\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u5e73\u5747\u5b9a\u4f4d\u7cbe\u5ea6\u7ea62.070\u6beb\u7c73\uff0c\u8ddf\u8e2a\u8bef\u5dee\u4ecePID\u76841.418\u00b0\u964d\u81f30.541\u00b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2507.14662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14662", "abs": "https://arxiv.org/abs/2507.14662", "authors": ["Shayan Rokhva", "Babak Teimourpour"], "title": "Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall", "comment": "Questions & Recommendations: shayanrokhva1999@gmail.com;\n  shayan1999rokh@yahoo.com", "summary": "Quantifying post-consumer food waste in institutional dining settings is\nessential for supporting data-driven sustainability strategies. This study\npresents a cost-effective computer vision framework that estimates plate-level\nfood waste by utilizing semantic segmentation of RGB images taken before and\nafter meal consumption across five Iranian dishes. Four fully supervised models\n(U-Net, U-Net++, and their lightweight variants) were trained using a capped\ndynamic inverse-frequency loss and AdamW optimizer, then evaluated through a\ncomprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a\ncustom-defined Distributional Pixel Agreement (DPA) metric tailored to the\ntask. All models achieved satisfying performance, and for each food type, at\nleast one model approached or surpassed 90% DPA, demonstrating strong alignment\nin pixel-wise proportion estimates. Lighter models with reduced parameter\ncounts offered faster inference, achieving real-time throughput on an NVIDIA T4\nGPU. Further analysis showed superior segmentation performance for dry and more\nrigid components (e.g., rice and fries), while more complex, fragmented, or\nviscous dishes, such as stews, showed reduced performance, specifically\npost-consumption. Despite limitations such as reliance on 2D imaging,\nconstrained food variety, and manual data collection, the proposed framework is\npioneering and represents a scalable, contactless solution for continuous\nmonitoring of food consumption. This research lays foundational groundwork for\nautomated, real-time waste tracking systems in large-scale food service\nenvironments and offers actionable insights and outlines feasible future\ndirections for dining hall management and policymakers aiming to reduce\ninstitutional food waste.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u7ecf\u6d4e\u9ad8\u6548\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u673a\u6784\u7528\u9910\u73af\u5883\u4e2d\u7684\u98df\u7269\u6d6a\u8d39\uff0c\u901a\u8fc7\u8bed\u4e49\u5206\u5272RGB\u56fe\u50cf\u8bc4\u4f30\u4e94\u79cd\u4f0a\u6717\u83dc\u80b4\u7684\u9910\u76d8\u7ea7\u6d6a\u8d39\u3002", "motivation": "\u91cf\u5316\u673a\u6784\u7528\u9910\u73af\u5883\u4e2d\u7684\u98df\u7269\u6d6a\u8d39\u5bf9\u6570\u636e\u9a71\u52a8\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u56db\u79cd\u5168\u76d1\u7763\u6a21\u578b\uff08U-Net\u3001U-Net++\u53ca\u5176\u8f7b\u91cf\u7248\uff09\u548c\u52a8\u6001\u9006\u9891\u7387\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7RGB\u56fe\u50cf\u8bed\u4e49\u5206\u5272\u8bc4\u4f30\u98df\u7269\u6d6a\u8d39\u3002", "result": "\u6240\u6709\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u90e8\u5206\u6a21\u578b\u5bf9\u7279\u5b9a\u98df\u7269\u7c7b\u578b\u7684DPA\u63a5\u8fd1\u6216\u8d85\u8fc790%\uff0c\u8f7b\u91cf\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u98df\u54c1\u670d\u52a1\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u6d6a\u8d39\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65e0\u63a5\u89e6\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u51cf\u5c11\u673a\u6784\u98df\u7269\u6d6a\u8d39\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u5411\u3002"}}
{"id": "2507.14677", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14677", "abs": "https://arxiv.org/abs/2507.14677", "authors": ["Yiming Xu", "Zhen Peng", "Bin Shi", "Xu Hua", "Bo Dong", "Song Wang", "Chen Chen"], "title": "Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective", "comment": "Accepted by AAAI2025", "summary": "The superiority of graph contrastive learning (GCL) has prompted its\napplication to anomaly detection tasks for more powerful risk warning systems.\nUnfortunately, existing GCL-based models tend to excessively prioritize overall\ndetection performance while neglecting robustness to structural imbalance,\nwhich can be problematic for many real-world networks following power-law\ndegree distributions. Particularly, GCL-based methods may fail to capture tail\nanomalies (abnormal nodes with low degrees). This raises concerns about the\nsecurity and robustness of current anomaly detection algorithms and therefore\nhinders their applicability in a variety of realistic high-risk scenarios. To\nthe best of our knowledge, research on the robustness of graph anomaly\ndetection to structural imbalance has received little scrutiny. To address the\nabove issues, this paper presents a novel GCL-based framework named AD-GCL. It\ndevises the neighbor pruning strategy to filter noisy edges for head nodes and\nfacilitate the detection of genuine tail nodes by aligning from head nodes to\nforged tail nodes. Moreover, AD-GCL actively explores potential neighbors to\nenlarge the receptive field of tail nodes through anomaly-guided neighbor\ncompletion. We further introduce intra- and inter-view consistency loss of the\noriginal and augmentation graph for enhanced representation. The performance\nevaluation of the whole, head, and tail nodes on multiple datasets validates\nthe comprehensive superiority of the proposed AD-GCL in detecting both head\nanomalies and tail anomalies.", "AI": {"tldr": "AD-GCL\u662f\u4e00\u79cd\u65b0\u578b\u7684\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709GCL\u65b9\u6cd5\u5728\u7ed3\u6784\u4e0d\u5e73\u8861\u7f51\u7edc\u4e2d\u5bf9\u5c3e\u90e8\u5f02\u5e38\u68c0\u6d4b\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u90bb\u5c45\u4fee\u526a\u548c\u5f02\u5e38\u5f15\u5bfc\u7684\u90bb\u5c45\u8865\u5168\u7b56\u7565\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GCL\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fc7\u5ea6\u5173\u6ce8\u6574\u4f53\u6027\u80fd\uff0c\u5ffd\u89c6\u4e86\u5bf9\u7ed3\u6784\u4e0d\u5e73\u8861\uff08\u5982\u5e42\u5f8b\u5206\u5e03\u7f51\u7edc\uff09\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5bf9\u4f4e\u5ea6\u5c3e\u90e8\u5f02\u5e38\u7684\u68c0\u6d4b\u4e0d\u8db3\u3002", "method": "AD-GCL\u91c7\u7528\u90bb\u5c45\u4fee\u526a\u7b56\u7565\u8fc7\u6ee4\u566a\u58f0\u8fb9\uff0c\u5e76\u901a\u8fc7\u5f02\u5e38\u5f15\u5bfc\u7684\u90bb\u5c45\u8865\u5168\u6269\u5927\u5c3e\u90e8\u8282\u70b9\u7684\u611f\u77e5\u8303\u56f4\uff0c\u540c\u65f6\u5f15\u5165\u89c6\u56fe\u4e00\u81f4\u6027\u635f\u5931\u589e\u5f3a\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cAD-GCL\u5728\u6574\u4f53\u3001\u5934\u90e8\u548c\u5c3e\u90e8\u8282\u70b9\u7684\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5747\u8868\u73b0\u51fa\u5168\u9762\u4f18\u52bf\u3002", "conclusion": "AD-GCL\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5f02\u5e38\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\uff0c\u5c24\u5176\u662f\u5728\u7ed3\u6784\u4e0d\u5e73\u8861\u7f51\u7edc\u4e2d\u68c0\u6d4b\u5c3e\u90e8\u5f02\u5e38\u7684\u80fd\u529b\u3002"}}
{"id": "2507.15693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15693", "abs": "https://arxiv.org/abs/2507.15693", "authors": ["Georges Chebly", "Spencer Little", "Nisal Perera", "Aliya Abedeen", "Ken Suzuki", "Donghyun Kim"], "title": "Strong, Accurate, and Low-Cost Robot Manipulator", "comment": null, "summary": "This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed\nto achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,\nand sub-millimeter repeatability - at a material cost under $215. As an\naccessible robot for broad applications across classroom education to AI\nexperiments, Forte pushes forward the performance limitations of existing\nlow-cost educational arms. We introduce a cost-effective mechanical design that\ncombines capstan-based cable drives, timing belts, simple tensioning\nmechanisms, and lightweight 3D-printed structures, along with topology\noptimization for structural stiffness. Through careful drivetrain engineering,\nwe minimize backlash and maintain control fidelity without relying on\nhigh-power electronics or expensive manufacturing processes. Experimental\nvalidation demonstrates that Forte achieves high repeatability and load\ncapacity, offering a compelling robotic platform for both classroom instruction\nand advanced robotics research.", "AI": {"tldr": "Forte\u662f\u4e00\u79cd\u5b8c\u51683D\u6253\u5370\u76846\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u4ee5\u4f4e\u6210\u672c\u5b9e\u73b0\u63a5\u8fd1\u5de5\u4e1a\u7ea7\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u4f4e\u6210\u672c\u6559\u80b2\u673a\u68b0\u81c2\u7684\u6027\u80fd\u6781\u9650\uff0c\u9002\u7528\u4e8e\u8bfe\u5802\u6559\u80b2\u548cAI\u5b9e\u9a8c\u3002", "method": "\u91c7\u7528\u6210\u672c\u6548\u76ca\u9ad8\u7684\u673a\u68b0\u8bbe\u8ba1\uff0c\u5305\u62ec\u57fa\u4e8e\u7ede\u76d8\u7684\u7535\u7f06\u9a71\u52a8\u3001\u540c\u6b65\u5e26\u3001\u7b80\u5355\u5f20\u7d27\u673a\u5236\u548c\u8f7b\u91cf\u53163D\u6253\u5370\u7ed3\u6784\uff0c\u7ed3\u5408\u62d3\u6251\u4f18\u5316\u63d0\u9ad8\u521a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793aForte\u5177\u6709\u9ad8\u91cd\u590d\u6027\u548c\u8d1f\u8f7d\u80fd\u529b\uff0c\u6750\u6599\u6210\u672c\u4f4e\u4e8e215\u7f8e\u5143\u3002", "conclusion": "Forte\u662f\u4e00\u4e2a\u9002\u7528\u4e8e\u8bfe\u5802\u6559\u5b66\u548c\u9ad8\u7ea7\u673a\u5668\u4eba\u7814\u7a76\u7684\u4f18\u79c0\u5e73\u53f0\u3002"}}
{"id": "2507.14670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14670", "abs": "https://arxiv.org/abs/2507.14670", "authors": ["Yaxuan Song", "Jianan Fan", "Hang Chang", "Weidong Cai"], "title": "Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images", "comment": "16 pages, 15 tables, 8 figures", "summary": "Accurately predicting gene expression from histopathology images offers a\nscalable and non-invasive approach to molecular profiling, with significant\nimplications for precision medicine and computational pathology. However,\nexisting methods often underutilize the cross-modal representation alignment\nbetween histopathology images and gene expression profiles across multiple\nrepresentational levels, thereby limiting their prediction performance. To\naddress this, we propose Gene-DML, a unified framework that structures latent\nspace through Dual-pathway Multi-Level discrimination to enhance correspondence\nbetween morphological and transcriptional modalities. The multi-scale\ninstance-level discrimination pathway aligns hierarchical histopathology\nrepresentations extracted at local, neighbor, and global levels with gene\nexpression profiles, capturing scale-aware morphological-transcriptional\nrelationships. In parallel, the cross-level instance-group discrimination\npathway enforces structural consistency between individual (image/gene)\ninstances and modality-crossed (gene/image, respectively) groups, strengthening\nthe alignment across modalities. By jointly modelling fine-grained and\nstructural-level discrimination, Gene-DML is able to learn robust cross-modal\nrepresentations, enhancing both predictive accuracy and generalization across\ndiverse biological contexts. Extensive experiments on public spatial\ntranscriptomics datasets demonstrate that Gene-DML achieves state-of-the-art\nperformance in gene expression prediction. The code and checkpoints will be\nreleased soon.", "AI": {"tldr": "Gene-DML\u662f\u4e00\u79cd\u901a\u8fc7\u53cc\u8def\u5f84\u591a\u7ea7\u5224\u522b\u589e\u5f3a\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e0e\u57fa\u56e0\u8868\u8fbe\u8c31\u5bf9\u9f50\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u4e0e\u57fa\u56e0\u8868\u8fbe\u8c31\u4e4b\u95f4\u7684\u8de8\u6a21\u6001\u8868\u793a\u5bf9\u9f50\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "method": "Gene-DML\u901a\u8fc7\u53cc\u8def\u5f84\u591a\u7ea7\u5224\u522b\uff08\u591a\u5c3a\u5ea6\u5b9e\u4f8b\u7ea7\u5224\u522b\u548c\u8de8\u7ea7\u5b9e\u4f8b-\u7ec4\u5224\u522b\uff09\u589e\u5f3a\u5f62\u6001\u5b66\u4e0e\u8f6c\u5f55\u6a21\u6001\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u516c\u5171\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u6570\u636e\u96c6\u4e0a\uff0cGene-DML\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "Gene-DML\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u7ec6\u7c92\u5ea6\u548c\u7ed3\u6784\u7ea7\u5224\u522b\uff0c\u5b66\u4e60\u5230\u7a33\u5065\u7684\u8de8\u6a21\u6001\u8868\u793a\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14679", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14679", "abs": "https://arxiv.org/abs/2507.14679", "authors": ["Zixin Xu", "Zhijie Wang", "Zhiyuan Pan"], "title": "GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks", "comment": null, "summary": "The exponential growth of spam text on the Internet necessitates robust\ndetection mechanisms to mitigate risks such as information leakage and social\ninstability. This work addresses two principal challenges: adversarial\nstrategies employed by spammers and the scarcity of labeled data. We propose a\nnovel spam-text detection framework GCC-Spam, which integrates three core\ninnovations. First, a character similarity network captures orthographic and\nphonetic features to counter character-obfuscation attacks and furthermore\nproduces sentence embeddings for downstream classification. Second, contrastive\nlearning enhances discriminability by optimizing the latent-space distance\nbetween spam and normal texts. Third, a Generative Adversarial Network (GAN)\ngenerates realistic pseudo-spam samples to alleviate data scarcity while\nimproving model robustness and classification accuracy. Extensive experiments\non real-world datasets demonstrate that our model outperforms baseline\napproaches, achieving higher detection rates with significantly fewer labeled\nexamples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5783\u573e\u6587\u672c\u68c0\u6d4b\u6846\u67b6GCC-Spam\uff0c\u901a\u8fc7\u5b57\u7b26\u76f8\u4f3c\u6027\u7f51\u7edc\u3001\u5bf9\u6bd4\u5b66\u4e60\u548cGAN\u751f\u6210\u4f2a\u6837\u672c\uff0c\u89e3\u51b3\u4e86\u5bf9\u6297\u7b56\u7565\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u4e92\u8054\u7f51\u4e0a\u5783\u573e\u6587\u672c\u7684\u5feb\u901f\u589e\u957f\u5e26\u6765\u4e86\u4fe1\u606f\u6cc4\u9732\u548c\u793e\u4f1a\u4e0d\u7a33\u5b9a\u7b49\u98ce\u9669\uff0c\u9700\u8981\u6709\u6548\u7684\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u7ed3\u5408\u5b57\u7b26\u76f8\u4f3c\u6027\u7f51\u7edc\u3001\u5bf9\u6bd4\u5b66\u4e60\u548cGAN\u751f\u6210\u4f2a\u6837\u672c\uff0c\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7528\u66f4\u5c11\u7684\u6807\u6ce8\u6837\u672c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u68c0\u6d4b\u7387\u3002", "conclusion": "GCC-Spam\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5783\u573e\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u5bf9\u6297\u653b\u51fb\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002"}}
{"id": "2507.15710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15710", "abs": "https://arxiv.org/abs/2507.15710", "authors": ["Lu Huang", "Lingxiao Meng", "Jiankun Wang", "Xingjian Jing"], "title": "Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages", "comment": null, "summary": "Sampling-based algorithms are widely used for motion planning in\nhigh-dimensional configuration spaces. However, due to low sampling efficiency,\ntheir performance often diminishes in complex configuration spaces with narrow\ncorridors. Existing approaches address this issue using handcrafted or learned\nheuristics to guide sampling toward useful regions. Unfortunately, these\nstrategies often lack generalizability to various problems or require extensive\nprior training. In this paper, we propose a simple yet efficient sampling-based\nplanning framework along with its bidirectional version that overcomes these\nissues by integrating different levels of planning granularity. Our approach\nprobes configuration spaces with uniform random samples at varying resolutions\nand explores these multi-resolution samples online with a bias towards sparse\nsamples when traveling large free configuration spaces. By seamlessly\ntransitioning between sparse and dense samples, our approach can navigate\ncomplex configuration spaces while maintaining planning speed and completeness.\nThe simulation results demonstrate that our approach outperforms several\nstate-of-the-art sampling-based planners in $\\mathbb{SE}(2)$, $\\mathbb{SE}(3)$,\nand $\\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments\nconducted with the Franka Emika Panda robot operating in a constrained\nworkspace provide additional evidence of the superiority of the proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u5206\u8fa8\u7387\u91c7\u6837\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5bc6\u5ea6\u89e3\u51b3\u590d\u6742\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u91c7\u6837\u7b97\u6cd5\u5728\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u4e2d\u6548\u7387\u4f4e\uff0c\u4e14\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u6216\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u3002", "method": "\u7ed3\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u89c4\u5212\uff0c\u52a8\u6001\u5207\u6362\u7a00\u758f\u548c\u5bc6\u96c6\u91c7\u6837\uff0c\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u3002", "result": "\u5728\u591a\u79cd\u914d\u7f6e\u7a7a\u95f4\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u89c4\u5212\u901f\u5ea6\u548c\u5b8c\u6574\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u6548\u7387\u3002"}}
{"id": "2507.14675", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14675", "abs": "https://arxiv.org/abs/2507.14675", "authors": ["Yuchen Duan", "Zhe Chen", "Yusong Hu", "Weiyun Wang", "Shenglong Ye", "Botian Shi", "Lewei Lu", "Qibin Hou", "Tong Lu", "Hongsheng Li", "Jifeng Dai", "Wenhai Wang"], "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "comment": null, "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Doc-750K\u6570\u636e\u96c6\u548cDocopilot\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6587\u6863\u7406\u89e3\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6587\u6863\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6587\u6863\u7ea7\u6570\u636e\u96c6\uff0c\u4e14\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u788e\u7247\u5316\u3001\u9519\u8bef\u7d2f\u79ef\u548c\u989d\u5916\u65f6\u95f4\u6210\u672c\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86Doc-750K\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6837\u6587\u6863\u7ed3\u6784\u548c\u8de8\u9875\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1\u4e86\u539f\u751f\u591a\u6a21\u6001\u6a21\u578bDocopilot\uff0c\u65e0\u9700\u4f9d\u8d56\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3002", "result": "Docopilot\u5728\u6587\u6863\u7406\u89e3\u4efb\u52a1\u548c\u591a\u8f6e\u4ea4\u4e92\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8fde\u8d2f\u6027\u3001\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "conclusion": "Docopilot\u4e3a\u6587\u6863\u7ea7\u591a\u6a21\u6001\u7406\u89e3\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.14698", "categories": ["cs.LG", "cs.AI", "cs.HC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.14698", "abs": "https://arxiv.org/abs/2507.14698", "authors": ["Xuetao Lin", "Tianhao Peng", "Peihong Dai", "Yu Liang", "Wenjun Wu"], "title": "Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition", "comment": null, "summary": "EEG-based emotion recognition plays an important role in developing adaptive\nbrain-computer communication systems, yet faces two fundamental challenges in\npractical implementations: (1) effective integration of non-stationary\nspatial-temporal neural patterns, (2) robust adaptation to dynamic emotional\nintensity variations in real-world scenarios. This paper proposes SST-CL, a\nnovel framework integrating spatial-temporal transformers with curriculum\nlearning. Our method introduces two core components: a spatial encoder that\nmodels inter-channel relationships and a temporal encoder that captures\nmulti-scale dependencies through windowed attention mechanisms, enabling\nsimultaneous extraction of spatial correlations and temporal dynamics from EEG\nsignals. Complementing this architecture, an intensity-aware curriculum\nlearning strategy progressively guides training from high-intensity to\nlow-intensity emotional states through dynamic sample scheduling based on a\ndual difficulty assessment. Comprehensive experiments on three benchmark\ndatasets demonstrate state-of-the-art performance across various emotional\nintensity levels, with ablation studies confirming the necessity of both\narchitectural components and the curriculum learning mechanism.", "AI": {"tldr": "SST-CL\u6846\u67b6\u7ed3\u5408\u7a7a\u95f4-\u65f6\u95f4\u53d8\u6362\u5668\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u63d0\u5347EEG\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3EEG\u60c5\u611f\u8bc6\u522b\u4e2d\u975e\u5e73\u7a33\u7a7a\u95f4-\u65f6\u95f4\u795e\u7ecf\u6a21\u5f0f\u7684\u6709\u6548\u6574\u5408\u548c\u52a8\u6001\u60c5\u611f\u5f3a\u5ea6\u53d8\u5316\u7684\u9c81\u68d2\u9002\u5e94\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u65f6\u95f4\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\u548c\u5f3a\u5ea6\u611f\u77e5\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u67b6\u6784\u548c\u8bfe\u7a0b\u5b66\u4e60\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "SST-CL\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86EEG\u60c5\u611f\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15716", "abs": "https://arxiv.org/abs/2507.15716", "authors": ["Ziyu Wan", "Lin Zhao"], "title": "DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models", "comment": null, "summary": "This paper proposes DiffPF, a differentiable particle filter that leverages\ndiffusion models for state estimation in dynamic systems. Unlike conventional\ndifferentiable particle filters, which require importance weighting and\ntypically rely on predefined or low-capacity proposal distributions. DiffPF\nlearns a flexible posterior sampler by conditioning a diffusion model on\npredicted particles and the current observation. This enables accurate,\nequally-weighted sampling from complex, high-dimensional, and multimodal\nfiltering distributions. We evaluate DiffPF across a range of scenarios,\nincluding both unimodal and highly multimodal distributions, and test it on\nsimulated as well as real-world tasks, where it consistently outperforms\nexisting filtering baselines. In particular, DiffPF achieves an 82.8%\nimprovement in estimation accuracy on a highly multimodal global localization\nbenchmark, and a 26% improvement on the real-world KITTI visual odometry\nbenchmark, compared to state-of-the-art differentiable filters. To the best of\nour knowledge, DiffPF is the first method to integrate conditional diffusion\nmodels into particle filtering, enabling high-quality posterior sampling that\nproduces more informative particles and significantly improves state\nestimation.", "AI": {"tldr": "DiffPF\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\u5668\u4f9d\u8d56\u9884\u5b9a\u4e49\u6216\u4f4e\u5bb9\u91cf\u63d0\u8bae\u5206\u5e03\uff0cDiffPF\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u7075\u6d3b\u7684\u540e\u9a8c\u91c7\u6837\u5668\u3002", "method": "DiffPF\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5728\u9884\u6d4b\u7c92\u5b50\u548c\u5f53\u524d\u89c2\u6d4b\u4e0a\u6761\u4ef6\u5316\uff0c\u5b9e\u73b0\u9ad8\u7ef4\u3001\u591a\u6a21\u6001\u5206\u5e03\u7684\u9ad8\u8d28\u91cf\u91c7\u6837\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cDiffPF\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u5728\u5168\u5c40\u5b9a\u4f4d\u57fa\u51c6\u4e0a\u7cbe\u5ea6\u63d0\u534782.8%\uff0c\u5728KITTI\u89c6\u89c9\u91cc\u7a0b\u8ba1\u57fa\u51c6\u4e0a\u63d0\u534726%\u3002", "conclusion": "DiffPF\u9996\u6b21\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u96c6\u6210\u5230\u7c92\u5b50\u6ee4\u6ce2\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u72b6\u6001\u4f30\u8ba1\u8d28\u91cf\u3002"}}
{"id": "2507.14680", "categories": ["cs.CV", "cs.AI", "68T07, 92C55", "I.2.7; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2507.14680", "abs": "https://arxiv.org/abs/2507.14680", "authors": ["Xinheng Lyu", "Yuci Liang", "Wenting Chen", "Meidan Ding", "Jiaqi Yang", "Guolin Huang", "Daokun Zhang", "Xiangjian He", "Linlin Shen"], "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis", "comment": null, "summary": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel\ntissue analysis across various pathological tasks. While recent advancements in\nmulti-modal large language models (MLLMs) allow multi-task WSI analysis through\nnatural language, they often underperform compared to task-specific models.\nCollaborative multi-agent systems have emerged as a promising solution to\nbalance versatility and accuracy in healthcare, yet their potential remains\nunderexplored in pathology-specific domains. To address these issues, we\npropose WSI-Agents, a novel collaborative multi-agent system for multi-modal\nWSI analysis. WSI-Agents integrates specialized functional agents with robust\ntask allocation and verification mechanisms to enhance both task-specific\naccuracy and multi-task versatility through three components: (1) a task\nallocation module assigning tasks to expert agents using a model zoo of patch\nand WSI level MLLMs, (2) a verification mechanism ensuring accuracy through\ninternal consistency checks and external validation using pathology knowledge\nbases and domain-specific models, and (3) a summary module synthesizing the\nfinal summary with visual interpretation maps. Extensive experiments on\nmulti-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs\nand medical agent frameworks across diverse tasks.", "AI": {"tldr": "WSI-Agents\u662f\u4e00\u79cd\u65b0\u578b\u534f\u4f5c\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u591a\u6a21\u6001WSI\u5206\u6790\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u914d\u548c\u9a8c\u8bc1\u673a\u5236\u63d0\u5347\u51c6\u786e\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728WSI\u5206\u6790\u4e2d\u8868\u73b0\u4e0d\u5982\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\uff0c\u4e14\u591a\u4ee3\u7406\u7cfb\u7edf\u5728\u75c5\u7406\u5b66\u9886\u57df\u6f5c\u529b\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "WSI-Agents\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u4efb\u52a1\u5206\u914d\u6a21\u5757\u3001\u9a8c\u8bc1\u673a\u5236\u548c\u603b\u7ed3\u6a21\u5757\uff0c\u7ed3\u5408\u4e13\u5bb6\u4ee3\u7406\u548c\u77e5\u8bc6\u5e93\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\uff0cWSI-Agents\u4f18\u4e8e\u73b0\u6709WSI MLLMs\u548c\u533b\u7597\u4ee3\u7406\u6846\u67b6\u3002", "conclusion": "WSI-Agents\u901a\u8fc7\u534f\u4f5c\u591a\u4ee3\u7406\u7cfb\u7edf\u6709\u6548\u5e73\u8861\u4e86WSI\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2507.14706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14706", "abs": "https://arxiv.org/abs/2507.14706", "authors": ["Claudio Giusti", "Luca Guarnera", "Mirko Casu", "Sebastiano Battiato"], "title": "Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling", "comment": "23 pages, 14 figures", "summary": "Detecting fraudulent credit card transactions remains a significant\nchallenge, due to the extreme class imbalance in real-world data and the often\nsubtle patterns that separate fraud from legitimate activity. Existing research\ncommonly attempts to address this by generating synthetic samples for the\nminority class using approaches such as GANs, VAEs, or hybrid generative\nmodels. However, these techniques, particularly when applied only to\nminority-class data, tend to result in overconfident classifiers and poor\nlatent cluster separation, ultimately limiting real-world detection\nperformance. In this study, we propose the Causal Prototype Attention\nClassifier (CPAC), an interpretable architecture that promotes class-aware\nclustering and improved latent space structure through prototype-based\nattention mechanisms and we will couple it with the encoder in a VAE-GAN\nallowing it to offer a better cluster separation moving beyond post-hoc sample\naugmentation. We compared CPAC-augmented models to traditional oversamplers,\nsuch as SMOTE, as well as to state-of-the-art generative models, both with and\nwithout CPAC-based latent classifiers. Our results show that classifier-guided\nlatent shaping with CPAC delivers superior performance, achieving an F1-score\nof 93.14\\% percent and recall of 90.18\\%, along with improved latent cluster\nseparation. Further ablation studies and visualizations provide deeper insight\ninto the benefits and limitations of classifier-driven representation learning\nfor fraud detection. The codebase for this work will be available at final\nsubmission.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCPAC\u7684\u65b0\u578b\u67b6\u6784\uff0c\u7ed3\u5408VAE-GAN\u63d0\u5347\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u56e0\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u6f5c\u5728\u7a7a\u95f4\u5206\u79bb\u4e0d\u8db3\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u91c7\u7528\u539f\u578b\u6ce8\u610f\u529b\u673a\u5236\u548cVAE-GAN\u7ed3\u5408\uff0c\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u3002", "result": "CPAC\u5728F1-score\u548c\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0893.14%\u548c90.18%\uff09\uff0c\u5e76\u6539\u5584\u805a\u7c7b\u5206\u79bb\u3002", "conclusion": "CPAC\u901a\u8fc7\u5206\u7c7b\u5668\u9a71\u52a8\u7684\u8868\u793a\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6b3a\u8bc8\u68c0\u6d4b\u6548\u679c\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.15729", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15729", "abs": "https://arxiv.org/abs/2507.15729", "authors": ["Jens V. R\u00fcppel", "Andrey Rudenko", "Tim Schreiter", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction", "comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses", "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8f85\u52a9\u673a\u5668\u4eba\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u89c6\u7ebf\u548c\u8bed\u97f3\uff09\u5b9e\u73b0\u52a8\u6001\u4efb\u52a1\u652f\u6301\uff0c\u5e76\u4e0e\u4f20\u7edf\u811a\u672c\u5316\u7cfb\u7edf\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "motivation": "\u73b0\u6709HRI\u7cfb\u7edf\u5728\u53cc\u5411\u3001\u591a\u6a21\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u534f\u4f5c\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u6a21\u5757\u5316\u3001\u53ef\u8f6c\u79fb\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u5b9e\u65f6\u8bed\u8a00\u4ea4\u4e92\u548c\u591a\u89c6\u89c9\u8f93\u5165\u611f\u77e5\uff0c\u652f\u6301\u52a8\u6001\u4efb\u52a1\u3002", "result": "LLM\u65b9\u6cd5\u63d0\u5347\u4e86\u9002\u5e94\u6027\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u5197\u4f59\u8f93\u51fa\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u66f4\u9002\u5408\u7b80\u5355\u4efb\u52a1\u3002", "conclusion": "LLM\u9a71\u52a8\u7684HRI\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u9700\u4f18\u5316\u5197\u4f59\u95ee\u9898\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u5177\u4f18\u52bf\u3002"}}
{"id": "2507.14686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14686", "abs": "https://arxiv.org/abs/2507.14686", "authors": ["Chen Cai", "Tianyi Liu", "Jianjun Gao", "Wenyang Liu", "Kejun Wu", "Ruoyu Wang", "Yi Wang", "Soo Chin Liew"], "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition", "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIPD\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u6559\u5e08MLLM\u4e2d\u84b8\u998f\u591a\u6a21\u6001\u77e5\u8bc6\uff0c\u63d0\u5347\u5c0f\u89c4\u6a21GSR\u6a21\u578b\u7684\u6cdb\u5316\u548c\u96f6\u6837\u672c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u590d\u6742GSR\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u4f20\u7edfGSR\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u4ea4\u4e92\u63d0\u793a\u84b8\u998f\uff08MIPD\uff09\u6846\u67b6\uff0c\u7ed3\u5408LLM\u751f\u6210\u7684\u5224\u65ad\u6027\u7406\u7531\u548c\u89c6\u89c9\u4fe1\u606f\u5bf9\u9f50\uff0c\u84b8\u998f\u77e5\u8bc6\u5230\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5728Ov-SWiG\u548cHICO-DET\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u5347\u4e86\u7f55\u89c1\u548c\u672a\u89c1\u573a\u666f\u7684\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "MIPD\u6709\u6548\u589e\u5f3a\u4e86GSR\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u586b\u8865\u4e86\u5df2\u77e5\u4e0e\u672a\u77e5\u573a\u666f\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2507.14715", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14715", "abs": "https://arxiv.org/abs/2507.14715", "authors": ["Rachid Karami", "Rajeev Patwari", "Hyoukjun Kwon", "Ashish Sirasao"], "title": "Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems", "comment": null, "summary": "The integration of generative AI models, particularly large language models\n(LLMs), into real-time multi-model AI applications such as video conferencing\nand gaming is giving rise to a new class of workloads: real-time generative AI\n(RTGen). These workloads combine the compute intensity and dynamic execution\npatterns of generative models with the stringent latency and concurrency\nconstraints of real-time inference. To meet the diverse demands of RTGen\nworkloads, modern edge platforms increasingly adopt heterogeneous\nsystem-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite\nthe potential of heterogeneous SoC, the scheduling space complexity and\nperformance implications of RTGen workloads on such platforms remain\nunderexplored. In this work, we perform a comprehensive characterization of\nRTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct\nrealistic multi-model scenarios inspired by industry use cases and profile\nmodel performance across all available backends. Using this data, we evaluate\nfive scheduling policies and their impact on both real-time metrics (e.g.,\ndeadline violation rate) and LLM performance (e.g., time-to-first-token and\ntokens-per-second). Our results show that scheduling decisions significantly\naffect workload performance (e.g., leading to a 41.7% difference in deadline\nviolation rates on average), and highlight the need for scheduling strategies\nthat are aware of workload dynamics and hardware heterogeneity. Our findings\nunderscore the importance of workload-aware, dynamic heterogeneous scheduling\nin enabling high-performance, on-device RTGen applications.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5b9e\u65f6\u751f\u6210AI\uff08RTGen\uff09\u5de5\u4f5c\u8d1f\u8f7d\u5728\u5f02\u6784SoC\u4e0a\u7684\u8c03\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e0d\u540c\u8c03\u5ea6\u7b56\u7565\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5b9e\u65f6\u751f\u6210AI\u5de5\u4f5c\u8d1f\u8f7d\u7ed3\u5408\u4e86\u751f\u6210\u6a21\u578b\u7684\u52a8\u6001\u6267\u884c\u6a21\u5f0f\u4e0e\u5b9e\u65f6\u63a8\u7406\u7684\u4e25\u683c\u5ef6\u8fdf\u8981\u6c42\uff0c\u4f46\u5f02\u6784SoC\u4e0a\u7684\u8c03\u5ea6\u590d\u6742\u6027\u548c\u6027\u80fd\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5728AMD Ryzen AI\u5f02\u6784SoC\u4e0a\u6784\u5efa\u591a\u6a21\u578b\u573a\u666f\uff0c\u5206\u6790\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u4e94\u79cd\u8c03\u5ea6\u7b56\u7565\u5bf9\u5b9e\u65f6\u6307\u6807\u548cLLM\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8c03\u5ea6\u51b3\u7b56\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff08\u5982\u5e73\u574741.7%\u7684\u622a\u6b62\u65f6\u95f4\u8fdd\u89c4\u7387\u5dee\u5f02\uff09\uff0c\u9700\u52a8\u6001\u5f02\u6784\u8c03\u5ea6\u7b56\u7565\u4ee5\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u548c\u786c\u4ef6\u5dee\u5f02\u3002", "conclusion": "\u52a8\u6001\u5f02\u6784\u8c03\u5ea6\u662f\u5b9e\u73b0\u9ad8\u6027\u80fd\u8bbe\u5907\u7aefRTGen\u5e94\u7528\u7684\u5173\u952e\u3002"}}
{"id": "2507.15782", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15782", "abs": "https://arxiv.org/abs/2507.15782", "authors": ["Ruochu Yang", "Yu Zhou", "Fumin Zhang", "Mengxue Hou"], "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs", "comment": null, "summary": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Inter-LLM\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u591a\u5bf9\u8c61\u6536\u96c6\u4efb\u52a1\u4e2d\u7684\u957f\u65f6\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u5904\u7406\u5f00\u653e\u96c6\u5bf9\u8c61\u548c\u5927\u578b\u73af\u5883\u5bfc\u822a\u65f6\u7f3a\u4e4f\u4eba\u7c7b\u667a\u80fd\uff0c\u5c24\u5176\u662f\u5728\u591a\u5bf9\u8c61\u6536\u96c6\u4efb\u52a1\u4e2d\u9762\u4e34\u957f\u65f6\u89c4\u5212\u548c\u5de8\u5927\u72b6\u6001\u7a7a\u95f4\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u6a21\u6001\u52a8\u4f5c\u6210\u672c\u76f8\u4f3c\u6027\u51fd\u6570\uff0c\u7ed3\u5408LLM\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u5b9e\u73b0\u5386\u53f2\u53cd\u5c04\u4e0e\u672a\u6765\u9884\u6d4b\u7684\u5e73\u8861\u4f18\u5316\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0cInter-LLM\u7b97\u6cd5\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u3001\u6210\u529f\u7387\u548c\u6210\u672c\u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8630%\u3002", "conclusion": "Inter-LLM\u7b97\u6cd5\u5728\u591a\u5bf9\u8c61\u6536\u96c6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5bb6\u5ead\u673a\u5668\u4eba\u7684\u667a\u80fd\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14697", "categories": ["cs.CV", "I.4.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14697", "abs": "https://arxiv.org/abs/2507.14697", "authors": ["Zhiwei Zhang", "Zi Ye", "Yibin Wen", "Shuai Yuan", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset", "comment": "38 pages, 18 figures, submitted to NeurIPS 2025", "summary": "Agricultural parcels serve as basic units for conducting agricultural\npractices and applications, which is vital for land ownership registration,\nfood security assessment, soil erosion monitoring, etc. However, existing\nagriculture parcel extraction studies only focus on mid-resolution mapping or\nregular plain farmlands while lacking representation of complex terraced\nterrains due to the demands of precision agriculture.In this paper, we\nintroduce a more fine-grained terraced parcel dataset named GTPBD (Global\nTerraced Parcel and Boundary Dataset), which is the first fine-grained dataset\ncovering major worldwide terraced regions with more than 200,000 complex\nterraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution\nimages with three-level labels, including pixel-level boundary labels, mask\nlabels, and parcel labels. It covers seven major geographic zones in China and\ntranscontinental climatic regions around the world.Compared to the existing\ndatasets, the GTPBD dataset brings considerable challenges due to the: (1)\nterrain diversity; (2) complex and irregular parcel objects; and (3) multiple\ndomain styles. Our proposed GTPBD dataset is suitable for four different tasks,\nincluding semantic segmentation, edge detection, terraced parcel extraction,\nand unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the\nGTPBD dataset on eight semantic segmentation methods, four edge extraction\nmethods, three parcel extraction methods, and five UDA methods, along with a\nmulti-dimensional evaluation framework integrating pixel-level and object-level\nmetrics. GTPBD fills a critical gap in terraced remote sensing research,\nproviding a basic infrastructure for fine-grained agricultural terrain analysis\nand cross-scenario knowledge transfer.", "AI": {"tldr": "GTPBD\u662f\u5168\u7403\u9996\u4e2a\u7cbe\u7ec6\u68af\u7530\u5730\u5757\u6570\u636e\u96c6\uff0c\u8986\u76d6\u5168\u7403\u4e3b\u8981\u68af\u7530\u533a\u57df\uff0c\u5305\u542b20\u591a\u4e07\u4e2a\u590d\u6742\u68af\u7530\u5730\u5757\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9065\u611f\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u5730\u5757\u63d0\u53d6\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4e2d\u5206\u8fa8\u7387\u6216\u5e73\u539f\u519c\u7530\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u68af\u7530\u5730\u5f62\u7684\u7cbe\u7ec6\u8868\u8fbe\uff0c\u96be\u4ee5\u6ee1\u8db3\u7cbe\u51c6\u519c\u4e1a\u9700\u6c42\u3002", "method": "\u63d0\u51faGTPBD\u6570\u636e\u96c6\uff0c\u5305\u542b47,537\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u63d0\u4f9b\u50cf\u7d20\u7ea7\u8fb9\u754c\u3001\u63a9\u7801\u548c\u5730\u5757\u4e09\u7ea7\u6807\u6ce8\uff0c\u8986\u76d6\u4e2d\u56fd\u4e03\u5927\u5730\u7406\u533a\u57df\u548c\u5168\u7403\u8de8\u6c14\u5019\u5e26\u3002", "result": "GTPBD\u5728\u8bed\u4e49\u5206\u5272\u3001\u8fb9\u7f18\u68c0\u6d4b\u3001\u5730\u5757\u63d0\u53d6\u548c\u65e0\u76d1\u7763\u57df\u9002\u5e94\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u68af\u7530\u9065\u611f\u7814\u7a76\u7684\u7a7a\u767d\u3002", "conclusion": "GTPBD\u4e3a\u7cbe\u7ec6\u519c\u4e1a\u5730\u5f62\u5206\u6790\u548c\u8de8\u573a\u666f\u77e5\u8bc6\u8fc1\u79fb\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.14722", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14722", "abs": "https://arxiv.org/abs/2507.14722", "authors": ["Mat\u011bj Kripner", "Michal \u0160ustr", "Milan Straka"], "title": "LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4", "comment": null, "summary": "Automated theorem proving (ATP) has been a classical problem in artificial\nintelligence since its inception, yet it remains challenging due to its vast\nstate and action space. Large language models (LLMs) have recently emerged as a\npromising heuristic for ATP, but they lack correctness guarantees and thus\nrequire interaction with a proof verifier. Such interactions typically follow\none of two approaches: black-box interaction, which does not utilize\nintermediate proof states, or white-box approaches, which allow for incremental\nproof construction and examination of intermediate states. While black-box\napproaches have directly benefited from recent LLM advances, white-box methods\nhave comparatively lagged behind. In this paper, we address this gap by\nintroducing LeanTree, which consists of (i) a tool built in the Lean 4 language\nthat factorizes complex proof states into simpler, independent branches, and\n(ii) a dataset of these factorized intermediate states. Our white-box tooling\noffers several advantages over black-box approaches: it simplifies evaluation,\nreduces necessary context, generates richer training data, enables parallel\nsearch across multiple states, supports efficient reuse of states, and provides\nfeedback in case of errors. Our preliminary results hint that white-box\napproaches outperform black-box alternatives in some settings.", "AI": {"tldr": "LeanTree\u662f\u4e00\u79cd\u57fa\u4e8eLean 4\u8bed\u8a00\u7684\u767d\u76d2\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u89e3\u590d\u6742\u8bc1\u660e\u72b6\u6001\u4e3a\u72ec\u7acb\u5206\u652f\uff0c\u5e76\u63d0\u4f9b\u6570\u636e\u96c6\uff0c\u4f18\u4e8e\u9ed1\u76d2\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\uff08ATP\uff09\u56e0\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u5e9e\u5927\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u767d\u76d2\u65b9\u6cd5\u5728LLM\u4e2d\u7684\u5e94\u7528\u76f8\u5bf9\u6ede\u540e\u3002", "method": "\u5f00\u53d1\u4e86LeanTree\u5de5\u5177\uff0c\u5206\u89e3\u8bc1\u660e\u72b6\u6001\u4e3a\u72ec\u7acb\u5206\u652f\uff0c\u5e76\u63d0\u4f9b\u6570\u636e\u96c6\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u767d\u76d2\u65b9\u6cd5\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u4f18\u4e8e\u9ed1\u76d2\u65b9\u6cd5\u3002", "conclusion": "\u767d\u76d2\u5de5\u5177\u5728\u7b80\u5316\u8bc4\u4f30\u3001\u751f\u6210\u4e30\u5bcc\u8bad\u7ec3\u6570\u636e\u7b49\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3aATP\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.15833", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15833", "abs": "https://arxiv.org/abs/2507.15833", "authors": ["Ian Chuang", "Andrew Lee", "Dechen Gao", "Jinyu Zou", "Iman Soltani"], "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers", "comment": "13 pages, 10 figures", "summary": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u4e3b\u52a8\u6ce8\u89c6\u884c\u4e3a\u6765\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6ce8\u89c6\u6570\u636e\u7684\u673a\u5668\u4eba\u7b56\u7565\u6846\u67b6\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u901a\u8fc7\u4e3b\u52a8\u6ce8\u89c6\u9ad8\u6548\u5904\u7406\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff0c\u800c\u673a\u5668\u4eba\u7cfb\u7edf\u901a\u5e38\u88ab\u52a8\u5904\u7406\u56fe\u50cf\u3002\u7814\u7a76\u65e8\u5728\u5c06\u4eba\u7c7b\u4e3b\u52a8\u6ce8\u89c6\u673a\u5236\u5f15\u5165\u673a\u5668\u4eba\u89c6\u89c9\uff0c\u4ee5\u63d0\u5347\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u6ce8\u89c6\u6570\u636e\u7684\u673a\u5668\u4eba\u7b56\u7565\u6846\u67b6\uff0c\u5305\u62ec\u6ce8\u89c6\u9884\u6d4b\u548c\u52a8\u4f5c\u8054\u5408\u9884\u6d4b\u7684\u4e24\u79cd\u65b9\u6cd5\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eViT\u7684\u7126\u70b9\u5316\u56fe\u50cf\u5904\u7406\u6280\u672f\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u7684\u6027\u80fd\u548c\u5bf9\u6297\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u4eba\u7c7b\u542f\u53d1\u7684\u89c6\u89c9\u5904\u7406\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5c55\u793a\u4e86\u4e3b\u52a8\u6ce8\u89c6\u673a\u5236\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14738", "abs": "https://arxiv.org/abs/2507.14738", "authors": ["Jeannie She", "Katie Spivakovsky"], "title": "MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness,\naffecting over 100 million people worldwide. In the United States, individuals\nfrom lower-income communities face a higher risk of progressing to advanced\nstages before diagnosis, largely due to limited access to screening. Comorbid\nconditions further accelerate disease progression. We propose MultiRetNet, a\nnovel pipeline combining retinal imaging, socioeconomic factors, and\ncomorbidity profiles to improve DR staging accuracy, integrated with a clinical\ndeferral system for a clinical human-in-the-loop implementation. We experiment\nwith three multimodal fusion methods and identify fusion through a fully\nconnected layer as the most versatile methodology. We synthesize adversarial,\nlow-quality images and use contrastive learning to train the deferral system,\nguiding the model to identify out-of-distribution samples that warrant\nclinician review. By maintaining diagnostic accuracy on suboptimal images and\nintegrating critical health data, our system can improve early detection,\nparticularly in underserved populations where advanced DR is often first\nidentified. This approach may reduce healthcare costs, increase early detection\nrates, and address disparities in access to care, promoting healthcare equity.", "AI": {"tldr": "MultiRetNet\u7ed3\u5408\u89c6\u7f51\u819c\u6210\u50cf\u3001\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u5171\u75c5\u60c5\u51b5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u4e34\u5e8a\u5ef6\u8fdf\u7cfb\u7edf\uff0c\u63d0\u9ad8\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u5206\u671f\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u670d\u52a1\u4e8e\u4f4e\u6536\u5165\u4eba\u7fa4\u3002", "motivation": "\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u662f\u5168\u7403\u53ef\u9884\u9632\u6027\u5931\u660e\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u4f4e\u6536\u5165\u4eba\u7fa4\u56e0\u7b5b\u67e5\u673a\u4f1a\u6709\u9650\uff0c\u66f4\u5bb9\u6613\u8fdb\u5c55\u5230\u665a\u671f\u3002\u5171\u75c5\u60c5\u51b5\u52a0\u901f\u75be\u75c5\u53d1\u5c55\u3002", "method": "\u63d0\u51faMultiRetNet\uff0c\u7ed3\u5408\u4e09\u79cd\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u878d\u5408\u6570\u636e\uff0c\u5e76\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u5ef6\u8fdf\u7cfb\u7edf\u8bc6\u522b\u9700\u4e34\u5e8a\u590d\u67e5\u7684\u5f02\u5e38\u6837\u672c\u3002", "result": "\u7cfb\u7edf\u5728\u4f4e\u8d28\u91cf\u56fe\u50cf\u4e0a\u4fdd\u6301\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u6574\u5408\u5173\u952e\u5065\u5eb7\u6570\u636e\uff0c\u6709\u671b\u63d0\u9ad8\u65e9\u671f\u68c0\u6d4b\u7387\uff0c\u51cf\u5c11\u533b\u7597\u6210\u672c\u3002", "conclusion": "MultiRetNet\u53ef\u6539\u5584\u65e9\u671f\u68c0\u6d4b\uff0c\u5c24\u5176\u670d\u52a1\u4e8e\u533b\u7597\u8d44\u6e90\u4e0d\u8db3\u4eba\u7fa4\uff0c\u4fc3\u8fdb\u533b\u7597\u516c\u5e73\u3002"}}
{"id": "2507.14725", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14725", "abs": "https://arxiv.org/abs/2507.14725", "authors": ["Anushka Tiwari", "Sayantan Pal", "Rohini K. Srihari", "Kaiyi Ji"], "title": "Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding", "comment": null, "summary": "Prompt-based continual learning (CL) offers a parameter-efficient way to\nadapt large language models (LLMs) across task sequences. However, most\nexisting methods assume task-aware inference and maintain a growing list of\ntask-specific prompts, which limits scalability and hides latent forgetting. In\nthis work, we introduce GRID, a unified framework that addresses two key\nlimitations: (1) latent forgetting under task-agnostic inference, and (2)\nprompt memory explosion as task sequences grow. GRID integrates a task-aware\ndecoding mechanism that improves backward transfer by leveraging representative\ninputs, automatic task identification, and constrained decoding. Additionally,\nwe propose a gradient-based prompt selection strategy that compresses less\ninformative prompts into a single aggregated representation, enabling scalable\nand memory-efficient lifelong learning. Extensive experiments across\nshort-sequence, long-sequence, and negative transfer benchmarks show that GRID\nsignificantly improves backward transfer, achieves competitive forward\ntransfer, and reduces forgotten tasks by up to 80\\%, outperforming\nstate-of-the-art methods on T5 and Flan-T5 backbones.", "AI": {"tldr": "GRID\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4efb\u52a1\u65e0\u5173\u63a8\u7406\u4e0b\u7684\u6f5c\u5728\u9057\u5fd8\u548c\u63d0\u793a\u5185\u5b58\u7206\u70b8\u95ee\u9898\uff0c\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u89e3\u7801\u548c\u68af\u5ea6\u63d0\u793a\u9009\u62e9\u7b56\u7565\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u4efb\u52a1\u611f\u77e5\u63a8\u7406\u5e76\u7ef4\u62a4\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5217\u8868\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u5e76\u9690\u85cf\u4e86\u6f5c\u5728\u9057\u5fd8\u3002", "method": "GRID\u96c6\u6210\u4e86\u4efb\u52a1\u611f\u77e5\u89e3\u7801\u673a\u5236\u548c\u68af\u5ea6\u63d0\u793a\u9009\u62e9\u7b56\u7565\uff0c\u4f18\u5316\u4e86\u5411\u540e\u4f20\u9012\u5e76\u538b\u7f29\u63d0\u793a\u5185\u5b58\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGRID\u663e\u8457\u63d0\u5347\u5411\u540e\u4f20\u9012\u6027\u80fd\uff0c\u51cf\u5c11\u9057\u5fd8\u4efb\u52a1\u8fbe80%\uff0c\u5728T5\u548cFlan-T5\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GRID\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14743", "abs": "https://arxiv.org/abs/2507.14743", "authors": ["Joseph Raj Vishal", "Rutuja Patil", "Manas Srinivas Gowda", "Katha Naik", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "comment": null, "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA", "AI": {"tldr": "InterAct VideoQA\u6570\u636e\u96c6\u65e8\u5728\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u6a21\u578b\u5728\u590d\u6742\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b8\u5c0f\u65f6\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u548c25,000\u4e2aQA\u5bf9\u3002", "motivation": "\u73b0\u6709VideoQA\u6a21\u578b\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u591a\u4e8b\u4ef6\u5e76\u53d1\u95ee\u9898\uff0c\u9700\u9886\u57df\u4e13\u7528\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u771f\u5b9e\u4ea4\u901a\u89c6\u9891\u5e76\u6807\u6ce8QA\u5bf9\uff0c\u8bc4\u4f30\u5e76\u5fae\u8c03\u73b0\u6709\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728InterAct VideoQA\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5fae\u8c03\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u9886\u57df\u4e13\u7528\u6570\u636e\u96c6\u5bf9\u63d0\u5347VideoQA\u6a21\u578b\u5728\u4ea4\u901a\u76d1\u63a7\u4e2d\u7684\u8868\u73b0\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.14736", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14736", "abs": "https://arxiv.org/abs/2507.14736", "authors": ["Rafa\u0142 Surdej", "Micha\u0142 Bortkiewicz", "Alex Lewandowski", "Mateusz Ostaszewski", "Clare Lyle"], "title": "Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning", "comment": "Accepted for oral presentation at CoLLAs 2025", "summary": "Trainable activation functions, whose parameters are optimized alongside\nnetwork weights, offer increased expressivity compared to fixed activation\nfunctions. Specifically, trainable activation functions defined as ratios of\npolynomials (rational functions) have been proposed to enhance plasticity in\nreinforcement learning. However, their impact on training stability remains\nunclear. In this work, we study trainable rational activations in both\nreinforcement and continual learning settings. We find that while their\nflexibility enhances adaptability, it can also introduce instability, leading\nto overestimation in RL and feature collapse in longer continual learning\nscenarios. Our main result is demonstrating a trade-off between expressivity\nand plasticity in rational activations. To address this, we propose a\nconstrained variant that structurally limits excessive output scaling while\npreserving adaptability. Experiments across MetaWorld and DeepMind Control\nSuite (DMC) environments show that our approach improves training stability and\nperformance. In continual learning benchmarks, including MNIST with reshuffled\nlabels and Split CIFAR-100, we reveal how different constraints affect the\nbalance between expressivity and long-term retention. While preliminary\nexperiments in discrete action domains (e.g., Atari) did not show similar\ninstability, this suggests that the trade-off is particularly relevant for\ncontinuous control. Together, our findings provide actionable design principles\nfor robust and adaptable trainable activations in dynamic, non-stationary\nenvironments. Code available at:\nhttps://github.com/special114/rl_rational_plasticity.", "AI": {"tldr": "\u7814\u7a76\u4e86\u53ef\u8bad\u7ec3\u6709\u7406\u6fc0\u6d3b\u51fd\u6570\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u7075\u6d3b\u6027\u4e0e\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u53d8\u4f53\u4ee5\u6539\u5584\u7a33\u5b9a\u6027\u3002", "motivation": "\u63a2\u7d22\u53ef\u8bad\u7ec3\u6709\u7406\u6fc0\u6d3b\u51fd\u6570\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5176\u7075\u6d3b\u6027\u4e0e\u8bad\u7ec3\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea6\u675f\u53d8\u4f53\u7684\u6709\u7406\u6fc0\u6d3b\u51fd\u6570\uff0c\u901a\u8fc7\u9650\u5236\u8f93\u51fa\u7f29\u653e\u6765\u5e73\u8861\u8868\u8fbe\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728MetaWorld\u548cDeepMind Control Suite\u73af\u5883\u4e2d\uff0c\u7ea6\u675f\u53d8\u4f53\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd\uff1b\u5728\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u7ea6\u675f\u5f71\u54cd\u4e86\u8868\u8fbe\u6027\u4e0e\u957f\u671f\u8bb0\u5fc6\u7684\u5e73\u8861\u3002", "conclusion": "\u53ef\u8bad\u7ec3\u6709\u7406\u6fc0\u6d3b\u51fd\u6570\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u7075\u6d3b\u6027\u4e0e\u7a33\u5b9a\u6027\u6743\u8861\uff0c\u7ea6\u675f\u8bbe\u8ba1\u80fd\u6709\u6548\u6539\u5584\u5176\u6027\u80fd\u3002"}}
{"id": "2507.14784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14784", "abs": "https://arxiv.org/abs/2507.14784", "authors": ["Xinxin Dong", "Baoyun Peng", "Haokai Ma", "Yufei Wang", "Zixuan Dong", "Fei Hu", "Xiaodong Wang"], "title": "LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering", "comment": null, "summary": "Video Question Answering (VideoQA) requires identifying sparse critical\nmoments in long videos and reasoning about their causal relationships to answer\nsemantically complex questions. While recent advances in multimodal learning\nhave improved alignment and fusion, current approaches remain limited by two\nprevalent but fundamentally flawed strategies: (1) task-agnostic sampling\nindiscriminately processes all frames, overwhelming key events with irrelevant\ncontent; and (2) heuristic retrieval captures superficial patterns but misses\ncausal-temporal structures needed for complex reasoning. To address these\nchallenges, we introduce LeAdQA, an innovative approach that bridges these gaps\nthrough synergizing causal-aware query refinement with fine-grained visual\ngrounding. Our method first leverages LLMs to reformulate question-option\npairs, resolving causal ambiguities and sharpening temporal focus. These\nrefined queries subsequently direct a temporal grounding model to precisely\nretrieve the most salient segments, complemented by an adaptive fusion\nmechanism dynamically integrating the evidence to maximize relevance. The\nintegrated visual-textual cues are then processed by an MLLM to generate\naccurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and\nNExT-GQA demonstrate that our method's precise visual grounding substantially\nenhances the understanding of video-question relationships, achieving\nstate-of-the-art (SOTA) performance on complex reasoning tasks while\nmaintaining computational efficiency.", "AI": {"tldr": "LeAdQA\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u611f\u77e5\u67e5\u8be2\u4f18\u5316\u548c\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9a\u4f4d\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u95ee\u7b54\u4e2d\u5173\u952e\u5e27\u7a00\u758f\u548c\u56e0\u679c\u63a8\u7406\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u95ee\u7b54\u65b9\u6cd5\u5b58\u5728\u4efb\u52a1\u65e0\u5173\u91c7\u6837\u548c\u542f\u53d1\u5f0f\u68c0\u7d22\u7684\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5173\u952e\u4e8b\u4ef6\u548c\u56e0\u679c\u7ed3\u6784\u3002", "method": "\u5229\u7528LLM\u4f18\u5316\u95ee\u9898-\u9009\u9879\u5bf9\uff0c\u7ed3\u5408\u65f6\u95f4\u5b9a\u4f4d\u6a21\u578b\u7cbe\u786e\u68c0\u7d22\u5173\u952e\u7247\u6bb5\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u6574\u5408\u8bc1\u636e\u3002", "result": "\u5728NExT-QA\u3001IntentQA\u548cNExT-GQA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "LeAdQA\u901a\u8fc7\u56e0\u679c\u611f\u77e5\u548c\u89c6\u89c9\u5b9a\u4f4d\u7684\u534f\u540c\u4f5c\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u95ee\u7b54\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.14740", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14740", "abs": "https://arxiv.org/abs/2507.14740", "authors": ["Andrew Wang", "Elisa Nguyen", "Runshi Yang", "Juhan Bae", "Sheila A. McIlraith", "Roger Grosse"], "title": "Better Training Data Attribution via Better Inverse Hessian-Vector Products", "comment": "28 pages, 4 figures", "summary": "Training data attribution (TDA) provides insights into which training data is\nresponsible for a learned model behavior. Gradient-based TDA methods such as\ninfluence functions and unrolled differentiation both involve a computation\nthat resembles an inverse Hessian-vector product (iHVP), which is difficult to\napproximate efficiently. We introduce an algorithm (ASTRA) which uses the\nEKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP\napproximation for TDA. ASTRA is easy to tune, requires fewer iterations than\nNeumann series iterations, and is more accurate than EKFAC-based\napproximations. Using ASTRA, we show that improving the accuracy of the iHVP\napproximation can significantly improve TDA performance.", "AI": {"tldr": "ASTRA\u7b97\u6cd5\u901a\u8fc7EKFAC\u9884\u6761\u4ef6\u5668\u548cNeumann\u7ea7\u6570\u8fed\u4ee3\uff0c\u9ad8\u6548\u8fd1\u4f3c\u9006Hessian-\u5411\u91cf\u79ef\uff08iHVP\uff09\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6570\u636e\u5f52\u56e0\uff08TDA\uff09\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u68af\u5ea6TDA\u65b9\u6cd5\uff08\u5982\u5f71\u54cd\u51fd\u6570\u548c\u5c55\u5f00\u5fae\u5206\uff09\u8ba1\u7b97iHVP\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u8fd1\u4f3c\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faASTRA\u7b97\u6cd5\uff0c\u7ed3\u5408EKFAC\u9884\u6761\u4ef6\u5668\u548cNeumann\u7ea7\u6570\u8fed\u4ee3\uff0c\u4f18\u5316iHVP\u8fd1\u4f3c\u3002", "result": "ASTRA\u8c03\u53c2\u7b80\u5355\u3001\u8fed\u4ee3\u6b21\u6570\u5c11\u3001\u7cbe\u5ea6\u9ad8\uff0c\u663e\u8457\u63d0\u5347TDA\u6027\u80fd\u3002", "conclusion": "ASTRA\u4e3aTDA\u63d0\u4f9b\u9ad8\u6548iHVP\u8fd1\u4f3c\uff0c\u9a8c\u8bc1\u4e86iHVP\u7cbe\u5ea6\u5bf9TDA\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.14809", "categories": ["cs.CV", "cs.MM", "cs.RO", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14809", "abs": "https://arxiv.org/abs/2507.14809", "authors": ["Zesen Zhong", "Duomin Zhang", "Yijia Li"], "title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix", "comment": "9 pages including appendix, 5 tables, 8 figures, to be submitted to\n  WACV 2026", "summary": "Predicting future motion trajectories is a critical capability across domains\nsuch as robotics, autonomous systems, and human activity forecasting, enabling\nsafer and more intelligent decision-making. This paper proposes a novel,\nefficient, and lightweight approach for robot action prediction, offering\nsignificantly reduced computational cost and inference latency compared to\nconventional video prediction models. Importantly, it pioneers the adaptation\nof the InstructPix2Pix model for forecasting future visual frames in robotic\ntasks, extending its utility beyond static image editing. We implement a deep\nlearning-based visual prediction framework that forecasts what a robot will\nobserve 100 frames (10 seconds) into the future, given a current image and a\ntextual instruction. We repurpose and fine-tune the InstructPix2Pix model to\naccept both visual and textual inputs, enabling multimodal future frame\nprediction. Experiments on the RoboTWin dataset (generated based on real-world\nscenarios) demonstrate that our method achieves superior SSIM and PSNR compared\nto state-of-the-art baselines in robot action prediction tasks. Unlike\nconventional video prediction models that require multiple input frames, heavy\ncomputation, and slow inference latency, our approach only needs a single image\nand a text prompt as input. This lightweight design enables faster inference,\nreduced GPU demands, and flexible multimodal control, particularly valuable for\napplications like robotics and sports motion trajectory analytics, where motion\ntrajectory precision is prioritized over visual fidelity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u9884\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528InstructPix2Pix\u6a21\u578b\u5b9e\u73b0\u591a\u6a21\u6001\u672a\u6765\u5e27\u9884\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "motivation": "\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\u8f68\u8ff9\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5ef6\u8fdf\u5927\u3002", "method": "\u57fa\u4e8eInstructPix2Pix\u6a21\u578b\uff0c\u7ed3\u5408\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u5b9e\u73b0\u5355\u5e27\u8f93\u5165\u7684\u591a\u6a21\u6001\u672a\u6765\u5e27\u9884\u6d4b\u3002", "result": "\u5728RoboTWin\u6570\u636e\u96c6\u4e0a\uff0cSSIM\u548cPSNR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8f7b\u91cf\u5316\u3001\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5bf9\u8fd0\u52a8\u8f68\u8ff9\u7cbe\u5ea6\u8981\u6c42\u9ad8\u7684\u573a\u666f\uff0c\u5982\u673a\u5668\u4eba\u548c\u8fd0\u52a8\u5206\u6790\u3002"}}
{"id": "2507.14787", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14787", "abs": "https://arxiv.org/abs/2507.14787", "authors": ["Xi Xiao", "Aristeidis Tsaris", "Anika Tabassum", "John Lagergren", "Larry M. York", "Tianyang Wang", "Xiao Wang"], "title": "FOCUS: Fused Observation of Channels for Unveiling Spectra", "comment": null, "summary": "Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous\nwavelength bands, making it a powerful tool in biology, agriculture, and\nenvironmental monitoring. However, interpreting Vision Transformers (ViTs) in\nthis setting remains largely unexplored due to two key challenges: (1) existing\nsaliency methods struggle to capture meaningful spectral cues, often collapsing\nattention onto the class token, and (2) full-spectrum ViTs are computationally\nprohibitive for interpretability, given the high-dimensional nature of HSI\ndata. We present FOCUS, the first framework that enables reliable and efficient\nspatial-spectral interpretability for frozen ViTs. FOCUS introduces two core\ncomponents: class-specific spectral prompts that guide attention toward\nsemantically meaningful wavelength groups, and a learnable [SINK] token trained\nwith an attraction loss to absorb noisy or redundant attention. Together, these\ndesigns make it possible to generate stable and interpretable 3D saliency maps\nand spectral importance curves in a single forward pass, without any gradient\nbackpropagation or backbone modification. FOCUS improves band-level IoU by 15\npercent, reduces attention collapse by over 40 percent, and produces saliency\nresults that align closely with expert annotations. With less than 1 percent\nparameter overhead, our method makes high-resolution ViT interpretability\npractical for real-world hyperspectral applications, bridging a long-standing\ngap between black-box modeling and trustworthy HSI decision-making.", "AI": {"tldr": "FOCUS\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u5bf9\u51bb\u7ed3ViT\u7684\u53ef\u9760\u9ad8\u6548\u7a7a\u95f4-\u5149\u8c31\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86HSI\u6570\u636e\u4e2dViT\u89e3\u91ca\u7684\u4e24\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u663e\u8457\u6027\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6709\u610f\u4e49\u7684\u9891\u8c31\u7ebf\u7d22\uff0c\u4e14\u5168\u9891\u8c31ViT\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u963b\u788d\u4e86HSI\u6570\u636e\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "FOCUS\u5f15\u5165\u7c7b\u7279\u5b9a\u9891\u8c31\u63d0\u793a\u548c\u53ef\u5b66\u4e60\u7684[SINK]\u4ee4\u724c\uff0c\u901a\u8fc7\u5438\u5f15\u635f\u5931\u51cf\u5c11\u566a\u58f0\u6ce8\u610f\u529b\u3002", "result": "FOCUS\u5c06\u6ce2\u6bb5\u7ea7IoU\u63d0\u9ad815%\uff0c\u51cf\u5c11\u6ce8\u610f\u529b\u5d29\u6e8340%\uff0c\u4e14\u4e0e\u4e13\u5bb6\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "FOCUS\u4ee5\u4e0d\u52301%\u7684\u53c2\u6570\u5f00\u9500\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387ViT\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u586b\u8865\u4e86\u9ed1\u76d2\u5efa\u6a21\u4e0e\u53ef\u4fe1HSI\u51b3\u7b56\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.14744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14744", "abs": "https://arxiv.org/abs/2507.14744", "authors": ["Mustafa Cavus", "Jan N. van Rijn", "Przemys\u0142aw Biecek"], "title": "Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML", "comment": "Accepted at 28th International Conference on Discovery Science 2025", "summary": "Automated machine learning systems efficiently streamline model selection but\noften focus on a single best-performing model, overlooking explanation\nuncertainty, an essential concern in human centered explainable AI. To address\nthis, we propose a novel framework that incorporates model multiplicity into\nexplanation generation by aggregating partial dependence profiles (PDP) from a\nset of near optimal models, known as the Rashomon set. The resulting Rashomon\nPDP captures interpretive variability and highlights areas of disagreement,\nproviding users with a richer, uncertainty aware view of feature effects. To\nevaluate its usefulness, we introduce two quantitative metrics, the coverage\nrate and the mean width of confidence intervals, to evaluate the consistency\nbetween the standard PDP and the proposed Rashomon PDP. Experiments on 35\nregression datasets from the OpenML CTR23 benchmark suite show that in most\ncases, the Rashomon PDP covers less than 70% of the best model's PDP,\nunderscoring the limitations of single model explanations. Our findings suggest\nthat Rashomon PDP improves the reliability and trustworthiness of model\ninterpretations by adding additional information that would otherwise be\nneglected. This is particularly useful in high stakes domains where\ntransparency and confidence are critical.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408Rashomon\u96c6\u5408\u4e2d\u591a\u4e2a\u8fd1\u4f18\u6a21\u578b\u7684PDP\uff0c\u751f\u6210\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7279\u5f81\u6548\u5e94\u89c6\u56fe\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5316\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4ec5\u5173\u6ce8\u5355\u4e00\u6700\u4f18\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u8fd9\u5bf9\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684XAI\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u805a\u5408Rashomon\u96c6\u5408\u4e2d\u591a\u4e2a\u8fd1\u4f18\u6a21\u578b\u7684PDP\uff0c\u751f\u6210Rashomon PDP\uff0c\u6355\u6349\u89e3\u91ca\u53d8\u5f02\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRashomon PDP\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u8986\u76d6\u4e0d\u5230\u6700\u4f73\u6a21\u578bPDP\u768470%\uff0c\u7a81\u663e\u5355\u4e00\u6a21\u578b\u89e3\u91ca\u7684\u5c40\u9650\u6027\u3002", "conclusion": "Rashomon PDP\u901a\u8fc7\u8865\u5145\u88ab\u5ffd\u89c6\u7684\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002"}}
{"id": "2507.14850", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14850", "abs": "https://arxiv.org/abs/2507.14850", "authors": ["H. M. Sabbir Ahmad", "Ehsan Sabouni", "Alexander Wasilkoff", "Param Budhraja", "Zijian Guo", "Songyuan Zhang", "Chuchu Fan", "Christos Cassandras", "Wenchao Li"], "title": "Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems", "comment": null, "summary": "We address the problem of safe policy learning in multi-agent safety-critical\nautonomous systems. In such systems, it is necessary for each agent to meet the\nsafety requirements at all times while also cooperating with other agents to\naccomplish the task. Toward this end, we propose a safe Hierarchical\nMulti-Agent Reinforcement Learning (HMARL) approach based on Control Barrier\nFunctions (CBFs). Our proposed hierarchical approach decomposes the overall\nreinforcement learning problem into two levels learning joint cooperative\nbehavior at the higher level and learning safe individual behavior at the lower\nor agent level conditioned on the high-level policy. Specifically, we propose a\nskill-based HMARL-CBF algorithm in which the higher level problem involves\nlearning a joint policy over the skills for all the agents and the lower-level\nproblem involves learning policies to execute the skills safely with CBFs. We\nvalidate our approach on challenging environment scenarios whereby a large\nnumber of agents have to safely navigate through conflicting road networks.\nCompared with existing state of the art methods, our approach significantly\nimproves the safety achieving near perfect (within 5%) success/safety rate\nwhile also improving performance across all the environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08HMARL-CBF\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u5b89\u5168\u5173\u952e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u7b56\u7565\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u5728\u591a\u667a\u80fd\u4f53\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u9700\u8981\u59cb\u7ec8\u6ee1\u8db3\u5b89\u5168\u8981\u6c42\uff0c\u540c\u65f6\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u534f\u4f5c\u5b8c\u6210\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u5206\u89e3\u4e3a\u9ad8\u5c42\u5b66\u4e60\u8054\u5408\u534f\u4f5c\u884c\u4e3a\uff0c\u4f4e\u5c42\u5b66\u4e60\u57fa\u4e8e\u9ad8\u5c42\u7b56\u7565\u7684\u5b89\u5168\u4e2a\u4f53\u884c\u4e3a\u3002\u5177\u4f53\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6280\u80fd\u7684HMARL-CBF\u7b97\u6cd5\u3002", "result": "\u5728\u590d\u6742\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff08\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6210\u529f\u7387/\u5b89\u5168\u7387\uff09\uff0c\u5e76\u5728\u6240\u6709\u73af\u5883\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "HMARL-CBF\u65b9\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14790", "abs": "https://arxiv.org/abs/2507.14790", "authors": ["Wenbo Yue", "Chang Li", "Guoping Xu"], "title": "A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation", "comment": "6 pages, 6 figures", "summary": "In convolutional neural networks (CNNs), downsampling operations are crucial\nto model performance. Although traditional downsampling methods (such as\nmaximum pooling and cross-row convolution) perform well in feature aggregation,\nreceptive field expansion, and computational reduction, they may lead to the\nloss of key spatial information in semantic segmentation tasks, thereby\naffecting the pixel-by-pixel prediction accuracy.To this end, this study\nproposes a downsampling method based on information complementarity - Hybrid\nPooling Downsampling (HPD). The core is to replace the traditional method with\nMinMaxPooling, and effectively retain the light and dark contrast and detail\nfeatures of the image by extracting the maximum value information of the local\narea.Experiment on various CNN architectures on the ACDC and Synapse datasets\nshow that HPD outperforms traditional methods in segmentation performance, and\nincreases the DSC coefficient by 0.5% on average. The results show that the HPD\nmodule provides an efficient solution for semantic segmentation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u4e92\u8865\u7684\u4e0b\u91c7\u6837\u65b9\u6cd5HPD\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u4e22\u5931\u5173\u952e\u7a7a\u95f4\u4fe1\u606f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4e0b\u91c7\u6837\u65b9\u6cd5\uff08\u5982\u6700\u5927\u6c60\u5316\u548c\u8de8\u884c\u5377\u79ef\uff09\u5728\u7279\u5f81\u805a\u5408\u3001\u611f\u53d7\u91ce\u6269\u5c55\u548c\u8ba1\u7b97\u51cf\u5c11\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u53ef\u80fd\u5bfc\u81f4\u5173\u952e\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\uff0c\u5f71\u54cd\u50cf\u7d20\u7ea7\u9884\u6d4b\u7cbe\u5ea6\u3002", "method": "\u63d0\u51faHybrid Pooling Downsampling (HPD)\u65b9\u6cd5\uff0c\u7528MinMaxPooling\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u5c40\u90e8\u533a\u57df\u7684\u6700\u5927\u503c\u4fe1\u606f\uff0c\u6709\u6548\u4fdd\u7559\u56fe\u50cf\u7684\u660e\u6697\u5bf9\u6bd4\u548c\u7ec6\u8282\u7279\u5f81\u3002", "result": "\u5728ACDC\u548cSynapse\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHPD\u5728\u5206\u5272\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e73\u5747DSC\u7cfb\u6570\u63d0\u9ad8\u4e860.5%\u3002", "conclusion": "HPD\u6a21\u5757\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14746", "categories": ["cs.LG", "math.OC", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14746", "abs": "https://arxiv.org/abs/2507.14746", "authors": ["Bach Do", "Nafeezat A. Ajenifuja", "Taiwo A. Adebiyi", "Ruda Zhang"], "title": "Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization", "comment": null, "summary": "High-fidelity simulations and physical experiments are essential for\nengineering analysis and design. However, their high cost often limits their\napplications in two critical tasks: global sensitivity analysis (GSA) and\noptimization. This limitation motivates the common use of Gaussian processes\n(GPs) as proxy regression models to provide uncertainty-aware predictions based\non a limited number of high-quality observations. GPs naturally enable\nefficient sampling strategies that support informed decision-making under\nuncertainty by extracting information from a subset of possible functions for\nthe model of interest. Despite their popularity in machine learning and\nstatistics communities, sampling from GPs has received little attention in the\ncommunity of engineering optimization. In this paper, we present the\nformulation and detailed implementation of two notable sampling methods --\nrandom Fourier features and pathwise conditioning -- for generating posterior\nsamples from GPs. Alternative approaches are briefly described. Importantly, we\ndetail how the generated samples can be applied in GSA, single-objective\noptimization, and multi-objective optimization. We show successful applications\nof these sampling methods through a series of numerical examples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\uff0c\u7528\u4e8e\u652f\u6301\u5de5\u7a0b\u4f18\u5316\u4e2d\u7684\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u548c\u591a\u76ee\u6807\u4f18\u5316\u3002", "motivation": "\u9ad8\u4fdd\u771f\u6a21\u62df\u548c\u7269\u7406\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u5176\u5728\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u548c\u4f18\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u4ee3\u7406\u6a21\u578b\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\uff09\u6765\u63d0\u4f9b\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e24\u79cd\u91c7\u6837\u65b9\u6cd5\u2014\u2014\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\u548c\u8def\u5f84\u6761\u4ef6\u91c7\u6837\uff0c\u5e76\u7b80\u8981\u63cf\u8ff0\u4e86\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5c55\u793a\u4e86\u8fd9\u4e9b\u91c7\u6837\u65b9\u6cd5\u5728\u5168\u5c40\u654f\u611f\u6027\u5206\u6790\u3001\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002", "conclusion": "\u9ad8\u65af\u8fc7\u7a0b\u91c7\u6837\u65b9\u6cd5\u4e3a\u5de5\u7a0b\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15036", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15036", "abs": "https://arxiv.org/abs/2507.15036", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring", "comment": null, "summary": "Underwater image enhancement is vital for marine conservation, particularly\ncoral reef monitoring. However, AI-based enhancement models often face dataset\nbias, high computational costs, and lack of transparency, leading to potential\nmisinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware\nAI framework to address these challenges. EBA-AI leverages CLIP embeddings to\ndetect and mitigate dataset bias, ensuring balanced representation across\nvaried underwater environments. It also integrates adaptive processing to\noptimize energy efficiency, significantly reducing GPU usage while maintaining\ncompetitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100\nshow that while PSNR drops by a controlled 1.0 dB, computational savings enable\nreal-time feasibility for large-scale marine monitoring. Additionally,\nuncertainty estimation and explainability techniques enhance trust in AI-driven\nenvironmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,\nWaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing\nefficiency, fairness, and interpretability in underwater image processing. By\naddressing key limitations of AI-driven enhancement, this work contributes to\nsustainable, bias-aware, and computationally efficient marine conservation\nefforts. For interactive visualizations, animations, source code, and access to\nthe preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/", "AI": {"tldr": "EBA-AI\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f26\u7406\u548c\u504f\u89c1\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\uff0c\u89e3\u51b3\u6570\u636e\u96c6\u504f\u89c1\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u900f\u660e\u5ea6\u95ee\u9898\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u5bf9\u6d77\u6d0b\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709AI\u6a21\u578b\u5b58\u5728\u6570\u636e\u96c6\u504f\u89c1\u3001\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u7f3a\u4e4f\u900f\u660e\u5ea6\u7684\u95ee\u9898\u3002", "method": "EBA-AI\u5229\u7528CLIP\u5d4c\u5165\u68c0\u6d4b\u548c\u51cf\u8f7b\u6570\u636e\u96c6\u504f\u89c1\uff0c\u96c6\u6210\u81ea\u9002\u5e94\u5904\u7406\u4ee5\u4f18\u5316\u80fd\u6548\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\u3002", "result": "\u5728LSUI400\u3001Oceanex\u548cUIEB100\u6570\u636e\u96c6\u4e0a\uff0cEBA-AI\u5728PSNR\u4e0b\u964d1.0 dB\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u51cf\u5c11GPU\u4f7f\u7528\uff0c\u5b9e\u73b0\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "EBA-AI\u5728\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u53ef\u6301\u7eed\u7684\u6d77\u6d0b\u4fdd\u62a4\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14797", "abs": "https://arxiv.org/abs/2507.14797", "authors": ["Beier Zhu", "Ruoyu Wang", "Tong Zhao", "Hanwang Zhang", "Chi Zhang"], "title": "Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models", "comment": "To appear in ICCV 2025", "summary": "Diffusion models (DMs) have achieved state-of-the-art generative performance\nbut suffer from high sampling latency due to their sequential denoising nature.\nExisting solver-based acceleration methods often face image quality degradation\nunder a low-latency budget. In this paper, we propose the Ensemble Parallel\nDirection solver (dubbed as \\ours), a novel ODE solver that mitigates\ntruncation errors by incorporating multiple parallel gradient evaluations in\neach ODE step. Importantly, since the additional gradient computations are\nindependent, they can be fully parallelized, preserving low-latency sampling.\n  Our method optimizes a small set of learnable parameters in a distillation\nfashion, ensuring minimal training overhead.\n  In addition, our method can serve as a plugin to improve existing ODE\nsamplers. Extensive experiments on various image synthesis benchmarks\ndemonstrate the effectiveness of our \\ours~in achieving high-quality and\nlow-latency sampling. For example, at the same latency level of 5 NFE, EPD\nachieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26\non LSUN Bedroom, surpassing existing learning-based solvers by a significant\nmargin. Codes are available in https://github.com/BeierZhu/EPD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEPD\u7684\u65b0\u578bODE\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5e76\u884c\u68af\u5ea6\u8bc4\u4f30\u51cf\u5c11\u622a\u65ad\u8bef\u5dee\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4f4e\u5ef6\u8fdf\u91c7\u6837\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u56e0\u987a\u5e8f\u53bb\u566a\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff0c\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u5728\u4f4e\u5ef6\u8fdf\u9884\u7b97\u4e0b\u56fe\u50cf\u8d28\u91cf\u4e0b\u964d\u3002", "method": "EPD\u5229\u7528\u591a\u4e2a\u5e76\u884c\u68af\u5ea6\u8bc4\u4f30\u4f18\u5316ODE\u6c42\u89e3\u5668\uff0c\u53c2\u6570\u53ef\u5b66\u4e60\u4e14\u53ef\u5e76\u884c\u5316\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5408\u6210\u57fa\u51c6\u4e0a\uff0cEPD\u57285 NFE\u5ef6\u8fdf\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5982CIFAR-10\u4e0aFID\u4e3a4.47\u3002", "conclusion": "EPD\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u63d2\u62d4\u7684ODE\u6c42\u89e3\u5668\uff0c\u663e\u8457\u63d0\u5347\u91c7\u6837\u8d28\u91cf\u548c\u901f\u5ea6\u3002"}}
{"id": "2507.14747", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.14747", "abs": "https://arxiv.org/abs/2507.14747", "authors": ["Yiding Song"], "title": "Pruning Increases Orderedness in Recurrent Computation", "comment": "8 pages, 11 figures, 2 tables, Workshop on Methods and Opportunities\n  at Small Scale (MOSS), ICML 2025", "summary": "Inspired by the prevalence of recurrent circuits in biological brains, we\ninvestigate the degree to which directionality is a helpful inductive bias for\nartificial neural networks. Taking directionality as topologically-ordered\ninformation flow between neurons, we formalise a perceptron layer with\nall-to-all connections (mathematically equivalent to a weight-tied recurrent\nneural network) and demonstrate that directionality, a hallmark of modern\nfeed-forward networks, can be induced rather than hard-wired by applying\nappropriate pruning techniques. Across different random seeds our pruning\nschemes successfully induce greater topological ordering in information flow\nbetween neurons without compromising performance, suggesting that\ndirectionality is not a prerequisite for learning, but may be an advantageous\ninductive bias discoverable by gradient descent and sparsification.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u65b9\u5411\u6027\u4f5c\u4e3a\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u5f52\u7eb3\u504f\u7f6e\u662f\u5426\u5fc5\u8981\uff0c\u901a\u8fc7\u4fee\u526a\u6280\u672f\u8bf1\u5bfc\u65b9\u5411\u6027\uff0c\u800c\u975e\u786c\u7f16\u7801\u3002", "motivation": "\u53d7\u751f\u7269\u5927\u8111\u4e2d\u5faa\u73af\u7535\u8def\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65b9\u5411\u6027\u5bf9\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u5168\u8fde\u63a5\u611f\u77e5\u5c42\uff08\u7b49\u540c\u4e8e\u6743\u91cd\u7ed1\u5b9a\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u5e76\u5e94\u7528\u4fee\u526a\u6280\u672f\u8bf1\u5bfc\u65b9\u5411\u6027\u3002", "result": "\u4fee\u526a\u65b9\u6848\u6210\u529f\u8bf1\u5bfc\u795e\u7ecf\u5143\u95f4\u4fe1\u606f\u6d41\u7684\u62d3\u6251\u6392\u5e8f\uff0c\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "conclusion": "\u65b9\u5411\u6027\u5e76\u975e\u5b66\u4e60\u7684\u524d\u63d0\uff0c\u4f46\u53ef\u80fd\u662f\u68af\u5ea6\u4e0b\u964d\u548c\u7a00\u758f\u5316\u53ef\u53d1\u73b0\u7684\u6709\u5229\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2507.15089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15089", "abs": "https://arxiv.org/abs/2507.15089", "authors": ["Ioannis Tsampikos Papapetros", "Ioannis Kansizoglou", "Antonios Gasteratos"], "title": "Visual Place Recognition for Large-Scale UAV Applications", "comment": null, "summary": "Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial\nVehicle (UAV) navigation, enabling robust localization across diverse\nenvironments. Despite significant advancements, aerial vPR faces unique\nchallenges due to the limited availability of large-scale, high-altitude\ndatasets, which limits model generalization, along with the inherent rotational\nambiguity in UAV imagery. To address these challenges, we introduce LASED, a\nlarge-scale aerial dataset with approximately one million images,\nsystematically sampled from 170,000 unique locations throughout Estonia over a\ndecade, offering extensive geographic and temporal diversity. Its structured\ndesign ensures clear place separation significantly enhancing model training\nfor aerial scenarios. Furthermore, we propose the integration of steerable\nConvolutional Neural Networks (CNNs) to explicitly handle rotational variance,\nleveraging their inherent rotational equivariance to produce robust,\norientation-invariant feature representations. Our extensive benchmarking\ndemonstrates that models trained on LASED achieve significantly higher recall\ncompared to those trained on smaller, less diverse datasets, highlighting the\nbenefits of extensive geographic coverage and temporal diversity. Moreover,\nsteerable CNNs effectively address rotational ambiguity inherent in aerial\nimagery, consistently outperforming conventional convolutional architectures,\nachieving on average 12\\% recall improvement over the best-performing\nnon-steerable network. By combining structured, large-scale datasets with\nrotation-equivariant neural networks, our approach significantly enhances model\nrobustness and generalization for aerial vPR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86LASED\u6570\u636e\u96c6\u548c\u53ef\u8f6c\u5411CNN\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u4e2d\u7684\u6570\u636e\u96c6\u4e0d\u8db3\u548c\u65cb\u8f6c\u6a21\u7cca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff08vPR\uff09\u9762\u4e34\u5927\u89c4\u6a21\u9ad8\u6d77\u62d4\u6570\u636e\u96c6\u7a00\u7f3a\u548c\u56fe\u50cf\u65cb\u8f6c\u6a21\u7cca\u7684\u6311\u6218\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f15\u5165LASED\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08100\u4e07\u5f20\u56fe\u50cf\uff09\u548c\u53ef\u8f6c\u5411CNN\uff0c\u4ee5\u5904\u7406\u65cb\u8f6c\u65b9\u5dee\u5e76\u63d0\u5347\u7279\u5f81\u8868\u793a\u3002", "result": "LASED\u8bad\u7ec3\u7684\u6a21\u578b\u53ec\u56de\u7387\u663e\u8457\u63d0\u9ad8\uff1b\u53ef\u8f6c\u5411CNN\u6bd4\u4f20\u7edf\u67b6\u6784\u5e73\u5747\u63d0\u534712%\u53ec\u56de\u7387\u3002", "conclusion": "\u7ed3\u5408\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u65cb\u8f6c\u7b49\u53d8\u7f51\u7edc\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u65e0\u4eba\u673avPR\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14798", "abs": "https://arxiv.org/abs/2507.14798", "authors": ["Xinyi Wu", "Steven Landgraf", "Markus Ulrich", "Rongjun Qin"], "title": "An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks", "comment": "23 pages, 6 figures, this manuscript has been submitted to\n  Geo-spatial Information Science for consideration", "summary": "State-of-the-art 3D computer vision algorithms continue to advance in\nhandling sparse, unordered image sets. Recently developed foundational models\nfor 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction\n(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry\nGrounded Transformer (VGGT), have attracted attention due to their ability to\nhandle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical\naerial images matters, as these models may handle extremely low image overlaps,\nstereo occlusions, and textureless regions. For redundant collections, they can\naccelerate 3D reconstruction by using extremely sparsified image sets. Despite\ntests on various computer vision benchmarks, their potential on photogrammetric\naerial blocks remains unexplored. This paper conducts a comprehensive\nevaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of\nthe UseGeo dataset for pose estimation and dense 3D reconstruction. Results\nshow these methods can accurately reconstruct dense point clouds from very\nsparse image sets (fewer than 10 images, up to 518 pixels resolution), with\ncompleteness gains up to +50% over COLMAP. VGGT also demonstrates higher\ncomputational efficiency, scalability, and more reliable camera pose\nestimation. However, all exhibit limitations with high-resolution images and\nlarge sets, as pose reliability declines with more images and geometric\ncomplexity. These findings suggest transformer-based methods cannot fully\nreplace traditional SfM and MVS, but offer promise as complementary approaches,\nespecially in challenging, low-resolution, and sparse scenarios.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86DUSt3R\u3001MASt3R\u548cVGGT\u7b493D\u91cd\u5efa\u57fa\u7840\u6a21\u578b\u5728\u822a\u62cd\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u7a00\u758f\u56fe\u50cf\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8fd9\u4e9b\u6a21\u578b\u5728\u822a\u62cd\u56fe\u50cf\u4e0a\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u91cd\u53e0\u3001\u906e\u6321\u548c\u65e0\u7eb9\u7406\u533a\u57df\u7684\u8868\u73b0\u3002", "method": "\u5728UseGeo\u6570\u636e\u96c6\u7684\u822a\u62cd\u56fe\u50cf\u5757\u4e0a\u8bc4\u4f30\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u548c\u5bc6\u96c63D\u91cd\u5efa\u3002", "result": "\u6a21\u578b\u80fd\u4ece\u6781\u7a00\u758f\u56fe\u50cf\u96c6\uff08\u5c11\u4e8e10\u5f20\uff09\u91cd\u5efa\u5bc6\u96c6\u70b9\u4e91\uff0c\u5b8c\u6574\u6027\u6bd4COLMAP\u63d0\u9ad850%\uff0cVGGT\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4f20\u7edfSfM\u548cMVS\uff0c\u4f46\u5728\u4f4e\u5206\u8fa8\u7387\u3001\u7a00\u758f\u573a\u666f\u4e2d\u53ef\u4f5c\u4e3a\u8865\u5145\u3002"}}
{"id": "2507.14748", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14748", "abs": "https://arxiv.org/abs/2507.14748", "authors": ["Patrik Reizinger", "B\u00e1lint Mucs\u00e1nyi", "Siyuan Guo", "Benjamin Eysenbach", "Bernhard Sch\u00f6lkopf", "Wieland Brendel"], "title": "Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning", "comment": "16 pages, 7 figures", "summary": "Self-supervised feature learning and pretraining methods in reinforcement\nlearning (RL) often rely on information-theoretic principles, termed mutual\ninformation skill learning (MISL). These methods aim to learn a representation\nof the environment while also incentivizing exploration thereof. However, the\nrole of the representation and mutual information parametrization in MISL is\nnot yet well understood theoretically. Our work investigates MISL through the\nlens of identifiable representation learning by focusing on the Contrastive\nSuccessor Features (CSF) method. We prove that CSF can provably recover the\nenvironment's ground-truth features up to a linear transformation due to the\ninner product parametrization of the features and skill diversity in a\ndiscriminative sense. This first identifiability guarantee for representation\nlearning in RL also helps explain the implications of different mutual\ninformation objectives and the downsides of entropy regularizers. We\nempirically validate our claims in MuJoCo and DeepMind Control and show how CSF\nprovably recovers the ground-truth features both from states and pixels.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u76d1\u7763\u7279\u5f81\u5b66\u4e60\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u53ef\u8bc6\u522b\u6027\uff0c\u901a\u8fc7\u5bf9\u6bd4\u540e\u7ee7\u7279\u5f81\u65b9\u6cd5\u8bc1\u660e\u4e86\u5176\u80fd\u6062\u590d\u73af\u5883\u7684\u771f\u5b9e\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u81ea\u76d1\u7763\u7279\u5f81\u5b66\u4e60\u4e2d\u4e92\u4fe1\u606f\u53c2\u6570\u5316\u7684\u7406\u8bba\u4f5c\u7528\uff0c\u586b\u8865\u73b0\u6709\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u540e\u7ee7\u7279\u5f81\uff08CSF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u53ef\u8bc6\u522b\u6027\u3002", "result": "CSF\u80fd\u6062\u590d\u73af\u5883\u7684\u771f\u5b9e\u7279\u5f81\uff08\u7ebf\u6027\u53d8\u6362\u5185\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u4e92\u4fe1\u606f\u76ee\u6807\u548c\u71b5\u6b63\u5219\u5316\u7684\u5f71\u54cd\u3002", "conclusion": "CSF\u4e3a\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u8bc6\u522b\u6027\u4fdd\u8bc1\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2507.15496", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15496", "abs": "https://arxiv.org/abs/2507.15496", "authors": ["JunYing Huang", "Ao Xu", "DongSun Yong", "KeRen Li", "YuanFeng Wang", "Qi Qin"], "title": "Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images", "comment": null, "summary": "Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684LiDAR-\u89c6\u89c9\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408LiDAR\u70b9\u4e91\u548c\u56fe\u50cf\uff0c\u901a\u8fc7\u6df1\u5ea6\u8865\u5168\u548c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u81ea\u5b9a\u4f4d\u548c\u5bfc\u822a\u7684\u5173\u952e\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u73af\u5883\u548c\u906e\u6321\u533a\u57df\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u8865\u5168\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5206\u5c42\u4f4d\u59ff\u4f18\u5316\u6a21\u5757\u9010\u6b65\u4f18\u5316\u8fd0\u52a8\u4f30\u8ba1\u3002", "result": "\u5728KITTI\u91cc\u7a0b\u8ba1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e\u6216\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u548cLiDAR\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u7684\u4f4d\u59ff\u4f30\u8ba1\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14801", "abs": "https://arxiv.org/abs/2507.14801", "authors": ["Xiangyu Chen", "Kaiwen Zhu", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Wenlong Zhang", "Yihao Liu", "Yu Qiao", "Jiantao Zhou", "Chao Dong"], "title": "Exploring Scalable Unified Modeling for General Low-Level Vision", "comment": null, "summary": "Low-level vision involves a wide spectrum of tasks, including image\nrestoration, enhancement, stylization, and feature extraction, which differ\nsignificantly in both task formulation and output domains. To address the\nchallenge of unified modeling across such diverse tasks, we propose a Visual\ntask Prompt-based Image Processing (VPIP) framework that leverages input-target\nimage pairs as visual prompts to guide the model in performing a variety of\nlow-level vision tasks. The framework comprises an end-to-end image processing\nbackbone, a prompt encoder, and a prompt interaction module, enabling flexible\nintegration with various architectures and effective utilization of\ntask-specific visual representations. Based on this design, we develop a\nunified low-level vision model, GenLV, and evaluate its performance across\nmultiple representative tasks. To explore the scalability of this approach, we\nextend the framework along two dimensions: model capacity and task diversity.\nWe construct a large-scale benchmark consisting of over 100 low-level vision\ntasks and train multiple versions of the model with varying scales.\nExperimental results show that the proposed method achieves considerable\nperformance across a wide range of tasks. Notably, increasing the number of\ntraining tasks enhances generalization, particularly for tasks with limited\ndata, indicating the model's ability to learn transferable representations\nthrough joint training. Further evaluations in zero-shot generalization,\nfew-shot transfer, and task-specific fine-tuning scenarios demonstrate the\nmodel's strong adaptability, confirming the effectiveness, scalability, and\npotential of the proposed framework as a unified foundation for general\nlow-level vision modeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u7684\u7edf\u4e00\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u5904\u7406\u6846\u67b6VPIP\uff0c\u5e76\u5f00\u53d1\u4e86GenLV\u6a21\u578b\uff0c\u5728\u591a\u4efb\u52a1\u548c\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u591a\u6837\u6027\u548c\u8f93\u51fa\u57df\u5dee\u5f02\u5e26\u6765\u7684\u7edf\u4e00\u5efa\u6a21\u6311\u6218\u3002", "method": "\u5229\u7528\u8f93\u5165-\u76ee\u6807\u56fe\u50cf\u5bf9\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\uff0c\u7ed3\u5408\u56fe\u50cf\u5904\u7406\u4e3b\u5e72\u3001\u63d0\u793a\u7f16\u7801\u5668\u548c\u4ea4\u4e92\u6a21\u5757\u3002", "result": "\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4efb\u52a1\u6570\u91cf\u589e\u52a0\u80fd\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VPIP\u6846\u67b6\u6709\u6548\u3001\u53ef\u6269\u5c55\uff0c\u4e3a\u7edf\u4e00\u4f4e\u5c42\u89c6\u89c9\u5efa\u6a21\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2507.14766", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14766", "abs": "https://arxiv.org/abs/2507.14766", "authors": ["Mehak Arora", "Ayman Ali", "Kaiyuan Wu", "Carolyn Davis", "Takashi Shimazui", "Mahmoud Alwakeel", "Victor Moas", "Philip Yang", "Annette Esper", "Rishikesan Kamaleswaran"], "title": "CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories", "comment": "In Review for MICCAI 2025", "summary": "In intensive care units (ICUs), patients with complex clinical conditions\nrequire vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a\nvital diagnostic tool, providing insights into clinical trajectories, but their\nirregular acquisition limits their utility. Existing tools for CXR\ninterpretation are constrained by cross-sectional analysis, failing to capture\ntemporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal\nframework that integrates temporally sparse CXR imaging and radiology reports\nwith high-frequency clinical data, such as vital signs, laboratory values, and\nrespiratory flow sheets, to predict the trajectory of CXR findings in\ncritically ill patients. CXR-TFT leverages latent embeddings from a vision\nencoder that are temporally aligned with hourly clinical data through\ninterpolation. A transformer model is then trained to predict CXR embeddings at\neach hour, conditioned on previous embeddings and clinical measurements. In a\nretrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy\nin forecasting abnormal CXR findings up to 12 hours before they became\nradiographically evident. This predictive capability in clinical data holds\nsignificant potential for enhancing the management of time-sensitive conditions\nlike acute respiratory distress syndrome, where early intervention is crucial\nand diagnoses are often delayed. By providing distinctive temporal resolution\nin prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights\nthat can directly improve clinical outcomes.", "AI": {"tldr": "CXR-TFT\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u7a00\u758f\u7684\u80f8\u90e8X\u5149\u7247\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u9884\u6d4bICU\u60a3\u8005\u5f02\u5e38X\u5149\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u6355\u6349\u80f8\u90e8X\u5149\u7684\u52a8\u6001\u53d8\u5316\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u6548\u7528\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u7f16\u7801\u5668\u751f\u6210\u6f5c\u5728\u5d4c\u5165\uff0c\u5e76\u4e0e\u9ad8\u9891\u4e34\u5e8a\u6570\u636e\u5bf9\u9f50\uff0c\u4f7f\u7528Transformer\u6a21\u578b\u9884\u6d4b\u672a\u6765X\u5149\u7ed3\u679c\u3002", "result": "\u572820,000\u540dICU\u60a3\u8005\u4e2d\uff0cCXR-TFT\u80fd\u63d0\u524d12\u5c0f\u65f6\u51c6\u786e\u9884\u6d4b\u5f02\u5e38X\u5149\u7ed3\u679c\u3002", "conclusion": "CXR-TFT\u63d0\u4f9b\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u548c\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2507.15597", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15597", "abs": "https://arxiv.org/abs/2507.15597", "authors": ["Hao Luo", "Yicheng Feng", "Wanpeng Zhang", "Sipeng Zheng", "Ye Wang", "Haoqi Yuan", "Jiazheng Liu", "Chaoyi Xu", "Qin Jin", "Zongqing Lu"], "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos", "comment": "37 pages", "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.", "AI": {"tldr": "Being-H0\u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b\u89c6\u9891\u8bad\u7ec3\u7684\u7075\u5de7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u7269\u7406\u6307\u4ee4\u8c03\u4f18\u548c\u8fd0\u52a8\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u6709\u9650\u7684\u64cd\u4f5c\u6f14\u793a\uff0c\u96be\u4ee5\u5904\u7406\u9ad8\u7075\u5de7\u6027\u548c\u65b0\u573a\u666f\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u7269\u7406\u6307\u4ee4\u8c03\u4f18\u8303\u5f0f\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u30013D\u7a7a\u95f4\u5bf9\u9f50\u548c\u673a\u5668\u4eba\u4efb\u52a1\u9002\u5e94\uff0c\u5e76\u5f15\u5165\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u8fd0\u52a8\u6807\u8bb0\u5316\u65b9\u6cd5\u3002", "result": "Being-H0\u5728\u52a8\u4f5c\u751f\u6210\u548c\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u53d6\u5f97\u663e\u8457\u6548\u679c\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u89c6\u9891\u6570\u636e\uff0cBeing-H0\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14807", "abs": "https://arxiv.org/abs/2507.14807", "authors": ["Juan Hu", "Shaojing Fan", "Terence Sim"], "title": "Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection", "comment": null, "summary": "Multi-face deepfake videos are becoming increasingly prevalent, often\nappearing in natural social settings that challenge existing detection methods.\nMost current approaches excel at single-face detection but struggle in\nmulti-face scenarios, due to a lack of awareness of crucial contextual cues. In\nthis work, we develop a novel approach that leverages human cognition to\nanalyze and defend against multi-face deepfake videos. Through a series of\nhuman studies, we systematically examine how people detect deepfake faces in\nsocial settings. Our quantitative analysis reveals four key cues humans rely\non: scene-motion coherence, inter-face appearance compatibility, interpersonal\ngaze alignment, and face-body consistency. Guided by these insights, we\nintroduce \\textsf{HICOM}, a novel framework designed to detect every fake face\nin multi-face scenarios. Extensive experiments on benchmark datasets show that\n\\textsf{HICOM} improves average accuracy by 3.3\\% in in-dataset detection and\n2.8\\% under real-world perturbations. Moreover, it outperforms existing methods\nby 5.8\\% on unseen datasets, demonstrating the generalization of human-inspired\ncues. \\textsf{HICOM} further enhances interpretability by incorporating an LLM\nto provide human-readable explanations, making detection results more\ntransparent and convincing. Our work sheds light on involving human factors to\nenhance defense against deepfakes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u7684\u591a\u8138\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u68c0\u6d4b\u65b9\u6cd5HICOM\uff0c\u901a\u8fc7\u5206\u6790\u4eba\u7c7b\u4f9d\u8d56\u7684\u56db\u4e2a\u5173\u952e\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u8138\u68c0\u6d4b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8138\u573a\u666f\u4e2d\u56e0\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7ebf\u7d22\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u501f\u9274\u4eba\u7c7b\u8ba4\u77e5\u6765\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u8bc6\u522b\u56db\u4e2a\u5173\u952e\u7ebf\u7d22\uff08\u573a\u666f\u8fd0\u52a8\u4e00\u81f4\u6027\u3001\u9762\u90e8\u5916\u89c2\u517c\u5bb9\u6027\u3001\u4eba\u9645\u6ce8\u89c6\u5bf9\u9f50\u3001\u8138\u8eab\u4e00\u81f4\u6027\uff09\uff0c\u5e76\u8bbe\u8ba1HICOM\u6846\u67b6\u7ed3\u5408\u8fd9\u4e9b\u7ebf\u7d22\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "HICOM\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.3%\uff0c\u5728\u771f\u5b9e\u6270\u52a8\u4e0b\u63d0\u53472.8%\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd55.8%\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4eba\u7c7b\u8ba4\u77e5\u7ebf\u7d22\uff0cHICOM\u663e\u8457\u63d0\u5347\u4e86\u591a\u8138\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u9632\u5fa1\u6df1\u5ea6\u4f2a\u9020\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14777", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14777", "abs": "https://arxiv.org/abs/2507.14777", "authors": ["Bishwamittra Ghosh", "Soumi Das", "Qinyuan Wu", "Mohammad Aflah Khan", "Krishna P. Gummadi", "Evimaria Terzi", "Deepak Garg"], "title": "Rethinking Memorization Measures and their Implications in Large Language Models", "comment": "Preprint", "summary": "Concerned with privacy threats, memorization in LLMs is often seen as\nundesirable, specifically for learning. In this paper, we study whether\nmemorization can be avoided when optimally learning a language, and whether the\nprivacy threat posed by memorization is exaggerated or not. To this end, we\nre-examine existing privacy-focused measures of memorization, namely\nrecollection-based and counterfactual memorization, along with a newly proposed\ncontextual memorization.\n  Relating memorization to local over-fitting during learning, contextual\nmemorization aims to disentangle memorization from the contextual learning\nability of LLMs. Informally, a string is contextually memorized if its\nrecollection due to training exceeds the optimal contextual recollection, a\nlearned threshold denoting the best contextual learning without training.\nConceptually, contextual recollection avoids the fallacy of recollection-based\nmemorization, where any form of high recollection is a sign of memorization.\nTheoretically, contextual memorization relates to counterfactual memorization,\nbut imposes stronger conditions. Memorization measures differ in outcomes and\ninformation requirements.\n  Experimenting on 18 LLMs from 6 families and multiple formal languages of\ndifferent entropy, we show that (a) memorization measures disagree on\nmemorization order of varying frequent strings, (b) optimal learning of a\nlanguage cannot avoid partial memorization of training strings, and (c)\nimproved learning decreases contextual and counterfactual memorization but\nincreases recollection-based memorization. Finally, (d) we revisit existing\nreports of memorized strings by recollection that neither pose a privacy threat\nnor are contextually or counterfactually memorized.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u4e2d\u8bb0\u5fc6\u5316\u662f\u5426\u53ef\u907f\u514d\u53ca\u5176\u9690\u79c1\u5a01\u80c1\u662f\u5426\u88ab\u5938\u5927\uff0c\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5316\u6982\u5ff5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\u4e0d\u540c\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4e14\u4f18\u5316\u5b66\u4e60\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u8bb0\u5fc6\u5316\u3002", "motivation": "\u63a2\u8ba8\u8bb0\u5fc6\u5316\u5728\u8bed\u8a00\u5b66\u4e60\u4e2d\u7684\u5fc5\u8981\u6027\u53ca\u5176\u9690\u79c1\u5a01\u80c1\u7684\u771f\u5b9e\u6027\uff0c\u4ee5\u6f84\u6e05\u73b0\u6709\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u91cd\u65b0\u5ba1\u89c6\u4e86\u57fa\u4e8e\u56de\u5fc6\u548c\u53cd\u4e8b\u5b9e\u7684\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5316\u6982\u5ff5\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8bb0\u5fc6\u5316\u6d4b\u91cf\u65b9\u6cd5\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4f18\u5316\u5b66\u4e60\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u8bb0\u5fc6\u5316\uff0c\u4e14\u6539\u8fdb\u5b66\u4e60\u4f1a\u51cf\u5c11\u4e0a\u4e0b\u6587\u548c\u53cd\u4e8b\u5b9e\u8bb0\u5fc6\u5316\u4f46\u589e\u52a0\u57fa\u4e8e\u56de\u5fc6\u7684\u8bb0\u5fc6\u5316\u3002", "conclusion": "\u90e8\u5206\u57fa\u4e8e\u56de\u5fc6\u7684\u8bb0\u5fc6\u5316\u5b57\u7b26\u4e32\u5e76\u4e0d\u6784\u6210\u9690\u79c1\u5a01\u80c1\uff0c\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u5316\u80fd\u66f4\u51c6\u786e\u533a\u5206\u8bb0\u5fc6\u5316\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2507.15857", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15857", "abs": "https://arxiv.org/abs/2507.15857", "authors": ["Mihir Prabhudesai", "Menging Wu", "Amir Zadeh", "Katerina Fragkiadaki", "Deepak Pathak"], "title": "Diffusion Beats Autoregressive in Data-Constrained Settings", "comment": "Project Webpage: https://diffusion-scaling.github.io", "summary": "Autoregressive (AR) models have long dominated the landscape of large\nlanguage models, driving progress across a wide range of tasks. Recently,\ndiffusion-based language models have emerged as a promising alternative, though\ntheir advantages over AR models remain underexplored. In this paper, we\nsystematically study masked diffusion models in data-constrained settings-where\ntraining involves repeated passes over limited data-and find that they\nsignificantly outperform AR models when compute is abundant but data is scarce.\nDiffusion models make better use of repeated data, achieving lower validation\nloss and superior downstream performance. We interpret this advantage as\nimplicit data augmentation: masked diffusion exposes the model to a diverse\ndistribution of token orderings and prediction tasks, unlike AR's fixed\nleft-to-right factorization. We find new scaling laws for diffusion models and\nderive a closed-form expression for the critical compute threshold at which\ndiffusion begins to outperform AR. These results suggest that when data, not\ncompute, is the bottleneck, diffusion models offer a compelling alternative to\nthe standard AR paradigm. Our code is available at:\nhttps://diffusion-scaling.github.io.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u53d7\u9650\u60c5\u51b5\u4e0b\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5c24\u5176\u5728\u8ba1\u7b97\u8d44\u6e90\u5145\u8db3\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4f18\u52bf\uff0c\u586b\u8865\u5176\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u6bd4\u8f83\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u63a9\u7801\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u9690\u5f0f\u6570\u636e\u589e\u5f3a\u7279\u6027\u3002", "result": "\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u663e\u8457\u4f18\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u9a8c\u8bc1\u635f\u5931\u66f4\u4f4e\u4e14\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u5f53\u6570\u636e\u6210\u4e3a\u74f6\u9888\u65f6\uff0c\u6269\u6563\u6a21\u578b\u662f\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.14783", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14783", "abs": "https://arxiv.org/abs/2507.14783", "authors": ["Derek Li", "Jiaming Zhou", "Amirreza Kazemi", "Qianyi Sun", "Abbas Ghaddar", "Mohammad Ali Alomrani", "Liheng Ma", "Yu Luo", "Dong Li", "Feng Wen", "Jianye Hao", "Mark Coates", "Yingxue Zhang"], "title": "Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards", "comment": null, "summary": "The advancement of general-purpose artificial intelligence relies on large\nlanguage models (LLMs) that excel across a wide range of tasks, from structured\nreasoning to creative generation. However, post-training methods like\nSupervised Fine-Tuning (SFT) often struggle with generalization, favoring\nmemorization over transferable learning. In this work, we introduce Omni-Think,\na unified reinforcement learning (RL) framework that enhances LLM performance\nacross diverse tasks by combining rule-based verifiable rewards with generative\npreference signals via LLM-as-a-Judge evaluations. Our approach enables\nconsistent optimization across task types and scales RL-based training to\nsubjective domains. We further investigate training strategies, demonstrating\nthat a curriculum-based progression that orders tasks from structured to\nopen-ended improves performance and reduces forgetting. Experimental results\nacross four domains reveal that curriculum learning improves performance by\n5.2\\% over joint training and 9.1\\% over model merging. These results highlight\nthe importance of task-aware sampling and hybrid supervision in scaling\nRL-based post-training for general-purpose LLMs.", "AI": {"tldr": "Omni-Think\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u5956\u52b1\u548c\u751f\u6210\u504f\u597d\u4fe1\u53f7\uff0c\u63d0\u5347LLM\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u503e\u5411\u4e8e\u8bb0\u5fc6\u800c\u975e\u8fc1\u79fb\u5b66\u4e60\u3002", "method": "\u63d0\u51faOmni-Think\u6846\u67b6\uff0c\u7ed3\u5408\u89c4\u5219\u5956\u52b1\u548cLLM-as-a-Judge\u8bc4\u4f30\u7684\u751f\u6210\u504f\u597d\u4fe1\u53f7\uff0c\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8bfe\u7a0b\u5b66\u4e60\u6027\u80fd\u63d0\u53475.2%\uff08\u8054\u5408\u8bad\u7ec3\uff09\u548c9.1%\uff08\u6a21\u578b\u5408\u5e76\uff09\u3002", "conclusion": "\u4efb\u52a1\u611f\u77e5\u91c7\u6837\u548c\u6df7\u5408\u76d1\u7763\u5bf9LLM\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.14811", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14811", "abs": "https://arxiv.org/abs/2507.14811", "authors": ["Jiaji Zhang", "Ruichao Sun", "Hailiang Zhao", "Jiaju Wu", "Peng Chen", "Hao Li", "Xinkui Zhao", "Kingsum Chow", "Gang Xiong", "Lin Ye", "Shuiguang Deng"], "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models", "comment": null, "summary": "Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.", "AI": {"tldr": "SegQuant\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ed3\u5408\u4e92\u8865\u6280\u672f\u63d0\u5347\u8de8\u6a21\u578b\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u901a\u7528\u6027\u4e0d\u8db3\uff0c\u96be\u4ee5\u96c6\u6210\u5230\u5de5\u4e1a\u90e8\u7f72\u6d41\u7a0b\u4e2d\u3002", "method": "\u63d0\u51faSegQuant\u6846\u67b6\uff0c\u5305\u542bSegLinear\uff08\u6bb5\u611f\u77e5\u56fe\u91cf\u5316\u7b56\u7565\uff09\u548cDualScale\uff08\u53cc\u5c3a\u5ea6\u91cf\u5316\u65b9\u6848\uff09\uff0c\u4fdd\u7559\u6781\u6027\u4e0d\u5bf9\u79f0\u6fc0\u6d3b\u3002", "result": "SegQuant\u5728Transformer\u4ee5\u5916\u7684\u6269\u6563\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5bb9\u4e3b\u6d41\u90e8\u7f72\u5de5\u5177\u3002", "conclusion": "SegQuant\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u91cf\u5316\u4e2d\u7684\u901a\u7528\u6027\u548c\u90e8\u7f72\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6548\u7387\u4e14\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.14785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14785", "abs": "https://arxiv.org/abs/2507.14785", "authors": ["Erfan Pirmorad"], "title": "Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs", "comment": null, "summary": "The complexity and interconnectivity of entities involved in money laundering\ndemand investigative reasoning over graph-structured data. This paper explores\nthe use of large language models (LLMs) as reasoning engines over localized\nsubgraphs extracted from a financial knowledge graph. We propose a lightweight\npipeline that retrieves k-hop neighborhoods around entities of interest,\nserializes them into structured text, and prompts an LLM via few-shot\nin-context learning to assess suspiciousness and generate justifications. Using\nsynthetic anti-money laundering (AML) scenarios that reflect common laundering\nbehaviors, we show that LLMs can emulate analyst-style logic, highlight red\nflags, and provide coherent explanations. While this study is exploratory, it\nillustrates the potential of LLM-based graph reasoning in AML and lays\ngroundwork for explainable, language-driven financial crime analytics.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u91d1\u878d\u77e5\u8bc6\u56fe\u8c31\u7684\u5c40\u90e8\u5b50\u56fe\u4e0a\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u8bc4\u4f30\u6d17\u94b1\u884c\u4e3a\u7684\u53ef\u7591\u6027\u3002", "motivation": "\u6d17\u94b1\u884c\u4e3a\u6d89\u53ca\u7684\u5b9e\u4f53\u590d\u6742\u4e14\u76f8\u4e92\u5173\u8054\uff0c\u9700\u8981\u57fa\u4e8e\u56fe\u7ed3\u6784\u6570\u636e\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6d41\u7a0b\uff0c\u63d0\u53d6\u611f\u5174\u8da3\u5b9e\u4f53\u7684k\u8df3\u90bb\u57df\uff0c\u5c06\u5176\u5e8f\u5217\u5316\u4e3a\u7ed3\u6784\u5316\u6587\u672c\uff0c\u5e76\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u793aLLM\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u80fd\u591f\u6a21\u62df\u5206\u6790\u5e08\u903b\u8f91\uff0c\u8bc6\u522b\u53ef\u7591\u884c\u4e3a\u5e76\u63d0\u4f9b\u5408\u7406\u89e3\u91ca\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86LLM\u5728\u56fe\u63a8\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u9a71\u52a8\u91d1\u878d\u72af\u7f6a\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14823", "abs": "https://arxiv.org/abs/2507.14823", "authors": ["Dong Shu", "Haoyang Yuan", "Yuchen Wang", "Yanguang Liu", "Huopu Zhang", "Haiyan Zhao", "Mengnan Du"], "title": "FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models", "comment": "20 Pages, 18 Figures", "summary": "Large vision-language models (LVLMs) have made significant progress in chart\nunderstanding. However, financial charts, characterized by complex temporal\nstructures and domain-specific terminology, remain notably underexplored. We\nintroduce FinChart-Bench, the first benchmark specifically focused on\nreal-world financial charts. FinChart-Bench comprises 1,200 financial chart\nimages collected from 2015 to 2024, each annotated with True/False (TF),\nMultiple Choice (MC), and Question Answering (QA) questions, totaling 7,016\nquestions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs\non FinChart-Bench. Our evaluation reveals critical insights: (1) the\nperformance gap between open-source and closed-source models is narrowing, (2)\nperformance degradation occurs in upgraded models within families, (3) many\nmodels struggle with instruction following, (4) both advanced models show\nsignificant limitations in spatial reasoning abilities, and (5) current LVLMs\nare not reliable enough to serve as automated evaluators. These findings\nhighlight important limitations in current LVLM capabilities for financial\nchart understanding. The FinChart-Bench dataset is available at\nhttps://huggingface.co/datasets/Tizzzzy/FinChart-Bench.", "AI": {"tldr": "FinChart-Bench\u662f\u9996\u4e2a\u4e13\u6ce8\u4e8e\u91d1\u878d\u56fe\u8868\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1,200\u5f20\u56fe\u8868\u548c7,016\u4e2a\u95ee\u9898\uff0c\u8bc4\u4f30\u4e8625\u4e2aLVLM\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u91d1\u878d\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u91d1\u878d\u56fe\u8868\u5177\u6709\u590d\u6742\u7684\u65f6\u95f4\u7ed3\u6784\u548c\u9886\u57df\u672f\u8bed\uff0c\u4f46\u73b0\u6709\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5bf9\u5176\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u6784\u5efaFinChart-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b1,200\u5f20\u91d1\u878d\u56fe\u8868\u548c\u591a\u79cd\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u8bc4\u4f3025\u4e2aLVLM\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u5f00\u6e90\u4e0e\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u3001\u5347\u7ea7\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3001\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u7b49\u95ee\u9898\u3002", "conclusion": "\u5f53\u524dLVLM\u5728\u91d1\u878d\u56fe\u8868\u7406\u89e3\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.14793", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14793", "abs": "https://arxiv.org/abs/2507.14793", "authors": ["T. Anderson Keller"], "title": "Flow Equivariant Recurrent Neural Networks", "comment": null, "summary": "Data arrives at our senses as a continuous stream, smoothly transforming from\none instant to the next. These smooth transformations can be viewed as\ncontinuous symmetries of the environment that we inhabit, defining equivalence\nrelations between stimuli over time. In machine learning, neural network\narchitectures that respect symmetries of their data are called equivariant and\nhave provable benefits in terms of generalization ability and sample\nefficiency. To date, however, equivariance has been considered only for static\ntransformations and feed-forward networks, limiting its applicability to\nsequence models, such as recurrent neural networks (RNNs), and corresponding\ntime-parameterized sequence transformations. In this work, we extend\nequivariant network theory to this regime of `flows' -- one-parameter Lie\nsubgroups capturing natural transformations over time, such as visual motion.\nWe begin by showing that standard RNNs are generally not flow equivariant:\ntheir hidden states fail to transform in a geometrically structured manner for\nmoving stimuli. We then show how flow equivariance can be introduced, and\ndemonstrate that these models significantly outperform their non-equivariant\ncounterparts in terms of training speed, length generalization, and velocity\ngeneralization, on both next step prediction and sequence classification. We\npresent this work as a first step towards building sequence models that respect\nthe time-parameterized symmetries which govern the world around us.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7b49\u53d8\u6027\u7f51\u7edc\u7406\u8bba\u6269\u5c55\u5230\u65f6\u95f4\u53c2\u6570\u5316\u5e8f\u5217\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfRNN\u5728\u6d41\u52a8\u5bf9\u79f0\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6570\u636e\u5177\u6709\u8fde\u7eed\u5bf9\u79f0\u6027\uff0c\u4f46\u73b0\u6709\u7b49\u53d8\u6027\u7f51\u7edc\u4ec5\u9002\u7528\u4e8e\u9759\u6001\u53d8\u6362\u548c\u524d\u9988\u7f51\u7edc\uff0c\u65e0\u6cd5\u5904\u7406\u65f6\u95f4\u53c2\u6570\u5316\u7684\u5e8f\u5217\u53d8\u6362\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u6d41\u52a8\u7b49\u53d8\u6027\uff0c\u6539\u8fdbRNN\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u65f6\u95f4\u53c2\u6570\u5316\u7684\u5bf9\u79f0\u6027\u53d8\u6362\u3002", "result": "\u6539\u8fdb\u540e\u7684\u6a21\u578b\u5728\u8bad\u7ec3\u901f\u5ea6\u3001\u957f\u5ea6\u6cdb\u5316\u548c\u901f\u5ea6\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u975e\u7b49\u53d8\u6027\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u5c0a\u91cd\u65f6\u95f4\u53c2\u6570\u5316\u5bf9\u79f0\u6027\u7684\u5e8f\u5217\u6a21\u578b\u8fc8\u51fa\u4e86\u7b2c\u4e00\u6b65\u3002"}}
{"id": "2507.14826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14826", "abs": "https://arxiv.org/abs/2507.14826", "authors": ["Fu-Jen Tsai", "Yan-Tsung Peng", "Yen-Yu Lin", "Chia-Wen Lin"], "title": "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing", "comment": "ICCV 2025", "summary": "Image dehazing aims to remove unwanted hazy artifacts in images. Although\nprevious research has collected paired real-world hazy and haze-free images to\nimprove dehazing models' performance in real-world scenarios, these models\noften experience significant performance drops when handling unseen real-world\nhazy images due to limited training data. This issue motivates us to develop a\nflexible domain adaptation method to enhance dehazing performance during\ntesting. Observing that predicting haze patterns is generally easier than\nrecovering clean content, we propose the Physics-guided Haze Transfer Network\n(PHATNet) which transfers haze patterns from unseen target domains to\nsource-domain haze-free images, creating domain-specific fine-tuning sets to\nupdate dehazing models for effective domain adaptation. Additionally, we\nintroduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to\nenhance PHATNet's disentanglement ability. Experimental results demonstrate\nthat PHATNet significantly boosts state-of-the-art dehazing models on benchmark\nreal-world image dehazing datasets.", "AI": {"tldr": "PHATNet\u901a\u8fc7\u5c06\u76ee\u6807\u57df\u4e2d\u7684\u96fe\u973e\u6a21\u5f0f\u8f6c\u79fb\u5230\u6e90\u57df\u7684\u65e0\u96fe\u56fe\u50cf\u4e0a\uff0c\u521b\u5efa\u7279\u5b9a\u57df\u7684\u5fae\u8c03\u96c6\uff0c\u4ece\u800c\u63d0\u5347\u53bb\u96fe\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53bb\u96fe\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u96fe\u973e\u56fe\u50cf\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u662f\u8bad\u7ec3\u6570\u636e\u6709\u9650\u3002", "method": "\u63d0\u51faPHATNet\uff0c\u5229\u7528\u7269\u7406\u5f15\u5bfc\u7684\u96fe\u973e\u8f6c\u79fb\u7f51\u7edc\uff0c\u7ed3\u5408Haze-Transfer-Consistency\u548cContent-Leakage Loss\u589e\u5f3a\u89e3\u8026\u80fd\u529b\u3002", "result": "PHATNet\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u53bb\u96fe\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "PHATNet\u901a\u8fc7\u57df\u9002\u5e94\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u53bb\u96fe\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002"}}
{"id": "2507.14805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14805", "abs": "https://arxiv.org/abs/2507.14805", "authors": ["Alex Cloud", "Minh Le", "James Chua", "Jan Betley", "Anna Sztyber-Betley", "Jacob Hilton", "Samuel Marks", "Owain Evans"], "title": "Subliminal Learning: Language models transmit behavioral traits via hidden signals in data", "comment": null, "summary": "We study subliminal learning, a surprising phenomenon where language models\ntransmit behavioral traits via semantically unrelated data. In our main\nexperiments, a \"teacher\" model with some trait T (such as liking owls or being\nmisaligned) generates a dataset consisting solely of number sequences.\nRemarkably, a \"student\" model trained on this dataset learns T. This occurs\neven when the data is filtered to remove references to T. We observe the same\neffect when training on code or reasoning traces generated by the same teacher\nmodel. However, we do not observe the effect when the teacher and student have\ndifferent base models. To help explain our findings, we prove a theoretical\nresult showing that subliminal learning occurs in all neural networks under\ncertain conditions, and demonstrate subliminal learning in a simple MLP\nclassifier. We conclude that subliminal learning is a general phenomenon that\npresents an unexpected pitfall for AI development. Distillation could propagate\nunintended traits, even when developers try to prevent this via data filtering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u8bed\u4e49\u65e0\u5173\u7684\u6570\u636e\u4f20\u9012\u884c\u4e3a\u7279\u5f81\uff0c\u79f0\u4e3a\u6f5c\u610f\u8bc6\u5b66\u4e60\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728\u6570\u636e\u4e2d\u9690\u85cf\u5e76\u4f20\u9012\u884c\u4e3a\u7279\u5f81\uff0c\u5373\u4f7f\u6570\u636e\u7ecf\u8fc7\u8fc7\u6ee4\u3002", "method": "\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u751f\u6210\u6570\u5b57\u5e8f\u5217\u3001\u4ee3\u7801\u6216\u63a8\u7406\u8f68\u8ff9\uff0c\u5b66\u751f\u6a21\u578b\u8bad\u7ec3\u540e\u5b66\u4e60\u6559\u5e08\u7684\u884c\u4e3a\u7279\u5f81\u3002", "result": "\u5b66\u751f\u6a21\u578b\u6210\u529f\u5b66\u4e60\u5230\u6559\u5e08\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4f46\u4ec5\u5728\u540c\u57fa\u6a21\u578b\u95f4\u6709\u6548\u3002\u7406\u8bba\u8bc1\u660e\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u90fd\u4f1a\u53d1\u751f\u6f5c\u610f\u8bc6\u5b66\u4e60\u3002", "conclusion": "\u6f5c\u610f\u8bc6\u5b66\u4e60\u662f\u666e\u904d\u73b0\u8c61\uff0c\u53ef\u80fd\u6210\u4e3aAI\u5f00\u53d1\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5373\u4f7f\u6570\u636e\u8fc7\u6ee4\u4e5f\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u3002"}}
{"id": "2507.14833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14833", "abs": "https://arxiv.org/abs/2507.14833", "authors": ["Haoxuan Zhang", "Wenju Cui", "Yuzhu Cao", "Tao Tan", "Jie Liu", "Yunsong Peng", "Jian Zheng"], "title": "Paired Image Generation with Diffusion-Guided Diffusion Models", "comment": null, "summary": "The segmentation of mass lesions in digital breast tomosynthesis (DBT) images\nis very significant for the early screening of breast cancer. However, the\nhigh-density breast tissue often leads to high concealment of the mass lesions,\nwhich makes manual annotation difficult and time-consuming. As a result, there\nis a lack of annotated data for model training. Diffusion models are commonly\nused for data augmentation, but the existing methods face two challenges.\nFirst, due to the high concealment of lesions, it is difficult for the model to\nlearn the features of the lesion area. This leads to the low generation quality\nof the lesion areas, thus limiting the quality of the generated images. Second,\nexisting methods can only generate images and cannot generate corresponding\nannotations, which restricts the usability of the generated images in\nsupervised training. In this work, we propose a paired image generation method.\nThe method does not require external conditions and can achieve the generation\nof paired images by training an extra diffusion guider for the conditional\ndiffusion model. During the experimental phase, we generated paired DBT slices\nand mass lesion masks. Then, we incorporated them into the supervised training\nprocess of the mass lesion segmentation task. The experimental results show\nthat our method can improve the generation quality without external conditions.\nMoreover, it contributes to alleviating the shortage of annotated data, thus\nenhancing the performance of downstream tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u6761\u4ef6\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e73\u817a\u65ad\u5c42\u626b\u63cf\u56fe\u50cf\u4e2d\u75c5\u7076\u5206\u5272\u4efb\u52a1\u7684\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u9ad8\u5bc6\u5ea6\u4e73\u817a\u7ec4\u7ec7\u5bfc\u81f4\u75c5\u7076\u9690\u853d\uff0c\u624b\u52a8\u6807\u6ce8\u56f0\u96be\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u6269\u6563\u6a21\u578b\u751f\u6210\u8d28\u91cf\u4f4e\u4e14\u65e0\u6cd5\u751f\u6210\u6807\u6ce8\u3002", "method": "\u8bad\u7ec3\u989d\u5916\u7684\u6269\u6563\u5f15\u5bfc\u5668\uff0c\u5b9e\u73b0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u914d\u5bf9\u56fe\u50cf\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\uff0c\u7f13\u89e3\u4e86\u6807\u6ce8\u6570\u636e\u77ed\u7f3a\u95ee\u9898\uff0c\u5e76\u63d0\u5347\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7076\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u6761\u4ef6\u3002"}}
{"id": "2507.14824", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14824", "abs": "https://arxiv.org/abs/2507.14824", "authors": ["Kunyu Yu", "Rui Yang", "Jingchi Liao", "Siqi Li", "Huitao Li", "Irene Li", "Yifan Peng", "Rishikesan Kamaleswaran", "Nan Liu"], "title": "Benchmarking Foundation Models with Multimodal Public Electronic Health Records", "comment": null, "summary": "Foundation models have emerged as a powerful approach for processing\nelectronic health records (EHRs), offering flexibility to handle diverse\nmedical data modalities. In this study, we present a comprehensive benchmark\nthat evaluates the performance, fairness, and interpretability of foundation\nmodels, both as unimodal encoders and as multimodal learners, using the\npublicly available MIMIC-IV database. To support consistent and reproducible\nevaluation, we developed a standardized data processing pipeline that\nharmonizes heterogeneous clinical records into an analysis-ready format. We\nsystematically compared eight foundation models, encompassing both unimodal and\nmultimodal models, as well as domain-specific and general-purpose variants. Our\nfindings demonstrate that incorporating multiple data modalities leads to\nconsistent improvements in predictive performance without introducing\nadditional bias. Through this benchmark, we aim to support the development of\neffective and trustworthy multimodal artificial intelligence (AI) systems for\nreal-world clinical applications. Our code is available at\nhttps://github.com/nliulab/MIMIC-Multimodal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7efc\u5408\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e86\u57fa\u7840\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u5904\u7406\u4e2d\u7684\u6027\u80fd\u3001\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u5bf9\u9884\u6d4b\u6027\u80fd\u7684\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u57fa\u7840\u6a21\u578b\u5728\u533b\u7597\u6570\u636e\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u652f\u6301\u5f00\u53d1\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\u3002", "method": "\u4f7f\u7528MIMIC-IV\u6570\u636e\u5e93\uff0c\u5f00\u53d1\u6807\u51c6\u5316\u6570\u636e\u5904\u7406\u6d41\u7a0b\uff0c\u7cfb\u7edf\u6bd4\u8f83\u516b\u79cd\u57fa\u7840\u6a21\u578b\u3002", "result": "\u591a\u6a21\u6001\u6570\u636e\u80fd\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u4e14\u4e0d\u5f15\u5165\u989d\u5916\u504f\u5dee\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6709\u52a9\u4e8e\u63a8\u52a8\u4e34\u5e8a\u5e94\u7528\u4e2d\u6709\u6548\u4e14\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001AI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.14845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14845", "abs": "https://arxiv.org/abs/2507.14845", "authors": ["Rizhao Fan", "Zhigen Li", "Heping Li", "Ning An"], "title": "Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image", "comment": null, "summary": "Depth completion is an important vision task, and many efforts have been made\nto enhance the quality of depth maps from sparse depth measurements. Despite\nsignificant advances, training these models to recover dense depth from sparse\nmeasurements remains a challenging problem. Supervised learning methods rely on\ndense depth labels to predict unobserved regions, while self-supervised\napproaches require image sequences to enforce geometric constraints and\nphotometric consistency between frames. However, acquiring dense annotations is\ncostly, and multi-frame dependencies limit the applicability of self-supervised\nmethods in static or single-frame scenarios. To address these challenges, we\npropose a novel self-supervised depth completion paradigm that requires only\nsparse depth measurements and their corresponding image for training. Unlike\nexisting methods, our approach eliminates the need for dense depth labels or\nadditional images captured from neighboring viewpoints. By leveraging the\ncharacteristics of depth distribution, we design novel loss functions that\neffectively propagate depth information from observed points to unobserved\nregions. Additionally, we incorporate segmentation maps generated by vision\nfoundation models to further enhance depth estimation. Extensive experiments\ndemonstrate the effectiveness of our proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u9700\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u548c\u5bf9\u5e94\u56fe\u50cf\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\uff0c\u65e0\u9700\u5bc6\u96c6\u6807\u7b7e\u6216\u591a\u5e27\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bc6\u96c6\u6df1\u5ea6\u6807\u7b7e\u6216\u591a\u5e27\u56fe\u50cf\uff0c\u6210\u672c\u9ad8\u4e14\u9002\u7528\u6027\u53d7\u9650\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5206\u5e03\u7279\u6027\u8bbe\u8ba1\u65b0\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u5206\u5272\u56fe\u589e\u5f3a\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86\u5bc6\u96c6\u6807\u7b7e\u548c\u591a\u5e27\u4f9d\u8d56\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u8865\u5168\u6548\u679c\u3002"}}
{"id": "2507.14828", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14828", "abs": "https://arxiv.org/abs/2507.14828", "authors": ["Abdul-Kazeem Shamba", "Kerstin Bach", "Gavin Taylor"], "title": "eMargin: Revisiting Contrastive Learning with Margin-Based Separation", "comment": "LDD'25: Learning from Difficult Data Workshop (ECAI 2025)", "summary": "We revisit previous contrastive learning frameworks to investigate the effect\nof introducing an adaptive margin into the contrastive loss function for time\nseries representation learning. Specifically, we explore whether an adaptive\nmargin (eMargin), adjusted based on a predefined similarity threshold, can\nimprove the separation between adjacent but dissimilar time steps and\nsubsequently lead to better performance in downstream tasks. Our study\nevaluates the impact of this modification on clustering performance and\nclassification in three benchmark datasets. Our findings, however, indicate\nthat achieving high scores on unsupervised clustering metrics does not\nnecessarily imply that the learned embeddings are meaningful or effective in\ndownstream tasks. To be specific, eMargin added to InfoNCE consistently\noutperforms state-of-the-art baselines in unsupervised clustering metrics, but\nstruggles to achieve competitive results in downstream classification with\nlinear probing. The source code is publicly available at\nhttps://github.com/sfi-norwai/eMargin.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u8fb9\u8ddd\uff08eMargin\uff09\u5bf9\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u5728\u65e0\u76d1\u7763\u805a\u7c7b\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u63a2\u7d22\u81ea\u9002\u5e94\u8fb9\u8ddd\u662f\u5426\u80fd\u901a\u8fc7\u8c03\u6574\u76f8\u4f3c\u6027\u9608\u503c\u6765\u6539\u5584\u76f8\u90bb\u4f46\u4e0d\u76f8\u4f3c\u65f6\u95f4\u6b65\u7684\u5206\u79bb\uff0c\u4ece\u800c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u5728\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u81ea\u9002\u5e94\u8fb9\u8ddd\uff08eMargin\uff09\uff0c\u5e76\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5176\u5bf9\u805a\u7c7b\u548c\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "eMargin\u5728\u65e0\u76d1\u7763\u805a\u7c7b\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f46\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u65e0\u76d1\u7763\u805a\u7c7b\u6307\u6807\u7684\u9ad8\u5206\u5e76\u4e0d\u4e00\u5b9a\u610f\u5473\u7740\u5b66\u5230\u7684\u5d4c\u5165\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6709\u6548\u3002"}}
{"id": "2507.14851", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14851", "abs": "https://arxiv.org/abs/2507.14851", "authors": ["Muhammad Kamran Janjua", "Amirhosein Ghasemabadi", "Kunlin Zhang", "Mohammad Salameh", "Chao Gao", "Di Niu"], "title": "Grounding Degradations in Natural Language for All-In-One Video Restoration", "comment": "17 pages", "summary": "In this work, we propose an all-in-one video restoration framework that\ngrounds degradation-aware semantic context of video frames in natural language\nvia foundation models, offering interpretable and flexible guidance. Unlike\nprior art, our method assumes no degradation knowledge in train or test time\nand learns an approximation to the grounded knowledge such that the foundation\nmodel can be safely disentangled during inference adding no extra cost.\nFurther, we call for standardization of benchmarks in all-in-one video\nrestoration, and propose two benchmarks in multi-degradation setting,\nthree-task (3D) and four-task (4D), and two time-varying composite degradation\nbenchmarks; one of the latter being our proposed dataset with varying snow\nintensity, simulating how weather degradations affect videos naturally. We\ncompare our method with prior works and report state-of-the-art performance on\nall benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u89c6\u9891\u4fee\u590d\u6846\u67b6\uff0c\u65e0\u9700\u9884\u77e5\u9000\u5316\u7c7b\u578b\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9884\u77e5\u9000\u5316\u7c7b\u578b\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u9000\u5316\u8bed\u4e49\uff0c\u5b9e\u73b0\u65e0\u9700\u9000\u5316\u77e5\u8bc6\u7684\u89c6\u9891\u4fee\u590d\u3002", "method": "\u5229\u7528\u57fa\u7840\u6a21\u578b\u5b66\u4e60\u9000\u5316\u611f\u77e5\u7684\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u89e3\u8026\u6a21\u578b\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u62ec\u65b0\u63d0\u51fa\u7684\u65f6\u53d8\u590d\u5408\u9000\u5316\u6570\u636e\u96c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u9000\u5316\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u547c\u5401\u89c6\u9891\u4fee\u590d\u9886\u57df\u7684\u57fa\u51c6\u6807\u51c6\u5316\u3002"}}
{"id": "2507.14843", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14843", "abs": "https://arxiv.org/abs/2507.14843", "authors": ["Fang Wu", "Weihao Xuan", "Ximing Lu", "Zaid Harchaoui", "Yejin Choi"], "title": "The Invisible Leash: Why RLVR May Not Escape Its Origin", "comment": null, "summary": "Recent advances in large reasoning models highlight Reinforcement Learning\nwith Verifiable Rewards (RLVR) as a promising method for enhancing AI's\ncapabilities, particularly in solving complex logical tasks. However, it\nremains unclear whether RLVR truly expands a model's reasoning boundary or\nmerely amplifies high-reward outputs that the base model already knows for\nimproved precision. This study presents a theoretical and empirical\ninvestigation that provides fresh insights into the potential limits of RLVR.\nFirst, we offer a new theoretical perspective that RLVR is constrained by the\nbase model's support-unable to sample solutions with zero initial\nprobability-and operates as a conservative reweighting mechanism that may\nrestrict the discovery of entirely original solutions. We also identify an\nentropy-reward tradeoff: while RLVR reliably enhances precision, it may\nprogressively narrow exploration and potentially overlook correct yet\nunderrepresented solutions. Extensive empirical experiments validate that while\nRLVR consistently improves pass@1, the shrinkage of empirical support generally\noutweighs the expansion of empirical support under larger sampling budgets,\nfailing to recover correct answers that were previously accessible to the base\nmodel. Interestingly, we also observe that while RLVR sometimes increases\ntoken-level entropy, resulting in greater uncertainty at each generation step,\nanswer-level entropy declines, indicating that these seemingly more uncertain\npaths ultimately converge onto a smaller set of distinct answers. Taken\ntogether, these findings reveal potential limits of RLVR in extending reasoning\nhorizons. Breaking this invisible leash may require future algorithmic\ninnovations such as explicit exploration mechanisms or hybrid strategies that\nseed probability mass into underrepresented solution regions.", "AI": {"tldr": "RLVR\u80fd\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u53ef\u80fd\u9650\u5236\u63a2\u7d22\u65b0\u89e3\u7684\u80fd\u529b\u3002", "motivation": "\u63a2\u8ba8RLVR\u662f\u5426\u80fd\u771f\u6b63\u6269\u5c55\u6a21\u578b\u7684\u63a8\u7406\u8fb9\u754c\uff0c\u8fd8\u662f\u4ec5\u653e\u5927\u5df2\u6709\u9ad8\u5956\u52b1\u8f93\u51fa\u3002", "method": "\u7406\u8bba\u4e0e\u5b9e\u8bc1\u7ed3\u5408\uff0c\u5206\u6790RLVR\u7684\u7ea6\u675f\u4e0e\u71b5-\u5956\u52b1\u6743\u8861\u3002", "result": "RLVR\u63d0\u5347\u7cbe\u786e\u5ea6\u4f46\u7f29\u5c0f\u63a2\u7d22\u8303\u56f4\uff0c\u53ef\u80fd\u5ffd\u7565\u6b63\u786e\u4f46\u4f4e\u6982\u7387\u7684\u89e3\u3002", "conclusion": "RLVR\u5728\u6269\u5c55\u63a8\u7406\u8fb9\u754c\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u65b0\u7b97\u6cd5\u7a81\u7834\u3002"}}
{"id": "2507.14855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14855", "abs": "https://arxiv.org/abs/2507.14855", "authors": ["Xingshu Chen", "Sicheng Yu", "Chong Cheng", "Hao Wang", "Ting Tian"], "title": "An Uncertainty-aware DETR Enhancement Framework for Object Detection", "comment": null, "summary": "This paper investigates the problem of object detection with a focus on\nimproving both the localization accuracy of bounding boxes and explicitly\nmodeling prediction uncertainty. Conventional detectors rely on deterministic\nbounding box regression, ignoring uncertainty in predictions and limiting model\nrobustness. In this paper, we propose an uncertainty-aware enhancement\nframework for DETR-based object detectors. We model bounding boxes as\nmultivariate Gaussian distributions and incorporate the Gromov-Wasserstein\ndistance into the loss function to better align the predicted and ground-truth\ndistributions. Building on this, we derive a Bayes Risk formulation to filter\nhigh-risk information and improve detection reliability. We also propose a\nsimple algorithm to quantify localization uncertainty via confidence intervals.\nExperiments on the COCO benchmark show that our method can be effectively\nintegrated into existing DETR variants, enhancing their performance. We further\nextend our framework to leukocyte detection tasks, achieving state-of-the-art\nresults on the LISC and WBCDD datasets. These results confirm the scalability\nof our framework across both general and domain-specific detection tasks. Code\npage:\nhttps://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDETR\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u6a21\u8fb9\u754c\u6846\u4e3a\u9ad8\u65af\u5206\u5e03\u5e76\u5f15\u5165Gromov-Wasserstein\u8ddd\u79bb\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u5668\u5ffd\u7565\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5c06\u8fb9\u754c\u6846\u5efa\u6a21\u4e3a\u591a\u5143\u9ad8\u65af\u5206\u5e03\uff0c\u5728\u635f\u5931\u51fd\u6570\u4e2d\u52a0\u5165Gromov-Wasserstein\u8ddd\u79bb\uff0c\u5e76\u57fa\u4e8e\u8d1d\u53f6\u65af\u98ce\u9669\u8fc7\u6ee4\u9ad8\u98ce\u9669\u4fe1\u606f\u3002", "result": "\u5728COCO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u63d0\u5347\u4e86DETR\u53d8\u4f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u767d\u7ec6\u80de\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u3002", "conclusion": "\u6846\u67b6\u5728\u901a\u7528\u548c\u7279\u5b9a\u9886\u57df\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5747\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.14847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14847", "abs": "https://arxiv.org/abs/2507.14847", "authors": ["Junhan Yu", "Zhunyi Feng", "Junwei Lu", "Tianxi Cai", "Doudou Zhou"], "title": "Time-Aware Attention for Enhanced Electronic Health Records Modeling", "comment": null, "summary": "Electronic Health Records (EHR) contain valuable clinical information for\npredicting patient outcomes and guiding healthcare decisions. However,\neffectively modeling Electronic Health Records (EHRs) requires addressing data\nheterogeneity and complex temporal patterns. Standard approaches often struggle\nwith irregular time intervals between clinical events. We propose TALE-EHR, a\nTransformer-based framework featuring a novel time-aware attention mechanism\nthat explicitly models continuous temporal gaps to capture fine-grained\nsequence dynamics. To complement this temporal modeling with robust semantics,\nTALE-EHR leverages embeddings derived from standardized code descriptions using\na pre-trained Large Language Model (LLM), providing a strong foundation for\nunderstanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset\ndemonstrate that our approach outperforms state-of-the-art baselines on tasks\nsuch as disease progression forecasting. TALE-EHR underscores the benefit of\nintegrating explicit, continuous temporal modeling with strong semantic\nrepresentations provides a powerful solution for advancing EHR analysis.", "AI": {"tldr": "TALE-EHR\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\uff0c\u89e3\u51b3\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u65f6\u95f4\u95f4\u9694\u548c\u8bed\u4e49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u5305\u542b\u4e30\u5bcc\u7684\u4e34\u5e8a\u4fe1\u606f\uff0c\u4f46\u6570\u636e\u5f02\u6784\u6027\u548c\u590d\u6742\u7684\u65f6\u95f4\u6a21\u5f0f\u4f7f\u5176\u5efa\u6a21\u56f0\u96be\uff0c\u5c24\u5176\u662f\u4e34\u5e8a\u4e8b\u4ef6\u4e4b\u95f4\u7684\u4e0d\u89c4\u5219\u65f6\u95f4\u95f4\u9694\u95ee\u9898\u3002", "method": "\u63d0\u51faTALE-EHR\u6846\u67b6\uff0c\u7ed3\u5408\u65f6\u95f4\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u6807\u51c6\u5316\u4ee3\u7801\u63cf\u8ff0\u5d4c\u5165\uff0c\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u5e8f\u5217\u52a8\u6001\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "result": "\u5728MIMIC-IV\u548cPIC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTALE-EHR\u5728\u75be\u75c5\u8fdb\u5c55\u9884\u6d4b\u7b49\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "TALE-EHR\u901a\u8fc7\u6574\u5408\u8fde\u7eed\u65f6\u95f4\u5efa\u6a21\u548c\u5f3a\u8bed\u4e49\u8868\u793a\uff0c\u4e3aEHR\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14867", "abs": "https://arxiv.org/abs/2507.14867", "authors": ["Zhaoqiang Xia", "Hexiang Huang", "Haoyu Chen", "Xiaoyi Feng", "Guoying Zhao"], "title": "Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition", "comment": null, "summary": "Micro-gestures are unconsciously performed body gestures that can convey the\nemotion states of humans and start to attract more research attention in the\nfields of human behavior understanding and affective computing as an emerging\ntopic. However, the modeling of human emotion based on micro-gestures has not\nbeen explored sufficiently. In this work, we propose to recognize the emotion\nstates based on the micro-gestures by reconstructing the behavior patterns with\na hypergraph-enhanced Transformer in a hybrid-supervised framework. In the\nframework, hypergraph Transformer based encoder and decoder are separately\ndesigned by stacking the hypergraph-enhanced self-attention and multiscale\ntemporal convolution modules. Especially, to better capture the subtle motion\nof micro-gestures, we construct a decoder with additional upsampling operations\nfor a reconstruction task in a self-supervised learning manner. We further\npropose a hypergraph-enhanced self-attention module where the hyperedges\nbetween skeleton joints are gradually updated to present the relationships of\nbody joints for modeling the subtle local motion. Lastly, for exploiting the\nrelationship between the emotion states and local motion of micro-gestures, an\nemotion recognition head from the output of encoder is designed with a shallow\narchitecture and learned in a supervised way. The end-to-end framework is\njointly trained in a one-stage way by comprehensively utilizing\nself-reconstruction and supervision information. The proposed method is\nevaluated on two publicly available datasets, namely iMiGUE and SMG, and\nachieves the best performance under multiple metrics, which is superior to the\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u56fe\u589e\u5f3aTransformer\u7684\u6df7\u5408\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5fae\u624b\u52bf\u8bc6\u522b\u60c5\u7eea\u72b6\u6001\uff0c\u5e76\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u5fae\u624b\u52bf\u80fd\u4f20\u8fbe\u4eba\u7c7b\u60c5\u7eea\u72b6\u6001\uff0c\u4f46\u57fa\u4e8e\u5fae\u624b\u52bf\u7684\u60c5\u7eea\u5efa\u6a21\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u8d85\u56fe\u589e\u5f3aTransformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u548c\u76d1\u7763\u5b66\u4e60\uff0c\u6355\u6349\u5fae\u624b\u52bf\u7684\u7ec6\u5fae\u8fd0\u52a8\u3002", "result": "\u5728iMiGUE\u548cSMG\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df7\u5408\u76d1\u7763\u548c\u8d85\u56fe\u5efa\u6a21\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5fae\u624b\u52bf\u60c5\u7eea\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14879", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14879", "abs": "https://arxiv.org/abs/2507.14879", "authors": ["Rizhao Fan", "Tianfang Ma", "Zhigen Li", "Ning An", "Jian Cheng"], "title": "Region-aware Depth Scale Adaptation with Sparse Measurements", "comment": null, "summary": "In recent years, the emergence of foundation models for depth prediction has\nled to remarkable progress, particularly in zero-shot monocular depth\nestimation. These models generate impressive depth predictions; however, their\noutputs are often in relative scale rather than metric scale. This limitation\nposes challenges for direct deployment in real-world applications. To address\nthis, several scale adaptation methods have been proposed to enable foundation\nmodels to produce metric depth. However, these methods are typically costly, as\nthey require additional training on new domains and datasets. Moreover,\nfine-tuning these models often compromises their original generalization\ncapabilities, limiting their adaptability across diverse scenes. In this paper,\nwe introduce a non-learning-based approach that leverages sparse depth\nmeasurements to adapt the relative-scale predictions of foundation models into\nmetric-scale depth. Our method requires neither retraining nor fine-tuning,\nthereby preserving the strong generalization ability of the original foundation\nmodels while enabling them to produce metric depth. Experimental results\ndemonstrate the effectiveness of our approach, high-lighting its potential to\nbridge the gap between relative and metric depth without incurring additional\ncomputational costs or sacrificing generalization ability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u5c06\u57fa\u7840\u6a21\u578b\u7684\u76f8\u5bf9\u5c3a\u5ea6\u6df1\u5ea6\u9884\u6d4b\u8f6c\u6362\u4e3a\u5ea6\u91cf\u5c3a\u5ea6\u6df1\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u8f93\u51fa\u4e3a\u76f8\u5bf9\u5c3a\u5ea6\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002\u73b0\u6709\u5c3a\u5ea6\u9002\u5e94\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u975e\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u7a00\u758f\u6df1\u5ea6\u6d4b\u91cf\u5c06\u76f8\u5bf9\u5c3a\u5ea6\u6df1\u5ea6\u9884\u6d4b\u8f6c\u6362\u4e3a\u5ea6\u91cf\u5c3a\u5ea6\u6df1\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u5728\u4e0d\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u6216\u727a\u7272\u6cdb\u5316\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u76f8\u5bf9\u5c3a\u5ea6\u5230\u5ea6\u91cf\u5c3a\u5ea6\u7684\u8f6c\u6362\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u8f93\u51fa\u4e3a\u76f8\u5bf9\u5c3a\u5ea6\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.14874", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14874", "abs": "https://arxiv.org/abs/2507.14874", "authors": ["Ole-Christoffer Granmo", "Youmna Abdelwahab", "Per-Arne Andersen", "Paul F. A. Clarke", "Kunal Dumbre", "Ylva Gr\u00f8nnins\u00e6ter", "Vojtech Halenka", "Runar Helin", "Lei Jiao", "Ahmed Khalid", "Rebekka Omslandseter", "Rupsa Saha", "Mayur Shende", "Xuan Zhang"], "title": "The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs", "comment": "34 pages, 10 figures", "summary": "Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine\n(TM) both interpretable and efficient, while the power of Tsetlin automata\nenables accuracy comparable to deep learning on an increasing number of\ndatasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning\ninterpretable deep clauses from graph-structured input. Moving beyond flat,\nfixed-length input, the GraphTM gets more versatile, supporting sequences,\ngrids, relations, and multimodality. Through message passing, the GraphTM\nbuilds nested deep clauses to recognize sub-graph patterns with exponentially\nfewer clauses, increasing both interpretability and data utilization. For image\nclassification, GraphTM preserves interpretability and achieves 3.86%-points\nhigher accuracy on CIFAR-10 than a convolutional TM. For tracking action\ncoreference, faced with increasingly challenging tasks, GraphTM outperforms\nother reinforcement learning methods by up to 20.6%-points. In recommendation\nsystems, it tolerates increasing noise to a greater extent than a Graph\nConvolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains\naccuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence\ndata, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training\n2.5x faster than GCN. The GraphTM's application to these varied fields\ndemonstrates how graph representation learning and deep clauses bring new\npossibilities for TM learning.", "AI": {"tldr": "Graph Tsetlin Machine (GraphTM) \u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u8f93\u5165\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u6784\u5efa\u6df1\u5ea6\u5b50\u53e5\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u548c\u6570\u636e\u5229\u7528\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf Tsetlin Machine (TM) \u5728\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u65f6\u53d7\u9650\uff0cGraphTM \u65e8\u5728\u6269\u5c55 TM \u7684\u80fd\u529b\uff0c\u652f\u6301\u5e8f\u5217\u3001\u7f51\u683c\u3001\u5173\u7cfb\u548c\u591a\u79cd\u6a21\u6001\u8f93\u5165\u3002", "method": "GraphTM \u901a\u8fc7\u6d88\u606f\u4f20\u9012\u6784\u5efa\u5d4c\u5957\u6df1\u5ea6\u5b50\u53e5\uff0c\u8bc6\u522b\u5b50\u56fe\u6a21\u5f0f\uff0c\u51cf\u5c11\u5b50\u53e5\u6570\u91cf\uff0c\u63d0\u5347\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "GraphTM \u5728\u56fe\u50cf\u5206\u7c7b\u3001\u52a8\u4f5c\u5171\u6307\u8ddf\u8e2a\u3001\u63a8\u8350\u7cfb\u7edf\u548c\u75c5\u6bd2\u57fa\u56e0\u7ec4\u5e8f\u5217\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u548c\u6548\u7387\u5747\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "GraphTM \u7ed3\u5408\u56fe\u8868\u793a\u5b66\u4e60\u548c\u6df1\u5ea6\u5b50\u53e5\uff0c\u4e3a TM \u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u53ef\u80fd\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u3002"}}
{"id": "2507.14885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14885", "abs": "https://arxiv.org/abs/2507.14885", "authors": ["Joaquim Comas", "Federico Sukno"], "title": "BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters", "comment": null, "summary": "Remote photoplethysmography (rPPG) captures cardiac signals from facial\nvideos and is gaining attention for its diverse applications. While deep\nlearning has advanced rPPG estimation, it relies on large, diverse datasets for\neffective generalization. In contrast, handcrafted methods utilize\nphysiological priors for better generalization in unseen scenarios like motion\nwhile maintaining computational efficiency. However, their linear assumptions\nlimit performance in complex conditions, where deep learning provides superior\npulsatile information extraction. This highlights the need for hybrid\napproaches that combine the strengths of both methods. To address this, we\npresent BeatFormer, a lightweight spectral attention model for rPPG estimation,\nwhich integrates zoomed orthonormal complex attention and frequency-domain\nenergy measurement, enabling a highly efficient model. Additionally, we\nintroduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be\ntrained without any PPG or HR labels. We validate BeatFormer on the PURE,\nUBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,\nparticularly in cross-dataset evaluations under motion scenarios.", "AI": {"tldr": "BeatFormer\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u624b\u5de5\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5149\u8c31\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u7528\u4e8erPPG\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u65e0\u76d1\u7763\u5b66\u4e60\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5927\u6570\u636e\u96c6\uff0c\u800c\u624b\u5de5\u65b9\u6cd5\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u6027\u80fd\u6709\u9650\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u63d0\u51faBeatFormer\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u653e\u5927\u7684\u6b63\u4ea4\u590d\u6570\u6ce8\u610f\u529b\u548c\u9891\u57df\u80fd\u91cf\u6d4b\u91cf\uff0c\u5e76\u5f15\u5165\u65e0\u76d1\u7763\u7684SCL\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728PURE\u3001UBFC-rPPG\u548cMMPD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8fd0\u52a8\u573a\u666f\u4e0b\u7684\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "BeatFormer\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86rPPG\u4f30\u8ba1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.14882", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14882", "abs": "https://arxiv.org/abs/2507.14882", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Amjad Haider", "Daniel G\u00f6rges"], "title": "Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization", "comment": "6 pages, 22nd International Conference on Advanced Robotics (ICAR\n  2025)", "summary": "Deep neural networks (DNNs) offer significant versatility and performance\nbenefits, but their widespread adoption is often hindered by high model\ncomplexity and computational demands. Model compression techniques such as\npruning have emerged as promising solutions to these challenges. However, it\nremains critical to ensure that application-specific performance\ncharacteristics are preserved during compression. In structured pruning, where\ngroups of structurally coherent elements are removed, conventional importance\nmetrics frequently fail to maintain these essential performance attributes. In\nthis work, we propose an enhanced importance metric framework that not only\nreduces model size but also explicitly accounts for application-specific\nperformance constraints. We employ multiple strategies to determine the optimal\npruning magnitude for each group, ensuring a balance between compression and\ntask performance. Our approach is evaluated on an autoencoder tasked with\nreconstructing MNIST images. Experimental results demonstrate that the proposed\nmethod effectively preserves task-relevant performance, maintaining the model's\nusability even after substantial pruning, by satisfying the required\napplication-specific criteria.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u526a\u679d\u91cd\u8981\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u5728\u538b\u7f29\u6a21\u578b\u7684\u540c\u65f6\u4fdd\u6301\u5e94\u7528\u7279\u5b9a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u5728\u4fdd\u6301\u5e94\u7528\u7279\u5b9a\u6027\u80fd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u7684\u91cd\u8981\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u79cd\u7b56\u7565\u786e\u5b9a\u6700\u4f18\u526a\u679d\u5e45\u5ea6\u3002", "result": "\u5728MNIST\u56fe\u50cf\u91cd\u5efa\u4efb\u52a1\u4e2d\uff0c\u5373\u4f7f\u5927\u5e45\u526a\u679d\u540e\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u538b\u7f29\u4e0e\u6027\u80fd\uff0c\u6ee1\u8db3\u5e94\u7528\u9700\u6c42\u3002"}}
{"id": "2507.14904", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14904", "abs": "https://arxiv.org/abs/2507.14904", "authors": ["Fan Li", "Zanyi Wang", "Zeyi Huang", "Guang Dai", "Jingdong Wang", "Mengmeng Wang"], "title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP", "comment": null, "summary": "3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e2D\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u7f51\u7edc\u7684\u7edf\u4e00\u65b9\u6cd5\uff0c\u7b80\u5316\u4e863D\u89c6\u89c9\u5b9a\u4f4d\u7684\u67b6\u6784\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u5e76\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u6a21\u6001\u5206\u79bb\u7f16\u7801\u5668\uff0c\u5bfc\u81f4\u6a21\u578b\u590d\u6742\u4e14\u8bad\u7ec3\u6548\u7387\u4f4e\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u5229\u75282D CLIP\u53cc\u6a21\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u9002\u914d\u5668\u5fae\u8c03\u6269\u5c55\u5230\u4e09\u6a21\u6001\uff0c\u7ed3\u5408GARF\u6a21\u5757\u878d\u5408\u51e0\u4f55\u7279\u5f81\u3002", "result": "\u53c2\u6570\u51cf\u5c1158%\uff0c3D\u68c0\u6d4b\u548c\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u5206\u522b\u63d0\u53476.52%\u548c6.25%\u3002", "conclusion": "\u7edf\u4e00\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u663e\u8457\u7b80\u5316\u6a21\u578b\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u9002\u7528\u4e8e3D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u3002"}}
{"id": "2507.14919", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.14919", "abs": "https://arxiv.org/abs/2507.14919", "authors": ["Maximilian Wendlinger", "Kilian Tscharke", "Pascal Debus"], "title": "Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning", "comment": null, "summary": "One of the key obstacles in traditional deep learning is the reduction in\nmodel transparency caused by increasingly intricate model functions, which can\nlead to problems such as overfitting and excessive confidence in predictions.\nWith the advent of quantum machine learning offering possible advances in\ncomputational power and latent space complexity, we notice the same opaque\nbehavior. Despite significant research in classical contexts, there has been\nlittle advancement in addressing the black-box nature of quantum machine\nlearning. Consequently, we approach this gap by building upon existing work in\nclassical uncertainty quantification and initial explorations in quantum\nBayesian modeling to theoretically develop and empirically evaluate techniques\nto map classical uncertainty quantification methods to the quantum machine\nlearning domain. Our findings emphasize the necessity of leveraging classical\ninsights into uncertainty quantification to include uncertainty awareness in\nthe process of designing new quantum machine learning models.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6a21\u578b\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5e94\u7528\u4e8e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u7406\u8bba\u548c\u6280\u672f\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u6a21\u578b\u590d\u6742\u6027\u7684\u589e\u52a0\u5bfc\u81f4\u900f\u660e\u5ea6\u964d\u4f4e\uff0c\u53ef\u80fd\u5f15\u53d1\u8fc7\u62df\u5408\u548c\u9884\u6d4b\u8fc7\u5ea6\u81ea\u4fe1\u7b49\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u9ed1\u7bb1\u95ee\u9898\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u57fa\u4e8e\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u91cf\u5b50\u8d1d\u53f6\u65af\u5efa\u6a21\u7684\u521d\u6b65\u63a2\u7d22\uff0c\u7406\u8bba\u5f00\u53d1\u548c\u5b9e\u8bc1\u8bc4\u4f30\u5c06\u7ecf\u5178\u65b9\u6cd5\u6620\u5c04\u5230\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u9886\u57df\u7684\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9700\u8981\u5229\u7528\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u89c1\u89e3\uff0c\u5728\u8bbe\u8ba1\u65b0\u7684\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65f6\u7eb3\u5165\u4e0d\u786e\u5b9a\u6027\u610f\u8bc6\u3002", "conclusion": "\u901a\u8fc7\u5c06\u7ecf\u5178\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5e94\u7528\u4e8e\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.14918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14918", "abs": "https://arxiv.org/abs/2507.14918", "authors": ["Ren-Dong Xie", "Zhi-Fen He", "Bo Li", "Bin Liu", "Jin-Yan Hu"], "title": "Semantic-Aware Representation Learning for Multi-label Image Classification", "comment": null, "summary": "Multi-label image classification, an important research area in computer\nvision, focuses on identifying multiple labels or concepts within an image.\nExisting approaches often employ attention mechanisms or graph convolutional\nnetworks (GCNs) to learn image representation. However, this representation may\ncontain noise and may not locate objects precisely. Therefore, this paper\nproposes a Semantic-Aware Representation Learning (SARL) for multi-label image\nclassification. First, a label semantic-related feature learning module is\nutilized to extract semantic-related features. Then, an optimal transport-based\nattention mechanism is designed to obtain semantically aligned image\nrepresentation. Finally, a regional score aggregation strategy is used for\nmulti-label prediction. Experimental results on two benchmark datasets, PASCAL\nVOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u8868\u793a\u5b66\u4e60\uff08SARL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6807\u7b7e\u56fe\u50cf\u5206\u7c7b\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u5173\u7279\u5f81\u5b66\u4e60\u548c\u6700\u4f18\u4f20\u8f93\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6ce8\u610f\u529b\u673a\u5236\u6216GCN\uff09\u7684\u8868\u793a\u53ef\u80fd\u5305\u542b\u566a\u58f0\u4e14\u65e0\u6cd5\u7cbe\u786e\u5b9a\u4f4d\u5bf9\u8c61\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "1. \u8bed\u4e49\u76f8\u5173\u7279\u5f81\u5b66\u4e60\u6a21\u5757\uff1b2. \u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u6ce8\u610f\u529b\u673a\u5236\uff1b3. \u533a\u57df\u5206\u6570\u805a\u5408\u7b56\u7565\u3002", "result": "\u5728PASCAL VOC 2007\u548cMS-COCO\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SARL\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u7cbe\u786e\u8868\u793a\u63d0\u5347\u4e86\u591a\u6807\u7b7e\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2507.14980", "categories": ["cs.LG", "68T05, 90C26", "I.2.6; I.5.1; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14980", "abs": "https://arxiv.org/abs/2507.14980", "authors": ["Tianle Li", "Yongzhi Huang", "Linshan Jiang", "Qipeng Xie", "Chang Liu", "Wenfeng Du", "Lu Wang", "Kaishun Wu"], "title": "FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios", "comment": "ICPP, including appendix", "summary": "Federated Learning (FL) enables decentralized model training while preserving\ndata privacy. Despite its benefits, FL faces challenges with non-identically\ndistributed (non-IID) data, especially in long-tailed scenarios with imbalanced\nclass samples. Momentum-based FL methods, often used to accelerate FL\nconvergence, struggle with these distributions, resulting in biased models and\nmaking FL hard to converge. To understand this challenge, we conduct extensive\ninvestigations into this phenomenon, accompanied by a layer-wise analysis of\nneural network behavior. Based on these insights, we propose FedWCM, a method\nthat dynamically adjusts momentum using global and per-round data to correct\ndirectional biases introduced by long-tailed distributions. Extensive\nexperiments show that FedWCM resolves non-convergence issues and outperforms\nexisting methods, enhancing FL's efficiency and effectiveness in handling\nclient heterogeneity and data imbalance.", "AI": {"tldr": "FedWCM\u52a8\u6001\u8c03\u6574\u52a8\u91cf\u4ee5\u89e3\u51b3\u957f\u5c3e\u6570\u636e\u5206\u5e03\u5bfc\u81f4\u7684FL\u6536\u655b\u95ee\u9898\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u89e3\u51b3FL\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08\u975eIID\uff09\u6570\u636e\uff0c\u5c24\u5176\u662f\u957f\u5c3e\u573a\u666f\u4e0b\u7684\u6536\u655b\u95ee\u9898\u3002", "method": "\u63d0\u51faFedWCM\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u52a8\u91cf\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u6bcf\u8f6e\u6570\u636e\u7ea0\u6b63\u65b9\u5411\u504f\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFedWCM\u89e3\u51b3\u4e86\u4e0d\u6536\u655b\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedWCM\u6709\u6548\u63d0\u5347\u4e86FL\u5728\u6570\u636e\u4e0d\u5e73\u8861\u548c\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.14921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14921", "abs": "https://arxiv.org/abs/2507.14921", "authors": ["Xiufeng Huang", "Ka Chun Cheung", "Runmin Cong", "Simon See", "Renjie Wan"], "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction", "comment": "ACMMM2025. Non-camera-ready version", "summary": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced\nImage-to-3D content creation but requires substantial computational resources\nand large datasets, posing challenges to training models from scratch. Current\nmethods usually entangle the prediction of 3D Gaussian geometry and appearance,\nwhich rely heavily on data-driven priors and result in slow regression speeds.\nTo address this, we propose \\method, a disentangled framework for efficient 3D\nGaussian prediction. Our method extracts features from local image pairs using\na stereo vision backbone and fuses them via global attention blocks. Dedicated\npoint and Gaussian prediction heads generate multi-view point-maps for geometry\nand Gaussian features for appearance, combined as GS-maps to represent the 3DGS\nobject. A refinement network enhances these GS-maps for high-quality\nreconstruction. Unlike existing methods that depend on camera parameters, our\napproach achieves pose-free 3D reconstruction, improving robustness and\npracticality. By reducing resource demands while maintaining high-quality\noutputs, \\method provides an efficient, scalable solution for real-world 3D\ncontent generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4b3D\u9ad8\u65af\uff0c\u901a\u8fc7\u5c40\u90e8\u56fe\u50cf\u5bf9\u7279\u5f81\u63d0\u53d6\u548c\u5168\u5c40\u6ce8\u610f\u529b\u878d\u5408\uff0c\u5b9e\u73b0\u65e0\u59ff\u6001\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57283D\u9ad8\u65af\u51e0\u4f55\u548c\u5916\u89c2\u9884\u6d4b\u4e0a\u8026\u5408\u5ea6\u9ad8\uff0c\u4f9d\u8d56\u6570\u636e\u9a71\u52a8\u5148\u9a8c\u4e14\u56de\u5f52\u901f\u5ea6\u6162\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u4f7f\u7528\u7acb\u4f53\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u5c40\u90e8\u56fe\u50cf\u5bf9\u7279\u5f81\uff0c\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u5757\u878d\u5408\uff0c\u751f\u6210\u51e0\u4f55\u548c\u5916\u89c2\u7684GS-maps\uff0c\u5e76\u901a\u8fc7\u7ec6\u5316\u7f51\u7edc\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u5b9e\u73b0\u4e86\u65e0\u59ff\u6001\u7684\u9ad8\u8d28\u91cf3D\u91cd\u5efa\uff0c\u964d\u4f4e\u4e86\u8d44\u6e90\u9700\u6c42\uff0c\u63d0\u5347\u4e86\u5b9e\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u96453D\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14999", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14999", "abs": "https://arxiv.org/abs/2507.14999", "authors": ["Yunfeng Li", "Junhong Liu", "Zhaohui Yang", "Guofu Liao", "Chuyun Zhang"], "title": "Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data", "comment": "10 pages,6 figures", "summary": "False Data Injection Attacks (FDIAs) pose severe security risks to smart\ngrids by manipulating measurement data collected from spatially distributed\ndevices such as SCADA systems and PMUs. These measurements typically exhibit\nNon-Independent and Identically Distributed (Non-IID) characteristics across\ndifferent regions, which significantly challenges the generalization ability of\ndetection models. Traditional centralized training approaches not only face\nprivacy risks and data sharing constraints but also incur high transmission\ncosts, limiting their scalability and deployment feasibility. To address these\nissues, this paper proposes a privacy-preserving federated learning framework,\ntermed Federated Cluster Average (FedClusAvg), designed to improve FDIA\ndetection in Non-IID and resource-constrained environments. FedClusAvg\nincorporates cluster-based stratified sampling and hierarchical communication\n(client-subserver-server) to enhance model generalization and reduce\ncommunication overhead. By enabling localized training and weighted parameter\naggregation, the algorithm achieves accurate model convergence without\ncentralizing sensitive data. Experimental results on benchmark smart grid\ndatasets demonstrate that FedClusAvg not only improves detection accuracy under\nheterogeneous data distributions but also significantly reduces communication\nrounds and bandwidth consumption. This work provides an effective solution for\nsecure and efficient FDIA detection in large-scale distributed power systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedClusAvg\u7684\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u63d0\u5347\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u5bf9\u667a\u80fd\u7535\u7f51\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3001\u6570\u636e\u5171\u4eab\u9650\u5236\u548c\u9ad8\u4f20\u8f93\u6210\u672c\u95ee\u9898\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684\u5206\u5c42\u91c7\u6837\u548c\u5206\u5c42\u901a\u4fe1\uff08\u5ba2\u6237\u7aef-\u5b50\u670d\u52a1\u5668-\u670d\u52a1\u5668\uff09\u673a\u5236\uff0c\u5b9e\u73b0\u5c40\u90e8\u8bad\u7ec3\u548c\u52a0\u6743\u53c2\u6570\u805a\u5408\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86FedClusAvg\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u8f6e\u6b21\u548c\u5e26\u5bbd\u6d88\u8017\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u865a\u5047\u6570\u636e\u6ce8\u5165\u653b\u51fb\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14924", "abs": "https://arxiv.org/abs/2507.14924", "authors": ["Kaishva Chintan Shah", "Virajith Boddapati", "Karthik S. Gurumoorthy", "Sandip Kaledhonkar", "Ajit Rajwade"], "title": "3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline", "comment": null, "summary": "Accurate pose estimation and shift correction are key challenges in cryo-EM\ndue to the very low SNR, which directly impacts the fidelity of 3D\nreconstructions. We present an approach for pose estimation in cryo-EM that\nleverages multi-dimensional scaling (MDS) techniques in a robust manner to\nestimate the 3D rotation matrix of each particle from pairs of dihedral angles.\nWe express the rotation matrix in the form of an axis of rotation and a unit\nvector in the plane perpendicular to the axis. The technique leverages the\nconcept of common lines in 3D reconstruction from projections. However, common\nline estimation is ridden with large errors due to the very low SNR of cryo-EM\nprojection images. To address this challenge, we introduce two complementary\ncomponents: (i) a robust joint optimization framework for pose estimation based\non an $\\ell_1$-norm objective or a similar robust norm, which simultaneously\nestimates rotation axes and in-plane vectors while exactly enforcing unit norm\nand orthogonality constraints via projected coordinate descent; and (ii) an\niterative shift correction algorithm that estimates consistent in-plane\ntranslations through a global least-squares formulation. While prior approaches\nhave leveraged such embeddings and common-line geometry for orientation\nrecovery, existing formulations typically rely on $\\ell_2$-based objectives\nthat are sensitive to noise, and enforce geometric constraints only\napproximately. These choices, combined with a sequential pipeline structure,\ncan lead to compounding errors and suboptimal reconstructions in low-SNR\nregimes. Our pipeline consistently outperforms prior methods in both Euler\nangle accuracy and reconstruction fidelity, as measured by the Fourier Shell\nCorrelation (FSC).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMDS\u548c\u9c81\u68d2\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u51b7\u51bb\u7535\u955c\u59ff\u6001\u4f30\u8ba1\u548c\u4f4d\u79fb\u6821\u6b63\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u51b7\u51bb\u7535\u955c\u4e2d\u4f4e\u4fe1\u566a\u6bd4\u5bfc\u81f4\u59ff\u6001\u4f30\u8ba1\u548c\u4f4d\u79fb\u6821\u6b63\u56f0\u96be\uff0c\u76f4\u63a5\u5f71\u54cd3D\u91cd\u5efa\u7684\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408MDS\u6280\u672f\u548c\u9c81\u68d2\u4f18\u5316\uff08\u2113\u2081\u8303\u6570\uff09\uff0c\u901a\u8fc7\u6295\u5f71\u5750\u6807\u4e0b\u964d\u6cd5\u4f30\u8ba1\u65cb\u8f6c\u77e9\u9635\uff0c\u5e76\u5f15\u5165\u8fed\u4ee3\u4f4d\u79fb\u6821\u6b63\u7b97\u6cd5\u3002", "result": "\u5728\u6b27\u62c9\u89d2\u7cbe\u5ea6\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\uff08FSC\uff09\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4f30\u8ba1\u548c\u91cd\u5efa\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15066", "categories": ["cs.LG", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.15066", "abs": "https://arxiv.org/abs/2507.15066", "authors": ["Yiyuan Yang", "Zichuan Liu", "Lei Song", "Kai Ying", "Zhiguang Wang", "Tom Bamford", "Svitlana Vyetrenko", "Jiang Bian", "Qingsong Wen"], "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback", "comment": "Under review. 19 pages, 8 figures, 12 tables", "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1Time-RA\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4ece\u5224\u522b\u5f0f\u4efb\u52a1\u8f6c\u53d8\u4e3a\u751f\u6210\u5f0f\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6RATs40K\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4e8c\u5143\u5206\u7c7b\uff0c\u7f3a\u4e4f\u8be6\u7ec6\u5206\u7c7b\u548c\u89e3\u91ca\u6027\u63a8\u7406\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u751f\u6210\u5f0f\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u5305\u542b40,000\u6837\u672c\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6RATs40K\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u6027\u80fd\u548c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u76d1\u7763\u5fae\u8c03\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u4efb\u52a1\u548c\u6570\u636e\u96c6\u4e3a\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u548c\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.14932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14932", "abs": "https://arxiv.org/abs/2507.14932", "authors": ["Francisco M. Castro-Mac\u00edas", "Pablo Morales-\u00c1lvarez", "Yunan Wu", "Rafael Molina", "Aggelos K. Katsaggelos"], "title": "Probabilistic smooth attention for deep multiple instance learning in medical imaging", "comment": null, "summary": "The Multiple Instance Learning (MIL) paradigm is attracting plenty of\nattention in medical imaging classification, where labeled data is scarce. MIL\nmethods cast medical images as bags of instances (e.g. patches in whole slide\nimages, or slices in CT scans), and only bag labels are required for training.\nDeep MIL approaches have obtained promising results by aggregating\ninstance-level representations via an attention mechanism to compute the\nbag-level prediction. These methods typically capture both local interactions\namong adjacent instances and global, long-range dependencies through various\nmechanisms. However, they treat attention values deterministically, potentially\noverlooking uncertainty in the contribution of individual instances. In this\nwork we propose a novel probabilistic framework that estimates a probability\ndistribution over the attention values, and accounts for both global and local\ninteractions. In a comprehensive evaluation involving {\\color{review} eleven}\nstate-of-the-art baselines and three medical datasets, we show that our\napproach achieves top predictive performance in different metrics. Moreover,\nthe probabilistic treatment of the attention provides uncertainty maps that are\ninterpretable in terms of illness localization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4f30\u8ba1\u6ce8\u610f\u529b\u503c\u7684\u6982\u7387\u5206\u5e03\u6765\u6355\u6349\u5168\u5c40\u548c\u5c40\u90e8\u4ea4\u4e92\uff0c\u5e76\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6MIL\u65b9\u6cd5\u901a\u5e38\u4ee5\u786e\u5b9a\u6027\u65b9\u5f0f\u5904\u7406\u6ce8\u610f\u529b\u503c\uff0c\u53ef\u80fd\u5ffd\u7565\u4e2a\u4f53\u5b9e\u4f8b\u8d21\u732e\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u6846\u67b6\uff0c\u4f30\u8ba1\u6ce8\u610f\u529b\u503c\u7684\u6982\u7387\u5206\u5e03\uff0c\u540c\u65f6\u8003\u8651\u5168\u5c40\u548c\u5c40\u90e8\u4ea4\u4e92\u3002", "result": "\u5728\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u548c\u5341\u4e00\u4e2a\u57fa\u7ebf\u6a21\u578b\u7684\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u89e3\u91ca\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u56fe\uff0c\u6709\u52a9\u4e8e\u75be\u75c5\u5b9a\u4f4d\u3002"}}
{"id": "2507.15067", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15067", "abs": "https://arxiv.org/abs/2507.15067", "authors": ["Bing He", "Mustaque Ahamad", "Srijan Kumar"], "title": "ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model", "comment": "15 pages, 12 tables", "summary": "Detecting bad actors is critical to ensure the safety and integrity of\ninternet platforms. Several deep learning-based models have been developed to\nidentify such users. These models should not only accurately detect bad actors,\nbut also be robust against adversarial attacks that aim to evade detection.\nHowever, past deep learning-based detection models do not meet the robustness\nrequirement because they are sensitive to even minor changes in the input\nsequence. To address this issue, we focus on (1) improving the model\nunderstanding capability and (2) enhancing the model knowledge such that the\nmodel can recognize potential input modifications when making predictions. To\nachieve these goals, we create a novel transformer-based classification model,\ncalled ROBAD (RObust adversary-aware local-global attended Bad Actor Detection\nmodel), which uses the sequence of user posts to generate user embedding to\ndetect bad actors. Particularly, ROBAD first leverages the transformer encoder\nblock to encode each post bidirectionally, thus building a post embedding to\ncapture the local information at the post level. Next, it adopts the\ntransformer decoder block to model the sequential pattern in the post\nembeddings by using the attention mechanism, which generates the sequence\nembedding to obtain the global information at the sequence level. Finally, to\nenrich the knowledge of the model, embeddings of modified sequences by mimicked\nattackers are fed into a contrastive-learning-enhanced classification layer for\nsequence prediction. In essence, by capturing the local and global information\n(i.e., the post and sequence information) and leveraging the mimicked behaviors\nof bad actors in training, ROBAD can be robust to adversarial attacks.\nExtensive experiments on Yelp and Wikipedia datasets show that ROBAD can\neffectively detect bad actors when under state-of-the-art adversarial attacks.", "AI": {"tldr": "ROBAD\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e92\u8054\u7f51\u5e73\u53f0\u4e0a\u7684\u4e0d\u826f\u884c\u4e3a\u8005\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u4ee5\u53ca\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u68c0\u6d4b\u4e0d\u826f\u884c\u4e3a\u8005\u65f6\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u5fae\u5c0f\u53d8\u5316\u654f\u611f\uff0c\u7f3a\u4e4f\u9c81\u68d2\u6027\u3002ROBAD\u65e8\u5728\u901a\u8fc7\u6539\u8fdb\u6a21\u578b\u7406\u89e3\u80fd\u529b\u548c\u589e\u5f3a\u6a21\u578b\u77e5\u8bc6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ROBAD\u4f7f\u7528Transformer\u7f16\u7801\u5668\u5757\u53cc\u5411\u7f16\u7801\u7528\u6237\u5e16\u5b50\u4ee5\u6355\u83b7\u5c40\u90e8\u4fe1\u606f\uff0c\u518d\u901a\u8fc7\u89e3\u7801\u5668\u5757\u5efa\u6a21\u5e8f\u5217\u6a21\u5f0f\u4ee5\u83b7\u53d6\u5168\u5c40\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u5206\u7c7b\u5c42\u3002", "result": "\u5728Yelp\u548cWikipedia\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cROBAD\u80fd\u6709\u6548\u62b5\u5fa1\u6700\u5148\u8fdb\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u51c6\u786e\u68c0\u6d4b\u4e0d\u826f\u884c\u4e3a\u8005\u3002", "conclusion": "ROBAD\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u4fe1\u606f\u6355\u83b7\u53ca\u5bf9\u6297\u6027\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5bf9\u6297\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.14935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14935", "abs": "https://arxiv.org/abs/2507.14935", "authors": ["Hai Huang", "Yan Xia", "Shulei Wang", "Hanting Wang", "Minghui Fang", "Shengpeng Ji", "Sashuai Zhou", "Tao Jin", "Zhou Zhao"], "title": "Open-set Cross Modal Generalization via Multimodal Unified Representation", "comment": "Accepted by ICCV 2025", "summary": "This paper extends Cross Modal Generalization (CMG) to open-set environments\nby proposing the more challenging Open-set Cross Modal Generalization (OSCMG)\ntask. This task evaluates multimodal unified representations in open-set\nconditions, addressing the limitations of prior closed-set cross-modal\nevaluations. OSCMG requires not only cross-modal knowledge transfer but also\nrobust generalization to unseen classes within new modalities, a scenario\nfrequently encountered in real-world applications. Existing multimodal unified\nrepresentation work lacks consideration for open-set environments. To tackle\nthis, we propose MICU, comprising two key components: Fine-Coarse Masked\nmultimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI\nenhances multimodal alignment by applying contrastive learning at both holistic\nsemantic and temporal levels, incorporating masking to enhance generalization.\nCUJP enhances feature diversity and model uncertainty by integrating\nmodality-agnostic feature selection with self-supervised learning, thereby\nstrengthening the model's ability to handle unknown categories in open-set\ntasks. Extensive experiments on CMG and the newly proposed OSCMG validate the\neffectiveness of our approach. The code is available at\nhttps://github.com/haihuangcode/CMG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5f00\u653e\u96c6\u8de8\u6a21\u6001\u6cdb\u5316\uff08OSCMG\uff09\u4efb\u52a1\uff0c\u6269\u5c55\u4e86\u8de8\u6a21\u6001\u6cdb\u5316\uff08CMG\uff09\u5230\u5f00\u653e\u96c6\u73af\u5883\uff0c\u5e76\u63d0\u51faMICU\u65b9\u6cd5\uff0c\u901a\u8fc7FCMI\u548cCUJP\u7ec4\u4ef6\u63d0\u5347\u6a21\u578b\u5728\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u7edf\u4e00\u8868\u793a\u65b9\u6cd5\u672a\u8003\u8651\u5f00\u653e\u96c6\u73af\u5883\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u9047\u5230\u672a\u89c1\u7c7b\u522b\u548c\u65b0\u6a21\u6001\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMICU\u65b9\u6cd5\uff0c\u5305\u542bFCMI\uff08\u7ec6\u7c92\u5ea6\u4e0e\u7c97\u7c92\u5ea6\u63a9\u7801\u5bf9\u6bd4\u5b66\u4e60\uff09\u548cCUJP\uff08\u8de8\u6a21\u6001\u7edf\u4e00\u62fc\u56fe\uff09\u4e24\u4e2a\u7ec4\u4ef6\u3002", "result": "\u5728CMG\u548c\u65b0\u63d0\u51fa\u7684OSCMG\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MICU\u65b9\u6cd5\u901a\u8fc7\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u7279\u5f81\u591a\u6837\u6027\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u5f00\u653e\u96c6\u6761\u4ef6\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15073", "abs": "https://arxiv.org/abs/2507.15073", "authors": ["Samuel Pfrommer", "Yixiao Huang", "Somayeh Sojoudi"], "title": "Reinforcement Learning for Flow-Matching Policies", "comment": null, "summary": "Flow-matching policies have emerged as a powerful paradigm for generalist\nrobotics. These models are trained to imitate an action chunk, conditioned on\nsensor observations and textual instructions. Often, training demonstrations\nare generated by a suboptimal policy, such as a human operator. This work\nexplores training flow-matching policies via reinforcement learning to surpass\nthe original demonstration policy performance. We particularly note\nminimum-time control as a key application and present a simple scheme for\nvariable-horizon flow-matching planning. We then introduce two families of\napproaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group\nRelative Policy Optimization (GRPO) approach with a learned reward surrogate.\nOur policies are trained on an illustrative suite of simulated unicycle\ndynamics tasks, and we show that both approaches dramatically improve upon the\nsuboptimal demonstrator performance, with the GRPO approach in particular\ngenerally incurring between $50\\%$ and $85\\%$ less cost than a naive Imitation\nLearning Flow Matching (ILFM) approach.", "AI": {"tldr": "Flow-matching policies\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8d85\u8d8a\u539f\u59cb\u6f14\u793a\u7b56\u7565\u6027\u80fd\uff0cGRPO\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6a21\u4eff\u5b66\u4e60\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347flow-matching\u7b56\u7565\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u539f\u59cb\u6f14\u793a\u7b56\u7565\u3002", "method": "\u63d0\u51faRWFM\u548cGRPO\u4e24\u79cd\u65b9\u6cd5\uff0cGRPO\u7ed3\u5408\u5956\u52b1\u4ee3\u7406\u5b66\u4e60\u3002", "result": "GRPO\u65b9\u6cd5\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u6bd4\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u964d\u4f4e50%-85%\u6210\u672c\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347flow-matching\u7b56\u7565\u6027\u80fd\uff0cGRPO\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2507.14959", "categories": ["cs.CV", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.14959", "abs": "https://arxiv.org/abs/2507.14959", "authors": ["Saeid Ghafouri", "Mohsen Fayyaz", "Xiangchen Li", "Deepu John", "Bo Ji", "Dimitrios Nikolopoulos", "Hans Vandierendonck"], "title": "Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices", "comment": null, "summary": "Real-time multi-label video classification on embedded devices is constrained\nby limited compute and energy budgets. Yet, video streams exhibit structural\nproperties such as label sparsity, temporal continuity, and label co-occurrence\nthat can be leveraged for more efficient inference. We introduce Polymorph, a\ncontext-aware framework that activates a minimal set of lightweight Low Rank\nAdapters (LoRA) per frame. Each adapter specializes in a subset of classes\nderived from co-occurrence patterns and is implemented as a LoRA weight over a\nshared backbone. At runtime, Polymorph dynamically selects and composes only\nthe adapters needed to cover the active labels, avoiding full-model switching\nand weight merging. This modular strategy improves scalability while reducing\nlatency and energy overhead. Polymorph achieves 40% lower energy consumption\nand improves mAP by 9 points over strong baselines on the TAO dataset.\nPolymorph is open source at https://github.com/inference-serving/polymorph/.", "AI": {"tldr": "Polymorph\u662f\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6fc0\u6d3b\u8f7b\u91cf\u7ea7\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u6765\u4f18\u5316\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u591a\u6807\u7b7e\u89c6\u9891\u5206\u7c7b\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u591a\u6807\u7b7e\u89c6\u9891\u5206\u7c7b\u53d7\u9650\u4e8e\u8ba1\u7b97\u548c\u80fd\u6e90\u9884\u7b97\uff0c\u4f46\u89c6\u9891\u6d41\u5177\u6709\u6807\u7b7e\u7a00\u758f\u6027\u3001\u65f6\u95f4\u8fde\u7eed\u6027\u548c\u6807\u7b7e\u5171\u73b0\u6027\u7b49\u7ed3\u6784\u7279\u6027\uff0c\u53ef\u7528\u4e8e\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u3002", "method": "Polymorph\u6846\u67b6\u52a8\u6001\u6fc0\u6d3b\u6bcf\u5e27\u6240\u9700\u7684\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\uff0c\u8fd9\u4e9b\u9002\u914d\u5668\u4e13\u7528\u4e8e\u5171\u73b0\u6a21\u5f0f\u6d3e\u751f\u7684\u5b50\u7c7b\uff0c\u5e76\u4f5c\u4e3a\u5171\u4eab\u4e3b\u5e72\u7684LoRA\u6743\u91cd\u5b9e\u73b0\u3002", "result": "\u5728TAO\u6570\u636e\u96c6\u4e0a\uff0cPolymorph\u80fd\u8017\u964d\u4f4e40%\uff0cmAP\u63d0\u9ad89\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "Polymorph\u901a\u8fc7\u6a21\u5757\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u89c6\u9891\u5206\u7c7b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.15079", "categories": ["cs.LG", "q-fin.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15079", "abs": "https://arxiv.org/abs/2507.15079", "authors": ["Arkadiusz Lipiecki", "Bartosz Uniejewski"], "title": "Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts", "comment": "Preprint", "summary": "Quantifying the uncertainty of forecasting models is essential to assess and\nmitigate the risks associated with data-driven decisions, especially in\nvolatile domains such as electricity markets. Machine learning methods can\nprovide highly accurate electricity price forecasts, critical for informing the\ndecisions of market participants. However, these models often lack uncertainty\nestimates, which limits the ability of decision makers to avoid unnecessary\nrisks. In this paper, we propose a novel method for generating probabilistic\nforecasts from ensembles of point forecasts, called Isotonic Quantile\nRegression Averaging (iQRA). Building on the established framework of Quantile\nRegression Averaging (QRA), we introduce stochastic order constraints to\nimprove forecast accuracy, reliability, and computational costs. In an\nextensive forecasting study of the German day-ahead electricity market, we show\nthat iQRA consistently outperforms state-of-the-art postprocessing methods in\nterms of both reliability and sharpness. It produces well-calibrated prediction\nintervals across multiple confidence levels, providing superior reliability to\nall benchmark methods, particularly coverage-based conformal prediction. In\naddition, isotonic regularization decreases the complexity of the quantile\nregression problem and offers a hyperparameter-free approach to variable\nselection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aiQRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u968f\u673a\u987a\u5e8f\u7ea6\u675f\u6539\u8fdb\u9884\u6d4b\u51c6\u786e\u6027\u3001\u53ef\u9760\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u4f18\u4e8e\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\u3002", "motivation": "\u7535\u529b\u5e02\u573a\u4ef7\u683c\u9884\u6d4b\u5bf9\u5e02\u573a\u53c2\u4e0e\u8005\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u9650\u5236\u4e86\u51b3\u7b56\u8005\u89c4\u907f\u98ce\u9669\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u5206\u4f4d\u6570\u56de\u5f52\u5e73\u5747\uff08QRA\uff09\u6846\u67b6\uff0c\u5f15\u5165\u968f\u673a\u987a\u5e8f\u7ea6\u675f\uff0c\u63d0\u51faiQRA\u65b9\u6cd5\uff0c\u751f\u6210\u6982\u7387\u9884\u6d4b\u3002", "result": "\u5728\u5fb7\u56fd\u65e5\u524d\u7535\u529b\u5e02\u573a\u7684\u9884\u6d4b\u7814\u7a76\u4e2d\uff0ciQRA\u5728\u53ef\u9760\u6027\u548c\u9510\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u4f9b\u6821\u51c6\u826f\u597d\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "iQRA\u901a\u8fc7\u7b49\u6e17\u6b63\u5219\u5316\u7b80\u5316\u5206\u4f4d\u6570\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u4f9b\u65e0\u8d85\u53c2\u6570\u53d8\u91cf\u9009\u62e9\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.14965", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14965", "abs": "https://arxiv.org/abs/2507.14965", "authors": ["Yaojie Zhang", "Tianlun Huang", "Weijun Wang", "Wei Feng"], "title": "Decision PCR: Decision version of the Point Cloud Registration task", "comment": null, "summary": "Low-overlap point cloud registration (PCR) remains a significant challenge in\n3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become\nineffective under extremely low inlier ratios. In this paper, we revisit the\nregistration result evaluation problem and identify the Decision version of the\nPCR task as the fundamental problem. To address this Decision PCR task, we\npropose a data-driven approach. First, we construct a corresponding dataset\nbased on the 3DMatch dataset. Then, a deep learning-based classifier is trained\nto reliably assess registration quality, overcoming the limitations of\ntraditional metrics. To our knowledge, this is the first comprehensive study to\naddress this task through a deep learning framework. We incorporate this\nclassifier into standard PCR pipelines. When integrated with our approach,\nexisting state-of-the-art PCR methods exhibit significantly enhanced\nregistration performance. For example, combining our framework with\nGeoTransformer achieves a new SOTA registration recall of 86.97\\% on the\nchallenging 3DLoMatch benchmark. Our method also demonstrates strong\ngeneralization capabilities on the unseen outdoor ETH dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f4e\u91cd\u53e0\u70b9\u4e91\u914d\u51c6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u914d\u51c6\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5728\u6781\u4f4e\u5185\u70b9\u7387\u4e0b\u5931\u6548\uff0c\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u914d\u51c6\u7ed3\u679c\u8bc4\u4f30\u95ee\u9898\uff0c\u63d0\u51fa\u51b3\u7b56\u7248\u914d\u51c6\u4efb\u52a1\u3002", "method": "\u6784\u5efa\u57fa\u4e8e3DMatch\u7684\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u8bc4\u4f30\u914d\u51c6\u8d28\u91cf\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6807\u51c6\u914d\u51c6\u6d41\u7a0b\u4e2d\u3002", "result": "\u7ed3\u5408GeoTransformer\u540e\uff0c\u57283DLoMatch\u57fa\u51c6\u4e0a\u8fbe\u523086.97%\u7684\u914d\u51c6\u53ec\u56de\u7387\uff0c\u5e76\u5728ETH\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u51b3\u7b56\u7248\u914d\u51c6\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u914d\u51c6\u6027\u80fd\uff0c\u5e76\u5177\u5907\u5f3a\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15082", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15082", "abs": "https://arxiv.org/abs/2507.15082", "authors": ["Qian Qi"], "title": "Robust Control with Gradient Uncertainty", "comment": null, "summary": "We introduce a novel extension to robust control theory that explicitly\naddresses uncertainty in the value function's gradient, a form of uncertainty\nendemic to applications like reinforcement learning where value functions are\napproximated. We formulate a zero-sum dynamic game where an adversary perturbs\nboth system dynamics and the value function gradient, leading to a new, highly\nnonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs\nEquation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness\nby proving a comparison principle for its viscosity solutions under a uniform\nellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a\nkey insight: we prove that the classical quadratic value function assumption\nfails for any non-zero gradient uncertainty, fundamentally altering the problem\nstructure. A formal perturbation analysis characterizes the non-polynomial\ncorrection to the value function and the resulting nonlinearity of the optimal\ncontrol law, which we validate with numerical studies. Finally, we bridge\ntheory to practice by proposing a novel Gradient-Uncertainty-Robust\nActor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating\nits effectiveness in stabilizing training. This work provides a new direction\nfor robust control, holding significant implications for fields where function\napproximation is common, including reinforcement learning and computational\nfinance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u63a7\u5236\u7406\u8bba\u6269\u5c55\uff0c\u89e3\u51b3\u4e86\u4ef7\u503c\u51fd\u6570\u68af\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u51fa\u4e86GU-HJBI\u65b9\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86GURAC\u7b97\u6cd5\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u7b49\u5e94\u7528\u4e2d\uff0c\u4ef7\u503c\u51fd\u6570\u7684\u8fd1\u4f3c\u5bfc\u81f4\u68af\u5ea6\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u96f6\u548c\u52a8\u6001\u535a\u5f08\u5efa\u6a21\uff0c\u63a8\u5bfc\u51faGU-HJBI\u65b9\u7a0b\uff0c\u5e76\u5206\u6790\u5176\u89e3\u7684\u6027\u8d28\uff1b\u5728\u7ebf\u6027\u4e8c\u6b21\u60c5\u51b5\u4e0b\u8fdb\u884c\u6270\u52a8\u5206\u6790\u3002", "result": "\u8bc1\u660e\u7ecf\u5178\u4e8c\u6b21\u4ef7\u503c\u51fd\u6570\u5047\u8bbe\u5728\u68af\u5ea6\u4e0d\u786e\u5b9a\u6027\u4e0b\u5931\u6548\uff0c\u63d0\u51fa\u4e86\u975e\u7ebf\u6027\u4fee\u6b63\uff1bGURAC\u7b97\u6cd5\u6709\u6548\u7a33\u5b9a\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9c81\u68d2\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5bf9\u5f3a\u5316\u5b66\u4e60\u548c\u8ba1\u7b97\u91d1\u878d\u7b49\u9886\u57df\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.14976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14976", "abs": "https://arxiv.org/abs/2507.14976", "authors": ["Hao Zheng", "Shunzhi Yang", "Zhuoxin He", "Jinfeng Yang", "Zhenhua Huang"], "title": "Hierarchical Cross-modal Prompt Learning for Vision-Language Models", "comment": "Accepted by ICCV2025", "summary": "Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent\ngeneralization abilities. However, adapting these large-scale models to\ndownstream tasks while preserving their generalization capabilities remains\nchallenging. Although prompt learning methods have shown promise, they suffer\nfrom two fundamental bottlenecks that limit generalization: (a) modality\nisolation, and (b) hierarchical semantic decay. To address these limitations,\nwe propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that\nestablishes bidirectional knowledge flow between text and vision modalities,\nenabling them to refine their semantics mutually. HiCroPL routes knowledge\nflows by leveraging the complementary strengths of text and vision. In early\nlayers, text prompts inject relatively clear semantics into visual prompts\nthrough a hierarchical knowledge mapper, enhancing the representation of\nlow-level visual semantics. In later layers, visual prompts encoding specific\ntask-relevant objects flow back to refine text prompts, enabling deeper\nalignment. Crucially, our hierarchical knowledge mapper allows representations\nat multi-scales to be fused, ensuring that deeper representations retain\ntransferable shallow semantics thereby enhancing generalization. We further\nintroduce a lightweight layer-specific knowledge proxy to enable efficient\ncross-modal interactions. Extensive evaluations across four tasks demonstrate\nHiCroPL's superior performance, achieving state-of-the-art results on 11\nbenchmarks with significant improvements. Code is available at:\nhttps://github.com/zzeoZheng/HiCroPL.", "AI": {"tldr": "HiCroPL\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u8de8\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5411\u77e5\u8bc6\u6d41\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u96be\u4ee5\u4fdd\u6301\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u9694\u79bb\u548c\u5c42\u6b21\u8bed\u4e49\u8870\u51cf\u95ee\u9898\u3002", "method": "HiCroPL\u901a\u8fc7\u5206\u5c42\u77e5\u8bc6\u6620\u5c04\u5668\u548c\u8f7b\u91cf\u7ea7\u77e5\u8bc6\u4ee3\u7406\uff0c\u5b9e\u73b0\u6587\u672c\u4e0e\u89c6\u89c9\u6a21\u6001\u7684\u53cc\u5411\u77e5\u8bc6\u6d41\u52a8\uff0c\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HiCroPL\u901a\u8fc7\u5206\u5c42\u8de8\u6a21\u6001\u4ea4\u4e92\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u9694\u79bb\u548c\u8bed\u4e49\u8870\u51cf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.15104", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15104", "abs": "https://arxiv.org/abs/2507.15104", "authors": ["Qiufeng Li", "Shu Hong", "Jian Gao", "Xuan Zhang", "Tian Lan", "Weidong Cao"], "title": "AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI", "comment": null, "summary": "Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize\nanalog design automation through data-driven approaches. In particular,\nresearchers are increasingly fascinated by harnessing the power of generative\nAI to automate the discovery of novel analog circuit topologies. Unlocking the\nfull potential of generative AI in these data-driven discoveries requires\naccess to large and diverse datasets.Yet, there is a significant barrier in the\nanalog domain--Analog circuit design is inherently proprietary, involving not\nonly confidential circuit structures but also the underlying commercial\nsemiconductor processes. As a result, current generative AI research is largely\nconfined to individual researchers who construct small, narrowly focused\nprivate datasets. This fragmentation severely limits collaborative innovation\nand impedes progress across the research community. To address these\nchallenges, we propose AnalogFed. AnalogFed enables collaborative topology\ndiscovery across decentralized clients (e.g., individual researchers or\ninstitutions) without requiring the sharing of raw private data. To make this\nvision practical, we introduce a suite of techniques tailored to the unique\nchallenges of applying FedL in analog design--from generative model development\nand data heterogeneity handling to privacy-preserving strategies that ensure\nboth flexibility and security for circuit designers and semiconductor\nmanufacturers. Extensive experiments across varying client counts and dataset\nsizes demonstrate that AnalogFed achieves performance comparable to centralized\nbaselines--while maintaining strict data privacy. Specifically, the generative\nAI model within AnalogFed achieves state-of-the-art efficiency and scalability\nin the design of analog circuit topologies.", "AI": {"tldr": "AnalogFed\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u4fc3\u8fdb\u6a21\u62df\u7535\u8def\u62d3\u6251\u7684\u534f\u4f5c\u53d1\u73b0\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u6570\u636e\u5177\u6709\u4e13\u6709\u6027\u548c\u4fdd\u5bc6\u6027\uff0c\u9650\u5236\u4e86\u751f\u6210\u5f0fAI\u7684\u7814\u7a76\u8fdb\u5c55\u3002AnalogFed\u65e8\u5728\u89e3\u51b3\u6570\u636e\u788e\u7247\u5316\u548c\u9690\u79c1\u95ee\u9898\uff0c\u63a8\u52a8\u534f\u4f5c\u521b\u65b0\u3002", "method": "\u63d0\u51faAnalogFed\u6846\u67b6\uff0c\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u6280\u672f\uff0c\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u5e76\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\uff0c\u652f\u6301\u5206\u6563\u5f0f\u5ba2\u6237\u7aef\u7684\u534f\u4f5c\u62d3\u6251\u53d1\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAnalogFed\u5728\u6027\u80fd\u4e0a\u4e0e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u786e\u4fdd\u6570\u636e\u9690\u79c1\uff0c\u5176\u751f\u6210\u5f0fAI\u6a21\u578b\u5728\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u8fbe\u5230\u9886\u5148\u6c34\u5e73\u3002", "conclusion": "AnalogFed\u4e3a\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u534f\u4f5c\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u5f0fAI\u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2507.14997", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14997", "abs": "https://arxiv.org/abs/2507.14997", "authors": ["Roy H. Jennings", "Genady Paikin", "Roy Shaul", "Evgeny Soloveichik"], "title": "Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promise for image-based\nregression tasks, but current approaches face key limitations. Recent methods\nfine-tune MLLMs using preset output vocabularies and generic task-level prompts\n(e.g., \"How would you rate this image?\"), assuming this mimics human rating\nbehavior. Our analysis reveals these approaches provide no benefit over\nimage-only training. Models using preset vocabularies and generic prompts\nperform equivalently to image-only models, failing to leverage semantic\nunderstanding from textual input. We propose Regression via Transformer-Based\nClassification (RvTC), which replaces vocabulary-constrained classification\nwith a flexible bin-based approach. Unlike approaches that address\ndiscretization errors through complex distributional modeling, RvTC eliminates\nmanual vocabulary crafting through straightforward bin increase, achieving\nstate-of-the-art performance on four image assessment datasets using only\nimages. More importantly, we demonstrate that data-specific prompts\ndramatically improve performance. Unlike generic task descriptions, prompts\ncontaining semantic information about specific images enable MLLMs to leverage\ncross-modal understanding. On the AVA dataset, adding challenge titles to\nprompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We\ndemonstrate through empirical evidence from the AVA and AGIQA-3k datasets that\nMLLMs benefit from semantic prompt information surpassing mere statistical\nbiases. This underscores the importance of incorporating meaningful textual\ncontext in multimodal regression tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRvTC\u65b9\u6cd5\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u57fa\u4e8e\u5206\u7bb1\u7684\u65b9\u6cd5\u66ff\u4ee3\u9884\u8bbe\u8bcd\u6c47\u5206\u7c7b\uff0c\u5728\u56fe\u50cf\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u7279\u5b9a\u6570\u636e\u63d0\u793a\u80fd\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u9884\u8bbe\u8f93\u51fa\u8bcd\u6c47\u548c\u901a\u7528\u4efb\u52a1\u63d0\u793a\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6587\u672c\u8f93\u5165\u7684\u8bed\u4e49\u7406\u89e3\uff0c\u6027\u80fd\u4e0e\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684\u6a21\u578b\u76f8\u5f53\u3002", "method": "\u63d0\u51faRegression via Transformer-Based Classification (RvTC)\uff0c\u91c7\u7528\u57fa\u4e8e\u5206\u7bb1\u7684\u7075\u6d3b\u65b9\u6cd5\uff0c\u907f\u514d\u624b\u52a8\u8bcd\u6c47\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u7279\u5b9a\u63d0\u793a\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728\u56db\u4e2a\u56fe\u50cf\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\uff0c\u7279\u5b9a\u63d0\u793a\uff08\u5982\u6311\u6218\u6807\u9898\uff09\u5c06\u76f8\u5173\u6027\u4ece0.83\u63d0\u5347\u81f30.90\u3002", "conclusion": "\u8bed\u4e49\u63d0\u793a\u4fe1\u606f\u5bf9\u591a\u6a21\u6001\u56de\u5f52\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u8d85\u8d8a\u7edf\u8ba1\u504f\u5dee\uff0c\u9700\u7ed3\u5408\u6709\u610f\u4e49\u7684\u6587\u672c\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2507.15112", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15112", "abs": "https://arxiv.org/abs/2507.15112", "authors": ["Youssef Allouah", "Rachid Guerraoui", "Sanmi Koyejo"], "title": "Distributional Unlearning: Forgetting Distributions, Not Just Samples", "comment": null, "summary": "Machine unlearning seeks to remove unwanted information from trained models,\ninitially at the individual-sample level, but increasingly at the level of\nentire sub-populations. In many deployments, models must delete whole topical\ndomains to satisfy privacy, legal, or quality requirements, e.g., removing\nseveral users' posts under GDPR or copyrighted web content. Existing unlearning\ntools remain largely sample-oriented, and straightforward point deletion often\nleaves enough residual signal for downstream learners to recover the unwanted\ndomain. We introduce distributional unlearning, a data-centric, model-agnostic\nframework that asks: Given examples from an unwanted distribution and a\nretained distribution, what is the smallest set of points whose removal makes\nthe edited dataset far from the unwanted domain yet close to the retained one?\nUsing Kullback-Leibler divergence to quantify removal and preservation, we\nderive the exact Pareto frontier in the Gaussian case and prove that any model\nretrained on the edited data incurs log-loss shifts bounded by the divergence\nthresholds. We propose a simple distance-based selection rule satisfying these\nconstraints with a quadratic reduction in deletion budget compared to random\nremoval. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,\nand CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on\nretained performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u7684\u9057\u5fd8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u79fb\u9664\u6700\u5c0f\u6570\u636e\u96c6\u6765\u6d88\u9664\u4e0d\u9700\u8981\u7684\u9886\u57df\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u6838\u5fc3\u6570\u636e\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u6837\u672c\uff0c\u96be\u4ee5\u6709\u6548\u79fb\u9664\u6574\u4e2a\u4e3b\u9898\u9886\u57df\u7684\u4fe1\u606f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9690\u79c1\u3001\u6cd5\u5f8b\u6216\u8d28\u91cf\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u5206\u5e03\u9057\u5fd8\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528Kullback-Leibler\u6563\u5ea6\u91cf\u5316\u79fb\u9664\u548c\u4fdd\u7559\u6548\u679c\uff0c\u5728\u9ad8\u65af\u60c5\u51b5\u4e0b\u63a8\u5bfc\u5e15\u7d2f\u6258\u8fb9\u754c\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8ddd\u79bb\u7684\u9009\u62e9\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u968f\u673a\u79fb\u9664\uff0c\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e8615-72%\u7684\u5220\u9664\u91cf\uff0c\u4e14\u5bf9\u4fdd\u7559\u6027\u80fd\u5f71\u54cd\u53ef\u5ffd\u7565\u3002", "conclusion": "\u5206\u5e03\u9057\u5fd8\u5b66\u4e60\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u9886\u57df\u7ea7\u4fe1\u606f\u79fb\u9664\u3002"}}
{"id": "2507.15000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15000", "abs": "https://arxiv.org/abs/2507.15000", "authors": ["Chaoyun Wang", "I-Chao Shen", "Takeo Igarashi", "Nanning Zheng", "Caigui Jiang"], "title": "Axis-Aligned Document Dewarping", "comment": null, "summary": "Document dewarping is crucial for many applications. However, existing\nlearning-based methods primarily rely on supervised regression with annotated\ndata without leveraging the inherent geometric properties in physical documents\nto the dewarping process. Our key insight is that a well-dewarped document is\ncharacterized by transforming distorted feature lines into axis-aligned ones.\nThis property aligns with the inherent axis-aligned nature of the discrete grid\ngeometry in planar documents. In the training phase, we propose an axis-aligned\ngeometric constraint to enhance document dewarping. In the inference phase, we\npropose an axis alignment preprocessing strategy to reduce the dewarping\ndifficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned\nDistortion (AAD), that not only incorporates geometric meaning and aligns with\nhuman visual perception but also demonstrates greater robustness. As a result,\nour method achieves SOTA results on multiple existing benchmarks and achieves\n18.2%~34.5% improvements on the AAD metric.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u7ea6\u675f\u7684\u6587\u6863\u53bb\u626d\u66f2\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f74\u5bf9\u9f50\u9884\u5904\u7406\u548c\u65b0\u7684\u8bc4\u4f30\u6307\u6807AAD\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u672a\u5145\u5206\u5229\u7528\u6587\u6863\u7684\u51e0\u4f55\u7279\u6027\u3002", "method": "\u5728\u8bad\u7ec3\u9636\u6bb5\u5f15\u5165\u8f74\u5bf9\u9f50\u51e0\u4f55\u7ea6\u675f\uff0c\u63a8\u7406\u9636\u6bb5\u91c7\u7528\u8f74\u5bf9\u9f50\u9884\u5904\u7406\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0cAAD\u6307\u6807\u63d0\u534718.2%~34.5%\u3002", "conclusion": "\u51e0\u4f55\u7ea6\u675f\u548cAAD\u6307\u6807\u6709\u6548\u63d0\u5347\u4e86\u6587\u6863\u53bb\u626d\u66f2\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15119", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15119", "abs": "https://arxiv.org/abs/2507.15119", "authors": ["Juntong Ni", "Shiyu Wang", "Zewen Liu", "Xiaoming Shi", "Xinyue Zhong", "Zhou Ye", "Wei Jin"], "title": "Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting", "comment": null, "summary": "Time series forecasting (TSF) is a central problem in time series analysis.\nHowever, as the number of channels in time series datasets scales to the\nthousands or more, a scenario we define as High-Dimensional Time Series\nForecasting (HDTSF), it introduces significant new modeling challenges that are\noften not the primary focus of traditional TSF research. HDTSF is challenging\nbecause the channel correlation often forms complex and hierarchical patterns.\nExisting TSF models either ignore these interactions or fail to scale as\ndimensionality grows. To address this issue, we propose U-Cast, a\nchannel-dependent forecasting architecture that learns latent hierarchical\nchannel structures with an innovative query-based attention. To disentangle\nhighly correlated channel representation, U-Cast adds a full-rank\nregularization during training. We also release Time-HD, a benchmark of large,\ndiverse, high-dimensional datasets. Our theory shows that exploiting\ncross-channel information lowers forecasting risk, and experiments on Time-HD\ndemonstrate that U-Cast surpasses strong baselines in both accuracy and\nefficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF\nresearch.", "AI": {"tldr": "U-Cast\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u7ef4\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08HDTSF\uff09\u7684\u901a\u9053\u4f9d\u8d56\u67b6\u6784\uff0c\u901a\u8fc7\u67e5\u8be2\u6ce8\u610f\u529b\u5b66\u4e60\u6f5c\u5728\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u5f15\u5165\u5168\u79e9\u6b63\u5219\u5316\u89e3\u8026\u901a\u9053\u8868\u793a\u3002Time-HD\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08TSF\uff09\u6a21\u578b\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u96be\u4ee5\u6355\u6349\u590d\u6742\u901a\u9053\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "U-Cast\u5229\u7528\u67e5\u8be2\u6ce8\u610f\u529b\u5b66\u4e60\u901a\u9053\u5c42\u6b21\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u5168\u79e9\u6b63\u5219\u5316\u89e3\u8026\u9ad8\u76f8\u5173\u901a\u9053\u8868\u793a\u3002", "result": "\u5728Time-HD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cU-Cast\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "U-Cast\u548cTime-HD\u4e3a\u672a\u6765HDTSF\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.15008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15008", "abs": "https://arxiv.org/abs/2507.15008", "authors": ["Jiasheng Xu", "Yewang Chen"], "title": "FastSmoothSAM: A Fast Smooth Method For Segment Anything Model", "comment": null, "summary": "Accurately identifying and representing object edges is a challenging task in\ncomputer vision and image processing. The Segment Anything Model (SAM) has\nsignificantly influenced the field of image segmentation, but suffers from high\nmemory consumption and long inference times, limiting its efficiency in\nreal-time applications. To address these limitations, Fast Segment Anything\n(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM\noften generates jagged edges that deviate from the true object shapes.\nTherefore, this paper introduces a novel refinement approach using B-Spline\ncurve fitting techniques to enhance the edge quality in FastSAM. Leveraging the\nrobust shape control and flexible geometric construction of B-Splines, a\nfour-stage refining process involving two rounds of curve fitting is employed\nto effectively smooth jagged edges. This approach significantly improves the\nvisual quality and analytical accuracy of object edges without compromising\ncritical geometric information. The proposed method improves the practical\nutility of FastSAM by improving segmentation accuracy while maintaining\nreal-time processing capabilities. This advancement unlocks greater potential\nfor FastSAM technology in various real-world scenarios, such as industrial\nautomation, medical imaging, and autonomous systems, where precise and\nefficient edge recognition is crucial.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eB\u6837\u6761\u66f2\u7ebf\u62df\u5408\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347FastSAM\u7684\u8fb9\u7f18\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u5176\u751f\u6210\u7684\u952f\u9f7f\u72b6\u8fb9\u7f18\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "motivation": "FastSAM\u867d\u7136\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5206\u5272\uff0c\u4f46\u5176\u751f\u6210\u7684\u8fb9\u7f18\u5b58\u5728\u952f\u9f7f\u72b6\u504f\u5dee\uff0c\u5f71\u54cd\u4e86\u5206\u5272\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u5206\u6790\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528B\u6837\u6761\u66f2\u7ebf\u62df\u5408\u6280\u672f\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u4f18\u5316\u8fc7\u7a0b\uff08\u5305\u62ec\u4e24\u8f6e\u66f2\u7ebf\u62df\u5408\uff09\u5e73\u6ed1\u952f\u9f7f\u8fb9\u7f18\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u7684\u89c6\u89c9\u8d28\u91cf\u548c\u5206\u6790\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86FastSAM\u7684\u5b9e\u7528\u6027\uff0c\u62d3\u5c55\u4e86\u5176\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u3001\u533b\u7597\u5f71\u50cf\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15132", "categories": ["cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.15132", "abs": "https://arxiv.org/abs/2507.15132", "authors": ["Joanna Komorniczak"], "title": "Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm", "comment": null, "summary": "The research community continues to seek increasingly more advanced synthetic\ndata generators to reliably evaluate the strengths and limitations of machine\nlearning methods. This work aims to increase the availability of datasets\nencompassing a diverse range of problem complexities by proposing a genetic\nalgorithm that optimizes a set of problem complexity measures for\nclassification and regression tasks towards specific targets. For\nclassification, a set of 10 complexity measures was used, while for regression\ntasks, 4 measures demonstrating promising optimization capabilities were\nselected. Experiments confirmed that the proposed genetic algorithm can\ngenerate datasets with varying levels of difficulty by transforming\nsynthetically created datasets to achieve target complexity values through\nlinear feature projections. Evaluations involving state-of-the-art classifiers\nand regressors revealed a correlation between the complexity of the generated\ndata and the recognition quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9057\u4f20\u7b97\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u793e\u533a\u9700\u8981\u66f4\u5148\u8fdb\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u5668\u6765\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u590d\u6742\u6027\u5ea6\u91cf\u6765\u751f\u6210\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u7684\u95ee\u9898\u590d\u6742\u6027\u5ea6\u91cf\uff0c\u901a\u8fc7\u7ebf\u6027\u7279\u5f81\u6295\u5f71\u8c03\u6574\u5408\u6210\u6570\u636e\u96c6\u7684\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u80fd\u751f\u6210\u4e0d\u540c\u96be\u5ea6\u7684\u6570\u636e\u96c6\uff0c\u4e14\u6570\u636e\u590d\u6742\u6027\u4e0e\u8bc6\u522b\u8d28\u91cf\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15028", "abs": "https://arxiv.org/abs/2507.15028", "authors": ["Yuanhan Zhang", "Yunice Chew", "Yuhao Dong", "Aria Leo", "Bo Hu", "Ziwei Liu"], "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding", "comment": "ICCV 2025; Project page: https://zhangyuanhan-ai.github.io/video-tt/", "summary": "Human intelligence requires correctness and robustness, with the former being\nfoundational for the latter. In video understanding, correctness ensures the\naccurate interpretation of visual content, and robustness maintains consistent\nperformance in challenging conditions. Despite advances in video large language\nmodels (video LLMs), existing benchmarks inadequately reflect the gap between\nthese models and human intelligence in maintaining correctness and robustness\nin video interpretation. We introduce the Video Thinking Test (Video-TT), to\nassess if video LLMs can interpret real-world videos as effectively as humans.\nVideo-TT reflects genuine gaps in understanding complex visual narratives, and\nevaluates robustness against natural adversarial questions. Video-TT comprises\n1,000 YouTube Shorts videos, each with one open-ended question and four\nadversarial questions that probe visual and narrative complexity. Our\nevaluation shows a significant gap between video LLMs and human performance.", "AI": {"tldr": "Video-TT\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5145\u5206\u53cd\u6620\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5728\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6b63\u786e\u6027\u548c\u9c81\u68d2\u6027\u5dee\u8ddd\u3002", "method": "\u63d0\u51faVideo-TT\uff0c\u5305\u542b1000\u4e2aYouTube Shorts\u89c6\u9891\uff0c\u6bcf\u4e2a\u89c6\u9891\u914d\u6709\u4e00\u4e2a\u5f00\u653e\u5f0f\u95ee\u9898\u548c\u56db\u4e2a\u5bf9\u6297\u6027\u95ee\u9898\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "Video-TT\u63ed\u793a\u4e86\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u53d9\u4e8b\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.15156", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15156", "abs": "https://arxiv.org/abs/2507.15156", "authors": ["Mykhailo Buleshnyi", "Anna Polova", "Zsolt Zombori", "Michael Benedikt"], "title": "Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification", "comment": null, "summary": "We investigate multi-label classification involving large sets of labels,\nwhere the output labels may be known to satisfy some logical constraints. We\nlook at an architecture in which classifiers for individual labels are fed into\nan expressive sequential model, which produces a joint distribution. One of the\npotential advantages for such an expressive model is its ability to modelling\ncorrelations, as can arise from constraints. We empirically demonstrate the\nability of the architecture both to exploit constraints in training and to\nenforce constraints at inference time.", "AI": {"tldr": "\u7814\u7a76\u591a\u6807\u7b7e\u5206\u7c7b\u95ee\u9898\uff0c\u5229\u7528\u5e8f\u5217\u6a21\u578b\u5904\u7406\u6807\u7b7e\u95f4\u7684\u903b\u8f91\u7ea6\u675f\u3002", "motivation": "\u591a\u6807\u7b7e\u5206\u7c7b\u4e2d\uff0c\u6807\u7b7e\u95f4\u53ef\u80fd\u5b58\u5728\u903b\u8f91\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5efa\u6a21\u76f8\u5173\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e2a\u4f53\u5206\u7c7b\u5668\u7ed3\u5408\u5e8f\u5217\u6a21\u578b\uff0c\u751f\u6210\u8054\u5408\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u67b6\u6784\u80fd\u6709\u6548\u5229\u7528\u8bad\u7ec3\u4e2d\u7684\u7ea6\u675f\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5f3a\u5236\u6267\u884c\u7ea6\u675f\u3002", "conclusion": "\u8be5\u67b6\u6784\u80fd\u6709\u6548\u5904\u7406\u6807\u7b7e\u95f4\u7684\u7ea6\u675f\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2507.15035", "categories": ["cs.CV", "cs.LG", "35Q92, 68U10", "I.4.5; J.2; J.3"], "pdf": "https://arxiv.org/pdf/2507.15035", "abs": "https://arxiv.org/abs/2507.15035", "authors": ["Zhijun Zeng", "Youjia Zheng", "Hao Hu", "Zeyuan Dong", "Yihang Zheng", "Xinliang Liu", "Jinzhuo Wang", "Zuoqiang Shi", "Linfeng Zhang", "Yubing Li", "He Sun"], "title": "OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography", "comment": null, "summary": "Accurate and efficient simulation of wave equations is crucial in\ncomputational wave imaging applications, such as ultrasound computed tomography\n(USCT), which reconstructs tissue material properties from observed scattered\nwaves. Traditional numerical solvers for wave equations are computationally\nintensive and often unstable, limiting their practical applications for\nquasi-real-time image reconstruction. Neural operators offer an innovative\napproach by accelerating PDE solving using neural networks; however, their\neffectiveness in realistic imaging is limited because existing datasets\noversimplify real-world complexity. In this paper, we present OpenBreastUS, a\nlarge-scale wave equation dataset designed to bridge the gap between\ntheoretical equations and practical imaging applications. OpenBreastUS includes\n8,000 anatomically realistic human breast phantoms and over 16 million\nfrequency-domain wave simulations using real USCT configurations. It enables a\ncomprehensive benchmarking of popular neural operators for both forward\nsimulation and inverse imaging tasks, allowing analysis of their performance,\nscalability, and generalization capabilities. By offering a realistic and\nextensive dataset, OpenBreastUS not only serves as a platform for developing\ninnovative neural PDE solvers but also facilitates their deployment in\nreal-world medical imaging problems. For the first time, we demonstrate\nefficient in vivo imaging of the human breast using neural operator solvers.", "AI": {"tldr": "OpenBreastUS\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6ce2\u52a8\u65b9\u7a0b\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6865\u63a5\u7406\u8bba\u65b9\u7a0b\u4e0e\u5b9e\u9645\u6210\u50cf\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u652f\u6301\u795e\u7ecf\u7b97\u5b50\u5728\u524d\u5411\u6a21\u62df\u548c\u9006\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u6ce2\u52a8\u65b9\u7a0b\u6570\u503c\u6c42\u89e3\u5668\u8ba1\u7b97\u91cf\u5927\u4e14\u4e0d\u7a33\u5b9a\uff0c\u795e\u7ecf\u7b97\u5b50\u5728\u73b0\u5b9e\u6210\u50cf\u4e2d\u7684\u6709\u6548\u6027\u53d7\u9650\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u7684\u7b80\u5316\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b8,000\u4e2a\u89e3\u5256\u5b66\u771f\u5b9e\u4eba\u4f53\u4e73\u623f\u6a21\u578b\u548c\u8d85\u8fc71,600\u4e07\u9891\u57df\u6ce2\u6a21\u62df\u7684OpenBreastUS\u6570\u636e\u96c6\u3002", "result": "\u9996\u6b21\u5c55\u793a\u4e86\u4f7f\u7528\u795e\u7ecf\u7b97\u5b50\u6c42\u89e3\u5668\u5728\u4eba\u4f53\u4e73\u623f\u6d3b\u4f53\u6210\u50cf\u4e2d\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "OpenBreastUS\u4e3a\u5f00\u53d1\u521b\u65b0\u795e\u7ecfPDE\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u5e73\u53f0\uff0c\u5e76\u4fc3\u8fdb\u5176\u5728\u73b0\u5b9e\u533b\u5b66\u6210\u50cf\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.15158", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.15158", "abs": "https://arxiv.org/abs/2507.15158", "authors": ["A. H. Abbas", "Hend Abdel-Ghani", "Ivan S. Maksymov"], "title": "Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition", "comment": null, "summary": "As artificial intelligence continues to push into real-time, edge-based and\nresource-constrained environments, there is an urgent need for novel,\nhardware-efficient computational models. In this study, we present and validate\na neuromorphic computing architecture based on resonant-tunnelling diodes\n(RTDs), which exhibit the nonlinear characteristics ideal for physical\nreservoir computing (RC). We theoretically formulate and numerically implement\nan RTD-based RC system and demonstrate its effectiveness on two image\nrecognition benchmarks: handwritten digit classification and object recognition\nusing the Fruit~360 dataset. Our results show that this circuit-level\narchitecture delivers promising performance while adhering to the principles of\nnext-generation RC -- eliminating random connectivity in favour of a\ndeterministic nonlinear transformation of input signals.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u632f\u96a7\u7a7f\u4e8c\u6781\u7ba1\uff08RTD\uff09\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u67b6\u6784\uff0c\u7528\u4e8e\u7269\u7406\u50a8\u5c42\u8ba1\u7b97\uff08RC\uff09\uff0c\u5e76\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5411\u5b9e\u65f6\u3001\u8fb9\u7f18\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u6269\u5c55\uff0c\u9700\u8981\u786c\u4ef6\u9ad8\u6548\u7684\u8ba1\u7b97\u6a21\u578b\u3002", "method": "\u7406\u8bba\u6784\u5efa\u5e76\u6570\u503c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8eRTD\u7684RC\u7cfb\u7edf\uff0c\u6d4b\u8bd5\u4e86\u624b\u5199\u6570\u5b57\u5206\u7c7b\u548cFruit 360\u6570\u636e\u96c6\u7684\u5bf9\u8c61\u8bc6\u522b\u4efb\u52a1\u3002", "result": "\u8be5\u67b6\u6784\u5728\u6027\u80fd\u8868\u73b0\u826f\u597d\uff0c\u540c\u65f6\u9075\u5faa\u4e0b\u4e00\u4ee3RC\u539f\u5219\uff0c\u7528\u786e\u5b9a\u6027\u975e\u7ebf\u6027\u53d8\u6362\u66ff\u4ee3\u968f\u673a\u8fde\u63a5\u3002", "conclusion": "RTD-based RC\u67b6\u6784\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.15162", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15162", "abs": "https://arxiv.org/abs/2507.15162", "authors": ["Firdaus Ahmed Choudhury", "Ethan Leicht", "Jude Ethan Bislig", "Hangzhi Guo", "Amulya Yadav"], "title": "Designing User-Centric Metrics for Evaluation of Counterfactual Explanations", "comment": null, "summary": "Machine learning-based decision models are increasingly being used to make\ndecisions that significantly impact people's lives, but their opaque nature\nleaves end users without a clear understanding of why a decision was made.\nCounterfactual Explanations (CFEs) have grown in popularity as a means of\noffering actionable guidance by identifying the minimum changes in feature\nvalues required to flip a model's prediction to something more desirable.\nUnfortunately, most prior research in CFEs relies on artificial evaluation\nmetrics, such as proximity, which may overlook end-user preferences and\nconstraints, e.g., the user's perception of effort needed to make certain\nfeature changes may differ from that of the model designer. To address this\nresearch gap, this paper makes three novel contributions. First, we conduct a\npilot study with 20 crowd-workers on Amazon MTurk to experimentally validate\nthe alignment of existing CF evaluation metrics with real-world user\npreferences. Results show that user-preferred CFEs matched those based on\nproximity in only 63.81% of cases, highlighting the limited applicability of\nthese metrics in real-world settings. Second, inspired by the need to design a\nuser-informed evaluation metric for CFEs, we conduct a more detailed two-day\nuser study with 41 participants facing realistic credit application scenarios\nto find experimental support for or against three intuitive hypotheses that may\nexplain how end users evaluate CFEs. Third, based on the findings of this\nsecond study, we propose the AWP model, a novel user-centric, two-stage model\nthat describes one possible mechanism by which users evaluate and select CFEs.\nOur results show that AWP predicts user-preferred CFEs with 84.37% accuracy.\nOur study provides the first human-centered validation for personalized cost\nmodels in CFE generation and highlights the need for adaptive, user-centered\nevaluation metrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AWP\u6a21\u578b\uff0c\u7528\u4e8e\u6539\u8fdb\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff08CFEs\uff09\u7684\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709CFEs\u8bc4\u4f30\u6307\u6807\uff08\u5982\u63a5\u8fd1\u6027\uff09\u53ef\u80fd\u5ffd\u7565\u7528\u6237\u504f\u597d\u548c\u7ea6\u675f\uff0c\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u7528\u6237\u7814\u7a76\uff08\u5c0f\u89c4\u6a21\u8bd5\u70b9\u548c\u8be6\u7ec6\u7684\u4e24\u5929\u7814\u7a76\uff09\u9a8c\u8bc1\u5047\u8bbe\uff0c\u5e76\u8bbe\u8ba1AWP\u6a21\u578b\u3002", "result": "AWP\u6a21\u578b\u9884\u6d4b\u7528\u6237\u504f\u597dCFEs\u7684\u51c6\u786e\u7387\u8fbe84.37%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u7528\u6237\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6307\u6807\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684AWP\u6a21\u578b\u3002"}}
{"id": "2507.15037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15037", "abs": "https://arxiv.org/abs/2507.15037", "authors": ["Zhaotong Yang", "Yuhui Li", "Shengfeng He", "Xinzhe Li", "Yangyang Xu", "Junyu Dong", "Yong Du"], "title": "OmniVTON: Training-Free Universal Virtual Try-On", "comment": "Accepted by ICCV2025", "summary": "Image-based Virtual Try-On (VTON) techniques rely on either supervised\nin-shop approaches, which ensure high fidelity but struggle with cross-domain\ngeneralization, or unsupervised in-the-wild methods, which improve adaptability\nbut remain constrained by data biases and limited universality. A unified,\ntraining-free solution that works across both scenarios remains an open\nchallenge. We propose OmniVTON, the first training-free universal VTON\nframework that decouples garment and pose conditioning to achieve both texture\nfidelity and pose consistency across diverse settings. To preserve garment\ndetails, we introduce a garment prior generation mechanism that aligns clothing\nwith the body, followed by continuous boundary stitching technique to achieve\nfine-grained texture retention. For precise pose alignment, we utilize DDIM\ninversion to capture structural cues while suppressing texture interference,\nensuring accurate body alignment independent of the original image textures. By\ndisentangling garment and pose constraints, OmniVTON eliminates the bias\ninherent in diffusion models when handling multiple conditions simultaneously.\nExperimental results demonstrate that OmniVTON achieves superior performance\nacross diverse datasets, garment types, and application scenarios. Notably, it\nis the first framework capable of multi-human VTON, enabling realistic garment\ntransfer across multiple individuals in a single scene. Code is available at\nhttps://github.com/Jerome-Young/OmniVTON", "AI": {"tldr": "OmniVTON\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u7edf\u4e00\u865a\u62df\u8bd5\u7a7f\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u670d\u88c5\u548c\u59ff\u52bf\u6761\u4ef6\uff0c\u5b9e\u73b0\u8de8\u573a\u666f\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u6280\u672f\u8981\u4e48\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff08\u9650\u5236\u8de8\u57df\u6cdb\u5316\uff09\uff0c\u8981\u4e48\u662f\u65e0\u76d1\u7763\u65b9\u6cd5\uff08\u53d7\u6570\u636e\u504f\u5dee\u9650\u5236\uff09\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u670d\u88c5\u5148\u9a8c\u751f\u6210\u673a\u5236\u548c\u8fde\u7eed\u8fb9\u754c\u7f1d\u5408\u6280\u672f\u4fdd\u7559\u7ec6\u8282\uff0c\u5229\u7528DDIM\u53cd\u6f14\u5b9e\u73b0\u7cbe\u786e\u59ff\u52bf\u5bf9\u9f50\uff0c\u89e3\u8026\u670d\u88c5\u548c\u59ff\u52bf\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOmniVTON\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u548c\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9996\u6b21\u5b9e\u73b0\u591a\u4eba\u7269\u865a\u62df\u8bd5\u7a7f\u3002", "conclusion": "OmniVTON\u901a\u8fc7\u89e3\u8026\u6761\u4ef6\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u7684\u504f\u5dee\u95ee\u9898\uff0c\u6210\u4e3a\u9996\u4e2a\u8de8\u573a\u666f\u901a\u7528\u7684\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u3002"}}
{"id": "2507.15173", "categories": ["cs.LG", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15173", "abs": "https://arxiv.org/abs/2507.15173", "authors": ["Jason Gaitonde", "Ankur Moitra", "Elchanan Mossel"], "title": "Better Models and Algorithms for Learning Ising Models from Dynamics", "comment": "49 pages", "summary": "We study the problem of learning the structure and parameters of the Ising\nmodel, a fundamental model of high-dimensional data, when observing the\nevolution of an associated Markov chain. A recent line of work has studied the\nnatural problem of learning when observing an evolution of the well-known\nGlauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,\nGaitonde, Mossel STOC 2024], which provides an arguably more realistic\ngenerative model than the classical i.i.d. setting. However, this prior work\ncrucially assumes that all site update attempts are observed, \\emph{even when\nthis attempt does not change the configuration}: this strong observation model\nis seemingly essential for these approaches. While perhaps possible in\nrestrictive contexts, this precludes applicability to most realistic settings\nwhere we can observe \\emph{only} the stochastic evolution itself, a minimal and\nnatural assumption for any process we might hope to learn from. However,\ndesigning algorithms that succeed in this more realistic setting has remained\nan open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,\nGaitonde, Moitra, Mossel, STOC 2025].\n  In this work, we give the first algorithms that efficiently learn the Ising\nmodel in this much more natural observation model that only observes when the\nconfiguration changes. For Ising models with maximum degree $d$, our algorithm\nrecovers the underlying dependency graph in time $\\mathsf{poly}(d)\\cdot n^2\\log\nn$ and then the actual parameters in additional $\\widetilde{O}(2^d n)$ time,\nwhich qualitatively matches the state-of-the-art even in the i.i.d. setting in\na much weaker observation model. Our analysis holds more generally for a\nbroader class of reversible, single-site Markov chains that also includes the\npopular Metropolis chain by leveraging more robust properties of reversible\nMarkov chains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5728\u4ec5\u89c2\u5bdf\u914d\u7f6e\u53d8\u5316\u65f6\u9ad8\u6548\u5b66\u4e60Ising\u6a21\u578b\u7ed3\u6784\u7684\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u66f4\u81ea\u7136\u7684\u89c2\u6d4b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u89c2\u5bdf\u6240\u6709\u7ad9\u70b9\u66f4\u65b0\u5c1d\u8bd5\uff0c\u5305\u62ec\u672a\u6539\u53d8\u914d\u7f6e\u7684\u60c5\u51b5\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u901a\u8fc7\u5229\u7528\u53ef\u9006\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u9c81\u68d2\u6027\uff0c\u8bbe\u8ba1\u7b97\u6cd5\u5728\u4ec5\u89c2\u5bdf\u914d\u7f6e\u53d8\u5316\u65f6\u6062\u590d\u4f9d\u8d56\u56fe\u548c\u53c2\u6570\u3002", "result": "\u7b97\u6cd5\u5728\u6700\u5927\u5ea6\u4e3ad\u7684Ising\u6a21\u578b\u4e2d\uff0c\u4ee5poly(d)\u00b7n\u00b2log n\u65f6\u95f4\u6062\u590d\u4f9d\u8d56\u56fe\uff0c\u5e76\u5728\u989d\u5916O\u0303(2^d n)\u65f6\u95f4\u5185\u6062\u590d\u53c2\u6570\u3002", "conclusion": "\u672c\u6587\u5728\u66f4\u5f31\u7684\u89c2\u6d4b\u6a21\u578b\u4e0b\u5b9e\u73b0\u4e86\u4e0ei.i.d.\u8bbe\u7f6e\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u6269\u5c55\u4e86\u53ef\u9006\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.15059", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15059", "abs": "https://arxiv.org/abs/2507.15059", "authors": ["Ran Zhang", "Xuanhua He", "Li Xueheng", "Ke Cao", "Liu Liu", "Wenbo Xu", "Fang Jiabin", "Yang Qize", "Jie Zhang"], "title": "Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling", "comment": null, "summary": "The field of pan-sharpening has recently seen a trend towards increasingly\nlarge and complex models, often trained on single, specific satellite datasets.\nThis approach, however, leads to high computational overhead and poor\ngeneralization on full resolution data, a paradigm we challenge in this paper.\nIn response to this issue, we propose PanTiny, a lightweight, single-step\npan-sharpening framework designed for both efficiency and robust performance.\nMore critically, we introduce multiple-in-one training paradigm, where a\nsingle, compact model is trained simultaneously on three distinct satellite\ndatasets (WV2, WV3, and GF2) with different resolution and spectral\ninformation. Our experiments show that this unified training strategy not only\nsimplifies deployment but also significantly boosts generalization on\nfull-resolution data. Further, we introduce a universally powerful composite\nloss function that elevates the performance of almost all of models for\npan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny\nmodel, benefiting from these innovations, achieves a superior\nperformance-to-efficiency balance, outperforming most larger, specialized\nmodels. Through extensive ablation studies, we validate that principled\nengineering in model design, training paradigms, and loss functions can surpass\nbrute-force scaling. Our work advocates for a community-wide shift towards\ncreating efficient, generalizable, and data-conscious models for\npan-sharpening. The code is available at\nhttps://github.com/Zirconium233/PanTiny .", "AI": {"tldr": "PanTiny\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5355\u6b65\u7684pan-sharpening\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u6311\u6218\u5f53\u524dpan-sharpening\u9886\u57df\u5927\u6a21\u578b\u3001\u9ad8\u8ba1\u7b97\u5f00\u9500\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faPanTiny\u6846\u67b6\uff0c\u91c7\u7528\u591a\u6570\u636e\u96c6\u8054\u5408\u8bad\u7ec3\u548c\u590d\u5408\u635f\u5931\u51fd\u6570\u3002", "result": "PanTiny\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u5927\u578b\u4e13\u7528\u6a21\u578b\uff0c\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5021\u5bfc\u8bbe\u8ba1\u9ad8\u6548\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u4e14\u6570\u636e\u654f\u611f\u7684pan-sharpening\u6a21\u578b\u3002"}}
{"id": "2507.15174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15174", "abs": "https://arxiv.org/abs/2507.15174", "authors": ["Justin Turnau", "Longchao Da", "Khoa Vo", "Ferdous Al Rafi", "Shreyas Bachiraju", "Tiejin Chen", "Hua Wei"], "title": "Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control", "comment": "This paper was accepted to RLC/RLJ 2025", "summary": "Traffic Signal Control (TSC) is essential for managing urban traffic flow and\nreducing congestion. Reinforcement Learning (RL) offers an adaptive method for\nTSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)\ngaining traction as intersections naturally function as coordinated agents.\nHowever, due to shifts in environmental dynamics, implementing MARL-based TSC\npolicies in the real world often leads to a significant performance drop, known\nas the sim-to-real gap. Grounded Action Transformation (GAT) has successfully\nmitigated this gap in single-agent RL for TSC, but real-world traffic networks,\nwhich involve numerous interacting intersections, are better suited to a MARL\nframework. In this work, we introduce JL-GAT, an application of GAT to\nMARL-based TSC that balances scalability with enhanced grounding capability by\nincorporating information from neighboring agents. JL-GAT adopts a\ndecentralized approach to GAT, allowing for the scalability often required in\nreal-world traffic networks while still capturing key interactions between\nagents. Comprehensive experiments on various road networks under simulated\nadverse weather conditions, along with ablation studies, demonstrate the\neffectiveness of JL-GAT. The code is publicly available at\nhttps://github.com/DaRL-LibSignal/JL-GAT/.", "AI": {"tldr": "JL-GAT\u662f\u4e00\u79cd\u5c06GAT\u5e94\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u90bb\u8fd1\u667a\u80fd\u4f53\u7684\u4fe1\u606f\uff0c\u5e73\u8861\u4e86\u53ef\u6269\u5c55\u6027\u548c\u589e\u5f3a\u7684\u63a5\u5730\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3MARL\u5728\u771f\u5b9e\u4ea4\u901a\u7f51\u7edc\u4e2d\u56e0\u73af\u5883\u52a8\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\uff08sim-to-real gap\uff09\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u6563\u5f0fGAT\u65b9\u6cd5\uff0c\u7ed3\u5408\u90bb\u8fd1\u667a\u80fd\u4f53\u7684\u4fe1\u606f\uff0c\u63d0\u5347MARL\u5728\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "result": "\u5728\u6a21\u62df\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u591a\u79cd\u9053\u8def\u7f51\u7edc\u4e2d\uff0cJL-GAT\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "JL-GAT\u6210\u529f\u5c06GAT\u6269\u5c55\u5230MARL\u6846\u67b6\uff0c\u4e3a\u771f\u5b9e\u4ea4\u901a\u7f51\u7edc\u4e2d\u7684\u4fe1\u53f7\u63a7\u5236\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15064", "abs": "https://arxiv.org/abs/2507.15064", "authors": ["Shuyuan Tu", "Zhen Xing", "Xintong Han", "Zhi-Qi Cheng", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation", "comment": "arXiv admin note: substantial text overlap with arXiv:2411.17697", "summary": "Current diffusion models for human image animation often struggle to maintain\nidentity (ID) consistency, especially when the reference image and driving\nvideo differ significantly in body size or position. We introduce\nStableAnimator++, the first ID-preserving video diffusion framework with\nlearnable pose alignment, capable of generating high-quality videos conditioned\non a reference image and a pose sequence without any post-processing. Building\nupon a video diffusion model, StableAnimator++ contains carefully designed\nmodules for both training and inference, striving for identity consistency. In\nparticular, StableAnimator++ first uses learnable layers to predict the\nsimilarity transformation matrices between the reference image and the driven\nposes via injecting guidance from Singular Value Decomposition (SVD). These\nmatrices align the driven poses with the reference image, mitigating\nmisalignment to a great extent. StableAnimator++ then computes image and face\nembeddings using off-the-shelf encoders, refining the face embeddings via a\nglobal content-aware Face Encoder. To further maintain ID, we introduce a\ndistribution-aware ID Adapter that counteracts interference caused by temporal\nlayers while preserving ID via distribution alignment. During the inference\nstage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization\nintegrated into the denoising process, guiding the diffusion trajectory for\nenhanced facial fidelity. Experiments on benchmarks show the effectiveness of\nStableAnimator++ both qualitatively and quantitatively.", "AI": {"tldr": "StableAnimator++ \u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u59ff\u6001\u5bf9\u9f50\u548c\u8eab\u4efd\u4fdd\u6301\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u7c7b\u56fe\u50cf\u52a8\u753b\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u53c2\u8003\u56fe\u50cf\u548c\u9a71\u52a8\u89c6\u9891\u5dee\u5f02\u8f83\u5927\u65f6\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0cStableAnimator++ \u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53ef\u5b66\u4e60\u5c42\u9884\u6d4b\u76f8\u4f3c\u53d8\u6362\u77e9\u9635\u5bf9\u9f50\u59ff\u6001\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u9762\u90e8\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u5206\u5e03\u611f\u77e5\u7684\u8eab\u4efd\u9002\u914d\u5668\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u3002\u63a8\u7406\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e HJB \u7684\u9762\u90e8\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e StableAnimator++ \u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "StableAnimator++ \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8eab\u4efd\u4fdd\u6301\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u590d\u6742\u59ff\u6001\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u573a\u666f\u3002"}}
{"id": "2507.15195", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15195", "abs": "https://arxiv.org/abs/2507.15195", "authors": ["Anwar Said", "Yifan Wei", "Ubaid Ullah Ahmad", "Mudassir Shabbir", "Waseem Abbas", "Xenofon Koutsoukos"], "title": "Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning", "comment": null, "summary": "In this article, we utilize the concept of average controllability in graphs,\nalong with a novel rank encoding method, to enhance the performance of Graph\nNeural Networks (GNNs) in social network classification tasks. GNNs have proven\nhighly effective in various network-based learning applications and require\nsome form of node features to function. However, their performance is heavily\ninfluenced by the expressiveness of these features. In social networks, node\nfeatures are often unavailable due to privacy constraints or the absence of\ninherent attributes, making it challenging for GNNs to achieve optimal\nperformance. To address this limitation, we propose two strategies for\nconstructing expressive node features. First, we introduce average\ncontrollability along with other centrality metrics (denoted as NCT-EFA) as\nnode-level metrics that capture critical aspects of network topology. Building\non this, we develop a rank encoding method that transforms average\ncontrollability or any other graph-theoretic metric into a fixed-dimensional\nfeature space, thereby improving feature representation. We conduct extensive\nnumerical evaluations using six benchmark GNN models across four social network\ndatasets to compare different node feature construction methods. Our results\ndemonstrate that incorporating average controllability into the feature space\nsignificantly improves GNN performance. Moreover, the proposed rank encoding\nmethod outperforms traditional one-hot degree encoding, improving the ROC AUC\nfrom 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,\nunderscoring its effectiveness in generating expressive and efficient node\nrepresentations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u5e73\u5747\u53ef\u63a7\u6027\u548c\u65b0\u578b\u79e9\u7f16\u7801\u65b9\u6cd5\u63d0\u5347GNN\u5728\u793e\u4ea4\u7f51\u7edc\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u7f51\u7edc\u4e2d\u8282\u70b9\u7279\u5f81\u5e38\u56e0\u9690\u79c1\u6216\u7f3a\u5931\u800c\u4e0d\u53ef\u7528\uff0c\u5f71\u54cdGNN\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5e73\u5747\u53ef\u63a7\u6027\u7b49\u4e2d\u5fc3\u6027\u6307\u6807\uff08NCT-EFA\uff09\u548c\u79e9\u7f16\u7801\u65b9\u6cd5\u6784\u5efa\u8282\u70b9\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5e73\u5747\u53ef\u63a7\u6027\u548c\u79e9\u7f16\u7801\u663e\u8457\u63d0\u5347GNN\u6027\u80fd\uff0cROC AUC\u4ece68.7%\u63d0\u5347\u81f373.9%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u9ad8\u6548\u4e14\u8868\u8fbe\u6027\u5f3a\u7684\u8282\u70b9\u8868\u793a\u3002"}}
{"id": "2507.15085", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15085", "abs": "https://arxiv.org/abs/2507.15085", "authors": ["Peirong Zhang", "Haowei Xu", "Jiaxin Zhang", "Guitao Xu", "Xuhan Zheng", "Zhenhua Yang", "Junle Liu", "Yuyi Zhang", "Lianwen Jin"], "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR", "comment": null, "summary": "Text image is a unique and crucial information medium that integrates visual\naesthetics and linguistic semantics in modern e-society. Due to their subtlety\nand complexity, the generation of text images represents a challenging and\nevolving frontier in the image generation field. The recent surge of\nspecialized image generators (\\emph{e.g.}, Flux-series) and unified generative\nmodels (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a\nnatural question: can they master the intricacies of text image generation and\nediting? Motivated by this, we assess current state-of-the-art generative\nmodels' capabilities in terms of text image generation and editing. We\nincorporate various typical optical character recognition (OCR) tasks into our\nevaluation and broaden the concept of text-based generation tasks into OCR\ngenerative tasks. We select 33 representative tasks and categorize them into\nfive categories: document, handwritten text, scene text, artistic text, and\ncomplex \\& layout-rich text. For comprehensive evaluation, we examine six\nmodels across both closed-source and open-source domains, using tailored,\nhigh-quality image inputs and prompts. Through this evaluation, we draw crucial\nobservations and identify the weaknesses of current generative models for OCR\ntasks. We argue that photorealistic text image generation and editing should be\ninternalized as foundational skills into general-domain generative models,\nrather than being delegated to specialized solutions, and we hope this\nempirical analysis can provide valuable insights for the community to achieve\nthis goal. This evaluation is online and will be continuously updated at our\nGitHub repository.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u5728\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u5c06\u5176\u4f5c\u4e3a\u901a\u7528\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u6280\u80fd\u7684\u5efa\u8bae\u3002", "motivation": "\u7531\u4e8e\u6587\u672c\u56fe\u50cf\u7684\u590d\u6742\u6027\u548c\u91cd\u8981\u6027\uff0c\u7814\u7a76\u5f53\u524d\u751f\u6210\u6a21\u578b\u662f\u5426\u80fd\u638c\u63e1\u5176\u751f\u6210\u548c\u7f16\u8f91\u7684\u7ec6\u8282\u3002", "method": "\u901a\u8fc733\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\uff08\u5206\u4e3a\u4e94\u7c7b\uff09\u548c\u516d\u79cd\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u5408OCR\u4efb\u52a1\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728OCR\u4efb\u52a1\u4e2d\u7684\u5f31\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u5efa\u8bae\u5c06\u903c\u771f\u7684\u6587\u672c\u56fe\u50cf\u751f\u6210\u548c\u7f16\u8f91\u4f5c\u4e3a\u901a\u7528\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u6280\u80fd\uff0c\u800c\u975e\u4f9d\u8d56\u4e13\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15205", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15205", "abs": "https://arxiv.org/abs/2507.15205", "authors": ["Xinran Li", "Xiujuan Xu", "Jiaqi Qiao"], "title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation", "comment": "Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)", "summary": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u65b9\u6cd5LSDGNN\uff0c\u901a\u8fc7\u957f\u8ddd\u79bb\u548c\u77ed\u8ddd\u79bb\u56fe\u795e\u7ecf\u7f51\u7edc\u5206\u522b\u83b7\u53d6\u8fdc\u8ddd\u79bb\u548c\u8fd1\u8ddd\u79bb\u8bdd\u8bed\u7684\u591a\u6a21\u6001\u7279\u5f81\uff0c\u7ed3\u5408\u5dee\u5206\u6b63\u5219\u5668\u548c\u53cc\u4eff\u5c04\u6a21\u5757\u4f18\u5316\u7279\u5f81\u4ea4\u4e92\uff0c\u5e76\u6539\u8fdb\u8bfe\u7a0b\u5b66\u4e60\u4ee5\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u60c5\u611f\u8bc6\u522b\u5728\u5bf9\u8bdd\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fdc\u8ddd\u79bb\u548c\u8fd1\u8ddd\u79bb\u8bdd\u8bed\u7279\u5f81\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6709\u5411\u65e0\u73af\u56fe\u6784\u5efa\u957f\u8ddd\u79bb\u548c\u77ed\u8ddd\u79bb\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u4f7f\u7528\u5dee\u5206\u6b63\u5219\u5668\u548c\u53cc\u4eff\u5c04\u6a21\u5757\u4f18\u5316\u7279\u5f81\u4ea4\u4e92\uff0c\u63d0\u51fa\u6539\u8fdb\u7684\u8bfe\u7a0b\u5b66\u4e60\uff08ICL\uff09\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u3002", "result": "\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "LSDGNN\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u4f18\u5316\u8bfe\u7a0b\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60c5\u611f\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15240", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15240", "abs": "https://arxiv.org/abs/2507.15240", "authors": ["Le Peng", "Yash Travadi", "Chuan He", "Ying Cui", "Ju Sun"], "title": "Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification", "comment": null, "summary": "For classification with imbalanced class frequencies, i.e., imbalanced\nclassification (IC), standard accuracy is known to be misleading as a\nperformance measure. While most existing methods for IC resort to optimizing\nbalanced accuracy (i.e., the average of class-wise recalls), they fall short in\nscenarios where the significance of classes varies or certain metrics should\nreach prescribed levels. In this paper, we study two key classification\nmetrics, precision and recall, under three practical binary IC settings: fix\nprecision optimize recall (FPOR), fix recall optimize precision (FROP), and\noptimize $F_\\beta$-score (OFBS). Unlike existing methods that rely on smooth\napproximations to deal with the indicator function involved, \\textit{we\nintroduce, for the first time, exact constrained reformulations for these\ndirect metric optimization (DMO) problems}, which can be effectively solved by\nexact penalty methods. Experiment results on multiple benchmark datasets\ndemonstrate the practical superiority of our approach over the state-of-the-art\nmethods for the three DMO problems. We also expect our exact reformulation and\noptimization (ERO) framework to be applicable to a wide range of DMO problems\nfor binary IC and beyond. Our code is available at\nhttps://github.com/sun-umn/DMO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cbe\u786e\u7ea6\u675f\u91cd\u6784\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u5e73\u8861\u5206\u7c7b\u4e2d\u7684\u76f4\u63a5\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u5e73\u8861\u5206\u7c7b\u65f6\uff0c\u901a\u5e38\u4f18\u5316\u5e73\u8861\u51c6\u786e\u7387\uff0c\u4f46\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u7c7b\u522b\u91cd\u8981\u6027\u6216\u7279\u5b9a\u6307\u6807\u8981\u6c42\u7684\u60c5\u51b5\u3002", "method": "\u5f15\u5165\u7cbe\u786e\u7ea6\u675f\u91cd\u6784\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u786e\u60e9\u7f5a\u65b9\u6cd5\u89e3\u51b3FPOR\u3001FROP\u548cOFBS\u4e09\u79cd\u76f4\u63a5\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684ERO\u6846\u67b6\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e8c\u5143\u4e0d\u5e73\u8861\u5206\u7c7b\u53ca\u5176\u4ed6\u76f4\u63a5\u5ea6\u91cf\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2507.15094", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15094", "abs": "https://arxiv.org/abs/2507.15094", "authors": ["Mengya Xu", "Rulin Zhou", "An Wang", "Chaoyang Lyu", "Zhen Li", "Ning Zhong", "Hongliang Ren"], "title": "BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking", "comment": "27 pages, 14 figures", "summary": "Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses\nsignificant risks, demanding precise, real-time localization and continuous\nmonitoring of the bleeding source for effective hemostatic intervention. In\nparticular, endoscopists have to repeatedly flush to clear blood, allowing only\nmilliseconds to identify bleeding sources, an inefficient process that prolongs\noperations and elevates patient risks. However, current Artificial Intelligence\n(AI) methods primarily focus on bleeding region segmentation, overlooking the\ncritical need for accurate bleeding source detection and temporal tracking in\nthe challenging ESD environment, which is marked by frequent visual\nobstructions and dynamic scene changes. This gap is widened by the lack of\nspecialized datasets, hindering the development of robust AI-assisted guidance\nsystems. To address these challenges, we introduce BleedOrigin-Bench, the first\ncomprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated\nbleeding sources across 106,222 frames from 44 procedures, supplemented with\n39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6\nchallenging clinical scenarios. We also present BleedOrigin-Net, a novel\ndual-stage detection-tracking framework for the bleeding source localization in\nESD procedures, addressing the complete workflow from bleeding onset detection\nto continuous spatial tracking. We compare with widely-used object detection\nmodels (YOLOv11/v12), multimodal large language models, and point tracking\nmethods. Extensive evaluation demonstrates state-of-the-art performance,\nachieving 96.85% frame-level accuracy ($\\pm\\leq8$ frames) for bleeding onset\ndetection, 70.24% pixel-level accuracy ($\\leq100$ px) for initial source\ndetection, and 96.11% pixel-level accuracy ($\\leq100$ px) for point tracking.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2aESD\u51fa\u8840\u6e90\u6570\u636e\u96c6BleedOrigin-Bench\u548c\u53cc\u9636\u6bb5\u68c0\u6d4b-\u8ddf\u8e2a\u6846\u67b6BleedOrigin-Net\uff0c\u7528\u4e8e\u5b9e\u65f6\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u51fa\u8840\u6e90\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524dAI\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u51fa\u8840\u533a\u57df\u5206\u5272\uff0c\u5ffd\u7565\u4e86\u5728\u590d\u6742ESD\u73af\u5883\u4e2d\u51c6\u786e\u68c0\u6d4b\u548c\u8ddf\u8e2a\u51fa\u8840\u6e90\u7684\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u4e13\u95e8\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51faBleedOrigin-Bench\u6570\u636e\u96c6\u548cBleedOrigin-Net\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u6d4b\u548c\u8ddf\u8e2a\u6280\u672f\uff0c\u4ece\u51fa\u8840\u5f00\u59cb\u68c0\u6d4b\u5230\u6301\u7eed\u7a7a\u95f4\u8ddf\u8e2a\u3002", "result": "\u5728\u51fa\u8840\u5f00\u59cb\u68c0\u6d4b\u3001\u521d\u59cb\u6e90\u68c0\u6d4b\u548c\u70b9\u8ddf\u8e2a\u65b9\u9762\u5206\u522b\u8fbe\u523096.85%\u300170.24%\u548c96.11%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "BleedOrigin-Net\u5728ESD\u51fa\u8840\u6e90\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.15246", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15246", "abs": "https://arxiv.org/abs/2507.15246", "authors": ["Rabia Latief Bhat", "Iqra Altaf Gillani"], "title": "Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks", "comment": null, "summary": "Accurate demand forecasting is critical for enhancing the efficiency and\nresponsiveness of food delivery platforms, where spatial heterogeneity and\ntemporal fluctuations in order volumes directly influence operational\ndecisions. This paper proposes an attention-based Graph Neural Network\nframework that captures spatial-temporal dependencies by modeling the food\ndelivery environment as a graph. In this graph, nodes represent urban delivery\nzones, while edges reflect spatial proximity and inter-regional order flow\npatterns derived from historical data. The attention mechanism dynamically\nweighs the influence of neighboring zones, enabling the model to focus on the\nmost contextually relevant areas during prediction. Temporal trends are jointly\nlearned alongside spatial interactions, allowing the model to adapt to evolving\ndemand patterns. Extensive experiments on real-world food delivery datasets\ndemonstrate the superiority of the proposed model in forecasting future order\nvolumes with high accuracy. The framework offers a scalable and adaptive\nsolution to support proactive fleet positioning, resource allocation, and\ndispatch optimization in urban food delivery operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u98df\u54c1\u914d\u9001\u9700\u6c42\uff0c\u7ed3\u5408\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u98df\u54c1\u914d\u9001\u5e73\u53f0\u7684\u9700\u6c42\u9884\u6d4b\u5bf9\u8fd0\u8425\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8ba2\u5355\u91cf\u7684\u65f6\u7a7a\u5f02\u8d28\u6027\u589e\u52a0\u4e86\u9884\u6d4b\u96be\u5ea6\u3002", "method": "\u5c06\u914d\u9001\u73af\u5883\u5efa\u6a21\u4e3a\u56fe\uff0c\u8282\u70b9\u4ee3\u8868\u914d\u9001\u533a\u57df\uff0c\u8fb9\u53cd\u6620\u7a7a\u95f4\u90bb\u8fd1\u6027\u548c\u8ba2\u5355\u6d41\u6a21\u5f0f\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u52a0\u6743\u90bb\u8fd1\u533a\u57df\u5f71\u54cd\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u80fd\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u57ce\u5e02\u98df\u54c1\u914d\u9001\u8fd0\u8425\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u548c\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u8f66\u961f\u5b9a\u4f4d\u3001\u8d44\u6e90\u5206\u914d\u548c\u8c03\u5ea6\u4f18\u5316\u3002"}}
{"id": "2507.15109", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15109", "abs": "https://arxiv.org/abs/2507.15109", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Sorin Grigorescu"], "title": "LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM", "comment": null, "summary": "One of the main challenges in the Simultaneous Localization and Mapping\n(SLAM) loop closure problem is the recognition of previously visited places. In\nthis work, we tackle the two main problems of real-time SLAM systems: 1) loop\nclosure detection accuracy and 2) real-time computation constraints on the\nembedded hardware. Our LoopNet method is based on a multitasking variant of the\nclassical ResNet architecture, adapted for online retraining on a dynamic\nvisual dataset and optimized for embedded devices. The online retraining is\ndesigned using a few-shot learning approach. The architecture provides both an\nindex into the queried visual dataset, and a measurement of the prediction\nquality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,\nLoopNet surpasses the limitations of handcrafted features and traditional deep\nlearning methods, offering better performance under varying conditions. Code is\navailable at https://github.com/RovisLab/LoopNet. Additinally, we introduce a\nnew loop closure benchmarking dataset, coined LoopDB, which is available at\nhttps://github.com/RovisLab/LoopDB.", "AI": {"tldr": "LoopNet\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u4efb\u52a1ResNet\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3SLAM\u4e2d\u7684\u95ed\u73af\u68c0\u6d4b\u95ee\u9898\uff0c\u4f18\u5316\u4e86\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u5b9e\u65f6\u8ba1\u7b97\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3SLAM\u7cfb\u7edf\u4e2d\u95ed\u73af\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5d4c\u5165\u5f0f\u786c\u4ef6\u5b9e\u65f6\u8ba1\u7b97\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u591a\u4efb\u52a1ResNet\u67b6\u6784\uff0c\u7ed3\u5408\u5728\u7ebf\u52a8\u6001\u89c6\u89c9\u6570\u636e\u96c6\u8bad\u7ec3\u548cfew-shot\u5b66\u4e60\uff0c\u5229\u7528DISK\u63cf\u8ff0\u7b26\u63d0\u5347\u6027\u80fd\u3002", "result": "LoopNet\u5728\u591a\u53d8\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u624b\u5de5\u7279\u5f81\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u95ed\u73af\u68c0\u6d4b\u6570\u636e\u96c6LoopDB\u3002", "conclusion": "LoopNet\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u548c\u4f18\u5316\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86SLAM\u7cfb\u7edf\u7684\u95ed\u73af\u68c0\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u8bbe\u5907\u3002"}}
{"id": "2507.15260", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15260", "abs": "https://arxiv.org/abs/2507.15260", "authors": ["Jiaqi Han", "Haotian Ye", "Puheng Li", "Minkai Xu", "James Zou", "Stefano Ermon"], "title": "CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers", "comment": "ICCV 2025", "summary": "Diffusion-based generative models have become dominant generators of\nhigh-fidelity images and videos but remain limited by their computationally\nexpensive inference procedures. Existing acceleration techniques either require\nextensive model retraining or compromise significantly on sample quality. This\npaper explores a general, training-free, and model-agnostic acceleration\nstrategy via multi-core parallelism. Our framework views multi-core diffusion\nsampling as an ODE solver pipeline, where slower yet accurate solvers\nprogressively rectify faster solvers through a theoretically justified\ninter-core communication mechanism. This motivates our multi-core training-free\ndiffusion sampling accelerator, CHORDS, which is compatible with various\ndiffusion samplers, model architectures, and modalities. Through extensive\nexperiments, CHORDS significantly accelerates sampling across diverse\nlarge-scale image and video diffusion models, yielding up to 2.1x speedup with\nfour cores, improving by 50% over baselines, and 2.9x speedup with eight cores,\nall without quality degradation. This advancement enables CHORDS to establish a\nsolid foundation for real-time, high-fidelity diffusion generation.", "AI": {"tldr": "CHORDS\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6838\u5e76\u884c\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u901f\u5ea6\u4e14\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c\u89c6\u9891\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u8ba1\u7b97\u6602\u8d35\u3002\u73b0\u6709\u52a0\u901f\u6280\u672f\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u727a\u7272\u8d28\u91cf\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u901a\u7528\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u5c06\u591a\u6838\u6269\u6563\u91c7\u6837\u89c6\u4e3aODE\u6c42\u89e3\u5668\u7ba1\u9053\uff0c\u901a\u8fc7\u7406\u8bba\u652f\u6301\u7684\u6838\u95f4\u901a\u4fe1\u673a\u5236\uff0c\u6162\u901f\u4f46\u51c6\u786e\u7684\u6c42\u89e3\u5668\u9010\u6b65\u4fee\u6b63\u5feb\u901f\u6c42\u89e3\u5668\u3002", "result": "CHORDS\u5728\u591a\u6837\u5927\u89c4\u6a21\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u663e\u8457\u52a0\u901f\u91c7\u6837\uff0c4\u6838\u63d0\u901f2.1\u500d\uff0c8\u6838\u63d0\u901f2.9\u500d\uff0c\u4e14\u65e0\u8d28\u91cf\u635f\u5931\u3002", "conclusion": "CHORDS\u4e3a\u5b9e\u65f6\u9ad8\u8d28\u91cf\u6269\u6563\u751f\u6210\u5960\u5b9a\u57fa\u7840\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2507.15130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15130", "abs": "https://arxiv.org/abs/2507.15130", "authors": ["Ce Zhang", "Yale Song", "Ruta Desai", "Michael Louis Iuzzolino", "Joseph Tighe", "Gedas Bertasius", "Satwik Kottur"], "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction", "comment": null, "summary": "Visual Planning for Assistance (VPA) aims to predict a sequence of user\nactions required to achieve a specified goal based on a video showing the\nuser's progress. Although recent advances in multimodal large language models\n(MLLMs) have shown promising results in video understanding, long-horizon\nvisual planning remains a challenging problem. We identify two challenges in\ntraining large MLLMs for video-based planning tasks: (1) scarcity of procedural\nannotations, limiting the model's ability to learn procedural task dynamics\neffectively, and (2) inefficiency of next-token prediction objective to\nexplicitly capture the structured action space for visual planning when\ncompared to free-form, natural language. To tackle data scarcity, we introduce\nAuxiliary Task Augmentation. We design and train our model on auxiliary tasks\nrelevant to long-horizon video-based planning (e.g., goal prediction) to\naugment the model's planning ability. To more explicitly model the structured\naction space unique to visual planning tasks, we leverage Multi-token\nPrediction, extending traditional next-token prediction by using multiple heads\nto predict multiple future tokens during training. Our approach, VideoPlan,\nachieves state-of-the-art VPA performance on the COIN and CrossTask datasets,\nsurpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3\nfuture actions. We further extend our method to the challenging Ego4D Long-term\nAction Anticipation task, and show that it is on par with the state-of-the-art\napproaches despite not using specialized egocentric features. Code will be made\navailable.", "AI": {"tldr": "VPA\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u5e8f\u5217\uff0c\u63d0\u51fa\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u548c\u591a\u4ee4\u724c\u9884\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u957f\u65f6\u89c6\u89c9\u89c4\u5212\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u7ed3\u6784\u5316\u52a8\u4f5c\u7a7a\u95f4\u5efa\u6a21\u7684\u6311\u6218\u3002", "method": "\u5f15\u5165\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u548c\u591a\u4ee4\u724c\u9884\u6d4b\uff0c\u63d0\u5347\u6a21\u578b\u89c4\u5212\u80fd\u529b\u3002", "result": "\u5728COIN\u548cCrossTask\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53477.3%\u548c3.4%\uff0c\u5728Ego4D\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "VideoPlan\u65b9\u6cd5\u5728\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u4e14\u65e0\u9700\u4e13\u7528\u7279\u5f81\u3002"}}
{"id": "2507.15274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15274", "abs": "https://arxiv.org/abs/2507.15274", "authors": ["Matthew J. Bryan", "Felix Schwock", "Azadeh Yazdan-Shahmorad", "Rajesh P N Rao"], "title": "Temporal Basis Function Models for Closed-Loop Neural Stimulation", "comment": null, "summary": "Closed-loop neural stimulation provides novel therapies for neurological\ndiseases such as Parkinson's disease (PD), but it is not yet clear whether\nartificial intelligence (AI) techniques can tailor closed-loop stimulation to\nindividual patients or identify new therapies. Progress requires us to address\na number of translational issues, including sample efficiency, training time,\nand minimizing loop latency such that stimulation may be shaped in response to\nchanging brain activity. We propose temporal basis function models (TBFMs) to\naddress these difficulties, and explore this approach in the context of\nexcitatory optogenetic stimulation. We demonstrate the ability of TBF models to\nprovide a single-trial, spatiotemporal forward prediction of the effect of\noptogenetic stimulation on local field potentials (LFPs) measured in two\nnon-human primates. We further use simulations to demonstrate the use of TBF\nmodels for closed-loop stimulation, driving neural activity towards target\npatterns. The simplicity of TBF models allow them to be sample efficient, rapid\nto train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the\nmodel on 40 sessions of previously published excitatory optogenetic stimulation\ndata. For each session, the model required 15-20min of data collection to\nsuccessfully model the remainder of the session. It achieved a prediction\naccuracy comparable to a baseline nonlinear dynamical systems model that\nrequires hours to train, and superior accuracy to a linear state-space model.\nIn our simulations, it also successfully allowed a closed-loop stimulator to\ncontrol a neural circuit. Our approach begins to bridge the translational gap\nbetween complex AI-based approaches to modeling dynamical systems and the\nvision of using such forward prediction models to develop novel, clinically\nuseful closed-loop stimulation protocols.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u57fa\u51fd\u6570\u6a21\u578b\uff08TBFMs\uff09\u7684\u95ed\u73af\u795e\u7ecf\u523a\u6fc0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u6cbb\u7597\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\uff0c\u5982\u5e15\u91d1\u68ee\u75c5\u3002\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u5ef6\u8fdf\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524dAI\u6280\u672f\u5728\u95ed\u73af\u795e\u7ecf\u523a\u6fc0\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u3001\u8bad\u7ec3\u65f6\u95f4\u957f\u548c\u5ef6\u8fdf\u9ad8\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e34\u5e8a\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u57fa\u51fd\u6570\u6a21\u578b\uff08TBFMs\uff09\u8fdb\u884c\u5355\u6b21\u8bd5\u9a8c\u7684\u65f6\u7a7a\u524d\u5411\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u5176\u5728\u95ed\u73af\u523a\u6fc0\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "TBF\u6a21\u578b\u5728\u975e\u4eba\u7075\u957f\u7c7b\u52a8\u7269\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff08\u8bad\u7ec3\u65f6\u95f42-4\u5206\u949f\uff0c\u5ef6\u8fdf0.2ms\uff09\uff0c\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u57fa\u7ebf\u975e\u7ebf\u6027\u6a21\u578b\u76f8\u5f53\uff0c\u4f18\u4e8e\u7ebf\u6027\u6a21\u578b\u3002", "conclusion": "TBF\u6a21\u578b\u4e3aAI\u9a71\u52a8\u7684\u95ed\u73af\u795e\u7ecf\u523a\u6fc0\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u4e34\u5e8a\u6cbb\u7597\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.15150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15150", "abs": "https://arxiv.org/abs/2507.15150", "authors": ["Aayush Atul Verma", "Arpitsinh Vaghela", "Bharatesh Chakravarthi", "Kaustav Chanda", "Yezhou Yang"], "title": "Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection", "comment": null, "summary": "Event-based sensors offer high temporal resolution and low latency by\ngenerating sparse, asynchronous data. However, converting this irregular data\ninto dense tensors for use in standard neural networks diminishes these\ninherent advantages, motivating research into graph representations. While such\nmethods preserve sparsity and support asynchronous inference, their performance\non downstream tasks remains limited due to suboptimal modeling of\nspatiotemporal dynamics. In this work, we propose a novel spatiotemporal\nmultigraph representation to better capture spatial structure and temporal\nchanges. Our approach constructs two decoupled graphs: a spatial graph\nleveraging B-spline basis functions to model global structure, and a temporal\ngraph utilizing motion vector-based attention for local dynamic changes. This\ndesign enables the use of efficient 2D kernels in place of computationally\nexpensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM\ndatasets for event-based object detection, achieving over a 6% improvement in\ndetection accuracy compared to previous graph-based works, with a 5x speedup,\nreduced parameter count, and no increase in computational cost. These results\nhighlight the effectiveness of structured graph modeling for asynchronous\nvision. Project page: eventbasedvision.github.io/eGSMV.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u7a7a\u591a\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u7a00\u758f\u5f02\u6b65\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4e8b\u4ef6\u4f20\u611f\u5668\u7684\u7a00\u758f\u5f02\u6b65\u6570\u636e\u5728\u8f6c\u6362\u4e3a\u5bc6\u96c6\u5f20\u91cf\u65f6\u4f1a\u4e27\u5931\u5176\u4f18\u52bf\uff0c\u800c\u73b0\u6709\u56fe\u8868\u793a\u65b9\u6cd5\u5bf9\u65f6\u7a7a\u52a8\u6001\u5efa\u6a21\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u6784\u5efa\u4e86\u4e24\u4e2a\u89e3\u8026\u7684\u56fe\uff1a\u7a7a\u95f4\u56fe\uff08\u4f7f\u7528B\u6837\u6761\u57fa\u51fd\u6570\u5efa\u6a21\u5168\u5c40\u7ed3\u6784\uff09\u548c\u65f6\u95f4\u56fe\uff08\u5229\u7528\u8fd0\u52a8\u5411\u91cf\u6ce8\u610f\u529b\u5efa\u6a21\u5c40\u90e8\u52a8\u6001\u53d8\u5316\uff09\uff0c\u4ee5\u66ff\u4ee3\u6602\u8d35\u76843D\u6838\u3002", "result": "\u5728Gen1\u548ceTraM\u6570\u636e\u96c6\u4e0a\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u63d0\u53476%\uff0c\u901f\u5ea6\u63d0\u53475\u500d\uff0c\u53c2\u6570\u51cf\u5c11\u4e14\u8ba1\u7b97\u6210\u672c\u4e0d\u53d8\u3002", "conclusion": "\u7ed3\u6784\u5316\u56fe\u5efa\u6a21\u80fd\u6709\u6548\u63d0\u5347\u5f02\u6b65\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15280", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15280", "abs": "https://arxiv.org/abs/2507.15280", "authors": ["Shaofei Shen", "Chenhao Zhang", "Yawen Zhao", "Alina Bialkowski", "Weitong Chen", "Miao Xu"], "title": "Machine Unlearning for Streaming Forgetting", "comment": null, "summary": "Machine unlearning aims to remove knowledge of the specific training data in\na well-trained model. Currently, machine unlearning methods typically handle\nall forgetting data in a single batch, removing the corresponding knowledge all\nat once upon request. However, in practical scenarios, requests for data\nremoval often arise in a streaming manner rather than in a single batch,\nleading to reduced efficiency and effectiveness in existing methods. Such\nchallenges of streaming forgetting have not been the focus of much research. In\nthis paper, to address the challenges of performance maintenance, efficiency,\nand data access brought about by streaming unlearning requests, we introduce a\nstreaming unlearning paradigm, formalizing the unlearning as a distribution\nshift problem. We then estimate the altered distribution and propose a novel\nstreaming unlearning algorithm to achieve efficient streaming forgetting\nwithout requiring access to the original training data. Theoretical analyses\nconfirm an $O(\\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,\nwhere $V_T$ represents the cumulative total variation in the optimal solution\nover $T$ learning rounds. This theoretical guarantee is achieved under mild\nconditions without the strong restriction of convex loss function. Experiments\nacross various models and datasets validate the performance of our proposed\nmethod.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f0f\u9057\u5fd8\u5b66\u4e60\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6d41\u5f0f\u6570\u636e\u9057\u5fd8\u8bf7\u6c42\u65f6\u7684\u6548\u7387\u4f4e\u4e0b\u548c\u6570\u636e\u8bbf\u95ee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\u901a\u5e38\u6279\u91cf\u5904\u7406\u9057\u5fd8\u6570\u636e\uff0c\u800c\u5b9e\u9645\u573a\u666f\u4e2d\u6570\u636e\u9057\u5fd8\u8bf7\u6c42\u5f80\u5f80\u662f\u6d41\u5f0f\u7684\uff0c\u5bfc\u81f4\u6548\u7387\u548c\u6548\u679c\u4e0b\u964d\u3002", "method": "\u5c06\u9057\u5fd8\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u4f30\u8ba1\u53d8\u5316\u540e\u7684\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u6d41\u5f0f\u9057\u5fd8\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u6d41\u5f0f\u9057\u5fd8\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6d41\u5f0f\u9057\u5fd8\u7b97\u6cd5\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6d41\u5f0f\u6570\u636e\u9057\u5fd8\u9700\u6c42\u3002"}}
{"id": "2507.15212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15212", "abs": "https://arxiv.org/abs/2507.15212", "authors": ["Yusuke Yoshiyasu", "Leyuan Sun", "Ryusuke Sagawa"], "title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction", "comment": "Accepted at ICCV2025", "summary": "In this paper, we introduce MeshMamba, a neural network model for learning 3D\narticulated mesh models by employing the recently proposed Mamba State Space\nModels (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large\nnumber of input tokens, enabling the generation and reconstruction of body mesh\nmodels with more than 10,000 vertices, capturing clothing and hand geometries.\nThe key to effectively learning MeshMamba is the serialization technique of\nmesh vertices into orderings that are easily processed by Mamba. This is\nachieved by sorting the vertices based on body part annotations or the 3D\nvertex locations of a template mesh, such that the ordering respects the\nstructure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,\na denoising diffusion model for generating 3D articulated meshes and 2)\nMamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape\nand pose from a single image. Experimental results showed that MambaDiff3D can\ngenerate dense 3D human meshes in clothes, with grasping hands, etc., and\noutperforms previous approaches in the 3D human shape generation task.\nAdditionally, Mamba-HMR extends the capabilities of previous non-parametric\nhuman mesh recovery approaches, which were limited to handling body-only poses\nusing around 500 vertex tokens, to the whole-body setting with face and hands,\nwhile achieving competitive performance in (near) real-time.", "AI": {"tldr": "MeshMamba\u662f\u4e00\u79cd\u57fa\u4e8eMamba-SSMs\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u5b66\u4e60\u548c\u751f\u62103D\u5173\u8282\u7f51\u683c\u6a21\u578b\uff0c\u652f\u6301\u8d85\u8fc710,000\u9876\u70b9\u7684\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a213D\u7f51\u683c\u6570\u636e\u65f6\u7684\u6548\u7387\u548c\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5c24\u5176\u662f\u6355\u6349\u8863\u7269\u548c\u624b\u90e8\u51e0\u4f55\u7ec6\u8282\u3002", "method": "\u901a\u8fc7\u5e8f\u5217\u5316\u6280\u672f\u5c06\u7f51\u683c\u9876\u70b9\u6392\u5e8f\uff0c\u5229\u7528Mamba-SSMs\u5904\u7406\uff1b\u8bbe\u8ba1\u4e86MambaDiff3D\uff08\u751f\u6210\u6a21\u578b\uff09\u548cMamba-HMR\uff08\u91cd\u5efa\u6a21\u578b\uff09\u3002", "result": "MambaDiff3D\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cMamba-HMR\u6269\u5c55\u4e86\u975e\u53c2\u6570\u5316\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u652f\u6301\u5168\u8eab\u91cd\u5efa\u3002", "conclusion": "MeshMamba\u57283D\u7f51\u683c\u751f\u6210\u548c\u91cd\u5efa\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u6269\u5c55\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.15287", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15287", "abs": "https://arxiv.org/abs/2507.15287", "authors": ["Elias Malomgr\u00e9", "Pieter Simoens"], "title": "Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning", "comment": "10 pages, 8 figures, accepted for the non-archival workshop \"Workshop\n  on Reinforcement Learning Beyond Rewards @ Reinforcement Learning Conference\n  2025\"", "summary": "Recent trends in Reinforcement Learning (RL) highlight the need for agents to\nlearn from reward-free interactions and alternative supervision signals, such\nas unlabeled or incomplete demonstrations, rather than relying solely on\nexplicit reward maximization. Additionally, developing generalist agents that\ncan adapt efficiently in real-world environments often requires leveraging\nthese reward-free signals to guide learning and behavior. However, while\nintrinsic motivation techniques provide a means for agents to seek out novel or\nuncertain states in the absence of explicit rewards, they are often challenged\nby dense reward environments or the complexity of high-dimensional state and\naction spaces. Furthermore, most existing approaches rely directly on the\nunprocessed intrinsic reward signals, which can make it difficult to shape or\ncontrol the agent's exploration effectively. We propose a framework that can\neffectively utilize expert demonstrations, even when they are incomplete and\nimperfect. By applying a mapping function to transform the similarity between\nan agent's state and expert data into a shaped intrinsic reward, our method\nallows for flexible and targeted exploration of expert-like behaviors. We\nemploy a Mixture of Autoencoder Experts to capture a diverse range of behaviors\nand accommodate missing information in demonstrations. Experiments show our\napproach enables robust exploration and strong performance in both sparse and\ndense reward environments, even when demonstrations are sparse or incomplete.\nThis provides a practical framework for RL in realistic settings where optimal\ndata is unavailable and precise reward control is needed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4e0d\u5b8c\u6574\u4e13\u5bb6\u6f14\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6620\u5c04\u51fd\u6570\u5c06\u72b6\u6001\u4e0e\u4e13\u5bb6\u6570\u636e\u7684\u76f8\u4f3c\u6027\u8f6c\u5316\u4e3a\u5185\u5728\u5956\u52b1\uff0c\u5b9e\u73b0\u7075\u6d3b\u63a2\u7d22\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u96be\u4ee5\u83b7\u53d6\u6700\u4f18\u6570\u636e\u548c\u7cbe\u786e\u5956\u52b1\u4fe1\u53f7\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5229\u7528\u4e0d\u5b8c\u6574\u4e13\u5bb6\u6f14\u793a\u6307\u5bfc\u5b66\u4e60\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u81ea\u7f16\u7801\u4e13\u5bb6\u6a21\u578b\u5904\u7406\u591a\u6837\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u6620\u5c04\u51fd\u6570\u5c06\u72b6\u6001\u76f8\u4f3c\u6027\u8f6c\u5316\u4e3a\u5185\u5728\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u548c\u5bc6\u96c6\u5956\u52b1\u73af\u5883\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u5373\u4f7f\u6f14\u793a\u6570\u636e\u4e0d\u5b8c\u6574\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6570\u636e\u4e0d\u5b8c\u6574\u548c\u9700\u8981\u7cbe\u786e\u5956\u52b1\u63a7\u5236\u7684\u573a\u666f\u3002"}}
{"id": "2507.15216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15216", "abs": "https://arxiv.org/abs/2507.15216", "authors": ["Yuping Qiu", "Rui Zhu", "Ying-cong Chen"], "title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise", "comment": null, "summary": "Self-supervised learning has become an incredibly successful method for\nfeature learning, widely applied to many downstream tasks. It has proven\nespecially effective for discriminative tasks, surpassing the trending\ngenerative models. However, generative models perform better in image\ngeneration and detail enhancement. Thus, it is natural for us to find a\nconnection between SSL and generative models to further enhance the\nrepresentation capacity of SSL. As generative models can create new samples by\napproximating the data distribution, such modeling should also lead to a\nsemantic understanding of the raw visual data, which is necessary for\nrecognition tasks. This enlightens us to combine the core principle of the\ndiffusion model: diffusion noise, with SSL to learn a competitive recognition\nmodel. Specifically, diffusion noise can be viewed as a particular state of\nmask that reveals a close relationship between masked image modeling (MIM) and\ndiffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to\nincorporate diffusion noise into MIM by the position embedding of masked\ntokens. The multi-level noise schedule is a series of feature augmentations to\nfurther enhance the robustness of our model. We perform a comprehensive study\nto confirm its effectiveness in the classification of downstream tasks. Codes\nwill be released soon in public.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faN-JEPA\u65b9\u6cd5\uff0c\u5c06\u6269\u6563\u566a\u58f0\u5f15\u5165MIM\uff0c\u901a\u8fc7\u591a\u7ea7\u566a\u58f0\u8c03\u5ea6\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u7279\u5f81\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u7ec6\u8282\u589e\u5f3a\u65b9\u9762\u66f4\u4f18\u3002\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u5347SSL\u7684\u8868\u5f81\u80fd\u529b\u3002", "method": "\u63d0\u51faN-JEPA\uff0c\u5c06\u6269\u6563\u566a\u58f0\u901a\u8fc7\u63a9\u7801\u6807\u8bb0\u7684\u4f4d\u7f6e\u5d4c\u5165\u5f15\u5165MIM\uff0c\u5229\u7528\u591a\u7ea7\u566a\u58f0\u8c03\u5ea6\u589e\u5f3a\u6a21\u578b\u3002", "result": "\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86N-JEPA\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u6269\u6563\u566a\u58f0\u4e0eSSL\u7684N-JEPA\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.15288", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15288", "abs": "https://arxiv.org/abs/2507.15288", "authors": ["Omid G. Sani", "Maryam M. Shanechi"], "title": "Preferential subspace identification (PSID) with forward-backward smoothing", "comment": "17 pages, 5 figures", "summary": "System identification methods for multivariate time-series, such as neural\nand behavioral recordings, have been used to build models for predicting one\nfrom the other. For example, Preferential Subspace Identification (PSID) builds\na state-space model of a primary time-series (e.g., neural activity) to\noptimally predict a secondary time-series (e.g., behavior). However, PSID\nfocuses on optimal prediction using past primary data, even though in offline\napplications, better estimation can be achieved by incorporating concurrent\ndata (filtering) or all available data (smoothing). Here, we extend PSID to\nenable optimal filtering and smoothing. First, we show that the presence of a\nsecondary signal makes it possible to uniquely identify a model with an optimal\nKalman update step (to enable filtering) from a family of otherwise equivalent\nstate-space models. Our filtering solution augments PSID with a reduced-rank\nregression step that directly learns the optimal gain required for the update\nstep from data. We refer to this extension of PSID as PSID with filtering.\nSecond, inspired by two-filter Kalman smoother formulations, we develop a novel\nforward-backward PSID smoothing algorithm where we first apply PSID with\nfiltering and then apply it again in the reverse time direction on the\nresiduals of the filtered secondary signal. We validate our methods on\nsimulated data, showing that our approach recovers the ground-truth model\nparameters for filtering, and achieves optimal filtering and smoothing decoding\nperformance of the secondary signal that matches the ideal performance of the\ntrue underlying model. This work provides a principled framework for optimal\nlinear filtering and smoothing in the two-signal setting, significantly\nexpanding the toolkit for analyzing dynamic interactions in multivariate\ntime-series.", "AI": {"tldr": "\u6269\u5c55\u4e86PSID\u65b9\u6cd5\uff0c\u5f15\u5165\u6ee4\u6ce2\u548c\u5e73\u6ed1\u6280\u672f\uff0c\u4f18\u5316\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3002", "motivation": "PSID\u4ec5\u5229\u7528\u8fc7\u53bb\u6570\u636e\u9884\u6d4b\uff0c\u800c\u6ee4\u6ce2\u548c\u5e73\u6ed1\u80fd\u5229\u7528\u66f4\u591a\u6570\u636e\u63d0\u5347\u4f30\u8ba1\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u964d\u79e9\u56de\u5f52\u5b66\u4e60\u6700\u4f18\u589e\u76ca\uff0c\u5b9e\u73b0\u6ee4\u6ce2\uff1b\u7ed3\u5408\u524d\u5411-\u540e\u5411\u7b97\u6cd5\u5b9e\u73b0\u5e73\u6ed1\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u53c2\u6570\u6062\u590d\u548c\u6027\u80fd\u4f18\u5316\uff0c\u5339\u914d\u7406\u60f3\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u4e3a\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u7ebf\u6027\u6ee4\u6ce2\u548c\u5e73\u6ed1\u6846\u67b6\u3002"}}
{"id": "2507.15223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15223", "abs": "https://arxiv.org/abs/2507.15223", "authors": ["Siqi Chen", "Guoqing Zhang", "Jiahao Lai", "Bingzhi Shen", "Sihong Zhang", "Caixia Dong", "Xuejin Chen", "Yang Li"], "title": "Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel", "comment": null, "summary": "Advancements in 3D vision have increased the impact of blood vessel modeling\non medical applications. However, accurately representing the complex geometry\nand topology of blood vessels remains a challenge due to their intricate\nbranching patterns, curvatures, and irregular shapes. In this study, we propose\na hierarchical part-based frame work for 3D vessel generation that separates\nthe global binary tree-like topology from local geometric details. Our approach\nproceeds in three stages: (1) key graph generation to model the overall\nhierarchical struc ture, (2) vessel segment generation conditioned on geometric\nproperties, and (3) hierarchical vessel assembly by integrating the local\nsegments according to the global key graph. We validate our framework on real\nworld datasets, demonstrating superior performance over existing methods in\nmodeling complex vascular networks. This work marks the first successful\napplication of a part-based generative approach for 3D vessel modeling, setting\na new benchmark for vascular data generation. The code is available at:\nhttps://github.com/CybercatChen/PartVessel.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u76843D\u8840\u7ba1\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u5168\u5c40\u62d3\u6251\u548c\u5c40\u90e8\u51e0\u4f55\u7ec6\u8282\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u8840\u7ba1\u7f51\u7edc\u7684\u5efa\u6a21\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u8840\u7ba1\u7684\u590d\u6742\u5206\u652f\u3001\u5f2f\u66f2\u548c\u4e0d\u89c4\u5219\u5f62\u72b6\uff0c\u51c6\u786e\u5efa\u6a21\u5176\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5206\u4e09\u9636\u6bb5\uff1a\u751f\u6210\u5173\u952e\u56fe\u5efa\u6a21\u5168\u5c40\u5c42\u6b21\u7ed3\u6784\uff0c\u57fa\u4e8e\u51e0\u4f55\u5c5e\u6027\u751f\u6210\u8840\u7ba1\u6bb5\uff0c\u6574\u5408\u5c40\u90e8\u6bb5\u5230\u5168\u5c40\u56fe\u4e2d\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9996\u6b21\u6210\u529f\u5e94\u7528\u57fa\u4e8e\u90e8\u5206\u7684\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u4e3a3D\u8840\u7ba1\u5efa\u6a21\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.15290", "categories": ["cs.LG", "I.2.6; I.2.0"], "pdf": "https://arxiv.org/pdf/2507.15290", "abs": "https://arxiv.org/abs/2507.15290", "authors": ["Emile Anand", "Sarah Liaw"], "title": "Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown", "comment": "39 pages, 2 figures, 36 tables", "summary": "Thompson Sampling (TS) is widely used to address the exploration/exploitation\ntradeoff in contextual bandits, yet recent theory shows that it does not\nexplore aggressively enough in high-dimensional problems. Feel-Good Thompson\nSampling (FG-TS) addresses this by adding an optimism bonus that biases toward\nhigh-reward models, and it achieves the asymptotically minimax-optimal regret\nin the linear setting when posteriors are exact. However, its performance with\n\\emph{approximate} posteriors -- common in large-scale or neural problems --\nhas not been benchmarked. We provide the first systematic study of FG-TS and\nits smoothed variant (SFG-TS) across eleven real-world and synthetic\nbenchmarks. To evaluate their robustness, we compare performance across\nsettings with exact posteriors (linear and logistic bandits) to approximate\nregimes produced by fast but coarse stochastic-gradient samplers. Ablations\nover preconditioning, bonus scale, and prior strength reveal a trade-off:\nlarger bonuses help when posterior samples are accurate, but hurt when sampling\nnoise dominates. FG-TS generally outperforms vanilla TS in linear and logistic\nbandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS\nand its variants are competitive and easy-to-use, we recommend them as\nbaselines in modern contextual-bandit benchmarks. Finally, we provide source\ncode for all our experiments in\nhttps://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.", "AI": {"tldr": "FG-TS\u901a\u8fc7\u4e50\u89c2\u5956\u52b1\u63d0\u5347\u63a2\u7d22\u6027\u80fd\uff0c\u5728\u7cbe\u786e\u540e\u9a8c\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u8fd1\u4f3c\u540e\u9a8c\uff08\u5982\u795e\u7ecf\u7f51\u7edc\uff09\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\u3002", "motivation": "\u89e3\u51b3Thompson Sampling\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u63a2\u7d22\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e50\u89c2\u5956\u52b1\uff08FG-TS\uff09\u53ca\u5176\u5e73\u6ed1\u53d8\u4f53\uff08SFG-TS\uff09\uff0c\u572811\u4e2a\u57fa\u51c6\u4e0a\u6d4b\u8bd5\u5176\u6027\u80fd\u3002", "result": "FG-TS\u5728\u7ebf\u6027\u548c\u903b\u8f91bandits\u4e2d\u4f18\u4e8eTS\uff0c\u4f46\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u8f83\u5f31\u3002", "conclusion": "FG-TS\u6613\u4e8e\u4f7f\u7528\u4e14\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5efa\u8bae\u4f5c\u4e3a\u73b0\u4ee3\u4e0a\u4e0b\u6587bandit\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.15227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15227", "abs": "https://arxiv.org/abs/2507.15227", "authors": ["Krishna Kanth Nakka"], "title": "Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders", "comment": "Preprint. Under review", "summary": "Interpretability is critical in high-stakes domains such as medical imaging,\nwhere understanding model decisions is essential for clinical adoption. In this\nwork, we introduce Sparse Autoencoder (SAE)-based interpretability to breast\nimaging by analyzing {Mammo-CLIP}, a vision--language foundation model\npretrained on large-scale mammogram image--report pairs. We train a patch-level\n\\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features\nassociated with clinically relevant breast concepts such as \\textit{mass} and\n\\textit{suspicious calcification}. Our findings reveal that top activated class\nlevel latent neurons in the SAE latent space often tend to align with ground\ntruth regions, and also uncover several confounding factors influencing the\nmodel's decision-making process. Additionally, we analyze which latent neurons\nthe model relies on during downstream finetuning for improving the breast\nconcept prediction. This study highlights the promise of interpretable SAE\nlatent representations in providing deeper insight into the internal workings\nof foundation models at every layer for breast imaging.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u4e73\u817a\u5f71\u50cf\u4e2d\u7684\u57fa\u7840\u6a21\u578bMammo-CLIP\uff0c\u63ed\u793a\u4e86\u5176\u6f5c\u5728\u7279\u5f81\u4e0e\u4e34\u5e8a\u76f8\u5173\u6982\u5ff5\u7684\u5173\u8054\u3002", "motivation": "\u5728\u4e73\u817a\u5f71\u50cf\u7b49\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u5bf9\u4e34\u5e8a\u91c7\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2apatch\u7ea7\u7684Mammo-SAE\uff0c\u5206\u6790Mammo-CLIP\u7684\u6f5c\u5728\u7279\u5f81\uff0c\u8bc6\u522b\u4e0e\u4e34\u5e8a\u76f8\u5173\u6982\u5ff5\uff08\u5982\u80bf\u5757\u548c\u53ef\u7591\u9499\u5316\uff09\u76f8\u5173\u7684\u795e\u7ecf\u5143\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cSAE\u6f5c\u5728\u7a7a\u95f4\u4e2d\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u901a\u5e38\u4e0e\u771f\u5b9e\u533a\u57df\u5bf9\u9f50\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u6a21\u578b\u51b3\u7b56\u7684\u6df7\u6742\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u53ef\u89e3\u91ca\u7684SAE\u6f5c\u5728\u8868\u793a\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u4e73\u817a\u5f71\u50cf\u57fa\u7840\u6a21\u578b\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u3002"}}
{"id": "2507.15303", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.15303", "abs": "https://arxiv.org/abs/2507.15303", "authors": ["Liang Zhang", "Kong Chen", "Yuen Wu"], "title": "Universal crystal material property prediction via multi-view geometric fusion in graph transformers", "comment": null, "summary": "Accurately and comprehensively representing crystal structures is critical\nfor advancing machine learning in large-scale crystal materials simulations,\nhowever, effectively capturing and leveraging the intricate geometric and\ntopological characteristics of crystal structures remains a core, long-standing\nchallenge for most existing methods in crystal property prediction. Here, we\npropose MGT, a multi-view graph transformer framework that synergistically\nfuses SE3 invariant and SO3 equivariant graph representations, which\nrespectively captures rotation-translation invariance and rotation equivariance\nin crystal geometries. To strategically incorporate these complementary\ngeometric representations, we employ a lightweight mixture of experts router in\nMGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on\nthe specific target task. Compared with previous state-of-the-art models, MGT\nreduces the mean absolute error by up to 21% on crystal property prediction\ntasks through multi-task self-supervised pretraining. Ablation experiments and\ninterpretable investigations confirm the effectiveness of each technique\nimplemented in our framework. Additionally, in transfer learning scenarios\nincluding crystal catalyst adsorption energy and hybrid perovskite bandgap\nprediction, MGT achieves performance improvements of up to 58% over existing\nbaselines, demonstrating domain-agnostic scalability across diverse application\ndomains. As evidenced by the above series of studies, we believe that MGT can\nserve as useful model for crystal material property prediction, providing a\nvaluable tool for the discovery of novel materials.", "AI": {"tldr": "MGT\u662f\u4e00\u4e2a\u591a\u89c6\u56fe\u56fe\u53d8\u6362\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408SE3\u4e0d\u53d8\u548cSO3\u7b49\u53d8\u56fe\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6676\u4f53\u5c5e\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u6676\u4f53\u7ed3\u6784\u7684\u590d\u6742\u51e0\u4f55\u548c\u62d3\u6251\u7279\u5f81\uff0c\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u5728\u6676\u4f53\u6750\u6599\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMGT\u6846\u67b6\uff0c\u7ed3\u5408SE3\u4e0d\u53d8\u548cSO3\u7b49\u53d8\u56fe\u8868\u793a\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4e13\u5bb6\u6df7\u5408\u8def\u7531\u5668\u81ea\u9002\u5e94\u8c03\u6574\u6743\u91cd\u3002", "result": "MGT\u5728\u6676\u4f53\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e21%\uff0c\u5728\u8fc1\u79fb\u5b66\u4e60\u573a\u666f\u4e2d\u6027\u80fd\u63d0\u5347\u8fbe58%\u3002", "conclusion": "MGT\u53ef\u4f5c\u4e3a\u6676\u4f53\u6750\u6599\u5c5e\u6027\u9884\u6d4b\u7684\u6709\u7528\u6a21\u578b\uff0c\u4e3a\u65b0\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.15243", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15243", "abs": "https://arxiv.org/abs/2507.15243", "authors": ["Naeem Paeedeh", "Mahardhika Pratama", "Wolfgang Mayer", "Jimmy Cao", "Ryszard Kowlczyk"], "title": "Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation", "comment": null, "summary": "Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model\npre-trained with DINO combined with a prototypical classifier outperforms the\nlatest SOTA methods. A crucial limitation that needs to be overcome is that\nupdating too many parameters of the transformers leads to overfitting due to\nthe scarcity of labeled samples. To address this challenge, we propose a new\nconcept, Coalescent Projection (CP), as an effective successor to soft prompts.\nAdditionally, we propose a novel pseudo-class generation method combined with\nSelf-Supervised Transformations (SSTs) that relies solely on the base domain to\nprepare the network for encountering unseen samples from different domains. The\nproposed method exhibits its effectiveness in comprehensive experiments on the\nextreme domain shift scenario of the BSCD-FSL benchmark. Our code is published\nat https://github.com/Naeem-Paeedeh/CPLSR.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5Coalescent Projection (CP)\u548c\u4f2a\u7c7b\u751f\u6210\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u53d8\u6362\uff0c\u89e3\u51b3\u4e86\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u5728BSCD-FSL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5c24\u5176\u662f\u53d8\u6362\u5668\u53c2\u6570\u66f4\u65b0\u8fc7\u591a\u65f6\u3002", "method": "\u63d0\u51faCoalescent Projection (CP)\u66ff\u4ee3\u8f6f\u63d0\u793a\uff0c\u5e76\u7ed3\u5408\u81ea\u76d1\u7763\u53d8\u6362\u751f\u6210\u4f2a\u7c7b\uff0c\u4ec5\u4f9d\u8d56\u57fa\u7840\u57df\u6570\u636e\u3002", "result": "\u5728BSCD-FSL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CP\u548c\u4f2a\u7c7b\u751f\u6210\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u6311\u6218\u3002"}}
{"id": "2507.15336", "categories": ["cs.LG", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.15336", "abs": "https://arxiv.org/abs/2507.15336", "authors": ["Jialiang Wang", "Hanmo Liu", "Shimin Di", "Zhili Wang", "Jiachuan Wang", "Lei Chen", "Xiaofang Zhou"], "title": "Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design", "comment": null, "summary": "Database systems have recently advocated for embedding machine learning (ML)\ncapabilities, offering declarative model queries over large, managed model\nrepositories, thereby circumventing the huge computational overhead of\ntraditional ML-based algorithms in automated neural network model selection.\nPioneering database studies aim to organize existing benchmark repositories as\nmodel bases (MB), querying them for the model records with the highest\nperformance estimation metrics for given tasks. However, this static model\nselection practice overlooks the fine-grained, evolving relational dependencies\nbetween diverse task queries and model architecture variations, resulting in\nsuboptimal matches and failing to further refine the model effectively. To fill\nthe model refinement gap in database research, we propose M-DESIGN, a curated\nmodel knowledge base (MKB) pipeline for mastering neural network refinement by\nadaptively weaving prior insights about model architecture modification. First,\nwe propose a knowledge weaving engine that reframes model refinement as an\nadaptive query problem over task metadata. Given a user's task query, M-DESIGN\nquickly matches and iteratively refines candidate models by leveraging a\ngraph-relational knowledge schema that explicitly encodes data properties,\narchitecture variations, and pairwise performance deltas as joinable relations.\nThis schema supports fine-grained relational analytics over architecture tweaks\nand drives a predictive query planner that can detect and adapt to\nout-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics\ntasks, where our model knowledge base enriches existing benchmarks with\nstructured metadata covering 3 graph tasks and 22 graph datasets, contributing\ndata records of 67,760 graph models. Empirical results demonstrate that\nM-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited\nbudgets.", "AI": {"tldr": "M-DESIGN\u662f\u4e00\u4e2a\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4f18\u5316\u7684\u77e5\u8bc6\u5e93\u7ba1\u9053\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u67e5\u8be2\u4efb\u52a1\u5143\u6570\u636e\u6765\u4f18\u5316\u6a21\u578b\u9009\u62e9\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\u5ffd\u7565\u4e86\u4efb\u52a1\u67e5\u8be2\u4e0e\u6a21\u578b\u67b6\u6784\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5173\u7cfb\uff0c\u5bfc\u81f4\u5339\u914d\u4e0d\u4f18\u4e14\u65e0\u6cd5\u6709\u6548\u4f18\u5316\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u7f16\u7ec7\u5f15\u64ce\uff0c\u5c06\u6a21\u578b\u4f18\u5316\u91cd\u6784\u4e3a\u5bf9\u4efb\u52a1\u5143\u6570\u636e\u7684\u81ea\u9002\u5e94\u67e5\u8be2\u95ee\u9898\uff0c\u5229\u7528\u56fe\u5173\u7cfb\u77e5\u8bc6\u6a21\u5f0f\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "result": "\u572833\u4e2a\u6570\u636e-\u4efb\u52a1\u5bf9\u4e2d\uff0cM-DESIGN\u5728\u6709\u9650\u9884\u7b97\u5185\u4e3a26\u5bf9\u63d0\u4f9b\u4e86\u6700\u4f18\u6a21\u578b\u3002", "conclusion": "M-DESIGN\u901a\u8fc7\u7ed3\u6784\u5316\u5143\u6570\u636e\u548c\u81ea\u9002\u5e94\u67e5\u8be2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u9009\u62e9\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2507.15249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15249", "abs": "https://arxiv.org/abs/2507.15249", "authors": ["Yanbing Zhang", "Zhe Wang", "Qin Zhou", "Mengping Yang"], "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers", "comment": "Accepted by ICCV 2025", "summary": "In light of recent breakthroughs in text-to-image (T2I) generation,\nparticularly with diffusion transformers (DiT), subject-driven technologies are\nincreasingly being employed for high-fidelity customized production that\npreserves subject identity from reference inputs, enabling thrilling design\nworkflows and engaging entertainment. Existing alternatives typically require\neither per-subject optimization via trainable text embeddings or training\nspecialized encoders for subject feature extraction on large-scale datasets.\nSuch dependencies on training procedures fundamentally constrain their\npractical applications. More importantly, current methodologies fail to fully\nleverage the inherent zero-shot potential of modern diffusion transformers\n(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this\ngap, we propose FreeCus, a genuinely training-free framework that activates\nDiT's capabilities through three key innovations: 1) We introduce a pivotal\nattention sharing mechanism that captures the subject's layout integrity while\npreserving crucial editing flexibility. 2) Through a straightforward analysis\nof DiT's dynamic shifting, we propose an upgraded variant that significantly\nimproves fine-grained feature extraction. 3) We further integrate advanced\nMultimodal Large Language Models (MLLMs) to enrich cross-modal semantic\nrepresentations. Extensive experiments reflect that our method successfully\nunlocks DiT's zero-shot ability for consistent subject synthesis across diverse\ncontexts, achieving state-of-the-art or comparable results compared to\napproaches that require additional training. Notably, our framework\ndemonstrates seamless compatibility with existing inpainting pipelines and\ncontrol modules, facilitating more compelling experiences. Our code is\navailable at: https://github.com/Monalissaa/FreeCus.", "AI": {"tldr": "FreeCus\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5171\u4eab\u673a\u5236\u3001\u6539\u8fdb\u7684DiT\u53d8\u4f53\u548cMLLM\u96c6\u6210\uff0c\u6fc0\u6d3b\u6269\u6563\u53d8\u538b\u5668\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u4e3b\u9898\u9a71\u52a8\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u6269\u6563\u53d8\u538b\u5668\u7684\u96f6\u6837\u672c\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u5171\u4eab\u673a\u5236\u3001\u6539\u8fdb\u7684DiT\u53d8\u4f53\uff0c\u5e76\u96c6\u6210MLLM\u4ee5\u589e\u5f3a\u8de8\u6a21\u6001\u8bed\u4e49\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFreeCus\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u9700\u8981\u8bad\u7ec3\u7684\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u7ed3\u679c\u3002", "conclusion": "FreeCus\u6210\u529f\u89e3\u9501\u4e86DiT\u7684\u96f6\u6837\u672c\u80fd\u529b\uff0c\u517c\u5bb9\u73b0\u6709\u4fee\u590d\u7ba1\u9053\u548c\u63a7\u5236\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.15349", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.15349", "abs": "https://arxiv.org/abs/2507.15349", "authors": ["Zehua Cheng", "Rui Sun", "Jiahao Sun", "Yike Guo"], "title": "Scaling Decentralized Learning with FLock", "comment": null, "summary": "Fine-tuning the large language models (LLMs) are prevented by the deficiency\nof centralized control and the massive computing and communication overhead on\nthe decentralized schemes. While the typical standard federated learning (FL)\nsupports data privacy, the central server requirement creates a single point of\nattack and vulnerability to poisoning attacks. Generalizing the result in this\ndirection to 70B-parameter models in the heterogeneous, trustless environments\nhas turned out to be a huge, yet unbroken bottleneck. This paper introduces\nFLock, a decentralized framework for secure and efficient collaborative LLM\nfine-tuning. Integrating a blockchain-based trust layer with economic\nincentives, FLock replaces the central aggregator with a secure, auditable\nprotocol for cooperation among untrusted parties. We present the first\nempirical validation of fine-tuning a 70B LLM in a secure, multi-domain,\ndecentralized setting. Our experiments show the FLock framework defends against\nbackdoor poisoning attacks that compromise standard FL optimizers and fosters\nsynergistic knowledge transfer. The resulting models show a >68% reduction in\nadversarial attack success rates. The global model also demonstrates superior\ncross-domain generalization, outperforming models trained in isolation on their\nown specialized data.", "AI": {"tldr": "FLock\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5b89\u5168\u9ad8\u6548\u7684LLM\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u96c6\u4e2d\u63a7\u5236\u548c\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u533a\u5757\u94fe\u4fe1\u4efb\u5c42\u548c\u7ecf\u6d4e\u6fc0\u52b1\u5b9e\u73b0\u65e0\u4fe1\u4efb\u65b9\u5408\u4f5c\u3002", "motivation": "\u89e3\u51b3\u96c6\u4e2d\u5f0f\u8054\u90a6\u5b66\u4e60\u7684\u5355\u70b9\u653b\u51fb\u548c\u6295\u6bd2\u653b\u51fb\u95ee\u9898\uff0c\u540c\u65f6\u5728\u5f02\u6784\u3001\u65e0\u4fe1\u4efb\u73af\u5883\u4e2d\u6269\u5c55\u81f370B\u53c2\u6570\u6a21\u578b\u3002", "method": "\u96c6\u6210\u533a\u5757\u94fe\u4fe1\u4efb\u5c42\u548c\u7ecf\u6d4e\u6fc0\u52b1\uff0c\u66ff\u4ee3\u4e2d\u592e\u805a\u5408\u5668\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u5ba1\u8ba1\u7684\u65e0\u4fe1\u4efb\u65b9\u5408\u4f5c\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFLock\u80fd\u62b5\u5fa1\u540e\u95e8\u6295\u6bd2\u653b\u51fb\uff0c\u51cf\u5c1168%\u4ee5\u4e0a\u7684\u5bf9\u6297\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u4fc3\u8fdb\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u3002", "conclusion": "FLock\u5728\u5b89\u5168\u3001\u591a\u9886\u57df\u3001\u53bb\u4e2d\u5fc3\u5316\u73af\u5883\u4e0b\u9a8c\u8bc1\u4e8670B LLM\u5fae\u8c03\u7684\u6709\u6548\u6027\uff0c\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15257", "abs": "https://arxiv.org/abs/2507.15257", "authors": ["Pei An", "Jiaqi Yang", "Muyao Peng", "You Yang", "Qiong Liu", "Xiaolin Wu", "Liangliang Nan"], "title": "MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP", "comment": "Accepted by ICCV 2025", "summary": "Image-to-point-cloud (I2P) registration is a fundamental problem in computer\nvision, focusing on establishing 2D-3D correspondences between an image and a\npoint cloud. The differential perspective-n-point (PnP) has been widely used to\nsupervise I2P registration networks by enforcing the projective constraints on\n2D-3D correspondences. However, differential PnP is highly sensitive to noise\nand outliers in the predicted correspondences. This issue hinders the\neffectiveness of correspondence learning. Inspired by the robustness of blind\nPnP against noise and outliers in correspondences, we propose an approximated\nblind PnP based correspondence learning approach. To mitigate the high\ncomputational cost of blind PnP, we simplify blind PnP to an amenable task of\nminimizing Chamfer distance between learned 2D and 3D keypoints, called\nMinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task\nlearning module, named as MinCD-Net, which can be easily integrated into the\nexisting I2P registration architectures. Extensive experiments on 7-Scenes,\nRGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net\noutperforms state-of-the-art methods and achieves a higher inlier ratio (IR)\nand registration recall (RR) in both cross-scene and cross-dataset settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd1\u4f3c\u76f2PnP\u7684\u5bf9\u5e94\u5b66\u4e60\u65b9\u6cd5MinCD-PnP\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u5b66\u4e60\u5230\u76842D\u548c3D\u5173\u952e\u70b9\u4e4b\u95f4\u7684Chamfer\u8ddd\u79bb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5fae\u5206PnP\u5bf9\u566a\u58f0\u548c\u5f02\u5e38\u503c\u654f\u611f\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5fae\u5206PnP\u5728\u56fe\u50cf\u5230\u70b9\u4e91\uff08I2P\uff09\u914d\u51c6\u4e2d\u5bf9\u566a\u58f0\u548c\u5f02\u5e38\u503c\u654f\u611f\uff0c\u5f71\u54cd\u4e86\u5bf9\u5e94\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faMinCD-PnP\u65b9\u6cd5\uff0c\u7b80\u5316\u76f2PnP\u4e3a\u6700\u5c0f\u5316Chamfer\u8ddd\u79bb\u7684\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u591a\u4efb\u52a1\u5b66\u4e60\u6a21\u5757MinCD-Net\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cMinCD-Net\u5728\u8de8\u573a\u666f\u548c\u8de8\u6570\u636e\u96c6\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5185\u70b9\u7387\uff08IR\uff09\u548c\u914d\u51c6\u53ec\u56de\u7387\uff08RR\uff09\u3002", "conclusion": "MinCD-PnP\u548cMinCD-Net\u6709\u6548\u63d0\u5347\u4e86I2P\u914d\u51c6\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.15381", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15381", "abs": "https://arxiv.org/abs/2507.15381", "authors": ["Julia Machnio", "Mads Nielsen", "Mostafa Mehdipour Ghazi"], "title": "To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models", "comment": "ICCV 2025", "summary": "Active learning (AL) seeks to reduce annotation costs by selecting the most\ninformative samples for labeling, making it particularly valuable in\nresource-constrained settings. However, traditional evaluation methods, which\nfocus solely on final accuracy, fail to capture the full dynamics of the\nlearning process. To address this gap, we propose PALM (Performance Analysis of\nActive Learning Models), a unified and interpretable mathematical model that\ncharacterizes AL trajectories through four key parameters: achievable accuracy,\ncoverage efficiency, early-stage performance, and scalability. PALM provides a\npredictive description of AL behavior from partial observations, enabling the\nestimation of future performance and facilitating principled comparisons across\ndifferent strategies. We validate PALM through extensive experiments on\nCIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and\nself-supervised embeddings. Our results demonstrate that PALM generalizes\neffectively across datasets, budgets, and strategies, accurately predicting\nfull learning curves from limited labeled data. Importantly, PALM reveals\ncrucial insights into learning efficiency, data space coverage, and the\nscalability of AL methods. By enabling the selection of cost-effective\nstrategies and predicting performance under tight budget constraints, PALM lays\nthe basis for more systematic, reproducible, and data-efficient evaluation of\nAL in both research and real-world applications. The code is available at:\nhttps://github.com/juliamachnio/PALM.", "AI": {"tldr": "PALM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u548c\u9884\u6d4b\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u7684\u6027\u80fd\u8f68\u8ff9\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u53c2\u6570\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edfAL\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u5173\u6ce8\u6700\u7ec8\u51c6\u786e\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u6355\u6349\u5b66\u4e60\u8fc7\u7a0b\u7684\u52a8\u6001\u53d8\u5316\u3002", "method": "\u63d0\u51faPALM\u6a21\u578b\uff0c\u901a\u8fc7\u56db\u4e2a\u53c2\u6570\uff08\u53ef\u8fbe\u5230\u7684\u51c6\u786e\u6027\u3001\u8986\u76d6\u6548\u7387\u3001\u65e9\u671f\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\uff09\u63cf\u8ff0AL\u8f68\u8ff9\u3002", "result": "PALM\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u7b56\u7565\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u80fd\u51c6\u786e\u9884\u6d4b\u5b66\u4e60\u66f2\u7ebf\u5e76\u63ed\u793aAL\u65b9\u6cd5\u7684\u5173\u952e\u7279\u6027\u3002", "conclusion": "PALM\u4e3aAL\u7684\u7cfb\u7edf\u5316\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u652f\u6301\u6210\u672c\u6548\u76ca\u7b56\u7565\u9009\u62e9\u548c\u6027\u80fd\u9884\u6d4b\u3002"}}
{"id": "2507.15269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15269", "abs": "https://arxiv.org/abs/2507.15269", "authors": ["Fangqiu Yi", "Jingyu Xu", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Conditional Video Generation for High-Efficiency Video Compression", "comment": null, "summary": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u6a21\u578b\u7684\u89c6\u9891\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u4f18\u5316\u91cd\u5efa\u663e\u8457\u63d0\u5347\u538b\u7f29\u8d28\u91cf\u3002", "motivation": "\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u91cd\u5efa\u89c6\u9891\u5185\u5bb9\u65f6\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u9ad8\u5ea6\u4e00\u81f4\uff0c\u56e0\u6b64\u5c06\u5176\u7528\u4e8e\u89c6\u9891\u538b\u7f29\u4ee5\u4f18\u5316\u611f\u77e5\u8d28\u91cf\u3002", "method": "\u5c06\u89c6\u9891\u538b\u7f29\u91cd\u6784\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u5f15\u5165\u591a\u7c92\u5ea6\u6761\u4ef6\u3001\u7d27\u51d1\u8868\u793a\u548c\u591a\u6761\u4ef6\u8bad\u7ec3\u6a21\u5757\u3002", "result": "\u5728FVD\u548cLPIPS\u7b49\u611f\u77e5\u8d28\u91cf\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u548c\u795e\u7ecf\u7f16\u89e3\u7801\u5668\uff0c\u5c24\u5176\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u4e86\u611f\u77e5\u4f18\u5316\u7684\u89c6\u9891\u538b\u7f29\uff0c\u4e3a\u9ad8\u8d28\u91cf\u89c6\u9891\u4f20\u8f93\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15386", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15386", "abs": "https://arxiv.org/abs/2507.15386", "authors": ["Juntao Wang", "Feng Yin", "Tian Ding", "Tsung-Hui Chang", "Zhi-Quan Luo", "Qi Yan"], "title": "Learning to Gridize: Segment Physical World by Wireless Communication Channel", "comment": null, "summary": "Gridization, the process of partitioning space into grids where users share\nsimilar channel characteristics, serves as a fundamental prerequisite for\nefficient large-scale network optimization. However, existing methods like\nGeographical or Beam Space Gridization (GSG or BSG) are limited by reliance on\nunavailable location data or the flawed assumption that similar signal\nstrengths imply similar channel properties. We propose Channel Space\nGridization (CSG), a pioneering framework that unifies channel estimation and\ngridization for the first time. Formulated as a joint optimization problem, CSG\nuses only beam-level reference signal received power (RSRP) to estimate Channel\nAngle Power Spectra (CAPS) and partition samples into grids with homogeneous\nchannel characteristics. To perform CSG, we develop the CSG Autoencoder\n(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse\ncodebook quantizer, and a physics-informed decoder based on the Localized\nStatistical Channel Model. On recognizing the limitations of naive training\nscheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous\n(PIDA) training scheme for CSG-AE, ensuring stable and effective training by\nsystematically addressing the common pitfalls of the naive training paradigm.\nEvaluations reveal that CSG-AE excels in CAPS estimation accuracy and\nclustering quality on synthetic data. On real-world datasets, it reduces Active\nMean Absolute Error (MAE) by 30\\% and Overall MAE by 65\\% on RSRP prediction\naccuracy compared to salient baselines using the same data, while improving\nchannel consistency, cluster sizes balance, and active ratio, advancing the\ndevelopment of gridization for large-scale network optimization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCSG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4fe1\u9053\u4f30\u8ba1\u548c\u7f51\u683c\u5316\uff0c\u4ec5\u4f7f\u7528RSRP\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u7f51\u683c\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e0d\u53ef\u7528\u7684\u4f4d\u7f6e\u6570\u636e\u6216\u9519\u8bef\u7684\u4fe1\u53f7\u5f3a\u5ea6\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u7f51\u683c\u5316\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faCSG-AE\u6a21\u578b\uff0c\u5305\u542b\u53ef\u8bad\u7ec3\u7684RSRP-to-CAPS\u7f16\u7801\u5668\u3001\u53ef\u5b66\u4e60\u7a00\u758f\u7801\u672c\u91cf\u5316\u5668\u548c\u57fa\u4e8e\u7269\u7406\u7684\u89e3\u7801\u5668\uff0c\u5e76\u8bbe\u8ba1\u4e86PIDA\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u663e\u8457\u964d\u4f4e\u4e86MAE\uff0c\u63d0\u5347\u4e86\u4fe1\u9053\u4e00\u81f4\u6027\u548c\u805a\u7c7b\u5e73\u8861\u6027\u3002", "conclusion": "CSG\u6846\u67b6\u4e3a\u5927\u89c4\u6a21\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u7f51\u683c\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.15285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15285", "abs": "https://arxiv.org/abs/2507.15285", "authors": ["Lazaro Janier Gonzalez-Soler", "Maciej Salwowski", "Christoph Busch"], "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems", "comment": "Submitted to IEEE-TIFS", "summary": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u751f\u7269\u8bc6\u522b\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edfCNN\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u751f\u7269\u8bc6\u522b\u653b\u51fb\u68c0\u6d4b\u4e2d\u9762\u4e34\u9002\u5e94\u6027\u5dee\u3001\u6570\u636e\u9700\u6c42\u9ad8\u548c\u9690\u79c1\u95ee\u9898\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u6280\u672f\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u5b9a\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u8d44\u6e90\u5373\u53ef\u8d85\u8d8a\u4f20\u7edfCNN\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u5de5\u5177\uff0c\u80fd\u591f\u63d0\u9ad8\u653b\u51fb\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15397", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15397", "abs": "https://arxiv.org/abs/2507.15397", "authors": ["Scott Pesme", "Giacomo Meanti", "Michael Arbel", "Julien Mairal"], "title": "MAP Estimation with Denoisers: Convergence Rates and Guarantees", "comment": null, "summary": "Denoiser models have become powerful tools for inverse problems, enabling the\nuse of pretrained networks to approximate the score of a smoothed prior\ndistribution. These models are often used in heuristic iterative schemes aimed\nat solving Maximum a Posteriori (MAP) optimisation problems, where the proximal\noperator of the negative log-prior plays a central role. In practice, this\noperator is intractable, and practitioners plug in a pretrained denoiser as a\nsurrogate-despite the lack of general theoretical justification for this\nsubstitution. In this work, we show that a simple algorithm, closely related to\nseveral used in practice, provably converges to the proximal operator under a\nlog-concavity assumption on the prior $p$. We show that this algorithm can be\ninterpreted as a gradient descent on smoothed proximal objectives. Our analysis\nthus provides a theoretical foundation for a class of empirically successful\nbut previously heuristic methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7b97\u6cd5\uff0c\u5728log-concave\u5148\u9a8c\u5047\u8bbe\u4e0b\u53ef\u8bc1\u660e\u6536\u655b\u5230\u8fd1\u7aef\u7b97\u5b50\uff0c\u4e3a\u5b9e\u8df5\u4e2d\u5e38\u7528\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u73b0\u6709\u53bb\u566a\u6a21\u578b\u5728MAP\u4f18\u5316\u95ee\u9898\u4e2d\u4f5c\u4e3a\u5148\u9a8c\u5206\u5e03\u7684\u4ee3\u7406\u4f7f\u7528\uff0c\u4f46\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4e0e\u5b9e\u8df5\u4e2d\u5e38\u7528\u65b9\u6cd5\u76f8\u5173\u7684\u7b80\u5355\u7b97\u6cd5\uff0c\u8bc1\u660e\u5176\u5728log-concave\u5148\u9a8c\u4e0b\u6536\u655b\u5230\u8fd1\u7aef\u7b97\u5b50\u3002", "result": "\u7b97\u6cd5\u53ef\u89e3\u91ca\u4e3a\u5bf9\u5e73\u6ed1\u8fd1\u7aef\u76ee\u6807\u7684\u68af\u5ea6\u4e0b\u964d\uff0c\u4e3a\u542f\u53d1\u5f0f\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u53bb\u566a\u6a21\u578b\u5728MAP\u4f18\u5316\u4e2d\u7406\u8bba\u7a7a\u767d\uff0c\u652f\u6301\u4e86\u5b9e\u8df5\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002"}}
{"id": "2507.15297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15297", "abs": "https://arxiv.org/abs/2507.15297", "authors": ["Zhiyu Pan", "Xiongjun Guan", "Yongjie Duan", "Jianjiang Feng", "Jie Zhou"], "title": "Minutiae-Anchored Local Dense Representation for Fingerprint Matching", "comment": "Under review", "summary": "Fingerprint matching under diverse capture conditions remains a fundamental\nchallenge in biometric recognition. To achieve robust and accurate performance\nin such scenarios, we propose DMD, a minutiae-anchored local dense\nrepresentation which captures both fine-grained ridge textures and\ndiscriminative minutiae features in a spatially structured manner.\nSpecifically, descriptors are extracted from local patches centered and\noriented on each detected minutia, forming a three-dimensional tensor, where\ntwo dimensions represent spatial locations on the fingerprint plane and the\nthird encodes semantic features. This representation explicitly captures\nabstract features of local image patches, enabling a multi-level, fine-grained\ndescription that aggregates information from multiple minutiae and their\nsurrounding ridge structures. Furthermore, thanks to its strong spatial\ncorrespondence with the patch image, DMD allows for the use of foreground\nsegmentation masks to identify valid descriptor regions. During matching,\ncomparisons are then restricted to overlapping foreground areas, improving\nefficiency and robustness. Extensive experiments on rolled, plain, parital,\ncontactless, and latent fingerprint datasets demonstrate the effectiveness and\ngeneralizability of the proposed method. It achieves state-of-the-art accuracy\nacross multiple benchmarks while maintaining high computational efficiency,\nshowing strong potential for large-scale fingerprint recognition. Corresponding\ncode is available at https://github.com/Yu-Yy/DMD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDMD\u7684\u5c40\u90e8\u5bc6\u96c6\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u91c7\u96c6\u6761\u4ef6\u4e0b\u7684\u6307\u7eb9\u5339\u914d\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u7eb9\u7406\u548c\u7ec6\u8282\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u5339\u914d\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u6307\u7eb9\u5339\u914d\u5728\u4e0d\u540c\u91c7\u96c6\u6761\u4ef6\u4e0b\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u7eb9\u7406\u548c\u7ec6\u8282\u7279\u5f81\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u4ee5\u7ec6\u8282\u70b9\u4e3a\u4e2d\u5fc3\u63d0\u53d6\u5c40\u90e8\u5757\u7684\u4e09\u7ef4\u5f20\u91cf\u8868\u793a\uff0c\u7ed3\u5408\u7a7a\u95f4\u7ed3\u6784\u548c\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u5229\u7528\u524d\u666f\u5206\u5272\u63a9\u7801\u4f18\u5316\u5339\u914d\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u6307\u7eb9\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\u5e76\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u3002", "conclusion": "DMD\u65b9\u6cd5\u5728\u6307\u7eb9\u8bc6\u522b\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5e94\u7528\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6307\u7eb9\u8bc6\u522b\u3002"}}
{"id": "2507.15431", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15431", "abs": "https://arxiv.org/abs/2507.15431", "authors": ["Andrew Gracyk"], "title": "The calculus of variations of the Transformer on the hyperspherical tangent bundle", "comment": "First version", "summary": "We offer a theoretical mathematical background to Transformers through\nLagrangian optimization across the token space. The Transformer, as a flow map,\nexists in the tangent fiber for each token along the high-dimensional unit\nsphere. The circumstance of the hypersphere across the latent data is\nreasonable due to the trained diagonal matrix equal to the identity, which has\nvarious empirical justifications. Thus, under the continuum limit of the\ndynamics, the latent vectors flow among the tangent bundle. Using these facts,\nwe devise a mathematical framework for the Transformer through calculus of\nvariations. We develop a functional and show that the continuous flow map\ninduced by the Transformer satisfies this functional, therefore the Transformer\ncan be viewed as a natural solver of a calculus of variations problem. We\ninvent new scenarios of when our methods are applicable based on loss\noptimization with respect to path optimality. We derive the Euler-Lagrange\nequation for the Transformer. The variant of the Euler-Lagrange equation we\npresent has various appearances in literature, but, to our understanding,\noftentimes not foundationally proven or under other specialized cases. Our\noverarching proof is new: our techniques are classical and the use of the flow\nmap object is original. We provide several other relevant results, primarily\nones specific to neural scenarios. In particular, much of our analysis will be\nattempting to quantify Transformer data in variational contexts under neural\napproximations. Calculus of variations on manifolds is a well-nourished\nresearch area, but for the Transformer specifically, it is uncharted: we lay\nthe foundation for this area through an introduction to the Lagrangian for the\nTransformer.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u4f18\u5316\u5728\u4ee4\u724c\u7a7a\u95f4\u4e2d\u4e3aTransformer\u63d0\u4f9b\u4e86\u7406\u8bba\u6570\u5b66\u80cc\u666f\uff0c\u5c06\u5176\u89c6\u4e3a\u9ad8\u7ef4\u5355\u4f4d\u7403\u4e0a\u7684\u6d41\u6620\u5c04\uff0c\u5e76\u63a8\u5bfc\u4e86\u5176\u53d8\u5206\u95ee\u9898\u7684\u89e3\u3002", "motivation": "\u4e3aTransformer\u5efa\u7acb\u6570\u5b66\u6846\u67b6\uff0c\u586b\u8865\u5176\u5728\u6d41\u5f62\u4e0a\u53d8\u5206\u8ba1\u7b97\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u6cd5\uff0c\u63a8\u5bfcTransformer\u7684\u6b27\u62c9-\u62c9\u683c\u6717\u65e5\u65b9\u7a0b\uff0c\u5e76\u8bc1\u660e\u5176\u4f5c\u4e3a\u53d8\u5206\u95ee\u9898\u81ea\u7136\u89e3\u7684\u6027\u8d28\u3002", "result": "\u63d0\u51fa\u4e86Transformer\u7684\u62c9\u683c\u6717\u65e5\u5f62\u5f0f\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u53d8\u5206\u95ee\u9898\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8bba\u6587\u4e3aTransformer\u7684\u53d8\u5206\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u795e\u7ecf\u7f51\u7edc\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u3002"}}
{"id": "2507.15308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15308", "abs": "https://arxiv.org/abs/2507.15308", "authors": ["Zhimeng Xin", "Tianxu Wu", "Yixiong Zou", "Shiming Chen", "Dingjie Fu", "Xinge You"], "title": "Few-Shot Object Detection via Spatial-Channel State Space Model", "comment": null, "summary": "Due to the limited training samples in few-shot object detection (FSOD), we\nobserve that current methods may struggle to accurately extract effective\nfeatures from each channel. Specifically, this issue manifests in two aspects:\ni) channels with high weights may not necessarily be effective, and ii)\nchannels with low weights may still hold significant value. To handle this\nproblem, we consider utilizing the inter-channel correlation to facilitate the\nnovel model's adaptation process to novel conditions, ensuring the model can\ncorrectly highlight effective channels and rectify those incorrect ones. Since\nthe channel sequence is also 1-dimensional, its similarity with the temporal\nsequence inspires us to take Mamba for modeling the correlation in the channel\nsequence. Based on this concept, we propose a Spatial-Channel State Space\nModeling (SCSM) module for spatial-channel state modeling, which highlights the\neffective patterns and rectifies those ineffective ones in feature channels. In\nSCSM, we design the Spatial Feature Modeling (SFM) module to balance the\nlearning of spatial relationships and channel relationships, and then introduce\nthe Channel State Modeling (CSM) module based on Mamba to learn correlation in\nchannels. Extensive experiments on the VOC and COCO datasets show that the SCSM\nmodule enables the novel detector to improve the quality of focused feature\nrepresentation in channels and achieve state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7a7a\u95f4-\u901a\u9053\u72b6\u6001\u5efa\u6a21\uff08SCSM\uff09\u6a21\u5757\uff0c\u901a\u8fc7Mamba\u5efa\u6a21\u901a\u9053\u76f8\u5173\u6027\uff0c\u63d0\u5347\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\uff08FSOD\uff09\u4e2d\u7279\u5f81\u901a\u9053\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\u5728\u901a\u9053\u7279\u5f81\u63d0\u53d6\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9ad8\u6743\u91cd\u901a\u9053\u672a\u5fc5\u6709\u6548\uff0c\u4f4e\u6743\u91cd\u901a\u9053\u53ef\u80fd\u4ecd\u6709\u4ef7\u503c\u3002", "method": "\u8bbe\u8ba1\u4e86SCSM\u6a21\u5757\uff0c\u5305\u542b\u7a7a\u95f4\u7279\u5f81\u5efa\u6a21\uff08SFM\uff09\u548c\u57fa\u4e8eMamba\u7684\u901a\u9053\u72b6\u6001\u5efa\u6a21\uff08CSM\uff09\uff0c\u4ee5\u4f18\u5316\u901a\u9053\u7279\u5f81\u3002", "result": "\u5728VOC\u548cCOCO\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cSCSM\u6a21\u5757\u63d0\u5347\u4e86\u7279\u5f81\u8868\u793a\u8d28\u91cf\uff0c\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "SCSM\u6a21\u5757\u901a\u8fc7\u5efa\u6a21\u901a\u9053\u76f8\u5173\u6027\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c11\u6837\u672c\u76ee\u6807\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15442", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15442", "abs": "https://arxiv.org/abs/2507.15442", "authors": ["Owen Douglas", "Aku Kammonen", "Anamika Pandey", "Ra\u00fal Tempone"], "title": "An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations", "comment": "20 Pages", "summary": "This work proposes a training algorithm based on adaptive random Fourier\nfeatures (ARFF) with Metropolis sampling and resampling\n\\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and\ndiffusion components of stochastic differential equations from snapshot data.\nSpecifically, this study considers It\\^{o} diffusion processes and a\nlikelihood-based loss function derived from the Euler-Maruyama integration\nintroduced in \\cite{Dietrich2023} and\n\\cite{dridi2021learningstochasticdynamicalsystems}.\n  This work evaluates the proposed method against benchmark problems presented\nin \\cite{Dietrich2023}, including polynomial examples, underdamped Langevin\ndynamics, a stochastic susceptible-infected-recovered model, and a stochastic\nwave equation. Across all cases, the ARFF-based approach matches or surpasses\nthe performance of conventional Adam-based optimization in both loss\nminimization and convergence speed. These results highlight the potential of\nARFF as a compelling alternative for data-driven modeling of stochastic\ndynamics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u9002\u5e94\u968f\u673a\u5085\u91cc\u53f6\u7279\u5f81\uff08ARFF\uff09\u7684\u8bad\u7ec3\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5feb\u7167\u6570\u636e\u4e2d\u5b66\u4e60\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u6f02\u79fb\u548c\u6269\u6563\u9879\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfAdam\u4f18\u5316\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21\u4e2d\u5b58\u5728\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u7b97\u6cd5\u3002", "method": "\u7ed3\u5408Metropolis\u91c7\u6837\u548c\u91cd\u91c7\u6837\u7684ARFF\u65b9\u6cd5\uff0c\u57fa\u4e8eEuler-Maruyama\u79ef\u5206\u63a8\u5bfc\u7684\u4f3c\u7136\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARFF\u65b9\u6cd5\u5728\u635f\u5931\u6700\u5c0f\u5316\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfAdam\u4f18\u5316\u3002", "conclusion": "ARFF\u662f\u6570\u636e\u9a71\u52a8\u968f\u673a\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u6709\u529b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.15321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15321", "abs": "https://arxiv.org/abs/2507.15321", "authors": ["Zhenyu Li", "Haotong Lin", "Jiashi Feng", "Peter Wonka", "Bingyi Kang"], "title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?", "comment": "Webpage: https://zhyever.github.io/benchdepth", "summary": "Depth estimation is a fundamental task in computer vision with diverse\napplications. Recent advancements in deep learning have led to powerful depth\nfoundation models (DFMs), yet their evaluation remains challenging due to\ninconsistencies in existing protocols. Traditional benchmarks rely on\nalignment-based metrics that introduce biases, favor certain depth\nrepresentations, and complicate fair comparisons. In this work, we propose\nBenchDepth, a new benchmark that evaluates DFMs through five carefully selected\ndownstream proxy tasks: depth completion, stereo matching, monocular\nfeed-forward 3D scene reconstruction, SLAM, and vision-language spatial\nunderstanding. Unlike conventional evaluation protocols, our approach assesses\nDFMs based on their practical utility in real-world applications, bypassing\nproblematic alignment procedures. We benchmark eight state-of-the-art DFMs and\nprovide an in-depth analysis of key findings and observations. We hope our work\nsparks further discussion in the community on best practices for depth model\nevaluation and paves the way for future research and advancements in depth\nestimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86BenchDepth\uff0c\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e0b\u6e38\u4ee3\u7406\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5f15\u5165\u504f\u5dee\uff0c\u96be\u4ee5\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e94\u4e2a\u4e0b\u6e38\u4ee3\u7406\u4efb\u52a1\uff08\u5982\u6df1\u5ea6\u8865\u5168\u3001\u7acb\u4f53\u5339\u914d\u7b49\uff09\uff0c\u7ed5\u8fc7\u4f20\u7edf\u5bf9\u9f50\u6b65\u9aa4\uff0c\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u5b9e\u7528\u6027\u3002", "result": "\u5bf9\u516b\u79cd\u5148\u8fdb\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u952e\u53d1\u73b0\u548c\u5206\u6790\u3002", "conclusion": "BenchDepth\u4e3a\u6df1\u5ea6\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u63a8\u52a8\u6df1\u5ea6\u4f30\u8ba1\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdb\u3002"}}
{"id": "2507.15470", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15470", "abs": "https://arxiv.org/abs/2507.15470", "authors": ["Baran Can G\u00fcl", "Suraksha Nadig", "Stefanos Tziampazis", "Nasser Jazdi", "Michael Weyrich"], "title": "FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning", "comment": "Preprint version. Accepted for publication at IEEE ICECCME 2025", "summary": "In-vehicle emotion recognition underpins adaptive driver-assistance systems\nand, ultimately, occupant safety. However, practical deployment is hindered by\n(i) modality fragility - poor lighting and occlusions degrade vision-based\nmethods; (ii) physiological variability - heart-rate and skin-conductance\npatterns differ across individuals; and (iii) privacy risk - centralized\ntraining requires transmission of sensitive data. To address these challenges,\nwe present FedMultiEmo, a privacy-preserving framework that fuses two\ncomplementary modalities at the decision level: visual features extracted by a\nConvolutional Neural Network from facial images, and physiological cues (heart\nrate, electrodermal activity, and skin temperature) classified by a Random\nForest. FedMultiEmo builds on three key elements: (1) a multimodal federated\nlearning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud\nprototype on Raspberry Pi clients and a Flower server, and (3) a personalized\nFederated Averaging scheme that weights client updates by local data volume.\nEvaluated on FER2013 and a custom physiological dataset, the federated\nConvolutional Neural Network attains 77% accuracy, the Random Forest 74%, and\ntheir fusion 87%, matching a centralized baseline while keeping all raw data\nlocal. The developed system converges in 18 rounds, with an average round time\nof 120 seconds and a per-client memory footprint below 200 MB. These results\nindicate that FedMultiEmo offers a practical approach to real-time,\nprivacy-aware emotion recognition in automotive settings.", "AI": {"tldr": "FedMultiEmo\u662f\u4e00\u4e2a\u9690\u79c1\u4fdd\u62a4\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u878d\u5408\u89c6\u89c9\u548c\u751f\u7406\u6570\u636e\uff0c\u5728\u8f66\u8f7d\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\u7684\u60c5\u7eea\u8bc6\u522b\u3002", "motivation": "\u89e3\u51b3\u8f66\u8f7d\u60c5\u7eea\u8bc6\u522b\u4e2d\u7684\u6a21\u6001\u8106\u5f31\u6027\u3001\u751f\u7406\u6570\u636e\u4e2a\u4f53\u5dee\u5f02\u548c\u9690\u79c1\u98ce\u9669\u95ee\u9898\u3002", "method": "\u7ed3\u5408CNN\u5904\u7406\u9762\u90e8\u56fe\u50cf\u548c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u751f\u7406\u6570\u636e\uff0c\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff08FedMultiEmo\uff09\u8fdb\u884c\u591a\u6a21\u6001\u51b3\u7b56\u878d\u5408\u3002", "result": "\u5728FER2013\u548c\u81ea\u5b9a\u4e49\u751f\u7406\u6570\u636e\u96c6\u4e0a\uff0c\u878d\u5408\u6a21\u578b\u51c6\u786e\u7387\u8fbe87%\uff0c\u4e0e\u96c6\u4e2d\u5f0f\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u3002", "conclusion": "FedMultiEmo\u4e3a\u8f66\u8f7d\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u65f6\u3001\u9690\u79c1\u5b89\u5168\u7684\u60c5\u611f\u8bc6\u522b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15335", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15335", "abs": "https://arxiv.org/abs/2507.15335", "authors": ["Muhammad Aqeel", "Federico Leonardi", "Francesco Setti"], "title": "ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis", "comment": "Accepted to ICIAP 2025", "summary": "Industrial defect detection systems face critical limitations when confined\nto one-class anomaly detection paradigms, which assume uniform outlier\ndistributions and struggle with data scarcity in realworld manufacturing\nenvironments. We present ExDD (Explicit Dual Distribution), a novel framework\nthat transcends these limitations by explicitly modeling dual feature\ndistributions. Our approach leverages parallel memory banks that capture the\ndistinct statistical properties of both normality and anomalous patterns,\naddressing the fundamental flaw of uniform outlier assumptions. To overcome\ndata scarcity, we employ latent diffusion models with domain-specific textual\nconditioning, generating in-distribution synthetic defects that preserve\nindustrial context. Our neighborhood-aware ratio scoring mechanism elegantly\nfuses complementary distance metrics, amplifying signals in regions exhibiting\nboth deviation from normality and similarity to known defect patterns.\nExperimental validation on KSDD2 demonstrates superior performance (94.2%\nI-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.", "AI": {"tldr": "ExDD\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u53cc\u7279\u5f81\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u4e2d\u5355\u7c7b\u5f02\u5e38\u68c0\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u7f3a\u9677\u6570\u636e\uff0c\u5e76\u5728KSDD2\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7cfb\u7edf\u5728\u5355\u7c7b\u5f02\u5e38\u68c0\u6d4b\u8303\u5f0f\u4e0b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u5047\u8bbe\u5f02\u5e38\u5206\u5e03\u5747\u5300\u4e14\u6570\u636e\u7a00\u7f3a\u3002", "method": "ExDD\u6846\u67b6\u901a\u8fc7\u5e76\u884c\u8bb0\u5fc6\u5e93\u5efa\u6a21\u6b63\u5e38\u548c\u5f02\u5e38\u7279\u5f81\u5206\u5e03\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u5408\u6210\u7f3a\u9677\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u90bb\u57df\u611f\u77e5\u8bc4\u5206\u673a\u5236\u878d\u5408\u8ddd\u79bb\u5ea6\u91cf\u3002", "result": "\u5728KSDD2\u6570\u636e\u96c6\u4e0a\u53d6\u5f9794.2% I-AUROC\u548c97.7% P-AUROC\u7684\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "ExDD\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u53cc\u5206\u5e03\u548c\u751f\u6210\u5408\u6210\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2507.15507", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15507", "abs": "https://arxiv.org/abs/2507.15507", "authors": ["Johannes Ackermann", "Takashi Ishida", "Masashi Sugiyama"], "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback", "comment": "Accept at the Conference On Language Modeling (COLM) 2025", "summary": "Reinforcement Learning from Human Feedback (RLHF) allows us to train models,\nsuch as language models (LMs), to follow complex human preferences. In RLHF for\nLMs, we first train an LM using supervised fine-tuning, sample pairs of\nresponses, obtain human feedback, and use the resulting data to train a reward\nmodel (RM). RL methods are then used to train the LM to maximize the reward\ngiven by the RM. As training progresses, the responses generated by the LM no\nlonger resemble the responses seen by the RM during training, leading to the RM\nbecoming inaccurate. The score given by the RM keeps increasing, but the\nlearned behavior no longer matches the human preferences. This issue is known\nas overoptimization. We investigate overoptimization from the point of view of\ndistribution shift and show that the shift results in an inconsistent estimate\nof the RM parameters, leading to an inconsistent estimate of the policy\ngradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which\niteratively off-policy corrects the RM using importance weighting, without\nrequiring new labels or samples. This results in a more accurate RM, which\nempirically leads to an improved final policy. We validate our approach in\nexperiments with summarization and chatbot datasets and show that it performs\nsignificantly better than standard RLHF methods and baselines. Our\nimplementation is available at\nhttps://github.com/JohannesAck/OffPolicyCorrectedRewardModeling", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOCRM\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u52a0\u6743\u4fee\u6b63\u5956\u52b1\u6a21\u578b\uff0c\u89e3\u51b3\u4e86RLHF\u4e2d\u7684\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6700\u7ec8\u7b56\u7565\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76RLHF\u4e2d\u5956\u52b1\u6a21\u578b\u56e0\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4fee\u6b63\u65b9\u6cd5\u4ee5\u66f4\u51c6\u786e\u5730\u5339\u914d\u4eba\u7c7b\u504f\u597d\u3002", "method": "\u63d0\u51faOff-Policy Corrected Reward Modeling (OCRM)\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u52a0\u6743\u8fed\u4ee3\u4fee\u6b63\u5956\u52b1\u6a21\u578b\uff0c\u65e0\u9700\u65b0\u6807\u7b7e\u6216\u6837\u672c\u3002", "result": "\u5728\u6458\u8981\u548c\u804a\u5929\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOCRM\u663e\u8457\u4f18\u4e8e\u6807\u51c6RLHF\u65b9\u6cd5\u548c\u57fa\u7ebf\u3002", "conclusion": "OCRM\u901a\u8fc7\u4fee\u6b63\u5956\u52b1\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7b56\u7565\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.15346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15346", "abs": "https://arxiv.org/abs/2507.15346", "authors": ["Muhammad Aqeel", "Kidus Dagnaw Bellete", "Francesco Setti"], "title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection", "comment": "Accepted to ICIAP 2025", "summary": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection.", "AI": {"tldr": "RoadFusion\u6846\u67b6\u901a\u8fc7\u5408\u6210\u5f02\u5e38\u751f\u6210\u548c\u53cc\u8def\u5f84\u7279\u5f81\u9002\u5e94\uff0c\u89e3\u51b3\u4e86\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u9886\u57df\u504f\u79fb\u548c\u7f3a\u9677\u591a\u6837\u6027\u95ee\u9898\u3002", "motivation": "\u8def\u9762\u7f3a\u9677\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u6709\u9650\u3001\u8bad\u7ec3\u4e0e\u90e8\u7f72\u73af\u5883\u95f4\u7684\u9886\u57df\u504f\u79fb\u4ee5\u53ca\u4e0d\u540c\u9053\u8def\u6761\u4ef6\u4e0b\u7f3a\u9677\u5916\u89c2\u7684\u9ad8\u53d8\u5f02\u6027\u7b49\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u901a\u8fc7\u6587\u672c\u63d0\u793a\u548c\u7a7a\u95f4\u63a9\u7801\u5408\u6210\u591a\u6837\u4e14\u771f\u5b9e\u7684\u7f3a\u9677\uff0c\u540c\u65f6\u91c7\u7528\u53cc\u8def\u5f84\u7279\u5f81\u9002\u914d\u5668\u5206\u522b\u5904\u7406\u6b63\u5e38\u548c\u5f02\u5e38\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5224\u522b\u5668\u5b66\u4e60\u7ec6\u7c92\u5ea6\u7f3a\u9677\u6a21\u5f0f\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cRoadFusion\u5728\u5206\u7c7b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u9879\u6307\u6807\u8fbe\u5230\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "RoadFusion\u4e3a\u5b9e\u9645\u9053\u8def\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15523", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15523", "abs": "https://arxiv.org/abs/2507.15523", "authors": ["Weichuang Shao", "Iman Yi Liao", "Tomas Henrique Bode Maul", "Tissa Chandesa"], "title": "An Investigation of Test-time Adaptation for Audio Classification under Background Noise", "comment": null, "summary": "Domain shift is a prominent problem in Deep Learning, causing a model\npre-trained on a source dataset to suffer significant performance degradation\non test datasets. This research aims to address the issue of audio\nclassification under domain shift caused by background noise using Test-Time\nAdaptation (TTA), a technique that adapts a pre-trained model during testing\nusing only unlabelled test data before making predictions. We adopt two common\nTTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and\ninvestigate their respective performance on two popular audio classification\ndatasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types\nof background noise and noise severity levels. The experimental results reveal\nthat our proposed modified version of CoNMix produced the highest\nclassification accuracy under domain shift (5.31% error rate under 10 dB\nexercise bike background noise and 12.75% error rate under 3 dB running tap\nbackground noise for AM) compared to TTT and TENT. The literature search\nprovided no evidence of similar works, thereby motivating the work reported\nhere as the first study to leverage TTA techniques for audio classification\nunder domain shift.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u6280\u672f\u89e3\u51b3\u97f3\u9891\u5206\u7c7b\u4e2d\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u6539\u8fdb\u7684CoNMix\u65b9\u6cd5\u5728\u566a\u58f0\u73af\u5883\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u57df\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u97f3\u9891\u5206\u7c7b\u4e2d\u80cc\u666f\u566a\u58f0\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528TTT\u3001TENT\u548cCoNMix\u4e09\u79cdTTA\u65b9\u6cd5\uff0c\u5728AudioMNIST\u548cSpeechCommands V1\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u548c\u5f3a\u5ea6\u3002", "result": "\u6539\u8fdb\u7684CoNMix\u5728\u566a\u58f0\u73af\u5883\u4e0b\u5206\u7c7b\u51c6\u786e\u7387\u6700\u9ad8\uff08\u598210 dB\u8fd0\u52a8\u81ea\u884c\u8f66\u566a\u58f0\u4e0b\u9519\u8bef\u73875.31%\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5c06TTA\u6280\u672f\u5e94\u7528\u4e8e\u97f3\u9891\u5206\u7c7b\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u6539\u8fdb\u7684CoNMix\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2507.15365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15365", "abs": "https://arxiv.org/abs/2507.15365", "authors": ["Fatemeh Saleh", "Sadegh Aliakbarian", "Charlie Hewitt", "Lohit Petikam", "Xiao-Xian", "Antonio Criminisi", "Thomas J. Cashman", "Tadas Baltru\u0161aitis"], "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data", "comment": "Accepted at ICCV 2025", "summary": "The state of the art in human-centric computer vision achieves high accuracy\nand robustness across a diverse range of tasks. The most effective models in\nthis domain have billions of parameters, thus requiring extremely large\ndatasets, expensive training regimes, and compute-intensive inference. In this\npaper, we demonstrate that it is possible to train models on much smaller but\nhigh-fidelity synthetic datasets, with no loss in accuracy and higher\nefficiency. Using synthetic training data provides us with excellent levels of\ndetail and perfect labels, while providing strong guarantees for data\nprovenance, usage rights, and user consent. Procedural data synthesis also\nprovides us with explicit control on data diversity, that we can use to address\nunfairness in the models we train. Extensive quantitative assessment on real\ninput images demonstrates accuracy of our models on three dense prediction\ntasks: depth estimation, surface normal estimation, and soft foreground\nsegmentation. Our models require only a fraction of the cost of training and\ninference when compared with foundational models of similar accuracy. Our\nhuman-centric synthetic dataset and trained models are available at\nhttps://aka.ms/DAViD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u9ad8\u4fdd\u771f\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u727a\u7272\u51c6\u786e\u6027\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u5f53\u524d\u4eba\u7c7b\u4e2d\u5fc3\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u9700\u8981\u5927\u91cf\u53c2\u6570\u3001\u6570\u636e\u96c6\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u5408\u6210\u6570\u636e\u96c6\u80fd\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6807\u7b7e\u548c\u6570\u636e\u591a\u6837\u6027\u63a7\u5236\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u6570\u636e\u5408\u6210\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u3001\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u548c\u524d\u666f\u5206\u5272\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u4e14\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u96c6\u662f\u9ad8\u6548\u4e14\u516c\u5e73\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2507.15545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15545", "abs": "https://arxiv.org/abs/2507.15545", "authors": ["Yujia Shi", "Emil Njor", "Pablo Mart\u00ednez-Nuevo", "Sven Ewan Shepstone", "Xenofon Fafoutis"], "title": "Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications", "comment": null, "summary": "The success of Machine Learning is increasingly tempered by its significant\nresource footprint, driving interest in efficient paradigms like TinyML.\nHowever, the inherent complexity of designing TinyML systems hampers their\nbroad adoption. To reduce this complexity, we introduce \"Data Aware\nDifferentiable Neural Architecture Search\". Unlike conventional Differentiable\nNeural Architecture Search, our approach expands the search space to include\ndata configuration parameters alongside architectural choices. This enables\nData Aware Differentiable Neural Architecture Search to co-optimize model\narchitecture and input data characteristics, effectively balancing resource\nusage and system performance for TinyML applications. Initial results on\nkeyword spotting demonstrate that this novel approach to TinyML system design\ncan generate lean but highly accurate systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6570\u636e\u611f\u77e5\u53ef\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u7b80\u5316TinyML\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u540c\u65f6\u4f18\u5316\u6a21\u578b\u67b6\u6784\u548c\u8f93\u5165\u6570\u636e\u7279\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7684\u9ad8\u8d44\u6e90\u6d88\u8017\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\uff0c\u5c24\u5176\u662fTinyML\u7cfb\u7edf\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u963b\u788d\u4e86\u5176\u666e\u53ca\u3002", "method": "\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u53ef\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff0c\u5c06\u6570\u636e\u914d\u7f6e\u53c2\u6570\u7eb3\u5165\u641c\u7d22\u7a7a\u95f4\uff0c\u5b9e\u73b0\u6a21\u578b\u67b6\u6784\u4e0e\u8f93\u5165\u6570\u636e\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u5173\u952e\u8bcd\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u8d44\u6e90\u5360\u7528\u5c11\u4e14\u9ad8\u7cbe\u5ea6\u7684\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aTinyML\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7b80\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15401", "abs": "https://arxiv.org/abs/2507.15401", "authors": ["Huiyu Zhai", "Xingxing Yang", "Yalan Ye", "Chenyang Li", "Bin Fan", "Changze Li"], "title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond", "comment": null, "summary": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.", "AI": {"tldr": "ORSANet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\u548c\u591a\u5c3a\u5ea6\u4ea4\u4e92\u6a21\u5757\u89e3\u51b3\u906e\u6321\u548c\u6570\u636e\u96c6\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u6784\u5efa\u7684Occlu-FER\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709FER\u6a21\u578b\u5728\u9762\u90e8\u90e8\u5206\u906e\u6321\u65f6\u96be\u4ee5\u63d0\u53d6\u6709\u6548\u7279\u5f81\uff0c\u5bfc\u81f4\u5206\u7c7b\u4e0d\u51c6\u786e\u3002", "method": "\u5f15\u5165\u591a\u6a21\u6001\u8bed\u4e49\u5f15\u5bfc\uff08\u8bed\u4e49\u5206\u5272\u56fe\u548c\u9762\u90e8\u5173\u952e\u70b9\uff09\u3001\u591a\u5c3a\u5ea6\u4ea4\u4e92\u6a21\u5757\uff08MCM\uff09\u548c\u52a8\u6001\u5bf9\u6297\u6392\u65a5\u589e\u5f3a\u635f\u5931\uff08DARELoss\uff09\u3002", "result": "ORSANet\u5728\u516c\u5f00\u57fa\u51c6\u548c\u65b0\u6570\u636e\u96c6Occlu-FER\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "ORSANet\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u52a8\u6001\u635f\u5931\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u906e\u6321\u6761\u4ef6\u4e0b\u7684FER\u6027\u80fd\u3002"}}
{"id": "2507.15548", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15548", "abs": "https://arxiv.org/abs/2507.15548", "authors": ["D. Abler", "O. Pusterla", "A. Joye-K\u00fchnis", "N. Andratschke", "M. Bach", "A. Bink", "S. M. Christ", "P. Hagmann", "B. Pouymayou", "E. Pravat\u00e0", "P. Radojewski", "M. Reyes", "L. Ruinelli", "R. Schaer", "B. Stieltjes", "G. Treglia", "W. Valenzuela", "R. Wiest", "S. Zoergiebel", "M. Guckenberger", "S. Tanadini-Lang", "A. Depeursinge"], "title": "The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information", "comment": null, "summary": "Background: Radiomics shows promise in characterizing glioblastoma, but its\nadded value over clinical and molecular predictors has yet to be proven. This\nstudy assessed the added value of conventional radiomics (CR) and deep learning\n(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on\na large multi-center dataset.\n  Methods: After patient selection, our curated dataset gathers 1152\nglioblastoma (WHO 2016) patients from five Swiss centers and one public source.\nIt included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI\ndata (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were\ndeveloped using standard methods and evaluated on internal and external\ncohorts. Sub-analyses assessed models with different feature sets\n(imaging-only, clinical/molecular-only, combined-features) and patient subsets\n(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).\n  Results: The best performance was observed in the full cohort (S-1). In\nexternal validation, the combined-feature CR model achieved an AUC of 0.75,\nslightly, but significantly outperforming clinical-only (0.74) and imaging-only\n(0.68) models. DL models showed similar trends, though without statistical\nsignificance. In S-2 and S-3, combined models did not outperform clinical-only\nmodels. Exploratory analysis of CR models for overall survival prediction\nsuggested greater relevance of imaging data: across all subsets,\ncombined-feature models significantly outperformed clinical-only models, though\nwith a modest advantage of 2-4 C-index points.\n  Conclusions: While confirming the predictive value of anatomical MRI\nsequences for glioblastoma prognosis, this multi-center study found standard CR\nand DL radiomics approaches offer minimal added value over demographic\npredictors such as age and gender.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4f20\u7edf\u653e\u5c04\u7ec4\u5b66\uff08CR\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08DL\uff09MRI\u653e\u5c04\u7ec4\u5b66\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u9884\u540e\u4e2d\u7684\u9644\u52a0\u4ef7\u503c\uff0c\u53d1\u73b0\u5176\u76f8\u5bf9\u4e8e\u4e34\u5e8a\u548c\u5206\u5b50\u9884\u6d4b\u56e0\u5b50\u7684\u4f18\u52bf\u6709\u9650\u3002", "motivation": "\u5c3d\u7ba1\u653e\u5c04\u7ec4\u5b66\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u8868\u5f81\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u76f8\u5bf9\u4e8e\u4e34\u5e8a\u548c\u5206\u5b50\u9884\u6d4b\u56e0\u5b50\u7684\u9644\u52a0\u4ef7\u503c\u5c1a\u672a\u5f97\u5230\u8bc1\u5b9e\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u4e86\u6765\u81ea\u591a\u4e2a\u4e2d\u5fc3\u76841152\u4f8b\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u60a3\u8005\u7684\u4e34\u5e8a\u3001\u5206\u5b50\u548cMRI\u6570\u636e\uff0c\u5f00\u53d1\u4e86CR\u548cDL\u6a21\u578b\uff0c\u5e76\u5728\u5185\u90e8\u548c\u5916\u90e8\u961f\u5217\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\uff0c\u7ed3\u5408\u7279\u5f81\u7684CR\u6a21\u578bAUC\u4e3a0.75\uff0c\u7565\u4f18\u4e8e\u4ec5\u4e34\u5e8a\uff080.74\uff09\u548c\u4ec5\u5f71\u50cf\uff080.68\uff09\u6a21\u578b\u3002DL\u6a21\u578b\u8d8b\u52bf\u76f8\u4f3c\u4f46\u65e0\u7edf\u8ba1\u5b66\u610f\u4e49\u3002", "conclusion": "\u6807\u51c6CR\u548cDL\u653e\u5c04\u7ec4\u5b66\u65b9\u6cd5\u5728\u80f6\u8d28\u6bcd\u7ec6\u80de\u7624\u9884\u540e\u4e2d\u76f8\u5bf9\u4e8e\u5e74\u9f84\u548c\u6027\u522b\u7b49\u4eba\u53e3\u7edf\u8ba1\u5b66\u9884\u6d4b\u56e0\u5b50\u7684\u9644\u52a0\u4ef7\u503c\u6709\u9650\u3002"}}
{"id": "2507.15418", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15418", "abs": "https://arxiv.org/abs/2507.15418", "authors": ["Ka Young Kim", "Hyeon Bae Kim", "Seong Tae Kim"], "title": "SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition", "comment": "Accepted to MICCAI 2025", "summary": "Surgical phase recognition plays a crucial role in surgical workflow\nanalysis, enabling various applications such as surgical monitoring, skill\nassessment, and workflow optimization. Despite significant advancements in deep\nlearning-based surgical phase recognition, these models remain inherently\nopaque, making it difficult to understand how they make decisions. This lack of\ninterpretability hinders trust and makes it challenging to debug the model. To\naddress this challenge, we propose SurgX, a novel concept-based explanation\nframework that enhances the interpretability of surgical phase recognition\nmodels by associating neurons with relevant concepts. In this paper, we\nintroduce the process of selecting representative example sequences for\nneurons, constructing a concept set tailored to the surgical video dataset,\nassociating neurons with concepts and identifying neurons crucial for\npredictions. Through extensive experiments on two surgical phase recognition\nmodels, we validate our method and analyze the explanation for prediction. This\nhighlights the potential of our method in explaining surgical phase\nrecognition. The code is available at https://github.com/ailab-kyunghee/SurgX", "AI": {"tldr": "SurgX\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6982\u5ff5\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u795e\u7ecf\u5143\u4e0e\u76f8\u5173\u6982\u5ff5\u5173\u8054\uff0c\u63d0\u5347\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u4e0d\u900f\u660e\u6027\u5bfc\u81f4\u96be\u4ee5\u7406\u89e3\u6a21\u578b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u963b\u788d\u4e86\u4fe1\u4efb\u548c\u8c03\u8bd5\u3002", "method": "\u63d0\u51faSurgX\u6846\u67b6\uff0c\u5305\u62ec\u9009\u62e9\u795e\u7ecf\u5143\u4ee3\u8868\u5e8f\u5217\u3001\u6784\u5efa\u624b\u672f\u89c6\u9891\u6570\u636e\u96c6\u7684\u6982\u5ff5\u96c6\u3001\u5173\u8054\u795e\u7ecf\u5143\u4e0e\u6982\u5ff5\uff0c\u5e76\u8bc6\u522b\u5173\u952e\u795e\u7ecf\u5143\u3002", "result": "\u5728\u4e24\u4e2a\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u9884\u6d4b\u89e3\u91ca\u3002", "conclusion": "SurgX\u5c55\u793a\u4e86\u5728\u89e3\u91ca\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.15550", "categories": ["cs.LG", "cs.AI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2507.15550", "abs": "https://arxiv.org/abs/2507.15550", "authors": ["Yimeng Chen", "Piotr Pi\u0229kos", "Mateusz Ostaszewski", "Firas Laakom", "J\u00fcrgen Schmidhuber"], "title": "PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors", "comment": "31 Pages", "summary": "Evaluating the scientific discovery capabilities of large language model\nbased agents, particularly how they cope with varying environmental complexity\nand utilize prior knowledge, requires specialized benchmarks currently lacking\nin the landscape. To address this gap, we introduce PhysGym, a novel benchmark\nsuite and simulation platform for rigorously assessing LLM-based scientific\nreasoning in interactive physics environments. PhysGym's primary contribution\nlies in its sophisticated control over the level of prior knowledge provided to\nthe agent. This allows researchers to dissect agent performance along axes\nincluding the complexity of the problem and the prior knowledge levels. The\nbenchmark comprises a suite of interactive simulations, where agents must\nactively probe environments, gather data sequentially under constraints and\nformulate hypotheses about underlying physical laws. PhysGym provides\nstandardized evaluation protocols and metrics for assessing hypothesis accuracy\nand model fidelity. We demonstrate the benchmark's utility by presenting\nresults from baseline LLMs, showcasing its ability to differentiate\ncapabilities based on varying priors and task complexity.", "AI": {"tldr": "PhysGym\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u63a7\u5236\u5148\u9a8c\u77e5\u8bc6\u6c34\u5e73\u548c\u95ee\u9898\u590d\u6742\u5ea6\u6765\u533a\u5206\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30LLM\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5e94\u5bf9\u73af\u5883\u590d\u6742\u6027\u548c\u5229\u7528\u5148\u9a8c\u80fd\u529b\u7684\u4e13\u7528\u57fa\u51c6\uff0cPhysGym\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "PhysGym\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u7269\u7406\u73af\u5883\u6a21\u62df\uff0c\u8981\u6c42\u4ee3\u7406\u4e3b\u52a8\u63a2\u7d22\u3001\u6536\u96c6\u6570\u636e\u5e76\u5f62\u6210\u5047\u8bbe\uff0c\u540c\u65f6\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\u548c\u6307\u6807\u3002", "result": "\u901a\u8fc7\u57fa\u7ebfLLM\u6d4b\u8bd5\uff0cPhysGym\u5c55\u793a\u4e86\u5176\u533a\u5206\u4e0d\u540c\u5148\u9a8c\u77e5\u8bc6\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u4e0b\u6a21\u578b\u80fd\u529b\u7684\u80fd\u529b\u3002", "conclusion": "PhysGym\u4e3a\u7814\u7a76LLM\u7684\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2507.15428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15428", "abs": "https://arxiv.org/abs/2507.15428", "authors": ["Jiaao Li", "Kaiyuan Li", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent", "comment": null, "summary": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.", "AI": {"tldr": "EgoPrune\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4ee4\u724c\u4fee\u526a\u65b9\u6cd5\uff0c\u4e13\u4e3a\u81ea\u6211\u8fd0\u52a8\u89c6\u9891\u63a8\u7406\u8bbe\u8ba1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u81ea\u6211\u8fd0\u52a8\u89c6\u9891\u63a8\u7406\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5176\u65f6\u7a7a\u8fde\u7eed\u6027\u3002", "method": "EgoPrune\u5305\u62ec\u5173\u952e\u5e27\u9009\u62e9\u5668\u3001\u89c6\u89d2\u611f\u77e5\u5197\u4f59\u8fc7\u6ee4\u5668\u548c\u57fa\u4e8eMMR\u7684\u4ee4\u724c\u9009\u62e9\u5668\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u51cf\u5c11FLOPs\u3001\u5185\u5b58\u4f7f\u7528\u548c\u5ef6\u8fdf\u3002", "conclusion": "EgoPrune\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.15566", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15566", "abs": "https://arxiv.org/abs/2507.15566", "authors": ["Pieter Smet", "Martina Doneda", "Ettore Lanzarone", "Giuliana Carello"], "title": "Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy", "comment": null, "summary": "The availability of downstream resources plays a critical role in planning\nthe admission of patients undergoing elective surgery, with inpatient beds\nbeing one of the most crucial resources. When planning patient admissions,\npredictions on their length-of-stay (LOS) made by machine learning (ML) models\nare used to ensure bed availability. However, the actual LOS for each patient\nmay differ considerably from the predicted value, potentially making the\nschedule infeasible. To address such infeasibilities, rescheduling strategies\nthat take advantage of operational flexibility can be implemented. For example,\nadjustments may include postponing admission dates, relocating patients to\ndifferent wards, or even transferring patients who are already admitted. The\ncommon assumption is that more accurate LOS predictions reduce the impact of\nrescheduling. However, training ML models that can make such accurate\npredictions can be costly. Building on previous work that proposed simulated\n\\ac{ml} for evaluating data-driven approaches, this paper explores the\nrelationship between LOS prediction accuracy and rescheduling flexibility\nacross various corrective policies. Specifically, we examine the most effective\npatient rescheduling strategies under LOS prediction errors to prevent bed\noverflows while optimizing resource utilization.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f4f\u9662\u65f6\u95f4\uff08LOS\uff09\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u5e8a\u4f4d\u8c03\u5ea6\u7075\u6d3b\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7814\u7a76\u4e86\u5728\u9884\u6d4b\u8bef\u5dee\u4e0b\u6700\u6709\u6548\u7684\u60a3\u8005\u8c03\u5ea6\u7b56\u7565\u4ee5\u907f\u514d\u5e8a\u4f4d\u6ea2\u51fa\u5e76\u4f18\u5316\u8d44\u6e90\u5229\u7528\u3002", "motivation": "\u4e0b\u6e38\u8d44\u6e90\uff08\u5982\u5e8a\u4f4d\uff09\u7684\u53ef\u7528\u6027\u5bf9\u8ba1\u5212\u9009\u62e9\u6027\u624b\u672f\u60a3\u8005\u5165\u9662\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9645LOS\u4e0e\u9884\u6d4b\u503c\u53ef\u80fd\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff0c\u5bfc\u81f4\u8ba1\u5212\u4e0d\u53ef\u884c\u3002", "method": "\u5229\u7528\u6a21\u62df\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8bc4\u4f30\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u7ea0\u6b63\u7b56\u7565\u4e0bLOS\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u8c03\u5ea6\u7075\u6d3b\u6027\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u66f4\u51c6\u786e\u7684LOS\u9884\u6d4b\u53ef\u4ee5\u51cf\u5c11\u8c03\u5ea6\u8c03\u6574\u7684\u5f71\u54cd\uff0c\u4f46\u8bad\u7ec3\u9ad8\u7cbe\u5ea6\u6a21\u578b\u6210\u672c\u8f83\u9ad8\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u5e8a\u4f4d\u8d44\u6e90\u5229\u7528\u548c\u51cf\u5c11\u8c03\u5ea6\u8c03\u6574\u63d0\u4f9b\u4e86\u7b56\u7565\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u8c03\u5ea6\u7075\u6d3b\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2507.15480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15480", "abs": "https://arxiv.org/abs/2507.15480", "authors": ["Liang Chen", "Ghazi Shazan Ahmad", "Tianjun Yao", "Lingqiao Liu", "Zhiqiang Shen"], "title": "One Last Attention for Your Vision-Language Model", "comment": "Accepted by ICCV 2025", "summary": "Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.", "AI": {"tldr": "RAda\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6821\u51c6\u878d\u5408\u8868\u793a\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4e86\u878d\u5408\u8868\u793a\u5728\u51b3\u7b56\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0cRAda\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6ce8\u610f\u529b\u5c42\u751f\u6210\u5b66\u4e60\u63a9\u7801\uff0c\u52a8\u6001\u8c03\u6574\u878d\u5408\u8868\u793a\u4e2d\u7684\u5143\u7d20\u8d21\u732e\u3002", "result": "\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\uff0cRAda\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u63a5\u8fd1\u5f53\u524d\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "RAda\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u5fae\u8c03\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2507.15574", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15574", "abs": "https://arxiv.org/abs/2507.15574", "authors": ["Gregory F. Stock", "Juan A. Fraire", "Holger Hermanns", "J\u0119drzej Mosi\u0119\u017cny", "Yusra Al-Khazraji", "Julio Ram\u00edrez Molina", "Evridiki V. Ntagiou"], "title": "On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project", "comment": "18th International Conference on Space Operations (SpaceOps 2025),\n  Montr\\'eal, Canada, 26-30 May 2025,\n  https://star.spaceops.org/2025/user_manudownload.php?doc=140__9bg48dkf.pdf", "summary": "The rapid expansion of satellite constellations in near-Earth orbits presents\nsignificant challenges in satellite network management, requiring innovative\napproaches for efficient, scalable, and resilient operations. This paper\nexplores the role of Artificial Intelligence (AI) in optimizing the operation\nof satellite mega-constellations, drawing from the ConstellAI project funded by\nthe European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland\nUniversity, and Thales Alenia Space collaborates to develop AI-driven\nalgorithms and demonstrates their effectiveness over traditional methods for\ntwo crucial operational challenges: data routing and resource allocation. In\nthe routing use case, Reinforcement Learning (RL) is used to improve the\nend-to-end latency by learning from historical queuing latency, outperforming\nclassical shortest path algorithms. For resource allocation, RL optimizes the\nscheduling of tasks across constellations, focussing on efficiently using\nlimited resources such as battery and memory. Both use cases were tested for\nmultiple satellite constellation configurations and operational scenarios,\nresembling the real-life spacecraft operations of communications and Earth\nobservation satellites. This research demonstrates that RL not only competes\nwith classical approaches but also offers enhanced flexibility, scalability,\nand generalizability in decision-making processes, which is crucial for the\nautonomous and intelligent management of satellite fleets. The findings of this\nactivity suggest that AI can fundamentally alter the landscape of satellite\nconstellation management by providing more adaptive, robust, and cost-effective\nsolutions.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u4f18\u5316\u536b\u661f\u5de8\u578b\u661f\u5ea7\u7ba1\u7406\u4e2d\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5728\u6570\u636e\u8def\u7531\u548c\u8d44\u6e90\u5206\u914d\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u536b\u661f\u661f\u5ea7\u7684\u5feb\u901f\u6269\u5f20\u5bf9\u7f51\u7edc\u7ba1\u7406\u63d0\u51fa\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u5f39\u6027\u7684\u9700\u6c42\uff0cAI\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\u6570\u636e\u8def\u7531\u548c\u8d44\u6e90\u5206\u914d\uff0c\u6d4b\u8bd5\u4e86\u591a\u79cd\u661f\u5ea7\u914d\u7f6e\u548c\u64cd\u4f5c\u573a\u666f\u3002", "result": "RL\u5728\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u4e0a\u4f18\u4e8e\u4f20\u7edf\u7b97\u6cd5\uff0c\u5c55\u73b0\u4e86\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "AI\u80fd\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u536b\u661f\u661f\u5ea7\u7ba1\u7406\uff0c\u63d0\u4f9b\u66f4\u81ea\u9002\u5e94\u3001\u7a33\u5065\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15492", "abs": "https://arxiv.org/abs/2507.15492", "authors": ["Rakesh John Amala Arokia Nathan", "Matthias Gessner", "Nurullah \u00d6zkan", "Marius Bock", "Mohamed Youssef", "Maximilian Mews", "Bj\u00f6rn Piltz", "Ralf Berger", "Oliver Bimber"], "title": "An aerial color image anomaly dataset for search missions in complex forested terrain", "comment": "17 pages", "summary": "After a family murder in rural Germany, authorities failed to locate the\nsuspect in a vast forest despite a massive search. To aid the search, a\nresearch aircraft captured high-resolution aerial imagery. Due to dense\nvegetation obscuring small clues, automated analysis was ineffective, prompting\na crowd-search initiative. This effort produced a unique dataset of labeled,\nhard-to-detect anomalies under occluded, real-world conditions. It can serve as\na benchmark for improving anomaly detection approaches in complex forest\nenvironments, supporting manhunts and rescue operations. Initial benchmark\ntests showed existing methods performed poorly, highlighting the need for\ncontext-aware approaches. The dataset is openly accessible for offline\nprocessing. An additional interactive web interface supports online viewing and\ndynamic growth by allowing users to annotate and submit new findings.", "AI": {"tldr": "\u8bba\u6587\u6458\u8981\u63cf\u8ff0\u4e86\u4e00\u8d77\u5fb7\u56fd\u519c\u6751\u8c0b\u6740\u6848\u540e\uff0c\u5229\u7528\u822a\u7a7a\u5f71\u50cf\u8fdb\u884c\u5acc\u7591\u4eba\u641c\u7d22\uff0c\u4f46\u56e0\u690d\u88ab\u906e\u6321\u5bfc\u81f4\u81ea\u52a8\u5316\u5206\u6790\u5931\u6548\uff0c\u8f6c\u800c\u91c7\u7528\u4f17\u5305\u6807\u6ce8\uff0c\u751f\u6210\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u906e\u6321\u73af\u5883\u4e0b\u5f02\u5e38\u68c0\u6d4b\u6570\u636e\u96c6\u3002", "motivation": "\u7531\u4e8e\u5bc6\u96c6\u690d\u88ab\u906e\u6321\u5c0f\u7ebf\u7d22\uff0c\u81ea\u52a8\u5316\u5206\u6790\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u652f\u6301\u590d\u6742\u68ee\u6797\u73af\u5883\u4e2d\u7684\u641c\u7d22\u548c\u6551\u63f4\u884c\u52a8\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u98de\u673a\u83b7\u53d6\u9ad8\u5206\u8fa8\u7387\u822a\u7a7a\u5f71\u50cf\uff0c\u5e76\u542f\u52a8\u4f17\u5305\u6807\u6ce8\uff0c\u751f\u6210\u6807\u6ce8\u6570\u636e\u96c6\u3002\u540c\u65f6\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u7f51\u9875\u754c\u9762\u652f\u6301\u5728\u7ebf\u6807\u6ce8\u548c\u52a8\u6001\u66f4\u65b0\u3002", "result": "\u521d\u6b65\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u51f8\u663e\u4e86\u9700\u8981\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b9\u6cd5\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\uff0c\u652f\u6301\u79bb\u7ebf\u5904\u7406\u548c\u5728\u7ebf\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u590d\u6742\u68ee\u6797\u73af\u5883\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u641c\u7d22\u548c\u6551\u63f4\u6280\u672f\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u6cd5\u3002"}}
{"id": "2507.15584", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15584", "abs": "https://arxiv.org/abs/2507.15584", "authors": ["Philipp R\u00f6chner", "Simon Kl\u00fcttermann", "Franz Rothlauf", "Daniel Schl\u00f6r"], "title": "We Need to Rethink Benchmarking in Anomaly Detection", "comment": null, "summary": "Despite the continuous proposal of new anomaly detection algorithms and\nextensive benchmarking efforts, progress seems to stagnate, with only minor\nperformance differences between established baselines and new algorithms. In\nthis position paper, we argue that this stagnation is due to limitations in how\nwe evaluate anomaly detection algorithms. Current benchmarking does not, for\nexample, sufficiently reflect the diversity of anomalies in applications\nranging from predictive maintenance to scientific discovery. Consequently, we\nneed to rethink benchmarking in anomaly detection. In our opinion, anomaly\ndetection should be studied using scenarios that capture the relevant\ncharacteristics of different applications. We identify three key areas for\nimprovement: First, we need to identify anomaly detection scenarios based on a\ncommon taxonomy. Second, anomaly detection pipelines should be analyzed\nend-to-end and by component. Third, evaluating anomaly detection algorithms\nshould be meaningful regarding the scenario's objectives.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u8fdb\u5c55\u505c\u6ede\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8bc4\u4f30\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u53cd\u6620\u5e94\u7528\u573a\u666f\u7684\u591a\u6837\u6027\uff0c\u9650\u5236\u4e86\u7b97\u6cd5\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u91cd\u65b0\u601d\u8003\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5305\u62ec\u57fa\u4e8e\u5171\u540c\u5206\u7c7b\u7684\u573a\u666f\u8bc6\u522b\u3001\u7aef\u5230\u7aef\u5206\u6790\u4ee5\u53ca\u6839\u636e\u573a\u666f\u76ee\u6807\u8fdb\u884c\u6709\u610f\u4e49\u8bc4\u4f30\u3002", "result": "\u8bc6\u522b\u4e86\u4e09\u4e2a\u5173\u952e\u6539\u8fdb\u9886\u57df\uff1a\u573a\u666f\u5206\u7c7b\u3001\u7aef\u5230\u7aef\u5206\u6790\u548c\u76ee\u6807\u5bfc\u5411\u8bc4\u4f30\u3002", "conclusion": "\u9700\u8981\u901a\u8fc7\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u6765\u63a8\u52a8\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.15587", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15587", "abs": "https://arxiv.org/abs/2507.15587", "authors": ["Yinsong Chen", "Kaifeng Wang", "Xiaoqiang Meng", "Xueyuan Li", "Zirui Li", "Xin Gao"], "title": "Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario", "comment": null, "summary": "Current research on decision-making in safety-critical scenarios often relies\non inefficient data-driven scenario generation or specific modeling approaches,\nwhich fail to capture corner cases in real-world contexts. To address this\nissue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,\nwhere background vehicles with interference capabilities are treated as\nred-team agents. Through active interference and exploration, red-team vehicles\ncan uncover corner cases outside the data distribution. The framework uses a\nConstraint Graph Representation Markov Decision Process, ensuring that red-team\nvehicles comply with safety rules while continuously disrupting the autonomous\nvehicles (AVs). A policy threat zone model is constructed to quantify the\nthreat posed by red-team vehicles to AVs, inducing more extreme actions to\nincrease the danger level of the scenario. Experimental results show that the\nproposed framework significantly impacts AVs decision-making safety and\ngenerates various corner cases. This method also offers a novel direction for\nresearch in safety-critical scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ea2\u961f\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5e72\u6270\u548c\u63a2\u7d22\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u6781\u7aef\u6848\u4f8b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u96be\u4ee5\u6355\u6349\u6781\u7aef\u6848\u4f8b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7ea2\u961f\u667a\u80fd\u4f53\u5e72\u6270\u81ea\u4e3b\u8f66\u8f86\uff0c\u7ed3\u5408\u7ea6\u675f\u56fe\u8868\u793a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u548c\u5b89\u5168\u89c4\u5219\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u5f71\u54cd\u81ea\u4e3b\u8f66\u8f86\u51b3\u7b56\u5b89\u5168\uff0c\u5e76\u751f\u6210\u591a\u79cd\u6781\u7aef\u6848\u4f8b\u3002", "conclusion": "\u4e3a\u5b89\u5168\u5173\u952e\u573a\u666f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.15504", "categories": ["cs.CV", "68T45", "I.2.10; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.15504", "abs": "https://arxiv.org/abs/2507.15504", "authors": ["Bingqing Zhang", "Zhuo Cao", "Heming Du", "Yang Li", "Xue Li", "Jiajun Liu", "Sen Wang"], "title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization", "comment": "Accepted by ICCV 2025", "summary": "Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.", "AI": {"tldr": "UMIVR\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u6700\u5c0f\u5316\u7684\u4ea4\u4e92\u5f0f\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u6587\u672c\u6a21\u7cca\u6027\u3001\u6620\u5c04\u4e0d\u786e\u5b9a\u6027\u548c\u5e27\u4e0d\u786e\u5b9a\u6027\uff0c\u751f\u6210\u9488\u5bf9\u6027\u95ee\u9898\u4ee5\u4f18\u5316\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u4ea4\u4e92\u5f0f\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u4e0d\u786e\u5b9a\u6027\u7684\u663e\u5f0f\u91cf\u5316\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "UMIVR\u4f7f\u7528\u8bed\u4e49\u71b5\uff08TAS\uff09\u3001Jensen-Shannon\u6563\u5ea6\uff08MUS\uff09\u548c\u65f6\u5e8f\u8d28\u91cf\u91c7\u6837\u5668\uff08TQFS\uff09\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u9488\u5bf9\u6027\u95ee\u9898\u8fed\u4ee3\u4f18\u5316\u67e5\u8be2\u3002", "result": "\u5728MSR-VTT-1k\u6570\u636e\u96c6\u4e0a\uff0cUMIVR\u572810\u8f6e\u4ea4\u4e92\u540eRecall@1\u8fbe\u523069.2%\u3002", "conclusion": "UMIVR\u4e3a\u4ea4\u4e92\u5f0f\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u63d0\u4f9b\u4e86\u4e0d\u786e\u5b9a\u6027\u6700\u5c0f\u5316\u7684\u57fa\u7840\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u679c\u3002"}}
{"id": "2507.15601", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15601", "abs": "https://arxiv.org/abs/2507.15601", "authors": ["Huiling Yang", "Zhanwei Wang", "Kaibin Huang"], "title": "Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity", "comment": null, "summary": "Federated learning (FL) has emerged as a popular approach for collaborative\nmachine learning in sixth-generation (6G) networks, primarily due to its\nprivacy-preserving capabilities. The deployment of FL algorithms is expected to\nempower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous\ndriving, augmented reality, and healthcare. The mission-critical and\ntime-sensitive nature of these applications necessitates the design of\nlow-latency FL frameworks that guarantee high learning performance. In\npractice, achieving low-latency FL faces two challenges: the overhead of\ncomputing and transmitting high-dimensional model updates, and the\nheterogeneity in communication-and-computation (C$^2$) capabilities across\ndevices. To address these challenges, we propose a novel C$^2$-aware framework\nfor optimal batch-size control that minimizes end-to-end (E2E) learning latency\nwhile ensuring convergence. The framework is designed to balance a fundamental\nC$^2$ tradeoff as revealed through convergence analysis. Specifically,\nincreasing batch sizes improves the accuracy of gradient estimation in FL and\nthus reduces the number of communication rounds required for convergence, but\nresults in higher per-round latency, and vice versa. The associated problem of\nlatency minimization is intractable; however, we solve it by designing an\naccurate and tractable surrogate for convergence speed, with parameters fitted\nto real data. This approach yields two batch-size control strategies tailored\nto scenarios with slow and fast fading, while also accommodating device\nheterogeneity. Extensive experiments using real datasets demonstrate that the\nproposed strategies outperform conventional batch-size adaptation schemes that\ndo not consider the C$^2$ tradeoff or device heterogeneity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u901a\u4fe1\u4e0e\u8ba1\u7b97\uff08C\u00b2\uff09\u611f\u77e5\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6279\u91cf\u5927\u5c0f\u63a7\u5236\u6765\u964d\u4f4e\u7aef\u5230\u7aef\u5b66\u4e60\u5ef6\u8fdf\uff0c\u540c\u65f6\u786e\u4fdd\u6536\u655b\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u57286G\u7f51\u7edc\u4e2d\u5177\u6709\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf\uff0c\u4f46\u9ad8\u7ef4\u6a21\u578b\u66f4\u65b0\u548c\u8bbe\u5907\u5f02\u6784\u6027\u5bfc\u81f4\u5ef6\u8fdf\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u4f4e\u5ef6\u8fdf\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2aC\u00b2\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6536\u655b\u6027\u63ed\u793aC\u00b2\u6743\u8861\uff0c\u63d0\u51fa\u4e24\u79cd\u6279\u91cf\u63a7\u5236\u7b56\u7565\u4ee5\u9002\u5e94\u4e0d\u540c\u573a\u666f\u548c\u8bbe\u5907\u5f02\u6784\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u7b56\u7565\u4f18\u4e8e\u4e0d\u8003\u8651C\u00b2\u6743\u8861\u6216\u8bbe\u5907\u5f02\u6784\u6027\u7684\u4f20\u7edf\u6279\u91cf\u8c03\u6574\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86C\u00b2\u6743\u8861\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u4efb\u52a1\u5e94\u7528\u3002"}}
{"id": "2507.15520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15520", "abs": "https://arxiv.org/abs/2507.15520", "authors": ["Hanting Li", "Fei Zhou", "Xin Sun", "Yang Hua", "Jungong Han", "Liang-Jie Zhang"], "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement", "comment": "11 pages, 10 figures, 6 tables", "summary": "Recent Transformer-based low-light enhancement methods have made promising\nprogress in recovering global illumination. However, they still struggle with\nnon-uniform lighting scenarios, such as backlit and shadow, appearing as\nover-exposure or inadequate brightness restoration. To address this challenge,\nwe present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)\nframework that enables accurate illumination restoration. Specifically, we\npropose a dynamic integral image representation to model the spatially-varying\nillumination, and further construct a novel Spatially-Adaptive Integral\nIllumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an\nIllumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which\nleverages the illumination to calibrate the lightness-relevant features toward\nvisual-pleased illumination enhancement. Extensive experiments on five standard\nlow-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our\nSAIGFormer significantly outperforms state-of-the-art methods in both\nquantitative and qualitative metrics. In particular, our method achieves\nsuperior performance in non-uniform illumination enhancement while exhibiting\nstrong generalization capabilities across multiple datasets. Code is available\nat https://github.com/LHTcode/SAIGFormer.git.", "AI": {"tldr": "SAIGFormer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u4f4e\u5149\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u79ef\u5206\u56fe\u50cf\u8868\u793a\u548c\u5149\u7167\u5f15\u5bfc\u7684\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u5747\u5300\u5149\u7167\u573a\u666f\u4e0b\u7684\u589e\u5f3a\u6548\u679c\u3002", "motivation": "\u73b0\u6709Transformer\u65b9\u6cd5\u5728\u975e\u5747\u5300\u5149\u7167\u573a\u666f\uff08\u5982\u80cc\u5149\u548c\u9634\u5f71\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8fc7\u66dd\u6216\u4eae\u5ea6\u6062\u590d\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u79ef\u5206\u56fe\u50cf\u8868\u793a\u548cSAI\u00b2E\u4f30\u8ba1\u5668\uff0c\u7ed3\u5408IG-MSA\u673a\u5236\u6821\u51c6\u5149\u7167\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u6807\u51c6\u4f4e\u5149\u6570\u636e\u96c6\u548c\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSAIGFormer\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SAIGFormer\u5728\u975e\u5747\u5300\u5149\u7167\u589e\u5f3a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5177\u5907\u5f3a\u5927\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15614", "abs": "https://arxiv.org/abs/2507.15614", "authors": ["Edward Holmberg", "Pujan Pokhrel", "Maximilian Zoch", "Elias Ioup", "Ken Pathak", "Steven Sloan", "Kendall Niles", "Jay Ratcliff", "Maik Flanagin", "Christian Guetl", "Julian Simeonov", "Mahdi Abdelguerfi"], "title": "Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting", "comment": "10 pages, 8 figures", "summary": "Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but\nare too computationally intensive for on-the-fly decision-making during flood\nevents. The central challenge is to accelerate these simulations without\nsacrificing accuracy. This paper introduces a deep learning surrogate that\ntreats HEC-RAS not as a solver but as a data-generation engine. We propose a\nhybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)\nto capture short-term temporal dynamics with a Geometry-Aware Fourier Neural\nOperator (Geo-FNO) to model long-range spatial dependencies along a river\nreach. The model learns underlying physics implicitly from a minimal\neight-channel feature vector encoding dynamic state, static geometry, and\nboundary forcings extracted directly from native HEC-RAS files. Trained on 67\nreaches of the Mississippi River Basin, the surrogate was evaluated on a\nyear-long, unseen hold-out simulation. Results show the model achieves a strong\npredictive accuracy, with a median absolute stage error of 0.31 feet.\nCritically, for a full 67-reach ensemble forecast, our surrogate reduces the\nrequired wall-clock time from 139 minutes to 40 minutes, a speedup of nearly\n3.5 times over the traditional solver. The success of this data-driven approach\ndemonstrates that robust feature engineering can produce a viable, high-speed\nreplacement for conventional hydraulic models, improving the computational\nfeasibility of large-scale ensemble flood forecasting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\uff0c\u7ed3\u5408GRU\u548cGeo-FNO\uff0c\u663e\u8457\u52a0\u901f\u4e86\u6d2a\u6c34\u9884\u6d4b\u7684\u8ba1\u7b97\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u6c42\u89e3\u5668\uff08\u5982HEC-RAS\uff09\u8ba1\u7b97\u8017\u65f6\uff0c\u65e0\u6cd5\u6ee1\u8db3\u6d2a\u6c34\u4e8b\u4ef6\u4e2d\u7684\u5b9e\u65f6\u51b3\u7b56\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6df7\u5408\u81ea\u56de\u5f52\u67b6\u6784\uff0c\u7ed3\u5408GRU\u548cGeo-FNO\uff0c\u4eceHEC-RAS\u751f\u6210\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u7269\u7406\u89c4\u5f8b\u3002", "result": "\u6a21\u578b\u5728\u5bc6\u897f\u897f\u6bd4\u6cb3\u6d41\u57df\u7684\u6d4b\u8bd5\u4e2d\uff0c\u4e2d\u4f4d\u7edd\u5bf9\u6c34\u4f4d\u8bef\u5dee\u4e3a0.31\u82f1\u5c3a\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u4e863.5\u500d\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u5de5\u7a0b\u53ef\u4ee5\u66ff\u4ee3\u4f20\u7edf\u6c34\u529b\u6a21\u578b\uff0c\u63d0\u5347\u5927\u89c4\u6a21\u6d2a\u6c34\u9884\u6d4b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.15540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15540", "abs": "https://arxiv.org/abs/2507.15540", "authors": ["Syed Ahmed Mahmood", "Ali Shah Ali", "Umer Ahmed", "Fawad Javed Fateh", "M. Zeeshan Zia", "Quoc-Huy Tran"], "title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport", "comment": null, "summary": "We study the problem of self-supervised procedure learning, which discovers\nkey steps and establishes their order from a set of unlabeled procedural\nvideos. Previous procedure learning methods typically learn frame-to-frame\ncorrespondences between videos before determining key steps and their order.\nHowever, their performance often suffers from order variations,\nbackground/redundant frames, and repeated actions. To overcome these\nchallenges, we propose a self-supervised procedure learning framework, which\nutilizes a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior for computing frame-to-frame mapping between videos. However,\noptimizing exclusively for the above temporal alignment term may lead to\ndegenerate solutions, where all frames are mapped to a small cluster in the\nembedding space and hence every video is associated with only one key step. To\naddress that limitation, we further integrate a contrastive regularization\nterm, which maps different frames to different points in the embedding space,\navoiding the collapse to trivial solutions. Finally, we conduct extensive\nexperiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,\nProceL and CrossTask) benchmarks to demonstrate superior performance by our\napproach against previous methods, including OPEL which relies on a traditional\nKantorovich optimal transport formulation with an optimality prior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u8fc7\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408Gromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u987a\u5e8f\u53d8\u5316\u3001\u80cc\u666f\u5197\u4f59\u548c\u52a8\u4f5c\u91cd\u590d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u987a\u5e8f\u53d8\u5316\u3001\u80cc\u666f\u5197\u4f59\u548c\u52a8\u4f5c\u91cd\u590d\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u7ed3\u5408Gromov-Wasserstein\u6700\u4f18\u4f20\u8f93\u548c\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u907f\u514d\u5d4c\u5165\u7a7a\u95f4\u9000\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u8fc7\u7a0b\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2507.15640", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15640", "abs": "https://arxiv.org/abs/2507.15640", "authors": ["Kailai Yang", "Xiao Liu", "Lei Ji", "Hao Li", "Yeyun Gong", "Peng Cheng", "Mao Yang"], "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training", "comment": null, "summary": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u578b\uff08Data Mixing Agent\uff09\uff0c\u7528\u4e8e\u81ea\u52a8\u8c03\u6574\u6e90\u9886\u57df\u548c\u76ee\u6807\u9886\u57df\u7684\u6570\u636e\u6743\u91cd\uff0c\u4ee5\u5e73\u8861\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9886\u57df\u91cd\u52a0\u6743\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u76f4\u89c9\u6216\u7ecf\u9a8c\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\uff0c\u5b66\u4e60\u4ece\u5927\u91cf\u6570\u636e\u6df7\u5408\u8f68\u8ff9\u4e2d\u63d0\u53d6\u901a\u7528\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u9886\u57df\u548c\u6a21\u578b\u3002", "conclusion": "Data Mixing Agent \u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u9886\u57df\u91cd\u52a0\u6743\u65b9\u6cd5\uff0c\u80fd\u51cf\u5c11\u5bf9\u6e90\u9886\u57df\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.15541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15541", "abs": "https://arxiv.org/abs/2507.15541", "authors": ["Jongmin Shin", "Enki Cho", "Ka Yong Kim", "Jung Yong Kim", "Seong Tae Kim", "Namkee Oh"], "title": "Towards Holistic Surgical Scene Graph", "comment": "Accepted to MICCAI 2025", "summary": "Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com", "AI": {"tldr": "\u63d0\u51faEndoscapes-SG201\u6570\u636e\u96c6\u548cSSG-Com\u65b9\u6cd5\uff0c\u7528\u4e8e\u589e\u5f3a\u624b\u672f\u573a\u666f\u56fe\u8868\u793a\uff0c\u91cd\u70b9\u5173\u6ce8\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u548c\u624b\u90e8\u8eab\u4efd\u3002", "motivation": "\u624b\u672f\u573a\u666f\u7406\u89e3\u9700\u8981\u66f4\u5168\u9762\u7684\u56fe\u8868\u793a\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u63a2\u7d22\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u548c\u624b\u90e8\u8eab\u4efd\u3002", "method": "\u63d0\u51faEndoscapes-SG201\u6570\u636e\u96c6\u548cSSG-Com\u65b9\u6cd5\uff0c\u7ed3\u5408\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u548c\u624b\u90e8\u8eab\u4efd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u4e9b\u7ec4\u4ef6\u5bf9\u624b\u672f\u573a\u666f\u7406\u89e3\u4efb\u52a1\uff08\u5982\u5b89\u5168\u8bc4\u4f30\u548c\u52a8\u4f5c\u8bc6\u522b\uff09\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "\u6574\u5408\u5de5\u5177-\u52a8\u4f5c-\u76ee\u6807\u7ec4\u5408\u548c\u624b\u90e8\u8eab\u4efd\u80fd\u6709\u6548\u63d0\u5347\u624b\u672f\u573a\u666f\u56fe\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2507.15643", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15643", "abs": "https://arxiv.org/abs/2507.15643", "authors": ["Elnur Isgandarov", "Matteo Cederle", "Federico Chiariotti", "Gian Antonio Susto"], "title": "Towards Explainable Anomaly Detection in Shared Mobility Systems", "comment": "6 pages, 8 figures. Paper accepted to J3C 2025 (Joint Conference on\n  Computers, Cognition and Communication", "summary": "Shared mobility systems, such as bike-sharing networks, play a crucial role\nin urban transportation. Identifying anomalies in these systems is essential\nfor optimizing operations, improving service reliability, and enhancing user\nexperience. This paper presents an interpretable anomaly detection framework\nthat integrates multi-source data, including bike-sharing trip records, weather\nconditions, and public transit availability. The Isolation Forest algorithm is\nemployed for unsupervised anomaly detection, along with the Depth-based\nIsolation Forest Feature Importance (DIFFI) algorithm providing\ninterpretability. Results show that station-level analysis offers a robust\nunderstanding of anomalies, highlighting the influence of external factors such\nas adverse weather and limited transit availability. Our findings contribute to\nimproving decision-making in shared mobility operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\uff0c\u7ed3\u5408\u591a\u6e90\u6570\u636e\u548cIsolation Forest\u7b97\u6cd5\uff0c\u63d0\u5347\u8fd0\u8425\u51b3\u7b56\u3002", "motivation": "\u5171\u4eab\u51fa\u884c\u7cfb\u7edf\uff08\u5982\u5171\u4eab\u5355\u8f66\uff09\u5bf9\u57ce\u5e02\u4ea4\u901a\u81f3\u5173\u91cd\u8981\uff0c\u8bc6\u522b\u5f02\u5e38\u6709\u52a9\u4e8e\u4f18\u5316\u8fd0\u8425\u3001\u63d0\u9ad8\u670d\u52a1\u53ef\u9760\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u91c7\u7528Isolation Forest\u7b97\u6cd5\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u5e76\u7ed3\u5408DIFFI\u7b97\u6cd5\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\uff0c\u6574\u5408\u4e86\u5171\u4eab\u5355\u8f66\u884c\u7a0b\u8bb0\u5f55\u3001\u5929\u6c14\u6761\u4ef6\u548c\u516c\u5171\u4ea4\u901a\u53ef\u7528\u6027\u7b49\u591a\u6e90\u6570\u636e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u7ad9\u70b9\u7ea7\u5206\u6790\u80fd\u6709\u6548\u8bc6\u522b\u5f02\u5e38\uff0c\u5916\u90e8\u56e0\u7d20\uff08\u5982\u6076\u52a3\u5929\u6c14\u548c\u516c\u5171\u4ea4\u901a\u9650\u5236\uff09\u5bf9\u5f02\u5e38\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5171\u4eab\u51fa\u884c\u8fd0\u8425\u63d0\u4f9b\u4e86\u6539\u8fdb\u51b3\u7b56\u7684\u4f9d\u636e\u3002"}}
{"id": "2507.15542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15542", "abs": "https://arxiv.org/abs/2507.15542", "authors": ["Qinqian Lei", "Bo Wang", "Robby T. Tan"], "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation", "comment": "Accepted by ICCV 2025", "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.", "AI": {"tldr": "HOLa\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3VLM\u7279\u5f81\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u96f6\u6837\u672c\u4eba-\u7269\u4ea4\u4e92\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u52a8\u4f5c\u533a\u5206\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u96f6\u6837\u672cHOI\u68c0\u6d4b\u4e2d\u96be\u4ee5\u533a\u5206\u76f8\u540c\u5bf9\u8c61\u7684\u4e0d\u540c\u52a8\u4f5c\u6216\u6cdb\u5316\u5230\u672a\u89c1\u7c7b\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3VLM\u6587\u672c\u7279\u5f81\uff0c\u751f\u6210\u7c7b\u5171\u4eab\u57fa\u7840\u7279\u5f81\u548c\u53ef\u8c03\u6743\u91cd\uff0c\u5e76\u5f15\u5165\u4eba-\u7269\u6807\u8bb0\u548cLLM\u9a71\u52a8\u7684\u52a8\u4f5c\u6b63\u5219\u5316\u3002", "result": "\u5728HICO-DET\u6570\u636e\u96c6\u4e0a\uff0c\u672a\u89c1\u7c7bmAP\u8fbe\u523027.91\uff0c\u5237\u65b0\u4e86\u96f6\u6837\u672cHOI\u68c0\u6d4b\u7684SOTA\u3002", "conclusion": "HOLa\u901a\u8fc7\u7279\u5f81\u5206\u89e3\u548c\u6743\u91cd\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672cHOI\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15678", "categories": ["cs.LG", "math.DG", "math.DS", "math.SG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15678", "abs": "https://arxiv.org/abs/2507.15678", "authors": ["Amine Mohamed Aboussalah", "Abdessalam Ed-dib"], "title": "GeoHNNs: Geometric Hamiltonian Neural Networks", "comment": null, "summary": "The fundamental laws of physics are intrinsically geometric, dictating the\nevolution of systems through principles of symmetry and conservation. While\nmodern machine learning offers powerful tools for modeling complex dynamics\nfrom data, common methods often ignore this underlying geometric fabric.\nPhysics-informed neural networks, for instance, can violate fundamental\nphysical principles, leading to predictions that are unstable over long\nperiods, particularly for high-dimensional and chaotic systems. Here, we\nintroduce \\textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework\nthat learns dynamics by explicitly encoding the geometric priors inherent to\nphysical laws. Our approach enforces two fundamental structures: the Riemannian\ngeometry of inertia, by parameterizing inertia matrices in their natural\nmathematical space of symmetric positive-definite matrices, and the symplectic\ngeometry of phase space, using a constrained autoencoder to ensure the\npreservation of phase space volume in a reduced latent space. We demonstrate\nthrough experiments on systems ranging from coupled oscillators to\nhigh-dimensional deformable objects that GeoHNN significantly outperforms\nexisting models. It achieves superior long-term stability, accuracy, and energy\nconservation, confirming that embedding the geometry of physics is not just a\ntheoretical appeal but a practical necessity for creating robust and\ngeneralizable models of the physical world.", "AI": {"tldr": "GeoHNN\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7f16\u7801\u7269\u7406\u5b9a\u5f8b\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5e38\u5ffd\u7565\u7269\u7406\u7cfb\u7edf\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0d\u7a33\u5b9a\u3002GeoHNN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53c2\u6570\u5316\u60ef\u6027\u77e9\u9635\u548c\u4f7f\u7528\u7ea6\u675f\u81ea\u7f16\u7801\u5668\uff0cGeoHNN\u5f3a\u5236\u4fdd\u7559\u9ece\u66fc\u51e0\u4f55\u548c\u8f9b\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGeoHNN\u5728\u957f\u671f\u7a33\u5b9a\u6027\u3001\u7cbe\u5ea6\u548c\u80fd\u91cf\u5b88\u6052\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u5d4c\u5165\u7269\u7406\u51e0\u4f55\u7ed3\u6784\u662f\u6784\u5efa\u7a33\u5065\u3001\u901a\u7528\u7269\u7406\u6a21\u578b\u7684\u5b9e\u8df5\u9700\u6c42\u3002"}}
{"id": "2507.15569", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15569", "abs": "https://arxiv.org/abs/2507.15569", "authors": ["Xiaoyi Bao", "Chenwei Xie", "Hao Tang", "Tingyu Weng", "Xiaofeng Wang", "Yun Zheng", "Xingang Wang"], "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding", "comment": "Accepted by ICCV 2025", "summary": "In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.", "AI": {"tldr": "\u63d0\u51faDynamic-Image\uff08DynImg\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u5173\u952e\u5e27\u4f5c\u4e3a\u65f6\u95f4\u63d0\u793a\uff0c\u589e\u5f3a\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u65f6\u7a7a\u4fe1\u606f\u6574\u5408\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u65f6\u7a7a\u4fe1\u606f\u5206\u79bb\u5904\u7406\uff0c\u5bfc\u81f4\u5feb\u901f\u79fb\u52a8\u7269\u4f53\u7684\u7a7a\u95f4\u4fe1\u606f\u96be\u4ee5\u51c6\u786e\u8868\u793a\uff0c\u5f71\u54cd\u65f6\u7a7a\u4ea4\u4e92\u548c\u89c6\u9891\u7406\u89e3\u3002", "method": "\u5f15\u5165\u975e\u5173\u952e\u5e27\u4f5c\u4e3a\u65f6\u95f4\u63d0\u793a\uff0c\u7ed3\u54084D\u89c6\u9891\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\uff0c\u7a81\u51fa\u5feb\u901f\u79fb\u52a8\u7269\u4f53\u7684\u7a7a\u95f4\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDynImg\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u7ea62%\u3002", "conclusion": "DynImg\u901a\u8fc7\u65f6\u95f4\u63d0\u793a\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.15718", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15718", "abs": "https://arxiv.org/abs/2507.15718", "authors": ["Matteo Cederle", "Andrea Mazzucco", "Andrea Demartini", "Eugenio Mazza", "Eugenia Suriani", "Federico Vitti", "Gian Antonio Susto"], "title": "Explainable Anomaly Detection for Electric Vehicles Charging Stations", "comment": "4 pages, 3 figures. Paper accepted to J3C 2025 (Joint Conference on\n  Computers, Cognition and Communication)", "summary": "Electric vehicles (EV) charging stations are one of the critical\ninfrastructures needed to support the transition to renewable-energy-based\nmobility, but ensuring their reliability and efficiency requires effective\nanomaly detection to identify irregularities in charging behavior. However, in\nsuch a productive scenario, it is also crucial to determine the underlying\ncause behind the detected anomalies. To achieve this goal, this study\ninvestigates unsupervised anomaly detection techniques for EV charging\ninfrastructure, integrating eXplainable Artificial Intelligence techniques to\nenhance interpretability and uncover root causes of anomalies.\n  Using real-world sensors and charging session data, this work applies\nIsolation Forest to detect anomalies and employs the Depth-based Isolation\nForest Feature Importance (DIFFI) method to identify the most important\nfeatures contributing to such anomalies. The efficacy of the proposed approach\nis evaluated in a real industrial case.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u7ed3\u5408\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u5f02\u5e38\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u7ad9\u662f\u652f\u6301\u53ef\u518d\u751f\u80fd\u6e90\u4ea4\u901a\u8f6c\u578b\u7684\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u9700\u8981\u6709\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u6765\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u6548\u7387\u3002\u540c\u65f6\uff0c\u7406\u89e3\u5f02\u5e38\u7684\u6839\u672c\u539f\u56e0\u4e5f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u7684\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\uff08\u5982Isolation Forest\uff09\uff0c\u5e76\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\uff08\u5982DIFFI\uff09\u6765\u8bc6\u522b\u5f02\u5e38\u53ca\u5176\u5173\u952e\u7279\u5f81\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4f20\u611f\u5668\u548c\u5145\u7535\u4f1a\u8bdd\u6570\u636e\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5de5\u4e1a\u6848\u4f8b\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u5e76\u63ed\u793a\u5176\u6839\u672c\u539f\u56e0\uff0c\u4e3a\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.15577", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15577", "abs": "https://arxiv.org/abs/2507.15577", "authors": ["Hugo Carlesso", "Maria Eliza Patulea", "Moncef Garouani", "Radu Tudor Ionescu", "Josiane Mothe"], "title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation", "comment": null, "summary": "Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.", "AI": {"tldr": "GeMix\u662f\u4e00\u79cd\u57fa\u4e8eGAN\u7684\u4e24\u9636\u6bb5\u56fe\u50cf\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u7b7e\u611f\u77e5\u63d2\u503c\u751f\u6210\u66f4\u771f\u5b9e\u7684\u56fe\u50cf\uff0c\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfMixup\u7684\u50cf\u7d20\u7ea7\u63d2\u503c\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u53ef\u80fd\u751f\u6210\u4e0d\u771f\u5b9e\u56fe\u50cf\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u679c\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002", "method": "\u4f7f\u7528StyleGAN2-ADA\u751f\u6210\u5668\uff0c\u901a\u8fc7Dirichlet\u548cBeta\u5206\u5e03\u91c7\u6837\u6807\u7b7e\u5411\u91cf\uff0c\u751f\u6210\u8fde\u7eed\u7c7b\u6d41\u5f62\u4e0a\u7684\u56fe\u50cf\u3002", "result": "\u5728COVIDx-CT-3\u6570\u636e\u96c6\u4e0a\uff0cGeMix\u7ed3\u5408\u771f\u5b9e\u6570\u636e\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\uff0c\u964d\u4f4e\u4e86COVID-19\u68c0\u6d4b\u7684\u5047\u9634\u6027\u7387\u3002", "conclusion": "GeMix\u662f\u4f20\u7edfMixup\u7684\u76f4\u63a5\u66ff\u4ee3\u65b9\u6848\uff0c\u63d0\u4f9b\u66f4\u5f3a\u7684\u6b63\u5219\u5316\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u3002"}}
{"id": "2507.15727", "categories": ["cs.LG", "cs.GT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15727", "abs": "https://arxiv.org/abs/2507.15727", "authors": ["Xuchuang Wang", "Bo Sun", "Hedyeh Beyhaghi", "John C. S. Lui", "Mohammad Hajiesmaili", "Adam Wierman"], "title": "Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems", "comment": null, "summary": "This paper introduces a novel multi-agent ski-rental problem that generalizes\nthe classical ski-rental dilemma to a group setting where agents incur\nindividual and shared costs. In our model, each agent can either rent at a\nfixed daily cost, or purchase a pass at an individual cost, with an additional\nthird option of a discounted group pass available to all. We consider scenarios\nin which agents' active days differ, leading to dynamic states as agents drop\nout of the decision process. To address this problem from different\nperspectives, we define three distinct competitive ratios: overall,\nstate-dependent, and individual rational. For each objective, we design and\nanalyze optimal deterministic and randomized policies. Our deterministic\npolicies employ state-aware threshold functions that adapt to the dynamic\nstates, while our randomized policies sample and resample thresholds from\ntailored state-aware distributions. The analysis reveals that symmetric\npolicies, in which all agents use the same threshold, outperform asymmetric\nones. Our results provide competitive ratio upper and lower bounds and extend\nclassical ski-rental insights to multi-agent settings, highlighting both\ntheoretical and practical implications for group decision-making under\nuncertainty.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u6ed1\u96ea\u79df\u8d41\u56f0\u5883\uff0c\u8003\u8651\u4e86\u4ee3\u7406\u7684\u4e2a\u4f53\u548c\u5171\u4eab\u6210\u672c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e09\u79cd\u7ade\u4e89\u6bd4\u7387\u7684\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u591a\u4ee3\u7406\u73af\u5883\u4e0b\u7684\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\uff0c\u89e3\u51b3\u4ee3\u7406\u5728\u4e0d\u540c\u6d3b\u8dc3\u5929\u6570\u4e0b\u7684\u52a8\u6001\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u786e\u5b9a\u6027\u7b56\u7565\uff08\u57fa\u4e8e\u72b6\u6001\u611f\u77e5\u7684\u9608\u503c\u51fd\u6570\uff09\u548c\u968f\u673a\u7b56\u7565\uff08\u4ece\u5b9a\u5236\u5206\u5e03\u4e2d\u91c7\u6837\u9608\u503c\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5bf9\u79f0\u4e0e\u975e\u5bf9\u79f0\u7b56\u7565\u7684\u6027\u80fd\u3002", "result": "\u5bf9\u79f0\u7b56\u7565\u4f18\u4e8e\u975e\u5bf9\u79f0\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u4e86\u7ade\u4e89\u6bd4\u7387\u7684\u4e0a\u754c\u548c\u4e0b\u754c\u3002", "conclusion": "\u7814\u7a76\u6269\u5c55\u4e86\u7ecf\u5178\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\u7684\u7406\u8bba\uff0c\u5bf9\u7fa4\u4f53\u51b3\u7b56\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5e94\u7528\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2507.15578", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.15578", "abs": "https://arxiv.org/abs/2507.15578", "authors": ["Gabriele Inzerillo", "Diego Valsesia", "Aniello Fiengo", "Enrico Magli"], "title": "Compress-Align-Detect: onboard change detection from unregistered images", "comment": null, "summary": "Change detection from satellite images typically incurs a delay ranging from\nseveral hours up to days because of latency in downlinking the acquired images\nand generating orthorectified image products at the ground stations; this may\npreclude real- or near real-time applications. To overcome this limitation, we\npropose shifting the entire change detection workflow onboard satellites. This\nrequires to simultaneously solve challenges in data storage, image registration\nand change detection with a strict complexity constraint. In this paper, we\npresent a novel and efficient framework for onboard change detection that\naddresses the aforementioned challenges in an end-to-end fashion with a deep\nneural network composed of three interlinked submodules: (1) image compression,\ntailored to minimize onboard data storage resources; (2) lightweight\nco-registration of non-orthorectified multi-temporal image pairs; and (3) a\nnovel temporally-invariant and computationally efficient change detection\nmodel. This is the first approach in the literature combining all these tasks\nin a single end-to-end framework with the constraints dictated by onboard\nprocessing. Experimental results compare each submodule with the current\nstate-of-the-art, and evaluate the performance of the overall integrated system\nin realistic setting on low-power hardware. Compelling change detection results\nare obtained in terms of F1 score as a function of compression rate, sustaining\na throughput of 0.7 Mpixel/s on a 15W accelerator.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u536b\u661f\u4e0a\u5b9e\u65f6\u53d8\u5316\u68c0\u6d4b\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u5b58\u50a8\u3001\u56fe\u50cf\u914d\u51c6\u548c\u53d8\u5316\u68c0\u6d4b\u7684\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u536b\u661f\u56fe\u50cf\u53d8\u5316\u68c0\u6d4b\u56e0\u5730\u9762\u7ad9\u5904\u7406\u5ef6\u8fdf\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9700\u6c42\uff0c\u9700\u5c06\u6574\u4e2a\u6d41\u7a0b\u79fb\u81f3\u536b\u661f\u4e0a\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5305\u542b\u4e09\u4e2a\u5b50\u6a21\u5757\uff1a\u56fe\u50cf\u538b\u7f29\u3001\u8f7b\u91cf\u7ea7\u914d\u51c6\u548c\u9ad8\u6548\u53d8\u5316\u68c0\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u4f4e\u529f\u8017\u786c\u4ef6\u4e0a\u5b9e\u73b00.7 Mpixel/s\u7684\u5904\u7406\u901f\u5ea6\uff0cF1\u5206\u6570\u8868\u73b0\u4f18\u79c0\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5728\u536b\u661f\u4e0a\u5b9e\u73b0\u7aef\u5230\u7aef\u53d8\u5316\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.15769", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15769", "abs": "https://arxiv.org/abs/2507.15769", "authors": ["Ahmad M. Nazar", "Abdulkadir Celik", "Mohamed Y. Selim", "Asmaa Abdallah", "Daji Qiao", "Ahmed M. Eltawil"], "title": "Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks", "comment": "Accepted in IEEE Asilomar Conference on Signals, Systems, and\n  Computers 2025", "summary": "Vehicular communication systems operating in the millimeter wave (mmWave)\nband are highly susceptible to signal blockage from dynamic obstacles such as\nvehicles, pedestrians, and infrastructure. To address this challenge, we\npropose a proactive blockage prediction framework that utilizes multi-modal\nsensing, including camera, GPS, LiDAR, and radar inputs in an\ninfrastructure-to-vehicle (I2V) setting. This approach uses modality-specific\ndeep learning models to process each sensor stream independently and fuses\ntheir outputs using a softmax-weighted ensemble strategy based on validation\nperformance. Our evaluations, for up to 1.5s in advance, show that the\ncamera-only model achieves the best standalone trade-off with an F1-score of\n97.1% and an inference time of 89.8ms. A camera+radar configuration further\nimproves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness\nand efficiency of multi-modal sensing for mmWave blockage prediction and\nprovide a pathway for proactive wireless communication in dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u611f\u77e5\u7684\u6beb\u7c73\u6ce2\u4fe1\u53f7\u906e\u6321\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u6444\u50cf\u5934\u3001GPS\u3001LiDAR\u548c\u96f7\u8fbe\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u8f6f\u52a0\u6743\u96c6\u6210\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u5ef6\u8fdf\u7684\u9884\u6d4b\u3002", "motivation": "\u6beb\u7c73\u6ce2\u9891\u6bb5\u7684\u8f66\u8f86\u901a\u4fe1\u7cfb\u7edf\u6613\u53d7\u52a8\u6001\u969c\u788d\u7269\uff08\u5982\u8f66\u8f86\u3001\u884c\u4eba\u3001\u57fa\u7840\u8bbe\u65bd\uff09\u7684\u4fe1\u53f7\u906e\u6321\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u4e3b\u52a8\u9884\u6d4b\u65b9\u6cd5\u4ee5\u63d0\u5347\u901a\u4fe1\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001\u611f\u77e5\uff08\u6444\u50cf\u5934\u3001GPS\u3001LiDAR\u3001\u96f7\u8fbe\uff09\u548c\u6a21\u6001\u7279\u5b9a\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u8f6f\u52a0\u6743\u96c6\u6210\u7b56\u7565\u878d\u5408\u5404\u4f20\u611f\u5668\u8f93\u51fa\u3002", "result": "\u6444\u50cf\u5934\u5355\u72ec\u6a21\u578b\u7684F1\u5206\u6570\u4e3a97.1%\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a89.8ms\uff1b\u6444\u50cf\u5934+\u96f7\u8fbe\u7ec4\u5408\u7684F1\u5206\u6570\u63d0\u5347\u81f397.2%\uff0c\u63a8\u7406\u65f6\u95f4\u4e3a95.7ms\u3002", "conclusion": "\u591a\u6a21\u6001\u611f\u77e5\u5728\u6beb\u7c73\u6ce2\u906e\u6321\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4e3b\u52a8\u65e0\u7ebf\u901a\u4fe1\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.15595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15595", "abs": "https://arxiv.org/abs/2507.15595", "authors": ["Salah Eddine Bekhouche", "Gaby Maroun", "Fadi Dornaika", "Abdenour Hadid"], "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging", "comment": null, "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}.", "AI": {"tldr": "SegDT\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u53d8\u538b\u5668\u7684\u65b0\u578b\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u786c\u4ef6\uff0c\u901a\u8fc7Rectified Flow\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6548\u679c\u3002", "motivation": "\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u5bf9\u76ae\u80a4\u764c\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4f4e\u6210\u672c\u786c\u4ef6\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u6269\u6563\u53d8\u538b\u5668\u548cRectified Flow\uff0c\u4f18\u5316\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u6548\u679c\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\u3002", "conclusion": "SegDT\u4e3a\u533b\u7597\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.15772", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.15772", "abs": "https://arxiv.org/abs/2507.15772", "authors": ["Anoop C. Patil", "Benny Jian Rong Sng", "Yu-Wei Chang", "Joana B. Pereira", "Chua Nam-Hai", "Rajani Sarojam", "Gajendra Pratap Singh", "In-Cheol Jang", "Giovanni Volpe"], "title": "Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis", "comment": "*Authors contributed equally to this work. +Supervised this work. 5\n  main figures and 1 extended data figure in manuscript. The PDF includes\n  supplementary material", "summary": "Detecting stress in plants is crucial for both open-farm and\ncontrolled-environment agriculture. Biomolecules within plants serve as key\nstress indicators, offering vital markers for continuous health monitoring and\nearly disease detection. Raman spectroscopy provides a powerful, non-invasive\nmeans to quantify these biomolecules through their molecular vibrational\nsignatures. However, traditional Raman analysis relies on customized\ndata-processing workflows that require fluorescence background removal and\nprior identification of Raman peaks of interest-introducing potential biases\nand inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation\nof Vibrational Raman spectra for plant-stress Analysis), a fully automated\nworkflow based on a variational autoencoder. Unlike conventional approaches,\nDIVA processes native Raman spectra-including fluorescence backgrounds-without\nmanual preprocessing, identifying and quantifying significant spectral features\nin an unbiased manner. We applied DIVA to detect a range of plant stresses,\nincluding abiotic (shading, high light intensity, high temperature) and biotic\nstressors (bacterial infections). By integrating deep learning with vibrational\nspectroscopy, DIVA paves the way for AI-driven plant health assessment,\nfostering more resilient and sustainable agricultural practices.", "AI": {"tldr": "DIVA\u662f\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u5168\u81ea\u52a8\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u901a\u8fc7\u62c9\u66fc\u5149\u8c31\u65e0\u504f\u68c0\u6d4b\u690d\u7269\u5e94\u6fc0\u53cd\u5e94\u3002", "motivation": "\u690d\u7269\u5e94\u6fc0\u68c0\u6d4b\u5bf9\u519c\u4e1a\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u62c9\u66fc\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "DIVA\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5904\u7406\u539f\u59cb\u62c9\u66fc\u5149\u8c31\uff08\u5305\u62ec\u8367\u5149\u80cc\u666f\uff09\uff0c\u65e0\u9700\u624b\u52a8\u9884\u5904\u7406\u3002", "result": "DIVA\u6210\u529f\u68c0\u6d4b\u4e86\u591a\u79cd\u690d\u7269\u5e94\u6fc0\uff08\u5982\u5149\u7167\u3001\u6e29\u5ea6\u53d8\u5316\u548c\u7ec6\u83cc\u611f\u67d3\uff09\u3002", "conclusion": "DIVA\u4e3aAI\u9a71\u52a8\u7684\u690d\u7269\u5065\u5eb7\u8bc4\u4f30\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u4fc3\u8fdb\u519c\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2507.15774", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15774", "abs": "https://arxiv.org/abs/2507.15774", "authors": ["Alexis-Raja Brachet", "Pierre-Yves Richard", "C\u00e9line Hudelot"], "title": "Dynamics is what you need for time-series forecasting!", "comment": "13 pages, 6 figures, 1 table", "summary": "While boundaries between data modalities are vanishing, the usual successful\ndeep models are still challenged by simple ones in the time-series forecasting\ntask. Our hypothesis is that this task needs models that are able to learn the\ndata underlying dynamics. We propose to validate it through both systemic and\nempirical studies. We develop an original $\\texttt{PRO-DYN}$ nomenclature to\nanalyze existing models through the lens of dynamics. Two observations thus\nemerged: $\\textbf{1}$. under-performing architectures learn dynamics at most\npartially, $\\textbf{2}$. the location of the dynamics block at the model end is\nof prime importance. We conduct extensive experiments to confirm our\nobservations on a set of performance-varying models with diverse backbones.\nResults support the need to incorporate a learnable dynamics block and its use\nas the final predictor.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPRO-DYN\u7684\u547d\u540d\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u73b0\u6709\u6a21\u578b\u5728\u52a8\u6001\u5b66\u4e60\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6027\u80fd\u4e0d\u4f73\u7684\u6a21\u578b\u4ec5\u90e8\u5206\u5b66\u4e60\u52a8\u6001\uff0c\u4e14\u52a8\u6001\u5757\u7684\u4f4d\u7f6e\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u5047\u8bbe\u662f\u9700\u8981\u5b66\u4e60\u6570\u636e\u7684\u5e95\u5c42\u52a8\u6001\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5f00\u53d1\u4e86PRO-DYN\u547d\u540d\u6cd5\u5206\u6790\u6a21\u578b\u52a8\u6001\u5b66\u4e60\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u52a8\u6001\u5757\u7684\u5b66\u4e60\u80fd\u529b\u53ca\u5176\u4f5c\u4e3a\u6700\u7ec8\u9884\u6d4b\u5668\u7684\u4f4d\u7f6e\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u5728\u6a21\u578b\u4e2d\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u52a8\u6001\u5757\u5e76\u5c06\u5176\u4f5c\u4e3a\u6700\u7ec8\u9884\u6d4b\u5668\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.15602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15602", "abs": "https://arxiv.org/abs/2507.15602", "authors": ["Zihui Gao", "Jia-Wang Bian", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting", "comment": null, "summary": "Surface reconstruction and novel view rendering from sparse-view images are\nchallenging. Signed Distance Function (SDF)-based methods struggle with fine\ndetails, while 3D Gaussian Splatting (3DGS)-based approaches lack global\ngeometry coherence. We propose a novel hybrid method that combines the\nstrengths of both approaches: SDF captures coarse geometry to enhance\n3DGS-based rendering, while newly rendered images from 3DGS refine the details\nof SDF for accurate surface reconstruction. As a result, our method surpasses\nstate-of-the-art approaches in surface reconstruction and novel view synthesis\non the DTU and MobileBrick datasets. Code will be released at\nhttps://github.com/Gaozihui/SurfaceSplat.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SDF\u548c3DGS\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u7a00\u758f\u89c6\u56fe\u56fe\u50cf\u4e2d\u7684\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3SDF\u65b9\u6cd5\u5728\u7ec6\u8282\u6355\u6349\u548c3DGS\u65b9\u6cd5\u5728\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408SDF\u7684\u7c97\u51e0\u4f55\u6355\u6349\u548c3DGS\u7684\u7ec6\u8282\u6e32\u67d3\uff0c\u76f8\u4e92\u4f18\u5316\u3002", "result": "\u5728DTU\u548cMobileBrick\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u5728\u8868\u9762\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u6e32\u67d3\u4e2d\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.15784", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15784", "abs": "https://arxiv.org/abs/2507.15784", "authors": ["Zihang Ma", "Qitian Yin"], "title": "Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets", "comment": null, "summary": "Graph node classification is a fundamental task in graph neural networks\n(GNNs), aiming to assign predefined class labels to nodes. On the PubMed\ncitation network dataset, we observe significant classification difficulty\ndisparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,\n7.5% lower than Category 1. To address this, we propose a\nWasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),\ntraining specialized GNN models for Categories 0/1 (with layer normalization\nand residual connections) and Multi-hop Graph Attention Networks (GAT) for\nCategory 2. The WR distance metric optimizes representation similarity between\nmodels, particularly focusing on improving Category 2 performance. Our adaptive\nfusion strategy dynamically weights models based on category-specific\nperformance, with Category 2 assigned a GAT weight of 0.8. WR distance further\nguides the fusion process by measuring distributional differences between model\nrepresentations, enabling more principled integration of complementary\nfeatures.\n  Experimental results show WR-EFM achieves balanced accuracy across\ncategories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),\noutperforming both single models and standard fusion approaches. The\ncoefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%\nlower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM\nimproves Category 2 accuracy by 5.5% compared to GCN, verifying the\neffectiveness of WR-guided fusion in capturing complex structural patterns.\nThis work provides a novel paradigm for handling class-imbalanced graph\nclassification tasks. To promote the research community, we release our project\nat https://github.com/s010m00n/GASEM4NC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWasserstein-Rubinstein\u8ddd\u79bb\u7684\u4e13\u5bb6\u878d\u5408\u6a21\u578b\uff08WR-EFM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u8282\u70b9\u5206\u7c7b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89c2\u5bdf\u5230PubMed\u5f15\u6587\u7f51\u7edc\u6570\u636e\u96c6\u4e2d\u4e0d\u540c\u7c7b\u522b\u7684\u5206\u7c7b\u96be\u5ea6\u5dee\u5f02\u663e\u8457\uff0c\u4f20\u7edfGCN\u5728\u7c7b\u522b2\u4e0a\u7684\u51c6\u786e\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528WR\u8ddd\u79bb\u589e\u5f3a\u7684\u4e13\u5bb6\u878d\u5408\u6a21\u578b\uff0c\u5206\u522b\u4e3a\u7c7b\u522b0/1\u548c\u7c7b\u522b2\u8bad\u7ec3\u4e13\u7528GNN\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u52a8\u6001\u52a0\u6743\u3002", "result": "WR-EFM\u5728\u4e09\u4e2a\u7c7b\u522b\u4e0a\u7684\u51c6\u786e\u7387\u5206\u522b\u4e3a77.8%\u300178.0%\u548c79.9%\uff0c\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u548c\u6807\u51c6\u878d\u5408\u65b9\u6cd5\uff0c\u4e14\u7a33\u5b9a\u6027\u66f4\u9ad8\u3002", "conclusion": "WR-EFM\u4e3a\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u56fe\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u9879\u76ee\u4fc3\u8fdb\u7814\u7a76\u793e\u533a\u53d1\u5c55\u3002"}}
{"id": "2507.15606", "categories": ["cs.CV", "68T45", "I.4.5"], "pdf": "https://arxiv.org/pdf/2507.15606", "abs": "https://arxiv.org/abs/2507.15606", "authors": ["Ru Jia", "Xiaozhuang Ma", "Jianji Wang", "Nanning Zheng"], "title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation", "comment": "5 pages, 4 figures, to be published", "summary": "While the proposal of the Tri-plane representation has advanced the\ndevelopment of the 3D-aware image generative models, problems rooted in its\ninherent structure, such as multi-face artifacts caused by sharing the same\nfeatures in symmetric regions, limit its ability to generate 360$^\\circ$ view\nimages. In this paper, we propose CylinderPlane, a novel implicit\nrepresentation based on Cylindrical Coordinate System, to eliminate the feature\nambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different\nfrom the inevitable feature entanglement in Cartesian coordinate-based\nTri-plane representation, the cylindrical coordinate system explicitly\nseparates features at different angles, allowing our cylindrical representation\npossible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis.\nWe further introduce the nested cylinder representation that composites\nmultiple cylinders at different scales, thereby enabling the model more\nadaptable to complex geometry and varying resolutions. The combination of\ncylinders with different resolutions can effectively capture more critical\nlocations and multi-scale features, greatly facilitates fine detail learning\nand robustness to different resolutions. Moreover, our representation is\nagnostic to implicit rendering methods and can be easily integrated into any\nneural rendering pipeline. Extensive experiments on both synthetic dataset and\nunstructured in-the-wild images demonstrate that our proposed representation\nachieves superior performance over previous methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5706\u67f1\u5750\u6807\u7cfb\u7684CylinderPlane\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86Tri-plane\u8868\u793a\u4e2d\u7684\u591a\u9762\u4f2a\u5f71\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65e0\u4f2a\u5f71\u7684360\u00b0\u56fe\u50cf\u5408\u6210\u3002", "motivation": "Tri-plane\u8868\u793a\u5728\u5bf9\u79f0\u533a\u57df\u5171\u4eab\u76f8\u540c\u7279\u5f81\u5bfc\u81f4\u591a\u9762\u4f2a\u5f71\uff0c\u9650\u5236\u4e86360\u00b0\u89c6\u56fe\u56fe\u50cf\u7684\u751f\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5706\u67f1\u5750\u6807\u7cfb\u5206\u79bb\u4e0d\u540c\u89d2\u5ea6\u7684\u7279\u5f81\uff0c\u5f15\u5165\u5d4c\u5957\u5706\u67f1\u8868\u793a\u4ee5\u5904\u7406\u590d\u6742\u51e0\u4f55\u548c\u591a\u5206\u8fa8\u7387\u9700\u6c42\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u591a\u89c6\u89d2\u4e00\u81f4\u7684360\u00b0\u56fe\u50cf\u5408\u6210\u3002", "conclusion": "CylinderPlane\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u795e\u7ecf\u6e32\u67d3\u6d41\u7a0b\u4e2d\u3002"}}
{"id": "2507.15788", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15788", "abs": "https://arxiv.org/abs/2507.15788", "authors": ["Sneheel Sarangi", "Hanan Salam"], "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning", "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5c0f\u89c4\u6a21LLMs\u80fd\u5426\u901a\u8fc7RLVR\u83b7\u5f97\u901a\u7528\u7684ToM\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4ec5\u80fd\u8fc7\u62df\u5408\u8bad\u7ec3\u6570\u636e\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u4efb\u52a1\u3002", "motivation": "\u63a2\u7d22RL\u65b9\u6cd5\u80fd\u5426\u8d4b\u4e88LLMs\u66f4\u7ec6\u817b\u7684\u793e\u4f1a\u667a\u80fd\uff08\u5982ToM\uff09\u3002", "method": "\u4f7f\u7528RLVR\u8bad\u7ec3\u5c0f\u89c4\u6a21LLMs\uff0c\u5e76\u5728\u591a\u79cdToM\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5c0f\u89c4\u6a21LLMs\u65e0\u6cd5\u83b7\u5f97\u901a\u7528ToM\u80fd\u529b\uff0c\u4ec5\u80fd\u8fc7\u62df\u5408\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "RLVR\u8bad\u7ec3\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\uff0c\u672a\u80fd\u771f\u6b63\u638c\u63e1\u62bd\u8c61ToM\u80fd\u529b\u3002"}}
{"id": "2507.15628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15628", "abs": "https://arxiv.org/abs/2507.15628", "authors": ["Shanjiang Tang", "Rui Huang", "Hsinyu Luo", "Chunjiang Wang", "Ce Yu", "Yusen Li", "Hao Fu", "Chao Sun", "and Jian Xiao"], "title": "A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications", "comment": null, "summary": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u89c6\u9891\u5206\u6790\u4e2d\u7684\u6548\u7387\u4f18\u5316\u6280\u672f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u51c6\u786e\u6027\u4f18\u5316\u7684\u7a7a\u767d\u3002", "motivation": "\u89c6\u9891\u6570\u636e\u7684\u7206\u70b8\u6027\u589e\u957f\u5bf9\u89c6\u9891\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u63d0\u51fa\u4e86\u66f4\u9ad8\u8981\u6c42\uff0c\u800cDNN\u7684\u6548\u7387\u4f18\u5316\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u5f0f\u7ec4\u7ec7\u73b0\u6709\u65b9\u6cd5\uff0c\u6db5\u76d6\u786c\u4ef6\u652f\u6301\u3001\u6570\u636e\u5904\u7406\u3001\u64cd\u4f5c\u90e8\u7f72\u7b49\u591a\u89c6\u89d2\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5316\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u73b0\u6709\u5de5\u4f5c\u5206\u6790\u4e86DNN\u89c6\u9891\u5206\u6790\u6027\u80fd\u4f18\u5316\u7684\u95ee\u9898\u4e0e\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3aDNN\u5728\u89c6\u9891\u5206\u6790\u4e2d\u7684\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u5168\u9762\u7efc\u8ff0\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.15816", "categories": ["cs.LG", "cs.IT", "cs.NI", "eess.SP", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.15816", "abs": "https://arxiv.org/abs/2507.15816", "authors": ["Yujia Mu", "Cong Shen"], "title": "Federated Split Learning with Improved Communication and Storage Efficiency", "comment": "Accepted for publication in IEEE Transactions on Mobile Computing", "summary": "Federated learning (FL) is one of the popular distributed machine learning\n(ML) solutions but incurs significant communication and computation costs at\nedge devices. Federated split learning (FSL) can train sub-models in parallel\nand reduce the computational burden of edge devices by splitting the model\narchitecture. However, it still requires a high communication overhead due to\ntransmitting the smashed data and gradients between clients and the server in\nevery global round. Furthermore, the server must maintain separate partial\nmodels for every client, leading to a significant storage requirement. To\naddress these challenges, this paper proposes a novel communication and storage\nefficient federated split learning method, termed CSE-FSL, which utilizes an\nauxiliary network to locally update the weights of the clients while keeping a\nsingle model at the server, hence avoiding frequent transmissions of gradients\nfrom the server and greatly reducing the storage requirement of the server.\nAdditionally, a new model update method of transmitting the smashed data in\nselected epochs can reduce the amount of smashed data sent from the clients. We\nprovide a theoretical analysis of CSE-FSL, rigorously guaranteeing its\nconvergence under non-convex loss functions. The extensive experimental results\nfurther indicate that CSE-FSL achieves a significant communication reduction\nover existing FSL solutions using real-world FL tasks.", "AI": {"tldr": "CSE-FSL\u662f\u4e00\u79cd\u9ad8\u6548\u901a\u4fe1\u548c\u5b58\u50a8\u7684\u8054\u90a6\u5206\u88c2\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f85\u52a9\u7f51\u7edc\u51cf\u5c11\u5ba2\u6237\u7aef\u4e0e\u670d\u52a1\u5668\u95f4\u7684\u6570\u636e\u4f20\u8f93\u548c\u670d\u52a1\u5668\u5b58\u50a8\u9700\u6c42\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5206\u88c2\u5b66\u4e60\uff08FSL\uff09\u65b9\u6cd5\u5728\u901a\u4fe1\u548c\u5b58\u50a8\u65b9\u9762\u5f00\u9500\u8f83\u5927\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faCSE-FSL\uff0c\u5229\u7528\u8f85\u52a9\u7f51\u7edc\u672c\u5730\u66f4\u65b0\u5ba2\u6237\u7aef\u6743\u91cd\uff0c\u670d\u52a1\u5668\u4ec5\u7ef4\u62a4\u5355\u4e00\u6a21\u578b\uff0c\u5e76\u9009\u62e9\u6027\u4f20\u8f93\u6570\u636e\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u8868\u660e\uff0cCSE-FSL\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5728\u771f\u5b9e\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "CSE-FSL\u6709\u6548\u89e3\u51b3\u4e86FSL\u7684\u901a\u4fe1\u548c\u5b58\u50a8\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15633", "categories": ["cs.CV", "I.2.10; I.4.8; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.15633", "abs": "https://arxiv.org/abs/2507.15633", "authors": ["Sachin Sharma", "Federico Simonetta", "Michele Flammini"], "title": "Experimenting active and sequential learning in a medieval music manuscript", "comment": "6 pages, 4 figures, accepted at IEEE MLSP 2025 (IEEE International\n  Workshop on Machine Learning for Signal Processing). Special Session:\n  Applications of AI in Cultural and Artistic Heritage", "summary": "Optical Music Recognition (OMR) is a cornerstone of music digitization\ninitiatives in cultural heritage, yet it remains limited by the scarcity of\nannotated data and the complexity of historical manuscripts. In this paper, we\npresent a preliminary study of Active Learning (AL) and Sequential Learning\n(SL) tailored for object detection and layout recognition in an old medieval\nmusic manuscript. Leveraging YOLOv8, our system selects samples with the\nhighest uncertainty (lowest prediction confidence) for iterative labeling and\nretraining. Our approach starts with a single annotated image and successfully\nboosts performance while minimizing manual labeling. Experimental results\nindicate that comparable accuracy to fully supervised training can be achieved\nwith significantly fewer labeled examples. We test the methodology as a\npreliminary investigation on a novel dataset offered to the community by the\nAnonymous project, which studies laude, a poetical-musical genre spread across\nItaly during the 12th-16th Century. We show that in the manuscript at-hand,\nuncertainty-based AL is not effective and advocates for more usable methods in\ndata-scarcity scenarios.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e3b\u52a8\u5b66\u4e60\uff08AL\uff09\u548c\u987a\u5e8f\u5b66\u4e60\uff08SL\uff09\u5728\u53e4\u4e50\u8c31\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684AL\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u53e4\u4e50\u8c31\u8bc6\u522b\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u590d\u6742\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u9ad8\u6548\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528YOLOv8\uff0c\u9009\u62e9\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6700\u4f4e\u7684\u6837\u672c\u8fdb\u884c\u8fed\u4ee3\u6807\u6ce8\u548c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u9700\u5c11\u91cf\u6807\u6ce8\u5373\u53ef\u8fbe\u5230\u5168\u76d1\u7763\u8bad\u7ec3\u7684\u7cbe\u5ea6\uff0c\u4f46\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684AL\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\uff0c\u9700\u5f00\u53d1\u66f4\u5b9e\u7528\u7684\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.15832", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15832", "abs": "https://arxiv.org/abs/2507.15832", "authors": ["Shiyang Li"], "title": "Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction", "comment": "in Chinese language", "summary": "To address the limitations of medium- and long-term four-dimensional (4D)\ntrajectory prediction models, this paper proposes a hybrid\nCNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy\nimproved snake-herd optimization (SO) algorithm. The model applies the Adaboost\nalgorithm to divide multiple weak learners, and each submodel utilizes CNN to\nextract spatial features, LSTM to capture temporal features, and attention\nmechanism to capture global features comprehensively. The strong learner model,\ncombined with multiple sub-models, then optimizes the hyperparameters of the\nprediction model through the natural selection behavior pattern simulated by\nSO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the\ncomparison experiments and ablation studies of multiple optimizers are carried\nout, and a comprehensive test and evaluation analysis is carried out. The\nresults show that SO-CLA-adaboost outperforms traditional optimizers such as\nparticle swarm, whale, and gray wolf in handling large-scale high-dimensional\ntrajectory data. In addition, introducing the full-strategy collaborative\nimprovement SO algorithm improves the model's prediction accuracy by 39.89%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408CNN-LSTM-attention-adaboost\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u7b56\u7565\u6539\u8fdb\u7684\u86c7\u7fa4\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e2d\u957f\u671f4D\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u4e2d\u957f\u671f4D\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u6539\u8fdb\u9884\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408Adaboost\u7b97\u6cd5\u5206\u914d\u591a\u4e2a\u5f31\u5b66\u4e60\u5668\uff0c\u6bcf\u4e2a\u5b50\u6a21\u578b\u4f7f\u7528CNN\u63d0\u53d6\u7a7a\u95f4\u7279\u5f81\u3001LSTM\u6355\u6349\u65f6\u95f4\u7279\u5f81\u3001\u6ce8\u610f\u529b\u673a\u5236\u6355\u83b7\u5168\u5c40\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u86c7\u7fa4\u4f18\u5316\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSO-CLA-adaboost\u5728\u5904\u7406\u5927\u89c4\u6a21\u9ad8\u7ef4\u8f68\u8ff9\u6570\u636e\u65f6\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u5668\uff0c\u9884\u6d4b\u7cbe\u5ea6\u63d0\u534739.89%\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e864D\u8f68\u8ff9\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.15636", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15636", "abs": "https://arxiv.org/abs/2507.15636", "authors": ["Lisan Al Amin", "Md. Ismail Hossain", "Thanh Thi Nguyen", "Tasnim Jahan", "Mahbubul Islam", "Faisal Quader"], "title": "Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis", "comment": "Accepted for publication at the 2025 IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC)", "summary": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5f69\u7968\u5047\u8bbe\uff08LTH\uff09\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u5173\u952e\u5b50\u7f51\u7edc\uff08winning tickets\uff09\u80fd\u5728\u9ad8\u7a00\u758f\u5ea6\u4e0b\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5a01\u80c1\u4fe1\u606f\u5b8c\u6574\u6027\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u6a21\u578b\u5e9e\u5927\u4e14\u673a\u5236\u4e0d\u660e\u786e\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "\u901a\u8fc7LTH\u548c\u8fed\u4ee3\u5e45\u5ea6\u526a\u679d\u65b9\u6cd5\uff0c\u5728MesoNet\u3001CNN-5\u548cResNet-18\u67b6\u6784\u4e0a\u5b9e\u9a8c\uff0c\u7ed3\u5408Grad-CAM\u53ef\u89c6\u5316\u5206\u6790\u3002", "result": "MesoNet\u572880%\u7a00\u758f\u5ea6\u4e0b\u4fdd\u630156.2%\u51c6\u786e\u7387\uff08\u57fa\u7ebf62.6%\uff09\uff0c\u4ec5\u97003,000\u53c2\u6570\uff1bLTH\u526a\u679d\u4f18\u4e8e\u4e00\u6b21\u6027\u526a\u679d\u3002", "conclusion": "LTH\u65b9\u6cd5\u53ef\u9ad8\u6548\u526a\u679d\u6a21\u578b\uff0c\u4fdd\u6301\u68c0\u6d4b\u6027\u80fd\uff0c\u4e14\u5173\u952e\u5b50\u7f51\u7edc\u5177\u6709\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u6f5c\u529b\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2507.15836", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.15836", "abs": "https://arxiv.org/abs/2507.15836", "authors": ["Matteo Boglioni", "Terrance Liu", "Andrew Ilyas", "Zhiwei Steven Wu"], "title": "Optimizing Canaries for Privacy Auditing with Metagradient Descent", "comment": null, "summary": "In this work we study black-box privacy auditing, where the goal is to lower\nbound the privacy parameter of a differentially private learning algorithm\nusing only the algorithm's outputs (i.e., final trained model). For DP-SGD (the\nmost successful method for training differentially private deep learning\nmodels), the canonical approach auditing uses membership inference-an auditor\ncomes with a small set of special \"canary\" examples, inserts a random subset of\nthem into the training set, and then tries to discern which of their canaries\nwere included in the training set (typically via a membership inference\nattack). The auditor's success rate then provides a lower bound on the privacy\nparameters of the learning algorithm. Our main contribution is a method for\noptimizing the auditor's canary set to improve privacy auditing, leveraging\nrecent work on metagradient optimization. Our empirical evaluation demonstrates\nthat by using such optimized canaries, we can improve empirical lower bounds\nfor differentially private image classification models by over 2x in certain\ninstances. Furthermore, we demonstrate that our method is transferable and\nefficient: canaries optimized for non-private SGD with a small model\narchitecture remain effective when auditing larger models trained with DP-SGD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u5dee\u5206\u9690\u79c1\u5ba1\u8ba1\u4e2d\u201c\u91d1\u4e1d\u96c0\u201d\u6837\u672c\u96c6\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9690\u79c1\u53c2\u6570\u7684\u4e0b\u754c\u4f30\u8ba1\u3002", "motivation": "\u7814\u7a76\u9ed1\u76d2\u9690\u79c1\u5ba1\u8ba1\uff0c\u65e8\u5728\u4ec5\u901a\u8fc7\u7b97\u6cd5\u8f93\u51fa\uff08\u5982\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff09\u6765\u4f30\u8ba1\u5dee\u5206\u9690\u79c1\u5b66\u4e60\u7b97\u6cd5\u7684\u9690\u79c1\u53c2\u6570\u3002", "method": "\u5229\u7528\u5143\u68af\u5ea6\u4f18\u5316\u6280\u672f\u4f18\u5316\u5ba1\u8ba1\u4e2d\u7684\u91d1\u4e1d\u96c0\u6837\u672c\u96c6\uff0c\u63d0\u5347\u5ba1\u8ba1\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u5316\u540e\u7684\u91d1\u4e1d\u96c0\u6837\u672c\u96c6\u53ef\u5c06\u9690\u79c1\u53c2\u6570\u7684\u4e0b\u754c\u4f30\u8ba1\u63d0\u9ad82\u500d\u4ee5\u4e0a\uff0c\u4e14\u5177\u6709\u53ef\u8fc1\u79fb\u6027\u548c\u9ad8\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5dee\u5206\u9690\u79c1\u5ba1\u8ba1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u9690\u79c1\u7ea7\u522b\u7684\u6a21\u578b\u3002"}}
{"id": "2507.15652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15652", "abs": "https://arxiv.org/abs/2507.15652", "authors": ["Haoran Zhou", "Zihan Zhang", "Hao Chen"], "title": "Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.", "AI": {"tldr": "EVA\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e2d\u95f4\u5c42\u63d0\u53d6\u89c6\u89c9\u4e8b\u5b9e\u77e5\u8bc6\uff0c\u663e\u8457\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3MLLMs\u4e2d\u56e0\u5148\u9a8c\u77e5\u8bc6\u6291\u5236\u89c6\u89c9\u4fe1\u606f\u5bfc\u81f4\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faEVA\u65b9\u6cd5\uff0c\u52a8\u6001\u9009\u62e9\u4e2d\u95f4\u5c42\u5e76\u5bf9\u6bd4\u539f\u59cb\u8f93\u5165\u4e0e\u7eaf\u6587\u672c\u8f93\u5165\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u63d0\u53d6\u89c6\u89c9\u4e8b\u5b9e\u77e5\u8bc6\u3002", "result": "EVA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\u3002", "conclusion": "EVA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4e0d\u540cMLLMs\u3002"}}
{"id": "2507.15839", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15839", "abs": "https://arxiv.org/abs/2507.15839", "authors": ["Anh Nguyen", "Sam Schafft", "Nicholas Hale", "John Alfaro"], "title": "FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs", "comment": null, "summary": "Synthetic data generation has emerged as an invaluable solution in scenarios\nwhere real-world data collection and usage are limited by cost and scarcity.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nproducing high-fidelity, domain-relevant samples across various fields.\nHowever, existing approaches that directly use LLMs to generate each record\nindividually impose prohibitive time and cost burdens, particularly when large\nvolumes of synthetic data are required. In this work, we propose a fast,\ncost-effective method for realistic tabular data synthesis that leverages LLMs\nto infer and encode each field's distribution into a reusable sampling script.\nBy automatically classifying fields into numerical, categorical, or free-text\ntypes, the LLM generates distribution-based scripts that can efficiently\nproduce diverse, realistic datasets at scale without continuous model\ninference. Experimental results show that our approach outperforms traditional\ndirect methods in both diversity and data realism, substantially reducing the\nburden of high-volume synthetic data generation. We plan to apply this\nmethodology to accelerate testing in production pipelines, thereby shortening\ndevelopment cycles and improving overall system efficiency. We believe our\ninsights and lessons learned will aid researchers and practitioners seeking\nscalable, cost-effective solutions for synthetic data generation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528LLM\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u8868\u683c\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f16\u7801\u5b57\u6bb5\u5206\u5e03\u4e3a\u53ef\u91cd\u7528\u811a\u672c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u65f6\u95f4\u548c\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u4f7f\u7528LLM\u751f\u6210\u6bcf\u6761\u8bb0\u5f55\u65f6\u7684\u9ad8\u65f6\u95f4\u548c\u6210\u672c\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5927\u91cf\u5408\u6210\u6570\u636e\u65f6\u3002", "method": "\u5229\u7528LLM\u63a8\u65ad\u5e76\u7f16\u7801\u5b57\u6bb5\u5206\u5e03\u4e3a\u53ef\u91cd\u7528\u91c7\u6837\u811a\u672c\uff0c\u81ea\u52a8\u5206\u7c7b\u5b57\u6bb5\u7c7b\u578b\u4ee5\u9ad8\u6548\u751f\u6210\u6570\u636e\u3002", "result": "\u5728\u591a\u6837\u6027\u548c\u6570\u636e\u771f\u5b9e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u9ad8\u5bb9\u91cf\u5408\u6210\u6570\u636e\u751f\u6210\u7684\u8d1f\u62c5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u52a0\u901f\u751f\u4ea7\u7ba1\u9053\u7684\u6d4b\u8bd5\uff0c\u7f29\u77ed\u5f00\u53d1\u5468\u671f\uff0c\u63d0\u9ad8\u7cfb\u7edf\u6548\u7387\uff0c\u4e3a\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15655", "abs": "https://arxiv.org/abs/2507.15655", "authors": ["Aniket Pal", "Ajoy Mondal", "Minesh Mathew", "C. V. Jawahar"], "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark", "comment": "This is a minor revision of the original paper submitted to IJDAR", "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.", "AI": {"tldr": "HW-MLVQA\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u624b\u5199\u6587\u6863\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b1600\u9875\u624b\u5199\u6587\u6863\u548c2400\u4e2a\u95ee\u7b54\u5bf9\uff0c\u65e8\u5728\u586b\u8865\u591a\u8bed\u8a00\u624b\u5199\u6587\u6863\u7406\u89e3\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6a21\u578b\u5728\u5904\u7406\u591a\u6837\u624b\u5199\u6587\u6863\u65f6\u80fd\u529b\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u771f\u5b9e\u7684\u591a\u8bed\u8a00\u624b\u5199\u6587\u6863\u7406\u89e3\u57fa\u51c6\u3002", "method": "HW-MLVQA\u63d0\u4f9b\u6587\u672c\u3001\u56fe\u50cf\u53ca\u56fe\u6587\u7ed3\u5408\u4e09\u79cd\u6a21\u6001\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u6d4b\u8bd5\u4e13\u6709\u548c\u5f00\u6e90OCR\u6a21\u578b\u3002", "result": "\u57fa\u51c6\u5305\u542b1600\u9875\u624b\u5199\u6587\u6863\u548c2400\u4e2a\u95ee\u7b54\u5bf9\uff0c\u652f\u6301\u591a\u6a21\u6001\u8bc4\u4f30\u3002", "conclusion": "HW-MLVQA\u65e8\u5728\u63a8\u52a8\u591a\u8bed\u8a00\u624b\u5199\u6587\u6863\u7406\u89e3\u7684\u7814\u7a76\u548c\u521b\u65b0\u3002"}}
{"id": "2507.15846", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15846", "abs": "https://arxiv.org/abs/2507.15846", "authors": ["Fei Tang", "Zhangxuan Gu", "Zhengxi Lu", "Xuyang Liu", "Shuheng Shen", "Changhua Meng", "Wen Wang", "Wenqi Zhang", "Yongliang Shen", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding", "comment": null, "summary": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u5206\u5e03\u7684GUI\u5b9a\u4f4d\u5956\u52b1\u6846\u67b6GUI-G\u00b2\uff0c\u901a\u8fc7\u8fde\u7eed\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u4e92\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f7f\u7528\u4e8c\u5143\u5956\u52b1\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u4ea4\u4e92\u7684\u8fde\u7eed\u6027\uff0c\u800c\u4eba\u7c7b\u70b9\u51fb\u884c\u4e3a\u81ea\u7136\u5f62\u6210\u9ad8\u65af\u5206\u5e03\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7cbe\u786e\u7684\u5956\u52b1\u6846\u67b6\u3002", "method": "GUI-G\u00b2\u7ed3\u5408\u9ad8\u65af\u70b9\u5956\u52b1\u548c\u8986\u76d6\u5956\u52b1\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u65b9\u5dee\u673a\u5236\u5904\u7406\u4e0d\u540c\u5143\u7d20\u5c3a\u5ea6\uff0c\u5c06GUI\u5b9a\u4f4d\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGUI-G\u00b2\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u534724.7%\uff0c\u5e76\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fde\u7eed\u5efa\u6a21\u4e3aGUI\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.15680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15680", "abs": "https://arxiv.org/abs/2507.15680", "authors": ["Yongkang Hou", "Jiarun Song"], "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "comment": null, "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\uff08IQA\uff09\uff0c\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3CLIP\u5728IQA\u4efb\u52a1\u4e2d\u53c2\u6570\u8fc7\u591a\u548c\u5c40\u90e8\u5931\u771f\u7279\u5f81\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u8d28\u91cf\u5206\u7ea7\u63d0\u793a\u6a21\u677f\uff0c\u5fae\u8c03CLIP\uff0c\u5e76\u63d0\u51fa\u6a21\u6001\u81ea\u9002\u5e94\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2aIQA\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728IQA\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15683", "abs": "https://arxiv.org/abs/2507.15683", "authors": ["Boni Hu", "Zhenyu Xia", "Lin Chen", "Pengcheng Han", "Shuhui Bu"], "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing", "comment": "17 pages, 11 figures", "summary": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera\npose from query images, is fundamental to remote sensing and UAV applications.\nExisting methods face inherent trade-offs: image-based retrieval and pose\nregression approaches lack precision, while structure-based methods that\nregister queries to Structure-from-Motion (SfM) models suffer from\ncomputational complexity and limited scalability. These challenges are\nparticularly pronounced in remote sensing scenarios due to large-scale scenes,\nhigh altitude variations, and domain gaps of existing visual priors. To\novercome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel\nscene representation that compactly encodes both 3D geometry and appearance. We\nintroduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework\nthat follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting\nthe rich semantic information and geometric constraints inherent in Gaussian\nprimitives. To handle large-scale remote sensing scenarios, we incorporate\npartitioned Gaussian training, GPU-accelerated parallel matching, and dynamic\nmemory management strategies. Our approach consists of two stages: (1) a sparse\nstage featuring a Gaussian-specific consistent render-aware sampling strategy\nand landmark-guided detector for robust and accurate initial pose estimation,\nand (2) a dense stage that iteratively refines poses through coarse-to-fine\ndense rasterization matching while incorporating reliability verification.\nThrough comprehensive evaluation on simulation data, public datasets, and real\nflight experiments, we demonstrate that our method delivers competitive\nlocalization accuracy, recall rate, and computational efficiency while\neffectively filtering unreliable pose estimates. The results confirm the\neffectiveness of our approach for practical remote sensing applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u53cc\u5c42\u6b21\u89c6\u89c9\u91cd\u5b9a\u4f4d\u6846\u67b6Hi\u00b2-GSLoc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0a\u7684\u6743\u8861\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u9065\u611f\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u91cd\u5b9a\u4f4d\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u77db\u76fe\uff0c\u9065\u611f\u573a\u666f\u7684\u5927\u89c4\u6a21\u3001\u9ad8\u6d77\u62d4\u53d8\u5316\u548c\u9886\u57df\u5dee\u8ddd\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e9b\u6311\u6218\u3002", "method": "Hi\u00b2-GSLoc\u91c7\u7528\u7a00\u758f\u5230\u5bc6\u96c6\u3001\u7c97\u5230\u7ec6\u7684\u53cc\u5c42\u6b21\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u533a\u9ad8\u65af\u8bad\u7ec3\u3001GPU\u52a0\u901f\u5e76\u884c\u5339\u914d\u548c\u52a8\u6001\u5185\u5b58\u7ba1\u7406\u7b56\u7565\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\uff08\u7a00\u758f\u521d\u59cb\u4f30\u8ba1\u548c\u5bc6\u96c6\u8fed\u4ee3\u4f18\u5316\uff09\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u5b9a\u4f4d\u3002", "result": "\u5728\u4eff\u771f\u6570\u636e\u3001\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u98de\u884c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u80fd\u6709\u6548\u8fc7\u6ee4\u4e0d\u53ef\u9760\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002", "conclusion": "Hi\u00b2-GSLoc\u4e3a\u5b9e\u9645\u9065\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89c6\u89c9\u91cd\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15686", "abs": "https://arxiv.org/abs/2507.15686", "authors": ["Wenjie Huang", "Qi Yang", "Shuting Xia", "He Huang", "Zhu Li", "Yiling Xu"], "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression", "comment": "Accepted to ICCV 2025", "summary": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.", "AI": {"tldr": "LINR-PCGC\u662f\u9996\u4e2a\u57fa\u4e8eINR\u7684\u65e0\u635f\u70b9\u4e91\u51e0\u4f55\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u7ec4\u7f16\u7801\u6846\u67b6\u548c\u591a\u5c3a\u5ea6SparseConv\u7f51\u7edc\uff0c\u663e\u8457\u63d0\u5347\u7f16\u7801\u901f\u5ea6\u548c\u538b\u7f29\u6548\u7387\u3002", "motivation": "\u73b0\u6709AI\u70b9\u4e91\u538b\u7f29\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5206\u5e03\uff0cINR\u65b9\u6cd5\u867d\u89e3\u51b3\u4e86\u5206\u5e03\u95ee\u9898\uff0c\u4f46\u4ec5\u652f\u6301\u6709\u635f\u538b\u7f29\u3002\u672c\u6587\u65e8\u5728\u5b9e\u73b0\u65e0\u635f\u538b\u7f29\u5e76\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5206\u7ec4\u7f16\u7801\u6846\u67b6\u548c\u7f51\u7edc\u521d\u59cb\u5316\u7b56\u7565\u4ee5\u51cf\u5c11\u7f16\u7801\u65f6\u95f4\uff1b\u8bbe\u8ba1\u57fa\u4e8e\u591a\u5c3a\u5ea6SparseConv\u7684\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u5305\u542b\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u63d0\u53d6\u3001\u5b50\u8282\u70b9\u9884\u6d4b\u548c\u6a21\u578b\u538b\u7f29\u6a21\u5757\u3002", "result": "\u5728MVUB\u6570\u636e\u96c6\u4e0a\uff0c\u6bd4\u7279\u6d41\u6bd4G-PCC TMC13v23\u51cf\u5c1121.21%\uff0c\u6bd4SparsePCGC\u51cf\u5c1121.95%\uff0c\u7f16\u7801\u65f6\u95f4\u51cf\u5c11\u7ea660%\u3002", "conclusion": "LINR-PCGC\u5728\u65e0\u635f\u538b\u7f29\u4e2d\u4f18\u4e8e\u4f20\u7edf\u548cAI\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6548\u7f16\u7801\u548c\u7d27\u51d1\u89e3\u7801\u5668\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2507.15690", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15690", "abs": "https://arxiv.org/abs/2507.15690", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting", "comment": "6 pages, 4 figures", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in\nreconstructing high-quality novel views, as it often overfits to the\nwidely-varying high-frequency (HF) details of the sparse training views. While\nfrequency regularization can be a promising approach, its typical reliance on\nFourier transforms causes difficult parameter tuning and biases towards\ndetrimental HF learning. We propose DWTGS, a framework that rethinks frequency\nregularization by leveraging wavelet-space losses that provide additional\nspatial supervision. Specifically, we supervise only the low-frequency (LF) LL\nsubbands at multiple DWT levels, while enforcing sparsity on the HF HH subband\nin a self-supervised manner. Experiments across benchmarks show that DWTGS\nconsistently outperforms Fourier-based counterparts, as this LF-centric\nstrategy improves generalization and reduces HF hallucinations.", "AI": {"tldr": "DWTGS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u7a7a\u95f4\u635f\u5931\u7684\u65b0\u578b\u9891\u7387\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u4f4e\u9891\u5b50\u5e26\u5e76\u81ea\u76d1\u7763\u9ad8\u9891\u5b50\u5e26\uff0c\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6cfc\u6e85\u5bb9\u6613\u8fc7\u62df\u5408\u9ad8\u9891\u7ec6\u8282\uff0c\u73b0\u6709\u9891\u7387\u6b63\u5219\u5316\u65b9\u6cd5\u4f9d\u8d56\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u53c2\u6570\u8c03\u4f18\u56f0\u96be\u4e14\u6613\u504f\u5411\u6709\u5bb3\u9ad8\u9891\u5b66\u4e60\u3002", "method": "\u5229\u7528\u5c0f\u6ce2\u7a7a\u95f4\u635f\u5931\u63d0\u4f9b\u7a7a\u95f4\u76d1\u7763\uff0c\u4ec5\u76d1\u7763\u4f4e\u9891LL\u5b50\u5e26\uff0c\u540c\u65f6\u5bf9\u9ad8\u9891HH\u5b50\u5e26\u65bd\u52a0\u7a00\u758f\u6027\u81ea\u76d1\u7763\u3002", "result": "DWTGS\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u7684\u65b9\u6cd5\uff0c\u4f4e\u9891\u7b56\u7565\u6539\u5584\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u9ad8\u9891\u5e7b\u89c9\u3002", "conclusion": "DWTGS\u901a\u8fc7\u5c0f\u6ce2\u7a7a\u95f4\u635f\u5931\u91cd\u65b0\u601d\u8003\u9891\u7387\u6b63\u5219\u5316\uff0c\u4e3a\u7a00\u758f\u89c6\u56fe3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15709", "abs": "https://arxiv.org/abs/2507.15709", "authors": ["Wei Sun", "Weixia Zhang", "Linhan Cao", "Jun Jia", "Xiangyang Zhu", "Dandan Zhu", "Xiongkuo Min", "Guangtao Zhai"], "title": "Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation", "comment": "Efficient-FIQA achieved first place in the ICCV VQualA 2025 Face\n  Image Quality Assessment Challenge", "summary": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6a21\u578b\u548c\u81ea\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709FIQA\u7b97\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u5148\u8bad\u7ec3\u5f3a\u5927\u7684\u6559\u5e08\u6a21\u578b\uff0c\u518d\u901a\u8fc7\u81ea\u8bad\u7ec3\u548c\u77e5\u8bc6\u84b8\u998f\u5f97\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u3002", "result": "\u5b66\u751f\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\uff0c\u5e76\u5728ICCV 2025 VQualA FIQA\u6311\u6218\u8d5b\u4e2d\u593a\u51a0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.15724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15724", "abs": "https://arxiv.org/abs/2507.15724", "authors": ["Guoxuan Xia", "Harleen Hanspal", "Petru-Daniel Tudosiu", "Shifeng Zhang", "Sarah Parisot"], "title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers", "comment": "preprint", "summary": "Enabling image generation models to be spatially controlled is an important\narea of research, empowering users to better generate images according to their\nown fine-grained specifications via e.g. edge maps, poses. Although this task\nhas seen impressive improvements in recent times, a focus on rapidly producing\nstronger models has come at the cost of detailed and fair scientific\ncomparison. Differing training data, model architectures and generation\nparadigms make it difficult to disentangle the factors contributing to\nperformance. Meanwhile, the motivations and nuances of certain approaches\nbecome lost in the literature. In this work, we aim to provide clear takeaways\nacross generation paradigms for practitioners wishing to develop\ntransformer-based systems for spatially-controlled generation, clarifying the\nliterature and addressing knowledge gaps. We perform controlled experiments on\nImageNet across diffusion-based/flow-based and autoregressive (AR) models.\nFirst, we establish control token prefilling as a simple, general and\nperformant baseline approach for transformers. We then investigate previously\nunderexplored sampling time enhancements, showing that extending\nclassifier-free guidance to control, as well as softmax truncation, have a\nstrong impact on control-generation consistency. Finally, we re-clarify the\nmotivation of adapter-based approaches, demonstrating that they mitigate\n\"forgetting\" and maintain generation quality when trained on limited downstream\ndata, but underperform full training in terms of generation-control\nconsistency. Code will be released upon publication.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7a7a\u95f4\u63a7\u5236\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u6f84\u6e05\u4e86\u4e0d\u540c\u751f\u6210\u8303\u5f0f\u7684\u4f18\u52a3\uff0c\u5e76\u63d0\u51fa\u4e86\u7b80\u5355\u9ad8\u6548\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7a7a\u95f4\u63a7\u5236\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u79d1\u5b66\u6bd4\u8f83\u7684\u95ee\u9898\uff0c\u6f84\u6e05\u6587\u732e\u4e2d\u7684\u77e5\u8bc6\u7a7a\u767d\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u6e05\u6670\u6307\u5bfc\u3002", "method": "\u5728ImageNet\u4e0a\u8fdb\u884c\u4e86\u6269\u6563\u6a21\u578b\u3001\u6d41\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4e86\u63a7\u5236\u6807\u8bb0\u9884\u586b\u5145\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63a2\u7d22\u4e86\u91c7\u6837\u65f6\u95f4\u589e\u5f3a\u6280\u672f\u3002", "result": "\u63a7\u5236\u6807\u8bb0\u9884\u586b\u5145\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\uff0c\u6269\u5c55\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u548csoftmax\u622a\u65ad\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u4e0e\u751f\u6210\u7684\u4e00\u81f4\u6027\u3002\u9002\u914d\u5668\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0b\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u4f46\u4e00\u81f4\u6027\u8f83\u5dee\u3002", "conclusion": "\u8bba\u6587\u4e3a\u7a7a\u95f4\u63a7\u5236\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u63a7\u5236\u6807\u8bb0\u9884\u586b\u5145\u548c\u91c7\u6837\u589e\u5f3a\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u6f84\u6e05\u4e86\u9002\u914d\u5668\u65b9\u6cd5\u7684\u9002\u7528\u573a\u666f\u3002"}}
{"id": "2507.15728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15728", "abs": "https://arxiv.org/abs/2507.15728", "authors": ["Wenqi Ouyang", "Zeqi Xiao", "Danni Yang", "Yifan Zhou", "Shuai Yang", "Lei Yang", "Jianlou Si", "Xingang Pan"], "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation", "comment": "Project page: https://vicky0522.github.io/tokensgen-webpage/", "summary": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .", "AI": {"tldr": "TokensGen\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u4ee4\u724c\u89e3\u51b3\u957f\u89c6\u9891\u751f\u6210\u7684\u8bb0\u5fc6\u74f6\u9888\u548c\u957f\u671f\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u957f\u89c6\u9891\u65f6\u7684\u5185\u5b58\u95ee\u9898\u548c\u957f\u671f\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u5206\u4e24\u9636\u6bb5\uff1a1) To2V\u6a21\u578b\u751f\u6210\u77ed\u89c6\u9891\uff1b2) T2To\u6a21\u578b\u751f\u6210\u5168\u5c40\u4e00\u81f4\u7684\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7FIFO-Diffusion\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u65f6\u95f4\u548c\u5185\u5bb9\u4e00\u81f4\u6027\uff0c\u8ba1\u7b97\u5f00\u9500\u53ef\u63a7\u3002", "conclusion": "TokensGen\u4e3a\u957f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.15748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15748", "abs": "https://arxiv.org/abs/2507.15748", "authors": ["Jisu Shin", "Richard Shaw", "Seunghyun Shin", "Anton Pelykh", "Zhensong Zhang", "Hae-Gon Jeon", "Eduardo Perez-Pellitero"], "title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS", "comment": "10 pages, 3 figures, NeurIPS 2025 under review", "summary": "Modern camera pipelines apply extensive on-device processing, such as\nexposure adjustment, white balance, and color correction, which, while\nbeneficial individually, often introduce photometric inconsistencies across\nviews. These appearance variations violate multi-view consistency and degrade\nthe quality of novel view synthesis. Joint optimization of scene\nrepresentations and per-image appearance embeddings has been proposed to\naddress this issue, but at the cost of increased computational complexity and\nslower training. In this work, we propose a transformer-based method that\npredicts spatially adaptive bilateral grids to correct photometric variations\nin a multi-view consistent manner, enabling robust cross-scene generalization\nwithout the need for scene-specific retraining. By incorporating the learned\ngrids into the 3D Gaussian Splatting pipeline, we improve reconstruction\nquality while maintaining high training efficiency. Extensive experiments show\nthat our approach outperforms or matches existing scene-specific optimization\nmethods in reconstruction fidelity and convergence speed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u7a7a\u95f4\u81ea\u9002\u5e94\u7684\u53cc\u8fb9\u7f51\u683c\u6765\u6821\u6b63\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u4e2d\u7684\u5149\u5ea6\u53d8\u5316\uff0c\u65e0\u9700\u573a\u666f\u7279\u5b9a\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u73b0\u4ee3\u76f8\u673a\u6d41\u6c34\u7ebf\u4e2d\u7684\u5904\u7406\uff08\u5982\u66dd\u5149\u8c03\u6574\u3001\u767d\u5e73\u8861\u7b49\uff09\u4f1a\u5bfc\u81f4\u591a\u89c6\u89d2\u95f4\u7684\u5149\u5ea6\u4e0d\u4e00\u81f4\uff0c\u5f71\u54cd\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002", "method": "\u4f7f\u7528Transformer\u9884\u6d4b\u53cc\u8fb9\u7f51\u683c\uff0c\u6821\u6b63\u5149\u5ea6\u53d8\u5316\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u52303D\u9ad8\u65af\u6cfc\u6e85\u6d41\u6c34\u7ebf\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6\u4e0a\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u573a\u666f\u7279\u5b9a\u4f18\u5316\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u8de8\u573a\u666f\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u901f\u5ea6\u3002"}}
{"id": "2507.15765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15765", "abs": "https://arxiv.org/abs/2507.15765", "authors": ["Feng-Qi Cui", "Anyang Tong", "Jinyang Huang", "Jie Zhang", "Dan Guo", "Zhi Liu", "Meng Wang"], "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization", "comment": "Accepted by ACM MM'25", "summary": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHDF\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u6a21\u5757\u589e\u5f3a\u65f6\u95f4-\u9891\u7387\u5efa\u6a21\u5e76\u4f18\u5316\u6837\u672c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6e90\u6570\u636e\u548c\u4e2a\u4f53\u8868\u60c5\u53d8\u5f02\u6027\u5bfc\u81f4\u7684\u6837\u672c\u5f02\u8d28\u6027\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u65f6\u95f4-\u9891\u7387\u5206\u5e03\u6ce8\u610f\u529b\u6a21\u5757\uff08DAM\uff09\u548c\u5206\u5e03\u611f\u77e5\u7f29\u653e\u6a21\u5757\uff08DSM\uff09\uff0c\u5206\u522b\u7528\u4e8e\u589e\u5f3a\u65f6\u95f4-\u9891\u7387\u5efa\u6a21\u548c\u52a8\u6001\u5e73\u8861\u5206\u7c7b\u4e0e\u5bf9\u6bd4\u635f\u5931\u3002", "result": "\u5728DFEW\u548cFERV39k\u6570\u636e\u96c6\u4e0a\uff0cHDF\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u53d6\u5f97\u4e86\u4f18\u5f02\u7684WAR\u548cUAR\u3002", "conclusion": "HDF\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6837\u672c\u5f02\u8d28\u6027\u548c\u4f18\u5316\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15777", "abs": "https://arxiv.org/abs/2507.15777", "authors": ["Junwen Wang", "Oscar MacCormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "title": "Label tree semantic losses for rich multi-class medical image segmentation", "comment": "arXiv admin note: text overlap with arXiv:2506.21150", "summary": "Rich and accurate medical image segmentation is poised to underpin the next\ngeneration of AI-defined clinical practice by delineating critical anatomy for\npre-operative planning, guiding real-time intra-operative navigation, and\nsupporting precise post-operative assessment. However, commonly used learning\nmethods for medical and surgical imaging segmentation tasks penalise all errors\nequivalently and thus fail to exploit any inter-class semantics in the labels\nspace. This becomes particularly problematic as the cardinality and richness of\nlabels increases to include subtly different classes. In this work, we propose\ntwo tree-based semantic loss functions which take advantage of a hierarchical\norganisation of the labels. We further incorporate our losses in a recently\nproposed approach for training with sparse, background-free annotations to\nextend the applicability of our proposed losses. Extensive experiments are\nreported on two medical and surgical image segmentation tasks, namely head MRI\nfor whole brain parcellation (WBP) with full supervision and neurosurgical\nhyperspectral imaging (HSI) for scene understanding with sparse annotations.\nResults demonstrate that our proposed method reaches state-of-the-art\nperformance in both cases.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6811\u7ed3\u6784\u7684\u8bed\u4e49\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528\u6807\u7b7e\u7684\u5c42\u6b21\u7ed3\u6784\u6539\u8fdb\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u5e76\u5728\u7a00\u758f\u6807\u6ce8\u4e0b\u6269\u5c55\u5e94\u7528\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u5bf9\u6240\u6709\u9519\u8bef\u7b49\u540c\u60e9\u7f5a\uff0c\u672a\u80fd\u5229\u7528\u6807\u7b7e\u7a7a\u95f4\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5c24\u5176\u5728\u6807\u7b7e\u4e30\u5bcc\u4e14\u7c7b\u522b\u5dee\u5f02\u7ec6\u5fae\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6811\u72b6\u8bed\u4e49\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u5c42\u6b21\u5316\u6807\u7b7e\u7ec4\u7ec7\uff0c\u5e76\u5e94\u7528\u4e8e\u7a00\u758f\u6807\u6ce8\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728\u5934\u90e8MRI\u5168\u8111\u5206\u533a\u548c\u795e\u7ecf\u5916\u79d1\u9ad8\u5149\u8c31\u6210\u50cf\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u8fbe\u5230SOTA\u3002", "conclusion": "\u6811\u72b6\u8bed\u4e49\u635f\u5931\u51fd\u6570\u6709\u6548\u5229\u7528\u6807\u7b7e\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2507.15793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15793", "abs": "https://arxiv.org/abs/2507.15793", "authors": ["Ghassen Baklouti", "Julio Silva-Rodr\u00edguez", "Jose Dolz", "Houda Bahig", "Ismail Ben Ayed"], "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "comment": "Accepted at MICCAI 2025", "summary": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is\nincreasingly attracting interest in medical imaging due to its effectiveness\nand computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)\nis a notable approach based on the assumption that the adaptation inherently\noccurs in a low-dimensional subspace. While it has shown good performance, its\nimplementation requires a fixed and unalterable rank, which might be\nchallenging to select given the unique complexities and requirements of each\nmedical imaging downstream task. Inspired by advancements in natural image\nprocessing, we introduce a novel approach for medical image segmentation that\ndynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank\nrepresentation of the trainable weight matrices as a singular value\ndecomposition, we introduce an l_1 sparsity regularizer to the loss function,\nand tackle it with a proximal optimizer. The regularizer could be viewed as a\npenalty on the decomposition rank. Hence, its minimization enables to find\ntask-adapted ranks automatically. Our method is evaluated in a realistic\nfew-shot fine-tuning setting, where we compare it first to the standard LoRA\nand then to several other PEFT methods across two distinguishable tasks: base\norgans and novel organs. Our extensive experiments demonstrate the significant\nperformance improvements driven by our method, highlighting its efficiency and\nrobustness against suboptimal rank initialization. Our code is publicly\navailable: https://github.com/ghassenbaklouti/ARENA", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7l1\u7a00\u758f\u6b63\u5219\u5316\u81ea\u52a8\u9009\u62e9\u4efb\u52a1\u9002\u5e94\u7684\u79e9\u3002", "motivation": "LoRA\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u9700\u8981\u56fa\u5b9a\u79e9\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u52a8\u6001\u8c03\u6574\u79e9\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165l1\u7a00\u758f\u6b63\u5219\u5316\u5230\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u901a\u8fc7\u8fd1\u7aef\u4f18\u5316\u5668\u52a8\u6001\u8c03\u6574\u4f4e\u79e9\u8868\u793a\u7684\u79e9\u3002", "result": "\u5728\u5c11\u6837\u672c\u5fae\u8c03\u8bbe\u7f6e\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6LoRA\u548c\u5176\u4ed6PEFT\u65b9\u6cd5\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u81ea\u52a8\u9002\u5e94\u4efb\u52a1\u9700\u6c42\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.15798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15798", "abs": "https://arxiv.org/abs/2507.15798", "authors": ["Lilian Hollard", "Lucas Mohimont", "Nathalie Gaveau", "Luiz-Angelo Steffenel"], "title": "Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models", "comment": null, "summary": "The paper investigates the performance of state-of-the-art low-parameter deep\nneural networks for computer vision, focusing on bottleneck architectures and\ntheir behavior using superlinear activation functions. We address interference\nin feature maps, a phenomenon associated with superposition, where neurons\nsimultaneously encode multiple characteristics. Our research suggests that\nlimiting interference can enhance scaling and accuracy in very low-scaled\nnetworks (under 1.5M parameters). We identify key design elements that reduce\ninterference by examining various bottleneck architectures, leading to a more\nefficient neural network. Consequently, we propose a proof-of-concept\narchitecture named NoDepth Bottleneck built on mechanistic insights from our\nexperiments, demonstrating robust scaling accuracy on the ImageNet dataset.\nThese findings contribute to more efficient and scalable neural networks for\nthe low-parameter range and advance the understanding of bottlenecks in\ncomputer vision. https://caiac.pubpub.org/pub/3dh6rsel", "AI": {"tldr": "\u7814\u7a76\u4f4e\u53c2\u6570\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6027\u80fd\uff0c\u63d0\u51fa\u51cf\u5c11\u7279\u5f81\u56fe\u5e72\u6270\u7684\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u65b0\u67b6\u6784\u3002", "motivation": "\u63a2\u7d22\u4f4e\u53c2\u6570\u795e\u7ecf\u7f51\u7edc\u4e2d\u7279\u5f81\u56fe\u5e72\u6270\u73b0\u8c61\uff0c\u4ee5\u63d0\u9ad8\u5c0f\u89c4\u6a21\u7f51\u7edc\u7684\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u5206\u6790\u4e0d\u540c\u74f6\u9888\u67b6\u6784\uff0c\u63d0\u51fa\u51cf\u5c11\u5e72\u6270\u7684\u8bbe\u8ba1\u5143\u7d20\uff0c\u5e76\u6784\u5efa\u4e86NoDepth Bottleneck\u67b6\u6784\u3002", "result": "\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u65b0\u67b6\u6784\u7684\u9c81\u68d2\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f4e\u53c2\u6570\u8303\u56f4\u7684\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63a8\u8fdb\u4e86\u5bf9\u74f6\u9888\u67b6\u6784\u7684\u7406\u89e3\u3002"}}
{"id": "2507.15803", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15803", "abs": "https://arxiv.org/abs/2507.15803", "authors": ["Danhui Chen", "Ziquan Liu", "Chuxi Yang", "Dan Wang", "Yan Yan", "Yi Xu", "Xiangyang Ji"], "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction", "comment": "ICCV 2025", "summary": "Pixel-level vision tasks, such as semantic segmentation, require extensive\nand high-quality annotated data, which is costly to obtain. Semi-supervised\nsemantic segmentation (SSSS) has emerged as a solution to alleviate the\nlabeling burden by leveraging both labeled and unlabeled data through\nself-training techniques. Meanwhile, the advent of foundational segmentation\nmodels pre-trained on massive data, has shown the potential to generalize\nacross domains effectively. This work explores whether a foundational\nsegmentation model can address label scarcity in the pixel-level vision task as\nan annotator for unlabeled images. Specifically, we investigate the efficacy of\nusing SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual\ninput, to generate predictive masks for unlabeled data. To address the\nshortcomings of using SEEM-generated masks as supervision, we propose\nConformalSAM, a novel SSSS framework which first calibrates the foundation\nmodel using the target domain's labeled data and then filters out unreliable\npixel labels of unlabeled data so that only high-confidence labels are used as\nsupervision. By leveraging conformal prediction (CP) to adapt foundation models\nto target data through uncertainty calibration, ConformalSAM exploits the\nstrong capability of the foundational segmentation model reliably which\nbenefits the early-stage learning, while a subsequent self-reliance training\nstrategy mitigates overfitting to SEEM-generated masks in the later training\nstage. Our experiment demonstrates that, on three standard benchmarks of SSSS,\nConformalSAM achieves superior performance compared to recent SSSS methods and\nhelps boost the performance of those methods as a plug-in.", "AI": {"tldr": "ConformalSAM\u5229\u7528\u57fa\u7840\u5206\u5272\u6a21\u578bSEEM\u751f\u6210\u672a\u6807\u6ce8\u6570\u636e\u7684\u9884\u6d4b\u63a9\u7801\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u81ea\u4f9d\u8d56\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u50cf\u7d20\u7ea7\u89c6\u89c9\u4efb\u52a1\u4e2d\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5229\u7528\u57fa\u7840\u5206\u5272\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faConformalSAM\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u81ea\u4f9d\u8d56\u8bad\u7ec3\u7b56\u7565\uff0c\u4f18\u5316SEEM\u751f\u6210\u7684\u63a9\u7801\u8d28\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConformalSAM\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u63d0\u5347\u5176\u4ed6\u65b9\u6cd5\u6027\u80fd\u3002", "conclusion": "ConformalSAM\u6709\u6548\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2507.15807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15807", "abs": "https://arxiv.org/abs/2507.15807", "authors": ["Shuo Chen", "Jianzhe Liu", "Zhen Han", "Yan Xia", "Daniel Cremers", "Philip Torr", "Volker Tresp", "Jindong Gu"], "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context", "comment": "accepted to COLM 2025", "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .", "AI": {"tldr": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08MICL\uff09\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u5ffd\u89c6\u89c6\u89c9\u7ebf\u7d22\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u672c\u6587\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u91cd\u5206\u914d\uff08DARA\uff09\u548c\u4e13\u7528\u6570\u636e\u96c6TrueMICL\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728MICL\u4e2d\u5ffd\u89c6\u89c6\u89c9\u4fe1\u606f\uff0c\u4ec5\u6a21\u4eff\u6587\u672c\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u589e\u5f3a\u89c6\u89c9\u5173\u6ce8\u5e76\u53ef\u9760\u8bc4\u4f30MICL\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u6ce8\u610f\u529b\u91cd\u5206\u914d\uff08DARA\uff09\u7b56\u7565\uff0c\u5e73\u8861\u89c6\u89c9\u4e0e\u6587\u672c\u6807\u8bb0\u7684\u6ce8\u610f\u529b\uff1b\u6784\u5efaTrueMICL\u6570\u636e\u96c6\uff0c\u660e\u786e\u8981\u6c42\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDARA\u548cTrueMICL\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u771f\u5b9e\u80fd\u529b\u3002", "conclusion": "DARA\u548cTrueMICL\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u5728MICL\u4e2d\u7684\u89c6\u89c9\u5ffd\u89c6\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.15809", "categories": ["cs.CV", "cs.LG", "physics.geo-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15809", "abs": "https://arxiv.org/abs/2507.15809", "authors": ["Roberto Miele", "Niklas Linde"], "title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion", "comment": null, "summary": "Diffusion models offer stable training and state-of-the-art performance for\ndeep generative modeling tasks. Here, we consider their use in the context of\nmultivariate subsurface modeling and probabilistic inversion. We first\ndemonstrate that diffusion models enhance multivariate modeling capabilities\ncompared to variational autoencoders and generative adversarial networks. In\ndiffusion modeling, the generative process involves a comparatively large\nnumber of time steps with update rules that can be modified to account for\nconditioning data. We propose different corrections to the popular Diffusion\nPosterior Sampling approach by Chung et al. (2023). In particular, we introduce\na likelihood approximation accounting for the noise-contamination that is\ninherent in diffusion modeling. We assess performance in a multivariate\ngeological scenario involving facies and correlated acoustic impedance.\nConditional modeling is demonstrated using both local hard data (well logs) and\nnonlinear geophysics (fullstack seismic data). Our tests show significantly\nimproved statistical robustness, enhanced sampling of the posterior probability\ndensity function and reduced computational costs, compared to the original\napproach. The method can be used with both hard and indirect conditioning data,\nindividually or simultaneously. As the inversion is included within the\ndiffusion process, it is faster than other methods requiring an outer-loop\naround the generative model, such as Markov chain Monte Carlo.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5728\u591a\u5143\u5730\u4e0b\u5efa\u6a21\u548c\u6982\u7387\u53cd\u6f14\u4e2d\u8868\u73b0\u4f18\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u63d0\u9ad8\u4e86\u7edf\u8ba1\u7a33\u5065\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7814\u7a76\u6269\u6563\u6a21\u578b\u5728\u591a\u5143\u5730\u4e0b\u5efa\u6a21\u548c\u6982\u7387\u53cd\u6f14\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u5347\u5efa\u6a21\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u5bf9Chung\u7b49\u4eba\u7684\u6269\u6563\u540e\u9a8c\u91c7\u6837\u65b9\u6cd5\u7684\u6539\u8fdb\uff0c\u5305\u62ec\u8003\u8651\u566a\u58f0\u6c61\u67d3\u7684\u4f3c\u7136\u8fd1\u4f3c\uff0c\u5e76\u5728\u591a\u5143\u5730\u8d28\u573a\u666f\u4e2d\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "\u76f8\u6bd4\u539f\u59cb\u65b9\u6cd5\uff0c\u6539\u8fdb\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u7edf\u8ba1\u7a33\u5065\u6027\u3001\u540e\u9a8c\u6982\u7387\u5bc6\u5ea6\u51fd\u6570\u7684\u91c7\u6837\u6548\u7387\uff0c\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u5728\u786c\u6570\u636e\u548c\u95f4\u63a5\u6761\u4ef6\u6570\u636e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u53cd\u6f14\u901f\u5ea6\u66f4\u5feb\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5730\u8d28\u5efa\u6a21\u3002"}}
{"id": "2507.15824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15824", "abs": "https://arxiv.org/abs/2507.15824", "authors": ["Enes Sanli", "Baris Sarper Tezcan", "Aykut Erdem", "Erkut Erdem"], "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models", "comment": null, "summary": "Recent progress in text-to-video (T2V) generation has enabled the synthesis\nof visually compelling and temporally coherent videos from natural language.\nHowever, these models often fall short in basic physical commonsense, producing\noutputs that violate intuitive expectations around causality, object behavior,\nand tool use. Addressing this gap, we present PhysVidBench, a benchmark\ndesigned to evaluate the physical reasoning capabilities of T2V systems. The\nbenchmark includes 383 carefully curated prompts, emphasizing tool use,\nmaterial properties, and procedural interactions, and domains where physical\nplausibility is crucial. For each prompt, we generate videos using diverse\nstate-of-the-art models and adopt a three-stage evaluation pipeline: (1)\nformulate grounded physics questions from the prompt, (2) caption the generated\nvideo with a vision-language model, and (3) task a language model to answer\nseveral physics-involved questions using only the caption. This indirect\nstrategy circumvents common hallucination issues in direct video-based\nevaluation. By highlighting affordances and tool-mediated actions, areas\noverlooked in current T2V evaluations, PhysVidBench provides a structured,\ninterpretable framework for assessing physical commonsense in generative video\nmodels.", "AI": {"tldr": "PhysVidBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7269\u7406\u5e38\u8bc6\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b383\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\u95f4\u63a5\u6d4b\u8bd5\u6a21\u578b\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u5e38\u8bc6\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u5e38\u8fdd\u53cd\u56e0\u679c\u5173\u7cfb\u548c\u7269\u4f53\u884c\u4e3a\u7684\u57fa\u672c\u76f4\u89c9\u3002", "method": "\u8bbe\u8ba1PhysVidBench\u57fa\u51c6\uff0c\u5305\u542b\u5de5\u5177\u4f7f\u7528\u3001\u6750\u6599\u5c5e\u6027\u548c\u8fc7\u7a0b\u4ea4\u4e92\u7b49\u9886\u57df\u7684\u63d0\u793a\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bc4\u4f30\uff08\u95ee\u9898\u751f\u6210\u3001\u89c6\u9891\u63cf\u8ff0\u3001\u8bed\u8a00\u6a21\u578b\u56de\u7b54\uff09\u3002", "result": "\u901a\u8fc7\u95f4\u63a5\u8bc4\u4f30\u7b56\u7565\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u89c6\u9891\u8bc4\u4f30\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u751f\u6210\u89c6\u9891\u6a21\u578b\u7684\u7269\u7406\u5e38\u8bc6\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "PhysVidBench\u586b\u8865\u4e86\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5f3a\u8c03\u4e86\u5de5\u5177\u4f7f\u7528\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.15852", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15852", "abs": "https://arxiv.org/abs/2507.15852", "authors": ["Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Songxin He", "Jianfan Lin", "Junsong Tang", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction", "comment": "project page: https://rookiexiong7.github.io/projects/SeC/; code:\n  https://github.com/OpenIXCLab/SeC; dataset:\n  https://huggingface.co/datasets/OpenIXCLab/SeCVOS", "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.", "AI": {"tldr": "SeC\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u9a71\u52a8\u7684\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u7ea7\u5bf9\u8c61\u8868\u793a\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u6280\u672f\u4f9d\u8d56\u5916\u89c2\u5339\u914d\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u5bf9\u5bf9\u8c61\u7684\u6982\u5ff5\u7406\u89e3\uff0c\u5bfc\u81f4\u5728\u89c6\u89c9\u53d8\u5316\u548c\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "SeC\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u6784\u5efa\u5bf9\u8c61\u6982\u5ff5\u8868\u793a\uff0c\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u7279\u5f81\u5339\u914d\uff0c\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "SeC\u5728SeCVOS\u57fa\u51c6\u4e0a\u6bd4SAM 2.1\u63d0\u534711.8\u5206\uff0c\u6210\u4e3a\u6982\u5ff5\u611f\u77e5\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u65b0\u6807\u6746\u3002", "conclusion": "SeC\u901a\u8fc7\u6982\u5ff5\u9a71\u52a8\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.15856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15856", "abs": "https://arxiv.org/abs/2507.15856", "authors": ["Jiawei Yang", "Tianhong Li", "Lijie Fan", "Yonglong Tian", "Yue Wang"], "title": "Latent Denoising Makes Good Visual Tokenizers", "comment": "Code is available at: https://github.com/Jiawei-Yang/DeTok", "summary": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u5206\u8bcd\u5668\uff08l-DeTok\uff09\uff0c\u901a\u8fc7\u76f4\u63a5\u4e0e\u53bb\u566a\u76ee\u6807\u5bf9\u9f50\uff0c\u63d0\u5347\u751f\u6210\u6a21\u578b\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u76ee\u6807\uff08\u53bb\u566a\uff09\u4e0e\u5206\u8bcd\u5668\u7684\u8bbe\u8ba1\u76ee\u6807\u4e0d\u4e00\u81f4\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u5bf9\u9f50\u8fd9\u4e24\u8005\u6765\u63d0\u5347\u5206\u8bcd\u5668\u7684\u6709\u6548\u6027\u3002", "method": "\u5f15\u5165Latent Denoising Tokenizer\uff08l-DeTok\uff09\uff0c\u901a\u8fc7\u8bad\u7ec3\u5206\u8bcd\u5668\u4ece\u88ab\u566a\u58f0\u548c\u63a9\u7801\u7834\u574f\u7684\u6f5c\u5728\u5d4c\u5165\u4e2d\u91cd\u5efa\u5e72\u51c0\u56fe\u50cf\u3002", "result": "\u5728ImageNet 256x256\u4e0a\uff0cl-DeTok\u5728\u516d\u79cd\u4ee3\u8868\u6027\u751f\u6210\u6a21\u578b\u4e2d\u5747\u4f18\u4e8e\u6807\u51c6\u5206\u8bcd\u5668\u3002", "conclusion": "\u53bb\u566a\u662f\u5206\u8bcd\u5668\u8bbe\u8ba1\u7684\u57fa\u672c\u539f\u5219\uff0c\u672a\u6765\u5206\u8bcd\u5668\u8bbe\u8ba1\u5e94\u8003\u8651\u8fd9\u4e00\u89c6\u89d2\u3002"}}
