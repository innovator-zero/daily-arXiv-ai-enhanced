{"id": "2508.09325", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.09325", "abs": "https://arxiv.org/abs/2508.09325", "authors": ["Alexandre Brown", "Glen Berseth"], "title": "SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning", "comment": null, "summary": "Visual reinforcement learning (RL) is challenging due to the need to learn\nboth perception and actions from high-dimensional inputs and noisy rewards.\nAlthough large perception models exist, integrating them effectively into RL\nfor visual generalization and improved sample efficiency remains unclear. We\npropose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment\nAnything (SAM) for object-centric decomposition and YOLO-World to ground\nsegments semantically via text prompts. It includes a novel transformer-based\narchitecture that supports a dynamic number of segments at each time step and\neffectively learns which segments to focus on using online RL, without using\nhuman labels. By evaluating SegDAC over a challenging visual generalization\nbenchmark using Maniskill3, which covers diverse manipulation tasks under\nstrong visual perturbations, we demonstrate that SegDAC achieves significantly\nbetter visual generalization, doubling prior performance on the hardest setting\nand matching or surpassing prior methods in sample efficiency across all\nevaluated tasks."}
