<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]
- [cs.LG](#cs.LG) [Total: 57]
- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving](https://arxiv.org/abs/2508.21080)
*Ali K. AlShami,Ryan Rabinowitz,Maged Shoman,Jianwu Fang,Lukas Picek,Shao-Yuan Lo,Steve Cruz,Khang Nhut Lam,Nachiket Kamod,Lei-Lei Li,Jugal Kalita,Terrance E. Boult*

Main category: cs.CV

TL;DR: 2COOOL研讨会旨在解决自动驾驶中的新场景问题，推动异常检测、开放集识别等领域的发展。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶尚未完全安全的原因在于处理新场景的能力不足，需要改进感知和决策。

Method: 通过研讨会汇集研究者和行业专家，探讨异常检测、开放词汇建模等方法。

Result: 研讨会将促进新算法和系统的开发，提升自动驾驶的安全性。

Conclusion: 2COOOL研讨会为自动驾驶领域的新颖性处理提供了重要平台。

Abstract: As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.

</details>


### [2] [Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images](https://arxiv.org/abs/2508.21088)
*Alireza Golkarieh,Kiana Kiashemshaki,Sajjad Rezvani Boroujeni*

Main category: cs.CV

TL;DR: 研究评估了三种深度学习方法在牙科全景X光片自动分类中的表现，发现结合CNN和随机森林的混合模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索深度学习在牙科X光片自动分类中的应用，以提高诊断效率和准确性。

Method: 使用1,512张X光片数据集，评估了自定义CNN、混合模型（CNN+传统分类器）和预训练模型（如VGG16）。

Result: 混合CNN-随机森林模型准确率最高（85.4%），优于自定义CNN（74.3%）和预训练模型（VGG16为82.3%）。

Conclusion: 混合模型在牙科诊断中表现优异，但需更大数据集和临床验证。

Abstract: This study investigates deep learning methods for automated classification of
dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs
with 11,137 expert-verified annotations across four conditions fillings,
cavities, implants, and impacted teeth was used. After preprocessing and class
balancing, three approaches were evaluated: a custom convolutional neural
network (CNN), hybrid models combining CNN feature extraction with traditional
classifiers, and fine-tuned pre-trained architectures. Experiments employed 5
fold cross validation with accuracy, precision, recall, and F1 score as
evaluation metrics. The hybrid CNN Random Forest model achieved the highest
performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.
Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by
Xception and ResNet50. Results show that hybrid models improve discrimination
of morphologically similar conditions and provide efficient, reliable
performance. These findings suggest that combining CNN-based feature extraction
with ensemble classifiers offers a practical path toward automated dental
diagnostic support, while also highlighting the need for larger datasets and
further clinical validation.

</details>


### [3] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

TL;DR: Q-Align通过Query-Query对齐解决注意力泄漏问题，提升零样本外观迁移的语义对齐效果。


<details>
  <summary>Details</summary>
Motivation: 大规模图像生成模型在零样本外观迁移中存在注意力泄漏问题，影响语义映射。

Method: 提出Q-Align，包含Query-Query对齐、Key-Value重排和注意力细化三个核心贡献。

Result: Q-Align在外观保真度上优于现有方法，同时保持结构一致性。

Conclusion: Q-Align有效解决了注意力泄漏问题，提升了零样本外观迁移的性能。

Abstract: We observe that zero-shot appearance transfer with large-scale image
generation models faces a significant challenge: Attention Leakage. This
challenge arises when the semantic mapping between two images is captured by
the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing
Query-Query alignment to mitigate attention leakage and improve the semantic
alignment in zero-shot appearance transfer. Q-Align incorporates three core
contributions: (1) Query-Query alignment, facilitating the sophisticated
spatial semantic mapping between two images; (2) Key-Value rearrangement,
enhancing feature correspondence through realignment; and (3) Attention
refinement using rearranged keys and values to maintain semantic consistency.
We validate the effectiveness of Q-Align through extensive experiments and
analysis, and Q-Align outperforms state-of-the-art methods in appearance
fidelity while maintaining competitive structure preservation.

</details>


### [4] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

TL;DR: ERTACache是一种用于加速扩散模型推理的缓存框架，通过减少计算开销并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型因迭代推理过程导致计算开销大，现有缓存策略会引入质量下降问题。

Method: 提出ERTACache框架，通过离线残差分析、动态调整积分区间和线性化误差模型来优化缓存。

Result: 实验显示ERTACache在图像和视频生成任务中实现2倍加速，且保持或提升视觉质量。

Conclusion: ERTACache有效平衡了扩散模型的效率与质量，适用于实际应用。

Abstract: Diffusion models suffer from substantial computational overhead due to their
inherently iterative inference process. While feature caching offers a
promising acceleration strategy by reusing intermediate outputs across
timesteps, naive reuse often incurs noticeable quality degradation. In this
work, we formally analyze the cumulative error introduced by caching and
decompose it into two principal components: feature shift error, caused by
inaccuracies in cached outputs, and step amplification error, which arises from
error propagation under fixed timestep schedules. To address these issues, we
propose ERTACache, a principled caching framework that jointly rectifies both
error types. Our method employs an offline residual profiling stage to identify
reusable steps, dynamically adjusts integration intervals via a
trajectory-aware correction coefficient, and analytically approximates
cache-induced errors through a closed-form residual linearization model.
Together, these components enable accurate and efficient sampling under
aggressive cache reuse. Extensive experiments across standard image and video
generation benchmarks show that ERTACache achieves up to 2x inference speedup
while consistently preserving or even improving visual quality. Notably, on the
state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x
acceleration with minimal VBench degradation, effectively maintaining baseline
fidelity while significantly improving efficiency. The code is available at
https://github.com/bytedance/ERTACache.

</details>


### [5] [Video-LLMs with Temporal Visual Screening](https://arxiv.org/abs/2508.21094)
*Zheyu Fan,Jiateng Liu,Yuji Zhang,Zihan Wang,Yi R.,Fung,Manling Li,Heng Ji*

Main category: cs.CV

TL;DR: 论文提出了一种名为Temporal Visual Screening (TVS)的新任务，通过保留关键视频片段、同步重构查询并保持答案一致性，优化视频问答和指令调优数据的预处理。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型（Video-LLMs）由于稀疏帧采样和训练中帧间推理监督不足，难以捕捉细粒度时间语义。

Method: TVS作为模块化前端适配器任务，集成到视频指令调优和视频问答流程中，优化推理负担和认知负载分配。

Result: 实验表明，TVS在训练和推理阶段分别带来7.33%和34.6%的相对增益，显著提升视频语言理解能力。

Conclusion: TVS通过时间信息筛选有效改进了视频语言理解，为视频问答和指令调优提供了新思路。

Abstract: Humans naturally perform temporal screening by dragging the progress bar and
focusing on salient temporal segments, but current Video Large Language Models
(Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse
frame sampling and insufficient inter-frame reasoning supervision during their
training. To address this, Inspired by well-established cognitive science
principles, we propose Temporal Visual Screening (TVS), a new task that
universally pre-processes video question answering and instruction tuning data
by: (1) retaining focus-critical video segments, (2) synchronously
reconstructing queries to their most direct form while preserving answer
consistency, and (3) keeping the invariance and consistency for any possible
answer. TVS is formulated as a modular front-end adapter task that can be
seamlessly integrated into both Video Instruction Tuning (training) and Video
Question Answering (inference) pipelines. TVS optimizes distribution of
reasoning burden and cognitive load; during training, it aligns queries with
focus-critical visual information; at inference, it enables query-aware segment
focus and streamlined query representations. In particular, we curate the first
benchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior
approaches on seemingly similar tasks by 0.47 in F-1 score on video trimming
while achieving competitive query rewriting performance. Experiments
demonstrate that incorporating TVS yields relative gains of 7.33% (training)
and 34.6% (inference), demonstrating the effectiveness of temporal information
screening for improving video-language understanding.

</details>


### [6] [ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments](https://arxiv.org/abs/2508.21096)
*Zhe Han,Charlie Budd,Gongyu Zhang,Huanyu Tian,Christos Bergeles,Tom Vercauteren*

Main category: cs.CV

TL;DR: 论文提出了一种基于骨骼姿态标注的手术工具定位方法，并发布了ROBUST-MIPS数据集和标注工具。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的分割方法受限于标注数据的多样性，骨骼姿态标注在语义丰富性和标注效率之间取得了平衡。

Method: 通过ROBUST-MIPS数据集结合工具姿态和实例分割标注，并建立简单基准测试验证姿态标注的适用性。

Result: 实验表明姿态标注在手术工具定位中表现高质量，并发布了数据集和标注软件。

Conclusion: 骨骼姿态标注是一种高效的手术工具定位方法，有望加速标注数据的增长。

Abstract: Localisation of surgical tools constitutes a foundational building block for
computer-assisted interventional technologies. Works in this field typically
focus on training deep learning models to perform segmentation tasks.
Performance of learning-based approaches is limited by the availability of
diverse annotated data. We argue that skeletal pose annotations are a more
efficient annotation approach for surgical tools, striking a balance between
richness of semantic information and ease of annotation, thus allowing for
accelerated growth of available annotated data. To encourage adoption of this
annotation style, we present, ROBUST-MIPS, a combined tool pose and tool
instance segmentation dataset derived from the existing ROBUST-MIS dataset. Our
enriched dataset facilitates the joint study of these two annotation styles and
allow head-to-head comparison on various downstream tasks. To demonstrate the
adequacy of pose annotations for surgical tool localisation, we set up a simple
benchmark using popular pose estimation methods and observe high-quality
results. To ease adoption, together with the dataset, we release our benchmark
models and custom tool pose annotation software.

</details>


### [7] [Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](https://arxiv.org/abs/2508.21099)
*Xiangtao Meng,Yingkai Dong,Ning Yu,Li Wang,Zheng Li,Shanqing Guo*

Main category: cs.CV

TL;DR: Safe-Control是一种创新的即插即用安全补丁，用于减少T2I模型中的不安全内容生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管T2I生成模型有进步，但其滥用风险引发安全担忧，现有安全机制易受规避或需大量调整。

Method: 通过数据驱动策略和安全感知条件，Safe-Control向锁定T2I模型注入安全控制信号，支持灵活合并多种安全补丁。

Result: 在六种T2I模型上评估，Safe-Control将不安全内容生成概率降至7%，优于基线方法的约20%。

Conclusion: Safe-Control有效减少不安全内容生成，保持良性图像质量，兼容类似去噪架构的T2I模型。

Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their
potential for misuse or even abuse raises serious safety concerns. Model
developers have made tremendous efforts to introduce safety mechanisms that can
address these concerns in T2I models. However, the existing safety mechanisms,
whether external or internal, either remain susceptible to evasion under
distribution shifts or require extensive model-specific adjustments. To address
these limitations, we introduce Safe-Control, an innovative plug-and-play
safety patch designed to mitigate unsafe content generation in T2I models.
Using data-driven strategies and safety-aware conditions, Safe-Control injects
safety control signals into the locked T2I model, acting as an update in a
patch-like manner. Model developers can also construct various safety patches
to meet the evolving safety requirements, which can be flexibly merged into a
single, unified patch. Its plug-and-play design further ensures adaptability,
making it compatible with other T2I models of similar denoising architecture.
We conduct extensive evaluations on six diverse and public T2I models.
Empirical results highlight that Safe-Control is effective in reducing unsafe
content generation across six diverse T2I models with similar generative
architectures, yet it successfully maintains the quality and text alignment of
benign images. Compared to seven state-of-the-art safety mechanisms, including
both external and internal defenses, Safe-Control significantly outperforms all
baselines in reducing unsafe content generation. For example, it reduces the
probability of unsafe content generation to 7%, compared to approximately 20%
for most baseline methods, under both unsafe prompts and the latest adversarial
attacks.

</details>


### [8] [GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions](https://arxiv.org/abs/2508.21102)
*Kei Katsumata,Yui Iioka,Naoki Hosomi,Teruhisa Misu,Kentaro Yamada,Komei Sugiura*

Main category: cs.CV

TL;DR: GENNAV是一种新方法，用于从自然语言指令和图像中预测目标区域的存在并生成分割掩码，特别适用于模糊边界的stuff-type目标区域。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理stuff-type目标区域、无目标或多目标时表现不佳，因此需要一种更有效的解决方案。

Method: 提出GENNAV，结合目标存在预测和多目标分割掩码生成，并通过GRiN-Drive基准进行评估。

Result: GENNAV在标准评估指标上优于基线方法，并在真实世界实验中展示了零样本迁移的鲁棒性。

Conclusion: GENNAV是一种高效且鲁棒的方法，适用于复杂环境中的目标区域识别任务。

Abstract: We focus on the task of identifying the location of target regions from a
natural language instruction and a front camera image captured by a mobility.
This task is challenging because it requires both existence prediction and
segmentation, particularly for stuff-type target regions with ambiguous
boundaries. Existing methods often underperform in handling stuff-type target
regions, in addition to absent or multiple targets. To overcome these
limitations, we propose GENNAV, which predicts target existence and generates
segmentation masks for multiple stuff-type target regions. To evaluate GENNAV,
we constructed a novel benchmark called GRiN-Drive, which includes three
distinct types of samples: no-target, single-target, and multi-target. GENNAV
achieved superior performance over baseline methods on standard evaluation
metrics. Furthermore, we conducted real-world experiments with four automobiles
operated in five geographically distinct urban areas to validate its zero-shot
transfer performance. In these experiments, GENNAV outperformed baseline
methods and demonstrated its robustness across diverse real-world environments.
The project page is available at https://gennav.vercel.app/.

</details>


### [9] [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113)
*Jie Jiang,Qi Yang,Bolin Ni,Shiming Xiang,Han Hu,Houwen Peng*

Main category: cs.CV

TL;DR: R-4B是一种自适应思考的多模态大语言模型，通过双模式退火和策略优化，根据问题复杂度决定是否激活思考过程，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有MLLM在简单问题上冗余思考的低效问题。

Method: 采用双模式退火和Bi-mode Policy Optimization（BPO），分两阶段训练模型。

Result: 在25个基准测试中表现优异，优于Qwen2.5-VL-7B，并与更大模型Kimi-VL-A3B-Thinking-2506性能相当。

Conclusion: R-4B通过自适应思考机制，实现了高效且高性能的推理能力。

Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking
capabilities have demonstrated remarkable performance on complex reasoning
problems. However, this thinking process is redundant for simple problems
solvable without complex reasoning. To address this inefficiency, we propose
R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on
problem complexity. The central idea of R-4B is to empower the model with both
thinking and non-thinking capabilities using bi-mode annealing, and apply
Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in
determining whether to activate the thinking process. Specifically, we first
train the model on a carefully curated dataset spanning various topics, which
contains samples from both thinking and non-thinking modes. Then it undergoes a
second phase of training under an improved GRPO framework, where the policy
model is forced to generate responses from both modes for each input query.
Experimental results show that R-4B achieves state-of-the-art performance
across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks
and achieves performance comparable to larger models such as
Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower
computational cost.

</details>


### [10] [HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection](https://arxiv.org/abs/2508.21135)
*Harris Song,Tuan-Anh Vu,Sanjith Menon,Sriram Narasimhan,M. Khalid Jawed*

Main category: cs.CV

TL;DR: HiddenObject是一个基于Mamba的多模态融合框架，用于检测遮挡或伪装的目标，在RGB、热成像和深度数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统RGB检测方法在遮挡、伪装和光照变化下表现不佳，需要更鲁棒的多模态方法。

Method: 通过Mamba机制融合RGB、热成像和深度数据，提取互补特征并生成统一表示。

Result: 在多个基准数据集上达到或超越现有方法，验证了融合设计的有效性。

Conclusion: Mamba融合架构能显著提升多模态目标检测性能，尤其在复杂视觉条件下。

Abstract: Detecting hidden or partially concealed objects remains a fundamental
challenge in multimodal environments, where factors like occlusion, camouflage,
and lighting variations significantly hinder performance. Traditional RGB-based
detection methods often fail under such adverse conditions, motivating the need
for more robust, modality-agnostic approaches. In this work, we present
HiddenObject, a fusion framework that integrates RGB, thermal, and depth data
using a Mamba-based fusion mechanism. Our method captures complementary signals
across modalities, enabling enhanced detection of obscured or camouflaged
targets. Specifically, the proposed approach identifies modality-specific
features and fuses them in a unified representation that generalizes well
across challenging scenarios. We validate HiddenObject across multiple
benchmark datasets, demonstrating state-of-the-art or competitive performance
compared to existing methods. These results highlight the efficacy of our
fusion design and expose key limitations in current unimodal and na\"ive fusion
strategies. More broadly, our findings suggest that Mamba-based fusion
architectures can significantly advance the field of multimodal object
detection, especially under visually degraded or complex conditions.

</details>


### [11] [RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration](https://arxiv.org/abs/2508.21154)
*Ao Shen,Xueming Fu,Junfeng Jiang,Qiang Zeng,Ye Tang,Zhengming Chen,Luming Nong,Feng Wang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: RadGS-Reg框架通过联合3D Radiative Gaussians重建和3D/3D注册，解决了CT/X射线注册中的高精度和实时性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统方法因空间信息丢失和领域差距而受限，现有方法在噪声X射线下表现不佳。

Method: 结合学习型RadGS重建和Counterfactual Attention Learning机制，采用患者特定预训练策略。

Result: 在内部数据集上表现优于现有方法。

Conclusion: RadGS-Reg在CT/X射线注册中实现了先进性能。

Abstract: Computed Tomography (CT)/X-ray registration in image-guided navigation
remains challenging because of its stringent requirements for high accuracy and
real-time performance. Traditional "render and compare" methods, relying on
iterative projection and comparison, suffer from spatial information loss and
domain gap. 3D reconstruction from biplanar X-rays supplements spatial and
shape information for 2D/3D registration, but current methods are limited by
dense-view requirements and struggles with noisy X-rays. To address these
limitations, we introduce RadGS-Reg, a novel framework for vertebral-level
CT/X-ray registration through joint 3D Radiative Gaussians (RadGS)
reconstruction and 3D/3D registration. Specifically, our biplanar X-rays
vertebral RadGS reconstruction module explores learning-based RadGS
reconstruction method with a Counterfactual Attention Learning (CAL) mechanism,
focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific
pre-training strategy progressively adapts the RadGS-Reg from simulated to real
data while simultaneously learning vertebral shape prior knowledge. Experiments
on in-house datasets demonstrate the state-of-the-art performance for both
tasks, surpassing existing methods. The code is available at:
https://github.com/shenao1995/RadGS_Reg.

</details>


### [12] [SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4](https://arxiv.org/abs/2508.21169)
*Kevin Mayer,Alex Vesel,Xinyi Zhao,Martin Fischer*

Main category: cs.CV

TL;DR: SYNBUILD-3D是一个包含620万合成3D住宅建筑的多模态数据集，支持LoD 4级别的语义丰富建模。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模公开标注数据集，自动生成精确且语义丰富的3D建筑模型仍具挑战性。

Method: 通过合成数据生成多模态数据集，包括3D线框、平面图和LiDAR点云。

Result: 数据集支持开发生成式AI算法，用于自动化创建语义一致的3D建筑模型。

Conclusion: SYNBUILD-3D为3D建筑建模研究提供了重要资源，推动自动化生成技术的发展。

Abstract: 3D building models are critical for applications in architecture, energy
simulation, and navigation. Yet, generating accurate and semantically rich 3D
buildings automatically remains a major challenge due to the lack of
large-scale annotated datasets in the public domain. Inspired by the success of
synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,
and multi-modal dataset of over 6.2 million synthetic 3D residential buildings
at Level of Detail (LoD) 4. In the dataset, each building is represented
through three distinct modalities: a semantically enriched 3D wireframe graph
at LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a
LiDAR-like roof point cloud (Modality III). The semantic annotations for each
building wireframe are derived from the corresponding floor plan images and
include information on rooms, doors, and windows. Through its tri-modal nature,
future work can use SYNBUILD-3D to develop novel generative AI algorithms that
automate the creation of 3D building models at LoD 4, subject to predefined
floor plan layouts and roof geometries, while enforcing semantic-geometric
consistency. Dataset and code samples are publicly available at
https://github.com/kdmayer/SYNBUILD-3D.

</details>


### [13] [Radially Distorted Homographies, Revisited](https://arxiv.org/abs/2508.21190)
*Mårten Wadenbäck,Marcus Valtonen Örnhag,Johan Edstedt*

Main category: cs.CV

TL;DR: 论文提出了一种统一的方法来解决三种不同配置的径向畸变与单应性估计问题，新求解器在速度和精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在计算机视觉中，单应性估计常受镜头畸变影响，尤其是径向畸变。现有方法分别处理三种畸变配置，缺乏统一解决方案。

Method: 提出了一种统一的方法，能够同时处理三种径向畸变配置（单图像畸变、相同畸变和独立畸变），并构建了快速、稳定的最小求解器。

Result: 新求解器在速度上优于现有方法，同时保持相似精度，并在鱼眼相机图像等基准测试中表现良好。

Conclusion: 论文提供了一种高效且统一的解决方案，适用于多种径向畸变配置的单应性估计问题，具有实际应用价值。

Abstract: Homographies are among the most prevalent transformations occurring in
geometric computer vision and projective geometry, and homography estimation is
consequently a crucial step in a wide assortment of computer vision tasks. When
working with real images, which are often afflicted with geometric distortions
caused by the camera lens, it may be necessary to determine both the homography
and the lens distortion-particularly the radial component, called radial
distortion-simultaneously to obtain anything resembling useful estimates. When
considering a homography with radial distortion between two images, there are
three conceptually distinct configurations for the radial distortion; (i)
distortion in only one image, (ii) identical distortion in the two images, and
(iii) independent distortion in the two images. While these cases have been
addressed separately in the past, the present paper provides a novel and
unified approach to solve all three cases. We demonstrate how the proposed
approach can be used to construct new fast, stable, and accurate minimal
solvers for radially distorted homographies. In all three cases, our proposed
solvers are faster than the existing state-of-the-art solvers while maintaining
similar accuracy. The solvers are tested on well-established benchmarks
including images taken with fisheye cameras. The source code for our solvers
will be made available in the event our paper is accepted for publication.

</details>


### [14] [GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability](https://arxiv.org/abs/2508.21197)
*Zhenghao He,Sanchit Sinha,Guangzhi Xiong,Aidong Zhang*

Main category: cs.CV

TL;DR: 论文提出了全局概念激活向量（GCAV），通过对比学习和注意力机制统一不同层的CAV，解决了跨层概念不一致的问题，提升了概念解释的稳定性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 独立计算的不同层CAV存在不一致性，导致跨层比较不可靠，需要一种统一的方法来提升概念解释的连贯性。

Method: 采用对比学习对齐跨层概念表示，并通过注意力机制融合生成全局CAV（GCAV），同时提出TGCAV方法进行评估。

Result: 实验表明GCAV显著减少了TCAV分数的方差，增强了概念定位能力，并提高了对抗扰动的鲁棒性。

Conclusion: GCAV通过整合跨层信息，提供了更全面和可解释的深度学习模型概念编码理解。

Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for
interpreting deep neural networks by quantifying their sensitivity to
human-defined concepts. However, when computed independently at different
layers, CAVs often exhibit inconsistencies, making cross-layer comparisons
unreliable. To address this issue, we propose the Global Concept Activation
Vector (GCAV), a novel framework that unifies CAVs into a single, semantically
consistent representation. Our method leverages contrastive learning to align
concept representations across layers and employs an attention-based fusion
mechanism to construct a globally integrated CAV. By doing so, our method
significantly reduces the variance in TCAV scores while preserving concept
relevance, ensuring more stable and reliable concept attributions. To evaluate
the effectiveness of GCAV, we introduce Testing with Global Concept Activation
Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We
conduct extensive experiments on multiple deep neural networks, demonstrating
that our method effectively mitigates concept inconsistency across layers,
enhances concept localization, and improves robustness against adversarial
perturbations. By integrating cross-layer information into a coherent
framework, our method offers a more comprehensive and interpretable
understanding of how deep learning models encode human-defined concepts. Code
and models are available at https://github.com/Zhenghao-He/GCAV.

</details>


### [15] [Generalizable Object Re-Identification via Visual In-Context Prompting](https://arxiv.org/abs/2508.21222)
*Zhizhong Huang,Xiaoming Liu*

Main category: cs.CV

TL;DR: VICP框架通过结合LLM和视觉基础模型，无需参数调整即可推广到新类别，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有ReID方法缺乏泛化能力且需要大量标注数据，自监督学习难以捕捉身份敏感特征。

Method: VICP利用LLM推断语义规则，通过动态视觉提示指导VFM提取特征，无需数据集特定训练。

Result: 在ShopID10K和多个ReID基准测试中，VICP在新类别上表现优异。

Conclusion: VICP为ReID提供了一种无需重新训练的高效泛化解决方案。

Abstract: Current object re-identification (ReID) methods train domain-specific models
(e.g., for persons or vehicles), which lack generalization and demand costly
labeled data for new categories. While self-supervised learning reduces
annotation needs by learning instance-wise invariance, it struggles to capture
\textit{identity-sensitive} features critical for ReID. This paper proposes
Visual In-Context Prompting~(VICP), a novel framework where models trained on
seen categories can directly generalize to unseen novel categories using only
\textit{in-context examples} as prompts, without requiring parameter
adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer
semantic identity rules from few-shot positive/negative pairs through
task-specific prompting, which then guides a VFM (\eg, DINO) to extract
ID-discriminative features via \textit{dynamic visual prompts}. By aligning
LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables
generalization to novel categories, eliminating the need for dataset-specific
retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K
object instances from e-commerce platforms, featuring multi-view images and
cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks
demonstrate that VICP outperforms baselines by a clear margin on unseen
categories. Code is available at https://github.com/Hzzone/VICP.

</details>


### [16] [Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with Auto3DSeg](https://arxiv.org/abs/2508.21227)
*Keshav Jha,William Sharp,Dominic LaBella*

Main category: cs.CV

TL;DR: 论文研究了基于MRI的胰腺肿瘤自动分割方法，使用SegResNet模型在PANTHER挑战赛的两个任务中评估性能，结果显示在不同MRI序列下性能差异显著，强调了标准化大数据集的重要性。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤的精确分割对诊断和治疗至关重要，但由于解剖学变异性和数据集有限，自动分割仍具挑战性。

Method: 采用SegResNet模型和Auto3DSeg架构，通过5折交叉验证和STAPLE集成方法，在两个MRI数据集（T1和T2加权）上进行训练和评估。

Result: Task 1的DSC为0.56，Task 2降至0.33，表明不同MRI序列对分割性能有显著影响。

Conclusion: 尽管性能一般，研究展示了自动分割的潜力，并强调需要更大、标准化的MRI数据集以提高模型的鲁棒性和临床实用性。

Abstract: Accurate delineation of pancreatic tumors is critical for diagnosis,
treatment planning, and outcome assessment, yet automated segmentation remains
challenging due to anatomical variability and limited dataset availability. In
this study, SegResNet models, as part of the Auto3DSeg architecture, were
trained and evaluated on two MRI-based pancreatic tumor segmentation tasks as
part of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold
cross-validation with STAPLE ensembling after focusing on an anatomically
relevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic
MRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI
with expert annotated pancreas and tumor labels. The Pancreatic Tumor
Segmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases
with expert annotated pancreas and tumor labels. Algorithm-automated
segmentation performance of pancreatic tumor was assessed using Dice Similarity
Coefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean
Average Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1,
the algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD
of 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC
of 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203
mm. These findings illustrate the challenges of MRI-based pancreatic tumor
segmentation with small datasets, highlighting variability introduced by
different MRI sequences. Despite modest performance, the results demonstrate
potential for automated delineation and emphasize the need for larger,
standardized MRI datasets to improve model robustness and clinical utility.

</details>


### [17] [Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation](https://arxiv.org/abs/2508.21254)
*Yidong Zhao,Peter Kellman,Hui Xue,Tongyun Yang,Yi Zhang,Yuchi Han,Orlando Simonetti,Qian Tao*

Main category: cs.CV

TL;DR: Reverse Imaging是一种基于物理学的数据增强和域适应方法，通过推断底层自旋属性来解决心脏MRI分割模型的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 心脏MRI图像对比度因成像协议不同而变化，导致预训练分割模型难以泛化。

Method: 通过解决非线性逆问题推断自旋属性，利用扩散模型学习自旋先验分布，生成任意新序列的图像。

Result: Reverse Imaging实现了对不同图像对比度和成像协议的高精度分割，显著提升了泛化能力。

Conclusion: 该方法通过物理驱动的数据增强和域适应，从根本上解决了心脏MRI分割的泛化问题。

Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI)
struggle to generalize across different imaging sequences due to significant
variations in image contrast. These variations arise from changes in imaging
protocols, yet the same fundamental spin properties, including proton density,
T1, and T2 values, govern all acquired images. With this core principle, we
introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data
augmentation and domain adaptation to fundamentally solve the generalization
problem. Our method reversely infers the underlying spin properties from
observed cardiac MRI images, by solving ill-posed nonlinear inverse problems
regularized by the prior distribution of spin properties. We acquire this "spin
prior" by learning a generative diffusion model from the multiparametric
SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which
offers joint cardiac T1 and T2 maps. Our method enables approximate but
meaningful spin-property estimates from MR images, which provide an
interpretable "latent variable" that lead to highly flexible image synthesis of
arbitrary novel sequences. We show that Reverse Imaging enables highly accurate
segmentation across vastly different image contrasts and imaging protocols,
realizing wide-spectrum generalization of cardiac MRI segmentation.

</details>


### [18] [PHD: Personalized 3D Human Body Fitting with Point Diffusion](https://arxiv.org/abs/2508.21257)
*Hsuan-I Ho,Chen Guo,Po-Chen Wu,Ivan Shugurov,Chengcheng Tang,Abhay Mittal,Sizhe An,Manuel Kaufmann,Linguang Zhang*

Main category: cs.CV

TL;DR: PHD是一种个性化3D人体网格恢复方法，通过用户特定形状信息提高视频中的姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统HMR方法忽略用户特定身体形状和3D姿态合理性，PHD通过解耦形状校准和姿态拟合来解决这一问题。

Method: PHD首先校准用户身体形状，然后基于形状条件使用Point Diffusion Transformer进行姿态拟合。

Result: PHD提高了骨盆对齐和绝对姿态精度，且仅需合成数据训练，可无缝集成现有3D姿态估计器。

Conclusion: PHD通过个性化形状和姿态拟合，显著提升了3D人体姿态估计的精度和实用性。

Abstract: We introduce PHD, a novel approach for personalized 3D human mesh recovery
(HMR) and body fitting that leverages user-specific shape information to
improve pose estimation accuracy from videos. Traditional HMR methods are
designed to be user-agnostic and optimized for generalization. While these
methods often refine poses using constraints derived from the 2D image to
improve alignment, this process compromises 3D accuracy by failing to jointly
account for person-specific body shapes and the plausibility of 3D poses. In
contrast, our pipeline decouples this process by first calibrating the user's
body shape and then employing a personalized pose fitting process conditioned
on that shape. To achieve this, we develop a body shape-conditioned 3D pose
prior, implemented as a Point Diffusion Transformer, which iteratively guides
the pose fitting via a Point Distillation Sampling loss. This learned 3D pose
prior effectively mitigates errors arising from an over-reliance on 2D
constraints. Consequently, our approach improves not only pelvis-aligned pose
accuracy but also absolute pose accuracy -- an important metric often
overlooked by prior work. Furthermore, our method is highly data-efficient,
requiring only synthetic data for training, and serves as a versatile
plug-and-play module that can be seamlessly integrated with existing 3D pose
estimators to enhance their performance. Project page:
https://phd-pose.github.io/

</details>


### [19] [Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning](https://arxiv.org/abs/2508.21363)
*Yuquan Bi,Hongsong Wang,Xinli Shi,Zhipeng Gui,Jie Gui,Yuan Yan Tang*

Main category: cs.CV

TL;DR: 提出了一种高效的基于扩散模型的3D人体姿态估计框架，通过分层时间剪枝策略减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高保真3D人体姿态时计算成本高，需要优化。

Method: 采用分层时间剪枝策略（HTP），包括时间相关性增强剪枝、稀疏聚焦时间MHSA和掩码引导姿态标记剪枝。

Result: 在Human3.6M和MPI-INF-3DHP数据集上，训练和推理计算量显著减少，推理速度提升81.1%，性能达到最优。

Conclusion: HTP策略有效降低了计算成本，同时保持了高性能。

Abstract: Diffusion models have demonstrated strong capabilities in generating
high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis
requirements incur substantial computational cost. In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics. HTP operates in a staged, top-down manner: (1)
Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by
analyzing inter-frame motion correlations through adaptive temporal graph
construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the
resulting frame-level sparsity to reduce attention computation, focusing on
motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs
fine-grained semantic pruning via clustering, retaining only the most
informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.

</details>


### [20] [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](https://arxiv.org/abs/2508.21542)
*Ziwei Liao,Mohamed Sayed,Steven L. Waslander,Sara Vicente,Daniyar Turmukhambetov,Michael Firman*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型的方法，从单张图像重建完整的3D场景，包括遮挡部分。


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅在遮挡和未观测区域重建失败的问题，克服传统方法模糊、不真实和无法捕捉多模态解释的局限性。

Method: 使用变分自重构器（Variational AutoReconstructor）从2D图像自监督学习潜在空间，并在其上训练扩散模型。

Result: 生成高质量、多样化的3D重建样本，能够完成遮挡区域的重建，支持360度渲染。

Conclusion: 该方法通过生成式建模解决了单图像3D重建的挑战，显著提升了遮挡区域的完成质量。

Abstract: Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single "mode" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.

</details>


### [21] [Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image](https://arxiv.org/abs/2508.21371)
*Qingran Miao,Haixia Wang,Haohao Sun,Yilong Zhang*

Main category: cs.CV

TL;DR: Print2Volume框架通过2D指纹生成合成3D OCT指纹数据，解决了数据稀缺问题，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: OCT数据获取成本高且耗时，导致大规模数据集稀缺，阻碍了深度学习模型的发展。

Method: 框架包括2D风格转换、3D结构扩展网络和OCT真实性细化器（基于3D GAN），生成高质量合成数据。

Result: 生成42万样本，预训练模型在ZJUT-EIFD基准上将EER从15.62%降至2.50%。

Conclusion: Print2Volume有效克服数据稀缺问题，显著提升指纹识别性能。

Abstract: Optical Coherence Tomography (OCT) enables the acquisition of
high-resolution, three-dimensional fingerprint data, capturing rich subsurface
structures for robust biometric recognition. However, the high cost and
time-consuming nature of OCT data acquisition have led to a scarcity of
large-scale public datasets, significantly hindering the development of
advanced algorithms, particularly data-hungry deep learning models. To address
this critical bottleneck, this paper introduces Print2Volume, a novel framework
for generating realistic, synthetic OCT-based 3D fingerprints from 2D
fingerprint image. Our framework operates in three sequential stages: (1) a 2D
style transfer module that converts a binary fingerprint into a grayscale
images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D
Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D
anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that
renders the structural volume with authentic textures, speckle noise, and other
imaging characteristics. Using Print2Volume, we generated a large-scale
synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the
high quality of our synthetic data and its significant impact on recognition
performance. By pre-training a recognition model on our synthetic data and
fine-tuning it on a small real-world dataset, we achieved a remarkable
reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD
benchmark, proving the effectiveness of our approach in overcoming data
scarcity.

</details>


### [22] [GLENDA: Gynecologic Laparoscopy Endometriosis Dataset](https://arxiv.org/abs/2508.21398)
*Andreas Leibetseder,Sabrina Kletz,Klaus Schoeffmann,Simon Keckstein,Jörg Keckstein*

Main category: cs.CV

TL;DR: 论文介绍了GLENDA数据集，用于支持妇科腹腔镜手术中子宫内膜异位症的计算机视觉和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 手动分析手术视频耗时且繁琐，需要更高效的计算机视觉和机器学习方法，但医疗领域样本数据稀缺。

Method: 发布首个包含子宫内膜异位症区域标注的图像数据集GLENDA，与医学专家合作创建。

Result: GLENDA是首个此类数据集，填补了医疗领域样本数据的空白。

Conclusion: GLENDA数据集为妇科腹腔镜手术的自动化分析提供了重要资源，推动了相关技术的发展。

Abstract: Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is
performed via a live feed of a patient's abdomen surveying the insertion and
handling of various instruments for conducting treatment. Adopting this kind of
surgical intervention not only facilitates a great variety of treatments, the
possibility of recording said video streams is as well essential for numerous
post-surgical activities, such as treatment planning, case documentation and
education. Nonetheless, the process of manually analyzing surgical recordings,
as it is carried out in current practice, usually proves tediously
time-consuming. In order to improve upon this situation, more sophisticated
computer vision as well as machine learning approaches are actively developed.
Since most of such approaches heavily rely on sample data, which especially in
the medical field is only sparsely available, with this work we publish the
Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset
containing region-based annotations of a common medical condition named
endometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the
first of its kind and it has been created in collaboration with leading medical
experts in the field.

</details>


### [23] [Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation](https://arxiv.org/abs/2508.21399)
*Sabrina Kletz,Klaus Schoeffmann,Jenny Benois-Pineau,Heinrich Husslein*

Main category: cs.CV

TL;DR: 该论文研究了在腹腔镜妇科手术视频中分割和识别手术器械的方法，使用基于区域的完全卷积网络进行实例感知分割和识别。


<details>
  <summary>Details</summary>
Motivation: 手术视频记录已成为医学内窥镜领域的重要信息来源，但自动内容索引仍面临挑战，尤其是手术器械的分割和识别。

Method: 采用基于区域的完全卷积网络，分别进行二进制分割（器械与背景）和多类器械识别（器械类型）。

Result: 实验表明，即使训练样本较少，也能高精度定位和分割器械区域，但器械类型识别仍具挑战性。

Conclusion: 该方法在器械分割上表现良好，但器械识别仍需改进，尤其是针对相似度高的器械。

Abstract: Recorded videos from surgeries have become an increasingly important
information source for the field of medical endoscopy, since the recorded
footage shows every single detail of the surgery. However, while video
recording is straightforward these days, automatic content indexing - the basis
for content-based search in a medical video archive - is still a great
challenge due to the very special video content. In this work, we investigate
segmentation and recognition of surgical instruments in videos recorded from
laparoscopic gynecology. More precisely, we evaluate the achievable performance
of segmenting surgical instruments from their background by using a
region-based fully convolutional network for instance-aware (1) instrument
segmentation as well as (2) instrument recognition. While the first part
addresses only binary segmentation of instances (i.e., distinguishing between
instrument or background) we also investigate multi-class instrument
recognition (i.e., identifying the type of instrument). Our evaluation results
show that even with a moderately low number of training examples, we are able
to localize and segment instrument regions with a pretty high accuracy.
However, the results also reveal that determining the particular instrument is
still very challenging, due to the inherently high similarity of surgical
instruments.

</details>


### [24] [SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing](https://arxiv.org/abs/2508.21402)
*Jakub Straka,Ivan Gruber*

Main category: cs.CV

TL;DR: SatDINO是一种基于DINO的自监督学习方法，专为卫星图像设计，性能优于MAE等现有方法，并提出了GSD编码和自适应视图采样等新增强技术。


<details>
  <summary>Details</summary>
Motivation: 利用自监督学习处理大量未标记的遥感数据，提升卫星图像的表征学习能力。

Method: 采用DINO对比自监督方法，提出SatDINO模型，并引入GSD编码和自适应视图采样等增强技术。

Result: SatDINO在多个数据集和测试设置中优于MAE等现有方法，并在多个基准测试中取得竞争性结果。

Conclusion: SatDINO为卫星图像的表征学习提供了高效解决方案，其增强技术可独立应用。

Abstract: Self-supervised learning has emerged as a powerful tool for remote sensing,
where large amounts of unlabeled data are available. In this work, we
investigate the use of DINO, a contrastive self-supervised method, for
pretraining on remote sensing imagery. We introduce SatDINO, a model tailored
for representation learning in satellite imagery. Through extensive experiments
on multiple datasets in multiple testing setups, we demonstrate that SatDINO
outperforms other state-of-the-art methods based on much more common masked
autoencoders (MAE) and achieves competitive results in multiple benchmarks.
  We also provide a rigorous ablation study evaluating SatDINO's individual
components. Finally, we propose a few novel enhancements, such as a new way to
incorporate ground sample distance (GSD) encoding and adaptive view sampling.
These enhancements can be used independently on our SatDINO model. Our code and
trained models are available at: https://github.com/strakaj/SatDINO.

</details>


### [25] [Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives](https://arxiv.org/abs/2508.21418)
*Gernot Fiala,Markus Plass,Robert Harb,Peter Regitnig,Kristijan Skok,Wael Al Zoughbi,Carmen Zerner,Paul Torke,Michaela Kargl,Heimo Müller,Tomas Brazdil,Matej Gallo,Jaroslav Kubín,Roman Stoklasa,Rudolf Nenutil,Norman Zerbe,Andreas Holzinger,Petr Holub*

Main category: cs.CV

TL;DR: 提出了一种生成全切片图像（WSI）2D索引图和特定应用领域分析机制的通用框架，用于解决WSI元数据标准化问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏WSI元数据标准，手动检查不适合大规模数据集，需要自动化解决方案。

Method: 通过三层组织（来源、组织类型、病理变化）生成详细的组织图，提供WSI内容的细粒度信息。

Result: 在临床病理学中验证了方法的可行性，实现了不同目录间的互操作性。

Conclusion: 提出的标准在WSI目录、机器学习和图表示中展示了优势和适用性。

Abstract: A Whole Slide Image (WSI) is a high-resolution digital image created by
scanning an entire glass slide containing a biological specimen, such as tissue
sections or cell samples, at multiple magnifications. These images can be
viewed, analyzed, shared digitally, and are used today for Artificial
Intelligence (AI) algorithm development. WSIs are used in a variety of fields,
including pathology for diagnosing diseases and oncology for cancer research.
They are also utilized in neurology, veterinary medicine, hematology,
microbiology, dermatology, pharmacology, toxicology, immunology, and forensic
science.
  When assembling cohorts for the training or validation of an AI algorithm, it
is essential to know what is present on such a WSI. However, there is currently
no standard for this metadata, so such selection has mainly been done through
manual inspection, which is not suitable for large collections with several
million objects.
  We propose a general framework to generate a 2D index map for WSI and a
profiling mechanism for specific application domains. We demonstrate this
approach in the field of clinical pathology, using common syntax and semantics
to achieve interoperability between different catalogs.
  Our approach augments each WSI collection with a detailed tissue map that
provides fine-grained information about the WSI content. The tissue map is
organized into three layers: source, tissue type, and pathological alterations,
with each layer assigning segments of the WSI to specific classes.
  We illustrate the advantages and applicability of the proposed standard
through specific examples in WSI catalogs, Machine Learning (ML), and
graph-based WSI representations.

</details>


### [26] [Unsupervised Incremental Learning Using Confidence-Based Pseudo-Labels](https://arxiv.org/abs/2508.21424)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出了一种基于置信度伪标签的无监督增量学习方法（ICPL），用于从未标记数据中学习新类别，并在多个数据集上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，新类别不断出现且数据通常未标记，现有增量学习方法依赖全标记数据，不切实际。

Method: 利用置信度伪标签替代人工标注，将其集成到多种CIL方法中，并通过置信度选择优化性能。

Result: ICPL在CIFAR100和ImageNet100上性能接近监督方法，且在细粒度数据集上表现优异，计算复杂度低。

Conclusion: ICPL在无监督增量学习中表现优异，优于现有方法，适用于资源受限环境。

Abstract: Deep learning models have achieved state-of-the-art performance in many
computer vision tasks. However, in real-world scenarios, novel classes that
were unseen during training often emerge, requiring models to acquire new
knowledge incrementally. Class-Incremental Learning (CIL) methods enable a
model to learn novel classes while retaining knowledge of previous classes.
However, these methods make the strong assumption that the incremental dataset
is fully labeled, which is unrealistic in practice. In this work, we propose an
unsupervised Incremental Learning method using Confidence-based Pseudo-labels
(ICPL), which replaces human annotations with pseudo-labels, enabling
incremental learning from unlabeled datasets. We integrate these pseudo-labels
into various CIL methods with confidence-based selection and evaluate
performance degradation on CIFAR100 and ImageNet100. Then, we compare our
approach to popular Class Incremental Novel Category Discovery (class-iNCD)
methods addressing similar challenges. Additionally, we apply our method to
fine-grained datasets to demonstrate its real-world practicality and measure
its computational complexity to validate its suitability for
resource-constrained environments. ICPL achieves competitive results compared
to supervised methods and outperforms state-of-the-art class-iNCD methods by
more than 5% in final accuracy.

</details>


### [27] [MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation](https://arxiv.org/abs/2508.21435)
*Francisco Caetano,Christiaan Viviers,Peter H. H. de With,Fons van der Sommen*

Main category: cs.CV

TL;DR: MedShift提出了一种基于Flow Matching和Schrodinger Bridges的生成模型，用于解决合成与真实X射线图像之间的跨域翻译问题，并在新数据集X-DigiSkull上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 合成医学数据在训练模型时具有可扩展性，但与真实临床数据存在显著域差距，限制了其泛化能力。本文旨在解决合成与真实X射线图像之间的跨域翻译问题。

Method: 提出MedShift模型，基于Flow Matching和Schrodinger Bridges，学习域无关的共享潜在空间，支持多域间的无缝翻译。

Result: 实验表明，MedShift在较小模型规模下表现优异，支持灵活调整以优先感知保真度或结构一致性。

Conclusion: MedShift是一种可扩展且通用的医学图像域适应解决方案，代码和数据集已公开。

Abstract: Synthetic medical data offers a scalable solution for training robust models,
but significant domain gaps limit its generalizability to real-world clinical
settings. This paper addresses the challenge of cross-domain translation
between synthetic and real X-ray images of the head, focusing on bridging
discrepancies in attenuation behavior, noise characteristics, and soft tissue
representation. We propose MedShift, a unified class-conditional generative
model based on Flow Matching and Schrodinger Bridges, which enables
high-fidelity, unpaired image translation across multiple domains. Unlike prior
approaches that require domain-specific training or rely on paired data,
MedShift learns a shared domain-agnostic latent space and supports seamless
translation between any pair of domains seen during training. We introduce
X-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays
under varying radiation doses, to benchmark domain translation models.
Experimental results demonstrate that, despite its smaller model size compared
to diffusion-based approaches, MedShift offers strong performance and remains
flexible at inference time, as it can be tuned to prioritize either perceptual
fidelity or structural consistency, making it a scalable and generalizable
solution for domain adaptation in medical imaging. The code and dataset are
available at https://caetas.github.io/medshift.html

</details>


### [28] [Trees as Gaussians: Large-Scale Individual Tree Mapping](https://arxiv.org/abs/2508.21437)
*Dimitri Gominski,Martin Brandt,Xiaoye Tong,Siyu Liu,Maurice Mugabowindekwe,Sizhuo Li,Florian Reiner,Andrew Davies,Rasmus Fensholt*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的全球尺度高分辨率树木检测方法，通过模拟树冠提取中心并生成覆盖图，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模监测单棵树的能力受限，现有全球产品无法在个体水平上识别树木。

Method: 使用深度学习模型，基于高斯核模拟树冠，从机载激光雷达数据自动提取训练点。

Result: 模型性能优异（R²=0.81），在不同生物群落中表现均衡，可通过人工标注进一步优化。

Conclusion: 该方法为全球高分辨率树木监测提供了可扩展框架，适用于未来卫星图像。

Abstract: Trees are key components of the terrestrial biosphere, playing vital roles in
ecosystem function, climate regulation, and the bioeconomy. However,
large-scale monitoring of individual trees remains limited by inadequate
modelling. Available global products have focused on binary tree cover or
canopy height, which do not explicitely identify trees at individual level. In
this study, we present a deep learning approach for detecting large individual
trees in 3-m resolution PlanetScope imagery at a global scale. We simulate tree
crowns with Gaussian kernels of scalable size, allowing the extraction of crown
centers and the generation of binary tree cover maps. Training is based on
billions of points automatically extracted from airborne lidar data, enabling
the model to successfully identify trees both inside and outside forests. We
compare against existing tree cover maps and airborne lidar with
state-of-the-art performance (fractional cover R$^2 = 0.81$ against aerial
lidar), report balanced detection metrics across biomes, and demonstrate how
detection can be further improved through fine-tuning with manual labels. Our
method offers a scalable framework for global, high-resolution tree monitoring,
and is adaptable to future satellite missions offering improved imagery.

</details>


### [29] [Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content](https://arxiv.org/abs/2508.21444)
*Jiayu Yang,Weijian Su,Songqian Zhang,Yuqi Han,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）在动态场景中存在数据量大和训练时间长的问题。本文提出了一种可扩展的高斯泼溅框架，通过分层组织高斯球和混合变形策略，显著提升了训练效率和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在动态场景中数据量大和训练时间长的问题，以实现高效训练和高保真渲染。

Method: 采用分层组织的高斯球结构，结合混合变形和生成策略，以及双向自适应掩码机制。

Result: 实验表明，该方法在视觉质量上优于现有技术，同时显著减少了训练时间。

Conclusion: 提出的框架在动态场景中实现了高效训练和高保真渲染，具有显著优势。

Abstract: 3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key
requirement for immersive applications. However, the extension of 3DGS to
dynamic scenes remains limitations on the substantial data volume of dense
Gaussians and the prolonged training time required for each frame. This paper
presents \M, a scalable Gaussian Splatting framework designed for efficient
training in streaming tasks. Specifically, Gaussian spheres are hierarchically
organized by scale within an anchor-based structure. Coarser-level Gaussians
represent the low-resolution structure of the scene, while finer-level
Gaussians, responsible for detailed high-fidelity rendering, are selectively
activated by the coarser-level Gaussians. To further reduce computational
overhead, we introduce a hybrid deformation and spawning strategy that models
motion of inter-frame through Gaussian deformation and triggers Gaussian
spawning to characterize wide-range motion. Additionally, a bidirectional
adaptive masking mechanism enhances training efficiency by removing static
regions and prioritizing informative viewpoints. Extensive experiments
demonstrate that \M~ achieves superior visual quality while significantly
reducing training time compared to state-of-the-art methods.

</details>


### [30] [One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist](https://arxiv.org/abs/2508.21451)
*Junha Song,Yongsik Jo,So Yeon Min,Quanting Xie,Taehwan Kim,Yonatan Bisk,Jaegul Choo*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级图像描述模型，性能接近大型多模态通用模型，并开发了Sharp-Eyed Refinement框架以解决视觉盲区问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在本地设备上部署时的高计算需求问题。

Method: 基于125M参数的语言模型实现轻量级描述模型，并通过Sharp-Eyed Refinement框架改进视觉注意力机制。

Result: 模型性能接近大型通用模型，但存在视觉盲区问题；新框架显著提升了描述质量。

Conclusion: 轻量级模型和Sharp-Eyed Refinement框架为本地设备应用提供了高效解决方案。

Abstract: Image captioning is fundamental for applications like video instruction
systems and exploration robots, yet deploying such models on local devices is
challenging due to the high computational demands of multimodal large language
models (MLLMs). To address this, we first explore lightweight captioning by
implementing a specialist based on a 125M-parameter language model, 56 times
smaller than LLaMA-7B, and evaluating its performance on both single-sentence
and detailed captioning tasks. Surprisingly, we find that our model can achieve
performance comparable to large multimodal generalists, suggesting its
potential to serve as a strong visual specialist for on-device applications.
While promising, our model also exhibits a limitation: like other MLLMs, it
suffers from visual blindness, occasionally resulting in semantic captioning
errors. We carry out toy experiments and investigate the underlying causes,
where we observe that the problems arise from ineffective attention mechanisms
and limited visual representations. To alleviate them, we develop a novel
captioning framework, Sharp-Eyed Refinement, which enhances caption quality
through improved visual grounding. At its core, our DeepLens extracts detailed
visual representations by concentrating on informative regions identified
during the initial glance. Our experiments confirm both the advantages of our
specialist over prior small captioning models and large generalists and the
effectiveness of our framework.

</details>


### [31] [Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification](https://arxiv.org/abs/2508.21458)
*Kaouther Mouheb,Marawan Elbatel,Janne Papma,Geert Jan Biessels,Jurgen Claassen,Huub Middelkoop,Barbara van Munster,Wiesje van der Flier,Inez Ramakers,Stefan Klein,Esther E. Bron*

Main category: cs.CV

TL;DR: 该论文研究了基础模型（FMs）在联邦学习（FL）系统中用于痴呆症诊断的性能和效率，重点评估了分类头架构、微调策略和聚合方法的影响。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在联邦学习系统中用于痴呆症诊断的潜力，填补相关研究的空白。

Method: 使用多队列脑MRI数据，系统评估分类头架构、微调策略和聚合方法对联邦FM调优的影响。

Result: 分类头架构显著影响性能，冻结FM编码器与完全微调效果相当，高级聚合方法优于标准联邦平均。

Conclusion: 研究结果为在分散临床环境中部署基础模型提供了实用见解，并指导未来方法开发的权衡。

Abstract: While foundation models (FMs) offer strong potential for AI-based dementia
diagnosis, their integration into federated learning (FL) systems remains
underexplored. In this benchmarking study, we systematically evaluate the
impact of key design choices: classification head architecture, fine-tuning
strategy, and aggregation method, on the performance and efficiency of
federated FM tuning using brain MRI data. Using a large multi-cohort dataset,
we find that the architecture of the classification head substantially
influences performance, freezing the FM encoder achieves comparable results to
full fine-tuning, and advanced aggregation methods outperform standard
federated averaging. Our results offer practical insights for deploying FMs in
decentralized clinical settings and highlight trade-offs that should guide
future method development.

</details>


### [32] [Multi-Method Ensemble for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21463)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出了一种多方法集成（MME）评分，结合特征截断和评分函数，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只关注单一技术或特定OOD数据集，忽略了结合多种现有解决方案的潜力。

Method: 理论及实证证明特征截断和评分函数可有效结合，并提出MME评分统一多种OOD检测器。

Result: 在多个基准测试中显著优于现有方法，如在ImageNet-1K上FPR95降低6%。

Conclusion: MME评分通过结合多种技术提升了OOD检测的鲁棒性和性能。

Abstract: Detecting out-of-distribution (OOD) samples is essential for neural networks
operating in open-world settings, particularly in safety-critical applications.
Existing methods have improved OOD detection by leveraging two main techniques:
feature truncation, which increases the separation between in-distribution (ID)
and OOD samples, and scoring functions, which assign scores to distinguish
between ID and OOD data. However, most approaches either focus on a single
family of techniques or evaluate their effectiveness on a specific type of OOD
dataset, overlooking the potential of combining multiple existing solutions.
Motivated by this observation, we theoretically and empirically demonstrate
that state-of-the-art feature truncation and scoring functions can be
effectively combined. Moreover, we show that aggregating multiple scoring
functions enhances robustness against various types of OOD samples. Based on
these insights, we propose the Multi-Method Ensemble (MME) score, which unifies
state-of-the-art OOD detectors into a single, more effective scoring function.
Extensive experiments on both large-scale and small-scale benchmarks, covering
near-OOD and far-OOD scenarios, show that MME significantly outperforms recent
state-of-the-art methods across all benchmarks. Notably, using the BiT model,
our method achieves an average FPR95 of 27.57% on the challenging ImageNet-1K
benchmark, improving performance by 6% over the best existing baseline.

</details>


### [33] [Adversarial Patch Attack for Ship Detection via Localized Augmentation](https://arxiv.org/abs/2508.21472)
*Chun Liu,Panpan Ding,Zheng Zheng,Hailong Wang,Bingqian Zhu,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 提出了一种针对遥感图像中船舶检测的局部增强方法，通过仅增强目标区域，减少背景干扰，提高对抗补丁攻击的成功率和可转移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于DNN的船舶检测技术易受对抗补丁攻击，传统数据增强方法可能因过度增强背景或非目标区域引入干扰，导致误检。

Method: 提出局部增强方法，仅对目标区域进行增强，避免影响非目标区域，使损失函数更直接关注对抗补丁对检测模型的影响。

Result: 在HRSC2016数据集上的实验表明，该方法有效提高了对抗补丁攻击的成功率和可转移性。

Conclusion: 局部增强方法通过减少背景干扰，显著提升了对抗补丁攻击的效果。

Abstract: Current ship detection techniques based on remote sensing imagery primarily
rely on the object detection capabilities of deep neural networks (DNNs).
However, DNNs are vulnerable to adversarial patch attacks, which can lead to
misclassification by the detection model or complete evasion of the targets.
Numerous studies have demonstrated that data transformation-based methods can
improve the transferability of adversarial examples. However, excessive
augmentation of image backgrounds or irrelevant regions may introduce
unnecessary interference, resulting in false detections of the object detection
model. These errors are not caused by the adversarial patches themselves but
rather by the over-augmentation of background and non-target areas. This paper
proposes a localized augmentation method that applies augmentation only to the
target regions, avoiding any influence on non-target areas. By reducing
background interference, this approach enables the loss function to focus more
directly on the impact of the adversarial patch on the detection model, thereby
improving the attack success rate. Experiments conducted on the HRSC2016
dataset demonstrate that the proposed method effectively increases the success
rate of adversarial patch attacks and enhances their transferability.

</details>


### [34] [ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding](https://arxiv.org/abs/2508.21496)
*Hao Lu,Jiahao Wang,Yaolun Zhang,Ruohui Wang,Xuanyu Zheng,Yepeng Tang,Dahua Lin,Lewei Lu*

Main category: cs.CV

TL;DR: 论文提出了ELV-Halluc基准，首次系统研究长视频中的语义聚合幻觉（SAH），并通过实验验证其存在与影响因素，同时提出缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态大语言模型（Video-MLLMs）在长视频中易产生语义聚合幻觉（SAH），但此前研究主要关注短视频，忽略了SAH的复杂性。

Method: 引入ELV-Halluc基准，分析SAH的成因，并提出位置编码策略和DPO策略来缓解SAH。

Result: 实验证实SAH随语义复杂度增加而加剧，且模型在语义快速变化时更易产生SAH。通过DPO策略，SAH比例显著降低27.7%。

Conclusion: 论文填补了长视频幻觉研究的空白，为未来模型优化提供了方向。

Abstract: Video multimodal large language models (Video-MLLMs) have achieved remarkable
progress in video understanding. However, they remain vulnerable to
hallucination-producing content inconsistent with or unrelated to video inputs.
Previous video hallucination benchmarks primarily focus on short-videos. They
attribute hallucinations to factors such as strong language priors, missing
frames, or vision-language biases introduced by the visual encoder. While these
causes indeed account for most hallucinations in short videos, they still
oversimplify the cause of hallucinations. Sometimes, models generate incorrect
outputs but with correct frame-level semantics. We refer to this type of
hallucination as Semantic Aggregation Hallucination (SAH), which arises during
the process of aggregating frame-level semantics into event-level semantic
groups. Given that SAH becomes particularly critical in long videos due to
increased semantic complexity across multiple events, it is essential to
separate and thoroughly investigate the causes of this type of hallucination.
To address the above issues, we introduce ELV-Halluc, the first benchmark
dedicated to long-video hallucination, enabling a systematic investigation of
SAH. Our experiments confirm the existence of SAH and show that it increases
with semantic complexity. Additionally, we find that models are more prone to
SAH on rapidly changing semantics. Moreover, we discuss potential approaches to
mitigate SAH. We demonstrate that positional encoding strategy contributes to
alleviating SAH, and further adopt DPO strategy to enhance the model's ability
to distinguish semantics within and across events. To support this, we curate a
dataset of 8K adversarial data pairs and achieve improvements on both
ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.

</details>


### [35] [Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation](https://arxiv.org/abs/2508.21529)
*Ronan Docherty,Antonis Vamvakeros,Samuel J. Cooper*

Main category: cs.CV

TL;DR: 提出一种卷积神经网络用于上采样低分辨率基础模型特征，提升显微镜图像分割效率。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在微图像和大尺寸图像中表现不佳的问题。

Method: 训练卷积神经网络上采样低分辨率特征，应用于多种显微镜图像分割。

Result: 上采样特征显著提升分割质量，减少标注需求。

Conclusion: 该方法高效且适用于多种复杂场景。

Abstract: Feature foundation models - usually vision transformers - offer rich semantic
descriptors of images, useful for downstream tasks such as (interactive)
segmentation and object detection. For computational efficiency these
descriptors are often patch-based, and so struggle to represent the fine
features often present in micrographs; they also struggle with the large image
sizes present in materials and biological image analysis. In this work, we
train a convolutional neural network to upsample low-resolution (i.e, large
patch size) foundation model features with reference to the input image. We
apply this upsampler network (without any further training) to efficiently
featurise and then segment a variety of microscopy images, including plant
cells, a lithium-ion battery cathode and organic crystals. The richness of
these upsampled features admits separation of hard to segment phases, like
hairline cracks. We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.

</details>


### [36] [HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones](https://arxiv.org/abs/2508.21539)
*Hao Ruan,Jinliang Lin,Yingxin Lai,Zhiming Luo,Shaozi Li*

Main category: cs.CV

TL;DR: HCCM框架通过区域-全局对比学习和匹配学习，解决了无人机场景中视觉-语言理解的挑战，并在多个数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 无人机场景中的宽视场和复杂语义对视觉-语言理解提出了挑战，现有方法缺乏细粒度语义和动态环境适应性。

Method: 提出HCCM框架，包括区域-全局图像-文本对比学习（RG-ITC）和匹配学习（RG-ITM），以及动量对比与蒸馏机制（MCD）。

Result: 在GeoText-1652和ERA数据集上，HCCM分别达到28.8%和39.93%的召回率，优于现有方法。

Conclusion: HCCM通过分层跨粒度学习，显著提升了无人机场景中的视觉-语言理解能力，并展示了强大的零样本泛化能力。

Abstract: Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such
as target matching and navigation. However, the wide field of view and complex
compositional semantics in drone scenarios pose challenges for vision-language
understanding. Mainstream Vision-Language Models (VLMs) emphasize global
alignment while lacking fine-grained semantics, and existing hierarchical
methods depend on precise entity partitioning and strict containment, limiting
effectiveness in dynamic environments. To address this, we propose the
Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM)
framework with two components: (1) Region-Global Image-Text Contrastive
Learning (RG-ITC), which avoids precise scene partitioning and captures
hierarchical local-to-global semantics by contrasting local visual regions with
global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),
which dispenses with rigid constraints and instead evaluates local semantic
consistency within global cross-modal representations, enhancing compositional
reasoning. Moreover, drone text descriptions are often incomplete or ambiguous,
destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation
(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM
achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text
retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot
generalization with 39.93% mean recall (mR), outperforming fine-tuned
baselines.

</details>


### [37] [EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting](https://arxiv.org/abs/2508.21550)
*Yujin Park,Haejun Chung,Ikbeom Jang*

Main category: cs.CV

TL;DR: EZ-Sort结合CLIP模型和不确定性采样，显著减少人工标注成本，同时保持或提高排序可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决传统成对比较方法标注成本高的问题，利用CLIP模型和自动化比较提升效率。

Method: 1. 使用CLIP模型进行零样本预排序；2. 初始化基于桶的Elo分数；3. 运行不确定性引导的MergeSort。

Result: 在多个数据集上，EZ-Sort减少人工标注成本90.5%，比先前工作节省19.8%，同时保持或提高可靠性。

Conclusion: 结合CLIP先验和不确定性采样，提供了一种高效且可扩展的成对排序解决方案。

Abstract: Pairwise comparison is often favored over absolute rating or ordinal
classification in subjective or difficult annotation tasks due to its improved
reliability. However, exhaustive comparisons require a massive number of
annotations (O(n^2)). Recent work has greatly reduced the annotation burden
(O(n log n)) by actively sampling pairwise comparisons using a sorting
algorithm. We further improve annotation efficiency by (1) roughly pre-ordering
items using the Contrastive Language-Image Pre-training (CLIP) model
hierarchically without training, and (2) replacing easy, obvious human
comparisons with automated comparisons. The proposed EZ-Sort first produces a
CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,
and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation
was conducted using various datasets: face-age estimation (FGNET), historical
image chronology (DHCI), and retinal image quality assessment (EyePACS). It
showed that EZ-Sort reduced human annotation cost by 90.5% compared to
exhaustive pairwise comparisons and by 19.8% compared to prior work (when n =
100), while improving or maintaining inter-rater reliability. These results
demonstrate that combining CLIP-based priors with uncertainty-aware sampling
yields an efficient and scalable solution for pairwise ranking.

</details>


### [38] [ECHO: Ego-Centric modeling of Human-Object interactions](https://arxiv.org/abs/2508.21556)
*Ilya A. Petrov,Vladimir Guzov,Riccardo Marin,Emre Aksan,Xu Chen,Daniel Cremers,Thabo Beeler,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: ECHO是一个从头部和手腕跟踪数据中恢复人类-物体交互（HOI）的统一框架，首次实现了人类姿态、物体运动和接触的联合建模。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备的普及，从第一人称视角建模HOI变得重要，但现有方法缺乏灵活性。

Method: ECHO采用Diffusion Transformer架构和三变量扩散过程，在头部中心坐标系中运行，支持灵活输入配置。

Result: ECHO在HOI重建中表现优于现有方法，达到最先进水平。

Conclusion: ECHO为第一人称视角的HOI建模提供了高效且灵活的解决方案。

Abstract: Modeling human-object interactions (HOI) from an egocentric perspective is a
largely unexplored yet important problem due to the increasing adoption of
wearable devices, such as smart glasses and watches. We investigate how much
information about interaction can be recovered from only head and wrists
tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object
interactions), which, for the first time, proposes a unified framework to
recover three modalities: human pose, object motion, and contact from such
minimal observation. ECHO employs a Diffusion Transformer architecture and a
unique three-variate diffusion process, which jointly models human motion,
object trajectory, and contact sequence, allowing for flexible input
configurations. Our method operates in a head-centric canonical space,
enhancing robustness to global orientation. We propose a conveyor-based
inference, which progressively increases the diffusion timestamp with the frame
position, allowing us to process sequences of any length. Through extensive
evaluation, we demonstrate that ECHO outperforms existing methods that do not
offer the same flexibility, setting a state-of-the-art in egocentric HOI
reconstruction.

</details>


### [39] [How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images](https://arxiv.org/abs/2508.21565)
*Juneyoung Ro,Namwoo Kim,Yoonjin Yoon*

Main category: cs.CV

TL;DR: 研究比较了三种视觉语言模型（BLIP-2、InstructBLIP和LLaVA-1.5）在城市场景中的空间推理能力，发现零样本表现尚可，但通过合成数据集微调后性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索通用视觉语言模型在城市场景中的空间推理能力，填补该领域的研究空白。

Method: 构建合成VQA数据集，结合分割、深度和物体检测预测，并使用LLM生成链式推理答案进行微调。

Result: 零样本表现合理，微调后性能显著提升，尤其是对否定和反事实等复杂问题。

Conclusion: 城市空间推理是视觉语言模型的新挑战，合成数据集是适应专业领域的实用方法。

Abstract: Effectively understanding urban scenes requires fine-grained spatial
reasoning about objects, layouts, and depth cues. However, how well current
vision-language models (VLMs), pretrained on general scenes, transfer these
abilities to urban domain remains underexplored. To address this gap, we
conduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,
and LLaVA-1.5-evaluating both zero-shot performance and the effects of
fine-tuning with a synthetic VQA dataset specific to urban scenes. We construct
such dataset from segmentation, depth, and object detection predictions of
street-view images, pairing each question with LLM-generated Chain-of-Thought
(CoT) answers for step-by-step reasoning supervision. Results show that while
VLMs perform reasonably well in zero-shot settings, fine-tuning with our
synthetic CoT-supervised dataset substantially boosts performance, especially
for challenging question types such as negation and counterfactuals. This study
introduces urban spatial reasoning as a new challenge for VLMs and demonstrates
synthetic dataset construction as a practical path for adapting general-purpose
models to specialized domains.

</details>


### [40] [Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging](https://arxiv.org/abs/2508.21580)
*Nico Albert Disch,Yannick Kirchhoff,Robin Peretzke,Maximilian Rokuss,Saikat Roy,Constantin Ulrich,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: TFM是一种统一的生成轨迹方法，用于学习医学影像中的时间动态分布，支持3D体积和多时间点预测。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在医学影像中多关注单时间点或特定任务，缺乏对时间动态的细粒度空间预测能力。

Method: 提出Temporal Flow Matching (TFM)，支持3D体积、多时间点扫描和不规则采样，并能退化为最近图像预测器。

Result: 在三个公共纵向数据集上，TFM超越自然影像的时空方法，成为4D医学影像预测的新基准。

Conclusion: TFM填补了医学影像时间动态建模的空白，提供了鲁棒的预测方法。

Abstract: Understanding temporal dynamics in medical imaging is crucial for
applications such as disease progression modeling, treatment planning and
anatomical development tracking. However, most deep learning methods either
consider only single temporal contexts, or focus on tasks like classification
or regression, limiting their ability for fine-grained spatial predictions.
While some approaches have been explored, they are often limited to single
timepoints, specific diseases or have other technical restrictions. To address
this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified
generative trajectory method that (i) aims to learn the underlying temporal
distribution, (ii) by design can fall back to a nearest image predictor, i.e.
predicting the last context image (LCI), as a special case, and (iii) supports
$3D$ volumes, multiple prior scans, and irregular sampling. Extensive
benchmarks on three public longitudinal datasets show that TFM consistently
surpasses spatio-temporal methods from natural imaging, establishing a new
state-of-the-art and robust baseline for $4D$ medical image prediction.

</details>


### [41] [Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer](https://arxiv.org/abs/2508.21581)
*Daniël Boeke,Cedrik Blommestijn,Rebecca N. Wray,Kalina Chupetlovska,Shangqi Gao,Zeyu Gao,Regina G. H. Beets-Tan,Mireia Crispin-Ortuzar,James O. Jones,Wilson Silva,Ines P. Machado*

Main category: cs.CV

TL;DR: 研究评估了多模态（CT和WSI）深度学习框架在ccRCC复发预测中的表现，发现病理学数据（WSI）优于CT，中间融合策略进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: Leibovich评分在ccRCC复发风险分层中应用广泛，但分辨率有限且缺乏影像信息，需改进。

Method: 采用预训练编码器和Cox生存模型的深度学习框架，测试了单模态、晚期融合和中间融合策略。

Result: WSI模型优于CT模型，中间融合性能最佳，接近Leibovich评分。

Conclusion: 多模态融合可行，未来需探索更强大的融合策略和CT编码器。

Abstract: Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is
essential for guiding postoperative surveillance and treatment. The Leibovich
score remains widely used for stratifying distant recurrence risk but offers
limited patient-level resolution and excludes imaging information. This study
evaluates multimodal recurrence prediction by integrating preoperative computed
tomography (CT) and postoperative histopathology whole-slide images (WSIs). A
modular deep learning framework with pretrained encoders and Cox-based survival
modeling was tested across unimodal, late fusion, and intermediate fusion
setups. In a real-world ccRCC cohort, WSI-based models consistently
outperformed CT-only models, underscoring the prognostic strength of pathology.
Intermediate fusion further improved performance, with the best model
(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random
tie-breaking narrowed the gap between the clinical baseline and learned models,
suggesting discretization may overstate individualized performance. Using
simple embedding concatenation, radiology added value primarily through fusion.
These findings demonstrate the feasibility of foundation model-based multimodal
integration for personalized ccRCC risk prediction. Future work should explore
more expressive fusion strategies, larger multimodal datasets, and
general-purpose CT encoders to better match pathology modeling capacity.

</details>


### [42] [Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation](https://arxiv.org/abs/2508.21657)
*Haomiao Zhang,Zhangyuan Li,Yanling Piao,Zhi Li,Xiaodong Wang,Miao Cao,Xiongfei Su,Qiang Song,Xin Yuan*

Main category: cs.CV

TL;DR: 提出了一种基于深度展开网络（DUN）的计算机生成全息（CGH）方法，解决了现有方法在解释性、全局依赖性和工作距离上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有CGH方法存在解释性差、全局依赖性捕捉不足和工作距离受限的问题。

Method: 通过深度展开网络（DUN）分解梯度下降为自适应带宽保留模型（ABPM）和相位域复值去噪器（PCD），提升灵活性和性能。

Result: 在模拟和真实数据上实现了超过35 dB的PSNR，达到最先进水平。

Conclusion: DUN方法在CGH中表现出更高的灵活性和性能，解决了现有方法的局限性。

Abstract: Computer-generated holography (CGH) has gained wide attention with deep
learning-based algorithms. However, due to its nonlinear and ill-posed nature,
challenges remain in achieving accurate and stable reconstruction.
Specifically, ($i$) the widely used end-to-end networks treat the
reconstruction model as a black box, ignoring underlying physical
relationships, which reduces interpretability and flexibility. ($ii$) CNN-based
CGH algorithms have limited receptive fields, hindering their ability to
capture long-range dependencies and global context. ($iii$) Angular spectrum
method (ASM)-based models are constrained to finite near-fields.In this paper,
we propose a Deep Unfolding Network (DUN) that decomposes gradient descent into
two modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domain
complex-valued denoiser (PCD), providing more flexibility. ABPM allows for
wider working distances compared to ASM-based methods. At the same time, PCD
leverages its complex-valued deformable self-attention module to capture global
features and enhance performance, achieving a PSNR over 35 dB. Experiments on
simulated and real data show state-of-the-art results.

</details>


### [43] [Towards Interactive Lesion Segmentation in Whole-Body PET/CT with Promptable Models](https://arxiv.org/abs/2508.21680)
*Maximilian Rokuss,Yannick Kirchhoff,Fabian Isensee,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: 论文提出了一种基于用户提示的交互式PET/CT病灶分割方法，通过扩展nnU-Net框架并引入欧氏距离变换编码，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决PET/CT中因示踪剂异质性、生理性摄取和多中心差异导致的病灶分割挑战，同时满足临床实践中人机交互的需求。

Method: 扩展nnU-Net框架，将用户提供的前景和背景点击编码为额外输入通道，并比较了欧氏距离变换和高斯核的表现。

Result: 基于欧氏距离变换的模型在交叉验证中表现最佳，显著减少了假阳性和假阴性。

Conclusion: 交互式提示模型在多示踪剂、多中心PET/CT中具有高效分割潜力，代码已开源。

Abstract: Whole-body PET/CT is a cornerstone of oncological imaging, yet accurate
lesion segmentation remains challenging due to tracer heterogeneity,
physiological uptake, and multi-center variability. While fully automated
methods have advanced substantially, clinical practice benefits from approaches
that keep humans in the loop to efficiently refine predicted masks. The
autoPET/CT IV challenge addresses this need by introducing interactive
segmentation tasks based on simulated user prompts. In this work, we present
our submission to Task 1. Building on the winning autoPET III nnU-Net pipeline,
we extend the framework with promptable capabilities by encoding user-provided
foreground and background clicks as additional input channels. We
systematically investigate representations for spatial prompts and demonstrate
that Euclidean Distance Transform (EDT) encodings consistently outperform
Gaussian kernels. Furthermore, we propose online simulation of user
interactions and a custom point sampling strategy to improve robustness under
realistic prompting conditions. Our ensemble of EDT-based models, trained with
and without external data, achieves the strongest cross-validation performance,
reducing both false positives and false negatives compared to baseline models.
These results highlight the potential of promptable models to enable efficient,
user-guided segmentation workflows in multi-tracer, multi-center PET/CT. Code
is publicly available at https://github.com/MIC-DKFZ/autoPET-interactive

</details>


### [44] [Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping](https://arxiv.org/abs/2508.21689)
*Fatih Erdoğan,Merve Rabia Barın,Fatma Güney*

Main category: cs.CV

TL;DR: 提出了一种基于几何映射和概率投影机制的新方法，用于从相机图像构建高精地图，通过置信度分数优化映射和过滤无关元素，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标准映射技术（如基于注意力的方法），存在泛化问题，容易产生虚假道路元素，因此需要更精确的映射方法。

Method: 结合几何映射和概率投影机制，利用置信度分数优化映射对齐场景并过滤无关元素，同时改进时间处理以累积可靠信息。

Result: 在nuScenes和Argoverse2数据集上表现优于现有方法，尤其在长感知范围内效果显著。

Conclusion: 新方法通过几何映射和概率投影机制显著提升了高精地图构建的准确性和泛化能力。

Abstract: Constructing high-definition (HD) maps from sensory input requires accurately
mapping the road elements in image space to the Bird's Eye View (BEV) space.
The precision of this mapping directly impacts the quality of the final
vectorized HD map. Existing HD mapping approaches outsource the projection to
standard mapping techniques, such as attention-based ones. However, these
methods struggle with accuracy due to generalization problems, often
hallucinating non-existent road elements. Our key idea is to start with a
geometric mapping based on camera parameters and adapt it to the scene to
extract relevant map information from camera images. To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation. In addition,
we improve temporal processing by using confidence scores to selectively
accumulate reliable information over time. Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization. The improvements
are particularly pronounced on nuScenes and in the challenging long perception
range. Our code and model checkpoints are available at
https://github.com/Fatih-Erdogan/mapping-like-skeptic .

</details>


### [45] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 论文提出从基于单词的OCR转向基于行的OCR，以提高准确性和效率，并贡献了一个新的数据集。


<details>
  <summary>Details</summary>
Motivation: 传统OCR技术因字符分割易出错且无法充分利用语言模型，而现代技术转向单词级OCR，但单词分割成为新的瓶颈。

Method: 提出从单词级OCR过渡到行级OCR，绕过单词检测错误，并提供更大的句子上下文以更好地利用语言模型。

Result: 实验显示，行级OCR在端到端准确率上提升了5.4%，效率提高了4倍。

Conclusion: 行级OCR在准确性和效率上均有显著提升，未来可结合大型语言模型进一步优化。

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


### [46] [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](https://arxiv.org/abs/2508.21712)
*Alvaro Patricio,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: FLORA是一种轻量级合成数据生成方法，通过LoRA微调显著降低计算需求，仅需10%的数据即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在数据增强中资源消耗大的问题。

Method: 使用Flux 1.1 Dev扩散模型，通过LoRA微调，减少计算需求。

Result: 在7个数据集上，仅用500张合成图像即超越ODGEN基线（5000张图像），mAP@.50:.95提升高达21.3%。

Conclusion: FLORA证明了高效且高质量的数据生成方法比暴力生成更有效，更具实用性。

Abstract: Recent advances in diffusion-based generative models have demonstrated
significant potential in augmenting scarce datasets for object detection tasks.
Nevertheless, most recent models rely on resource-intensive full fine-tuning of
large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA
V100) and thousands of synthetic images. To address these limitations, we
propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation
pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces
computational requirements, enabling synthetic dataset generation with a
consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our
approach on seven diverse object detection datasets. Our results demonstrate
that training object detectors with just 500 synthetic images generated by our
approach yields superior detection performance compared to models trained on
5000 synthetic images from the ODGEN baseline, achieving improvements of up to
21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost. This work demonstrates that a quality and efficiency-focused approach is
more effective than brute-force generation, making advanced synthetic data
creation more practical and accessible for real-world scenarios.

</details>


### [47] [Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks](https://arxiv.org/abs/2508.21715)
*Amirhossein Nazeri,Wael Hafez*

Main category: cs.CV

TL;DR: 通过监测CNN激活层的熵变化，无需修改模型即可检测对抗性扰动，实现90%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性扰动检测方法需要昂贵重新训练或修改网络架构，且可能影响模型性能。本文旨在找到一种无需修改模型即可检测对抗性扰动的方法。

Method: 通过并行熵监测VGG-16的激活层，发现对抗性扰动会导致早期卷积层的熵值变化7%。

Result: 实验显示，该方法能实现90%的检测准确率，假阳性和假阴性率低于20%。

Conclusion: CNN激活层的熵变化可以独立用于评估模型可靠性，实现实时对抗性扰动检测，且不影响原始模型性能。

Abstract: Convolutional Neural Networks (CNNs) have become the foundation of modern
computer vision, achieving unprecedented accuracy across diverse image
recognition tasks. While these networks excel on in-distribution data, they
remain vulnerable to adversarial perturbations imperceptible input
modifications that cause misclassification with high confidence. However,
existing detection methods either require expensive retraining, modify network
architecture, or degrade performance on clean inputs. Here we show that
adversarial perturbations create immediate, detectable entropy signatures in
CNN activations that can be monitored without any model modification. Using
parallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs
consistently shift activation entropy by 7% in early convolutional layers,
enabling 90% detection accuracy with false positives and false negative rates
below 20%. The complete separation between clean and adversarial entropy
distributions reveals that CNNs inherently encode distribution shifts in their
activation patterns. This work establishes that CNN reliability can be assessed
through activation entropy alone, enabling practical deployment of
self-diagnostic vision systems that detect adversarial inputs in real-time
without compromising original model performance.

</details>


### [48] [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](https://arxiv.org/abs/2508.21732)
*João Valente,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: CAD2DMD-SET工具通过合成数据提升大型视觉语言模型（LVLMs）在复杂场景下读取数字测量设备（DMDs）的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在复杂现实场景（如遮挡、运动模糊）中读取DMDs表现不佳，需改进。

Method: 利用3D CAD模型和高保真渲染生成合成数据集CAD2DMD-SET，并创建真实验证集DMDBench。

Result: 使用合成数据微调LVLMs显著提升性能（如InternVL得分提高200%）。

Conclusion: CAD2DMD-SET有效增强LVLMs在复杂条件下的鲁棒性，工具将开源以支持社区扩展。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
impressive capabilities across various multimodal tasks. They continue,
however, to struggle with trivial scenarios such as reading values from Digital
Measurement Devices (DMDs), particularly in real-world conditions involving
clutter, occlusions, extreme viewpoints, and motion blur; common in
head-mounted cameras and Augmented Reality (AR) applications. Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.
Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks. This
demonstrates that the CAD2DMD-SET training dataset substantially improves the
robustness and performance of LVLMs when operating under the previously stated
challenging conditions. The CAD2DMD-SET tool is expected to be released as
open-source once the final version of this manuscript is prepared, allowing the
community to add different measurement devices and generate their own datasets.

</details>


### [49] [Learning from Silence and Noise for Visual Sound Source Localization](https://arxiv.org/abs/2508.21761)
*Xavier Juanola,Giovana Morais,Magdalena Fuentes,Gloria Haro*

Main category: cs.CV

TL;DR: 论文提出了一种新的训练策略和指标，用于改进视觉声源定位任务，特别是在处理负面音频（如静音、噪声和屏幕外声音）时的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在低音频-视觉语义对应（如静音、噪声和屏幕外声音）情况下表现不佳，且评估仅限于正面案例。

Method: 提出了一种新的训练策略（SSL-SaN），结合静音和噪声训练，并引入新指标衡量特征对齐和分离性。

Result: SSL-SaN在自监督模型中达到最先进性能，同时改进了对负面音频的鲁棒性。

Conclusion: 论文通过新训练策略和指标，显著提升了视觉声源定位任务在复杂场景中的性能。

Abstract: Visual sound source localization is a fundamental perception task that aims
to detect the location of sounding sources in a video given its audio. Despite
recent progress, we identify two shortcomings in current methods: 1) most
approaches perform poorly in cases with low audio-visual semantic
correspondence such as silence, noise, and offscreen sounds, i.e. in the
presence of negative audio; and 2) most prior evaluations are limited to
positive cases, where both datasets and metrics convey scenarios with a single
visible sound source in the scene. To address this, we introduce three key
contributions. First, we propose a new training strategy that incorporates
silence and noise, which improves performance in positive cases, while being
more robust against negative sounds. Our resulting self-supervised model,
SSL-SaN, achieves state-of-the-art performance compared to other
self-supervised models, both in sound localization and cross-modal retrieval.
Second, we propose a new metric that quantifies the trade-off between alignment
and separability of auditory and visual features across positive and negative
audio-visual pairs. Third, we present IS3+, an extended and improved version of
the IS3 synthetic dataset with negative audio.
  Our data, metrics and code are available on the
https://xavijuanola.github.io/SSL-SaN/.

</details>


### [50] [UItron: Foundational GUI Agent with Advanced Perception and Planning](https://arxiv.org/abs/2508.21767)
*Zhixiong Zeng,Jing Huang,Liming Zheng,Wenkang Han,Yufeng Zhong,Lei Chen,Longrong Yang,Yingjie Chu,Yuzhi He,Lin Ma*

Main category: cs.CV

TL;DR: UItron是一个开源的GUI基础模型，通过数据工程和交互基础设施提升GUI代理的性能，尤其在中文应用场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: GUI代理在实现通用人工智能中至关重要，但现有模型在操作轨迹、交互基础设施和基础模型能力方面存在不足。

Method: UItron采用监督微调和课程强化学习框架，结合系统数据工程和交互环境，提升GUI感知、定位和规划能力。

Result: UItron在GUI感知、定位和规划基准测试中表现优异，尤其在中文应用场景中取得显著进展。

Conclusion: UItron通过系统性数据工程和交互基础设施，推动GUI代理向实际应用迈进一步。

Abstract: GUI agent aims to enable automated operations on Mobile/PC devices, which is
an important task toward achieving artificial general intelligence. The rapid
advancement of VLMs accelerates the development of GUI agents, owing to their
powerful capabilities in visual understanding and task planning. However,
building a GUI agent remains a challenging task due to the scarcity of
operation trajectories, the availability of interactive infrastructure, and the
limitation of initial capabilities in foundation models. In this work, we
introduce UItron, an open-source foundational model for automatic GUI agents,
featuring advanced GUI perception, grounding, and planning capabilities. UItron
highlights the necessity of systemic data engineering and interactive
infrastructure as foundational components for advancing GUI agent development.
It not only systematically studies a series of data engineering strategies to
enhance training effects, but also establishes an interactive environment
connecting both Mobile and PC devices. In training, UItron adopts supervised
finetuning over perception and planning tasks in various GUI scenarios, and
then develop a curriculum reinforcement learning framework to enable complex
reasoning and exploration for online environments. As a result, UItron achieves
superior performance in benchmarks of GUI perception, grounding, and planning.
In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions. To this end, we manually collect over one
million steps of operation trajectories across the top 100 most popular apps,
and build the offline and online agent evaluation environments. Experimental
results demonstrate that UItron achieves significant progress in Chinese app
scenarios, propelling GUI agents one step closer to real-world application.

</details>


### [51] [Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations](https://arxiv.org/abs/2508.21769)
*Ha Min Son,Zhe Zhao,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新方法CLIP-DCA，通过增强领域感知和分离分类特征来提升CLIP在域泛化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前域泛化评估对基础模型如CLIP可能不够挑战性，且未充分测试真正未见数据场景。

Method: 采用两种方法评估CLIP在域泛化中的表现，并提出CLIP-DCA方法，通过增强领域感知和分离分类特征来改进性能。

Result: CLIP-DCA在更具挑战性的评估中表现显著优于现有方法，尤其是在更OOD的数据集上。

Conclusion: 增强领域感知是基础模型中实现有效域不变分类的前提，CLIP-DCA方法为此提供了有效解决方案。

Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is
challenging, as web-scale pretraining data potentially covers many existing
benchmarks. Consequently, current DG evaluation may neither be sufficiently
challenging nor adequately test genuinely unseen data scenarios. To better
assess the performance of CLIP on DG in-the-wild, a scenario where CLIP
encounters challenging unseen data, we consider two approaches: (1) evaluating
on 33 diverse datasets with quantified out-of-distribution (OOD) scores after
fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'
some domains as an approximation. We observe that CLIP's performance
deteriorates significantly on more OOD datasets. To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations). Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization. We instead
hypothesize that enhancing domain awareness is a prerequisite for effective
domain-invariant classification in foundation models. CLIP-DCA identifies and
enhances domain awareness within CLIP's encoders using a separate domain head
and synthetically generated diverse domain data. Simultaneously, it encourages
domain-invariant classification through disentanglement from the domain
features. CLIP-DCA shows significant improvements within this challenging
evaluation compared to existing methods, particularly on datasets that are more
OOD.

</details>


### [52] [What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos](https://arxiv.org/abs/2508.21770)
*Qiyue Sun,Qiming Huang,Yang Yang,Hongjun Wang,Jianbo Jiao*

Main category: cs.CV

TL;DR: 论文研究了非典型视频数据对开放世界学习的益处，发现其能提升OOD检测、NCD和ZSAR任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探索非典型视频数据在开放世界学习中的作用，填补现有研究对非常见数据关注的不足。

Method: 收集非典型视频数据集，用于模型训练，并评估其在OOD检测、NCD和ZSAR任务中的表现。

Result: 非典型数据提升性能，类别多样性和语义多样性分别对OOD检测和NCD任务有显著帮助。

Conclusion: 非典型视频数据对开放世界学习有益，鼓励未来研究关注此类数据。

Abstract: Humans usually show exceptional generalisation and discovery ability in the
open world, when being shown uncommon new concepts. Whereas most existing
studies in the literature focus on common typical data from closed sets,
open-world novel discovery is under-explored in videos. In this paper, we are
interested in asking: \textit{What if atypical unusual videos are exposed in
the learning process?} To this end, we collect a new video dataset consisting
of various types of unusual atypical data (\eg sci-fi, animation, \etc). To
study how such atypical data may benefit open-world learning, we feed them into
the model training process for representation learning. Focusing on three key
tasks in open-world learning: out-of-distribution (OOD) detection, novel
category discovery (NCD), and zero-shot action recognition (ZSAR), we found
that even straightforward learning approaches with atypical data consistently
improve performance across various settings. Furthermore, we found that
increasing the categorical diversity of the atypical samples further boosts OOD
detection performance. Additionally, in the NCD task, using a smaller yet more
semantically diverse set of atypical samples leads to better performance
compared to using a larger but more typical dataset. In the ZSAR setting, the
semantic diversity of atypical videos helps the model generalise better to
unseen action classes. These observations in our extensive experimental
evaluations reveal the benefits of atypical videos for visual representation
learning in the open world, together with the newly proposed dataset,
encouraging further studies in this direction.

</details>


### [53] [Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering](https://arxiv.org/abs/2508.21773)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 提出了一种无监督视频持续学习（uVCL）的新方法，解决了视频数据在无标签和任务边界情况下的学习问题。


<details>
  <summary>Details</summary>
Motivation: 视频数据复杂且富含时空信息，但现有研究主要集中在有监督持续学习上，缺乏对无监督场景的探索。

Method: 使用无监督视频Transformer网络提取特征，结合核密度估计（KDE）作为非参数概率表示，动态扩展内存簇以捕获新知识。

Result: 在UCF101、HMDB51和Something-to-Something V2数据集上验证了方法的有效性，显著提升了多任务学习性能。

Conclusion: 提出的方法为无监督视频持续学习提供了可行的解决方案，并在标准数据集上表现出色。

Abstract: We propose a realistic scenario for the unsupervised video learning where
neither task boundaries nor labels are provided when learning a succession of
tasks. We also provide a non-parametric learning solution for the
under-explored problem of unsupervised video continual learning. Videos
represent a complex and rich spatio-temporal media information, widely used in
many applications, but which have not been sufficiently explored in
unsupervised continual learning. Prior studies have only focused on supervised
continual learning, relying on the knowledge of labels and task boundaries,
while having labeled data is costly and not practical. To address this gap, we
study the unsupervised video continual learning (uVCL). uVCL raises more
challenges due to the additional computational and memory requirements of
processing videos when compared to images. We introduce a general benchmark
experimental protocol for uVCL by considering the learning of unstructured
video data categories during each task. We propose to use the Kernel Density
Estimation (KDE) of deep embedded video features extracted by unsupervised
video transformer networks as a non-parametric probabilistic representation of
the data. We introduce a novelty detection criterion for the incoming new task
data, dynamically enabling the expansion of memory clusters, aiming to capture
new knowledge when learning a succession of tasks. We leverage the use of
transfer learning from the previous tasks as an initial state for the knowledge
transfer to the current learning task. We found that the proposed methodology
substantially enhances the performance of the model when successively learning
many tasks. We perform in-depth evaluations on three standard video action
recognition datasets, including UCF101, HMDB51, and Something-to-Something V2,
without using any labels or class boundaries.

</details>


### [54] [A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI](https://arxiv.org/abs/2508.21775)
*Omer Faruk Durugol,Maximilian Rokuss,Yannick Kirchhoff,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: 该论文提出了一种基于nnU-Net框架的多阶段预训练策略，用于MRI中胰腺导管腺癌（PDAC）的自动分割，通过数据增强和模型集成在有限数据下取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决MRI中PDAC分割因肿瘤组织对比度差和标注数据稀缺而面临的挑战。

Method: 采用nnU-Net框架，结合多阶段预训练策略，从通用解剖模型逐步微调至目标MRI模态，并通过数据增强和模型集成优化性能。

Result: 在PANTHER挑战中，Task 1和Task 2分别取得了0.661和0.523的Tumor Dice分数，边界精度达到最优（MASD 5.46 mm，HD95 17.33 mm）。

Conclusion: 该方法为有限数据和复杂医学影像任务提供了高效解决方案，展示了模型集成和多阶段预训练的有效性。

Abstract: Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is
critical for clinical workflows but is hindered by poor tumor-tissue contrast
and a scarcity of annotated data. This paper details our submission to the
PANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and
therapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the
nnU-Net framework and leverages a deep, multi-stage cascaded pre-training
strategy, starting from a general anatomical foundation model and sequentially
fine-tuning on CT pancreatic lesion datasets and the target MRI modalities.
Through extensive five-fold cross-validation, we systematically evaluated data
augmentation schemes and training schedules. Our analysis revealed a critical
trade-off, where aggressive data augmentation produced the highest volumetric
accuracy, while default augmentations yielded superior boundary precision
(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).
For our final submission, we exploited this finding by constructing custom,
heterogeneous ensembles of specialist models, essentially creating a mix of
experts. This metric-aware ensembling strategy proved highly effective,
achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523
for Task 2. Our work presents a robust methodology for developing specialized,
high-performance models in the context of limited data and complex medical
imaging tasks (Team MIC-DKFZ).

</details>


### [55] [Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight](https://arxiv.org/abs/2508.21777)
*Ugur Dinc,Jibak Sarkar,Philipp Schubert,Sabine Semrau,Thomas Weissmann,Andre Karius,Johann Brand,Bernd-Niklas Axer,Ahmed Gomaa,Pluvio Stephan,Ishita Sheth,Sogand Beirami,Annette Schwarz,Udo Gaipl,Benjamin Frey,Christoph Bert,Stefanie Corradini,Rainer Fietkau,Florian Putz*

Main category: cs.CV

TL;DR: GPT-5在放射肿瘤学临床决策支持中表现出色，优于GPT-4和GPT-3.5，但仍需专家监督。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-5在放射肿瘤学中的性能，验证其作为临床决策支持工具的潜力。

Method: 使用两个基准测试：ACR放射肿瘤学培训考试和真实病例评估，由专家评分。

Result: GPT-5在多项选择测试中准确率达92.8%，病例评估中正确性和全面性评分高，幻觉罕见。

Conclusion: GPT-5在放射肿瘤学中表现优异，但需进一步改进和专家监督以确保临床安全。

Abstract: Introduction: Large language models (LLM) have shown great potential in
clinical decision support. GPT-5 is a novel LLM system that has been
specifically marketed towards oncology use.
  Methods: Performance was assessed using two complementary benchmarks: (i) the
ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300
multiple-choice items, and (ii) a curated set of 60 authentic radiation
oncologic vignettes representing diverse disease sites and treatment
indications. For the vignette evaluation, GPT-5 was instructed to generate
concise therapeutic plans. Four board-certified radiation oncologists rated
correctness, comprehensiveness, and hallucinations. Inter-rater reliability was
quantified using Fleiss' \k{appa}.
  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,
outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were
most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's
treatment recommendations were rated highly for correctness (mean 3.24/4, 95%
CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).
Hallucinations were rare with no case reaching majority consensus for their
presence. Inter-rater agreement was low (Fleiss' \k{appa} 0.083 for
correctness), reflecting inherent variability in clinical judgment. Errors
clustered in complex scenarios requiring precise trial knowledge or detailed
clinical adaptation.
  Discussion: GPT-5 clearly outperformed prior model variants on the radiation
oncology multiple-choice benchmark. Although GPT-5 exhibited favorable
performance in generating real-world radiation oncology treatment
recommendations, correctness ratings indicate room for further improvement.
While hallucinations were infrequent, the presence of substantive errors
underscores that GPT-5-generated recommendations require rigorous expert
oversight before clinical implementation.

</details>


### [56] [TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank](https://arxiv.org/abs/2508.21795)
*Jiawei Liu,Jiahe Hou,Wei Wang,Jinsong Du,Yang Cong,Huijie Fan*

Main category: cs.CV

TL;DR: TMUAD提出了一种三记忆框架，通过文本和图像记忆库统一检测结构和逻辑异常，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖精心设计的图像特征提取器和记忆库，难以有效捕捉逻辑异常。

Method: 构建类级文本记忆库、对象级图像记忆库和补丁级记忆库，多级检索和融合异常分数。

Result: 在七个公开数据集上达到最先进性能。

Conclusion: TMUAD通过协作记忆库统一检测结构和逻辑异常，效果显著。

Abstract: Anomaly detection, which aims to identify anomalies deviating from normal
patterns, is challenging due to the limited amount of normal data available.
Unlike most existing unified methods that rely on carefully designed image
feature extractors and memory banks to capture logical relationships between
objects, we introduce a text memory bank to enhance the detection of logical
anomalies. Specifically, we propose a Three-Memory framework for Unified
structural and logical Anomaly Detection (TMUAD). First, we build a class-level
text memory bank for logical anomaly detection by the proposed logic-aware text
extractor, which can capture rich logical descriptions of objects from input
images. Second, we construct an object-level image memory bank that preserves
complete object contours by extracting features from segmented objects. Third,
we employ visual encoders to extract patch-level image features for
constructing a patch-level memory bank for structural anomaly detection. These
three complementary memory banks are used to retrieve and compare normal images
that are most similar to the query image, compute anomaly scores at multiple
levels, and fuse them into a final anomaly score. By unifying structural and
logical anomaly detection through collaborative memory banks, TMUAD achieves
state-of-the-art performance across seven publicly available datasets involving
industrial and medical domains. The model and code are available at
https://github.com/SIA-IDE/TMUAD.

</details>


### [57] [VoCap: Video Object Captioning and Segmentation from Any Prompt](https://arxiv.org/abs/2508.21809)
*Jasper Uijlings,Xingyi Zhou,Xiuye Gu,Arsha Nagrani,Anurag Arnab,Alireza Fathi,David Ross,Cordelia Schmid*

Main category: cs.CV

TL;DR: VoCap是一个灵活的视频模型，支持多模态提示（文本、框或掩码），生成时空掩码和对象描述，同时解决视频对象分割、引用表达分割和对象描述任务。


<details>
  <summary>Details</summary>
Motivation: 视频中对象的细粒度定位和语义属性理解是视频理解的基础任务，但数据标注成本高。

Method: 通过预处理带有真实掩码的视频，利用大型视觉语言模型生成伪对象描述，构建SAV-Caption数据集，并在混合数据集上训练VoCap模型。

Result: VoCap在引用表达视频对象分割上达到SOTA，在半监督视频对象分割上表现优异，并建立了视频对象描述基准。

Conclusion: VoCap模型和SAV-Caption数据集为视频理解任务提供了高效解决方案，并公开了数据集。

Abstract: Understanding objects in videos in terms of fine-grained localization masks
and detailed semantic properties is a fundamental task in video understanding.
In this paper, we propose VoCap, a flexible video model that consumes a video
and a prompt of various modalities (text, box or mask), and produces a
spatio-temporal masklet with a corresponding object-centric caption. As such
our model addresses simultaneously the tasks of promptable video object
segmentation, referring expression segmentation, and object captioning. Since
obtaining data for this task is tedious and expensive, we propose to annotate
an existing large-scale segmentation dataset (SAV) with pseudo object captions.
We do so by preprocessing videos with their ground-truth masks to highlight the
object of interest and feed this to a large Vision Language Model (VLM). For an
unbiased evaluation, we collect manual annotations on the validation set. We
call the resulting dataset SAV-Caption. We train our VoCap model at scale on a
SAV-Caption together with a mix of other image and video datasets. Our model
yields state-of-the-art results on referring expression video object
segmentation, is competitive on semi-supervised video object segmentation, and
establishes a benchmark for video object captioning. Our dataset will be made
available at https://github.com/google-deepmind/vocap.

</details>


### [58] [The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning](https://arxiv.org/abs/2508.21816)
*Yiming Lin,Yuchen Niu,Shang Wang,Kaizhu Huang,Qiufeng Wang,Xiao-Bo Jin*

Main category: cs.CV

TL;DR: 论文提出了一种新的多标签学习方法（SPMLL）来解决视觉事件识别中的动词分类问题，并设计了GE-VerbMLP模型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将动词分类视为单标签问题，但忽略了视觉事件识别中固有的语义模糊性，即同一图像可能对应多个合理的动词类别。

Method: 将动词分类重新定义为单正例多标签学习（SPMLL）问题，并设计了结合图神经网络和对抗训练的GE-VerbMLP模型。

Result: 在真实数据集上，模型在多标签评估中实现了超过3%的MAP提升，同时在传统单标签指标上保持竞争力。

Conclusion: 研究表明动词分类本质上是多标签问题，提出的SPMLL框架和GE-VerbMLP模型有效解决了这一问题，为场景识别任务提供了新思路。

Abstract: Context recognition (SR) is a fundamental task in computer vision that aims
to extract structured semantic summaries from images by identifying key events
and their associated entities. Specifically, given an input image, the model
must first classify the main visual events (verb classification), then identify
the participating entities and their semantic roles (semantic role labeling),
and finally localize these entities in the image (semantic role localization).
Existing methods treat verb classification as a single-label problem, but we
show through a comprehensive analysis that this formulation fails to address
the inherent ambiguity in visual event recognition, as multiple verb categories
may reasonably describe the same image. This paper makes three key
contributions: First, we reveal through empirical analysis that verb
classification is inherently a multi-label problem due to the ubiquitous
semantic overlap between verb categories. Second, given the impracticality of
fully annotating large-scale datasets with multiple labels, we propose to
reformulate verb classification as a single positive multi-label learning
(SPMLL) problem - a novel perspective in SR research. Third, we design a
comprehensive multi-label evaluation benchmark for SR that is carefully
designed to fairly evaluate model performance in a multi-label setting. To
address the challenges of SPMLL, we futher develop the Graph Enhanced Verb
Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to
capture label correlations and adversarial training to optimize decision
boundaries. Extensive experiments on real-world datasets show that our approach
achieves more than 3\% MAP improvement while remaining competitive on
traditional top-1 and top-5 accuracy metrics.

</details>


### [59] [DriveQA: Passing the Driving Knowledge Test](https://arxiv.org/abs/2508.21824)
*Maolin Wei,Wanzhou Liu,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: DriveQA是一个开源基准测试，用于评估LLM和MLLM在驾驶知识测试中的表现，发现其在复杂场景中存在不足，但通过微调和预训练可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLM和MLLM在驾驶知识测试中的能力，填补现有自动驾驶基准在交通规则和边缘案例理解上的不足。

Method: 提出DriveQA基准测试，涵盖交通规则、标志和场景，通过实验分析模型在文本和视觉任务中的表现。

Result: LLM和MLLM在基础规则上表现良好，但在复杂场景中存在弱点；微调和预训练显著提升性能，尤其在标志识别和决策方面。

Conclusion: DriveQA为模型提供了全面的驾驶知识训练和评估平台，证明了其在提升下游任务性能中的有效性。

Abstract: If a Large Language Model (LLM) were to take a driving knowledge test today,
would it pass? Beyond standard spatial and visual question-answering (QA) tasks
on current autonomous driving benchmarks, driving knowledge tests require a
complete understanding of all traffic rules, signage, and right-of-way
principles. To pass this test, human drivers must discern various edge cases
that rarely appear in real-world datasets. In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios. Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [60] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 提出了一种混合字符串相似性、主题建模、层次聚类和基于规则的管道方法，用于聚类交易对手方，解决了自然语言模型不适用的问题，并在真实数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决银行支付消息系统中交易对手方聚类的问题，弥补自然语言模型在此场景的不足，提升反欺诈和资金追踪的效率。

Method: 结合字符串相似性、主题建模、层次聚类和基于规则的管道方法，支持未知聚类数量，并设计基于精确率和召回率的评估指标。

Result: 在真实标注数据集上显著优于基于规则的基线方法，同时保留了规则系统的可解释性，减少了人工审查需求。

Conclusion: 该方法有效提升了交易对手方聚类的性能，适用于制裁调查等场景，能够更好地控制遗漏实体变体的风险。

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [61] [Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI](https://arxiv.org/abs/2508.21101)
*Dilruk Perera,Gousia Habib,Qianyi Xu,Daniel J. Tan,Kai He,Erik Cambria,Mengling Feng*

Main category: cs.LG

TL;DR: 本文探讨了强化学习（RL）在医疗保健中的变革性作用，从预测转向主动干预决策，并分析了其技术、应用及伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 传统医疗AI仅预测结果，而RL能通过长期目标优化决策，为临床环境带来主动智能。

Method: 综述了RL技术（如模型基与无模型方法、离线学习）及在重症监护、慢性病等领域的应用。

Result: RL在医疗中展现出潜力，但也面临伦理、部署和奖励设计等挑战。

Conclusion: RL不仅是工具，更是医疗AI向主动临床智能转变的关键，需关注安全与人本对齐。

Abstract: Reinforcement learning (RL) marks a fundamental shift in how artificial
intelligence is applied in healthcare. Instead of merely predicting outcomes,
RL actively decides interventions with long term goals. Unlike traditional
models that operate on fixed associations, RL systems learn through trial,
feedback, and long-term reward optimization, introducing transformative
possibilities and new risks. From an information fusion lens, healthcare RL
typically integrates multi-source signals such as vitals, labs clinical notes,
imaging and device telemetry using temporal and decision-level mechanisms.
These systems can operate within centralized, federated, or edge architectures
to meet real-time clinical constraints, and naturally span data, features and
decision fusion levels. This survey explore RL's rise in healthcare as more
than a set of tools, rather a shift toward agentive intelligence in clinical
environments. We first structure the landscape of RL techniques including
model-based and model-free methods, offline and batch-constrained approaches,
and emerging strategies for reward specification and uncertainty calibration
through the lens of healthcare constraints. We then comprehensively analyze RL
applications spanning critical care, chronic disease, mental health,
diagnostics, and robotic assistance, identifying their trends, gaps, and
translational bottlenecks. In contrast to prior reviews, we critically analyze
RL's ethical, deployment, and reward design challenges, and synthesize lessons
for safe, human-aligned policy learning. This paper serves as both a a
technical roadmap and a critical reflection of RL's emerging transformative
role in healthcare AI not as prediction machinery, but as agentive clinical
intelligence.

</details>


### [62] [Spatiotemporal EEG-Based Emotion Recognition Using SAM Ratings from Serious Games with Hybrid Deep Learning](https://arxiv.org/abs/2508.21103)
*Abdul Rehman,Ilona Heldal,Jerry Chun-Wei Lin*

Main category: cs.LG

TL;DR: 提出了一种多粒度EEG情绪分类框架，在GAMEEMO数据集上表现优异，LSTM-GRU模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于二元情绪预测或特定被试分类，缺乏泛化性和实际应用价值。

Method: 采用结构化预处理策略，结合统计和频域特征提取，评估多种模型（如Random Forest、LSTM-GRU等）。

Result: LSTM-GRU模型在二元情绪任务中F1-score达0.932，多类和多标签分类准确率分别为94.5%和90.6%。

Conclusion: 该框架在多粒度情绪分类中表现优异，为实际情感计算系统提供了可行方案。

Abstract: Recent advancements in EEG-based emotion recognition have shown promising
outcomes using both deep learning and classical machine learning approaches;
however, most existing studies focus narrowly on binary valence prediction or
subject-specific classification, which limits generalizability and deployment
in real-world affective computing systems. To address this gap, this paper
presents a unified, multigranularity EEG emotion classification framework built
on the GAMEEMO dataset, which consists of 14-channel EEG recordings and
continuous self-reported emotion ratings (boring, horrible, calm, and funny)
from 28 subjects across four emotion-inducing gameplay scenarios. Our pipeline
employs a structured preprocessing strategy that comprises temporal window
segmentation, hybrid statistical and frequency-domain feature extraction, and
z-score normalization to convert raw EEG signals into robust, discriminative
input vectors. Emotion labels are derived and encoded across three
complementary axes: (i) binary valence classification based on the averaged
polarity of positive and negative emotion ratings, and (ii) Multi-class emotion
classification, where the presence of the most affective state is predicted.
(iii) Fine-grained multi-label representation via binning each emotion into 10
ordinal classes. We evaluate a broad spectrum of models, including Random
Forest, XGBoost, and SVM, alongside deep neural architectures such as LSTM,
LSTM-GRU, and CNN-LSTM. Among these, the LSTM-GRU model consistently
outperforms the others, achieving an F1-score of 0.932 in the binary valence
task and 94.5% and 90.6% in both multi-class and Multi-Label emotion
classification.

</details>


### [63] [PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning](https://arxiv.org/abs/2508.21104)
*Wenfeng Feng,Penghong Zhao,Guochao Jiang,Chuzhan Hao,Yuewei Zhang,Hao Wang*

Main category: cs.LG

TL;DR: PVPO是一种通过优势参考锚点和数据预采样增强的高效强化学习方法，解决了传统方法依赖多次采样和比较的问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统无批评强化学习方法依赖多次采样和比较，容易陷入局部最优且计算成本高。

Method: 使用参考模型提前计算奖励分数作为参考锚点，并通过数据预采样选择高增益数据。

Result: 在九个数据集上实现SOTA性能，表现出强大的泛化能力和可扩展性。

Conclusion: PVPO通过减少对多次采样的依赖和优化数据选择，显著提升了训练效率和性能。

Abstract: Critic-free reinforcement learning methods, particularly group policies, have
attracted considerable attention for their efficiency in complex tasks.
However, these methods rely heavily on multiple sampling and comparisons within
the policy to estimate advantage, which may cause the policy to fall into local
optimum and increase computational cost. To address these issues, we propose
PVPO, an efficient reinforcement learning method enhanced by an advantage
reference anchor and data pre-sampling. Specifically, we use the reference
model to rollout in advance and employ the calculated reward score as a
reference anchor. Our approach effectively corrects the cumulative bias
introduced by intra-group comparisons and significantly reduces reliance on the
number of rollouts. Meanwhile, the reference model can assess sample difficulty
during data pre-sampling, enabling effective selection of high-gain data to
improve training efficiency. Experiments conducted on nine datasets across two
domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our
approach not only demonstrates robust generalization across multiple tasks, but
also exhibits scalable performance across models of varying scales.

</details>


### [64] [Dynamic Low-rank Approximation of Full-Matrix Preconditioner for Training Generalized Linear Models](https://arxiv.org/abs/2508.21106)
*Tatyana Matveeva,Aleksandr Katrutsa,Evgeny Frolov*

Main category: cs.LG

TL;DR: AdaGram是一种高效的全矩阵自适应梯度优化器，通过低秩近似和矩阵积分方法减少计算和内存开销，性能优于或匹配对角自适应优化器。


<details>
  <summary>Details</summary>
Motivation: 解决传统自适应梯度方法（如Adagrad）因使用对角预条件矩阵而无法捕捉参数相关性的问题，同时避免全矩阵方法的高计算和内存成本。

Method: 利用快速对称分解计算预条件更新方向，并通过矩阵积分方法保持预条件器的低秩结构。

Result: 在标准机器学习任务中，AdaGram在低秩近似（如秩五）下收敛更快或性能与对角自适应优化器相当。

Conclusion: AdaGram为大规模模型的自适应优化提供了一种可扩展的解决方案。

Abstract: Adaptive gradient methods like Adagrad and its variants are widespread in
large-scale optimization. However, their use of diagonal preconditioning
matrices limits the ability to capture parameter correlations. Full-matrix
adaptive methods, approximating the exact Hessian, can model these correlations
and may enable faster convergence. At the same time, their computational and
memory costs are often prohibitive for large-scale models. To address this
limitation, we propose AdaGram, an optimizer that enables efficient full-matrix
adaptive gradient updates. To reduce memory and computational overhead, we
utilize fast symmetric factorization for computing the preconditioned update
direction at each iteration. Additionally, we maintain the low-rank structure
of a preconditioner along the optimization trajectory using matrix integrator
methods. Numerical experiments on standard machine learning tasks show that
AdaGram converges faster or matches the performance of diagonal adaptive
optimizers when using rank five and smaller rank approximations. This
demonstrates AdaGram's potential as a scalable solution for adaptive
optimization in large models.

</details>


### [65] [An Explainable, Attention-Enhanced, Bidirectional Long Short-Term Memory Neural Network for Joint 48-Hour Forecasting of Temperature, Irradiance, and Relative Humidity](https://arxiv.org/abs/2508.21109)
*Georgios Vamvouras,Konstantinos Braimakis,Christos Tzivanidis*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的48小时温度、太阳辐照度和相对湿度预测框架，用于智能HVAC系统的模型预测控制。


<details>
  <summary>Details</summary>
Motivation: 支持智能HVAC系统的模型预测控制，通过高精度和可解释的天气预测提升能源效率。

Method: 使用堆叠双向LSTM网络（BiLSTM）结合注意力机制，联合预测三个变量，并利用历史气象数据（2019-2022）进行训练。

Result: 模型在温度、辐照度和湿度预测上的平均绝对误差分别为1.3°C、31 W/m²和6.7%，优于现有方法。

Conclusion: 该框架通过多变量预测和可解释性，为能源高效的建筑控制提供了可靠的短期气象预测。

Abstract: This paper presents a Deep Learning (DL) framework for 48-hour forecasting of
temperature, solar irradiance, and relative humidity to support Model
Predictive Control (MPC) in smart HVAC systems. The approach employs a stacked
Bidirectional Long Short-Term Memory (BiLSTM) network with attention, capturing
temporal and cross-feature dependencies by jointly predicting all three
variables. Historical meteorological data (2019-2022) with encoded cyclical
time features were used for training, while 2023 data evaluated generalization.
The model achieved Mean Absolute Errors of 1.3 degrees Celsius (temperature),
31 W/m2 (irradiance), and 6.7 percentage points (humidity), outperforming
state-of-the-art numerical weather prediction and machine learning benchmarks.
Integrated Gradients quantified feature contributions, and attention weights
revealed temporal patterns, enhancing interpretability. By combining
multivariate forecasting, attention-based DL, and explainability, this work
advances data-driven weather prediction. The demonstrated accuracy and
transparency highlight the framework's potential for energy-efficient building
control through reliable short-term meteorological forecasting.

</details>


### [66] [Automating the Deep Space Network Data Systems; A Case Study in Adaptive Anomaly Detection through Agentic AI](https://arxiv.org/abs/2508.21111)
*Evan J. Chou,Lisa S. Locke,Harvey M. Soldan*

Main category: cs.LG

TL;DR: 该研究旨在利用机器学习技术检测NASA深空网络（DSN）设备中的异常和退化问题，通过预测分析和实时统计计算，结合强化学习和大型语言模型，构建了一个完整的数据管道和智能代理系统。


<details>
  <summary>Details</summary>
Motivation: DSN设备长期运行会导致性能退化，可能中断数据流并威胁依赖DSN的航天器连接。研究目的是帮助工程师直接通过数据定位问题，确保未来太空任务的持续运行。

Method: 研究采用了多种机器学习技术，包括预测分析、统计计算、强化学习子系统和大型语言模型，构建了一个数据管道系统，并整合了智能代理系统进行复杂推理。

Result: 实现了从数据提取到异常检测的完整工作流，能够实时识别异常数据并分类其严重程度，同时为每个异常提供解释。

Conclusion: 该研究成功开发了一个综合系统，能够有效检测和分类DSN设备中的异常，为未来维护和任务提供了可靠支持。

Abstract: The Deep Space Network (DSN) is NASA's largest network of antenna facilities
that generate a large volume of multivariate time-series data. These facilities
contain DSN antennas and transmitters that undergo degradation over long
periods of time, which may cause costly disruptions to the data flow and
threaten the earth-connection of dozens of spacecraft that rely on the Deep
Space Network for their lifeline. The purpose of this study was to experiment
with different methods that would be able to assist JPL engineers with directly
pinpointing anomalies and equipment degradation through collected data, and
continue conducting maintenance and operations of the DSN for future space
missions around our universe. As such, we have researched various machine
learning techniques that can fully reconstruct data through predictive
analysis, and determine anomalous data entries within real-time datasets
through statistical computations and thresholds. On top of the fully trained
and tested machine learning models, we have also integrated the use of a
reinforcement learning subsystem that classifies identified anomalies based on
severity level and a Large Language Model that labels an explanation for each
anomalous data entry, all of which can be improved and fine-tuned over time
through human feedback/input. Specifically, for the DSN transmitters, we have
also implemented a full data pipeline system that connects the data extraction,
parsing, and processing workflow all together as there was no coherent program
or script for performing these tasks before. Using this data pipeline system,
we were able to then also connect the models trained from DSN antenna data,
completing the data workflow for DSN anomaly detection. This was all wrapped
around and further connected by an agentic AI system, where complex reasoning
was utilized to determine the classifications and predictions of anomalous
data.

</details>


### [67] [Adaptive LLM Routing under Budget Constraints](https://arxiv.org/abs/2508.21141)
*Pranoy Panda,Raghav Magazine,Chaitanya Devaguptapu,Sho Takemori,Vishal Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种基于上下文多臂老虎机问题的LLM路由方法PILOT，通过共享嵌入空间和在线反馈优化路由决策，同时考虑用户预算限制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法假设已知最优查询-LLM配对，但实际场景缺乏此类完整映射且查询动态变化，需自适应决策。

Method: 构建查询与LLM的共享嵌入空间，利用离线人类偏好数据初始化，通过在线老虎机反馈优化；提出PILOT算法（基于LinUCB扩展）和在线成本策略（多选择背包问题）。

Result: PILOT能自适应选择最优LLM，无需对所有查询进行全模型推理，且能高效处理用户预算限制。

Conclusion: 将LLM路由建模为上下文老虎机问题有效解决了动态查询和预算约束的挑战，PILOT展示了优越的适应性和资源效率。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing,
but their varying capabilities and costs pose challenges in practical
applications. LLM routing addresses this by dynamically selecting the most
suitable LLM for each query/task. Previous approaches treat this as a
supervised learning problem, assuming complete knowledge of optimal query-LLM
pairings. However, real-world scenarios lack such comprehensive mappings and
face evolving user queries. We thus propose to study LLM routing as a
contextual bandit problem, enabling adaptive decision-making using bandit
feedback without requiring exhaustive inference across all LLMs for all queries
(in contrast to supervised routing). To address this problem, we develop a
shared embedding space for queries and LLMs, where query and LLM embeddings are
aligned to reflect their affinity. This space is initially learned from offline
human preference data and refined through online bandit feedback. We
instantiate this idea through Preference-prior Informed Linucb fOr adaptive
rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets
for model routing, we introduce an online cost policy modeled as a multi-choice
knapsack problem, ensuring resource-efficient routing.

</details>


### [68] [Privacy Auditing Synthetic Data Release through Local Likelihood Attacks](https://arxiv.org/abs/2508.21146)
*Joshua Ward,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: 本文提出了一种名为Gen-LRA的新型成员推理攻击方法，用于检测合成数据中的隐私泄露问题，无需模型知识或访问权限，通过评估测试数据对局部似然比的影响来实现高效攻击。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据隐私审计方法依赖启发式和不合理假设，难以准确描述和检测隐私泄露问题。

Method: 提出Generative Likelihood Ratio Attack (Gen-LRA)，一种无需模型知识的No-Box MIA方法，通过评估测试数据对局部似然比的影响进行攻击。

Result: Gen-LRA在多种数据集、模型架构和攻击参数下表现优于其他MIA方法。

Conclusion: Gen-LRA是一种有效的隐私审计工具，揭示了生成模型过拟合带来的隐私风险。

Abstract: Auditing the privacy leakage of synthetic data is an important but unresolved
problem. Most existing privacy auditing frameworks for synthetic data rely on
heuristics and unreasonable assumptions to attack the failure modes of
generative models, exhibiting limited capability to describe and detect the
privacy exposure of training data through synthetic data release. In this
paper, we study designing Membership Inference Attacks (MIAs) that specifically
exploit the observation that tabular generative models tend to significantly
overfit to certain regions of the training distribution. Here, we propose
Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally
efficient No-Box MIA that, with no assumption of model knowledge or access,
formulates its attack by evaluating the influence a test observation has in a
surrogate model's estimation of a local likelihood ratio over the synthetic
data. Assessed over a comprehensive benchmark spanning diverse datasets, model
architectures, and attack parameters, we find that Gen-LRA consistently
dominates other MIAs for generative models across multiple performance metrics.
These results underscore Gen-LRA's effectiveness as a privacy auditing tool for
the release of synthetic data, highlighting the significant privacy risks posed
by generative model overfitting in real-world applications.

</details>


### [69] [Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks](https://arxiv.org/abs/2508.21172)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出了一种基于时间残差连接的深度未训练RNN（DeepResESN），显著提升了记忆能力和长期时间建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统ESN在长期信息处理上表现不佳，需要改进。

Method: 引入时间残差连接，研究不同正交配置对网络动态的影响，并进行数学分析。

Result: 实验证明DeepResESN在多种时间序列任务上优于传统浅层和深层RC。

Conclusion: DeepResESN通过残差连接有效提升了长期信息处理能力，具有理论和实践优势。

Abstract: Echo State Networks (ESNs) are a particular type of untrained Recurrent
Neural Networks (RNNs) within the Reservoir Computing (RC) framework, popular
for their fast and efficient learning. However, traditional ESNs often struggle
with long-term information processing. In this paper, we introduce a novel
class of deep untrained RNNs based on temporal residual connections, called
Deep Residual Echo State Networks (DeepResESNs). We show that leveraging a
hierarchy of untrained residual recurrent layers significantly boosts memory
capacity and long-term temporal modeling. For the temporal residual
connections, we consider different orthogonal configurations, including
randomly generated and fixed-structure configurations, and we study their
effect on network dynamics. A thorough mathematical analysis outlines necessary
and sufficient conditions to ensure stable dynamics within DeepResESN. Our
experiments on a variety of time series tasks showcase the advantages of the
proposed approach over traditional shallow and deep RC.

</details>


### [70] [Detecting Domain Shifts in Myoelectric Activations: Challenges and Opportunities in Stream Learning](https://arxiv.org/abs/2508.21278)
*Yibin Sun,Nick Lim,Guilherme Weigert Cassales,Heitor Murilo Gomes,Bernhard Pfahringer,Albert Bifet,Anany Dwivedi*

Main category: cs.LG

TL;DR: 论文探讨了使用数据流学习技术检测肌电信号中的域偏移，评估了多种漂移检测方法，并指出了当前技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 肌电信号的非平稳性使得检测域偏移具有挑战性，研究旨在探索实时检测方法以提升模型稳定性。

Method: 采用KPCA和余弦核预处理数据，并评估了CUSUM、Page-Hinckley和ADWIN等漂移检测方法。

Result: 当前方法在实时检测肌电信号域偏移时性能有限，但流式学习方法显示出潜力。

Conclusion: 流式学习方法有助于稳定肌电解码模型，但需进一步研究以提高鲁棒性和准确性。

Abstract: Detecting domain shifts in myoelectric activations poses a significant
challenge due to the inherent non-stationarity of electromyography (EMG)
signals. This paper explores the detection of domain shifts using data stream
(DS) learning techniques, focusing on the DB6 dataset from the Ninapro
database. We define domains as distinct time-series segments based on different
subjects and recording sessions, applying Kernel Principal Component Analysis
(KPCA) with a cosine kernel to pre-process and highlight these shifts. By
evaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and
ADWIN, we reveal the limitations of current techniques in achieving high
performance for real-time domain shift detection in EMG signals. Our results
underscore the potential of streaming-based approaches for maintaining stable
EMG decoding models, while highlighting areas for further research to enhance
robustness and accuracy in real-world scenarios.

</details>


### [71] [FUTURE: Flexible Unlearning for Tree Ensemble](https://arxiv.org/abs/2508.21181)
*Ziheng Chen,Jin Huang,Jiali Cheng,Yuchan Guo,Mengjie Wang,Lalitesh Morishetti,Kaushiki Nag,Hadi Amiri*

Main category: cs.LG

TL;DR: FUTURE是一种新型的树集成遗忘算法，通过梯度优化解决样本遗忘问题，适用于大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘算法难以泛化到复杂集成模型且效率低下，需改进以适应数据隐私需求。

Method: 采用概率模型近似和梯度优化框架，实现端到端的有效遗忘。

Result: 在真实数据集上，FUTURE表现出显著且成功的遗忘性能。

Conclusion: FUTURE为树集成模型提供了一种高效、通用的遗忘解决方案。

Abstract: Tree ensembles are widely recognized for their effectiveness in
classification tasks, achieving state-of-the-art performance across diverse
domains, including bioinformatics, finance, and medical diagnosis. With
increasing emphasis on data privacy and the \textit{right to be forgotten},
several unlearning algorithms have been proposed to enable tree ensembles to
forget sensitive information. However, existing methods are often tailored to a
particular model or rely on the discrete tree structure, making them difficult
to generalize to complex ensembles and inefficient for large-scale datasets. To
address these limitations, we propose FUTURE, a novel unlearning algorithm for
tree ensembles. Specifically, we formulate the problem of forgetting samples as
a gradient-based optimization task. In order to accommodate
non-differentiability of tree ensembles, we adopt the probabilistic model
approximations within the optimization framework. This enables end-to-end
unlearning in an effective and efficient manner. Extensive experiments on
real-world datasets show that FUTURE yields significant and successful
unlearning performance.

</details>


### [72] [Manifold Trajectories in Next-Token Prediction: From Replicator Dynamics to Softmax Equilibrium](https://arxiv.org/abs/2508.21186)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: 论文将大语言模型中的解码步骤描述为概率单纯形上的约束变分原理，证明了在固定上下文和温度下，下一个令牌分布会平滑收敛到softmax平衡。


<details>
  <summary>Details</summary>
Motivation: 形式化解码步骤的数学基础，提供对温度、top-k和核采样等实践问题的精确理解。

Method: 通过约束变分原理和乘法权重更新（熵镜像）分析解码过程，推导连续时间极限的复制流。

Result: 证明解码分布平滑收敛到softmax平衡，温度是时间的精确缩放，top-k和核采样限制流到特定面。

Conclusion: 为解码步骤提供严格数学框架，解释实践中的温度调节和采样策略，为路径依赖调整提供理论基础。

Abstract: Decoding in large language models is often described as scoring tokens and
normalizing with softmax. We give a minimal, self-contained account of this
step as a constrained variational principle on the probability simplex. The
discrete, normalization-respecting ascent is the classical
multiplicative-weights (entropic mirror) update; its continuous-time limit is
the replicator flow. From these ingredients we prove that, for a fixed context
and temperature, the next-token distribution follows a smooth trajectory inside
the simplex and converges to the softmax equilibrium. This formalizes the
common ``manifold traversal'' intuition at the output-distribution level. The
analysis yields precise, practice-facing consequences: temperature acts as an
exact rescaling of time along the same trajectory, while top-k and nucleus
sampling restrict the flow to a face with identical guarantees. We also outline
a controlled account of path-dependent score adjustments and their connection
to loop-like, hallucination-style behavior. We make no claims about training
dynamics or internal representations; those are deferred to future work.

</details>


### [73] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: 研究发现，强化学习在大型语言模型中的反直觉现象（如单样本训练、不精确奖励信号、负样本训练）仅在模型与任务高度对齐时有效，而在更具挑战性的场景中，传统RL方法仍更可靠。


<details>
  <summary>Details</summary>
Motivation: 探索强化学习在大型语言模型中表现出的反直觉现象（如单样本训练效果等同全数据集、奖励信号不精确仍有效等）背后的原因及适用条件。

Method: 通过系统实验验证不同模型架构和任务领域，分析模型与任务对齐程度（以pass@k准确率为指标）对RL效果的影响。

Result: 反直觉现象仅在模型与任务高度对齐时成立；在低对齐场景中，传统RL方法仍更有效。

Conclusion: 模型与任务的对齐程度是决定RL反直觉现象是否成立的关键因素，需谨慎应用这些现象于实际任务。

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [74] [Class Incremental Continual Learning with Self-Organizing Maps and Variational Autoencoders Using Synthetic Replay](https://arxiv.org/abs/2508.21240)
*Pujan Thapa,Alexander Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 提出了一种基于自组织映射（SOM）和变分自编码器（VAE）的生成式持续学习框架，无需存储原始数据或任务标签，实现了内存高效的样本重放。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中内存效率低和任务标签依赖的问题，同时支持高维和低维输入数据的处理。

Method: 结合SOM和VAE，针对不同维度输入设计不同操作模式，通过存储统计信息生成合成样本用于重放。

Result: 在CIFAR-10和CIFAR-100上分别提升10%和7%的性能，优于无内存方法，与基于内存的方法竞争。

Conclusion: 该方法是一种可扩展、无需任务标签且内存高效的持续学习解决方案，同时支持学习过程可视化和生成模型应用。

Abstract: This work introduces a novel generative continual learning framework based on
self-organizing maps (SOMs) and variational autoencoders (VAEs) to enable
memory-efficient replay, eliminating the need to store raw data samples or task
labels. For high-dimensional input spaces, such as of CIFAR-10 and CIFAR-100,
we design a scheme where the SOM operates over the latent space learned by a
VAE, whereas, for lower-dimensional inputs, such as those found in MNIST and
FashionMNIST, the SOM operates in a standalone fashion. Our method stores a
running mean, variance, and covariance for each SOM unit, from which synthetic
samples are then generated during future learning iterations. For the VAE-based
method, generated samples are then fed through the decoder to then be used in
subsequent replay. Experimental results on standard class-incremental
benchmarks show that our approach performs competitively with state-of-the-art
memory-based methods and outperforms memory-free methods, notably improving
over best state-of-the-art single class incremental performance on CIFAR-10 and
CIFAR-100 by nearly $10$\% and $7$\%, respectively. Our methodology further
facilitates easy visualization of the learning process and can also be utilized
as a generative model post-training. Results show our method's capability as a
scalable, task-label-free, and memory-efficient solution for continual
learning.

</details>


### [75] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 提出了一种基于元学习的混合专家（MoE）框架，动态结合三种先进CFD替代模型，显著降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 高保真CFD模拟计算成本高，现有ML替代模型架构多样但无单一最优方案。

Method: 使用MoE模型，通过门控网络动态结合DoMINO、X-MeshGraphNet和FigConvNet三种模型，并引入熵正则化防止模型崩溃。

Result: 在DrivAerML数据集上验证，MoE模型显著降低L-2误差，优于单一模型和集成平均。

Conclusion: MoE框架通过结合不同架构优势，构建了更鲁棒、准确的复合替代模型。

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [76] [RelP: Faithful and Efficient Circuit Discovery via Relevance Patching](https://arxiv.org/abs/2508.21258)
*Farnoush Rezaei Jafari,Oliver Eberle,Ashkan Khakzar,Neel Nanda*

Main category: cs.LG

TL;DR: Relevance Patching (RelP) 是一种基于 Layer-wise Relevance Propagation (LRP) 的新方法，用于在机制解释性中更高效且准确地定位模型行为。


<details>
  <summary>Details</summary>
Motivation: 激活修补（activation patching）在机制解释性中用于定位模型行为，但计算成本高；而属性修补（attribution patching）虽然更快，但在深度非线性网络中噪声大且可靠性低。

Method: RelP 使用 LRP 的传播系数替代属性修补中的局部梯度，通过反向传播输出并重新分配相关性，保持计算效率的同时提高准确性。

Result: RelP 在多个模型和任务中表现优于标准属性修补，特别是在 GPT-2 Large 的间接对象识别任务中，Pearson 相关系数从 0.006 提升到 0.956。

Conclusion: RelP 提供了一种更高效且准确的替代方案，适用于机制解释性任务，且无需额外计算成本。

Abstract: Activation patching is a standard method in mechanistic interpretability for
localizing the components of a model responsible for specific behaviors, but it
is computationally expensive to apply at scale. Attribution patching offers a
faster, gradient-based approximation, yet suffers from noise and reduced
reliability in deep, highly non-linear networks. In this work, we introduce
Relevance Patching (RelP), which replaces the local gradients in attribution
patching with propagation coefficients derived from Layer-wise Relevance
Propagation (LRP). LRP propagates the network's output backward through the
layers, redistributing relevance to lower-level components according to local
propagation rules that ensure properties such as relevance conservation or
improved signal-to-noise ratio. Like attribution patching, RelP requires only
two forward passes and one backward pass, maintaining computational efficiency
while improving faithfulness. We validate RelP across a range of models and
tasks, showing that it more accurately approximates activation patching than
standard attribution patching, particularly when analyzing residual stream and
MLP outputs in the Indirect Object Identification (IOI) task. For instance, for
MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation
of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by
RelP. Additionally, we compare the faithfulness of sparse feature circuits
identified by RelP and Integrated Gradients (IG), showing that RelP achieves
comparable faithfulness without the extra computational cost associated with
IG.

</details>


### [77] [Owen Sampling Accelerates Contribution Estimation in Federated Learning](https://arxiv.org/abs/2508.21261)
*Hossein KhademSohi,Hadi Hemmati,Jiayu Zhou,Steve Drew*

Main category: cs.LG

TL;DR: FedOwen框架通过Owen采样高效近似Shapley值，结合自适应客户端选择策略，显著提升联邦学习的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中准确评估客户端贡献对公平奖励和高效模型收敛至关重要，但传统Shapley值计算复杂度高。

Method: 提出FedOwen框架，利用Owen采样近似Shapley值，并采用自适应客户端选择策略平衡高价值与低采样客户端。

Result: 在相同通信轮数下，FedOwen比现有方法在非IID基准上实现高达23%的最终准确率提升。

Conclusion: FedOwen通过高效近似和智能客户端选择，显著提升了联邦学习的性能和实用性。

Abstract: Federated Learning (FL) aggregates information from multiple clients to train
a shared global model without exposing raw data. Accurately estimating each
client's contribution is essential not just for fair rewards, but for selecting
the most useful clients so the global model converges faster. The Shapley value
is a principled choice, yet exact computation scales exponentially with the
number of clients, making it infeasible for large federations. We propose
FedOwen, an efficient framework that uses Owen sampling to approximate Shapley
values under the same total evaluation budget as existing methods while keeping
the approximation error small. In addition, FedOwen uses an adaptive client
selection strategy that balances exploiting high-value clients with exploring
under-sampled ones, reducing bias and uncovering rare but informative data.
Under a fixed valuation cost, FedOwen achieves up to 23 percent higher final
accuracy within the same number of communication rounds compared to
state-of-the-art baselines on non-IID benchmarks.

</details>


### [78] [Guess-and-Learn (G&L): Measuring the Cumulative Error Cost of Cold-Start Adaptation](https://arxiv.org/abs/2508.21270)
*Roland Arnold*

Main category: cs.LG

TL;DR: G&L v1.0提出了一种评估机器学习模型冷启动适应性的方法，通过测量模型在无标签数据集上学习时的累积错误，揭示了传统评估忽略的早期学习成本。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法过于关注最终准确性，忽视了模型在冷启动阶段的适应成本。G&L旨在填补这一空白，量化早期学习的错误成本。

Method: G&L定义了四种实验路径（Scratch/Pretrained × Online/Batch），通过测量模型在无标签数据集上的错误轨迹，评估其适应速度、选择质量和偏差。

Result: 实验表明，小模型在早期适应中表现更好，而预训练的效果因领域而异。当前模型与理想“oracle参考带”仍有较大差距。

Conclusion: G&L为开发既准确又可靠的模型提供了可复现的框架，补充了传统评估方法的不足。

Abstract: Evaluation of machine learning models typically emphasizes final accuracy,
overlooking the cost of adaptation: the cumulative errors incurred while
learning from scratch. Guess-and- Learn (G&L) v1.0 addresses this gap by
measuring cold-start adaptability - the total mistakes a model makes while
sequentially labeling an unlabeled dataset. At each step, the learner selects
an instance, predicts its label, receives the ground truth, and updates
parameters under either online (per-sample) or batch (delayed) mode. The
resulting error trajectory exposes adaptation speed, selection quality, and
bias - dynamics invisible to endpoint metrics.
  G&L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) to
disentangle the effects of initialization and update frequency. We formalize
the protocol, relate it to classical mistake-bound theory, and estimate a
heuristic "oracle reference band" for MNIST as a plausibility reference.
Baseline experiments on MNIST and AG News, spanning classical methods
(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), and
pretrained transformers (ViT-B/16, BERT-base), reveal systematic differences in
early-phase efficiency: smaller models can adapt with fewer initial errors,
while pretraining benefits vary by domain. Across settings, current models
remain well above the oracle band, highlighting an adaptability gap.
  By quantifying the mistake cost of early learning, G&L complements
conventional benchmarks and provides a reproducible framework for developing
learners that are not only accurate in the limit but also reliable from the
first examples.

</details>


### [79] [CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams](https://arxiv.org/abs/2508.21273)
*Ashok Devireddy,Shunping Huang*

Main category: cs.LG

TL;DR: CALM框架通过实时微调和LLM辅助判断，提升非平稳时间序列流中的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统离线模型在数据统计特性变化时性能下降，CALM旨在解决这一问题。

Method: 基于Apache Beam和TimesFm，结合实时微调和LLM语义判断。

Result: 在TSB-UAD基准测试中，CALM的ROC AUC得分优于静态模型。

Conclusion: CALM有效提升了动态流环境中的异常检测性能。

Abstract: The detection of anomalies in non-stationary time-series streams is a
critical but challenging task across numerous industrial and scientific
domains. Traditional models, trained offline, suffer significant performance
degradation when faced with concept drift, where the underlying statistical
properties of the data change over time. This paper introduces CALM
(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework for
real-time anomaly detection designed to address this challenge. CALM is built
on the Apache Beam distributed processing framework and leverages the TimesFm
foundation model for forecasting-based anomaly detection. The framework's
novelty lies in two core contributions. First, it implements a closed-loop,
continuous fine-tuning mechanism that allows the anomaly detection model to
adapt to evolving data patterns in near real-time. Second, it introduces an
LLM-as-a-Judge component, a Large Language Model that provides semantic,
context-aware judgments on detected anomalies to curate a high-quality training
dataset, deciding whether an anomaly represents transient noise or a meaningful
pattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Our
results demonstrate that the continuously fine-tuned model improves the ROC AUC
score in most datasets compared to the static, pre-trained base model,
validating the efficacy of our adaptive, LLM-guided approach to maintaining
high-performance anomaly detection in dynamic streaming environments.

</details>


### [80] [MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems](https://arxiv.org/abs/2508.21296)
*Shihao Ji,Zihui Song*

Main category: cs.LG

TL;DR: MyGO是一种受生物睡眠-觉醒周期启发的终身学习框架，通过生成记忆模型避免存储原始数据，解决了数据隐私和存储限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖存储样本或复杂正则化，面临数据隐私、存储限制和任务差异导致的性能下降问题。

Method: MyGO在“觉醒”阶段学习新任务并训练生成记忆模型，在“睡眠”阶段生成伪数据并通过知识蒸馏整合新旧知识。

Result: 在Split-MNIST和Split-AG News基准测试中，MyGO显著减轻灾难性遗忘并保持高平均准确率。

Conclusion: MyGO框架在隐私和存储效率方面具有优势，且在不同领域表现出有效性。

Abstract: Continual or Lifelong Learning aims to develop models capable of acquiring
new knowledge from a sequence of tasks without catastrophically forgetting what
has been learned before. Existing approaches often rely on storing samples from
previous tasks (experience replay) or employing complex regularization terms to
protect learned weights. However, these methods face challenges related to data
privacy, storage limitations, and performance degradation when tasks are
dissimilar. To address these challenges, we introduce MyGO (Memory Yielding
Generative Offline-consolidation), a novel lifelong learning framework inspired
by the biological wake-sleep cycle. During the "wake" phase, the system rapidly
learns a new task and trains a compact generative model (Generative Memory,
G-mem) to capture its data distribution. During the "sleep" phase, the system
enters an offline state, using all learned G-mem models to generate pseudo-data
("dreams") and consolidate new and old knowledge into a core feature extractor
via knowledge distillation. This approach obviates the need to store any raw
data, retaining only compact generative models, which offers significant
advantages in privacy and storage efficiency. We evaluate MyGO on computer
vision (Split-MNIST) and natural language processing (Split-AG News)
benchmarks, comparing it against a sequential fine-tuning baseline. The results
demonstrate that MyGO significantly mitigates catastrophic forgetting and
maintains high average accuracy across tasks, proving the framework's
effectiveness and domain-generality.

</details>


### [81] [Improving Fisher Information Estimation and Efficiency for LoRA-based LLM Unlearning](https://arxiv.org/abs/2508.21300)
*Yejin Kim,Eunwon Kim,Buru Chang,Junsuk Choe*

Main category: cs.LG

TL;DR: VILA是一种新型的机器学习遗忘框架，通过改进FILA方法，显著提高了参数识别准确性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）在生成敏感信息时的遗忘问题，避免高昂的重新训练成本。

Method: 提出VILA框架，明确考虑FILA忽略的假设，优化参数识别，并减少对完整模型的访问需求。

Result: VILA在参数效率和训练速度上分别比FILA提高了100倍和40倍，并在多个基准测试中达到最优性能。

Conclusion: VILA是一种高效且准确的遗忘方法，适用于LLMs的敏感信息处理。

Abstract: LLMs have demonstrated remarkable performance across various tasks but face
challenges related to unintentionally generating outputs containing sensitive
information. A straightforward approach to address this issue is to retrain the
model after excluding the problematic data. However, this approach incurs
prohibitively high computational costs. To overcome this limitation, machine
unlearning has emerged as a promising solution that can effectively remove
sensitive information without the need to retrain the model from scratch.
Recently, FILA has been proposed as a parameter-efficient unlearning method by
integrating LoRA adapters. Specifically, it calculates the Fisher information
to identify parameters associated with the forget set and assigns them to LoRA
adapters for updates. Despite its innovative approach, FILA still requires
access to all model parameters and does not adequately account for fundamental
assumptions underlying Fisher information, leading to inaccuracies in
importance estimation. To address these limitations, we propose VILA, a novel
unlearning framework that explicitly considers the assumptions overlooked in
FILA, thereby enhancing the accuracy of parameter identification for the forget
set. Moreover, VILA significantly reduces computational costs by enabling
parameter identification without accessing the entire model. Our method
achieves up to 100x higher parameter efficiency and 40x faster training speed
compared to FILA, and sets new state-of-the-art performance on benchmarks
including TOFU, WMDP, and MUSE. Our code is available at
https://github.com/kyj93790/VILA.

</details>


### [82] [Convergence of regularized agent-state-based Q-learning in POMDPs](https://arxiv.org/abs/2508.21314)
*Amit Sinha,Matthieu Geist,Aditya Mahajan*

Main category: cs.LG

TL;DR: 本文提出了一个框架，用于理解实践中常用的Q-learning强化学习算法的收敛性，特别是针对带有策略正则化的基于代理状态的Q-learning（RASQL）。


<details>
  <summary>Details</summary>
Motivation: 研究Q-learning算法在实际应用中的收敛性，尤其是当算法使用非信念状态的代理状态和策略正则化时。

Method: 提出并分析了一种简化的Q-learning算法（RASQL），并证明了其在温和技术条件下的收敛性。

Result: RASQL收敛于一个适当定义的正则化MDP的固定点，且该结果适用于学习周期性策略的变体。

Conclusion: 数值实验验证了理论收敛行为，表明RASQL在实际应用中的有效性。

Abstract: In this paper, we present a framework to understand the convergence of
commonly used Q-learning reinforcement learning algorithms in practice. Two
salient features of such algorithms are: (i)~the Q-table is recursively updated
using an agent state (such as the state of a recurrent neural network) which is
not a belief state or an information state and (ii)~policy regularization is
often used to encourage exploration and stabilize the learning algorithm. We
investigate the simplest form of such Q-learning algorithms which we call
regularized agent-state-based Q-learning (RASQL) and show that it converges
under mild technical conditions to the fixed point of an appropriately defined
regularized MDP, which depends on the stationary distribution induced by the
behavioral policy. We also show that a similar analysis continues to work for a
variant of RASQL that learns periodic policies. We present numerical examples
to illustrate that the empirical convergence behavior matches with the proposed
theoretical limit.

</details>


### [83] [Distribution-Aware Feature Selection for SAEs](https://arxiv.org/abs/2508.21324)
*Narmeen Oozeer,Nirmalendu Prakash,Michael Lan,Alice Rigg,Amirali Abdullah*

Main category: cs.LG

TL;DR: Sampled-SAE改进BatchTopK，通过评分和采样特征池，平衡全局一致性和细粒度重建。


<details>
  <summary>Details</summary>
Motivation: 解决BatchTopK中的“激活彩票”问题，即高幅度特征可能挤掉更有信息量的低幅度特征。

Method: 通过评分特征（L2范数或熵）形成候选池，再从中选择Top-K特征，调整参数l以平衡全局与局部特征。

Result: 在Pythia-160M上，不同l值在不同指标上表现各异，需权衡共享结构、重建保真度和下游性能。

Conclusion: Sampled-SAE将BatchTopK扩展为可调、分布感知的方法家族。

Abstract: Sparse autoencoders (SAEs) decompose neural activations into interpretable
features. A widely adopted variant, the TopK SAE, reconstructs each token from
its K most active latents. However, this approach is inefficient, as some
tokens carry more information than others. BatchTopK addresses this limitation
by selecting top activations across a batch of tokens. This improves average
reconstruction but risks an "activation lottery," where rare high-magnitude
features crowd out more informative but lower-magnitude ones. To address this
issue, we introduce Sampled-SAE: we score the columns (representing features)
of the batch activation matrix (via $L_2$ norm or entropy), forming a candidate
pool of size $Kl$, and then apply Top-$K$ to select tokens across the batch
from the restricted pool of features. Varying $l$ traces a spectrum between
batch-level and token-specific selection. At $l=1$, tokens draw only from $K$
globally influential features, while larger $l$ expands the pool toward
standard BatchTopK and more token-specific features across the batch. Small $l$
thus enforces global consistency; large $l$ favors fine-grained reconstruction.
On Pythia-160M, no single value optimizes $l$ across all metrics: the best
choice depends on the trade-off between shared structure, reconstruction
fidelity, and downstream performance. Sampled-SAE thus reframes BatchTopK as a
tunable, distribution-aware family.

</details>


### [84] [Stage-Diff: Stage-wise Long-Term Time Series Generation Based on Diffusion Models](https://arxiv.org/abs/2508.21330)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: Stage-Diff是一种基于扩散模型的分阶段生成模型，用于生成长时间序列，有效平衡了长期依赖和数据分布漂移。


<details>
  <summary>Details</summary>
Motivation: 长时间序列具有复杂的长期依赖和数据分布漂移，现有生成模型难以有效处理这些挑战。

Method: 通过分阶段序列生成和阶段间信息传递，结合渐进序列分解和多通道融合建模。

Result: 在多个真实数据集上的实验验证了Stage-Diff的有效性。

Conclusion: Stage-Diff能够有效生成长时间序列，平衡了长期依赖和分布漂移问题。

Abstract: Generative models have been successfully used in the field of time series
generation. However, when dealing with long-term time series, which span over
extended periods and exhibit more complex long-term temporal patterns, the task
of generation becomes significantly more challenging. Long-term time series
exhibit long-range temporal dependencies, but their data distribution also
undergoes gradual changes over time. Finding a balance between these long-term
dependencies and the drift in data distribution is a key challenge. On the
other hand, long-term time series contain more complex interrelationships
between different feature sequences, making the task of effectively capturing
both intra-sequence and inter-sequence dependencies another important
challenge. To address these issues, we propose Stage-Diff, a staged generative
model for long-term time series based on diffusion models. First, through
stage-wise sequence generation and inter-stage information transfer, the model
preserves long-term sequence dependencies while enabling the modeling of data
distribution shifts. Second, within each stage, progressive sequence
decomposition is applied to perform channel-independent modeling at different
time scales, while inter-stage information transfer utilizes multi-channel
fusion modeling. This approach combines the robustness of channel-independent
modeling with the information fusion advantages of multi-channel modeling,
effectively balancing the intra-sequence and inter-sequence dependencies of
long-term time series. Extensive experiments on multiple real-world datasets
validate the effectiveness of Stage-Diff in long-term time series generation
tasks.

</details>


### [85] [DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks](https://arxiv.org/abs/2508.21340)
*Xuan Hou,Shuhan Liu,Zhaohui Peng,Yaohui Chu,Yue Zhang,Yining Wang*

Main category: cs.LG

TL;DR: 提出了一种名为DLGAN的双层生成对抗网络模型，用于解决时间序列合成中难以保持时间依赖性和特征准确性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列合成方法基于随机序列进行时间建模，难以确保生成序列的时间依赖性，且难以准确捕捉原始时间序列的特征信息。

Method: DLGAN将时间序列生成过程分解为序列特征提取和序列重建两阶段，结合自编码器和GAN，确保重建过程能恢复时间依赖性，并生成符合真实序列特征的合成向量。

Result: 在四个公开数据集上的实验表明，该模型在多种评估指标上均表现出优越性。

Conclusion: DLGAN是一种简单但有效的时间序列生成模型，能够更好地保持时间依赖性和特征准确性。

Abstract: Time series synthesis is an effective approach to ensuring the secure
circulation of time series data. Existing time series synthesis methods
typically perform temporal modeling based on random sequences to generate
target sequences, which often struggle to ensure the temporal dependencies in
the generated time series. Additionally, directly modeling temporal features on
random sequences makes it challenging to accurately capture the feature
information of the original time series. To address the above issues, we
propose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer
\textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named
\textbf{DLGAN}. The model decomposes the time series generation process into
two stages: sequence feature extraction and sequence reconstruction. First,
these two stages form a complete time series autoencoder, enabling supervised
learning on the original time series to ensure that the reconstruction process
can restore the temporal dependencies of the sequence. Second, a Generative
Adversarial Network (GAN) is used to generate synthetic feature vectors that
align with the real-time sequence feature vectors, ensuring that the generator
can capture the temporal features from real time series. Extensive experiments
on four public datasets demonstrate the superiority of this model across
various evaluation metrics.

</details>


### [86] [Adaptive Heavy-Tailed Stochastic Gradient Descent](https://arxiv.org/abs/2508.21353)
*Bodu Gong,Gustavo Enrique Batista,Pierre Lafaye de Micheaux*

Main category: cs.LG

TL;DR: AHTSGD是一种自适应调整噪声分布的优化算法，通过动态适应损失景观的锐度，加速收敛到宽盆地，提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模神经网络优化中因依赖训练损失导致的泛化问题，利用宽盆地的稳定性提升模型性能。

Method: 在训练早期注入重尾噪声增强探索，随着锐度稳定逐渐过渡到轻尾噪声，动态适应损失景观。

Result: 在MNIST、CIFAR-10和SVHN等基准测试中优于SGD和其他噪声方法，加速早期训练并提升泛化能力。

Conclusion: AHTSGD通过动态噪声调整有效提升模型泛化性能，对学习率选择具有鲁棒性。

Abstract: In the era of large-scale neural network models, optimization algorithms
often struggle with generalization due to an overreliance on training loss. One
key insight widely accepted in the machine learning community is the idea that
wide basins (regions around a local minimum where the loss increases gradually)
promote better generalization by offering greater stability to small changes in
input data or model parameters. In contrast, sharp minima are typically more
sensitive and less stable. Motivated by two key empirical observations - the
inherent heavy-tailed distribution of gradient noise in stochastic gradient
descent and the Edge of Stability phenomenon during neural network training, in
which curvature grows before settling at a plateau, we introduce Adaptive Heavy
Tailed Stochastic Gradient Descent (AHTSGD). The algorithm injects
heavier-tailed noise into the optimizer during the early stages of training to
enhance exploration and gradually transitions to lighter-tailed noise as
sharpness stabilizes. By dynamically adapting to the sharpness of the loss
landscape throughout training, AHTSGD promotes accelerated convergence to wide
basins. AHTSGD is the first algorithm to adjust the nature of injected noise
into an optimizer based on the Edge of Stability phenomenon. AHTSGD
consistently outperforms SGD and other noise-based methods on benchmarks like
MNIST and CIFAR-10, with marked gains on noisy datasets such as SVHN. It
ultimately accelerates early training from poor initializations and improves
generalization across clean and noisy settings, remaining robust to learning
rate choices.

</details>


### [87] [Iterative Inference in a Chess-Playing Neural Network](https://arxiv.org/abs/2508.21380)
*Elias Sandmann,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 神经网络在构建表示时并非总是平滑渐进，而是存在复杂的非平滑过程。


<details>
  <summary>Details</summary>
Motivation: 探究神经网络是通过平滑渐进还是复杂计算过程构建表示。

Method: 通过扩展logit lens分析Leela Chess Zero的策略网络。

Result: 发现策略分布常呈非平滑轨迹，如早期发现但随后丢弃的正确解。

Conclusion: 与语言模型的平滑收敛不同，棋类模型的表示构建更复杂。

Abstract: Do neural networks build their representations through smooth, gradual
refinement, or via more complex computational processes? We investigate this by
extending the logit lens to analyze the policy network of Leela Chess Zero, a
superhuman chess engine. We find strong monotonic trends in playing strength
and puzzle-solving ability across layers, yet policy distributions frequently
follow non-smooth trajectories. Evidence for this includes correct puzzle
solutions that are discovered early but subsequently discarded, move rankings
that remain poorly correlated with final outputs, and high policy divergence
until late in the network. These findings contrast with the smooth
distributional convergence typically observed in language models.

</details>


### [88] [PMODE: Theoretically Grounded and Modular Mixture Modeling](https://arxiv.org/abs/2508.21396)
*Robert A. Vandermeulen*

Main category: cs.LG

TL;DR: PMODE是一个模块化的混合模型框架，结合参数和非参数组件，通过数据分区和独立估计实现高效建模。MV-PMODE是其高维扩展，性能媲美深度模型。


<details>
  <summary>Details</summary>
Motivation: 解决混合模型中不同分布族组件的建模问题，并扩展至高维场景。

Method: 通过数据分区和独立密度估计构建混合模型，MV-PMODE扩展至高维。

Result: 在CIFAR-10异常检测中表现优异，接近深度模型。

Conclusion: PMODE框架灵活高效，适用于复杂和高维数据。

Abstract: We introduce PMODE (Partitioned Mixture Of Density Estimators), a general and
modular framework for mixture modeling with both parametric and nonparametric
components. PMODE builds mixtures by partitioning the data and fitting separate
estimators to each subset. It attains near-optimal rates for this estimator
class and remains valid even when the mixture components come from different
distribution families. As an application, we develop MV-PMODE, which scales a
previously theoretical approach to high-dimensional density estimation to
settings with thousands of dimensions. Despite its simplicity, it performs
competitively against deep baselines on CIFAR-10 anomaly detection.

</details>


### [89] [Benchmarking the State of Networks with a Low-Cost Method Based on Reservoir Computing](https://arxiv.org/abs/2508.21420)
*Felix Simon Reimers,Carl-Hendrik Peters,Stefano Nichele*

Main category: cs.LG

TL;DR: 利用挪威移动网络数据，提出了一种低成本、非侵入式的方法来监控通信和移动网络状态，基于储层计算框架。


<details>
  <summary>Details</summary>
Motivation: 开发一种利用现有数据集和储层计算框架的低成本、通用方法来监控网络状态。

Method: 将网络数据转化为储层计算模型，通过代理任务评估模型性能，并与网络状态关联。

Result: 实验表明模型性能与网络状态相关，且对网络扰动敏感。

Conclusion: 该方法可作为概念验证，未来可用于实时监控和识别网络弱点。

Abstract: Using data from mobile network utilization in Norway, we showcase the
possibility of monitoring the state of communication and mobility networks with
a non-invasive, low-cost method. This method transforms the network data into a
model within the framework of reservoir computing and then measures the model's
performance on proxy tasks. Experimentally, we show how the performance on
these proxies relates to the state of the network. A key advantage of this
approach is that it uses readily available data sets and leverages the
reservoir computing framework for an inexpensive and largely agnostic method.
Data from mobile network utilization is available in an anonymous, aggregated
form with multiple snapshots per day. This data can be treated like a weighted
network. Reservoir computing allows the use of weighted, but untrained networks
as a machine learning tool. The network, initialized as a so-called echo state
network (ESN), projects incoming signals into a higher dimensional space, on
which a single trained layer operates. This consumes less energy than deep
neural networks in which every weight of the network is trained. We use
neuroscience inspired tasks and trained our ESN model to solve them. We then
show how the performance depends on certain network configurations and also how
it visibly decreases when perturbing the network. While this work serves as
proof of concept, we believe it can be elevated to be used for near-real-time
monitoring as well as the identification of possible weak spots of both mobile
communication networks as well as transportation networks.

</details>


### [90] [Rethinking Layer-wise Model Merging through Chain of Merges](https://arxiv.org/abs/2508.21421)
*Pietro Buzzega,Riccardo Salami,Angelo Porrello,Simone Calderara*

Main category: cs.LG

TL;DR: CoM提出了一种层间合并方法，通过自回归方式更新激活统计量，解决了现有合并技术中的分布不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有合并技术独立处理每层，忽略了层间依赖关系，导致分布不匹配问题。

Method: 提出Chain of Merges (CoM)，通过自回归方式更新激活统计量，显式考虑层间交互。

Result: CoM在标准基准测试中实现了最先进的性能。

Conclusion: CoM通过条件最优更新有效缓解了协变量偏移导致的性能下降。

Abstract: Fine-tuning pretrained models has become a standard pathway to achieve
state-of-the-art performance across a wide range of domains, leading to a
proliferation of task-specific model variants. As the number of such
specialized modules in-creases, merging them into a unified model without
retraining has become a critical challenge. Existing merging techniques often
rely on interference heuristics,importance weighting, or activation matching
while treating each layer independently, thereby failing to account for the
inter-layer dependencies inherent in deep networks. This simplification leads
to distributional mismatches, especially inactivation-based methods, when
changes in early layers are not properly reflected in downstream ones. We
identify these mismatches as a form of internal covariate shift, comparable to
the phenomenon encountered in the initial phases of neural networks training.
To address it, we propose Chain of Merges (CoM), a layer-wise merging procedure
that updates activation statistics in an auto-regressive fashion, explicitly
accounting for cross-layer interactions. CoM produces a coherent merged model
through a series of conditionally optimal updates, effectively mitigating
degradation caused by covariate shift. Experiments on standard bench-marks
demonstrate that CoM achieves state-of-the-art performance.

</details>


### [91] [Quantum enhanced ensemble GANs for anomaly detection in continuous biomanufacturing](https://arxiv.org/abs/2508.21438)
*Rajiv Kailasanathan,William R. Clements,Mohammad Reza Boskabadi,Shawn M. Gibford,Emmanouil Papadakis,Christopher J. Savoie,Seyed Soheil Mansouri*

Main category: cs.LG

TL;DR: 提出了一种基于GAN集成的新型无监督异常检测框架，用于连续生物制造中的异常检测，并探索了混合量子/经典GAN方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 连续生物制造过程中即使微小偏差也可能影响产量和稳定性，需要高效的异常检测方法。

Method: 使用GAN集成框架，并测试了混合量子/经典GAN方法在模拟和真实量子处理器上的表现。

Result: 混合量子/经典方法提高了异常检测率。

Conclusion: 混合量子/经典方法在复杂连续生物制造过程中具有实际应用潜力。

Abstract: The development of continuous biomanufacturing processes requires robust and
early anomaly detection, since even minor deviations can compromise yield and
stability, leading to disruptions in scheduling, reduced weekly production, and
diminished economic performance. These processes are inherently complex and
exhibit non-linear dynamics with intricate relationships between process
variables, thus making advanced methods for anomaly detection essential for
efficient operation. In this work, we present a novel framework for
unsupervised anomaly detection in continuous biomanufacturing based on an
ensemble of generative adversarial networks (GANs). We first establish a
benchmark dataset simulating both normal and anomalous operation regimes in a
continuous process for the production of a small molecule. We then demonstrate
the effectiveness of our GAN-based framework in detecting anomalies caused by
sudden feedstock variability. Finally, we evaluate the impact of using a hybrid
quantum/classical GAN approach with both a simulated quantum circuit and a real
photonic quantum processor on anomaly detection performance. We find that the
hybrid approach yields improved anomaly detection rates. Our work shows the
potential of hybrid quantum/classical approaches for solving real-world
problems in complex continuous biomanufacturing processes.

</details>


### [92] [Beyond expected value: geometric mean optimization for long-term policy performance in reinforcement learning](https://arxiv.org/abs/2508.21443)
*Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 提出了一种结合标准期望累积奖励和时间平均增长率的强化学习算法，以优化个体轨迹的长期表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法优化期望累积奖励，但在实际应用中，个体轨迹的表现可能更重要。

Method: 定义了时间平均增长率的Bellman算子，提出修正几何均值作为估计器，并将其作为正则化项嵌入目标函数。

Result: 在模拟实验中，该算法优于传统强化学习方法。

Conclusion: 新算法能同时利用期望平均和时间平均，提升个体轨迹的长期表现。

Abstract: Reinforcement learning (RL) algorithms typically optimize the expected
cumulative reward, i.e., the expected value of the sum of scalar rewards an
agent receives over the course of a trajectory. The expected value averages the
performance over an infinite number of trajectories. However, when deploying
the agent in the real world, this ensemble average may be uninformative for the
performance of individual trajectories. Thus, in many applications, optimizing
the long-term performance of individual trajectories might be more desirable.
In this work, we propose a novel RL algorithm that combines the standard
ensemble average with the time-average growth rate, a measure for the long-term
performance of individual trajectories. We first define the Bellman operator
for the time-average growth rate. We then show that, under multiplicative
reward dynamics, the geometric mean aligns with the time-average growth rate.
To address more general and unknown reward dynamics, we propose a modified
geometric mean with $N$-sliding window that captures the path-dependency as an
estimator for the time-average growth rate. This estimator is embedded as a
regularizer into the objective, forming a practical algorithm and enabling the
policy to benefit from ensemble average and time-average simultaneously. We
evaluate our algorithm in challenging simulations, where it outperforms
conventional RL methods.

</details>


### [93] [Normalized Maximum Likelihood Code-Length on Riemannian Manifold Data Spaces](https://arxiv.org/abs/2508.21466)
*Kota Fukuzawa,Atsushi Suzuki,Kenji Yamanishi*

Main category: cs.LG

TL;DR: 提出了一种新的Riemannian流形NML（Rm-NML），适用于非欧几里得空间的数据分析，特别是具有层次结构的图数据。


<details>
  <summary>Details</summary>
Motivation: 现有NML方法主要基于欧几里得空间，且依赖于坐标系选择，难以扩展到Riemannian流形。

Method: 定义了Rm-NML，并扩展了现有NML计算技术到Riemannian流形，特别针对对称空间简化了计算。

Result: Rm-NML在坐标变换下不变，且在欧几里得空间中与常规NML一致。

Conclusion: Rm-NML为Riemannian流形上的数据分析提供了新工具，并在双曲空间上进行了实际应用验证。

Abstract: In recent years, with the large-scale expansion of graph data, there has been
an increased focus on Riemannian manifold data spaces other than Euclidean
space. In particular, the development of hyperbolic spaces has been remarkable,
and they have high expressive power for graph data with hierarchical
structures. Normalized Maximum Likelihood (NML) is employed in regret
minimization and model selection. However, existing formulations of NML have
been developed primarily in Euclidean spaces and are inherently dependent on
the choice of coordinate systems, making it non-trivial to extend NML to
Riemannian manifolds. In this study, we define a new NML that reflects the
geometric structure of Riemannian manifolds, called the Riemannian manifold NML
(Rm-NML). This Rm-NML is invariant under coordinate transformations and
coincides with the conventional NML under the natural parameterization in
Euclidean space. We extend existing computational techniques for NML to the
setting of Riemannian manifolds. Furthermore, we derive a method to simplify
the computation of Rm-NML on Riemannian symmetric spaces, which encompass data
spaces of growing interest such as hyperbolic spaces. To illustrate the
practical application of our proposed method, we explicitly computed the Rm-NML
for normal distributions on hyperbolic spaces.

</details>


### [94] [Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration](https://arxiv.org/abs/2508.21468)
*Seungyeon Choi,Hwanhee Kim,Chihyun Park,Dahyeon Lee,Seungyong Lee,Yoonju Kim,Hyoungjoon Park,Sein Kwon,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: CByG框架通过贝叶斯流网络改进3D分子生成，综合评估结合亲和力、合成可行性和选择性，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于扩散的生成模型在药物发现中忽视了合成可行性和选择性等关键性质，CByG旨在填补这一空白。

Method: CByG将贝叶斯流网络扩展为基于梯度的条件生成模型，整合属性特定指导。

Result: CByG在多个关键评估标准上显著优于基线模型。

Conclusion: CByG框架在真实药物发现应用中表现出高效性和实用性。

Abstract: Recent advances in Structure-based Drug Design (SBDD) have leveraged
generative models for 3D molecular generation, predominantly evaluating model
performance by binding affinity to target proteins. However, practical drug
discovery necessitates high binding affinity along with synthetic feasibility
and selectivity, critical properties that were largely neglected in previous
evaluations. To address this gap, we identify fundamental limitations of
conventional diffusion-based generative models in effectively guiding molecule
generation toward these diverse pharmacological properties. We propose CByG, a
novel framework extending Bayesian Flow Network into a gradient-based
conditional generative model that robustly integrates property-specific
guidance. Additionally, we introduce a comprehensive evaluation scheme
incorporating practical benchmarks for binding affinity, synthetic feasibility,
and selectivity, overcoming the limitations of conventional evaluation methods.
Extensive experiments demonstrate that our proposed CByG framework
significantly outperforms baseline models across multiple essential evaluation
criteria, highlighting its effectiveness and practicality for real-world drug
discovery applications.

</details>


### [95] [Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning](https://arxiv.org/abs/2508.21488)
*Pascal R. van der Vaart,Neil Yorke-Smith,Matthijs T. J. Spaan*

Main category: cs.LG

TL;DR: 论文探讨了贝叶斯深度Q学习中的冷后验效应，挑战了常见的高斯似然假设，并提出改进先验的方法。


<details>
  <summary>Details</summary>
Motivation: 研究贝叶斯强化学习中后验近似准确性之外的先验和似然假设的准确性。

Method: 通过统计测试验证高斯似然假设的合理性，并提出改进先验的简单解决方案。

Result: 发现冷后验效应，即降低后验温度能提高性能，且高斯似然假设常被违反。

Conclusion: 未来研究应聚焦于开发更合适的似然和先验，以提升贝叶斯强化学习算法的性能。

Abstract: Uncertainty quantification in reinforcement learning can greatly improve
exploration and robustness. Approximate Bayesian approaches have recently been
popularized to quantify uncertainty in model-free algorithms. However, so far
the focus has been on improving the accuracy of the posterior approximation,
instead of studying the accuracy of the prior and likelihood assumptions
underlying the posterior. In this work, we demonstrate that there is a cold
posterior effect in Bayesian deep Q-learning, where contrary to theory,
performance increases when reducing the temperature of the posterior. To
identify and overcome likely causes, we challenge common assumptions made on
the likelihood and priors in Bayesian model-free algorithms. We empirically
study prior distributions and show through statistical tests that the common
Gaussian likelihood assumption is frequently violated. We argue that developing
more suitable likelihoods and priors should be a key focus in future Bayesian
reinforcement learning research and we offer simple, implementable solutions
for better priors in deep Q-learning that lead to more performant Bayesian
algorithms.

</details>


### [96] [Failure Prediction Is a Better Performance Proxy for Early-Exit Networks Than Calibration](https://arxiv.org/abs/2508.21495)
*Piotr Kubaty,Filip Szatkowski,Metod Jazbec,Bartosz Wójcik*

Main category: cs.LG

TL;DR: 本文探讨了早期退出模型中校准方法的局限性，并提出失败预测作为更可靠的性能指标。


<details>
  <summary>Details</summary>
Motivation: 现有早期退出模型依赖基于置信度的退出策略，但校准方法可能误导性能评估，需要更可靠的指标。

Method: 通过实证分析校准方法的不足，提出使用失败预测作为替代指标。

Result: 实验表明，未校准网络可能优于校准网络，失败预测与效率提升强相关。

Conclusion: 失败预测是设计和评估早期退出模型更可靠的指标，优于传统校准方法。

Abstract: Early-exit models speed up inference by attaching internal classifiers to
intermediate layers of the model and allowing computation to stop once a
prediction satisfies an exit criterion. Most early-exit methods rely on
confidence-based exit strategies, which motivated some works to calibrate
intermediate classifiers to improve the performance of the entire model. In
this paper, we show that calibration measures can be misleading indicators of
the performance of multi-exit models: a well-calibrated classifier may still
waste computation, and common calibration methods do not preserve the sample
ranking within a classifier. We demonstrate empirical cases where miscalibrated
networks outperform calibrated ones. As an alternative, we propose to use
failure prediction as a more useful proxy for early-exit model performance.
Unlike calibration, failure prediction accounts for changes in the ranking of
samples and shows a strong correlation with efficiency improvements, making it
a more dependable basis for designing and evaluating early-exit models.

</details>


### [97] [Spiking Decision Transformers: Local Plasticity, Phase-Coding, and Dendritic Routing for Low-Power Sequence Control](https://arxiv.org/abs/2508.21505)
*Vishal Pandey,Debasmita Biswas*

Main category: cs.LG

TL;DR: SNN-DT结合了Transformer和脉冲神经网络，实现了高效低能耗的序列决策。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在边缘设备上能耗高的问题，同时保留序列建模能力。

Method: 在自注意力块中嵌入LIF神经元，使用替代梯度训练，引入生物启发的模块。

Result: 在经典控制任务上性能相当或更好，能耗降低四个数量级。

Conclusion: SNN-DT为嵌入式设备提供了实时低功耗控制的新途径。

Abstract: Reinforcement learning agents based on Transformer architectures have
achieved impressive performance on sequential decision-making tasks, but their
reliance on dense matrix operations makes them ill-suited for
energy-constrained, edge-oriented platforms. Spiking neural networks promise
ultra-low-power, event-driven inference, yet no prior work has seamlessly
merged spiking dynamics with return-conditioned sequence modeling. We present
the Spiking Decision Transformer (SNN-DT), which embeds Leaky
Integrate-and-Fire neurons into each self-attention block, trains end-to-end
via surrogate gradients, and incorporates biologically inspired three-factor
plasticity, phase-shifted spike-based positional encodings, and a lightweight
dendritic routing module. Our implementation matches or exceeds standard
Decision Transformer performance on classic control benchmarks (CartPole-v1,
MountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes
per decision, an energy proxy suggesting over four orders-of-magnitude
reduction in per inference energy. By marrying sequence modeling with
neuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power
control on embedded and wearable devices.

</details>


### [98] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: 评估LLMs在贷款审批任务中的性能和公平性，发现序列化格式和上下文学习对结果有显著影响。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理表格数据和保证公平性方面存在挑战，研究其在贷款审批任务中的表现。

Method: 使用来自加纳、德国和美国的贷款审批数据集，评估零样本和上下文学习能力。

Result: 序列化格式显著影响性能和公平性，上下文学习提升性能但对公平性影响不一。

Conclusion: 强调有效表格数据表示和公平性模型对提升LLMs在金融决策中的可靠性至关重要。

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [99] [On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature](https://arxiv.org/abs/2508.21513)
*Geri Skenderi*

Main category: cs.LG

TL;DR: GNNs在解决SAT问题时性能下降，原因是图Ricci曲率揭示了局部连接瓶颈，导致长距离依赖难以压缩。


<details>
  <summary>Details</summary>
Motivation: 探究GNNs在解决SAT问题时性能下降的根本原因，是否与图Ricci曲率相关。

Method: 通过图Ricci曲率分析随机k-SAT公式的二分图，证明其负曲率与问题难度相关，并验证GNNs的oversquashing现象。

Result: 曲率是问题复杂度的强指标，可预测GNNs性能。

Conclusion: 图Ricci曲率揭示了GNNs在SAT问题中的局限性，为未来设计提供了方向。

Abstract: Graph Neural Networks (GNNs) have recently shown promise as solvers for
Boolean Satisfiability Problems (SATs) by operating on graph representations of
logical formulas. However, their performance degrades sharply on harder
instances, raising the question of whether this reflects fundamental
architectural limitations. In this work, we provide a geometric explanation
through the lens of graph Ricci Curvature (RC), which quantifies local
connectivity bottlenecks. We prove that bipartite graphs derived from random
k-SAT formulas are inherently negatively curved, and that this curvature
decreases with instance difficulty. Building on this, we show that GNN-based
SAT solvers are affected by oversquashing, a phenomenon where long-range
dependencies become impossible to compress into fixed-length representations.
We validate our claims empirically across different SAT benchmarks and confirm
that curvature is both a strong indicator of problem complexity and can be used
to predict performance. Finally, we connect our findings to design principles
of existing solvers and outline promising directions for future work.

</details>


### [100] [What Data is Really Necessary? A Feasibility Study of Inference Data Minimization for Recommender Systems](https://arxiv.org/abs/2508.21547)
*Jens Leysen,Marco Favier,Bart Goethals*

Main category: cs.LG

TL;DR: 研究表明，减少推荐系统中的隐式反馈数据在技术上是可行的，但实际应用仍受技术和用户因素影响。


<details>
  <summary>Details</summary>
Motivation: 数据最小化是法律原则，但如何在依赖大量个人数据的推荐系统中实现这一原则仍具挑战性。

Method: 提出新问题框架，分析多种最小化技术，并研究影响其有效性的关键因素。

Result: 技术可行，但实际效果取决于技术设置和用户特征。

Conclusion: 数据最小化技术可行，但实际应用复杂，难以制定通用标准。

Abstract: Data minimization is a legal principle requiring personal data processing to
be limited to what is necessary for a specified purpose. Operationalizing this
principle for recommender systems, which rely on extensive personal data,
remains a significant challenge. This paper conducts a feasibility study on
minimizing implicit feedback inference data for such systems. We propose a
novel problem formulation, analyze various minimization techniques, and
investigate key factors influencing their effectiveness. We demonstrate that
substantial inference data reduction is technically feasible without
significant performance loss. However, its practicality is critically
determined by two factors: the technical setting (e.g., performance targets,
choice of model) and user characteristics (e.g., history size, preference
complexity). Thus, while we establish its technical feasibility, we conclude
that data minimization remains practically challenging and its dependence on
the technical and user context makes a universal standard for data `necessity'
difficult to implement.

</details>


### [101] [Comprehensive Signal Quality Evaluation of a Wearable Textile ECG Garment: A Sex-Balanced Study](https://arxiv.org/abs/2508.21554)
*Maximilian P. Oppelt,Tobias S. Zech,Sarah H. Lorenz,Laurenz Ottmann,Jan Steffan,Bjoern M. Eskofier,Nadine R. Lang-Richter,Norman Pfeiffer*

Main category: cs.LG

TL;DR: 新型可穿戴纺织服装通过创新电极布局减少噪声和运动伪影，提升ECG信号质量，并通过性别平衡评估验证其适用性。


<details>
  <summary>Details</summary>
Motivation: 解决传统ECG设备在信号质量和性别适应性上的不足，推动可穿戴健康技术的发展。

Method: 采用15名男性和15名女性参与者，通过信号质量指数、生理参数分析、机器学习分类、形态学分析和电极角度研究等多维度评估。

Result: 纺织系统信号质量与参考设备高度一致，分类性能稳健，并揭示了性别特异性对信号采集的影响。

Conclusion: 纺织ECG服装在生理监测和心理状态检测中具有实用潜力，强调性别特异性设计的重要性。

Abstract: We introduce a novel wearable textile-garment featuring an innovative
electrode placement aimed at minimizing noise and motion artifacts, thereby
enhancing signal fidelity in Electrocardiography (ECG) recordings. We present a
comprehensive, sex-balanced evaluation involving 15 healthy males and 15
healthy female participants to ensure the device's suitability across
anatomical and physiological variations. The assessment framework encompasses
distinct evaluation approaches: quantitative signal quality indices to
objectively benchmark device performance; rhythm-based analyzes of
physiological parameters such as heart rate and heart rate variability; machine
learning classification tasks to assess application-relevant predictive
utility; morphological analysis of ECG features including amplitude and
interval parameters; and investigations of the effects of electrode projection
angle given by the textile / body shape, with all analyzes stratified by sex to
elucidate sex-specific influences. Evaluations were conducted across various
activity phases representing real-world conditions. The results demonstrate
that the textile system achieves signal quality highly concordant with
reference devices in both rhythm and morphological analyses, exhibits robust
classification performance, and enables identification of key sex-specific
determinants affecting signal acquisition. These findings underscore the
practical viability of textile-based ECG garments for physiological monitoring
as well as psychophysiological state detection. Moreover, we identify the
importance of incorporating sex-specific design considerations to ensure
equitable and reliable cardiac diagnostics in wearable health technologies.

</details>


### [102] [Limitations of Physics-Informed Neural Networks: a Study on Smart Grid Surrogation](https://arxiv.org/abs/2508.21559)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: PINNs在智能电网建模中表现出色，通过物理约束提升泛化能力，优于传统数据驱动方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺和物理一致性问题，提升智能电网动态建模的准确性和可靠性。

Method: 使用基于物理的损失函数训练PINNs，并与XGBoost、随机森林和线性回归进行比较。

Result: PINNs在误差减少和动态操作中表现更优，尤其在状态转换和物理可行性方面。

Conclusion: PINNs是智能电网建模的范式转变工具，结合了数据驱动灵活性和物理原理严谨性。

Abstract: Physics-Informed Neural Networks (PINNs) present a transformative approach
for smart grid modeling by integrating physical laws directly into learning
frameworks, addressing critical challenges of data scarcity and physical
consistency in conventional data-driven methods. This paper evaluates PINNs'
capabilities as surrogate models for smart grid dynamics, comparing their
performance against XGBoost, Random Forest, and Linear Regression across three
key experiments: interpolation, cross-validation, and episodic trajectory
prediction. By training PINNs exclusively through physics-based loss functions
(enforcing power balance, operational constraints, and grid stability) we
demonstrate their superior generalization, outperforming data-driven models in
error reduction. Notably, PINNs maintain comparatively lower MAE in dynamic
grid operations, reliably capturing state transitions in both random and
expert-driven control scenarios, while traditional models exhibit erratic
performance. Despite slight degradation in extreme operational regimes, PINNs
consistently enforce physical feasibility, proving vital for safety-critical
applications. Our results contribute to establishing PINNs as a
paradigm-shifting tool for smart grid surrogation, bridging data-driven
flexibility with first-principles rigor. This work advances real-time grid
control and scalable digital twins, emphasizing the necessity of physics-aware
architectures in mission-critical energy systems.

</details>


### [103] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: 提出InsightTab框架，通过数据蒸馏提升LLM在表格分类任务中的表现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在结构化数据分类中的变异性挑战，借鉴人类学习过程提升分类效果。

Method: 结合规则总结、策略示例和反思学习，通过LLM与数据建模技术的深度协作实现数据蒸馏。

Result: 在九个数据集上表现优于现有方法，验证了蒸馏过程的有效性。

Conclusion: InsightTab能有效利用标注数据并管理偏差，提升LLM在表格分类中的性能。

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


### [104] [OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories](https://arxiv.org/abs/2508.21570)
*Bo Li,Yingqi Feng,Ming Jin,Xin Zheng,Yufei Tang,Laurent Cherubin,Alan Wee-Chung Liew,Can Wang,Qinghua Lu,Jingwei Yao,Shirui Pan,Hong Zhang,Xingquan Zhu*

Main category: cs.LG

TL;DR: OASIS是一种新型扩散对抗框架，用于解决海洋盐度数据稀疏、不规则和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 海洋盐度对环流、气候和海洋生态系统至关重要，但传统测量方法存在局限性，机器学习模型在稀疏数据下表现不佳。

Method: 提出OASIS，一种扩散对抗框架，结合物理协变量，解决数据稀疏和噪声问题。

Result: OASIS能够有效处理稀疏和噪声数据，优于传统方法和普通机器学习模型。

Conclusion: OASIS为海洋盐度数据填补提供了一种创新且有效的解决方案。

Abstract: Ocean salinity plays a vital role in circulation, climate, and marine
ecosystems, yet its measurement is often sparse, irregular, and noisy,
especially in drifter-based datasets. Traditional approaches, such as remote
sensing and optimal interpolation, rely on linearity and stationarity, and are
limited by cloud cover, sensor drift, and low satellite revisit rates. While
machine learning models offer flexibility, they often fail under severe
sparsity and lack principled ways to incorporate physical covariates without
specialized sensors. In this paper, we introduce the OceAn Salinity Imputation
System (OASIS), a novel diffusion adversarial framework designed to address
these challenges.

</details>


### [105] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: 证明了随机梯度下降在训练过参数化两层PINNs时的线性收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究随机梯度下降在训练物理信息神经网络（PINNs）时的收敛性，扩展了现有关于梯度下降的研究。

Method: 分析了随机优化方法引入的动态随机性，关键在于确保训练过程中Gram矩阵的正定性。

Result: 在高概率意义下，证明了随机梯度下降/流的线性收敛性。

Conclusion: 为随机算法训练的神经网络提供了理论保证，并揭示了优化过程的动态特性。

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


### [106] [Physics-Informed Spectral Modeling for Hyperspectral Imaging](https://arxiv.org/abs/2508.21618)
*Zuzanna Gawrysiak,Krzysztof Krawiec*

Main category: cs.LG

TL;DR: PhISM是一种无监督的物理信息深度学习架构，能够解耦高光谱观测数据并用连续基函数建模。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱数据分类和回归任务中需要大量标注数据的问题，同时提供可解释的潜在表示。

Method: 采用物理信息深度学习架构，无监督学习解耦高光谱数据并建模。

Result: 在多个分类和回归基准测试中优于现有方法，且所需标注数据较少。

Conclusion: PhISM不仅性能优越，还能通过可解释的潜在表示提供额外洞见。

Abstract: We present PhISM, a physics-informed deep learning architecture that learns
without supervision to explicitly disentangle hyperspectral observations and
model them with continuous basis functions. \mname outperforms prior methods on
several classification and regression benchmarks, requires limited labeled
data, and provides additional insights thanks to interpretable latent
representation.

</details>


### [107] [Introduction to the Analysis of Probabilistic Decision-Making Algorithms](https://arxiv.org/abs/2508.21620)
*Agustinus Kristiadi*

Main category: cs.LG

TL;DR: 本文旨在为非专家提供关于概率决策算法理论分析的易读、自包含介绍，涵盖多臂老虎机、贝叶斯优化和树搜索算法。


<details>
  <summary>Details</summary>
Motivation: 科学发现中实验成本高昂，决策理论算法能高效利用数据，降低实验成本。但现有理论分析对非专家不友好。

Method: 介绍常用概率决策算法的理论分析，包括多臂老虎机、贝叶斯优化和树搜索算法。

Result: 提供对算法行为的理论理解，为下一代算法开发提供见解。

Conclusion: 本文填补了非专家难以理解决策算法理论的空白，有助于推动科学发现中的高效决策方法。

Abstract: Decision theories offer principled methods for making choices under various
types of uncertainty. Algorithms that implement these theories have been
successfully applied to a wide range of real-world problems, including
materials and drug discovery. Indeed, they are desirable since they can
adaptively gather information to make better decisions in the future, resulting
in data-efficient workflows. In scientific discovery, where experiments are
costly, these algorithms can thus significantly reduce the cost of
experimentation. Theoretical analyses of these algorithms are crucial for
understanding their behavior and providing valuable insights for developing
next-generation algorithms. However, theoretical analyses in the literature are
often inaccessible to non-experts. This monograph aims to provide an
accessible, self-contained introduction to the theoretical analysis of commonly
used probabilistic decision-making algorithms, including bandit algorithms,
Bayesian optimization, and tree search algorithms. Only basic knowledge of
probability theory and statistics, along with some elementary knowledge about
Gaussian processes, is assumed.

</details>


### [108] [Predicting Social Media Engagement from Emotional and Temporal Features](https://arxiv.org/abs/2508.21650)
*Yunwoo Kim,Junhyuk Hwang*

Main category: cs.LG

TL;DR: 机器学习方法通过情感和时间特征预测社交媒体参与度（评论和点赞），模型对点赞预测效果极佳（R²=0.98），但对评论预测较差（R²=0.41）。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用情感和时间特征预测社交媒体参与度，以优化内容策略。

Method: 使用基于HistGradientBoostingRegressor的多目标回归模型，对数变换处理偏态目标数据。

Result: 模型对点赞预测效果极佳（R²=0.98），但对评论预测较差（R²=0.41），表明评论受其他未捕获因素影响。

Conclusion: 情感和时间特征能有效预测点赞，但评论预测需更多特征支持。

Abstract: We present a machine learning approach for predicting social media engagement
(comments and likes) from emotional and temporal features. The dataset contains
600 songs with annotations for valence, arousal, and related sentiment metrics.
A multi target regression model based on HistGradientBoostingRegressor is
trained on log transformed engagement ratios to address skewed targets.
Performance is evaluated with both a custom order of magnitude accuracy and
standard regression metrics, including the coefficient of determination (R^2).
Results show that emotional and temporal metadata, together with existing view
counts, predict future engagement effectively. The model attains R^2 = 0.98 for
likes but only R^2 = 0.41 for comments. This gap indicates that likes are
largely driven by readily captured affective and exposure signals, whereas
comments depend on additional factors not represented in the current feature
set.

</details>


### [109] [Activation Subspaces for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21695)
*Barış Zöngür,Robin Hesse,Stefan Roth*

Main category: cs.LG

TL;DR: 提出了一种基于权重矩阵奇异值分解的OOD检测方法ActSub，通过分解激活空间的关键与非关键成分，分别处理Far-OOD和Near-OOD场景，取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法在分布偏移较大或较小时效果有限，需针对性解决。

Method: 利用分类头权重矩阵的奇异值分解，将激活空间分解为关键和非关键成分，分别用于Near-OOD和Far-OOD检测。

Result: 在多个标准OOD基准测试中达到SOTA性能。

Conclusion: ActSub通过分离激活空间成分，有效提升了不同分布偏移下的OOD检测能力。

Abstract: To ensure the reliability of deep models in real-world applications,
out-of-distribution (OOD) detection methods aim to distinguish samples close to
the training distribution (in-distribution, ID) from those farther away (OOD).
In this work, we propose a novel OOD detection method that utilizes singular
value decomposition of the weight matrix of the classification head to
decompose the model's activations into decisive and insignificant components,
which contribute maximally, respectively minimally, to the final classifier
output. We find that the subspace of insignificant components more effectively
distinguishes ID from OOD data than raw activations in regimes of large
distribution shifts (Far-OOD). This occurs because the classification objective
leaves the insignificant subspace largely unaffected, yielding features that
are ''untainted'' by the target classification task. Conversely, in regimes of
smaller distribution shifts (Near-OOD), we find that activation shaping methods
profit from only considering the decisive subspace, as the insignificant
component can cause interference in the activation space. By combining two
findings into a single approach, termed ActSub, we achieve state-of-the-art
results in various standard OOD benchmarks.

</details>


### [110] [Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety](https://arxiv.org/abs/2508.21722)
*Siddharth Mangalik,Ojas Deshpande,Adithya V. Ganesan,Sean A. P. Clouston,H. Andrew Schwartz*

Main category: cs.LG

TL;DR: 论文提出了一种基于统计学习的纵向回归不连续性设计（LRDD）框架，用于预测社区心理健康评分的不连续性和斜率变化，优于传统静态方法。


<details>
  <summary>Details</summary>
Motivation: 估计社区特定心理健康事件的影响对公共卫生政策至关重要，传统预测方法效果有限。

Method: 将LRDD扩展到统计学习框架，结合历史评分、动态协变量和外生变量预测不连续性和斜率变化。

Result: 在预测COVID-19事件对美国县焦虑评分的影响时，模型表现优于传统方法（不连续性r=+0.46，斜率r=+0.65）。

Conclusion: 该方法为预测未来或假设事件对特定社区的独特影响提供了新可能性。

Abstract: Estimating community-specific mental health effects of local events is vital
for public health policy. While forecasting mental health scores alone offers
limited insights into the impact of events on community well-being,
quasi-experimental designs like the Longitudinal Regression Discontinuity
Design (LRDD) from econometrics help researchers derive more effects that are
more likely to be causal from observational data. LRDDs aim to extrapolate the
size of changes in an outcome (e.g. a discontinuity in running scores for
anxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond
traditional forecasting into a statistical learning framework whereby future
discontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear
trajectories) are estimated given a location's history of the score, dynamic
covariates (other running assessments), and exogenous variables (static
representations). Applying our framework to predict discontinuities in the
anxiety of US counties from COVID-19 events, we found the task was difficult
but more achievable as the sophistication of models was increased, with the
best results coming from integrating exogenous and dynamic covariates. Our
approach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$
for slope) over traditional static community representations. Discontinuity
forecasting raises new possibilities for estimating the idiosyncratic effects
of potential future or hypothetical events on specific communities.

</details>


### [111] [Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL, Rogue Software and Auto-SNL](https://arxiv.org/abs/2508.21739)
*Hamza Ezzaoui Rahali,Abhilasha Dave,Larry Ruckman,Mohammad Mehdi Rahimifar,Audrey C. Therrien,James J. Russel,Ryan T. Herbst*

Main category: cs.LG

TL;DR: SLAC开发了SNL框架，用于在FPGA上部署实时机器学习推理模型，并推出Auto-SNL工具简化模型转换，性能优于现有工具hls4ml。


<details>
  <summary>Details</summary>
Motivation: 处理LCLS-II FEL产生的高达1 TB/s的数据流，传统方法成本高且延迟高，需要实时机器学习解决方案。

Method: 开发SNL框架，支持动态更新FPGA模型权重，无需重新合成；推出Auto-SNL工具，将Python模型转换为SNL兼容代码。

Result: SNL在多数测试架构中延迟表现优于hls4ml，部分情况下还能节省FPGA资源。

Conclusion: SNL为高能物理、医学影像等领域提供了高效、灵活的实时数据处理方案。

Abstract: The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline
experiments at rates of up to 1~MHz, with detectors producing data throughputs
exceeding 1 TB/s. Managing such massive data streams presents significant
challenges, as transmission and storage infrastructures become prohibitively
expensive. Machine learning (ML) offers a promising solution for real-time data
reduction, but conventional implementations introduce excessive latency, making
them unsuitable for high-speed experimental environments. To address these
challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized
framework designed to deploy real-time ML inference models on
Field-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to
dynamically update model weights without requiring FPGA resynthesis, enhancing
flexibility for adaptive learning applications. To further enhance usability
and accessibility, we introduce Auto-SNL, a Python extension that streamlines
the process of converting Python-based neural network models into
SNL-compatible high-level synthesis code. This paper presents a benchmark
comparison against hls4ml, the current state-of-the-art tool, across multiple
neural network architectures, fixed-point precisions, and synthesis
configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL
achieves competitive or superior latency in most tested architectures, while in
some cases also offering FPGA resource savings. This adaptation demonstrates
SNL's versatility, opening new opportunities for researchers and academics in
fields such as high-energy physics, medical imaging, robotics, and many more.

</details>


### [112] [UniMLR: Modeling Implicit Class Significance for Multi-Label Ranking](https://arxiv.org/abs/2508.21772)
*V. Bugra Yesilkaynak,Emine Dari,Alican Mertan,Gozde Unal*

Main category: cs.LG

TL;DR: 提出UniMLR，一种新的多标签排名（MLR）范式，利用正标签的排名信息建模类别相关性，统一排名与分类任务，并通过合成数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MLR框架仅利用标签的二分类信息，忽略了正标签间的排名信息，限制了性能提升。

Method: 提出UniMLR，将类别相关性建模为概率分布，利用正标签排名信息，并引入合成数据集Ranked MNISTs解决数据稀缺和标注偏差问题。

Result: 实验证明UniMLR能准确学习正标签的排名顺序，与真实值一致且与底层显著性值成比例。

Conclusion: UniMLR通过统一排名与分类任务，显著提升了MLR性能，并在合成与真实数据集上验证了其有效性。

Abstract: Existing multi-label ranking (MLR) frameworks only exploit information
deduced from the bipartition of labels into positive and negative sets.
Therefore, they do not benefit from ranking among positive labels, which is the
novel MLR approach we introduce in this paper. We propose UniMLR, a new MLR
paradigm that models implicit class relevance/significance values as
probability distributions using the ranking among positive labels, rather than
treating them as equally important. This approach unifies ranking and
classification tasks associated with MLR. Additionally, we address the
challenges of scarcity and annotation bias in MLR datasets by introducing eight
synthetic datasets (Ranked MNISTs) generated with varying
significance-determining factors, providing an enriched and controllable
experimental environment. We statistically demonstrate that our method
accurately learns a representation of the positive rank order, which is
consistent with the ground truth and proportional to the underlying
significance values. Finally, we conduct comprehensive empirical experiments on
both real-world and synthetic datasets, demonstrating the value of our proposed
framework.

</details>


### [113] [Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling](https://arxiv.org/abs/2508.21785)
*Peng Yang,Zhengdong Huang,Zicheng Xie,Wentao Tian,Jingyu Liu,Lunhong Dong*

Main category: cs.LG

TL;DR: 提出了一种处理心率预测中数据异质性的框架，通过随机特征丢弃和时间感知注意力模块，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 心率预测在现实应用中面临数据异质性（设备源和用户差异），现有方法无法同时解决这两类问题。

Method: 采用随机特征丢弃处理设备源异质性，时间感知注意力模块和对比学习目标处理用户异质性。

Result: 在ParroTao和FitRec数据集上分别提升17%和15%，学习到的表征具有强判别力。

Conclusion: 提出的框架有效解决了数据异质性，具有实际应用价值。

Abstract: Heart rate prediction is vital for personalized health monitoring and
fitness, while it frequently faces a critical challenge when deploying in
real-world: data heterogeneity. We classify it in two key dimensions: source
heterogeneity from fragmented device markets with varying feature sets, and
user heterogeneity reflecting distinct physiological patterns across
individuals and activities. Existing methods either discard device-specific
information, or fail to model user-specific differences, limiting their
real-world performance. To address this, we propose a framework that learns
latent representations agnostic to both heterogeneity, enabling downstream
predictors to work consistently under heterogeneous data patterns.
Specifically, we introduce a random feature dropout strategy to handle source
heterogeneity, making the model robust to various feature sets. To manage user
heterogeneity, we employ a time-aware attention module to capture long-term
physiological traits and use a contrastive learning objective to build a
discriminative representation space. To reflect the heterogeneous nature of
real-world data, we created and publicly released a new benchmark dataset,
ParroTao. Evaluations on both ParroTao and the public FitRec dataset show that
our model significantly outperforms existing baselines by 17% and 15%,
respectively. Furthermore, analysis of the learned representations demonstrates
their strong discriminative power, and one downstream application task confirm
the practical value of our model.

</details>


### [114] [MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction](https://arxiv.org/abs/2508.21793)
*Xiaoyang Wang,Christopher C. Yang*

Main category: cs.LG

TL;DR: MoE-Health是一种新型的混合专家框架，用于医疗预测中的多模态融合，能够灵活处理不同模态数据并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实医疗数据常存在模态不完整或多样化的挑战，现有方法难以适应这种复杂性。

Method: 通过专家网络和动态门控机制，根据可用模态动态选择和组合专家。

Result: 在MIMIC-IV数据集上，MoE-Health在多项临床预测任务中表现优于现有方法。

Conclusion: MoE-Health能有效整合多模态信息，适用于数据异构的医疗环境。

Abstract: Healthcare systems generate diverse multimodal data, including Electronic
Health Records (EHR), clinical notes, and medical images. Effectively
leveraging this data for clinical prediction is challenging, particularly as
real-world samples often present with varied or incomplete modalities. Existing
approaches typically require complete modality data or rely on manual selection
strategies, limiting their applicability in real-world clinical settings where
data availability varies across patients and institutions. To address these
limitations, we propose MoE-Health, a novel Mixture of Experts framework
designed for robust multimodal fusion in healthcare prediction. MoE-Health
architecture is specifically developed to handle samples with differing
modalities and improve performance on critical clinical tasks. By leveraging
specialized expert networks and a dynamic gating mechanism, our approach
dynamically selects and combines relevant experts based on available data
modalities, enabling flexible adaptation to varying data availability
scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical
clinical prediction tasks: in-hospital mortality prediction, long length of
stay, and hospital readmission prediction. Experimental results demonstrate
that MoE-Health achieves superior performance compared to existing multimodal
fusion methods while maintaining robustness across different modality
availability patterns. The framework effectively integrates multimodal
information, offering improved predictive performance and robustness in
handling heterogeneous and incomplete healthcare data, making it particularly
suitable for deployment in diverse healthcare environments with heterogeneous
data availability.

</details>


### [115] [QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.21810)
*Jessica Liang,Anirudh Bharadwaj*

Main category: cs.LG

TL;DR: QR-LoRA是一种参数高效微调方法，通过QR分解提取正交基，仅训练标量系数，显著减少参数数量，性能优于全微调和标准LoRA。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）的参数量庞大，需要高效的微调方法。标准LoRA及其变体存在计算成本高和解释性差的问题。

Method: 使用QR分解提取预训练权重矩阵的正交基，将LoRA更新表示为这些基向量的线性组合，仅训练标量系数。

Result: 在GLUE任务上，QR-LoRA性能匹配或优于全微调、标准LoRA和SVD-LoRA，参数减少1000倍以上。

Conclusion: QR-LoRA提供了一种高效且结构清晰的微调方法，显著降低参数数量并保持性能。

Abstract: The growing scale of Large Language Models (LLMs) has necessitated the
development of parameter-efficient fine-tuning techniques. Low-Rank Adaptation
(LoRA) has emerged as a promising approach, reducing the number of trainable
parameters by applying low-rank updates to pretrained weights. While standard
LoRA learns both update factors directly, several recent variants first
initialize those matrices via an SVD of the pretrained weights -- an operation
that can be expensive on large models and yields singular vectors that are not
always easy to interpret. In this work, we extract an orthonormal basis from
the pretrained weight matrix using QR decomposition with column pivoting, and
then express the LoRA update as a linear combination of these basis vectors --
training only the scalar coefficients, which imposes clear structure on
adaptation and drastically reduces parameter count. Experiments across GLUE
tasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,
standard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular
value decomposition) with as few as 601 parameters -- a reduction of over 1000x
compared to full fine-tuning and 77x fewer than typical LoRA setups.

</details>


### [116] [Achieving Hilbert-Schmidt Independence Under Rényi Differential Privacy for Fair and Private Data Generation](https://arxiv.org/abs/2508.21815)
*Tobias Hyrup,Emmanouil Panagiotou,Arjun Roy,Arthur Zimek,Eirini Ntoutsi,Peter Schneider-Kamp*

Main category: cs.LG

TL;DR: FLIP是一种基于Transformer的变分自编码器，结合潜在扩散技术生成异构表格数据，旨在解决隐私和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR、HIPAA等隐私法规和AI责任框架的普及，真实数据的使用面临更多限制，合成数据生成成为解决方案。FLIP旨在在隐私和公平性约束下生成任务无关的公平数据。

Method: FLIP采用Rényi差分隐私（RDP）约束训练，通过平衡采样和潜在空间中的神经元激活模式对齐（使用CKA）来确保隐私和公平性。

Result: 实验表明，FLIP在差分隐私约束下显著提高了任务无关公平性，并在多种下游任务中表现良好。

Conclusion: FLIP为隐私和公平性约束下的数据生成提供了有效的解决方案，具有广泛适用性。

Abstract: As privacy regulations such as the GDPR and HIPAA and responsibility
frameworks for artificial intelligence such as the AI Act gain traction, the
ethical and responsible use of real-world data faces increasing constraints.
Synthetic data generation has emerged as a promising solution to risk-aware
data sharing and model development, particularly for tabular datasets that are
foundational to sensitive domains such as healthcare. To address both privacy
and fairness concerns in this setting, we propose FLIP (Fair Latent
Intervention under Privacy guarantees), a transformer-based variational
autoencoder augmented with latent diffusion to generate heterogeneous tabular
data. Unlike the typical setup in fairness-aware data generation, we assume a
task-agnostic setup, not reliant on a fixed, defined downstream task, thus
offering broader applicability. To ensure privacy, FLIP employs R\'enyi
differential privacy (RDP) constraints during training and addresses fairness
in the input space with RDP-compatible balanced sampling that accounts for
group-specific noise levels across multiple sampling rates. In the latent
space, we promote fairness by aligning neuron activation patterns across
protected groups using Centered Kernel Alignment (CKA), a similarity measure
extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment
encourages statistical independence between latent representations and the
protected feature. Empirical results demonstrate that FLIP effectively provides
significant fairness improvements for task-agnostic fairness and across diverse
downstream tasks under differential privacy constraints.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [117] [EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control](https://arxiv.org/abs/2508.21112)
*Delin Qu,Haoming Song,Qizhi Chen,Zhaoqing Chen,Xianqiang Gao,Xinyi Ye,Qi Lv,Modi Shi,Guanghui Ren,Cheng Ruan,Maoqing Yao,Haoran Yang,Jiacheng Bao,Bin Zhao,Dong Wang*

Main category: cs.RO

TL;DR: EO-Robotics提出了一种统一的多模态基础模型EO-1和数据集EO-Data1.5M，通过交错视觉-文本-动作预训练，提升了机器人控制和多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型在交错推理和交互方面仍无法达到人类水平，需要更灵活的解决方案。

Method: EO-1采用统一架构处理多模态输入，并利用包含150万样本的EO-Data1.5M数据集，结合自回归解码和流匹配去噪训练。

Result: 实验表明，EO-1在开放世界理解和泛化方面表现优异，适用于多种长时程和灵巧操作任务。

Conclusion: EO-Robiotics为开发高级具身基础模型提供了有价值的架构、数据和方法论参考。

Abstract: The human ability to seamlessly perform multimodal reasoning and physical
interaction in the open world is a core goal for general-purpose embodied
intelligent systems. Recent vision-language-action (VLA) models, which are
co-trained on large-scale robot and visual-text data, have demonstrated notable
progress in general robot control. However, they still fail to achieve
human-level flexibility in interleaved reasoning and interaction. In this work,
introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is
a unified embodied foundation model that achieves superior performance in
multimodal embodied reasoning and robot control through interleaved
vision-text-action pre-training. The development of EO-1 is based on two key
pillars: (i) a unified architecture that processes multimodal inputs
indiscriminately (image, text, video, and action), and (ii) a massive,
high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains
over 1.5 million samples with emphasis on interleaved vision-text-action
comprehension. EO-1 is trained through synergies between auto-regressive
decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot
action generation and multimodal embodied reasoning. Extensive experiments
demonstrate the effectiveness of interleaved vision-text-action learning for
open-world understanding and generalization, validated through a variety of
long-horizon, dexterous manipulation tasks across multiple embodiments. This
paper details the architecture of EO-1, the data construction strategy of
EO-Data1.5M, and the training methodology, offering valuable insights for
developing advanced embodied foundation models.

</details>


### [118] [Observer Design for Optical Flow-Based Visual-Inertial Odometry with Almost-Global Convergence](https://arxiv.org/abs/2508.21163)
*Tarek Bouazza,Soulaimane Berkane,Minh-Duc Hua,Tarek Hamel*

Main category: cs.RO

TL;DR: 提出了一种结合光流和IMU测量的级联观测器架构，用于连续单目视觉-惯性里程计（VIO），通过融合光流速度方向与IMU数据，实现速度和重力方向的估计。


<details>
  <summary>Details</summary>
Motivation: 解决单目VIO中速度和重力方向估计的挑战，通过融合光流和IMU数据提升估计精度和稳定性。

Method: 使用全局指数稳定的Riccati观测器融合光流速度方向与IMU数据，设计互补观测器进行姿态估计，并通过梯度下降算法从稀疏光流数据中提取速度方向。

Result: 仿真结果表明，所提算法在速度和重力方向估计上有效，级联观测器架构具有几乎全局渐近稳定性。

Conclusion: 提出的级联观测器架构在单目VIO中表现优异，为速度和姿态估计提供了稳定且高效的解决方案。

Abstract: This paper presents a novel cascaded observer architecture that combines
optical flow and IMU measurements to perform continuous monocular
visual-inertial odometry (VIO). The proposed solution estimates body-frame
velocity and gravity direction simultaneously by fusing velocity direction
information from optical flow measurements with gyro and accelerometer data.
This fusion is achieved using a globally exponentially stable Riccati observer,
which operates under persistently exciting translational motion conditions. The
estimated gravity direction in the body frame is then employed, along with an
optional magnetometer measurement, to design a complementary observer on
$\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer
architecture is shown to be almost globally asymptotically stable. To extract
the velocity direction from sparse optical flow data, a gradient descent
algorithm is developed to solve a constrained minimization problem on the unit
sphere. The effectiveness of the proposed algorithms is validated through
simulation results.

</details>


### [119] [Multi-robot Path Planning and Scheduling via Model Predictive Optimal Transport (MPC-OT)](https://arxiv.org/abs/2508.21205)
*Usman A. Khan,Mouhacine Benosman,Wenliang Liu,Federico Pecora,Joseph W. Durham*

Main category: cs.RO

TL;DR: 提出了一种基于最优传输理论和模型预测控制的多机器人路径规划与调度新方法。


<details>
  <summary>Details</summary>
Motivation: 多机器人在共享空间中导航时，传统方法可能导致路径重叠和死锁。

Method: 将空间离散化为单元格，利用最优传输理论规划最优且不重叠的路径，并结合模型预测控制处理动态问题。

Result: 方法在最坏情况下需要O(K³logK)计算量，良好情况下为O(K²logK)，并能处理不可避免的路径重叠。

Conclusion: 该方法有效解决了多机器人导航中的路径规划和调度问题，兼具计算效率和动态适应性。

Abstract: In this paper, we propose a novel methodology for path planning and
scheduling for multi-robot navigation that is based on optimal transport theory
and model predictive control. We consider a setup where $N$ robots are tasked
to navigate to $M$ targets in a common space with obstacles. Mapping robots to
targets first and then planning paths can result in overlapping paths that lead
to deadlocks. We derive a strategy based on optimal transport that not only
provides minimum cost paths from robots to targets but also guarantees
non-overlapping trajectories. We achieve this by discretizing the space of
interest into $K$ cells and by imposing a ${K\times K}$ cost structure that
describes the cost of transitioning from one cell to another. Optimal transport
then provides \textit{optimal and non-overlapping} cell transitions for the
robots to reach the targets that can be readily deployed without any scheduling
considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$
computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for
well-behaved problems. To further accommodate potentially overlapping
trajectories (unavoidable in certain situations) as well as robot dynamics, we
show that a temporal structure can be integrated into optimal transport with
the help of \textit{replans} and \textit{model predictive control}.

</details>


### [120] [Uncertainty-Aware Ankle Exoskeleton Control](https://arxiv.org/abs/2508.21221)
*Fatima Mumtaza Tourk,Bishoy Galoaa,Sanat Shajan,Aaron J. Young,Michael Everett,Max K. Shepherd*

Main category: cs.RO

TL;DR: 提出了一种不确定性感知控制框架，使踝关节外骨骼能够在不同场景中安全运行，自动在遇到不熟悉动作时断开辅助。


<details>
  <summary>Details</summary>
Motivation: 现有外骨骼控制器设计用于受控环境中的离散动作，限制了其在实际场景中的应用。

Method: 使用不确定性估计器对动作进行分类（训练集内或外），并评估了三种架构（模型集成、自编码器、生成对抗网络）。

Result: 在线测试显示不确定性估计器能够在训练集内外任务间切换辅助（F1: 89.2）。

Conclusion: 该框架为外骨骼在非结构化日常环境中安全自主支持人类运动提供了新途径。

Abstract: Lower limb exoskeletons show promise to assist human movement, but their
utility is limited by controllers designed for discrete, predefined actions in
controlled environments, restricting their real-world applicability. We present
an uncertainty-aware control framework that enables ankle exoskeletons to
operate safely across diverse scenarios by automatically disengaging when
encountering unfamiliar movements. Our approach uses an uncertainty estimator
to classify movements as similar (in-distribution) or different
(out-of-distribution) relative to actions in the training set. We evaluated
three architectures (model ensembles, autoencoders, and generative adversarial
networks) on an offline dataset and tested the strongest performing
architecture (ensemble of gait phase estimators) online. The online test
demonstrated the ability of our uncertainty estimator to turn assistance on and
off as the user transitioned between in-distribution and out-of-distribution
tasks (F1: 89.2). This new framework provides a path for exoskeletons to safely
and autonomously support human movement in unstructured, everyday environments.

</details>


### [121] [Remarks on stochastic cloning and delayed-state filtering](https://arxiv.org/abs/2508.21260)
*Tara Mina,Lindsey Marinello,John Christian*

Main category: cs.RO

TL;DR: 本文重新审视了延迟状态卡尔曼滤波器，证明其与随机克隆方法在状态和协方差更新上等效，但计算和内存效率更高。


<details>
  <summary>Details</summary>
Motivation: 解决机器人导航中延迟状态测量的相关性处理问题，澄清卡尔曼滤波器无法处理此类测量的误解。

Method: 采用广义卡尔曼滤波器公式，避免状态扩充，直接处理延迟状态测量。

Result: 延迟状态卡尔曼滤波器与随机克隆方法效果相同，但计算和内存效率更高。

Conclusion: 延迟状态卡尔曼滤波器是处理延迟状态测量的高效替代方案，无需状态扩充。

Abstract: Many estimation problems in robotics and navigation involve measurements that
depend on prior states. A prominent example is odometry, which measures the
relative change between states over time. Accurately handling these
delayed-state measurements requires capturing their correlations with prior
state estimates, and a widely used approach is stochastic cloning (SC), which
augments the state vector to account for these correlations.
  This work revisits a long-established but often overlooked alternative--the
delayed-state Kalman filter--and demonstrates that a properly derived filter
yields exactly the same state and covariance update as SC, without requiring
state augmentation. Moreover, the generalized Kalman filter formulation
provides computational advantages, while also reducing memory requirements for
higher-dimensional states.
  Our findings clarify a common misconception that Kalman filter variants are
inherently unable to handle correlated delayed-state measurements,
demonstrating that an alternative formulation achieves the same results more
efficiently.

</details>


### [122] [Mini Autonomous Car Driving based on 3D Convolutional Neural Networks](https://arxiv.org/abs/2508.21271)
*Pablo Moraes,Monica Rodriguez,Kristofer S. Kappel,Hiago Sodre,Santiago Fernandez,Igor Nunes,Bruna Guterres,Ricardo Grando*

Main category: cs.RO

TL;DR: 论文提出了一种基于RGB-D信息和3D CNN的微型自动驾驶汽车控制方法，在模拟环境中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统开发面临高复杂性、长训练周期和不确定性等挑战，微型自动驾驶汽车（MACs）作为低成本测试平台，便于快速评估机器学习模型。

Method: 使用RGB-D信息和3D CNN进行MAC的自动驾驶控制，并在模拟环境中与RNN进行对比。

Result: 3D CNN在任务完成率、圈速和驾驶一致性方面表现优于RNN，且模型架构和赛道复杂性影响泛化能力。

Conclusion: 3D CNN在微型自动驾驶汽车控制中展现出潜力，为复杂环境下的算法验证提供了有效方案。

Abstract: Autonomous driving applications have become increasingly relevant in the
automotive industry due to their potential to enhance vehicle safety,
efficiency, and user experience, thereby meeting the growing demand for
sophisticated driving assistance features. However, the development of reliable
and trustworthy autonomous systems poses challenges such as high complexity,
prolonged training periods, and intrinsic levels of uncertainty. Mini
Autonomous Cars (MACs) are used as a practical testbed, enabling validation of
autonomous control methodologies on small-scale setups. This simplified and
cost-effective environment facilitates rapid evaluation and comparison of
machine learning models, which is particularly useful for algorithms requiring
online training. To address these challenges, this work presents a methodology
based on RGB-D information and three-dimensional convolutional neural networks
(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the
proposed approach against recurrent neural networks (RNNs), with architectures
trained and tested on two simulated tracks with distinct environmental
features. Performance was assessed using task completion success, lap-time
metrics, and driving consistency. Results highlight how architectural
modifications and track complexity influence the models' generalization
capability and vehicle control performance. The proposed 3D CNN demonstrated
promising results when compared with RNNs.

</details>


### [123] [Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609](https://arxiv.org/abs/2508.21272)
*Jaehong Oh,Seungjun Jung,Sawoong Kim*

Main category: cs.RO

TL;DR: 论文首次将法律动作掩码深度Q网络与安全ZYZ重抓策略应用于欠驱动夹爪6自由度协作机器人，实现自主Soma立方体组装学习。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中的组合动作空间爆炸、不安全运动规划和系统组装策略学习等关键挑战。

Method: 结合法律动作掩码DQN与分层架构，将Q函数估计分解为方向和位置分量，降低计算复杂度。

Result: 通过渐进式课程学习，在三个难度级别上分别达到100%、92.9%和39.9%的成功率。

Conclusion: 该方法在欠驱动协作机器人上实现了高效的自主组装学习。

Abstract: This paper presents the first comprehensive application of legal-action
masked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated
gripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly
learning. Our approach represents the first systematic integration of
constraint-aware reinforcement learning with singularity-safe motion planning
on a Doosan M0609 collaborative robot. We address critical challenges in
robotic manipulation: combinatorial action space explosion, unsafe motion
planning, and systematic assembly strategy learning. Our system integrates a
legal-action masked DQN with hierarchical architecture that decomposes
Q-function estimation into orientation and position components, reducing
computational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining
solution completeness. The robot-friendly reward function encourages
ground-first, vertically accessible assembly sequences aligned with
manipulation constraints. Curriculum learning across three progressive
difficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training
efficiency: 100\% success rate for Level 1 within 500 episodes, 92.9\% for
Level 2, and 39.9\% for Level 3 over 105,300 total training episodes.

</details>


### [124] [Observability-driven Assignment of Heterogeneous Sensors for Multi-Target Tracking](https://arxiv.org/abs/2508.21309)
*Seyed Ali Rakhshan,Mehdi Golestani,He Kong*

Main category: cs.RO

TL;DR: 提出一种基于拟阵理论的贪婪算法，用于异构传感器（机器人）的多目标跟踪任务分配，优化跟踪质量。


<details>
  <summary>Details</summary>
Motivation: 解决异构传感器（机器人）在跟踪多目标时的分配问题，以最小化目标状态估计的不确定性。

Method: 将机器人分为两类（充分感知和有限感知），利用拟阵理论设计贪婪分配算法，动态分配机器人到目标。

Result: 算法在多项式时间内实现，对任意跟踪质量函数有1/3近似保证，对子模函数有1/2近似保证。仿真验证了其有效性和鲁棒性。

Conclusion: 算法在性能和实用性上接近最优分配，适用于实际应用。

Abstract: This paper addresses the challenge of assigning heterogeneous sensors (i.e.,
robots with varying sensing capabilities) for multi-target tracking. We
classify robots into two categories: (1) sufficient sensing robots, equipped
with range and bearing sensors, capable of independently tracking targets, and
(2) limited sensing robots, which are equipped with only range or bearing
sensors and need to at least form a pair to collaboratively track a target. Our
objective is to optimize tracking quality by minimizing uncertainty in target
state estimation through efficient robot-to-target assignment. By leveraging
matroid theory, we propose a greedy assignment algorithm that dynamically
allocates robots to targets to maximize tracking quality. The algorithm
guarantees constant-factor approximation bounds of 1/3 for arbitrary tracking
quality functions and 1/2 for submodular functions, while maintaining
polynomial-time complexity. Extensive simulations demonstrate the algorithm's
effectiveness in accurately estimating and tracking targets over extended
periods. Furthermore, numerical results confirm that the algorithm's
performance is close to that of the optimal assignment, highlighting its
robustness and practical applicability.

</details>


### [125] [Robust Real-Time Coordination of CAVs: A Distributed Optimization Framework under Uncertainty](https://arxiv.org/abs/2508.21322)
*Haojie Bai,Yang Wang,Cong Guo,Xiongwei Zhao,Hai Zhu*

Main category: cs.RO

TL;DR: 论文提出了一种新型车辆协调框架，通过控制轨迹分布、并行优化算法和交互注意力机制，显著提升了安全性和实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态不确定环境中车辆协调的安全性和实时性能挑战。

Method: 1) 轨迹分布控制；2) ADMM-DTN算法；3) 交互注意力机制。

Result: 碰撞率降低40.79%，计算需求减少14.1%，在复杂环境中表现鲁棒。

Conclusion: 框架在安全性和实时性能上优于现有方法，具有强扩展性。

Abstract: Achieving both safety guarantees and real-time performance in cooperative
vehicle coordination remains a fundamental challenge, particularly in dynamic
and uncertain environments. This paper presents a novel coordination framework
that resolves this challenge through three key innovations: 1) direct control
of vehicles' trajectory distributions during coordination, formulated as a
robust cooperative planning problem with adaptive enhanced safety constraints,
ensuring a specified level of safety regarding the uncertainty of the
interactive trajectory, 2) a fully parallel ADMM-based distributed trajectory
negotiation (ADMM-DTN) algorithm that efficiently solves the optimization
problem while allowing configurable negotiation rounds to balance solution
quality and computational resources, and 3) an interactive attention mechanism
that selectively focuses on critical interactive participants to further
enhance computational efficiency. Both simulation results and practical
experiments demonstrate that our framework achieves significant advantages in
safety (reducing collision rates by up to 40.79\% in various scenarios) and
real-time performance compared to state-of-the-art methods, while maintaining
strong scalability with increasing vehicle numbers. The proposed interactive
attention mechanism further reduces the computational demand by 14.1\%. The
framework's effectiveness is further validated through real-world experiments
with unexpected dynamic obstacles, demonstrating robust coordination in complex
environments. The experiment demo could be found at
https://youtu.be/4PZwBnCsb6Q.

</details>


### [126] [Multi-Modal Model Predictive Path Integral Control for Collision Avoidance](https://arxiv.org/abs/2508.21364)
*Alberto Bertipaglia,Dariu M. Gavrila,Barys Shyrokau*

Main category: cs.RO

TL;DR: 提出了一种基于多模态模型预测路径积分控制算法的自动驾驶车辆运动规划与决策方法，通过Sobol序列采样和分析解避免碰撞，优于标准方法。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶车辆在复杂环境中（如高/低摩擦路面、移动障碍物）的运动规划和决策问题，避免次优解。

Method: 使用多模态控制算法，结合Sobol序列采样和非线性单轨车辆模型，强制轮胎力约束以确保稳定性。

Result: 在高保真仿真中，算法成功避障并保持车辆稳定，优于标准模型预测路径积分方法。

Conclusion: 多模态控制算法在复杂场景中表现优异，为自动驾驶车辆提供了更可靠的决策方案。

Abstract: This paper proposes a novel approach to motion planning and decision-making
for automated vehicles, using a multi-modal Model Predictive Path Integral
control algorithm. The method samples with Sobol sequences around the prior
input and incorporates analytical solutions for collision avoidance. By
leveraging multiple modes, the multi-modal control algorithm explores diverse
trajectories, such as manoeuvring around obstacles or stopping safely before
them, mitigating the risk of sub-optimal solutions. A non-linear single-track
vehicle model with a Fiala tyre serves as the prediction model, and tyre force
constraints within the friction circle are enforced to ensure vehicle stability
during evasive manoeuvres. The optimised steering angle and longitudinal
acceleration are computed to generate a collision-free trajectory and to
control the vehicle. In a high-fidelity simulation environment, we demonstrate
that the proposed algorithm can successfully avoid obstacles, keeping the
vehicle stable while driving a double lane change manoeuvre on high and
low-friction road surfaces and occlusion scenarios with moving obstacles,
outperforming a standard Model Predictive Path Integral approach.

</details>


### [127] [Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation](https://arxiv.org/abs/2508.21375)
*Anuj Pasricha,Joewie Koh,Jay Vakil,Alessandro Roncone*

Main category: cs.RO

TL;DR: 论文提出了一种基于去噪扩散模型的新轨迹生成方法，能够显著提高机器人工作空间的有效载荷能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法基于最坏情况配置的额定载荷限制了机器人的实际能力，导致工作空间利用率不足。

Method: 使用去噪扩散模型，将载荷约束直接纳入规划过程，生成动态可行的关节空间轨迹。

Result: 实验证明，7自由度机器人工作空间的67.6%仍可承载超过额定载荷3倍的重量。

Conclusion: 研究强调了在运动规划算法中更细致考虑载荷动态的重要性。

Abstract: Nominal payload ratings for articulated robots are typically derived from
worst-case configurations, resulting in uniform payload constraints across the
entire workspace. This conservative approach severely underutilizes the robot's
inherent capabilities -- our analysis demonstrates that manipulators can safely
handle payloads well above nominal capacity across broad regions of their
workspace while staying within joint angle, velocity, acceleration, and torque
limits. To address this gap between assumed and actual capability, we propose a
novel trajectory generation approach using denoising diffusion models that
explicitly incorporates payload constraints into the planning process. Unlike
traditional sampling-based methods that rely on inefficient trial-and-error,
optimization-based methods that are prohibitively slow, or kinodynamic planners
that struggle with problem dimensionality, our approach generates dynamically
feasible joint-space trajectories in constant time that can be directly
executed on physical hardware without post-processing. Experimental validation
on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the
workspace remains accessible even with payloads exceeding 3 times the nominal
capacity. This expanded operational envelope highlights the importance of a
more nuanced consideration of payload dynamics in motion planning algorithms.

</details>


### [128] [RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation](https://arxiv.org/abs/2508.21378)
*Chenduo Ying,Linkang Du,Peng Cheng,Yuanchao Shu*

Main category: cs.RO

TL;DR: RoboInspector 是一个用于分析和改进 LLM 生成的机器人策略代码可靠性的管道，通过任务复杂性和指令粒度两个视角揭示不可靠行为，并提出反馈驱动的优化方法。


<details>
  <summary>Details</summary>
Motivation: 尽管 LLMs 在机器人控制中表现出色，但由于任务多样性和用户指令复杂性，策略代码生成的可靠性仍面临挑战。

Method: 设计 RoboInspector 管道，分析任务复杂性和指令粒度对代码可靠性的影响，并通过实验识别四种不可靠行为。

Result: 实验发现四种主要不可靠行为，并提出反馈驱动的优化方法，将策略代码可靠性提升高达 35%。

Conclusion: RoboInspector 为 LLM 驱动的机器人操作提供了可靠性分析和改进的实用工具。

Abstract: Large language models (LLMs) demonstrate remarkable capabilities in reasoning
and code generation, enabling robotic manipulation to be initiated with just a
single instruction. The LLM carries out various tasks by generating policy code
required to control the robot. Despite advances in LLMs, achieving reliable
policy code generation remains a significant challenge due to the diverse
requirements of real-world tasks and the inherent complexity of user
instructions. In practice, different users may provide distinct instructions to
drive the robot for the same task, which may cause the unreliability of policy
code generation. To bridge this gap, we design RoboInspector, a pipeline to
unveil and characterize the unreliability of the policy code for LLM-enabled
robotic manipulation from two perspectives: the complexity of the manipulation
task and the granularity of the instruction. We perform comprehensive
experiments with 168 distinct combinations of tasks, instructions, and LLMs in
two prominent frameworks. The RoboInspector identifies four main unreliable
behaviors that lead to manipulation failure. We provide a detailed
characterization of these behaviors and their underlying causes, giving insight
for practical development to reduce unreliability. Furthermore, we introduce a
refinement approach guided by failure policy code feedback that improves the
reliability of policy code generation by up to 35% in LLM-enabled robotic
manipulation, evaluated in both simulation and real-world environments.

</details>


### [129] [Assessing Human Cooperation for Enhancing Social Robot Navigation](https://arxiv.org/abs/2508.21455)
*Hariharan Arunachalam,Phani Teja Singamaneni,Rachid Alami*

Main category: cs.RO

TL;DR: 论文提出了一种基于几何分析和人类合作性的机器人导航策略，通过有效沟通解决人机交互中的意图理解问题。


<details>
  <summary>Details</summary>
Motivation: 机器人导航在人类环境中常因无法理解人类意图或人类不了解机器人计划而失败，需要改进。

Method: 通过几何分析上下文和人类合作性，在适当时间进行有效沟通，并提出评估方法。

Result: 提出了区分合作与非合作人类的评估指标，并展示了如何生成适当的语言或动作响应。

Conclusion: 几何推理和有效沟通能显著提升机器人在人机交互中的表现。

Abstract: Socially aware robot navigation is a planning paradigm where the robot
navigates in human environments and tries to adhere to social constraints while
interacting with the humans in the scene. These navigation strategies were
further improved using human prediction models, where the robot takes the
potential future trajectory of humans while computing its own. Though these
strategies significantly improve the robot's behavior, it faces difficulties
from time to time when the human behaves in an unexpected manner. This happens
as the robot fails to understand human intentions and cooperativeness, and the
human does not have a clear idea of what the robot is planning to do. In this
paper, we aim to address this gap through effective communication at an
appropriate time based on a geometric analysis of the context and human
cooperativeness in head-on crossing scenarios. We provide an assessment
methodology and propose some evaluation metrics that could distinguish a
cooperative human from a non-cooperative one. Further, we also show how
geometric reasoning can be used to generate appropriate verbal responses or
robot actions.

</details>


### [130] [Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting](https://arxiv.org/abs/2508.21501)
*Pierrick Lorang,Hong Lu,Johannes Huemer,Patrik Zips,Matthias Scheutz*

Main category: cs.RO

TL;DR: 提出了一种新的神经符号框架，通过结合符号域抽象和连续控制策略，从少量技能演示中高效学习长时程任务。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法通常局限于短时程技能，需要大量数据，且难以处理长时程任务或泛化到任务变体和分布偏移。

Method: 将高级任务结构抽象为图，通过答案集编程求解器发现符号规则，并使用扩散策略模仿学习训练低级控制器。高层筛选器聚焦于最小观察和动作空间。

Result: 在六个领域（包括机器人操作和自动化叉车）中验证，仅需五个技能演示即可高效学习，表现出强大的零样本和少样本泛化能力，且决策可解释。

Conclusion: 该框架能够捕捉复杂状态转移（包括非空间和时间关系），在有限演示数据中优于纯数据驱动方法。

Abstract: Imitation learning enables intelligent systems to acquire complex behaviors
with minimal supervision. However, existing methods often focus on
short-horizon skills, require large datasets, and struggle to solve
long-horizon tasks or generalize across task variations and distribution
shifts. We propose a novel neuro-symbolic framework that jointly learns
continuous control policies and symbolic domain abstractions from a few skill
demonstrations. Our method abstracts high-level task structures into a graph,
discovers symbolic rules via an Answer Set Programming solver, and trains
low-level controllers using diffusion policy imitation learning. A high-level
oracle filters task-relevant information to focus each controller on a minimal
observation and action space. Our graph-based neuro-symbolic framework enables
capturing complex state transitions, including non-spatial and temporal
relations, that data-driven learning or clustering techniques often fail to
discover in limited demonstration datasets. We validate our approach in six
domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers
of Hanoi environments, and a distinct Automated Forklift domain with two
environments. The results demonstrate high data efficiency with as few as five
skill demonstrations, strong zero- and few-shot generalizations, and
interpretable decision making.

</details>


### [131] [Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler](https://arxiv.org/abs/2508.21549)
*Liding Zhang,Kuanqi Cai,Yu Zhang,Zhenshan Bing,Chaoqun Wang,Fan Wu,Sami Haddadin,Alois Knoll*

Main category: cs.RO

TL;DR: MIT*是一种新型路径规划器，通过预定义启发式集合和自适应采样策略，显著提高了初始收敛速度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于采样的规划器在未找到解决方案时需要重新采样整个配置空间，耗时且计算成本高。

Method: MIT*利用先验可接受解成本构建启发式集合，采用自适应采样器和稀疏碰撞检查。

Result: MIT*在R^4到R^16问题中优于现有单查询采样规划器，并成功应用于实际机器人操作任务。

Conclusion: MIT*通过优化初始收敛和计算效率，显著提升了路径规划性能。

Abstract: Path planning in robotics often involves solving continuously valued,
high-dimensional problems. Popular informed approaches include graph-based
searches, such as A*, and sampling-based methods, such as Informed RRT*, which
utilize informed set and anytime strategies to expedite path optimization
incrementally. Informed sampling-based planners define informed sets as subsets
of the problem domain based on the current best solution cost. However, when no
solution is found, these planners re-sample and explore the entire
configuration space, which is time-consuming and computationally expensive.
This article introduces Multi-Informed Trees (MIT*), a novel planner that
constructs estimated informed sets based on prior admissible solution costs
before finding the initial solution, thereby accelerating the initial
convergence rate. Moreover, MIT* employs an adaptive sampler that dynamically
adjusts the sampling strategy based on the exploration process. Furthermore,
MIT* utilizes length-related adaptive sparse collision checks to guide lazy
reverse search. These features enhance path cost efficiency and computation
times while ensuring high success rates in confined scenarios. Through a series
of simulations and real-world experiments, it is confirmed that MIT*
outperforms existing single-query, sampling-based planners for problems in R^4
to R^16 and has been successfully applied to real-world robot manipulation
tasks. A video showcasing our experimental results is available at:
https://youtu.be/30RsBIdexTU

</details>


### [132] [Learning Agile Gate Traversal via Analytical Optimal Policy Gradient](https://arxiv.org/abs/2508.21592)
*Tianchen Sun,Bingheng Wang,Longbin Tang,Yichao Gao,Lin Zhao*

Main category: cs.RO

TL;DR: 提出了一种混合框架，通过离线训练的神经网络在线调整MPC参数，实现了高效、精确的无人机穿越狭窄门。


<details>
  <summary>Details</summary>
Motivation: 传统模块化飞行堆栈设计复杂，端到端强化学习方法样本效率低且可解释性差，需要一种更高效的方法。

Method: 结合离线训练的神经网络和在线MPC参数调整，通过预测参考姿态和成本函数权重优化飞行。

Result: 硬件实验表明，该方法在狭窄门穿越中表现优异，样本效率显著高于端到端强化学习。

Conclusion: 混合框架在无人机飞行任务中实现了高效、精确的控制，优于传统方法。

Abstract: Traversing narrow gates presents a significant challenge and has become a
standard benchmark for evaluating agile and precise quadrotor flight.
Traditional modularized autonomous flight stacks require extensive design and
parameter tuning, while end-to-end reinforcement learning (RL) methods often
suffer from low sample efficiency and limited interpretability. In this work,
we present a novel hybrid framework that adaptively fine-tunes model predictive
control (MPC) parameters online using outputs from a neural network (NN)
trained offline. The NN jointly predicts a reference pose and cost-function
weights, conditioned on the coordinates of the gate corners and the current
drone state. To achieve efficient training, we derive analytical policy
gradients not only for the MPC module but also for an optimization-based gate
traversal detection module. Furthermore, we introduce a new formulation of the
attitude tracking error that admits a simplified representation, facilitating
effective learning with bounded gradients. Hardware experiments demonstrate
that our method enables fast and accurate quadrotor traversal through narrow
gates in confined environments. It achieves several orders of magnitude
improvement in sample efficiency compared to naive end-to-end RL approaches.

</details>


### [133] [The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics](https://arxiv.org/abs/2508.21635)
*Nicolas Soncini,Javier Cremona,Erica Vidal,Maximiliano García,Gastón Castro,Taihú Pire*

Main category: cs.RO

TL;DR: 多模态数据集，用于农业机器人定位、建图、感知和导航算法的开发和基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决农业环境中机器人面临的复杂挑战，如自然光照变化、运动模糊、崎岖地形等。

Method: 通过多传感器（红外相机、彩色相机、IMU、GNSS等）同步采集数据，提供6自由度地面真值和长轨迹闭环。

Result: 展示了现有SLAM方法在农业环境中的局限性。

Conclusion: 数据集支持农业机器人算法的开发与评估，公开了数据和工具。

Abstract: We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.

</details>


### [134] [Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators](https://arxiv.org/abs/2508.21677)
*Bernhard Wullt,Johannes Köhler,Per Mattsson,Mikeal Norrlöf,Thomas B. Schön*

Main category: cs.RO

TL;DR: 提出了一种新型模型预测控制（MPC）方法，结合鲁棒管MPC和走廊规划算法，实现快速、安全的机械臂运动规划。


<details>
  <summary>Details</summary>
Motivation: 工业机械臂在复杂环境中运行时，模型不确定性增加了安全运动规划的难度，需要一种能保证安全且快速的控制方法。

Method: 采用鲁棒管MPC和走廊规划算法，形成凸优化问题，实现快速求解。

Result: 在模拟环境中，该方法在模型不确定性和运动速度上优于基准方法。

Conclusion: 该方法在复杂环境中表现出高效性和实用性，适用于工业机械臂的快速安全运动规划。

Abstract: Industrial manipulators are normally operated in cluttered environments,
making safe motion planning important. Furthermore, the presence of
model-uncertainties make safe motion planning more difficult. Therefore, in
practice the speed is limited in order to reduce the effect of disturbances.
There is a need for control methods that can guarantee safe motions that can be
executed fast. We address this need by suggesting a novel model predictive
control (MPC) solution for manipulators, where our two main components are a
robust tube MPC and a corridor planning algorithm to obtain collision-free
motion. Our solution results in a convex MPC, which we can solve fast, making
our method practically useful. We demonstrate the efficacy of our method in a
simulated environment with a 6 DOF industrial robot operating in cluttered
environments with uncertainties in model parameters. We outperform benchmark
methods, both in terms of being able to work under higher levels of model
uncertainties, while also yielding faster motion.

</details>


### [135] [Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?](https://arxiv.org/abs/2508.21690)
*Olger Siebinga,David Abbink*

Main category: cs.RO

TL;DR: 研究行人互动中的"人行道萨尔萨舞"现象，利用强化学习代理与行人模型互动，降低感知风险。


<details>
  <summary>Details</summary>
Motivation: 研究行人互动中的失败案例（如"人行道萨尔萨舞"）以理解隐性沟通机制，为机器人设计安全行为提供依据。

Method: 使用基于CEI框架的行人行为模型，通过强化学习代理学习与行人互动的方法。

Result: 基本RL代理成功与CEI模型互动；风险规避型RL代理通过动作传达意图，显著降低感知风险。

Conclusion: 该方法前景广阔，值得进一步探索。

Abstract: Pedestrians approaching each other on a sidewalk sometimes end up in an
awkward interaction known as the "sidewalk salsa": they both (repeatedly)
deviate to the same side to avoid a collision. This provides an interesting use
case to study interactions between pedestrians and mobile robots because, in
the vast majority of cases, this phenomenon is avoided through a negotiation
based on implicit communication. Understanding how it goes wrong and how
pedestrians end up in the sidewalk salsa will therefore provide insight into
the implicit communication. This understanding can be used to design safe and
acceptable robotic behaviour. In a previous attempt to gain this understanding,
a model of pedestrian behaviour based on the Communication-Enabled Interaction
(CEI) framework was developed that can replicate the sidewalk salsa. However,
it is unclear how to leverage this model in robotic planning and
decision-making since it violates the assumptions of game theory, a much-used
framework in planning and decision-making. Here, we present a proof-of-concept
for an approach where a Reinforcement Learning (RL) agent leverages the model
to learn how to interact with pedestrians. The results show that a basic RL
agent successfully learned to interact with the CEI model. Furthermore, a
risk-averse RL agent that had access to the perceived risk of the CEI model
learned how to effectively communicate its intention through its motion and
thereby substantially lowered the perceived risk, and displayed effort by the
modelled pedestrian. These results show this is a promising approach and
encourage further exploration.

</details>


### [136] [QuadKAN: KAN-Enhanced Quadruped Motion Control via End-to-End Reinforcement Learning](https://arxiv.org/abs/2508.19153)
*Allen Wang,Gavin Tao*

Main category: cs.RO

TL;DR: QuadKAN结合强化学习与视觉引导，通过Kolmogorov-Arnold网络和样条编码实现稳健的四足运动控制，提升效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 结合本体感觉与视觉以实现更稳健的运动控制。

Method: 提出QuadKAN框架，使用样条编码和融合头，结合MMDR和PPO进行端到端训练。

Result: 在多样地形中表现优于现有方法，实现更高回报、更远距离和更少碰撞。

Conclusion: 样条参数化策略为视觉引导运动提供简单、有效且可解释的解决方案。

Abstract: We address vision-guided quadruped motion control with reinforcement learning
(RL) and highlight the necessity of combining proprioception with vision for
robust control. We propose QuadKAN, a spline-parameterized cross-modal policy
instantiated with Kolmogorov-Arnold Networks (KANs). The framework incorporates
a spline encoder for proprioception and a spline fusion head for
proprioception-vision inputs. This structured function class aligns the
state-to-action mapping with the piecewise-smooth nature of gait, improving
sample efficiency, reducing action jitter and energy consumption, and providing
interpretable posture-action sensitivities. We adopt Multi-Modal Delay
Randomization (MMDR) and perform end-to-end training with Proximal Policy
Optimization (PPO). Evaluations across diverse terrains, including both even
and uneven surfaces and scenarios with static or dynamic obstacles, demonstrate
that QuadKAN achieves consistently higher returns, greater distances, and fewer
collisions than state-of-the-art (SOTA) baselines. These results show that
spline-parameterized policies offer a simple, effective, and interpretable
alternative for robust vision-guided locomotion. A repository will be made
available upon acceptance.

</details>
