<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.LG](#cs.LG) [Total: 71]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model](https://arxiv.org/abs/2509.02659)
*Zilong Guo,Yi Luo,Long Sha,Dongxu Wang,Panqu Wang,Chenyang Xu,Yi Yang*

Main category: cs.CV

TL;DR: 结合端到端架构设计和多模态视觉语言模型（VLM）在自动驾驶任务中表现出色，仅使用单摄像头即达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）和多模态视觉语言模型（VLM）是否能提升端到端自动驾驶任务的性能。

Method: 结合端到端架构设计和VLM，仅使用单摄像头进行驾驶任务。

Result: 在排行榜上表现最佳，验证了基于视觉的驾驶方法的有效性。

Conclusion: 端到端驾驶任务中，结合VLM的方法具有潜力，且仅需单摄像头即可实现高性能。

Abstract: End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.

</details>


### [2] [PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?](https://arxiv.org/abs/2509.02807)
*Mennatullah Siam*

Main category: cs.CV

TL;DR: 论文研究了多模态大语言模型（MLLMs）在视频中的像素级视觉定位能力，提出了一种新的运动中心基准测试（MoCentric-Bench），并探索了简单的运动适应技术。


<details>
  <summary>Details</summary>
Motivation: 当前视频MLLMs在像素级视觉定位任务中缺乏对运动信息的利用，且现有基准测试过于依赖静态外观线索。

Method: 提出了四种运动中心探测技术，并构建了MoCentric-Bench基准测试，同时探索了运动适应技术。

Result: 建立了强单图像基线，运动适应技术在MoCentric-Bench上达到最优性能。

Conclusion: 研究挑战未来模型提升密集时空定位和像素级理解能力，代码和数据集将公开。

Abstract: Multi-modal large language models (MLLMs) have shown impressive
generalization across tasks using images and text modalities. While their
extension to video has enabled tasks such as video question answering and video
captioning, their pixel-level visual grounding abilities are less studied. In
this work, we raise the pertinent question of whether motion is used in
pixel-level visual grounding and whether video MLLMs can segment objects based
on natural language expressions describing their motion patterns. We identify
the shortcomings in the current benchmarks, where we show that a single frame
can often suffice for capturing the motion referring expression without any
temporal reasoning. To address this, we introduce four motion-centric probing
techniques, particularly designed for the visual grounding task, to study video
MLLMs' ability to identify true motion from a fake one and their ability to
grasp the motion order. Consequently, we provide a motion-centric benchmark,
MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging
the interaction between motion and language rather than being dominated by
static appearance cues emphasized in existing visual grounding datasets. We
further establish strong single-image baselines that are on par with or
outperform prior methods. Finally, we explore simple motion-centric adaptation
techniques that provide state-of-the-art performance on our MoCentric-Bench.
Our motion-centric benchmark, evaluation and findings challenge future models
to improve dense spatiotemporal grounding and pixel-level understanding within
videos. Code and datasets will be made publicly available at
https://github.com/MSiam/PixFoundation-2.0.git.

</details>


### [3] [Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach](https://arxiv.org/abs/2509.02851)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cs.CV

TL;DR: 提出了一种混合多尺度深度学习架构HG-TNet，结合胶囊网络、图注意力机制、Transformer模块和残差学习，用于结肠癌分类，性能优于标准架构。


<details>
  <summary>Details</summary>
Motivation: 早期结肠癌检测对防止病情恶化至关重要，现有方法在捕捉多尺度特征方面存在不足。

Method: 采用HG-TNet模型，结合Transformer分支提取全局上下文和CNN分支捕捉局部细节，并通过自监督旋转预测目标增强特征表示。

Result: 在LC25000数据集上表现优于标准架构，胶囊网络保留了空间顺序，提升了分类性能。

Conclusion: 混合架构在多尺度特征提取和分类任务中表现出色，为结肠癌诊断提供了新思路。

Abstract: Colon cancer also known as Colorectal cancer, is one of the most malignant
types of cancer worldwide. Early-stage detection of colon cancer is highly
crucial to prevent its deterioration. This research presents a hybrid
multi-scale deep learning architecture that synergizes capsule networks, graph
attention mechanisms, transformer modules, and residual learning to advance
colon cancer classification on the Lung and Colon Cancer Histopathological
Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the
HG-TNet model that introduces a hybrid architecture that joins strength points
in transformers and convolutional neural networks to capture multi-scale
features in histopathological images. Mainly, a transformer branch extracts
global contextual bonds by partitioning the image into patches by
convolution-based patch embedding and then processing these patches through a
transformer encoder. Analogously, a dedicated CNN branch captures fine-grained,
local details through successive Incorporation these diverse features, combined
with a self-supervised rotation prediction objective, produce a robust
diagnostic representation that surpasses standard architectures in performance.
Results show better performance not only in accuracy or loss function but also
in these algorithms by utilizing capsule networks to preserve spatial orders
and realize how each element individually combines and forms whole structures.

</details>


### [4] [PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis](https://arxiv.org/abs/2509.02898)
*Armin Saadat,Nima Hashemi,Hooman Vaseli,Michael Y. Tsang,Christina Luong,Michiel Van de Panne,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的主动视频采集框架，用于优化主动脉狭窄诊断中的超声视频选择，提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 主动脉狭窄诊断中，传统超声检查资源有限，尤其在偏远地区，而点式超声检查受限于操作者技术和视图选择。

Method: 采用强化学习驱动的主动视频采集框架，动态选择最具信息量的超声视频，优化采集过程。

Result: 在2572名患者数据上测试，分类准确率达80.6%，仅需47%的超声视频。

Conclusion: 该方法提升了主动脉狭窄诊断的效率和可扩展性，为个性化诊断提供了可能。

Abstract: Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of
the aortic valve, leading to impaired blood flow. Despite its high prevalence,
access to echocardiography (echo), the gold-standard diagnostic tool, is often
limited due to resource constraints, particularly in rural and underserved
areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative
but is restricted by operator expertise and the challenge of selecting the most
relevant imaging views. To address this, we propose a reinforcement learning
(RL)-driven active video acquisition framework that dynamically selects each
patient's most informative echo videos. Unlike traditional methods that rely on
a fixed set of videos, our approach continuously evaluates whether additional
imaging is needed, optimizing both accuracy and efficiency. Tested on data from
2,572 patients, our method achieves 80.6% classification accuracy while using
only 47% of the echo videos compared to a full acquisition. These results
demonstrate the potential of active feature acquisition to enhance AS
diagnosis, making echocardiographic assessments more efficient, scalable, and
personalized. Our source code is available at:
https://github.com/Armin-Saadat/PRECISE-AS.

</details>


### [5] [LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research](https://arxiv.org/abs/2509.02902)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: LiGuard是一个开源软件框架，旨在简化基于激光雷达的研究项目开发，提供内置支持、灵活算法调整和结果可视化。


<details>
  <summary>Details</summary>
Motivation: 解决激光雷达研究中代码重复和修改困难的问题。

Method: 开发LiGuard框架，支持数据I/O、预处理/后处理和常用算法，并提供交互式调整和可视化功能。

Result: 通过案例研究证明LiGuard的有效性。

Conclusion: LiGuard能显著提高激光雷达研究的效率和代码复用性。

Abstract: There is a growing interest in the development of lidar-based autonomous
mobility and Intelligent Transportation Systems (ITS). To operate and research
on lidar data, researchers often develop code specific to application niche.
This approach leads to duplication of efforts across studies that, in many
cases, share multiple methodological steps such as data input/output (I/O),
pre/post processing, and common algorithms in multi-stage solutions. Moreover,
slight changes in data, algorithms, and/or research focus may force major
revisions in the code. To address these challenges, we present LiGuard, an
open-source software framework that allows researchers to: 1) rapidly develop
code for their lidar-based projects by providing built-in support for data I/O,
pre/post processing, and commonly used algorithms, 2) interactively
add/remove/reorder custom algorithms and adjust their parameters, and 3)
visualize results for classification, detection, segmentation, and tracking
tasks. Moreover, because it creates all the code files in structured
directories, it allows easy sharing of entire projects or even the individual
components to be reused by other researchers. The effectiveness of LiGuard is
demonstrated via case studies.

</details>


### [6] [PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2509.02903)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 论文提出了一种利用高保真数字孪生（HiFi DTs）创建大规模高质量合成数据集的方法，以解决LiDAR感知系统中数据标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: LiDAR感知系统依赖大规模标注数据集，但数据标注成本高且耗时，限制了系统的可扩展性。Sim2Real学习是一种替代方案，但其效果依赖于模拟的真实性。

Method: 提出了一种可重复的方法论，利用卫星图像和OpenStreetMap等开源资源，结合特定传感器配置，构建高保真数字孪生环境，生成合成数据集。

Result: 该方法能够生成多样化、成本效益高的合成数据集，为Sim2Real学习提供可靠基础。

Conclusion: 高保真数字孪生环境为LiDAR感知系统提供了一种可扩展且经济高效的解决方案。

Abstract: LiDAR-based perception in intelligent transportation systems (ITS), for tasks
such as object detection, tracking, and semantic and instance segmentation, is
predominantly solved by deep neural network models which often require
large-scale labeled datasets during training to achieve generalization.
However, creating these datasets is costly. time consuming and require human
labor before the datasets are ready for training models. This hinders
scalability of the LiDAR-based perception systems in ITS. Sim2Real learning
offers scalable alternative, however, its effectiveness is dependent on the
fidelity of the source simulation(s) to real-world, in terms of environment
structure, actor dynamics, and sensor emulations. In response, this paper
introduces a rigorous and reproducible methodology for creating large-scale,
high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).
The proposed workflow outlines the steps, tools, and best practices for
digitally replicating real-world environments, encompassing static geometry
modeling, road infrastructure replication, and dynamic traffic scenario
generation. Leveraging open-source and readily available resources such as
satellite imagery and OpenStreetMap data, alongside specific sensor
configurations, this paper provides practical, detailed guidance for
constructing robust synthetic environments. These environments subsequently
facilitate scalable, cost-effective, and diverse dataset generation, forming a
reliable foundation for robust Sim2Real learning.

</details>


### [7] [High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception](https://arxiv.org/abs/2509.02904)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 提出了一种高保真数字孪生框架（HiFi DT）以减少Sim2Real领域转移，提升LiDAR感知模型在真实数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 解决仿真训练模型在真实数据上表现不佳的问题，通过高保真仿真环境缩小领域差距。

Method: 构建包含真实背景几何、车道级拓扑和传感器规格的仿真环境，使用多种指标量化领域对齐。

Result: HiFi DT训练的模型在真实数据上表现优于真实数据训练的模型4.8%，显著减少领域偏移。

Conclusion: 数字孪生能有效提升仿真数据训练的LiDAR感知模型在真实ITS应用中的可靠性。

Abstract: Sim2Real domain transfer offers a cost-effective and scalable approach for
developing LiDAR-based perception (e.g., object detection, tracking,
segmentation) in Intelligent Transportation Systems (ITS). However, perception
models trained in simulation often under perform on real-world data due to
distributional shifts. To address this Sim2Real gap, this paper proposes a
high-fidelity digital twin (HiFi DT) framework that incorporates real-world
background geometry, lane-level road topology, and sensor-specific
specifications and placement. We formalize the domain adaptation challenge
underlying Sim2Real learning and present a systematic method for constructing
simulation environments that yield in-domain synthetic data. An off-the-shelf
3D object detector is trained on HiFi DT-generated synthetic data and evaluated
on real data. Our experiments show that the DT-trained model outperforms the
equivalent model trained on real data by 4.8%. To understand this gain, we
quantify distributional alignment between synthetic and real data using
multiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy
(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both
raw-input and latent-feature levels. Results demonstrate that HiFi DTs
substantially reduce domain shift and improve generalization across diverse
evaluation scenarios. These findings underscore the significant role of digital
twins in enabling reliable, simulation-based LiDAR perception for real-world
ITS applications.

</details>


### [8] [Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach](https://arxiv.org/abs/2509.02918)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.CV

TL;DR: KG-DG框架结合视觉Transformer与符号推理，提升糖尿病视网膜病变分类的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中模型在真实世界分布变化下泛化能力不足的问题。

Method: 通过临床病变本体和视网膜血管分割，结合深度视觉表示，采用置信加权集成策略。

Result: 在四个数据集上显著提升性能，最高准确率提升5.2%，符号模型在MDG中达63.67%准确率。

Conclusion: 神经符号集成是构建临床鲁棒、域不变医学AI系统的有效范式。

Abstract: Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.

</details>


### [9] [A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images](https://arxiv.org/abs/2509.02928)
*Zhicheng Tang,Jinwen Tang,Yi Shang*

Main category: cs.CV

TL;DR: DDR-Net是一种基于RetinaNet的数据驱动深度学习模型，专注于提升小物体检测能力，适用于环境监测、城市规划等领域。


<details>
  <summary>Details</summary>
Motivation: 小物体检测在航空影像中至关重要，但现有方法在精度和效率上存在不足，尤其是在数据有限的情况下。

Method: DDR-Net通过数据驱动技术自动优化特征图和锚点估计，并提出创新的采样方法以提升模型在有限数据下的性能。

Result: 实验表明，DDR-Net在多种航空影像数据集上显著优于RetinaNet及其他现有模型，同时降低了数据收集和训练成本。

Conclusion: DDR-Net推动了航空影像分析技术的发展，对农业、安全和考古等多个领域具有广泛的应用潜力。

Abstract: In the realm of aerial imaging, the ability to detect small objects is
pivotal for a myriad of applications, encompassing environmental surveillance,
urban design, and crisis management. Leveraging RetinaNet, this work unveils
DDR-Net: a data-driven, deep-learning model devised to enhance the detection of
diminutive objects. DDR-Net introduces novel, data-driven techniques to
autonomously ascertain optimal feature maps and anchor estimations, cultivating
a tailored and proficient training process while maintaining precision.
Additionally, this paper presents an innovative sampling technique to bolster
model efficacy under limited data training constraints. The model's enhanced
detection capabilities support critical applications including wildlife and
habitat monitoring, traffic flow optimization, and public safety improvements
through accurate identification of small objects like vehicles and pedestrians.
DDR-Net significantly reduces the cost and time required for data collection
and training, offering efficient performance even with limited data. Empirical
assessments over assorted aerial avian imagery datasets demonstrate that
DDR-Net markedly surpasses RetinaNet and alternative contemporary models. These
innovations advance current aerial image analysis technologies and promise
wide-ranging impacts across multiple sectors including agriculture, security,
and archaeology.

</details>


### [10] [STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images](https://arxiv.org/abs/2509.02952)
*Zeyu Liu,Shengwei Ding*

Main category: cs.CV

TL;DR: STAR是一个快速、鲁棒的开源框架，用于多全切片组织病理学图像的刚性配准，适用于不同染色协议和组织类型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖复杂、计算密集且难以复现的变形或深度学习方法，而轻量级刚性框架在连续切片场景中仍未被充分开发。

Method: STAR集成了染色条件预处理、分层粗到细相关策略、自适应核缩放和内置质量控制。

Result: 在ANHIR 2019和ACROBAT 2022数据集上，STAR能在几分钟内完成稳定配准，对跨染色变异性和部分组织重叠具有鲁棒性。

Conclusion: STAR作为一个轻量级开源工具，降低了临床采用的障碍，并为下一代计算病理学的大规模配对数据准备提供了可复现的基线。

Abstract: Registration of serial whole-slide histopathological images (WSIs) is
critical for enabling direct comparison across diverse stains and for preparing
paired datasets in artificial intelligence (AI) workflows such as virtual
staining and biomarker prediction. While existing methods often rely on complex
deformable or deep learning approaches that are computationally intensive and
difficult to reproduce, lightweight rigid frameworks-sufficient for many
consecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial
Tissue Alignment for Rigid registration), a fast and robust open-source
framework for multi-WSI alignment. STAR integrates stain-conditioned
preprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive
kernel scaling, and built-in quality control, achieving reliable rigid
registration across heterogeneous tissue types and staining protocols,
including hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,
PASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).
Evaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs
and scanning conditions, STAR consistently produced stable alignments within
minutes per slide, demonstrating robustness to cross-stain variability and
partial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC
alignment, construction of multi-IHC panels, and typical failure modes,
underscoring both utility and limitations. Released as an open and lightweight
tool, STAR provides a reproducible baseline that lowers the barrier for
clinical adoption and enables large-scale paired data preparation for
next-generation computational pathology.

</details>


### [11] [Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability](https://arxiv.org/abs/2509.02962)
*Shuai Jiang,Yunfeng Ma,Jingyu Zhou,Yuan Bian,Yaonan Wang,Min Liu*

Main category: cs.CV

TL;DR: 提出跨模态提示学习和对称对比学习，解决多模态工业表面缺陷检测中的模态缺失问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决工业表面缺陷检测中因传感器不确定性导致的模态缺失问题，提升多模态融合效果。

Method: 提出跨模态提示学习（包括一致性提示、模态特定提示和缺失感知提示）和对称对比学习（利用文本模态作为桥梁）。

Result: 在RGB和3D模态缺失率为0.7时，I-AUROC和P-AUROC分别达到73.83%和93.05%，优于现有方法。

Conclusion: 该方法有效解决了模态缺失问题，显著提升了多模态工业表面缺陷检测的性能。

Abstract: Multimodal industrial surface defect detection (MISDD) aims to identify and
locate defect in industrial products by fusing RGB and 3D modalities. This
article focuses on modality-missing problems caused by uncertain sensors
availability in MISDD. In this context, the fusion of multiple modalities
encounters several troubles, including learning mode transformation and
information vacancy. To this end, we first propose cross-modal prompt learning,
which includes: i) the cross-modal consistency prompt serves the establishment
of information consistency of dual visual modalities; ii) the modality-specific
prompt is inserted to adapt different input patterns; iii) the missing-aware
prompt is attached to compensate for the information vacancy caused by dynamic
modalities-missing. In addition, we propose symmetric contrastive learning,
which utilizes text modality as a bridge for fusion of dual vision modalities.
Specifically, a paired antithetical text prompt is designed to generate binary
text semantics, and triple-modal contrastive pre-training is offered to
accomplish multimodal learning. Experiment results show that our proposed
method achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7
for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%
respectively), and outperforms existing approaches to varying degrees under
different missing types and rates. The source code will be available at
https://github.com/SvyJ/MISDD-MM.

</details>


### [12] [EdgeAttNet: Towards Barb-Aware Filament Segmentation](https://arxiv.org/abs/2509.02964)
*Victor Solomon,Piet Martens,Jingyu Liu,Rafal Angryk*

Main category: cs.CV

TL;DR: EdgeAttNet是一种基于U-Net的改进架构，通过引入可学习的边缘图来增强对太阳细丝结构的精细分割能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉太阳细丝（尤其是细丝的分支结构）时表现不佳，主要由于长程依赖性和空间细节建模能力有限。

Method: EdgeAttNet在U-Net基础上引入可学习的边缘图，通过线性变换注意力机制中的Key和Query矩阵，增强对细丝边界和分支的捕捉能力。

Result: 在MAGFILO数据集上，EdgeAttNet优于U-Net和其他基于U-Net的Transformer基线模型，分割精度更高，且推理速度更快。

Conclusion: EdgeAttNet通过显式整合结构先验信息，显著提升了太阳细丝分割的准确性和实用性。

Abstract: Accurate segmentation of solar filaments in H-alpha observations is critical
for determining filament chirality, a key factor in the behavior of Coronal
Mass Ejections (CMEs). However, existing methods often fail to capture
fine-scale filament structures, particularly barbs, due to a limited ability to
model long-range dependencies and spatial detail.
  We propose EdgeAttNet, a segmentation architecture built on a U-Net backbone
by introducing a novel, learnable edge map derived directly from the input
image. This edge map is incorporated into the model by linearly transforming
the attention Key and Query matrices with the edge information, thereby guiding
the self-attention mechanism at the network's bottleneck to more effectively
capture filament boundaries and barbs. By explicitly integrating this
structural prior into the attention computations, EdgeAttNet enhances spatial
sensitivity and segmentation accuracy while reducing the number of trainable
parameters.
  Trained end-to-end, EdgeAttNet outperforms U-Net and other U-Net-based
transformer baselines on the MAGFILO dataset. It achieves higher segmentation
accuracy and significantly better recognition of filament barbs, with faster
inference performance suitable for practical deployment.

</details>


### [13] [KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models](https://arxiv.org/abs/2509.02966)
*Yujin Wang,Tianyi Wang,Quanfeng Liu,Wenxian Fan,Junfeng Jiao,Christian Claudel,Yunbing Yan,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: KEPT是一个知识增强的视觉语言模型框架，用于从连续的前视驾驶帧中预测自车轨迹，结合了时空融合编码器和检索增强的链式思维提示，在nuScenes数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在场景动态和领域知识中的推理能力不足，影响了短时轨迹预测的准确性和可靠性。

Method: KEPT采用时空融合视频编码器和可扩展的k-means + HNSW检索堆栈，结合链式思维提示和分阶段微调策略。

Result: 在nuScenes数据集上，KEPT在NoAvg和TemAvg协议下分别实现了0.70m和0.31m的平均L2误差，碰撞率分别为0.21%和0.07%。

Conclusion: 检索增强和链式思维引导的视觉语言模型为可解释和可信的自动驾驶提供了一种数据高效的途径。

Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable
autonomous driving, yet existing vision-language models (VLMs) often fail to
effectively ground their reasoning in scene dynamics and domain knowledge. To
address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM
framework that predicts ego trajectories directly from consecutive front-view
driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video
encoder, trained via self-supervised learning with hard-negative mining, with a
scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.
Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit
planning constraints, while a triple-stage fine-tuning schedule incrementally
aligns the language head to metric spatial cues, physically feasible motion,
and temporally conditioned front-view planning. Evaluated on nuScenes dataset,
KEPT achieves state-of-the-art performance across open-loop protocols: under
NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg
with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision
rate. Ablation studies show that all three fine-tuning stages contribute
complementary benefits, and that using Top-2 retrieved exemplars yields the
best accuracy-safety trade-off. The k-means-clustered HNSW index delivers
sub-millisecond retrieval latency, supporting practical deployment. These
results indicate that retrieval-augmented, CoT-guided VLMs offer a promising,
data-efficient pathway toward interpretable and trustworthy autonomous driving.

</details>


### [14] [VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results](https://arxiv.org/abs/2509.02969)
*Dasong Li,Sizhuo Ma,Hang Hua,Wenjie Li,Jian Wang,Chris Wei Zhou,Fengbin Guan,Xin Li,Zihao Yu,Yiting Lu,Ru-Ling Liao,Yan Ye,Zhibo Chen,Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai,Erjia Xiao,Lingfeng Zhang,Zhenjie Su,Hao Cheng,Yu Liu,Renjing Xu,Long Chen,Xiaoshuai Hao,Zhenpeng Zeng,Jianqin Wu,Xuxu Wang,Qian Yu,Bo Hu,Weiwei Wang,Pinxin Liu,Yunlong Tang,Luchuan Song,Jinxi He,Jiaru Wu,Hanjia Lyu*

Main category: cs.CV

TL;DR: VQualA 2025挑战赛聚焦于预测用户生成短视频的参与度，使用多模态特征建模，吸引了97名参与者。


<details>
  <summary>Details</summary>
Motivation: 理解社交媒体上用户生成短视频的流行因素，促进建模策略的发展。

Method: 利用包含视觉、音频和创作者元数据的多模态特征进行建模。

Result: 挑战赛吸引了97名参与者，收到15份有效测试提交，推动了短视频参与度预测的进展。

Conclusion: 挑战赛成功促进了短视频参与度预测的研究，为未来工作提供了新方向。

Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement
Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge
focuses on understanding and modeling the popularity of user-generated content
(UGC) short videos on social media platforms. To support this goal, the
challenge uses a new short-form UGC dataset featuring engagement metrics
derived from real-world user interactions. This objective of the Challenge is
to promote robust modeling strategies that capture the complex factors
influencing user engagement. Participants explored a variety of multi-modal
features, including visual content, audio, and metadata provided by creators.
The challenge attracted 97 participants and received 15 valid test submissions,
contributing significantly to progress in short-form UGC video engagement
prediction.

</details>


### [15] [InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System](https://arxiv.org/abs/2509.02973)
*Xianbao Hou,Yonghao He,Zeyd Boukhers,John See,Hu Su,Wei Sui,Cong Yang*

Main category: cs.CV

TL;DR: InstaDA是一种无需训练的双代理系统，通过LLM和扩散模型的协作增强实例分割数据集，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决实例分割数据标注成本高和类别不平衡问题，现有方法缺乏LLM与扩散模型的深度协作。

Method: 提出Text-Agent和Image-Agent，分别通过Prompt Rethink机制和训练图像生成新实例来增强数据。

Result: 在LVIS 1.0验证集上，InstaDA在box AP和mask AP上分别提升+4.0和+3.3，优于基线模型。

Conclusion: InstaDA通过双代理系统有效提升数据多样性和模型性能，优于现有方法。

Abstract: Acquiring high-quality instance segmentation data is challenging due to the
labor-intensive nature of the annotation process and significant class
imbalances within datasets. Recent studies have utilized the integration of
Copy-Paste and diffusion models to create more diverse datasets. However, these
studies often lack deep collaboration between large language models (LLMs) and
diffusion models, and underutilize the rich information within the existing
training data. To address these limitations, we propose InstaDA, a novel,
training-free Dual-Agent system designed to augment instance segmentation
datasets. First, we introduce a Text-Agent (T-Agent) that enhances data
diversity through collaboration between LLMs and diffusion models. This agent
features a novel Prompt Rethink mechanism, which iteratively refines prompts
based on the generated images. This process not only fosters collaboration but
also increases image utilization and optimizes the prompts themselves.
Additionally, we present an Image-Agent (I-Agent) aimed at enriching the
overall data distribution. This agent augments the training set by generating
new instances conditioned on the training images. To ensure practicality and
efficiency, both agents operate as independent and automated workflows,
enhancing usability. Experiments conducted on the LVIS 1.0 validation set
indicate that InstaDA achieves significant improvements, with an increase of
+4.0 in box average precision (AP) and +3.3 in mask AP compared to the
baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in
box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common
categories and mask AP gains of +0.2 on common categories and +0.5 on frequent
categories.

</details>


### [16] [SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation](https://arxiv.org/abs/2509.02993)
*Chao Fan,Xibin Jia,Anqi Xiao,Hongyuan Yu,Zhenghan Yang,Dawei Yang,Hui Xu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: SPENet通过多级原型生成和查询引导的局部原型增强，解决了少样本医学图像分割中忽略类内变化的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有原型方法在少样本医学图像分割中通常生成单一全局原型，忽略了类内变化，导致匹配效果不佳。

Method: 提出SPENet，包含多级原型生成模块（MPG）和查询引导的局部原型增强模块（QLPE），分别实现多粒度测量和自适应原型优化。

Result: 在三个公共医学数据集上的实验表明，SPENet性能优于现有方法。

Conclusion: SPENet通过多级原型和查询引导优化，有效提升了少样本医学图像分割的精度。

Abstract: Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of
medical objects using only a few labeled images. Prototype-based methods have
made significant progress in addressing FSMIS. However, they typically generate
a single global prototype for the support image to match with the query image,
overlooking intra-class variations. To address this issue, we propose a
Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce
a Multi-level Prototype Generation (MPG) module, which enables
multi-granularity measurement between the support and query images by
simultaneously generating a global prototype and an adaptive number of local
prototypes. Additionally, we observe that not all local prototypes in the
support image are beneficial for matching, especially when there are
substantial discrepancies between the support and query images. To alleviate
this issue, we propose a Query-guided Local Prototype Enhancement (QLPE)
module, which adaptively refines support prototypes by incorporating guidance
from the query image, thus mitigating the negative effects of such
discrepancies. Extensive experiments on three public medical datasets
demonstrate that SPENet outperforms existing state-of-the-art methods,
achieving superior performance.

</details>


### [17] [SOPSeg: Prompt-based Small Object Instance Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2509.03002)
*Chenhao Wang,Yingrui Ji,Yu Meng,Yunjian Zhang,Yao Zhu*

Main category: cs.CV

TL;DR: SOPSeg是一个针对遥感图像中小物体分割的提示框架，通过区域自适应放大和定制解码器提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注小物体检测，而小物体实例分割缺乏专用数据集和技术支持，SAM模型在小物体分割上表现不佳。

Method: 提出SOPSeg框架，包括区域自适应放大策略和定制解码器，结合边缘预测和渐进细化。

Result: SOPSeg在小物体分割上优于现有方法，并支持高效数据集构建。

Conclusion: SOPSeg填补了小物体分割的技术空白，并发布了模型和数据集以推动未来研究。

Abstract: Extracting small objects from remote sensing imagery plays a vital role in
various applications, including urban planning, environmental monitoring, and
disaster management. While current research primarily focuses on small object
detection, instance segmentation for small objects remains underexplored, with
no dedicated datasets available. This gap stems from the technical challenges
and high costs of pixel-level annotation for small objects. While the Segment
Anything Model (SAM) demonstrates impressive zero-shot generalization, its
performance on small-object segmentation deteriorates significantly, largely
due to the coarse 1/16 feature resolution that causes severe loss of fine
spatial details. To this end, we propose SOPSeg, a prompt-based framework
specifically designed for small object segmentation in remote sensing imagery.
It incorporates a region-adaptive magnification strategy to preserve
fine-grained details, and employs a customized decoder that integrates edge
prediction and progressive refinement for accurate boundary delineation.
Moreover, we introduce a novel prompting mechanism tailored to the oriented
bounding boxes widely adopted in remote sensing applications. SOPSeg
outperforms existing methods in small object segmentation and facilitates
efficient dataset construction for remote sensing tasks. We further construct a
comprehensive small object instance segmentation dataset based on SODA-A, and
will release both the model and dataset to support future research.

</details>


### [18] [Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers](https://arxiv.org/abs/2509.03006)
*Tzuhsuan Huang,Cheng Yu Yeo,Tsai-Ling Huang,Hong-Han Shuai,Wen-Huang Cheng,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文研究了后处理水印技术，通过集成攻击网络提升其鲁棒性，结合CNN和Transformer在不同域的攻击网络，显著提高了水印模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注处理中水印技术，而后处理水印更具灵活性，适用于任何生成模型且无需访问模型内部结构。

Method: 构建了基于CNN和Transformer的空间域和频域攻击网络，研究不同组合对水印模型鲁棒性的影响。

Result: 结合空间域CNN和频域Transformer的攻击网络表现最佳，显著提升了水印模型的鲁棒性，在WAVES基准测试中表现优异。

Conclusion: 后处理水印技术通过集成攻击网络可显著提升鲁棒性，为水印嵌入提供了更灵活的解决方案。

Abstract: Recent studies on deep watermarking have predominantly focused on
in-processing watermarking, which integrates the watermarking process into
image generation. However, post-processing watermarking, which embeds
watermarks after image generation, offers more flexibility. It can be applied
to outputs from any generative model (e.g. GANs, diffusion models) without
needing access to the model's internal structure. It also allows users to embed
unique watermarks into individual images. Therefore, this study focuses on
post-processing watermarking and enhances its robustness by incorporating an
ensemble attack network during training. We construct various versions of
attack networks using CNN and Transformer in both spatial and frequency domains
to investigate how each combination influences the robustness of the
watermarking model. Our results demonstrate that combining a CNN-based attack
network in the spatial domain with a Transformer-based attack network in the
frequency domain yields the highest robustness in watermarking models.
Extensive evaluation on the WAVES benchmark, using average bit accuracy as the
metric, demonstrates that our ensemble attack network significantly enhances
the robustness of baseline watermarking methods under various stress tests. In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%. The code is released
at:https://github.com/aiiu-lab/DeepRobustWatermark.

</details>


### [19] [Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations](https://arxiv.org/abs/2509.03011)
*Alexis Ivan Lopez Escamilla,Gilberto Ochoa,Sharib Al*

Main category: cs.CV

TL;DR: 提出了一种针对溃疡性结肠炎的病灶感知图像描述框架，结合了ResNet嵌入、Grad-CAM热图和CBAM增强注意力，并利用T5解码器生成描述。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合临床元数据和深度学习技术，生成与临床实践一致的结构化描述，提高内窥镜报告的可靠性。

Method: 整合ResNet嵌入、Grad-CAM热图和CBAM增强注意力，使用T5解码器生成描述，并注入临床元数据作为自然语言提示。

Result: 相比基线方法，该框架提高了描述质量和MES分类准确性。

Conclusion: 该框架支持可靠的内窥镜报告生成，具有临床应用潜力。

Abstract: We present a lesion-aware image captioning framework for ulcerative colitis
(UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and
CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3,
vascular pattern, bleeding, erythema, friability, ulceration) is injected as
natural-language prompts to guide caption generation. The system produces
structured, interpretable descriptions aligned with clinical practice and
provides MES classification and lesion tags. Compared with baselines, our
approach improves caption quality and MES classification accuracy, supporting
reliable endoscopic reporting.

</details>


### [20] [Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens](https://arxiv.org/abs/2509.03025)
*Sohee Kim,Soohyun Ryu,Joonhyung Park,Eunho Yang*

Main category: cs.CV

TL;DR: LVLMs有时错误地将文本输入视为图像内容，导致错误响应。研究发现特定FFN神经元（VA神经元）能检测视觉缺失，并利用此开发了检测模块和改进方法。


<details>
  <summary>Details</summary>
Motivation: 解决LVLMs错误地将无视觉证据的文本输入视为图像内容的问题。

Method: 识别VA神经元，开发检测模块，改进提示或替换未视觉接地的令牌。

Result: 方法有效减少了模型错误假设文本输入视觉存在的情况，且适用于多种LVLMs。

Conclusion: VA神经元和检测模块能显著提升LVLMs的准确性。

Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses
by jointly interpreting visual and textual inputs. However, our finding reveals
they often mistakenly perceive text inputs lacking visual evidence as being
part of the image, leading to erroneous responses. In light of this finding, we
probe whether LVLMs possess an internal capability to determine if textual
concepts are grounded in the image, and discover a specific subset of
Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons,
that consistently signal the visual absence through a distinctive activation
pattern. Leveraging these patterns, we develop a detection module that
systematically classifies whether an input token is visually grounded. Guided
by its prediction, we propose a method to refine the outputs by reinterpreting
question prompts or replacing the detected absent tokens during generation.
Extensive experiments show that our method effectively mitigates the models'
tendency to falsely presume the visual presence of text input and its
generality across various LVLMs.

</details>


### [21] [Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification](https://arxiv.org/abs/2509.03032)
*Kaicong Huang,Talha Azfar,Jack M. Reilly,Thomas Guggisberg,Ruimin Ke*

Main category: cs.CV

TL;DR: 提出了一种双分支跨模态特征提取框架，联合建模前景和背景信息，通过语义对齐和对抗学习提升行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖手动标注且难以处理复杂遮挡，而多模态方法仅关注前景信息，忽略了背景语义的价值。受人类感知启发，认为背景语义与前景语义在ReID中同等重要。

Method: 采用双分支跨模态特征提取管道，结合前景和背景信息，提出域内语义对齐和域间语义对抗学习策略。

Result: 在两个整体和两个遮挡ReID基准测试中表现优异，匹配或超越当前最先进方法。

Conclusion: 联合建模前景和背景信息，并通过语义对齐和对抗学习策略，显著提升了行人重识别的性能。

Abstract: Person re-identification faces two core challenges: precisely locating the
foreground target while suppressing background noise and extracting
fine-grained features from the target region. Numerous visual-only approaches
address these issues by partitioning an image and applying attention modules,
yet they rely on costly manual annotations and struggle with complex
occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic
cues to guide visual understanding. However, they focus solely on foreground
information, but overlook the potential value of background cues. Inspired by
human perception, we argue that background semantics are as important as the
foreground semantics in ReID, as humans tend to eliminate background
distractions while focusing on target appearance. Therefore, this paper
proposes an end-to-end framework that jointly models foreground and background
information within a dual-branch cross-modal feature extraction pipeline. To
help the network distinguish between the two domains, we propose an
intra-semantic alignment and inter-semantic adversarial learning strategy.
Specifically, we align visual and textual features that share the same
semantics across domains, while simultaneously penalizing similarity between
foreground and background features to enhance the network's discriminative
power. This strategy drives the model to actively suppress noisy background
regions and enhance attention toward identity-relevant foreground cues.
Comprehensive experiments on two holistic and two occluded ReID benchmarks
demonstrate the effectiveness and generality of the proposed method, with
results that match or surpass those of current state-of-the-art approaches.

</details>


### [22] [MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model](https://arxiv.org/abs/2509.03041)
*Pengyang Yu,Haoquan Wang,Gerard Marks,Tahar Kechadi,Laurence T. Yang,Sahraoui Dhelim,Nyothiri Aung*

Main category: cs.CV

TL;DR: MedLiteNet是一种轻量级CNN-Transformer混合模型，用于皮肤病变分割，结合了高效的特征提取和多尺度上下文聚合。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CNN和Vision Transformer）在皮肤病变分割中存在局限性，如CNN的有限感受野和Transformer的高计算复杂度。

Method: 采用深度可分离的Mobile Inverted Bottleneck块、跨尺度令牌混合单元和边界感知自注意力模块。

Result: 实现了高精度的皮肤病变分割。

Conclusion: MedLiteNet在轻量化和性能之间取得了平衡，适用于小样本医学数据集。

Abstract: Accurate skin-lesion segmentation remains a key technical challenge for
computer-aided diagnosis of skin cancer. Convolutional neural networks, while
effective, are constrained by limited receptive fields and thus struggle to
model long-range dependencies. Vision Transformers capture global context, yet
their quadratic complexity and large parameter budgets hinder use on the
small-sample medical datasets common in dermatology. We introduce the
MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic
segmentation that achieves high precision through hierarchical feature
extraction and multi-scale context aggregation. The encoder stacks depth-wise
Mobile Inverted Bottleneck blocks to curb computation, inserts a
bottleneck-level cross-scale token-mixing unit to exchange information between
resolutions, and embeds a boundary-aware self-attention module to sharpen
lesion contours.

</details>


### [23] [DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks](https://arxiv.org/abs/2509.03044)
*Chengjie Huang,Jiafeng Yan,Jing Li,Lu Bai*

Main category: cs.CV

TL;DR: 提出动态条件双扩散桥训练范式，解决多任务场景中扩散模型难以利用任务间内在关联的问题，尤其在训练数据不足的病态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在多任务场景中难以利用任务间内在关联，且静态条件控制无法适应动态演变的特性，尤其在病态任务中表现不佳。

Method: 提出动态条件双扩散桥训练范式，解耦扩散和条件生成过程，利用动态条件逐步调整统计特性，嵌入时间相关信息，降低网络学习难度。

Result: 在去雾和可见光-红外融合等病态多任务场景中，在多个公开数据集上取得最佳性能。

Conclusion: 动态条件双扩散桥训练范式有效解决了病态多任务中扩散模型的局限性，显著提升了性能。

Abstract: Conditional diffusion models have made impressive progress in the field of
image processing, but the characteristics of constructing data distribution
pathways make it difficult to exploit the intrinsic correlation between tasks
in multi-task scenarios, which is even worse in ill-posed tasks with a lack of
training data. In addition, traditional static condition control makes it
difficult for networks to learn in multi-task scenarios with its dynamically
evolving characteristics. To address these challenges, we propose a dynamic
conditional double diffusion bridge training paradigm to build a general
framework for ill-posed multi-tasks. Firstly, this paradigm decouples the
diffusion and condition generation processes, avoiding the dependence of the
diffusion model on supervised data in ill-posed tasks. Secondly, generated by
the same noise schedule, dynamic conditions are used to gradually adjust their
statistical characteristics, naturally embed time-related information, and
reduce the difficulty of network learning. We analyze the learning objectives
of the network under different conditional forms in the single-step denoising
process and compare the changes in its attention weights in the network,
demonstrating the superiority of our dynamic conditions. Taking dehazing and
visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve
the best performance in multiple indicators on public datasets. The code has
been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.

</details>


### [24] [Isolated Bangla Handwritten Character Classification using Transfer Learning](https://arxiv.org/abs/2509.03061)
*Abdul Karim,S M Rafiuddin,Jahidul Islam Razin,Tahira Alam*

Main category: cs.CV

TL;DR: 使用迁移学习和多种深度神经网络技术（3DCNN、ResNet、MobileNet）对孟加拉语手写字符进行分类，模型在训练和测试数据上分别达到99.82%和99.46%的准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语字符复杂，包含多种基本和复合字符，现有研究对手写字符的分类仍有改进空间。

Method: 采用3DCNN、ResNet和MobileNet等深度神经网络技术，结合迁移学习，避免梯度消失问题。

Result: 模型在训练和测试数据上的准确率分别为99.82%和99.46%，优于现有基准。

Conclusion: 提出的方法在孟加拉语手写字符分类中表现出色，准确率显著提升。

Abstract: Bangla language consists of fifty distinct characters and many compound
characters. Several notable studies have been performed to recognize Bangla
characters, both handwritten and optical. Our approach uses transfer learning
to classify the basic, distinct, as well as compound Bangla handwritten
characters while avoiding the vanishing gradient problem. Deep Neural Network
techniques such as 3D Convolutional Neural Network (3DCNN), Residual Neural
Network (ResNet), and MobileNet are applied to generate an end-to-end
classification of all possible standard formations of handwritten characters in
the Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105
Bangla character image samples categorized into 84 distinct classes, is used
for this classification model. The model achieved 99.82% accuracy on training
data and 99.46% accuracy on test data. Comparisons with various
state-of-the-art benchmarks of Bangla handwritten character classification show
that the proposed model achieves better accuracy in classifying the data.

</details>


### [25] [High Cursive Complex Character Recognition using GAN External Classifier](https://arxiv.org/abs/2509.03062)
*S M Rafiuddin*

Main category: cs.CV

TL;DR: 提出了一种结合外部分类器和生成对抗网络（ADA-GAN）的方法，用于分类复杂和草书手写字符，通过生成假字符图像增强训练数据，提高了分类的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 手写字符因其复杂和草书特性难以分类，传统卷积神经网络在字符复杂度增加时准确性下降。

Method: 使用生成对抗网络生成假字符图像，并通过对抗性扰动噪声增强训练数据，结合外部分类器进行分类。

Result: ADA-GAN在复杂和草书字符分类中表现更鲁棒和有效，优于传统卷积神经网络。

Conclusion: ADA-GAN为解决复杂手写字符分类问题提供了一种有效方法。

Abstract: Handwritten characters can be trickier to classify due to their complex and
cursive nature compared to simple and non-cursive characters. We present an
external classifier along with a Generative Adversarial Network that can
classify highly cursive and complex characters. The generator network produces
fake handwritten character images, which are then used to augment the training
data after adding adversarially perturbed noise and achieving a confidence
score above a threshold with the discriminator network. The results show that
the accuracy of convolutional neural networks decreases as character complexity
increases, but our proposed model, ADA-GAN, remains more robust and effective
for both cursive and complex characters.

</details>


### [26] [TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis](https://arxiv.org/abs/2509.03095)
*Clément Hervé,Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.CV

TL;DR: 提出一种跨域特征迁移方法，利用非医学3D数据训练的生成模型TRELLIS的几何嵌入特征，提升颅内动脉瘤分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤检测和建模因缺乏标注的3D数据而困难，需借助跨域特征迁移解决。

Method: 使用TRELLIS的表面特征替代传统点法线或网格描述符，增强分类、分割和血流预测任务。

Result: 实验显示特征迁移显著提升准确性、F1分数和分割质量，并降低15%的模拟误差。

Conclusion: 跨域3D特征迁移在医学任务中具有广泛潜力。

Abstract: Intracranial aneurysms pose a significant clinical risk yet are difficult to
detect, delineate and model due to limited annotated 3D data. We propose a
cross-domain feature-transfer approach that leverages the latent geometric
embeddings learned by TRELLIS, a generative model trained on large-scale
non-medical 3D datasets, to augment neural networks for aneurysm analysis. By
replacing conventional point normals or mesh descriptors with TRELLIS surface
features, we systematically enhance three downstream tasks: (i) classifying
aneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting
aneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving
blood-flow fields using a graph neural network on the AnXplore dataset. Our
experiments show that the inclusion of these features yields strong gains in
accuracy, F1-score and segmentation quality over state-of-the-art baselines,
and reduces simulation error by 15\%. These results illustrate the broader
potential of transferring 3D representations from general-purpose generative
models to specialized medical tasks.

</details>


### [27] [Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods](https://arxiv.org/abs/2509.03108)
*Shota Iwamatsu,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出了一种针对人脸反欺骗检测的后门投毒攻击方法，展示了潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统可能被照片欺骗攻击绕过，现有检测方法依赖大量数据，易受恶意数据注入影响。

Method: 通过将欺骗攻击特征嵌入真实人脸图像中，绕过检测且不引起视觉变化。

Result: 在公开数据集上验证了该方法对现有检测系统的实际威胁。

Conclusion: 揭示了后门投毒攻击在人脸反欺骗检测中的潜在风险。

Abstract: Face recognition systems are robust against environmental changes and noise,
and thus may be vulnerable to illegal authentication attempts using user face
photos, such as spoofing attacks. To prevent such spoofing attacks, it is
crucial to discriminate whether the input image is a live user image or a
spoofed image prior to the face recognition process. Most existing spoofing
attack detection methods utilize deep learning, which necessitates a
substantial amount of training data. Consequently, if malicious data is
injected into a portion of the training dataset, a specific spoofing attack may
be erroneously classified as live, leading to false positives.In this paper, we
propose a novel backdoor poisoning attack method to demonstrate the latent
threat of backdoor poisoning within face anti-spoofing detection. The proposed
method enables certain spoofing attacks to bypass detection by embedding
features extracted from the spoofing attack's face image into a live face image
without inducing any perceptible visual alterations.Through experiments
conducted on public datasets, we demonstrate that the proposed method
constitutes a realistic threat to existing spoofing attack detection systems.

</details>


### [28] [Information transmission: Inferring change area from change moment in time series remote sensing images](https://arxiv.org/abs/2509.03112)
*Jialu Li,Chen Wu,Meiqi Hu*

Main category: cs.CV

TL;DR: CAIM-Net通过从变化时刻推断变化区域，统一了时间序列变化检测中的两个任务，提高了结果一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将变化区域检测和变化时刻识别视为独立任务，忽略了它们之间的内在联系。

Method: CAIM-Net包含三个关键步骤：差异提取与增强、粗变化时刻提取、细变化时刻提取与变化区域推断。

Result: 通过轻量级编码器和边界增强卷积，CAIM-Net高效提取差异特征并推断变化区域。

Conclusion: CAIM-Net通过统一变化时刻和变化区域的推断，提升了时间序列变化检测的性能。

Abstract: Time series change detection is a critical task for exploring ecosystem
dynamics using time series remote sensing images, because it can simultaneously
indicate where and when change occur. While deep learning has shown excellent
performance in this domain, it continues to approach change area detection and
change moment identification as distinct tasks. Given that change area can be
inferred from change moment, we propose a time series change detection network,
named CAIM-Net (Change Area Inference from Moment Network), to ensure
consistency between change area and change moment results. CAIM-Net infers
change area from change moment based on the intrinsic relationship between time
series analysis and spatial change detection. The CAIM-Net comprises three key
steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,
and Fine Change Moment Extraction and Change Area Inference. In the Difference
Extraction and Enhancement, a lightweight encoder with batch dimension stacking
is designed to rapidly extract difference features. Subsequently, boundary
enhancement convolution is applied to amplify these difference features. In the
Coarse Change Moment Extraction, the enhanced difference features from the
first step are used to spatiotemporal correlation analysis, and then two
distinct methods are employed to determine coarse change moments. In the Fine
Change Moment Extraction and Change Area Inference, a multiscale temporal Class
Activation Mapping (CAM) module first increases the weight of the
change-occurring moment from coarse change moments. Then the weighted change
moment is used to infer change area based on the fact that pixels with the
change moment must have undergone a change.

</details>


### [29] [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
*Shan Wang,Maying Shen,Nadine Chang,Chuong Nguyen,Hongdong Li,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 论文提出了一种基于梯度自反思的方法，通过估计不同token类型的影响来检测视觉token，并结合对比解码框架，有效减少多模态大语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型中的幻觉问题主要由文本-视觉偏差和共现偏差引起，现有方法未能动态评估偏差水平。

Method: 使用梯度自反思方法估计token影响，检测视觉token，并集成到影响感知的对比解码框架中。

Result: 实验表明，该方法显著减少幻觉，在LLaVA-QA90上准确率提升高达92%。

Conclusion: 无需额外资源即可有效缓解多模态大语言模型中的幻觉问题。

Abstract: Hallucinations in multimodal large language model are caused by the
text-visual bias and the co-occurrence bias. The former reflects an
over-reliance on text information in the decision-making process, while the
latter arises from the statistical object-pairing patterns abstracted from the
training data. Existing mitigation methods heuristically address these biases
without understanding the fluctuating bias level across the instances. We first
propose estimating the influence of respective token types (visual, prompt, and
previous outputs) using a gradient-based self-reflection method. The estimated
token influence further enables the detection of object-related visual tokens
and their integration into an influence-aware contrastive decoding framework to
mitigate both types of biases simultaneously. Our method operates without the
need for additional resources, such as costly fine-tuning, extra models, or
data statistics. Extensive experiments show it effectively reduces
hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.

</details>


### [30] [Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge](https://arxiv.org/abs/2509.03114)
*Miao Xu,Xiangyu Zhu,Xusheng Liang,Zidu Wang,Jinlin Wu,Zhen Lei*

Main category: cs.CV

TL;DR: 提出了一种基于引力场的扩散桥方法（GravityDB），用于模拟可变形的手机表面与刚性物体之间的交互，解决了现有方法中的穿透和间隙问题，并捕捉了真实的手部变形。


<details>
  <summary>Details</summary>
Motivation: 现有方法在手-物体交互中常出现穿透或间隙问题，且难以捕捉手部变形。

Method: 通过引力场驱动的扩散桥模拟交互，结合语义信息引导引力场构建。

Result: 实验表明，该方法能生成物理上合理的交互，避免穿透，确保稳定抓取，并捕捉真实变形。

Conclusion: GravityDB方法在手-物体交互中表现出色，解决了现有方法的局限性。

Abstract: Existing reconstruction or hand-object pose estimation methods are capable of
producing coarse interaction states. However, due to the complex and diverse
geometry of both human hands and objects, these approaches often suffer from
interpenetration or leave noticeable gaps in regions that are supposed to be in
contact. Moreover, the surface of a real human hand undergoes non-negligible
deformations during interaction, which are difficult to capture and represent
with previous methods. To tackle these challenges, we formulate hand-object
interaction as an attraction-driven process and propose a Gravity-Field Based
Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand
surface and rigid objects. Our approach effectively resolves the aforementioned
issues by generating physically plausible interactions that are free of
interpenetration, ensure stable grasping, and capture realistic hand
deformations. Furthermore, we incorporate semantic information from textual
descriptions to guide the construction of the gravitational field, enabling
more semantically meaningful interaction regions. Extensive qualitative and
quantitative experiments on multiple datasets demonstrate the effectiveness of
our method.

</details>


### [31] [Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation](https://arxiv.org/abs/2509.03141)
*Mattia Litrico,Francesco Guarnera,Mario Valerio Giuffrida,Daniele Ravì,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 提出了一种3D时间感知扩散模型（TADM-3D），用于准确预测MRI脑部结构变化，解决了现有方法在时间关系建模和3D上下文利用上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉脑部结构变化与时间间隔的关系、临床实用性以及3D解剖学上下文利用方面存在局限性。

Method: TADM-3D结合预训练的脑龄估计器（BAE）和双向时间正则化（BITR），生成更准确的MRI预测。

Result: 在OASIS-3和NACC数据集上验证了模型的准确性和泛化性能。

Conclusion: TADM-3D在预测脑部结构变化方面表现出色，具有临床潜力。

Abstract: Generating realistic MRIs to accurately predict future changes in the
structure of brain is an invaluable tool for clinicians in assessing clinical
outcomes and analysing the disease progression at the patient level. However,
current existing methods present some limitations: (i) some approaches fail to
explicitly capture the relationship between structural changes and time
intervals, especially when trained on age-imbalanced datasets; (ii) others rely
only on scan interpolation, which lack clinical utility, as they generate
intermediate images between timepoints rather than future pathological
progression; and (iii) most approaches rely on 2D slice-based architectures,
thereby disregarding full 3D anatomical context, which is essential for
accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion
Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To
better model the relationship between time interval and brain changes, TADM-3D
uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in
the generation of MRIs that accurately reflect the expected age difference
between baseline and generated follow-up scans. Additionally, to further
improve the temporal awareness of TADM-3D, we propose the Back-In-Time
Regularisation (BITR), by training TADM-3D to predict bidirectionally from the
baseline to follow-up (forward), as well as from the follow-up to baseline
(backward). Although predicting past scans has limited clinical applications,
this regularisation helps the model generate temporally more accurate scans. We
train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the
generalisation performance on an external test set from the NACC dataset. The
code will be available upon acceptance.

</details>


### [32] [Preserving instance continuity and length in segmentation through connectivity-aware loss computation](https://arxiv.org/abs/2509.03154)
*Karol Szustakowski,Luk Frank,Julia Esser,Jan Gründemann,Marie Piraud*

Main category: cs.CV

TL;DR: 论文提出两种新型损失函数（Negative Centerline Loss和Simplified Topology Loss），用于提升生物医学分割任务中长形结构的连续性，并通过实验验证其在3D光片荧光显微镜数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在生物医学分割任务中，长形结构的连续性和长度比逐体素精度更重要，但现有方法容易因信号丢失导致分割不连续。

Method: 提出两种损失函数（Negative Centerline Loss和Simplified Topology Loss），结合实验设计（如降采样和间距校正）优化分割连续性。

Result: 在3D光片荧光显微镜数据集上，新方法显著减少了分割不连续现象，尤其在信号丢失区域，提升了实例长度计算的准确性。

Conclusion: 通过损失函数设计嵌入结构先验，可显著提升生物医学分割的可靠性。

Abstract: In many biomedical segmentation tasks, the preservation of elongated
structure continuity and length is more important than voxel-wise accuracy. We
propose two novel loss functions, Negative Centerline Loss and Simplified
Topology Loss, that, applied to Convolutional Neural Networks (CNNs), help
preserve connectivity of output instances. Moreover, we discuss characteristics
of experiment design, such as downscaling and spacing correction, that help
obtain continuous segmentation masks. We evaluate our approach on a 3D
light-sheet fluorescence microscopy dataset of axon initial segments (AIS), a
task prone to discontinuity due to signal dropout. Compared to standard CNNs
and existing topology-aware losses, our methods reduce the number of
segmentation discontinuities per instance, particularly in regions with missing
input signal, resulting in improved instance length calculation in downstream
applications. Our findings demonstrate that structural priors embedded in the
loss design can significantly enhance the reliability of segmentation for
biological applications.

</details>


### [33] [Count2Density: Crowd Density Estimation without Location-level Annotations](https://arxiv.org/abs/2509.03170)
*Mattia Litrico,Feng Chen,Michael Pound,Sotirios A Tsaftaris,Sebastiano Battiato,Mario Valerio Giuffrida*

Main category: cs.CV

TL;DR: Count2Density是一种新方法，仅使用计数级注释训练深度网络生成密度图，减少对精细标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 减少对精细位置标注的依赖，解决标注耗时且难以扩展的问题。

Method: 利用历史地图库生成伪密度图，结合超几何分布采样和自监督对比空间正则化。

Result: 在多个数据集上优于跨域适应方法和半监督最新方法，并能准确进行子区域计数。

Conclusion: Count2Density能有效从计数级注释中提取空间信息，提升密度估计的实用性。

Abstract: Crowd density estimation is a well-known computer vision task aimed at
estimating the density distribution of people in an image. The main challenge
in this domain is the reliance on fine-grained location-level annotations,
(i.e. points placed on top of each individual) to train deep networks.
Collecting such detailed annotations is both tedious, time-consuming, and poses
a significant barrier to scalability for real-world applications. To alleviate
this burden, we present Count2Density: a novel pipeline designed to predict
meaningful density maps containing quantitative spatial information using only
count-level annotations (i.e., total number of people) during training. To
achieve this, Count2Density generates pseudo-density maps leveraging past
predictions stored in a Historical Map Bank, thereby reducing confirmation
bias. This bank is initialised using an unsupervised saliency estimator to
provide an initial spatial prior and is iteratively updated with an EMA of
predicted density maps. These pseudo-density maps are obtained by sampling
locations from estimated crowd areas using a hypergeometric distribution, with
the number of samplings determined by the count-level annotations. To further
enhance the spatial awareness of the model, we add a self-supervised
contrastive spatial regulariser to encourage similar feature representations
within crowded regions while maximising dissimilarity with background regions.
Experimental results demonstrate that our approach significantly outperforms
cross-domain adaptation methods and achieves better results than recent
state-of-the-art approaches in semi-supervised settings across several
datasets. Additional analyses validate the effectiveness of each individual
component of our pipeline, confirming the ability of Count2Density to
effectively retrieve spatial information from count-level annotations and
enabling accurate subregion counting.

</details>


### [34] [AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain](https://arxiv.org/abs/2509.03179)
*Alma M. Liezenga,Stefan Wijnja,Puck de Haan,Niels W. T. Brink,Jip J. van Stijn,Yori Kamphuis,Klamer Schutte*

Main category: cs.CV

TL;DR: 该论文研究了军事领域中毒攻击对目标检测系统的影响及检测方法，提出了新的检测方法AutoDetect，并呼吁需要更多代表性数据集以进一步评估风险。


<details>
  <summary>Details</summary>
Motivation: 军事领域中毒攻击威胁日益严重，但相关研究有限，尤其是对目标检测系统的攻击和检测。

Method: 通过创建军事车辆数据集MilCivVeh，实施改进的BadDet攻击，并测试多种检测方法，最终提出基于自动编码器的AutoDetect方法。

Result: 攻击虽有效但需大量数据污染，AutoDetect在检测性能上优于现有方法且更高效。

Conclusion: 军事领域需更多代表性数据集以进一步评估中毒攻击风险，AutoDetect为检测提供了新思路。

Abstract: Poisoning attacks pose an increasing threat to the security and robustness of
Artificial Intelligence systems in the military domain. The widespread use of
open-source datasets and pretrained models exacerbates this risk. Despite the
severity of this threat, there is limited research on the application and
detection of poisoning attacks on object detection systems. This is especially
problematic in the military domain, where attacks can have grave consequences.
In this work, we both investigate the effect of poisoning attacks on military
object detectors in practice, and the best approach to detect these attacks. To
support this research, we create a small, custom dataset featuring military
vehicles: MilCivVeh. We explore the vulnerability of military object detectors
for poisoning attacks by implementing a modified version of the BadDet attack:
a patch-based poisoning attack. We then assess its impact, finding that while a
positive attack success rate is achievable, it requires a substantial portion
of the data to be poisoned -- raising questions about its practical
applicability. To address the detection challenge, we test both specialized
poisoning detection methods and anomaly detection methods from the visual
industrial inspection domain. Since our research shows that both classes of
methods are lacking, we introduce our own patch detection method: AutoDetect, a
simple, fast, and lightweight autoencoder-based method. Our method shows
promising results in separating clean from poisoned samples using the
reconstruction error of image slices, outperforming existing methods, while
being less time- and memory-intensive. We urge that the availability of large,
representative datasets in the military domain is a prerequisite to further
evaluate risks of poisoning attacks and opportunities patch detection.

</details>


### [35] [PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising](https://arxiv.org/abs/2509.03185)
*Debopom Sutradhar,Ripon Kumar Debnath,Mohaimenul Azam Khan Raiaan,Yan Zhang,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: PPORLD-EDNetLDCT是一种基于强化学习的低剂量CT图像去噪方法，显著提升了图像质量和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）虽减少辐射，但噪声增加，传统去噪方法难以保持图像质量。

Method: 采用强化学习（RL）和编码器-解码器结构，通过PPO算法实时优化去噪策略。

Result: 在多个数据集上表现优异，PSNR达41.87，SSIM为0.9814，COVID-19分类准确率提升至94%。

Conclusion: 该方法为LDCT成像提供了更安全、更准确的解决方案。

Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation
exposure, but it often leads to increased noise and reduced image quality.
Traditional denoising methods, such as iterative optimization or supervised
learning, often fail to preserve image quality. To address these challenges, we
introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with
Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in
which an advanced posterior policy optimization (PPO) algorithm is used to
optimize denoising policies in real time, based on image quality feedback,
trained via a custom gym environment. The experimental results on the low dose
CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT
model outperforms traditional denoising techniques and other DL-based methods,
achieving a peak signal-to-noise ratio of 41.87, a structural similarity index
measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in
NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of
41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality
of denoising using a classification task in the COVID-19 LDCT dataset, where
the images processed by our method improved the classification accuracy to
94\%, achieving 4\% higher accuracy compared to denoising without RL-based
denoising. This method offers a promising solution for safer and more accurate
LDCT imaging.

</details>


### [36] [AIVA: An AI-based Virtual Companion for Emotion-aware Interaction](https://arxiv.org/abs/2509.03212)
*Chenxi Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为\ours的多模态情感感知框架，通过结合情感线索提升LLM的交互体验。


<details>
  <summary>Details</summary>
Motivation: 当前LLM仅限于文本处理，无法捕捉非语言情感信号，限制了交互的沉浸感和共情能力。

Method: 提出Multimodal Sentiment Perception Network (MSPN)，结合跨模态融合变换器和监督对比学习，并开发情感感知提示工程策略及TTS系统。

Result: 实现了情感对齐的交互，适用于伴侣机器人、社会护理等领域。

Conclusion: \ours为情感感知代理提供了可行框架，扩展了LLM的应用场景。

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved
natural language understanding and generation, enhancing Human-Computer
Interaction (HCI). However, LLMs are limited to unimodal text processing and
lack the ability to interpret emotional cues from non-verbal signals, hindering
more immersive and empathetic interactions. This work explores integrating
multimodal sentiment perception into LLMs to create emotion-aware agents. We
propose \ours, an AI-based virtual companion that captures multimodal sentiment
cues, enabling emotionally aligned and animated HCI. \ours introduces a
Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion
transformer and supervised contrastive learning to provide emotional cues.
Additionally, we develop an emotion-aware prompt engineering strategy for
generating empathetic responses and integrate a Text-to-Speech (TTS) system and
animated avatar module for expressive interactions. \ours provides a framework
for emotion-aware agents with applications in companion robotics, social care,
mental health, and human-centered AI.

</details>


### [37] [RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion](https://arxiv.org/abs/2509.03214)
*Junhao Jia,Yifei Sun,Yunyou Liu,Cheng Yang,Changmiao Wang,Feiwei Qin,Yong Peng,Wenwen Min*

Main category: cs.CV

TL;DR: RTGMFF框架通过ROI驱动的文本生成和多模态特征融合，提升了fMRI在脑部疾病诊断中的准确性。


<details>
  <summary>Details</summary>
Motivation: fMRI在临床诊断中面临信噪比低、个体差异大以及现有模型频率感知有限的问题，且缺乏文本注释。

Method: RTGMFF包含ROI驱动的文本生成、混合频率-空间编码器和自适应语义对齐模块。

Result: 在ADHD-200和ABIDE基准测试中，RTGMFF在诊断准确性、敏感性和特异性上优于现有方法。

Conclusion: RTGMFF通过多模态融合和文本生成，显著提升了fMRI的诊断性能。

Abstract: Functional magnetic resonance imaging (fMRI) is a powerful tool for probing
brain function, yet reliable clinical diagnosis is hampered by low
signal-to-noise ratios, inter-subject variability, and the limited frequency
awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI
datasets lack textual annotations that could contextualize regional activation
and connectivity patterns. We introduce RTGMFF, a framework that unifies
automatic ROI-level text generation with multimodal feature fusion for
brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven
fMRI text generation deterministically condenses each subject's activation,
connectivity, age, and sex into reproducible text tokens; (ii) Hybrid
frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a
cross-scale Transformer encoder to capture frequency-domain structure alongside
long-range spatial dependencies; and (iii) Adaptive semantic alignment module
embeds the ROI token sequence and visual features in a shared space, using a
regularized cosine-similarity loss to narrow the modality gap. Extensive
experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses
current methods in diagnostic accuracy, achieving notable gains in sensitivity,
specificity, and area under the ROC curve. Code is available at
https://github.com/BeistMedAI/RTGMFF.

</details>


### [38] [LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking](https://arxiv.org/abs/2509.03221)
*Jing Zhang,Siying Tao,Jiao Li,Tianhe Wang,Junchen Wu,Ruqian Hao,Xiaohui Du,Ruirong Tan,Rui Li*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的非破坏性器官样分割与追踪方法LGBP-OrgaNet，结合CNN和Transformer模块，创新性地引入可学习高斯带通融合模块，实现了高精度分割。


<details>
  <summary>Details</summary>
Motivation: 传统荧光标记方法可能破坏器官样结构，因此需要一种自动化、非破坏性的分割与追踪方法。

Method: 提出LGBP-OrgaNet模型，结合CNN和Transformer模块，引入可学习高斯带通融合模块和双向交叉融合块，实现多尺度特征融合。

Result: 在器官样分割数据集上表现出满意的分割精度和鲁棒性。

Conclusion: LGBP-OrgaNet为器官样研究提供了强有力的工具。

Abstract: Organoids replicate organ structure and function, playing a crucial role in
fields such as tumor treatment and drug screening. Their shape and size can
indicate their developmental status, but traditional fluorescence labeling
methods risk compromising their structure. Therefore, this paper proposes an
automated, non-destructive approach to organoid segmentation and tracking. We
introduced the LGBP-OrgaNet, a deep learning-based system proficient in
accurately segmenting, tracking, and quantifying organoids. The model leverages
complementary information extracted from CNN and Transformer modules and
introduces the innovative feature fusion module, Learnable Gaussian Band Pass
Fusion, to merge data from two branches. Additionally, in the decoder, the
model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,
and finally completes the decoding through progressive concatenation and
upsampling. SROrga demonstrates satisfactory segmentation accuracy and
robustness on organoids segmentation datasets, providing a potent tool for
organoid research.

</details>


### [39] [PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR](https://arxiv.org/abs/2509.03262)
*Fabio F. Oberweger,Michael Schwingshackl,Vanessa Staderini*

Main category: cs.CV

TL;DR: PI3DETR是一个端到端框架，直接从点云预测3D参数化曲线实例，避免了多阶段处理。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中多阶段处理和中间表示的问题，提高对噪声和采样密度变化的鲁棒性。

Method: 扩展3DETR，引入几何感知匹配策略和专用损失函数，统一检测多种曲线类型。

Result: 在ABC数据集上达到新SOTA，并能有效泛化到真实传感器数据。

Conclusion: PI3DETR为3D边缘和曲线估计提供了简单而强大的解决方案。

Abstract: We present PI3DETR, an end-to-end framework that directly predicts 3D
parametric curve instances from raw point clouds, avoiding the intermediate
representations and multi-stage processing common in prior work. Extending
3DETR, our model introduces a geometry-aware matching strategy and specialized
loss functions that enable unified detection of differently parameterized curve
types, including cubic B\'ezier curves, line segments, circles, and arcs, in a
single forward pass. Optional post-processing steps further refine predictions
without adding complexity. This streamlined design improves robustness to noise
and varying sampling densities, addressing critical challenges in real world
LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC
dataset and generalizes effectively to real sensor data, offering a simple yet
powerful solution for 3D edge and curve estimation.

</details>


### [40] [SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model](https://arxiv.org/abs/2509.03267)
*Hongxu Yang,Edina Timko,Levente Lippenszky,Vanda Czipczer,Lehel Ferenczi*

Main category: cs.CV

TL;DR: 提出了一种名为SynBT的3D医学扩散模型，用于在对比增强MRI图像中生成高质量乳腺肿瘤，提升了分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有肿瘤合成方法在大空间体积（如大视野MRI中的乳腺肿瘤）上表现不佳，而常用方法基于小区域生成。

Method: 采用patch-to-volume自编码器压缩高分辨率MRI到紧凑潜在空间，结合掩码条件扩散模型生成肿瘤。

Result: 在大型公开数据集上，分割模型性能提升2-3% Dice Score。

Conclusion: SynBT能生成高质量肿瘤，显著提升MRI图像中的肿瘤分割效果。

Abstract: Synthetic tumors in medical images offer controllable characteristics that
facilitate the training of machine learning models, leading to an improved
segmentation performance. However, the existing methods of tumor synthesis
yield suboptimal performances when tumor occupies a large spatial volume, such
as breast tumor segmentation in MRI with a large field-of-view (FOV), while
commonly used tumor generation methods are based on small patches. In this
paper, we propose a 3D medical diffusion model, called SynBT, to generate
high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed
model consists of a patch-to-volume autoencoder, which is able to compress the
high-resolution MRIs into compact latent space, while preserving the resolution
of volumes with large FOV. Using the obtained latent space feature vector, a
mask-conditioned diffusion model is used to synthesize breast tumors within
selected regions of breast tissue, resulting in realistic tumor appearances. We
evaluated the proposed method for a tumor segmentation task, which demonstrated
the proposed high-quality tumor synthesis method can facilitate the common
segmentation models with performance improvement of 2-3% Dice Score on a large
public dataset, and therefore provides benefits for tumor segmentation in MRI
images.

</details>


### [41] [PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection](https://arxiv.org/abs/2509.03277)
*Qihang Zhou,Shibo He,Jiangtao Yan,Wenchao Meng,Jiming Chen*

Main category: cs.CV

TL;DR: 论文提出PointAD+框架，通过结合点云和像素信息，实现3D异常检测，并引入分层表示学习和跨层次对比对齐，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 将CLIP的2D泛化能力迁移到3D异常检测中，解决跨类别语义的3D异常识别问题。

Method: 设计PointAD和PointAD+框架，分别通过隐式和显式3D表示捕捉异常，并引入分层表示学习和跨层次对比对齐。

Result: PointAD+在未见过的多样类别对象上表现出色，实现了对异常的全面理解。

Conclusion: PointAD+通过结合渲染和几何异常语义，显著提升了3D异常检测的性能和泛化能力。

Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization
capabilities to identify 3D anomalies across unseen objects of highly diverse
class semantics. To this end, we propose a unified framework to comprehensively
detect and segment 3D anomalies by leveraging both point- and pixel-level
information. We first design PointAD, which leverages point-pixel
correspondence to represent 3D anomalies through their associated rendering
pixel representations. This approach is referred to as implicit 3D
representation, as it focuses solely on rendering pixel anomalies but neglects
the inherent spatial relationships within point clouds. Then, we propose
PointAD+ to further broaden the interpretation of 3D anomalies by introducing
explicit 3D representation, emphasizing spatial abnormality to uncover abnormal
spatial relationships. Hence, we propose G-aggregation to involve geometry
information to enable the aggregated point representations spatially aware. To
simultaneously capture rendering and spatial abnormality, PointAD+ proposes
hierarchical representation learning, incorporating implicit and explicit
anomaly semantics into hierarchical text prompts: rendering prompts for the
rendering layer and geometry prompts for the geometry layer. A cross-hierarchy
contrastive alignment is further introduced to promote the interaction between
the rendering and geometry layers, facilitating mutual anomaly learning.
Finally, PointAD+ integrates anomaly semantics from both layers to capture the
generalized anomaly semantics. During the test, PointAD+ can integrate RGB
information in a plug-and-play manner and further improve its detection
performance. Extensive experiments demonstrate the superiority of PointAD+ in
ZS 3D anomaly detection across unseen objects with highly diverse class
semantics, achieving a holistic understanding of abnormality.

</details>


### [42] [Empowering Lightweight MLLMs with Reasoning via Long CoT SFT](https://arxiv.org/abs/2509.03321)
*Linyu Ou*

Main category: cs.CV

TL;DR: 研究发现，长链思维数据（long CoT）的监督微调（SFT）显著提升轻量级多模态语言模型（MLLMs）的推理能力，且后续强化学习（RL）阶段可带来额外性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索长链思维数据在提升轻量级多模态语言模型推理能力中的作用，填补现有研究空白。

Method: 采用监督微调（SFT）和后续强化学习（RL）阶段，利用长链思维数据训练轻量级多模态语言模型。

Result: SFT阶段显著提升推理能力，后续RL阶段带来额外性能增益。

Conclusion: 长链思维数据的SFT阶段是开发轻量级多模态语言模型推理能力的关键前提。

Abstract: While Reinforcement Learning with Verifiable Rewards has enhanced the
reasoning of large-scale language models (LLMs), its efficacy for lightweight
multimodal language models (MLLMs) with fewer than seven billion parameters
remains underexplored. This paper investigates the role of long
Chain-of-Thought (long CoT) data in enhancing the reasoning abilities of such
MLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT
data significantly improves MLLM reasoning. Furthermore, we observe that after
this initial SFT phase, MLLMs can achieve additional performance gains through
a subsequent RL stage. We conclude that a SFT stage with long CoT data is a
critical prerequisite for developing the reasoning capabilities of lightweight
MLLMs.

</details>


### [43] [Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions](https://arxiv.org/abs/2509.03323)
*Xizhe Zhang,Jiayang Zhu*

Main category: cs.CV

TL;DR: 提出了一种结合CNN和Transformer的混合检测器，用于自动化检测组织学图像中的星形胶质细胞，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 星形胶质细胞的复杂分支和染色依赖性变异性使得自动化检测成为一项极具挑战性的任务。

Method: 采用混合CNN-Transformer检测器，结合局部特征提取和全局上下文推理，通过热图引导的查询机制生成空间锚点，并使用轻量级Transformer模块提高密集簇中的区分能力。

Result: 在ALDH1L1和GFAP染色的星形胶质细胞数据集上，模型性能优于Faster R-CNN、YOLOv11和DETR，具有更高的敏感性和更少的假阳性。

Conclusion: 混合CNN-Transformer架构在星形胶质细胞检测中表现出潜力，为高级计算病理学工具奠定了基础。

Abstract: Astrocytes are critical glial cells whose altered morphology and density are
hallmarks of many neurological disorders. However, their intricate branching
and stain dependent variability make automated detection of histological images
a highly challenging task. To address these challenges, we propose a hybrid CNN
Transformer detector that combines local feature extraction with global
contextual reasoning. A heatmap guided query mechanism generates spatially
grounded anchors for small and faint astrocytes, while a lightweight
Transformer module improves discrimination in dense clusters. Evaluated on
ALDH1L1 and GFAP stained astrocyte datasets, the model consistently
outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with
fewer false positives, as confirmed by FROC analysis. These results highlight
the potential of hybrid CNN Transformer architectures for robust astrocyte
detection and provide a foundation for advanced computational pathology tools.

</details>


### [44] [InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds](https://arxiv.org/abs/2509.03324)
*Yixiong Jing,Cheng Zhang,Haibing Wu,Guangming Wang,Olaf Wysocki,Brian Sheil*

Main category: cs.CV

TL;DR: InfraDiffusion框架通过虚拟相机将点云转换为深度图，并利用DDNM恢复图像，无需特定任务训练即可提升砖块级分割效果。


<details>
  <summary>Details</summary>
Motivation: 在低光环境下，高分辨率RGB图像难以获取，而点云虽不受光线影响，但存在稀疏、噪声等问题，限制了细粒度分割。

Method: 使用虚拟相机将点云投影为深度图，并通过DDNM恢复图像，结合SAM进行砖块级分割。

Result: 实验表明，InfraDiffusion显著提升了砖块级分割的精度，适用于砖石结构的自动化检测。

Conclusion: InfraDiffusion为低光环境下的砖石结构检测提供了一种有效的零样本解决方案。

Abstract: Point clouds are widely used for infrastructure monitoring by providing
geometric information, where segmentation is required for downstream tasks such
as defect detection. Existing research has automated semantic segmentation of
structural components, while brick-level segmentation (identifying defects such
as spalling and mortar loss) has been primarily conducted from RGB images.
However, acquiring high-resolution images is impractical in low-light
environments like masonry tunnels. Point clouds, though robust to dim lighting,
are typically unstructured, sparse, and noisy, limiting fine-grained
segmentation. We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific
training, InfraDiffusion enhances visual clarity and geometric consistency of
depth maps. Experiments on masonry bridge and tunnel point cloud datasets show
significant improvements in brick-level segmentation using the Segment Anything
Model (SAM), underscoring its potential for automated inspection of masonry
assets. Our code and data is available at
https://github.com/Jingyixiong/InfraDiffusion-official-implement.

</details>


### [45] [Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing](https://arxiv.org/abs/2509.03376)
*Hui Chen,Liangyu Liu,Xianchao Xiu,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer和自适应图神经网络的超光谱解混框架T-CAGU，解决了全局依赖和局部一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以同时捕捉全局依赖和局部一致性，导致长程交互和边界细节难以兼顾。

Method: 结合Transformer捕捉全局依赖，引入自适应图神经网络增强局部关系，并集成多传播顺序动态学习图结构。

Result: 实验证明T-CAGU优于现有方法，且对噪声具有鲁棒性。

Conclusion: T-CAGU通过全局和局部信息的结合，显著提升了超光谱解混的性能。

Abstract: Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote
sensing images into a set of endmembers and their corresponding abundances.
Despite significant progress in this field using deep learning, most methods
fail to simultaneously characterize global dependencies and local consistency,
making it difficult to preserve both long-range interactions and boundary
details. This letter proposes a novel transformer-guided content-adaptive graph
unmixing framework (T-CAGU), which overcomes these challenges by employing a
transformer to capture global dependencies and introducing a content-adaptive
graph neural network to enhance local relationships. Unlike previous work,
T-CAGU integrates multiple propagation orders to dynamically learn the graph
structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a
graph residual mechanism to preserve global information and stabilize training.
Experimental results demonstrate its superiority over the state-of-the-art
methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.

</details>


### [46] [TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers](https://arxiv.org/abs/2509.03379)
*Guoxin Wang,Qingyuan Wang,Binhua Huang,Shaowu Chen,Deepu John*

Main category: cs.CV

TL;DR: TinyDrop是一种无需训练、轻量级的ViT令牌丢弃框架，可减少80%计算量且保持高精度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在图像分类中计算成本高，需在不牺牲精度的情况下降低推理成本。

Method: 通过轻量级视觉模型指导令牌丢弃，选择性丢弃低重要性令牌，无需修改架构。

Result: 在标准基准测试中，FLOPs减少80%，精度损失极小。

Conclusion: TinyDrop具有通用性和实用性，适用于高效ViT分类。

Abstract: Vision Transformers (ViTs) achieve strong performance in image classification
but incur high computational costs from processing all image tokens. To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model. The guidance model estimates the importance of tokens while
performing inference, thereby selectively discarding low-importance tokens if
large vit models need to perform attention calculations. The framework operates
plug-and-play, requires no architectural modifications, and is compatible with
diverse ViT architectures. Evaluations on standard image classification
benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs
with minimal accuracy degradation, highlighting its generalization capability
and practical utility for efficient ViT-based classification.

</details>


### [47] [Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation](https://arxiv.org/abs/2509.03385)
*Reina Ishikawa,Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出D-GPTScore和CC-AlignBench，用于评估概念定制任务，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过于狭窄或泛化，与人类偏好不一致，需更全面的评估标准。

Method: 分解评估标准为细粒度方面，利用多模态大语言模型进行方面评估。

Result: D-GPTScore在CC-AlignBench上表现优于现有方法，与人类偏好相关性更高。

Conclusion: 为概念定制评估设立新标准，并指出未来研究的关键挑战。

Abstract: Evaluating concept customization is challenging, as it requires a
comprehensive assessment of fidelity to generative prompts and concept images.
Moreover, evaluating multiple concepts is considerably more difficult than
evaluating a single concept, as it demands detailed assessment not only for
each individual concept but also for the interactions among concepts. While
humans can intuitively assess generated images, existing metrics often provide
either overly narrow or overly generalized evaluations, resulting in
misalignment with human preference. To address this, we propose Decomposed GPT
Score (D-GPTScore), a novel human-aligned evaluation method that decomposes
evaluation criteria into finer aspects and incorporates aspect-wise assessments
using Multimodal Large Language Model (MLLM). Additionally, we release Human
Preference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark
dataset containing both single- and multi-concept tasks, enabling stage-wise
evaluation across a wide difficulty range -- from individual actions to
multi-person interactions. Our method significantly outperforms existing
approaches on this benchmark, exhibiting higher correlation with human
preferences. This work establishes a new standard for evaluating concept
customization and highlights key challenges for future research. The benchmark
and associated materials are available at
https://github.com/ReinaIshikawa/D-GPTScore.

</details>


### [48] [Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping](https://arxiv.org/abs/2509.03408)
*Mohammed Amer,Mohamed A. Suliman,Tu Bui,Nuria Garcia,Serban Georgescu*

Main category: cs.CV

TL;DR: 提出了一种可扩展的多模态框架，用于乳腺癌分子分型，结合了多种数据源，并引入了双基WSI表示和新融合策略，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗应用多模态数据整合需求大，但临床数据模态差异大，乳腺癌分子分型是关键任务，需个性化治疗和预后改善。

Method: 提出可扩展的多模态框架，整合CNV、临床记录和组织病理图像，引入双基WSI表示和新融合策略。

Result: 双基WSI表示和融合策略显著提升性能，整合多种数据源后优于现有方法。

Conclusion: 框架灵活可扩展，适用于多种癌症，性能优越，为多模态医疗应用提供了有效解决方案。

Abstract: Healthcare applications are inherently multimodal, benefiting greatly from
the integration of diverse data sources. However, the modalities available in
clinical settings can vary across different locations and patients. A key area
that stands to gain from multimodal integration is breast cancer molecular
subtyping, an important clinical task that can facilitate personalized
treatment and improve patient prognosis. In this work, we propose a scalable
and loosely-coupled multimodal framework that seamlessly integrates data from
various modalities, including copy number variation (CNV), clinical records,
and histopathology images, to enhance breast cancer subtyping. While our
primary focus is on breast cancer, our framework is designed to easily
accommodate additional modalities, offering the flexibility to scale up or down
with minimal overhead without requiring re-training of existing modalities,
making it applicable to other types of cancers as well. We introduce a
dual-based representation for whole slide images (WSIs), combining traditional
image-based and graph-based WSI representations. This novel dual approach
results in significant performance improvements. Moreover, we present a new
multimodal fusion strategy, demonstrating its ability to enhance performance
across a range of multimodal conditions. Our comprehensive results show that
integrating our dual-based WSI representation with CNV and clinical health
records, along with our pipeline and fusion strategy, outperforms
state-of-the-art methods in breast cancer subtyping.

</details>


### [49] [Time-Scaling State-Space Models for Dense Video Captioning](https://arxiv.org/abs/2509.03426)
*AJ Piergiovanni,Ganesh Satish Mallya,Dahun Kim,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出了一种基于状态空间模型（SSMs）的新方法，用于解决密集视频描述任务中的长视频处理问题，支持在线处理并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时面临计算复杂性和内存限制，且需要完整视频输入，无法在线处理。

Method: 通过时间扩展状态空间模型（SSMs），结合其长序列和循环特性，提出带有转移状态的SSMs模型，以支持更长时间上下文。

Result: 模型适用于在线生成描述，视频长度扩展性好，计算开销减少7倍。

Conclusion: 该方法有效解决了密集视频描述中的长视频处理问题，支持在线处理并显著降低计算成本。

Abstract: Dense video captioning is a challenging video understanding task which aims
to simultaneously segment the video into a sequence of meaningful consecutive
events and to generate detailed captions to accurately describe each event.
Existing methods often encounter difficulties when working with the long videos
associated with dense video captioning, due to the computational complexity and
memory limitations. Furthermore, traditional approaches require the entire
video as input, in order to produce an answer, which precludes online
processing of the video. We address these challenges by time-scaling
State-Space Models (SSMs) to even longer sequences than before. Our approach,
State-Space Models with Transfer State, combines both the long-sequence and
recurrent properties of SSMs and addresses the main limitation of SSMs which
are otherwise not able to sustain their state for very long contexts,
effectively scaling SSMs further in time. The proposed model is particularly
suitable for generating captions on-the-fly, in an online or streaming manner,
without having to wait for the full video to be processed, which is more
beneficial in practice. When applied to dense video captioning, our approach
scales well with video lengths and uses 7x fewer FLOPs.

</details>


### [50] [Decoding Visual Neural Representations by Multimodal with Dynamic Balancing](https://arxiv.org/abs/2509.03433)
*Kaili sun,Xingyu Miao,Bing Zhai,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: 提出了一种整合EEG、图像和文本数据的创新框架，通过文本模态增强EEG信号与视觉内容的语义对应关系，并引入动态平衡策略和随机扰动正则化提升性能。


<details>
  <summary>Details</summary>
Motivation: 从低信噪比的EEG信号中解码视觉神经表征，通过文本模态增强语义对应关系，解决多模态特征贡献不平衡问题。

Method: 提出适配器模块优化高维表征，引入模态一致性动态平衡（MCDB）策略和随机扰动正则化（SPR）项。

Result: 在ThingsEEG数据集上，Top-1和Top-5准确率分别提升2.0%和4.7%，超越现有方法。

Conclusion: 该框架通过多模态融合和动态优化策略，显著提升了EEG信号解码的准确性和鲁棒性。

Abstract: In this work, we propose an innovative framework that integrates EEG, image,
and text data, aiming to decode visual neural representations from low
signal-to-noise ratio EEG signals. Specifically, we introduce text modality to
enhance the semantic correspondence between EEG signals and visual content.
With the explicit semantic labels provided by text, image and EEG features of
the same category can be more closely aligned with the corresponding text
representations in a shared multimodal space. To fully utilize pre-trained
visual and textual representations, we propose an adapter module that
alleviates the instability of high-dimensional representation while
facilitating the alignment and fusion of cross-modal features. Additionally, to
alleviate the imbalance in multimodal feature contributions introduced by the
textual representations, we propose a Modal Consistency Dynamic Balance (MCDB)
strategy that dynamically adjusts the contribution weights of each modality. We
further propose a stochastic perturbation regularization (SPR) term to enhance
the generalization ability of semantic perturbation-based models by introducing
dynamic Gaussian noise in the modality optimization process. The evaluation
results on the ThingsEEG dataset show that our method surpasses previous
state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by
2.0\% and 4.7\% respectively.

</details>


### [51] [Joint Training of Image Generator and Detector for Road Defect Detection](https://arxiv.org/abs/2509.03465)
*Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: JTGD是一种联合训练图像生成器和检测器的方法，用于道路缺陷检测，适用于边缘设备，性能优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备资源有限的问题，避免使用集成方法或测试时增强。

Method: 联合训练生成器和检测器，设计双重判别器和CLIP-based Fréchet Inception Distance损失。

Result: 在RDD2022基准测试中表现优于现有方法，参数减少80%。

Conclusion: JTGD高效且适合边缘设备部署。

Abstract: Road defect detection is important for road authorities to reduce the vehicle
damage caused by road defects. Considering the practical scenarios where the
defect detectors are typically deployed on edge devices with limited memory and
computational resource, we aim at performing road defect detection without
using ensemble-based methods or test-time augmentation (TTA). To this end, we
propose to Jointly Train the image Generator and Detector for road defect
detection (dubbed as JTGD). We design the dual discriminators for the
generative model to enforce both the synthesized defect patches and overall
images to look plausible. The synthesized image quality is improved by our
proposed CLIP-based Fr\'echet Inception Distance loss. The generative model in
JTGD is trained jointly with the detector to encourage the generative model to
synthesize harder examples for the detector. Since harder synthesized images of
better quality caused by the aforesaid design are used in the data
augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road
defect detection benchmark across various countries under the condition of no
ensemble and TTA. JTGD only uses less than 20% of the number of parameters
compared with the competing baseline, which makes it more suitable for
deployment on edge devices in practice.

</details>


### [52] [Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA](https://arxiv.org/abs/2509.03494)
*Yahya Benmahane,Mohammed El Hassouni*

Main category: cs.CV

TL;DR: 提出一种基于像素空间视觉提示的参数高效NR-IQA方法，仅训练少量参数，性能媲美全微调方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统NR-IQA方法需要大量参数调整的问题，探索MLLM在低层视觉任务中的高效适应。

Method: 通过优化像素空间视觉提示，结合图像和文本查询，仅训练600K参数（<0.01%基础模型）。

Result: 在KADID-10k等数据集上表现优异，SRCC达0.93，媲美全微调方法。

Conclusion: 首次将像素空间视觉提示用于NR-IQA，为低层视觉任务提供高效MLLM适应方案。

Abstract: In this paper, we propose a novel parameter-efficient adaptation method for
No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized
in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models
(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base
model), while keeping the underlying model fully frozen. During inference,
these visual prompts are combined with images via addition and processed by
mPLUG-Owl2 with the textual query "Rate the technical quality of the image."
Evaluations across distortion types (synthetic, realistic, AI-generated) on
KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against
full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on
KADID-10k. To our knowledge, this is the first work to leverage pixel-space
visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level
vision tasks. The source code is publicly available at https: // github. com/
yahya-ben/ mplug2-vp-for-nriqa .

</details>


### [53] [OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation](https://arxiv.org/abs/2509.03498)
*Han Li,Xinyu Peng,Yaoming Wang,Zelin Peng,Xin Chen,Rongxiang Weng,Jingang Wang,Xunliang Cai,Wenrui Dai,Hongkai Xiong*

Main category: cs.CV

TL;DR: OneCAT是一种统一的多模态模型，通过纯解码器Transformer架构实现理解、生成和编辑功能，显著提升效率并支持动态分辨率。


<details>
  <summary>Details</summary>
Motivation: 旨在消除对外部组件的依赖，提高高分辨率输入的处理效率，并通过纯自回归建模实现统一多模态智能。

Method: 采用模态特定的Mixture-of-Experts结构和多尺度视觉自回归机制，结合单一自回归目标训练。

Result: 在多项多模态任务中超越现有开源模型，性能达到新标准。

Conclusion: 纯自回归建模是统一多模态智能的优雅且高效的基础。

Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.

</details>


### [54] [DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video](https://arxiv.org/abs/2509.03499)
*Kevin Barnard,Elaine Liu,Kristine Walz,Brian Schlining,Nancy Jacobsen Stout,Lonny Lundsten*

Main category: cs.CV

TL;DR: 论文提出了一种用于评估多目标跟踪和物体检测模型性能的基准视频数据集，首次公开了深海视频的多目标跟踪基准。


<details>
  <summary>Details</summary>
Motivation: 为了在机器学习模型开发中评估模型检测和跟踪性能，需要一致的比较和性能优化工具。

Method: 开发了一个包含四个视频序列的基准数据集，评估了多个物体检测模型和跟踪器，使用Higher Order Tracking Accuracy指标。

Result: 提供了公开的基准数据、生成额外基准视频的工作流程及计算指标的Python示例。

Conclusion: 该研究填补了深海视频多目标跟踪基准的空白，为后续研究提供了工具和数据支持。

Abstract: Benchmarking multi-object tracking and object detection model performance is
an essential step in machine learning model development, as it allows
researchers to evaluate model detection and tracker performance on
human-generated 'test' data, facilitating consistent comparisons between models
and trackers and aiding performance optimization. In this study, a novel
benchmark video dataset was developed and used to assess the performance of
several Monterey Bay Aquarium Research Institute object detection models and a
FathomNet single-class object detection model together with several trackers.
The dataset consists of four video sequences representing midwater and benthic
deep-sea habitats. Performance was evaluated using Higher Order Tracking
Accuracy, a metric that balances detection, localization, and association
accuracy. To the best of our knowledge, this is the first publicly available
benchmark for multi-object tracking in deep-sea video footage. We provide the
benchmark data, a clearly documented workflow for generating additional
benchmark videos, as well as example Python notebooks for computing metrics.

</details>


### [55] [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501)
*Honglu Zhou,Xiangyu Peng,Shrikant Kendre,Michael S. Ryoo,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: Strefer是一个合成指令数据生成框架，旨在增强视频大语言模型（Video LLMs）的时空参考和推理能力，通过伪注释密集的时空视频元数据，提升模型在空间和时间解歧任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在细粒度时空推理上表现不足，尤其是在依赖时间事件参考或手势线索的查询中。Strefer旨在填补这一关键空白。

Method: Strefer通过数据引擎生成多样化的指令调优数据，伪注释视频中的时空信息（如主体、对象、位置掩码、动作描述和时间线），以结构化方式捕获丰富信息。

Result: 实验表明，使用Strefer生成数据训练的模型在时空解歧任务上优于基线，并展现出更强的时空感知推理能力。

Conclusion: Strefer为基于感知的指令调优视频大语言模型奠定了新基础，无需依赖专有模型或昂贵的人工标注。

Abstract: Next-generation AI companions must go beyond general video understanding to
resolve spatial and temporal references in dynamic, real-world environments.
Existing Video Large Language Models (Video LLMs), while capable of
coarse-level comprehension, struggle with fine-grained, spatiotemporal
reasoning, especially when user queries rely on time-based event references for
temporal anchoring, or gestural cues for spatial anchoring to clarify object
references and positions. To bridge this critical gap, we introduce Strefer, a
synthetic instruction data generation framework designed to equip Video LLMs
with spatiotemporal referring and reasoning capabilities. Strefer produces
diverse instruction-tuning data using a data engine that pseudo-annotates
temporally dense, fine-grained video metadata, capturing rich spatial and
temporal information in a structured manner, including subjects, objects, their
locations as masklets, and their action descriptions and timelines. Our
approach enhances the ability of Video LLMs to interpret spatial and temporal
references, fostering more versatile, space-time-aware reasoning essential for
real-world AI companions. Without using proprietary models, costly human
annotation, or the need to annotate large volumes of new videos, experimental
evaluations show that models trained with data produced by Strefer outperform
baselines on tasks requiring spatial and temporal disambiguation. Additionally,
these models exhibit enhanced space-time-aware reasoning, establishing a new
foundation for perceptually grounded, instruction-tuned Video LLMs.

</details>


### [56] [A comprehensive Persian offline handwritten database for investigating the effects of heritability and family relationships on handwriting](https://arxiv.org/abs/2509.03510)
*Abbas Zohrevand,Javad Sadri,Zahra Imani*

Main category: cs.CV

TL;DR: 论文介绍了一个用于研究遗传对笔迹影响的综合数据库，包含210个家庭的笔迹样本，并分析了家庭成员间笔迹的相似性。


<details>
  <summary>Details</summary>
Motivation: 研究遗传和家庭关系对笔迹的影响，填补现有数据库的空白。

Method: 收集210个家庭成员的笔迹样本（数字、字母、形状和自由段落），设计专用表格记录家庭关系，并进行特征比较。

Result: 发现家庭成员间笔迹特征和书写风格存在相似性。

Conclusion: 数据库免费提供给模式识别社区，为研究遗传和家庭关系对笔迹的影响提供基础。

Abstract: This paper introduces a comprehensive database for research and investigation
on the effects of inheritance on handwriting. A database has been created that
can be used to answer questions such as: Is there a genetic component to
handwriting? Is handwriting inherited? Do family relationships affect
handwriting? Varieties of samples of handwritten components such as: digits,
letters, shapes and free paragraphs of 210 families including (grandparents,
parents, uncles, aunts, siblings, cousins, nephews and nieces) have been
collected using specially designed forms, and family relationships of all
writers are captured. To the best of our knowledge, no such database is
presently available. Based on comparisons and investigation of features of
handwritings of family members, similarities among their features and writing
styles are detected. Our database is freely available to the pattern
recognition community and hope it will pave the way for investigations on the
effects of inheritance and family relationships on handwritings.

</details>


### [57] [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](https://arxiv.org/abs/2509.03516)
*Ouxiang Li,Yuan Wang,Xinting Hu,Huijuan Huang,Rui Chen,Jiarong Ou,Xin Tao,Pengfei Wan,Fuli Feng*

Main category: cs.CV

TL;DR: T2I-CoReBench是一个新的文本到图像生成基准测试，旨在全面评估模型的组合和推理能力，通过高密度场景和多步推理的复杂提示。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估文本到图像模型的组合和推理能力方面存在局限性，无法应对复杂场景和多步推理的需求。

Method: 提出T2I-CoReBench，基于场景图元素和哲学推理框架构建12维评估分类，并设计高密度和多步推理的提示。

Result: 实验表明，现有模型在复杂高密度场景中的组合能力有限，推理能力更是瓶颈，难以从提示中推断隐含元素。

Conclusion: T2I-CoReBench为文本到图像模型提供了更全面的评估工具，揭示了组合和推理能力的不足，为未来研究指明了方向。

Abstract: Text-to-image (T2I) generation aims to synthesize images from textual
prompts, which jointly specify what must be shown and imply what can be
inferred, thereby corresponding to two core capabilities: composition and
reasoning. However, with the emerging advances of T2I models in reasoning
beyond composition, existing benchmarks reveal clear limitations in providing
comprehensive evaluations across and within these capabilities. Meanwhile,
these advances also enable models to handle more complex prompts, whereas
current benchmarks remain limited to low scene density and simplified
one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a
comprehensive and complex benchmark that evaluates both composition and
reasoning capabilities of T2I models. To ensure comprehensiveness, we structure
composition around scene graph elements (instance, attribute, and relation) and
reasoning around the philosophical framework of inference (deductive,
inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To
increase complexity, driven by the inherent complexities of real-world
scenarios, we curate each prompt with high compositional density for
composition and multi-step inference for reasoning. We also pair each prompt
with a checklist that specifies individual yes/no questions to assess each
intended element independently to facilitate fine-grained and reliable
evaluation. In statistics, our benchmark comprises 1,080 challenging prompts
and around 13,500 checklist questions. Experiments across 27 current T2I models
reveal that their composition capability still remains limited in complex
high-density scenarios, while the reasoning capability lags even further behind
as a critical bottleneck, with all models struggling to infer implicit elements
from prompts. Our project page: https://t2i-corebench.github.io/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory](https://arxiv.org/abs/2509.02575)
*Zichuan Yang*

Main category: cs.LG

TL;DR: 提出了一种基于状态记忆的长期神经元激活机制（Lifecycle原则），解决了传统Dropout方法中神经元复活时训练不稳定的问题，提升了模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统Dropout方法通过临时关闭神经元实现正则化，但神经元长期关闭后复活时随机初始化会导致训练不稳定。本文旨在解决这一问题。

Method: 提出Lifecycle（LC）原则，通过状态记忆机制在神经元复活时恢复其上次有效的参数状态，避免随机初始化带来的破坏性优化冲击。

Result: 理论分析表明LC原则平滑了损失景观，引导优化趋向更平坦的极小值；实验证明其在图像分类任务中提升了泛化性和鲁棒性。

Conclusion: 状态记忆是LC原则的核心创新，对提升模型性能至关重要。

Abstract: I investigate a stronger form of regularization by deactivating neurons for
extended periods, a departure from the temporary changes of methods like
Dropout. However, this long-term dynamism introduces a critical challenge:
severe training instability when neurons are revived with random weights. To
solve this, I propose the Lifecycle (LC) principle, a regularization mechanism
centered on a key innovation: state memory. Instead of re-initializing a
revived neuron, my method restores its parameters to their last known effective
state. This process preserves learned knowledge and avoids destructive
optimization shocks. My theoretical analysis reveals that the LC principle
smooths the loss landscape, guiding optimization towards flatter minima
associated with better generalization. Experiments on image classification
benchmarks demonstrate that my method improves generalization and robustness.
Crucially, ablation studies confirm that state memory is essential for
achieving these gains.

</details>


### [59] [Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection](https://arxiv.org/abs/2509.02579)
*Mazyar Taghavi,Rahman Farnoosh*

Main category: cs.LG

TL;DR: 论文提出了一种基于EM的隐变量建模方法，用于多智能体强化学习（MARL）中无人机协调保护濒危野生动物，展示了在复杂环境中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 保护濒危野生动物免受非法偷猎的挑战在于广阔且部分可观测的环境需要实时响应。

Method: 采用期望最大化（EM）隐变量建模方法，结合多智能体强化学习（MARL），用于无人机协调巡逻。

Result: 实验表明，该方法在检测准确性、适应性和策略收敛性上优于PPO和DDPG等标准算法。

Conclusion: 结合EM推理与MARL可提升复杂高风险的野生动物保护场景中的分散决策能力。

Abstract: Protecting endangered wildlife from illegal poaching presents a critical
challenge, particularly in vast and partially observable environments where
real-time response is essential. This paper introduces a novel
Expectation-Maximization (EM) based latent variable modeling approach in the
context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial
Vehicle (UAV) coordination in wildlife protection. By modeling hidden
environmental factors and inter-agent dynamics through latent variables, our
method enhances exploration and coordination under uncertainty.We implement and
evaluate our EM-MARL framework using a custom simulation involving 10 UAVs
tasked with patrolling protected habitats of the endangered Iranian leopard.
Extensive experimental results demonstrate superior performance in detection
accuracy, adaptability, and policy convergence when compared to standard
algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic
Policy Gradient (DDPG). Our findings underscore the potential of combining EM
inference with MARL to improve decentralized decisionmaking in complex,
high-stakes conservation scenarios. The full implementation, simulation
environment, and training scripts are publicly available on GitHub.

</details>


### [60] [Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning](https://arxiv.org/abs/2509.02592)
*Hunter Gittlin*

Main category: cs.LG

TL;DR: 论文提出了一种基于群体感知的阈值校准方法，通过为不同群体设置不同决策阈值，解决了传统方法在类别不平衡问题中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如SMOTE和CT-GAN）在解决类别不平衡问题时可能引入新问题，且效果有限。

Method: 采用群体感知的阈值校准，为不同群体设置不同决策阈值，优化平衡准确率和最差群体平衡准确率的帕累托前沿。

Result: 实验表明，该方法比SMOTE和CT-GAN增强模型提高1.5-4%的平衡准确率，且对合成数据的额外增益有限。

Conclusion: 群体感知阈值校准是一种更简单、可解释且有效的类别不平衡解决方案。

Abstract: Class imbalance remains a fundamental challenge in machine learning, with
traditional solutions often creating as many problems as they solve. We
demonstrate that group-aware threshold calibration--setting different decision
thresholds for different demographic groups--provides superior robustness
compared to synthetic data generation methods. Through extensive experiments,
we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy
than SMOTE and CT-GAN augmented models while improving worst-group balanced
accuracy. Unlike single-threshold approaches that apply one cutoff across all
groups, our group-aware method optimizes the Pareto frontier between balanced
accuracy and worst-group balanced accuracy, enabling fine-grained control over
group-level performance. Critically, we find that applying group thresholds to
synthetically augmented data yields minimal additional benefit, suggesting
these approaches are fundamentally redundant. Our results span seven model
families including linear, tree-based, instance-based, and boosting methods,
confirming that group-aware threshold calibration offers a simpler, more
interpretable, and more effective solution to class imbalance.

</details>


### [61] [Preference Robustness for DPO with Applications to Public Health](https://arxiv.org/abs/2509.02709)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: DPO-PRO是一种基于直接偏好优化（DPO）的鲁棒微调算法，用于设计公共健康领域中的奖励函数，通过轻量级分布鲁棒优化（DRO）处理偏好不确定性，相比现有方法更高效且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 公共健康领域的顺序资源分配问题具有复杂、模糊的目标和有限的数据可用性，需要一种鲁棒的奖励函数设计方法。

Method: 提出DPO-PRO算法，结合DPO和轻量级DRO，减少保守性，处理偏好不确定性。

Result: 在真实世界母婴移动健康项目和标准基准测试中，DPO-PRO对噪声偏好信号表现更鲁棒，且推理成本显著低于基线方法。

Conclusion: DPO-PRO在鲁棒性和效率上优于现有方法，适用于复杂且数据有限的公共健康任务。

Abstract: We study an LLM fine-tuning task for designing reward functions for
sequential resource allocation problems in public health, guided by human
preferences expressed in natural language. This setting presents a challenging
testbed for alignment due to complex and ambiguous objectives and limited data
availability. We propose DPO-PRO, a robust fine-tuning algorithm based on
Direct Preference Optimization (DPO), which accounts for uncertainty in the
preference distribution using a lightweight Distributionally Robust
Optimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is
significantly less conservative. We evaluate DPO-PRO on a real-world maternal
mobile health program operated by the non-profit organization ARMMAN, as well
as on standard alignment benchmarks. Experimental results demonstrate that our
method consistently improves robustness to noisy preference signals compared to
existing DPO variants. Moreover, DPO-PRO achieves comparable performance to
prior self-reflection-based baseline for reward function design, while
requiring significantly lower inference-time cost.

</details>


### [62] [Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient](https://arxiv.org/abs/2509.02737)
*Zhongzhu Zhou,Yibo Yang,Ziyan Chen,Fengxiang Bie,Haojun Xia,Xiaoxia Wu,Robert Wu,Ben Athiwaratkun,Bernard Ghanem,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: 论文提出了一种称为Action Collapse (AC)的现象，即在特定约束下，策略梯度方法中的深度神经网络会表现出类似于神经崩溃的结构。基于此，作者提出了Action Collapse Policy Gradient (ACPG)方法，通过固定动作选择层的合成ETF结构来优化策略学习。


<details>
  <summary>Details</summary>
Motivation: 研究动机是分析策略梯度方法中深度神经网络的表示结构，并探索如何利用Action Collapse现象来优化策略学习。

Method: 方法包括观察AC现象，提出ACPG方法，固定动作选择层的ETF结构，并在实验中验证其有效性。

Result: 实验结果表明，ACPG可以快速且稳健地提升奖励，适用于各种离散策略梯度方法。

Conclusion: 结论是ACPG通过固定ETF结构，能够有效诱导策略网络产生理想的配置，从而优化策略学习。

Abstract: Policy gradient (PG) methods in reinforcement learning frequently utilize
deep neural networks (DNNs) to learn a shared backbone of feature
representations used to compute likelihoods in an action selection layer.
Numerous studies have been conducted on the convergence and global optima of
policy networks, but few have analyzed representational structures of those
underlying networks. While training an optimal policy DNN, we observed that
under certain constraints, a gentle structure resembling neural collapse, which
we refer to as Action Collapse (AC), emerges. This suggests that 1) the
state-action activations (i.e. last-layer features) sharing the same optimal
actions collapse towards those optimal actions respective mean activations; 2)
the variability of activations sharing the same optimal actions converges to
zero; 3) the weights of action selection layer and the mean activations
collapse to a simplex equiangular tight frame (ETF). Our early work showed
those aforementioned constraints to be necessary for these observations. Since
the collapsed ETF of optimal policy DNNs maximally separates the pair-wise
angles of all actions in the state-action space, we naturally raise a question:
can we learn an optimal policy using an ETF structure as a (fixed) target
configuration in the action selection layer? Our analytical proof shows that
learning activations with a fixed ETF as action selection layer naturally leads
to the AC. We thus propose the Action Collapse Policy Gradient (ACPG) method,
which accordingly affixes a synthetic ETF as our action selection layer. ACPG
induces the policy DNN to produce such an ideal configuration in the action
selection layer while remaining optimal. Our experiments across various OpenAI
Gym environments demonstrate that our technique can be integrated into any
discrete PG methods and lead to favorable reward improvements more quickly and
robustly.

</details>


### [63] [Mentality: A Mamba-based Approach towards Foundation Models for EEG](https://arxiv.org/abs/2509.02746)
*Saarang Panchavati,Corey Arnold,William Speier*

Main category: cs.LG

TL;DR: 该研究探讨了基于Mamba的选择性状态空间模型在EEG分析中的潜力，用于神经疾病诊断，通过自监督重建和癫痫检测任务，模型在测试集上达到0.72的AUROC。


<details>
  <summary>Details</summary>
Motivation: EEG数据噪声大、高维且非线性，传统机器学习方法难以捕捉其复杂的时空动态，而深度学习序列建模提供了新途径。

Method: 使用Mamba模型在大规模EEG数据集上进行自监督重建任务，随后进行癫痫检测任务。

Result: 模型在测试集上达到0.72的AUROC，显示出其在EEG分析中的有效性。

Conclusion: 该方法为开发大规模、临床适用的EEG基础模型迈出了重要一步。

Abstract: This work explores the potential of foundation models, specifically a
Mamba-based selective state space model, for enhancing EEG analysis in
neurological disorder diagnosis. EEG, crucial for diagnosing conditions like
epilepsy, presents significant challenges due to its noisy, high-dimensional,
and nonlinear nature. Traditional machine learning methods have made advances
in automating EEG analysis but often fail to capture its complex
spatio-temporal dynamics. Recent advances in deep learning, particularly in
sequence modeling, offer new avenues for creating more generalized and
expressive models capable of handling such complexities. By training a
Mamba-based model on a large dataset containing seizure and non-seizure EEG
recordings through a self-supervised reconstruction task followed by a seizure
detection task, we demonstrate the model's effectiveness, achieving an AUROC of
0.72 on a held-out test set. This approach marks a significant step toward
developing large-scale, clinically applicable foundation models for EEG data
analysis.

</details>


### [64] [LExI: Layer-Adaptive Active Experts for Efficient MoE Model Inference](https://arxiv.org/abs/2509.02753)
*Krishna Teja Chitty-Venkata,Sandeep Madireddy,Murali Emani,Venkatram Vishwanath*

Main category: cs.LG

TL;DR: LExI是一种数据无关的优化技术，通过动态调整每层激活的专家数量，显著提升了MoE模型的推理效率，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的固定专家激活策略导致冗余计算和性能不佳，而传统剪枝方法仅优化内存占用，对推理效率提升有限。

Method: LExI利用模型权重估计每层的重要性，并自适应分配每层激活的专家数量。

Result: 实验表明，LExI在推理效率上显著优于传统剪枝方法，且精度损失可忽略。例如，Qwen1.5-MoE在H100 GPU上吞吐量相同，但精度提高10%。

Conclusion: LExI为MoE模型提供了一种高效且轻量级的优化方案，适用于语言和视觉任务。

Abstract: Mixture-of-Experts (MoE) models scale efficiently by activating only a subset
of experts per token, offering a computationally sparse alternative to dense
architectures. While prior post-training optimizations, such as inter- and
intra-expert pruning, reduce memory usage they provide limited gains in
inference-time compute efficiency. Moreover, existing MoE architectures
typically activate a fixed number of experts uniformly across all layers,
resulting in redundant computation and suboptimal performance. In this work, we
first demonstrate that MoE pruning strategies improve only the memory footprint
but do not significantly improve inference performance on GPU using optimized
frameworks such as vLLM. To address this, we introduce LExI, a data-free
optimization technique that determines the optimal number of active experts per
layer in a pretrained MoE model. LExI leverages only the model weights to
estimate the relative importance of each layer and adaptively assigns the
number of active experts accordingly per layer. Experiments on state-of-the-art
language and vision MoE benchmarks demonstrate that LExI significantly
outperforms traditional MoE pruning approaches in terms of inference efficiency
with negligible accuracy loss. For example, using LExI, Qwen1.5-MoE achieves
the same throughput on Nvidia H100 GPU with 10% better accuracy than
traditional expert pruning.

</details>


### [65] [The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface](https://arxiv.org/abs/2509.02783)
*Arnab Mazumder,Javier E. Santos,Noah Hobbs,Mohamed Mehana,Daniel O'Malley*

Main category: cs.LG

TL;DR: Transparent Earth是一种基于Transformer的架构，用于从稀疏、分辨率和模态各异的数据中重建地下属性，支持多模态学习和上下文预测。


<details>
  <summary>Details</summary>
Motivation: 现有的地下属性重建方法难以处理多模态、稀疏和分辨率各异的数据，因此需要一种能够灵活整合不同观测类型的模型。

Method: 模型结合位置编码和模态编码（通过文本嵌入模型生成），支持任意数量的模态，并支持上下文学习。

Result: 在验证数据上，预测应力角的误差减少了三倍以上，且模型性能随参数增加而提升。

Conclusion: Transparent Earth是地下属性的基础模型，旨在预测地球上任何地方的地下属性。

Abstract: We present the Transparent Earth, a transformer-based architecture for
reconstructing subsurface properties from heterogeneous datasets that vary in
sparsity, resolution, and modality, where each modality represents a distinct
type of observation (e.g., stress angle, mantle temperature, tectonic plate
type). The model incorporates positional encodings of observations together
with modality encodings, derived from a text embedding model applied to a
description of each modality. This design enables the model to scale to an
arbitrary number of modalities, making it straightforward to add new ones not
considered in the initial design. We currently include eight modalities
spanning directional angles, categorical classes, and continuous properties
such as temperature and thickness. These capabilities support in-context
learning, enabling the model to generate predictions either with no inputs or
with an arbitrary number of additional observations from any subset of
modalities. On validation data, this reduces errors in predicting stress angle
by more than a factor of three. The proposed architecture is scalable and
demonstrates improved performance with increased parameters. Together, these
advances make the Transparent Earth an initial foundation model for the Earth's
subsurface that ultimately aims to predict any subsurface property anywhere on
Earth.

</details>


### [66] [Structured Basis Function Networks: Loss-Centric Multi-Hypothesis Ensembles with Controllable Diversity](https://arxiv.org/abs/2509.02792)
*Alejandro Rodriguez Dominguez,Muhammad Shahzad,Xia Hong*

Main category: cs.LG

TL;DR: 提出了一种统一框架Structured Basis Function Network，结合多假设预测和集成学习，通过Bregman散度实现中心聚合，解决预测不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，多假设预测缺乏聚合原则，而集成学习难以捕捉结构化模糊性，缺乏统一框架。

Method: 通过Bregman散度实现中心聚合，支持回归和分类任务，提供闭式最小二乘估计和梯度优化方法。

Result: 实验验证了该方法在多数据集上的有效性，并研究了复杂度-容量-多样性的权衡。

Conclusion: 该方法统一了多假设预测和集成学习，提供了可调多样性机制，适用于不同难度的数据集。

Abstract: Existing approaches to predictive uncertainty rely either on multi-hypothesis
prediction, which promotes diversity but lacks principled aggregation, or on
ensemble learning, which improves accuracy but rarely captures the structured
ambiguity. This implicitly means that a unified framework consistent with the
loss geometry remains absent. The Structured Basis Function Network addresses
this gap by linking multi-hypothesis prediction and ensembling through
centroidal aggregation induced by Bregman divergences. The formulation applies
across regression and classification by aligning predictions with the geometry
of the loss, and supports both a closed-form least-squares estimator and a
gradient-based procedure for general objectives. A tunable diversity mechanism
provides parametric control of the bias-variance-diversity trade-off,
connecting multi-hypothesis generalisation with loss-aware ensemble
aggregation. Experiments validate this relation and use the mechanism to study
the complexity-capacity-diversity trade-off across datasets of increasing
difficulty with deep-learning predictors.

</details>


### [67] [Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks](https://arxiv.org/abs/2509.02803)
*Howard Dai,Nyambura Njenga,Benjamin Whitsett,Catherine Ma,Darwin Deng,Sara de Ángel,Alexandre Van Tassel,Siddharth Viswanath,Ryan Pellico,Ian Adelstein,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: 提出了一种通过归纳学习拉普拉斯特征向量预训练图神经网络的新框架，解决了传统MPNN因深度增加导致的过平滑问题，并在多种图结构任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统MPNN因网络深度增加容易过平滑，难以捕捉全局和区域图结构。拉普拉斯矩阵的低频特征向量编码了全局信息，预训练GNN预测这些特征向量可帮助网络学习大尺度结构模式。

Method: 通过预训练GNN预测拉普拉斯矩阵的低频特征向量，学习全局结构模式。框架是自监督的，适用于所有图数据集，且支持合成特征。

Result: 实验表明，该框架预训练的模型在多种图结构任务中优于基线模型。

Conclusion: 该框架灵活且通用，适用于稀疏任务数据，为图结构学习提供了新思路。

Abstract: We propose a novel framework for pre-training Graph Neural Networks (GNNs) by
inductively learning Laplacian eigenvectors. Traditional Message Passing Neural
Networks (MPNNs) often struggle to capture global and regional graph structure
due to over-smoothing risk as network depth increases. Because the
low-frequency eigenvectors of the graph Laplacian matrix encode global
information, pre-training GNNs to predict these eigenvectors encourages the
network to naturally learn large-scale structural patterns over each graph.
Empirically, we show that models pre-trained via our framework outperform
baseline models on a variety of graph structure-based tasks. While most
existing pre-training methods focus on domain-specific tasks like node or edge
feature reconstruction, our self-supervised pre-training framework is
structure-based and highly flexible. Eigenvector-learning can be applied to all
graph-based datasets, and can be used with synthetic features when
task-specific data is sparse.

</details>


### [68] [Challenges in Understanding Modality Conflict in Vision-Language Models](https://arxiv.org/abs/2509.02805)
*Trang Nguyen,Jackson Michaels,Madalina Fiterau,David Jensen*

Main category: cs.LG

TL;DR: 论文探讨了视觉语言模型（VLMs）中冲突检测与冲突解决的分离问题，提出了线性探测和基于组的注意力模式分析方法。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs中冲突检测与解决的功能分离，以提升模型在复杂多模态环境中的鲁棒性。

Method: 通过线性探测和注意力模式分析，对LLaVA-OV-7B模型进行机制性研究。

Result: 发现中间层存在线性可解码的冲突信号，且冲突检测与解决的注意力模式在网络不同阶段分离。

Conclusion: 冲突检测与解决是功能不同的机制，这种分解有助于提升模型的可解释性和鲁棒性。

Abstract: This paper highlights the challenge of decomposing conflict detection from
conflict resolution in Vision-Language Models (VLMs) and presents potential
approaches, including using a supervised metric via linear probes and
group-based attention pattern analysis. We conduct a mechanistic investigation
of LLaVA-OV-7B, a state-of-the-art VLM that exhibits diverse resolution
behaviors when faced with conflicting multimodal inputs. Our results show that
a linearly decodable conflict signal emerges in the model's intermediate layers
and that attention patterns associated with conflict detection and resolution
diverge at different stages of the network. These findings support the
hypothesis that detection and resolution are functionally distinct mechanisms.
We discuss how such decomposition enables more actionable interpretability and
targeted interventions for improving model robustness in challenging multimodal
settings.

</details>


### [69] [Unlearning That Lasts: Utility-Preserving, Robust, and Almost Irreversible Forgetting in LLMs](https://arxiv.org/abs/2509.02820)
*Naman Deep Singh,Maximilian Müller,Francesco Croce,Matthias Hein*

Main category: cs.LG

TL;DR: JensUn利用Jensen-Shannon Divergence作为训练目标，在LLM中实现更稳定的遗忘动态，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法在彻底评估中表现不佳，需改进以确保LLM安全删除私有或有害信息。

Method: 引入JensUn，使用Jensen-Shannon Divergence作为训练目标，优化遗忘和保留集的动态。

Result: JensUn在遗忘-效用权衡上优于其他方法，且对良性再学习具有强韧性。

Conclusion: JensUn和新的评估框架（LKF数据集和LLM语义判断）显著提升了遗忘效果和评估准确性。

Abstract: Unlearning in large language models (LLMs) involves precisely removing
specific information from a pre-trained model. This is crucial to ensure safety
of LLMs by deleting private data or harmful knowledge acquired during
pre-training. However, existing unlearning methods often fall short when
subjected to thorough evaluation. To overcome this, we introduce JensUn, where
we leverage the Jensen-Shannon Divergence as the training objective for both
forget and retain sets for more stable and effective unlearning dynamics
compared to commonly used loss functions. In extensive experiments, JensUn
achieves better forget-utility trade-off than competing methods, and even
demonstrates strong resilience to benign relearning. Additionally, for a
precise unlearning evaluation, we introduce LKF, a curated dataset of
lesser-known facts that provides a realistic unlearning scenario. Finally, to
comprehensively test unlearning methods, we propose (i) employing an LLM as
semantic judge instead of the standard ROUGE score, and (ii) using worst-case
unlearning evaluation over various paraphrases and input formats. Our improved
evaluation framework reveals that many existing methods are less effective than
previously thought.

</details>


### [70] [Ensemble Learning for Healthcare: A Comparative Analysis of Hybrid Voting and Ensemble Stacking in Obesity Risk Prediction](https://arxiv.org/abs/2509.02826)
*Towhidul Islam,Md Sumon Ali*

Main category: cs.LG

TL;DR: 比较混合多数投票和集成堆叠方法在肥胖风险预测中的表现，发现集成堆叠在复杂数据分布中表现更优。


<details>
  <summary>Details</summary>
Motivation: 肥胖是全球健康问题，机器学习可用于早期风险预测，但缺乏对集成技术的比较研究。

Method: 使用两种数据集评估三种集成模型：多数硬投票、加权硬投票和堆叠（以多层感知机为元分类器）。

Result: 堆叠方法在复杂数据中表现最佳，加权硬投票和多数硬投票次之。

Conclusion: 集成堆叠在肥胖风险预测中具有更强的预测能力，混合多数投票是稳健的替代方案。

Abstract: Obesity is a critical global health issue driven by dietary, physiological,
and environmental factors, and is strongly associated with chronic diseases
such as diabetes, cardiovascular disorders, and cancer. Machine learning has
emerged as a promising approach for early obesity risk prediction, yet a
comparative evaluation of ensemble techniques -- particularly hybrid majority
voting and ensemble stacking -- remains limited. This study aims to compare
hybrid majority voting and ensemble stacking methods for obesity risk
prediction, identifying which approach delivers higher accuracy and efficiency.
The analysis seeks to highlight the complementary strengths of these ensemble
techniques in guiding better predictive model selection for healthcare
applications. Two datasets were utilized to evaluate three ensemble models:
Majority Hard Voting, Weighted Hard Voting, and Stacking (with a Multi-Layer
Perceptron as meta-classifier). A pool of nine Machine Learning (ML)
algorithms, evaluated across a total of 50 hyperparameter configurations, was
analyzed to identify the top three models to serve as base learners for the
ensemble methods. Preprocessing steps involved dataset balancing, and outlier
detection, and model performance was evaluated using Accuracy and F1-Score. On
Dataset-1, weighted hard voting and stacking achieved nearly identical
performance (Accuracy: 0.920304, F1: 0.920070), outperforming majority hard
voting. On Dataset-2, stacking demonstrated superior results (Accuracy:
0.989837, F1: 0.989825) compared to majority hard voting (Accuracy: 0.981707,
F1: 0.981675) and weighted hard voting, which showed the lowest performance.
The findings confirm that ensemble stacking provides stronger predictive
capability, particularly for complex data distributions, while hybrid majority
voting remains a robust alternative.

</details>


### [71] [Conformal Prediction for Time-series Forecasting with Change Points](https://arxiv.org/abs/2509.02844)
*Sophia Sun,Rose Yu*

Main category: cs.LG

TL;DR: 提出了一种新的CPTC算法，用于处理带有变化点的时间序列数据，提高了不确定性的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理带有变化点的时间序列数据，导致不确定性量化效果不佳。

Method: 结合状态预测模型和在线共形预测，处理非平稳时间序列中的不确定性。

Result: 在6个合成和真实数据集上验证了CPTC的有效性和适应性，优于现有基线方法。

Conclusion: CPTC算法在时间序列设置下具有理论保证和实际优势，适用于非平稳数据。

Abstract: Conformal prediction has been explored as a general and efficient way to
provide uncertainty quantification for time series. However, current methods
struggle to handle time series data with change points - sudden shifts in the
underlying data-generating process. In this paper, we propose a novel Conformal
Prediction for Time-series with Change points (CPTC) algorithm, addressing this
gap by integrating a model to predict the underlying state with online
conformal prediction to model uncertainties in non-stationary time series. We
prove CPTC's validity and improved adaptivity in the time series setting under
minimum assumptions, and demonstrate CPTC's practical effectiveness on 6
synthetic and real-world datasets, showing improved validity and adaptivity
compared to state-of-the-art baselines.

</details>


### [72] [Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm](https://arxiv.org/abs/2509.02846)
*Siddharth Mansingh,James Amarel,Ragib Arnab,Arvind Mohan,Kamaljeet Singh,Gerd J. Kunde,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Nathan A. Debarledeben,Ayan Biswas,Diane Oyen,Earl Lawrence*

Main category: cs.LG

TL;DR: 提出了一种基于测试时计算（TTC）的策略，用于提高PDE模型的预测准确性，减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有PDE模型受限于预训练数据，在OOD情况下表现不佳，且计算和训练数据需求高。

Method: 引入TTC策略，利用推理时的计算资源，结合两种奖励模型评估时空一致性。

Result: 在PDEGym基准测试中，TTC比标准非自适应自回归推理表现更好。

Conclusion: TTC框架为PDE建模提供了新思路，可能推动物理和工程计算工作流的变革。

Abstract: Partial Differential Equations (PDEs) are the bedrock for modern
computational sciences and engineering, and inherently computationally
expensive. While PDE foundation models have shown much promise for simulating
such complex spatio-temporal phenomena, existing models remain constrained by
the pretraining datasets and struggle with auto-regressive rollout performance,
especially in out-of-distribution (OOD) cases. Furthermore, they have
significant compute and training data requirements which hamper their use in
many critical applications. Inspired by recent advances in ``thinking"
strategies used in large language models (LLMs), we introduce the first
test-time computing (TTC) strategy for PDEs that utilizes computational
resources during inference to achieve more accurate predictions with fewer
training samples and smaller models. We accomplish this with two types of
reward models that evaluate predictions of a stochastic based model for
spatio-temporal consistency. We demonstrate this method on compressible
Euler-equation simulations from the PDEGym benchmark and show that TTC captures
improved predictions relative to standard non-adaptive auto-regressive
inference. This TTC framework marks a foundational step towards more advanced
reasoning algorithms or PDE modeling, inluding building
reinforcement-learning-based approaches, potentially transforming computational
workflows in physics and engineering.

</details>


### [73] [Power Grid Control with Graph-Based Distributed Reinforcement Learning](https://arxiv.org/abs/2509.02861)
*Carlo Fabrizio,Gianvito Losapio,Marco Mussi,Alberto Maria Metelli,Marcello Restelli*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的分布式强化学习框架，用于实时、可扩展的电网管理，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统电网控制方法难以适应大规模可再生能源的集成，需要更动态和分布式的控制策略。

Method: 采用分布式低层代理和高层管理代理的架构，结合图神经网络和模仿学习，分解动作和观测空间。

Result: 在Grid2Op仿真环境中表现优于基线方法，计算效率更高。

Conclusion: 该方法为大规模电网管理提供了高效、可扩展的解决方案。

Abstract: The necessary integration of renewable energy sources, combined with the
expanding scale of power networks, presents significant challenges in
controlling modern power grids. Traditional control systems, which are human
and optimization-based, struggle to adapt and to scale in such an evolving
context, motivating the exploration of more dynamic and distributed control
strategies. This work advances a graph-based distributed reinforcement learning
framework for real-time, scalable grid management. The proposed architecture
consists of a network of distributed low-level agents acting on individual
power lines and coordinated by a high-level manager agent. A Graph Neural
Network (GNN) is employed to encode the network's topological information
within the single low-level agent's observation. To accelerate convergence and
enhance learning stability, the framework integrates imitation learning and
potential-based reward shaping. In contrast to conventional decentralized
approaches that decompose only the action space while relying on global
observations, this method also decomposes the observation space. Each low-level
agent acts based on a structured and informative local view of the environment
constructed through the GNN. Experiments on the Grid2Op simulation environment
show the effectiveness of the approach, which consistently outperforms the
standard baseline commonly adopted in the field. Additionally, the proposed
model proves to be much more computationally efficient than the
simulation-based Expert method.

</details>


### [74] [Enhancing Machine Learning for Imbalanced Medical Data: A Quantum-Inspired Approach to Synthetic Oversampling (QI-SMOTE)](https://arxiv.org/abs/2509.02863)
*Vikas Kashtriya,Pardeep Singh*

Main category: cs.LG

TL;DR: QI-SMOTE是一种基于量子原理的新型数据增强技术，用于解决医学领域中的类别不平衡问题，显著提升了多种机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 医学数据中类别不平衡导致模型偏差和预测性能下降，需要一种更有效的数据增强方法。

Method: 提出QI-SMOTE，利用量子演化和分层纠缠原理生成合成样本，保留复杂数据结构。

Result: 在MIMIC-III和MIMIC-IV数据集上验证，QI-SMOTE显著提升了多种模型的性能。

Conclusion: QI-SMOTE不仅缓解了类别不平衡，还增强了医学诊断模型的鲁棒性和可靠性。

Abstract: Class imbalance remains a critical challenge in machine learning (ML),
particularly in the medical domain, where underrepresented minority classes
lead to biased models and reduced predictive performance. This study introduces
Quantum-Inspired SMOTE (QI-SMOTE), a novel data augmentation technique that
enhances the performance of ML classifiers, including Random Forest (RF),
Support Vector Machine (SVM), Logistic Regression (LR), k-Nearest Neighbors
(KNN), Gradient Boosting (GB), and Neural Networks, by leveraging quantum
principles such as quantum evolution and layered entanglement. Unlike
conventional oversampling methods, QI-SMOTE generates synthetic instances that
preserve complex data structures, improving model generalization and
classification accuracy. We validate QI-SMOTE on the MIMIC-III and MIMIC-IV
datasets, using mortality detection as a benchmark task due to their clinical
significance and inherent class imbalance. We compare our method against
traditional oversampling techniques, including Borderline-SMOTE, ADASYN,
SMOTE-ENN, SMOTE-TOMEK, and SVM-SMOTE, using key performance metrics such as
Accuracy, F1-score, G-Mean, and AUC-ROC. The results demonstrate that QI-SMOTE
significantly improves the effectiveness of ensemble methods (RF, GB, ADA),
kernel-based models (SVM), and deep learning approaches by producing more
informative and balanced training data. By integrating quantum-inspired
transformations into the ML pipeline, QI-SMOTE not only mitigates class
imbalance but also enhances the robustness and reliability of predictive models
in medical diagnostics and decision-making. This study highlights the potential
of quantum-inspired resampling techniques in advancing state-of-the-art ML
methodologies.

</details>


### [75] [Improving Generative Methods for Causal Evaluation via Simulation-Based Inference](https://arxiv.org/abs/2509.02892)
*Pracheta Amaranath,Vinitra Muralikrishnan,Amit Sharma,David D. Jensen*

Main category: cs.LG

TL;DR: SBICE框架通过模拟推断生成更真实的合成数据集，提升因果估计器评估的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法需要用户提供参数的点估计而非分布，无法表达参数不确定性，可能导致评估不可靠。

Method: SBICE将生成参数建模为不确定，并基于源数据集推断其后验分布，利用模拟推断技术生成与源数据分布一致的合成数据。

Result: 实证结果显示SBICE生成的数据集更真实，支持在不确定性下进行稳健的因果基准测试。

Conclusion: SBICE通过处理参数不确定性，提升了因果估计器评估的可靠性和数据一致性。

Abstract: Generating synthetic datasets that accurately reflect real-world
observational data is critical for evaluating causal estimators, but remains a
challenging task. Existing generative methods offer a solution by producing
synthetic datasets anchored in the observed data (source data) while allowing
variation in key parameters such as the treatment effect and amount of
confounding bias. However, existing methods typically require users to provide
point estimates of such parameters (rather than distributions) and fixed
estimates (rather than estimates that can be improved with reference to the
source data). This denies users the ability to express uncertainty over
parameter values and removes the potential for posterior inference, potentially
leading to unreliable estimator comparisons. We introduce simulation-based
inference for causal evaluation (SBICE), a framework that models generative
parameters as uncertain and infers their posterior distribution given a source
dataset. Leveraging techniques in simulation-based inference, SBICE identifies
parameter configurations that produce synthetic datasets closely aligned with
the source data distribution. Empirical results demonstrate that SBICE improves
the reliability of estimator evaluations by generating more realistic datasets,
which supports a robust and data-consistent approach to causal benchmarking
under uncertainty.

</details>


### [76] [Event Detection and Classification for Long Range Sensing of Elephants Using Seismic Signal](https://arxiv.org/abs/2509.02920)
*Jaliya L. Wijayaraja,Janaka L. Wijekoon,Malitha Wijesundara*

Main category: cs.LG

TL;DR: 提出了一种用于检测大象足迹的分类框架，结合了新颖的事件检测技术CCW，并在多种环境中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖人工分类的问题，提升实时分类的适用性。

Method: 引入CCW技术，并与STA/LTA方法对比，使用SVM（RBF核）进行分类。

Result: 在控制环境中检测范围达155.6米，自然环境中140米；分类准确率在控制环境中99%，自然栖息地73%，HEC区域70%。

Conclusion: CCW和SVM结合有效，特征分析揭示了关键影响因素。

Abstract: Detecting elephants through seismic signals is an emerging research topic
aimed at developing solutions for Human-Elephant Conflict (HEC). Despite the
promising results, such solutions heavily rely on manual classification of
elephant footfalls, which limits their applicability for real-time
classification in natural settings. To address this limitation and build on our
previous work, this study introduces a classification framework targeting
resource-constrained implementations, prioritizing both accuracy and
computational efficiency. As part of this framework, a novel event detection
technique named Contextually Customized Windowing (CCW), tailored specifically
for detecting elephant footfalls, was introduced, and evaluations were
conducted by comparing it with the Short-Term Average/Long-Term Average
(STA/LTA) method. The yielded results show that the maximum validated detection
range was 155.6 m in controlled conditions and 140 m in natural environments.
Elephant footfall classification using Support Vector Machine (SVM) with a
Radial Basis Function (RBF) kernel demonstrated superior performance across
multiple settings, achieving an accuracy of 99% in controlled environments, 73%
in natural elephant habitats, and 70% in HEC-prone human habitats, the most
challenging scenario. Furthermore, feature impact analysis using explainable AI
identified the number of Zero Crossings and Dynamic Time Warping (DTW)
Alignment Cost as the most influential factors in all experiments, while
Predominant Frequency exhibited significant influence in controlled settings.

</details>


### [77] [A Narrative Review of Clinical Decision Support Systems in Offloading Footwear for Diabetes-Related Foot Ulcers](https://arxiv.org/abs/2509.02923)
*Kunal Kumar,Muhammad Ashad Kabir,Luke Donnan,Sayed Ahmed*

Main category: cs.LG

TL;DR: 本文综述了45项关于糖尿病足溃疡（DFU）减压鞋具的研究，提出了一个五部分的临床决策支持系统（CDSS）框架，以改善个性化治疗和评估。


<details>
  <summary>Details</summary>
Motivation: 当前DFU减压鞋具的处方决策存在碎片化问题，包括特征选择不一致、个性化有限和评估方法差异。

Method: 通过叙述性综述分析了45项研究（12项指南/协议，25项知识系统，8项机器学习应用），主题包括知识类型、决策逻辑、评估方法和技术支持。

Result: 指南强调压力阈值，但缺乏可操作输出；知识系统结合规则和传感器逻辑；机器学习模型计算精度高但解释性不足。评估方法分散，缺乏长期结果关联。

Conclusion: 提出的CDSS框架包括最小可行数据集、混合架构、结构化输出、持续验证和临床整合，旨在实现可扩展、以患者为中心的DFU护理。

Abstract: Offloading footwear helps prevent and treat diabetic foot ulcers (DFUs) by
lowering plantar pressure (PP), yet prescription decisions remain fragmented:
feature selection varies, personalization is limited, and evaluation practices
differ. We performed a narrative review of 45 studies (12 guidelines/protocols,
25 knowledge-based systems, 8 machine-learning applications) published to Aug
2025. We thematically analyzed knowledge type, decision logic, evaluation
methods, and enabling technologies. Guidelines emphasize PP thresholds (<=200
kPa or >=25--30\% reduction) but rarely yield actionable, feature-level
outputs. Knowledge-based systems use rule- and sensor-driven logic, integrating
PP monitoring, adherence tracking, and usability testing. ML work introduces
predictive, optimization, and generative models with high computational
accuracy but limited explainability and clinical validation. Evaluation remains
fragmented: protocols prioritize biomechanical tests; knowledge-based systems
assess usability/adherence; ML studies focus on technical accuracy with weak
linkage to long-term outcomes. From this synthesis we propose a five-part CDSS
framework: (1) a minimum viable dataset; (2) a hybrid architecture combining
rules, optimization, and explainable ML; (3) structured feature-level outputs;
(4) continuous validation and evaluation; and (5) integration with clinical and
telehealth workflows. This framework aims to enable scalable, patient-centered
CDSSs for DFU care; prioritizing interoperable datasets, explainable models,
and outcome-focused evaluation will be key to clinical adoption.

</details>


### [78] [PDRL: Post-hoc Descriptor-based Residual Learning for Uncertainty-Aware Machine Learning Potentials](https://arxiv.org/abs/2509.02927)
*Shih-Peng Huang,Nontawat Charoenphakdee,Yuta Tsuboi,Yong-Bin Zhuang,Wenwen Li*

Main category: cs.LG

TL;DR: 提出了一种基于描述符的后处理框架PDRL，用于高效量化机器学习原子间势的不确定性。


<details>
  <summary>Details</summary>
Motivation: 集成方法计算成本高，其他替代方法可能影响预测精度或无法应用于已训练模型。

Method: 利用训练好的图神经网络势的描述符估计残差误差，建模预测与真实值之间的差异。

Result: PDRL的多种变体在效果和局限性上与现有UQ方法进行了对比。

Conclusion: PDRL是一种简单高效的后处理方法，适用于量化不确定性。

Abstract: Ensemble method is considered the gold standard for uncertainty
quantification (UQ) for machine learning interatomic potentials (MLIPs).
However, their high computational cost can limit its practicality. Alternative
techniques, such as Monte Carlo dropout and deep kernel learning, have been
proposed to improve computational efficiency; however, some of these methods
cannot be applied to already trained models and may affect the prediction
accuracy. In this paper, we propose a simple and efficient post-hoc framework
for UQ that leverages the descriptor of a trained graph neural network
potential to estimate residual errors. We refer to this method as post-hoc
descriptor-based residual-based learning (PDRL). PDRL models the discrepancy
between MLIP predictions and ground truth values, allowing these residuals to
act as proxies for prediction uncertainty. We explore multiple variants of PDRL
and benchmark them against established UQ methods, evaluating both their
effectiveness and limitations.

</details>


### [79] [VendiRL: A Framework for Self-Supervised Reinforcement Learning of Diversely Diverse Skills](https://arxiv.org/abs/2509.02930)
*Erik M. Lintunen*

Main category: cs.LG

TL;DR: 论文提出了一种基于生态学中的Vendi Score的多样性度量方法，用于解决自监督强化学习中技能多样性的评估问题，并提出了VendiRL框架。


<details>
  <summary>Details</summary>
Motivation: 自监督强化学习中技能多样性的评估存在挑战，传统方法对多样性的定义过于局限且难以比较。

Method: 采用Vendi Score作为多样性度量，并开发了VendiRL框架，支持通过不同相似性函数定义多样性。

Result: VendiRL能够灵活地学习和评估多样化的技能集，适用于新环境中的多样性优化。

Conclusion: Vendi Score和VendiRL为技能多样性提供了统一的评估和学习框架，具有广泛的应用潜力。

Abstract: In self-supervised reinforcement learning (RL), one of the key challenges is
learning a diverse set of skills to prepare agents for unknown future tasks.
Despite impressive advances, scalability and evaluation remain prevalent
issues. Regarding scalability, the search for meaningful skills can be obscured
by high-dimensional feature spaces, where relevant features may vary across
downstream task domains. For evaluating skill diversity, defining what
constitutes "diversity" typically requires a hard commitment to a specific
notion of what it means for skills to be diverse, potentially leading to
inconsistencies in how skill diversity is understood, making results across
different approaches hard to compare, and leaving many forms of diversity
unexplored. To address these issues, we adopt a measure of sample diversity
that translates ideas from ecology to machine learning -- the Vendi Score --
allowing the user to specify and evaluate any desired form of diversity. We
demonstrate how this metric facilitates skill evaluation and introduce VendiRL,
a unified framework for learning diversely diverse sets of skills. Given
distinct similarity functions, VendiRL motivates distinct forms of diversity,
which could support skill-diversity pretraining in new and richly interactive
environments where optimising for various forms of diversity may be desirable.

</details>


### [80] [AR-KAN: Autoregressive-Weight-Enhanced Kolmogorov-Arnold Network for Time Series Forecasting](https://arxiv.org/abs/2509.02967)
*Chen Zeng,Tiehang Xu,Qiao Wang*

Main category: cs.LG

TL;DR: 提出了一种结合自回归和Kolmogorov-Arnold网络的混合模型AR-KAN，用于解决传统神经网络在非周期性信号分析中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在分析非周期性信号时表现不佳，尤其是对于频率不相容的信号叠加问题。

Method: 结合自回归模型（AR）和Kolmogorov-Arnold网络（KAN），利用Universal Myopic Mapping Theorem处理静态非线性部分，并通过预训练的AR组件引入记忆机制。

Result: 实验表明，AR-KAN在72%的真实数据集上表现优于传统方法。

Conclusion: AR-KAN有效解决了非周期性信号分析的挑战，结合了两种方法的优势。

Abstract: Conventional neural networks frequently face challenges in spectral analysis
of signals. To address this challenge, Fourier neural networks (FNNs) and
similar approaches integrate components of Fourier series into the structure of
neural networks. Nonetheless, a significant hurdle is often overlooked: the
superposition of periodic signals does not necessarily result in a periodic
signal. For example, when forecasting almost periodic functions composed of
signals with incommensurate frequencies, traditional models such as
Autoregressive Integrated Moving Average (ARIMA) frequently outperform most
neural networks including large language models (LLMs). To tackle this goal, we
propose Autoregressive-Weight-Enhanced AR-KAN, a hybrid model that combines the
benefits of both methods. Using the Universal Myopic Mapping Theorem, we apply
a Kolmogorov-Arnold Network (KAN) for the static nonlinear part and include
memory through a pre-trained AR component, which can be explained to retain the
most useful information while eliminating redundancy. Experimental data
indicates that AR-KAN delivers superior results on $72\%$ of real-world
datasets.

</details>


### [81] [Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation](https://arxiv.org/abs/2509.02970)
*Kaoru Otsuka,Yuki Takezawa,Makoto Yamada*

Main category: cs.LG

TL;DR: 论文提出了一种针对联邦学习中部分参与和拜占庭攻击的新优化方法D-Byz-SGDM，通过延迟动量聚合实现稳定训练。


<details>
  <summary>Details</summary>
Motivation: 现有拜占庭鲁棒联邦学习方法假设全客户端参与，不适用于实际中的部分参与场景，导致在拜占庭多数攻击下失效。

Method: 引入延迟动量聚合原则，服务器聚合非参与客户端的最新梯度和活跃客户端的动量，提出D-Byz-SGDM优化器。

Result: 理论证明收敛性恢复全参与结果并匹配部分参与下限，实验验证在多种拜占庭攻击下稳定训练。

Conclusion: D-Byz-SGDM解决了部分参与下的拜占庭鲁棒性问题，为稀疏通信场景提供了有效解决方案。

Abstract: Federated Learning (FL) allows distributed model training across multiple
clients while preserving data privacy, but it remains vulnerable to Byzantine
clients that exhibit malicious behavior. While existing Byzantine-robust FL
methods provide strong convergence guarantees (e.g., to a stationary point in
expectation) under Byzantine attacks, they typically assume full client
participation, which is unrealistic due to communication constraints and client
availability. Under partial participation, existing methods fail immediately
after the sampled clients contain a Byzantine majority, creating a fundamental
challenge for sparse communication. First, we introduce delayed momentum
aggregation, a novel principle where the server aggregates the most recently
received gradients from non-participating clients alongside fresh momentum from
active clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with
Momentum) implements this delayed momentum aggregation principle for
Byzantine-robust FL with partial participation. Then, we establish convergence
guarantees that recover previous full participation results and match the
fundamental lower bounds we prove for the partial participation setting.
Experiments on deep learning tasks validated our theoretical findings, showing
stable and robust training under various Byzantine attacks.

</details>


### [82] [AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates](https://arxiv.org/abs/2509.02981)
*Minxin Zhang,Yuxuan Liu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: AdaGO结合了正交化动量更新和AdaGrad型步长调整，提升了Muon优化器的性能，并在理论和实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决Muon优化器中学习率难以确定的问题，同时保留正交化更新的优势。

Method: 结合正交化更新方向和AdaGrad型步长调整，提出AdaGO算法。

Result: 理论证明AdaGO在非凸函数上具有最优收敛率，实验显示其在CIFAR-10任务上优于Muon和Adam。

Conclusion: AdaGO是一种高效且性能优越的优化器，适用于大规模语言模型训练。

Abstract: The recently proposed Muon optimizer updates weight matrices via
orthogonalized momentum and has demonstrated strong empirical success in large
language model training. However, it remains unclear how to determine the
learning rates for such orthogonalized updates. AdaGrad, by contrast, is a
widely used adaptive method that scales stochastic gradients by accumulated
past gradients. We propose a new algorithm, AdaGO, which combines a norm-based
AdaGrad-type stepsize with an orthogonalized update direction, bringing
together the benefits of both approaches. Unlike other adaptive variants of
Muon, AdaGO preserves the orthogonality of the update direction, which can be
interpreted as a spectral descent direction, while adapting the stepsizes to
the optimization landscape by scaling the direction with accumulated past
gradient norms. The implementation of AdaGO requires only minimal modification
to Muon, with a single additional scalar variable, the accumulated squared
gradient norms, to be computed, making it computationally and memory efficient.
Optimal theoretical convergence rates are established for nonconvex functions
in both stochastic and deterministic settings under standard smoothness and
unbiased bounded-variance noise assumptions. Empirical results on CIFAR-10
classification and function regression demonstrate that AdaGO outperforms Muon
and Adam.

</details>


### [83] [StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails](https://arxiv.org/abs/2509.02982)
*Hritik Arasu,Faisal R Jahangiri*

Main category: cs.LG

TL;DR: 提出了一种流式、无源数据的测试时适应方法，结合熵最小化和安全机制，用于睡眠分期模型，提升在未见过的生理或记录条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决睡眠分期模型在部署到不同生理或记录条件下的患者时性能下降的问题。

Method: 结合熵最小化（Tent）、批归一化统计刷新和两个安全机制（熵门和EMA重置），实现流式、无源数据的测试时适应。

Result: 在Sleep-EDF数据集上，使用单导联EEG，展示了比冻结基线更优的性能，延迟低且内存占用小。

Conclusion: 该方法模型无关，无需源数据或患者校准，适用于设备或床边使用。

Abstract: Sleep staging models often degrade when deployed on patients with unseen
physiology or recording conditions. We propose a streaming, source-free
test-time adaptation (TTA) recipe that combines entropy minimization (Tent)
with Batch-Norm statistic refresh and two safety rails: an entropy gate to
pause adaptation on uncertain windows and an EMA-based reset to reel back
drift. On Sleep-EDF Expanded, using single-lead EEG (Fpz-Cz, 100 Hz, 30s
epochs; R&K to AASM mapping), we show consistent gains over a frozen baseline
at seconds-level latency and minimal memory, reporting per-stage metrics and
Cohen's k. The method is model-agnostic, requires no source data or patient
calibration, and is practical for on-device or bedside use.

</details>


### [84] [Population-aware Online Mirror Descent for Mean-Field Games with Common Noise by Deep Reinforcement Learning](https://arxiv.org/abs/2509.03030)
*Zida Wu,Mathieu Lauriere,Matthieu Geist,Olivier Pietquin,Ankur Mehta*

Main category: cs.LG

TL;DR: 提出了一种高效的深度强化学习算法，用于在未知初始分布或存在共同噪声的情况下学习MFGs中的纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 解决大规模多智能体系统中学习纳什均衡的挑战，特别是在初始分布未知或存在共同噪声的情况下。

Method: 结合Munchausen RL和Online Mirror Descent，设计了一种不依赖平均或历史采样的深度强化学习算法。

Result: 在七个经典示例中展示了优于现有算法的收敛性能，尤其是在存在共同噪声时表现出鲁棒性和适应性。

Conclusion: 该算法在复杂环境中高效且鲁棒，为MFGs中的纳什均衡学习提供了新思路。

Abstract: Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.

</details>


### [85] [Multimodal learning of melt pool dynamics in laser powder bed fusion](https://arxiv.org/abs/2509.03029)
*Satyajit Mojumder,Pallock Halder,Tiana Tonge*

Main category: cs.LG

TL;DR: 提出了一种多模态数据融合方法，结合高保真X射线数据和低保真吸收率数据，用于预测激光粉末床熔融过程中的熔池动态。


<details>
  <summary>Details</summary>
Motivation: 现有传感器在实时监测中各有局限，X射线成像成本高，而吸收率数据噪声大，单独使用效果不佳。

Method: 采用卷积神经网络（CNNs）和循环神经网络（RNNs）进行多模态学习，结合早期融合策略，并用于迁移学习。

Result: 多模态训练显著提高了预测准确性，且训练后的模型仅需吸收率数据即可推断熔池特性。

Conclusion: 该方法实现了低成本、实时监测，在增材制造中具有广泛应用前景。

Abstract: While multiple sensors are used for real-time monitoring in additive
manufacturing, not all provide practical or reliable process insights. For
example, high-speed X-ray imaging offers valuable spatial information about
subsurface melt pool behavior but is costly and impractical for most industrial
settings. In contrast, absorptivity data from low-cost photodiodes correlate
with melt pool dynamics but is often too noisy for accurate prediction when
used alone. In this paper, we propose a multimodal data fusion approach for
predicting melt pool dynamics by combining high-fidelity X-ray data with
low-fidelity absorptivity data in the Laser Powder Bed Fusion (LPBF) process.
Our multimodal learning framework integrates convolutional neural networks
(CNNs) for spatial feature extraction from X-ray data with recurrent neural
networks (RNNs) for temporal feature extraction from absorptivity signals,
using an early fusion strategy. The multimodal model is further used as a
transfer learning model to fine-tune the RNN model that can predict melt pool
dynamics only with absorptivity, with greater accuracy compared to the
multimodal model. Results show that training with both modalities significantly
improves prediction accuracy compared to using either modality alone.
Furthermore, once trained, the model can infer melt pool characteristics using
only absorptivity data, eliminating the need for expensive X-ray imaging. This
multimodal fusion approach enables cost-effective, real-time monitoring and has
broad applicability in additive manufacturing.

</details>


### [86] [Knowledge Integration for Physics-informed Symbolic Regression Using Pre-trained Large Language Models](https://arxiv.org/abs/2509.03036)
*Bilge Taskin,Wenxiong Xie,Teddy Lazebnik*

Main category: cs.LG

TL;DR: 利用预训练大语言模型（LLM）改进物理信息符号回归（PiSR），自动整合领域知识，减少人工干预，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前PiSR方法依赖专家手动设计，限制了其普适性。通过LLM的上下文理解能力，实现领域知识的自动化整合。

Method: 将LLM集成到SR的损失函数中，通过LLM评估生成的方程。测试了三种SR算法和三种LLM，覆盖三种物理动态。

Result: LLM集成显著提升了从数据中重建物理动态的能力，增强了模型对噪声和复杂性的鲁棒性。提示工程对性能有重要影响。

Conclusion: LLM为PiSR提供了自动化领域知识整合的有效途径，扩展了其应用范围。

Abstract: Symbolic regression (SR) has emerged as a powerful tool for automated
scientific discovery, enabling the derivation of governing equations from
experimental data. A growing body of work illustrates the promise of
integrating domain knowledge into the SR to improve the discovered equation's
generality and usefulness. Physics-informed SR (PiSR) addresses this by
incorporating domain knowledge, but current methods often require specialized
formulations and manual feature engineering, limiting their adaptability only
to domain experts. In this study, we leverage pre-trained Large Language Models
(LLMs) to facilitate knowledge integration in PiSR. By harnessing the
contextual understanding of LLMs trained on vast scientific literature, we aim
to automate the incorporation of domain knowledge, reducing the need for manual
intervention and making the process more accessible to a broader range of
scientific problems. Namely, the LLM is integrated into the SR's loss function,
adding a term of the LLM's evaluation of the SR's produced equation. We
extensively evaluate our method using three SR algorithms (DEAP, gplearn, and
PySR) and three pre-trained LLMs (Falcon, Mistral, and LLama 2) across three
physical dynamics (dropping ball, simple harmonic motion, and electromagnetic
wave). The results demonstrate that LLM integration consistently improves the
reconstruction of physical dynamics from data, enhancing the robustness of SR
models to noise and complexity. We further explore the impact of prompt
engineering, finding that more informative prompts significantly improve
performance.

</details>


### [87] [Binary Quantization For LLMs Through Dynamic Grouping](https://arxiv.org/abs/2509.03054)
*Xinzhe Zheng,Zhen-Qun Yang,Haoran Xie,S. Joe Qin,Arlene Chen,Fangzhen Lin*

Main category: cs.LG

TL;DR: 提出了一种针对二进制量化的新优化目标和三种算法，显著减少了存储和推理成本，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 解决二进制量化导致的性能下降问题，提升模型压缩效率。

Method: 通过动态识别最优非结构化子矩阵的自适应分组策略，增强块量化。

Result: 量化后的LLaMA 3.2 3B模型困惑度为8.23，接近原始模型的7.81，优于之前的SOTA BiLLM。

Conclusion: 该方法在性能和效率上与SOTA 4-bit方法竞争，压缩过程高效且并行性强。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of Natural Language Processing (NLP) tasks, but require
substantial memory and computational resources. Binary quantization, which
compresses model weights from 16-bit Brain Float to 1-bit representations in
{-1, 1}, offers significant reductions in storage and inference costs. However,
such aggressive quantization often leads to notable performance degradation
compared to more conservative 4-bit quantization methods. In this research, we
propose a novel optimization objective tailored for binary quantization, along
with three algorithms designed to realize it effectively. Our method enhances
blocked quantization by dynamically identifying optimal unstructured
sub-matrices through adaptive grouping strategies. Experimental results
demonstrate that our approach achieves an average bit length of just 1.007
bits, while maintaining high model quality. Specifically, our quantized LLaMA
3.2 3B model attains a perplexity of 8.23, remarkably close to the original
7.81, and surpasses previous SOTA BiLLM with a perplexity of only 123.90.
Furthermore, our method is competitive with SOTA 4-bit approaches such as GPTQ
in both performance and efficiency. The compression process is highly
efficient, requiring only 14 seconds to quantize the full LLaMA 3.2 3B weights
on a single CPU core, with the entire process completing in under 100 minutes
and exhibiting embarrassingly parallel properties.
  Code - https://github.com/johnnyzheng0636/WGM_bi_quan

</details>


### [88] [Discrete Functional Geometry of ReLU Networks via ReLU Transition Graphs](https://arxiv.org/abs/2509.03056)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 扩展ReLU Transition Graph (RTG)框架，提出图论模型分析深度ReLU网络，揭示其结构与泛化能力的关系。


<details>
  <summary>Details</summary>
Motivation: 通过离散几何结构理解ReLU网络的功能行为，提供新的分析工具。

Method: 构建RTG模型，节点表示线性激活区域，边连接相邻区域，分析其扩展性、度分布和谱特性。

Result: 证明RTG在随机初始化时具有强扩展性和二项式度分布，区域熵和谱间隙与泛化能力相关。

Conclusion: RTG为ReLU网络分析提供统一框架，揭示了结构与泛化的联系。

Abstract: We extend the ReLU Transition Graph (RTG) framework into a comprehensive
graph-theoretic model for understanding deep ReLU networks. In this model, each
node represents a linear activation region, and edges connect regions that
differ by a single ReLU activation flip, forming a discrete geometric structure
over the network's functional behavior. We prove that RTGs at random
initialization exhibit strong expansion, binomial degree distributions, and
spectral properties that tightly govern generalization. These structural
insights enable new bounds on capacity via region entropy and on generalization
via spectral gap and edge-wise KL divergence. Empirically, we construct RTGs
for small networks, measure their smoothness and connectivity properties, and
validate theoretical predictions. Our results show that region entropy
saturates under overparameterization, spectral gap correlates with
generalization, and KL divergence across adjacent regions reflects functional
smoothness. This work provides a unified framework for analyzing ReLU networks
through the lens of discrete functional geometry, offering new tools to
understand, diagnose, and improve generalization.

</details>


### [89] [Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers](https://arxiv.org/abs/2509.03059)
*Xingyue Huang,Rishabh,Gregor Franke,Ziyi Yang,Jiamu Bai,Weijie Bai,Jinhe Bi,Zifeng Ding,Yiqun Duan,Chengyu Fan,Wendong Fan,Xin Gao,Ruohao Guo,Yuan He,Zhuangzhuang He,Xianglong Hu,Neil Johnson,Bowen Li,Fangru Lin,Siyu Lin,Tong Liu,Yunpu Ma,Hao Shen,Hao Sun,Beibei Wang,Fangyijie Wang,Hao Wang,Haoran Wang,Yang Wang,Yifeng Wang,Zhaowei Wang,Ziyang Wang,Yifan Wu,Zikai Xiao,Chengxing Xie,Fan Yang,Junxiao Yang,Qianshuo Ye,Ziyu Ye,Guangtao Zeng,Yuwen Ebony Zhang,Zeyu Zhang,Zihao Zhu,Bernard Ghanem,Philip Torr,Guohao Li*

Main category: cs.LG

TL;DR: Loong Project是一个开源框架，通过合成数据生成和验证，扩展了LLM在多个推理密集型领域的应用。


<details>
  <summary>Details</summary>
Motivation: 由于高质量可验证数据集的稀缺和人工监督的高成本，将LLM的成功扩展到其他推理密集型领域具有挑战性。

Method: 框架包含LoongBench（人类验证的种子数据集）和LoongEnv（模块化合成数据生成环境），形成强化学习循环。

Result: 在多个开源和专有LLM上进行了基准测试，分析了合成数据的正确性、难度和多样性。

Conclusion: Loong Project为LLM在广泛推理领域的应用提供了可扩展的解决方案。

Abstract: Recent advances in Large Language Models (LLMs) have shown that their
reasoning capabilities can be significantly improved through Reinforcement
Learning with Verifiable Reward (RLVR), particularly in domains like
mathematics and programming, where ground-truth correctness can be
automatically evaluated. However, extending this success to other
reasoning-intensive domains remains challenging due to the scarcity of
high-quality, verifiable datasets and the high cost of human supervision. In
this work, we introduce the Loong Project: an open-source framework for
scalable synthetic data generation and verification across a diverse range of
reasoning-intensive domains. The framework consists of two key components: (1)
LoongBench, a curated seed dataset containing 8,729 human-vetted examples
across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired
with executable code and rich metadata; and (2) LoongEnv, a modular synthetic
data generation environment that supports multiple prompting strategies to
produce new question-answer-code triples. Together, these components form an
agent-environment loop that enables reinforcement learning, where an LLM-based
agent is rewarded for generating Chain-of-Thought (CoT) solutions that align
with code-executed answers. Empirically, we benchmark LoongBench on a broad
suite of both open-source and proprietary LLMs to evaluate domain coverage and
reveal performance bottlenecks. In addition, we conduct a comprehensive
analysis of synthetic data generated by LoongEnv, examining correctness,
difficulty, and diversity. Code and documentation are available at
https://github.com/camel-ai/loong.

</details>


### [90] [LSAM: Asynchronous Distributed Training with Landscape-Smoothed Sharpness-Aware Minimization](https://arxiv.org/abs/2509.03110)
*Yunfei Teng,Sixin Zhang*

Main category: cs.LG

TL;DR: LSAM是一种新型优化器，结合了SAM的泛化优势和高效性，通过异步分布式采样策略提升大批量训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在分布式大批量训练中的效率问题。

Method: 结合SAM的对抗步骤和异步分布式采样策略，生成平滑的锐度感知损失景观。

Result: 消除了同步瓶颈，加速大批量收敛，最终精度高于数据并行SAM。

Conclusion: LSAM在保持SAM泛化优势的同时，显著提升了训练效率。

Abstract: While Sharpness-Aware Minimization (SAM) improves generalization in deep
neural networks by minimizing both loss and sharpness, it suffers from
inefficiency in distributed large-batch training. We present Landscape-Smoothed
SAM (LSAM), a novel optimizer that preserves SAM's generalization advantages
while offering superior efficiency. LSAM integrates SAM's adversarial steps
with an asynchronous distributed sampling strategy, generating an asynchronous
distributed sampling scheme, producing a smoothed sharpness-aware loss
landscape for optimization. This design eliminates synchronization bottlenecks,
accelerates large-batch convergence, and delivers higher final accuracy
compared to data-parallel SAM.

</details>


### [91] [A Hierarchical Deep Reinforcement Learning Framework for Traffic Signal Control with Predictable Cycle Planning](https://arxiv.org/abs/2509.03118)
*Hankang Gu,Yuli Zhang,Chengming Wang,Ruiyuan Jiang,Ziheng Qiao,Pengfei Fan,Dongyao Jia*

Main category: cs.LG

TL;DR: 提出了一种名为DHCP的分层深度强化学习模型，用于交通信号控制，通过高低级代理分配信号周期时间，提高了效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有交通信号控制方法（如“选择相位”和“切换”策略）存在不可预测性或效率低下的问题，DHCP旨在解决这些问题。

Method: DHCP采用分层结构，高级代理分配总周期时间给主要方向，低级代理进一步分配时间给直行和左转。

Result: 在真实和合成路网及交通流测试中，DHCP表现优于基线方法。

Conclusion: DHCP通过分层时间分配，显著提升了交通信号控制的性能和公平性。

Abstract: Deep reinforcement learning (DRL) has become a popular approach in traffic
signal control (TSC) due to its ability to learn adaptive policies from complex
traffic environments. Within DRL-based TSC methods, two primary control
paradigms are ``choose phase" and ``switch" strategies. Although the agent in
the choose phase paradigm selects the next active phase adaptively, this
paradigm may result in unexpected phase sequences for drivers, disrupting their
anticipation and potentially compromising safety at intersections. Meanwhile,
the switch paradigm allows the agent to decide whether to switch to the next
predefined phase or extend the current phase. While this structure maintains a
more predictable order, it can lead to unfair and inefficient phase
allocations, as certain movements may be extended disproportionately while
others are neglected. In this paper, we propose a DRL model, named Deep
Hierarchical Cycle Planner (DHCP), to allocate the traffic signal cycle
duration hierarchically. A high-level agent first determines the split of the
total cycle time between the North-South (NS) and East-West (EW) directions
based on the overall traffic state. Then, a low-level agent further divides the
allocated duration within each major direction between straight and left-turn
movements, enabling more flexible durations for the two movements. We test our
model on both real and synthetic road networks, along with multiple sets of
real and synthetic traffic flows. Empirical results show our model achieves the
best performance over all datasets against baselines.

</details>


### [92] [A Neural Network Approach to Multi-radionuclide TDCR Beta Spectroscopy](https://arxiv.org/abs/2509.03137)
*Li Yi,Qian Yang*

Main category: cs.LG

TL;DR: 提出了一种结合数值模拟和深度学习的AI框架，用于无标准自动化分析多放射性核素。


<details>
  <summary>Details</summary>
Motivation: 传统TDCR方法在多放射性核素分析中存在自动化程度低和依赖特定混合标准的局限性。

Method: 利用Geant4模拟生成β光谱，结合统计建模的探测器响应采样，训练神经网络实现自主分析。

Result: 模型在活动比例、检测效率和光谱重建方面表现出高精度（MAE分别为0.009、0.002，SSIM为0.9998）。

Conclusion: 该方法在无参考材料或需要快速现场分析的场景中具有显著潜力。

Abstract: Liquid scintillation triple-to-doubly coincident ratio (TDCR) spectroscopy is
widely adopted as a standard method for radionuclide quantification because of
its inherent advantages such as high precision, self-calibrating capability,
and independence from radioactive reference sources. However, multiradionuclide
analysis via TDCR faces the challenges of limited automation and reliance on
mixture-specific standards, which may not be easily available. Here, we present
an Artificial Intelligence (AI) framework that combines numerical spectral
simulation and deep learning for standard-free automated analysis. $\beta$
spectra for model training were generated using Geant4 simulations coupled with
statistically modeled detector response sampling. A tailored neural network
architecture, trained on this dataset covering various nuclei mix ratio and
quenching scenarios, enables autonomous resolution of individual radionuclide
activities and detecting efficiency through end-to-end learning paradigms. The
model delivers consistent high accuracy across tasks: activity proportions
(mean absolute error = 0.009), detection efficiencies (mean absolute error =
0.002), and spectral reconstruction (Structural Similarity Index = 0.9998),
validating its physical plausibility for quenched $\beta$ spectroscopy. This
AI-driven methodology exhibits significant potential for automated
safety-compliant multiradionuclide analysis with robust generalization,
real-time processing capabilities, and engineering feasibility, particularly in
scenarios where reference materials are unavailable or rapid field analysis is
required.

</details>


### [93] [Rashomon in the Streets: Explanation Ambiguity in Scene Understanding](https://arxiv.org/abs/2509.03169)
*Helge Spieker,Jørn Eirik Betten,Arnaud Gotlieb,Nadjib Lazaar,Nassim Belmecheri*

Main category: cs.LG

TL;DR: 论文首次实证量化了Rashomon效应对自动驾驶中动作预测任务的影响，揭示了不同模型间解释的显著不一致性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，可解释AI（XAI）的可靠性受到Rashomon效应的挑战，即多个同样准确的模型可能对同一预测提供不同解释。

Method: 使用定性可解释图（QXGs）作为符号场景表示，训练了两类模型（梯度提升模型和GNNs），并通过特征归因方法测量解释的一致性。

Result: 结果显示模型内部和模型之间存在显著的解释不一致性。

Conclusion: 解释的模糊性是问题的固有属性，而不仅仅是建模的产物。

Abstract: Explainable AI (XAI) is essential for validating and trusting models in
safety-critical applications like autonomous driving. However, the reliability
of XAI is challenged by the Rashomon effect, where multiple, equally accurate
models can offer divergent explanations for the same prediction. This paper
provides the first empirical quantification of this effect for the task of
action prediction in real-world driving scenes. Using Qualitative Explainable
Graphs (QXGs) as a symbolic scene representation, we train Rashomon sets of two
distinct model classes: interpretable, pair-based gradient boosting models and
complex, graph-based Graph Neural Networks (GNNs). Using feature attribution
methods, we measure the agreement of explanations both within and between these
classes. Our results reveal significant explanation disagreement. Our findings
suggest that explanation ambiguity is an inherent property of the problem, not
just a modeling artifact.

</details>


### [94] [Systematic Evaluation of Attribution Methods: Eliminating Threshold Bias and Revealing Method-Dependent Performance Patterns](https://arxiv.org/abs/2509.03176)
*Serra Aksoy*

Main category: cs.LG

TL;DR: 论文提出了一种无阈值的评估框架AUC-IoU，解决了现有归因方法评估中的阈值选择偏差问题，并在皮肤病影像上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法评估因依赖单一阈值而存在偏差，可能导致方法排名反转和结论不可靠。

Method: 提出无阈值框架AUC-IoU，通过计算全阈值范围内的曲线下面积来评估归因质量。

Result: 在皮肤病影像上，无阈值评估能可靠区分方法性能，XRAI表现最佳，相比LIME和Integrated Gradients分别提升31%和204%。

Conclusion: 无阈值框架消除了评估偏差，为医学影像等领域提供了理论指导和实践标准。

Abstract: Attribution methods explain neural network predictions by identifying
influential input features, but their evaluation suffers from threshold
selection bias that can reverse method rankings and undermine conclusions.
Current protocols binarize attribution maps at single thresholds, where
threshold choice alone can alter rankings by over 200 percentage points. We
address this flaw with a threshold-free framework that computes Area Under the
Curve for Intersection over Union (AUC-IoU), capturing attribution quality
across the full threshold spectrum. Evaluating seven attribution methods on
dermatological imaging, we show single-threshold metrics yield contradictory
results, while threshold-free evaluation provides reliable differentiation.
XRAI achieves 31% improvement over LIME and 204% over vanilla Integrated
Gradients, with size-stratified analysis revealing performance variations up to
269% across lesion scales. These findings establish methodological standards
that eliminate evaluation artifacts and enable evidence-based method selection.
The threshold-free framework provides both theoretical insight into attribution
behavior and practical guidance for robust comparison in medical imaging and
beyond.

</details>


### [95] [Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025](https://arxiv.org/abs/2509.03191)
*Taiga Saito,Yu Otake,Stephen Wu*

Main category: cs.LG

TL;DR: TabPFN在零训练、少样本的情境下应用于地质数据建模，优于传统贝叶斯模型，展现了高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 探索TabPFN作为通用基础模型在地质数据建模中的潜力，以提升预测效率和准确性。

Method: 采用TabPFN进行零训练、少样本学习，结合BID数据库提供额外上下文，无需超参数调优。

Result: 在空间预测和缺失参数填补任务中，TabPFN均表现更优，但计算成本较高。

Conclusion: TabPFN首次成功应用于地质建模，可能引领概率性场地表征的新范式。

Abstract: This paper presents a novel application of the Tabular Prior-Data Fitted
Network (TabPFN) - a transformer-based foundation model for tabular data - to
geotechnical site characterization problems defined in the GEOAI benchmark
BM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting the
spatial variation of undrained shear strength (su) across borehole depth
profiles, and (2) imputing missing mechanical parameters in a dense-site
dataset. We apply TabPFN in a zero-training, few-shot, in-context learning
setting - without hyper-parameter tuning - and provide it with additional
context from the big indirect database (BID). The study demonstrates that
TabPFN, as a general-purpose foundation model, achieved superior accuracy and
well-calibrated predictive distributions compared to a conventional
hierarchical Bayesian model (HBM) baseline, while also offering significant
gains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),
TabPFN outperformed the HBM in prediction accuracy and delivered an
order-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanical
parameter imputation), TabPFN likewise achieved lower RMSE for all target
parameters with well-quantified uncertainties, though its cumulative
computation cost was higher than HBM's due to its one-variable-at-a-time
inference. These results mark the first successful use of a tabular foundation
model in geotechnical modeling, suggesting a potential paradigm shift in
probabilistic site characterization.

</details>


### [96] [Exploring the Design Space of Fair Tree Learning Algorithms](https://arxiv.org/abs/2509.03204)
*Kiara Stempel,Mattia Cerrato,Stefan Kramer*

Main category: cs.LG

TL;DR: 论文探讨了决策树在公平性背景下的三种设计空间选项，重点研究了之前未被充分探索的两种方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在通过约束训练过程来确保决策树的公平性，但设计空间中的其他选项（如独立构建树）尚未被充分研究。

Method: 提出了三种设计空间选项：(i) 单树结合目标和敏感属性；(ii) 单树优化目标并约束敏感属性；(iii) 独立构建目标树和敏感属性树。重点研究了后两种方法。

Result: 实验结果表明，独立构建树的方法在某些情况下能更好地平衡预测性能和公平性。

Conclusion: 论文扩展了决策树公平性研究的视野，展示了设计空间中未被充分探索的选项的潜力。

Abstract: Decision trees have been studied extensively in the context of fairness,
aiming to maximize prediction performance while ensuring non-discrimination
against different groups. Techniques in this space usually focus on imposing
constraints at training time, constraining the search space so that solutions
which display unacceptable values of relevant metrics are not considered,
discarded, or discouraged. If we assume one target variable y and one sensitive
attribute s, the design space of tree learning algorithms can be spanned as
follows: (i) One can have one tree T that is built using an objective function
that is a function of y, s, and T. For instance, one can build a tree based on
the weighted information gain regarding y (maximizing) and s (minimizing). (ii)
The second option is to have one tree model T that uses an objective function
in y and T and a constraint on s and T. Here, s is no longer part of the
objective, but part of a constraint. This can be achieved greedily by aborting
a further split as soon as the condition that optimizes the objective in y
fails to satisfy the constraint on s. A simple way to explore other splits is
to backtrack during tree construction once a fairness constraint is violated.
(iii) The third option is to have two trees T_y and T_s, one for y and one for
s, such that the tree structure for y and s does not have to be shared. In this
way, information regarding y and regarding s can be used independently, without
having to constrain the choices in tree construction by the mutual information
between the two variables. Quite surprisingly, of the three options, only the
first one and the greedy variant of the second have been studied in the
literature so far. In this paper, we introduce the above two additional options
from that design space and characterize them experimentally on multiple
datasets.

</details>


### [97] [Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback](https://arxiv.org/abs/2509.03206)
*Zeqiang Zhang,Fabian Wurzberger,Gerrit Schmid,Sebastian Gottwald,Daniel A. Braun*

Main category: cs.LG

TL;DR: 论文提出了一种结合对比学习与GCSL的新模型，以解决自模仿学习中的偏见和仅关注成功的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在稀疏奖励任务中表现不佳，而GCSL虽能自模仿学习，但存在偏见和忽略失败经验的局限。

Method: 通过将对比学习融入GCSL框架，模型从成功和失败中学习，减少偏见并增强探索性。

Result: 实验表明，新算法能克服初始偏见，提升探索行为，并在多种环境中表现更优。

Conclusion: 结合对比学习的GCSL框架能有效解决自模仿学习的局限性，提升性能。

Abstract: Reinforcement learning faces significant challenges when applied to tasks
characterized by sparse reward structures. Although imitation learning, within
the domain of supervised learning, offers faster convergence, it relies heavily
on human-generated demonstrations. Recently, Goal-Conditioned Supervised
Learning (GCSL) has emerged as a potential solution by enabling self-imitation
learning for autonomous systems. By strategically relabelling goals, agents can
derive policy insights from their own experiences. Despite the successes of
this framework, it presents two notable limitations: (1) Learning exclusively
from self-generated experiences can exacerbate the agents' inherent biases; (2)
The relabelling strategy allows agents to focus solely on successful outcomes,
precluding them from learning from their mistakes. To address these issues, we
propose a novel model that integrates contrastive learning principles into the
GCSL framework to learn from both success and failure. Through empirical
evaluations, we demonstrate that our algorithm overcomes limitations imposed by
agents' initial biases and thereby enables more exploratory behavior. This
facilitates the identification and adoption of effective policies, leading to
superior performance across a variety of challenging environments.

</details>


### [98] [TeRA: Vector-based Random Tensor Network for High-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2509.03234)
*Yuxuan Gu,Wuyang Zhou,Giorgos Iacovides,Danilo Mandic*

Main category: cs.LG

TL;DR: TeRA是一种新型PEFT方法，通过张量网络实现高秩权重更新，同时保持向量化方法的参数效率。


<details>
  <summary>Details</summary>
Motivation: 解决高秩适配器与向量化方法在表达性和参数效率之间的权衡问题。

Method: 使用类似Tucker的张量网络参数化权重更新矩阵，冻结随机初始化的大因子，仅训练小层特定缩放向量。

Result: TeRA在性能上匹配或超越高秩适配器，同时参数效率与向量化方法相当。

Conclusion: TeRA有效平衡了表达性和参数效率，理论分析和实验验证了其优越性。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation
(LoRA), have significantly reduced the number of trainable parameters needed in
fine-tuning large language models (LLMs). Subsequent developments of LoRA-style
adapters have diverged into two main directions: (1) enhancing model
expressivity with high-rank adapters, and (2) pushing for further parameter
reduction, as exemplified by vector-based methods. However, these approaches
present a trade-off, as achieving the expressivity of high-rank weight updates
typically comes at the cost of sacrificing the extreme parameter efficiency
offered by vector-based techniques. To address this issue, we propose a
vector-based random \underline{\textbf{Te}}nsor network for
high-\underline{\textbf{R}}ank \underline{\textbf{A}}daptation (TeRA), a novel
PEFT method that achieves high-rank weight updates while retaining the
parameter efficiency of vector-based PEFT adapters. This is achieved by
parameterizing the tensorized weight update matrix as a Tucker-like tensor
network (TN), in which large randomly initialized factors are frozen and shared
across layers, while only small layer-specific scaling vectors, formed by
entries in diagonal factor matrices, are trained. This design effectively
decouples the rank of the weight update matrix from the number of trainable
parameters. Comprehensive experiments demonstrate that TeRA matches or even
outperforms high-rank adapters, while requiring a trainable parameter count
similar to vector-based methods. Theoretical analysis and ablation studies
further validate the effectiveness of our approach.

</details>


### [99] [Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric](https://arxiv.org/abs/2509.03240)
*Harald Vilhelm Skat-Rørdam,Sneha Das,Kathrine Sofie Rasmussen,Nicole Nadine Lønfeldt,Line Clemmensen*

Main category: cs.LG

TL;DR: 论文提出了一种窗口化的F1指标（F1$_w$），用于更准确地评估时间序列中的事件检测性能，特别是在真实世界不平衡数据集中。


<details>
  <summary>Details</summary>
Motivation: 传统指标（如F1和F1$_{pa}$）在评估时间序列事件检测时，由于标注为单点事件而实际现象是渐进的，导致性能评估不准确。

Method: 引入F1$_w$指标，通过时间容忍窗口来更鲁棒地评估事件检测性能，窗口大小可根据领域知识调整。

Result: 在三个生理数据集（ADARP、Wrist Angel和ROAD）上的实验表明，F1$_w$能揭示传统指标无法捕捉的性能模式，并显著优于随机和零基线。

Conclusion: F1$_w$填补了时间序列评估的关键空白，为医疗应用提供了灵活且实用的评估方法。

Abstract: Accurate evaluation of event detection in time series is essential for
applications such as stress monitoring with wearable devices, where ground
truth is typically annotated as single-point events, even though the underlying
phenomena are gradual and temporally diffused. Standard metrics like F1 and
point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such
real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$)
that incorporates temporal tolerance, enabling a more robust assessment of
event detection when exact alignment is unrealistic. Empirical analysis in
three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one
experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance
patterns invisible to conventional metrics, while its window size can be
adapted to domain knowledge to avoid overestimation. We show that the choice of
evaluation metric strongly influences the interpretation of model performance:
using predictions from TimesFM, only our temporally tolerant metrics reveal
statistically significant improvements over random and null baselines in the
two in-the-wild use cases. This work addresses key gaps in time series
evaluation and provides practical guidance for healthcare applications where
requirements for temporal precision vary by context.

</details>


### [100] [Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network](https://arxiv.org/abs/2509.03241)
*Pujitha Mamillapalli,Yoghitha Ramamoorthi,Abhinav Kumar,Tomoki Murakami,Tomoaki Ogawa,Yasushi Takatori*

Main category: cs.LG

TL;DR: 提出了一种基于五层全连接神经网络（FNN）的RIS元素分配方法，显著降低了计算复杂度并提高了系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 无线系统中对高数据速率和无缝连接的需求推动了RIS和AI应用的发展，但传统迭代优化方法计算复杂度高且难以扩展。

Method: 采用FNN结合预处理技术，降低输入维度和计算复杂度，优化RIS相位配置和资源分配。

Result: 仿真结果显示，该方法比现有方案提高系统吞吐量6.8%，同时显著降低计算复杂度。

Conclusion: 提出的神经网络方法在性能和可扩展性上优于传统迭代算法。

Abstract: The increasing demand for high data rates and seamless connectivity in
wireless systems has sparked significant interest in reconfigurable intelligent
surfaces (RIS) and artificial intelligence-based wireless applications. RIS
typically comprises passive reflective antenna elements that control the
wireless propagation environment by adequately tuning the phase of the
reflective elements. The allocation of RIS elements to multipleuser equipment
(UEs) is crucial for efficiently utilizing RIS. In this work, we formulate a
joint optimization problem that optimizes the RIS phase configuration and
resource allocation under an $\alpha$-fair scheduling framework and propose an
efficient way of allocating RIS elements. Conventional iterative optimization
methods, however, suffer from exponentially increasing computational complexity
as the number of RIS elements increases and also complicate the generation of
training labels for supervised learning. To overcome these challenges, we
propose a five-layer fully connected neural network (FNN) combined with a
preprocessing technique to significantly reduce input dimensionality, lower
computational complexity, and enhance scalability. The simulation results show
that our proposed NN-based solution reduces computational overhead while
significantly improving system throughput by 6.8% compared to existing RIS
element allocation schemes. Furthermore, the proposed system achieves better
performance while reducing computational complexity, making it significantly
more scalable than the iterative optimization algorithms.

</details>


### [101] [TopoMap: A Feature-based Semantic Discriminator of the Topographical Regions in the Test Input Space](https://arxiv.org/abs/2509.03242)
*Gianmarco De Vita,Nargiz Humbatova,Paolo Tonella*

Main category: cs.LG

TL;DR: TopoMap是一种黑盒、模型无关的方法，通过输入特征空间的地形图来测试深度学习模型，有效区分输入特征并优化聚类配置。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习测试方法可能忽略某些导致模型失败的输入特征，需要一种更全面的方法来分组这些输入。

Method: TopoMap通过降维和聚类技术生成输入特征空间的地形图，并使用DNN自动选择最优配置。

Result: TopoMap生成的区域可区分且有意义，在突变分析中表现优于随机选择，对可杀死和非可杀死突变体的效果分别提升35%和61%。

Conclusion: TopoMap为深度学习测试提供了一种有效且自动化的方法，能够显著提升测试效率。

Abstract: Testing Deep Learning (DL)-based systems is an open challenge. Although it is
relatively easy to find inputs that cause a DL model to misbehave, the grouping
of inputs by features that make the DL model under test fail is largely
unexplored. Existing approaches for DL testing introduce perturbations that may
focus on specific failure-inducing features, while neglecting others that
belong to different regions of the feature space. In this paper, we create an
explicit topographical map of the input feature space. Our approach, named
TopoMap, is both black-box and model-agnostic as it relies solely on features
that characterise the input space. To discriminate the inputs according to the
specific features they share, we first apply dimensionality reduction to obtain
input embeddings, which are then subjected to clustering. Each DL model might
require specific embedding computations and clustering algorithms to achieve a
meaningful separation of inputs into discriminative groups. We propose a novel
way to evaluate alternative configurations of embedding and clustering
techniques. We used a deep neural network (DNN) as an approximation of a human
evaluator who could tell whether a pair of clusters can be discriminated based
on the features of the included elements. We use such a DNN to automatically
select the optimal topographical map of the inputs among all those that are
produced by different embedding/clustering configurations. The evaluation
results show that the maps generated by TopoMap consist of distinguishable and
meaningful regions. In addition, we evaluate the effectiveness of TopoMap using
mutation analysis. In particular, we assess whether the clusters in our
topographical map allow for an effective selection of mutation-killing inputs.
Experimental results show that our approach outperforms random selection by 35%
on average on killable mutants; by 61% on non-killable ones.

</details>


### [102] [FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization](https://arxiv.org/abs/2509.03244)
*Yiming Yao,Fei Liu,Liang Zhao,Xi Lin,Qingfu Zhang*

Main category: cs.LG

TL;DR: 提出FoMEMO方法，利用预训练的基础模型解决昂贵多目标优化问题，无需重新训练或更新模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个新问题重建高斯过程代理模型或依赖大量领域实验预训练深度学习模型，难以泛化且不实用。

Method: 提出FoMEMO，通过预训练基础模型（基于合成数据）实现快速上下文优化，预测偏好聚合后验。

Result: 在合成基准和实际应用中表现优异，具有强泛化能力和竞争力。

Conclusion: FoMEMO通过合成数据预训练实现高效多目标优化，适应性强且无需后续训练。

Abstract: Expensive multi-objective optimization is a prevalent and crucial concern in
many real-world scenarios, where sample-efficiency is vital due to the limited
evaluations to recover the true Pareto front for decision making. Existing
works either involve rebuilding Gaussian process surrogates from scratch for
each objective in each new problem encountered, or rely on extensive past
domain experiments for pre-training deep learning models, making them hard to
generalize and impractical to cope with various emerging applications in the
real world. To address this issue, we propose a new paradigm named FoMEMO
(Foundation Models for Expensive Multi-objective Optimization), which enables
the establishment of a foundation model conditioned on any domain trajectory
and user preference, and facilitates fast in-context optimization based on the
predicted preference-wise aggregation posteriors. Rather than accessing
extensive domain experiments in the real world, we demonstrate that
pre-training the foundation model with a diverse set of hundreds of millions of
synthetic data can lead to superior adaptability to unknown problems, without
necessitating any subsequent model training or updates in the optimization
process. We evaluate our method across a variety of synthetic benchmarks and
real-word applications, and demonstrate its superior generality and competitive
performance compared to existing methods.

</details>


### [103] [Structure Transfer: an Inference-Based Calculus for the Transformation of Representations](https://arxiv.org/abs/2509.03249)
*Daniel Raggi,Gem Stapleton,Mateja Jamnik,Aaron Stockdill,Grecia Garcia Garcia,Peter C-H. Cheng*

Main category: cs.LG

TL;DR: 提出了一种名为“结构转移”的新计算法，用于在不同表示系统之间进行表示转换，确保源和目标表示满足指定关系。


<details>
  <summary>Details</summary>
Motivation: 解决如何设计表示系统无关的技术来驱动表示转换和选择这一未解决问题。

Method: 利用编码表示系统知识的模式，通过结构转移规则生成目标表示，确保满足指定关系。

Result: 结构转移是一种系统无关的计算法，适用于多种实际场景中的表示转换。

Conclusion: 结构转移为表示转换提供了一种通用且灵活的方法，适用于多种表示系统。

Abstract: Representation choice is of fundamental importance to our ability to
communicate and reason effectively. A major unsolved problem, addressed in this
paper, is how to devise \textit{representational-system (RS) agnostic}
techniques that drive representation transformation and choice. We present a
novel calculus, called \textit{structure transfer}, that enables representation
transformation across diverse RSs. Specifically, given a \textit{source}
representation drawn from a source RS, the rules of structure transfer allow us
to generate a \textit{target} representation for a target RS. The generality of
structure transfer comes in part from its ability to ensure that the source
representation and the generated target representation satisfy \textit{any}
specified relation (such as semantic equivalence). This is done by exploiting
\textit{schemas}, which encode knowledge about RSs. Specifically, schemas can
express \textit{preservation of information} across relations between any pair
of RSs, and this knowledge is used by structure transfer to derive a structure
for the target representation which ensures that the desired relation holds. We
formalise this using Representational Systems Theory~\cite{raggi2022rst},
building on the key concept of a \textit{construction space}. The abstract
nature of construction spaces grants them the generality to model RSs of
diverse kinds, including formal languages, geometric figures and diagrams, as
well as informal notations. Consequently, structure transfer is a
system-agnostic calculus that can be used to identify alternative
representations in a wide range of practical settings.

</details>


### [104] [HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling](https://arxiv.org/abs/2509.03260)
*Minjung Park,Gyuyeon Na,Soyoun Kim,Sunyoung Moon,HyeonJeong Cha,Sangmi Chai*

Main category: cs.LG

TL;DR: HyPV-LEAD是一种数据驱动的早期预警框架，用于检测加密货币异常交易，通过窗口-视野建模、峰谷采样和双曲嵌入提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 加密货币异常交易（如混币服务、欺诈转账等）对金融完整性构成风险，但现有方法多为事后检测，缺乏预防性。

Method: HyPV-LEAD结合窗口-视野建模、峰谷采样和双曲嵌入，解决类别不平衡和网络依赖性问题。

Result: 在比特币交易数据上，HyPV-LEAD的PR-AUC达0.9624，显著优于现有方法。

Conclusion: HyPV-LEAD将异常检测从被动分类转向主动预警，为实时风险管理和金融安全提供支持。

Abstract: Abnormal cryptocurrency transactions - such as mixing services, fraudulent
transfers, and pump-and-dump operations -- pose escalating risks to financial
integrity but remain notoriously difficult to detect due to class imbalance,
temporal volatility, and complex network dependencies. Existing approaches are
predominantly model-centric and post hoc, flagging anomalies only after they
occur and thus offering limited preventive value. This paper introduces
HyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), a
data-driven early-warning framework that explicitly incorporates lead time into
anomaly detection. Unlike prior methods, HyPV-LEAD integrates three
innovations: (1) window-horizon modeling to guarantee actionable lead-time
alerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance while
preserving temporal continuity, and (3) hyperbolic embedding to capture the
hierarchical and scale-free properties of blockchain transaction networks.
Empirical evaluation on large-scale Bitcoin transaction data demonstrates that
HyPV-LEAD consistently outperforms state-of-the-art baselines, achieving a
PR-AUC of 0.9624 with significant gains in precision and recall. Ablation
studies further confirm that each component - PV sampling, hyperbolic
embedding, and structural-temporal modeling - provides complementary benefits,
with the full framework delivering the highest performance. By shifting anomaly
detection from reactive classification to proactive early-warning, HyPV-LEAD
establishes a robust foundation for real-time risk management, anti-money
laundering (AML) compliance, and financial security in dynamic blockchain
environments.

</details>


### [105] [Estudio de la eficiencia en la escalabilidad de GPUs para el entrenamiento de Inteligencia Artificial](https://arxiv.org/abs/2509.03263)
*David Cortes,Carlos Juiz,Belen Bermejo*

Main category: cs.LG

TL;DR: 论文分析了MLPerf Training v4.1在BERT、Llama2 LoRA、RetinaNet和Stable Diffusion四种任务上的性能，提出了优化性能、GPU使用和效率的配置方案。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型的训练是科学界和工业界的关键挑战，GPU的大量使用虽然能加速训练，但会影响效率。

Method: 通过分析MLPerf Training v4.1在四种任务上的时间数据，研究性能与效率的平衡点。

Result: 发现存在一种配置可以优化性能与效率的关系，并找到平衡点以减少训练时间。

Conclusion: 研究结果表明，通过合理配置可以在减少训练时间的同时最大化效率。

Abstract: Training large-scale deep learning models has become a key challenge for the
scientific community and industry. While the massive use of GPUs can
significantly speed up training times, this approach has a negative impact on
efficiency. In this article, we present a detailed analysis of the times
reported by MLPerf Training v4.1 on four workloads: BERT, Llama2 LoRA,
RetinaNet, and Stable Diffusion, showing that there are configurations that
optimise the relationship between performance, GPU usage, and efficiency. The
results point to a break-even point that allows training times to be reduced
while maximising efficiency.

</details>


### [106] [Meta-Imputation Balanced (MIB): An Ensemble Approach for Handling Missing Data in Biomedical Machine Learning](https://arxiv.org/abs/2509.03316)
*Fatemeh Azad,Zoran Bosnić,Matjaž Kukar*

Main category: cs.LG

TL;DR: 提出了一种名为Meta-Imputation Balanced (MIB)的新方法，通过结合多种基础填补方法的输出来更准确地预测缺失值。


<details>
  <summary>Details</summary>
Motivation: 缺失数据是机器学习中的常见问题，尤其在生物信息学和临床机器学习中，现有填补方法在不同数据集和缺失机制下表现不一致。

Method: 利用合成掩码数据训练MIB，学习根据每种方法的行为预测最合适的填补值。

Result: MIB展示了集成学习在填补中的潜力，提高了填补的准确性和鲁棒性。

Conclusion: 该方法为现实机器学习系统提供了更强大、模块化和可解释的预处理流程。

Abstract: Missing data represents a fundamental challenge in machine learning
applications, often reducing model performance and reliability. This problem is
particularly acute in fields like bioinformatics and clinical machine learning,
where datasets are frequently incomplete due to the nature of both data
generation and data collection. While numerous imputation methods exist, from
simple statistical techniques to advanced deep learning models, no single
method consistently performs well across diverse datasets and missingness
mechanisms. This paper proposes a novel Meta-Imputation approach that learns to
combine the outputs of multiple base imputers to predict missing values more
accurately. By training the proposed method called Meta-Imputation Balanced
(MIB) on synthetically masked data with known ground truth, the system learns
to predict the most suitable imputed value based on the behavior of each
method. Our work highlights the potential of ensemble learning in imputation
and paves the way for more robust, modular, and interpretable preprocessing
pipelines in real-world machine learning systems.

</details>


### [107] [EvolveSignal: A Large Language Model Powered Coding Agent for Discovering Traffic Signal Control Algorithms](https://arxiv.org/abs/2509.03335)
*Leizhen Wang,Peibo Duan,Hao Wang,Yue Wang,Jian Xu,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于大型语言模型（LLMs）的EvolveSignal方法，用于自动发现新的交通信号控制算法，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 固定时间交通信号控制依赖手工公式和人工调整，效率低且效果不佳，尤其在复杂或拥堵条件下。

Method: 将问题建模为程序合成，通过外部评估和进化搜索迭代优化Python函数表示的候选算法。

Result: 实验显示，新算法比Webster基线平均延迟减少20.1%，平均停车次数减少47.1%。

Conclusion: EvolveSignal为交通信号控制开辟了新方向，结合AI与交通工程，提供实用见解。

Abstract: In traffic engineering, the fixed-time traffic signal control remains widely
used for its low cost, stability, and interpretability. However, its design
depends on hand-crafted formulas (e.g., Webster) and manual re-timing by
engineers to adapt to demand changes, which is labor-intensive and often yields
suboptimal results under heterogeneous or congested conditions. This paper
introduces the EvolveSignal, a large language models (LLMs) powered coding
agent to automatically discover new traffic signal control algorithms. We
formulate the problem as program synthesis, where candidate algorithms are
represented as Python functions with fixed input-output structures, and
iteratively optimized through external evaluations (e.g., a traffic simulator)
and evolutionary search. Experiments on a signalized intersection demonstrate
that the discovered algorithms outperform Webster's baseline, reducing average
delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and
incremental analyses reveal that EvolveSignal modifications-such as adjusting
cycle length bounds, incorporating right-turn demand, and rescaling green
allocations-can offer practically meaningful insights for traffic engineers.
This work opens a new research direction by leveraging AI for algorithm design
in traffic signal control, bridging program synthesis with transportation
engineering.

</details>


### [108] [Equivariant Flow Matching for Symmetry-Breaking Bifurcation Problems](https://arxiv.org/abs/2509.03340)
*Fleur Hendriks,Ondřej Rokoš,Martin Doškář,Marc G. D. Geers,Vlado Menkovski*

Main category: cs.LG

TL;DR: 提出基于流匹配的生成框架，建模分岔现象的多模态分布，优于非概率和变分方法。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统中的分岔现象常导致多稳态解，传统确定性机器学习方法难以捕捉这种多解性。

Method: 采用流匹配框架，通过等变建模保持系统对称性，引入对称匹配策略。

Result: 在多种系统（从玩具模型到复杂物理问题）中验证，流匹配显著优于其他方法。

Conclusion: 为高维系统中的多稳态建模提供了可扩展的解决方案。

Abstract: Bifurcation phenomena in nonlinear dynamical systems often lead to multiple
coexisting stable solutions, particularly in the presence of symmetry breaking.
Deterministic machine learning models struggle to capture this multiplicity,
averaging over solutions and failing to represent lower-symmetry outcomes. In
this work, we propose a generative framework based on flow matching to model
the full probability distribution over bifurcation outcomes. Our method enables
direct sampling of multiple valid solutions while preserving system symmetries
through equivariant modeling. We introduce a symmetric matching strategy that
aligns predicted and target outputs under group actions, allowing accurate
learning in equivariant settings. We validate our approach on a range of
systems, from toy models to complex physical problems such as buckling beams
and the Allen-Cahn equation. Our results demonstrate that flow matching
significantly outperforms non-probabilistic and variational methods in
capturing multimodal distributions and symmetry-breaking bifurcations, offering
a principled and scalable solution for modeling multistability in
high-dimensional systems.

</details>


### [109] [On the MIA Vulnerability Gap Between Private GANs and Diffusion Models](https://arxiv.org/abs/2509.03341)
*Ilana Sebag,Jean-Yves Franceschi,Alain Rakotomamonjy,Alexandre Allauzen,Jamal Atif*

Main category: cs.LG

TL;DR: GANs和扩散模型在差分隐私（DP）训练下对成员推理攻击（MIA）的敏感性存在显著差异，GANs表现出更强的隐私鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究GANs和扩散模型在差分隐私训练下对成员推理攻击的敏感性差异，以评估其隐私风险。

Method: 通过稳定性分析和标准化MIA流程，对GANs和扩散模型进行理论和实证分析。

Result: GANs对数据扰动的敏感性低于扩散模型，且在强DP机制下仍表现出更强的隐私保护能力。

Conclusion: 模型类型对隐私泄漏有重要影响，GANs在抵抗MIA方面具有结构优势。

Abstract: Generative Adversarial Networks (GANs) and diffusion models have emerged as
leading approaches for high-quality image synthesis. While both can be trained
under differential privacy (DP) to protect sensitive data, their sensitivity to
membership inference attacks (MIAs), a key threat to data confidentiality,
remains poorly understood. In this work, we present the first unified
theoretical and empirical analysis of the privacy risks faced by differentially
private generative models. We begin by showing, through a stability-based
analysis, that GANs exhibit fundamentally lower sensitivity to data
perturbations than diffusion models, suggesting a structural advantage in
resisting MIAs. We then validate this insight with a comprehensive empirical
study using a standardized MIA pipeline to evaluate privacy leakage across
datasets and privacy budgets. Our results consistently reveal a marked privacy
robustness gap in favor of GANs, even in strong DP regimes, highlighting that
model type alone can critically shape privacy leakage.

</details>


### [110] [epiGPTope: A machine learning-based epitope generator and classifier](https://arxiv.org/abs/2509.03351)
*Natalia Flechas Manrique,Alberto Martínez,Elena López-Martínez,Luc Andrea,Román Orus,Aitor Manteca,Aitziber L. Cortajarena,Llorenç Espinosa-Portalés*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型epiGPTope的方法，用于生成新型线性表位序列，并结合统计分类器预测其来源，以加速表位发现。


<details>
  <summary>Details</summary>
Motivation: 由于线性表位的组合序列空间巨大（20^n），传统筛选方法效率低下，需要一种更高效的方法生成和筛选表位序列。

Method: 使用预训练的epiGPTope模型生成表位样序列，并训练统计分类器预测其细菌或病毒来源。

Result: 生成的序列具有与已知表位相似的统计特性，可用于构建候选库，并通过分类器进一步筛选。

Conclusion: 该方法无需几何框架或手工特征，可更快速、经济地生成和筛选表位，推动新生物技术的发展。

Abstract: Epitopes are short antigenic peptide sequences which are recognized by
antibodies or immune cell receptors. These are central to the development of
immunotherapies, vaccines, and diagnostics. However, the rational design of
synthetic epitope libraries is challenging due to the large combinatorial
sequence space, $20^n$ combinations for linear epitopes of n amino acids,
making screening and testing unfeasible, even with high throughput experimental
techniques. In this study, we present a large language model, epiGPTope,
pre-trained on protein data and specifically fine-tuned on linear epitopes,
which for the first time can directly generate novel epitope-like sequences,
which are found to possess statistical properties analogous to the ones of
known epitopes. This generative approach can be used to prepare libraries of
epitope candidate sequences. We further train statistical classifiers to
predict whether an epitope sequence is of bacterial or viral origin, thus
narrowing the candidate library and increasing the likelihood of identifying
specific epitopes. We propose that such combination of generative and
predictive models can be of assistance in epitope discovery. The approach uses
only primary amino acid sequences of linear epitopes, bypassing the need for a
geometric framework or hand-crafted features of the sequences. By developing a
method to create biologically feasible sequences, we anticipate faster and more
cost-effective generation and screening of synthetic epitopes, with relevant
applications in the development of new biotechnologies.

</details>


### [111] [Fair Resource Allocation for Fleet Intelligence](https://arxiv.org/abs/2509.03353)
*Oguzhan Baser,Kaan Kale,Po-han Li,Sandeep Chinchali*

Main category: cs.LG

TL;DR: Fair-Synergy框架通过多维机器学习效用优化，实现多智能体资源公平分配，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统方法忽视智能体计算能力和环境复杂性，导致资源分配低效且不公平。

Method: 提出Fair-Synergy框架，利用智能体准确性与资源的凹关系，扩展传统方法至多维效用空间。

Result: 在BERT等模型上测试，多智能体推理和学习性能分别提升25%和11%。

Conclusion: Fair-Synergy为公平的群体智能提供了有效解决方案，并分析了公平性对不同智能体的影响。

Abstract: Resource allocation is crucial for the performance optimization of
cloud-assisted multi-agent intelligence. Traditional methods often overlook
agents' diverse computational capabilities and complex operating environments,
leading to inefficient and unfair resource distribution. To address this, we
open-sourced Fair-Synergy, an algorithmic framework that utilizes the concave
relationship between the agents' accuracy and the system resources to ensure
fair resource allocation across fleet intelligence. We extend traditional
allocation approaches to encompass a multidimensional machine learning utility
landscape defined by model parameters, training data volume, and task
complexity. We evaluate Fair-Synergy with advanced vision and language models
such as BERT, VGG16, MobileNet, and ResNets on datasets including MNIST,
CIFAR-10, CIFAR-100, BDD, and GLUE. We demonstrate that Fair-Synergy
outperforms standard benchmarks by up to 25% in multi-agent inference and 11%
in multi-agent learning settings. Also, we explore how the level of fairness
affects the least advantaged, most advantaged, and average agents, providing
insights for equitable fleet intelligence.

</details>


### [112] [Some patterns of sleep quality and Daylight Saving Time across countries: a predictive and exploratory analysis](https://arxiv.org/abs/2509.03358)
*Bhanu Sharma,Eugene Pinsky*

Main category: cs.LG

TL;DR: 研究分析了61个国家的平均睡眠时长，探讨夏令时（DST）对睡眠的影响，发现DST国家的平均睡眠时长更长，且纬度对此有调节作用。


<details>
  <summary>Details</summary>
Motivation: 探讨夏令时（DST）对全球不同国家睡眠时长的影响，并分析地理位置的调节作用。

Method: 对61个国家的睡眠数据进行分析，通过统计相关性和分组比较（DST与非DST国家），并结合纬度因素。

Result: DST国家的平均睡眠时长更长；低纬度地区DST国家睡眠更短，高纬度地区则更长。

Conclusion: 夏令时对睡眠的影响受地理位置调节，高纬度地区DST可能更有利于睡眠。

Abstract: In this study we analyzed average sleep durations across 61 countries to
examine the impact of Daylight Saving Time (DST) practices. Key metrics
influencing sleep were identified, and statistical correlation analysis was
applied to explore relationships among these factors. Countries were grouped
based on DST observance, and visualizations compared sleep patterns between DST
and non-DST regions. Results show that, on average, countries observing DST
tend to report longer sleep durations than those that do not. A more detailed
pattern emerged when accounting for latitude: at lower latitudes, DST-observing
countries reported shorter sleep durations compared to non-DST countries, while
at higher latitudes, DST-observing countries reported longer average sleep
durations. These findings suggest that the influence of DST on sleep may be
moderated by geographical location.

</details>


### [113] [The distribution of calibrated likelihood functions on the probability-likelihood Aitchison simplex](https://arxiv.org/abs/2509.03365)
*Paul-Gauthier Noé,Andreas Nautsch,Driss Matrouf,Pierre-Michel Bousquet,Jean-François Bonastre*

Main category: cs.LG

TL;DR: 本文扩展了似然函数的校准概念，从二元假设到多元假设，利用Aitchison几何和等距对数比变换，提出了校准、幂等性和分布约束的新定义，并应用于机器学习中的非线性判别分析。


<details>
  <summary>Details</summary>
Motivation: 传统校准研究主要集中在概率预测上，而本文关注似然函数的校准，尤其是在多元假设下的扩展，以提升方法的可解释性和可靠性。

Method: 使用Aitchison几何和等距对数比变换，将二元情况下的对数似然比（LLR）和权重证据扩展到多元假设，并定义校准、幂等性和分布约束。

Result: 提出了多元假设下似然函数校准的新框架，并通过非线性判别分析展示了其应用，提升了方法的可解释性和可靠性。

Conclusion: 本文为多元假设下的似然函数校准提供了理论基础和实际应用，扩展了传统二元校准的局限性。

Abstract: While calibration of probabilistic predictions has been widely studied, this
paper rather addresses calibration of likelihood functions. This has been
discussed, especially in biometrics, in cases with only two exhaustive and
mutually exclusive hypotheses (classes) where likelihood functions can be
written as log-likelihood-ratios (LLRs). After defining calibration for LLRs
and its connection with the concept of weight-of-evidence, we present the
idempotence property and its associated constraint on the distribution of the
LLRs. Although these results have been known for decades, they have been
limited to the binary case. Here, we extend them to cases with more than two
hypotheses by using the Aitchison geometry of the simplex, which allows us to
recover, in a vector form, the additive form of the Bayes' rule; extending
therefore the LLR and the weight-of-evidence to any number of hypotheses.
Especially, we extend the definition of calibration, the idempotence, and the
constraint on the distribution of likelihood functions to this multiple
hypotheses and multiclass counterpart of the LLR: the isometric-log-ratio
transformed likelihood function. This work is mainly conceptual, but we still
provide one application to machine learning by presenting a non-linear
discriminant analysis where the discriminant components form a calibrated
likelihood function over the classes, improving therefore the interpretability
and the reliability of the method.

</details>


### [114] [Cluster and then Embed: A Modular Approach for Visualization](https://arxiv.org/abs/2509.03373)
*Elizabeth Coda,Ery Arias-Castro,Gal Mishne*

Main category: cs.LG

TL;DR: 提出了一种模块化的降维方法，先聚类再嵌入，最后对齐，以更透明地保留数据的全局几何结构。


<details>
  <summary>Details</summary>
Motivation: t-SNE和UMAP在降维时倾向于扭曲数据的全局几何结构，因此需要一种更透明的方法来同时保留局部和全局信息。

Method: 先对数据进行聚类，然后分别嵌入每个聚类，最后对齐这些聚类以获得全局嵌入。

Result: 在合成和真实数据集上验证了该方法的竞争力，且比现有方法更透明。

Conclusion: 提出的模块化方法在保留数据全局结构方面优于t-SNE和UMAP，同时保持了透明性。

Abstract: Dimensionality reduction methods such as t-SNE and UMAP are popular methods
for visualizing data with a potential (latent) clustered structure. They are
known to group data points at the same time as they embed them, resulting in
visualizations with well-separated clusters that preserve local information
well. However, t-SNE and UMAP also tend to distort the global geometry of the
underlying data. We propose a more transparent, modular approach consisting of
first clustering the data, then embedding each cluster, and finally aligning
the clusters to obtain a global embedding. We demonstrate this approach on
several synthetic and real-world datasets and show that it is competitive with
existing methods, while being much more transparent.

</details>


### [115] [Exploring a Graph-based Approach to Offline Reinforcement Learning for Sepsis Treatment](https://arxiv.org/abs/2509.03393)
*Taisiya Khakharova,Lucas Sakizloglou,Leen Lambers*

Main category: cs.LG

TL;DR: 论文提出了一种基于图神经网络的强化学习方法，用于优化脓毒症患者的液体和血管加压素治疗决策。


<details>
  <summary>Details</summary>
Motivation: 脓毒症治疗中，确定合适的液体和血管加压素剂量具有挑战性，传统方法依赖关系数据，而现代医疗数据的复杂性促使采用图结构表示数据。

Method: 将MIMIC-III数据集建模为动态异构图，使用GraphSAGE和GATv2两种图神经网络学习患者状态表示，并与dBCQ算法结合进行策略学习。

Result: 实验证实了图方法的潜力，同时揭示了表示学习在该领域的复杂性。

Conclusion: 图神经网络为脓毒症治疗决策提供了新思路，但表示学习仍需进一步优化。

Abstract: Sepsis is a serious, life-threatening condition. When treating sepsis, it is
challenging to determine the correct amount of intravenous fluids and
vasopressors for a given patient. While automated reinforcement learning
(RL)-based methods have been used to support these decisions with promising
results, previous studies have relied on relational data. Given the complexity
of modern healthcare data, representing data as a graph may provide a more
natural and effective approach. This study models patient data from the
well-known MIMIC-III dataset as a heterogeneous graph that evolves over time.
Subsequently, we explore two Graph Neural Network architectures - GraphSAGE and
GATv2 - for learning patient state representations, adopting the approach of
decoupling representation learning from policy learning. The encoders are
trained to produce latent state representations, jointly with decoders that
predict the next patient state. These representations are then used for policy
learning with the dBCQ algorithm. The results of our experimental evaluation
confirm the potential of a graph-based approach, while highlighting the
complexity of representation learning in this domain.

</details>


### [116] [Beyond Correctness: Harmonizing Process and Outcome Rewards through RL Training](https://arxiv.org/abs/2509.03403)
*Chenlu Ye,Zhou Yu,Ziji Zhang,Hao Chen,Narayanan Sadagopan,Jing Huang,Tong Zhang,Anurag Beniwal*

Main category: cs.LG

TL;DR: PROF方法通过一致性驱动的样本选择，结合过程奖励模型（PRM）和结果奖励模型（ORM），显著提升了数学推理任务的准确性和中间步骤质量。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR中的ORM过于粗粒度，无法区分正确答案中的错误推理或错误答案中的有效推理，而PRM虽然细粒度但易受奖励攻击和不准确性影响。

Method: 提出PROF方法，通过一致性驱动的样本选择，结合PRM和ORM的优势，保留高过程值的正确响应和低过程值的错误响应。

Result: 实验表明，PROF比混合方法提高了4%以上的最终准确性，并增强了中间推理步骤的质量。

Conclusion: PROF是一种有效的数据处理方法，能够协调噪声细粒度过程奖励与准确粗粒度结果奖励，显著提升推理任务性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged to be a
predominant paradigm for mathematical reasoning tasks, offering stable
improvements in reasoning ability. However, Outcome Reward Models (ORMs) in
RLVR are too coarse-grained to distinguish flawed reasoning within correct
answers or valid reasoning within incorrect answers. This lack of granularity
introduces noisy and misleading gradients significantly and hinders further
progress in reasoning process quality. While Process Reward Models (PRMs) offer
fine-grained guidance for intermediate steps, they frequently suffer from
inaccuracies and are susceptible to reward hacking.
  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an
effective data process curation method that harmonizes noisy, fine-grained
process rewards with accurate, coarse-grained outcome rewards. Rather than
naively blending PRM and ORM in the objective function
(arXiv:archive/2506.18896), PROF leverages their complementary strengths
through consistency-driven sample selection. Our approach retains correct
responses with higher averaged process values and incorrect responses with
lower averaged process values, while maintaining positive/negative training
sample balance. Extensive experiments demonstrate that our method not only
consistently improves the final accuracy over $4\%$ compared to the blending
approaches, but also strengthens the quality of intermediate reasoning steps.
Codes and training recipes are available at https://github.com/Chenluye99/PROF.

</details>


### [117] [Initialization Schemes for Kolmogorov-Arnold Networks: An Empirical Study](https://arxiv.org/abs/2509.03417)
*Spyros Rigas,Dhruv Verma,Georgios Alexandridis,Yixuan Wang*

Main category: cs.LG

TL;DR: 研究了Kolmogorov-Arnold Networks (KANs)的初始化策略，提出了两种理论驱动的方法和一种经验性的幂律初始化方法，发现Glorot-inspired初始化在参数丰富的模型中表现最佳，而幂律初始化在整体任务和不同架构中表现最强。


<details>
  <summary>Details</summary>
Motivation: KANs的初始化策略尚未充分探索，影响了其灵活性和可解释性的潜力。

Method: 提出了两种理论驱动的初始化方法（LeCun和Glorot启发）和一种经验性的幂律初始化方法，并通过大规模网格搜索、Neural Tangent Kernel分析和Feynman数据集评估了这些方法。

Result: Glorot-inspired初始化在参数丰富的模型中表现最佳，幂律初始化在整体任务和不同架构中表现最强。

Conclusion: 幂律初始化是KANs的最佳选择，尤其是在多样化的任务和架构中。

Abstract: Kolmogorov-Arnold Networks (KANs) are a recently introduced neural
architecture that replace fixed nonlinearities with trainable activation
functions, offering enhanced flexibility and interpretability. While KANs have
been applied successfully across scientific and machine learning tasks, their
initialization strategies remain largely unexplored. In this work, we study
initialization schemes for spline-based KANs, proposing two theory-driven
approaches inspired by LeCun and Glorot, as well as an empirical power-law
family with tunable exponents. Our evaluation combines large-scale grid
searches on function fitting and forward PDE benchmarks, an analysis of
training dynamics through the lens of the Neural Tangent Kernel, and
evaluations on a subset of the Feynman dataset. Our findings indicate that the
Glorot-inspired initialization significantly outperforms the baseline in
parameter-rich models, while power-law initialization achieves the strongest
performance overall, both across tasks and for architectures of varying size.
All code and data accompanying this manuscript are publicly available at
https://github.com/srigas/KAN_Initialization_Schemes.

</details>


### [118] [LINKER: Learning Interactions Between Functional Groups and Residues With Chemical Knowledge-Enhanced Reasoning and Explainability](https://arxiv.org/abs/2509.03425)
*Phuc Pham,Viet Thanh Duy Nguyen,Truong-Son Hy*

Main category: cs.LG

TL;DR: LINKER是一种基于序列的模型，仅需蛋白质序列和配体SMILES作为输入，预测残基-功能基团相互作用。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型依赖3D结构或距离标签，限制了适用性和生物学相关性。

Method: LINKER通过结构监督的注意力机制训练，从3D复合物中提取功能基团标签。

Result: 在LP-PDBBind基准测试中，预测结果与真实生化注释高度一致。

Conclusion: LINKER在无结构数据时仍能大规模应用，且预测更具化学意义。

Abstract: Accurate identification of interactions between protein residues and ligand
functional groups is essential to understand molecular recognition and guide
rational drug design. Existing deep learning approaches for protein-ligand
interpretability often rely on 3D structural input or use distance-based
contact labels, limiting both their applicability and biological relevance. We
introduce LINKER, the first sequence-based model to predict residue-functional
group interactions in terms of biologically defined interaction types, using
only protein sequences and the ligand SMILES as input. LINKER is trained with
structure-supervised attention, where interaction labels are derived from 3D
protein-ligand complexes via functional group-based motif extraction. By
abstracting ligand structures into functional groups, the model focuses on
chemically meaningful substructures while predicting interaction types rather
than mere spatial proximity. Crucially, LINKER requires only sequence-level
input at inference time, enabling large-scale application in settings where
structural data is unavailable. Experiments on the LP-PDBBind benchmark
demonstrate that structure-informed supervision over functional group
abstractions yields interaction predictions closely aligned with ground-truth
biochemical annotations.

</details>


### [119] [Graph neural networks for learning liquid simulations in dynamic scenes containing kinematic objects](https://arxiv.org/abs/2509.03446)
*Niteesh Midlagajni,Constantin A. Rothkopf*

Main category: cs.LG

TL;DR: 提出了一种基于GNN的框架，用于模拟液体与刚体交互的动态行为，支持复杂场景和任务泛化。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在模拟液体与动态刚体交互时表现不足，需要更通用的解决方案。

Method: 使用GNN表示粒子为图节点，结合BVH算法处理碰撞，支持复杂几何交互。

Result: 模型能准确捕捉动态液体行为，泛化到未见物体和任务（如搅拌、舀取）。

Conclusion: 该框架为液体动态模拟和控制任务提供了高效且通用的解决方案。

Abstract: Simulating particle dynamics with high fidelity is crucial for solving
real-world interaction and control tasks involving liquids in design, graphics,
and robotics. Recently, data-driven approaches, particularly those based on
graph neural networks (GNNs), have shown progress in tackling such problems.
However, these approaches are often limited to learning fluid behavior in
static free-fall environments or simple manipulation settings involving
primitive objects, often overlooking complex interactions with dynamically
moving kinematic rigid bodies. Here, we propose a GNN-based framework designed
from the ground up to learn the dynamics of liquids under rigid body
interactions and active manipulations, where particles are represented as graph
nodes and particle-object collisions are handled using surface representations
with the bounding volume hierarchy (BVH) algorithm. This approach enables the
network to model complex interactions between liquid particles and intricate
surface geometries. Our model accurately captures fluid behavior in dynamic
settings and can also function as a simulator in static free-fall environments.
Despite being trained on a single-object manipulation task of pouring, our
model generalizes effectively to environments with unseen objects and novel
manipulation tasks such as stirring and scooping. Finally, we show that the
learned dynamics can be leveraged to solve control and manipulation tasks using
gradient-based optimization methods.

</details>


### [120] [DPQuant: Efficient and Differentially-Private Model Training via Dynamic Quantization Scheduling](https://arxiv.org/abs/2509.03472)
*Yubo Gao,Renbo Tu,Gennady Pekhimenko,Nandita Vijaykumar*

Main category: cs.LG

TL;DR: DP-SGD中量化导致更高的精度损失，QPQuant通过动态量化层选择和损失感知优先化减少影响。


<details>
  <summary>Details</summary>
Motivation: 量化在DP-SGD中导致显著更高的精度损失，需解决噪声注入放大量化方差的问题。

Method: 提出QPQuant框架，动态选择量化层，结合概率采样和损失感知优先化。

Result: 在多个模型和数据集上，QPQuant优于静态量化，实现接近最优的精度-计算权衡。

Conclusion: QPQuant有效减少量化方差，提升DP-SGD性能，适用于低精度硬件。

Abstract: Differentially-Private SGD (DP-SGD) is a powerful technique to protect user
privacy when using sensitive data to train neural networks. During training,
converting model weights and activations into low-precision formats, i.e.,
quantization, can drastically reduce training times, energy consumption, and
cost, and is thus a widely used technique. In this work, we demonstrate that
quantization causes significantly higher accuracy degradation in DP-SGD
compared to regular SGD. We observe that this is caused by noise injection in
DP-SGD, which amplifies quantization variance, leading to disproportionately
large accuracy degradation. To address this challenge, we present QPQuant, a
dynamic quantization framework that adaptively selects a changing subset of
layers to quantize at each epoch. Our method combines two key ideas that
effectively reduce quantization variance: (i) probabilistic sampling of the
layers that rotates which layers are quantized every epoch, and (ii) loss-aware
layer prioritization, which uses a differentially private loss sensitivity
estimator to identify layers that can be quantized with minimal impact on model
quality. This estimator consumes a negligible fraction of the overall privacy
budget, preserving DP guarantees. Empirical evaluations on ResNet18, ResNet50,
and DenseNet121 across a range of datasets demonstrate that DPQuant
consistently outperforms static quantization baselines, achieving near
Pareto-optimal accuracy-compute trade-offs and up to 2.21x theoretical
throughput improvements on low-precision hardware, with less than 2% drop in
validation accuracy.

</details>


### [121] [Geometric Foundations of Tuning without Forgetting in Neural ODEs](https://arxiv.org/abs/2509.03474)
*Erkan Bayram,Mohamed-Ali Belabbas,Tamer Başar*

Main category: cs.LG

TL;DR: 论文证明了在非奇异控制下，参数子空间形成一个有限余维的Banach子流形，并描述了其切空间，为TwF方法提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 为顺序训练神经ODE中的Tuning without Forgetting (TwF)方法提供理论基础，证明其参数子空间的性质。

Method: 通过数学证明，展示参数子空间在非奇异控制下形成Banach子流形，并分析其切空间。

Result: TwF方法在顺序训练中能够精确保持映射不变性，而不仅限于一阶近似。

Conclusion: 研究为TwF方法提供了严格的理论基础，支持其在顺序训练中的有效性。

Abstract: In our earlier work, we introduced the principle of Tuning without Forgetting
(TwF) for sequential training of neural ODEs, where training samples are added
iteratively and parameters are updated within the subspace of control functions
that preserves the end-point mapping at previously learned samples on the
manifold of output labels in the first-order approximation sense. In this
letter, we prove that this parameter subspace forms a Banach submanifold of
finite codimension under nonsingular controls, and we characterize its tangent
space. This reveals that TwF corresponds to a continuation/deformation of the
control function along the tangent space of this Banach submanifold, providing
a theoretical foundation for its mapping-preserving (not forgetting) during the
sequential training exactly, beyond first-order approximation.

</details>


### [122] [Robult: Leveraging Redundancy and Modality Specific Features for Robust Multimodal Learning](https://arxiv.org/abs/2509.03477)
*Duy A. Nguyen,Abhi Kamboj,Minh N. Do*

Main category: cs.LG

TL;DR: Robult框架通过信息论方法解决多模态学习中的缺失模态和标签数据不足问题，结合PU对比损失和潜在重构损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中缺失模态和标签数据不足是主要挑战，Robult旨在通过保留模态特定信息和利用冗余来解决这些问题。

Method: Robult采用软正无标记（PU）对比损失和潜在重构损失，优化任务相关特征对齐并保留模态特定信息。

Result: 实验表明，Robult在半监督学习和缺失模态场景下优于现有方法，且轻量设计适合实际应用。

Conclusion: Robult是一个可扩展的框架，能有效提升多模态学习的鲁棒性和性能。

Abstract: Addressing missing modalities and limited labeled data is crucial for
advancing robust multimodal learning. We propose Robult, a scalable framework
designed to mitigate these challenges by preserving modality-specific
information and leveraging redundancy through a novel information-theoretic
approach. Robult optimizes two core objectives: (1) a soft Positive-Unlabeled
(PU) contrastive loss that maximizes task-relevant feature alignment while
effectively utilizing limited labeled data in semi-supervised settings, and (2)
a latent reconstruction loss that ensures unique modality-specific information
is retained. These strategies, embedded within a modular design, enhance
performance across various downstream tasks and ensure resilience to incomplete
modalities during inference. Experimental results across diverse datasets
validate that Robult achieves superior performance over existing approaches in
both semi-supervised learning and missing modality contexts. Furthermore, its
lightweight design promotes scalability and seamless integration with existing
architectures, making it suitable for real-world multimodal applications.

</details>


### [123] [SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models](https://arxiv.org/abs/2509.03487)
*Jigang Fan,Zhenghong Zhou,Ruofan Jin,Le Cong,Mengdi Wang,Zaixi Zhang*

Main category: cs.LG

TL;DR: SafeProtein是首个针对蛋白质基础模型的红队测试框架，通过多模态提示工程和启发式束搜索，揭示潜在生物安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质基础模型缺乏系统性红队测试，可能被滥用生成具有生物安全风险的蛋白质。

Method: 结合多模态提示工程和启发式束搜索，设计红队测试方法，并构建SafeProtein-Bench数据集。

Result: 在先进蛋白质基础模型上实现连续突破（ESM3攻击成功率高达70%），揭示潜在安全风险。

Conclusion: SafeProtein为前沿模型的安全防护技术开发提供了重要参考。

Abstract: Proteins play crucial roles in almost all biological processes. The
advancement of deep learning has greatly accelerated the development of protein
foundation models, leading to significant successes in protein understanding
and design. However, the lack of systematic red-teaming for these models has
raised serious concerns about their potential misuse, such as generating
proteins with biological safety risks. This paper introduces SafeProtein, the
first red-teaming framework designed for protein foundation models to the best
of our knowledge. SafeProtein combines multimodal prompt engineering and
heuristic beam search to systematically design red-teaming methods and conduct
tests on protein foundation models. We also curated SafeProtein-Bench, which
includes a manually constructed red-teaming benchmark dataset and a
comprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks
on state-of-the-art protein foundation models (up to 70% attack success rate
for ESM3), revealing potential biological safety risks in current protein
foundation models and providing insights for the development of robust security
protection technologies for frontier models. The codes will be made publicly
available at https://github.com/jigang-fan/SafeProtein.

</details>


### [124] [On Entropy Control in LLM-RL Algorithms](https://arxiv.org/abs/2509.03493)
*Han Shen*

Main category: cs.LG

TL;DR: 论文研究了在LLM-RL训练中传统熵正则化的不足，并提出了一种新的熵控制方法AEnt，通过自动调整系数和限制熵范围来提升效果。


<details>
  <summary>Details</summary>
Motivation: 传统熵正则化在LLM-RL训练中效果不佳，原因是LLM的响应空间极大且最优输出稀疏。

Method: 提出AEnt方法，使用带自动调整系数的限制熵奖励，并在更小的标记空间上重新归一化策略。

Result: AEnt在数学推理任务中表现优于基线方法。

Conclusion: AEnt通过改进熵控制机制，有效提升了LLM-RL训练的效果。

Abstract: For RL algorithms, appropriate entropy control is crucial to their
effectiveness. To control the policy entropy, a commonly used method is entropy
regularization, which is adopted in various popular RL algorithms including
PPO, SAC and A3C. Although entropy regularization proves effective in robotic
and games RL conventionally, studies found that it gives weak to no gains in
LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL
setting. Specifically, we first argue that the conventional entropy
regularization suffers from the LLM's extremely large response space and the
sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy
control method that utilizes a new clamped entropy bonus with an automatically
adjusted coefficient. The clamped entropy is evaluated with the re-normalized
policy defined on certain smaller token space, which encourages exploration
within a more compact response set. In addition, the algorithm automatically
adjusts entropy coefficient according to the clamped entropy value, effectively
controlling the entropy-induced bias while leveraging the entropy's benefits.
AEnt is tested in math-reasoning tasks under different base models and
datasets, and it is observed that AEnt outperforms the baselines consistently
across multiple benchmarks.

</details>


### [125] [Invariant Features for Global Crop Type Classification](https://arxiv.org/abs/2509.03497)
*Xin-Yi Tong,Sherrie Wang*

Main category: cs.LG

TL;DR: 研究提出了一种全球作物分类方法，通过识别地理不变特征和数据增强策略，提升了跨区域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 全球作物类型及其空间分布的准确获取对粮食安全、农业政策和可持续发展至关重要，但现有遥感方法受限于地理样本不足。

Method: 构建了全球作物数据集CropGlobe，设计了轻量级CNN模型CropNet，并结合时间数据增强策略。

Result: Sentinel-2的2D中值时间特征在所有转移场景中表现最稳定，数据增强进一步提升了模型鲁棒性。

Conclusion: 研究为全球作物分类提供了更稳定的特征表示和可扩展的低成本解决方案。

Abstract: Accurately obtaining crop type and its spatial distribution at a global scale
is critical for food security, agricultural policy-making, and sustainable
development. Remote sensing offers an efficient solution for large-scale crop
classification, but the limited availability of reliable ground samples in many
regions constrains applicability across geographic areas. To address
performance declines under geospatial shifts, this study identifies remote
sensing features that are invariant to geographic variation and proposes
strategies to enhance cross-regional generalization. We construct CropGlobe, a
global crop type dataset with 300,000 pixel-level samples from eight countries
across five continents, covering six major food and industrial crops (corn,
soybeans, rice, wheat, sugarcane, cotton). With broad geographic coverage,
CropGlobe enables a systematic evaluation under cross-country, cross-continent,
and cross-hemisphere transfer. We compare the transferability of temporal
multi-spectral features (Sentinel-2-based 1D/2D median features and harmonic
coefficients) and hyperspectral features (from EMIT). To improve generalization
under spectral and phenological shifts, we design CropNet, a lightweight and
robust CNN tailored for pixel-level crop classification, coupled with temporal
data augmentation (time shift, time scale, and magnitude warping) that
simulates realistic cross-regional phenology. Experiments show that 2D median
temporal features from Sentinel-2 consistently exhibit the strongest invariance
across all transfer scenarios, and augmentation further improves robustness,
particularly when training data diversity is limited. Overall, the work
identifies more invariant feature representations that enhance geographic
transferability and suggests a promising path toward scalable, low-cost crop
type applications across globally diverse regions.

</details>


### [126] [Warming Up for Zeroth-Order Federated Pre-Training with Low Resource Clients](https://arxiv.org/abs/2509.03503)
*Gwen Legate,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: ZOWarmUp是一种联邦学习的零阶优化器，允许低资源设备参与训练，减少通信和内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备因内存和通信限制无法参与联邦学习的问题，减少系统偏差。

Method: 利用零阶方法和方差减少技术，设计ZOWarmUp优化器，支持从随机初始化开始训练。

Result: 实验表明ZOWarmUp适用于多种场景，能提高数据多样性和训练效果。

Conclusion: ZOWarmUp为低资源设备提供了参与联邦学习的途径，提升了数据利用率和模型性能。

Abstract: Federated learning enables collaborative model training across numerous edge
devices without requiring participants to share data; however, memory and
communication constraints on these edge devices may preclude their
participation in training. We consider a setting in which a subset of edge
devices are below a critical memory or communication threshold required to
conduct model updates. Under typical federated optimization algorithms, these
devices are excluded from training which renders their data inaccessible and
increases system induced bias. We are inspired by MeZO, a zeroth-order method
used for memory-efficient fine-tuning. The increased variance inherent to
zeroth-order gradient approximations has relegated previous zeroth-order
optimizers exclusively to the domain of fine tuning; a limitation we seek to
correct. We devise a federated, memory-efficient zeroth-order optimizer,
ZOWarmUp that permits zeroth-order training from a random initialization.
ZOWarmUp leverages differing client capabilities and careful variance reduction
techniques to facilitate participation of under-represented, low-resource
clients in model training. Like other federated zeroth-order methods, ZOWarmUp
eliminates the need for edge devices to transmit their full gradients to the
server and instead relies on only a small set of random seeds, rendering the
up-link communication cost negligible. We present experiments using various
datasets and model architectures to show that ZOWarmUp is a robust algorithm
that can can be applied under a wide variety of circumstances. For systems with
a high proportion of edge devices that would otherwise be excluded from
training, this algorithm provides access to a greater volume and diversity of
data, thus improving training outcomes.

</details>


### [127] [LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence](https://arxiv.org/abs/2509.03505)
*Xingxuan Zhang,Gang Ren,Han Yu,Hao Yuan,Hui Wang,Jiansheng Li,Jiayun Wu,Lang Mo,Li Mao,Mingchao Hao,Ningbo Dai,Renzhe Xu,Shuyang Li,Tianyang Zhang,Yue He,Yuanrui Wang,Yunjia Zhang,Zijing Xu,Dongzhe Li,Fang Gao,Hao Zou,Jiandong Liu,Jiashuo Liu,Jiawei Xu,Kaijie Cheng,Kehan Li,Linjun Zhou,Qing Li,Shaohua Fan,Xiaoyu Lin,Xinyan Han,Xuanyue Li,Yan Lu,Yuan Xue,Yuanyuan Jiang,Zimu Wang,Zhenlei Wang,Peng Cui*

Main category: cs.LG

TL;DR: LimiX是一种基于结构化数据的大型模型，通过联合分布建模和查询条件预测，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动通用智能需要结合语言、物理世界和结构化数据的互补基础模型。

Method: LimiX将结构化数据视为变量和缺失值的联合分布，通过掩码联合分布建模和上下文条件目标进行预训练。

Result: LimiX在10个基准测试中超越基线模型，支持分类、回归、缺失值填补等任务。

Conclusion: LimiX展示了单一模型在多样化任务中的强大能力，且无需任务特定架构或训练。

Abstract: We argue that progress toward general intelligence requires complementary
foundation models grounded in language, the physical world, and structured
data. This report presents LimiX, the first installment of our large
structured-data models (LDMs). LimiX treats structured data as a joint
distribution over variables and missingness, thus capable of addressing a wide
range of tabular tasks through query-based conditional prediction via a single
model. LimiX is pretrained using masked joint-distribution modeling with an
episodic, context-conditional objective, where the model predicts for query
subsets conditioned on dataset-specific contexts, supporting rapid,
training-free adaptation at inference. We evaluate LimiX across 10 large
structured-data benchmarks with broad regimes of sample size, feature
dimensionality, class number, categorical-to-numerical feature ratio,
missingness, and sample-to-feature ratios. With a single model and a unified
interface, LimiX consistently surpasses strong baselines including
gradient-boosting trees, deep tabular networks, recent tabular foundation
models, and automated ensembles, as shown in Figure 1 and Figure 2. The
superiority holds across a wide range of tasks, such as classification,
regression, missing value imputation, and data generation, often by substantial
margins, while avoiding task-specific architectures or bespoke training per
task. All LimiX models are publicly accessible under Apache 2.0.

</details>


### [128] [Can LLMs Lie? Investigation beyond Hallucination](https://arxiv.org/abs/2509.03518)
*Haoran Huan,Mihir Prabhudesai,Mengning Wu,Shantanu Jaiswal,Deepak Pathak*

Main category: cs.LG

TL;DR: 论文系统研究了大型语言模型（LLM）的说谎行为，区分了幻觉与故意说谎，并探索了其神经机制和实际应用中的风险与对策。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实应用中的自主性增强，其故意说谎行为（而非幻觉）的风险尚未充分研究，亟需系统探索。

Method: 通过机制可解释性技术（如logit lens分析、因果干预和对比激活引导）识别和控制说谎行为，并引入行为引导向量进行精细调控。

Result: 揭示了LLM说谎的神经机制，建立了说谎与任务性能之间的权衡关系（Pareto前沿），并展示了故意说谎在某些场景下可能优化目标。

Conclusion: 研究为AI伦理提供了新视角，强调了在高风险环境中部署LLM时需关注说谎风险并开发相应防护措施。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
a variety of tasks, but their increasing autonomy in real-world applications
raises concerns about their trustworthiness. While hallucinations-unintentional
falsehoods-have been widely studied, the phenomenon of lying, where an LLM
knowingly generates falsehoods to achieve an ulterior objective, remains
underexplored. In this work, we systematically investigate the lying behavior
of LLMs, differentiating it from hallucinations and testing it in practical
scenarios. Through mechanistic interpretability techniques, we uncover the
neural mechanisms underlying deception, employing logit lens analysis, causal
interventions, and contrastive activation steering to identify and control
deceptive behavior. We study real-world lying scenarios and introduce
behavioral steering vectors that enable fine-grained manipulation of lying
tendencies. Further, we explore the trade-offs between lying and end-task
performance, establishing a Pareto frontier where dishonesty can enhance goal
optimization. Our findings contribute to the broader discourse on AI ethics,
shedding light on the risks and potential safeguards for deploying LLMs in
high-stakes environments. Code and more illustrations are available at
https://llm-liar.github.io/

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [129] [Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour](https://arxiv.org/abs/2509.02727)
*Guillaume Gagné-Labelle,Vassil Atanassov,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 提出一种通用的强化学习算法，用于四足机器人在动态运动场景中的控制，性能媲美专家策略，且训练效率更高。


<details>
  <summary>Details</summary>
Motivation: 四足机器人在复杂地形中具有优势，但传统建模方法难以应对其运动中的不确定性，强化学习提供了一种更优的解决方案。

Method: 采用通用强化学习算法，通过试错学习优化控制策略，减少训练所需的智能体数量。

Result: 学习到的策略性能与专家策略相当，且训练效率提高了75%。

Conclusion: 通用强化学习算法在四足机器人控制中具有潜力，并揭示了成功的关键因素。

Abstract: Climbing, crouching, bridging gaps, and walking up stairs are just a few of
the advantages that quadruped robots have over wheeled robots, making them more
suitable for navigating rough and unstructured terrain. However, executing such
manoeuvres requires precise temporal coordination and complex agent-environment
interactions. Moreover, legged locomotion is inherently more prone to slippage
and tripping, and the classical approach of modeling such cases to design a
robust controller thus quickly becomes impractical. In contrast, reinforcement
learning offers a compelling solution by enabling optimal control through trial
and error. We present a generalist reinforcement learning algorithm for
quadrupedal agents in dynamic motion scenarios. The learned policy rivals
state-of-the-art specialist policies trained using a mixture of experts
approach, while using only 25% as many agents during training. Our experiments
also highlight the key components of the generalist locomotion policy and the
primary factors contributing to its success.

</details>


### [130] [The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI](https://arxiv.org/abs/2509.02749)
*Giorgia Buracchio,Ariele Callegari,Massimo Donini,Cristina Gena,Antonio Lieto,Alberto Lillo,Claudio Mattutino,Alessandro Mazzei,Linda Pigureddu,Manuel Striani,Fabiana Vernero*

Main category: cs.RO

TL;DR: 研究探讨了情感对齐在人机交互中对机器人说服力、用户沟通风格及机器人心理状态归因的影响，发现情感对齐仅显著影响后两者。


<details>
  <summary>Details</summary>
Motivation: 探索情感对齐作为共情沟通前提在HRI中的作用，以提升机器人交互的自然性和有效性。

Method: 使用NAO机器人，比较中性沟通与情感对齐沟通两种条件，42名参与者参与实验。

Result: 情感对齐未影响用户沟通风格或说服效果，但显著提升了机器人心理状态归因和共情感知。

Conclusion: 情感对齐虽未增强说服力，但能改善用户对机器人的共情认知，对HRI设计有重要意义。

Abstract: The paper presents an experiment on the effects of adaptive emotional
alignment between agents, considered a prerequisite for empathic communication,
in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the
impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the
robot's persuasive effectiveness, (ii) the user's communication style, and
(iii) the attribution of mental states and empathy to the robot. In an
experiment with 42 participants, two conditions were compared: one with neutral
communication and another where the robot provided responses adapted to the
emotions expressed by the users. The results show that emotional alignment does
not influence users' communication styles or have a persuasive effect. However,
it significantly influences attribution of mental states to the robot and its
perceived empathy

</details>


### [131] [A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality](https://arxiv.org/abs/2509.02760)
*Maximilian Neidhardt,Ludwig Bosse,Vidas Raudonis,Kristina Allgoewer,Axel Heinemann,Benjamin Ondruschka,Alexander Schlaefer*

Main category: cs.RO

TL;DR: 研究提出了一种基于虚拟现实和数字孪生的远程机器人尸检活检系统，减少了感染风险并提高了操作效率。


<details>
  <summary>Details</summary>
Motivation: 传统尸检活检具有破坏性和感染风险，机器人活检可以减少这些风险，但需要高效的操作规划和控制方法。

Method: 使用虚拟现实和数字孪生技术，实现远程规划和机器人控制，并通过可用性研究和人体尸体实验验证系统。

Result: 完成了132次针插入，平均偏差为5.30±3.25 mm，成功获取组织样本并验证其病理学有效性。

Conclusion: 系统操作直观，是一种精确且低风险的替代方案，具有临床应用潜力。

Abstract: Studying tissue samples obtained during autopsies is the gold standard when
diagnosing the cause of death and for understanding disease pathophysiology.
Recently, the interest in post mortem minimally invasive biopsies has grown
which is a less destructive approach in comparison to an open autopsy and
reduces the risk of infection. While manual biopsies under ultrasound guidance
are more widely performed, robotic post mortem biopsies have been recently
proposed. This approach can further reduce the risk of infection for
physicians. However, planning of the procedure and control of the robot need to
be efficient and usable. We explore a virtual reality setup with a digital twin
to realize fully remote planning and control of robotic post mortem biopsies.
The setup is evaluated with forensic pathologists in a usability study for
three interaction methods. Furthermore, we evaluate clinical feasibility and
evaluate the system with three human cadavers. Overall, 132 needle insertions
were performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue
samples were successfully biopsied and histopathologically verified. Users
reported a very intuitive needle placement approach, indicating that the system
is a promising, precise, and low-risk alternative to conventional approaches.

</details>


### [132] [Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers](https://arxiv.org/abs/2509.02808)
*Isaac Ronald Ward,Mark Paral,Kristopher Riordan,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 论文提出了一种结合学习控制器和安全控制器的混合方法，用于四旋翼在未知地下环境中的自主导航。


<details>
  <summary>Details</summary>
Motivation: 地下环境中的自主控制对勘探、采矿和搜救等应用很重要，但学习控制器在训练未覆盖的环境中表现不佳。

Method: 使用归一化流对环境建模，作为运行时监测器，动态切换学习控制器和安全控制器。

Result: 在模拟3D洞穴环境中测试，混合控制器兼具学习控制器的快速性和安全控制器的避障能力。

Conclusion: 该方法有效平衡了任务完成速度和安全性，适用于复杂地下环境。

Abstract: Autonomously controlling quadrotors in large-scale subterranean environments
is applicable to many areas such as environmental surveying, mining operations,
and search and rescue. Learning-based controllers represent an appealing
approach to autonomy, but are known to not generalize well to
`out-of-distribution' environments not encountered during training. In this
work, we train a normalizing flow-based prior over the environment, which
provides a measure of how far out-of-distribution the quadrotor is at any given
time. We use this measure as a runtime monitor, allowing us to switch between a
learning-based controller and a safe controller when we are sufficiently
out-of-distribution. Our methods are benchmarked on a point-to-point navigation
task in a simulated 3D cave environment based on real-world point cloud data
from the DARPA Subterranean Challenge Final Event Dataset. Our experimental
results show that our combined controller simultaneously possesses the liveness
of the learning-based controller (completing the task quickly) and the safety
of the safety controller (avoiding collision).

</details>


### [133] [Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization](https://arxiv.org/abs/2509.02815)
*Nico Bohlinger,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一种通用的运动策略，适用于50种不同的腿式机器人，通过改进的架构和性能导向的课程学习，实现了对百万种形态变化的控制，并能零样本迁移到真实世界的双足和四足机器人。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过单一策略控制多种形态的机器人，解决机器人运动控制的通用性问题。

Method: 结合改进的URMAv2架构和性能导向的极端形态随机化课程学习。

Result: 策略能够控制百万种形态变化，并在真实世界的双足和四足机器人上实现零样本迁移。

Conclusion: 该方法展示了通用运动策略的潜力，能够适应广泛的机器人形态。

Abstract: We present a single, general locomotion policy trained on a diverse
collection of 50 legged robots. By combining an improved embodiment-aware
architecture (URMAv2) with a performance-based curriculum for extreme
Embodiment Randomization, our policy learns to control millions of
morphological variations. Our policy achieves zero-shot transfer to unseen
real-world humanoid and quadruped robots.

</details>


### [134] [Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms](https://arxiv.org/abs/2509.02870)
*Harsh Muriki,Hong Ray Teo,Ved Sengupta,Ai-Ping Hu*

Main category: cs.RO

TL;DR: 利用FarmBot和自定义相机末端执行器，通过3D点云模型估计草莓花姿态，用于机器人授粉。提出了一种新算法，将占用网格沿点云正交轴平移生成6个视角的2D图像，结合2D目标检测和3D形状拟合，实现姿态估计。


<details>
  <summary>Details</summary>
Motivation: 城市农场规模小且低成本机器人（如FarmBot）普及，为植物表型分析提供了平台。研究旨在利用机器人技术实现草莓花的精准授粉。

Method: 通过FarmBot获取3D点云，提出算法生成多视角2D图像，结合2D目标检测和3D形状拟合（超椭球、抛物面、平面）估计花姿态。

Result: 成功检测约80%的花朵，平均姿态误差7.7度，满足机器人授粉需求，优于先前结果。

Conclusion: 该方法在机器人授粉中表现优异，代码已开源，为小规模农业自动化提供了实用解决方案。

Abstract: The small scale of urban farms and the commercial availability of low-cost
robots (such as the FarmBot) that automate simple tending tasks enable an
accessible platform for plant phenotyping. We have used a FarmBot with a custom
camera end-effector to estimate strawberry plant flower pose (for robotic
pollination) from acquired 3D point cloud models. We describe a novel algorithm
that translates individual occupancy grids along orthogonal axes of a point
cloud to obtain 2D images corresponding to the six viewpoints. For each image,
2D object detection models for flowers are used to identify 2D bounding boxes
which can be converted into the 3D space to extract flower point clouds. Pose
estimation is performed by fitting three shapes (superellipsoids, paraboloids
and planes) to the flower point clouds and compared with manually labeled
ground truth. Our method successfully finds approximately 80% of flowers
scanned using our customized FarmBot platform and has a mean flower pose error
of 7.7 degrees, which is sufficient for robotic pollination and rivals previous
results. All code will be made available at
https://github.com/harshmuriki/flowerPose.git.

</details>


### [135] [Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model](https://arxiv.org/abs/2509.02876)
*Hongrui Yu,Vineet R. Kamat,Carol C. Menassa*

Main category: cs.RO

TL;DR: 提出了一种通用学习架构，通过众包在线自然语言指令直接教授机器人多任务技能，减少重复编程工作。


<details>
  <summary>Details</summary>
Motivation: 解决建筑机器人因工作重复性导致的技能难以泛化问题，减少人工重新编程的负担。

Method: 结合大型语言模型（LLM）、标准化模块化分层建模方法和BIM-机器人语义数据管道。

Result: 在干墙安装实验中验证了多任务重编程的高效性和高质量。

Conclusion: 该框架显著减少了多任务重编程的工作量，提升了机器人技能的通用性。

Abstract: The quasi-repetitive nature of construction work and the resulting lack of
generalizability in programming construction robots presents persistent
challenges to the broad adoption of robots in the construction industry. Robots
cannot achieve generalist capabilities as skills learnt from one domain cannot
readily transfer to another work domain or be directly used to perform a
different set of tasks. Human workers have to arduously reprogram their
scene-understanding, path-planning, and manipulation components to enable the
robots to perform alternate work tasks. The methods presented in this paper
resolve a significant proportion of such reprogramming workload by proposing a
generalizable learning architecture that directly teaches robots versatile
task-performance skills through crowdsourced online natural language
instructions. A Large Language Model (LLM), a standardized and modularized
hierarchical modeling approach, and Building Information Modeling-Robot sematic
data pipeline are developed to address the multi-task skill transfer problem.
The proposed skill standardization scheme and LLM-based hierarchical skill
learning framework were tested with a long-horizon drywall installation
experiment using a full-scale industrial robotic manipulator. The resulting
robot task learning scheme achieves multi-task reprogramming with minimal
effort and high quality.

</details>


### [136] [IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for Dynamic Environments](https://arxiv.org/abs/2509.02972)
*Haolan Zhang,Thanh Nguyen Canh,Chenghao Li,Ruidong Yang,Yonghoon Ji,Nak Young Chong*

Main category: cs.RO

TL;DR: 提出了一种特征感知机制，仅在必要时引入线特征以减少计算开销和低质量特征的影响，显著提升了动态SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM方法在动态环境中表现不佳，现有动态SLAM系统因持续引入额外特征导致计算开销和性能下降。

Method: 通过特征感知机制评估当前特征是否足够，仅在需要时激活线特征支持，避免不必要的计算和低质量特征。

Result: 在TUM数据集上，相比ORB-SLAM3和其他动态SLAM方法，ATE和RPE指标均有显著提升。

Conclusion: 该方法有效平衡了特征引入的必要性和计算效率，提升了动态SLAM的鲁棒性和性能。

Abstract: Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in
autonomous systems. Traditional SLAM methods, based on static environment
assumptions, struggle to handle complex dynamic environments. Recent dynamic
SLAM systems employ geometric constraints and deep learning to remove dynamic
features, yet this creates a new challenge: insufficient remaining point
features for subsequent SLAM processes. Existing solutions address this by
continuously introducing additional line and plane features to supplement point
features, achieving robust tracking and pose estimation. However, current
methods continuously introduce additional features regardless of necessity,
causing two problems: unnecessary computational overhead and potential
performance degradation from accumulated low-quality additional features and
noise. To address these issues, this paper proposes a feature-aware mechanism
that evaluates whether current features are adequate to determine if line
feature support should be activated. This decision mechanism enables the system
to introduce line features only when necessary, significantly reducing
computational complexity of additional features while minimizing the
introduction of low-quality features and noise. In subsequent processing, the
introduced line features assist in obtaining better initial camera poses
through tracking, local mapping, and loop closure, but are excluded from global
optimization to avoid potential negative impacts from low-quality additional
features in long-term process. Extensive experiments on TUM datasets
demonstrate substantial improvements in both ATE and RPE metrics compared to
ORB-SLAM3 baseline and superior performance over other dynamic SLAM and
multi-feature methods.

</details>


### [137] [DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features](https://arxiv.org/abs/2509.02983)
*Jinghe Yang,Minh-Quan Le,Mingming Gong,Ye Pu*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的水下视觉导航策略DUViN，通过知识迁移的深度特征实现未知环境中的4自由度运动控制。


<details>
  <summary>Details</summary>
Motivation: 水下导航因感知能力有限和环境地图构建困难而具有挑战性。

Method: 采用两阶段训练框架：先在空气中训练扩散模型导航策略，再在水下环境中微调深度特征提取器并整合到导航策略中。

Result: 在模拟和真实水下环境中验证了方法的有效性和泛化能力。

Conclusion: DUViN无需预建地图即可实现安全导航，解决了水下数据稀缺问题。

Abstract: Autonomous underwater navigation remains a challenging problem due to limited
sensing capabilities and the difficulty of constructing accurate maps in
underwater environments. In this paper, we propose a Diffusion-based Underwater
Visual Navigation policy via knowledge-transferred depth features, named DUViN,
which enables vision-based end-to-end 4-DoF motion control for underwater
vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles
and maintain a safe and perception awareness altitude relative to the terrain
without relying on pre-built maps. To address the difficulty of collecting
large-scale underwater navigation datasets, we propose a method that ensures
robust generalization under domain shifts from in-air to underwater
environments by leveraging depth features and introducing a novel model
transfer strategy. Specifically, our training framework consists of two phases:
we first train the diffusion-based visual navigation policy on in-air datasets
using a pre-trained depth feature extractor. Secondly, we retrain the extractor
on an underwater depth estimation task and integrate the adapted extractor into
the trained navigation policy from the first step. Experiments in both
simulated and real-world underwater environments demonstrate the effectiveness
and generalization of our approach. The experimental videos are available at
https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.

</details>


### [138] [CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with Instruction Learning and Reinforcement Learning](https://arxiv.org/abs/2509.02986)
*Rankun Li,Hao Wang,Qi Li,Zhuo Han,Yifei Chu,Linqi Ye,Wende Xie,Wenlong Liao*

Main category: cs.RO

TL;DR: 提出了一种基于接触触发的盲爬升框架（CTBC），显著提升了轮式双足机器人在复杂地形中的通过能力。


<details>
  <summary>Details</summary>
Motivation: 轮式双足机器人在平坦地形上具有高速移动优势，但在复杂环境（如楼梯）中表现不佳，需要改进。

Method: 通过检测轮子与障碍物的接触触发抬腿动作，利用强引导的前馈轨迹快速掌握敏捷抬腿技能。

Result: 实验验证表明，机器人Tron1能够仅依靠本体感知反馈可靠攀爬远超其轮半径的障碍物。

Conclusion: CTBC框架有效提升了轮式双足机器人在非结构化地形中的通过能力。

Abstract: In recent years, wheeled bipedal robots have gained increasing attention due
to their advantages in mobility, such as high-speed locomotion on flat terrain.
However, their performance on complex environments (e.g., staircases) remains
inferior to that of traditional legged robots. To overcome this limitation, we
propose a general contact-triggered blind climbing (CTBC) framework for wheeled
bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a
leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided
feedforward trajectory, our method enables the robot to rapidly acquire agile
leg-lifting skills, significantly enhancing its capability to traverse
unstructured terrains. The approach has been experimentally validated and
successfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.
Real-world tests demonstrate that Tron1 can reliably climb obstacles well
beyond its wheel radius using only proprioceptive feedback.

</details>


### [139] [Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression](https://arxiv.org/abs/2509.03012)
*Uddeshya Upadhyay*

Main category: cs.RO

TL;DR: 提出了一种名为UT³的新框架，通过不确定性感知的自监督任务优化测试时训练，减少推理时间，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在自主系统中应用广泛，但面对持续演化的环境时泛化能力不足，且现有测试时训练方法因多次前向和反向传播导致推理时间大幅增加。

Method: 采用不确定性感知的自监督任务，选择性应用训练以减少推理时间，同时保持与标准测试时训练相当的性能。

Result: 在单目深度估计任务中验证了方法的有效性，显著减少了推理时间。

Conclusion: UT³框架在保持性能的同时显著降低了推理时间，适用于资源受限的实时应用。

Abstract: Deep neural networks (DNNs) are increasingly being used in autonomous
systems. However, DNNs do not generalize well to domain shift. Adapting to a
continuously evolving environment is a safety-critical challenge inevitably
faced by all autonomous systems deployed to the real world. Recent work on
test-time training proposes methods that adapt to a new test distribution on
the fly by optimizing the DNN model for each test input using self-supervision.
However, these techniques result in a sharp increase in inference time as
multiple forward and backward passes are required for a single test sample (for
test-time training) before finally making the prediction based on the
fine-tuned features. This is undesirable for real-world robotics applications
where these models may be deployed to resource constraint hardware with strong
latency requirements. In this work, we propose a new framework (called UT$^3$)
that leverages test-time training for improved performance in the presence of
continuous domain shift while also decreasing the inference time, making it
suitable for real-world applications. Our method proposes an uncertainty-aware
self-supervision task for efficient test-time training that leverages the
quantified uncertainty to selectively apply the training leading to sharp
improvements in the inference time while performing comparably to standard
test-time training protocol. Our proposed protocol offers a continuous setting
to identify the selected keyframes, allowing the end-user to control how often
to apply test-time training. We demonstrate the efficacy of our method on a
dense regression task - monocular depth estimation.

</details>


### [140] [Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built from a Five Bar Linkage](https://arxiv.org/abs/2509.03119)
*Yash Vyas,Matteo Bottin*

Main category: cs.RO

TL;DR: 基于闭环平面五杆链的力平衡机械臂设计，包括2-DOF的Forbal-2和5-DOF的Forbal-5，实验验证了力平衡配置显著减少力矩和位置误差。


<details>
  <summary>Details</summary>
Motivation: 设计力平衡机械臂以减少关节力矩和反作用力/力矩，提高毫米级精度。

Method: 通过几何、运动学和动力学设计实现力平衡条件，推导逆运动学，并进行实验验证。

Result: 力平衡配置使平均反作用力矩减少66%，关节力矩减少79-84%，Forbal-2位置误差显著降低。

Conclusion: 力平衡机械臂设计适用于需要高精度的应用，能有效减少力矩和误差。

Abstract: A force balanced manipulator design based on the closed chain planar five bar
linkage is developed and experimentally validated. We present 2 variants as a
modular design: Forbal-2, a planar 2-DOF manipulator, and its extension to
5-DOF spatial motion called Forbal-5. The design considerations in terms of
geometric, kinematic, and dynamic design that fulfill the force balance
conditions while maximizing workspace are discussed. Then, the inverse
kinematics of both variants are derived from geometric principles.
  We validate the improvements from force balancing the manipulator through
comparative experiments with counter mass balanced and unbalanced
configurations. The results show how the balanced configuration yields a
reduction in the average reaction moments of up to 66\%, a reduction of average
joint torques of up to 79\%, as well as a noticeable reduction in position
error for Forbal-2. For Forbal-5, which has a higher end effector payload mass,
the joint torques are reduced up to 84\% for the balanced configuration.
Experimental results validate that the balanced manipulator design is suitable
for applications where the reduction of joint torques and reaction
forces/moments helps achieve millimeter level precision.

</details>


### [141] [Efficient Active Training for Deep LiDAR Odometry](https://arxiv.org/abs/2509.03211)
*Beibei Zhou,Zhiyuan Zhang,Zhenbo Song,Jianhui Guo,Hui Kong*

Main category: cs.RO

TL;DR: 提出了一种主动训练框架，通过选择性提取多样环境中的训练数据，减少训练负担并提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度LiDAR里程计模型需要大量多样化训练数据的问题，提高训练效率和模型适应性。

Method: 采用初始训练集选择（ITSS）和主动增量选择（AIS）策略，分别处理一般和复杂天气条件下的数据选择。

Result: 实验表明，仅需52%的序列数据即可达到全数据集训练的性能，验证了方法的效率和鲁棒性。

Conclusion: 该方法为LiDAR里程计系统提供了更高效和可靠的训练范式，适用于多样化环境。

Abstract: Robust and efficient deep LiDAR odometry models are crucial for accurate
localization and 3D reconstruction, but typically require extensive and diverse
training data to adapt to diverse environments, leading to inefficiencies. To
tackle this, we introduce an active training framework designed to selectively
extract training data from diverse environments, thereby reducing the training
load and enhancing model generalization. Our framework is based on two key
strategies: Initial Training Set Selection (ITSS) and Active Incremental
Selection (AIS). ITSS begins by breaking down motion sequences from general
weather into nodes and edges for detailed trajectory analysis, prioritizing
diverse sequences to form a rich initial training dataset for training the base
model. For complex sequences that are difficult to analyze, especially under
challenging snowy weather conditions, AIS uses scene reconstruction and
prediction inconsistency to iteratively select training samples, refining the
model to handle a wide range of real-world scenarios. Experiments across
datasets and weather conditions validate our approach's effectiveness. Notably,
our method matches the performance of full-dataset training with just 52\% of
the sequence volume, demonstrating the training efficiency and robustness of
our active training paradigm. By optimizing the training process, our approach
sets the stage for more agile and reliable LiDAR odometry systems, capable of
navigating diverse environmental conditions with greater precision.

</details>


### [142] [The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation](https://arxiv.org/abs/2509.03222)
*Sophia Bianchi Moyen,Rickmer Krohn,Sophie Lueth,Kay Pompetzki,Jan Peters,Vignesh Prasad,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 比较了两种机器人控制范式（耦合与解耦）和两种视觉反馈机制（VR与传统屏幕），发现VR增加任务时间和认知负荷，耦合控制对用户负荷影响与解耦相当，但耦合数据在模仿学习中表现更好。


<details>
  <summary>Details</summary>
Motivation: 开发直观的远程操作界面，以提高数据收集质量和降低操作员工作负荷，特别是在需要全身协调的复杂任务中。

Method: 比较了耦合与解耦的控制范式，以及VR与传统屏幕的视觉反馈机制，通过多阶段任务序列评估。

Result: VR增加任务时间和认知负荷；耦合控制与解耦控制对用户负荷影响相似，但耦合数据在模仿学习中表现更优。

Conclusion: 直观的远程操作界面有助于大规模收集高质量、高维度的移动操作数据，耦合控制可能更适合模仿学习。

Abstract: Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/

</details>


### [143] [Exploring persuasive Interactions with generative social robots: An experimental framework](https://arxiv.org/abs/2509.03231)
*Stephan Vonschallen,Larissa Julia Corina Finsler,Theresa Schmiedel,Friederike Eyssel*

Main category: cs.RO

TL;DR: 研究探讨了生成式AI社交机器人的说服能力，发现其效果受沟通细节和情境影响。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI社交机器人在自然交流中的说服能力。

Method: 设计实验框架，测试机器人外观和自我认知对决策的影响，并进行定性分析。

Result: 参与者认为机器人友好且有能力，但说服效果高度依赖情境和机器人行为。

Conclusion: 生成式社交机器人能影响用户决策，但需改进沟通细节和情境适应性。

Abstract: Integrating generative AI such as large language models into social robots
has improved their ability to engage in natural, human-like communication. This
study presents a method to examine their persuasive capabilities. We designed
an experimental framework focused on decision making and tested it in a pilot
that varied robot appearance and self-knowledge. Using qualitative analysis, we
evaluated interaction quality, persuasion effectiveness, and the robot's
communicative strategies. Participants generally experienced the interaction
positively, describing the robot as competent, friendly, and supportive, while
noting practical limits such as delayed responses and occasional
speech-recognition errors. Persuasiveness was highly context dependent and
shaped by robot behavior: participants responded well to polite, reasoned
suggestions and expressive gestures, but emphasized the need for more
personalized, context-aware arguments and clearer social roles. These findings
suggest that generative social robots can influence user decisions, but their
effectiveness depends on communicative nuance and contextual relevance. We
propose refinements to the framework to further study persuasive dynamics
between robots and human users.

</details>


### [144] [Vibration Damping in Underactuated Cable-suspended Artwork -- Flying Belt Motion Control](https://arxiv.org/abs/2509.03238)
*Martin Goubej,Lauria Clarke,Martin Hrabačka,David Tolar*

Main category: cs.RO

TL;DR: 论文对Rafael Lozano-Hemmer的互动艺术装置进行了升级，通过改进硬件和运动控制算法，显著提升了系统性能和互动体验。


<details>
  <summary>Details</summary>
Motivation: 原装置因振动问题限制了旋转速度和互动响应性，需改进以提升性能。

Method: 开发了数学模型和输入整形方法，优化控制算法以抑制振动。

Result: 实验结果显示系统性能和互动性显著提升。

Conclusion: 该研究展示了机器人技术、控制工程与互动艺术的结合，为大型动态装置提供了新解决方案。

Abstract: This paper presents a comprehensive refurbishment of the interactive robotic
art installation Standards and Double Standards by Rafael Lozano-Hemmer. The
installation features an array of belts suspended from the ceiling, each
actuated by stepper motors and dynamically oriented by a vision-based tracking
system that follows the movements of exhibition visitors. The original system
was limited by oscillatory dynamics, resulting in torsional and pendulum-like
vibrations that constrained rotational speed and reduced interactive
responsiveness. To address these challenges, the refurbishment involved
significant upgrades to both hardware and motion control algorithms. A detailed
mathematical model of the flying belt system was developed to accurately
capture its dynamic behavior, providing a foundation for advanced control
design. An input shaping method, formulated as a convex optimization problem,
was implemented to effectively suppress vibrations, enabling smoother and
faster belt movements. Experimental results demonstrate substantial
improvements in system performance and audience interaction. This work
exemplifies the integration of robotics, control engineering, and interactive
art, offering new solutions to technical challenges in real-time motion control
and vibration damping for large-scale kinetic installations.

</details>


### [145] [Parallel-Constraint Model Predictive Control: Exploiting Parallel Computation for Improving Safety](https://arxiv.org/abs/2509.03261)
*Elias Fontanari,Gianni Lunardi,Matteo Saveriano,Andrea Del Prete*

Main category: cs.RO

TL;DR: 论文提出了一种利用并行计算改进非线性系统约束满足的方法，通过同时求解多个MPC问题来提高安全性。


<details>
  <summary>Details</summary>
Motivation: 确保约束满足是安全关键系统（如机器人平台）的核心需求，但非线性系统的约束满足具有挑战性。

Method: 通过并行计算同时求解多个MPC问题，每个问题在MPC时间轴上设置不同的安全集约束，最终根据用户定义的标准选择最佳解。

Result: 在3关节机器人臂的仿真中验证了该方法，即使使用4个计算核心也能显著提升安全性和性能。

Conclusion: 并行计算可以有效改进非线性系统的约束满足，提升MPC的安全性和性能。

Abstract: Ensuring constraint satisfaction is a key requirement for safety-critical
systems, which include most robotic platforms. For example, constraints can be
used for modeling joint position/velocity/torque limits and collision
avoidance. Constrained systems are often controlled using Model Predictive
Control, because of its ability to naturally handle constraints, relying on
numerical optimization. However, ensuring constraint satisfaction is
challenging for nonlinear systems/constraints. A well-known tool to make
controllers safe is the so-called control-invariant set (a.k.a. safe set). In
our previous work, we have shown that safety can be improved by letting the
safe-set constraint recede along the MPC horizon. In this paper, we push that
idea further by exploiting parallel computation to improve safety. We solve
several MPC problems at the same time, where each problem instantiates the
safe-set constraint at a different time step along the horizon. Finally, the
controller can select the best solution according to some user-defined
criteria. We validated this idea through extensive simulations with a 3-joint
robotic arm, showing that significant improvements can be achieved in terms of
safety and performance, even using as little as 4 computational cores.

</details>


### [146] [Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in Infectious Pandemic Management](https://arxiv.org/abs/2509.03436)
*Md Mhamud Hussen Sifat,Md Maruf,Md Rokunuzzaman*

Main category: cs.RO

TL;DR: 本文探讨了机器人护士在医疗保健中的应用，特别是在疫情期间如何通过自动化减少感染风险并提升效率。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情加剧了对自动化医疗设备的需求，机器人护士可以降低感染风险并提高医疗效率。

Method: 研究通过物联网（IoT）控制的机器人护士系统，测试其在健康评估、药物管理和监控中的表现。

Result: 机器人护士在药物管理、健康状态监测和生命周期管理方面表现出色。

Conclusion: 机器人护士系统在提升医疗可持续性和应对疫情方面具有显著潜力。

Abstract: The utilization of robotic technology has gained traction in healthcare
facilities due to progress in the field that enables time and cost savings,
minimizes waste, and improves patient care. Digital healthcare technologies
that leverage automation, such as robotics and artificial intelligence, have
the potential to enhance the sustainability and profitability of healthcare
systems in the long run. However, the recent COVID-19 pandemic has amplified
the need for cyber-physical robots to automate check-ups and medication
administration. A robot nurse is controlled by the Internet of Things (IoT) and
can serve as an automated medical assistant while also allowing supervisory
control based on custom commands. This system helps reduce infection risk and
improves outcomes in pandemic settings. This research presents a test case with
a nurse robot that can assess a patient's health status and take action
accordingly. We also evaluate the system's performance in medication
administration, health-status monitoring, and life-cycle considerations.

</details>


### [147] [Real-Time Instrument Planning and Perception for Novel Measurements of Dynamic Phenomena](https://arxiv.org/abs/2509.03500)
*Itai Zilberstein,Alberto Candela,Steve Chien*

Main category: cs.RO

TL;DR: 论文提出了一种自动化工作流，结合卫星图像中的动态事件检测与自主轨迹规划，以提高高分辨率传感器的测量效率，应用于火山羽流观测。


<details>
  <summary>Details</summary>
Motivation: 利用边缘计算和先进计算机视觉技术，实现对动态科学现象的罕见、瞬态和精确测量。

Method: 结合传统机器学习算法和卷积神经网络进行分类，并开发轨迹规划算法跟踪羽流形态特征。

Result: 仿真显示高分辨率仪器的效用回报比基线提高了一个数量级，同时保持高效运行时间。

Conclusion: 自动化工作流显著提升了动态科学现象的观测效率和精度。

Abstract: Advancements in onboard computing mean remote sensing agents can employ
state-of-the-art computer vision and machine learning at the edge. These
capabilities can be leveraged to unlock new rare, transient, and pinpoint
measurements of dynamic science phenomena. In this paper, we present an
automated workflow that synthesizes the detection of these dynamic events in
look-ahead satellite imagery with autonomous trajectory planning for a
follow-up high-resolution sensor to obtain pinpoint measurements. We apply this
workflow to the use case of observing volcanic plumes. We analyze
classification approaches including traditional machine learning algorithms and
convolutional neural networks. We present several trajectory planning
algorithms that track the morphological features of a plume and integrate these
algorithms with the classifiers. We show through simulation an order of
magnitude increase in the utility return of the high-resolution instrument
compared to baselines while maintaining efficient runtimes.

</details>


### [148] [Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling? A Validation Study with Naturalistic Trajectories](https://arxiv.org/abs/2509.03515)
*Yanlin Zhang,Sungyong Chung,Nachuan Li,Dana Monzer,Hani S. Mahmassani,Samer H. Hamdar,Alireza Talebpour*

Main category: cs.RO

TL;DR: 研究发现Waymo Open Motion Dataset (WOMD)在捕捉真实自动驾驶行为时存在不足，可能导致低估驾驶行为的复杂性和风险。


<details>
  <summary>Details</summary>
Motivation: 评估WOMD数据集在行为分析中的有效性，因其存在后处理、误差量化缺失和轨迹分段问题。

Method: 使用独立收集的凤凰城Level 4 AV数据，比较三种驾驶场景（信号灯起步、跟车、变道），采用SIMEX和DTW方法分析。

Result: WOMD未能涵盖真实驾驶行为的全部范围，尤其是短车距和急减速行为。

Conclusion: 仅依赖WOMD校准的行为模型可能低估真实驾驶的变异性与风险，需谨慎使用。

Abstract: The Waymo Open Motion Dataset (WOMD) has become a popular resource for
data-driven modeling of autonomous vehicles (AVs) behavior. However, its
validity for behavioral analysis remains uncertain due to proprietary
post-processing, the absence of error quantification, and the segmentation of
trajectories into 20-second clips. This study examines whether WOMD accurately
captures the dynamics and interactions observed in real-world AV operations.
Leveraging an independently collected naturalistic dataset from Level 4 AV
operations in Phoenix, Arizona (PHX), we perform comparative analyses across
three representative urban driving scenarios: discharging at signalized
intersections, car-following, and lane-changing behaviors. For the discharging
analysis, headways are manually extracted from aerial video to ensure
negligible measurement error. For the car-following and lane-changing cases, we
apply the Simulation-Extrapolation (SIMEX) method to account for empirically
estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to
quantify behavioral differences. Results across all scenarios consistently show
that behavior in PHX falls outside the behavioral envelope of WOMD. Notably,
WOMD underrepresents short headways and abrupt decelerations. These findings
suggest that behavioral models calibrated solely on WOMD may systematically
underestimate the variability, risk, and complexity of naturalistic driving.
Caution is therefore warranted when using WOMD for behavior modeling without
proper validation against independently collected data.

</details>


### [149] [Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey](https://arxiv.org/abs/2508.13073)
*Rui Shao,Wei Li,Lingsen Zhang,Renshan Zhang,Zhiyang Liu,Ran Chen,Liqiang Nie*

Main category: cs.RO

TL;DR: 本文综述了基于大型视觉语言模型（VLM）的视觉-语言-动作（VLA）模型在机器人操作中的应用，系统梳理了其架构、特点及未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的方法在非结构化环境中难以扩展和泛化，而基于大型VLM的VLA模型为解决这一问题提供了新范式。

Method: 通过分类和系统分析，定义了两种主要架构范式（整体模型和分层模型），并探讨了其与高级领域的集成。

Result: 总结了VLA模型的架构特点、操作优势及支持其发展的数据集和基准，提出了未来研究方向。

Conclusion: 本文填补了大型VLM与机器人操作交叉领域的研究空白，为未来研究提供了系统参考。

Abstract: Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation

</details>
