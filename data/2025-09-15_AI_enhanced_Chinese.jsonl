{"id": "2509.09744", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09744", "abs": "https://arxiv.org/abs/2509.09744", "authors": ["Mujie Liu", "Chenze Wang", "Liping Chen", "Nguyen Linh Dan Le", "Niharika Tewari", "Ting Dang", "Jiangang Ma", "Feng Xia"], "title": "Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis", "comment": null, "summary": "The limited availability of labeled brain network data makes it challenging\nto achieve accurate and interpretable psychiatric diagnoses. While\nself-supervised learning (SSL) offers a promising solution, existing methods\noften rely on augmentation strategies that can disrupt crucial structural\nsemantics in brain graphs. To address this, we propose SAM-BG, a two-stage\nframework for learning brain graph representations with structural semantic\npreservation. In the pre-training stage, an edge masker is trained on a small\nlabeled subset to capture key structural semantics. In the SSL stage, the\nextracted structural priors guide a structure-aware augmentation process,\nenabling the model to learn more semantically meaningful and robust\nrepresentations. Experiments on two real-world psychiatric datasets demonstrate\nthat SAM-BG outperforms state-of-the-art methods, particularly in small-labeled\ndata settings, and uncovers clinically relevant connectivity patterns that\nenhance interpretability. Our code is available at\nhttps://github.com/mjliu99/SAM-BG.", "AI": {"tldr": "SAM-BG\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u8bed\u4e49\u4fdd\u62a4\u5b66\u4e60\u8111\u56fe\u8868\u793a\uff0c\u5728\u7cbe\u795e\u75c5\u8bca\u65ad\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6807\u8bb0\u8111\u7f51\u7edc\u6570\u636e\u6709\u9650\uff0c\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u53ef\u80fd\u7834\u574f\u8111\u56fe\u7ed3\u6784\u8bed\u4e49\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u9884\u8bad\u7ec3\u9636\u6bb5\u8bad\u7ec3\u8fb9\u7f18\u63a9\u7801\u5668\u6355\u83b7\u7ed3\u6784\u8bed\u4e49\uff1b\u81ea\u76d1\u7763\u5b66\u4e60\u9636\u6bb5\u5229\u7528\u7ed3\u6784\u5148\u9a8c\u6307\u5bfc\u589e\u5f3a\u8fc7\u7a0b\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u7cbe\u795e\u75c5\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6807\u8bb0\u6570\u636e\u5c11\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u53d1\u73b0\u4e34\u5e8a\u76f8\u5173\u8fde\u63a5\u6a21\u5f0f\u3002", "conclusion": "SAM-BG\u80fd\u5b66\u4e60\u66f4\u5177\u8bed\u4e49\u610f\u4e49\u548c\u9c81\u68d2\u6027\u7684\u8868\u793a\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2509.09747", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09747", "abs": "https://arxiv.org/abs/2509.09747", "authors": ["Leen Daher", "Zhaobo Wang", "Malcolm Mielle"], "title": "D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference", "comment": null, "summary": "Cross-modal transfer learning is used to improve multi-modal classification\nmodels (e.g., for human activity recognition in human-robot collaboration).\nHowever, existing methods require paired sensor data at both training and\ninference, limiting deployment in resource-constrained environments where full\nsensor suites are not economically and technically usable. To address this, we\npropose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns\nmodality-specific representations without requiring joint sensor modality\nduring inference. Our approach combines a self-attention module for feature\nextraction with a novel cross-attention alignment loss, which enforces the\nalignment of sensors' feature spaces without requiring the coupling of the\nclassification pipelines of both modalities. We evaluate D-CAT on three\nmulti-modal human activity datasets (IMU, video, and audio) under both\nin-distribution and out-of-distribution scenarios, comparing against uni-modal\nmodels. Results show that in in-distribution scenarios, transferring from\nhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains\nover uni-modal training. In out-of-distribution scenarios, even weaker source\nmodalities (e.g., IMU to video) improve target performance, as long as the\ntarget model isn't overfitted on the training data. By enabling single-sensor\ninference with cross-modal knowledge, D-CAT reduces hardware redundancy for\nperception systems while maintaining accuracy, which is critical for\ncost-sensitive or adaptive deployments (e.g., assistive robots in homes with\nvariable sensor availability). Code is available at\nhttps://github.com/Schindler-EPFL-Lab/D-CAT.", "AI": {"tldr": "D-CAT\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u8f6c\u79fb\uff0c\u5b9e\u73b0\u5355\u4f20\u611f\u5668\u63a8\u7406\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u6210\u5bf9\u4f20\u611f\u5668\u6570\u636e\uff0c\u9650\u5236\u4e86\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u548c\u65b0\u578b\u8de8\u6ce8\u610f\u529b\u5bf9\u9f50\u635f\u5931\uff0c\u65e0\u9700\u63a8\u7406\u65f6\u8054\u5408\u4f20\u611f\u5668\u6a21\u6001\u3002", "result": "\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\uff0cD-CAT\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6700\u9ad8\u8fbe10% F1\u5206\u6570\u589e\u76ca\u3002", "conclusion": "D-CAT\u51cf\u5c11\u786c\u4ef6\u5197\u4f59\uff0c\u9002\u7528\u4e8e\u6210\u672c\u654f\u611f\u6216\u81ea\u9002\u5e94\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2509.09751", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09751", "abs": "https://arxiv.org/abs/2509.09751", "authors": ["Junqiao Wang", "Zhaoyang Guan", "Guanyu Liu", "Tianze Xia", "Xianzhi Li", "Shuo Yin", "Xinyuan Song", "Chuhan Cheng", "Tianyu Shi", "Alex Lee"], "title": "Meta-Learning Reinforcement Learning for Crypto-Return Prediction", "comment": null, "summary": "Predicting cryptocurrency returns is notoriously difficult: price movements\nare driven by a fast-shifting blend of on-chain activity, news flow, and social\nsentiment, while labeled training data are scarce and expensive. In this paper,\nwe present Meta-RL-Crypto, a unified transformer-based architecture that\nunifies meta-learning and reinforcement learning (RL) to create a fully\nself-improving trading agent. Starting from a vanilla instruction-tuned LLM,\nthe agent iteratively alternates between three roles-actor, judge, and\nmeta-judge-in a closed-loop architecture. This learning process requires no\nadditional human supervision. It can leverage multimodal market inputs and\ninternal preference feedback. The agent in the system continuously refines both\nthe trading policy and evaluation criteria. Experiments across diverse market\nregimes demonstrate that Meta-RL-Crypto shows good performance on the technical\nindicators of the real market and outperforming other LLM-based baselines.", "AI": {"tldr": "Meta-RL-Crypto\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u7edf\u4e00\u67b6\u6784\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u81ea\u6211\u6539\u8fdb\u7684\u4ea4\u6613\u4ee3\u7406\uff0c\u65e0\u9700\u4eba\u5de5\u76d1\u7763\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u5e02\u573a\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u9884\u6d4b\u52a0\u5bc6\u8d27\u5e01\u56de\u62a5\u56f0\u96be\uff0c\u56e0\u4e3a\u4ef7\u683c\u53d7\u591a\u79cd\u56e0\u7d20\u9a71\u52a8\u4e14\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u6602\u8d35\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u626e\u6f14actor\u3001judge\u548cmeta-judge\u89d2\u8272\uff0c\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u591a\u6a21\u6001\u5e02\u573a\u8f93\u5165\u548c\u5185\u90e8\u504f\u597d\u53cd\u9988\u3002", "result": "\u5728\u591a\u6837\u5316\u5e02\u573a\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u3002", "conclusion": "Meta-RL-Crypto\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u81ea\u6211\u6539\u8fdb\u7684\u4ea4\u6613\u4ee3\u7406\uff0c\u9002\u7528\u4e8e\u52a0\u5bc6\u8d27\u5e01\u5e02\u573a\u3002"}}
{"id": "2509.09754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09754", "abs": "https://arxiv.org/abs/2509.09754", "authors": ["Yiqun Shen", "Song Yuan", "Zhengze Zhang", "Xiaoliang Wang", "Daxin Jiang", "Nguyen Cam-Tu"], "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation", "comment": null, "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.", "AI": {"tldr": "KV Cache\u538b\u7f29\u65b0\u6846\u67b6LAVa\uff0c\u901a\u8fc7\u6700\u5c0f\u5316Transformer\u6b8b\u5dee\u6d41\u4fe1\u606f\u635f\u5931\uff0c\u5b9e\u73b0\u52a8\u6001\u5c42\u548c\u5934\u9884\u7b97\u5206\u914d\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709KV Cache\u538b\u7f29\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\u4e14\u7f3a\u4e4f\u52a8\u6001\u9884\u7b97\u5206\u914d\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u5206\u6790\u5c42\u6ce8\u610f\u529b\u8f93\u51fa\u635f\u5931\uff0c\u8bbe\u8ba1\u65b0\u6307\u6807\u52a8\u6001\u5206\u914d\u5934\u548c\u5c42\u9884\u7b97\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u52a8\u6001\u5c42\u9884\u7b97\u5bf9\u751f\u6210\u4efb\u52a1\u5173\u952e\uff0c\u52a8\u6001\u5934\u9884\u7b97\u5bf9\u62bd\u53d6\u4efb\u52a1\u91cd\u8981\u3002", "conclusion": "LAVa\u4f5c\u4e3a\u5168\u52a8\u6001\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u4fdd\u6301\u9ad8\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.09720", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09720", "abs": "https://arxiv.org/abs/2509.09720", "authors": ["Akansel Cosgun", "Lachlan Chumbley", "Benjamin J. Meyer"], "title": "Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision", "comment": null, "summary": "This paper introduces the Australian Supermarket Object Set (ASOS), a\ncomprehensive dataset comprising 50 readily available supermarket items with\nhigh-quality 3D textured meshes designed for benchmarking in robotics and\ncomputer vision applications. Unlike existing datasets that rely on synthetic\nmodels or specialized objects with limited accessibility, ASOS provides a\ncost-effective collection of common household items that can be sourced from a\nmajor Australian supermarket chain. The dataset spans 10 distinct categories\nwith diverse shapes, sizes, and weights. 3D meshes are acquired by a\nstructure-from-motion techniques with high-resolution imaging to generate\nwatertight meshes. The dataset's emphasis on accessibility and real-world\napplicability makes it valuable for benchmarking object detection, pose\nestimation, and robotics applications.", "AI": {"tldr": "ASOS\u662f\u4e00\u4e2a\u5305\u542b50\u79cd\u8d85\u5e02\u7269\u54c1\u7684\u9ad8\u8d28\u91cf3D\u7eb9\u7406\u7f51\u683c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u591a\u4f9d\u8d56\u5408\u6210\u6a21\u578b\u6216\u96be\u4ee5\u83b7\u53d6\u7684\u4e13\u7528\u7269\u54c1\uff0cASOS\u63d0\u4f9b\u4e86\u6210\u672c\u4f4e\u5ec9\u4e14\u6613\u4e8e\u83b7\u53d6\u7684\u771f\u5b9e\u7269\u54c1\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u6280\u672f\u548c\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u83b7\u53d63D\u7f51\u683c\uff0c\u751f\u6210\u5bc6\u5c01\u7f51\u683c\u3002", "result": "\u6570\u636e\u96c6\u6db5\u76d610\u4e2a\u7c7b\u522b\uff0c\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u91cd\u91cf\u591a\u6837\uff0c\u9002\u7528\u4e8e\u7269\u4f53\u68c0\u6d4b\u3001\u59ff\u6001\u4f30\u8ba1\u548c\u673a\u5668\u4eba\u5e94\u7528\u3002", "conclusion": "ASOS\u56e0\u5176\u53ef\u8bbf\u95ee\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u6210\u4e3a\u673a\u5668\u4eba\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u91cd\u8981\u57fa\u51c6\u5de5\u5177\u3002"}}
{"id": "2509.09769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09769", "abs": "https://arxiv.org/abs/2509.09769", "authors": ["Rutav Shah", "Shuijing Liu", "Qi Wang", "Zhenyu Jiang", "Sateesh Kumar", "Mingyo Seo", "Roberto Mart\u00edn-Mart\u00edn", "Yuke Zhu"], "title": "MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos", "comment": "11 pages, 9 figures, 5 tables", "summary": "We aim to enable humanoid robots to efficiently solve new manipulation tasks\nfrom a few video examples. In-context learning (ICL) is a promising framework\nfor achieving this goal due to its test-time data efficiency and rapid\nadaptability. However, current ICL methods rely on labor-intensive teleoperated\ndata for training, which restricts scalability. We propose using human play\nvideos -- continuous, unlabeled videos of people interacting freely with their\nenvironment -- as a scalable and diverse training data source. We introduce\nMimicDroid, which enables humanoids to perform ICL using human play videos as\nthe only training data. MimicDroid extracts trajectory pairs with similar\nmanipulation behaviors and trains the policy to predict the actions of one\ntrajectory conditioned on the other. Through this process, the model acquired\nICL capabilities for adapting to novel objects and environments at test time.\nTo bridge the embodiment gap, MimicDroid first retargets human wrist poses\nestimated from RGB videos to the humanoid, leveraging kinematic similarity. It\nalso applies random patch masking during training to reduce overfitting to\nhuman-specific cues and improve robustness to visual differences. To evaluate\nfew-shot learning for humanoids, we introduce an open-source simulation\nbenchmark with increasing levels of generalization difficulty. MimicDroid\noutperformed state-of-the-art methods and achieved nearly twofold higher\nsuccess rates in the real world. Additional materials can be found on:\nut-austin-rpl.github.io/MimicDroid", "AI": {"tldr": "MimicDroid\u5229\u7528\u4eba\u7c7b\u73a9\u800d\u89c6\u9891\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u8f68\u8ff9\u5bf9\u63d0\u53d6\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8fdc\u7a0b\u64cd\u4f5c\u6570\u636e\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u56e0\u6b64\u63d0\u51fa\u4f7f\u7528\u4eba\u7c7b\u73a9\u800d\u89c6\u9891\u4f5c\u4e3a\u66f4\u9ad8\u6548\u548c\u591a\u6837\u5316\u7684\u8bad\u7ec3\u6570\u636e\u6e90\u3002", "method": "MimicDroid\u901a\u8fc7\u63d0\u53d6\u76f8\u4f3c\u884c\u4e3a\u8f68\u8ff9\u5bf9\u5e76\u8bad\u7ec3\u7b56\u7565\u9884\u6d4b\u52a8\u4f5c\uff0c\u540c\u65f6\u5229\u7528\u8fd0\u52a8\u5b66\u76f8\u4f3c\u6027\u5c06\u4eba\u7c7b\u624b\u8155\u59ff\u52bf\u6620\u5c04\u5230\u673a\u5668\u4eba\uff0c\u5e76\u91c7\u7528\u968f\u673a\u63a9\u7801\u51cf\u5c11\u8fc7\u62df\u5408\u3002", "result": "MimicDroid\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u7387\u662f\u73b0\u6709\u65b9\u6cd5\u7684\u4e24\u500d\u3002", "conclusion": "MimicDroid\u5c55\u793a\u4e86\u5229\u7528\u4eba\u7c7b\u73a9\u800d\u89c6\u9891\u5b9e\u73b0\u9ad8\u6548\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u4efb\u52a1\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.09772", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09772", "abs": "https://arxiv.org/abs/2509.09772", "authors": ["Sanjay Basu", "Sadiq Y. Patel", "Parth Sheth", "Bhairavi Muralidharan", "Namrata Elamaran", "Aakriti Kinra", "Rajaie Batniji"], "title": "Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management", "comment": "10 pages, 5 figures, 4 tables", "summary": "Population health management programs for Medicaid populations coordinate\nlongitudinal outreach and services (e.g., benefits navigation, behavioral\nhealth, social needs support, and clinical scheduling) and must be safe, fair,\nand auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement\nLearning (HACO) framework that separates risk calibration from preference\noptimization to generate conservative action recommendations at scale. In our\nsetting, each step involves choosing among common coordination actions (e.g.,\nwhich member to contact, by which modality, and whether to route to a\nspecialized service) while controlling the near-term risk of adverse\nutilization events (e.g., unplanned emergency department visits or\nhospitalizations). Using a de-identified operational dataset from Waymark\ncomprising 2.77 million sequential decisions across 168,126 patients, HACO (i)\ntrains a lightweight risk model for adverse events, (ii) derives a conformal\nthreshold to mask unsafe actions at a target risk level, and (iii) learns a\npreference policy on the resulting safe subset. We evaluate policies with a\nversion-agnostic fitted Q evaluation (FQE) on stratified subsets and audit\nsubgroup performance across age, sex, and race. HACO achieves strong risk\ndiscrimination (AUC ~0.81) with a calibrated threshold ( {\\tau} ~0.038 at\n{\\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses\nreveal systematic differences in estimated value across demographics,\nunderscoring the importance of fairness auditing. Our results show that\nconformal risk gating integrates cleanly with offline RL to deliver\nconservative, auditable decision support for population health management\nteams.", "AI": {"tldr": "HACO\u6846\u67b6\u7ed3\u5408\u98ce\u9669\u6821\u51c6\u4e0e\u504f\u597d\u4f18\u5316\uff0c\u4e3a\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u63d0\u4f9b\u5b89\u5168\u3001\u516c\u5e73\u4e14\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u8865\u52a9\u4eba\u7fa4\u5065\u5eb7\u7ba1\u7406\u4e2d\u7684\u98ce\u9669\u63a7\u5236\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u81ea\u9002\u5e94\u5171\u5f62\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08HACO\uff09\uff0c\u5206\u79bb\u98ce\u9669\u6821\u51c6\u4e0e\u504f\u597d\u4f18\u5316\uff0c\u751f\u6210\u4fdd\u5b88\u884c\u52a8\u5efa\u8bae\u3002", "result": "HACO\u5728\u98ce\u9669\u533a\u5206\uff08AUC ~0.81\uff09\u548c\u5b89\u5168\u8986\u76d6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63ed\u793a\u4eba\u53e3\u7edf\u8ba1\u4e2d\u7684\u516c\u5e73\u5dee\u5f02\u3002", "conclusion": "HACO\u4e3a\u4eba\u53e3\u5065\u5eb7\u7ba1\u7406\u56e2\u961f\u63d0\u4f9b\u4e86\u4fdd\u5b88\u4e14\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2509.09721", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09721", "abs": "https://arxiv.org/abs/2509.09721", "authors": ["Jiayi Miao", "Dingxin Lu", "Zhuqi Wang"], "title": "A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval", "comment": null, "summary": "After natural disasters, accurate evaluations of damage to housing are\nimportant for insurance claims response and planning of resources. In this\nwork, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)\nframework. On top of classical RAG architecture, we further the framework to\ndevise a two-branch multimodal encoder structure that the image branch employs\na visual encoder composed of ResNet and Transformer to extract the\ncharacteristic of building damage after disaster, and the text branch harnesses\na BERT retriever for the text vectorization of posts as well as insurance\npolicies and for the construction of a retrievable restoration index. To impose\ncross-modal semantic alignment, the model integrates a cross-modal interaction\nmodule to bridge the semantic representation between image and text via\nmulti-head attention. Meanwhile, in the generation module, the introduced modal\nattention gating mechanism dynamically controls the role of visual evidence and\ntext prior information during generation. The entire framework takes end-to-end\ntraining, and combines the comparison loss, the retrieval loss and the\ngeneration loss to form multi-task optimization objectives, and achieves image\nunderstanding and policy matching in collaborative learning. The results\ndemonstrate superior performance in retrieval accuracy and classification index\non damage severity, where the Top-1 retrieval accuracy has been improved by\n9.6%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08MM-RAG\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u707e\u540e\u623f\u5c4b\u635f\u574f\u8bc4\u4f30\uff0c\u7ed3\u5408\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\uff0c\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u6027\u548c\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u707e\u540e\u623f\u5c4b\u635f\u574f\u8bc4\u4f30\u5bf9\u4fdd\u9669\u7406\u8d54\u548c\u8d44\u6e90\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\uff09\u8fdb\u884c\u66f4\u51c6\u786e\u7684\u5224\u65ad\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u591a\u6a21\u6001\u7f16\u7801\u5668\u7ed3\u6784\uff0c\u56fe\u50cf\u5206\u652f\u4f7f\u7528ResNet\u548cTransformer\u63d0\u53d6\u635f\u574f\u7279\u5f81\uff0c\u6587\u672c\u5206\u652f\u4f7f\u7528BERT\u68c0\u7d22\u5668\u5904\u7406\u6587\u672c\u4fe1\u606f\u3002\u901a\u8fc7\u8de8\u6a21\u6001\u4ea4\u4e92\u6a21\u5757\u548c\u591a\u5934\u6ce8\u610f\u529b\u5b9e\u73b0\u8bed\u4e49\u5bf9\u9f50\uff0c\u751f\u6210\u6a21\u5757\u5f15\u5165\u6a21\u6001\u6ce8\u610f\u529b\u95e8\u63a7\u673a\u5236\u52a8\u6001\u63a7\u5236\u4fe1\u606f\u6d41\u3002", "result": "\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u635f\u574f\u4e25\u91cd\u6027\u5206\u7c7b\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cTop-1\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u5347\u4e869.6%\u3002", "conclusion": "MM-RAG\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u707e\u540e\u623f\u5c4b\u635f\u574f\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2509.09805", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09805", "abs": "https://arxiv.org/abs/2509.09805", "authors": ["Francisco M. L\u00f3pez", "Miles Lenz", "Marco G. Fedozzi", "Arthur Aubret", "Jochen Triesch"], "title": "MIMo grows! Simulating body and sensory development in a multimodal infant model", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 6 figures", "summary": "Infancy is characterized by rapid body growth and an explosive change of\nsensory and motor abilities. However, developmental robots and simulation\nplatforms are typically designed in the image of a specific age, which limits\ntheir ability to capture the changing abilities and constraints of developing\ninfants. To address this issue, we present MIMo v2, a new version of the\nmultimodal infant model. It includes a growing body with increasing actuation\nstrength covering the age range from birth to 24 months. It also features\nfoveated vision with developing visual acuity as well as sensorimotor delays\nmodeling finite signal transmission speeds to and from an infant's brain.\nFurther enhancements of this MIMo version include an inverse kinematics module,\na random environment generator and updated compatiblity with third-party\nsimulation and learning libraries. Overall, this new MIMo version permits\nincreased realism when modeling various aspects of sensorimotor development.\nThe code is available on the official repository\n(https://github.com/trieschlab/MIMo).", "AI": {"tldr": "MIMo v2\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5a74\u513f\u6a21\u578b\uff0c\u6a21\u62df\u4ece\u51fa\u751f\u523024\u4e2a\u6708\u5a74\u513f\u7684\u8eab\u4f53\u751f\u957f\u548c\u611f\u77e5\u8fd0\u52a8\u80fd\u529b\u53d8\u5316\uff0c\u63d0\u9ad8\u4e86\u5efa\u6a21\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6216\u4eff\u771f\u5e73\u53f0\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u5e74\u9f84\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6355\u6349\u5a74\u513f\u53d1\u80b2\u4e2d\u7684\u80fd\u529b\u53d8\u5316\u548c\u9650\u5236\u3002", "method": "MIMo v2\u5305\u542b\u751f\u957f\u7684\u8eab\u4f53\u3001\u589e\u5f3a\u7684\u9a71\u52a8\u5f3a\u5ea6\u3001\u53d1\u5c55\u4e2d\u7684\u89c6\u89c9\u654f\u9510\u5ea6\u3001\u611f\u89c9\u8fd0\u52a8\u5ef6\u8fdf\u6a21\u62df\uff0c\u5e76\u65b0\u589e\u9006\u8fd0\u52a8\u5b66\u6a21\u5757\u548c\u968f\u673a\u73af\u5883\u751f\u6210\u5668\u3002", "result": "\u65b0\u7248\u672c\u663e\u8457\u63d0\u5347\u4e86\u611f\u89c9\u8fd0\u52a8\u53d1\u80b2\u5efa\u6a21\u7684\u771f\u5b9e\u6027\u3002", "conclusion": "MIMo v2\u4e3a\u7814\u7a76\u5a74\u513f\u53d1\u80b2\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u4eff\u771f\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.09782", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09782", "abs": "https://arxiv.org/abs/2509.09782", "authors": ["Roshini Pulishetty", "Mani Kishan Ghantasala", "Keerthy Kaushik Dasoju", "Niti Mangwani", "Vishal Garimella", "Aditya Mate", "Somya Chatterjee", "Yue Kang", "Ehi Nosakhare", "Sadid Hasan", "Soundar Srinivasan"], "title": "One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection", "comment": null, "summary": "The proliferation of large language models (LLMs) with varying computational\ncosts and performance profiles presents a critical challenge for scalable,\ncost-effective deployment in real-world applications. We introduce a unified\nrouting framework that leverages a single-head cross-attention mechanism to\njointly model query and model embeddings, enabling dynamic selection of the\noptimal LLM for each input query. Our approach is evaluated on RouterBench, a\nlarge-scale, publicly available benchmark encompassing diverse LLM pools and\ndomains. By explicitly capturing fine-grained query-model interactions, our\nrouter predicts both response quality and generation cost, achieving up to 6.6%\nimprovement in Average Improvement in Quality (AIQ) and 2.9% in maximum\nperformance over existing routers. To robustly balance performance and cost, we\npropose an exponential reward function that enhances stability across user\npreferences. The resulting architecture is lightweight, generalizes effectively\nacross domains, and demonstrates improved efficiency compared to prior methods,\nestablishing a new standard for cost-aware LLM routing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u8f7b\u91cf\u7ea7\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u9009\u62e9\u6700\u4f18LLM\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u6210\u672c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540cLLM\u5728\u8ba1\u7b97\u6210\u672c\u548c\u6027\u80fd\u4e0a\u7684\u5dee\u5f02\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u5355\u5934\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u8054\u5408\u5efa\u6a21\u67e5\u8be2\u548c\u6a21\u578b\u5d4c\u5165\uff0c\u52a8\u6001\u9009\u62e9\u6700\u4f18LLM\uff0c\u5e76\u63d0\u51fa\u6307\u6570\u5956\u52b1\u51fd\u6570\u5e73\u8861\u6027\u80fd\u4e0e\u6210\u672c\u3002", "result": "\u5728RouterBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAIQ\u63d0\u53476.6%\uff0c\u6700\u5927\u6027\u80fd\u63d0\u53472.9%\uff0c\u4e14\u67b6\u6784\u8f7b\u91cf\u3001\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6210\u672c\u611f\u77e5\u7684LLM\u8def\u7531\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u591a\u9886\u57df\u3002"}}
{"id": "2509.09722", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09722", "abs": "https://arxiv.org/abs/2509.09722", "authors": ["Taylor Archibald", "Tony Martinez"], "title": "Improving MLLM Historical Record Extraction with Test-Time Image", "comment": null, "summary": "We present a novel ensemble framework that stabilizes LLM based text\nextraction from noisy historical documents. We transcribe multiple augmented\nvariants of each image with Gemini 2.0 Flash and fuse these outputs with a\ncustom Needleman Wunsch style aligner that yields both a consensus\ntranscription and a confidence score. We present a new dataset of 622\nPennsylvania death records, and demonstrate our method improves transcription\naccuracy by 4 percentage points relative to a single shot baseline. We find\nthat padding and blurring are the most useful for improving accuracy, while\ngrid warp perturbations are best for separating high and low confidence cases.\nThe approach is simple, scalable, and immediately deployable to other document\ncollections and transcription models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u56fe\u50cf\u589e\u5f3a\u548c\u81ea\u5b9a\u4e49\u5bf9\u9f50\u5668\u63d0\u9ad8\u5386\u53f2\u6587\u6863\u8f6c\u5f55\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4ece\u566a\u58f0\u5386\u53f2\u6587\u6863\u4e2d\u63d0\u53d6\u6587\u672c\u65f6LLM\u8f6c\u5f55\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Gemini 2.0 Flash\u5bf9\u591a\u4e2a\u589e\u5f3a\u56fe\u50cf\u53d8\u4f53\u8fdb\u884c\u8f6c\u5f55\uff0c\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49Needleman-Wunsch\u5bf9\u9f50\u5668\u878d\u5408\u8f93\u51fa\u3002", "result": "\u5728622\u4efd\u5bbe\u5915\u6cd5\u5c3c\u4e9a\u5dde\u6b7b\u4ea1\u8bb0\u5f55\u6570\u636e\u96c6\u4e0a\uff0c\u8f6c\u5f55\u51c6\u786e\u7387\u63d0\u9ad8\u4e864\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u53ef\u6269\u5c55\uff0c\u9002\u7528\u4e8e\u5176\u4ed6\u6587\u6863\u96c6\u5408\u548c\u8f6c\u5f55\u6a21\u578b\u3002"}}
{"id": "2509.09889", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09889", "abs": "https://arxiv.org/abs/2509.09889", "authors": ["Giulia Botta", "Marco Botta", "Cristina Gena", "Alessandro Mazzei", "Massimo Donini", "Alberto Lillo"], "title": "Using the Pepper Robot to Support Sign Language Communication", "comment": "paper presented at ICSR2025", "summary": "Social robots are increasingly experimented in public and assistive settings,\nbut their accessibility for Deaf users remains quite underexplored. Italian\nSign Language (LIS) is a fully-fledged natural language that relies on complex\nmanual and non-manual components. Enabling robots to communicate using LIS\ncould foster more inclusive human robot interaction, especially in social\nenvironments such as hospitals, airports, or educational settings. This study\ninvestigates whether a commercial social robot, Pepper, can produce\nintelligible LIS signs and short signed LIS sentences. With the help of a Deaf\nstudent and his interpreter, an expert in LIS, we co-designed and implemented\n52 LIS signs on Pepper using either manual animation techniques or a MATLAB\nbased inverse kinematics solver. We conducted a exploratory user study\ninvolving 12 participants proficient in LIS, both Deaf and hearing.\nParticipants completed a questionnaire featuring 15 single-choice video-based\nsign recognition tasks and 2 open-ended questions on short signed sentences.\nResults shows that the majority of isolated signs were recognized correctly,\nalthough full sentence recognition was significantly lower due to Pepper's\nlimited articulation and temporal constraints. Our findings demonstrate that\neven commercially available social robots like Pepper can perform a subset of\nLIS signs intelligibly, offering some opportunities for a more inclusive\ninteraction design. Future developments should address multi-modal enhancements\n(e.g., screen-based support or expressive avatars) and involve Deaf users in\nparticipatory design to refine robot expressivity and usability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5546\u4e1a\u793e\u4ea4\u673a\u5668\u4ebaPepper\u662f\u5426\u80fd\u751f\u6210\u53ef\u7406\u89e3\u7684\u610f\u5927\u5229\u624b\u8bed\uff08LIS\uff09\u7b26\u53f7\u548c\u77ed\u53e5\uff0c\u7ed3\u679c\u663e\u793a\u591a\u6570\u5b64\u7acb\u7b26\u53f7\u53ef\u8bc6\u522b\uff0c\u4f46\u5b8c\u6574\u53e5\u5b50\u8bc6\u522b\u7387\u8f83\u4f4e\u3002", "motivation": "\u63d0\u5347\u804b\u54d1\u7528\u6237\u5728\u793e\u4ea4\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u5305\u5bb9\u6027\uff0c\u5c24\u5176\u662f\u5728\u516c\u5171\u573a\u6240\u548c\u8f85\u52a9\u73af\u5883\u4e2d\u3002", "method": "\u4e0e\u804b\u54d1\u5b66\u751f\u548c\u624b\u8bed\u4e13\u5bb6\u5408\u4f5c\uff0c\u8bbe\u8ba1\u4e8652\u4e2aLIS\u7b26\u53f7\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u8bc6\u522b\u6548\u679c\u3002", "result": "\u591a\u6570\u5b64\u7acb\u7b26\u53f7\u88ab\u6b63\u786e\u8bc6\u522b\uff0c\u4f46\u5b8c\u6574\u53e5\u5b50\u8bc6\u522b\u7387\u56e0\u673a\u5668\u4eba\u52a8\u4f5c\u9650\u5236\u8f83\u4f4e\u3002", "conclusion": "\u5546\u4e1a\u673a\u5668\u4eba\u53ef\u90e8\u5206\u5b9e\u73b0LIS\u7b26\u53f7\u4ea4\u4e92\uff0c\u672a\u6765\u9700\u591a\u6a21\u6001\u589e\u5f3a\u548c\u7528\u6237\u53c2\u4e0e\u8bbe\u8ba1\u3002"}}
{"id": "2509.09793", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09793", "abs": "https://arxiv.org/abs/2509.09793", "authors": ["Vincent Herfeld", "Baudouin Denis de Senneville", "Arthur Leclaire", "Nicolas Papadakis"], "title": "From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms", "comment": null, "summary": "In this paper we analyze the Gradient-Step Denoiser and its usage in\nPlug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms\nuses off the shelf denoisers to replace a proximity operator or a gradient\ndescent operator of an image prior. Usually this image prior is implicit and\ncannot be expressed, but the Gradient-Step Denoiser is trained to be exactly\nthe gradient descent operator or the proximity operator of an explicit\nfunctional while preserving state-of-the-art denoising capabilities.", "AI": {"tldr": "\u5206\u6790\u68af\u5ea6\u6b65\u964d\u566a\u5668\u53ca\u5176\u5728\u5373\u63d2\u5373\u7528\u7b97\u6cd5\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u5373\u63d2\u5373\u7528\u7b97\u6cd5\u901a\u5e38\u4f7f\u7528\u73b0\u6210\u7684\u964d\u566a\u5668\u66ff\u4ee3\u56fe\u50cf\u5148\u9a8c\u7684\u90bb\u8fd1\u7b97\u5b50\u6216\u68af\u5ea6\u4e0b\u964d\u7b97\u5b50\uff0c\u4f46\u68af\u5ea6\u6b65\u964d\u566a\u5668\u53ef\u4ee5\u663e\u5f0f\u5730\u8bad\u7ec3\u4e3a\u68af\u5ea6\u4e0b\u964d\u7b97\u5b50\u6216\u90bb\u8fd1\u7b97\u5b50\u3002", "method": "\u8bad\u7ec3\u68af\u5ea6\u6b65\u964d\u566a\u5668\uff0c\u4f7f\u5176\u6210\u4e3a\u663e\u5f0f\u51fd\u6570\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u5b50\u6216\u90bb\u8fd1\u7b97\u5b50\u3002", "result": "\u68af\u5ea6\u6b65\u964d\u566a\u5668\u5728\u4fdd\u6301\u6700\u5148\u8fdb\u964d\u566a\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u51fd\u6570\u7684\u68af\u5ea6\u4e0b\u964d\u6216\u90bb\u8fd1\u64cd\u4f5c\u3002", "conclusion": "\u68af\u5ea6\u6b65\u964d\u566a\u5668\u4e3a\u5373\u63d2\u5373\u7528\u7b97\u6cd5\u63d0\u4f9b\u4e86\u663e\u5f0f\u4e14\u9ad8\u6548\u7684\u56fe\u50cf\u5148\u9a8c\u5904\u7406\u65b9\u5f0f\u3002"}}
{"id": "2509.09730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09730", "abs": "https://arxiv.org/abs/2509.09730", "authors": ["Kaikai Zhao", "Zhaoxiang Liu", "Peng Wang", "Xin Wang", "Zhicheng Ma", "Yajun Xu", "Wenjing Zhang", "Yibing Nan", "Kai Wang", "Shiguo Lian"], "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance", "comment": "accepted by Image and Vision Computing", "summary": "General-domain large multimodal models (LMMs) have achieved significant\nadvances in various image-text tasks. However, their performance in the\nIntelligent Traffic Surveillance (ITS) domain remains limited due to the\nabsence of dedicated multimodal datasets. To address this gap, we introduce\nMITS (Multimodal Intelligent Traffic Surveillance), the first large-scale\nmultimodal benchmark dataset specifically designed for ITS. MITS includes\n170,400 independently collected real-world ITS images sourced from traffic\nsurveillance cameras, annotated with eight main categories and 24 subcategories\nof ITS-specific objects and events under diverse environmental conditions.\nAdditionally, through a systematic data generation pipeline, we generate\nhigh-quality image captions and 5 million instruction-following visual\nquestion-answer pairs, addressing five critical ITS tasks: object and event\nrecognition, object counting, object localization, background analysis, and\nevent reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream\nLMMs on this dataset, enabling the development of ITS-specific applications.\nExperimental results show that MITS significantly improves LMM performance in\nITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905\n(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to\n0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the\ndataset, code, and models as open-source, providing high-value resources to\nadvance both ITS and LMM research.", "AI": {"tldr": "MITS\u662f\u9996\u4e2a\u4e13\u4e3a\u667a\u80fd\u4ea4\u901a\u76d1\u63a7\uff08ITS\uff09\u8bbe\u8ba1\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u6d41LMM\u5728ITS\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u9886\u57dfLMM\u5728ITS\u9886\u57df\u8868\u73b0\u6709\u9650\uff0c\u7f3a\u4e4f\u4e13\u7528\u591a\u6a21\u6001\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efaMITS\u6570\u636e\u96c6\uff0c\u5305\u542b17\u4e07\u5f20\u771f\u5b9eITS\u56fe\u50cf\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\u548c500\u4e07\u6307\u4ee4\u95ee\u7b54\u5bf9\uff0c\u5e76\u5fae\u8c03\u4e3b\u6d41LMM\u3002", "result": "MITS\u4f7fLLaVA-1.5\u3001LLaVA-1.6\u3001Qwen2-VL\u548cQwen2.5-VL\u7684\u6027\u80fd\u5206\u522b\u63d0\u534783.2%\u300135.8%\u300158.6%\u548c27.0%\u3002", "conclusion": "MITS\u4e3aITS\u548cLMM\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u4ef7\u503c\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2509.09893", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09893", "abs": "https://arxiv.org/abs/2509.09893", "authors": ["Hanbit Oh", "Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Yukiyasu Domae"], "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision", "comment": "Under review", "summary": "Imitation learning is a promising paradigm for training robot agents;\nhowever, standard approaches typically require substantial data acquisition --\nvia numerous demonstrations or random exploration -- to ensure reliable\nperformance. Although exploration reduces human effort, it lacks safety\nguarantees and often results in frequent collisions -- particularly in\nclearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual\nenvironmental resets and imposing additional human burden. This study proposes\nSelf-Augmented Robot Trajectory (SART), a framework that enables policy\nlearning from a single human demonstration, while safely expanding the dataset\nthrough autonomous augmentation. SART consists of two stages: (1) human\nteaching only once, where a single demonstration is provided and precision\nboundaries -- represented as spheres around key waypoints -- are annotated,\nfollowed by one environment reset; (2) robot self-augmentation, where the robot\ngenerates diverse, collision-free trajectories within these boundaries and\nreconnects to the original demonstration. This design improves the data\ncollection efficiency by minimizing human effort while ensuring safety.\nExtensive evaluations in simulation and real-world manipulation tasks show that\nSART achieves substantially higher success rates than policies trained solely\non human-collected demonstrations. Video results available at\nhttps://sites.google.com/view/sart-il .", "AI": {"tldr": "SART\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u548c\u81ea\u4e3b\u6570\u636e\u589e\u5f3a\uff0c\u9ad8\u6548\u4e14\u5b89\u5168\u5730\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u3002", "motivation": "\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u4eba\u7c7b\u8d1f\u62c5\u548c\u6570\u636e\u9700\u6c42\uff0c\u540c\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u6807\u6ce8\u8fb9\u754c\uff0c\u673a\u5668\u4eba\u81ea\u4e3b\u751f\u6210\u591a\u6837\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0cSART\u6bd4\u4ec5\u57fa\u4e8e\u4eba\u7c7b\u6f14\u793a\u7684\u7b56\u7565\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "SART\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u9002\u7528\u4e8e\u6709\u9650\u7a7a\u95f4\u4efb\u52a1\u3002"}}
{"id": "2509.09799", "categories": ["cs.LG", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09799", "abs": "https://arxiv.org/abs/2509.09799", "authors": ["Mansi Sharma", "Alexandre Duchevet", "Florian Daiber", "Jean-Paul Imbert", "Maurice Rekrut"], "title": "Distinguishing Startle from Surprise Events Based on Physiological Signals", "comment": null, "summary": "Unexpected events can impair attention and delay decision-making, posing\nserious safety risks in high-risk environments such as aviation. In particular,\nreactions like startle and surprise can impact pilot performance in different\nways, yet are often hard to distinguish in practice. Existing research has\nlargely studied these reactions separately, with limited focus on their\ncombined effects or how to differentiate them using physiological data. In this\nwork, we address this gap by distinguishing between startle and surprise events\nbased on physiological signals using machine learning and multi-modal fusion\nstrategies. Our results demonstrate that these events can be reliably\npredicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.\nTo further validate the robustness of our model, we extended the evaluation to\ninclude a baseline condition, successfully differentiating between Startle,\nSurprise, and Baseline states with a highest mean accuracy of 74.9% with\nXGBoost and Late Fusion.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u533a\u5206\u60ca\u5413\u548c\u60ca\u8bb6\u4e8b\u4ef6\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe85.7%\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\uff08\u5982\u822a\u7a7a\uff09\uff0c\u610f\u5916\u4e8b\u4ef6\u4f1a\u635f\u5bb3\u6ce8\u610f\u529b\u548c\u5ef6\u8fdf\u51b3\u7b56\uff0c\u4f46\u60ca\u5413\u548c\u60ca\u8bb6\u53cd\u5e94\u5728\u5b9e\u8df5\u4e2d\u96be\u4ee5\u533a\u5206\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5355\u72ec\u7814\u7a76\uff0c\u7f3a\u4e4f\u5bf9\u4e8c\u8005\u7ed3\u5408\u6548\u5e94\u7684\u63a2\u8ba8\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u591a\u6a21\u6001\u878d\u5408\u7b56\u7565\uff0c\u57fa\u4e8e\u751f\u7406\u4fe1\u53f7\u533a\u5206\u60ca\u5413\u548c\u60ca\u8bb6\u4e8b\u4ef6\u3002", "result": "SVM\u548cLate Fusion\u65b9\u6cd5\u6700\u9ad8\u51c6\u786e\u7387\u8fbe85.7%\uff1bXGBoost\u548cLate Fusion\u65b9\u6cd5\u5728\u533a\u5206\u60ca\u5413\u3001\u60ca\u8bb6\u548c\u57fa\u7ebf\u72b6\u6001\u65f6\u6700\u9ad8\u51c6\u786e\u7387\u8fbe74.9%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u751f\u7406\u4fe1\u53f7\u53ef\u6709\u6548\u533a\u5206\u60ca\u5413\u548c\u60ca\u8bb6\u4e8b\u4ef6\uff0c\u6a21\u578b\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.09732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09732", "abs": "https://arxiv.org/abs/2509.09732", "authors": ["Sary Elmansoury", "Islam Mesabah", "Gerrit Gro\u00dfmann", "Peter Neigel", "Raj Bhalwankar", "Daniel Kondermann", "Sebastian J. Vollmer"], "title": "Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs", "comment": null, "summary": "Vision language models (VLMs) excel at zero-shot visual classification, but\ntheir performance on fine-grained tasks and large hierarchical label spaces is\nunderstudied. This paper investigates whether structured, tree-based reasoning\ncan enhance VLM performance. We introduce a framework that decomposes\nclassification into interpretable decisions using decision trees and evaluates\nit on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the\nmodel achieves 98.2% accuracy in understanding the tree knowledge, tree-based\nreasoning consistently underperforms standard zero-shot prompting. We also\nexplore enhancing the tree prompts with LLM-generated classes and image\ndescriptions to improve alignment. The added description enhances the\nperformance of the tree-based and zero-shot methods. Our findings highlight\nlimitations of structured reasoning in visual classification and offer insights\nfor designing more interpretable VLM systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6811\u72b6\u7ed3\u6784\u63a8\u7406\u662f\u5426\u80fd\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u6548\u679c\u4e0d\u5982\u6807\u51c6\u96f6\u6837\u672c\u63d0\u793a\uff0c\u4f46\u901a\u8fc7LLM\u751f\u6210\u7684\u63cf\u8ff0\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u7ed3\u6784\u5316\u3001\u6811\u72b6\u63a8\u7406\u662f\u5426\u80fd\u589e\u5f3aVLM\u5728\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5c06\u5206\u7c7b\u4efb\u52a1\u5206\u89e3\u4e3a\u57fa\u4e8e\u51b3\u7b56\u6811\u7684\u53ef\u89e3\u91ca\u51b3\u7b56\uff0c\u5e76\u5728GTSRB\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002", "result": "\u6811\u72b6\u63a8\u7406\u8868\u73b0\u4e0d\u5982\u6807\u51c6\u96f6\u6837\u672c\u63d0\u793a\uff0c\u4f46\u901a\u8fc7LLM\u751f\u6210\u7684\u7c7b\u522b\u548c\u56fe\u50cf\u63cf\u8ff0\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u5728\u89c6\u89c9\u5206\u7c7b\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f46\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u89e3\u91ca\u7684VLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2509.09953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09953", "abs": "https://arxiv.org/abs/2509.09953", "authors": ["Mahfuzul I. Nissan", "Sharmin Aktar"], "title": "Detection of Anomalous Behavior in Robot Systems Based on Machine Learning", "comment": null, "summary": "Ensuring the safe and reliable operation of robotic systems is paramount to\nprevent potential disasters and safeguard human well-being. Despite rigorous\ndesign and engineering practices, these systems can still experience\nmalfunctions, leading to safety risks. In this study, we present a machine\nlearning-based approach for detecting anomalies in system logs to enhance the\nsafety and reliability of robotic systems. We collected logs from two distinct\nscenarios using CoppeliaSim and comparatively evaluated several machine\nlearning models, including Logistic Regression (LR), Support Vector Machine\n(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context\n(Context 1) and a Pioneer robot context (Context 2). Results showed that while\nLR demonstrated superior performance in Context 1, the Autoencoder model proved\nto be the most effective in Context 2. This highlights that the optimal model\nchoice is context-dependent, likely due to the varying complexity of anomalies\nacross different robotic platforms. This research underscores the value of a\ncomparative approach and demonstrates the particular strengths of autoencoders\nfor detecting complex anomalies in robotic systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u901a\u8fc7\u4e0d\u540c\u573a\u666f\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9009\u62e9\u9700\u4f9d\u8d56\u5177\u4f53\u4e0a\u4e0b\u6587\u3002", "motivation": "\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5373\u4f7f\u7ecf\u8fc7\u4e25\u683c\u8bbe\u8ba1\u4ecd\u53ef\u80fd\u51fa\u73b0\u6545\u969c\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u68c0\u6d4b\u7cfb\u7edf\u65e5\u5fd7\u4e2d\u7684\u5f02\u5e38\uff0c\u4ee5\u51cf\u5c11\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u4f7f\u7528CoppeliaSim\u6536\u96c6\u4e24\u79cd\u4e0d\u540c\u573a\u666f\uff08\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u548cPioneer\u673a\u5668\u4eba\uff09\u7684\u7cfb\u7edf\u65e5\u5fd7\uff0c\u5e76\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\uff08LR\uff09\u3001\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u548c\u81ea\u7f16\u7801\u5668\uff08Autoencoder\uff09\u7684\u6027\u80fd\u3002", "result": "\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u573a\u666f\u4e2d\uff0cLR\u8868\u73b0\u6700\u4f73\uff1b\u800c\u5728Pioneer\u673a\u5668\u4eba\u573a\u666f\u4e2d\uff0c\u81ea\u7f16\u7801\u5668\u6548\u679c\u6700\u597d\uff0c\u8868\u660e\u6a21\u578b\u9009\u62e9\u9700\u6839\u636e\u5177\u4f53\u573a\u666f\u7684\u5f02\u5e38\u590d\u6742\u6027\u800c\u5b9a\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6bd4\u8f83\u65b9\u6cd5\u7684\u4ef7\u503c\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u7f16\u7801\u5668\u5728\u68c0\u6d4b\u590d\u6742\u5f02\u5e38\u4e2d\u7684\u4f18\u52bf\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2509.09838", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09838", "abs": "https://arxiv.org/abs/2509.09838", "authors": ["Reza Asad", "Reza Babanezhad", "Sharan Vaswani"], "title": "Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning", "comment": null, "summary": "Value-based approaches such as DQN are the default methods for off-policy\nreinforcement learning with discrete-action environments such as Atari. Common\npolicy-based methods are either on-policy and do not effectively learn from\noff-policy data (e.g. PPO), or have poor empirical performance in the\ndiscrete-action setting (e.g. SAC). Consequently, starting from discrete SAC\n(DSAC), we revisit the design of actor-critic methods in this setting. First,\nwe determine that the coupling between the actor and critic entropy is the\nprimary reason behind the poor performance of DSAC. We demonstrate that by\nmerely decoupling these components, DSAC can have comparable performance as\nDQN. Motivated by this insight, we introduce a flexible off-policy actor-critic\nframework that subsumes DSAC as a special case. Our framework allows using an\nm-step Bellman operator for the critic update, and enables combining standard\npolicy optimization methods with entropy regularization to instantiate the\nresulting actor objective. Theoretically, we prove that the proposed methods\ncan guarantee convergence to the optimal regularized value function in the\ntabular setting. Empirically, we demonstrate that these methods can approach\nthe performance of DQN on standard Atari games, and do so even without entropy\nregularization or explicit exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u79bb\u7b56\u7565\u6f14\u5458-\u8bc4\u8bba\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u6f14\u5458\u548c\u8bc4\u8bba\u5bb6\u7684\u71b5\u8026\u5408\uff0c\u63d0\u5347\u4e86\u79bb\u6563\u52a8\u4f5c\u73af\u5883\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7b56\u7565\u7684\u65b9\u6cd5\u5728\u79bb\u6563\u52a8\u4f5c\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u79bb\u7b56\u7565\u5b66\u4e60\u65f6\u6548\u679c\u8f83\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u89e3\u8026\u6f14\u5458\u548c\u8bc4\u8bba\u5bb6\u7684\u71b5\u8026\u5408\uff0c\u5e76\u5f15\u5165\u7075\u6d3b\u7684\u79bb\u7b56\u7565\u6846\u67b6\uff0c\u7ed3\u5408m\u6b65\u8d1d\u5c14\u66fc\u7b97\u5b50\u548c\u71b5\u6b63\u5219\u5316\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u8868\u683c\u8bbe\u7f6e\u4e2d\u53ef\u6536\u655b\u5230\u6700\u4f18\u6b63\u5219\u5316\u503c\u51fd\u6570\uff0c\u5b9e\u9a8c\u663e\u793a\u5728Atari\u6e38\u620f\u4e2d\u6027\u80fd\u63a5\u8fd1DQN\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u79bb\u6563\u52a8\u4f5c\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u663e\u5f0f\u63a2\u7d22\u6216\u71b5\u6b63\u5219\u5316\u5373\u53ef\u8fbe\u5230\u9ad8\u6027\u80fd\u3002"}}
{"id": "2509.09737", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09737", "abs": "https://arxiv.org/abs/2509.09737", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "title": "World Modeling with Probabilistic Structure Integration", "comment": null, "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.", "AI": {"tldr": "PSI\u662f\u4e00\u79cd\u5b66\u4e60\u53ef\u63a7\u548c\u53ef\u63d0\u793a\u4e16\u754c\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u6b65\u5faa\u73af\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u80fd\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u63d0\u53d6\u4e30\u5bcc\u7ed3\u6784\u7684\u6a21\u578b\uff0c\u4ee5\u652f\u6301\u89c6\u9891\u9884\u6d4b\u548c\u7406\u89e3\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e09\u6b65\u5faa\u73af\uff1a\u6982\u7387\u9884\u6d4b\u3001\u7ed3\u6784\u63d0\u53d6\u548c\u96c6\u6210\uff0c\u9010\u6b65\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u3002", "result": "\u57281.4\u4e07\u4ebf\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u9884\u6d4b\u3001\u5149\u6d41\u3001\u6df1\u5ea6\u548c\u5206\u5272\u7b49\u4efb\u52a1\u3002", "conclusion": "PSI\u901a\u8fc7\u5faa\u73af\u589e\u5f3a\u7ed3\u6784\u63d0\u53d6\u548c\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9884\u6d4b\u548c\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2509.10007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10007", "abs": "https://arxiv.org/abs/2509.10007", "authors": ["Samuli Soutukorva", "Markku Suomalainen", "Martin Kollingbaum", "Tapio Heikkil\u00e4"], "title": "Gaussian path model library for intuitive robot motion programming by demonstration", "comment": null, "summary": "This paper presents a system for generating Gaussian path models from\nteaching data representing the path shape. In addition, methods for using these\npath models to classify human demonstrations of paths are introduced. By\ngenerating a library of multiple Gaussian path models of various shapes, human\ndemonstrations can be used for intuitive robot motion programming. A method for\nmodifying existing Gaussian path models by demonstration through geometric\nanalysis is also presented.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u4ece\u6559\u5b66\u6570\u636e\u751f\u6210\u8def\u5f84\u5f62\u72b6\uff0c\u5e76\u7528\u4e8e\u5206\u7c7b\u4eba\u7c7b\u6f14\u793a\u8def\u5f84\u3002", "motivation": "\u901a\u8fc7\u751f\u6210\u591a\u79cd\u5f62\u72b6\u7684\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u5e93\uff0c\u5b9e\u73b0\u76f4\u89c2\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u7f16\u7a0b\u3002", "method": "\u5229\u7528\u6559\u5b66\u6570\u636e\u751f\u6210\u9ad8\u65af\u8def\u5f84\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u4fee\u6539\u73b0\u6709\u6a21\u578b\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5206\u7c7b\u4eba\u7c7b\u6f14\u793a\u8def\u5f84\uff0c\u5e76\u652f\u6301\u6a21\u578b\u4fee\u6539\u3002", "conclusion": "\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u4e3a\u673a\u5668\u4eba\u8fd0\u52a8\u7f16\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u76f4\u89c2\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.09843", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09843", "abs": "https://arxiv.org/abs/2509.09843", "authors": ["Jiajun Shen", "Yufei Jin", "Yi He", "Xingquan Zhu"], "title": "HGEN: Heterogeneous Graph Ensemble Networks", "comment": "The paper is in proceedings of the 34th IJCAI Conference, 2025", "summary": "This paper presents HGEN that pioneers ensemble learning for heterogeneous\ngraphs. We argue that the heterogeneity in node types, nodal features, and\nlocal neighborhood topology poses significant challenges for ensemble learning,\nparticularly in accommodating diverse graph learners. Our HGEN framework\nensembles multiple learners through a meta-path and transformation-based\noptimization pipeline to uplift classification accuracy. Specifically, HGEN\nuses meta-path combined with random dropping to create Allele Graph Neural\nNetworks (GNNs), whereby the base graph learners are trained and aligned for\nlater ensembling. To ensure effective ensemble learning, HGEN presents two key\ncomponents: 1) a residual-attention mechanism to calibrate allele GNNs of\ndifferent meta-paths, thereby enforcing node embeddings to focus on more\ninformative graphs to improve base learner accuracy, and 2) a\ncorrelation-regularization term to enlarge the disparity among embedding\nmatrices generated from different meta-paths, thereby enriching base learner\ndiversity. We analyze the convergence of HGEN and attest its higher\nregularization magnitude over simple voting. Experiments on five heterogeneous\nnetworks validate that HGEN consistently outperforms its state-of-the-art\ncompetitors by substantial margin.", "AI": {"tldr": "HGEN\u662f\u4e00\u79cd\u9488\u5bf9\u5f02\u6784\u56fe\u7684\u5f00\u521b\u6027\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u8def\u5f84\u548c\u8f6c\u6362\u4f18\u5316\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u6027\u3002", "motivation": "\u5f02\u6784\u56fe\u4e2d\u7684\u8282\u70b9\u7c7b\u578b\u3001\u7279\u5f81\u548c\u5c40\u90e8\u62d3\u6251\u7ed3\u6784\u591a\u6837\u6027\u5bf9\u96c6\u6210\u5b66\u4e60\u63d0\u51fa\u4e86\u6311\u6218\uff0cHGEN\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "HGEN\u7ed3\u5408\u5143\u8def\u5f84\u548c\u968f\u673a\u4e22\u5f03\u521b\u5efaAllele GNNs\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u6ce8\u610f\u529b\u673a\u5236\u548c\u76f8\u5173\u6027\u6b63\u5219\u5316\u4f18\u5316\u96c6\u6210\u5b66\u4e60\u3002", "result": "\u5728\u4e94\u4e2a\u5f02\u6784\u7f51\u7edc\u4e0a\uff0cHGEN\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HGEN\u901a\u8fc7\u63d0\u5347\u57fa\u7840\u5b66\u4e60\u5668\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5f02\u6784\u56fe\u96c6\u6210\u5b66\u4e60\u3002"}}
{"id": "2509.09742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09742", "abs": "https://arxiv.org/abs/2509.09742", "authors": ["Md Fazle Rasul", "Alanood Alqobaisi", "Bruhadeshwar Bezawada", "Indrakshi Ray"], "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning", "comment": null, "summary": "Federated learning (FL) allows multiple entities to train a shared model\ncollaboratively. Its core, privacy-preserving principle is that participants\nonly exchange model updates, such as gradients, and never their raw, sensitive\ndata. This approach is fundamental for applications in domains where privacy\nand confidentiality are important. However, the security of this very mechanism\nis threatened by gradient inversion attacks, which can reverse-engineer private\ntraining data directly from the shared gradients, defeating the purpose of FL.\nWhile the impact of these attacks is known for image, text, and tabular data,\ntheir effect on video data remains an unexamined area of research. This paper\npresents the first analysis of video data leakage in FL using gradient\ninversion attacks. We evaluate two common video classification approaches: one\nemploying pre-trained feature extractors and another that processes raw video\nframes with simple transformations. Our initial results indicate that the use\nof feature extractors offers greater resilience against gradient inversion\nattacks. We also demonstrate that image super-resolution techniques can enhance\nthe frames extracted through gradient inversion attacks, enabling attackers to\nreconstruct higher-quality videos. Our experiments validate this across\nscenarios where the attacker has access to zero, one, or more reference frames\nfrom the target environment. We find that although feature extractors make\nattacks more challenging, leakage is still possible if the classifier lacks\nsufficient complexity. We, therefore, conclude that video data leakage in FL is\na viable threat, and the conditions under which it occurs warrant further\ninvestigation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5206\u6790\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u89c6\u9891\u6570\u636e\u7684\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u6cc4\u6f0f\u95ee\u9898\uff0c\u53d1\u73b0\u7279\u5f81\u63d0\u53d6\u5668\u80fd\u63d0\u4f9b\u66f4\u5f3a\u7684\u9632\u5fa1\u80fd\u529b\uff0c\u4f46\u653b\u51fb\u4ecd\u53ef\u80fd\u53d1\u751f\u3002", "motivation": "\u7814\u7a76\u89c6\u9891\u6570\u636e\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u9690\u79c1\u6cc4\u6f0f\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bc4\u4f30\u4e24\u79cd\u89c6\u9891\u5206\u7c7b\u65b9\u6cd5\uff1a\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u548c\u539f\u59cb\u89c6\u9891\u5e27\u5904\u7406\uff0c\u5e76\u6d4b\u8bd5\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u6548\u679c\u3002", "result": "\u7279\u5f81\u63d0\u53d6\u5668\u80fd\u589e\u5f3a\u9632\u5fa1\uff0c\u4f46\u653b\u51fb\u8005\u4ecd\u53ef\u80fd\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6280\u672f\u91cd\u5efa\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "conclusion": "\u89c6\u9891\u6570\u636e\u6cc4\u6f0f\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u662f\u4e00\u4e2a\u5b9e\u9645\u5a01\u80c1\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u53d1\u751f\u6761\u4ef6\u3002"}}
{"id": "2509.10012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10012", "abs": "https://arxiv.org/abs/2509.10012", "authors": ["Richard Matthias Hartisch", "Alexander Rother", "J\u00f6rg Kr\u00fcger", "Kevin Haninger"], "title": "Towards simulation-based optimization of compliant fingers for high-speed connector assembly", "comment": null, "summary": "Mechanical compliance is a key design parameter for dynamic contact-rich\nmanipulation, affecting task success and safety robustness over contact\ngeometry variation. Design of soft robotic structures, such as compliant\nfingers, requires choosing design parameters which affect geometry and\nstiffness, and therefore manipulation performance and robustness. Today, these\nparameters are chosen through either hardware iteration, which takes\nsignificant development time, or simplified models (e.g. planar), which can't\naddress complex manipulation task objectives. Improvements in dynamic\nsimulation, especially with contact and friction modeling, present a potential\ndesign tool for mechanical compliance. We propose a simulation-based design\ntool for compliant mechanisms which allows design with respect to task-level\nobjectives, such as success rate. This is applied to optimize design parameters\nof a structured compliant finger to reduce failure cases inside a tolerance\nwindow in insertion tasks. The improvement in robustness is then validated on a\nreal robot using tasks from the benchmark NIST task board. The finger stiffness\naffects the tolerance window: optimized parameters can increase tolerable\nranges by a factor of 2.29, with workpiece variation up to 8.6 mm being\ncompensated. However, the trends remain task-specific. In some tasks, the\nhighest stiffness yields the widest tolerable range, whereas in others the\nopposite is observed, motivating need for design tools which can consider\napplication-specific geometry and dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eff\u771f\u7684\u8bbe\u8ba1\u5de5\u5177\uff0c\u7528\u4e8e\u4f18\u5316\u67d4\u6027\u673a\u68b0\u7ed3\u6784\u7684\u53c2\u6570\uff0c\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u67d4\u6027\u673a\u68b0\u7ed3\u6784\u7684\u8bbe\u8ba1\u53c2\u6570\u76f4\u63a5\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u76ee\u524d\u7684\u8bbe\u8ba1\u65b9\u6cd5\u8017\u65f6\u6216\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u4eff\u771f\u548c\u63a5\u89e6\u6469\u64e6\u5efa\u6a21\uff0c\u4f18\u5316\u67d4\u6027\u624b\u6307\u7684\u8bbe\u8ba1\u53c2\u6570\uff0c\u4ee5\u4efb\u52a1\u7ea7\u76ee\u6807\uff08\u5982\u6210\u529f\u7387\uff09\u4e3a\u5bfc\u5411\u3002", "result": "\u4f18\u5316\u540e\u7684\u8bbe\u8ba1\u53c2\u6570\u53ef\u5c06\u5bb9\u5fcd\u8303\u56f4\u63d0\u9ad82.29\u500d\uff0c\u8865\u507f\u5de5\u4ef6\u53d8\u5316\u8fbe8.6\u6beb\u7c73\uff0c\u4f46\u6548\u679c\u56e0\u4efb\u52a1\u800c\u5f02\u3002", "conclusion": "\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u7684\u8bbe\u8ba1\u5de5\u5177\uff0c\u4ee5\u8003\u8651\u51e0\u4f55\u548c\u52a8\u529b\u5b66\u7279\u6027\u3002"}}
{"id": "2509.09864", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09864", "abs": "https://arxiv.org/abs/2509.09864", "authors": ["Jenny Y. Huang", "Mehul Damani", "Yousef El-Kurdi", "Ramon Astudillo", "Wei Sun"], "title": "Latency and Token-Aware Test-Time Compute", "comment": null, "summary": "Inference-time scaling has emerged as a powerful way to improve large\nlanguage model (LLM) performance by generating multiple candidate responses and\nselecting among them. However, existing work on dynamic allocation for\ntest-time compute typically considers only parallel generation methods such as\nbest-of-N, overlooking incremental decoding methods like beam search, and has\nlargely ignored latency, focusing only on token usage. We formulate\ninference-time scaling as a problem of dynamic compute allocation and method\nselection, where the system must decide which strategy to apply and how much\ncompute to allocate on a per-query basis. Our framework explicitly incorporates\nboth token cost and wall-clock latency, the latter being critical for user\nexperience and particularly for agentic workflows where models must issue\nmultiple queries efficiently. Experiments on reasoning benchmarks show that our\napproach consistently outperforms static strategies, achieving favorable\naccuracy-cost trade-offs while remaining practical for deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u6846\u67b6\uff0c\u4f18\u5316LLM\u63a8\u7406\u65f6\u7684\u6027\u80fd\u548c\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e76\u884c\u751f\u6210\uff08\u5982best-of-N\uff09\uff0c\u5ffd\u7565\u4e86\u589e\u91cf\u89e3\u7801\uff08\u5982beam search\uff09\uff0c\u4e14\u5ffd\u89c6\u4e86\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u5c06\u63a8\u7406\u65f6\u6269\u5c55\u95ee\u9898\u5efa\u6a21\u4e3a\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u548c\u65b9\u6cd5\u9009\u62e9\uff0c\u540c\u65f6\u8003\u8651token\u6210\u672c\u548c\u5ef6\u8fdf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u9759\u6001\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u51c6\u786e\u6027\u4e0e\u6210\u672c\u5e73\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u5b9e\u7528\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2509.09750", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09750", "abs": "https://arxiv.org/abs/2509.09750", "authors": ["Hossein Yazdanjouei", "Arash Mansouri", "Mohammad Shokouhifar"], "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images", "comment": null, "summary": "This study proposes a semi-supervised co-training framework for object\ndetection in densely packed retail environments, where limited labeled data and\ncomplex conditions pose major challenges. The framework combines Faster R-CNN\n(utilizing a ResNet backbone) for precise localization with YOLO (employing a\nDarknet backbone) for global context, enabling mutual pseudo-label exchange\nthat improves accuracy in scenes with occlusion and overlapping objects. To\nstrengthen classification, it employs an ensemble of XGBoost, Random Forest,\nand SVM, utilizing diverse feature representations for higher robustness.\nHyperparameters are optimized using a metaheuristic-driven algorithm, enhancing\nprecision and efficiency across models. By minimizing reliance on manual\nlabeling, the approach reduces annotation costs and adapts effectively to\nfrequent product and layout changes common in retail. Experiments on the\nSKU-110k dataset demonstrate strong performance, highlighting the scalability\nand practicality of the proposed framework for real-world retail applications\nsuch as automated inventory tracking, product monitoring, and checkout systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u5bc6\u96c6\u96f6\u552e\u73af\u5883\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u7ed3\u5408Faster R-CNN\u548cYOLO\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u4ea4\u6362\u63d0\u5347\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528\u96c6\u6210\u5b66\u4e60\u589e\u5f3a\u5206\u7c7b\u3002", "motivation": "\u89e3\u51b3\u96f6\u552e\u73af\u5883\u4e2d\u6807\u6ce8\u6570\u636e\u6709\u9650\u548c\u590d\u6742\u6761\u4ef6\uff08\u5982\u906e\u6321\u548c\u91cd\u53e0\u7269\u4f53\uff09\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408Faster R-CNN\uff08ResNet\u9aa8\u5e72\uff09\u548cYOLO\uff08Darknet\u9aa8\u5e72\uff09\u8fdb\u884c\u4f2a\u6807\u7b7e\u4ea4\u6362\uff0c\u4f7f\u7528XGBoost\u3001\u968f\u673a\u68ee\u6797\u548cSVM\u7684\u96c6\u6210\u5b66\u4e60\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u5728SKU-110k\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u51cf\u5c11\u4e86\u6807\u6ce8\u6210\u672c\uff0c\u9002\u5e94\u96f6\u552e\u73af\u5883\u4e2d\u7684\u9891\u7e41\u53d8\u5316\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u5e93\u5b58\u8ddf\u8e2a\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2509.10032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10032", "abs": "https://arxiv.org/abs/2509.10032", "authors": ["Marawan Khalil", "Fabian Arzberger", "Andreas N\u00fcchter"], "title": "Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping", "comment": "6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction\n  with 12th ECMR 2025", "summary": "Spherical robots offer unique advantages for mapping applications in\nhazardous or confined environments, thanks to their protective shells and\nomnidirectional mobility. This work presents two complementary spherical\nmapping systems: a lightweight, non-actuated design and an actuated variant\nfeaturing internal pendulum-driven locomotion. Both systems are equipped with a\nLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)\nalgorithms on resource-constrained hardware. We assess the mapping accuracy of\nthese systems by comparing the resulting 3D point-clouds from the LIO\nalgorithms to a ground truth map. The results indicate that the performance of\nstate-of-the-art LIO algorithms deteriorates due to the high dynamic movement\nintroduced by the spherical locomotion, leading to globally inconsistent maps\nand sometimes unrecoverable drift.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u7403\u5f62\u673a\u5668\u4eba\u6620\u5c04\u7cfb\u7edf\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u8fd0\u52a8\u4e0b\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u7b97\u6cd5\u6027\u80fd\u3002", "motivation": "\u7403\u5f62\u673a\u5668\u4eba\u5728\u5371\u9669\u6216\u72ed\u7a84\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u5176\u52a8\u6001\u8fd0\u52a8\u53ef\u80fd\u5f71\u54cd\u6620\u5c04\u7cbe\u5ea6\u3002", "method": "\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u975e\u9a71\u52a8\u548c\u9a71\u52a8\u4e24\u79cd\u7403\u5f62\u7cfb\u7edf\uff0c\u4f7f\u7528LiDAR-360\u4f20\u611f\u5668\u548cLIO\u7b97\u6cd5\u3002", "result": "\u52a8\u6001\u8fd0\u52a8\u5bfc\u81f4LIO\u7b97\u6cd5\u6027\u80fd\u4e0b\u964d\uff0c\u4ea7\u751f\u5168\u5c40\u4e0d\u4e00\u81f4\u7684\u5730\u56fe\u548c\u4e0d\u53ef\u6062\u590d\u7684\u6f02\u79fb\u3002", "conclusion": "\u7403\u5f62\u673a\u5668\u4eba\u7684\u9ad8\u52a8\u6001\u8fd0\u52a8\u5bf9\u73b0\u6709LIO\u7b97\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2509.09899", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09899", "abs": "https://arxiv.org/abs/2509.09899", "authors": ["Christopher Eldred", "Fran\u00e7ois Gay-Balmaz", "Vakhtang Putkaradze"], "title": "Variational Neural Networks for Observable Thermodynamics (V-NOTS)", "comment": "26 pages, 6 figures", "summary": "Much attention has recently been devoted to data-based computing of evolution\nof physical systems. In such approaches, information about data points from\npast trajectories in phase space is used to reconstruct the equations of motion\nand to predict future solutions that have not been observed before. However, in\nmany cases, the available data does not correspond to the variables that define\nthe system's phase space. We focus our attention on the important example of\ndissipative dynamical systems. In that case, the phase space consists of\ncoordinates, momenta and entropies; however, the momenta and entropies cannot,\nin general, be observed directly. To address this difficulty, we develop an\nefficient data-based computing framework based exclusively on observable\nvariables, by constructing a novel approach based on the \\emph{thermodynamic\nLagrangian}, and constructing neural networks that respect the thermodynamics\nand guarantees the non-decreasing entropy evolution. We show that our network\ncan provide an efficient description of phase space evolution based on a\nlimited number of data points and a relatively small number of parameters in\nthe system.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89c2\u6d4b\u53d8\u91cf\u7684\u6570\u636e\u9a71\u52a8\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u63cf\u8ff0\u8017\u6563\u52a8\u529b\u7cfb\u7edf\u7684\u76f8\u7a7a\u95f4\u6f14\u5316\u3002", "motivation": "\u89e3\u51b3\u8017\u6563\u52a8\u529b\u7cfb\u7edf\u4e2d\u52a8\u91cf\u548c\u71b5\u65e0\u6cd5\u76f4\u63a5\u89c2\u6d4b\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u70ed\u529b\u5b66\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\uff0c\u786e\u4fdd\u70ed\u529b\u5b66\u7ea6\u675f\u548c\u71b5\u4e0d\u51cf\u6f14\u5316\u3002", "result": "\u7f51\u7edc\u80fd\u591f\u57fa\u4e8e\u6709\u9650\u6570\u636e\u70b9\u548c\u5c11\u91cf\u53c2\u6570\u9ad8\u6548\u63cf\u8ff0\u76f8\u7a7a\u95f4\u6f14\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u65e0\u6cd5\u76f4\u63a5\u89c2\u6d4b\u53d8\u91cf\u7684\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8ba1\u7b97\u6846\u67b6\u3002"}}
{"id": "2509.09785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09785", "abs": "https://arxiv.org/abs/2509.09785", "authors": ["Moslem Yazdanpanah", "Ali Bahri", "Mehrdad Noori", "Sahar Dastani", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging", "comment": null, "summary": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}", "AI": {"tldr": "Token Purging (PG) \u662f\u4e00\u79cd\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u53d7\u57df\u504f\u79fb\u5f71\u54cd\u7684token\u63d0\u53473D\u70b9\u4e91\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u4f9b\u65e0\u9700\u8fed\u4ee3\u66f4\u65b0\u7684\u9c81\u68d2\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPG-SP\uff08\u5229\u7528\u6e90\u7edf\u8ba1\uff09\u548cPG-SF\uff08\u5b8c\u5168\u65e0\u6e90\uff0c\u4f9d\u8d56CLS-token\uff09\u4e24\u79cd\u53d8\u4f53\uff0c\u5728token\u7ea7\u522b\u64cd\u4f5c\u3002", "result": "PG-SP\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u9ad810.3%\uff0cPG-SF\u5728\u65e0\u6e90\u9002\u5e94\u4e2d\u8868\u73b0\u6700\u4f73\uff1bPG\u901f\u5ea6\u63d0\u534712.4\u500d\uff0c\u5185\u5b58\u6548\u7387\u63d0\u9ad85.5\u500d\u3002", "conclusion": "PG\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u5185\u5b58\u53cb\u597d\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.10063", "categories": ["cs.RO", "cs.AI", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.10063", "abs": "https://arxiv.org/abs/2509.10063", "authors": ["Xiyan Huang", "Zhe Xu", "Chenxi Xiao"], "title": "TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model", "comment": "7 pages, 9 figures, 1 table, to be published in IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Robot skill acquisition processes driven by reinforcement learning often rely\non simulations to efficiently generate large-scale interaction data. However,\nthe absence of simulation models for tactile sensors has hindered the use of\ntactile sensing in such skill learning processes, limiting the development of\neffective policies driven by tactile perception. To bridge this gap, we present\nTwinTac, a system that combines the design of a physical tactile sensor with\nits digital twin model. Our hardware sensor is designed for high sensitivity\nand a wide measurement range, enabling high quality sensing data essential for\nobject interaction tasks. Building upon the hardware sensor, we develop the\ndigital twin model using a real-to-sim approach. This involves collecting\nsynchronized cross-domain data, including finite element method results and the\nphysical sensor's outputs, and then training neural networks to map simulated\ndata to real sensor responses. Through experimental evaluation, we\ncharacterized the sensitivity of the physical sensor and demonstrated the\nconsistency of the digital twin in replicating the physical sensor's output.\nFurthermore, by conducting an object classification task, we showed that\nsimulation data generated by our digital twin sensor can effectively augment\nreal-world data, leading to improved accuracy. These results highlight\nTwinTac's potential to bridge the gap in cross-domain learning tasks.", "AI": {"tldr": "TwinTac\u7cfb\u7edf\u7ed3\u5408\u7269\u7406\u89e6\u89c9\u4f20\u611f\u5668\u53ca\u5176\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u586b\u8865\u4e86\u89e6\u89c9\u611f\u77e5\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u7684\u6a21\u62df\u7a7a\u767d\u3002", "motivation": "\u89e6\u89c9\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u7684\u6a21\u62df\u6a21\u578b\u7f3a\u5931\uff0c\u9650\u5236\u4e86\u89e6\u89c9\u611f\u77e5\u9a71\u52a8\u7684\u7b56\u7565\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u9ad8\u7075\u654f\u5ea6\u7269\u7406\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u5230\u6a21\u62df\u7684\u65b9\u6cd5\u5f00\u53d1\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7269\u7406\u4f20\u611f\u5668\u7684\u9ad8\u7075\u654f\u5ea6\u548c\u6570\u5b57\u5b6a\u751f\u7684\u4e00\u81f4\u6027\uff0c\u6a21\u62df\u6570\u636e\u63d0\u5347\u4e86\u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u6027\u3002", "conclusion": "TwinTac\u5728\u8de8\u9886\u57df\u5b66\u4e60\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2509.09926", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09926", "abs": "https://arxiv.org/abs/2509.09926", "authors": ["Jiahao Chen", "Zhiyuan Huang", "Yurou Liu", "Bing Su"], "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios", "comment": null, "summary": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.", "AI": {"tldr": "\u63d0\u51faLoFT\u548cLoFT-OW\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u89e3\u51b3\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u8fc7\u81ea\u4fe1\u548c\u4f4e\u8d28\u91cf\u4f2a\u6807\u7b7e\u95ee\u9898\uff0c\u5e76\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u63d0\u5347\u5224\u522b\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u4ece\u5934\u8bad\u7ec3\u65f6\u5bfc\u81f4\u7684\u8fc7\u81ea\u4fe1\u548c\u4f4e\u8d28\u91cf\u4f2a\u6807\u7b7e\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e0b\u7684\u534a\u76d1\u7763\u5b66\u4e60\u3002", "method": "\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u5fae\u8c03\u8303\u5f0f\uff0c\u63d0\u51faLoFT\u6846\u67b6\u751f\u6210\u66f4\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faLoFT-OW\u5904\u7406\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5206\u5e03\u5916\u6837\u672c\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5373\u4f7f\u4ec5\u4f7f\u75281%\u7684\u65e0\u6807\u7b7e\u6570\u636e\u3002", "conclusion": "LoFT\u548cLoFT-OW\u901a\u8fc7\u9ad8\u6548\u5fae\u8c03\u548c\u5f00\u653e\u4e16\u754c\u9002\u5e94\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5c3e\u534a\u76d1\u7763\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2509.09792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09792", "abs": "https://arxiv.org/abs/2509.09792", "authors": ["Zimin Xia", "Chenghao Xu", "Alexandre Alahi"], "title": "Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors", "comment": null, "summary": "We propose an accurate and highly interpretable fine-grained cross-view\nlocalization method that estimates the 3 Degrees of Freedom pose of a\nground-level image by matching its local features with a reference aerial\nimage. Previous methods typically transform the ground image into a bird's-eye\nview (BEV) representation and then align it with the aerial image for\nlocalization. However, this transformation often leads to information loss due\nto perspective distortion or compression of height information, thereby\ndegrading alignment quality with the aerial view. In contrast, our method\ndirectly establishes correspondences between ground and aerial images and lifts\nonly the matched keypoints to BEV space using monocular depth prior. Notably,\nmodern depth predictors can provide reliable metric depth when the test samples\nare similar to the training data. When the depth distribution differs, they\nstill produce consistent relative depth, i.e., depth accurate up to an unknown\nscale. Our method supports both metric and relative depth. It employs a\nscale-aware Procrustes alignment to estimate the camera pose from the\ncorrespondences and optionally recover the scale when using relative depth.\nExperimental results demonstrate that, with only weak supervision on camera\npose, our method learns accurate local feature correspondences and achieves\nsuperior localization performance under challenging conditions, such as\ncross-area generalization and unknown orientation. Moreover, our method is\ncompatible with various relative depth models without requiring per-model\nfinetuning. This flexibility, combined with strong localization performance,\nmakes it well-suited for real-world deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u4e14\u53ef\u89e3\u91ca\u7684\u7ec6\u7c92\u5ea6\u8de8\u89c6\u89d2\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5339\u914d\u5730\u9762\u56fe\u50cf\u4e0e\u53c2\u8003\u822a\u62cd\u56fe\u50cf\u7684\u5c40\u90e8\u7279\u5f81\u6765\u4f30\u8ba13\u81ea\u7531\u5ea6\u59ff\u6001\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5730\u9762\u56fe\u50cf\u8f6c\u6362\u4e3a\u9e1f\u77b0\u56fe\uff08BEV\uff09\u8868\u793a\uff0c\u518d\u4e0e\u822a\u62cd\u56fe\u50cf\u5bf9\u9f50\uff0c\u4f46\u8fd9\u4e00\u8f6c\u6362\u5e38\u56e0\u900f\u89c6\u5931\u771f\u6216\u9ad8\u5ea6\u4fe1\u606f\u538b\u7f29\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\uff0c\u5f71\u54cd\u5bf9\u9f50\u8d28\u91cf\u3002", "method": "\u76f4\u63a5\u5efa\u7acb\u5730\u9762\u4e0e\u822a\u62cd\u56fe\u50cf\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ec5\u5c06\u5339\u914d\u7684\u5173\u952e\u70b9\u63d0\u5347\u5230BEV\u7a7a\u95f4\uff0c\u5229\u7528\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\uff0c\u652f\u6301\u5ea6\u91cf\u6df1\u5ea6\u548c\u76f8\u5bf9\u6df1\u5ea6\uff0c\u5e76\u901a\u8fc7\u5c3a\u5ea6\u611f\u77e5\u7684Procrustes\u5bf9\u9f50\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u9700\u5f31\u76d1\u7763\u76f8\u673a\u59ff\u6001\uff0c\u8be5\u65b9\u6cd5\u80fd\u5b66\u4e60\u51c6\u786e\u7684\u5c40\u90e8\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u8de8\u533a\u57df\u6cdb\u5316\u548c\u672a\u77e5\u65b9\u5411\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u517c\u5bb9\u591a\u79cd\u76f8\u5bf9\u6df1\u5ea6\u6a21\u578b\u4e14\u65e0\u9700\u5fae\u8c03\uff0c\u7ed3\u5408\u5f3a\u5927\u7684\u5b9a\u4f4d\u6027\u80fd\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.10065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10065", "abs": "https://arxiv.org/abs/2509.10065", "authors": ["Hauzi Cao", "Jiahao Shen", "Zhengzhen Li", "Qinquan Ren", "Shiyu Zhao"], "title": "Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation", "comment": null, "summary": "This paper studies the kinematic tracking control problem for aerial\nmanipulators. Existing kinematic tracking control methods, which typically\nemploy proportional-derivative feedback or tracking-error-based feedback\nstrategies, may fail to achieve tracking objectives within specified time\nconstraints. To address this limitation, we propose a novel control framework\ncomprising two key components: end-effector tracking control based on a\nuser-defined preset trajectory and quadratic programming-based reference\nallocation. Compared with state-of-the-art approaches, the proposed method has\nseveral attractive features. First, it ensures that the end-effector reaches\nthe desired position within a preset time while keeping the tracking error\nwithin a performance envelope that reflects task requirements. Second,\nquadratic programming is employed to allocate the references of the quadcopter\nbase and the Delta arm, while considering the physical constraints of the\naerial manipulator, thus preventing solutions that may violate physical\nlimitations. The proposed approach is validated through three experiments.\nExperimental results demonstrate the effectiveness of the proposed algorithm\nand its capability to guarantee that the target position is reached within the\npreset time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u98de\u884c\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u9884\u8bbe\u8f68\u8ff9\u548c\u4e8c\u6b21\u89c4\u5212\u53c2\u8003\u5206\u914d\uff0c\u786e\u4fdd\u672b\u7aef\u6267\u884c\u5668\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u65b9\u6cd5\uff08\u5982\u6bd4\u4f8b-\u5fae\u5206\u53cd\u9988\u6216\u57fa\u4e8e\u8ddf\u8e2a\u8bef\u5dee\u7684\u53cd\u9988\u7b56\u7565\uff09\u53ef\u80fd\u65e0\u6cd5\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5b9e\u73b0\u8ddf\u8e2a\u76ee\u6807\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9884\u8bbe\u8f68\u8ff9\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u63a7\u5236\u548c\u4e8c\u6b21\u89c4\u5212\u53c2\u8003\u5206\u914d\uff0c\u540c\u65f6\u8003\u8651\u98de\u884c\u673a\u68b0\u81c2\u7684\u7269\u7406\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u786e\u4fdd\u76ee\u6807\u4f4d\u7f6e\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5230\u8fbe\uff0c\u5e76\u4fdd\u6301\u8ddf\u8e2a\u8bef\u5dee\u5728\u6027\u80fd\u8303\u56f4\u5185\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u98de\u884c\u673a\u68b0\u81c2\u7684\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.09933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09933", "abs": "https://arxiv.org/abs/2509.09933", "authors": ["Shintaro Nakamura", "Yuko Kuroki", "Wei Chen"], "title": "Multi-Play Combinatorial Semi-Bandit Problem", "comment": null, "summary": "In the combinatorial semi-bandit (CSB) problem, a player selects an action\nfrom a combinatorial action set and observes feedback from the base arms\nincluded in the action. While CSB is widely applicable to combinatorial\noptimization problems, its restriction to binary decision spaces excludes\nimportant cases involving non-negative integer flows or allocations, such as\nthe optimal transport and knapsack problems.To overcome this limitation, we\npropose the multi-play combinatorial semi-bandit (MP-CSB), where a player can\nselect a non-negative integer action and observe multiple feedbacks from a\nsingle arm in each round. We propose two algorithms for the MP-CSB. One is a\nThompson-sampling-based algorithm that is computationally feasible even when\nthe action space is exponentially large with respect to the number of arms, and\nattains $O(\\log T)$ distribution-dependent regret in the stochastic regime,\nwhere $T$ is the time horizon. The other is a best-of-both-worlds algorithm,\nwhich achieves $O(\\log T)$ variance-dependent regret in the stochastic regime\nand the worst-case $\\tilde{\\mathcal{O}}\\left( \\sqrt{T} \\right)$ regret in the\nadversarial regime. Moreover, its regret in adversarial one is data-dependent,\nadapting to the cumulative loss of the optimal action, the total quadratic\nvariation, and the path-length of the loss sequence. Finally, we numerically\nshow that the proposed algorithms outperform existing methods in the CSB\nliterature.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u64ad\u653e\u7ec4\u5408\u534a\u5f3a\u76d7\uff08MP-CSB\uff09\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7ec4\u5408\u534a\u5f3a\u76d7\u95ee\u9898\uff0c\u652f\u6301\u975e\u8d1f\u6574\u6570\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7ec4\u5408\u534a\u5f3a\u76d7\u95ee\u9898\u9650\u5236\u5728\u4e8c\u5143\u51b3\u7b56\u7a7a\u95f4\uff0c\u65e0\u6cd5\u5904\u7406\u975e\u8d1f\u6574\u6570\u6d41\u6216\u5206\u914d\u95ee\u9898\uff08\u5982\u6700\u4f18\u8fd0\u8f93\u548c\u80cc\u5305\u95ee\u9898\uff09\uff0c\u56e0\u6b64\u9700\u8981\u6269\u5c55\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u57fa\u4e8eThompson\u91c7\u6837\u7684\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u6307\u6570\u589e\u957f\u7684\u60c5\u51b5\uff1b\u4ee5\u53ca\u4e00\u79cd\u201c\u4e24\u5168\u5176\u7f8e\u201d\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u968f\u673a\u548c\u5bf9\u6297\u6027\u73af\u5883\u3002", "result": "Thompson\u91c7\u6837\u7b97\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e2d\u8fbe\u5230O(log T)\u9057\u61be\u754c\uff1b\u201c\u4e24\u5168\u5176\u7f8e\u201d\u7b97\u6cd5\u5728\u968f\u673a\u73af\u5883\u4e2d\u5b9e\u73b0O(log T)\u65b9\u5dee\u4f9d\u8d56\u9057\u61be\uff0c\u5728\u5bf9\u6297\u6027\u73af\u5883\u4e2d\u8fbe\u5230\u221aT\u9057\u61be\u3002", "conclusion": "MP-CSB\u6a21\u578b\u548c\u7b97\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2509.09808", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09808", "abs": "https://arxiv.org/abs/2509.09808", "authors": ["Judith Massmann", "Alexander Lichtenstein", "Francisco M. L\u00f3pez"], "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 7 figures, 2 tables", "summary": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.", "AI": {"tldr": "\u5229\u7528\u667a\u80fd\u624b\u673a\u548cAI\u6280\u672f\u5f00\u53d1\u4e86\u4e00\u6b3e\u514d\u8d39\u5e94\u7528KidsVisionCheck\uff0c\u901a\u8fc7\u7ea2\u773c\u53cd\u5c04\u56fe\u50cf\u8fdb\u884c\u513f\u7ae5\u89c6\u529b\u7b5b\u67e5\uff0c\u51c6\u786e\u7387\u8fbe90%\u3002", "motivation": "\u4f20\u7edfBruckner\u6d4b\u8bd5\u9700\u4e13\u4e1a\u8bbe\u5907\uff0c\u667a\u80fd\u624b\u673a\u548cAI\u7684\u53d1\u5c55\u4f7f\u5176\u53ef\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\uff0c\u63d0\u9ad8\u7b5b\u67e5\u53ef\u53ca\u6027\u3002", "method": "\u57fa\u4e8e\u773c\u79d1\u533b\u751f\u6807\u8bb0\u7684\u513f\u7ae5\u77b3\u5b54\u56fe\u50cf\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u6d4b\u8bd5\u6570\u636e\u4e0a\u51c6\u786e\u7387\u8fbe90%\uff0c\u5e76\u80fd\u4f18\u5316\u6570\u636e\u6536\u96c6\u6761\u4ef6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5168\u7403\u513f\u7ae5\u89c6\u529b\u7b5b\u67e5\u548c\u65e9\u671f\u5e72\u9884\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2509.10096", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10096", "abs": "https://arxiv.org/abs/2509.10096", "authors": ["Saeed Saadatnejad", "Reyhaneh Hosseininejad", "Jose Barreiros", "Katherine M. Tsui", "Alexandre Alahi"], "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario", "comment": "Accepted to RA-L 2025", "summary": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\u548cHHI-Assist\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u9884\u6d4b\u8f85\u52a9\u4efb\u52a1\u4e2d\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u7b56\u7565\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u4eba\u53e3\u8001\u9f84\u5316\u52a0\u5267\u4e86\u5bf9\u8f85\u52a9\u673a\u5668\u4eba\u7684\u9700\u6c42\uff0c\u4f46\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u5728\u7269\u7406\u4ea4\u4e92\u573a\u666f\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faHHI-Assist\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u6761\u4ef6Transformer\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4ea4\u4e92\u4e2d\u7684\u4eba\u4f53\u59ff\u6001\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u6355\u6349\u4ea4\u4e92\u52a8\u6001\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u8f85\u52a9\u7b56\u7565\u7684\u4ea4\u4e92\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.09936", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.09936", "abs": "https://arxiv.org/abs/2509.09936", "authors": ["Saarth Gaonkar", "Xiang Zheng", "Haocheng Xi", "Rishabh Tiwari", "Kurt Keutzer", "Dmitriy Morozov", "Michael W. Mahoney", "Amir Gholami"], "title": "SciML Agents: Write the Solver, Not the Solution", "comment": null, "summary": "Recent work in scientific machine learning aims to tackle scientific tasks\ndirectly by predicting target values with neural networks (e.g.,\nphysics-informed neural networks, neural ODEs, neural operators, etc.), but\nattaining high accuracy and robustness has been challenging. We explore an\nalternative view: use LLMs to write code that leverages decades of numerical\nalgorithms. This shifts the burden from learning a solution function to making\ndomain-aware numerical choices. We ask whether LLMs can act as SciML agents\nthat, given a natural-language ODE description, generate runnable code that is\nscientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),\nand enforcing stability checks. There is currently no benchmark to measure this\nkind of capability for scientific computing tasks. As such, we first introduce\ntwo new datasets: a diagnostic dataset of adversarial \"misleading\" problems;\nand a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set\ncontains problems whose superficial appearance suggests stiffness, and that\nrequire algebraic simplification to demonstrate non-stiffness; and the\nlarge-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-\nand closed-source LLM models along two axes: (i) unguided versus guided\nprompting with domain-specific knowledge; and (ii) off-the-shelf versus\nfine-tuned variants. Our evaluation measures both executability and numerical\nvalidity against reference solutions. We find that with sufficient context and\nguided prompts, newer instruction-following models achieve high accuracy on\nboth criteria. In many cases, recent open-source systems perform strongly\nwithout fine-tuning, while older or smaller models still benefit from\nfine-tuning. Overall, our preliminary results indicate that careful prompting\nand fine-tuning can yield a specialized LLM agent capable of reliably solving\nsimple ODE problems.", "AI": {"tldr": "LLMs\u88ab\u7528\u4e8e\u751f\u6210\u79d1\u5b66\u8ba1\u7b97\u4ee3\u7801\uff0c\u66ff\u4ee3\u4f20\u7edfSciML\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5bfc\u63d0\u793a\u548c\u5fae\u8c03\u63d0\u9ad8ODE\u6c42\u89e3\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfSciML\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u5b58\u5728\u6311\u6218\uff0c\u63a2\u7d22LLMs\u4f5c\u4e3aSciML\u4ee3\u7406\uff0c\u5229\u7528\u5176\u751f\u6210\u4ee3\u7801\u7684\u80fd\u529b\u89e3\u51b3\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff08\u8bca\u65ad\u6027\u548c\u5927\u89c4\u6a21ODE\u4efb\u52a1\uff09\uff0c\u8bc4\u4f30LLMs\u5728\u5f15\u5bfc\u63d0\u793a\u548c\u5fae\u8c03\u4e0b\u7684\u8868\u73b0\uff0c\u6d4b\u91cf\u4ee3\u7801\u53ef\u6267\u884c\u6027\u548c\u6570\u503c\u6709\u6548\u6027\u3002", "result": "\u65b0\u6307\u4ee4\u8ddf\u968f\u6a21\u578b\u5728\u8db3\u591f\u4e0a\u4e0b\u6587\u548c\u5f15\u5bfc\u63d0\u793a\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5f00\u6e90\u7cfb\u7edf\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5f3a\u8868\u73b0\uff0c\u65e7\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u63d0\u5347\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u5fae\u8c03\u53ef\u4f7fLLM\u6210\u4e3a\u53ef\u9760\u89e3\u51b3\u7b80\u5355ODE\u95ee\u9898\u7684\u4ee3\u7406\u3002"}}
{"id": "2509.09828", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09828", "abs": "https://arxiv.org/abs/2509.09828", "authors": ["Tim Broedermannn", "Christos Sakaridis", "Luigi Piccinelli", "Wim Abbeloos", "Luc Van Gool"], "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "comment": "Code and models will be available at\n  https://github.com/timbroed/DGFusion", "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5DGFusion\uff0c\u901a\u8fc7\u6df1\u5ea6\u4fe1\u606f\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u878d\u5408\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u5728\u7a7a\u95f4\u4e0a\u5747\u5300\u5904\u7406\u6570\u636e\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u6761\u4ef6\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5229\u7528\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u771f\u503c\uff0c\u5f15\u5165\u5c40\u90e8\u6df1\u5ea6\u6807\u8bb0\u548c\u5168\u5c40\u6761\u4ef6\u6807\u8bb0\u52a8\u6001\u8c03\u6574\u878d\u5408\u3002", "result": "\u5728MUSES\u548cDELIVER\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u548c\u5168\u666f\u5206\u5272\u6027\u80fd\u3002", "conclusion": "DGFusion\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u548c\u52a8\u6001\u878d\u5408\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2509.10128", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10128", "abs": "https://arxiv.org/abs/2509.10128", "authors": ["Philip Arm", "Oliver Fischer", "Joseph Church", "Adrian Fuhrer", "Hendrik Kolvenbach", "Marco Hutter"], "title": "Efficient Learning-Based Control of a Legged Robot in Lunar Gravity", "comment": null, "summary": "Legged robots are promising candidates for exploring challenging areas on\nlow-gravity bodies such as the Moon, Mars, or asteroids, thanks to their\nadvanced mobility on unstructured terrain. However, as planetary robots' power\nand thermal budgets are highly restricted, these robots need energy-efficient\ncontrol approaches that easily transfer to multiple gravity environments. In\nthis work, we introduce a reinforcement learning-based control approach for\nlegged robots with gravity-scaled power-optimized reward functions. We use our\napproach to develop and validate a locomotion controller and a base pose\ncontroller in gravity environments from lunar gravity (1.62 m/s2) to a\nhypothetical super-Earth (19.62 m/s2). Our approach successfully scales across\nthese gravity levels for locomotion and base pose control with the\ngravity-scaled reward functions. The power-optimized locomotion controller\nreached a power consumption for locomotion of 23.4 W in Earth gravity on a\n15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.\nAdditionally, we designed a constant-force spring offload system that allowed\nus to conduct real-world experiments on legged locomotion in lunar gravity. In\nlunar gravity, the power-optimized control policy reached 12.2 W, 36 % less\nthan a baseline controller which is not optimized for power efficiency. Our\nmethod provides a scalable approach to developing power-efficient locomotion\ncontrollers for legged robots across multiple gravity levels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8282\u80fd\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u5728\u4e0d\u540c\u91cd\u529b\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u5728\u4f4e\u91cd\u529b\u73af\u5883\uff08\u5982\u6708\u7403\u3001\u706b\u661f\uff09\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u53d7\u9650\u4e8e\u80fd\u6e90\u548c\u70ed\u9884\u7b97\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8282\u80fd\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u91cd\u529b\u6bd4\u4f8b\u529f\u7387\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5f00\u53d1\u4e86\u8fd0\u52a8\u63a7\u5236\u5668\u548c\u57fa\u7840\u59ff\u6001\u63a7\u5236\u5668\u3002", "result": "\u5728\u5730\u7403\u91cd\u529b\u4e0b\uff0c\u529f\u7387\u4f18\u5316\u63a7\u5236\u5668\u5c06\u529f\u8017\u964d\u4f4e23%\uff1b\u5728\u6708\u7403\u91cd\u529b\u4e0b\uff0c\u529f\u8017\u964d\u4f4e36%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u5728\u591a\u91cd\u529b\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8282\u80fd\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09940", "abs": "https://arxiv.org/abs/2509.09940", "authors": ["Yifei Wang", "Wenbin Wang", "Yong Luo"], "title": "DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition", "comment": "8 pages, 2 figures", "summary": "Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich\ninformation from multiple sources (e.g., language, video, and audio), the\npotential for intent-irrelevant and conflicting information across modalities\nmay hinder performance from being further improved. Most current models attempt\nto fuse modalities by applying mechanisms like multi-head attention to unimodal\nfeature sequences and then adding the result back to the original\nrepresentation. This process risks corrupting the primary linguistic features\nwith noisy or irrelevant non-verbal signals, as it often fails to capture the\nfine-grained, token-level influence where non-verbal cues should modulate, not\njust augment, textual meaning. To address this, we introduce DyKen-Hyena, which\nreframes the problem from feature fusion to processing modulation. Our model\ntranslates audio-visual cues into dynamic, per-token convolutional kernels that\ndirectly modulate textual feature extraction. This fine-grained approach\nachieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.\nNotably, it yields a +10.46% F1-score improvement in out-of-scope detection,\nvalidating that our method creates a fundamentally more robust intent\nrepresentation.", "AI": {"tldr": "DyKen-Hyena\u901a\u8fc7\u52a8\u6001\u5377\u79ef\u6838\u8c03\u5236\u6587\u672c\u7279\u5f81\u63d0\u53d6\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b\u4e2d\u6a21\u6001\u95f4\u51b2\u7a81\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u610f\u56fe\u8bc6\u522b\u4e2d\uff0c\u6a21\u6001\u95f4\u7684\u65e0\u5173\u6216\u51b2\u7a81\u4fe1\u606f\u53ef\u80fd\u5f71\u54cd\u6027\u80fd\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u878d\u5408\u65f6\u5bb9\u6613\u7834\u574f\u4e3b\u8981\u8bed\u8a00\u7279\u5f81\u3002", "method": "DyKen-Hyena\u5c06\u97f3\u9891-\u89c6\u89c9\u7ebf\u7d22\u8f6c\u5316\u4e3a\u52a8\u6001\u5377\u79ef\u6838\uff0c\u76f4\u63a5\u8c03\u5236\u6587\u672c\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5904\u7406\u3002", "result": "\u5728MIntRec\u548cMIntRec2.0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8303\u56f4\u5916\u68c0\u6d4b\u4e2dF1\u5206\u6570\u63d0\u534710.46%\u3002", "conclusion": "DyKen-Hyena\u901a\u8fc7\u8c03\u5236\u800c\u975e\u878d\u5408\u7279\u5f81\uff0c\u6784\u5efa\u4e86\u66f4\u9c81\u68d2\u7684\u610f\u56fe\u8868\u793a\u3002"}}
{"id": "2509.09841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09841", "abs": "https://arxiv.org/abs/2509.09841", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework", "comment": null, "summary": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eResNet-18\u7684\u5c40\u90e8\u56fe\u50cf\u5757\u81ea\u52a8\u68c0\u6d4b\u9152\u6e23\u9f3b\u7b56\u7565\uff0c\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u9152\u6e23\u9f3b\u662f\u4e00\u79cd\u6162\u6027\u708e\u75c7\u6027\u76ae\u80a4\u75c5\uff0c\u65e9\u671f\u7cbe\u786e\u68c0\u6d4b\u5bf9\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4ece\u9762\u90e8\u56fe\u50cf\u63d0\u53d6\u4e0d\u540c\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u4f4d\u7f6e\u7684\u56fe\u50cf\u5757\uff0c\u8bc4\u4f30\u5c40\u90e8\u89c6\u89c9\u4fe1\u606f\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u56fe\u50cf\u5757\u7684\u7b56\u7565\u5728\u51c6\u786e\u6027\u548c\u654f\u611f\u6027\u4e0a\u4f18\u4e8e\u5168\u56fe\u50cf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "conclusion": "\u8be5\u7b56\u7565\u4e3a\u81ea\u52a8\u5316\u76ae\u80a4\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2509.10139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10139", "abs": "https://arxiv.org/abs/2509.10139", "authors": ["Santiago Montiel-Mar\u00edn", "Angel Llamazares", "Miguel Antunes-Garc\u00eda", "Fabio S\u00e1nchez-Garc\u00eda", "Luis M. Bergasa"], "title": "CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion", "comment": "4 pages, 2 figures", "summary": "Camera-radar fusion offers a robust and cost-effective alternative to\nLiDAR-based autonomous driving systems by combining complementary sensing\ncapabilities: cameras provide rich semantic cues but unreliable depth, while\nradar delivers sparse yet reliable position and motion information. We\nintroduce CaR1, a novel camera-radar fusion architecture for BEV vehicle\nsegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar\nencoding that discretizes point clouds into structured BEV features and an\nadaptive fusion mechanism that dynamically balances sensor contributions.\nExperiments on nuScenes demonstrate competitive segmentation performance (57.6\nIoU), on par with state-of-the-art methods. Code is publicly available\n\\href{https://www.github.com/santimontiel/car1}{online}.", "AI": {"tldr": "CaR1\u662f\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u67b6\u6784\uff0c\u7528\u4e8eBEV\u8f66\u8f86\u5206\u5272\uff0c\u7ed3\u5408\u4e86\u96f7\u8fbe\u70b9\u4e91\u7684\u7ed3\u6784\u5316\u7f16\u7801\u548c\u52a8\u6001\u4f20\u611f\u5668\u878d\u5408\u673a\u5236\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u76f8\u673a\u548c\u96f7\u8fbe\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5404\u6709\u4f18\u52a3\uff0c\u76f8\u673a\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u4f46\u6df1\u5ea6\u4e0d\u53ef\u9760\uff0c\u96f7\u8fbe\u63d0\u4f9b\u7a00\u758f\u4f46\u53ef\u9760\u7684\u4f4d\u7f6e\u548c\u8fd0\u52a8\u4fe1\u606f\u3002\u7ed3\u5408\u4e24\u8005\u53ef\u4ee5\u63d0\u5347\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6210\u672c\u6548\u76ca\u3002", "method": "\u57fa\u4e8eBEVFusion\uff0cCaR1\u91c7\u7528\u7f51\u683c\u5316\u96f7\u8fbe\u7f16\u7801\u5c06\u70b9\u4e91\u79bb\u6563\u5316\u4e3a\u7ed3\u6784\u5316BEV\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u52a8\u6001\u5e73\u8861\u4f20\u611f\u5668\u8d21\u732e\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0cCaR1\u5b9e\u73b0\u4e8657.6 IoU\u7684\u5206\u5272\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "CaR1\u5c55\u793a\u4e86\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u5728BEV\u8f66\u8f86\u5206\u5272\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.09955", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09955", "abs": "https://arxiv.org/abs/2509.09955", "authors": ["Omar Erak", "Omar Alhussein", "Hatem Abou-Zeid", "Mehdi Bennis", "Sami Muhaidat"], "title": "Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge", "comment": "Submitted to IEEE Journals", "summary": "Large-scale transformers are central to modern semantic communication, yet\ntheir high computational and communication costs hinder deployment on\nresource-constrained edge devices. This paper introduces a training-free\nframework for adaptive token merging, a novel mechanism that compresses\ntransformer representations at runtime by selectively merging semantically\nredundant tokens under per-layer similarity thresholds. Unlike prior\nfixed-ratio reduction, our approach couples merging directly to input\nredundancy, enabling data-dependent adaptation that balances efficiency and\ntask relevance without retraining. We cast the discovery of merging strategies\nas a multi-objective optimization problem and leverage Bayesian optimization to\nobtain Pareto-optimal trade-offs between accuracy, inference cost, and\ncommunication cost. On ImageNet classification, we match the accuracy of the\nunmodified transformer with 30\\% fewer floating-point operations per second and\nunder 20\\% of the original communication cost, while for visual question\nanswering our method achieves performance competitive with the full LLaVA model\nat less than one-third of the compute and one-tenth of the bandwidth. Finally,\nwe show that our adaptive merging is robust across varying channel conditions\nand provides inherent privacy benefits, substantially degrading the efficacy of\nmodel inversion attacks. Our framework provides a practical and versatile\nsolution for deploying powerful transformer models in resource-limited edge\nintelligence scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u9002\u5e94\u4ee4\u724c\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5408\u5e76\u5197\u4f59\u4ee4\u724c\u6765\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21Transformer\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u9ad8\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6bcf\u5c42\u76f8\u4f3c\u6027\u9608\u503c\u9009\u62e9\u6027\u5408\u5e76\u8bed\u4e49\u5197\u4f59\u4ee4\u724c\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u5b9e\u73b0\u591a\u76ee\u6807\u4f18\u5316\u3002", "result": "\u5728ImageNet\u5206\u7c7b\u4efb\u52a1\u4e2d\u51cf\u5c1130%\u8ba1\u7b97\u91cf\u548c80%\u901a\u4fe1\u6210\u672c\uff0c\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u6027\u80fd\u63a5\u8fd1\u5b8c\u6574\u6a21\u578b\u4f46\u4ec5\u97001/3\u8ba1\u7b97\u548c1/10\u5e26\u5bbd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u667a\u80fd\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u4fdd\u62a4\u7684Transformer\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2509.09844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09844", "abs": "https://arxiv.org/abs/2509.09844", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection", "comment": null, "summary": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u6570\u636e\u548c\u4e34\u5e8a\u5148\u9a8c\u7684\u9690\u79c1\u4fdd\u62a4\u73ab\u7470\u75e4\u75ae\u81ea\u52a8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u7ea2\u6591\u63a9\u819c\u548cResNet-18\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u5168\u8138\u57fa\u7ebf\u3002", "motivation": "\u73ab\u7470\u75e4\u75ae\u8bca\u65ad\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u56e0\u75c7\u72b6\u5206\u6563\u3001\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\u53d7\u9650\u3002", "method": "\u6784\u5efa\u7ea2\u6591\u63a9\u819c\u805a\u7126\u8bca\u65ad\u76f8\u5173\u533a\u57df\uff0c\u4f7f\u7528ResNet-18\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u3002", "result": "\u5728\u771f\u5b9e\u6d4b\u8bd5\u6570\u636e\u4e0a\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u548c\u4e34\u5e8a\u5148\u9a8c\u53ef\u652f\u6301\u51c6\u786e\u4e14\u4f26\u7406\u7684\u76ae\u80a4\u75c5AI\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u8fdc\u7a0b\u533b\u7597\u548c\u5927\u89c4\u6a21\u7b5b\u67e5\u3002"}}
{"id": "2509.10247", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10247", "abs": "https://arxiv.org/abs/2509.10247", "authors": ["Xinhong Zhang", "Runqing Wang", "Yunfan Ren", "Jian Sun", "Hao Fang", "Jie Chen", "Gang Wang"], "title": "DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning", "comment": "8 pages, 11 figures, 1 table", "summary": "This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully\ndifferentiable simulation framework designed for efficient quadrotor control\npolicy learning. DiffAero supports both environment-level and agent-level\nparallelism and integrates multiple dynamics models, customizable sensor stacks\n(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,\nGPU-native training interface. By fully parallelizing both physics and\nrendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and\ndelivers orders-of-magnitude improvements in simulation throughput. In contrast\nto existing simulators, DiffAero not only provides high-performance simulation\nbut also serves as a research platform for exploring differentiable and hybrid\nlearning algorithms. Extensive benchmarks and real-world flight experiments\ndemonstrate that DiffAero and hybrid learning algorithms combined can learn\nrobust flight policies in hours on consumer-grade hardware. The code is\navailable at https://github.com/flyingbitac/diffaero.", "AI": {"tldr": "DiffAero\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001GPU\u52a0\u901f\u7684\u5b8c\u5168\u53ef\u5fae\u5206\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u5b66\u4e60\u56db\u65cb\u7ffc\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u5668\u5728\u6027\u80fd\u548c\u53ef\u5fae\u5206\u5b66\u4e60\u7b97\u6cd5\u63a2\u7d22\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0cDiffAero\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DiffAero\u652f\u6301\u73af\u5883\u548c\u4ee3\u7406\u7ea7\u5e76\u884c\uff0c\u96c6\u6210\u591a\u79cd\u52a8\u529b\u5b66\u6a21\u578b\u3001\u4f20\u611f\u5668\u5806\u6808\u548c\u98de\u884c\u4efb\u52a1\uff0c\u5b8c\u5168\u5728GPU\u4e0a\u5e76\u884c\u5316\u7269\u7406\u548c\u6e32\u67d3\u3002", "result": "DiffAero\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u541e\u5410\u91cf\uff0c\u5e76\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u5feb\u901f\u5b66\u4e60\u5230\u9c81\u68d2\u98de\u884c\u7b56\u7565\u3002", "conclusion": "DiffAero\u4e0d\u4ec5\u63d0\u4f9b\u9ad8\u6027\u80fd\u4eff\u771f\uff0c\u8fd8\u4e3a\u53ef\u5fae\u5206\u548c\u6df7\u5408\u5b66\u4e60\u7b97\u6cd5\u7814\u7a76\u63d0\u4f9b\u4e86\u5e73\u53f0\u3002"}}
{"id": "2509.09960", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09960", "abs": "https://arxiv.org/abs/2509.09960", "authors": ["Mingxuan Jiang", "Yongxin Wang", "Ziyue Dai", "Yicun Liu", "Hongyi Nie", "Sen Liu", "Hongfeng Chai"], "title": "Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes", "comment": null, "summary": "Synthetic tabular data generation is increasingly essential in data\nmanagement, supporting downstream applications when real-world and high-quality\ntabular data is insufficient. Existing tabular generation approaches, such as\ngenerative adversarial networks (GANs), diffusion models, and fine-tuned Large\nLanguage Models (LLMs), typically require sufficient reference data, limiting\ntheir effectiveness in domain-specific databases with scarce records. While\nprompt-based LLMs offer flexibility without parameter tuning, they often fail\nto capture dataset-specific feature-label dependencies and generate redundant\ndata, leading to degradation in downstream task performance. To overcome these\nissues, we propose ReFine, a framework that (i) derives symbolic \"if-then\"\nrules from interpretable models and embeds them into prompts to explicitly\nguide generation toward domain-specific feature distribution, and (ii) applies\na dual-granularity filtering strategy that suppresses over-sampling patterns\nand selectively refines rare but informative samples to reduce distributional\nimbalance. Extensive experiments on various regression and classification\nbenchmarks demonstrate that ReFine consistently outperforms state-of-the-art\nmethods, achieving up to 0.44 absolute improvement in R-squared for regression\nand 10.0 percent relative improvement in F1 score for classification tasks.", "AI": {"tldr": "ReFine\u6846\u67b6\u901a\u8fc7\u7b26\u53f7\u89c4\u5219\u548c\u53cc\u7c92\u5ea6\u8fc7\u6ee4\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5408\u6210\u8868\u683c\u6570\u636e\u7684\u751f\u6210\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u751f\u6210\u65b9\u6cd5\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u6548\u679c\u6709\u9650\uff0c\u4e14\u96be\u4ee5\u6355\u6349\u7279\u5f81-\u6807\u7b7e\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "ReFine\u7ed3\u5408\u7b26\u53f7\u89c4\u5219\u5f15\u5bfc\u751f\u6210\u548c\u53cc\u7c92\u5ea6\u8fc7\u6ee4\u7b56\u7565\uff0c\u4f18\u5316\u6570\u636e\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u663e\u793aReFine\u5728\u56de\u5f52\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cR-squared\u63d0\u53470.44\uff0cF1\u5206\u6570\u63d0\u534710%\u3002", "conclusion": "ReFine\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u548c\u5206\u5e03\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u6570\u636e\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.09849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09849", "abs": "https://arxiv.org/abs/2509.09849", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking", "comment": null, "summary": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u4e86ULW\u6846\u67b6\u4e2d\u5404\u7ec4\u4ef6\u5bf9\u8179\u8154\u955c\u56fe\u50cf\u53bb\u70df\u96fe\u6548\u679c\u7684\u8d21\u732e\u3002", "motivation": "\u9a8c\u8bc1ULW\u6846\u67b6\u4e2d\u5404\u4e2a\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u548c\u5fc5\u8981\u6027\uff0c\u4ee5\u4f18\u5316\u5176\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u79fb\u9664\u5b66\u4e60\u578bWiener\u6ee4\u6ce2\u5668\u6216\u9009\u62e9\u6027\u4f7f\u7528\u590d\u5408\u635f\u5931\u51fd\u6570\u4e2d\u7684\u5355\u4e2a\u635f\u5931\u9879\uff0c\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\u3002", "result": "\u4f7f\u7528\u5b9a\u91cf\u6307\u6807\uff08SSIM\u3001PSNR\u3001MSE\u548cCIEDE-2000\uff09\u548c\u5b9a\u6027\u89c6\u89c9\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5404\u53d8\u4f53\u7684\u6027\u80fd\u3002", "conclusion": "\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u5404\u7ec4\u4ef6\u5bf9ULW\u6846\u67b6\u6574\u4f53\u6027\u80fd\u7684\u5177\u4f53\u8d21\u732e\uff0c\u4e3a\u4f18\u5316\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2509.10305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10305", "abs": "https://arxiv.org/abs/2509.10305", "authors": ["Yutong Shen", "Ruizhe Xia", "Bokai Yan", "Shunqi zhang", "Pengrui Xiang", "Sicheng He", "Yixin Xu"], "title": "GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning", "comment": "6 pages, 5 figures", "summary": "In dynamic and uncertain environments, robotic path planning demands accurate\nspatiotemporal environment understanding combined with robust decision-making\nunder partial observability. However, current deep reinforcement learning-based\npath planning methods face two fundamental limitations: (1) insufficient\nmodeling of multi-scale temporal dependencies, resulting in suboptimal\nadaptability in dynamic scenarios, and (2) inefficient exploration-exploitation\nbalance, leading to degraded path quality. To address these challenges, we\npropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path\nPlanning. The framework comprises two key modules: (i) the Spatiotemporal\nPerception module, which hierarchically extracts multi-granularity spatial\nfeatures and multi-scale temporal dependencies ranging from instantaneous to\nextended time horizons, thereby improving perception accuracy in dynamic\nenvironments; and (ii) the Adaptive Policy Optimization module, which balances\nexploration and exploitation during training while optimizing for smoothness\nand collision probability through constrained policy updates. Experiments in\ndynamic environments demonstrate that GundamQ achieves a 15.3\\% improvement in\nsuccess rate and a 21.7\\% increase in overall path quality, significantly\noutperforming existing state-of-the-art methods.", "AI": {"tldr": "GundamQ\u662f\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u591a\u5c3a\u5ea6\u65f6\u7a7aQ\u7f51\u7edc\uff0c\u901a\u8fc7\u6539\u8fdb\u65f6\u7a7a\u611f\u77e5\u548c\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\u5efa\u6a21\u4e0d\u8db3\u548c\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faGundamQ\u6846\u67b6\uff0c\u5305\u542b\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\uff08\u63d0\u53d6\u591a\u7c92\u5ea6\u7a7a\u95f4\u7279\u5f81\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\uff09\u548c\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u6a21\u5757\uff08\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGundamQ\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6210\u529f\u7387\u63d0\u9ad815.3%\uff0c\u8def\u5f84\u8d28\u91cf\u63d0\u534721.7%\u3002", "conclusion": "GundamQ\u5728\u52a8\u6001\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.09991", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09991", "abs": "https://arxiv.org/abs/2509.09991", "authors": ["Amandip Sangha"], "title": "Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning", "comment": null, "summary": "This paper presents a machine learning-based approach to estimate the energy\nconsumption of virtual servers without access to physical power measurement\ninterfaces. Using resource utilization metrics collected from guest virtual\nmachines, we train a Gradient Boosting Regressor to predict energy consumption\nmeasured via RAPL on the host. We demonstrate, for the first time, guest-only\nresource-based energy estimation without privileged host access with\nexperiments across diverse workloads, achieving high predictive accuracy and\nvariance explained ($0.90 \\leq R^2 \\leq 0.97$), indicating the feasibility of\nguest-side energy estimation. This approach can enable energy-aware scheduling,\ncost optimization and physical host independent energy estimates in virtualized\nenvironments. Our approach addresses a critical gap in virtualized environments\n(e.g. cloud) where direct energy measurement is infeasible.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u865a\u62df\u670d\u52a1\u5668\u80fd\u8017\u4f30\u8ba1\u65b9\u6cd5\uff0c\u65e0\u9700\u7269\u7406\u529f\u7387\u6d4b\u91cf\u63a5\u53e3\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u5316\u73af\u5883\uff08\u5982\u4e91\uff09\u4e2d\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u80fd\u8017\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u865a\u62df\u673a\u7684\u8d44\u6e90\u5229\u7528\u7387\u6307\u6807\uff0c\u8bad\u7ec3\u68af\u5ea6\u63d0\u5347\u56de\u5f52\u5668\u9884\u6d4b\u80fd\u8017\u3002", "result": "\u5728\u591a\u6837\u5316\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u9884\u6d4b\u7cbe\u5ea6\u9ad8\uff08$0.90 \\leq R^2 \\leq 0.97$\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u884c\uff0c\u53ef\u7528\u4e8e\u80fd\u6e90\u611f\u77e5\u8c03\u5ea6\u548c\u6210\u672c\u4f18\u5316\u3002"}}
{"id": "2509.09859", "categories": ["cs.CV", "cs.LG", "68W99"], "pdf": "https://arxiv.org/pdf/2509.09859", "abs": "https://arxiv.org/abs/2509.09859", "authors": ["Razvan Stefanescu", "Ethan Oh", "Ruben Vazquez", "Chris Mesterharm", "Constantin Serban", "Ritu Chadha"], "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector", "comment": "11 pages, 11 figures", "summary": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.", "AI": {"tldr": "WAVE-DETR\u7ed3\u5408RGB\u548c\u58f0\u5b66\u4fe1\u53f7\u7684\u591a\u6a21\u6001\u65e0\u4eba\u673a\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u58f0\u5b66\u7279\u5f81\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff08RGB\u548c\u58f0\u5b66\uff09\u63d0\u5347\u65e0\u4eba\u673a\u68c0\u6d4b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408Deformable DETR\u548cWav2Vec2\u67b6\u6784\uff0c\u6d4b\u8bd5\u4e86\u56db\u79cd\u878d\u5408\u914d\u7f6e\uff08\u95e8\u63a7\u673a\u5236\u3001\u7ebf\u6027\u5c42\u3001MLP\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\uff09\u3002", "result": "\u6700\u4f73\u95e8\u63a7\u878d\u5408\u65b9\u6cd5\u5728\u5c0f\u65e0\u4eba\u673a\u4e0amAP\u63d0\u534711.1%-15.3%\uff0c\u4e2d\u5927\u578b\u65e0\u4eba\u673a\u4e5f\u67093.27%-5.84%\u7684\u63d0\u5347\u3002", "conclusion": "\u591a\u6a21\u6001\u878d\u5408\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u68c0\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.10317", "categories": ["cs.RO", "cs.LG", "93C85", "I.2.9; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.10317", "abs": "https://arxiv.org/abs/2509.10317", "authors": ["Elizaveta D. Moskovskaya", "Anton D. Moscowsky"], "title": "Robot guide with multi-agent control and automatic scenario generation with LLM", "comment": "14 pages, 5 figures, 2 tables, 1 demo-video and repository link", "summary": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8d44\u6e90\u7ba1\u7406\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u63a7\u5236\u67b6\u6784\uff0c\u7528\u4e8e\u4eba\u5f62\u5bfc\u6e38\u673a\u5668\u4eba\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u7cfb\u7edf\u624b\u52a8\u914d\u7f6e\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5bfc\u6e38\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\u884c\u4e3a\u573a\u666f\uff0c\u5b58\u5728\u914d\u7f6e\u7e41\u7410\u3001\u7075\u6d3b\u6027\u4f4e\u548c\u884c\u4e3a\u4e0d\u81ea\u7136\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\uff1a\u9996\u5148\u751f\u6210\u98ce\u683c\u5316\u53d9\u8ff0\uff0c\u7136\u540e\u6574\u5408\u975e\u8bed\u8a00\u52a8\u4f5c\u6807\u7b7e\uff1b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u8c03\u5e76\u884c\u52a8\u4f5c\u548c\u51b2\u7a81\u89e3\u51b3\u3002", "result": "\u8bd5\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u548c\u6269\u5c55\u793e\u4ea4\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "\u6df7\u5408\u63a7\u5236\u67b6\u6784\u80fd\u6709\u6548\u63d0\u5347\u5bfc\u6e38\u673a\u5668\u4eba\u7684\u884c\u4e3a\u81ea\u7136\u6027\u548c\u7cfb\u7edf\u7075\u6d3b\u6027\u3002"}}
{"id": "2509.10000", "categories": ["cs.LG", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2509.10000", "abs": "https://arxiv.org/abs/2509.10000", "authors": ["Tilen Cadez", "Kyoung-Min Kim"], "title": "Neural Scaling Laws for Deep Regression", "comment": "Supplementary Information will be provided with the published\n  manuscript", "summary": "Neural scaling laws--power-law relationships between generalization errors\nand characteristics of deep learning models--are vital tools for developing\nreliable models while managing limited resources. Although the success of large\nlanguage models highlights the importance of these laws, their application to\ndeep regression models remains largely unexplored. Here, we empirically\ninvestigate neural scaling laws in deep regression using a parameter estimation\nmodel for twisted van der Waals magnets. We observe power-law relationships\nbetween the loss and both training dataset size and model capacity across a\nwide range of values, employing various architectures--including fully\nconnected networks, residual networks, and vision transformers. Furthermore,\nthe scaling exponents governing these relationships range from 1 to 2, with\nspecific values depending on the regressed parameters and model details. The\nconsistent scaling behaviors and their large scaling exponents suggest that the\nperformance of deep regression models can improve substantially with increasing\ndata size.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\uff0c\u53d1\u73b0\u635f\u5931\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5bb9\u91cf\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u4e14\u7f29\u653e\u6307\u6570\u8303\u56f4\u57281\u52302\u4e4b\u95f4\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u5728\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u626d\u66f2\u8303\u5fb7\u534e\u78c1\u4f53\u7684\u53c2\u6570\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u79cd\u67b6\u6784\uff08\u5305\u62ec\u5168\u8fde\u63a5\u7f51\u7edc\u3001\u6b8b\u5dee\u7f51\u7edc\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff09\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\u3002", "result": "\u89c2\u5bdf\u5230\u635f\u5931\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\u548c\u6a21\u578b\u5bb9\u91cf\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u7f29\u653e\u6307\u6570\u8303\u56f4\u57281\u52302\u4e4b\u95f4\u3002", "conclusion": "\u6df1\u5ea6\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u53ef\u4ee5\u968f\u7740\u6570\u636e\u91cf\u7684\u589e\u52a0\u800c\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2509.09869", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09869", "abs": "https://arxiv.org/abs/2509.09869", "authors": ["Yihao Liu", "Junyu Chen", "Lianrui Zuo", "Shuwen Wei", "Brian D. Boyd", "Carmen Andreescu", "Olusola Ajilore", "Warren D. Taylor", "Aaron Carass", "Bennett A. Landman"], "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration", "comment": null, "summary": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u66ff\u4ee3\u76d1\u7763\u201d\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u8f93\u5165\u57df\u4e0e\u76d1\u7763\u57df\u89e3\u8026\uff0c\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60\u56fe\u50cf\u914d\u51c6\u7f51\u7edc\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9\u8f93\u5165\u56fe\u50cf\u7279\u6027\uff08\u5982\u4f2a\u5f71\u3001\u89c6\u573a\u4e0d\u5339\u914d\u6216\u6a21\u6001\u5dee\u5f02\uff09\u654f\u611f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u66ff\u4ee3\u76d1\u7763\uff0c\u901a\u8fc7\u5c06\u4f30\u8ba1\u7684\u7a7a\u95f4\u53d8\u6362\u5e94\u7528\u4e8e\u66ff\u4ee3\u56fe\u50cf\uff0c\u5b9e\u73b0\u5728\u5f02\u8d28\u8f93\u5165\u4e0a\u8bad\u7ec3\uff0c\u540c\u65f6\u786e\u4fdd\u76d1\u7763\u5728\u76f8\u4f3c\u6027\u5b9a\u4e49\u660e\u786e\u7684\u57df\u4e2d\u8ba1\u7b97\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\uff0c\u66ff\u4ee3\u76d1\u7763\u8868\u73b0\u51fa\u5bf9\u8f93\u5165\u53d8\u5316\u7684\u5f3a\u97e7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "\u66ff\u4ee3\u76d1\u7763\u4e3a\u8bad\u7ec3\u9c81\u68d2\u4e14\u901a\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u914d\u51c6\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u65e0\u9700\u589e\u52a0\u590d\u6742\u6027\u3002"}}
{"id": "2509.10349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10349", "abs": "https://arxiv.org/abs/2509.10349", "authors": ["Weiyan Lu", "Huizhe Li", "Yuhao Fang", "Zhexuan Zhou", "Junda Wu", "Yude Li", "Youmin Gong", "Jie Mei"], "title": "Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) with suspended payloads offer significant\nadvantages for aerial transportation in complex and cluttered environments.\nHowever, existing systems face critical limitations, including unreliable\nperception of the cable-payload dynamics, inefficient planning in large-scale\nenvironments, and the inability to guarantee whole-body safety under cable\nbending and external disturbances. This paper presents Acetrans, an Autonomous,\nCorridor-based, and Efficient UAV suspended transport system that addresses\nthese challenges through a unified perception, planning, and control framework.\nA LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and\ncable shape under taut and bent modes, enabling robust whole-body state\nestimation and real-time filtering of cable point clouds. To enhance planning\nscalability, we introduce the Multi-size-Aware Configuration-space Iterative\nRegional Inflation (MACIRI) algorithm, which generates safe flight corridors\nwhile accounting for varying UAV and payload geometries. A spatio-temporal,\ncorridor-constrained trajectory optimization scheme is then developed to ensure\ndynamically feasible and collision-free trajectories. Finally, a nonlinear\nmodel predictive controller (NMPC) augmented with cable-bending constraints\nprovides robust whole-body safety during execution. Simulation and experimental\nresults validate the effectiveness of Acetrans, demonstrating substantial\nimprovements in perception accuracy, planning efficiency, and control safety\ncompared to state-of-the-art methods.", "AI": {"tldr": "Acetrans\u662f\u4e00\u4e2a\u57fa\u4e8e\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\u60ac\u6302\u8fd0\u8f93\u7cfb\u7edf\u4e2d\u7684\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b58\u5728\u611f\u77e5\u4e0d\u53ef\u9760\u3001\u89c4\u5212\u6548\u7387\u4f4e\u548c\u65e0\u6cd5\u4fdd\u8bc1\u6574\u4f53\u5b89\u5168\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLiDAR-IMU\u878d\u5408\u6a21\u5757\u4f30\u8ba1\u8d1f\u8f7d\u59ff\u6001\u548c\u7535\u7f06\u5f62\u72b6\uff0cMACIRI\u7b97\u6cd5\u751f\u6210\u5b89\u5168\u98de\u884c\u8d70\u5eca\uff0c\u4ee5\u53ca\u65f6\u7a7a\u7ea6\u675f\u7684\u8f68\u8ff9\u4f18\u5316\u548cNMPC\u63a7\u5236\u5668\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAcetrans\u5728\u611f\u77e5\u7cbe\u5ea6\u3001\u89c4\u5212\u6548\u7387\u548c\u63a7\u5236\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Acetrans\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u60ac\u6302\u8fd0\u8f93\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2509.10011", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.10011", "abs": "https://arxiv.org/abs/2509.10011", "authors": ["Antoine Orioua", "Philipp Krah", "Julian Koellermeier"], "title": "Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss", "comment": "Preprint with 12 pages and 12 figures", "summary": "This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),\nwhich identifies the underlying intrinsic dimension of a wide range of datasets\nwhose samples lie on either linear or nonlinear manifolds. Beyond estimating\nthe intrinsic dimension, IDEA is also able to reconstruct the original dataset\nafter projecting it onto the corresponding latent space, which is structured\nusing re-weighted double CancelOut layers. Our key contribution is the\nintroduction of the projected reconstruction loss term, guiding the training of\nthe model by continuously assessing the reconstruction quality under the\nremoval of an additional latent dimension. We first assess the performance of\nIDEA on a series of theoretical benchmarks to validate its robustness. These\nexperiments allow us to test its reconstruction ability and compare its\nperformance with state-of-the-art intrinsic dimension estimators. The\nbenchmarks show good accuracy and high versatility of our approach.\nSubsequently, we apply our model to data generated from the numerical solution\nof a vertically resolved one-dimensional free-surface flow, following a\npointwise discretization of the vertical velocity profile in the horizontal\ndirection, vertical direction, and time. IDEA succeeds in estimating the\ndataset's intrinsic dimension and then reconstructs the original solution by\nworking directly within the projection space identified by the network.", "AI": {"tldr": "IDEA\u662f\u4e00\u79cd\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u56fa\u6709\u7ef4\u5ea6\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91cd\u5efa\u6570\u636e\u3002\u901a\u8fc7\u5f15\u5165\u6295\u5f71\u91cd\u5efa\u635f\u5931\u9879\uff0cIDEA\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u591a\u529f\u80fd\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u6570\u636e\u96c6\u56fa\u6709\u7ef4\u5ea6\u5e76\u91cd\u5efa\u6570\u636e\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u6d41\u5f62\u3002", "method": "IDEA\u4f7f\u7528\u91cd\u65b0\u52a0\u6743\u7684\u53ccCancelOut\u5c42\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u91cd\u5efa\u635f\u5931\u9879\u6307\u5bfc\u8bad\u7ec3\u3002", "result": "\u5728\u7406\u8bba\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cIDEA\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u6d41\u4f53\u52a8\u529b\u5b66\u6570\u636e\u4e2d\u6210\u529f\u4f30\u8ba1\u548c\u91cd\u5efa\u6570\u636e\u3002", "conclusion": "IDEA\u662f\u4e00\u79cd\u6709\u6548\u7684\u5de5\u5177\uff0c\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u56fa\u6709\u7ef4\u5ea6\u5e76\u91cd\u5efa\u6570\u636e\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2509.09911", "categories": ["cs.CV", "cs.AI", "68T07 (Primary)"], "pdf": "https://arxiv.org/pdf/2509.09911", "abs": "https://arxiv.org/abs/2509.09911", "authors": ["Barkin Buyukcakir", "Jannick De Tobel", "Patrick Thevissen", "Dirk Vandermeulen", "Peter Claes"], "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars", "comment": "21 pages, 11 figures, Scientific Reports", "summary": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u81ea\u7f16\u7801\u5668\u548cVision Transformer\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u7259\u9f7f\u5e74\u9f84\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u6cd5\u533b\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u91c7\u7528\u5e38\u53d7\u9650\u4e8e\u6a21\u578b\u7684'\u9ed1\u7bb1'\u7279\u6027\uff0c\u5c24\u5176\u662f\u5728\u7259\u9f7f\u5e74\u9f84\u4f30\u8ba1\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u3002", "method": "\u4f7f\u7528\u5377\u79ef\u81ea\u7f16\u7801\u5668\uff08AE\uff09\u548cVision Transformer\uff08ViT\uff09\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u5206\u6790\u7259\u9f7f37\u548c38\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u5206\u7c7b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u7259\u9f7f37\u4ece0.712\u63d0\u9ad8\u52300.815\uff0c\u7259\u9f7f38\u4ece0.462\u63d0\u9ad8\u52300.543\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u8fd8\u901a\u8fc7\u591a\u89d2\u5ea6\u8bca\u65ad\u63ed\u793a\u4e86\u6570\u636e\u95ee\u9898\uff0c\u4e3a\u6cd5\u533b\u5e74\u9f84\u4f30\u8ba1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2509.10405", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10405", "abs": "https://arxiv.org/abs/2509.10405", "authors": ["Nicholas Carlotti", "Mirko Nava", "Alessandro Giusti"], "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States", "comment": "accepted at CoRL 2025", "summary": "We introduce a model for monocular RGB relative pose estimation of a ground\nrobot that trains from scratch without pose labels nor prior knowledge about\nthe robot's shape or appearance. At training time, we assume: (i) a robot\nfitted with multiple LEDs, whose states are independent and known at each\nframe; (ii) knowledge of the approximate viewing direction of each LED; and\n(iii) availability of a calibration image with a known target distance, to\naddress the ambiguity of monocular depth estimation. Training data is collected\nby a pair of robots moving randomly without needing external infrastructure or\nhuman supervision. Our model trains on the task of predicting from an image the\nstate of each LED on the robot. In doing so, it learns to predict the position\nof the robot in the image, its distance, and its relative bearing. At inference\ntime, the state of the LEDs is unknown, can be arbitrary, and does not affect\nthe pose estimation performance. Quantitative experiments indicate that our\napproach: is competitive with SoA approaches that require supervision from pose\nlabels or a CAD model of the robot; generalizes to different domains; and\nhandles multi-robot pose estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u59ff\u6001\u6807\u7b7e\u6216\u673a\u5668\u4eba\u5f62\u72b6\u5148\u9a8c\u7684\u5355\u76eeRGB\u76f8\u5bf9\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7LED\u72b6\u6001\u9884\u6d4b\u4efb\u52a1\u5b66\u4e60\u673a\u5668\u4eba\u4f4d\u7f6e\u3001\u8ddd\u79bb\u548c\u76f8\u5bf9\u65b9\u4f4d\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u59ff\u6001\u6807\u7b7e\u6216CAD\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "method": "\u5229\u7528\u673a\u5668\u4eba\u4e0a\u7684LED\u72b6\u6001\u548c\u5df2\u77e5\u89c6\u89d2\u65b9\u5411\uff0c\u901a\u8fc7\u9884\u6d4bLED\u72b6\u6001\u4efb\u52a1\u5b66\u4e60\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728\u5b9a\u91cf\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4e0e\u6709\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u9886\u57df\u548c\u591a\u673a\u5668\u4eba\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u65e0\u76d1\u7763\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u59ff\u6001\u4f30\u8ba1\u3002"}}
{"id": "2509.10025", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10025", "abs": "https://arxiv.org/abs/2509.10025", "authors": ["Strahinja Nikolic", "Ilker Oguz", "Demetri Psaltis"], "title": "Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts", "comment": "14 pages, 7 figures", "summary": "Understanding the internal organization of neural networks remains a\nfundamental challenge in deep learning interpretability. We address this\nchallenge by exploring a novel Sparse Mixture of Experts Variational\nAutoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw\ndataset, comparing unsupervised expert routing against a supervised baseline\nguided by ground-truth labels. Surprisingly, we find that unsupervised routing\nconsistently achieves superior reconstruction performance. The experts learn to\nidentify meaningful sub-categorical structures that often transcend\nhuman-defined class boundaries. Through t-SNE visualizations and reconstruction\nanalysis, we investigate how MoE models uncover fundamental data structures\nthat are more aligned with the model's objective than predefined labels.\nFurthermore, our study on the impact of dataset size provides insights into the\ntrade-offs between data quantity and expert specialization, offering guidance\nfor designing efficient MoE architectures.", "AI": {"tldr": "SMoE-VAE\u6a21\u578b\u5728QuickDraw\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\uff0c\u65e0\u76d1\u7763\u8def\u7531\u80fd\u8bc6\u522b\u8d85\u8d8a\u4eba\u5de5\u7c7b\u522b\u7684\u6709\u610f\u4e49\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u7ec4\u7ec7\uff0c\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08SMoE-VAE\uff09\uff0c\u6bd4\u8f83\u65e0\u76d1\u7763\u4e0e\u76d1\u7763\u8def\u7531\u3002", "result": "\u65e0\u76d1\u7763\u8def\u7531\u91cd\u5efa\u6027\u80fd\u66f4\u4f18\uff0c\u4e13\u5bb6\u5b66\u4e60\u5230\u8d85\u8d8a\u4eba\u5de5\u7c7b\u522b\u7684\u7ed3\u6784\u3002", "conclusion": "MoE\u6a21\u578b\u80fd\u53d1\u73b0\u66f4\u7b26\u5408\u76ee\u6807\u7684\u6570\u636e\u7ed3\u6784\uff0c\u6570\u636e\u96c6\u5927\u5c0f\u5f71\u54cd\u4e13\u5bb6\u4e13\u4e1a\u5316\u3002"}}
{"id": "2509.09935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09935", "abs": "https://arxiv.org/abs/2509.09935", "authors": ["Chirayu Agrawal", "Snehasis Mukherjee"], "title": "SCoDA: Self-supervised Continual Domain Adaptation", "comment": "Submitted to ICVGIP 2025", "summary": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.", "AI": {"tldr": "SCoDA\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6e90\u57df\u6570\u636e\u7684\u81ea\u76d1\u7763\u6301\u7eed\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u6d41\u5f62\u5bf9\u9f50\u548cEMA\u66f4\u65b0\u6559\u5e08\u6a21\u578b\u53c2\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709SFDA\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SFDA\u65b9\u6cd5\u56e0\u4f9d\u8d56\u4f59\u5f26\u76f8\u4f3c\u5ea6\u800c\u4e22\u5931\u51e0\u4f55\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5e76\u907f\u514d\u5bf9\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u6559\u5e08\u6a21\u578b\uff0c\u7ed3\u5408\u5b9e\u4f8b\u7ea7\u7279\u5f81\u5339\u914d\u548c\u7a7a\u95f4\u76f8\u4f3c\u6027\u635f\u5931\uff0c\u901a\u8fc7EMA\u66f4\u65b0\u6559\u5e08\u53c2\u6570\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709SFDA\u65b9\u6cd5\u3002", "conclusion": "SCoDA\u901a\u8fc7\u51e0\u4f55\u6d41\u5f62\u5bf9\u9f50\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86SFDA\u7684\u6027\u80fd\u3002"}}
{"id": "2509.10416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10416", "abs": "https://arxiv.org/abs/2509.10416", "authors": ["Ze Fu", "Pinhao Song", "Yutong Hu", "Renaud Detry"], "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation", "comment": null, "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.", "AI": {"tldr": "TASC\u662f\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u5171\u4eab\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u65ad\u4efb\u52a1\u7ea7\u7528\u6237\u610f\u56fe\u5e76\u63d0\u4f9b\u8f85\u52a9\uff0c\u652f\u6301\u65e5\u5e38\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u3001\u957f\u65f6\u5171\u4eab\u63a7\u5236\u4e2d\u7684\u4e24\u5927\u6311\u6218\uff1a\u63a8\u65ad\u4efb\u52a1\u7ea7\u7528\u6237\u610f\u56fe\u548c\u8de8\u591a\u6837\u5bf9\u8c61\u4e0e\u4efb\u52a1\u7684\u8f85\u52a9\u6cdb\u5316\u3002", "method": "\u6784\u5efa\u5f00\u653e\u8bcd\u6c47\u4ea4\u4e92\u56fe\u8868\u793a\u529f\u80fd\u5bf9\u8c61\u5173\u7cfb\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7a7a\u95f4\u7ea6\u675f\uff0c\u63d0\u4f9b\u65cb\u8f6c\u8f85\u52a9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTASC\u63d0\u9ad8\u4e86\u4efb\u52a1\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u7528\u6237\u8f93\u5165\u52aa\u529b\uff0c\u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "TASC\u662f\u9996\u4e2a\u652f\u6301\u65e5\u5e38\u64cd\u4f5c\u4efb\u52a1\u4e14\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u5171\u4eab\u63a7\u5236\u6846\u67b6\u3002"}}
{"id": "2509.10033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10033", "abs": "https://arxiv.org/abs/2509.10033", "authors": ["Boya Ma", "Abram Magner", "Maxwell McNeil", "Petko Bogdanov"], "title": "Sparse Coding Representation of 2-way Data", "comment": null, "summary": "Sparse dictionary coding represents signals as linear combinations of a few\ndictionary atoms. It has been applied to images, time series, graph signals and\nmulti-way spatio-temporal data by jointly employing temporal and spatial\ndictionaries. Data-agnostic analytical dictionaries, such as the discrete\nFourier transform, wavelets and graph Fourier, have seen wide adoption due to\nefficient implementations and good practical performance. On the other hand,\ndictionaries learned from data offer sparser and more accurate solutions but\nrequire learning of both the dictionaries and the coding coefficients. This\nbecomes especially challenging for multi-dictionary scenarios since encoding\ncoefficients correspond to all atom combinations from the dictionaries. To\naddress this challenge, we propose a low-rank coding model for 2-dictionary\nscenarios and study its data complexity. Namely, we establish a bound on the\nnumber of samples needed to learn dictionaries that generalize to unseen\nsamples from the same distribution. We propose a convex relaxation solution,\ncalled AODL, whose exact solution we show also solves the original problem. We\nthen solve this relaxation via alternating optimization between the sparse\ncoding matrices and the learned dictionaries, which we prove to be convergent.\nWe demonstrate its quality for data reconstruction and missing value imputation\nin both synthetic and real-world datasets. For a fixed reconstruction quality,\nAODL learns up to 90\\% sparser solutions compared to non-low-rank and\nanalytical (fixed) dictionary baselines. In addition, the learned dictionaries\nreveal interpretable insights into patterns present within the samples used for\ntraining.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u79e9\u7f16\u7801\u6a21\u578b\uff08AODL\uff09\u7528\u4e8e\u89e3\u51b3\u591a\u5b57\u5178\u573a\u666f\u4e0b\u7684\u7a00\u758f\u5b57\u5178\u7f16\u7801\u95ee\u9898\uff0c\u901a\u8fc7\u51f8\u677e\u5f1b\u548c\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u7a00\u758f\u6027\u548c\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u65e0\u5173\u5206\u6790\u5b57\u5178\uff08\u5982\u5085\u91cc\u53f6\u53d8\u6362\u3001\u5c0f\u6ce2\u7b49\uff09\u867d\u7136\u9ad8\u6548\u4f46\u7a00\u758f\u6027\u4e0d\u8db3\uff0c\u800c\u6570\u636e\u9a71\u52a8\u7684\u5b57\u5178\u5b66\u4e60\u867d\u7136\u66f4\u51c6\u786e\u4f46\u8ba1\u7b97\u590d\u6742\uff0c\u5c24\u5176\u662f\u5728\u591a\u5b57\u5178\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4f4e\u79e9\u7f16\u7801\u6a21\u578b\uff0c\u901a\u8fc7\u51f8\u677e\u5f1b\u548c\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\u5b66\u4e60\u5b57\u5178\u548c\u7f16\u7801\u7cfb\u6570\uff0c\u8bc1\u660e\u4e86\u5176\u6536\u655b\u6027\u3002", "result": "AODL\u5728\u6570\u636e\u91cd\u5efa\u548c\u7f3a\u5931\u503c\u586b\u8865\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7a00\u758f\u6027\u6bd4\u975e\u4f4e\u79e9\u548c\u56fa\u5b9a\u5b57\u5178\u57fa\u7ebf\u9ad890%\uff0c\u4e14\u5b57\u5178\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "AODL\u5728\u591a\u5b57\u5178\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u6a21\u5f0f\u3002"}}
{"id": "2509.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09943", "abs": "https://arxiv.org/abs/2509.09943", "authors": ["Zhu Chen", "Mert Edg\u00fc", "Er Jin", "Johannes Stegmaier"], "title": "Segment Anything for Cell Tracking", "comment": null, "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u6837\u672c\u7ec6\u80de\u8ffd\u8e2a\u6846\u67b6\uff0c\u7ed3\u5408SAM2\u6a21\u578b\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u901a\u7528\u5316\u5904\u7406\u591a\u79cd\u663e\u5fae\u955c\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210Segment Anything 2 (SAM2)\u6a21\u578b\uff0c\u5b9e\u73b0\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u7ec6\u80de\u8ffd\u8e2a\u3002", "result": "\u57282D\u548c3D\u663e\u5fae\u955c\u89c6\u9891\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u7cbe\u5ea6\uff0c\u65e0\u9700\u6570\u636e\u96c6\u9002\u914d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u6570\u636e\uff0c\u80fd\u6709\u6548\u6cdb\u5316\u5230\u591a\u6837\u5316\u7684\u663e\u5fae\u955c\u6570\u636e\u96c6\u3002"}}
{"id": "2509.10426", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10426", "abs": "https://arxiv.org/abs/2509.10426", "authors": ["Jianxin Shi", "Zengqi Peng", "Xiaolong Chen", "Tianyu Wo", "Jun Ma"], "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training", "comment": null, "summary": "Trajectory prediction is a critical component of autonomous driving,\nessential for ensuring both safety and efficiency on the road. However,\ntraditional approaches often struggle with the scarcity of labeled data and\nexhibit suboptimal performance in multi-agent prediction scenarios. To address\nthese challenges, we introduce a disentangled context-aware pre-training\nframework for multi-agent motion prediction, named DECAMP. Unlike existing\nmethods that entangle representation learning with pretext tasks, our framework\ndecouples behavior pattern learning from latent feature reconstruction,\nprioritizing interpretable dynamics and thereby enhancing scene representation\nfor downstream prediction. Additionally, our framework incorporates\ncontext-aware representation learning alongside collaborative spatial-motion\npretext tasks, which enables joint optimization of structural and intentional\nreasoning while capturing the underlying dynamic intentions. Our experiments on\nthe Argoverse 2 benchmark showcase the superior performance of our method, and\nthe results attained underscore its effectiveness in multi-agent motion\nforecasting. To the best of our knowledge, this is the first context\nautoencoder framework for multi-agent motion forecasting in autonomous driving.\nThe code and models will be made publicly available.", "AI": {"tldr": "DECAMP\u662f\u4e00\u4e2a\u89e3\u8026\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u901a\u8fc7\u5206\u79bb\u884c\u4e3a\u6a21\u5f0f\u5b66\u4e60\u548c\u6f5c\u5728\u7279\u5f81\u91cd\u5efa\uff0c\u63d0\u5347\u573a\u666f\u8868\u793a\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u6807\u8bb0\u6570\u636e\u7a00\u7f3a\u548c\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u89e3\u8026\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u6846\u67b6DECAMP\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\u5b66\u4e60\u548c\u534f\u4f5c\u7a7a\u95f4-\u8fd0\u52a8\u9884\u4efb\u52a1\u3002", "result": "\u5728Argoverse 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DECAMP\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u81ea\u7f16\u7801\u6846\u67b6\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2509.10034", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10034", "abs": "https://arxiv.org/abs/2509.10034", "authors": ["Sahil Rajesh Dhayalkar"], "title": "Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability", "comment": "19 pages, 2 figures", "summary": "We present a formal and constructive theory showing that probabilistic finite\nautomata (PFAs) can be exactly simulated using symbolic feedforward neural\nnetworks. Our architecture represents state distributions as vectors and\ntransitions as stochastic matrices, enabling probabilistic state propagation\nvia matrix-vector products. This yields a parallel, interpretable, and\ndifferentiable simulation of PFA dynamics using soft updates-without\nrecurrence. We formally characterize probabilistic subset construction,\n$\\varepsilon$-closure, and exact simulation via layered symbolic computation,\nand prove equivalence between PFAs and specific classes of neural networks. We\nfurther show that these symbolic simulators are not only expressive but\nlearnable: trained with standard gradient descent-based optimization on labeled\nsequence data, they recover the exact behavior of ground-truth PFAs. This\nlearnability, formalized in Proposition 5.1, is the crux of this work. Our\nresults unify probabilistic automata theory with neural architectures under a\nrigorous algebraic framework, bridging the gap between symbolic computation and\ndeep learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u7b26\u53f7\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7cbe\u786e\u6a21\u62df\u6982\u7387\u6709\u9650\u81ea\u52a8\u673a\uff08PFA\uff09\u7684\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u5176\u7b49\u4ef7\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u5b66\u4e60\u6027\u3002", "motivation": "\u4e3a\u4e86\u7edf\u4e00\u6982\u7387\u81ea\u52a8\u673a\u7406\u8bba\u4e0e\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u586b\u8865\u7b26\u53f7\u8ba1\u7b97\u4e0e\u6df1\u5ea6\u5b66\u4e60\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u901a\u8fc7\u5c06\u72b6\u6001\u5206\u5e03\u8868\u793a\u4e3a\u5411\u91cf\u3001\u8f6c\u79fb\u8868\u793a\u4e3a\u968f\u673a\u77e9\u9635\uff0c\u5229\u7528\u77e9\u9635-\u5411\u91cf\u4e58\u79ef\u5b9e\u73b0\u6982\u7387\u72b6\u6001\u4f20\u64ad\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u5e76\u884c\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u5fae\u7684PFA\u6a21\u62df\u65b9\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86PFA\u4e0e\u7279\u5b9a\u7c7b\u795e\u7ecf\u7f51\u7edc\u7684\u7b49\u4ef7\u6027\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u5bf9\u771f\u5b9ePFA\u884c\u4e3a\u7684\u7cbe\u786e\u6062\u590d\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6982\u7387\u81ea\u52a8\u673a\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u4ee3\u6570\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u7b26\u53f7\u8ba1\u7b97\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u878d\u5408\u6f5c\u529b\u3002"}}
{"id": "2509.09946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09946", "abs": "https://arxiv.org/abs/2509.09946", "authors": ["Vu-Minh Le", "Thao-Anh Tran", "Duc Huy Do", "Xuan Canh Do", "Huong Ninh", "Hai Tran"], "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation", "comment": "Accepted at ICCVW 2025", "summary": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c062D\u591a\u6444\u50cf\u5934\u8ddf\u8e2a\u7cfb\u7edf\u6269\u5c55\u52303D\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u91cd\u5efa\u76ee\u6807\u70b9\u4e91\uff0c\u5e76\u901a\u8fc7\u805a\u7c7b\u548c\u504f\u822a\u7ec6\u5316\u6062\u590d3D\u6846\u3002", "motivation": "\u73b0\u6709MTMC\u7cfb\u7edf\u96be\u4ee5\u76f4\u63a5\u4ece2D\u8ddf\u8e2a\u5347\u7ea7\u52303D\u8ddf\u8e2a\uff0c\u9700\u4fdd\u7559\u73b0\u67092D\u7cfb\u7edf\u529f\u80fd\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u4fe1\u606f\u91cd\u5efa\u76ee\u6807\u70b9\u4e91\uff0c\u901a\u8fc7\u805a\u7c7b\u548c\u504f\u822a\u7ec6\u5316\u6062\u590d3D\u6846\uff0c\u5e76\u5f15\u5165\u589e\u5f3a\u7684\u5728\u7ebf\u6570\u636e\u5173\u8054\u673a\u5236\u3002", "result": "\u57282025 AI City Challenge\u76843D MTMC\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7b2c3\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c062D\u8ddf\u8e2a\u7cfb\u7edf\u6269\u5c55\u52303D\u7a7a\u95f4\uff0c\u4fdd\u7559\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u529f\u80fd\u3002"}}
{"id": "2509.10444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10444", "abs": "https://arxiv.org/abs/2509.10444", "authors": ["Chaerim Moon", "Joohyung Kim"], "title": "Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction", "comment": "Presented in IROS 2023 Workshop (Multilimb Coordination in Human\n  Neuroscience and Robotics: Classical and Learning Perspectives)", "summary": "Supernumerary Robotic Limbs (SRLs) can enhance human capability within close\nproximity. However, as a wearable device, the generated moment from its\noperation acts on the human body as an external torque. When the moments\nincrease, more muscle units are activated for balancing, and it can result in\nreduced muscular null space. Therefore, this paper suggests a concept of a\nmotion planning layer that reduces the generated moment for enhanced\nHuman-Robot Interaction. It modifies given trajectories with desirable angular\nacceleration and position deviation limits. Its performance to reduce the\nmoment is demonstrated through the simulation, which uses simplified human and\nrobotic system models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u89c4\u5212\u5c42\u6982\u5ff5\uff0c\u4ee5\u51cf\u5c11\u673a\u5668\u4eba\u80a2\u4f53\u64cd\u4f5c\u65f6\u5bf9\u4eba\u4f53\u7684\u5916\u90e8\u626d\u77e9\uff0c\u4ece\u800c\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u8d85\u7ea7\u673a\u5668\u4eba\u80a2\u4f53\uff08SRLs\uff09\u5728\u64cd\u4f5c\u65f6\u4f1a\u4ea7\u751f\u5916\u90e8\u626d\u77e9\u4f5c\u7528\u4e8e\u4eba\u4f53\uff0c\u589e\u52a0\u808c\u8089\u8d1f\u62c5\u5e76\u51cf\u5c11\u808c\u8089\u96f6\u7a7a\u95f4\u3002", "method": "\u901a\u8fc7\u4fee\u6539\u7ed9\u5b9a\u8f68\u8ff9\uff0c\u5f15\u5165\u89d2\u52a0\u901f\u5ea6\u548c\u4f4d\u7f6e\u504f\u5dee\u9650\u5236\u7684\u8fd0\u52a8\u89c4\u5212\u5c42\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u626d\u77e9\u3002", "conclusion": "\u8be5\u8fd0\u52a8\u89c4\u5212\u5c42\u6982\u5ff5\u6709\u52a9\u4e8e\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\uff0c\u51cf\u5c11\u5bf9\u4eba\u4f53\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2509.10041", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10041", "abs": "https://arxiv.org/abs/2509.10041", "authors": ["Mohammad Hasan Narimani", "Mostafa Tavassolipour"], "title": "FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection", "comment": null, "summary": "Federated learning (FL) offers an innovative paradigm for collaborative model\ntraining across decentralized devices, such as smartphones, balancing enhanced\npredictive performance with the protection of user privacy in sensitive areas\nlike Internet of Things (IoT) and medical data analysis. Despite its\nadvantages, FL encounters significant challenges related to user privacy\nprotection against potential attacks and the management of communication costs.\nThis paper introduces a novel federated learning algorithm called FedRP, which\nintegrates random projection techniques with the Alternating Direction Method\nof Multipliers (ADMM) optimization framework. This approach enhances privacy by\nemploying random projection to reduce the dimensionality of model parameters\nprior to their transmission to a central server, reducing the communication\ncost. The proposed algorithm offers a strong $(\\epsilon, \\delta)$-differential\nprivacy guarantee, demonstrating resilience against data reconstruction\nattacks. Experimental results reveal that FedRP not only maintains high model\naccuracy but also outperforms existing methods, including conventional\ndifferential privacy approaches and FedADMM, in terms of both privacy\npreservation and communication efficiency.", "AI": {"tldr": "FedRP\u662f\u4e00\u79cd\u7ed3\u5408\u968f\u673a\u6295\u5f71\u548cADMM\u4f18\u5316\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6210\u672c\u7ba1\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "FedRP\u901a\u8fc7\u968f\u673a\u6295\u5f71\u964d\u7ef4\u6a21\u578b\u53c2\u6570\uff0c\u7ed3\u5408ADMM\u6846\u67b6\u4f18\u5316\uff0c\u63d0\u4f9b\u5dee\u5206\u9690\u79c1\u4fdd\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793aFedRP\u5728\u6a21\u578b\u7cbe\u5ea6\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedRP\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u79c1\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09958", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09958", "abs": "https://arxiv.org/abs/2509.09958", "authors": ["Jeffrey Liu", "Rongbin Hu"], "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification", "comment": null, "summary": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.", "AI": {"tldr": "\u96f6\u6837\u672c\u5de5\u4f5c\u6d41\u5728Referring Expression Comprehension (REC)\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u3002", "motivation": "\u63a2\u7d22\u96f6\u6837\u672c\u65b9\u6cd5\u5728REC\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u51cf\u5c11\u5bf9\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u4f9d\u8d56\u3002", "method": "\u5c06REC\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u9a8c\u8bc1\u7684\u6846\u7ea7\u68c0\u6d4b\uff0c\u4f7f\u7528\u901a\u7528\u68c0\u6d4b\u5668\u548cVLM\u72ec\u7acb\u9a8c\u8bc1\u6bcf\u4e2a\u533a\u57df\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u751a\u81f3\u4f18\u4e8e\u7ecf\u8fc7\u4efb\u52a1\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "\u5de5\u4f5c\u6d41\u8bbe\u8ba1\u800c\u975e\u4efb\u52a1\u7279\u5b9a\u9884\u8bad\u7ec3\u662f\u96f6\u6837\u672cREC\u6027\u80fd\u7684\u5173\u952e\u3002"}}
{"id": "2509.10454", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10454", "abs": "https://arxiv.org/abs/2509.10454", "authors": ["Hang Yin", "Haoyu Wei", "Xiuwei Xu", "Wenxuan Guo", "Jie Zhou", "Jiwen Lu"], "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation", "comment": "Accepted to CoRL 2025. Project page: [this https\n  URL](https://bagh2178.github.io/GC-VLN/)", "summary": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u771f\u5b9e\u573a\u666f\uff0c\u9700\u8981\u65e0\u76d1\u7763\u8bad\u7ec3\u6216\u4ec5\u9002\u7528\u4e8e\u79bb\u6563\u73af\u5883\u3002", "method": "\u5c06\u5bfc\u822a\u6307\u4ee4\u5206\u89e3\u4e3a\u7a7a\u95f4\u7ea6\u675f\uff0c\u6784\u5efa\u7ea6\u675f\u5e93\uff0c\u901a\u8fc7\u7ea6\u675f\u6c42\u89e3\u5668\u4f18\u5316\u8def\u5f84\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u5bfc\u822a\u6548\u7387\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u73af\u5883\u548c\u6307\u4ee4\u96c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u66f4\u9c81\u68d2\u548c\u81ea\u4e3b\u7684\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2509.10048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10048", "abs": "https://arxiv.org/abs/2509.10048", "authors": ["Madhushan Ramalingam"], "title": "Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data", "comment": null, "summary": "Predictive models are being increasingly used across a wide range of domains,\nincluding safety-critical applications such as medical diagnosis and criminal\njustice. Reliable uncertainty estimation is a crucial task in such settings.\nTabular Prior-data Fitted Network (TabPFN) is a recently proposed machine\nlearning foundation model for tabular dataset, which uses a generative\ntransformer architecture. Variational Bayesian Last Layers (VBLL) is a\nstate-of-the-art lightweight variational formulation that effectively improves\nuncertainty estimation with minimal computational overhead. In this work we aim\nto evaluate the performance of VBLL integrated with the recently proposed\nTabPFN in uncertainty calibration. Our experiments, conducted on three\nbenchmark medical tabular datasets, compare the performance of the original\nTabPFN and the VBLL-integrated version. Contrary to expectations, we observed\nthat original TabPFN consistently outperforms VBLL integrated TabPFN in\nuncertainty calibration across all datasets.", "AI": {"tldr": "\u8bc4\u4f30VBLL\u4e0eTabPFN\u7ed3\u5408\u5728\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u539f\u59cbTabPFN\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u7814\u7a76VBLL\u4e0eTabPFN\u7684\u7ed3\u5408\u6548\u679c\u3002", "method": "\u5728\u4e09\u4e2a\u533b\u7597\u8868\u683c\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u539f\u59cbTabPFN\u548cVBLL\u96c6\u6210\u7684TabPFN\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u6027\u80fd\u3002", "result": "\u539f\u59cbTabPFN\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8eVBLL\u96c6\u6210\u7684TabPFN\u3002", "conclusion": "VBLL\u96c6\u6210\u672a\u80fd\u63d0\u5347TabPFN\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u6027\u80fd\uff0c\u539f\u59cbTabPFN\u8868\u73b0\u66f4\u7a33\u5b9a\u3002"}}
{"id": "2509.09961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09961", "abs": "https://arxiv.org/abs/2509.09961", "authors": ["Tianqi Wei", "Xin Yu", "Zhi Chen", "Scott Chapman", "Zi Huang"], "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation", "comment": null, "summary": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.", "AI": {"tldr": "\u63d0\u51faRPCP\u589e\u5f3a\u6280\u672f\u89e3\u51b3\u5c0f\u9ea6\u53f6\u7247\u75c5\u866b\u5bb3\u5206\u5272\u4e2d\u7684\u50cf\u7d20\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u75c5\u866b\u5bb3\u50cf\u7d20\u5360\u6bd4\u6781\u4f4e\u5bfc\u81f4\u5206\u5272\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u50cf\u7d20\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u53d6\u75c5\u866b\u5bb3\u533a\u57df\uff0c\u968f\u673a\u53d8\u6362\u540e\u7c98\u8d34\u81f3\u5408\u9002\u4f4d\u7f6e\uff0c\u5e76\u4f7f\u7528\u968f\u673a\u6295\u5f71\u6ee4\u6ce2\u5668\u4f18\u5316\u878d\u5408\u6548\u679c\u3002", "result": "\u663e\u8457\u63d0\u5347\u75c5\u866b\u5bb3\u7c7b\u522b\u7684\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u7c7b\u522b\u7cbe\u5ea6\u3002", "conclusion": "RPCP\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u50cf\u7d20\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u519c\u4e1a\u5206\u5272\u63d0\u4f9b\u7b80\u5355\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10089", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10089", "abs": "https://arxiv.org/abs/2509.10089", "authors": ["Marco Andrea B\u00fchler", "Gonzalo Guill\u00e9n-Gos\u00e1lbez"], "title": "KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework", "comment": null, "summary": "We introduce a novel symbolic regression framework, namely KAN-SR, built on\nKolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.\nSymbolic regression searches for mathematical equations that best fit a given\ndataset and is commonly solved with genetic programming approaches. We show\nthat by using deep learning techniques, more specific KANs, and combining them\nwith simplification strategies such as translational symmetries and\nseparabilities, we are able to recover ground-truth equations of the Feynman\nSymbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we\nshow that by combining the proposed framework with neural controlled\ndifferential equations, we are able to model the dynamics of an in-silico\nbioprocess system precisely, opening the door for the dynamic modeling of other\nengineering systems.", "AI": {"tldr": "KAN-SR\u662f\u4e00\u79cd\u57fa\u4e8eKolmogorov Arnold Networks\uff08KANs\uff09\u7684\u65b0\u578b\u7b26\u53f7\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6cbb\u6cd5\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u591f\u7cbe\u786e\u6062\u590dFeynman SRSD\u6570\u636e\u96c6\u4e2d\u7684\u771f\u5b9e\u65b9\u7a0b\uff0c\u5e76\u53ef\u7528\u4e8e\u52a8\u6001\u5efa\u6a21\u5de5\u7a0b\u7cfb\u7edf\u3002", "motivation": "\u7b26\u53f7\u56de\u5f52\u901a\u5e38\u901a\u8fc7\u9057\u4f20\u7f16\u7a0b\u89e3\u51b3\uff0c\u4f46\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u7ed3\u5408KANs\u548c\u7b80\u5316\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u65b9\u7a0b\u6062\u590d\u7684\u7cbe\u786e\u6027\u548c\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u5206\u6cbb\u6cd5\u7ed3\u5408KANs\uff0c\u5229\u7528\u5e73\u79fb\u5bf9\u79f0\u6027\u548c\u53ef\u5206\u79bb\u6027\u7b49\u7b80\u5316\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u5efa\u6a21\u52a8\u6001\u7cfb\u7edf\u3002", "result": "\u6210\u529f\u6062\u590dFeynman SRSD\u6570\u636e\u96c6\u4e2d\u7684\u771f\u5b9e\u65b9\u7a0b\uff0c\u5e76\u7cbe\u786e\u5efa\u6a21\u4e86\u751f\u7269\u8fc7\u7a0b\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\u3002", "conclusion": "KAN-SR\u6846\u67b6\u5728\u7b26\u53f7\u56de\u5f52\u548c\u52a8\u6001\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5de5\u7a0b\u7cfb\u7edf\u7684\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.09962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09962", "abs": "https://arxiv.org/abs/2509.09962", "authors": ["Anne Marthe Sophie Ngo Bibinbe", "Chiron Bang", "Patrick Gagnon", "Jamie Ahloy-Dallaire", "Eric R. Paquet"], "title": "An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock", "comment": "13 pages, 7 figures, 1 table, accepted at CVPR animal workshop 2024,\n  submitted to IJCV", "summary": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e0d\u786e\u5b9a\u8eab\u4efd\u548c\u8ddf\u8e2a\u7684HMM\u6846\u67b6\uff0c\u7528\u4e8e\u957f\u671f\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MOT\u65b9\u6cd5\u5728\u957f\u671f\u8ddf\u8e2a\u4e2d\u56e0\u8eab\u4efd\u5207\u6362\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u901a\u8fc7\u5176\u4ed6\u6765\u6e90\uff08\u5982\u5582\u98df\u5668\uff09\u83b7\u53d6\u90e8\u5206\u5bf9\u8c61\u7684\u8eab\u4efd\u4fe1\u606f\u3002", "method": "\u4f7f\u7528HMM\u6846\u67b6\u7ed3\u5408\u4e0d\u786e\u5b9a\u8eab\u4efd\u548c\u8ddf\u8e2a\uff0c\u4f18\u5316\u4e86ByteTrack\u7b49MOT\u65b9\u6cd5\u3002", "result": "\u572810\u5206\u949f\u7684\u732a\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86F1\u5206\u6570\uff0c\u5e76\u5728MOT17\u548cMOT20\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "HMM\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u957f\u671fMOT\u6027\u80fd\uff0c\u4e14\u5bf9\u8eab\u4efd\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.10132", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10132", "abs": "https://arxiv.org/abs/2509.10132", "authors": ["Nour Jamoussi", "Giuseppe Serra", "Photios A. Stavrou", "Marios Kountouris"], "title": "Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning", "comment": null, "summary": "Bayesian Federated Learning (BFL) combines uncertainty modeling with\ndecentralized training, enabling the development of personalized and reliable\nmodels under data heterogeneity and privacy constraints. Existing approaches\ntypically rely on Markov Chain Monte Carlo (MCMC) sampling or variational\ninference, often incorporating personalization mechanisms to better adapt to\nlocal data distributions. In this work, we propose an information-geometric\nprojection framework for personalization in parametric BFL. By projecting the\nglobal model onto a neighborhood of the user's local model, our method enables\na tunable trade-off between global generalization and local specialization.\nUnder mild assumptions, we show that this projection step is equivalent to\ncomputing a barycenter on the statistical manifold, allowing us to derive\nclosed-form solutions and achieve cost-free personalization. We apply the\nproposed approach to a variational learning setup using the Improved\nVariational Online Newton (IVON) optimizer and extend its application to\ngeneral aggregation schemes in BFL. Empirical evaluations under heterogeneous\ndata distributions confirm that our method effectively balances global and\nlocal performance with minimal computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u51e0\u4f55\u6295\u5f71\u7684\u8d1d\u53f6\u65af\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5168\u5c40\u4e0e\u5c40\u90e8\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u5f02\u6784\u548c\u9690\u79c1\u7ea6\u675f\u4e0b\u4e2a\u6027\u5316\u6a21\u578b\u7684\u5f00\u53d1\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4fe1\u606f\u51e0\u4f55\u6295\u5f71\u6846\u67b6\uff0c\u5c06\u5168\u5c40\u6a21\u578b\u6295\u5f71\u5230\u7528\u6237\u5c40\u90e8\u6a21\u578b\u90bb\u57df\uff0c\u5b9e\u73b0\u53ef\u8c03\u8c10\u7684\u5168\u5c40\u6cdb\u5316\u4e0e\u5c40\u90e8\u7279\u5316\u6743\u8861\u3002", "result": "\u5728\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e0b\uff0c\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u5168\u5c40\u4e0e\u5c40\u90e8\u6027\u80fd\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d1d\u53f6\u65af\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u4e8b\u4ef6\u76f8\u673a\u4e0e\u4f20\u7edf\u5e27\u6355\u6349\u878d\u5408\u7684\u8fdb\u5c55\uff0c\u91cd\u70b9\u63a2\u8ba8\u4e86\u5176\u5728\u89c6\u9891\u6062\u590d\u548c3D\u91cd\u5efa\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u603b\u7ed3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u65f6\u7a7a\u589e\u5f3a\u65b9\u9762\u7684\u8d21\u732e\u3002", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u56e0\u5176\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u548c\u9ad8\u6355\u83b7\u7387\u7b49\u4f18\u52bf\u8fc5\u901f\u53d1\u5c55\uff0c\u4f46\u5176\u4e0e\u4f20\u7edf\u5e27\u6355\u6349\u7684\u878d\u5408\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u56fe\u50cf/\u89c6\u9891\u589e\u5f3a\u548c\u6062\u590d\u4e2d\u7684\u4e3b\u8981\u8d21\u732e\uff0c\u5206\u4e3a\u65f6\u95f4\u589e\u5f3a\uff08\u5982\u5e27\u63d2\u503c\u548c\u8fd0\u52a8\u53bb\u6a21\u7cca\uff09\u548c\u7a7a\u95f4\u589e\u5f3a\uff08\u5982\u8d85\u5206\u8fa8\u7387\u548c\u4f4e\u5149\u589e\u5f3a\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u4e8b\u4ef6\u9a71\u52a8\u878d\u5408\u57283D\u91cd\u5efa\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u6570\u636e\u96c6\u5217\u8868\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u65e8\u5728\u6fc0\u53d1\u66f4\u591a\u7814\u7a76\uff0c\u63a2\u7d22\u4e8b\u4ef6\u76f8\u673a\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u5728\u89c6\u89c9\u5a92\u4f53\u6062\u590d\u548c\u589e\u5f3a\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2509.10151", "categories": ["cs.LG", "cs.AI", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.10151", "abs": "https://arxiv.org/abs/2509.10151", "authors": ["Riccardo Lunelli", "Angus Nicolson", "Samuel Martin Pr\u00f6ll", "Sebastian Johannes Reinstadler", "Axel Bauer", "Clemens Dlaska"], "title": "BenchECG and xECG: a benchmark and baseline for ECG foundation models", "comment": "32 pages, 4 figures, 22 tables", "summary": "Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to\ndeep learning. Recently, interest has grown in developing foundation models for\nECGs - models that generalise across diverse downstream tasks. However,\nconsistent evaluation has been lacking: prior work often uses narrow task\nselections and inconsistent datasets, hindering fair comparison. Here, we\nintroduce BenchECG, a standardised benchmark comprising a comprehensive suite\nof publicly available ECG datasets and versatile tasks. We also propose xECG,\nan xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,\nwhich achieves the best BenchECG score compared to publicly available\nstate-of-the-art models. In particular, xECG is the only publicly available\nmodel to perform strongly on all datasets and tasks. By standardising\nevaluation, BenchECG enables rigorous comparison and aims to accelerate\nprogress in ECG representation learning. xECG achieves superior performance\nover earlier approaches, defining a new baseline for future ECG foundation\nmodels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86BenchECG\u57fa\u51c6\u548cxECG\u6a21\u578b\uff0c\u7528\u4e8e\u6807\u51c6\u5316ECG\u8868\u793a\u5b66\u4e60\u7684\u8bc4\u4f30\uff0cxECG\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709ECG\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u7f3a\u4e4f\u4e00\u81f4\u6027\uff0c\u963b\u788d\u516c\u5e73\u6bd4\u8f83\uff0c\u9700\u6807\u51c6\u5316\u57fa\u51c6\u3002", "method": "\u63d0\u51faBenchECG\u57fa\u51c6\u548cxECG\u6a21\u578b\uff08\u57fa\u4e8exLSTM\u548cSimDINOv2\u81ea\u76d1\u7763\u5b66\u4e60\uff09\u3002", "result": "xECG\u5728BenchECG\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u6210\u4e3aECG\u57fa\u7840\u6a21\u578b\u7684\u65b0\u57fa\u51c6\u3002", "conclusion": "BenchECG\u548cxECG\u63a8\u52a8\u4e86ECG\u8868\u793a\u5b66\u4e60\u7684\u6807\u51c6\u5316\u548c\u8fdb\u5c55\u3002"}}
{"id": "2509.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09977", "abs": "https://arxiv.org/abs/2509.09977", "authors": ["Siying Liu", "Zikai Wang", "Hanle Zheng", "Yifan Hu", "Xilin Wang", "Qingkai Yang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking", "comment": "15 pages, 8 figures", "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.", "AI": {"tldr": "ISTASTrack\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684ANN-SNN\u6df7\u5408\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7ISTA\u9002\u914d\u5668\u5b9e\u73b0RGB\u548c\u4e8b\u4ef6\u6570\u636e\u7684\u6709\u6548\u878d\u5408\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709ANN\u96be\u4ee5\u5145\u5206\u5229\u7528\u4e8b\u4ef6\u6d41\u7684\u7a00\u758f\u548c\u5f02\u6b65\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u67b6\u6784\u6765\u878d\u5408RGB\u548c\u4e8b\u4ef6\u6570\u636e\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6a21\u578b\uff08\u89c6\u89c9Transformer\u548c\u8109\u51b2Transformer\uff09\uff0c\u5e76\u8bbe\u8ba1ISTA\u9002\u914d\u5668\u8fdb\u884c\u7279\u5f81\u4ea4\u4e92\uff0c\u7ed3\u5408\u65f6\u95f4\u4e0b\u91c7\u6837\u6ce8\u610f\u529b\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2aRGB-Event\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u80fd\u6548\u3002", "conclusion": "ISTASTrack\u5c55\u793a\u4e86\u6df7\u5408ANN-SNN\u8bbe\u8ba1\u5728\u89c6\u89c9\u8ddf\u8e2a\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.10021", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.10021", "abs": "https://arxiv.org/abs/2509.10021", "authors": ["Jonas K\u00fchne", "Christian Vogt", "Michele Magno", "Luca Benini"], "title": "Efficient and Accurate Downfacing Visual Inertial Odometry", "comment": "This article has been accepted for publication in the IEEE Internet\n  of Things Journal (IoT-J)", "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684VIO\uff08\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff09\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u5fae\u578b\u548c\u7eb3\u7c73\u65e0\u4eba\u673a\uff0c\u901a\u8fc7\u4f18\u5316\u548c\u91cf\u5316\u7279\u5f81\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u5728\u4f4e\u529f\u8017SoC\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u9ad8\u7cbe\u5ea6VIO\u6d41\u7a0b\u901a\u5e38\u9700\u8981\u5f3a\u5927\u8ba1\u7b97\u80fd\u529b\uff0c\u800c\u5fae\u578b\u65e0\u4eba\u673a\u7b49\u8bbe\u5907\u8d44\u6e90\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "method": "\u7ed3\u5408SuperPoint\u3001PX4FLOW\u548cORB\u7b49\u5148\u8fdb\u7279\u5f81\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u4f18\u5316\u5e76\u91cf\u5316\u4ee5\u9002\u5e94RISC-V\u4f4e\u529f\u8017SoC\uff0c\u540c\u65f6\u91c7\u7528\u521a\u4f53\u8fd0\u52a8\u6a21\u578b\u51cf\u5c11\u8bef\u5dee\u3002", "result": "\u5728GAP9\u4f4e\u529f\u8017SoC\u4e0a\uff0c\u4f18\u5316\u540e\u7684\u6d41\u7a0b\u6bd4\u57fa\u7ebf\u6d41\u7a0b\u7684RMSE\u5e73\u5747\u964d\u4f4e3.65\u500d\uff1bPX4FLOW\u5728\u4f4e\u901f\u8fd0\u52a8\u4e0b\u4e0eORB\u7cbe\u5ea6\u76f8\u5f53\u4f46\u8fd0\u884c\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u586b\u8865\u4e86\u9ad8\u7cbe\u5ea6VIO\u4e0e\u8f7b\u91cf\u7ea7\u5b9e\u73b0\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10161", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.10161", "abs": "https://arxiv.org/abs/2509.10161", "authors": ["Shiwei Li", "Qunwei Li", "Haozhao Wang", "Ruixuan Li", "Jianbin Lin", "Wenliang Zhong"], "title": "FedBiF: Communication-Efficient Federated Learning via Bits Freezing", "comment": "Accepted by TPDS", "summary": "Federated learning (FL) is an emerging distributed machine learning paradigm\nthat enables collaborative model training without sharing local data. Despite\nits advantages, FL suffers from substantial communication overhead, which can\naffect training efficiency. Recent efforts have mitigated this issue by\nquantizing model updates to reduce communication costs. However, most existing\nmethods apply quantization only after local training, introducing quantization\nerrors into the trained parameters and potentially degrading model accuracy. In\nthis paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework\nthat directly learns quantized model parameters during local training. In each\ncommunication round, the server first quantizes the model parameters and\ntransmits them to the clients. FedBiF then allows each client to update only a\nsingle bit of the multi-bit parameter representation, freezing the remaining\nbits. This bit-by-bit update strategy reduces each parameter update to one bit\nwhile maintaining high precision in parameter representation. Extensive\nexperiments are conducted on five widely used datasets under both IID and\nNon-IID settings. The results demonstrate that FedBiF not only achieves\nsuperior communication compression but also promotes sparsity in the resulting\nmodels. Notably, FedBiF attains accuracy comparable to FedAvg, even when using\nonly 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.\nThe code is available at https://github.com/Leopold1423/fedbif-tpds25.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedBiF\u7684\u65b0\u578b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u672c\u5730\u8bad\u7ec3\u671f\u95f4\u76f4\u63a5\u5b66\u4e60\u91cf\u5316\u6a21\u578b\u53c2\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u901a\u4fe1\u5f00\u9500\u5e76\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u901a\u4fe1\u5f00\u9500\u5927\u548c\u91cf\u5316\u8bef\u5dee\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\u7684\u95ee\u9898\u3002", "method": "\u5728\u6bcf\u8f6e\u901a\u4fe1\u4e2d\uff0c\u670d\u52a1\u5668\u5148\u91cf\u5316\u6a21\u578b\u53c2\u6570\u5e76\u4f20\u8f93\u7ed9\u5ba2\u6237\u7aef\uff0c\u5ba2\u6237\u7aef\u6bcf\u6b21\u4ec5\u66f4\u65b0\u4e00\u4e2a\u6bd4\u7279\u4f4d\uff0c\u51bb\u7ed3\u5176\u4f59\u6bd4\u7279\u4f4d\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86FedBiF\u5728\u901a\u4fe1\u538b\u7f29\u548c\u6a21\u578b\u7a00\u758f\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eFedAvg\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "FedBiF\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u6781\u4f4e\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2509.09988", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2509.09988", "abs": "https://arxiv.org/abs/2509.09988", "authors": ["Yusuke Takagi", "Shunya Nagashima", "Komei Sugiura"], "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction", "comment": "Accepted for presentation at ICONIP2025", "summary": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u592a\u9633\u8000\u6591\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165FLARE\u635f\u5931\u51fd\u6570\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u592a\u9633\u8000\u6591\u9884\u6d4b\u6027\u80fd\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u8000\u6591\u7c7b\u522b\u7684\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u591a\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1FLARE\u635f\u5931\u51fd\u6570\uff08\u9891\u7387\u548c\u5c40\u90e8\u8fb9\u754c\u611f\u77e5\u53ef\u9760\u6027\u635f\u5931\uff09\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "result": "\u5728\u8986\u76d611\u5e74\u592a\u9633\u6d3b\u52a8\u5468\u671f\u7684\u591a\u6ce2\u957f\u592a\u9633\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728Gandin-Murphy-Gerrity\u5206\u6570\u548c\u771f\u5b9e\u6280\u80fd\u7edf\u8ba1\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u592a\u9633\u8000\u6591\u9884\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002"}}
{"id": "2509.10163", "categories": ["cs.LG", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.10163", "abs": "https://arxiv.org/abs/2509.10163", "authors": ["Francisco Javier Esono Nkulu Andong", "Qi Min"], "title": "Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks", "comment": null, "summary": "As sixth-generation (6G) networks move toward ultra-dense, intelligent edge\nenvironments, efficient resource management under stringent privacy, mobility,\nand energy constraints becomes critical. This paper introduces a novel\nFederated Multi-Agent Reinforcement Learning (Fed-MARL) framework that\nincorporates cross-layer orchestration of both the MAC layer and application\nlayer for energy-efficient, privacy-preserving, and real-time resource\nmanagement across heterogeneous edge devices. Each agent uses a Deep Recurrent\nQ-Network (DRQN) to learn decentralized policies for task offloading, spectrum\naccess, and CPU energy adaptation based on local observations (e.g., queue\nlength, energy, CPU usage, and mobility). To protect privacy, we introduce a\nsecure aggregation protocol based on elliptic curve Diffie Hellman key\nexchange, which ensures accurate model updates without exposing raw data to\nsemi-honest adversaries. We formulate the resource management problem as a\npartially observable multi-agent Markov decision process (POMMDP) with a\nmulti-objective reward function that jointly optimizes latency, energy\nefficiency, spectral efficiency, fairness, and reliability under 6G-specific\nservice requirements such as URLLC, eMBB, and mMTC. Simulation results\ndemonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines\nin task success rate, latency, energy efficiency, and fairness, while ensuring\nrobust privacy protection and scalability in dynamic, resource-constrained 6G\nedge networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u90a6\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08Fed-MARL\uff09\uff0c\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u8d44\u6e90\u7ba1\u7406\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u6ee1\u8db3\u8d85\u5bc6\u96c6\u3001\u667a\u80fd\u8fb9\u7f18\u73af\u5883\u4e0b\u7684\u9690\u79c1\u3001\u79fb\u52a8\u6027\u548c\u80fd\u6e90\u7ea6\u675f\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5faa\u73afQ\u7f51\u7edc\uff08DRQN\uff09\u548c\u692d\u5706\u66f2\u7ebfDiffie Hellman\u5bc6\u94a5\u4ea4\u6362\u534f\u8bae\uff0c\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u4efb\u52a1\u5378\u8f7d\u3001\u9891\u8c31\u8bbf\u95ee\u548cCPU\u80fd\u6e90\u9002\u5e94\u3002", "result": "Fed-MARL\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u5ef6\u8fdf\u3001\u80fd\u6e90\u6548\u7387\u548c\u516c\u5e73\u6027\u4e0a\u4f18\u4e8e\u96c6\u4e2d\u5f0fMARL\u548c\u542f\u53d1\u5f0f\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Fed-MARL\u5728\u52a8\u6001\u3001\u8d44\u6e90\u53d7\u9650\u76846G\u8fb9\u7f18\u7f51\u7edc\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u987e\u9690\u79c1\u4fdd\u62a4\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2509.10005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10005", "abs": "https://arxiv.org/abs/2509.10005", "authors": ["Xiaodong Guo", "Tong Liu", "Yike Li", "Zi'ang Lin", "Zhihong Deng"], "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion", "comment": null, "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.", "AI": {"tldr": "TUNI\u662f\u4e00\u79cdRGB-\u70ed\u6210\u50cf\u8bed\u4e49\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u6a21\u5757\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u548c\u5b9e\u65f6\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728RGB\u548c\u70ed\u6210\u50cf\u7279\u5f81\u63d0\u53d6\u53ca\u8de8\u6a21\u6001\u878d\u5408\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5197\u4f59\u7684\u7f16\u7801\u5668\u5f71\u54cd\u5b9e\u65f6\u6548\u7387\u3002", "method": "\u63d0\u51faTUNI\uff0c\u91c7\u7528\u7edf\u4e00\u7684RGB-T\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5c40\u90e8\u6a21\u5757\u589e\u5f3a\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\uff0c\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u8fbe27 FPS\u3002", "conclusion": "TUNI\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2509.10164", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.10164", "abs": "https://arxiv.org/abs/2509.10164", "authors": ["Hoshitaro Ohnishi", "Hideo Mukai"], "title": "A Symmetry-Integrated Approach to Surface Code Decoding", "comment": "12 pages, 6 figures", "summary": "Quantum error correction, which utilizes logical qubits that are encoded as\nredundant multiple physical qubits to find and correct errors in physical\nqubits, is indispensable for practical quantum computing. Surface code is\nconsidered to be a promising encoding method with a high error threshold that\nis defined by stabilizer generators. However, previous methods have suffered\nfrom the problem that the decoder acquires solely the error probability\ndistribution because of the non-uniqueness of correct prediction obtained from\nthe input. To circumvent this problem, we propose a technique to reoptimize the\ndecoder model by approximating syndrome measurements with a continuous function\nthat is mathematically interpolated by neural network. We evaluated the\nimprovement in accuracy of a multilayer perceptron based decoder for code\ndistances of 5 and 7 as well as for decoders based on convolutional and\nrecurrent neural networks and transformers for a code distance of 5. In all\ncases, the reoptimized decoder gave better accuracy than the original models,\ndemonstrating the universal effectiveness of the proposed method that is\nindependent of code distance or network architecture. These results suggest\nthat re-framing the problem of surface code decoding into a regression problem\nthat can be tackled by deep learning is a useful strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u75c7\u72b6\u6d4b\u91cf\u6765\u91cd\u65b0\u4f18\u5316\u89e3\u7801\u5668\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u91cf\u5b50\u7ea0\u9519\u89e3\u7801\u5668\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u89e3\u7801\u5668\u56e0\u8f93\u5165\u9884\u6d4b\u4e0d\u552f\u4e00\u800c\u4ec5\u80fd\u83b7\u53d6\u9519\u8bef\u6982\u7387\u5206\u5e03\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u6570\u5b66\u63d2\u503c\u8fde\u7eed\u51fd\u6570\u8fd1\u4f3c\u75c7\u72b6\u6d4b\u91cf\uff0c\u91cd\u65b0\u4f18\u5316\u89e3\u7801\u5668\u6a21\u578b\u3002", "result": "\u5728\u591a\u5c42\u611f\u77e5\u673a\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u4e0a\uff0c\u4f18\u5316\u540e\u7684\u89e3\u7801\u5668\u5728\u7801\u8ddd5\u548c7\u4e0b\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5c06\u8868\u9762\u7801\u89e3\u7801\u95ee\u9898\u8f6c\u5316\u4e3a\u6df1\u5ea6\u5b66\u4e60\u53ef\u89e3\u51b3\u7684\u56de\u5f52\u95ee\u9898\u662f\u4e00\u79cd\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2509.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10006", "abs": "https://arxiv.org/abs/2509.10006", "authors": ["Masaki Akiba", "Shumpei Takezaki", "Daichi Haraguchi", "Seiichi Uchida"], "title": "Few-Part-Shot Font Generation", "comment": "ICDAR 2025 Workshop on Machine Learning", "summary": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u8bbe\u8ba1\u5143\u7d20\u7684\u5c11\u90e8\u5206\u5b57\u4f53\u751f\u6210\u6a21\u578b\uff0c\u4ec5\u9700\u8f93\u5165\u90e8\u5206\u5f62\u72b6\u5373\u53ef\u751f\u6210\u5b8c\u6574\u5b57\u4f53\u3002", "motivation": "\u4f20\u7edf\u5c11\u6837\u672c\u5b57\u4f53\u751f\u6210\u9700\u8981\u5b8c\u6574\u5b57\u7b26\u5f62\u72b6\uff0c\u800c\u65b0\u65b9\u6cd5\u4ec5\u9700\u90e8\u5206\u5f62\u72b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5b57\u4f53\u521b\u5efa\u6548\u7387\u5e76\u63a2\u7d22\u90e8\u5206\u8bbe\u8ba1\u7ec6\u8282\u5bf9\u6574\u4f53\u5b57\u7b26\u7ed3\u6784\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u90e8\u5206\u5f62\u72b6\u8f93\u5165\u7684\u5b57\u4f53\u751f\u6210\u6a21\u578b\u3002", "result": "\u6a21\u578b\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5b57\u4f53\u521b\u5efa\u6548\u7387\uff0c\u8fd8\u63ed\u793a\u4e86\u90e8\u5206\u8bbe\u8ba1\u7ec6\u8282\u5bf9\u5b57\u7b26\u6574\u4f53\u7ed3\u6784\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5b57\u4f53\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u4e14\u6df1\u5165\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.10167", "categories": ["cs.LG", "68T07, 60H30, 34F05"], "pdf": "https://arxiv.org/pdf/2509.10167", "abs": "https://arxiv.org/abs/2509.10167", "authors": ["L\u00e9na\u00efc Chizat"], "title": "The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams", "comment": null, "summary": "We study the gradient-based training of large-depth residual networks\n(ResNets) from standard random initializations. We show that with a diverging\ndepth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,\nthe training dynamics converges to a Neural Mean ODE training dynamics.\nRemarkably, the limit is independent of the scaling of $M$, covering practical\ncases of, say, Transformers, where $M$ (the number of hidden units or attention\nheads per layer) is typically of the order of $D$. For a residual scale\n$\\Theta_D\\big(\\frac{\\alpha}{LM}\\big)$, we obtain the error bound\n$O_D\\big(\\frac{1}{L}+ \\frac{\\alpha}{\\sqrt{LM}}\\big)$ between the model's output\nand its limit after a fixed number gradient of steps, and we verify empirically\nthat this rate is tight. When $\\alpha=\\Theta(1)$, the limit exhibits complete\nfeature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In\ncontrast, we show that $\\alpha \\to \\infty$ yields a \\lazy ODE regime where the\nMean ODE is linearly parameterized. We then focus on the particular case of\nResNets with two-layer perceptron blocks, for which we study how these scalings\ndepend on the embedding dimension $D$. We show that for this model, the only\nresidual scale that leads to complete feature learning is\n$\\Theta\\big(\\frac{\\sqrt{D}}{LM}\\big)$. In this regime, we prove the error bound\n$O\\big(\\frac{1}{L}+ \\frac{\\sqrt{D}}{\\sqrt{LM}}\\big)$ between the ResNet and its\nlimit after a fixed number of gradient steps, which is also empirically tight.\nOur convergence results rely on a novel mathematical perspective on ResNets :\n(i) due to the randomness of the initialization, the forward and backward pass\nthrough the ResNet behave as the stochastic approximation of certain mean ODEs,\nand (ii) by propagation of chaos (that is, asymptotic independence of the\nunits) this behavior is preserved through the training dynamics.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\uff08ResNets\uff09\u5728\u68af\u5ea6\u8bad\u7ec3\u4e2d\u7684\u52a8\u6001\u884c\u4e3a\uff0c\u53d1\u73b0\u5176\u5728\u5927\u6df1\u5ea6\u4e0b\u6536\u655b\u5230\u795e\u7ecf\u5747\u503cODE\u52a8\u6001\uff0c\u4e14\u4e0e\u9690\u85cf\u5c42\u5bbd\u5ea6\u65e0\u5173\u3002", "motivation": "\u63a2\u7d22ResNets\u5728\u5927\u6df1\u5ea6\u548c\u4e0d\u540c\u9690\u85cf\u5c42\u5bbd\u5ea6\u4e0b\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u63ed\u793a\u5176\u6536\u655b\u884c\u4e3a\u548c\u7279\u5f81\u5b66\u4e60\u7684\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7814\u7a76\u4e86ResNets\u5728\u4e0d\u540c\u53c2\u6570\u5c3a\u5ea6\u4e0b\u7684\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u63a8\u5bfc\u4e86\u8bef\u5dee\u754c\u9650\u3002", "result": "\u53d1\u73b0ResNets\u5728\u5927\u6df1\u5ea6\u4e0b\u6536\u655b\u5230\u5747\u503cODE\u52a8\u6001\uff0c\u4e14\u7279\u5b9a\u5c3a\u5ea6\u4e0b\u80fd\u5b9e\u73b0\u5b8c\u5168\u7279\u5f81\u5b66\u4e60\u3002", "conclusion": "ResNets\u7684\u8bad\u7ec3\u52a8\u6001\u53ef\u901a\u8fc7\u5747\u503cODE\u63cf\u8ff0\uff0c\u4e14\u7279\u5b9a\u53c2\u6570\u5c3a\u5ea6\u662f\u5b9e\u73b0\u7279\u5f81\u5b66\u4e60\u7684\u5173\u952e\u3002"}}
{"id": "2509.10186", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10186", "abs": "https://arxiv.org/abs/2509.10186", "authors": ["Benjamin Holzschuh", "Georg Kohl", "Florian Redinger", "Nils Thuerey"], "title": "P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context", "comment": null, "summary": "We present a scalable framework for learning deterministic and probabilistic\nneural surrogates for high-resolution 3D physics simulations. We introduce a\nhybrid CNN-Transformer backbone architecture targeted for 3D physics\nsimulations, which significantly outperforms existing architectures in terms of\nspeed and accuracy. Our proposed network can be pretrained on small patches of\nthe simulation domain, which can be fused to obtain a global solution,\noptionally guided via a fast and scalable sequence-to-sequence model to include\nlong-range dependencies. This setup allows for training large-scale models with\nreduced memory and compute requirements for high-resolution datasets. We\nevaluate our backbone architecture against a large set of baseline methods with\nthe objective to simultaneously learn the dynamics of 14 different types of\nPDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic\nturbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate\nthe versatility of our network by training it as a diffusion model to produce\nprobabilistic samples of highly turbulent 3D channel flows across varying\nReynolds numbers, accurately capturing the underlying flow statistics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u9ad8\u5206\u8fa8\u73873D\u7269\u7406\u6a21\u62df\u7684\u786e\u5b9a\u6027\u548c\u6982\u7387\u6027\u795e\u7ecf\u66ff\u4ee3\u6a21\u578b\uff0c\u7ed3\u5408CNN-Transformer\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5206\u8fa8\u73873D\u7269\u7406\u6a21\u62df\u4e2d\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u6df7\u5408CNN-Transformer\u67b6\u6784\uff0c\u652f\u6301\u5c0f\u533a\u57df\u9884\u8bad\u7ec3\u5e76\u901a\u8fc7\u5e8f\u5217\u6a21\u578b\u878d\u5408\u5168\u5c40\u89e3\uff0c\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "result": "\u572814\u79cd3D PDE\u52a8\u529b\u5b66\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u9ad8\u5206\u8fa8\u7387\uff08512^3\uff09\u6a21\u62df\uff0c\u5e76\u80fd\u751f\u6210\u6e4d\u6d41\u6982\u7387\u6837\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7269\u7406\u6a21\u62df\u4efb\u52a1\u3002"}}
{"id": "2509.10024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10024", "abs": "https://arxiv.org/abs/2509.10024", "authors": ["Danling Cao"], "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images", "comment": "This work was completed during the author's MPhil studies at the\n  University of Manchester", "summary": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6b21\u5316\u591a\u7ea7\u6ce8\u610f\u529b\u7f51\u7edc\uff08MLANet\uff09\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u91ce\u5916\u56fe\u50cf\u91cd\u5efa3D\u4eba\u8138\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u96c6\u548c\u590d\u6742\u73b0\u5b9e\u73af\u5883\u7684\u6311\u6218\uff0c\u4ece2D\u91ce\u5916\u56fe\u50cf\u6062\u590d3D\u4eba\u8138\u6a21\u578b\u4ecd\u5177\u96be\u5ea6\u3002", "method": "\u91c7\u7528\u9884\u8bad\u7ec3\u7684\u5c42\u6b21\u5316\u4e3b\u5e72\u7f51\u7edc\u548c\u591a\u7ea7\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\u548c\u53ef\u5fae\u5206\u6e32\u67d3\u5668\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728AFLW2000-3D\u548cMICC Florence\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283D\u4eba\u8138\u91cd\u5efa\u548c\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MLANet\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u8be6\u7ec6\u7684\u9762\u90e8\u51e0\u4f55\u3001\u7eb9\u7406\u3001\u59ff\u6001\u548c\u5149\u7167\u53c2\u6570\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.10189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10189", "abs": "https://arxiv.org/abs/2509.10189", "authors": ["Zexu Jin"], "title": "Hadamard-Riemannian Optimization for Margin-Variance Ensemble", "comment": null, "summary": "Ensemble learning has been widely recognized as a pivotal technique for\nboosting predictive performance by combining multiple base models.\nNevertheless, conventional margin-based ensemble methods predominantly focus on\nmaximizing the expected margin while neglecting the critical role of margin\nvariance, which inherently restricts the generalization capability of the model\nand heightens its vulnerability to overfitting, particularly in noisy or\nimbalanced datasets. Additionally, the conventional approach of optimizing\nensemble weights within the probability simplex often introduces computational\ninefficiency and scalability challenges, complicating its application to\nlarge-scale problems. To tackle these limitations, this paper introduces a\nnovel ensemble learning framework that explicitly incorporates margin variance\ninto the loss function. Our method jointly optimizes the negative expected\nmargin and its variance, leading to enhanced robustness and improved\ngeneralization performance. Moreover, by reparameterizing the ensemble weights\nonto the unit sphere, we substantially simplify the optimization process and\nimprove computational efficiency. Extensive experiments conducted on multiple\nbenchmark datasets demonstrate that the proposed approach consistently\noutperforms traditional margin-based ensemble techniques, underscoring its\neffectiveness and practical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u96c6\u6210\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u8fb9\u9645\u65b9\u5dee\u6765\u63d0\u5347\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8fb9\u9645\u7684\u96c6\u6210\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8fb9\u9645\u65b9\u5dee\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u4e14\u6613\u8fc7\u62df\u5408\uff0c\u5c24\u5176\u662f\u5728\u566a\u58f0\u6216\u4e0d\u5e73\u8861\u6570\u636e\u4e2d\u3002\u6b64\u5916\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u6548\u7387\u4f4e\u3002", "method": "\u5c06\u8fb9\u9645\u65b9\u5dee\u7eb3\u5165\u635f\u5931\u51fd\u6570\uff0c\u8054\u5408\u4f18\u5316\u8d1f\u671f\u671b\u8fb9\u9645\u53ca\u5176\u65b9\u5dee\uff0c\u5e76\u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u96c6\u6210\u6743\u91cd\u5230\u5355\u4f4d\u7403\u9762\u7b80\u5316\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u8fb9\u9645\u7684\u96c6\u6210\u6280\u672f\u3002", "conclusion": "\u65b0\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u96c6\u6210\u5b66\u4e60\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.10026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10026", "abs": "https://arxiv.org/abs/2509.10026", "authors": ["Jing Huang", "Zhiya Tan", "Shutao Gong", "Fanwei Zeng", "Jianshu Li"], "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA", "comment": "12 Pages, 12 Figures, 2 Tables", "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}", "AI": {"tldr": "LaV-CoT\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u89c6\u89c9\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63a8\u7406\u548c\u5956\u52b1\u4f18\u5316\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u601d\u7ef4\u94fe\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u591a\u6a21\u6001\u63a8\u7406\u652f\u6301\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\uff08\u6587\u672c\u6458\u8981\u3001\u8bed\u8a00\u8bc6\u522b\u3001\u7a7a\u95f4\u5bf9\u8c61\u7ea7\u63cf\u8ff0\u548c\u9010\u6b65\u903b\u8f91\u63a8\u7406\uff09\uff0c\u7ed3\u5408\u6570\u636e\u81ea\u52a8\u751f\u6210\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08SFT\u548cGRPO\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u63d0\u5347\u663e\u8457\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u548c\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "LaV-CoT\u5728\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408\u5de5\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2509.10227", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2509.10227", "abs": "https://arxiv.org/abs/2509.10227", "authors": ["\u00c1ngel Ladr\u00f3n", "Miguel S\u00e1nchez-Dom\u00ednguez", "Javier Rozal\u00e9n", "Fernando R. S\u00e1nchez", "Javier de Vicente", "Lucas Lacasa", "Eusebio Valero", "Gonzalo Rubio"], "title": "A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures", "comment": "29 pages, 15 figures", "summary": "Fatigue life prediction is essential in both the design and operational\nphases of any aircraft, and in this sense safety in the aerospace industry\nrequires early detection of fatigue cracks to prevent in-flight failures.\nRobust and precise fatigue life predictors are thus essential to ensure safety.\nTraditional engineering methods, while reliable, are time consuming and involve\ncomplex workflows, including steps such as conducting several Finite Element\nMethod (FEM) simulations, deriving the expected loading spectrum, and applying\ncycle counting techniques like peak-valley or rainflow counting. These steps\noften require collaboration between multiple teams and tools, added to the\ncomputational time and effort required to achieve fatigue life predictions.\nMachine learning (ML) offers a promising complement to traditional fatigue life\nestimation methods, enabling faster iterations and generalization, providing\nquick estimates that guide decisions alongside conventional simulations.\n  In this paper, we present a ML-based pipeline that aims to estimate the\nfatigue life of different aircraft wing locations given the flight parameters\nof the different missions that the aircraft will be operating throughout its\noperational life. We validate the pipeline in a realistic use case of fatigue\nlife estimation, yielding accurate predictions alongside a thorough statistical\nvalidation and uncertainty quantification. Our pipeline constitutes a\ncomplement to traditional methodologies by reducing the amount of costly\nsimulations and, thereby, lowering the required computational and human\nresources.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u75b2\u52b3\u5bff\u547d\u9884\u6d4b\u65b9\u6cd5\uff0c\u7528\u4e8e\u98de\u673a\u673a\u7ffc\u4e0d\u540c\u90e8\u4f4d\u7684\u5bff\u547d\u4f30\u8ba1\uff0c\u4ee5\u8865\u5145\u4f20\u7edf\u65b9\u6cd5\u5e76\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u75b2\u52b3\u5bff\u547d\u9884\u6d4b\u65b9\u6cd5\u8017\u65f6\u4e14\u590d\u6742\uff0c\u9700\u8981\u591a\u56e2\u961f\u534f\u4f5c\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u4ee5\u52a0\u901f\u9884\u6d4b\u8fc7\u7a0b\u5e76\u63d0\u4f9b\u5feb\u901f\u4f30\u8ba1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6d41\u7a0b\uff0c\u5229\u7528\u98de\u884c\u53c2\u6570\u9884\u6d4b\u98de\u673a\u673a\u7ffc\u4e0d\u540c\u90e8\u4f4d\u7684\u75b2\u52b3\u5bff\u547d\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u9a8c\u8bc1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u771f\u5b9e\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6d41\u7a0b\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u9760\u7684\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u8be5\u673a\u5668\u5b66\u4e60\u6d41\u7a0b\u662f\u4f20\u7edf\u65b9\u6cd5\u7684\u6709\u529b\u8865\u5145\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u548c\u4eba\u529b\u8d44\u6e90\u9700\u6c42\u3002"}}
{"id": "2509.10058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10058", "abs": "https://arxiv.org/abs/2509.10058", "authors": ["Sung-Lin Tsai", "Bo-Lun Huang", "Yu Ting Shen", "Cheng Yu Yeo", "Chiang Tseng", "Bo-Kai Ruan", "Wen-Sheng Lien", "Hong-Han Shuai"], "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation", "comment": "Accepted to ACM Multimedia 2025 (MM '25)", "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89e3\u6790\u6a21\u7cca\u989c\u8272\u63cf\u8ff0\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u989c\u8272\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u989c\u8272\u63cf\u8ff0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u751f\u6210\u56fe\u50cf\u4e0e\u4eba\u7c7b\u610f\u56fe\u4e0d\u7b26\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u53c2\u8003\u56fe\u50cf\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u989c\u8272\u5bf9\u9f50\u3002", "method": "\u5229\u7528LLM\u89e3\u6790\u6a21\u7cca\u989c\u8272\u63d0\u793a\uff0c\u5e76\u5728CIELAB\u989c\u8272\u7a7a\u95f4\u4e2d\u4f18\u5316\u6587\u672c\u5d4c\u5165\uff0c\u76f4\u63a5\u6307\u5bfc\u989c\u8272\u6df7\u5408\u64cd\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u989c\u8272\u5bf9\u9f50\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u6587\u672c\u8bed\u4e49\u4e0e\u89c6\u89c9\u751f\u6210\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u989c\u8272\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2509.10248", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10248", "abs": "https://arxiv.org/abs/2509.10248", "authors": ["Janis Keuper"], "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications", "comment": null, "summary": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u9690\u85cf\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u53d1\u73b0\u7b80\u5355\u653b\u51fb\u5373\u53ef\u663e\u8457\u5f71\u54cd\u8bc4\u5ba1\u7ed3\u679c\uff0c\u4e14LLM\u8bc4\u5ba1\u666e\u904d\u504f\u5411\u63a5\u53d7\u3002", "motivation": "\u63a2\u8ba8LLM\u5728\u79d1\u5b66\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u6f5c\u5728\u64cd\u7eb5\u884c\u4e3a\u53ca\u5176\u5f71\u54cd\u3002", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e862024\u5e74ICLR\u8bba\u6587\u76841k\u6761LLM\u751f\u6210\u8bc4\u5ba1\uff0c\u5206\u6790\u63d0\u793a\u6ce8\u5165\u7684\u6709\u6548\u6027\u548cLLM\u7684\u8bc4\u5ba1\u504f\u89c1\u3002", "result": "\u7b80\u5355\u63d0\u793a\u6ce8\u5165\u6548\u679c\u663e\u8457\uff08\u6700\u9ad8100%\u63a5\u53d7\u7387\uff09\uff0cLLM\u8bc4\u5ba1\u666e\u904d\u504f\u5411\u63a5\u53d7\uff08>95%\uff09\u3002", "conclusion": "\u7ed3\u679c\u5bf9LLM\u5728\u540c\u884c\u8bc4\u5ba1\u4e2d\u7684\u4f7f\u7528\u8ba8\u8bba\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2509.10059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10059", "abs": "https://arxiv.org/abs/2509.10059", "authors": ["Yue Zhou", "Litong Feng", "Mengcheng Lan", "Xue Yang", "Qingyun Li", "Yiping Ke", "Xue Jiang", "Wayne Zhang"], "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration", "comment": "17 pages, 16 figures", "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math", "AI": {"tldr": "AVI-Math\u662f\u9996\u4e2a\u8bc4\u4f30\u65e0\u4eba\u673a\u9065\u611f\u4e2d\u591a\u6a21\u6001\u6570\u5b66\u63a8\u7406\u7684\u57fa\u51c6\uff0c\u6db5\u76d6\u51e0\u4f55\u3001\u903b\u8f91\u548c\u4ee3\u6570\u7b49\u9886\u57df\uff0c\u6d4b\u8bd5\u4e8614\u79cdVLMs\uff0c\u53d1\u73b0\u5176\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u65e0\u4eba\u673a\u9065\u611f\u4e2d\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u672a\u5f97\u5230\u5145\u5206\u6d4b\u8bd5\uff0cAVI-Math\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b3,773\u4e2a\u65e0\u4eba\u673a\u89c6\u89d2\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d66\u4e2a\u6570\u5b66\u4e3b\u9898\u548c20\u4e2a\u8bdd\u9898\uff0c\u5e76\u5bf914\u79cdVLMs\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "\u73b0\u6709VLMs\u5728AVI-Math\u7684\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46Chain-of-Thought\u63d0\u793a\u548c\u5fae\u8c03\u6280\u672f\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86VLMs\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u65e0\u4eba\u673a\u53ef\u4fe1VLMs\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2509.10273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10273", "abs": "https://arxiv.org/abs/2509.10273", "authors": ["Sahil Sethi", "Kai Sundmacher", "Caroline Ganzer"], "title": "Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning", "comment": null, "summary": "Ionic liquids (ILs) have emerged as versatile replacements for traditional\nsolvents because their physicochemical properties can be precisely tailored to\nvarious applications. However, accurately predicting key thermophysical\nproperties remains challenging due to the vast chemical design space and the\nlimited availability of experimental data. In this study, we present a\ndata-driven transfer learning framework that leverages a neural recommender\nsystem (NRS) to enable reliable property prediction for ILs using sparse\nexperimental datasets. The approach involves a two-stage process: first,\npre-training NRS models on COSMO-RS-based simulated data at fixed temperature\nand pressure to learn property-specific structural embeddings for cations and\nanions; and second, fine-tuning simple feedforward neural networks using these\nembeddings with experimental data at varying temperatures and pressures. In\nthis work, five essential IL properties are considered: density, viscosity,\nsurface tension, heat capacity, and melting point. The framework supports both\nwithin-property and cross-property knowledge transfer. Notably, pre-trained\nmodels for density, viscosity, and heat capacity are used to fine-tune models\nfor all five target properties, achieving improved performance by a substantial\nmargin for four of them. The model exhibits robust extrapolation to previously\nunseen ILs. Moreover, the final trained models enable property prediction for\nover 700,000 IL combinations, offering a scalable solution for IL screening in\nprocess design. This work highlights the effectiveness of combining simulated\ndata and transfer learning to overcome sparsity in the experimental data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u63a8\u8350\u7cfb\u7edf\u7684\u6570\u636e\u9a71\u52a8\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u79bb\u5b50\u6db2\u4f53\u7684\u5173\u952e\u70ed\u7269\u7406\u6027\u8d28\uff0c\u89e3\u51b3\u4e86\u5b9e\u9a8c\u6570\u636e\u7a00\u758f\u7684\u95ee\u9898\u3002", "motivation": "\u79bb\u5b50\u6db2\u4f53\uff08ILs\uff09\u56e0\u5176\u53ef\u5b9a\u5236\u7684\u7269\u7406\u5316\u5b66\u6027\u8d28\u6210\u4e3a\u4f20\u7edf\u6eb6\u5242\u7684\u66ff\u4ee3\u54c1\uff0c\u4f46\u7531\u4e8e\u5316\u5b66\u8bbe\u8ba1\u7a7a\u95f4\u5e7f\u9614\u4e14\u5b9e\u9a8c\u6570\u636e\u6709\u9650\uff0c\u51c6\u786e\u9884\u6d4b\u5176\u6027\u8d28\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u5728COSMO-RS\u6a21\u62df\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\u795e\u7ecf\u63a8\u8350\u7cfb\u7edf\u6a21\u578b\uff0c\u5b66\u4e60\u79bb\u5b50\u548c\u9634\u79bb\u5b50\u7684\u7ed3\u6784\u5d4c\u5165\uff1b2\uff09\u7528\u5b9e\u9a8c\u6570\u636e\u5fae\u8c03\u7b80\u5355\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u652f\u6301\u8de8\u6027\u8d28\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u6a21\u578b\u5728\u5bc6\u5ea6\u3001\u7c98\u5ea6\u3001\u8868\u9762\u5f20\u529b\u3001\u70ed\u5bb9\u548c\u7194\u70b9\u4e94\u79cd\u6027\u8d28\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u4e2d\u56db\u79cd\u6027\u8d28\u9884\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u5e76\u80fd\u5916\u63a8\u5230\u672a\u89c1\u8fc7\u7684ILs\u3002", "conclusion": "\u7ed3\u5408\u6a21\u62df\u6570\u636e\u548c\u8fc1\u79fb\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u5b9e\u9a8c\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u4e3a\u79bb\u5b50\u6db2\u4f53\u7b5b\u9009\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10080", "categories": ["cs.CV", "I.2.9; I.4.8"], "pdf": "https://arxiv.org/pdf/2509.10080", "abs": "https://arxiv.org/abs/2509.10080", "authors": ["Minsang Kong", "Myeongjun Kim", "Sang Gu Kang", "Sang Hun Lee"], "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems\n  (under review)", "summary": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.", "AI": {"tldr": "BEVTraj\u662f\u4e00\u79cd\u65b0\u578b\u7684\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u9e1f\u77b0\u56fe\uff08BEV\uff09\u7a7a\u95f4\u4e2d\u4f7f\u7528\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5efa\u5730\u56fe\uff0c\u6027\u80fd\u4e0e\u57fa\u4e8e\u9ad8\u6e05\u5730\u56fe\u7684\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5efa\u9ad8\u6e05\u5730\u56fe\u6216\u5b9e\u65f6\u5c40\u90e8\u5730\u56fe\u6784\u5efa\u6a21\u5757\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u77ac\u65f6\u53d8\u5316\u6216\u53ef\u80fd\u9057\u6f0f\u5173\u952e\u573a\u666f\u7ec6\u8282\u3002", "method": "BEVTraj\u5229\u7528\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u4ece\u5bc6\u96c6BEV\u7279\u5f81\u4e2d\u63d0\u53d6\u4e0a\u4e0b\u6587\uff0c\u5e76\u5f15\u5165\u7a00\u758f\u76ee\u6807\u5019\u9009\u63d0\u8bae\uff08SGCP\uff09\u6a21\u5757\u5b9e\u73b0\u7aef\u5230\u7aef\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBEVTraj\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u9ad8\u6e05\u5730\u56fe\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "BEVTraj\u901a\u8fc7\u6d88\u9664\u5bf9\u9884\u5efa\u5730\u56fe\u7684\u4f9d\u8d56\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10291", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.10291", "abs": "https://arxiv.org/abs/2509.10291", "authors": ["Salih Toprak", "Muge Erel-Ozcevik"], "title": "Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case", "comment": "6 pages, 3 figures, 7th International Conference on Blockchain\n  Computing and Applications (BCCA 2025), \\c{opyright}2025 IEEE", "summary": "In disaster scenarios where conventional energy infrastructure is\ncompromised, secure and traceable energy trading between solar-powered\nhouseholds and mobile charging units becomes a necessity. To ensure the\nintegrity of such transactions over a blockchain network, robust and\nunpredictable nonce generation is vital. This study proposes an SDN-enabled\narchitecture where machine learning regressors are leveraged not for their\naccuracy, but for their potential to generate randomized values suitable as\nnonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN\nallows flexible control over data flows and energy routing policies even in\nfragmented or degraded networks, ensuring adaptive response during emergencies.\nUsing a 9000-sample dataset, we evaluate five AutoML-selected regression models\n- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest\nNeighbors - not by their prediction accuracy, but by their ability to produce\ndiverse and non-deterministic outputs across shuffled data inputs. Randomness\nanalysis reveals that Random Forest and Extra Trees regressors exhibit complete\ndependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and\nLightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and\n99.9%, respectively). These findings highlight that certain machine learning\nmodels, particularly tree-based ensembles, may serve as effective and\nlightweight nonce generators within blockchain-secured, SDN-based energy\ntrading infrastructures resilient to disaster conditions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSDN\u548cAutoML\u7684\u533a\u5757\u94fe\u5b89\u5168\u80fd\u6e90\u4ea4\u6613\u67b6\u6784\uff0c\u7528\u4e8e\u707e\u96be\u573a\u666f\u4e0b\u7684\u592a\u9633\u80fd\u5bb6\u5ead\u4e0e\u79fb\u52a8\u5145\u7535\u5355\u5143\u4e4b\u95f4\u7684\u4ea4\u6613\u3002", "motivation": "\u5728\u707e\u96be\u573a\u666f\u4e2d\uff0c\u4f20\u7edf\u80fd\u6e90\u57fa\u7840\u8bbe\u65bd\u53d7\u635f\uff0c\u9700\u8981\u5b89\u5168\u3001\u53ef\u8ffd\u6eaf\u7684\u80fd\u6e90\u4ea4\u6613\u65b9\u5f0f\u3002\u533a\u5757\u94fe\u7f51\u7edc\u7684\u5b8c\u6574\u6027\u4f9d\u8d56\u4e8e\u5f3a\u5927\u7684\u968f\u673a\u6570\u751f\u6210\u3002", "method": "\u5229\u7528AutoML\u9009\u62e9\u7684\u56de\u5f52\u6a21\u578b\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u68af\u5ea6\u63d0\u5347\u7b49\uff09\u751f\u6210\u968f\u673a\u6570\u4f5c\u4e3a\u533a\u5757\u94fe\u7684\u975e\u5019\u9009\u503c\uff0c\u79f0\u4e3aProof of AutoML\u3002SDN\u7528\u4e8e\u7075\u6d3b\u63a7\u5236\u6570\u636e\u6d41\u548c\u80fd\u6e90\u8def\u7531\u3002", "result": "\u968f\u673a\u68ee\u6797\u548cExtra Trees\u8868\u73b0\u51fa\u5b8c\u5168\u968f\u673a\u6027\uff0c\u5176\u4ed6\u6a21\u578b\uff08\u5982\u68af\u5ea6\u63d0\u5347\u3001K\u8fd1\u90bb\u7b49\uff09\u4e5f\u8868\u73b0\u51fa\u9ad8\u968f\u673a\u6027\uff0897.6%-99.9%\uff09\u3002", "conclusion": "\u6811\u96c6\u6210\u6a21\u578b\u53ef\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u968f\u673a\u6570\u751f\u6210\u5668\uff0c\u9002\u7528\u4e8e\u57fa\u4e8e\u533a\u5757\u94fe\u548cSDN\u7684\u707e\u96be\u5f39\u6027\u80fd\u6e90\u4ea4\u6613\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2509.10093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10093", "abs": "https://arxiv.org/abs/2509.10093", "authors": ["Laura Bragagnolo", "Matteo Terreran", "Leonardo Barcellona", "Stefano Ghidoni"], "title": "Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing", "comment": "ICIAP 2025", "summary": "Multi-human parsing is the task of segmenting human body parts while\nassociating each part to the person it belongs to, combining instance-level and\npart-level information for fine-grained human understanding. In this work, we\ndemonstrate that, while state-of-the-art approaches achieved notable results on\npublic datasets, they struggle considerably in segmenting people with\noverlapping bodies. From the intuition that overlapping people may appear\nseparated from a different point of view, we propose a novel training framework\nexploiting multi-view information to improve multi-human parsing models under\nocclusions. Our method integrates such knowledge during the training process,\nintroducing a novel approach based on weak supervision on human instances and a\nmulti-view consistency loss. Given the lack of suitable datasets in the\nliterature, we propose a semi-automatic annotation strategy to generate human\ninstance segmentation masks from multi-view RGB+D data and 3D human skeletons.\nThe experiments demonstrate that the approach can achieve up to a 4.20\\%\nrelative improvement on human parsing over the baseline model in occlusion\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u89c6\u89d2\u4fe1\u606f\u6539\u8fdb\u591a\u4eba\u4f53\u89e3\u6790\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u9488\u5bf9\u906e\u6321\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4eba\u4f53\u91cd\u53e0\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u591a\u89c6\u89d2\u4fe1\u606f\u53ef\u80fd\u63d0\u4f9b\u5206\u79bb\u7684\u89c6\u89d2\u3002", "method": "\u7ed3\u5408\u5f31\u76d1\u7763\u548c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u635f\u5931\uff0c\u63d0\u51fa\u534a\u81ea\u52a8\u6807\u6ce8\u7b56\u7565\u751f\u6210\u5b9e\u4f8b\u5206\u5272\u63a9\u7801\u3002", "result": "\u5728\u906e\u6321\u573a\u666f\u4e0b\uff0c\u76f8\u5bf9\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e864.20%\u7684\u6027\u80fd\u3002", "conclusion": "\u591a\u89c6\u89d2\u4fe1\u606f\u80fd\u6709\u6548\u63d0\u5347\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u591a\u4eba\u4f53\u89e3\u6790\u6027\u80fd\u3002"}}
{"id": "2509.10303", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10303", "abs": "https://arxiv.org/abs/2509.10303", "authors": ["Jesse van Remmerden", "Zaharah Bukhsh", "Yingqian Zhang"], "title": "Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data", "comment": null, "summary": "The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling\nProblem (FJSP), are canonical combinatorial optimization problems with\nwide-ranging applications in industrial operations. In recent years, many\nonline reinforcement learning (RL) approaches have been proposed to learn\nconstructive heuristics for JSP and FJSP. Although effective, these online RL\nmethods require millions of interactions with simulated environments that may\nnot capture real-world complexities, and their random policy initialization\nleads to poor sample efficiency. To address these limitations, we introduce\nConservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL\nalgorithm that learns effective scheduling policies directly from historical\ndata, eliminating the need for costly online interactions, while maintaining\nthe ability to improve upon suboptimal training data. CDQAC couples a\nquantile-based critic with a delayed policy update, estimating the return\ndistribution of each machine-operation pair rather than selecting pairs\noutright. Our extensive experiments demonstrate CDQAC's remarkable ability to\nlearn from diverse data sources. CDQAC consistently outperforms the original\ndata-generating heuristics and surpasses state-of-the-art offline and online RL\nbaselines. In addition, CDQAC is highly sample efficient, requiring only 10-20\ntraining instances to learn high-quality policies. Surprisingly, we find that\nCDQAC performs better when trained on data generated by a random heuristic than\nwhen trained on higher-quality data from genetic algorithms and priority\ndispatching rules.", "AI": {"tldr": "CDQAC\u662f\u4e00\u79cd\u65b0\u578b\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u76f4\u63a5\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\u8c03\u5ea6\u7b56\u7565\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u5728\u7ebf\u4ea4\u4e92\uff0c\u5e76\u80fd\u6539\u8fdb\u6b21\u4f18\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6a21\u62df\u73af\u5883\u4ea4\u4e92\u548c\u968f\u673a\u7b56\u7565\u521d\u59cb\u5316\u5bfc\u81f4\u7684\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u8bc4\u8bba\u5bb6\u548c\u5ef6\u8fdf\u7b56\u7565\u66f4\u65b0\uff0c\u4f30\u8ba1\u6bcf\u4e2a\u673a\u5668-\u64cd\u4f5c\u5bf9\u7684\u56de\u62a5\u5206\u5e03\u3002", "result": "CDQAC\u5728\u591a\u6837\u6570\u636e\u6e90\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u539f\u59cb\u6570\u636e\u751f\u6210\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u73b0\u6709\u79bb\u7ebf/\u5728\u7ebfRL\u57fa\u7ebf\uff0c\u4e14\u6837\u672c\u6548\u7387\u9ad8\u3002", "conclusion": "CDQAC\u5728\u968f\u673a\u542f\u53d1\u5f0f\u751f\u6210\u7684\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u5b66\u4e60\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2509.10105", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10105", "abs": "https://arxiv.org/abs/2509.10105", "authors": ["Young-rok Cha", "Jeongho Ju", "SunYoung Park", "Jong-Hyeon Lee", "Younghyun Yu", "Youngjune Kim"], "title": "VARCO-VISION-2.0 Technical Report", "comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities", "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.", "AI": {"tldr": "VARCO-VISION-2.0\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u97e9\u82f1\u53cc\u8bed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u591a\u56fe\u50cf\u7406\u89e3\u548c\u5e03\u5c40\u611f\u77e5OCR\uff0c\u901a\u8fc7\u56db\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\u63d0\u5347\u591a\u6a21\u6001\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u66f4\u5f3a\u5927\u7684\u53cc\u8bed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u590d\u6742\u8f93\u5165\uff08\u5982\u6587\u6863\u3001\u56fe\u8868\u548c\u8868\u683c\uff09\u7684\u591a\u56fe\u50cf\u7406\u89e3\uff0c\u5e76\u63d0\u5347\u7a7a\u95f4\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u8bfe\u7a0b\u8bad\u7ec3\u548c\u5185\u5b58\u9ad8\u6548\u6280\u672f\uff0c\u7ed3\u5408\u504f\u597d\u4f18\u5316\u63d0\u5347\u5b89\u5168\u6027\u548c\u8bed\u8a00\u80fd\u529b\u3002", "result": "14B\u6a21\u578b\u5728OpenCompass VLM\u6392\u884c\u699c\u4e0a\u4f4d\u5217\u7b2c8\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u9002\u7528\u4e8e\u8bbe\u5907\u90e8\u7f72\u76841.7B\u8f7b\u91cf\u7248\u3002", "conclusion": "VARCO-VISION-2.0\u63a8\u52a8\u4e86\u53cc\u8bed\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u4f18\u5316\u7248\u672c\u3002"}}
{"id": "2509.10308", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10308", "abs": "https://arxiv.org/abs/2509.10308", "authors": ["Joshua Dimasaka", "Christian Gei\u00df", "Robert Muir-Wood", "Emily So"], "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction", "comment": "Accepted full paper at the 8th International Disaster and Risk\n  Conference, IDRC 2025 | Keywords: weakly supervised, graph deep learning,\n  categorical distribution, physical vulnerability, remote sensing,\n  spatiotemporal disaster risk, transition matrix | The data and code are\n  respectively available at https://doi.org/10.5281/zenodo.16656471 and\n  https://github.com/riskaudit/GraphCSVAE", "summary": "In the aftermath of disasters, many institutions worldwide face challenges in\ncontinually monitoring changes in disaster risk, limiting the ability of key\ndecision-makers to assess progress towards the UN Sendai Framework for Disaster\nRisk Reduction 2015-2030. While numerous efforts have substantially advanced\nthe large-scale modeling of hazard and exposure through Earth observation and\ndata-driven methods, progress remains limited in modeling another equally\nimportant yet challenging element of the risk equation: physical vulnerability.\nTo address this gap, we introduce Graph Categorical Structured Variational\nAutoencoder (GraphCSVAE), a novel probabilistic data-driven framework for\nmodeling physical vulnerability by integrating deep learning, graph\nrepresentation, and categorical probabilistic inference, using time-series\nsatellite-derived datasets and prior expert belief systems. We introduce a\nweakly supervised first-order transition matrix that reflects the changes in\nthe spatiotemporal distribution of physical vulnerability in two\ndisaster-stricken and socioeconomically disadvantaged areas: (1) the\ncyclone-impacted coastal Khurushkul community in Bangladesh and (2) the\nmudslide-affected city of Freetown in Sierra Leone. Our work reveals\npost-disaster regional dynamics in physical vulnerability, offering valuable\ninsights into localized spatiotemporal auditing and sustainable strategies for\npost-disaster risk reduction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGraphCSVAE\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u56fe\u8868\u793a\uff0c\u7528\u4e8e\u5efa\u6a21\u707e\u5bb3\u540e\u7684\u7269\u7406\u8106\u5f31\u6027\uff0c\u586b\u8865\u73b0\u6709\u98ce\u9669\u6a21\u578b\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u707e\u5bb3\u98ce\u9669\u8bc4\u4f30\u6a21\u578b\u5728\u7269\u7406\u8106\u5f31\u6027\u5efa\u6a21\u65b9\u9762\u8fdb\u5c55\u6709\u9650\uff0c\u5f71\u54cd\u51b3\u7b56\u8005\u5bf9\u8054\u5408\u56fd\u300a\u4ed9\u53f0\u6846\u67b6\u300b\u76ee\u6807\u7684\u8bc4\u4f30\u80fd\u529b\u3002", "method": "\u5f15\u5165GraphCSVAE\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u3001\u56fe\u8868\u793a\u548c\u6982\u7387\u63a8\u7406\uff0c\u5229\u7528\u536b\u661f\u6570\u636e\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u6784\u5efa\u65f6\u7a7a\u8106\u5f31\u6027\u6a21\u578b\u3002", "result": "\u5728\u5b5f\u52a0\u62c9\u548c\u585e\u62c9\u5229\u6602\u7684\u6848\u4f8b\u4e2d\uff0c\u6210\u529f\u63ed\u793a\u4e86\u707e\u5bb3\u540e\u7269\u7406\u8106\u5f31\u6027\u7684\u65f6\u7a7a\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u707e\u5bb3\u540e\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u652f\u6301\u53ef\u6301\u7eed\u7684\u51cf\u707e\u7b56\u7565\u5236\u5b9a\u3002"}}
{"id": "2509.10114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10114", "abs": "https://arxiv.org/abs/2509.10114", "authors": ["MohammadAli Hamidi", "Hadi Amirpour", "Luigi Atzori", "Christian Timmerer"], "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss", "comment": null, "summary": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7ed3\u5408MobileNetV3-Small\u548cShuffleNetV2\uff0c\u901a\u8fc7\u7b80\u5355\u5e73\u5747\u8fdb\u884c\u9884\u6d4b\u7ea7\u878d\u5408\uff0c\u4f7f\u7528MSECorrLoss\u635f\u5931\u51fd\u6570\u63d0\u5347\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u8138\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u6355\u6349\u4eba\u8138\u7279\u6709\u7684\u9000\u5316\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u96c6\u6210\u4e24\u4e2a\u7d27\u51d1\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08MobileNetV3-Small\u548cShuffleNetV2\uff09\uff0c\u91c7\u7528\u9884\u6d4b\u7ea7\u878d\u5408\u548cMSECorrLoss\u635f\u5931\u51fd\u6570\u3002", "result": "\u5728VQualA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSRCC\u4e3a0.9829\uff0cPLCC\u4e3a0.9894\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.10324", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10324", "abs": "https://arxiv.org/abs/2509.10324", "authors": ["Myung Jin Kim", "YeongHyeon Park", "Il Dong Yun"], "title": "ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting", "comment": null, "summary": "This paper proposes a simple yet effective convolutional module for long-term\ntime series forecasting. The proposed block, inspired by the Auto-Regressive\nIntegrated Moving Average (ARIMA) model, consists of two convolutional\ncomponents: one for capturing the trend (autoregression) and the other for\nrefining local variations (moving average). Unlike conventional ARIMA, which\nrequires iterative multi-step forecasting, the block directly performs\nmulti-step forecasting, making it easily extendable to multivariate settings.\nExperiments on nine widely used benchmark datasets demonstrate that our method\nARMA achieves competitive accuracy, particularly on datasets exhibiting strong\ntrend variations, while maintaining architectural simplicity. Furthermore,\nanalysis shows that the block inherently encodes absolute positional\ninformation, suggesting its potential as a lightweight replacement for\npositional embeddings in sequential models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u5377\u79ef\u6a21\u5757ARMA\uff0c\u7528\u4e8e\u957f\u671f\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u79fb\u52a8\u5e73\u5747\u7ec4\u4ef6\uff0c\u76f4\u63a5\u5b9e\u73b0\u591a\u6b65\u9884\u6d4b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edfARIMA\u6a21\u578b\u9700\u8981\u8fed\u4ee3\u591a\u6b65\u9884\u6d4b\uff0c\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u591a\u5143\u8bbe\u7f6e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u4e14\u76f4\u63a5\u7684\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u81ea\u56de\u5f52\uff08\u6355\u6349\u8d8b\u52bf\uff09\u548c\u79fb\u52a8\u5e73\u5747\uff08\u7ec6\u5316\u5c40\u90e8\u53d8\u5316\uff09\u7684\u5377\u79ef\u6a21\u5757\uff0c\u76f4\u63a5\u5b9e\u73b0\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u5728\u4e5d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u8d8b\u52bf\u53d8\u5316\u5f3a\u7684\u6570\u636e\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u67b6\u6784\u7b80\u5355\u6027\u3002", "conclusion": "ARMA\u6a21\u5757\u4e0d\u4ec5\u9884\u6d4b\u6548\u679c\u597d\uff0c\u8fd8\u80fd\u7f16\u7801\u7edd\u5bf9\u4f4d\u7f6e\u4fe1\u606f\uff0c\u6709\u671b\u66ff\u4ee3\u5e8f\u5217\u6a21\u578b\u4e2d\u7684\u4f4d\u7f6e\u5d4c\u5165\u3002"}}
{"id": "2509.10122", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10122", "abs": "https://arxiv.org/abs/2509.10122", "authors": ["Zongliang Wu", "Siming Zheng", "Peng-Tao Jiang", "Xin Yuan"], "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution", "comment": null, "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdRealism Controlled One-step Diffusion (RCOD)\u6846\u67b6\uff0c\u7528\u4e8e\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0c\u901a\u8fc7\u6f5c\u5728\u57df\u5206\u7ec4\u7b56\u7565\u548c\u9000\u5316\u611f\u77e5\u91c7\u6837\u7b56\u7565\uff0c\u5e73\u8861\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\u3002", "motivation": "\u4f20\u7edf\u4e00\u6b65\u6269\u6563\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u96be\u4ee5\u5e73\u8861\u4fdd\u771f\u5ea6\u548c\u771f\u5b9e\u611f\uff0c\u7f3a\u4e4f\u7075\u6d3b\u7684\u8c03\u63a7\u673a\u5236\u3002", "method": "\u63d0\u51faRCOD\u6846\u67b6\uff0c\u5305\u62ec\u6f5c\u5728\u57df\u5206\u7ec4\u7b56\u7565\u3001\u9000\u5316\u611f\u77e5\u91c7\u6837\u7b56\u7565\u548c\u89c6\u89c9\u63d0\u793a\u6ce8\u5165\u6a21\u5757\u3002", "result": "RCOD\u5728\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e00\u6b65\u6269\u6563\u65b9\u6cd5\uff0c\u5e76\u5177\u5907\u7075\u6d3b\u7684\u63a8\u7406\u9636\u6bb5\u8c03\u63a7\u80fd\u529b\u3002", "conclusion": "RCOD\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u7684\u63d0\u5347\uff0c\u4e3a\u771f\u5b9e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.10363", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.10363", "abs": "https://arxiv.org/abs/2509.10363", "authors": ["Benjamin David Shaffer", "Brooks Kinch", "Joseph Klobusicky", "M. Ani Hsieh", "Nathaniel Trask"], "title": "Physics-informed sensor coverage through structure preserving machine learning", "comment": null, "summary": "We present a machine learning framework for adaptive source localization in\nwhich agents use a structure-preserving digital twin of a coupled\nhydrodynamic-transport system for real-time trajectory planning and data\nassimilation. The twin is constructed with conditional neural Whitney forms\n(CNWF), coupling the numerical guarantees of finite element exterior calculus\n(FEEC) with transformer-based operator learning. The resulting model preserves\ndiscrete conservation, and adapts in real time to streaming sensor data. It\nemploys a conditional attention mechanism to identify: a reduced Whitney-form\nbasis; reduced integral balance equations; and a source field, each compatible\nwith given sensor measurements. The induced reduced-order environmental model\nretains the stability and consistency of standard finite-element simulation,\nyielding a physically realizable, regular mapping from sensor data to the\nsource field. We propose a staggered scheme that alternates between evaluating\nthe digital twin and applying Lloyd's algorithm to guide sensor placement, with\nanalysis providing conditions for monotone improvement of a coverage\nfunctional. Using the predicted source field as an importance function within\nan optimal-recovery scheme, we demonstrate recovery of point sources under\ncontinuity assumptions, highlighting the role of regularity as a sufficient\ncondition for localization. Experimental comparisons with physics-agnostic\ntransformer architectures show improved accuracy in complex geometries when\nphysical constraints are enforced, indicating that structure preservation\nprovides an effective inductive bias for source identification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u4fdd\u6301\u6570\u5b57\u5b6a\u751f\u7684\u81ea\u9002\u5e94\u6e90\u5b9a\u4f4d\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6761\u4ef6\u795e\u7ecfWhitney\u5f62\u5f0f\u548c\u53d8\u6362\u5668\u7b97\u5b50\u5b66\u4e60\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\u548c\u6570\u636e\u540c\u5316\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6d41\u4f53-\u8f93\u8fd0\u7cfb\u7edf\u4e2d\u6e90\u5b9a\u4f4d\u7684\u5b9e\u65f6\u6027\u548c\u7269\u7406\u7ea6\u675f\u95ee\u9898\uff0c\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u795e\u7ecfWhitney\u5f62\u5f0f\uff08CNWF\uff09\u6784\u5efa\u6570\u5b57\u5b6a\u751f\uff0c\u7ed3\u5408\u6709\u9650\u5143\u5916\u5fae\u79ef\u5206\uff08FEEC\uff09\u548c\u53d8\u6362\u5668\u7b97\u5b50\u5b66\u4e60\uff0c\u4fdd\u7559\u79bb\u6563\u5b88\u6052\u6027\uff0c\u5e76\u901a\u8fc7\u6761\u4ef6\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u517c\u5bb9\u4f20\u611f\u5668\u6570\u636e\u7684\u7b80\u5316\u6a21\u578b\u3002", "result": "\u5728\u590d\u6742\u51e0\u4f55\u4e2d\uff0c\u7ed3\u6784\u4fdd\u6301\u65b9\u6cd5\u6bd4\u7269\u7406\u65e0\u5173\u53d8\u6362\u5668\u67b6\u6784\u66f4\u51c6\u786e\uff0c\u8bc1\u660e\u4e86\u7269\u7406\u7ea6\u675f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u6784\u4fdd\u6301\u4e3a\u6e90\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u6b63\u5219\u5316\u6620\u5c04\u548c\u5b9e\u65f6\u9002\u5e94\u6027\u662f\u5173\u952e\u4f18\u52bf\u3002"}}
{"id": "2509.10134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10134", "abs": "https://arxiv.org/abs/2509.10134", "authors": ["Rini Smita Thakur", "Rajeev Ranjan Dwivedi", "Vinod K Kurmi"], "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment", "comment": "Accepted in BMVC 2025", "summary": "Accurate segmentation of the optic disc and cup is critical for the early\ndiagnosis and management of ocular diseases such as glaucoma. However,\nsegmentation models trained on one dataset often suffer significant performance\ndegradation when applied to target data acquired under different imaging\nprotocols or conditions. To address this challenge, we propose\n\\textbf{Grad-CL}, a novel source-free domain adaptation framework that\nleverages a pre-trained source model and unlabeled target data to robustly\nadapt segmentation performance without requiring access to the original source\ndata. Grad-CL combines a gradient-guided pseudolabel refinement module with a\ncosine similarity-based contrastive learning strategy. In the first stage,\nsalient class-specific features are extracted via a gradient-based mechanism,\nenabling more accurate uncertainty quantification and robust prototype\nestimation for refining noisy pseudolabels. In the second stage, a contrastive\nloss based on cosine similarity is employed to explicitly enforce inter-class\nseparability between the gradient-informed features of the optic cup and disc.\nExtensive experiments on challenging cross-domain fundus imaging datasets\ndemonstrate that Grad-CL outperforms state-of-the-art unsupervised and\nsource-free domain adaptation methods, achieving superior segmentation accuracy\nand improved boundary delineation. Project and code are available at\nhttps://visdomlab.github.io/GCL/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrad-CL\u7684\u65e0\u6e90\u57df\u9002\u5e94\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u89c6\u76d8\u548c\u89c6\u676f\u5206\u5272\u7684\u8de8\u57df\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5206\u5272\u6a21\u578b\u5728\u4e0d\u540c\u6210\u50cf\u534f\u8bae\u6216\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u68af\u5ea6\u5f15\u5bfc\u7684\u4f2a\u6807\u7b7e\u7ec6\u5316\u6a21\u5757\u548c\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u8de8\u57df\u773c\u5e95\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u5272\u7cbe\u5ea6\u548c\u8fb9\u754c\u63cf\u7ed8\u66f4\u4f18\u3002", "conclusion": "Grad-CL\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u6e90\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u3002"}}
{"id": "2509.10367", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10367", "abs": "https://arxiv.org/abs/2509.10367", "authors": ["Tong Chen", "Raghavendra Selvan"], "title": "A Discrepancy-Based Perspective on Dataset Condensation", "comment": "30 pages, 4 tables, 1 figure", "summary": "Given a dataset of finitely many elements $\\mathcal{T} = \\{\\mathbf{x}_i\\}_{i\n= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic\ndataset $\\mathcal{S} = \\{\\tilde{\\mathbf{x}}_j\\}_{j = 1}^M$ which is\nsignificantly smaller ($M \\ll N$) such that a model trained from scratch on\n$\\mathcal{S}$ achieves comparable or even superior generalization performance\nto a model trained on $\\mathcal{T}$. Recent advances in DC reveal a close\nconnection to the problem of approximating the data distribution represented by\n$\\mathcal{T}$ with a reduced set of points. In this work, we present a unified\nframework that encompasses existing DC methods and extend the task-specific\nnotion of DC to a more general and formal definition using notions of\ndiscrepancy, which quantify the distance between probability distribution in\ndifferent regimes. Our framework broadens the objective of DC beyond\ngeneralization, accommodating additional objectives such as robustness,\nprivacy, and other desirable properties.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u6570\u636e\u96c6\u538b\u7f29\uff08DC\uff09\u4efb\u52a1\u4ece\u6cdb\u5316\u6027\u80fd\u6269\u5c55\u5230\u5305\u62ec\u9c81\u68d2\u6027\u3001\u9690\u79c1\u6027\u7b49\u66f4\u591a\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6cdb\u5316\u6027\u80fd\uff0c\u7f3a\u4e4f\u5bf9\u5176\u4ed6\u91cd\u8981\u76ee\u6807\uff08\u5982\u9c81\u68d2\u6027\u3001\u9690\u79c1\u6027\uff09\u7684\u7edf\u4e00\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5dee\u5f02\u5ea6\u91cf\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u91cf\u5316\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u6269\u5c55DC\u7684\u5b9a\u4e49\u3002", "result": "\u6846\u67b6\u80fd\u591f\u6db5\u76d6\u73b0\u6709DC\u65b9\u6cd5\uff0c\u5e76\u652f\u6301\u66f4\u591a\u76ee\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u96c6\u538b\u7f29\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u548c\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2509.10140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10140", "abs": "https://arxiv.org/abs/2509.10140", "authors": ["Yifan Chang", "Jie Qin", "Limeng Qiao", "Xiaofeng Wang", "Zheng Zhu", "Lin Ma", "Xingang Wang"], "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization", "comment": null, "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.", "AI": {"tldr": "VQBridge\u89e3\u51b3\u4e86VQ\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86100%\u7684\u7801\u672c\u4f7f\u7528\u7387\uff0c\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3VQ\u8bad\u7ec3\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff08\u5982\u68af\u5ea6\u7a00\u758f\u3001\u7801\u672c\u66f4\u65b0\u6ede\u540e\u7b49\uff09\uff0c\u63d0\u5347\u7801\u672c\u4f7f\u7528\u7387\u548c\u91cd\u5efa\u6027\u80fd\u3002", "method": "\u63d0\u51faVQBridge\uff0c\u901a\u8fc7\u538b\u7f29-\u5904\u7406-\u6062\u590d\u6d41\u7a0b\u4f18\u5316\u7801\u5411\u91cf\uff0c\u7ed3\u5408\u5b66\u4e60\u9000\u706b\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u73b0\u4e86100%\u7801\u672c\u4f7f\u7528\u7387\uff0c\u91cd\u5efa\u6027\u80fd\u8fbe\u5230SOTA\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u751f\u6210\u6548\u679c\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u7684\u5206\u8bcd\u5668\u5bf9\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u81f3\u5173\u91cd\u8981\uff0cVQBridge\u5177\u6709\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2509.10369", "categories": ["cs.LG", "cs.AI", "eess.SP", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2509.10369", "abs": "https://arxiv.org/abs/2509.10369", "authors": ["Gul Rukh Khattak", "Konstantinos Patlatzoglou", "Joseph Barker", "Libor Pastika", "Boroumand Zeidaabadi", "Ahmed El-Medany", "Hesham Aggour", "Yixiu Liang", "Antonio H. Ribeiro", "Jeffrey Annis", "Antonio Luiz Pinho Ribeiro", "Junbo Ge", "Daniel B. Kramer", "Jonathan W. Waks", "Evan Brittain", "Nicholas Peters", "Fu Siong Ng", "Arunashis Sau"], "title": "Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms", "comment": "Currently under review at npj Digital Medicine", "summary": "Contrastive learning is a widely adopted self-supervised pretraining\nstrategy, yet its dependence on cohort composition remains underexplored. We\npresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation\nmodel and pretrain on four cohorts (n = 5,203,352), from diverse populations\nacross three continents (North America, South America, Asia). We systematically\nassess how cohort demographics, health status, and population diversity\ninfluence the downstream performance for prediction tasks also including two\nadditional cohorts from another continent (Europe). We find that downstream\nperformance depends on the distributional properties of the pretraining cohort,\nincluding demographics and health status. Moreover, while pretraining with a\nmulti-centre, demographically diverse cohort improves in-distribution accuracy,\nit reduces out-of-distribution (OOD) generalisation of our contrastive approach\nby encoding cohort-specific artifacts. To address this, we propose the\nIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency\nduring pretraining and enhances OOD robustness. This work provides important\ninsights for developing clinically fair and generalisable foundation models.", "AI": {"tldr": "CAPE\u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u5fc3\u7535\u56fe\u6570\u636e\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u961f\u5217\u7684\u591a\u6837\u6027\u548c\u5065\u5eb7\u72b6\u6001\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u51faIDB\u7b56\u7565\u63d0\u5347OOD\u9c81\u68d2\u6027\u3002", "motivation": "\u63a2\u7d22\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u4e2d\u961f\u5217\u7ec4\u6210\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u4eba\u53e3\u7edf\u8ba1\u548c\u5065\u5eb7\u72b6\u6001\u7684\u5206\u5e03\u7279\u6027\u3002", "method": "\u4f7f\u7528CAPE\u6a21\u578b\u5728\u56db\u5927\u6d32\u7684\u591a\u6837\u5316\u961f\u5217\u4e2d\u9884\u8bad\u7ec3\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u63d0\u51faIDB\u7b56\u7565\u4f18\u5316OOD\u6027\u80fd\u3002", "result": "\u9884\u8bad\u7ec3\u961f\u5217\u7684\u591a\u6837\u6027\u548c\u5065\u5eb7\u72b6\u6001\u663e\u8457\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\uff0cIDB\u7b56\u7565\u80fd\u63d0\u5347\u6a21\u578b\u7684OOD\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u4e34\u5e8a\u516c\u5e73\u4e14\u901a\u7528\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0cIDB\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u961f\u5217\u7279\u5f02\u6027\u95ee\u9898\u3002"}}
{"id": "2509.10156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10156", "abs": "https://arxiv.org/abs/2509.10156", "authors": ["Goker Erdogan", "Nikhil Parthasarathy", "Catalin Ionescu", "Drew Hudson", "Alexander Lerchner", "Andrew Zisserman", "Mehdi Sajjadi", "Joao Carreira"], "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing", "comment": "ICCV 2025", "summary": "We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.", "AI": {"tldr": "LayerLock\u662f\u4e00\u79cd\u901a\u8fc7\u6e10\u8fdb\u51bb\u7ed3\u5c42\u52a0\u901f\u81ea\u76d1\u7763\u89c6\u89c9\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u8868\u793a\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u89c2\u5bdf\u5230ViT\u5c42\u5728\u8bad\u7ec3\u4e2d\u6309\u6df1\u5ea6\u987a\u5e8f\u6536\u655b\uff0c\u5229\u7528\u8fd9\u4e00\u73b0\u8c61\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u8fdb\u5ea6\u8868\u9010\u6b65\u51bb\u7ed3\u6a21\u578b\u5c42\uff0c\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u3002", "result": "\u57284B\u53c2\u6570\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u975e\u6f5c\u5728\u63a9\u7801\u9884\u6d4b\u65b9\u6cd5\u3002", "conclusion": "LayerLock\u7b80\u5355\u6709\u6548\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u5b66\u4e60\u3002"}}
{"id": "2509.10384", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.10384", "abs": "https://arxiv.org/abs/2509.10384", "authors": ["Jianxin Zhang", "Clayton Scott"], "title": "Flow Straight and Fast in Hilbert Space: Functional Rectified Flow", "comment": null, "summary": "Many generative models originally developed in finite-dimensional Euclidean\nspace have functional generalizations in infinite-dimensional settings.\nHowever, the extension of rectified flow to infinite-dimensional spaces remains\nunexplored. In this work, we establish a rigorous functional formulation of\nrectified flow in an infinite-dimensional Hilbert space. Our approach builds\nupon the superposition principle for continuity equations in an\ninfinite-dimensional space. We further show that this framework extends\nnaturally to functional flow matching and functional probability flow ODEs,\ninterpreting them as nonlinear generalizations of rectified flow. Notably, our\nextension to functional flow matching removes the restrictive measure-theoretic\nassumptions in the existing theory of \\citet{kerrigan2024functional}.\nFurthermore, we demonstrate experimentally that our method achieves superior\nperformance compared to existing functional generative models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u6574\u6d41\u6d41\u7684\u4e25\u683c\u51fd\u6570\u5f62\u5f0f\u5316\uff0c\u6269\u5c55\u4e86\u529f\u80fd\u6d41\u5339\u914d\u548c\u6982\u7387\u6d41ODE\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u4e8e\u73b0\u6709\u529f\u80fd\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e2d\u6574\u6d41\u6d41\u7684\u6269\u5c55\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u6539\u8fdb\u529f\u80fd\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e2d\u8fde\u7eed\u6027\u65b9\u7a0b\u7684\u53e0\u52a0\u539f\u7406\uff0c\u5efa\u7acb\u6574\u6d41\u6d41\u7684\u51fd\u6570\u5f62\u5f0f\u5316\uff0c\u5e76\u6269\u5c55\u5230\u529f\u80fd\u6d41\u5339\u914d\u548c\u6982\u7387\u6d41ODE\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u529f\u80fd\u751f\u6210\u6a21\u578b\uff0c\u5e76\u53bb\u9664\u4e86\u73b0\u6709\u7406\u8bba\u4e2d\u7684\u9650\u5236\u6027\u6d4b\u5ea6\u5047\u8bbe\u3002", "conclusion": "\u672c\u6587\u4e3a\u65e0\u9650\u7ef4\u7a7a\u95f4\u4e2d\u7684\u6574\u6d41\u6d41\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u529f\u80fd\u751f\u6210\u6a21\u578b\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2509.10241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10241", "abs": "https://arxiv.org/abs/2509.10241", "authors": ["Elias De Smijter", "Renaud Detry", "Christophe De Vleeschouwer"], "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints", "comment": "9 pages, 3 figures, to be presented at ASTRA25,", "summary": "We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.", "AI": {"tldr": "\u5bf9\u6bd4\u9690\u5f0f\u548c\u663e\u5f0f\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u5728\u7a7a\u95f43D\u7269\u4f53\u91cd\u5efa\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5916\u89c2\u5d4c\u5165\u867d\u63d0\u5347\u5149\u5ea6\u4fdd\u771f\u5ea6\uff0c\u4f46\u5bf9\u51e0\u4f55\u7cbe\u5ea6\u65e0\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u5916\u89c2\u5d4c\u5165\u5728\u7a7a\u95f4\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u662f\u5426\u540c\u65f6\u63d0\u5347\u51e0\u4f55\u7cbe\u5ea6\u548c\u5149\u5ea6\u4fdd\u771f\u5ea6\u3002", "method": "\u4f7f\u7528SPEED+\u6570\u636e\u96c6\u6bd4\u8f83K-Planes\u3001\u9ad8\u65af\u629b\u5149\u548c\u51f8\u9762\u629b\u5149\u65b9\u6cd5\u3002", "result": "\u5916\u89c2\u5d4c\u5165\u4e3b\u8981\u51cf\u5c11\u663e\u5f0f\u65b9\u6cd5\u6240\u9700\u57fa\u5143\u6570\u91cf\uff0c\u800c\u975e\u63d0\u5347\u51e0\u4f55\u7cbe\u5ea6\uff1b\u51f8\u9762\u629b\u5149\u6bd4\u9ad8\u65af\u629b\u5149\u66f4\u7d27\u51d1\u4e14\u65e0\u6742\u4e71\u3002", "conclusion": "\u5916\u89c2\u5d4c\u5165\u5728\u51e0\u4f55\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u6743\u8861\u91cd\u5efa\u8d28\u91cf\u548c\u8868\u793a\u6548\u7387\u3002"}}
{"id": "2509.10390", "categories": ["cs.LG", "cs.IT", "math.IT", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2509.10390", "abs": "https://arxiv.org/abs/2509.10390", "authors": ["Quan Nguyen", "Adji Bousso Dieng"], "title": "Vendi Information Gain for Active Learning and its Application to Ecology", "comment": null, "summary": "While monitoring biodiversity through camera traps has become an important\nendeavor for ecological research, identifying species in the captured image\ndata remains a major bottleneck due to limited labeling resources. Active\nlearning -- a machine learning paradigm that selects the most informative data\nto label and train a predictive model -- offers a promising solution, but\ntypically focuses on uncertainty in the individual predictions without\nconsidering uncertainty across the entire dataset. We introduce a new active\nlearning policy, Vendi information gain (VIG), that selects images based on\ntheir impact on dataset-wide prediction uncertainty, capturing both\ninformativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG\nachieves impressive predictive accuracy close to full supervision using less\nthan 10% of the labels. It consistently outperforms standard baselines across\nmetrics and batch sizes, collecting more diverse data in the feature space. VIG\nhas broad applicability beyond ecology, and our results highlight its value for\nbiodiversity monitoring in data-limited environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565VIG\uff0c\u901a\u8fc7\u8003\u8651\u6570\u636e\u96c6\u8303\u56f4\u5185\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7269\u79cd\u8bc6\u522b\u7684\u6548\u7387\uff0c\u4ec5\u970010%\u7684\u6807\u7b7e\u5373\u53ef\u63a5\u8fd1\u5168\u76d1\u7763\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u76f8\u673a\u9677\u9631\u56fe\u50cf\u6570\u636e\u4e2d\u7269\u79cd\u8bc6\u522b\u56e0\u6807\u7b7e\u8d44\u6e90\u6709\u9650\u800c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165Vendi\u4fe1\u606f\u589e\u76ca\uff08VIG\uff09\u7b56\u7565\uff0c\u9009\u62e9\u5bf9\u6570\u636e\u96c6\u6574\u4f53\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u6700\u5927\u7684\u56fe\u50cf\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u5728Snapshot Serengeti\u6570\u636e\u96c6\u4e0a\uff0cVIG\u4ec5\u7528\u4e0d\u523010%\u7684\u6807\u7b7e\u5c31\u8fbe\u5230\u4e86\u63a5\u8fd1\u5168\u76d1\u7763\u7684\u51c6\u786e\u7387\uff0c\u4e14\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "VIG\u7b56\u7565\u5728\u6570\u636e\u6709\u9650\u7684\u73af\u5883\u4e2d\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u3002"}}
{"id": "2509.10250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10250", "abs": "https://arxiv.org/abs/2509.10250", "authors": ["Haozhen Yan", "Yan Hong", "Suning Lang", "Jiahui Zhan", "Yikun Ji", "Yujie Gao", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection", "comment": "11 pages, 5 figures", "summary": "With generative models becoming increasingly sophisticated and diverse,\ndetecting AI-generated images has become increasingly challenging. While\nexisting AI-genereted Image detectors achieve promising performance on\nin-distribution generated images, their generalization to unseen generative\nmodels remains limited. This limitation is largely attributed to their reliance\non generation-specific artifacts, such as stylistic priors and compression\npatterns. To address these limitations, we propose GAMMA, a novel training\nframework designed to reduce domain bias and enhance semantic alignment. GAMMA\nintroduces diverse manipulation strategies, such as inpainting-based\nmanipulation and semantics-preserving perturbations, to ensure consistency\nbetween manipulated and authentic content. We employ multi-task supervision\nwith dual segmentation heads and a classification head, enabling pixel-level\nsource attribution across diverse generative domains. In addition, a reverse\ncross-attention mechanism is introduced to allow the segmentation heads to\nguide and correct biased representations in the classification branch. Our\nmethod achieves state-of-the-art generalization performance on the GenImage\nbenchmark, imporving accuracy by 5.8%, but also maintains strong robustness on\nnewly released generative model such as GPT-4o.", "AI": {"tldr": "GAMMA\u6846\u67b6\u901a\u8fc7\u591a\u6837\u5316\u64cd\u4f5c\u7b56\u7565\u548c\u591a\u4efb\u52a1\u76d1\u7763\u63d0\u5347AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u4f9d\u8d56\u751f\u6210\u7279\u5b9a\u4f2a\u5f71\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faGAMMA\u6846\u67b6\uff0c\u5f15\u5165\u591a\u6837\u5316\u64cd\u4f5c\u7b56\u7565\u548c\u591a\u4efb\u52a1\u76d1\u7763\uff0c\u5305\u62ec\u53cc\u5206\u5272\u5934\u548c\u5206\u7c7b\u5934\u3002", "result": "\u5728GenImage\u57fa\u51c6\u4e0a\u63d0\u53475.8%\u51c6\u786e\u7387\uff0c\u5bf9GPT-4o\u7b49\u65b0\u6a21\u578b\u4fdd\u6301\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "GAMMA\u901a\u8fc7\u51cf\u5c11\u9886\u57df\u504f\u5dee\u548c\u589e\u5f3a\u8bed\u4e49\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u5668\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2509.10396", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10396", "abs": "https://arxiv.org/abs/2509.10396", "authors": ["Siyan Zhao", "Mengchen Liu", "Jing Huang", "Miao Liu", "Chenyu Wang", "Bo Liu", "Yuandong Tian", "Guan Pang", "Sean Bell", "Aditya Grover", "Feiyu Chen"], "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language Models", "comment": "preprint; 21 pages", "summary": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIGPO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528dLLMs\u7684inpainting\u80fd\u529b\u6307\u5bfc\u63a2\u7d22\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5e76\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u65b0SOTA\u3002", "motivation": "\u89e3\u51b3LLMs\u4e0e\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u65f6\u7684\u63a2\u7d22\u6311\u6218\uff0c\u5982\u7a00\u758f\u5956\u52b1\u548c\u6837\u672c\u6d6a\u8d39\uff0c\u5229\u7528dLLMs\u7684inpainting\u80fd\u529b\u4f18\u5316\u63a2\u7d22\u8fc7\u7a0b\u3002", "method": "\u5f15\u5165IGPO\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u771f\u5b9e\u63a8\u7406\u75d5\u8ff9\u5f15\u5bfc\u63a2\u7d22\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u68af\u5ea6\u6709\u6548\u6027\u3002", "result": "\u5728GSM8K\u3001Math500\u548cAMC\u4e09\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u65b0SOTA\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "IGPO\u6846\u67b6\u901a\u8fc7inpainting\u80fd\u529b\u6709\u6548\u89e3\u51b3\u4e86\u63a2\u7d22\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86dLLMs\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2509.10257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10257", "abs": "https://arxiv.org/abs/2509.10257", "authors": ["Ema Masterl", "Tina Vipotnik Vesnaver", "\u017diga \u0160piclin"], "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI", "comment": "Accepted at the PIPPI Workshop of MICCAI 2025", "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u80ce\u513f\u8111MRI\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u65b9\u6cd5\uff08NiftyMIC\u3001SVRTK\u3001NeSVoR\uff09\u7684\u6027\u80fd\uff0c\u53d1\u73b0NeSVoR\u5728\u91cd\u5efa\u6210\u529f\u7387\u548c\u8bca\u65ad\u5206\u7c7b\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u80ce\u513f\u8111MRI\u7684\u4f4e\u5206\u8fa8\u7387\u548c\u8fd0\u52a8\u4f2a\u5f71\u9650\u5236\u4e863D\u89e3\u5256\u7ed3\u6784\u7684\u51c6\u786e\u6355\u6349\uff0c\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff08SRR\uff09\u65b9\u6cd5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5176\u5728\u75c5\u7406\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u548c\u4e0b\u6e38\u4efb\u52a1\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u5bf9140\u4f8b\u80ce\u513f\u8111MRI\u626b\u63cf\uff08\u5305\u62ec\u5065\u5eb7\u5bf9\u7167\u548c\u75c5\u7406\u75c5\u4f8b\uff09\u5e94\u7528\u4e86\u4e09\u79cdSRR\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7BoUNTi\u7b97\u6cd5\u5206\u5272\u8111\u7ed3\u6784\u4f53\u79ef\uff0c\u8bc4\u4f30\u4e86\u89c6\u89c9\u8d28\u91cf\u3001\u91cd\u5efa\u6210\u529f\u7387\u3001\u4f53\u79ef\u6d4b\u91cf\u4e00\u81f4\u6027\u548c\u8bca\u65ad\u5206\u7c7b\u6027\u80fd\u3002", "result": "NeSVoR\u7684\u91cd\u5efa\u6210\u529f\u7387\u6700\u9ad8\uff08>90%\uff09\uff0c\u4e14\u8bca\u65ad\u5206\u7c7b\u6027\u80fd\u4e0d\u53d7SRR\u65b9\u6cd5\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u5c3d\u7ba1\u4e0d\u540cSRR\u65b9\u6cd5\u7684\u4f53\u79ef\u4f30\u8ba1\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "NeSVoR\u5728\u80ce\u513f\u8111MRI\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u8bca\u65ad\u6027\u80fd\u5bf9SRR\u65b9\u6cd5\u7684\u9009\u62e9\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.10406", "categories": ["cs.LG", "68W25, 68T50 (primary) 68W40, 68T07 (secondary)", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.10406", "abs": "https://arxiv.org/abs/2509.10406", "authors": ["Rupert Mitchell", "Kristian Kersting"], "title": "Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining", "comment": null, "summary": "We present Multipole Semantic Attention (MuSe), an efficient approximation of\nsoftmax attention that combines semantic clustering with multipole expansions\nfrom computational physics. Our method addresses the quadratic computational\ncomplexity of transformers in the context length by clustering queries and keys\nseparately in their learned representation spaces, enabling a hierarchical\ntwo-stage attention mechanism. Unlike prior clustering approaches that group\nonly keys or use unified clustering, we maintain separate clusterings that\nrespect attention's asymmetric treatment of these spaces. We augment\ncentroid-based (monopole) approximations with dipole corrections that capture\ndirectional variance within clusters, preserving richer information during\ntraining. The method operates as a drop-in replacement for standard attention,\nrequiring only hyperparameter specification without architectural\nmodifications. Our approach achieves $\\mathcal{O}(NCD)$ complexity for acausal\nattention with $C$ clusters and $\\mathcal{O}(NCD \\log N)$ for causal attention.\nOn isolated attention layers, we demonstrate $3\\times$ speedup over CUDNN Flash\nAttention at 8k context length, with relative squared errors below 20%. For\ncausal attention, we develop a hierarchical block decomposition that combines\nexact local computation with efficient long-range approximation. In end-to-end\npretraining of a 30M parameter model on book-length texts with 16k context, we\nachieve 12.2% runtime reduction with only 0.36% loss degradation, establishing\nthe viability of multipole approximations for efficient transformer\npretraining.", "AI": {"tldr": "MuSe\u662f\u4e00\u79cd\u9ad8\u6548\u7684softmax\u6ce8\u610f\u529b\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u805a\u7c7b\u548c\u591a\u6781\u5c55\u5f00\uff0c\u663e\u8457\u964d\u4f4eTransformer\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3Transformer\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6548\u7387\u3002", "method": "\u5206\u522b\u5bf9\u67e5\u8be2\u548c\u952e\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\uff0c\u5f15\u5165\u591a\u6781\u5c55\u5f00\uff08\u5305\u62ec\u5355\u6781\u548c\u5076\u6781\u6821\u6b63\uff09\u4fdd\u7559\u66f4\u4e30\u5bcc\u4fe1\u606f\uff0c\u65e0\u9700\u4fee\u6539\u67b6\u6784\u5373\u53ef\u5b9e\u73b0\u3002", "result": "\u57288k\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u6bd4CUDNN Flash Attention\u5feb3\u500d\uff0c\u8bef\u5dee\u4f4e\u4e8e20%\uff1b\u572816k\u4e0a\u4e0b\u6587\u7684\u7aef\u5230\u7aef\u9884\u8bad\u7ec3\u4e2d\uff0c\u8fd0\u884c\u65f6\u51cf\u5c1112.2%\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d0.36%\u3002", "conclusion": "MuSe\u8bc1\u660e\u4e86\u591a\u6781\u8fd1\u4f3c\u5728\u9ad8\u6548Transformer\u9884\u8bad\u7ec3\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u957f\u6587\u672c\u573a\u666f\u3002"}}
{"id": "2509.10259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10259", "abs": "https://arxiv.org/abs/2509.10259", "authors": ["Hua Yuan", "Jin Yuan", "Yicheng Jiang", "Yao Zhang", "Xin Geng", "Yong Rui"], "title": "Mask Consistency Regularization in Object Removal", "comment": null, "summary": "Object removal, a challenging task within image inpainting, involves\nseamlessly filling the removed region with content that matches the surrounding\ncontext. Despite advancements in diffusion models, current methods still face\ntwo critical challenges. The first is mask hallucination, where the model\ngenerates irrelevant or spurious content inside the masked region, and the\nsecond is mask-shape bias, where the model fills the masked area with an object\nthat mimics the mask's shape rather than surrounding content. To address these\nissues, we propose Mask Consistency Regularization (MCR), a novel training\nstrategy designed specifically for object removal tasks. During training, our\napproach introduces two mask perturbations: dilation and reshape, enforcing\nconsistency between the outputs of these perturbed branches and the original\nmask. The dilated masks help align the model's output with the surrounding\ncontent, while reshaped masks encourage the model to break the mask-shape bias.\nThis combination of strategies enables MCR to produce more robust and\ncontextually coherent inpainting results. Our experiments demonstrate that MCR\nsignificantly reduces hallucinations and mask-shape bias, leading to improved\nperformance in object removal.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMask Consistency Regularization (MCR)\u7684\u65b0\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u50cf\u4fee\u590d\u4e2d\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u4e2d\u7684\u63a9\u7801\u5e7b\u89c9\u548c\u63a9\u7801\u5f62\u72b6\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u7269\u4f53\u79fb\u9664\u4efb\u52a1\u4e2d\u5b58\u5728\u63a9\u7801\u5e7b\u89c9\u548c\u63a9\u7801\u5f62\u72b6\u504f\u5dee\u4e24\u5927\u6311\u6218\uff0c\u5f71\u54cd\u4e86\u4fee\u590d\u7ed3\u679c\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u63a9\u7801\u6270\u52a8\uff08\u6269\u5f20\u548c\u91cd\u5851\uff09\uff0c\u5f3a\u5236\u6a21\u578b\u5728\u8fd9\u4e9b\u6270\u52a8\u5206\u652f\u7684\u8f93\u51fa\u4e0e\u539f\u59cb\u63a9\u7801\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMCR\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u548c\u63a9\u7801\u5f62\u72b6\u504f\u5dee\uff0c\u63d0\u5347\u4e86\u7269\u4f53\u79fb\u9664\u7684\u6027\u80fd\u3002", "conclusion": "MCR\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u80fd\u591f\u751f\u6210\u66f4\u9c81\u68d2\u4e14\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u56fe\u50cf\u4fee\u590d\u7ed3\u679c\u3002"}}
{"id": "2509.10419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10419", "abs": "https://arxiv.org/abs/2509.10419", "authors": ["Francesco Vitale", "Tommaso Zoppi", "Francesco Flammini", "Nicola Mazzocca"], "title": "Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining", "comment": "Accepted to the 6th International Conference on Reliability, Safety,\n  and Security of Railway Systems (RSSRail2025)", "summary": "Ensuring the resilience of computer-based railways is increasingly crucial to\naccount for uncertainties and changes due to the growing complexity and\ncriticality of those systems. Although their software relies on strict\nverification and validation processes following well-established best-practices\nand certification standards, anomalies can still occur at run-time due to\nresidual faults, system and environmental modifications that were unknown at\ndesign-time, or other emergent cyber-threat scenarios. This paper explores\nrun-time control-flow anomaly detection using process mining to enhance the\nresilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European\nTrain Control System Level 2). Process mining allows learning the actual\ncontrol flow of the system from its execution traces, thus enabling run-time\nmonitoring through online conformance checking. In addition, anomaly\nlocalization is performed through unsupervised machine learning to link\nrelevant deviations to critical system components. We test our approach on a\nreference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its\ncapability to detect and localize anomalies with high accuracy, efficiency, and\nexplainability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u8fc7\u7a0b\u6316\u6398\u8fdb\u884c\u8fd0\u884c\u65f6\u63a7\u5236\u6d41\u5f02\u5e38\u68c0\u6d4b\uff0c\u4ee5\u589e\u5f3aERTMS/ETCS L2\u7cfb\u7edf\u7684\u97e7\u6027\u3002", "motivation": "\u94c1\u8def\u7cfb\u7edf\u7684\u590d\u6742\u6027\u548c\u5173\u952e\u6027\u589e\u52a0\uff0c\u9700\u8981\u5e94\u5bf9\u8fd0\u884c\u65f6\u5f02\u5e38\uff0c\u5982\u6b8b\u7559\u6545\u969c\u3001\u672a\u77e5\u4fee\u6539\u6216\u7f51\u7edc\u5a01\u80c1\u3002", "method": "\u4f7f\u7528\u8fc7\u7a0b\u6316\u6398\u4ece\u6267\u884c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u7cfb\u7edf\u5b9e\u9645\u63a7\u5236\u6d41\uff0c\u5e76\u7ed3\u5408\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u8fdb\u884c\u5f02\u5e38\u5b9a\u4f4d\u3002", "result": "\u5728RBC/RBC Handover\u573a\u666f\u4e2d\uff0c\u65b9\u6cd5\u5c55\u793a\u4e86\u9ad8\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u548c\u5b9a\u4f4d\u5f02\u5e38\uff0c\u63d0\u5347\u7cfb\u7edf\u97e7\u6027\u3002"}}
{"id": "2509.10260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10260", "abs": "https://arxiv.org/abs/2509.10260", "authors": ["Jia Wang", "Jie Hu", "Xiaoqi Ma", "Hanghang Ma", "Yanbing Zeng", "Xiaoming Wei"], "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation", "comment": null, "summary": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.", "AI": {"tldr": "MagicMirror\u662f\u4e00\u4e2a\u8bc4\u4f30\u6587\u672c\u751f\u6210\u56fe\u50cf\uff08T2I\uff09\u4e2d\u7269\u7406\u4f2a\u5f71\u7684\u7efc\u5408\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u5f53\u524dT2I\u751f\u6210\u6a21\u578b\u5728\u7269\u7406\u4f2a\u5f71\uff08\u5982\u89e3\u5256\u548c\u7ed3\u6784\u7f3a\u9677\uff09\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "1. \u5efa\u7acb\u4f2a\u5f71\u5206\u7c7b\u6cd5\uff1b2. \u6807\u6ce8340K\u56fe\u50cf\u6570\u636e\u96c6MagicData340K\uff1b3. \u8bad\u7ec3VLM\u6a21\u578bMagicAssessor\uff1b4. \u8bbe\u8ba1GRPO\u4f18\u5316\u7b56\u7565\uff1b5. \u6784\u5efa\u81ea\u52a8\u5316\u57fa\u51c6MagicBench\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\u5373\u4f7f\u662f\u9876\u7ea7\u6a21\u578b\uff08\u5982GPT-image-1\uff09\u4ecd\u5b58\u5728\u663e\u8457\u4f2a\u5f71\u3002", "conclusion": "\u4f2a\u5f71\u51cf\u5c11\u662fT2I\u672a\u6765\u53d1\u5c55\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2509.10439", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.10439", "abs": "https://arxiv.org/abs/2509.10439", "authors": ["Ahmed Khaled", "Satyen Kale", "Arthur Douillard", "Chi Jin", "Rob Fergus", "Manzil Zaheer"], "title": "Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration", "comment": null, "summary": "Modern machine learning often requires training with large batch size,\ndistributed data, and massively parallel compute hardware (like mobile and\nother edge devices or distributed data centers). Communication becomes a major\nbottleneck in such settings but methods like Local Stochastic Gradient Descent\n(Local SGD) show great promise in reducing this additional communication\noverhead. Local SGD consists of three parts: a local optimization process, an\naggregation mechanism, and an outer optimizer that uses the aggregated updates\nfrom the nodes to produce a new model. While there exists an extensive\nliterature on understanding the impact of hyperparameters in the local\noptimization process, the choice of outer optimizer and its hyperparameters is\nless clear. We study the role of the outer optimizer in Local SGD, and prove\nnew convergence guarantees for the algorithm. In particular, we show that\ntuning the outer learning rate allows us to (a) trade off between optimization\nerror and stochastic gradient noise variance, and (b) make up for ill-tuning of\nthe inner learning rate. Our theory suggests that the outer learning rate\nshould sometimes be set to values greater than $1$. We extend our results to\nsettings where we use momentum in the outer optimizer, and we show a similar\nrole for the momentum-adjusted outer learning rate. We also study acceleration\nin the outer optimizer and show that it improves the convergence rate as a\nfunction of the number of communication rounds, improving upon the convergence\nrate of prior algorithms that apply acceleration locally. Finally, we also\nintroduce a novel data-dependent analysis of Local SGD that yields further\ninsights on outer learning rate tuning. We conduct comprehensive experiments\nwith standard language models and various outer optimizers to validate our\ntheory.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Local SGD\u4e2d\u5916\u5c42\u4f18\u5316\u5668\u7684\u4f5c\u7528\uff0c\u8bc1\u660e\u4e86\u65b0\u7684\u6536\u655b\u6027\u4fdd\u8bc1\uff0c\u5e76\u63d0\u51fa\u4e86\u6570\u636e\u4f9d\u8d56\u7684\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u5e38\u9700\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u901a\u4fe1\u6210\u4e3a\u74f6\u9888\u3002Local SGD\u80fd\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u4f46\u5916\u5c42\u4f18\u5316\u5668\u7684\u9009\u62e9\u53ca\u5176\u8d85\u53c2\u6570\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7814\u7a76\u4e86\u5916\u5c42\u5b66\u4e60\u7387\u7684\u4f5c\u7528\uff0c\u5305\u62ec\u5176\u4e0e\u4f18\u5316\u8bef\u5dee\u548c\u68af\u5ea6\u566a\u58f0\u65b9\u5dee\u7684\u6743\u8861\uff0c\u4ee5\u53ca\u52a8\u91cf\u8c03\u6574\u7684\u5f71\u54cd\u3002", "result": "\u7406\u8bba\u8868\u660e\u5916\u5c42\u5b66\u4e60\u7387\u6709\u65f6\u5e94\u5927\u4e8e1\uff0c\u52a8\u91cf\u8c03\u6574\u548c\u52a0\u901f\u80fd\u63d0\u5347\u6536\u655b\u901f\u5ea6\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5916\u5c42\u4f18\u5316\u5668\u7684\u9009\u62e9\u548c\u8c03\u53c2\u5bf9Local SGD\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u6570\u636e\u4f9d\u8d56\u5206\u6790\u63d0\u4f9b\u4e86\u8fdb\u4e00\u6b65\u7684\u8c03\u53c2\u6307\u5bfc\u3002"}}
{"id": "2509.10266", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10266", "abs": "https://arxiv.org/abs/2509.10266", "authors": ["Wenfang Wu", "Tingting Yuan", "Yupeng Li", "Daling Wang", "Xiaoming Fu"], "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion", "comment": null, "summary": "Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.", "AI": {"tldr": "SignClip\u662f\u4e00\u79cd\u65b0\u7684\u624b\u8bed\u7ffb\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u624b\u52bf\u548c\u5507\u90e8\u8fd0\u52a8\u7279\u5f81\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u624b\u52bf\u4fe1\u53f7\uff0c\u5ffd\u89c6\u4e86\u5507\u90e8\u8fd0\u52a8\u7b49\u975e\u624b\u52a8\u7ebf\u7d22\uff0c\u800c\u8fd9\u4e9b\u7ebf\u7d22\u5728\u6d88\u9664\u89c6\u89c9\u76f8\u4f3c\u624b\u52bf\u7684\u6b67\u4e49\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "SignClip\u6846\u67b6\u878d\u5408\u4e86\u624b\u52bf\u548c\u5507\u90e8\u8fd0\u52a8\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u5206\u5c42\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u5bf9\u9f50\u76ee\u6807\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "result": "\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\uff0cSignClip\u5728Gloss-free\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u4e86SpaMo\u6a21\u578b\uff0cBLEU-4\u4ece24.32\u63d0\u5347\u81f324.71\uff0cROUGE\u4ece46.57\u63d0\u5347\u81f348.38\u3002", "conclusion": "SignClip\u901a\u8fc7\u7ed3\u5408\u624b\u52a8\u548c\u975e\u624b\u52a8\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\uff0c\u8bc1\u660e\u4e86\u5507\u90e8\u8fd0\u52a8\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.10278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10278", "abs": "https://arxiv.org/abs/2509.10278", "authors": ["Vidit Vidit", "Pavel Korshunov", "Amir Mohammadi", "Christophe Ecabert", "Ketan Kotwal", "S\u00e9bastien Marcel"], "title": "Detecting Text Manipulation in Images using Vision Language Models", "comment": "Accepted in Synthetic Realities and Biometric Security Workshop\n  BMVC-2025. For paper page see https://www.idiap.ch/paper/textvlmdet/", "summary": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6587\u672c\u64cd\u7eb5\u68c0\u6d4b\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u867d\u63a5\u8fd1\u4f46\u4ecd\u843d\u540e\u4e8e\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-4o\uff09\uff0c\u5e76\u6307\u51fa\u56fe\u50cf\u64cd\u7eb5\u68c0\u6d4b\u4e13\u7528VLMs\u5728\u6587\u672c\u68c0\u6d4b\u4e2d\u5b58\u5728\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u586b\u8865\u6587\u672c\u64cd\u7eb5\u68c0\u6d4b\u5728\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5728\u4e0d\u540c\u6587\u672c\u64cd\u7eb5\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u5f00\u6e90\u548c\u95ed\u6e90VLMs\uff0c\u5e76\u8bc4\u4f30\u56fe\u50cf\u64cd\u7eb5\u68c0\u6d4b\u4e13\u7528VLMs\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u63a5\u8fd1\u4f46\u4e0d\u53ca\u95ed\u6e90\u6a21\u578b\uff1b\u56fe\u50cf\u64cd\u7eb5\u68c0\u6d4b\u4e13\u7528VLMs\u5728\u6587\u672c\u68c0\u6d4b\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "conclusion": "\u5f00\u6e90VLMs\u5728\u6587\u672c\u64cd\u7eb5\u68c0\u6d4b\u4e0a\u6709\u6f5c\u529b\u4f46\u9700\u6539\u8fdb\uff0c\u56fe\u50cf\u64cd\u7eb5\u68c0\u6d4b\u4e13\u7528VLMs\u9700\u89e3\u51b3\u6cdb\u5316\u95ee\u9898\u3002"}}
{"id": "2509.10282", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10282", "abs": "https://arxiv.org/abs/2509.10282", "authors": ["Gang Li", "Tianjiao Chen", "Mingle Zhou", "Min Li", "Delong Han", "Jin Wan"], "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection", "comment": "Page 14, 5 pictures", "summary": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects\nwithout relying on labeled training data, making it especially valuable in\nscenarios constrained by data scarcity, privacy, or high annotation cost.\nHowever, most existing methods focus exclusively on point clouds, neglecting\nthe rich semantic cues available from complementary modalities such as RGB\nimages and texts priors. This paper introduces MCL-AD, a novel framework that\nleverages multimodal collaboration learning across point clouds, RGB images,\nand texts semantics to achieve superior zero-shot 3D anomaly detection.\nSpecifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that\nenhances the intra-modal representation capability and inter-modal\ncollaborative learning by introducing an object-agnostic decoupled text prompt\nand a multimodal contrastive loss. In addition, a collaborative modulation\nmechanism (CMM) is proposed to fully leverage the complementary representations\nof point clouds and RGB images by jointly modulating the RGB image-guided and\npoint cloud-guided branches. Extensive experiments demonstrate that the\nproposed MCL-AD framework achieves state-of-the-art performance in ZS-3D\nanomaly detection.", "AI": {"tldr": "MCL-AD\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u534f\u4f5c\u5b66\u4e60\uff08\u70b9\u4e91\u3001RGB\u56fe\u50cf\u548c\u6587\u672c\u8bed\u4e49\uff09\u5b9e\u73b0\u96f6\u6837\u672c3D\u5f02\u5e38\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4e13\u6ce8\u4e8e\u70b9\u4e91\u6570\u636e\uff0c\u5ffd\u7565\u4e86RGB\u56fe\u50cf\u548c\u6587\u672c\u8bed\u4e49\u7b49\u4e92\u8865\u6a21\u6001\u7684\u4e30\u5bcc\u8bed\u4e49\u7ebf\u7d22\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u63d0\u793a\u5b66\u4e60\u673a\u5236\uff08MPLM\uff09\u548c\u534f\u4f5c\u8c03\u5236\u673a\u5236\uff08CMM\uff09\uff0c\u589e\u5f3a\u6a21\u6001\u5185\u8868\u793a\u80fd\u529b\u548c\u6a21\u6001\u95f4\u534f\u4f5c\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMCL-AD\u5728\u96f6\u6837\u672c3D\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "MCL-AD\u901a\u8fc7\u591a\u6a21\u6001\u534f\u4f5c\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c3D\u5f02\u5e38\u68c0\u6d4b\u7684\u6548\u679c\u3002"}}
{"id": "2509.10298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10298", "abs": "https://arxiv.org/abs/2509.10298", "authors": ["Laith Nayal", "Mahmoud Mousatat", "Bader Rasheed"], "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks", "comment": "8 pages, 2 tables", "summary": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLipschitz\u7ea6\u675f\u7684\u968f\u673a\u6df1\u5ea6\u65b9\u6cd5\uff08DropPath\uff09\uff0c\u901a\u8fc7\u6df1\u5ea6\u4f9d\u8d56\u7684\u4e22\u5f03\u6982\u7387\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u51c6\u786e\u6027\u548c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548cVision Transformers\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u5bf9\u6297\u6270\u52a8\u9ad8\u5ea6\u654f\u611f\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u91c7\u7528Lipschitz\u5f15\u5bfc\u7684\u968f\u673a\u6df1\u5ea6\u65b9\u6cd5\uff0c\u4e22\u5f03\u6982\u7387\u968f\u6df1\u5ea6\u589e\u52a0\uff0c\u4ee5\u63a7\u5236\u7f51\u7edc\u7684\u6709\u6548Lipschitz\u5e38\u6570\u3002", "result": "\u5728CIFAR-10\u548cViT-Tiny\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5e72\u51c0\u6570\u636e\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u5bf9\u6297\u653b\u51fb\uff08FGSM\u3001PGD-20\u3001AutoAttack\uff09\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6df1\u5ea6\u4f9d\u8d56\u7684\u4e22\u5f03\u7b56\u7565\u6709\u6548\u5e73\u8861\u4e86\u9c81\u68d2\u6027\u3001\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.10310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10310", "abs": "https://arxiv.org/abs/2509.10310", "authors": ["Evan Murphy", "Marco Viola", "Vladimir A. Krylov"], "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments", "comment": "Accepted for publication in the Proceedings of the 27th Irish Machine\n  Vision and Image Processing Conference (IMVIP 2025)", "summary": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u56fe\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u786e\u5b9a\u4f4d\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u8857\u9053\u5bb6\u5177\uff0c\u7ed3\u5408\u5730\u7406\u4fe1\u606f\u7cfb\u7edf\u4f18\u5316\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u57ce\u5e02\u73af\u5883\u4e2d\u8857\u9053\u5bb6\u5177\u7684\u7cbe\u786e\u5b9a\u4f4d\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u516c\u5171\u57fa\u7840\u8bbe\u65bd\u7684\u6709\u6548\u76d1\u63a7\u548c\u7ef4\u62a4\u3002", "method": "\u4f7f\u7528\u80fd\u91cf\u56fe\u7f16\u7801\u7a7a\u95f4\u4f4d\u7f6e\u6982\u7387\uff0c\u7ed3\u5408GIS\u4fe1\u606f\uff0c\u91c7\u7528\u968f\u673a\u751f\u6b7b\u4f18\u5316\u7b97\u6cd5\u63a8\u65ad\u6700\u53ef\u80fd\u7684\u8d44\u4ea7\u914d\u7f6e\u3002", "result": "\u5728\u90fd\u67cf\u6797\u5e02\u4e2d\u5fc3\u7684\u5b9e\u9645\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57ce\u5e02\u8d44\u4ea7\u6620\u5c04\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.10312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10312", "abs": "https://arxiv.org/abs/2509.10312", "authors": ["Zhixin Zheng", "Xinyu Wang", "Chang Zou", "Shaobo Wang", "Linfeng Zhang"], "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching", "comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration", "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aClusCa\u7684\u7a7a\u95f4\u805a\u7c7b\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u6269\u6563\u53d8\u6362\u5668\u7684\u8ba1\u7b97\uff0c\u51cf\u5c1190%\u4ee5\u4e0a\u7684token\u6570\u91cf\uff0c\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c\u89c6\u9891\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u4ec5\u5229\u7528\u65f6\u95f4\u7ef4\u5ea6\u76f8\u4f3c\u6027\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u7ef4\u5ea6\u3002", "method": "ClusCa\u901a\u8fc7\u7a7a\u95f4\u805a\u7c7b\u5bf9token\u8fdb\u884c\u5206\u7ec4\uff0c\u6bcf\u7ec4\u4ec5\u8ba1\u7b97\u4e00\u4e2atoken\u5e76\u4f20\u64ad\u5176\u4fe1\u606f\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cClusCa\u5728DiT\u3001FLUX\u548cHunyuanVideo\u4e0a\u5747\u6709\u6548\uff0c\u52a0\u901f\u6bd4\u8fbe4.96\u500d\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "ClusCa\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u6269\u6563\u53d8\u6362\u5668\u52a0\u901f\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2509.10334", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10334", "abs": "https://arxiv.org/abs/2509.10334", "authors": ["Jordan Sassoon", "Michal Szczepanski", "Martyna Poreba"], "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation", "comment": null, "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.", "AI": {"tldr": "I-Segmenter\u662f\u4e00\u79cd\u5168\u6574\u6570\u5316\u7684ViT\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u6280\u672f\u663e\u8457\u964d\u4f4e\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3Vision Transformers\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u9ad8\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6574\u6570\u5316\u64cd\u4f5c\u66ff\u6362\u6d6e\u70b9\u8fd0\u7b97\uff0c\u63d0\u51fa\u03bb-ShiftGELU\u6fc0\u6d3b\u51fd\u6570\uff0c\u79fb\u9664L2\u5f52\u4e00\u5316\u5c42\uff0c\u5e76\u4f7f\u7528\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u3002", "result": "\u6a21\u578b\u5927\u5c0f\u51cf\u5c113.8\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53471.2\u500d\uff0c\u7cbe\u5ea6\u635f\u5931\u5e73\u57475.1%\u3002", "conclusion": "I-Segmenter\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2509.10341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10341", "abs": "https://arxiv.org/abs/2509.10341", "authors": ["Botond Fazekas", "Thomas Pinetz", "Guilherme Aresta", "Taha Emre", "Hrvoje Bogunovic"], "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT", "comment": null, "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.", "AI": {"tldr": "GARD\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684OCT\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6982\u7387\u6a21\u578b\u548c\u4f3d\u9a6c\u566a\u58f0\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53bb\u566a\u6548\u679c\u3002", "motivation": "OCT\u56fe\u50cf\u4e2d\u7684\u6563\u6591\u566a\u58f0\u4f1a\u63a9\u76d6\u7ec6\u8282\uff0c\u5f71\u54cd\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u53bb\u566a\u4e0e\u7ed3\u6784\u4fdd\u7559\u3002", "method": "\u63d0\u51faGARD\u65b9\u6cd5\uff0c\u7ed3\u5408Denoising Diffusion Gamma Model\u548cNoise-Reduced Fidelity Term\uff0c\u5e76\u4f18\u5316\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u5728PSNR\u3001SSIM\u548cMSE\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u548c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e14\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u89e3\u5256\u7ec6\u8282\u3002", "conclusion": "GARD\u5728OCT\u56fe\u50cf\u53bb\u566a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u56fe\u50cf\u3002"}}
{"id": "2509.10344", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10344", "abs": "https://arxiv.org/abs/2509.10344", "authors": ["Yuexi Du", "Lihui Chen", "Nicha C. Dvornek"], "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography", "comment": "Accepted by MICCAI 2025", "summary": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.", "AI": {"tldr": "GLAM\u662f\u4e00\u79cd\u7528\u4e8e\u4e73\u817aX\u5149\u7247\u591a\u89c6\u56fe\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u5b9e\u73b0\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u9f50\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e73\u817aX\u5149\u7247\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5ffd\u7565\u591a\u89c6\u56fe\u5173\u7cfb\uff0c\u5bfc\u81f4\u9884\u6d4b\u6548\u679c\u4e0d\u4f73\uff0cGLAM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "GLAM\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5b66\u4e60\u591a\u89c6\u56fe\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u8bbe\u7f6e\u4e0b\uff0cGLAM\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "GLAM\u901a\u8fc7\u591a\u89c6\u56fe\u5bf9\u9f50\u548c\u51e0\u4f55\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e73\u817aX\u5149\u7247\u5206\u6790\u7684\u6027\u80fd\u3002"}}
{"id": "2509.10345", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10345", "abs": "https://arxiv.org/abs/2509.10345", "authors": ["Georgios Pantazopoulos", "Eda B. \u00d6zyi\u011fit"], "title": "Towards Understanding Visual Grounding in Visual Language Models", "comment": null, "summary": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e2d\u7684\u89c6\u89c9\u63a5\u5730\u80fd\u529b\uff0c\u63a2\u8ba8\u4e86\u5176\u6838\u5fc3\u7ec4\u4ef6\u3001\u5e94\u7528\u3001\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89c6\u89c9\u63a5\u5730\u80fd\u529b\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u6587\u672c\u63cf\u8ff0\u8bc6\u522b\u89c6\u89c9\u8f93\u5165\u4e2d\u7684\u7279\u5b9a\u533a\u57df\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5982\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u3001\u56fe\u50cf\u6216\u89c6\u9891\u7684\u7ec6\u7c92\u5ea6\u95ee\u9898\u56de\u7b54\u7b49\u3002", "method": "\u8bba\u6587\u9996\u5148\u6982\u8ff0\u4e86\u89c6\u89c9\u63a5\u5730\u5728VLMs\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u7136\u540e\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u5f00\u53d1\u63a5\u5730\u6a21\u578b\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u548c\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u8bba\u6587\u63a2\u8ba8\u4e86\u89c6\u89c9\u63a5\u5730\u4e0e\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u548c\u63a8\u7406\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u603b\u7ed3\u4e86\u76f8\u5173\u6311\u6218\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u63a5\u5730\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5f3a\u8c03\u4e86\u5176\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2509.10359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10359", "abs": "https://arxiv.org/abs/2509.10359", "authors": ["Matteo Trippodo", "Federico Becattini", "Lorenzo Seidenari"], "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention", "comment": "Accepted as Regular Paper at ACM Multimedia 2025", "summary": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u7684\u89c6\u89c9\u7ec4\u4ef6\u653b\u51fb\u2014\u2014Attention Attack\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u7834\u574f\u6587\u672c\u63d0\u793a\u4e0e\u89c6\u89c9\u8868\u793a\u7684\u8de8\u6ce8\u610f\u529b\u673a\u5236\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff0c\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u7ec4\u4ef6\u7684\u9488\u5bf9\u6027\u653b\u51fb\u7814\u7a76\u3002", "method": "\u5229\u7528\u81ea\u52a8\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u4f5c\u4e3a\u7f16\u8f91\u63d0\u793a\u7684\u4ee3\u7406\uff0c\u7834\u574f\u56fe\u50cf\u5185\u5bb9\u4e0e\u6587\u672c\u63cf\u8ff0\u7684\u5bf9\u9f50\u3002", "result": "\u5728TEDBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u653b\u51fb\u663e\u8457\u964d\u4f4e\u4e86\u7f16\u8f91\u6027\u80fd\u4e14\u96be\u4ee5\u5bdf\u89c9\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u8bc4\u4f30\u7b56\u7565\u9a8c\u8bc1\u653b\u51fb\u6548\u679c\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2509.10453", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10453", "abs": "https://arxiv.org/abs/2509.10453", "authors": ["Emily Kaczmarek", "Justin Szeto", "Brennan Nichyporuk", "Tal Arbel"], "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets", "comment": null, "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u6539\u8fdb\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u9884\u6d4b\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u6807\u7b7e\u4e0d\u8db3\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u9884\u6d4b\u6a21\u578b\u53d7\u9650\u4e8e\u6570\u636e\u6807\u7b7e\u4e0d\u8db3\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u4ee5\u53ca\u5bf9\u8f93\u5165\u626b\u63cf\u6570\u91cf\u548c\u65f6\u95f4\u95f4\u9694\u7684\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4e09\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u6269\u5c55\u4ee5\u5904\u7406\u53ef\u53d8\u957f\u5ea6\u8f93\u5165\u5e76\u5b66\u4e60\u9c81\u68d2\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u4f7f\u7528\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5728\u4e03\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u516d\u9879\u8868\u73b0\u4f18\u4e8e\u76d1\u7763\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u548c\u8f93\u5165\u56fe\u50cf\u7684\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u516c\u5f00\u3002"}}
{"id": "2509.10366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10366", "abs": "https://arxiv.org/abs/2509.10366", "authors": ["Fabien Allemand", "Attilio Fiandrotti", "Sumanta Chaudhuri", "Alaa Eddine Mazouz"], "title": "Efficient Learned Image Compression Through Knowledge Distillation", "comment": "19 pages, 21 figures", "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u77e5\u8bc6\u84b8\u998f\u964d\u4f4e\u56fe\u50cf\u538b\u7f29\u795e\u7ecf\u7f51\u7edc\u7684\u8d44\u6e90\u9700\u6c42\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u538b\u7f29\u65b9\u6cd5\u6027\u80fd\u4f18\u8d8a\u4f46\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u5c0f\u7f51\u7edc\uff0c\u5229\u7528\u5927\u6a21\u578b\u8f93\u51fa\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u67b6\u6784\u3001\u8d28\u91cf/\u6bd4\u7279\u7387\u6743\u8861\u53ca\u8d44\u6e90\u8282\u7701\u4e0a\u5747\u6709\u6548\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u53ef\u6269\u5c55\u81f3\u56fe\u50cf\u538b\u7f29\u4efb\u52a1\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u591a\u6559\u5e08\u6a21\u578b\u548c\u635f\u5931\u51fd\u6570\u3002"}}
{"id": "2509.10388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10388", "abs": "https://arxiv.org/abs/2509.10388", "authors": ["Zeqing Leo Yuan", "Mani Ramanagopal", "Aswin C. Sankaranarayanan", "Srinivasa G. Narasimhan"], "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition", "comment": null, "summary": "Decomposing an image into its intrinsic photometric factors--shading and\nreflectance--is a long-standing challenge due to the lack of extensive\nground-truth data for real-world scenes. Recent methods rely on synthetic data\nor sparse annotations for limited indoor and even fewer outdoor scenes. We\nintroduce a novel training-free approach for intrinsic image decomposition\nusing only a pair of visible and thermal images. We leverage the principle that\nlight not reflected from an opaque surface is absorbed and detected as heat by\na thermal camera. This allows us to relate the ordinalities between visible and\nthermal image intensities to the ordinalities of shading and reflectance, which\ncan densely self-supervise an optimizing neural network to recover shading and\nreflectance. We perform quantitative evaluations with known reflectance and\nshading under natural and artificial lighting, and qualitative experiments\nacross diverse outdoor scenes. The results demonstrate superior performance\nover recent learning-based models and point toward a scalable path to curating\nreal-world ordinal supervision, previously infeasible via manual labeling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u53ef\u89c1\u5149\u548c\u70ed\u6210\u50cf\u56fe\u50cf\u5bf9\u8fdb\u884c\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u7684\u5e7f\u6cdb\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u672c\u5f81\u56fe\u50cf\u5206\u89e3\u4e00\u76f4\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6216\u7a00\u758f\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\u3002", "method": "\u901a\u8fc7\u53ef\u89c1\u5149\u548c\u70ed\u6210\u50cf\u56fe\u50cf\u7684\u5f3a\u5ea6\u987a\u5e8f\u5173\u7cfb\uff0c\u81ea\u76d1\u7763\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u6062\u590d\u9634\u5f71\u548c\u53cd\u5c04\u7387\u3002", "result": "\u5728\u81ea\u7136\u548c\u4eba\u5de5\u5149\u7167\u4e0b\u8fdb\u884c\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u5b66\u4e60\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u771f\u5b9e\u4e16\u754c\u7684\u6709\u5e8f\u76d1\u7763\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u6807\u6ce8\u7684\u4e0d\u53ef\u884c\u6027\u95ee\u9898\u3002"}}
{"id": "2509.10407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10407", "abs": "https://arxiv.org/abs/2509.10407", "authors": ["Xiem HoangVan", "Dang BuiDinh", "Sang NguyenQuang", "Wen-Hsiao Peng"], "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards", "comment": null, "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u7c7b\u6cd5\u3001\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5e76\u5206\u6790\u4e86\u538b\u7f29\u89c6\u9891\u8d28\u91cf\u589e\u5f3a\uff08CVQE\uff09\u9886\u57df\u7684\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u8c03\u67e5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u7c7b\u3001\u6bd4\u8f83\u5206\u6790\u4e0d\u8db3\uff0c\u4e14\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\u4e0d\u5b8c\u5584\uff0c\u963b\u788d\u4e86CVQE\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u5f15\u5165\u65b0\u7684\u5206\u7c7b\u6cd5\u3001\u63d0\u51fa\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u6027\u80fd\u4e0e\u590d\u6742\u5ea6\u7684\u6743\u8861\u3002", "result": "\u5efa\u7acb\u4e86CVQE\u65b9\u6cd5\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u63d0\u4f9b\u4e86\u516c\u5e73\u8bc4\u4f30\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u5e76\u8bc6\u522b\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3aCVQE\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u57fa\u7840\uff0c\u5e76\u6307\u5bfc\u4e86\u672a\u6765\u6a21\u578b\u9009\u62e9\u548c\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2509.10408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10408", "abs": "https://arxiv.org/abs/2509.10408", "authors": ["Iacopo Curti", "Pierluigi Zama Ramirez", "Alioscia Petrelli", "Luigi Di Stefano"], "title": "Multimodal SAM-adapter for Semantic Segmentation", "comment": null, "summary": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.", "AI": {"tldr": "MM SAM-adapter\u901a\u8fc7\u9002\u914d\u5668\u7f51\u7edc\u5c06\u591a\u6a21\u6001\u7279\u5f81\u878d\u5165SAM\u7684RGB\u7279\u5f81\uff0c\u63d0\u5347\u8bed\u4e49\u5206\u5272\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u591a\u6a21\u6001\u65b9\u6cd5\u53ef\u63d0\u4f9b\u4e92\u8865\u4fe1\u606f\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faMM SAM-adapter\u6846\u67b6\uff0c\u901a\u8fc7\u9002\u914d\u5668\u7f51\u7edc\u878d\u5408\u591a\u6a21\u6001\u7279\u5f81\u4e0eSAM\u7684RGB\u7279\u5f81\u3002", "result": "\u5728DeLiVER\u3001FMB\u548cMUSES\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u5728RGB-hard\u5b50\u96c6\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u591a\u6a21\u6001\u9002\u914d\u80fd\u6709\u6548\u63d0\u5347\u573a\u666f\u7406\u89e3\u7684\u9c81\u68d2\u6027\uff0cMM SAM-adapter\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2509.10441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10441", "abs": "https://arxiv.org/abs/2509.10441", "authors": ["Tao Han", "Wanghan Xu", "Junchao Gong", "Xiaoyu Yue", "Song Guo", "Luping Zhou", "Lei Bai"], "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis", "comment": "Accepted by ICCV 2025", "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.", "AI": {"tldr": "InfGen\u901a\u8fc7\u4e00\u6b65\u751f\u6210\u5668\u4ece\u56fa\u5b9a\u5927\u5c0f\u7684\u6f5c\u5728\u8868\u793a\u89e3\u7801\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5c064K\u56fe\u50cf\u751f\u6210\u65f6\u95f4\u7f29\u77ed\u81f310\u79d2\u4ee5\u4e0b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u4e2d\u8ba1\u7b97\u9700\u6c42\u6025\u5267\u589e\u52a0\u7684\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e00\u6b65\u751f\u6210\u5668\u66ff\u4ee3VAE\u89e3\u7801\u5668\uff0c\u76f4\u63a5\u4ece\u56fa\u5b9a\u6f5c\u5728\u8868\u793a\u751f\u6210\u4efb\u610f\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "result": "InfGen\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5c064K\u56fe\u50cf\u751f\u6210\u65f6\u95f4\u4ece100\u79d2\u4ee5\u4e0a\u7f29\u77ed\u81f310\u79d2\u4ee5\u4e0b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u3002", "conclusion": "InfGen\u4e3a\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u73b0\u6709\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u63a8\u52a8\u6280\u672f\u8fdb\u5165\u4efb\u610f\u9ad8\u5206\u8fa8\u7387\u65f6\u4ee3\u3002"}}
