<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 124]
- [cs.LG](#cs.LG) [Total: 125]
- [cs.RO](#cs.RO) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: PolypSeg-GradCAM是一种结合U-Net和Grad-CAM的可解释深度学习框架，用于结肠镜息肉分割，性能优异且透明。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要癌症之一，息肉是其前兆。手动分割耗时且易出错，现有深度学习方法缺乏可解释性，阻碍临床应用。

Method: 提出PolypSeg-GradCAM框架，结合U-Net和Grad-CAM，在Kvasir-SEG数据集（1000张内镜图像）上训练和评估。

Result: 测试集平均IoU达0.9257，训练/验证集Dice系数>0.96。Grad-CAM可视化显示预测基于临床相关区域。

Conclusion: PolypSeg-GradCAM兼具高精度和可解释性，有助于提升AI辅助结肠镜的可靠性，促进早期结直肠癌预防。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE是一种基于深度学习的远程眼科应用，用于通过视网膜图像自动检测糖尿病视网膜病变，准确率达85.4%。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致成年人视力丧失的主要原因，尤其在资源匮乏地区，亟需高效筛查方案。

Method: 使用多种卷积神经网络（如ResNet-18、EfficientNet-B0和SqueezeNet）开发并评估系统，以平衡准确性和计算效率。

Result: 最终模型分类准确率为85.4%，支持实时筛查，并具备云端扩展性和多用户框架。

Conclusion: AI驱动的远程医疗解决方案有望扩大糖尿病视网膜病变筛查的覆盖范围，尤其在偏远和资源受限地区。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了一种名为SIM的数据内在正则化框架，通过逆映射机制增强表示学习，并进一步优化为ρSIM以降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统正则化技术依赖启发式方法，效果不稳定，SIM旨在提供更通用的解决方案。

Method: 通过重构输入从变换后的输出，减少信息损失并优化梯度流，采用分块特征采样和投影方法降低复杂度。

Result: 在多个任务（如图像分类、少样本学习等）中表现优于基线方法，且与现有正则化方法正交。

Conclusion: SIM是一种模型无关、任务无关的正则化器，能有效提升多种任务的性能，代码已开源。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA是一种基于动量的自适应梯度反转攻击方法，显著提升了大规模批次场景下的多图像重建效果。


<details>
  <summary>Details</summary>
Motivation: 研究在单轮平均梯度（SAG）机制下，如何从批次平均梯度中恢复单个样本的信息。

Method: 提出MAGIA框架，结合组合重缩放和动量混合损失，优化梯度反转攻击。

Result: MAGIA在大型批次场景中显著优于现有方法，实现高保真多图像重建。

Conclusion: MAGIA在无需辅助信息的情况下，高效解决了梯度反转的挑战。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer是一个专门为阿拉伯文档OCR设计的视觉语言模型，通过大规模数据集和微调策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯文档OCR因草书字体、多样化字体、变音符号和从右到左的书写方向而具有挑战性，现有MLLMs在阿拉伯语上表现有限。

Method: 利用合成和真实文档的大规模数据集，采用仅解码器的微调策略对预训练MLLM进行适配。

Result: Baseer在阿拉伯文档OCR上显著优于现有开源和商业解决方案，WER为0.25，达到新SOTA。

Conclusion: 领域特定适配通用MLLMs对形态丰富的语言（如阿拉伯语）的高精度OCR具有重要价值。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种新的深度学习框架，将稀疏的InSAR时间序列数据转换为密集的时空张量，用于地面位移预测。


<details>
  <summary>Details</summary>
Motivation: 稀疏的InSAR数据难以直接用于预测地面位移，需要一种方法将其转换为密集数据以应用计算机视觉技术。

Method: 设计并实现了一种混合CNN-LSTM模型，用于同时学习空间模式和时间依赖性。

Result: 模型在Sentinel-1数据上表现优于基线方法，提供了更准确和空间一致的预测。

Conclusion: 时空深度学习方法在地面位移预测中具有高效性和潜力。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook框架是一种生成多样化数据集的方法，用于评估AI模型对基本概念的理解能力，发现模型在位置信息和几何形状方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 验证AI模型对基本概念（如物体识别、位置信息）的理解能力，为复杂任务奠定基础。

Method: 通过生成大量关于单个概念和语言变体的问题数据集，系统评估模型表现。

Result: 当代模型在物体识别上表现良好，但在位置信息和几何形状方面存在困难，如MobileVLM-V2模型答案不一致。

Conclusion: Scrapbook框架为生成多样化数据集提供了有效工具，有助于系统评估和改进AI模型性能。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 论文通过实证分析揭示了视觉-语言-视觉流程中信息损失的严重性，尤其是描述-生成瓶颈导致的感知和结构信息退化。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流中的广泛应用，量化视觉内容通过文本中介时的信息损失变得重要。

Method: 生成150对图像，通过描述-生成流程，并使用LPIPS、SSIM和颜色距离等指标测量信息保留。

Result: 99.3%的样本表现出显著的感知退化，91.5%的样本显示结构信息损失。

Conclusion: 描述-生成瓶颈是当代多模态系统中一个可测量且一致的局限性。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: AI-driven workflow利用高分辨率卫星影像自动推断屋顶属性，填补小岛屿发展中国家（SIDS）的结构数据空白，提升灾害风险评估能力。


<details>
  <summary>Details</summary>
Motivation: 小岛屿发展中国家（SIDS）缺乏详细建筑结构数据，影响灾害风险评估和城市韧性规划。

Method: 比较地理空间基础模型结合浅层分类器与微调深度学习模型在屋顶分类中的效果，并评估跨区域训练数据的影响。

Result: 最佳模型在屋顶坡度和材料分类中分别达到F1分数0.88和0.83。

Conclusion: 结合本地能力建设，AI和地球观测数据可为SIDS提供高效、基于证据的城市治理新工具。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: VLA-LPAF模块通过2D数据提升VLA模型的多视角适应性，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型因视觉特征视角差异导致的泛化能力受限问题。

Method: 提出轻量级模块VLA-LPAF，利用单视角图像微调并在潜在空间融合多视角观察。

Result: 在CALVIN、LIBERO和自定义基准上分别提升8%、15%和30%的任务成功率。

Conclusion: VLA-LPAF有效提升VLA模型的视角适应性和任务性能。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: URNet是一种用于事件相机立体深度估计的不确定性感知细化网络，通过局部-全局细化模块和KL散度不确定性建模提升性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟的优势，但现有方法在立体深度估计中仍有改进空间。

Method: 提出URNet，包含局部-全局细化模块和基于KL散度的不确定性建模方法。

Result: 在DSEC数据集上，URNet在定性和定量评估中均优于现有方法。

Conclusion: URNet通过结合局部-全局细化和不确定性建模，显著提升了事件相机立体深度估计的性能。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一种新型混合AI框架，用于从多梯度DWI和形态MRI数据中识别周围神经系统，显著提升了神经识别的准确性和空间精度。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和可能的神经受累，但周围神经的成像仍具挑战性。

Method: Visionerves采用两阶段流程：(A)使用深度学习模型自动分割解剖结构，(B)通过符号空间推理进行神经追踪和识别。

Result: 在10名子宫内膜异位症患者中，Visionerves的Dice分数提升高达25%，空间误差降至5毫米以下。

Conclusion: 该方法为子宫内膜异位症相关神经病变的非侵入性诊断提供了新途径。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive是首个专注于巴基斯坦驾驶环境的隐私保护多模态驾驶行为数据集，填补了新兴经济体驾驶行为数据的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多来自发达国家，缺乏新兴经济体的驾驶行为多样性，且驾驶员面部记录侵犯隐私。

Method: 使用定制Android应用收集智能手机惯性、GPS传感器数据与同步的路面视频，记录三种驾驶行为（正常、激进、危险）。

Result: 数据集包含原始、处理和语义层，适用于驾驶行为分类、交通安全分析和ADAS开发。

Conclusion: V-SenseDrive为智能交通解决方案提供了上下文感知的基础，填补了全球驾驶行为数据集的空白。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL是一系列多模态大语言模型，参数从3B到70B，通过创新的领域增强技术实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 开发适用于企业部署的领域增强多模态模型。

Method: 采用多阶段渐进训练和高精度数据合成管道。

Result: 在通用基准测试中表现优异，并在OCR、文档理解、数学推理和逻辑推理任务中取得领先成绩。

Conclusion: Qianfan-VL为开发领域增强多模态模型提供了有效方法，适合多样化企业场景。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow是一种基于ODE的新型去雾框架，通过将大气散射模型转化为ODE，提升真实场景去雾性能，仅需单步推理。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界去雾任务中缺乏配对数据和传统方法难以处理复杂场景的问题。

Method: 提出HazeFlow框架，利用ODE学习最优轨迹，并结合MCBM生成非均匀雾霾数据。

Result: 在多个真实去雾基准数据集上达到最先进性能。

Conclusion: HazeFlow通过ODE和MCBM的结合，显著提升了真实场景去雾的适应性和效果。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 论文提出了一种压缩版的EcoWeedNet，通过结构化通道剪枝、量化感知训练和TensorRT加速，显著减少了模型大小和计算量，同时提高了推理速度，在农业领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 农业边缘设备资源有限，部署深度学习模型困难，因此需要高效的压缩方法。

Method: 采用结构化通道剪枝、量化感知训练（QAT）和NVIDIA TensorRT加速。

Result: 模型大小减少68.5%，计算量减少3.2 GFLOPs，推理速度达184 FPS（FP16），性能优于YOLO11n和YOLO12n。

Conclusion: 压缩后的EcoWeedNet在精度农业中既高效又有效。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出了一种结合模态丢失和对比学习的多模态学习框架，用于处理模态不平衡和缺失问题，在临床数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 医疗诊断中多模态数据融合的需求增加，但现有方法对模态缺失和失衡问题处理不足。

Method: 引入可学习的模态标记和改进的对比学习目标，结合多模态表示。

Result: 在大规模临床数据集上实现最优性能，尤其在单模态场景下表现突出。

Conclusion: 该方法高效、通用，具有实际临床应用潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 本研究评估了9种分割架构在PE分割任务中的表现，发现3D U-Net效果最佳，CNN优于ViT，且预训练可能损害性能。


<details>
  <summary>Details</summary>
Motivation: 系统评估不同分割架构在肺栓塞（PE）分割任务中的性能，以指导未来研究和应用。

Method: 使用490个CTPA扫描数据集，在统一测试框架下评估9种CNN和ViT架构，比较预训练与随机初始化的效果。

Result: 3D U-Net表现最佳，CNN优于ViT，预训练可能降低性能，远端栓塞分割仍具挑战性。

Conclusion: 3D CNN架构（如U-Net）适合PE分割，但需注意预训练的影响和远端栓塞的挑战。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出了一种新颖的图神经网络，用于分离手形配置的静态和动态特征，提高了手语识别精度。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法很少显式建模手形，限制了识别准确性和语言分析。

Method: 结合解剖学启发的图结构和对比学习，处理手形识别中的类间细微差异和时序变化。

Result: 在37个手形类别的识别任务中达到46%的准确率（基线方法为25%）。

Conclusion: 该方法为手语序列中的结构化手形识别建立了首个基准，显著优于现有方法。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 研究探讨了分类任务对OOD检测性能的影响，发现任务选择与ID-OOD标准相关，且OOD检测性能与预测性能不一定一致。


<details>
  <summary>Details</summary>
Motivation: 在胎儿超声中，可靠的OOD检测对深度学习模型的安全部署至关重要，但现有研究多关注不确定性量化方法，而忽略了分类任务本身的影响。

Method: 通过实验比较了八种不确定性量化方法在四个分类任务中的表现。

Result: OOD检测性能显著依赖于任务选择，且最佳任务与ID-OOD标准相关；OOD检测性能与预测性能不一定一致。

Conclusion: 在医学图像分析中，需根据具体下游应用选择任务和不确定性策略。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 论文提出了OrthoLoC数据集和AdHoP技术，用于解决无人机视觉定位问题，并评估了领域偏移和数据分辨率的影响。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的场景下，现有视觉定位方法依赖大型图像数据库或3D模型不切实际，而正交地理数据轻量且易获取，但相关研究较少。

Method: 提出了OrthoLoC数据集，包含16,425张无人机图像，并设计了AdHoP技术优化特征匹配。

Result: AdHoP技术将匹配性能提升95%，定位误差降低63%。

Conclusion: OrthoLoC数据集和AdHoP技术为无人机视觉定位提供了新解决方案，并展示了领域偏移和数据分辨率对定位精度的影响。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出了一种无需训练数据的单图像异常定位方法SSDnet，利用卷积神经网络的归纳偏置，通过自重建实现异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中缺乏训练数据的问题，仅需测试图像即可进行异常定位。

Method: 设计基于块的训练框架，通过掩码、块打乱和小噪声避免恒等映射，使用感知损失捕捉结构信息。

Result: 在MVTec-AD和fabric数据集上分别达到0.99/0.60和0.98/0.67的AUROC/AUPRC，优于现有方法。

Conclusion: SSDnet无需外部数据或标签，在噪声或缺失像素情况下仍保持鲁棒性，代码将开源。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 提出了一种针对低资源语言（如孟加拉语）的视觉-语言模型，通过合成数据和多模态对齐损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中视觉-语言模型因数据稀缺和英语中心预训练导致的语义对齐问题。

Method: 使用LaBSE验证的EN-BN对和合成图像训练，结合MaxViT、mBART-50和轻量级桥接模块，提出三损失目标（PAL+InfoNCE+OT）。

Result: 在Flickr30k-1k和MSCOCO-1k上表现优异，BLEU-4、METEOR和BERTScore-F1显著提升，缩小真实与合成数据差距41%。

Conclusion: 多模态对齐损失有效提升低资源语言的视觉-语言模型性能，减少虚假匹配。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一个轻量级、实时运行的BEV框架，通过蒸馏大型规划导向模型（UniAD）的能力，支持完整的自动驾驶任务，参数减少78%，性能接近原模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型多模态感知规划模型在资源受限环境中的部署问题，提供实时、高效的自动驾驶解决方案。

Method: 采用模型无关的多阶段蒸馏策略，结合特征级、输出级和自适应区域感知监督，将多模态知识转移到轻量级BEV表示。

Result: 在nuScenes数据集上，检测mAP为39.0，运动预测minADE为1.08，碰撞率0.32，运行速度11 FPS。

Conclusion: TinyBEV证明了在资源受限环境下仍可保留全栈驾驶智能，弥合了大型模型与实时部署之间的差距。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 论文提出了一种新的标注策略，将球置于模糊条纹的中心，并标注模糊属性，提升了检测性能。同时提出了BlurBall模型，结合注意力机制，实现了最先进的检测结果。


<details>
  <summary>Details</summary>
Motivation: 运动模糊降低了快速移动物体的清晰度，现有标注方法在球拍运动中存在不对称性和忽略运动线索的问题。

Method: 提出新的标注策略，将球置于模糊条纹中心并标注模糊属性；提出BlurBall模型，结合多帧输入的注意力机制。

Result: 新标注策略提升了检测性能；BlurBall模型实现了最先进的检测结果，并改善了轨迹预测。

Conclusion: 利用模糊信息不仅提高了检测精度，还增强了轨迹预测的可靠性，对实时体育分析有益。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法MVP，通过关键帧检测和运动向量传播，在视频中减少检测器调用，同时保持零样本覆盖能力。


<details>
  <summary>Details</summary>
Motivation: 在视频中逐帧运行大型开放词汇检测器成本高昂，需要一种高效且无需训练的方法来减少检测器调用。

Method: 使用固定间隔的关键帧调用OWLv2检测器，通过压缩域运动向量传播检测结果到中间帧，结合3x3网格聚合和面积增长检查。

Result: 在ILSVRC2015-VID数据集上，MVP达到mAP@0.5=0.609，接近逐帧检测性能，且优于基于跟踪器的方法。

Conclusion: 压缩域传播是一种实用方法，可在减少检测器调用的同时保持视频中的零样本覆盖能力。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 研究探讨了如何通过简单的预处理技术提升现有高动态范围（HDR）光照估计模型的颜色鲁棒性，发现预训练的白平衡网络能显著提高颜色准确性。


<details>
  <summary>Details</summary>
Motivation: 颜色鲁棒性是实现视觉真实感的关键因素，但现有方法常忽略这一点。研究旨在探索简单适应技术是否能提升现有模型的颜色准确性。

Method: 使用包含多样光照颜色的HDR数据集，系统评估多种适应策略，重点测试预训练白平衡网络的效果。

Result: 预训练白平衡网络作为预处理步骤，显著提升了颜色鲁棒性，且无需重新训练光照估计模型。

Conclusion: 预训练白平衡网络是一种通用且高效的方法，可提升现有光照估计模型的颜色准确性。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的支票字段检测框架，结合视觉语言模型和多模态大语言模型，实现零样本检测，降低实际部署门槛。


<details>
  <summary>Details</summary>
Motivation: 支票在金融生态中广泛使用，但易受欺诈，需高效检测机制。传统方法依赖大量标注数据，资源稀缺。

Method: 利用视觉语言模型（VLM）和多模态大语言模型（MLLM），实现零样本检测支票关键字段。

Result: 在110张支票数据集上表现优异，具备强泛化能力，并可生成高质量标注数据。

Conclusion: 该框架为实时目标检测模型提供高质量数据基础，适应机构需求。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 论文评估了ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro在图表理解任务中的表现，发现它们在图表失真或遮挡时性能显著下降，并提出了CHART NOISe数据集以解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试假设图表干净且问题基于事实，而现实中的图表常包含失真和需要复杂推理的问题。

Method: 通过引入CHART NOISe数据集，结合图表失真、遮挡和反向不一致性测试，评估模型表现并提出缓解策略。

Result: 模型在失真或遮挡情况下性能下降，且容易产生幻觉（如数值捏造、趋势误解等）。

Conclusion: CHART NOISe为提升图表理解的鲁棒性和可靠性提供了严格测试基准。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于神经表示的4D-MRI重建方法，替代传统离散排序方法，显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统4D-MRI重建方法在捕捉时间变化性、工作流程复杂性和计算负载方面存在不足。

Method: 通过两个协同网络（SAN和TMN）将运动建模与图像重建结合，实现连续解剖表示和一致变形场。

Result: 新方法在19名志愿者数据上验证，能准确捕捉呼吸模式，处理时间从5小时降至15分钟，单帧推断时间小于1秒。

Conclusion: 该方法在4D放射治疗规划和实时自适应治疗中展现出强大潜力。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 论文评估了五种基于卡尔曼滤波的跟踪方法在快速移动小物体（如壁球）上的性能，发现DeepOCSORT误差最低，但所有方法均存在显著漂移，表明现有方法需改进。


<details>
  <summary>Details</summary>
Motivation: 快速移动小物体的精确跟踪是计算机视觉中的挑战性问题，尤其在体育机器人应用中，轻量且准确的跟踪系统能提升机器人感知和规划能力。

Method: 研究评估了五种基于卡尔曼滤波的跟踪方法（OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT），使用包含10,000帧标注壁球图像的自定义数据集，分析推理速度和更新频率对跟踪精度的影响。

Result: DeepOCSORT平均跟踪误差最低（31.15像素），ByteTrack处理速度最快（26.6ms），但所有方法均表现出显著的空间误差（3-11cm），误差率是标准跟踪基准的3-4倍。

Conclusion: 现有跟踪方法在处理快速移动小物体时存在根本性局限，需开发专门的方法以提升性能。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一种基于运动感知的自适应裁剪模块，用于压缩视频中的高效动作识别，无需训练且兼容多种骨干网络。


<details>
  <summary>Details</summary>
Motivation: 提高视频动作识别的效率和准确性，同时减少计算开销。

Method: 利用H.264视频中的运动向量定位运动密集区域，通过轻量级流程（包括去噪、合并、蒙特卡洛采样和自适应裁剪）生成裁剪区域。

Result: 在UCF101和CoViAR数据集上显著提升准确率或减少计算量，兼容多种骨干网络。

Conclusion: MoCrop在压缩视频中表现出高效性和通用性，适合实时部署。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于码本的自适应特征压缩框架（CAFC-SE），通过向量量化在低比特率条件下保留更多信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低比特率条件下表现不佳，保留冗余细节或学习过度集中的符号分布。

Method: 使用向量量化（VQ）将连续视觉特征映射到离散索引，并通过码本选择性传输到云端。

Result: 在低比特率条件下，CAFC-SE在速率和准确性方面表现优越。

Conclusion: CAFC-SE框架在低比特率条件下更有效，保留了更多信息性视觉模式。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet是一种超轻量级多核U型CNN，专为医学图像分割设计，具有高效的计算性能和更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学图像分割方法在计算资源和准确性上的不足，特别是在资源受限环境中的实时诊断需求。

Method: 设计了多核深度卷积块（MKDC）和多种注意力机制（通道、空间和分组门控注意力）来捕捉多分辨率空间关系。

Result: 在六个医学图像基准测试中，MK-UNet以极低的计算资源（0.316M参数和0.314G FLOPs）显著优于现有方法，如TransUNet和UNeXt。

Conclusion: MK-UNet在计算效率和分割性能上实现了突破，适用于资源受限的实时医疗诊断场景。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种新颖的可变形手术导航方法，通过结合术中3D重建和术前CT数据，填补手术视频与体积患者数据之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 解决手术视频与术前CT数据之间的不匹配问题，提供更准确的手术导航。

Method: 将3D高斯模型与CT网格绑定，通过光度监督联合优化高斯参数和网格变形，参数化每个高斯相对于其父网格三角形。

Result: 在猪内脏手术和人肝模拟数据上展示了BridgeSplat的有效性，能够基于单目RGB数据实现术前CT的合理变形。

Conclusion: BridgeSplat通过结合3D高斯模型和CT网格变形，为手术导航提供了更准确的数据对齐和更新方法。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 提出了一种名为DGLE的新框架，通过扩散模型优化伪标签，提升无源域自适应（SFDA）在遥感图像语义分割中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无源域数据时难以优化噪声伪标签，限制了自训练效果。

Method: 结合置信度过滤和超分辨率增强获取高质量初始伪标签，利用扩散模型传播并优化伪标签。

Result: DGLE有效避免了直接优化完整伪标签集的困难，显著提升了伪标签质量和模型性能。

Conclusion: DGLE为SFDA提供了一种高效的伪标签优化方法，适用于复杂分布场景。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 论文提出了一种在双曲空间中嵌入特征提取器的方法，用于改进粗到细的少样本类增量学习任务，通过双曲对比损失和全连接层优化模型，并在双曲空间中实现特征增强。


<details>
  <summary>Details</summary>
Motivation: 双曲空间在表示层次数据方面优于欧几里得空间，因此将其应用于粗到细的少样本类增量学习任务，以提升模型性能。

Method: 使用Poincaré球模型将特征提取器嵌入双曲空间，引入双曲对比损失和全连接层，并在双曲空间中通过最大熵分布生成增强特征。

Result: 在C2FSCIL基准测试中，该方法显著提高了粗类和细类的分类准确率。

Conclusion: 双曲空间的嵌入和特征增强方法有效提升了少样本类增量学习任务的性能。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种几何感知的两阶段框架，用于智能图像编辑中的对象移除，解决了现有方法无法处理因果视觉伪影的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在移除对象时，要么严格对齐掩码导致无法移除未明确掩码的因果伪影（如阴影和反射），要么采用松散对齐策略导致不可控和过度擦除。这些问题的根源在于忽略了对象几何存在与其视觉效果的因果关系。

Method: 提出两阶段框架：1）几何移除阶段，通过严格掩码对齐监督从几何（如深度）中移除对象；2）外观渲染阶段，基于更新的几何条件渲染真实感RGB图像。引入偏好驱动目标指导几何移除阶段的学习。

Result: 在两个流行基准测试中，方法在移除对象及其关联伪影方面达到最先进性能。

Conclusion: 几何感知的两阶段框架有效解决了对象移除中的因果伪影问题，实现了更可控和逼真的图像编辑。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 提出了一种名为SEGA的黑盒攻击方法，通过高斯平滑和梯度集成提升对NR-IQA模型的攻击迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有白盒攻击方法在更现实的黑盒场景中迁移性差，需解决此问题。

Method: 采用高斯平滑源模型梯度并集成，结合扰动过滤掩码确保不可感知性。

Result: 在CLIVE数据集上验证了SEGA的优越迁移性。

Conclusion: SEGA有效提升了黑盒攻击的迁移性，为NR-IQA模型安全性提供新视角。

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet是一种新型特征融合框架，通过参数自由的乘法交互直接整合基于Transformer的表征与生理基础的D-Markers，在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多任务学习框架在微笑情感识别中因辅助任务监督和复杂损失平衡导致的计算效率低下问题。

Method: 提出Hadamard乘法融合策略，系统评估15种融合方法，直接整合Transformer和D-Markers。

Result: 在四个基准数据集上达到最优性能（UvA-NEMO 88.7%, MMI 99.7%, SPOS 98.5%, BBC 100%），参数减少26%，训练简化。

Conclusion: HadaSmileNet高效且有效，适合实时情感计算的多媒体数据挖掘应用。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和3D高斯泼溅的动态人体与静态场景联合重建框架，通过事件引导损失提升快速运动区域的局部保真度。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频中动态人体与静态场景重建的困难，尤其是在快速运动导致运动模糊的情况下。事件相机的高时间分辨率为此提供了优势。

Method: 使用3D高斯泼溅联合建模人体和场景，通过可学习语义属性区分动态与静态高斯。提出事件引导损失以匹配亮度变化与事件流。

Result: 在两个基准数据集上实现了最先进的重建效果，PSNR/SSIM显著提升，LPIPS降低，尤其适用于高速运动目标。

Conclusion: 该方法无需外部人体掩码，简化了高斯集管理，有效解决了快速运动下的重建问题。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T框架通过语义元组、在线去重和LLM推理，实现了实时威胁监控的高效性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足实时性和决策可解释性的需求。

Method: 分解视频帧为语义元组，在线去重机制，以及基于Chain-of-Thought的LLM微调。

Result: 在XD-Violence和UCF-Crime数据集上，Live-E2T在准确性、实时性和可解释性上优于现有方法。

Conclusion: Live-E2T成功统一了实时性和可解释性，为威胁监控提供了新解决方案。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 论文提出了PhotoCritique数据集、PhotoEye模型和PhotoBench基准，以提升多模态大语言模型在美学视觉理解上的能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在美学视觉理解上表现不足，缺乏专业知识和多样性。

Method: 引入专业摄影师讨论构建的数据集，提出多视角视觉融合机制模型。

Result: 模型在现有基准和新基准上表现优于现有模型。

Conclusion: 通过专业数据集和模型设计，显著提升了美学视觉理解能力。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 提出了一种基于XMem模型的实时MRI引导放疗肿瘤分割框架，用于TrackRAD2025挑战赛。


<details>
  <summary>Details</summary>
Motivation: 提高MRI引导放疗中肿瘤跟踪的精度，以提升癌症治疗的准确性和安全性。

Method: 利用XMem模型的内存增强架构，在长序列cine-MRI中分割肿瘤，并实时跟踪肿瘤运动。

Result: 初步印象显示，该框架在有限标注数据下表现出合理的分割性能，满足临床实时需求。

Conclusion: 该工作为MRI引导放疗中的肿瘤跟踪提供了有效解决方案，尽管实验数据丢失，但初步结果令人鼓舞。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出了一种空间语义一致性模型（SSCM），用于多对比MRI超分辨率，通过动态空间变形模块和语义感知标记聚合块提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多对比MRI超分辨率中空间语义一致性的挑战，传统方法在空间对齐和高频细节恢复上表现不足。

Method: 结合动态空间变形模块、语义感知标记聚合块和空间频率融合块，实现空间对齐和语义一致性。

Result: 在公开和私有数据集上表现优异，参数更少且重建结果空间语义一致。

Conclusion: SSCM在多对比MRI超分辨率任务中实现了最先进的性能，同时保持空间和语义一致性。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出了一种名为OraPO的方法，结合FactScore奖励机制，在有限预算下高效完成放射学报告生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模数据和计算资源，而OraPO旨在通过轻量级设计和强化学习优化，解决资源受限问题。

Method: 采用单阶段强化学习训练，通过Oracle步骤将失败探索转化为监督信号，并结合FactScore奖励机制验证临床事实。

Result: 在CheXpert Plus数据集上达到SOTA性能（F1=0.341），且训练数据需求降低2-3个数量级。

Conclusion: OraPO和FactS的组合提供了一种高效、紧凑的解决方案，显著提升了临床挑战性案例的学习效率。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一种无需训练的框架，通过自适应多风格融合在扩散模型中实现可控的多参考风格融合。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只能接受单一风格图像，缺乏平衡多种风格影响的机制。

Method: AMSF通过语义标记分解模块编码所有风格图像和文本提示，并自适应注入扩散模型的交叉注意力层，再通过相似性感知重加权模块调整风格权重。

Result: AMSF在多风格融合效果上优于现有方法，且能无缝扩展至两种或更多风格。

Conclusion: AMSF为扩散模型中的多风格生成提供了实用解决方案。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: MLF-4DRCNet提出了一种多级融合4D雷达和相机图像的两阶段框架，用于3D目标检测，解决了现有方法忽略雷达点云稀疏性和不完整性的问题。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达在自动驾驶中成本低且鲁棒，但其点云稀疏且噪声大，限制了其在3D目标检测中的独立应用。现有雷达-相机融合方法多基于LiDAR-相机融合范式，忽略了雷达的固有缺陷。

Method: 提出MLF-4DRCNet框架，包含点级、场景级和提案级的多模态信息融合。关键模块包括ERPE（增强雷达点编码器）、HSFP（分层场景融合池化）和PLFE（提案级融合增强）。

Result: 在VoD和TJ4DRadSet数据集上，MLF-4DRCNet实现了最先进的性能，甚至在VoD数据集上接近LiDAR模型的性能。

Conclusion: MLF-4DRCNet通过多级融合有效提升了4D雷达和相机在3D目标检测中的性能，填补了现有方法的不足。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS是一种无需训练的双潜在空间引导框架，通过结构路径和语义路径解决扩散模型中图像反转的语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像编码为单一潜在向量时，难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移。

Method: PDLS将反转过程分解为结构路径和语义路径，通过最优控制问题（LQR）动态引导生成轨迹。

Result: 在FFHQ-1K和ImageNet-1K上的实验表明，PDLS在多种反转任务中比单潜在基线更忠实于原始图像和语义信息。

Conclusion: PDLS通过双路径动态引导，有效解决了语义漂移问题，提升了重建质量。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: Prima是一种基于视觉语言模型（VLM）的AI系统，用于神经影像学诊断，通过大规模MRI数据训练，显著提升诊断准确性并减少医疗系统压力。


<details>
  <summary>Details</summary>
Motivation: 全球MRI需求增长导致医疗系统压力增加，尤其是资源匮乏地区。Prima旨在通过AI技术优化诊断流程，减少医生负担。

Method: 利用22万例MRI研究数据训练Prima，采用分层视觉架构，并在30K MRI研究中测试其性能。

Result: Prima在52种神经疾病诊断中平均AUC达92.0，优于其他先进AI模型，并提供可解释的诊断建议。

Conclusion: Prima展示了AI在医疗系统中的潜力，尤其能缓解资源不平等问题，推动AI驱动的医疗进步。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出了一种新的推理框架Understanding-in-Generation (UiG)，通过将理解能力融入生成过程，提升统一模型的文本到图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将理解和生成过程分离，限制了生成能力的提升。

Method: 引入UiG框架，通过图像编辑将理解能力融入生成过程，逐步优化生成结果。

Result: 在TIIF基准测试中，长提示设置下性能提升3.92%。

Conclusion: UiG框架显著提升了文本到图像生成的性能，证明了理解与生成结合的有效性。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 论文提出了一个用于内窥镜图像的深度估计基准和合成数据集EndoSynth，通过微调基础模型显著提升了真实数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像领域缺乏高质量的基准和数据集，限制了深度估计模型的应用和性能评估。

Method: 提出了一个综合基准，评估了最先进的深度估计模型，并引入了合成数据集EndoSynth。

Result: 使用合成数据集微调的基础模型在真实数据上的准确性显著提升。

Conclusion: 该工作通过提供基准和数据集，推动了内窥镜图像深度估计领域的发展。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: LEAF-Mamba模型通过局部强调和自适应融合模块，提升了RGB-D显著目标检测的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡性能和计算效率方面存在挑战，尤其是局部语义不足和跨模态融合不充分的问题。

Method: 提出LEAF-Mamba模型，包含局部强调状态空间模块（LE-SSM）和自适应融合模块（AFM）。

Result: 在16种先进RGB-D SOD方法中表现最优，且在RGB-T SOD任务中展现出强大泛化能力。

Conclusion: LEAF-Mamba在显著目标检测任务中实现了高效且高性能的解决方案。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出了一种轻量级食品图像分类算法，结合窗口多头注意力机制和空间注意力机制，显著降低了计算成本并提高了分类性能。


<details>
  <summary>Details</summary>
Motivation: 食品图像分类在自动化质量控制中至关重要，但现有Vision Transformer模型参数量大、计算复杂度高，难以在资源受限环境中部署。

Method: 集成窗口多头注意力机制（WMHAM）和空间注意力机制（SAM），WMHAM通过窗口划分降低计算成本，SAM自适应强调关键空间区域。

Result: 在Food-101和Vireo Food-172数据集上分别达到95.24%和94.33%的准确率，同时显著减少参数和FLOPs。

Conclusion: 该方法在计算效率和分类性能之间实现了有效平衡，适合资源受限环境部署。

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA是一个三阶段框架，用于无标注的开放集土地覆盖发现、分割和描述，结合像素级精度与高级语义理解，适用于遥感影像分析。


<details>
  <summary>Details</summary>
Motivation: 解决遥感影像中开放集土地覆盖分析的挑战，包括无监督检测、分割和语义标注。

Method: 1) 使用SAM进行精确发现和掩码提取；2) 通过MLLM进行语义归因和上下文描述；3) 结合LLM评估和人工评分。

Result: OSDA在无需人工标注的情况下，支持多样卫星影像的鲁棒评估，适用于动态土地覆盖监测。

Conclusion: OSDA为自动化制图更新和大规模地球观测分析提供了可扩展且可解释的解决方案。

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2021挑战赛旨在通过利用植物标本馆数据改进对生物多样性丰富但数据稀缺地区的植物自动识别。


<details>
  <summary>Details</summary>
Motivation: 当前植物自动识别技术主要依赖北美和西欧的野外照片数据，而生物多样性丰富的热带地区数据稀缺。植物标本馆数据可能弥补这一不足。

Method: 挑战赛采用跨域分类任务，训练集包含数十万植物标本和数千张野外照片，并包含物种的形态和功能特征。测试集仅含野外照片。

Result: 评估了不同研究团队的方法和系统，分析了主要结果。

Conclusion: 植物标本馆数据可以提升对数据稀缺地区植物的自动识别能力。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: AGSwap是一种通过特征操作和动态优化机制实现跨类别对象融合的文本到图像生成方法，并引入了COF数据集。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在跨类别对象融合中产生的视觉混乱和语义不一致问题，并填补缺乏全面基准数据集的空白。

Method: 提出AGSwap方法，包括组嵌入交换和自适应组更新两个关键组件。

Result: AGSwap在简单和复杂提示下均优于现有方法。

Conclusion: AGSwap和COF数据集为跨类别对象融合提供了有效的解决方案和基准。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 论文总结了PlantCLEF 2019挑战赛的资源、评估方法和主要结果，旨在评估数据稀缺地区植物自动识别的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在植物识别方面取得了进展，但现有数据仅涵盖少数物种，而全球植物种类多达369K。PlantCLEF 2019挑战赛旨在填补这一空白，专注于圭亚那地盾和北亚马逊雨林等数据稀缺地区的植物识别。

Method: 挑战赛基于包含10K物种的数据集，比较了自动化系统与热带植物专家的性能。

Result: 论文总结了参赛研究组采用的方法和系统，并分析了主要结果。

Conclusion: PlantCLEF 2019挑战赛为数据稀缺地区的植物识别提供了重要资源和评估基准，推动了该领域的研究。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个无需训练的框架，利用冻结的通用基础模型实现零样本开放词汇遥感视觉定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于封闭词汇集，且依赖高质量数据集和微调，难以适应开放世界场景。

Method: 通过三个阶段：利用VLM获取跨注意力图、利用DM补充结构信息、通过注意力进化模块净化分割掩码。

Result: 在实验中表现优于现有弱监督和零样本方法。

Conclusion: RSVG-ZeroOV提供了一种高效且可扩展的解决方案，无需任务特定训练。

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出了一种属性提示组合（APC）框架，通过文本语义增强ReID的判别性和泛化性，结合快速-慢速训练策略（FSTS）平衡特定任务学习和通用表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型局限于单域或跨域场景，易过拟合或抑制身份判别特征，需结合文本语义提升性能。

Method: 设计了属性提示生成器（APG），包含语义属性字典（SAD）和提示组合模块（PCM），并采用FSTS策略平衡学习速度。

Result: 在常规和域泛化（DG）ReID数据集上表现优于现有方法，兼具判别性和泛化性。

Conclusion: APC框架通过文本语义和训练策略优化，显著提升了ReID任务的性能。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: OTCCLIP是一种基于最优传输的框架，用于重建图像-标题对，以防御CLIP模型中的数据中毒和后门攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法仅依赖全局表示，忽略了细粒度特征，可能导致错误的图像-标题对，影响CLIP预训练。

Method: 提出基于最优传输的距离度量，重新分配标题，并通过最优传输目标函数促进模态间和模态内的细粒度对齐。

Result: OTCCLIP成功降低了中毒攻击的成功率，并显著提升了CLIP在中毒数据集上的零样本和线性探测性能。

Conclusion: OTCCLIP通过细粒度特征对齐，有效防御数据中毒攻击，同时提升模型性能。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: LFI框架通过建模视觉理解的交互过程，提升视觉基础模型的知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型（VFMs）在从视觉语言模型（VLMs）迁移知识时存在局限性，主要因为其忽略了交互过程。

Method: 提出LFI框架，通过交互查询和交互监督技术，动态捕捉VLMs中的交互模式。

Result: 在多个基准测试中表现优异，如TinyImageNet分类和COCO检测/分割任务中分别提升3.3和1.6mAP/2.4AP。

Conclusion: LFI框架在跨域设置中表现突出，且具有认知对齐优势。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 提出了一种基于混合提示驱动的HyPSAM模型，用于RGB-T显著目标检测，通过动态融合网络和优化策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决RGB-T显著目标检测中特征融合不足和数据稀缺的问题。

Method: 结合动态融合网络（DFNet）生成初始显著图，并通过优化网络（P2RNet）引导SAM模型进行细化。

Result: 在三个公开数据集上达到最优性能，并展示出良好的通用性。

Conclusion: HyPSAM通过提示工程显著提升了RGB-T显著目标检测的性能和通用性。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE是一种多模态交叉注意力自编码器，通过整合文本先验、单目深度图和LiDAR点云，提升了对噪声和对抗性干扰的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云易受噪声、遮挡和对抗性干扰影响，现有自编码器在复杂现实条件下性能下降。

Method: 提出TriFusion-AE，融合文本、深度图和LiDAR点云的多模态信息，通过交叉注意力对齐语义、几何和空间特征。

Result: 在强对抗性攻击和重噪声下表现显著优于CNN自编码器，适用于低数据场景。

Conclusion: TriFusion-AE是一种模型无关的多模态融合框架，可提升点云自编码器的鲁棒性。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: COLT是一种增强开源视频LLMs的工具使用能力的方法，通过动态工具选择和工具代码本避免灾难性遗忘，并在VideoToolBench数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设工具库固定，难以适应工具数据不断演化的真实环境。

Method: COLT引入可学习的工具代码本作为工具特定记忆系统，动态选择相关工具。

Result: 在视频LLM基准和VideoToolBench数据集上达到最先进性能。

Conclusion: COLT成功解决了工具数据动态变化的问题，提升了视频LLMs的工具使用能力。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一种无需训练的方法，利用扩散模型提升稀疏视角3D高斯泼溅重建的质量，通过蒸馏方法和自适应渐进增强方案实现多视角一致性和细节修复。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角下的3D重建因视觉信息不足导致明显伪影，现有生成先验方法难以保证多视角一致性。

Method: 提出FixingGS，采用蒸馏方法提取更准确的扩散先验，并结合自适应渐进增强方案优化重建。

Result: 实验表明FixingGS在视觉质量和重建性能上优于现有方法。

Conclusion: FixingGS通过扩散先验和自适应增强，显著提升了稀疏视角3DGS重建的效果。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM通过非均匀分离权重和混合量化算法，显著提升了超低比特精度下的视觉语言模型效率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的高计算成本和内存需求限制了其在硬件受限环境中的应用。

Method: 提出Bi-VLM，基于高斯分位数非均匀分离权重，并通过显著性感知混合量化算法进行量化。

Result: 在多个基准测试中，Bi-VLM在视觉问答任务上优于SOTA 3%-47%，整体模型性能提升4%-45%。

Conclusion: Bi-VLM在超低比特精度下显著提升效率，同时通过令牌剪枝进一步优化性能。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT是一种自监督学习框架，通过离散表示瓶颈提升医学图像表示的可迁移性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有自监督学习方法在医学图像中因复杂架构或解剖学先验导致的泛化性差和捷径学习问题。

Method: 集成多尺度向量量化到自监督学习流程，强制模型学习结构感知特征。

Result: 在分类和分割任务中表现优异，尤其在低标签数据下高效。

Conclusion: DiSSECT在多个医学影像数据集中验证了其鲁棒性和泛化性。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 论文提出了一种结合热成像、深度学习和车联网技术的实时检测与驾驶员预警系统，以减少鹿车碰撞事故。


<details>
  <summary>Details</summary>
Motivation: 鹿车碰撞在美国造成大量人员伤亡和经济损失，同时威胁鹿群数量，亟需有效解决方案。

Method: 系统通过热成像摄像头采集数据，利用深度学习模型检测鹿，并通过车联网技术向驾驶员和周边车辆发送预警。

Result: 实验显示系统平均精度达98.84%，召回率95.96%，并在实地测试中表现优异，检测延迟低于100毫秒。

Conclusion: 研究证明热成像与车联网技术结合可有效减少鹿车碰撞，为未来应用提供了技术路径。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [70] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi框架通过对齐扩散模型与下游任务需求，显著提升手术数据稀缺场景下的深度学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决手术数据稀缺和扩散模型生成样本不一致或多样性不足的问题。

Method: 构建偏好和非偏好合成图像对，轻量级微调扩散模型以明确对齐下游目标。

Result: 在分类和分割任务中分别提升7-9%和2-10%，对少数类效果显著。

Conclusion: SAADi通过任务感知对齐有效缓解数据稀缺，推动手术视觉应用发展。

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [71] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出了一种基于模型神经网络的动态PET图像去噪方法，结合帧间空间相关性和帧内结构一致性，通过神经网络优化参数，显著提升了去噪性能。


<details>
  <summary>Details</summary>
Motivation: 动态PET图像在短时间帧内因统计量有限导致图像质量差，需要高效的去噪方法。

Method: 提出KMDS模型，利用帧间和帧内信息，并通过神经网络替代参数估计，形成端到端的KMDS-Net。

Result: 实验表明，KMDS-Net在模拟和真实数据上均优于基线方法，显著提升动态PET的时空分辨率。

Conclusion: KMDS-Net是一种有效的动态PET去噪方法，可显著提升图像质量。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [72] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 提出了一种结合光流分割标签插值和多任务学习的新框架，以解决手术场景中时空不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术（RAS）需要精确理解视觉数据，但现有方法多为单任务，且标注数据不足。

Method: 利用光流从标注关键帧传播标签到未标注帧，结合多任务学习。

Result: 提高了手术场景理解的准确性和效率。

Conclusion: 该方法增强了RAS的实用性。

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [73] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel是一个统一加速框架，显著提升多模态理解和生成任务的速度。


<details>
  <summary>Details</summary>
Motivation: 解决多模态模型中扩散去噪和自回归解码带来的高计算开销问题。

Method: 采用分治策略，结合推测解码和多阶段蒸馏过程。

Result: 多模态理解速度提升2倍以上，生成任务速度提升16.67倍（文本到图像）和22倍（图像编辑）。

Conclusion: Hyper-Bagel通过高效加速框架，实现了高质量输出的同时显著提升性能。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [74] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 研究评估了多模态大语言模型（LLMs）和视觉语言模型（VLMs）在基督教圣像单标签分类任务中的表现，发现Gemini-2.5 Pro和GPT-4o优于ResNet50基线模型。


<details>
  <summary>Details</summary>
Motivation: 评估通用VLMs和LLMs是否能解释通常由监督分类器处理的基督教圣像，并分析其性能。

Method: 使用三个数据集（ArtDL、ICONCLASS、Wikidata）进行基准测试，比较不同输入条件（类标签、Iconclass描述、少量样本学习）下的模型表现。

Result: Gemini-2.5 Pro和GPT-4o表现最佳，Wikidata数据集上准确率显著下降。类描述提升零样本性能，少量样本学习效果有限。

Conclusion: 通用多模态LLMs能胜任复杂文化遗产领域的分类任务，支持其在数字人文学科中的应用，未来可研究提示优化和其他分类策略。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [75] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 提出了一种可学习的重参数化图构造方法（LRGC），用于视觉图神经网络（ViG），通过键-查询注意力和软阈值重参数化选择边，无需超参数搜索，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统ViG模型依赖非参数化统计方法构建图，可能无法选择最佳邻域，且需要超参数搜索。

Method: LRGC通过键-查询注意力计算节点间关系，使用软阈值重参数化选择边，阈值可学习。

Result: 在ImageNet-1k数据集上，ViG-LRGC优于同类ViG模型。

Conclusion: LRGC提供了一种可学习的超参数无关的图构造方法，提升了ViG模型的性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [76] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种结构化反思方法，通过明确、可控和可训练的动作优化工具增强型大语言模型在多轮交互中的错误修复能力。


<details>
  <summary>Details</summary>
Motivation: 当前工具增强型大语言模型的自我反思方法依赖启发式提示或单向推理，导致多轮交互中错误修复能力脆弱且重复犯错。

Method: 提出结构化反思方法，结合DAPO和GSPO目标及定制奖励方案，优化‘反思-调用-最终’的逐步策略。

Result: 在BFCL v3和Tool-Reflection-Bench上的实验显示，多轮工具调用成功率和错误恢复能力显著提升，冗余调用减少。

Conclusion: 通过明确反思并直接优化，提高了工具交互的可靠性，并为代理从失败中学习提供了可复现的路径。

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [77] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出Point Prompt Defender框架，通过对抗强化学习自动优化点提示，提升SAM的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或手工制作的提示，限制了可扩展性和泛化能力。

Method: 构建双空间图表示图像块，通过攻击-防御范式训练攻击者和防御者代理，优化点提示。

Result: 实验表明，该方法有效提升SAM的鲁棒性和泛化能力。

Conclusion: 提出了一种灵活、可解释且即插即用的提示优化框架。

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [78] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds是一个多模态野生动物监测数据集，包含无人机图像、相机陷阱照片和视频以及生物声学记录，支持环境监测和濒危物种研究。


<details>
  <summary>Details</summary>
Motivation: 解决濒危物种研究、保护生态学和栖息地管理中的关键需求，推动多模态AI研究。

Method: 在220英亩的牧场中，通过无人机、相机陷阱和生物声学记录同步收集四天的数据。

Result: 展示了不同传感器在土地利用模式、物种检测、行为分析和栖息地监测中的互补优势。

Conclusion: 为多模态野生动物监测提供了可重复的协议和开放数据集，未来将扩展GPS跟踪数据和公民科学数据。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [79] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 提出了一个名为RS3DBench的新基准数据集，用于推动遥感图像的大规模3D视觉模型发展，包含54,951对图像和深度图，并提供了基于稳定扩散的深度估计模型。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏深度信息或深度与图像的对齐不精确，阻碍了3D视觉模型的发展。

Method: 构建了RS3DBench数据集，包含图像、深度图和文本描述，并开发了基于稳定扩散的深度估计模型。

Result: 数据集和模型在遥感图像3D理解任务中表现出色，提供了先进的多模态融合能力。

Conclusion: RS3DBench为遥感领域的3D视觉模型和地理人工智能发展提供了重要支持。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [80] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: DeblurSplat是一种无需Structure-from-Motion（SfM）的3D高斯泼溅去模糊方法，利用事件相机和预训练的密集立体模块（DUSt3R）直接从模糊图像中获取初始点云，并通过事件流提供细粒度监督信号，显著提升了去模糊效果和渲染效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖SfM计算相机位姿，容易引入累积误差，影响初始点云精度。此外，动态模糊问题需要更高效的解决方案。

Method: 1. 使用DUSt3R直接从模糊图像获取初始点云，避免相机位姿计算误差。2. 引入事件流，利用其对动态变化的高敏感性，解码潜在清晰图像以优化场景重建。

Result: 实验表明，DeblurSplat在生成高保真新视图和渲染效率方面均优于现有技术。

Conclusion: DeblurSplat通过结合事件相机和预训练模块，有效解决了动态模糊问题，同时避免了传统SfM的误差累积问题。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [81] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: MoiréNet是一种基于U-Net的卷积神经网络框架，通过结合频域和空间域特征有效去除图像中的莫尔条纹，参数效率高且性能优越。


<details>
  <summary>Details</summary>
Motivation: 莫尔条纹由显示像素和相机传感器网格之间的频谱混叠引起，表现为各向异性和多尺度伪影，对图像去莫尔条纹提出了挑战。

Method: MoiréNet包含方向性频域-空间编码器（DFSE）和频域-空间自适应选择器（FSAS），分别用于识别莫尔条纹方向和自适应抑制伪影。

Result: 实验表明，MoiréNet在公共数据集上达到最优性能，参数仅为5.513M，比ESDNet-L减少48%。

Conclusion: MoiréNet兼具高质量恢复和参数效率，适用于智能手机摄影、工业成像和增强现实等资源受限场景。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [82] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 论文提出了一种频率感知的音频-视觉分割框架（FAVS），通过频率域分解和重组解决音频与视觉模态间的矛盾，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉分割方法忽视了音频高频噪声与视觉高频细节之间的矛盾，导致性能不佳。

Method: 提出FAVS框架，包含频率域增强分解器（FDED）和协同跨模态一致性（SCMC）模块，分别用于频率分解和特征一致性增强。

Result: 在三个基准数据集上达到最先进性能，并通过可视化验证了模块的有效性。

Conclusion: FAVS通过频率域处理解决了模态矛盾，显著提升了分割效果，代码将开源。

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [83] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 本文综述了四种可解释人工智能（xAI）方法在视觉感知任务中的应用，分析了它们的机制、优缺点及评估指标。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常被视为“黑盒”，其决策过程难以解释，这在关键应用中引发可靠性问题。xAI旨在提供理解AI模型决策的方法。

Method: 本文调查了四种xAI方法：显著性图、概念瓶颈模型、基于原型的方法和混合方法，并分析了它们的机制和评估指标。

Result: 研究总结了这些方法的优缺点，为未来研究和应用提供了指导。

Conclusion: xAI方法在提升模型可解释性方面具有潜力，但仍需进一步研究以优化其性能和适用性。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [84] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的DDPM方法，用于生成高质量的合成LiDAR数据，以增强自动驾驶车辆的感知能力。


<details>
  <summary>Details</summary>
Motivation: 真实LiDAR数据收集耗时且易受噪声和稀疏性影响，限制了自动驾驶系统的性能。

Method: 采用改进的DDPM，结合新的噪声调度和时间步嵌入技术，生成更真实的点云数据。

Result: 在IAMCV和KITTI-360数据集上的实验表明，该方法优于现有基线，能有效处理噪声和稀疏数据。

Conclusion: 该方法为自动驾驶感知任务提供了高质量的数据增强方案，提升了系统性能。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [85] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: AGSSP是一种新的自监督预训练范式，通过异常先验指导表示学习，显著提升金属表面缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决自然图像预训练与工业数据之间的领域差距，以及现有自监督方法无法区分细微缺陷与复杂背景噪声的问题。

Method: 采用两阶段框架：1) 通过异常图蒸馏知识预训练模型骨干；2) 使用伪缺陷框预训练检测器。

Result: 在多个设置下性能显著提升，mAP@0.5提高10%，mAP@0.5:0.95提高11.4%。

Conclusion: AGSSP有效解决了领域差距问题，提升了缺陷检测性能，代码和数据集已开源。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [86] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出首个音频驱动的通用真实感头像合成方法，结合通用头部先验（UHAP）和音频模型，实现高保真度的几何与外观变化。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注几何变形，忽略音频相关的外观变化，需一种能同时处理几何和外观的通用模型。

Method: 训练跨身份多视角视频的UHAP，结合音频模型直接映射到UHAP潜在表达空间，通过单目编码器实现个性化。

Result: 生成的头像具有精确唇同步和丰富表情细节，在唇同步准确性、图像质量和感知真实感上优于几何方法。

Conclusion: 该方法首次实现通用音频驱动头像建模，同时捕捉几何和外观变化，表现优于现有方法。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [87] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的自动化流程，用于检测、跟踪和提取树突棘的动态特征。


<details>
  <summary>Details</summary>
Motivation: 树突棘是大脑兴奋性突触的关键结构，其动态分析对学习与记忆研究至关重要，但现有方法耗时且挑战大。

Method: 结合基于Transformer的检测模块、深度跟踪组件、时间跟踪模块和特征提取单元，处理3D+时间显微数据。

Result: 在开源和自建数据集上验证了方法的有效性，并发布了首个时间跟踪数据集。

Conclusion: 该方法为树突棘动态分析提供了可扩展的端到端解决方案，并公开了数据、代码和预训练权重。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [88] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出了一种结合视觉语言模型和预训练视觉模型的自学习零样本图像分类框架，无需标注数据即可动态适应。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在标注数据稀缺场景下的分类问题，利用视觉语言模型和预训练视觉模型提升性能。

Method: 通过置信度伪标签策略，在测试数据上直接训练轻量级分类器，结合VLM和视觉模型迭代优化特征。

Result: 在十个数据集上实验表明，该方法优于基线零样本方法。

Conclusion: 该方法无需微调VLM或依赖大型语言模型，有效减少了语义表示的依赖性。

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [89] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为ReflectiveGS的方法，通过利用镜面反射作为补充视角，提升了3D重建和视图合成的质量，并在MirrorScene3D数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 镜面环境对3D重建和新视图合成带来挑战，现有方法（如NeRF和3DGS）在镜面场景中表现不佳，且忽视了镜面反射的丰富信息。

Method: 提出了ReflectiveGS，扩展了3D高斯泼溅技术，将镜面反射视为补充视角而非对称伪影，从而提升场景几何和细节恢复。

Result: 在MirrorScene3D数据集上，ReflectiveGS在SSIM、PSNR、LPIPS和训练速度上均优于现有方法。

Conclusion: ReflectiveGS为镜面丰富的环境中的3D重建设立了新基准，展示了镜面反射信息的利用潜力。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [90] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 利用深度学习和GAN技术改进腹腔镜胆囊切除术中胆管的定位，以减少胆管损伤风险。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽恢复快、美容效果好，但胆管损伤风险高，影响患者生活质量。

Method: 构建并标注图像数据库，使用Yolo检测算法，结合数据增强和GAN生成合成数据。

Result: 实验结果表明该方法能有效定位胆管。

Conclusion: 该技术有望减少胆管损伤，提升手术安全性，同时讨论了伦理问题。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [91] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: Prompt-DAS是一个基于SAM的多任务框架，通过提示点实现高效的无监督和弱监督域适应分割。


<details>
  <summary>Details</summary>
Motivation: 解决大规模电子显微镜图像中多实例细胞器分割的标注效率问题。

Method: 结合提示点和辅助中心点检测任务，支持多种提示配置，并提出提示引导的对比学习。

Result: 在多个基准测试中优于现有UDA、WDA和SAM方法。

Conclusion: Prompt-DAS在标注效率和分割性能上表现出色，适用于多种域适应场景。

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [92] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: VIR-Bench是一个新的视频理解基准测试，专注于长距离旅行的地理时空轨迹，旨在提升多模态大语言模型（MLLMs）的能力。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准测试主要关注室内场景或短距离户外活动，缺乏对长距离旅行挑战的探索，而这对下一代MLLMs至关重要。

Method: 提出VIR-Bench，包含200个旅行视频，将行程重建作为评估任务，测试MLLMs的地理时空智能。

Result: 实验显示，即使是先进的MLLMs也难以在长时空尺度的视频中取得高分，验证了任务的挑战性。

Conclusion: VIR-Bench不仅有效评估模型，还能提升实际应用（如旅行规划代理）的性能。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [93] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 论文提出了一种细粒度的视觉语言推理框架，通过步骤级推理、过程奖励模型和强化学习，显著提升了推理质量和性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言推理方法在细粒度推理和中间步骤评估上存在不足，需要更透明和可评估的推理框架。

Method: 提出步骤级推理数据、过程奖励模型（PRM）和强化学习训练，构建透明且高效的推理框架。

Result: 模型在视觉语言基准测试中表现优异，并通过消融实验验证了各组件的重要性。

Conclusion: 该框架为视觉语言推理提供了新基准，并揭示了推理时间扩展的有趣特性。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [94] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于SAM和ViT的弱监督食物图像语义分割方法，利用CAM生成提示，无需像素级标注。


<details>
  <summary>Details</summary>
Motivation: 解决食物图像分割中需要大量像素级标注的问题，利用SAM和ViT的零样本能力。

Method: 使用ViT的CAM生成SAM的提示，结合图像预处理和多掩码策略。

Result: 在FoodSeg103数据集上，mIoU达到0.54，平均每图生成2.4个掩码。

Conclusion: 该方法可用于加速食物图像标注或集成到营养追踪应用中。

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [95] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet是一种基于动态学习的U-Net分割架构，旨在实现超声心动图分割的时间稳定性和精确性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图易受变形和噪声影响，导致帧间分割抖动，影响功能评估和临床解释。

Method: DyL-UNet通过动态学习构建Echo-Dynamics Graph (EDG)，结合Swin-Transformer编码器-解码器分支和Cardiac Phase-Dynamics Attention (CPDA)机制。

Result: 在CAMUS和EchoNet-Dynamic数据集上，DyL-UNet在保持分割精度的同时，实现了更好的时间一致性。

Conclusion: DyL-UNet为自动化临床超声心动图提供了可靠的解决方案。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [96] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: ColorBlindnessEval是一个新基准，用于评估视觉语言模型（VLMs）在受色盲测试启发的视觉对抗场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在复杂视觉模式中识别数字的能力，揭示其在对抗性环境中的局限性。

Method: 使用500张类似石原色盲测试的图像，通过Yes/No和开放式提示评估9个VLMs，并与人类表现对比。

Result: 实验显示VLMs在对抗性场景中识别数字的能力有限，存在幻觉问题。

Conclusion: ColorBlindnessEval可作为提升VLMs在关键应用中可靠性的工具。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [97] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian框架通过将扩散过程转移到小波域，显著提升了稀疏视图3D高斯对象重建的效率。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）在稀疏视图设置下性能下降，现有方法依赖计算密集的扩散模型修复，效率低下。

Method: 将扩散应用于低分辨率LL子带，高频子带用轻量网络优化，并提出在线随机掩码策略替代低效的留一法。

Result: 在Mip-NeRF 360和OmniObject3D数据集上，WaveletGaussian在保持渲染质量的同时大幅减少训练时间。

Conclusion: WaveletGaussian为稀疏视图3D重建提供了一种高效且性能优越的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [98] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i是Sa2VA的改进版本，解决了训练与推理不一致的问题，在多个视频基准测试中取得新突破。


<details>
  <summary>Details</summary>
Motivation: 发现Sa2VA在视频对象分割任务中未发挥全部潜力，主要原因是训练与推理过程不一致。

Method: 提出Sa2VA-i，修正训练与推理的不一致性。

Result: 在多个基准测试中显著提升性能，如MeViS（+11.6 J&F）、Ref-YT-VOS（+1.4）、Ref-DAVIS（+3.3）和ReVOS（+4.1）。

Conclusion: 强调了实现细节的重要性，为视频分割领域提供了有价值的见解。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [99] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，将多光谱数据以零样本方式输入通用多模态模型，提升遥感任务性能。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像在遥感应用中很重要，但现有机器学习模型训练成本高，且通用多模态模型无法处理多光谱信号。

Method: 通过调整输入空间和注入领域信息，将多光谱数据适配到RGB训练的通用多模态模型中。

Result: 在土地覆盖和土地利用分类任务中，Gemini2.5模型表现出显著的零样本性能提升。

Conclusion: 该方法使地理空间专业人员能轻松利用通用多模态模型，加速工作并受益于其推理能力。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [100] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V是一个多模态医学基础模型，结合图像分析与文本推理，提供从视觉定位到临床推理的统一流程。


<details>
  <summary>Details</summary>
Motivation: 现有医学成像模型泛化能力有限，临床需要精确的视觉定位、多模态整合和链式推理能力。

Method: 提出多模态训练方法，整合检测、分割和链式推理，支持像素级病灶定位和结构化报告生成。

Result: Citrus-V在多个基准测试中优于现有开源医学模型和专家级成像系统。

Conclusion: Citrus-V为病灶量化、自动化报告和可靠第二意见提供了统一解决方案。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 研究探讨了多模态大语言模型（MLLMs）在基础设施摄像头图像中零样本检测和描述交通事故的能力，结合视觉分析技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测对安全至关重要，但缺乏多样化标注数据，MLLMs的零样本能力可减少对标注数据的依赖。

Method: 使用CARLA模拟的DeepAccident数据集评估MLLMs，结合YOLO、Deep SORT和SAM等视觉分析技术优化提示。

Result: Pixtral表现最佳（F1-score 0.71，召回率83%），Gemini 1.5精度提升至90%，Gemma 3性能最平衡。

Conclusion: MLLMs结合视觉分析技术在实际交通监控中潜力巨大。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2是一种基于Transformer的在线长期点跟踪模型，通过架构改进、内存优化和合成训练策略提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决视频帧间点跟踪在显著外观变化、运动和遮挡下的长期一致性问题，适用于实时和流式应用。

Method: 采用因果处理框架和内存机制保持时间一致性，通过粗粒度块级分类和细化实现推理。

Result: 在五个合成和真实基准测试中达到最先进水平，超越现有在线跟踪器和离线方法。

Conclusion: 因果内存架构和纯合成数据训练是解决真实世界点跟踪问题的有效且可扩展方案。

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是一个多相机、多光谱同步系统，用于实时检测海豹和北极熊，显著减少数据处理时间。


<details>
  <summary>Details</summary>
Motivation: 提高冰区海豹和北极熊的空中调查效率，减少数据处理时间。

Method: 通过严格的校准和硬件同步，利用多光谱进行目标检测，并标注元数据。

Result: 数据处理时间减少80%，所有数据映射到世界平面以便快速评估。

Conclusion: KAMERA的开源软件和模型有望推动科学界的其他测绘和检测工作。

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX是一种神经形态协同推理架构，通过优化时空冗余和动态提前退出机制，显著降低数据传输和能耗，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在边缘计算中的延迟和能耗问题，以及边缘-云协同推理的高延迟和传输成本问题。

Method: 引入学习驱动的脉冲压缩模块和动态提前退出机制，优化时空冗余。

Result: 数据传输减少2048倍，边缘能耗降低90%以上，延迟减少3倍，精度损失小于2%。

Conclusion: NeuCODEX为资源受限环境中的高性能SNN部署提供了实用解决方案。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 提出了一种鲁棒的自监督立体匹配方法，通过引入视觉基础模型的先验和场景对应先验，提升在恶劣天气条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法在恶劣天气条件下性能显著下降，主要原因是CNN特征提取器在退化区域表现不佳，以及光度一致性假设失效。

Method: 注入视觉基础模型的鲁棒先验改进特征提取，并利用场景对应先验构建监督信号；通过合成数据集和两步训练范式（鲁棒自监督场景对应学习和恶劣天气蒸馏）实现。

Result: 在恶劣天气条件下，性能优于现有自监督方法。

Conclusion: 提出的方法通过鲁棒先验和场景对应学习，显著提升了自监督立体匹配在恶劣天气下的表现。

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: YOLO-LAN是一种基于YOLO的息肉检测方法，通过M2IoU损失、数据增强和负数据训练，在Kvasir-seg和BKAI-IGH NeoPolyp数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查中手动检测息肉存在不一致性和遗漏风险，深度学习可提供更准确和实时的诊断方案。

Method: 提出YOLO-LAN，使用M2IoU损失、多样化数据增强和负数据训练，模拟真实临床场景。

Result: 在Kvasir-seg数据集上，YOLOv12和YOLOv8分别达到mAP50 0.9619和0.9540，mAP50:95 0.8599和0.8487。

Conclusion: YOLO-LAN在息肉大小和位置检测上表现稳健，适用于AI辅助结直肠筛查。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 本文研究了LSVOS挑战赛中的MOSEv2赛道，通过改进SeC框架，分析了其长期记忆和概念感知记忆的作用，提升了半监督视频对象分割性能。


<details>
  <summary>Details</summary>
Motivation: 探索复杂半监督视频对象分割的解决方案，解决遮挡和重现等核心挑战。

Method: 分析和改进SeC框架，研究其长期记忆和概念感知记忆的功能。

Result: 在MOSEv2测试集上取得39.89%的JF分数，排名第一。

Conclusion: 长期记忆和概念感知记忆的结合有效提升了分割性能，解决了核心挑战。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型（VLMs）的视觉处理机制，提出了一种基于双流假设的解构方法，并改进了解码效率和空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs通过序列化处理图像，与人类视觉的并行性不符，且内部机制不透明，阻碍了理解和创新。

Method: 将视觉处理解构为对象识别和空间感知两部分，分别研究其机制，并提出了基于视觉解码器的令牌压缩算法和RoPE缩放技术。

Result: 验证了VLMs的两阶段对象识别过程和空间感知的几何结构，提高了解码效率和空间推理能力。

Conclusion: 研究深化了对VLM内部机制的理解，为未来架构设计提供了明确原则。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 论文提出了一种无需视觉编码器的文本到文本检索方法，通过生成结构化图像描述，解决了传统视觉语言模型的模态鸿沟和隐私问题，并在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型（如CLIP）存在模态鸿沟、浅层语言理解和高计算成本问题，作者希望通过纯文本方法改进检索任务。

Method: 采用文本到文本检索范式，利用VLLM生成结构化图像描述，替代传统视觉编码器，减少模态鸿沟并提升性能。

Result: 该方法在多个检索和组合性基准测试中达到零样本最优性能，且仅需少量GPU校准时间。

Conclusion: 纯文本检索方法在性能、隐私和计算效率上优于传统多模态模型，为检索任务提供了新思路。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 研究发现，组合性训练和长标题理解之间存在双向关系，高质量数据对提升模型性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索组合性与长标题理解之间的关系，以提升视觉语言模型的能力。

Method: 训练和评估针对组合性和长标题理解的不同模型，分析数据质量和模型设计的影响。

Result: 组合性训练提升长标题检索性能，长标题训练促进组合性，但数据质量和模型设计是关键。

Conclusion: 组合性理解和长标题理解是相互关联的能力，可通过高质量数据联合学习。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 提出了一种结合合成RGB图像、少量真实标注和GAN跨模态对齐的方法，显著提升了热图像中的植物分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决热图像中植物分割的低对比度和遮挡问题，提高高通量表型分析的准确性。

Method: 利用合成RGB图像和少量真实标注，通过GAN跨模态对齐增强分割模型。

Result: 结合合成数据和少量真实标注后，杂草和植物类别的分割性能分别提升了22%和17%。

Conclusion: 合成数据与有限真实标注的结合，通过生成模型跨域翻译，能显著提升复杂环境中的分割性能。

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: HyKid是一个开源数据集，包含48名儿童脑积水患者的3D MRI图像和专家标注的脑组织分割，用于评估脑积水。


<details>
  <summary>Details</summary>
Motivation: 儿童脑积水评估缺乏公开的专家标注数据集，尤其是脉络丛分割数据。

Method: 使用切片到体积算法重建高分辨率3D MRI，并由神经学家手动校正分割。临床数据通过检索增强生成框架提取。

Result: 脉络丛体积与总脑脊液体积强相关，预测模型AUC达0.87。

Conclusion: HyKid为神经影像算法开发提供了高质量基准，揭示了脉络丛在脑积水评估中的特征。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出了一种多尺度特征交互网络（MsFIN）用于从行车记录仪视频中早期预测事故，解决了特征级交互和多时间行为线索的挑战。


<details>
  <summary>Details</summary>
Motivation: 行车记录仪的普及和计算机视觉的进步使得从行车记录仪视角开发事故预测模型变得重要，但存在特征级交互和多时间行为线索的挑战。

Method: 设计了MsFIN网络，包含多尺度特征聚合、时间特征处理和多尺度特征后融合三层，利用Transformer架构和因果约束捕捉特征交互和时间演化。

Result: 在DAD和DADA数据集上，MsFIN在预测准确性和早期性上显著优于单尺度特征提取的现有模型。

Conclusion: MsFIN通过多尺度特征融合和上下文交互建模实现了优越性能，消融研究验证了各模块的有效性。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 论文提出了一种基于持续学习的数字人脸伪造检测方法，通过混合专家架构（MoE）和正交学习策略，有效应对不断演变的伪造技术。


<details>
  <summary>Details</summary>
Motivation: 解决现有检测模型难以快速适应新型伪造技术的问题，同时避免遗忘已学习的伪造类型。

Method: 采用发展性混合专家架构（MoE），分为Real-LoRA和Fake-LoRAs两组，利用正交损失和梯度防止干扰。

Result: 实验表明，该方法在数据集和伪造类型增量协议下均表现优异。

Conclusion: 该方法为持续学习在数字伪造检测中的应用提供了有效解决方案。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一个统一的多模态掩码扩散模型（MDM），支持图像理解和生成任务，具备对象定位、图像编辑和高分辨率合成等新能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型如MMaDa和Muddit仅支持简单的图像级理解和低分辨率生成，Lavida-O旨在通过统一模型提升多模态任务的性能。

Method: 采用弹性混合Transformer架构、通用文本条件和分层采样等新技术，结合规划和迭代自反思提升生成效果。

Result: 在RefCOCO、GenEval和ImgEdit等基准测试中表现最优，超越Qwen2.5-VL和FluxKontext-dev等模型，且推理速度更快。

Conclusion: Lavida-O是首个通过理解能力优化生成和编辑的统一MDM，实现了多模态任务的高效与高性能。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 论文提出了ConViS任务，通过预定义的关键语义概念计算视频对的相似性，并引入ConViS-Bench基准测试，评估模型与人类判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究视频相似性的多维度评估，弥补现有模型依赖全局相似性分数的不足，利用大语言模型提升视频理解能力。

Method: 提出ConViS任务，基于语义概念计算视频相似性，并构建ConViS-Bench基准测试，包含标注视频对和概念级相似性分数。

Result: 不同模型在ConViS任务上表现差异显著，某些概念对视频相似性估计更具挑战性。

Conclusion: ConViS-Bench为语言驱动的视频理解研究提供了重要资源，推动了多维度视频相似性评估的发展。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出了一种基于VQ-GAN的对抗性细化框架，用于高效压缩和表示人体运动数据。


<details>
  <summary>Details</summary>
Motivation: 高维度和冗余性使得连续人体运动理解成为计算机视觉中的核心挑战，需要高效的压缩和表示方法。

Method: 结合密集运动标记化和对抗性细化，消除重建伪影（如运动模糊和时间错位）。

Result: 在CMU Panoptic数据集上，SSIM指标优于dVAE基线9.31%，时间不稳定性降低37.1%。

Conclusion: 该方法在运动分析应用中具有实际部署可行性，并揭示了2D和3D运动的最优标记化策略。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 提出了一种新的Graph-Radiomic Learning (GrRAiL)描述符，用于临床MRI扫描中表征病灶内异质性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决常规影像学中难以区分恶性肿瘤与混淆病理的问题，传统方法忽略了复杂的空间关系。

Method: GrRAiL通过聚类子区域并计算图论指标，量化簇间空间关联，编码高阶空间关系。

Result: 在三个临床案例中，GrRAiL表现优于基线方法，测试准确率提升10%以上。

Conclusion: GrRAiL能有效区分恶性肿瘤与混淆病理，具有临床可行性。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS是首个仅使用自我中心视觉感知环境并导航的人类化身，通过解耦低层运动技能与高层视觉控制学习，实现了更接近人类行为的运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有的人类运动生成方法忽视了感知与运动的相互依赖，使用与人类感知差异巨大的任务特定‘感知’，导致行为不够自然。

Method: 首先在大规模运动捕捉数据集上训练运动先验模型，然后通过Q学习训练策略，将自我中心视觉输入映射为运动先验的高层控制命令。

Result: 实验表明，自我中心视觉能赋予化身人类般的运动特征，例如避开视觉障碍物。

Conclusion: 为化身配备人类般的传感器（如自我中心视觉）有望训练出行为更接近人类的化身。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 论文提出OverLayScore量化重叠边界框复杂度，并引入OverLayBench基准和CreatiLayout-AM模型，以提升重叠布局下的图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在边界框显著重叠的布局中表现不佳，需解决大重叠区域和语义区分小的重叠实例问题。

Method: 提出OverLayScore指标和OverLayBench基准，并开发CreatiLayout-AM模型。

Result: 分析显示现有基准偏向简单案例，新方法为复杂重叠场景提供更鲁棒的评估和生成方案。

Conclusion: 论文为复杂重叠布局下的图像生成奠定了基础，推动该领域发展。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识蒸馏为显式的3D高斯泼溅表示，无需多视图训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D重建方法依赖真实世界的多视图数据，而视频扩散模型的2D性质限制了其在机器人导航等模拟应用中的使用。

Method: 通过增强RGB解码器与3DGS解码器，利用视频扩散模型生成的合成数据进行训练，实现从文本或单图像合成3D场景。

Result: 实验结果表明，该框架在静态和动态3D场景生成中达到了最先进的性能。

Conclusion: 该框架有效解决了多视图数据依赖问题，扩展了视频扩散模型在3D场景生成中的应用。

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出了一种新的多视角前馈范式，用体素对齐的高斯分布替代像素对齐的高斯预测，解决了现有方法的局限性，并在多个基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素对齐高斯预测的方法存在对输入视角数量的依赖、视角偏差的密度分布以及对齐误差等问题，特别是在遮挡或低纹理情况下表现不佳。

Method: VolSplat通过直接从预测的3D体素网格生成高斯分布，避免了像素对齐对2D特征匹配的依赖，实现了多视角一致性，并基于3D场景复杂度自适应控制高斯密度。

Result: 在RealEstate10K和ScanNet等基准测试中，VolSplat表现出色，生成了更真实、视角一致的3D重建结果，并提升了新视角渲染质量。

Conclusion: VolSplat不仅提供了更优的结果，还为前馈式3D重建建立了一个更具扩展性的框架，为更广泛的研究社区铺平了道路。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for Vision Models](https://arxiv.org/abs/2509.15156)
*Haobo Yang,Minghao Guo,Dequan Yang,Wenyu Wang*

Main category: cs.CV

TL;DR: 论文探讨了将人类感知心理学中的几何视觉错觉引入深度学习模型，以提升图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型主要依赖大数据统计规律，缺乏对人类感知心理学的结构化洞察。

Method: 提出合成参数化几何错觉数据集，结合ImageNet分类任务，评估三种多源学习策略。

Result: 实验表明，几何错觉辅助监督能系统性提升泛化能力，尤其在复杂轮廓和精细纹理场景中。

Conclusion: 感知驱动的归纳偏置可增强CNN和Transformer架构的结构敏感性，为视觉模型设计提供新方向。

Abstract: Contemporary deep learning models have achieved impressive performance in
image classification by primarily leveraging statistical regularities within
large datasets, but they rarely incorporate structured insights drawn directly
from perceptual psychology. To explore the potential of perceptually motivated
inductive biases, we propose integrating classic geometric visual illusions
well-studied phenomena from human perception into standard image-classification
training pipelines. Specifically, we introduce a synthetic, parametric
geometric-illusion dataset and evaluate three multi-source learning strategies
that combine illusion recognition tasks with ImageNet classification
objectives. Our experiments reveal two key conceptual insights: (i)
incorporating geometric illusions as auxiliary supervision systematically
improves generalization, especially in visually challenging cases involving
intricate contours and fine textures; and (ii) perceptually driven inductive
biases, even when derived from synthetic stimuli traditionally considered
unrelated to natural image recognition, can enhance the structural sensitivity
of both CNN and transformer-based architectures. These results demonstrate a
novel integration of perceptual science and machine learning and suggest new
directions for embedding perceptual priors into vision model design.

</details>


### [124] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow通过条件感知重参数化简化了条件生成模型的训练，减少了模型需要学习的概率路径，从而提高了训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散和基于流的方法在条件生成建模中需要模型同时学习质量传输和条件注入，增加了模型的负担。

Method: 提出了CAR-Flow，通过学习一个轻量级的条件感知偏移来重新定位源分布和目标分布，从而缩短模型需要学习的概率路径。

Result: 在低维合成数据上可视化和量化了CAR的效果；在高维自然图像数据（ImageNet-256）上，CAR-Flow将SiT-XL/2的FID从2.07降低到1.68，仅增加了不到0.6%的参数。

Conclusion: CAR-Flow通过简化条件生成模型的训练过程，显著提高了生成质量，同时保持了模型的轻量级特性。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [125] [Machine Learnability as a Measure of Order in Aperiodic Sequences](https://arxiv.org/abs/2509.18103)
*Jennifer Dodgson,Michael Joedhitya,Adith Ramdas,Surender Suresh Kumar,Adarsh Singh Chauhan,Akira Rafhael,Wang Mingshu,Nordine Lotfi*

Main category: cs.LG

TL;DR: 利用图像机器学习模型分析Ulam螺旋中素数分布的规律性，发现高数值区域（如500m附近）比低数值区域（如25m以下）更易学习，暗示前者存在更明显的规律。


<details>
  <summary>Details</summary>
Motivation: 探索素数分布的确定性特征与统计随机性之间的关系，并验证机器学习是否能作为数论研究的新工具。

Method: 使用图像机器学习模型，比较Ulam螺旋中不同区域（高数值与低数值）的素数分布规律性。

Result: 高数值区域的模型表现更优，表明该区域素数分布更易学习；分类策略在不同区域有差异（低数值区聚焦素数模式，高数值区聚焦排除合数）。

Conclusion: 机器学习可作为数论实验工具，尤其在研究密码学相关素数模式方面具有潜力。

Abstract: Research on the distribution of prime numbers has revealed a dual character:
deterministic in definition yet exhibiting statistical behavior reminiscent of
random processes. In this paper we show that it is possible to use an
image-focused machine learning model to measure the comparative regularity of
prime number fields at specific regions of an Ulam spiral. Specifically, we
demonstrate that in pure accuracy terms, models trained on blocks extracted
from regions of the spiral in the vicinity of 500m outperform models trained on
blocks extracted from the region representing integers lower than 25m. This
implies existence of more easily learnable order in the former region than in
the latter. Moreover, a detailed breakdown of precision and recall scores seem
to imply that the model is favouring a different approach to classification in
different regions of the spiral, focusing more on identifying prime patterns at
lower numbers and more on eliminating composites at higher numbers. This aligns
with number theory conjectures suggesting that at higher orders of magnitude we
should see diminishing noise in prime number distributions, with averages
(density, AP equidistribution) coming to dominate, while local randomness
regularises after scaling by log x. Taken together, these findings point toward
an interesting possibility: that machine learning can serve as a new
experimental instrument for number theory. Notably, the method shows potential
1 for investigating the patterns in strong and weak primes for cryptographic
purposes.

</details>


### [126] [Data Valuation and Selection in a Federated Model Marketplace](https://arxiv.org/abs/2509.18104)
*Wenqian Li,Youjia Yang,Ruoxi Jia,Yan Pang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Wasserstein距离的联邦学习框架，用于数据市场中的模型交易，解决了数据估值和选择的挑战。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，如何从异构数据源中有效评估和选择数据是关键问题。本文旨在通过Wasserstein距离估计器提升数据市场的可信度。

Method: 提出了一种基于Wasserstein距离的估计器框架，用于预测模型性能并揭示数据异质性与联邦学习算法的兼容性。同时，采用分布式方法近似Wasserstein距离以保护隐私。

Result: 实验表明，该方法在标签倾斜、错误标记和无标记等场景下，能可靠地识别高性能数据组合。

Conclusion: 该框架为联邦学习模型市场提供了可靠的数据选择和估值方法，推动了数据共享和模型交易的发展。

Abstract: In the era of Artificial Intelligence (AI), marketplaces have become
essential platforms for facilitating the exchange of data products to foster
data sharing. Model transactions provide economic solutions in data
marketplaces that enhance data reusability and ensure the traceability of data
ownership. To establish trustworthy data marketplaces, Federated Learning (FL)
has emerged as a promising paradigm to enable collaborative learning across
siloed datasets while safeguarding data privacy. However, effective data
valuation and selection from heterogeneous sources in the FL setup remain key
challenges. This paper introduces a comprehensive framework centered on a
Wasserstein-based estimator tailored for FL. The estimator not only predicts
model performance across unseen data combinations but also reveals the
compatibility between data heterogeneity and FL aggregation algorithms. To
ensure privacy, we propose a distributed method to approximate Wasserstein
distance without requiring access to raw data. Furthermore, we demonstrate that
model performance can be reliably extrapolated under the neural scaling law,
enabling effective data selection without full-scale training. Extensive
experiments across diverse scenarios, such as label skew, mislabeled, and
unlabeled sources, show that our approach consistently identifies
high-performing data combinations, paving the way for more reliable FL-based
model marketplaces.

</details>


### [127] [BULL-ODE: Bullwhip Learning with Neural ODEs and Universal Differential Equations under Stochastic Demand](https://arxiv.org/abs/2509.18105)
*Nachiket N. Naik,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 论文研究了在随机需求下连续时间库存动态的学习，并量化了结构在预测牛鞭效应中的帮助或阻碍作用。


<details>
  <summary>Details</summary>
Motivation: 传统供应链模型通过控制/预测选择和信息共享解释牛鞭效应，但结构偏差在不同需求机制下是否有助于预测尚不明确。

Method: 使用单级测试平台，比较完全学习的NODE和物理约束的UDE，在三种需求机制（AR(1)、i.i.d.高斯和重尾对数正态）下进行训练和评估。

Result: 在结构化机制下，UDE表现更好；在重尾对数正态机制下，NODE更灵活。

Conclusion: 在噪声轻尾或时间相关时强制结构；在极端事件主导时放松结构。

Abstract: We study learning of continuous-time inventory dynamics under stochastic
demand and quantify when structure helps or hurts forecasting of the bullwhip
effect. BULL-ODE compares a fully learned Neural ODE (NODE) that models the
entire right-hand side against a physics-informed Universal Differential
Equation (UDE) that preserves conservation and order-up-to structure while
learning a small residual policy term. Classical supply chain models explain
the bullwhip through control/forecasting choices and information sharing, while
recent physics-informed and neural differential equation methods blend domain
constraints with learned components. It is unclear whether structural bias
helps or hinders forecasting under different demand regimes. We address this by
using a single-echelon testbed with three demand regimes - AR(1)
(autocorrelated), i.i.d. Gaussian, and heavy-tailed lognormal. Training is done
on varying fractions of each trajectory, followed by evaluation of multi-step
forecasts for inventory I, order rate O, and demand D. Across the structured
regimes, UDE consistently generalizes better: with 90% of the training horizon,
inventory RMSE drops from 4.92 (NODE) to 0.26 (UDE) under AR(1) and from 5.96
to 0.95 under Gaussian demand. Under heavy-tailed lognormal shocks, the
flexibility of NODE is better. These trends persist as train18 ing data
shrinks, with NODE exhibiting phase drift in extrapolation while UDE remains
stable but underreacts to rare spikes. Our results provide concrete guidance:
enforce structure when noise is light-tailed or temporally correlated; relax
structure when extreme events dominate. Beyond inventory control, the results
offer guidance for hybrid modeling in scientific and engineering systems:
enforce known structure when conservation laws and modest noise dominate, and
relax structure to capture extremes in settings where rare events drive
dynamics.

</details>


### [128] [Model-Based Transfer Learning for Real-Time Damage Assessment of Bridge Networks](https://arxiv.org/abs/2509.18106)
*Elisa Tomassini,Enrique García-Macías,Filippo Ubertini*

Main category: cs.LG

TL;DR: 提出基于神经网络的迁移学习方法，用于桥梁结构监测，实现跨结构知识迁移。


<details>
  <summary>Details</summary>
Motivation: 永久监测系统数据增加带来新机遇，但大范围桥梁网络管理面临可扩展性挑战，需高效跟踪和比较长期行为。

Method: 使用神经网络代理模型进行模型迁移学习，将训练于一座桥梁的模型适配到相似特性的另一座桥梁。

Result: 验证显示模型对损伤位置、程度和范围高度敏感，支持实时监测和网络级韧性提升。

Conclusion: 该方法促进智能监测策略，实现跨结构知识迁移，提升网络级监测能力。

Abstract: The growing use of permanent monitoring systems has increased data
availability, offering new opportunities for structural assessment but also
posing scalability challenges, especially across large bridge networks.
Managing multiple structures requires tracking and comparing long-term
behaviour efficiently. To address this, knowledge transfer between similar
structures becomes essential. This study proposes a model-based transfer
learning approach using neural network surrogate models, enabling a model
trained on one bridge to be adapted to another with similar characteristics.
These models capture shared damage mechanisms, supporting a scalable and
generalizable monitoring framework. The method was validated using real data
from two bridges. The transferred model was integrated into a Bayesian
inference framework for continuous damage assessment based on modal features
from monitoring data. Results showed high sensitivity to damage location,
severity, and extent. This approach enhances real-time monitoring and enables
cross-structure knowledge transfer, promoting smart monitoring strategies and
improved resilience at the network level.

</details>


### [129] [AdaMixT: Adaptive Weighted Mixture of Multi-Scale Expert Transformers for Time Series Forecasting](https://arxiv.org/abs/2509.18107)
*Huanyao Zhang,Jiaye Lin,Wentao Zhang,Haitao Yuan,Guoliang Li*

Main category: cs.LG

TL;DR: 提出了一种名为AdaMixT的新架构，通过多尺度特征融合和动态权重分配提升多元时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单尺度预定义补丁或缺乏有效的多尺度特征融合机制，限制了性能。

Method: AdaMixT引入多种补丁，结合通用预训练模型和领域特定模型进行多尺度特征提取，并通过门控网络动态分配权重。

Result: 在八个基准数据集上的实验表明AdaMixT在真实场景中表现优异。

Conclusion: AdaMixT通过自适应多尺度融合显著提升了预测准确性和泛化能力。

Abstract: Multivariate time series forecasting involves predicting future values based
on historical observations. However, existing approaches primarily rely on
predefined single-scale patches or lack effective mechanisms for multi-scale
feature fusion. These limitations hinder them from fully capturing the complex
patterns inherent in time series, leading to constrained performance and
insufficient generalizability. To address these challenges, we propose a novel
architecture named Adaptive Weighted Mixture of Multi-Scale Expert Transformers
(AdaMixT). Specifically, AdaMixT introduces various patches and leverages both
General Pre-trained Models (GPM) and Domain-specific Models (DSM) for
multi-scale feature extraction. To accommodate the heterogeneity of temporal
features, AdaMixT incorporates a gating network that dynamically allocates
weights among different experts, enabling more accurate predictions through
adaptive multi-scale fusion. Comprehensive experiments on eight widely used
benchmarks, including Weather, Traffic, Electricity, ILI, and four ETT
datasets, consistently demonstrate the effectiveness of AdaMixT in real-world
scenarios.

</details>


### [130] [Solve it with EASE](https://arxiv.org/abs/2509.18108)
*Adam Viktorin,Tomas Kadavy,Jozef Kovac,Michal Pluhacek,Roman Senkerik*

Main category: cs.LG

TL;DR: EASE是一个开源、模块化的框架，利用大语言模型（LLMs）迭代生成算法解决方案，集成了生成、测试、分析和评估功能。


<details>
  <summary>Details</summary>
Motivation: 简化算法设计和生成过程的复杂性，为用户提供透明且可扩展的平台。

Method: 通过反馈循环集成多个LLMs，分别担任生成、分析和评估角色，抽象化提示设计和模型管理的复杂性。

Result: 提供了一个可复用的框架，支持跨领域算法和生成解决方案的协同设计。

Conclusion: EASE为研究者和实践者提供了一个高效、可控的工具，用于算法开发和生成任务。

Abstract: This paper presents EASE (Effortless Algorithmic Solution Evolution), an
open-source and fully modular framework for iterative algorithmic solution
generation leveraging large language models (LLMs). EASE integrates generation,
testing, analysis, and evaluation into a reproducible feedback loop, giving
users full control over error handling, analysis, and quality assessment. Its
architecture supports the orchestration of multiple LLMs in complementary
roles-such as generator, analyst, and evaluator. By abstracting the complexity
of prompt design and model management, EASE provides a transparent and
extensible platform for researchers and practitioners to co-design algorithms
and other generative solutions across diverse domains.

</details>


### [131] [Machine Learning-Based Classification of Vessel Types in Straits Using AIS Tracks](https://arxiv.org/abs/2509.18109)
*Jonatan Katz Nielsen*

Main category: cs.LG

TL;DR: 利用AIS数据通过机器学习分类船舶类型，随机森林模型准确率达92.15%。


<details>
  <summary>Details</summary>
Motivation: 提高船舶类型识别的准确性，以增强海上安全监管和打击非法活动。

Method: 使用AIS数据提取31个轨迹特征，通过随机森林模型进行分类，并采用分组训练/测试和交叉验证避免数据泄露。

Result: 随机森林模型在测试集上达到92.15%的准确率，特征重要性分析显示桥梁位置比和最大速度最具区分性。

Conclusion: 轻量级AIS轨迹特征可实现实时船舶分类，未来可通过改进分段和集成模型进一步提升性能。

Abstract: Accurate recognition of vessel types from Automatic Identification System
(AIS) tracks is essential for safety oversight and combating illegal,
unreported, and unregulated (IUU) activity. This paper presents a strait-scale,
machine-learning pipeline that classifies moving vessels using only AIS data.
We analyze eight days of historical AIS from the Danish Maritime Authority
covering the Bornholm Strait in the Baltic Sea (January 22-30, 2025). After
forward/backward filling voyage records, removing kinematic and geospatial
outliers, and segmenting per-MMSI tracks while excluding stationary periods
($\ge 1$ h), we derive 31 trajectory-level features spanning kinematics (e.g.,
SOG statistics), temporal, geospatial (Haversine distances, spans), and
ship-shape attributes computed from AIS A/B/C/D reference points (length,
width, aspect ratio, bridge-position ratio). To avoid leakage, we perform
grouped train/test splits by MMSI and use stratified 5-fold cross-validation.
Across five classes (cargo, tanker, passenger, high-speed craft, fishing;
N=1{,}910 trajectories; test=382), tree-based models dominate: a Random Forest
with SMOTE attains 92.15% accuracy (macro-precision 94.11%, macro-recall
92.51%, macro-F1 93.27%) on the held-out test set, while a tuned RF reaches
one-vs-rest ROC-AUC up to 0.9897. Feature-importance analysis highlights the
bridge-position ratio and maximum SOG as the most discriminative signals;
principal errors occur between cargo and tanker, reflecting similar transit
behavior. We demonstrate operational value by backfilling missing ship types on
unseen data and discuss improvements such as DBSCAN based trip segmentation and
gradient-boosted ensembles to handle frequent-stop ferries and further lift
performance. The results show that lightweight features over AIS trajectories
enable real-time vessel type classification in straits.

</details>


### [132] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出了一种基于分块的PCA-Net框架，用于高效处理高维PDE解场，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 高维PDE解场的主成分分析（PCA）计算开销大，需要更高效的方法。

Method: 将解场分解为小块，在每个块内应用PCA，并在降维空间中训练神经算子。研究了两种分块方法：局部到全局和局部到局部PCA。

Result: 分块PCA显著减少了计算复杂度，同时保持高精度，处理时间比全局PCA快3.7到4倍。

Conclusion: 分块PCA是一种高效的PDE算子学习技术，平衡了计算效率和重建精度。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [133] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种基于上下文优化（CoOp）的新框架，结合子空间表示学习和提示调优，提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD方法仅依赖softmax概率，忽略了VLM特征嵌入的判别潜力。

Method: 通过将ID特征投影到提示向量张成的子空间，无关特征投影到正交零空间，提升ID-OOD可分性。

Result: 实验验证了该方法的有效性。

Conclusion: 该框架在保持高ID分类精度的同时，显著提升了OOD检测性能。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [134] [Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis](https://arxiv.org/abs/2509.18112)
*Sheng Wong,Ravi Shankar,Beth Albert,Gabriel Davis Jones*

Main category: cs.LG

TL;DR: 该论文比较了基础模型（FMs）和大语言模型（LLMs）在电子胎儿监护（EFM/CTG）分析中的表现，发现微调的LLMs优于其他方法。


<details>
  <summary>Details</summary>
Motivation: CTG分析对胎儿健康评估至关重要，但现有方法依赖主观临床解读，导致诊断准确性不一致。

Method: 系统比较了时间序列FMs、LLMs和特定于CTG的架构，评估了500多份CTG记录。

Result: 微调的LLMs在性能上优于基础模型和领域特定方法。

Conclusion: LLMs为临床CTG解读提供了有前景的替代方案，并为未来产前护理AI发展奠定了基础。

Abstract: Foundation models (FMs) and large language models (LLMs) demonstrate
remarkable capabilities across diverse domains through training on massive
datasets. These models have demonstrated exceptional performance in healthcare
applications, yet their potential for electronic fetal monitoring
(EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating
fetal well-being, remains largely underexplored. Antepartum CTG interpretation
presents unique challenges due to the complex nature of fetal heart rate (FHR)
patterns and uterine activity, requiring sophisticated analysis of long
time-series data. The assessment of CTG is heavily based on subjective clinical
interpretation, often leading to variability in diagnostic accuracy and
deviation from timely pregnancy care. This study presents the first
comprehensive comparison of state-of-the-art AI approaches for automated
antepartum CTG analysis. We systematically compare time-series FMs and LLMs
against established CTG-specific architectures. Our evaluation encompasses over
500 CTG recordings of varying durations reflecting real-world clinical
recordings, providing robust performance benchmarks across different modelling
paradigms. Our results demonstrate that fine-tuned LLMs achieve superior
performance compared to both foundation models and domain-specific approaches,
offering a promising alternative pathway for clinical CTG interpretation. These
findings provide critical insights into the relative strengths of different AI
methodologies for fetal monitoring applications and establish a foundation for
future clinical AI development in prenatal care.

</details>


### [135] [A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU](https://arxiv.org/abs/2509.18114)
*Javed I. Khan an Henry Uwabor Moye*

Main category: cs.LG

TL;DR: 提出了一种基于DPU的框架，用于实时检测和缓解多节点张量并行推理中的负载不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 大型基于Transformer的语言模型在自回归推理中面临运行时效率挑战，尤其是解码阶段的负载不平衡会导致吞吐量下降和延迟峰值。

Method: 利用BlueField-3 DPU卸载监控任务，分析GPU遥测和节点间通信模式，为推理控制器和调度器提供可操作的反馈。

Result: 研究旨在识别多GPU执行中的负载不平衡及其对计算性能的影响，并评估是否可通过DPU网络进行跟踪和缓解。

Conclusion: DPU辅助框架有望提升大型语言模型推理的效率和稳定性。

Abstract: Autoregressive inference in large transformer-based language models (LLMs)
presents significant challenges for runtime efficiency, particularly during the
decode phase where load imbalance across GPU shards can cause throughput
degradation and latency spikes. A DPU-assisted framework leveraged by
BlueField-3 Data Processing Units can enable real-time detection and mitigation
of load imbalance in multi-node tensor-parallel inference. By offloading
monitoring tasks to the DPU and analyzing GPU telemetry and inter-node
communication patterns, the resulting system can provide actionable feedback to
inference controllers and schedulers. The goal of this study is three-fold i)
identify the reported skews/imbalances/pathological conditions that arise in
muti-GPU execution of a) LLM tensor computing (both during training and
inference), b) identify their impact on computational performance, and c) make
a critical assessment if those can be tracked for potential mitigation from a
DPU network.

</details>


### [136] [Towards Scalable and Structured Spatiotemporal Forecasting](https://arxiv.org/abs/2509.18115)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种新颖的空间平衡注意力块，用于时空预测，通过子图划分和注意力机制平衡局部与全局空间相关性。


<details>
  <summary>Details</summary>
Motivation: 解决时空预测中如何在遵循空间邻近性的同时捕捉全局相关性的问题。

Method: 将空间图划分为子图，使用子图内注意力学习局部相关性，通过子图间注意力实现全局消息传递。

Result: 在真实数据集上，性能提升达7.7%，且运行成本低。

Conclusion: 该模型兼具可扩展性和高效性，易于实现，适用于中大规模时空预测。

Abstract: In this paper, we propose a novel Spatial Balance Attention block for
spatiotemporal forecasting. To strike a balance between obeying spatial
proximity and capturing global correlation, we partition the spatial graph into
a set of subgraphs and instantiate Intra-subgraph Attention to learn local
spatial correlation within each subgraph; to capture the global spatial
correlation, we further aggregate the nodes to produce subgraph representations
and achieve message passing among the subgraphs via Inter-subgraph Attention.
Building on the proposed Spatial Balance Attention block, we develop a
multiscale spatiotemporal forecasting model by progressively increasing the
subgraph scales. The resulting model is both scalable and able to produce
structured spatial correlation, and meanwhile, it is easy to implement. We
evaluate its efficacy and efficiency against the existing models on real-world
spatiotemporal datasets from medium to large sizes. The experimental results
show that it can achieve performance improvements up to 7.7% over the baseline
methods at low running costs.

</details>


### [137] [Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization](https://arxiv.org/abs/2509.18116)
*Nathan Egbuna,Saatvik Gaur,Sunishchal Dev,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: ALS通过离线计算单一向量优化推理成本，显著提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决测试时优化方法计算成本高的问题，使其适用于生产环境。

Method: 计算成功与失败生成隐藏状态的平均差异，作为单一向量应用于推理。

Result: 在GSM8K和MATH-500基准上，效率提升2-5倍，准确性优于基线。

Conclusion: ALS证明离线优化可显著提升推理效率，适用于实际部署。

Abstract: Test-time optimization remains impractical at scale due to prohibitive
inference costs\textemdash techniques like iterative refinement and multi-step
verification can require $10$--$100\times$ more compute per query than standard
decoding. Latent space test-time optimization methods like LatentSeek offer a
more direct approach by steering hidden representations, but still demand
expensive per-query optimization loops with multiple backward passes. We
propose Amortized Latent Steering (ALS), which collapses this iterative
optimization into a single offline-computed vector applied at constant cost
during inference. ALS computes the mean difference between hidden states from
successful versus unsuccessful generations, then uses this direction to
calibrate the model's hidden representations: when decoding drifts away from
the success manifold, ALS nudges activations back toward it. Across GSM8K and
MATH-$500$ benchmarks, ALS achieves $2$--$5\times$ speedup over iterative
methods while matching or surpassing greedy Chain-of-Thought (CoT) and
Self-Consistency baselines, yielding up to 101\% improvement in
efficiency--accuracy trade-off. These results show that much of latent
optimization's benefit can be captured offline, making sophisticated reasoning
techniques viable for production deployment. Code is available
at~\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}

</details>


### [138] [Robust and continuous machine learning of usage habits to adapt digital interfaces to user needs](https://arxiv.org/abs/2509.18117)
*Eric Petit,Denis Chêne*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的数字界面设计方法，通过贝叶斯统计建模用户浏览行为，动态适应用户习惯。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过个性化建模提升用户体验，而非依赖群体偏好。

Method: 采用在线增量学习的贝叶斯统计方法，适应数据少和环境变化的情况。

Result: 模拟实验表明，该方法在静态和非静态环境中均有效，能生成任务模型并保留先验知识。

Conclusion: 研究为自适应系统提供了新思路，优化用户导航和操作体验。

Abstract: The paper presents a machine learning approach to design digital interfaces
that can dynamically adapt to different users and usage strategies. The
algorithm uses Bayesian statistics to model users' browsing behavior, focusing
on their habits rather than group preferences. It is distinguished by its
online incremental learning, allowing reliable predictions even with little
data and in the case of a changing environment. This inference method generates
a task model, providing a graphical representation of navigation with the usage
statistics of the current user. The algorithm learns new tasks while preserving
prior knowledge. The theoretical framework is described, and simulations show
the effectiveness of the approach in stationary and non-stationary
environments. In conclusion, this research paves the way for adaptive systems
that improve the user experience by helping them to better navigate and act on
their interface.

</details>


### [139] [Decentor-V: Lightweight ML Training on Low-Power RISC-V Edge Devices](https://arxiv.org/abs/2509.18118)
*Marcelo Ribeiro,Diogo Costa,Gonçalo Moreira,Sandro Pinto,Tiago Gomes*

Main category: cs.LG

TL;DR: 论文提出了一种针对RISC-V MCU的8位量化L-SGD方法，显著减少了内存使用和训练时间，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 现代IoT设备缺乏GPU或专用加速器，导致本地训练困难，依赖云端服务引发隐私和连接性问题。联邦学习（FL）虽能解决这些问题，但需要高效优化算法。

Method: 扩展L-SGD至RISC-V MCU，并引入8位量化版本，评估其在Arm和RISC-V平台上的性能。

Result: 8位量化L-SGD在RISC-V上实现了内存使用减少4倍，训练速度提升2.2倍，且精度损失可忽略。

Conclusion: 量化L-SGD为RISC-V MCU提供了一种高效的本地训练解决方案，适用于资源受限的IoT设备。

Abstract: Modern IoT devices increasingly rely on machine learning solutions to process
data locally. However, the lack of graphics processing units (GPUs) or
dedicated accelerators on most platforms makes on-device training largely
infeasible, often requiring cloud-based services to perform this task. This
procedure often raises privacy-related concerns, and creates dependency on
reliable and always-on connectivity. Federated Learning (FL) is a new trend
that addresses these issues by enabling decentralized and collaborative
training directly on devices, but it requires highly efficient optimization
algorithms. L-SGD, a lightweight variant of stochastic gradient descent, has
enabled neural network training on Arm Cortex-M Microcontroller Units (MCUs).
This work extends L-SGD to RISC-V-based MCUs, an open and emerging architecture
that still lacks robust support for on-device training. L-SGD was evaluated on
both Arm and RISC-V platforms using 32-bit floating-point arithmetic,
highlighting the performance impact of the absence of Floating-Point Units
(FPUs) in RISC-V MCUs. To mitigate these limitations, we introduce an 8-bit
quantized version of L-SGD for RISC-V, which achieves nearly 4x reduction in
memory usage and a 2.2x speedup in training time, with negligible accuracy
degradation.

</details>


### [140] [MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents](https://arxiv.org/abs/2509.18119)
*Yifan Xu,Xiao Liu,Xinghan Liu,Jiaqi Fu,Hanchen Zhang,Bohao Jing,Shudan Zhang,Yuting Wang,Wenyi Zhao,Yuxiao Dong*

Main category: cs.LG

TL;DR: MOBILERL是一个在线强化学习框架，通过ADAGRPO算法和任务难度适应策略提升移动GUI代理的性能，在多个任务中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 开发高效的移动GUI代理面临任务难度分布不均和大规模环境采样的效率问题。

Method: 提出ADAGRPO算法，结合难度自适应正回放、失败课程过滤和最短路径奖励调整策略。

Result: MOBILERL-9B模型在AndroidWorld和AndroidLab上分别达到75.8%和46.8%的成功率。

Conclusion: MOBILERL框架有效稳定训练并提升性能，已应用于AutoGLM产品并开源。

Abstract: Building general-purpose graphical user interface (GUI) agents has become
increasingly promising with the progress in vision language models. However,
developing effective mobile GUI agents with reinforcement learning (RL) remains
challenging due to the heavy-tailed distribution of task difficulty and the
inefficiency of large-scale environment sampling. We present an online agentic
reinforcement learning framework MOBILERL to enhance GUI agents in mobile
environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)
algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and
failure curriculum filtering to adapt the model to different task difficulties.
We introduce the shortest path reward adjustment strategy to reshape rewards
concerning the task length in multi-turn agentic tasks. Those strategies
jointly stabilize RL training, improve sample efficiency, and generate strong
performance across diverse mobile apps and tasks. We apply MOBILERL to two open
models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B
model achieves state-of-the-art results in terms of success rates on both
AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted
in the AutoGLM products, and also open-sourced at
https://github.com/THUDM/MobileRL.

</details>


### [141] [A Coopetitive-Compatible Data Generation Framework for Cross-silo Federated Learning](https://arxiv.org/abs/2509.18120)
*Thanh Linh Nguyen,Quoc-Viet Pham*

Main category: cs.LG

TL;DR: CoCoGen是一个结合生成式AI和潜在博弈论的框架，用于解决跨组织联邦学习中的统计异质性和经济竞争问题。


<details>
  <summary>Details</summary>
Motivation: 研究跨组织联邦学习中因经济竞争和统计异质性导致的参与意愿低和社会福利未优化问题。

Method: 通过学习性能和效用公式建模竞争与异质性，将每轮训练视为加权潜在博弈，并设计生成式AI数据生成策略。

Result: 在Fashion-MNIST数据集上，CoCoGen在不同异质性和竞争水平下均优于基线方法。

Conclusion: CoCoGen能有效优化社会福利，提升组织参与联邦学习的积极性。

Abstract: Cross-silo federated learning (CFL) enables organizations (e.g., hospitals or
banks) to collaboratively train artificial intelligence (AI) models while
preserving data privacy by keeping data local. While prior work has primarily
addressed statistical heterogeneity across organizations, a critical challenge
arises from economic competition, where organizations may act as market rivals,
making them hesitant to participate in joint training due to potential utility
loss (i.e., reduced net benefit). Furthermore, the combined effects of
statistical heterogeneity and inter-organizational competition on
organizational behavior and system-wide social welfare remain underexplored. In
this paper, we propose CoCoGen, a coopetitive-compatible data generation
framework, leveraging generative AI (GenAI) and potential game theory to model,
analyze, and optimize collaborative learning under heterogeneous and
competitive settings. Specifically, CoCoGen characterizes competition and
statistical heterogeneity through learning performance and utility-based
formulations and models each training round as a weighted potential game. We
then derive GenAI-based data generation strategies that maximize social
welfare. Experimental results on the Fashion-MNIST dataset reveal how varying
heterogeneity and competition levels affect organizational behavior and
demonstrate that CoCoGen consistently outperforms baseline methods.

</details>


### [142] [Prediction of Coffee Ratings Based On Influential Attributes Using SelectKBest and Optimal Hyperparameters](https://arxiv.org/abs/2509.18124)
*Edmund Agyemang,Lawrence Agbota,Vincent Agbenyeavu,Peggy Akabuah,Bismark Bimpong,Christopher Attafuah*

Main category: cs.LG

TL;DR: 该研究应用监督机器学习算法，结合用户评论中的文本和数值属性预测咖啡评分，发现集成方法和多层感知器在性能上优于简单分类器。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过机器学习预测咖啡评分，以补充传统专业品鉴方法。

Method: 通过数据预处理（文本清洗、TF-IDF特征提取和SelectKBest选择），训练并评估六种模型（决策树、KNN、MLP、随机森林、Extra Trees和XGBoost）。

Result: 集成方法（Extra Trees、随机森林、XGBoost）和多层感知器在F1分数、G-mean和AUC等指标上表现更优。

Conclusion: 研究强调了特征选择和超参数调优的重要性，为感官产品评估提供了数据驱动的方法。

Abstract: This study explores the application of supervised machine learning algorithms
to predict coffee ratings based on a combination of influential textual and
numerical attributes extracted from user reviews. Through careful data
preprocessing including text cleaning, feature extraction using TF-IDF, and
selection with SelectKBest, the study identifies key factors contributing to
coffee quality assessments. Six models (Decision Tree, KNearest Neighbors,
Multi-layer Perceptron, Random Forest, Extra Trees, and XGBoost) were trained
and evaluated using optimized hyperparameters. Model performance was assessed
primarily using F1-score, Gmean, and AUC metrics. Results demonstrate that
ensemble methods (Extra Trees, Random Forest, and XGBoost), as well as
Multi-layer Perceptron, consistently outperform simpler classifiers (Decision
Trees and K-Nearest Neighbors) in terms of evaluation metrics such as F1
scores, G-mean and AUC. The findings highlight the essence of rigorous feature
selection and hyperparameter tuning in building robust predictive systems for
sensory product evaluation, offering a data driven approach to complement
traditional coffee cupping by expertise of trained professionals.

</details>


### [143] [NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment](https://arxiv.org/abs/2509.18125)
*Harsha Koduri*

Main category: cs.LG

TL;DR: NurseSchedRL是一种基于强化学习的护士排班框架，通过结构化状态编码和约束动作掩码优化资源分配，提升效率和减少疲劳。


<details>
  <summary>Details</summary>
Motivation: 医疗系统需要高效分配有限的护理资源，同时考虑技能差异、患者病情、员工疲劳和护理连续性，传统方法难以满足这些动态多约束需求。

Method: 提出NurseSchedRL框架，结合PPO算法和可行性掩码，动态适应患者到达和护士可用性。

Result: 在模拟实验中，NurseSchedRL比基线方法提高了排班效率，更好地匹配技能与需求，并减少疲劳。

Conclusion: 强化学习在复杂医疗资源管理中具有潜力。

Abstract: Healthcare systems face increasing pressure to allocate limited nursing
resources efficiently while accounting for skill heterogeneity, patient acuity,
staff fatigue, and continuity of care. Traditional optimization and heuristic
scheduling methods struggle to capture these dynamic, multi-constraint
environments. I propose NurseSchedRL, a reinforcement learning framework for
nurse-patient assignment that integrates structured state encoding, constrained
action masking, and attention-based representations of skills, fatigue, and
geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with
feasibility masks to ensure assignments respect real-world constraints, while
dynamically adapting to patient arrivals and varying nurse availability. In
simulation with realistic nurse and patient data, NurseSchedRL achieves
improved scheduling efficiency, better alignment of skills to patient needs,
and reduced fatigue compared to baseline heuristic and unconstrained RL
approaches. These results highlight the potential of reinforcement learning for
decision support in complex, high-stakes healthcare workforce management.

</details>


### [144] [Anomaly Detection in Electric Vehicle Charging Stations Using Federated Learning](https://arxiv.org/abs/2509.18126)
*Bishal K C,Amr Hilal,Pawan Thapa*

Main category: cs.LG

TL;DR: 论文研究了联邦学习在电动汽车充电站异常检测中的应用，分析了FedAvg和FedAvgM在异构系统和数据下的表现，发现FedAvgM在非独立同分布数据中表现更优。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车基础设施的扩展，保护充电站免受网络威胁变得至关重要。集中式入侵检测系统存在隐私问题，联邦学习成为有前景的替代方案。

Method: 通过实验评估FedAvg和FedAvgM在异构系统和数据下的性能，用于异常检测。

Result: FedAvg在独立同分布数据中表现优于集中式模型，但在非独立同分布数据中性能下降；FedAvgM在异构环境中表现更稳定，检测精度更高。

Conclusion: 联邦学习能有效处理异构性，FedAvgM是保护隐私且鲁棒的电动汽车充电站安全解决方案。

Abstract: Federated Learning (FL) is a decentralized training framework widely used in
IoT ecosystems that preserves privacy by keeping raw data local, making it
ideal for IoT-enabled cyber-physical systems with sensing and communication
like Smart Grids (SGs), Connected and Automated Vehicles (CAV), and Electric
Vehicle Charging Stations (EVCS). With the rapid expansion of electric vehicle
infrastructure, securing these IoT-based charging stations against cyber
threats has become critical. Centralized Intrusion Detection Systems (IDS)
raise privacy concerns due to sensitive network and user data, making FL a
promising alternative. However, current FL-based IDS evaluations overlook
practical challenges such as system heterogeneity and non-IID data. To address
these challenges, we conducted experiments to evaluate the performance of
federated learning for anomaly detection in EV charging stations under system
and data heterogeneity. We used FedAvg and FedAvgM, widely studied optimization
approaches, to analyze their effectiveness in anomaly detection. Under IID
settings, FedAvg achieves superior performance to centralized models using the
same neural network. However, performance degrades with non-IID data and system
heterogeneity. FedAvgM consistently outperforms FedAvg in heterogeneous
settings, showing better convergence and higher anomaly detection accuracy. Our
results demonstrate that FL can handle heterogeneity in IoT-based EVCS without
significant performance loss, with FedAvgM as a promising solution for robust,
privacy-preserving EVCS security.

</details>


### [145] [Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework](https://arxiv.org/abs/2509.18127)
*Jiaqi Weng,Han Zheng,Hanyu Zhang,Qinqin He,Jialing Tao,Hui Xue,Zhixuan Chu,Xiting Wang*

Main category: cs.LG

TL;DR: Safe-SAIL框架通过解释稀疏自编码器（SAE）特征，提升大语言模型（LLM）在安全领域的机制理解。


<details>
  <summary>Details</summary>
Motivation: 现有安全研究多关注模型输出或特定任务，难以应对更广泛的安全风险。SAE虽有助于解释模型行为，但缺乏对细粒度安全相关概念的解释。

Method: 提出Safe-SAIL框架，系统识别具有最佳概念解释性的SAE，解释安全相关神经元，并引入高效策略扩展解释过程。

Result: 将发布包含SAE检查点和可读神经元解释的工具包，支持安全风险的实证分析。

Conclusion: Safe-SAIL为LLM安全研究提供了新工具和方法，有助于更深入理解高风险行为。

Abstract: Increasing deployment of large language models (LLMs) in real-world
applications raises significant safety concerns. Most existing safety research
focuses on evaluating LLM outputs or specific safety tasks, limiting their
ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)
facilitate interpretability research to clarify model behavior by explaining
single-meaning atomic features decomposed from entangled signals. jHowever,
prior applications on SAEs do not interpret features with fine-grained
safety-related con- cepts, thus inadequately addressing safety-critical
behaviors, such as generating toxic responses and violating safety regu-
lations. For rigorous safety analysis, we must extract a rich and diverse set
of safety-relevant features that effectively capture these high-risk behaviors,
yet face two challenges: identifying SAEs with the greatest potential for
generating safety concept-specific neurons, and the prohibitively high cost of
detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a
framework for interpreting SAE features within LLMs to advance mechanistic
understanding in safety domains. Our approach systematically identifies SAE
with best concept-specific interpretability, explains safety-related neurons,
and introduces efficient strategies to scale up the in- terpretation process.
We will release a comprehensive toolkit including SAE checkpoints and
human-readable neuron ex- planations, which supports empirical analysis of
safety risks to promote research on LLM safety.

</details>


### [146] [Accounting for Uncertainty in Machine Learning Surrogates: A Gauss-Hermite Quadrature Approach to Reliability Analysis](https://arxiv.org/abs/2509.18128)
*Amirreza Tootchi,Xiaoping Du*

Main category: cs.LG

TL;DR: 提出一种高斯-埃尔米特积分方法，用于解耦嵌套不确定性，提高可靠性分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在可靠性分析中引入近似误差导致的不确定性，可能影响预测准确性。

Method: 使用一阶和二阶可靠性方法评估条件失效概率，并通过高斯-埃尔米特积分整合认知不确定性。

Result: 三个示例表明，该方法在保持计算效率的同时，比忽略模型不确定性的传统方法更可靠。

Conclusion: 高斯-埃尔米特积分方法能有效解耦不确定性，提供更可信的可靠性预测。

Abstract: Machine learning surrogates are increasingly employed to replace expensive
computational models for physics-based reliability analysis. However, their use
introduces epistemic uncertainty from model approximation errors, which couples
with aleatory uncertainty in model inputs, potentially compromising the
accuracy of reliability predictions. This study proposes a Gauss-Hermite
quadrature approach to decouple these nested uncertainties and enable more
accurate reliability analysis. The method evaluates conditional failure
probabilities under aleatory uncertainty using First and Second Order
Reliability Methods and then integrates these probabilities across realizations
of epistemic uncertainty. Three examples demonstrate that the proposed approach
maintains computational efficiency while yielding more trustworthy predictions
than traditional methods that ignore model uncertainty.

</details>


### [147] [Research on Metro Transportation Flow Prediction Based on the STL-GRU Combined Model](https://arxiv.org/abs/2509.18130)
*Zijie Zhou,Huichen Ma*

Main category: cs.LG

TL;DR: 本文提出了一种结合STL和GRU的地铁换乘客流预测模型，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 优化地铁运营计划和提高运输效率需要准确的换乘客流预测。

Method: 使用STL分解客流时间序列，结合GRU模型进行预测，并通过3σ原则处理异常值。

Result: STL-GRU模型在工作日（除周五）、周五和休息日的预测误差显著降低。

Conclusion: STL-GRU模型在换乘客流预测中表现优于其他方法，具有实际应用价值。

Abstract: In the metro intelligent transportation system, accurate transfer passenger
flow prediction is a key link in optimizing operation plans and improving
transportation efficiency. To further improve the theory of metro internal
transfer passenger flow prediction and provide more reliable support for
intelligent operation decisions, this paper innovatively proposes a metro
transfer passenger flow prediction model that integrates the Seasonal and Trend
decomposition using Loess (STL) method and Gated Recurrent Unit (GRU).In
practical application, the model first relies on the deep learning library
Keras to complete the construction and training of the GRU model, laying the
foundation for subsequent prediction; then preprocesses the original metro card
swiping data, uses the graph-based depth-first search algorithm to identify
passengers' travel paths, and further constructs the transfer passenger flow
time series; subsequently adopts the STL time series decomposition algorithm to
decompose the constructed transfer passenger flow time series into trend
component, periodic component and residual component, and uses the 3{\sigma}
principle to eliminate and fill the outliers in the residual component, and
finally completes the transfer passenger flow prediction.Taking the transfer
passenger flow data of a certain metro station as the research sample, the
validity of the model is verified. The results show that compared with Long
Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the combined model of
STL time series decomposition method and Long Short-Term Memory (STL-LSTM), the
STL-GRU combined prediction model significantly improves the prediction
accuracy of transfer passenger flow on weekdays (excluding Fridays), Fridays
and rest days, with the mean absolute percentage error (MAPE) of the prediction
results reduced by at least 2.3, 1.36 and 6.42 percentage points respectively.

</details>


### [148] [Two ways to knowledge?](https://arxiv.org/abs/2509.18131)
*Jean-Michel Tucny,Abhisek Ganguly,Santosh Ansumali,Sauro Succi*

Main category: cs.LG

TL;DR: 论文探讨了基于Transformer的机器学习在物理问题中的应用，发现其权重矩阵呈现随机性，与物理问题的结构无直接关联，表明机器学习与科学方法可能是互补的知识获取路径。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索机器学习（尤其是Transformer）在解决物理问题时的表现，以及其权重矩阵是否与物理问题的结构和数学特性直接相关。

Method: 通过分析Transformer在解决两个典型物理应用中的权重矩阵，观察其是否具有可解释性。

Result: 发现权重矩阵呈现随机性，与物理问题的结构无直接关联，且与路径积分技术的类比也无法完全解释这种随机性。

Conclusion: 机器学习与科学方法可能是互补的知识获取路径，但直接的可解释性仍难以实现，强调了缺乏洞察力时获取知识的潜在风险。

Abstract: It is shown that the weight matrices of transformer-based machine learning
applications to the solution of two representative physical applications show a
random-like character which bears no directly recognizable link to the physical
and mathematical structure of the physical problem under study. This suggests
that machine learning and the scientific method may represent two distinct and
potentially complementary paths to knowledge, even though a strict notion of
explainability in terms of direct correspondence between network parameters and
physical structures may remain out of reach. It is also observed that drawing a
parallel between transformer operation and (generalized) path-integration
techniques may account for the random-like nature of the weights, but still
does not resolve the tension with explainability. We conclude with some general
comments on the hazards of gleaning knowledge without the benefit of Insight.

</details>


### [149] [Self-Evolving LLMs via Continual Instruction Tuning](https://arxiv.org/abs/2509.18133)
*Le Huang,Jiazheng Kang,Cheng Hou,Zhe Zhao,Zhenxiang Yan,Chuan Shi,Ting Bai*

Main category: cs.LG

TL;DR: MoE-CL是一种参数高效的对抗性混合专家框架，用于工业规模的自进化持续指令调优，通过双专家设计解决灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在工业环境中，大型语言模型需要持续学习以适应动态任务，但现有方法存在灾难性遗忘问题。

Method: MoE-CL采用双专家设计：专用LoRA专家保留任务特定知识，共享LoRA专家实现跨任务迁移，并通过GAN任务感知判别器过滤噪声。

Result: 在MTL5和Tencent3基准测试中表现优异，Tencent视频平台A/B测试中减少15.3%人工审核成本。

Conclusion: MoE-CL适用于需要持续适应和稳定迁移的大规模工业部署。

Abstract: In real-world industrial settings, large language models (LLMs) must learn
continually to keep pace with diverse and evolving tasks, requiring
self-evolution to refine knowledge under dynamic data distributions. However,
existing continual learning (CL) approaches, such as replay and parameter
isolation, often suffer from catastrophic forgetting: training on new tasks
degrades performance on earlier ones by overfitting to the new distribution and
weakening generalization.We propose MoE-CL, a parameter-efficient adversarial
mixture-of-experts framework for industrial-scale, self-evolving continual
instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated
LoRA expert per task to preserve task-specific knowledge via parameter
independence, mitigating forgetting; and (2) a shared LoRA expert to enable
cross-task transfer. To prevent transferring task-irrelevant noise through the
shared pathway, we integrate a task-aware discriminator within a GAN. The
discriminator encourages the shared expert to pass only task-aligned
information during sequential training. Through adversarial learning, the
shared expert acquires generalized representations that mimic the
discriminator, while dedicated experts retain task-specific details, balancing
knowledge retention and cross-task generalization and thereby supporting
self-evolution.Extensive experiments on the public MTL5 benchmark and an
industrial Tencent3 benchmark validate the effectiveness of MoE-CL for
continual instruction tuning. In real-world A/B testing for content compliance
review on the Tencent Video platform, MoE-CL reduced manual review costs by
15.3%. These results demonstrate that MoE-CL is practical for large-scale
industrial deployment where continual adaptation and stable transfer are
critical.

</details>


### [150] [A Weighted Gradient Tracking Privacy-Preserving Method for Distributed Optimization](https://arxiv.org/abs/2509.18134)
*Furan Xie,Bing Liu,Li Chai*

Main category: cs.LG

TL;DR: 论文提出了一种加权梯度跟踪的分布式隐私保护算法，解决了梯度跟踪中的隐私泄露风险，并在不同步长下证明了算法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究分布式优化中的隐私保护问题，揭示梯度跟踪技术潜在的隐私泄露风险。

Method: 提出加权梯度跟踪算法，利用衰减权重因子消除隐私泄露风险，并分析其在时变异构步长下的收敛性。

Result: 算法在温和假设下精确收敛到最优解，数值模拟验证了其有效性。

Conclusion: 该算法有效解决了隐私泄露问题，适用于分布式估计和神经网络训练等场景。

Abstract: This paper investigates the privacy-preserving distributed optimization
problem, aiming to protect agents' private information from potential attackers
during the optimization process. Gradient tracking, an advanced technique for
improving the convergence rate in distributed optimization, has been applied to
most first-order algorithms in recent years. We first reveal the inherent
privacy leakage risk associated with gradient tracking. Building upon this
insight, we propose a weighted gradient tracking distributed privacy-preserving
algorithm, eliminating the privacy leakage risk in gradient tracking using
decaying weight factors. Then, we characterize the convergence of the proposed
algorithm under time-varying heterogeneous step sizes. We prove the proposed
algorithm converges precisely to the optimal solution under mild assumptions.
Finally, numerical simulations validate the algorithm's effectiveness through a
classical distributed estimation problem and the distributed training of a
convolutional neural network.

</details>


### [151] [SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting](https://arxiv.org/abs/2509.18135)
*Shaoxun Wang,Xingjun Zhang,Qianyang Li,Jiawei Cao,Zhendong Tan*

Main category: cs.LG

TL;DR: 提出了一种新的静态-动态图融合网络（SDGF），通过双路径图结构学习方法捕捉多尺度序列间相关性，显著提升了多元时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉多尺度序列间复杂且动态变化的关联关系，限制了多元时间序列预测的准确性。

Method: SDGF结合静态图（基于先验知识）和动态图（通过多级小波分解自适应学习），并利用注意力门控模块融合信息，最后通过多核扩张卷积网络加深时间模式理解。

Result: 在多个真实世界基准数据集上的实验验证了模型的有效性。

Conclusion: SDGF能够有效捕捉多尺度序列间相关性，显著提升预测性能。

Abstract: Inter-series correlations are crucial for accurate multivariate time series
forecasting, yet these relationships often exhibit complex dynamics across
different temporal scales. Existing methods are limited in modeling these
multi-scale dependencies and struggle to capture their intricate and evolving
nature. To address this challenge, this paper proposes a novel Static-Dynamic
Graph Fusion network (SDGF), whose core lies in capturing multi-scale
inter-series correlations through a dual-path graph structure learning
approach. Specifically, the model utilizes a static graph based on prior
knowledge to anchor long-term, stable dependencies, while concurrently
employing Multi-level Wavelet Decomposition to extract multi-scale features for
constructing an adaptively learned dynamic graph to capture associations at
different scales. We design an attention-gated module to fuse these two
complementary sources of information intelligently, and a multi-kernel dilated
convolutional network is then used to deepen the understanding of temporal
patterns. Comprehensive experiments on multiple widely used real-world
benchmark datasets demonstrate the effectiveness of our proposed model.

</details>


### [152] [From Parameters to Performance: A Data-Driven Study on LLM Structure and Development](https://arxiv.org/abs/2509.18136)
*Suqing Wang,Zuchao Li,Luohe Shi,Bo Du,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.LG

TL;DR: 论文通过大规模数据集和系统分析，研究了大型语言模型（LLM）结构配置与性能的关系，并提供了数据驱动的优化建议。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在规模和能力上迅速增长，但关于结构配置如何影响性能的系统性研究仍然缺乏。

Method: 利用包含多种开源LLM结构及其性能的大规模数据集，进行数据挖掘驱动的分析，并结合机制解释技术验证结果。

Result: 研究量化了结构配置与性能的关系，并提供了优化LLM的见解。

Conclusion: 工作旨在指导未来模型的针对性开发和应用，并公开了数据集。

Abstract: Large language models (LLMs) have achieved remarkable success across various
domains, driving significant technological advancements and innovations.
Despite the rapid growth in model scale and capability, systematic, data-driven
research on how structural configurations affect performance remains scarce. To
address this gap, we present a large-scale dataset encompassing diverse
open-source LLM structures and their performance across multiple benchmarks.
Leveraging this dataset, we conduct a systematic, data mining-driven analysis
to validate and quantify the relationship between structural configurations and
performance. Our study begins with a review of the historical development of
LLMs and an exploration of potential future trends. We then analyze how various
structural choices impact performance across benchmarks and further corroborate
our findings using mechanistic interpretability techniques. By providing
data-driven insights into LLM optimization, our work aims to guide the targeted
development and application of future models. We will release our dataset at
https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset

</details>


### [153] [LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods](https://arxiv.org/abs/2509.18137)
*Shaoheng Wang,Yao Lu,Yuqi Li,Yaxin Gao,Jiaqi Nie,Shanqing Yu,Yingli Tian,Qi Xuan*

Main category: cs.LG

TL;DR: 提出了一个名为LoRALib的统一基准，用于标准化LoRA-MoE方法的比较，并通过实验验证了LoRAMoE的优越性。


<details>
  <summary>Details</summary>
Motivation: LoRA在跨任务泛化能力上表现不足，现有LoRA-MoE方法缺乏统一标准，难以公平比较。

Method: 标准化40个下游任务的数据集，使用相同超参数微调，生成680个LoRA模块，并在3种LoRA-MoE方法上进行实验。

Result: LoRAMoE表现最佳，优先选择与目标任务相关的LoRA可进一步提升MoE性能。

Conclusion: LoRALib为未来研究提供了统一基准，实验结果为LoRA-MoE方法的优化提供了参考。

Abstract: As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation
(LoRA) can save significant costs in storage and computing, but its strong
adaptability to a single task is often accompanied by insufficient cross-task
generalization capabilities. To improve this, existing work combines LoRA with
mixture-of-experts (MoE) to enhance the model's adaptability through expert
modules and routing mechanisms. However, existing LoRA-MoE methods lack unified
standards in models, datasets, hyperparameters, and evaluation methods, making
it difficult to conduct fair comparisons between different methods. To this
end, we proposed a unified benchmark named LoRALib. Specifically, we
standardized datasets from $40$ downstream tasks into a unified format,
fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules
across $17$ model architectures. Based on this LoRA library, we conduct
large-scale experiments on $3$ representative LoRA-MoE methods and different
LoRA selection mechanisms using the open-sourced testing tool OpenCompass.
Extensive experiments show that LoRAMoE performs best, and that prioritizing
LoRAs relevant to the target task can further improve the performance of MoE.
We hope these findings will inspire future work. Our datasets and LoRA library
are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset
and https://huggingface.co/YaoLuzjut/models.

</details>


### [154] [Rank-Induced PL Mirror Descent: A Rank-Faithful Second-Order Algorithm for Sleeping Experts](https://arxiv.org/abs/2509.18138)
*Tiantian Zhang*

Main category: cs.LG

TL;DR: RIPLM是一种新算法，通过利用排名基准与分布基准的结构等价性，直接在排名诱导的Plackett-Luce参数化中更新，保持算法分布始终在排名诱导分布类中。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在专家身份上操作的局限性，确保算法分布与排名基准的等价性。

Method: 利用排名诱导的Plackett-Luce参数化直接更新，保持分布类不变。

Result: RIPLM是首个在睡眠专家设置中既保持排名忠实性又具有方差适应性的算法。

Conclusion: RIPLM通过结构等价性和参数化更新，实现了高效且适应性强的排名优化。

Abstract: We introduce a new algorithm, \emph{Rank-Induced Plackett--Luce Mirror
Descent (RIPLM)}, which leverages the structural equivalence between the
\emph{rank benchmark} and the \emph{distributional benchmark} established in
\citet{BergamOzcanHsu2022}. Unlike prior approaches that operate on expert
identities, RIPLM updates directly in the \emph{rank-induced Plackett--Luce
(PL)} parameterization. This ensures that the algorithm's played distributions
remain within the class of rank-induced distributions at every round,
preserving the equivalence with the rank benchmark. To our knowledge, RIPLM is
the first algorithm that is both (i) \emph{rank-faithful} and (ii)
\emph{variance-adaptive} in the sleeping experts setting.

</details>


### [155] [Comparative Analysis of FOLD-SE vs. FOLD-R++ in Binary Classification and XGBoost in Multi-Category Classification](https://arxiv.org/abs/2509.18139)
*Akshay Murthy,Shawn Sebastian,Manil Shangle,Huaduo Wang,Sopam Dasgupta,Gopal Gupta*

Main category: cs.LG

TL;DR: FOLD-SE在二进制和多类别分类中表现优于FOLD-R++和XGBoost，同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决机器学习模型在准确性和可解释性之间的权衡问题，探索规则基算法如FOLD-SE的潜力。

Method: 比较FOLD-SE与FOLD-R++在二进制分类中的表现，以及FOLD-SE与XGBoost在多类别分类中的表现，使用准确性、F1分数和处理时间作为评估指标。

Result: FOLD-SE在二进制分类中生成更少的规则但略逊于FOLD-R++的准确性；在多类别分类中优于XGBoost且更高效。

Conclusion: FOLD-SE在准确性和可解释性之间取得了平衡，是分类任务中的可行替代方案。

Abstract: Recently, the demand for Machine Learning (ML) models that can balance
accuracy, efficiency, and interpreability has grown significantly.
Traditionally, there has been a tradeoff between accuracy and explainability in
predictive models, with models such as Neural Networks achieving high accuracy
on complex datasets while sacrificing internal transparency. As such, new
rule-based algorithms such as FOLD-SE have been developed that provide tangible
justification for predictions in the form of interpretable rule sets. The
primary objective of this study was to compare FOLD-SE and FOLD-R++, both
rule-based classifiers, in binary classification and evaluate how FOLD-SE
performs against XGBoost, a widely used ensemble classifier, when applied to
multi-category classification. We hypothesized that because FOLD-SE can
generate a condensed rule set in a more explainable manner, it would lose
upwards of an average of 3 percent in accuracy and F1 score when compared with
XGBoost and FOLD-R++ in multiclass and binary classification, respectively. The
research used data collections for classification, with accuracy, F1 scores,
and processing time as the primary performance measures. Outcomes show that
FOLD-SE is superior to FOLD-R++ in terms of binary classification by offering
fewer rules but losing a minor percentage of accuracy and efficiency in
processing time; in tasks that involve multi-category classifications, FOLD-SE
is more precise and far more efficient compared to XGBoost, in addition to
generating a comprehensible rule set. The results point out that FOLD-SE is a
better choice for both binary tasks and classifications with multiple
categories. Therefore, these results demonstrate that rule-based approaches
like FOLD-SE can bridge the gap between explainability and performance,
highlighting their potential as viable alternatives to black-box models in
diverse classification tasks.

</details>


### [156] [A Machine Learning Framework for Pathway-Driven Therapeutic Target Discovery in Metabolic Disorders](https://arxiv.org/abs/2509.18140)
*Iram Wajahat,Amritpal Singh,Fazel Keshtkar,Syed Ahmad Chan Bukhari*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习与通路映射的新框架，用于预测T2DM高风险人群并发现潜在治疗靶点。


<details>
  <summary>Details</summary>
Motivation: 代谢疾病（如T2DM）对特定人群（如Pima印第安人）影响严重，需开发早期检测和精准干预方法。

Method: 使用逻辑回归和t检验分析Pima印第安人数据集，结合通路映射策略关联预测因子与关键信号通路。

Result: 模型准确率为78.43%，揭示了胰岛素信号、AMPK和PPAR通路，并提出多种潜在治疗策略。

Conclusion: 该框架为代谢疾病的精准医学提供了可解释且可扩展的解决方案。

Abstract: Metabolic disorders, particularly type 2 diabetes mellitus (T2DM), represent
a significant global health burden, disproportionately impacting genetically
predisposed populations such as the Pima Indians (a Native American tribe from
south central Arizona). This study introduces a novel machine learning (ML)
framework that integrates predictive modeling with gene-agnostic pathway
mapping to identify high-risk individuals and uncover potential therapeutic
targets. Using the Pima Indian dataset, logistic regression and t-tests were
applied to identify key predictors of T2DM, yielding an overall model accuracy
of 78.43%. To bridge predictive analytics with biological relevance, we
developed a pathway mapping strategy that links identified predictors to
critical signaling networks, including insulin signaling, AMPK, and PPAR
pathways. This approach provides mechanistic insights without requiring direct
molecular data. Building upon these connections, we propose therapeutic
strategies such as dual GLP-1/GIP receptor agonists, AMPK activators, SIRT1
modulators, and phytochemical, further validated through pathway enrichment
analyses. Overall, this framework advances precision medicine by offering
interpretable and scalable solutions for early detection and targeted
intervention in metabolic disorders. The key contributions of this work are:
(1) development of an ML framework combining logistic regression and principal
component analysis (PCA) for T2DM risk prediction; (2) introduction of a
gene-agnostic pathway mapping approach to generate mechanistic insights; and
(3) identification of novel therapeutic strategies tailored for high-risk
populations.

</details>


### [157] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT是一个全自动AI工具，用于从Kaplan-Meier图中高精度重建患者个体数据（IPD），无需人工干预。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动数字化，易出错且难以扩展，KM-GPT旨在解决这些问题。

Method: 结合图像预处理、GPT-5多模态推理和迭代重建算法，实现自动化IPD重建。

Result: 在合成和真实数据集上表现优异，支持胃癌免疫治疗试验的元分析。

Conclusion: KM-GPT通过自动化IPD重建，提升临床研究的可扩展性和决策支持能力。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [158] [AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation](https://arxiv.org/abs/2509.18144)
*Yubo Yang,Yichen Zhu,Bo Jiang*

Main category: cs.LG

TL;DR: AdaSTI是一种基于条件扩散模型的新型时空数据填补方法，通过双向S4模型和噪声感知机制显著提升了填补性能。


<details>
  <summary>Details</summary>
Motivation: 时空数据常因传感器故障等原因缺失，现有扩散模型在提取时空依赖关系时存在误差累积问题，且忽略了不同扩散步骤中噪声数据的变异性。

Method: 提出AdaSTI方法，结合双向S4模型（BiS4PI）进行预填补，并设计时空条件器（STC）和噪声感知时空网络（NAST）来动态捕捉依赖关系。

Result: 在三个真实数据集上，AdaSTI的填补误差降低达46.4%，优于现有方法。

Conclusion: AdaSTI通过自适应依赖建模和噪声感知机制，显著提升了时空数据填补的准确性和鲁棒性。

Abstract: Spatio-temporal data abounds in domain like traffic and environmental
monitoring. However, it often suffers from missing values due to sensor
malfunctions, transmission failures, etc. Recent years have seen continued
efforts to improve spatio-temporal data imputation performance. Recently
diffusion models have outperformed other approaches in various tasks, including
spatio-temporal imputation, showing competitive performance. Extracting and
utilizing spatio-temporal dependencies as conditional information is vital in
diffusion-based methods. However, previous methods introduce error accumulation
in this process and ignore the variability of the dependencies in the noisy
data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive
Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel
spatio-temporal imputation approach based on conditional diffusion model.
Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model
for pre-imputation with the imputed result used to extract conditional
information by our designed Spatio-Temporal Conditionalizer (STC)network. We
also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated
attention mechanism to capture the variant dependencies across diffusion steps.
Extensive experiments on three real-world datasets show that AdaSTI outperforms
existing methods in all the settings, with up to 46.4% reduction in imputation
error.

</details>


### [159] [Early Prediction of Multi-Label Care Escalation Triggers in the Intensive Care Unit Using Electronic Health Records](https://arxiv.org/abs/2509.18145)
*Syed Ahmad Chan Bukhari,Amritpal Singh,Shifath Hossain,Iram Wajahat*

Main category: cs.LG

TL;DR: 提出了一种多标签分类框架，用于预测ICU患者的护理升级触发因素（CETs），优于传统预警系统。


<details>
  <summary>Details</summary>
Motivation: 传统预警系统（如SOFA或MEWS）局限于单一结果，无法捕捉临床恶化的多维性。

Method: 使用MIMIC-IV数据库，基于前24小时ICU数据，定义并预测四种CETs（呼吸衰竭、血流动力学不稳定、肾功能损害和神经功能恶化）。

Result: XGBoost模型表现最佳，F1分数分别为呼吸0.66、血流动力学0.72、肾功能0.76、神经功能0.62。

Conclusion: 该框架具有早期、可解释的临床警报潜力，无需复杂的时间序列建模或自然语言处理。

Abstract: Intensive Care Unit (ICU) patients often present with complex, overlapping
signs of physiological deterioration that require timely escalation of care.
Traditional early warning systems, such as SOFA or MEWS, are limited by their
focus on single outcomes and fail to capture the multi-dimensional nature of
clinical decline. This study proposes a multi-label classification framework to
predict Care Escalation Triggers (CETs), including respiratory failure,
hemodynamic instability, renal compromise, and neurological deterioration,
using the first 24 hours of ICU data. Using the MIMIC-IV database, CETs are
defined through rule-based criteria applied to data from hours 24 to 72 (for
example, oxygen saturation below 90, mean arterial pressure below 65 mmHg,
creatinine increase greater than 0.3 mg/dL, or a drop in Glasgow Coma Scale
score greater than 2). Features are extracted from the first 24 hours and
include vital sign aggregates, laboratory values, and static demographics. We
train and evaluate multiple classification models on a cohort of 85,242 ICU
stays (80 percent training: 68,193; 20 percent testing: 17,049). Evaluation
metrics include per-label precision, recall, F1-score, and Hamming loss.
XGBoost, the best performing model, achieves F1-scores of 0.66 for respiratory,
0.72 for hemodynamic, 0.76 for renal, and 0.62 for neurologic deterioration,
outperforming baseline models. Feature analysis shows that clinically relevant
parameters such as respiratory rate, blood pressure, and creatinine are the
most influential predictors, consistent with the clinical definitions of the
CETs. The proposed framework demonstrates practical potential for early,
interpretable clinical alerts without requiring complex time-series modeling or
natural language processing.

</details>


### [160] [ConceptFlow: Hierarchical and Fine-grained Concept-Based Explanation for Convolutional Neural Networks](https://arxiv.org/abs/2509.18147)
*Xinyu Mu,Hui Dou,Furao Shen,Jian Zhao*

Main category: cs.LG

TL;DR: ConceptFlow是一个基于概念的CNN解释框架，通过追踪概念在层间的传播来模拟模型的内部推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了单个过滤器的语义角色和概念在层间的动态传播，ConceptFlow旨在解决这些问题。

Method: ConceptFlow包含概念注意力和概念路径两个组件，分别关联过滤器与概念并量化概念传播。

Result: 实验表明ConceptFlow能提供语义上有意义的解释，验证了其有效性。

Conclusion: ConceptFlow通过建模层次化概念路径，提供了对CNN内部逻辑的深入理解，并生成更忠实于人类的解释。

Abstract: Concept-based interpretability for Convolutional Neural Networks (CNNs) aims
to align internal model representations with high-level semantic concepts, but
existing approaches largely overlook the semantic roles of individual filters
and the dynamic propagation of concepts across layers. To address these
limitations, we propose ConceptFlow, a concept-based interpretability framework
that simulates the internal "thinking path" of a model by tracing how concepts
emerge and evolve across layers. ConceptFlow comprises two key components: (i)
concept attentions, which associate each filter with relevant high-level
concepts to enable localized semantic interpretation, and (ii) conceptual
pathways, derived from a concept transition matrix that quantifies how concepts
propagate and transform between filters. Together, these components offer a
unified and structured view of internal model reasoning. Experimental results
demonstrate that ConceptFlow yields semantically meaningful insights into model
reasoning, validating the effectiveness of concept attentions and conceptual
pathways in explaining decision behavior. By modeling hierarchical conceptual
pathways, ConceptFlow provides deeper insight into the internal logic of CNNs
and supports the generation of more faithful and human-aligned explanations.

</details>


### [161] [Sparse Training Scheme for Multimodal LLM](https://arxiv.org/abs/2509.18150)
*Kean Shi,Liang Chen,Haozhe Zhao,Baobao Chang*

Main category: cs.LG

TL;DR: 提出了一种基于稀疏表示的高效训练框架STS，用于解决多模态大语言模型训练效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）因输入序列长和层间计算利用率低导致训练效率低下。

Method: STS框架包含视觉令牌压缩器和层动态跳过器，分别压缩视觉令牌和动态跳过不必要的层计算。

Result: 在多个基准测试中验证了STS的有效性和高效性。

Conclusion: STS是一种广泛适用于多种MLLM架构的高效训练方案。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated outstanding
performance across a variety of domains. However, training MLLMs is often
inefficient due to the significantly longer input sequences introduced by
multimodal data and the low utilization of inter-layer computations. To address
this challenge, we shift the focus to the training process itself and propose a
novel training-efficient framework based on sparse representations, termed the
Sparse Training Scheme (STS). This scheme consists of two key components: the
Visual Token Compressor, which reduces the information load by compressing
visual tokens, and the Layer Dynamic Skipper, which mitigates the computational
overhead by dynamically skipping unnecessary layers in the language model
during both forward and backward passes. Our approach is broadly applicable to
diverse MLLM architectures and has been extensively evaluated on multiple
benchmarks, demonstrating its effectiveness and efficiency.

</details>


### [162] [HyperNAS: Enhancing Architecture Representation for NAS Predictor via Hypernetwork](https://arxiv.org/abs/2509.18151)
*Jindi Lv,Yuhao Zhou,Yuxin Tian,Qing Ye,Wentao Feng,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperNAS提出了一种新的神经预测器范式，通过全局编码方案和共享超网络增强架构表示学习，显著提升了NAS的性能预测能力。


<details>
  <summary>Details</summary>
Motivation: 解决神经架构搜索（NAS）中性能评估耗时且现有预测器泛化能力差的问题。

Method: 采用全局编码方案捕捉宏观结构信息，共享超网络辅助研究架构间模式，并使用动态自适应多任务损失确保训练稳定性。

Result: 在五个代表性搜索空间（包括ViTs）上表现优异，尤其在少样本场景下，达到97.60%的CIFAR-10和82.4%的ImageNet top-1准确率。

Conclusion: HyperNAS通过增强架构表示学习，显著提升了性能预测的效率和准确性，尤其在资源有限的情况下表现突出。

Abstract: Time-intensive performance evaluations significantly impede progress in
Neural Architecture Search (NAS). To address this, neural predictors leverage
surrogate models trained on proxy datasets, allowing for direct performance
predictions for new architectures. However, these predictors often exhibit poor
generalization due to their limited ability to capture intricate relationships
among various architectures. In this paper, we propose HyperNAS, a novel neural
predictor paradigm for enhancing architecture representation learning. HyperNAS
consists of two primary components: a global encoding scheme and a shared
hypernetwork. The global encoding scheme is devised to capture the
comprehensive macro-structure information, while the shared hypernetwork serves
as an auxiliary task to enhance the investigation of inter-architecture
patterns. To ensure training stability, we further develop a dynamic adaptive
multi-task loss to facilitate personalized exploration on the Pareto front.
Extensive experiments across five representative search spaces, including ViTs,
demonstrate the advantages of HyperNAS, particularly in few-shot scenarios. For
instance, HyperNAS strikes new state-of-the-art results, with 97.60\% top-1
accuracy on CIFAR-10 and 82.4\% top-1 accuracy on ImageNet, using at least
5.0$\times$ fewer samples.

</details>


### [163] [WLFM: A Well-Logs Foundation Model for Multi-Task and Cross-Well Geological Interpretation](https://arxiv.org/abs/2509.18152)
*Zhenyu Qi,Qing Yu,Jichen Wang,Yun-Bo Zhao,Zerui Li,Wenjun Lv*

Main category: cs.LG

TL;DR: WLFM是一种基于多曲线测井数据预训练的基础模型，通过自监督学习和多任务适应，显著提升了孔隙度估计和岩性分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决测井解释中工具响应异质性、噪声信号和标签有限的问题。

Method: 采用三阶段方法：地质标记化、自监督预训练（掩码标记建模和地层感知对比学习）和多任务适应（少样本微调）。

Result: WLFM在孔隙度估计（MSE 0.0041）和岩性分类（74.13%准确率）上优于基线模型，微调后进一步提升（MSE 0.0038，78.10%准确率）。

Conclusion: WLFM是一个可扩展、可解释且可迁移的地质AI骨干模型，具有多模态集成潜力。

Abstract: Well-log interpretation is fundamental for subsurface characterization but
remains challenged by heterogeneous tool responses, noisy signals, and limited
labels. We propose WLFM, a foundation model pretrained on multi-curve logs from
1200 wells, comprising three stages: tokenization of log patches into
geological tokens, self-supervised pretraining with masked-token modeling and
stratigraphy-aware contrastive learning, and multi-task adaptation with
few-shot fine-tuning. WLFM consistently outperforms state-of-the-art baselines,
achieving 0.0041 MSE in porosity estimation and 74.13\% accuracy in lithology
classification, while WLFM-Finetune further improves to 0.0038 MSE and 78.10\%
accuracy. Beyond predictive accuracy, WLFM exhibits emergent layer-awareness,
learns a reusable geological vocabulary, and reconstructs masked curves with
reasonable fidelity, though systematic offsets are observed in shallow and
ultra-deep intervals. Although boundary detection is not explicitly evaluated
here, clustering analyses suggest strong potential for future extension. These
results establish WLFM as a scalable, interpretable, and transferable backbone
for geological AI, with implications for multi-modal integration of logs,
seismic, and textual data.

</details>


### [164] [A deep reinforcement learning platform for antibiotic discovery](https://arxiv.org/abs/2509.18153)
*Hanqun Cao,Marcelo D. T. Torres,Jingjie Zhang,Zijun Gao,Fang Wu,Chunbin Gu,Jure Leskovec,Yejin Choi,Cesar de la Fuente-Nunez,Guangyong Chen,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: ApexAmphion是一个结合蛋白质语言模型和强化学习的深度学习框架，用于从头设计抗生素，实验显示其设计的肽具有高效抗菌活性。


<details>
  <summary>Details</summary>
Motivation: 抗生素耐药性（AMR）预计到2050年每年导致1000万人死亡，亟需新型抗生素。

Method: 框架结合64亿参数蛋白质语言模型和强化学习，通过微调和优化生成抗菌肽。

Result: 100个设计肽中，所有候选物均显示低MIC值（部分达纳摩尔级），99个具有广谱抗菌活性。

Conclusion: ApexAmphion提供了一种可扩展的肽抗生素设计平台，能快速生成高效候选分子。

Abstract: Antimicrobial resistance (AMR) is projected to cause up to 10 million deaths
annually by 2050, underscoring the urgent need for new antibiotics. Here we
present ApexAmphion, a deep-learning framework for de novo design of
antibiotics that couples a 6.4-billion-parameter protein language model with
reinforcement learning. The model is first fine-tuned on curated peptide data
to capture antimicrobial sequence regularities, then optimised with proximal
policy optimization against a composite reward that combines predictions from a
learned minimum inhibitory concentration (MIC) classifier with differentiable
physicochemical objectives. In vitro evaluation of 100 designed peptides showed
low MIC values (nanomolar range in some cases) for all candidates (100% hit
rate). Moreover, 99 our of 100 compounds exhibited broad-spectrum antimicrobial
activity against at least two clinically relevant bacteria. The lead molecules
killed bacteria primarily by potently targeting the cytoplasmic membrane. By
unifying generation, scoring and multi-objective optimization with deep
reinforcement learning in a single pipeline, our approach rapidly produces
diverse, potent candidates, offering a scalable route to peptide antibiotics
and a platform for iterative steering toward potency and developability within
hours.

</details>


### [165] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5是一个8B参数的高效多模态大语言模型，通过改进架构、数据策略和训练方法，在性能和效率上超越GPT-4o-latest和Qwen2.5-VL 72B。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的训练和推理效率是当前AI发展的瓶颈，MiniCPM-V 4.5旨在解决这一问题。

Method: 采用统一的3D-Resampler架构、无繁重数据工程的文档知识学习范式，以及混合强化学习策略。

Result: 在OpenCompass评估中表现优异，显著优于GPT-4o-latest和Qwen2.5-VL 72B，且在VideoMME基准测试中效率突出。

Conclusion: MiniCPM-V 4.5在高效性和性能上取得突破，为多模态大语言模型的普及和扩展提供了解决方案。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [166] [Developing Training Procedures for Piecewise-linear Spline Activation Functions in Neural Networks](https://arxiv.org/abs/2509.18161)
*William H Patty*

Main category: cs.LG

TL;DR: 通过优化激活函数形状，提升神经网络参数效率和准确性，实验显示比传统ReLU模型误差显著降低。


<details>
  <summary>Details</summary>
Motivation: 传统静态激活函数（如ReLU、tanh、sigmoid）可能不是最优选择，优化激活函数形状可提升模型性能。

Method: 提出并比较9种训练方法，研究参数化线性B样条激活函数的双优化动态。

Result: 在FNN中误差降低94%，CNN中降低51%，但增加了开发和训练复杂度及延迟。

Conclusion: 优化激活函数形状能显著提升模型性能，但需权衡开发复杂度和延迟成本。

Abstract: Activation functions in neural networks are typically selected from a set of
empirically validated, commonly used static functions such as ReLU, tanh, or
sigmoid. However, by optimizing the shapes of a network's activation functions,
we can train models that are more parameter-efficient and accurate by assigning
more optimal activations to the neurons. In this paper, I present and compare 9
training methodologies to explore dual-optimization dynamics in neural networks
with parameterized linear B-spline activation functions. The experiments
realize up to 94% lower end model error rates in FNNs and 51% lower rates in
CNNs compared to traditional ReLU-based models. These gains come at the cost of
additional development and training complexity as well as end model latency.

</details>


### [167] [A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge](https://arxiv.org/abs/2509.18162)
*Meraryslan Meraliyev,Cemil Turan,Shirali Kadyrov*

Main category: cs.LG

TL;DR: 论文研究了一种结合卡车和无人机的最后一公里配送方法，通过混合强化学习优化配送时间。


<details>
  <summary>Details</summary>
Motivation: 解决在电池管理约束下，如何高效协调卡车和无人机的配送任务，以最小化总完成时间。

Method: 提出混合强化学习求解器，结合ALNS卡车路径规划和指针/注意力策略调度无人机任务，使用时间线模拟器确保可行性。

Result: 在特定参数下，该方法平均完成时间为5.203±0.093，优于ALNS（5.349±0.038），接近NN（5.208±0.124）。

Conclusion: 混合强化学习方法在协调卡车和无人机配送中表现优异，显著优于传统方法，并提供了可复现的实现工具。

Abstract: We study last-mile delivery with one truck and one drone under explicit
battery management: the drone flies at twice the truck speed; each sortie must
satisfy an endurance budget; after every delivery the drone recharges on the
truck before the next launch. We introduce a hybrid reinforcement learning (RL)
solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a
small pointer/attention policy that schedules drone sorties. The policy decodes
launch--serve--rendezvous triplets with hard feasibility masks for endurance
and post-delivery recharge; a fast, exact timeline simulator enforces
launch/recovery handling and computes the true makespan used by masked
greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and
$R{=}0.1$, the method achieves an average makespan of \textbf{5.203}$\pm$0.093,
versus \textbf{5.349}$\pm$0.038 for ALNS and \textbf{5.208}$\pm$0.124 for NN --
i.e., \textbf{2.73\%} better than ALNS on average and within \textbf{0.10\%} of
NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance
and ties or beats NN on two of three seeds. A decomposition of the makespan
shows the expected truck--wait trade-off across heuristics; the learned
scheduler balances both to minimize the total completion time. We provide a
config-first implementation with plotting and significance-test utilities to
support replication.

</details>


### [168] [DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns](https://arxiv.org/abs/2509.18164)
*Ranfei Chen,Ming Chen*

Main category: cs.LG

TL;DR: DSFT是一种简单有效的扩散SFT策略，通过调整掩码策略和损失函数，提升dLLMs在数学和逻辑任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前dLLMs在数学和逻辑任务上表现不足，现有训练方法缺乏对这些模式的全面理解。

Method: 提出DSFT策略，调整掩码和损失函数，结合预训练和强化学习等方法。

Result: 在LLaDA和Dream系列模型上，数学和逻辑任务分别提升5-10%和2%。

Conclusion: DSFT为未来学习特定模式提供了启发，可灵活结合其他方法并应用于多种dLLMs。

Abstract: Diffusion large language models (dLLMs) have emerged as a new architecture
following auto regressive models. Their denoising process offers a powerful
generative advantage, but they present significant challenges in learning and
understanding numerically sensitive mathematical and order-sensitive logical
tasks. Current training methods, including pre-training, fine-tuning, and
reinforcement learning, focus primarily on improving general knowledge
retention and reasoning abilities, but lack a comprehensive understanding of
mathematical and logical patterns. We propose DSFT, a simple yet effective
Diffusion SFT strategy, by adjusting the masking strategy and loss function,
guiding models to understand mathematical and logical patterns. This strategy
can be flexibly combined with pre-training, reinforcement learning, and other
training methods. Validated on models such as LLaDA and Dream series, we prove
that DSFT on small-scale data can achieve improvements of 5-10% and
approximately 2% on mathematical and logical problems, respectively. This
inspiring masking approach offers insights for future learning of specific
patterns, which can be easily and efficiently combined with other training
methods and applied to various dLLMs. Our code is publicly available at
https://anonymous.4open.science/r/DSFT-0FFB/

</details>


### [169] [MobiGPT: A Foundation Model for Mobile Wireless Networks](https://arxiv.org/abs/2509.18166)
*Xiaoqian Qi,Haoye Chai,Yong Li*

Main category: cs.LG

TL;DR: MobiGPT是一个用于移动数据预测的基础模型，能够统一预测基站流量、用户应用使用和信道质量，通过软提示学习和时间掩码机制提升预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动数据预测方法依赖于定制化设计，增加了大规模异构网络的复杂性和部署成本，因此需要一种统一的预测模型。

Method: 提出MobiGPT模型，采用软提示学习方法理解不同数据类型特征，并引入时间掩码机制支持短期预测、长期预测和分布生成任务。

Result: 在真实数据集上，MobiGPT的预测准确性比现有模型提高了27.37%、20.08%和7.27%，并在零/少样本场景中表现出色。

Conclusion: MobiGPT作为一种基础模型，具有强大的泛化能力和迁移能力，适用于多样化的移动数据预测场景。

Abstract: With the rapid development of mobile communication technologies, future
mobile networks will offer vast services and resources for commuting,
production, daily life, and entertainment. Accurate and efficient forecasting
of mobile data (e.g., cell traffic, user behavior, channel quality) helps
operators monitor network state changes, orchestrate wireless resources, and
schedule infrastructure and users, thereby improving supply efficiency and
service quality. However, current forecasting paradigms rely on customized
designs with tailored models for exclusive data types. Such approaches increase
complexity and deployment costs under large-scale, heterogeneous networks
involving base stations, users, and channels. In this paper, we design a
foundation model for mobile data forecasting, MobiGPT, with a unified structure
capable of forecasting three data types: base station traffic, user app usage,
and channel quality. We propose a soft-prompt learning method to help the model
understand features of different data types, and introduce a temporal masking
mechanism to guide the model through three forecasting tasks: short-term
prediction, long-term prediction, and distribution generation, supporting
diverse optimization scenarios. Evaluations on real-world datasets with over
100,000 samples show that MobiGPT achieves accurate multi-type forecasting.
Compared to existing models, it improves forecasting accuracy by 27.37%,
20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits
superior zero/few-shot performance in unseen scenarios, with over 21.51%
improvement, validating its strong transferability as a foundation model.

</details>


### [170] [PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning](https://arxiv.org/abs/2509.18169)
*Hengbo Xiao,Jingyuan Fan,Xin Tong,Jingzhao Zhang,Chao Lu,Guannan He*

Main category: cs.LG

TL;DR: PiMoE是一种新型训练和推理架构，通过物理隔离的专家混合模型，将计算能力内生于神经网络，解决了现有LLMs无法高效整合高精度计算的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）无法内在地整合高精度计算，而多智能体方法存在通信开销和可扩展性限制。PiMoE旨在解决这些问题。

Method: PiMoE通过分别训练专家、文本到计算模块和路由器，将计算能力内生于神经网络，并在推理时通过路由器实现计算与推理的交替。

Result: PiMoE在推理-计算任务中表现优于LLM微调和多智能体方法，显著提高了准确性、响应延迟、令牌使用效率和GPU能耗。

Conclusion: PiMoE为下一代科学或工业智能系统提供了一种高效、可解释且可扩展的范式。

Abstract: Complex systems typically rely on high-precision numerical computation to
support decisions, but current large language models (LLMs) cannot yet
incorporate such computations as an intrinsic and interpretable capability with
existing architectures. Mainstream multi-agent approaches can leverage external
experts, but inevitably introduce communication overhead and suffer from
inefficient multimodal emergent capability and limited scalability. To this
end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and
inference architecture for integrating computation and reasoning. Instead of
the workflow paradigm of tool invocation, PiMoE endogenously integrates
computational capabilities into neural networks after separately training
experts, a text-to-computation module, and a router. At inference, the router
directs computation and reasoning at the token level, thereby enabling
iterative alternation within a single chain of thought. We evaluate PiMoE on
two reasoning-computation tasks against LLM finetuning and the multi-agent
system approaches. Results show that the PiMoE architecture achieves not only
higher accuracy than directly finetuning LLMs but also significant improvements
in response latency, token usage, and GPU energy consumption compared with
mainstream multi-agent approaches. PiMoE offers an efficient, interpretable,
and scalable paradigm for next-generation scientific or industrial intelligent
systems.

</details>


### [171] [FedIA: A Plug-and-Play Importance-Aware Gradient Pruning Aggregation Method for Domain-Robust Federated Graph Learning on Node Classification](https://arxiv.org/abs/2509.18171)
*Zhanting Zhou,KaHou Tam,Zeqin Wu,Pengzhao Sun,Jinbo Wang,Fengli Zhang*

Main category: cs.LG

TL;DR: FedIA框架通过投影优先策略解决联邦图学习中域偏移导致的梯度噪声问题，提升模型稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦图学习在域偏移情况下（如Twitch Gamers和多语言Wikipedia网络）导致客户端模型表示不兼容，传统聚合方法效果差。研究发现梯度噪声是主要原因。

Method: 提出FedIA框架，采用两阶段投影优先策略：服务器端使用top-ρ掩码保留5%最信息化的梯度坐标，并引入轻量级影响正则化动量权重抑制异常客户端。

Result: 在Twitch Gamers和Wikipedia图上，FedIA比九种基线方法表现更好，收敛更稳定且最终准确率更高。理论分析显示动态投影保持最优收敛速率。

Conclusion: FedIA通过投影优先策略有效解决梯度噪声问题，无需额外上行流量和服务器内存，适用于实际部署。

Abstract: Federated Graph Learning (FGL) under domain skew -- as observed on platforms
such as \emph{Twitch Gamers} and multilingual \emph{Wikipedia} networks --
drives client models toward incompatible representations, rendering naive
aggregation both unstable and ineffective. We find that the culprit is not the
weighting scheme but the \emph{noisy gradient signal}: empirical analysis of
baseline methods suggests that a vast majority of gradient dimensions can be
dominated by domain-specific variance. We therefore shift focus from
"aggregation-first" to a \emph{projection-first} strategy that denoises client
updates \emph{before} they are combined. The proposed FedIA framework realises
this \underline{I}mportance-\underline{A}ware idea through a two-stage,
plug-and-play pipeline: (i) a server-side top-$\rho$ mask keeps only the most
informative about 5% of coordinates, and (ii) a lightweight
influence-regularised momentum weight suppresses outlier clients. FedIA adds
\emph{no extra uplink traffic and only negligible server memory}, making it
readily deployable. On both homogeneous (Twitch Gamers) and heterogeneous
(Wikipedia) graphs, it yields smoother, more stable convergence and higher
final accuracy than nine strong baselines. A convergence sketch further shows
that dynamic projection maintains the optimal
$\mathcal{O}(\sigma^{2}/\sqrt{T})$ rate.

</details>


### [172] [SBVR: Summation of BitVector Representation for Efficient LLM Quantization](https://arxiv.org/abs/2509.18172)
*Wonjun Bang,Jongseok Park,Hongseung Yu,Kyungmin Bin,Kyunghan Lee*

Main category: cs.LG

TL;DR: 提出了一种名为SBVR的新型LLM量化方法，通过硬件友好的方式实现高斯分布编码，提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法（如RTN和codebook-based）在量化LLM时存在分布不匹配或内存访问效率低的问题，影响推理速度和准确性。

Method: SBVR方法将权重映射到非均匀表示点，匹配LLM权重的高斯分布，并设计专用CUDA内核直接执行SBVR格式的矩阵乘法。

Result: 在4位量化下，SBVR实现了最先进的困惑度和准确性，同时比FP16模型快2.21x-3.04x。

Conclusion: SBVR通过硬件友好的高斯分布编码和高效执行，显著提升了LLM量化的性能和速度。

Abstract: With the advent of large language models (LLMs), numerous Post-Training
Quantization (PTQ) strategies have been proposed to alleviate deployment
barriers created by their enormous parameter counts. Quantization achieves
compression by limiting the number of representable points in the data.
Therefore, the key to achieving efficient quantization is selecting the optimal
combination of representation points, or codes, for the given data. Existing
PTQ solutions adopt two major approaches to this problem: Round-To-Nearest
(RTN)-based methods and codebook-based methods. RTN-based methods map LLM
weights onto uniformly distributed integer grids, failing to account for the
Gaussian-like weight distribution of LLM weights. Codebook-based methods
mitigate this issue by constructing distribution-aware codebooks; however, they
suffer from random and strided memory access patterns, resulting in degraded
inference speed that is exacerbated by the limited size of GPU L1 cache. To
overcome these limitations, we propose a novel LLM quantization method, SBVR
(Summation of BitVector Representation), that enables Gaussian-like code
representation in a hardware-friendly manner for fast inference. SBVR maps
weight values to non-uniform representation points whose distribution follows
the actual distribution of LLM weights, enabling more accurate compression.
Additionally, we design a custom CUDA kernel that allows matrix-vector
multiplication directly in the SBVR format without decompression, thereby
enabling high-performance execution of SBVR-compressed models. Our evaluations
of SBVR on various models demonstrate state-of-the-art perplexity and accuracy
benchmark performance while delivering a 2.21x- 3.04x end-to-end
token-generation speedup over naive FP16 models in the 4-bit quantization
regime.

</details>


### [173] [TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://arxiv.org/abs/2509.18173)
*Hongyi Luo,Qing Cheng,Daniel Matos,Hari Krishna Gadi,Yanfeng Zhang,Lu Liu,Yongliang Wang,Niclas Zeller,Daniel Cremers,Liqiu Meng*

Main category: cs.LG

TL;DR: 论文提出了一个大规模基准测试，评估LLMs在地理空间路线认知上的能力，发现其存在局限性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在地理空间认知方面的能力，填补现有研究的不足。

Method: 创建大规模数据集，开发PathBuilder工具，提出新评估框架。

Result: LLMs在路线反转任务中表现不佳，存在鲁棒性低和错误自信问题。

Conclusion: LLMs在地理空间认知方面仍有改进空间，需进一步研究。

Abstract: Humans can interpret geospatial information through natural language, while
the geospatial cognition capabilities of Large Language Models (LLMs) remain
underexplored. Prior research in this domain has been constrained by
non-quantifiable metrics, limited evaluation datasets and unclear research
hierarchies. Therefore, we propose a large-scale benchmark and conduct a
comprehensive evaluation of the geospatial route cognition of LLMs. We create a
large-scale evaluation dataset comprised of 36000 routes from 12 metropolises
worldwide. Then, we introduce PathBuilder, a novel tool for converting natural
language instructions into navigation routes, and vice versa, bridging the gap
between geospatial information and natural language. Finally, we propose a new
evaluation framework and metrics to rigorously assess 11 state-of-the-art
(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs
exhibit limitation to reverse routes: most reverse routes neither return to the
starting point nor are similar to the optimal route. Additionally, LLMs face
challenges such as low robustness in route generation and high confidence for
their incorrect answers. Code\ \&\ Data available here:
\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}

</details>


### [174] [Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.18200)
*Yu Ti Huang*

Main category: cs.LG

TL;DR: 论文提出了一种多模态思维链（MCoT）框架，用于解决对话代理在复杂环境中将自我中心表述转换为全局方向的问题，并在中文对话导航任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决在GPS信号弱或无详细地图的复杂环境中，对话代理如何准确理解自我中心表述并将其转换为全局方向的问题。

Method: 提出多模态思维链（MCoT）框架，结合语音识别（ASR）转录和地标坐标，通过三步推理过程实现方向转换。

Result: MCoT在干净转录本上达到100%方向准确率，在ASR转录本上达到98.1%，优于单模态和非结构化基线。

Conclusion: 结构化MCoT空间推理在资源受限环境下具有潜力，为可解释和高效的导航提供了新路径。

Abstract: Conversational agents must translate egocentric utterances (e.g., "on my
right") into allocentric orientations (N/E/S/W). This challenge is particularly
critical in indoor or complex facilities where GPS signals are weak and
detailed maps are unavailable. While chain-of-thought (CoT) prompting has
advanced reasoning in language and vision tasks, its application to multimodal
spatial orientation remains underexplored. We introduce Conversational
Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese
conversational navigation projected from real-world environments, addressing
egocentric-to-allocentric reasoning in non-English and ASR-transcribed
scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which
integrates ASR-transcribed speech with landmark coordinates through a
structured three-step reasoning process: (1) extracting spatial relations, (2)
mapping coordinates to absolute directions, and (3) inferring user orientation.
A curriculum learning strategy progressively builds these capabilities on
Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of
resource-constrained settings. Experiments show that MCoT achieves 100%
orientation accuracy on clean transcripts and 98.1% with ASR transcripts,
substantially outperforming unimodal and non-structured baselines. Moreover,
MCoT demonstrates robustness under noisy conversational conditions, including
ASR recognition errors and multilingual code-switching. The model also
maintains high accuracy in cross-domain evaluation and resilience to linguistic
variation, domain shift, and referential ambiguity. These findings highlight
the potential of structured MCoT spatial reasoning as a path toward
interpretable and resource-efficient embodied navigation.

</details>


### [175] [Variational Task Vector Composition](https://arxiv.org/abs/2509.18208)
*Boyuan Zhang,Yingjun Du,Xiantong Zhen,Ling Shao*

Main category: cs.LG

TL;DR: 提出变分任务向量组合方法，通过贝叶斯框架估计潜在变量，提升任务向量组合的效率和效果。


<details>
  <summary>Details</summary>
Motivation: 观察到任务向量中存在结构冗余，希望通过稀疏性和样本特异性组合提升性能。

Method: 引入Spike-and-Slab先验和门控采样机制，选择最可靠的任务组件。

Result: 实验表明方法在所有数据集上均优于现有方法。

Conclusion: 为任务向量组合设立了新标准，提升了效率和效果。

Abstract: Task vectors capture how a model changes during fine-tuning by recording the
difference between pre-trained and task-specific weights. The composition of
task vectors, a key operator in task arithmetic, enables models to integrate
knowledge from multiple tasks without incurring additional inference costs. In
this paper, we propose variational task vector composition, where composition
coefficients are taken as latent variables and estimated in a Bayesian
inference framework. Unlike previous methods that operate at the task level,
our framework focuses on sample-specific composition. Motivated by the
observation of structural redundancy in task vectors, we introduce a
Spike-and-Slab prior that promotes sparsity and preserves only the most
informative components. To further address the high variance and sampling
inefficiency in sparse, high-dimensional spaces, we develop a gated sampling
mechanism that constructs a controllable posterior by filtering the composition
coefficients based on both uncertainty and importance. This yields a more
stable and interpretable variational framework by deterministically selecting
reliable task components, reducing sampling variance while improving
transparency and generalization. Experimental results demonstrate that our
method consistently outperforms existing approaches across all datasets by
selectively leveraging the most reliable and informative components in task
vectors. These findings highlight the practical value of our approach,
establishing a new standard for efficient and effective task vector
composition.

</details>


### [176] [MolPILE - large-scale, diverse dataset for molecular representation learning](https://arxiv.org/abs/2509.18353)
*Jakub Adamczyk,Jakub Poziemski,Franciszek Job,Mateusz Król,Maciej Makowski*

Main category: cs.LG

TL;DR: MolPILE是一个大规模、多样化的分子数据集，包含2.22亿个化合物，旨在解决现有小分子数据集的局限性，提升分子表示学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有小分子数据集在规模、多样性和质量上的不足限制了分子表示学习的效果，MolPILE填补了这一空白。

Method: 通过自动化流程从6个大型数据库中构建MolPILE，并对其进行了全面分析，展示了其在提升模型泛化性能上的优势。

Result: 在MolPILE上重新训练现有模型显著提升了泛化性能。

Conclusion: MolPILE为分子化学领域提供了一个类似ImageNet的标准化资源，解决了模型训练的关键需求。

Abstract: The size, diversity, and quality of pretraining datasets critically determine
the generalization ability of foundation models. Despite their growing
importance in chemoinformatics, the effectiveness of molecular representation
learning has been hindered by limitations in existing small molecule datasets.
To address this gap, we present MolPILE, large-scale, diverse, and rigorously
curated collection of 222 million compounds, constructed from 6 large-scale
databases using an automated curation pipeline. We present a comprehensive
analysis of current pretraining datasets, highlighting considerable
shortcomings for training ML models, and demonstrate how retraining existing
models on MolPILE yields improvements in generalization performance. This work
provides a standardized resource for model training, addressing the pressing
need for an ImageNet-like dataset in molecular chemistry.

</details>


### [177] [FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction](https://arxiv.org/abs/2509.18362)
*Yuxuan Cai,Xiaozhuan Liang,Xinghua Wang,Jin Ma,Haijin Liang,Jinwen Luo,Xinyu Zuo,Lisheng Duan,Yuyang Yin,Xi Chen*

Main category: cs.LG

TL;DR: FastMTP通过改进多步草稿质量，显著提升推测解码性能，实现LLM推理加速。


<details>
  <summary>Details</summary>
Motivation: 自回归生成的顺序性导致吞吐量瓶颈，限制了LLM的实际部署。FastMTP旨在探索多令牌预测（MTP）在推理加速中的潜力。

Method: 通过自蒸馏数据微调位置共享权重的MTP头，捕捉连续未来令牌的依赖关系，并结合动态词汇压缩减少计算开销。

Result: 在七个基准测试中，FastMTP平均加速2.03倍，输出质量无损，优于普通MTP 82%。

Conclusion: FastMTP提供了一种轻量级训练且易于部署的解决方案，显著加速LLM推理。

Abstract: As large language models (LLMs) become increasingly powerful, the sequential
nature of autoregressive generation creates a fundamental throughput bottleneck
that limits the practical deployment. While Multi-Token Prediction (MTP) has
demonstrated remarkable benefits for model training efficiency and performance,
its inherent potential for inference acceleration remains largely unexplored.
This paper introduces FastMTP, a simple yet effective method that improves
multi-step draft quality by aligning MTP training with its inference pattern,
significantly enhancing speculative decoding performance. Our approach
fine-tunes a single MTP head with position-shared weights on self-distilled
data, enabling it to capture dependencies among consecutive future tokens and
maintain high acceptance rates across multiple recursive draft steps. By
integrating language-aware dynamic vocabulary compression into the MTP head, we
further reduce computational overhead in the drafting process. Experimental
results across seven diverse benchmarks demonstrate that FastMTP achieves an
average of 2.03x speedup compared to standard next token prediction with
lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires
only lightweight training and seamlessly integrates with existing inference
frameworks, offering a practical and rapidly deployable solution for
accelerating LLM inference.

</details>


### [178] [Multi-Worker Selection based Distributed Swarm Learning for Edge IoT with Non-i.i.d. Data](https://arxiv.org/abs/2509.18367)
*Zhuoyu Yao,Yue Wang,Songyang Zhang,Yingshu Li,Zhipeng Cai,Zhi Tian*

Main category: cs.LG

TL;DR: 本文提出了一种新的多工作者选择算法M-DSL，用于解决分布式异构数据下的学习性能问题，并通过理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 分布式群学习（DSL）在边缘物联网中具有潜力，但非独立同分布（non-i.i.d.）数据会降低学习性能，且缺乏理论指导。

Method: 提出M-DSL算法，引入新的non-i.i.d.度量标准，指导多工作者选择，并进行理论分析和实验验证。

Result: 实验结果表明，M-DSL在异构数据集上显著提升了性能，增强了网络智能。

Conclusion: M-DSL有效解决了non-i.i.d.数据下的学习挑战，为分布式群学习提供了理论支持和实践指导。

Abstract: Recent advances in distributed swarm learning (DSL) offer a promising
paradigm for edge Internet of Things. Such advancements enhance data privacy,
communication efficiency, energy saving, and model scalability. However, the
presence of non-independent and identically distributed (non-i.i.d.) data pose
a significant challenge for multi-access edge computing, degrading learning
performance and diverging training behavior of vanilla DSL. Further, there
still lacks theoretical guidance on how data heterogeneity affects model
training accuracy, which requires thorough investigation. To fill the gap, this
paper first study the data heterogeneity by measuring the impact of non-i.i.d.
datasets under the DSL framework. This then motivates a new multi-worker
selection design for DSL, termed M-DSL algorithm, which works effectively with
distributed heterogeneous data. A new non-i.i.d. degree metric is introduced
and defined in this work to formulate the statistical difference among local
datasets, which builds a connection between the measure of data heterogeneity
and the evaluation of DSL performance. In this way, our M-DSL guides effective
selection of multiple works who make prominent contributions for global model
updates. We also provide theoretical analysis on the convergence behavior of
our M-DSL, followed by extensive experiments on different heterogeneous
datasets and non-i.i.d. data settings. Numerical results verify performance
improvement and network intelligence enhancement provided by our M-DSL beyond
the benchmarks.

</details>


### [179] [GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability](https://arxiv.org/abs/2509.18376)
*Burouj Armgaan,Eshan Jain,Harsh Pandey,Mahesh Chandran,Sayan Ranu*

Main category: cs.LG

TL;DR: GnnXemplar是一种新型的全局解释方法，通过选择代表性节点和生成自然语言规则来解释GNN的预测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有全局解释方法在小图中有效，但在大规模、高维属性图中表现不佳，需要更高效且可解释的方法。

Method: GnnXemplar基于认知科学中的范例理论，选择嵌入空间中的代表性节点，并通过LLM生成自然语言规则。

Result: 实验表明，GnnXemplar在保真度、可扩展性和人类可解释性上显著优于现有方法。

Conclusion: GnnXemplar为GNN的全局解释提供了一种高效且可理解的新方法。

Abstract: Graph Neural Networks (GNNs) are widely used for node classification, yet
their opaque decision-making limits trust and adoption. While local
explanations offer insights into individual predictions, global explanation
methods, those that characterize an entire class, remain underdeveloped.
Existing global explainers rely on motif discovery in small graphs, an approach
that breaks down in large, real-world settings where subgraph repetition is
rare, node attributes are high-dimensional, and predictions arise from complex
structure-attribute interactions. We propose GnnXemplar, a novel global
explainer inspired from Exemplar Theory from cognitive science. GnnXemplar
identifies representative nodes in the GNN embedding space, exemplars, and
explains predictions using natural language rules derived from their
neighborhoods. Exemplar selection is framed as a coverage maximization problem
over reverse k-nearest neighbors, for which we provide an efficient greedy
approximation. To derive interpretable rules, we employ a self-refining prompt
strategy using large language models (LLMs). Experiments across diverse
benchmarks show that GnnXemplar significantly outperforms existing methods in
fidelity, scalability, and human interpretability, as validated by a user study
with 60 participants.

</details>


### [180] [Graph Enhanced Trajectory Anomaly Detection](https://arxiv.org/abs/2509.18386)
*Jonathan Kabala Mbuya,Dieter Pfoser,Antonios Anastasopoulos*

Main category: cs.LG

TL;DR: GETAD框架通过结合道路网络拓扑、历史旅行模式和语义信息，利用图注意力网络和Transformer解码器，提升了轨迹异常检测的精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将轨迹视为采样点序列，忽略了道路网络的约束和连通性信息，导致检测效果受限。

Method: GETAD使用图注意力网络学习道路感知嵌入，结合Transformer解码器和多目标损失函数，提出CW NLL异常评分函数。

Result: 实验表明GETAD在真实和合成数据集上优于现有方法，尤其在道路约束环境中检测细微异常。

Conclusion: 结合图结构和上下文语义能实现更精确的轨迹异常检测。

Abstract: Trajectory anomaly detection is essential for identifying unusual and
unexpected movement patterns in applications ranging from intelligent
transportation systems to urban safety and fraud prevention.
  Existing methods only consider limited aspects of the trajectory nature and
its movement space by treating trajectories as sequences of sampled locations,
with sampling determined by positioning technology, e.g., GPS, or by high-level
abstractions such as staypoints. Trajectories are analyzed in Euclidean space,
neglecting the constraints and connectivity information of the underlying
movement network, e.g., road or transit networks.
  The proposed Graph Enhanced Trajectory Anomaly Detection (GETAD) framework
tightly integrates road network topology, segment semantics, and historical
travel patterns to model trajectory data. GETAD uses a Graph Attention Network
to learn road-aware embeddings that capture both physical attributes and
transition behavior, and augments these with graph-based positional encodings
that reflect the spatial layout of the road network.
  A Transformer-based decoder models sequential movement, while a
multiobjective loss function combining autoregressive prediction and supervised
link prediction ensures realistic and structurally coherent representations.
  To improve the robustness of anomaly detection, we introduce Confidence
Weighted Negative Log Likelihood (CW NLL), an anomaly scoring function that
emphasizes high-confidence deviations.
  Experiments on real-world and synthetic datasets demonstrate that GETAD
achieves consistent improvements over existing methods, particularly in
detecting subtle anomalies in road-constrained environments. These results
highlight the benefits of incorporating graph structure and contextual
semantics into trajectory modeling, enabling more precise and context-aware
anomaly detection.

</details>


### [181] [Towards Provable Emergence of In-Context Reinforcement Learning](https://arxiv.org/abs/2509.18389)
*Jiuqi Wang,Rohan Chandra,Shangtong Zhang*

Main category: cs.LG

TL;DR: 论文探讨了为什么强化学习预训练算法能生成支持上下文强化学习（ICRL）的网络参数，假设这些参数是预训练损失的极小值点，并通过案例研究初步验证了这一假设。


<details>
  <summary>Details</summary>
Motivation: 研究上下文强化学习（ICRL）现象背后的机制，即预训练参数如何支持ICRL，填补了现有研究的空白。

Method: 通过理论分析，证明在策略评估预训练的Transformer中，预训练损失的全局极小值点之一能够支持上下文时间差分学习。

Result: 案例研究表明，预训练损失的极小值点确实能够支持ICRL，验证了假设。

Conclusion: 论文为ICRL现象提供了理论支持，表明预训练损失的极小值点是实现ICRL的关键。

Abstract: Typically, a modern reinforcement learning (RL) agent solves a task by
updating its neural network parameters to adapt its policy to the task.
Recently, it has been observed that some RL agents can solve a wide range of
new out-of-distribution tasks without parameter updates after pretraining on
some task distribution. When evaluated in a new task, instead of making
parameter updates, the pretrained agent conditions its policy on additional
input called the context, e.g., the agent's interaction history in the new
task. The agent's performance increases as the information in the context
increases, with the agent's parameters fixed. This phenomenon is typically
called in-context RL (ICRL). The pretrained parameters of the agent network
enable the remarkable ICRL phenomenon. However, many ICRL works perform the
pretraining with standard RL algorithms. This raises the central question this
paper aims to address: Why can the RL pretraining algorithm generate network
parameters that enable ICRL? We hypothesize that the parameters capable of ICRL
are minimizers of the pretraining loss. This work provides initial support for
this hypothesis through a case study. In particular, we prove that when a
Transformer is pretrained for policy evaluation, one of the global minimizers
of the pretraining loss can enable in-context temporal difference learning.

</details>


### [182] [Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules](https://arxiv.org/abs/2509.18396)
*Doğay Altınel*

Main category: cs.LG

TL;DR: 本文综述了深度学习优化器的发展历程，从随机梯度下降到最新的优化器如Momentum、AdamW、Sophia和Muon，详细介绍了每种优化器的更新规则、技术特点及其在优化过程中的贡献。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化器的选择对模型训练效果至关重要，随着深度学习的快速发展，多种优化器被提出并受到关注。本文旨在为研究者提供一个全面的资源，理解当前优化器的现状并识别未来发展的潜在方向。

Method: 通过文献综述的方式，按时间顺序分析各种优化器，包括其更新规则、相关概念、变量、技术应用、默认超参数设置等。

Result: 总结了不同优化器的特点及其在优化过程中的作用，并提供了默认超参数设置的参考。

Conclusion: 本文为深度学习优化器的研究提供了全面的综述，同时指出了当前面临的挑战和未来可能的发展方向。

Abstract: Deep learning optimizers are optimization algorithms that enable deep neural
networks to learn. The effectiveness of learning is highly dependent on the
optimizer employed in the training process. Alongside the rapid advancement of
deep learning, a wide range of optimizers with different approaches have been
developed. This study aims to provide a review of various optimizers that have
been proposed and received attention in the literature. From Stochastic
gradient descent to the most recent ones such as Momentum, AdamW, Sophia, and
Muon in chronological order, optimizers are examined individually, and their
distinctive features are highlighted in the study. The update rule of each
optimizer is presented in detail, with an explanation of the associated
concepts and variables. The techniques applied by these optimizers, their
contributions to the optimization process, and their default hyperparameter
settings are also discussed. In addition, insights are offered into the open
challenges encountered in the optimization of deep learning models. Thus, a
comprehensive resource is provided both for understanding the current state of
optimizers and for identifying potential areas of future development.

</details>


### [183] [Explicit Path CGR: Maintaining Sequence Fidelity in Geometric Representations](https://arxiv.org/abs/2509.18408)
*Sarwan Ali*

Main category: cs.LG

TL;DR: 提出了一种新型的信息保留混沌游戏表示方法（R-CGR），解决了传统CGR方法在几何映射中丢失序列信息的问题，实现了完美的序列重建。


<details>
  <summary>Details</summary>
Motivation: 传统CGR方法在生物序列分析中存在序列信息丢失的局限性，R-CGR通过路径编码和有理数精度控制解决了这一问题。

Method: 结合显式路径编码和有理数精度控制，实现了序列的完全恢复，保留了每一步的位置和字符信息。

Result: 在生物序列分类任务中表现优异，与传统序列方法竞争，同时提供可解释的几何可视化。

Conclusion: R-CGR为生物信息学分析提供了新的途径，既保证了准确性，又实现了序列的完全恢复。

Abstract: We present a novel information-preserving Chaos Game Representation (CGR)
method, also called Reverse-CGR (R-CGR), for biological sequence analysis that
addresses the fundamental limitation of traditional CGR approaches - the loss
of sequence information during geometric mapping. Our method introduces
complete sequence recovery through explicit path encoding combined with
rational arithmetic precision control, enabling perfect sequence reconstruction
from stored geometric traces. Unlike purely geometric approaches, our
reversibility is achieved through comprehensive path storage that maintains
both positional and character information at each step. We demonstrate the
effectiveness of R-CGR on biological sequence classification tasks, achieving
competitive performance compared to traditional sequence-based methods while
providing interpretable geometric visualizations. The approach generates
feature-rich images suitable for deep learning while maintaining complete
sequence information through explicit encoding, opening new avenues for
interpretable bioinformatics analysis where both accuracy and sequence recovery
are essential.

</details>


### [184] [Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors](https://arxiv.org/abs/2509.18433)
*Chang Liu,Ladda Thiamwong,Yanjie Fu,Rui Xie*

Main category: cs.LG

TL;DR: 论文提出KANDI方法，结合Kolmogorov-Arnold网络和扩散策略，解决离线强化学习在医疗健康领域应用中的奖励函数推断和策略对齐问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在医疗健康领域应用面临奖励函数定义困难和策略与人类行为对齐的挑战，尤其是在老年人跌倒风险干预中。

Method: 使用Kolmogorov-Arnold网络估计奖励函数，结合扩散策略在Actor-Critic框架中优化动作生成。

Result: KANDI在D4RL基准测试中优于现有方法，并在实际临床试验中验证了其有效性。

Conclusion: KANDI为医疗健康领域的离线强化学习提供了有效解决方案，尤其在活动促进干预策略中表现突出。

Abstract: Utilizing offline reinforcement learning (RL) with real-world clinical data
is getting increasing attention in AI for healthcare. However, implementation
poses significant challenges. Defining direct rewards is difficult, and inverse
RL (IRL) struggles to infer accurate reward functions from expert behavior in
complex environments. Offline RL also encounters challenges in aligning learned
policies with observed human behavior in healthcare applications. To address
challenges in applying offline RL to physical activity promotion for older
adults at high risk of falls, based on wearable sensor activity monitoring, we
introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse
Reinforcement Learning (KANDI). By leveraging the flexible function
approximation in Kolmogorov-Arnold Networks, we estimate reward functions by
learning free-living environment behavior from low-fall-risk older adults
(experts), while diffusion-based policies within an Actor-Critic framework
provide a generative approach for action refinement and efficiency in offline
RL. We evaluate KANDI using wearable activity monitoring data in a two-arm
clinical trial from our Physio-feedback Exercise Program (PEER) study,
emphasizing its practical application in a fall-risk intervention program to
promote physical activity among older adults. Additionally, KANDI outperforms
state-of-the-art methods on the D4RL benchmark. These results underscore
KANDI's potential to address key challenges in offline RL for healthcare
applications, offering an effective solution for activity promotion
intervention strategies in healthcare.

</details>


### [185] [MeshODENet: A Graph-Informed Neural Ordinary Differential Equation Neural Network for Simulating Mesh-Based Physical Systems](https://arxiv.org/abs/2509.18445)
*Kangzheng Liu,Leixin Ma*

Main category: cs.LG

TL;DR: MeshODENet结合GNN和神经ODE，提升复杂结构力学问题的长期预测精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，GNN在长期预测中存在误差累积和不稳定性问题。

Method: 提出MeshODENet框架，结合GNN的空间推理和神经ODE的连续时间建模。

Result: 在非线性变形结构力学问题中显著优于基线模型，计算速度大幅提升。

Conclusion: MeshODENet为复杂结构系统的数据驱动建模提供了通用且高效的方法。

Abstract: The simulation of complex physical systems using a discretized mesh is a
cornerstone of applied mechanics, but traditional numerical solvers are often
computationally prohibitive for many-query tasks. While Graph Neural Networks
(GNNs) have emerged as powerful surrogate models for mesh-based data, their
standard autoregressive application for long-term prediction is often plagued
by error accumulation and instability. To address this, we introduce
MeshODENet, a general framework that synergizes the spatial reasoning of GNNs
with the continuous-time modeling of Neural Ordinary Differential Equations. We
demonstrate the framework's effectiveness and versatility on a series of
challenging structural mechanics problems, including one- and two-dimensional
elastic bodies undergoing large, non-linear deformations. The results
demonstrate that our approach significantly outperforms baseline models in
long-term predictive accuracy and stability, while achieving substantial
computational speed-ups over traditional solvers. This work presents a powerful
and generalizable approach for developing data-driven surrogates to accelerate
the analysis and modeling of complex structural systems.

</details>


### [186] [Fast Linear Solvers via AI-Tuned Markov Chain Monte Carlo-based Matrix Inversion](https://arxiv.org/abs/2509.18452)
*Anton Lebedev,Won Kyung Lee,Soumyadip Ghosh,Olha I. Yaman,Vassilis Kalantzis,Yingdong Lu,Tomasz Nowicki,Shashanka Ubaru,Lior Horesh,Vassil Alexandrov*

Main category: cs.LG

TL;DR: AI驱动的框架推荐MCMC参数，加速Krylov迭代，减少收敛迭代次数。


<details>
  <summary>Details</summary>
Motivation: 解决大规模稀疏线性系统中Krylov迭代收敛慢的问题，通过优化MCMC参数生成高效预处理器。

Method: 使用图神经网络预测预处理器速度，结合贝叶斯采集函数选择最优参数集。

Result: 在未见的病态系统上，框架用50%的搜索预算实现更好的预处理，减少约10%的收敛迭代次数。

Conclusion: AI框架为大规模系统中MCMC预处理器提供了有效路径。

Abstract: Large, sparse linear systems are pervasive in modern science and engineering,
and Krylov subspace solvers are an established means of solving them. Yet
convergence can be slow for ill-conditioned matrices, so practical deployments
usually require preconditioners. Markov chain Monte Carlo (MCMC)-based matrix
inversion can generate such preconditioners and accelerate Krylov iterations,
but its effectiveness depends on parameters whose optima vary across matrices;
manual or grid search is costly. We present an AI-driven framework recommending
MCMC parameters for a given linear system. A graph neural surrogate predicts
preconditioning speed from $A$ and MCMC parameters. A Bayesian acquisition
function then chooses the parameter sets most likely to minimise iterations. On
a previously unseen ill-conditioned system, the framework achieves better
preconditioning with 50\% of the search budget of conventional methods,
yielding about a 10\% reduction in iterations to convergence. These results
suggest a route for incorporating MCMC-based preconditioners into large-scale
systems.

</details>


### [187] [GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting](https://arxiv.org/abs/2509.18457)
*Ebrahim Farahmand,Reza Rahimi Azghan,Nooshin Taheri Chatrudi,Velarie Yaa Ansu-Baidoo,Eric Kim,Gautham Krishna Gudur,Mohit Malu,Owen Krueger,Edison Thomaz,Giulia Pedrielli,Pavan Turaga,Hassan Ghasemzadeh*

Main category: cs.LG

TL;DR: GluMind是一种基于Transformer的多模态框架，用于持续和长期的血糖预测，通过交叉注意力和多尺度注意力机制提升预测性能，并在实验中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决血糖预测中多源数据（如生理和行为信号）的整合问题，以及长期依赖和灾难性遗忘的挑战。

Method: 提出交叉注意力和多尺度注意力机制，结合知识保留技术，以Transformer为基础进行预测。

Result: 在AIREADI数据集上，GluMind的RMSE和MAE分别提升约15%和9%。

Conclusion: GluMind在血糖预测中表现出色，具有稳定性和适应性，适用于不同患者群体。

Abstract: This paper proposes GluMind, a transformer-based multimodal framework
designed for continual and long-term blood glucose forecasting. GluMind devises
two attention mechanisms, including cross-attention and multi-scale attention,
which operate in parallel and deliver accurate predictive performance.
Cross-attention effectively integrates blood glucose data with other
physiological and behavioral signals such as activity, stress, and heart rate,
addressing challenges associated with varying sampling rates and their adverse
impacts on robust prediction. Moreover, the multi-scale attention mechanism
captures long-range temporal dependencies. To mitigate catastrophic forgetting,
GluMind incorporates a knowledge retention technique into the transformer-based
forecasting model. The knowledge retention module not only enhances the model's
ability to retain prior knowledge but also boosts its overall forecasting
performance. We evaluate GluMind on the recently released AIREADI dataset,
which contains behavioral and physiological data collected from healthy people,
individuals with prediabetes, and those with type 2 diabetes. We examine the
performance stability and adaptability of GluMind in learning continuously as
new patient cohorts are introduced. Experimental results show that GluMind
consistently outperforms other state-of-the-art forecasting models, achieving
approximately 15% and 9% improvements in root mean squared error (RMSE) and
mean absolute error (MAE), respectively.

</details>


### [188] [Probabilistic Geometric Principal Component Analysis with application to neural data](https://arxiv.org/abs/2509.18469)
*Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: PGPCA是一种新的降维算法，通过结合非线性流形几何改进PPCA，适用于非欧几里得数据分布。


<details>
  <summary>Details</summary>
Motivation: 传统PPCA基于线性模型，无法处理非线性流形数据，PGPCA旨在解决这一问题。

Method: 开发PGPCA算法，结合非线性流形几何，并推导几何坐标系和EM算法。

Result: PGPCA在模拟和脑数据中优于PPCA，并能测试几何坐标系的有效性。

Conclusion: PGPCA为高维非线性流形数据提供了更有效的降维方法。

Abstract: Dimensionality reduction is critical across various domains of science
including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a
prominent dimensionality reduction method that provides a probabilistic
approach unlike the deterministic approach of PCA and serves as a connection
between PCA and Factor Analysis (FA). Despite their power, PPCA and its
extensions are mainly based on linear models and can only describe the data in
a Euclidean coordinate system. However, in many neuroscience applications, data
may be distributed around a nonlinear geometry (i.e., manifold) rather than
lying in the Euclidean space. We develop Probabilistic Geometric Principal
Component Analysis (PGPCA) for such datasets as a new dimensionality reduction
algorithm that can explicitly incorporate knowledge about a given nonlinear
manifold that is first fitted from these data. Further, we show how in addition
to the Euclidean coordinate system, a geometric coordinate system can be
derived for the manifold to capture the deviations of data from the manifold
and noise. We also derive a data-driven EM algorithm for learning the PGPCA
model parameters. As such, PGPCA generalizes PPCA to better describe data
distributions by incorporating a nonlinear manifold geometry. In simulations
and brain data analyses, we show that PGPCA can effectively model the data
distribution around various given manifolds and outperforms PPCA for such data.
Moreover, PGPCA provides the capability to test whether the new geometric
coordinate system better describes the data than the Euclidean one. Finally,
PGPCA can perform dimensionality reduction and learn the data distribution both
around and on the manifold. These capabilities make PGPCA valuable for
enhancing the efficacy of dimensionality reduction for analysis of
high-dimensional data that exhibit noise and are distributed around a nonlinear
manifold.

</details>


### [189] [Discrete-time diffusion-like models for speech synthesis](https://arxiv.org/abs/2509.18470)
*Xiaozhou Tan,Minghui Zhao,Mattias Cross,Anton Ragni*

Main category: cs.LG

TL;DR: 离散时间扩散模型在语音生成中表现优异，训练和推理更高效且一致。


<details>
  <summary>Details</summary>
Motivation: 解决连续时间扩散模型在训练和推理中的不一致性问题，并探索更高效的离散时间替代方案。

Method: 提出几种离散时间扩散过程变体，包括加性高斯噪声、乘性高斯噪声、模糊噪声及其混合。

Result: 离散时间模型在主观和客观语音质量上与连续时间模型相当，且训练和推理更高效。

Conclusion: 离散时间扩散模型是连续时间模型的有效替代，具有更高的效率和一致性。

Abstract: Diffusion models have attracted a lot of attention in recent years. These
models view speech generation as a continuous-time process. For efficient
training, this process is typically restricted to additive Gaussian noising,
which is limiting. For inference, the time is typically discretized, leading to
the mismatch between continuous training and discrete sampling conditions.
Recently proposed discrete-time processes, on the other hand, usually do not
have these limitations, may require substantially fewer inference steps, and
are fully consistent between training/inference conditions. This paper explores
some diffusion-like discrete-time processes and proposes some new variants.
These include processes applying additive Gaussian noise, multiplicative
Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises.
The experimental results suggest that discrete-time processes offer comparable
subjective and objective speech quality to their widely popular continuous
counterpart, with more efficient and consistent training and inference schemas.

</details>


### [190] [Individualized non-uniform quantization for vector search](https://arxiv.org/abs/2509.18471)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: NVQ是一种高效的非均匀向量量化技术，用于压缩高维嵌入向量，提升搜索效率。


<details>
  <summary>Details</summary>
Motivation: 高维嵌入向量的大尺寸导致存储和检索成本高，需要一种高效的压缩方法。

Method: 使用新颖的非线性方法构建非均匀向量量化器，每个向量单独学习量化器。

Result: NVQ在保持高精度的同时，计算成本低，优于现有技术。

Conclusion: NVQ是一种高效且精确的向量压缩技术，适用于高维数据。

Abstract: Embedding vectors are widely used for representing unstructured data and
searching through it for semantically similar items. However, the large size of
these vectors, due to their high-dimensionality, creates problems for modern
vector search techniques: retrieving large vectors from memory/storage is
expensive and their footprint is costly. In this work, we present NVQ
(non-uniform vector quantization), a new vector compression technique that is
computationally and spatially efficient in the high-fidelity regime. The core
in NVQ is to use novel parsimonious and computationally efficient
nonlinearities for building non-uniform vector quantizers. Critically, these
quantizers are \emph{individually} learned for each indexed vector. Our
experimental results show that NVQ exhibits improved accuracy compared to the
state of the art with a minimal computational cost.

</details>


### [191] [SimpleFold: Folding Proteins is Simpler than You Think](https://arxiv.org/abs/2509.18480)
*Yuyang Wang,Jiarui Lu,Navdeep Jaitly,Josh Susskind,Miguel Angel Bautista*

Main category: cs.LG

TL;DR: SimpleFold是一种基于流匹配的蛋白质折叠模型，仅使用通用Transformer块，挑战了传统依赖复杂领域特定架构的设计。


<details>
  <summary>Details</summary>
Motivation: 传统蛋白质折叠模型依赖复杂的领域特定架构，而SimpleFold探索是否可以通过通用架构实现高性能。

Method: SimpleFold采用标准Transformer块和自适应层，通过生成流匹配目标和结构项进行训练。

Result: SimpleFold-3B在标准基准测试中表现优异，且在集成预测中表现出色，同时在消费级硬件上高效部署。

Conclusion: SimpleFold证明了通用架构在蛋白质折叠中的潜力，为未来研究开辟了新的设计空间。

Abstract: Protein folding models have achieved groundbreaking results typically via a
combination of integrating domain knowledge into the architectural blocks and
training pipelines. Nonetheless, given the success of generative models across
different but related problems, it is natural to question whether these
architectural designs are a necessary condition to build performant models. In
this paper, we introduce SimpleFold, the first flow-matching based protein
folding model that solely uses general purpose transformer blocks. Protein
folding models typically employ computationally expensive modules involving
triangular updates, explicit pair representations or multiple training
objectives curated for this specific domain. Instead, SimpleFold employs
standard transformer blocks with adaptive layers and is trained via a
generative flow-matching objective with an additional structural term. We scale
SimpleFold to 3B parameters and train it on approximately 9M distilled protein
structures together with experimental PDB data. On standard folding benchmarks,
SimpleFold-3B achieves competitive performance compared to state-of-the-art
baselines, in addition SimpleFold demonstrates strong performance in ensemble
prediction which is typically difficult for models trained via deterministic
reconstruction objectives. Due to its general-purpose architecture, SimpleFold
shows efficiency in deployment and inference on consumer-level hardware.
SimpleFold challenges the reliance on complex domain-specific architectures
designs in protein folding, opening up an alternative design space for future
progress.

</details>


### [192] [Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints](https://arxiv.org/abs/2509.18483)
*Abhijit Sen,Illya V. Lukin,Kurt Jacobs,Lev Kaplan,Andrii G. Sotnikov,Denys I. Bondar*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息损失函数的Kolmogorov Arnold Networks（KANs）方法，用于高效预测量子动力学响应，显著减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 量子系统在高维希尔伯特空间中的演化难以建模，传统方法计算成本高，现有神经网络需要大量数据且存在物理解释性问题。

Method: 引入KANs并结合Ehrenfest定理的物理信息损失函数，提出Chain of KANs架构以嵌入时间因果关系。

Result: 仅需5.4%的训练样本（200 vs 3700），优于传统方法，保持物理一致性。

Conclusion: 物理信息KANs在减少数据需求的同时，保持了数学严谨性和物理一致性，优于传统黑盒模型。

Abstract: The prediction of quantum dynamical responses lies at the heart of modern
physics. Yet, modeling these time-dependent behaviors remains a formidable
challenge because quantum systems evolve in high-dimensional Hilbert spaces,
often rendering traditional numerical methods computationally prohibitive.
While large language models have achieved remarkable success in sequential
prediction, quantum dynamics presents a fundamentally different challenge:
forecasting the entire temporal evolution of quantum systems rather than merely
the next element in a sequence. Existing neural architectures such as recurrent
and convolutional networks often require vast training datasets and suffer from
spurious oscillations that compromise physical interpretability. In this work,
we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs)
augmented with physics-informed loss functions that enforce the Ehrenfest
theorems. Our method achieves superior accuracy with significantly less
training data: it requires only 5.4 percent of the samples (200) compared to
Temporal Convolution Networks (3,700). We further introduce the Chain of KANs,
a novel architecture that embeds temporal causality directly into the model
design, making it particularly well-suited for time series modeling. Our
results demonstrate that physics-informed KANs offer a compelling advantage
over conventional black-box models, maintaining both mathematical rigor and
physical consistency while dramatically reducing data requirements.

</details>


### [193] [Hybrid Data can Enhance the Utility of Synthetic Data for Training Anti-Money Laundering Models](https://arxiv.org/abs/2509.18499)
*Rachel Chung,Pratyush Nidhi Sharma,Mikko Siponen,Rohit Vadodaria,Luke Smith*

Main category: cs.LG

TL;DR: 论文提出了一种混合数据集方法，结合合成数据和公开可用的真实特征，以提升反洗钱模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和保密问题，缺乏真实数据训练反洗钱模型，合成数据虽能解决隐私问题，但纯合成数据集训练模型存在挑战。

Method: 使用混合数据集，结合合成数据和公开可用的真实特征。

Result: 混合数据集不仅保护隐私，还提高了模型实用性。

Conclusion: 混合数据集为金融机构提供了一种实用的反洗钱系统增强途径。

Abstract: Money laundering is a critical global issue for financial institutions.
Automated Anti-money laundering (AML) models, like Graph Neural Networks (GNN),
can be trained to identify illicit transactions in real time. A major issue for
developing such models is the lack of access to training data due to privacy
and confidentiality concerns. Synthetically generated data that mimics the
statistical properties of real data but preserves privacy and confidentiality
has been proposed as a solution. However, training AML models on purely
synthetic datasets presents its own set of challenges. This article proposes
the use of hybrid datasets to augment the utility of synthetic datasets by
incorporating publicly available, easily accessible, and real-world features.
These additions demonstrate that hybrid datasets not only preserve privacy but
also improve model utility, offering a practical pathway for financial
institutions to enhance AML systems.

</details>


### [194] [APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation](https://arxiv.org/abs/2509.18521)
*Yuzhen Zhou,Jiajun Li,Yusheng Su,Gowtham Ramesh,Zilin Zhu,Xiang Long,Chenyang Zhao,Jin Pan,Xiaodong Yu,Ze Wang,Kangrui Du,Jialian Wu,Ximeng Sun,Jiang Liu,Qiaolin Yu,Hao Chen,Zicheng Liu,Emad Barsoum*

Main category: cs.LG

TL;DR: APRIL通过部分滚动生成和回收未完成响应，显著提升RL训练效率，减少GPU空闲时间。


<details>
  <summary>Details</summary>
Motivation: RL训练中，滚动生成占用了90%以上的运行时间，且长尾分布导致GPU利用率低，限制了扩展性。

Method: 提出APRIL方法，通过超量请求滚动、提前终止并回收未完成响应，减少GPU空闲。

Result: 实验显示APRIL提升滚动吞吐量最高44%，加速收敛，最终准确率提高8%。

Conclusion: APRIL结合系统与算法优化，提升RL训练效率，适用于多种框架和硬件。

Abstract: Reinforcement learning (RL) has become a cornerstone in advancing large-scale
pre-trained language models (LLMs). Successive generations, including GPT-o
series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale
RL training to enhance reasoning and coding capabilities. To meet the
community's growing RL needs, numerous RL frameworks have been proposed. Most
of these frameworks primarily rely on inference engines for rollout generation
and training engines for policy updates. However, RL training remains
computationally expensive, with rollout generation accounting for more than 90%
of total runtime. In addition, its efficiency is often constrained by the
long-tail distribution of rollout response lengths, where a few lengthy
responses stall entire batches, leaving GPUs idle and underutilized. As model
and rollout sizes continue to grow, this bottleneck increasingly limits
scalability. To address this challenge, we propose Active Partial Rollouts in
Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the
rollout phase, APRIL over-provisions rollout requests, terminates once the
target number of responses is reached, and recycles incomplete responses for
continuation in future steps. This strategy ensures that no rollouts are
discarded while substantially reducing GPU idle time. Experiments show that
APRIL improves rollout throughput by at most 44% across commonly used RL
algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8%
higher final accuracy across tasks. Moreover, APRIL is both framework and
hardware agnostic, already integrated into the slime RL framework, and
deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies
system-level and algorithmic considerations in proposing APRIL, with the aim of
advancing RL training efficiency and inspiring further optimizations in RL
systems.

</details>


### [195] [Reverse-Complement Consistency for DNA Language Models](https://arxiv.org/abs/2509.18529)
*Mingqian Ma*

Main category: cs.LG

TL;DR: 提出了一种名为RCCR的模型无关微调方法，通过惩罚模型对序列及其反向互补序列预测的不一致性，显著提升了DNA语言模型的对称性鲁棒性。


<details>
  <summary>Details</summary>
Motivation: DNA的反向互补序列通常具有相同的生物学意义，但现有DNA语言模型常无法捕捉这种对称性，导致预测不一致，影响可靠性。

Method: 引入RCCR（反向互补一致性正则化），在微调过程中直接惩罚模型对序列及其反向互补序列预测的差异。

Result: 在多个基因组任务上，RCCR显著减少了预测翻转和错误，同时保持或提升了任务准确性。

Conclusion: RCCR通过将生物学先验直接融入学习过程，提供了一种高效且通用的模型微调方法，适用于多种生物学任务。

Abstract: A fundamental property of DNA is that the reverse complement (RC) of a
sequence often carries identical biological meaning. However, state-of-the-art
DNA language models frequently fail to capture this symmetry, producing
inconsistent predictions for a sequence and its RC counterpart, which
undermines their reliability. In this work, we introduce Reverse-Complement
Consistency Regularization (RCCR), a simple and model-agnostic fine-tuning
objective that directly penalizes the divergence between a model's prediction
on a sequence and the aligned prediction on its reverse complement. We evaluate
RCCR across three diverse backbones (Nucleotide Transformer, HyenaDNA,
DNABERT-2) on a wide range of genomic tasks, including sequence classification,
scalar regression, and profile prediction. Our experiments show that RCCR
substantially improves RC robustness by dramatically reducing prediction flips
and errors, all while maintaining or improving task accuracy compared to
baselines such as RC data augmentation and test-time averaging. By integrating
a key biological prior directly into the learning process, RCCR produces a
single, intrinsically robust, and computationally efficient model fine-tuning
recipe for diverse biology tasks.

</details>


### [196] [Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts](https://arxiv.org/abs/2509.18542)
*Qi Wang,Hanyang Peng,Yue Yu*

Main category: cs.LG

TL;DR: Symphony-MoE通过融合多个预训练模型的专家，解决了传统MoE模型专家多样性不足的问题，显著提升了多领域任务和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型通过复制单个预训练模型的FFN层构建专家，导致专家多样性受限。本文旨在利用多个异构预训练模型构建更强大的MoE模型。

Method: 提出Symphony-MoE框架：1）通过层感知融合和激活对齐实现无训练的参数协调；2）轻量级路由器训练协调整个架构。

Result: 实验表明，该方法成功整合异构专家，在多领域任务和泛化性能上显著超越基线。

Conclusion: Symphony-MoE有效解决了专家多样性问题，为构建高性能MoE模型提供了新思路。

Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating
large parameter sets sparsely, minimizing computational overhead. To circumvent
the prohibitive cost of training MoEs from scratch, recent work employs
upcycling, reusing a single pre-trained dense model by replicating its
feed-forward network (FFN) layers into experts. However, this limits expert
diversity, as all experts originate from a single pre-trained dense model. This
paper addresses this limitation by constructing powerful MoE models using
experts sourced from multiple identically-architected but disparate pre-trained
models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact
that these source models occupy disparate, dissonant regions of the parameter
space, making direct upcycling prone to severe performance degradation. To
overcome this, we propose Symphony-MoE, a novel two-stage framework designed to
harmonize these models into a single, coherent expert mixture. First, we
establish this harmony in a training-free manner: we construct a shared
backbone via a layer-aware fusion strategy and, crucially, alleviate parameter
misalignment among experts using activation-based functional alignment.
Subsequently, a single lightweight stage of router training coordinates the
entire architecture. Experiments demonstrate that our method successfully
integrates experts from heterogeneous sources, achieving an MoE model that
significantly surpasses baselines in multi-domain tasks and out-of-distribution
generalization.

</details>


### [197] [Global Minimizers of Sigmoid Contrastive Loss](https://arxiv.org/abs/2509.18552)
*Kiril Bangachev,Guy Bresler,Iliyas Noman,Yury Polyanskiy*

Main category: cs.LG

TL;DR: 论文通过理论分析解释了在Sigmoid损失下同步可训练逆温度和偏置的优势，并提出了新的组合对象$(\mathsf{m}, \mathsf{b}_{\mathsf{rel}})$-Constellations，用于解释SigLIP的成功和模态差距问题。


<details>
  <summary>Details</summary>
Motivation: 研究在对比预训练中同步可训练逆温度和偏置的理论优势，以改进模型性能。

Method: 提出$(\mathsf{m}, \mathsf{b}_{\mathsf{rel}})$-Constellations组合对象，分析其在Sigmoid损失下的作用，并通过实验验证。

Result: 理论解释了SigLIP的成功和模态差距问题，并提出了改进的损失函数重参数化方法。

Conclusion: 同步可训练逆温度和偏置在对比预训练中具有理论优势，改进的损失函数能提升训练动态。

Abstract: The meta-task of obtaining and aligning representations through contrastive
pretraining is steadily gaining importance since its introduction in CLIP and
ALIGN. In this paper we theoretically explain the advantages of synchronizing
with trainable inverse temperature and bias under the sigmoid loss, as
implemented in the recent SigLIP and SigLIP2 models of Google DeepMind.
Temperature and bias can drive the loss function to zero for a rich class of
configurations that we call $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations. $(\mathsf{m},
\mathsf{b}_{\mathsf{rel}})$-Constellations are a novel combinatorial object
related to spherical codes and are parametrized by a margin $\mathsf{m}$ and
relative bias $\mathsf{b}_{\mathsf{rel}}$. We use our characterization of
constellations to theoretically justify the success of SigLIP on retrieval, to
explain the modality gap present in SigLIP, and to identify the necessary
dimension for producing high-quality representations. Finally, we propose a
reparameterization of the sigmoid loss with explicit relative bias, which
improves training dynamics in experiments with synthetic data.

</details>


### [198] [Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia](https://arxiv.org/abs/2509.18568)
*Niharika Tewari,Nguyen Linh Dan Le,Mujie Liu,Jing Ren,Ziqi Xu,Tabinda Sarwar,Veeky Baths,Feng Xia*

Main category: cs.LG

TL;DR: 本文综述了可解释图神经网络（XGNNs）在痴呆症研究中的应用，包括疾病诊断、生物标志物识别和脑网络分析，并讨论了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 痴呆症的临床和生物学异质性使其诊断和亚型区分极具挑战性，而传统图神经网络（GNNs）在鲁棒性、数据稀缺和可解释性方面存在局限，限制了临床应用。

Method: 通过综述XGNNs在痴呆症研究中的应用，提出了一种针对痴呆相关任务的可解释性方法分类，并比较了现有模型在临床场景中的表现。

Result: XGNNs能够识别疾病相关生物标志物、分析脑网络破坏，并为临床医生提供透明见解，但其泛化性和未探索领域仍需改进。

Conclusion: 本文旨在指导未来研究，推动XGNNs在痴呆症研究中的可信、临床意义明确且可扩展的应用。

Abstract: Dementia is a progressive neurodegenerative disorder with multiple
etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal
dementia, and vascular dementia. Its clinical and biological heterogeneity
makes diagnosis and subtype differentiation highly challenging. Graph Neural
Networks (GNNs) have recently shown strong potential in modeling brain
connectivity, but their limited robustness, data scarcity, and lack of
interpretability constrain clinical adoption. Explainable Graph Neural Networks
(XGNNs) have emerged to address these barriers by combining graph-based
learning with interpretability, enabling the identification of disease-relevant
biomarkers, analysis of brain network disruptions, and provision of transparent
insights for clinicians. This paper presents the first comprehensive review
dedicated to XGNNs in dementia research. We examine their applications across
Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and
multi-disease diagnosis. A taxonomy of explainability methods tailored for
dementia-related tasks is introduced, alongside comparisons of existing models
in clinical scenarios. We also highlight challenges such as limited
generalizability, underexplored domains, and the integration of Large Language
Models (LLMs) for early detection. By outlining both progress and open
problems, this review aims to guide future work toward trustworthy, clinically
meaningful, and scalable use of XGNNs in dementia research.

</details>


### [199] [Interaction Topological Transformer for Multiscale Learning in Porous Materials](https://arxiv.org/abs/2509.18573)
*Dong Chen,Jian Liu,Chun-Long Chen,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出了一种名为Interaction Topological Transformer (ITT)的统一框架，用于高效预测多孔材料的多尺度结构-性能关系。


<details>
  <summary>Details</summary>
Motivation: 多孔材料的结构多样性导致其性能预测困难，现有方法难以同时捕捉局部化学环境和全局孔隙网络拓扑的影响。

Method: ITT通过新型交互拓扑结构捕捉多尺度材料信息，结合Transformer架构进行跨尺度联合推理，采用两阶段训练策略（自监督预训练和监督微调）。

Result: ITT在吸附、传输和稳定性等性能预测上达到最先进水平，具有高准确性和可迁移性。

Conclusion: ITT为结构多样化的多孔材料提供了一种可扩展的学习引导发现方法。

Abstract: Porous materials exhibit vast structural diversity and support critical
applications in gas storage, separations, and catalysis. However, predictive
modeling remains challenging due to the multiscale nature of structure-property
relationships, where performance is governed by both local chemical
environments and global pore-network topology. These complexities, combined
with sparse and unevenly distributed labeled data, hinder generalization across
material families. We propose the Interaction Topological Transformer (ITT), a
unified data-efficient framework that leverages novel interaction topology to
capture materials information across multiple scales and multiple levels,
including structural, elemental, atomic, and pairwise-elemental organization.
ITT extracts scale-aware features that reflect both compositional and
relational structure within complex porous frameworks, and integrates them
through a built-in Transformer architecture that supports joint reasoning
across scales. Trained using a two-stage strategy, i.e., self-supervised
pretraining on 0.6 million unlabeled structures followed by supervised
fine-tuning, ITT achieves state-of-the-art, accurate, and transferable
predictions for adsorption, transport, and stability properties. This framework
provides a principled and scalable path for learning-guided discovery in
structurally and chemically diverse porous materials.

</details>


### [200] [DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation](https://arxiv.org/abs/2509.18584)
*Mingchun Sun,Rongqiang Zhao,Jie Liu*

Main category: cs.LG

TL;DR: 提出DS-Diffusion模型，解决现有扩散模型在时间序列生成中的条件引导、分布偏差和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型需重新训练以适应特定条件，且生成数据与真实数据存在分布偏差，推理过程不透明。

Method: 开发基于风格引导核的扩散框架和时间信息分层去噪机制（THD）。

Result: 实验显示预测和判别分数分别降低5.56%和61.55%，分布偏差减少，推理更可解释。

Conclusion: DS-Diffusion提升了模型的灵活性、适应性和可解释性。

Abstract: Diffusion models are the mainstream approach for time series generation
tasks. However, existing diffusion models for time series generation require
retraining the entire framework to introduce specific conditional guidance.
There also exists a certain degree of distributional bias between the generated
data and the real data, which leads to potential model biases in downstream
tasks. Additionally, the complexity of diffusion models and the latent spaces
leads to an uninterpretable inference process. To address these issues, we
propose the data style-guided diffusion model (DS-Diffusion). In the
DS-Diffusion, a diffusion framework based on style-guided kernels is developed
to avoid retraining for specific conditions. The time-information based
hierarchical denoising mechanism (THD) is developed to reduce the
distributional bias between the generated data and the real data. Furthermore,
the generated samples can clearly indicate the data style from which they
originate. We conduct comprehensive evaluations using multiple public datasets
to validate our approach. Experimental results show that, compared to the
state-of-the-art model such as ImagenTime, the predictive score and the
discriminative score decrease by 5.56% and 61.55%, respectively. The
distributional bias between the generated data and the real data is further
reduced, the inference process is also more interpretable. Moreover, by
eliminating the need to retrain the diffusion model, the flexibility and
adaptability of the model to specific conditions are also enhanced.

</details>


### [201] [Reflect before Act: Proactive Error Correction in Language Models](https://arxiv.org/abs/2509.18607)
*Qiuhai Zeng,Sarvesh Rajkumar,Di Wang,Narendra Gyanchandani,Wenbo Yan*

Main category: cs.LG

TL;DR: REBACT通过引入反思步骤提升LLM决策能力，显著提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM决策方法存在错误累积和缺乏自我纠正机制的问题。

Method: 提出REBACT方法，在行动前加入反思步骤以实现即时错误纠正。

Result: 在ALFWorld、WebShop和TextCraft任务中，REBACT显著优于基线，最高提升24%成功率。

Conclusion: REBACT高效且计算成本低，适用于多种交互环境。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
interactive decision-making tasks, but existing methods often struggle with
error accumulation and lack robust self-correction mechanisms. We introduce
"Reflect before Act" (REBACT), a novel approach that enhances LLM-based
decision-making by introducing a critical reflect step prior to taking the next
action. This approach allows for immediate error correction, ensuring smooth
action path and adaptibity to environment feedback. We evaluate REBACT on three
diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results
demonstrate that REBACT significantly outperforms strong baselines, improving
success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld
(achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using
Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's
performance improvements are achieved with only a few modification steps,
demonstrating its computational efficiency.

</details>


### [202] [Flow marching for a generative PDE foundation model](https://arxiv.org/abs/2509.18611)
*Zituo Chen,Sili Deng*

Main category: cs.LG

TL;DR: Flow Marching算法结合神经算子学习与流匹配，构建生成式PDE基础模型，减少长期预测误差并支持不确定性感知生成。


<details>
  <summary>Details</summary>
Motivation: 现有PDE基础模型多为确定性Transformer，缺乏生成灵活性，难以满足科学与工程应用需求。

Method: 提出Flow Marching算法，结合P2VAE和FMT，通过联合采样噪声水平和物理时间步长，学习统一速度场。

Result: 模型在12种PDE家族的250万轨迹上训练，展示长期稳定性和不确定性分层结果。

Conclusion: 生成式PDE基础模型在真实应用中具有重要价值，Flow Marching为高效训练和不确定性建模提供了新方法。

Abstract: Pretraining on large-scale collections of PDE-governed spatiotemporal
trajectories has recently shown promise for building generalizable models of
dynamical systems. Yet most existing PDE foundation models rely on
deterministic Transformer architectures, which lack generative flexibility for
many science and engineering applications. We propose Flow Marching, an
algorithm that bridges neural operator learning with flow matching motivated by
an analysis of error accumulation in physical dynamical systems, and we build a
generative PDE foundation model on top of it. By jointly sampling the noise
level and the physical time step between adjacent states, the model learns a
unified velocity field that transports a noisy current state toward its clean
successor, reducing long-term rollout drift while enabling uncertainty-aware
ensemble generations. Alongside this core algorithm, we introduce a
Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states
into a compact latent space, and an efficient Flow Marching Transformer (FMT)
that combines a diffusion-forcing scheme with latent temporal pyramids,
achieving up to 15x greater computational efficiency than full-length video
diffusion models and thereby enabling large-scale pretraining at substantially
reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE
families and train suites of P2VAEs and FMTs at multiple scales. On downstream
evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot
adaptation, demonstrate long-term rollout stability over deterministic
counterparts, and present uncertainty-stratified ensemble results, highlighting
the importance of generative PDE foundation models for real-world applications.

</details>


### [203] [HyperAdapt: Simple High-Rank Adaptation](https://arxiv.org/abs/2509.18629)
*Abel Gurung,Joseph Campbell*

Main category: cs.LG

TL;DR: HyperAdapt是一种参数高效微调方法，通过行列缩放显著减少可训练参数，性能接近全微调。


<details>
  <summary>Details</summary>
Motivation: 基础模型在多样化任务中表现优异，但微调过程通常需要大量内存和计算资源。参数高效微调（PEFT）方法通过仅更新少量权重来解决这一问题。

Method: HyperAdapt通过对预训练权重矩阵应用行列缩放（通过对角矩阵），仅需n+m个可训练参数即可实现高秩更新。

Result: 在GLUE、算术推理和常识推理基准测试中，HyperAdapt性能接近全微调和其他先进PEFT方法，同时参数数量显著减少。

Conclusion: HyperAdapt是一种高效的微调方法，显著减少计算资源需求，同时保持高性能。

Abstract: Foundation models excel across diverse tasks, but adapting them to
specialized applications often requires fine-tuning, an approach that is memory
and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate
this by updating only a small subset of weights. In this paper, we introduce
HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces
the number of trainable parameters compared to state-of-the-art methods like
LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying
row- and column-wise scaling through diagonal matrices, thereby inducing a
high-rank update while requiring only $n+m$ trainable parameters for an $n
\times m$ matrix. Theoretically, we establish an upper bound on the rank of
HyperAdapt's updates, and empirically, we confirm that it consistently induces
high-rank transformations across model layers. Experiments on GLUE, arithmetic
reasoning, and commonsense reasoning benchmarks with models up to 14B
parameters demonstrate that HyperAdapt matches or nearly matches the
performance of full fine-tuning and state-of-the-art PEFT methods while using
orders of magnitude fewer trainable parameters.

</details>


### [204] [Subspace Clustering of Subspaces: Unifying Canonical Correlation Analysis and Subspace Clustering](https://arxiv.org/abs/2509.18653)
*Paris A. Karakasis,Nicholas D. Sidiropoulos*

Main category: cs.LG

TL;DR: 提出了一种新的高维数据聚类框架SCoS，直接对矩阵数据进行子空间聚类，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统子空间聚类方法假设数据为向量，无法直接处理矩阵数据，SCoS填补了这一空白。

Method: 基于块项分解（BTD）构建三阶张量，联合估计聚类成员和部分共享子空间。

Result: 在真实高光谱数据集上表现出更高的聚类准确性和鲁棒性，尤其在噪声和干扰下。

Conclusion: SCoS在高维应用中具有潜力，能有效捕捉超越单个数据向量的结构。

Abstract: We introduce a novel framework for clustering a collection of tall matrices
based on their column spaces, a problem we term Subspace Clustering of
Subspaces (SCoS). Unlike traditional subspace clustering methods that assume
vectorized data, our formulation directly models each data sample as a matrix
and clusters them according to their underlying subspaces. We establish
conceptual links to Subspace Clustering and Generalized Canonical Correlation
Analysis (GCCA), and clarify key differences that arise in this more general
setting. Our approach is based on a Block Term Decomposition (BTD) of a
third-order tensor constructed from the input matrices, enabling joint
estimation of cluster memberships and partially shared subspaces. We provide
the first identifiability results for this formulation and propose scalable
optimization algorithms tailored to large datasets. Experiments on real-world
hyperspectral imaging datasets demonstrate that our method achieves superior
clustering accuracy and robustness, especially under high noise and
interference, compared to existing subspace clustering techniques. These
results highlight the potential of the proposed framework in challenging
high-dimensional applications where structure exists beyond individual data
vectors.

</details>


### [205] [Towards Rational Pesticide Design with Graph Machine Learning Models for Ecotoxicology](https://arxiv.org/abs/2509.18703)
*Jakub Adamczyk*

Main category: cs.LG

TL;DR: 利用图机器学习加速安全、环保农药的设计，强调生态毒理学，创建了最大的蜜蜂毒性数据集ApisTox，评估了多种ML模型，发现药物化学方法在农药领域泛化性差，需开发领域专用模型。


<details>
  <summary>Details</summary>
Motivation: 受药物发现中计算机方法的启发，旨在开发更安全、环保的农药，解决农药设计中的生态毒理学问题。

Method: 创建ApisTox数据集，评估分子指纹、图核、GNN和预训练Transformer等ML模型在分子图分类中的表现。

Result: 药物化学中成功的ML方法在农药领域泛化性差，凸显领域专用模型和基准的必要性。

Conclusion: 未来工作将开发专用基准测试套件和针对农药发现挑战的ML模型。

Abstract: This research focuses on rational pesticide design, using graph machine
learning to accelerate the development of safer, eco-friendly agrochemicals,
inspired by in silico methods in drug discovery. With an emphasis on
ecotoxicology, the initial contributions include the creation of ApisTox, the
largest curated dataset on pesticide toxicity to honey bees. We conducted a
broad evaluation of machine learning (ML) models for molecular graph
classification, including molecular fingerprints, graph kernels, GNNs, and
pretrained transformers. The results show that methods successful in medicinal
chemistry often fail to generalize to agrochemicals, underscoring the need for
domain-specific models and benchmarks. Future work will focus on developing a
comprehensive benchmarking suite and designing ML models tailored to the unique
challenges of pesticide discovery.

</details>


### [206] [A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications](https://arxiv.org/abs/2509.18714)
*Zhenyu Tao,Wei Xu,Xiaohu You*

Main category: cs.LG

TL;DR: 本文提出了一种广义双模拟度量（GBSM），用于计算马尔可夫决策过程（MDP）对之间的状态相似性，解决了传统双模拟度量（BSM）在多MDP场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统BSM在多MDP任务（如策略迁移）中应用受限，缺乏严格的数学性质分析。本文旨在填补这一理论空白。

Method: 通过形式化定义GBSM，并证明其对称性、跨MDP三角不等式和相同状态空间的距离边界等三个基本性质。

Result: GBSM在策略迁移、状态聚合和基于采样的估计中提供了更严格的理论界限，并改进了样本复杂度。

Conclusion: GBSM在多MDP场景中表现出色，数值实验验证了其理论优势和实践有效性。

Abstract: The bisimulation metric (BSM) is a powerful tool for computing state
similarities within a Markov decision process (MDP), revealing that states
closer in BSM have more similar optimal value functions. While BSM has been
successfully utilized in reinforcement learning (RL) for tasks like state
representation learning and policy exploration, its application to multiple-MDP
scenarios, such as policy transfer, remains challenging. Prior work has
attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis
of its mathematical properties has limited further theoretical progress. In
this work, we formally establish a generalized bisimulation metric (GBSM)
between pairs of MDPs, which is rigorously proven with the three fundamental
properties: GBSM symmetry, inter-MDP triangle inequality, and the distance
bound on identical state spaces. Leveraging these properties, we theoretically
analyse policy transfer, state aggregation, and sampling-based estimation in
MDPs, obtaining explicit bounds that are strictly tighter than those derived
from the standard BSM. Additionally, GBSM provides a closed-form sample
complexity for estimation, improving upon existing asymptotic results based on
BSM. Numerical results validate our theoretical findings and demonstrate the
effectiveness of GBSM in multi-MDP scenarios.

</details>


### [207] [LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection](https://arxiv.org/abs/2509.18719)
*Bo Qu,Zhurong Wang,Daisuke Yagi,Zhen Xu,Yang Zhao,Yinan Shan,Frank Zahradnik*

Main category: cs.LG

TL;DR: 提出了一种结合强化学习和大语言模型的电子商务支付欺诈检测新方法，通过多阶段优化和LLM增强的奖励函数设计，显著提高了检测准确性和零样本能力。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法依赖人工设计奖励函数，复杂且低效。结合LLM的推理和编码能力，可以更高效地优化奖励函数，提升检测效果。

Method: 将交易风险建模为多步马尔可夫决策过程，利用LLM迭代优化奖励函数，强化学习模型在多阶段支付中优化风险检测。

Result: 实验证明，该方法在真实数据上表现出更高的准确性、鲁棒性和零样本能力。

Conclusion: LLM增强的强化学习框架在工业应用中具有巨大潜力，尤其在复杂任务中表现突出。

Abstract: This paper presents a novel approach to e-commerce payment fraud detection by
integrating reinforcement learning (RL) with Large Language Models (LLMs). By
framing transaction risk as a multi-step Markov Decision Process (MDP), RL
optimizes risk detection across multiple payment stages. Crafting effective
reward functions, essential for RL model success, typically requires
significant human expertise due to the complexity and variability in design.
LLMs, with their advanced reasoning and coding capabilities, are well-suited to
refine these functions, offering improvements over traditional methods. Our
approach leverages LLMs to iteratively enhance reward functions, achieving
better fraud detection accuracy and demonstrating zero-shot capability.
Experiments with real-world data confirm the effectiveness, robustness, and
resilience of our LLM-enhanced RL framework through long-term evaluations,
underscoring the potential of LLMs in advancing industrial RL applications.

</details>


### [208] [Theory of periodic convolutional neural network](https://arxiv.org/abs/2509.18744)
*Yuqing Liu*

Main category: cs.LG

TL;DR: 提出了一种新型周期性CNN架构，通过引入周期性边界条件，证明了其在特定高维空间中的逼近能力优于低维设置。


<details>
  <summary>Details</summary>
Motivation: 探索CNN在周期性数据（如包裹域图像分析、物理学习等）中的表现，填补理论空白。

Method: 设计周期性CNN，并理论证明其能逼近高维空间中的脊函数。

Result: 周期性CNN在高维脊函数逼近中表现优异，低维则无法实现。

Conclusion: 周期性CNN在理论和应用上均有重要意义，特别适合高维周期性数据问题。

Abstract: We introduce a novel convolutional neural network architecture, termed the
\emph{periodic CNN}, which incorporates periodic boundary conditions into the
convolutional layers. Our main theoretical contribution is a rigorous
approximation theorem: periodic CNNs can approximate ridge functions depending
on $d-1$ linear variables in a $d$-dimensional input space, while such
approximation is impossible in lower-dimensional ridge settings ($d-2$ or fewer
variables). This result establishes a sharp characterization of the expressive
power of periodic CNNs. Beyond the theory, our findings suggest that periodic
CNNs are particularly well-suited for problems where data naturally admits a
ridge-like structure of high intrinsic dimension, such as image analysis on
wrapped domains, physics-informed learning, and materials science. The work
thus both expands the mathematical foundation of CNN approximation theory and
highlights a class of architectures with surprising and practically relevant
approximation capabilities.

</details>


### [209] [MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model](https://arxiv.org/abs/2509.18751)
*Samuel Yoon,Jongwon Kim,Juyoung Ha,Young Myoung Ko*

Main category: cs.LG

TL;DR: MOMEMTO是一种基于时间序列基础模型（TFM）的异常检测方法，通过引入补丁级内存模块解决过泛化问题，并在多数据集上联合微调，表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的深度模型在时间序列异常检测中容易过泛化，且内存架构训练成本高，难以与TFM有效结合。

Method: 提出MOMEMTO，结合补丁级内存模块和多域训练策略，通过预训练编码器初始化内存项，并利用注意力机制更新。

Result: 在23个单变量基准数据集上，MOMEMTO在AUC和VUS指标上优于基线方法，并提升了主干TFM的性能。

Conclusion: MOMEMTO通过内存模块和多域训练有效解决了过泛化问题，提升了异常检测性能。

Abstract: Recently reconstruction-based deep models have been widely used for time
series anomaly detection, but as their capacity and representation capability
increase, these models tend to over-generalize, often reconstructing unseen
anomalies accurately. Prior works have attempted to mitigate this by
incorporating a memory architecture that stores prototypes of normal patterns.
Nevertheless, these approaches suffer from high training costs and have yet to
be effectively integrated with time series foundation models (TFMs). To address
these challenges, we propose \textbf{MOMEMTO}, a TFM for anomaly detection,
enhanced with a patch-based memory module to mitigate over-generalization. The
memory module is designed to capture representative normal patterns from
multiple domains and enables a single model to be jointly fine-tuned across
multiple datasets through a multi-domain training strategy. MOMEMTO initializes
memory items with latent representations from a pre-trained encoder, organizes
them into patch-level units, and updates them via an attention mechanism. We
evaluate our method using 23 univariate benchmark datasets. Experimental
results demonstrate that MOMEMTO, as a single model, achieves higher scores on
AUC and VUS metrics compared to baseline methods, and further enhances the
performance of its backbone TFM, particularly in few-shot learning scenarios.

</details>


### [210] [Diagonal Linear Networks and the Lasso Regularization Path](https://arxiv.org/abs/2509.18766)
*Raphaël Berthier*

Main category: cs.LG

TL;DR: 论文分析了对角线性网络的训练轨迹与Lasso正则化路径的紧密联系，时间作为逆正则化参数。


<details>
  <summary>Details</summary>
Motivation: 深化对角线性网络的理论分析，揭示其训练轨迹与Lasso路径的关系。

Method: 通过理论分析和模拟实验，验证对角线性网络训练与Lasso路径的联系。

Result: 在单调性假设下，两者联系精确；一般情况下为近似联系。

Conclusion: 对角线性网络的训练轨迹与Lasso正则化路径密切相关，时间扮演逆正则化参数的角色。

Abstract: Diagonal linear networks are neural networks with linear activation and
diagonal weight matrices. Their theoretical interest is that their implicit
regularization can be rigorously analyzed: from a small initialization, the
training of diagonal linear networks converges to the linear predictor with
minimal 1-norm among minimizers of the training loss. In this paper, we deepen
this analysis showing that the full training trajectory of diagonal linear
networks is closely related to the lasso regularization path. In this
connection, the training time plays the role of an inverse regularization
parameter. Both rigorous results and simulations are provided to illustrate
this conclusion. Under a monotonicity assumption on the lasso regularization
path, the connection is exact while in the general case, we show an approximate
connection.

</details>


### [211] [Probabilistic Machine Learning for Uncertainty-Aware Diagnosis of Industrial Systems](https://arxiv.org/abs/2509.18810)
*Arman Mohammadi,Mattias Krysander,Daniel Jung,Erik Frisk*

Main category: cs.LG

TL;DR: 提出了一种基于集成概率机器学习的诊断框架，用于量化预测不确定性，提升数据驱动的一致性诊断性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在故障诊断中应用广泛，但其预测置信度评估不足，尤其在一致性诊断中对误报敏感。

Method: 采用集成概率机器学习方法，自动量化预测不确定性。

Result: 通过消融和对比分析，在多个案例中验证了该方法在诊断指标上的持续改进。

Conclusion: 该框架显著提升了数据驱动一致性诊断的可靠性，尤其在减少误报方面表现突出。

Abstract: Deep neural networks has been increasingly applied in fault diagnostics,
where it uses historical data
  to capture systems behavior, bypassing the need for high-fidelity physical
models.
  However, despite their competence in prediction tasks, these models often
struggle with
  the evaluation of their confidence. This matter is particularly
  important in consistency-based diagnosis where decision logic is highly
sensitive to false alarms.
  To address this challenge, this work presents a diagnostic framework that
uses
  ensemble probabilistic machine learning to
  improve diagnostic characteristics of data driven consistency based diagnosis
  by quantifying and automating the prediction uncertainty.
  The proposed method is evaluated across several case studies using both
ablation
  and comparative analyses, showing consistent improvements across a range of
diagnostic metrics.

</details>


### [212] [Training-Free Data Assimilation with GenCast](https://arxiv.org/abs/2509.18811)
*Thomas Savary,François Rozet,Gilles Louppe*

Main category: cs.LG

TL;DR: 提出了一种基于预训练扩散模型的轻量级通用数据同化方法，无需额外训练，适用于动态系统状态估计。


<details>
  <summary>Details</summary>
Motivation: 数据同化在气象学、海洋学和机器人学等领域广泛应用，但现有方法可能复杂或需要额外训练。

Method: 结合粒子滤波算法，利用预训练的扩散模型（如GenCast）进行数据同化。

Result: 方法在GenCast（扩散模型生成全球集合天气预报）上验证有效。

Conclusion: 该方法为动态系统状态估计提供了一种轻量级且通用的解决方案。

Abstract: Data assimilation is widely used in many disciplines such as meteorology,
oceanography, and robotics to estimate the state of a dynamical system from
noisy observations. In this work, we propose a lightweight and general method
to perform data assimilation using diffusion models pre-trained for emulating
dynamical systems. Our method builds on particle filters, a class of data
assimilation algorithms, and does not require any further training. As a
guiding example throughout this work, we illustrate our methodology on GenCast,
a diffusion-based model that generates global ensemble weather forecasts.

</details>


### [213] [Graph-based Clustering Revisited: A Relaxation of Kernel $k$-Means Perspective](https://arxiv.org/abs/2509.18826)
*Wenlong Lyu,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.LG

TL;DR: 论文提出了一种低秩双随机聚类方法（LoRD），通过放松正交约束来获得概率聚类结果，并进一步引入块对角正则化（B-LoRD）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法过度放松约束，可能影响聚类效果，因此提出更严格的约束方法。

Method: 提出LoRD和B-LoRD模型，通过理论分析放松正交约束并引入块对角正则化，使用梯度下降算法优化。

Result: 实验验证了方法的有效性，代码已开源。

Conclusion: LoRD和B-LoRD在聚类任务中表现优异，理论分析和实验支持其有效性。

Abstract: The well-known graph-based clustering methods, including spectral clustering,
symmetric non-negative matrix factorization, and doubly stochastic
normalization, can be viewed as relaxations of the kernel $k$-means approach.
However, we posit that these methods excessively relax their inherent low-rank,
nonnegative, doubly stochastic, and orthonormal constraints to ensure numerical
feasibility, potentially limiting their clustering efficacy. In this paper,
guided by our theoretical analyses, we propose \textbf{Lo}w-\textbf{R}ank
\textbf{D}oubly stochastic clustering (\textbf{LoRD}), a model that only
relaxes the orthonormal constraint to derive a probabilistic clustering
results. Furthermore, we theoretically establish the equivalence between
orthogonality and block diagonality under the doubly stochastic constraint. By
integrating \textbf{B}lock diagonal regularization into LoRD, expressed as the
maximization of the Frobenius norm, we propose \textbf{B-LoRD}, which further
enhances the clustering performance. To ensure numerical solvability, we
transform the non-convex doubly stochastic constraint into a linear convex
constraint through the introduction of a class probability parameter. We
further theoretically demonstrate the gradient Lipschitz continuity of our LoRD
and B-LoRD enables the proposal of a globally convergent projected gradient
descent algorithm for their optimization. Extensive experiments validate the
effectiveness of our approaches. The code is publicly available at
https://github.com/lwl-learning/LoRD.

</details>


### [214] [Shared-Weights Extender and Gradient Voting for Neural Network Expansion](https://arxiv.org/abs/2509.18842)
*Nikolas Chatzis,Ioannis Kordonis,Manos Theodosis,Petros Maragos*

Main category: cs.LG

TL;DR: 提出Shared-Weights Extender (SWE)和Steepest Voting Distributor (SVoD)方法，有效防止神经网络扩展中新神经元失效问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络扩展中新神经元常失效的问题，提升扩展效率。

Method: SWE通过耦合新老神经元防止失效；SVoD基于梯度分配神经元。

Result: 在四个数据集上验证，有效抑制神经元失效，性能优于其他方法。

Conclusion: SWE和SVoD方法能高效扩展神经网络，提升性能。

Abstract: Expanding neural networks during training is a promising way to augment
capacity without retraining larger models from scratch. However, newly added
neurons often fail to adjust to a trained network and become inactive,
providing no contribution to capacity growth. We propose the Shared-Weights
Extender (SWE), a novel method explicitly designed to prevent inactivity of new
neurons by coupling them with existing ones for smooth integration. In
parallel, we introduce the Steepest Voting Distributor (SVoD), a gradient-based
method for allocating neurons across layers during deep network expansion. Our
extensive benchmarking on four datasets shows that our method can effectively
suppress neuron inactivity and achieve better performance compared to other
expanding methods and baselines.

</details>


### [215] [NGRPO: Negative-enhanced Group Relative Policy Optimization](https://arxiv.org/abs/2509.18851)
*Gongrui Nan,Siye Chen,Jing Huang,Mengyu Lu,Dexun Wang,Chunmei Xie,Weiqi Xiong,Xianzhou Zeng,Qixuan Zhou,Yadong Li,Xingzhong Xu*

Main category: cs.LG

TL;DR: NGRPO通过优势校准和非对称剪裁解决了GRPO在均匀错误组中学习信号丢失的问题，显著提升了数学推理能力。


<details>
  <summary>Details</summary>
Motivation: GRPO在处理均匀正确或错误组时无法学习，尤其是均匀错误组导致零梯度，失去学习信号。

Method: NGRPO引入优势校准（假设虚拟最大奖励样本）和非对称剪裁（放松正样本更新，严格负样本约束）。

Result: 在MATH500、AMC23和AIME2025等数学基准上，NGRPO显著优于PPO、GRPO等基线。

Conclusion: NGRPO能有效从均匀错误中学习，稳定提升数学推理能力。

Abstract: RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs)
across various tasks. However, GRPO, a representative RLVR algorithm, suffers
from a critical limitation: when all responses within a group are either
entirely correct or entirely incorrect, the model fails to learn from these
homogeneous responses. This is particularly problematic for homogeneously
incorrect groups, where GRPO's advantage function yields a value of zero,
leading to null gradients and the loss of valuable learning signals. To
overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy
Optimization), an algorithm designed to convert homogeneous errors into robust
learning signals. First, NGRPO introduces Advantage Calibration. This mechanism
hypothesizes the existence of a virtual maximum-reward sample during advantage
calculation, thereby altering the mean and variance of rewards within a group
and ensuring that the advantages for homogeneously incorrect samples are no
longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the
update magnitude for positive samples while imposing stricter constraints on
that of negative samples. This serves to stabilize the exploration pressure
introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B
demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO,
DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and
AIME2025. These results validate NGRPO's ability to learn from homogeneous
errors, leading to stable and substantial improvements in mathematical
reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.

</details>


### [216] [Exploring Heterophily in Graph-level Tasks](https://arxiv.org/abs/2509.18893)
*Qinhan Hou,Yilun Zheng,Xichun Zhang,Sitao Luan,Jing Tang*

Main category: cs.LG

TL;DR: 论文分析了异质性在图级任务中的影响，提出了频率自适应模型优于频率主导模型的结论。


<details>
  <summary>Details</summary>
Motivation: 研究异质性在图级学习中的作用，填补了节点级任务与图级任务之间的研究空白。

Method: 结合理论分析和实证验证，引入图级标注方案的分类，并通过基于能量的梯度流分析揭示关键见解。

Result: 实验证明频率自适应模型在合成数据集和真实分子属性预测中表现更优。

Conclusion: 为图级学习中的异质性提供了新的理论理解，并指导了高效GNN架构的设计。

Abstract: While heterophily has been widely studied in node-level tasks, its impact on
graph-level tasks remains unclear. We present the first analysis of heterophily
in graph-level learning, combining theoretical insights with empirical
validation. We first introduce a taxonomy of graph-level labeling schemes, and
focus on motif-based tasks within local structure labeling, which is a popular
labeling scheme. Using energy-based gradient flow analysis, we reveal a key
insight: unlike frequency-dominated regimes in node-level tasks, motif
detection requires mixed-frequency dynamics to remain flexible across multiple
spectral components. Our theory shows that motif objectives are inherently
misaligned with global frequency dominance, demanding distinct architectural
considerations. Experiments on synthetic datasets with controlled heterophily
and real-world molecular property prediction support our findings, showing that
frequency-adaptive model outperform frequency-dominated models. This work
establishes a new theoretical understanding of heterophily in graph-level
learning and offers guidance for designing effective GNN architectures.

</details>


### [217] [Enhancing the Effectiveness and Durability of Backdoor Attacks in Federated Learning through Maximizing Task Distinction](https://arxiv.org/abs/2509.18904)
*Zhaoxin Wang,Handing Wang,Cong Tian,Yaochu Jin*

Main category: cs.LG

TL;DR: 提出了一种动态优化后门触发器的方法，以解耦后门任务与主任务，提升攻击持久性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的分布式特性暴露了新的攻击面，现有后门攻击依赖固定触发器，易被防御机制稀释。

Method: 采用min-max框架动态优化触发器，内层最大化毒化与良性样本性能差距，外层注入自适应触发器。

Result: 在计算机视觉和自然语言任务中，该方法在六种防御算法下优于六种现有攻击方法。

Conclusion: 该方法攻击性能优异，且易于与现有后门攻击技术集成。

Abstract: Federated learning allows multiple participants to collaboratively train a
central model without sharing their private data. However, this distributed
nature also exposes new attack surfaces. In particular, backdoor attacks allow
attackers to implant malicious behaviors into the global model while
maintaining high accuracy on benign inputs. Existing attacks usually rely on
fixed patterns or adversarial perturbations as triggers, which tightly couple
the main and backdoor tasks. This coupling makes them vulnerable to dilution by
honest updates and limits their persistence under federated defenses. In this
work, we propose an approach to decouple the backdoor task from the main task
by dynamically optimizing the backdoor trigger within a min-max framework. The
inner layer maximizes the performance gap between poisoned and benign samples,
ensuring that the contributions of benign users have minimal impact on the
backdoor. The outer process injects the adaptive triggers into the local model.
We evaluate our method on both computer vision and natural language tasks, and
compare it with six backdoor attack methods under six defense algorithms.
Experimental results show that our method achieves good attack performance and
can be easily integrated into existing backdoor attack techniques.

</details>


### [218] [Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning](https://arxiv.org/abs/2509.18930)
*Alex Schutz,Victor-Alexandru Darvariu,Efimia Panagiotaki,Bruno Lacerda,Nick Hawes*

Main category: cs.LG

TL;DR: GNARL框架通过将算法学习问题转化为马尔可夫决策过程，结合模仿和强化学习，解决了NAR的局限性，并在多种图问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: NAR存在无法构造有效解、无法处理多解问题、在NP难问题上表现差等问题，需要新的方法。

Method: 将算法学习问题转化为马尔可夫决策过程，提出GNARL框架，结合模仿和强化学习。

Result: 在多个CLRS-30问题上取得高精度，NP难问题上表现优于NAR，且无需专家算法。

Conclusion: GNARL框架扩展了NAR的能力，适用于更广泛的图问题，包括缺乏专家算法的情况。

Abstract: Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks
to execute classic algorithms by supervised learning. Despite its successes,
important limitations remain: inability to construct valid solutions without
post-processing and to reason about multiple correct ones, poor performance on
combinatorial NP-hard problems, and inapplicability to problems for which
strong algorithms are not yet known. To address these limitations, we reframe
the problem of learning algorithm trajectories as a Markov Decision Process,
which imposes structure on the solution construction procedure and unlocks the
powerful tools of imitation and reinforcement learning (RL). We propose the
GNARL framework, encompassing the methodology to translate problem formulations
from NAR to RL and a learning architecture suitable for a wide range of
graph-based problems. We achieve very high graph accuracy results on several
CLRS-30 problems, performance matching or exceeding much narrower NAR
approaches for NP-hard problems and, remarkably, applicability even when
lacking an expert algorithm.

</details>


### [219] [Towards Privacy-Aware Bayesian Networks: A Credal Approach](https://arxiv.org/abs/2509.18949)
*Niccolò Rocchi,Fabio Stella,Cassio de Campos*

Main category: cs.LG

TL;DR: 论文提出使用信用网络（CN）来平衡贝叶斯网络（BN）的隐私和效用，避免传统噪声方法的效用损失。


<details>
  <summary>Details</summary>
Motivation: 隐私问题日益重要，传统BN在公开时可能泄露敏感信息，而现有保护方法（如添加噪声）会显著降低模型效用。

Method: 通过信用网络（CN）对BN进行模糊处理，减少追踪攻击的成功率，同时保持有意义的推理能力。

Result: 实验表明，CN能有效保护隐私且不牺牲模型效用，通过调整超参数可进一步优化隐私保护效果。

Conclusion: CN为开发隐私感知的概率图模型提供了一种原则性、实用且有效的方法。

Abstract: Bayesian networks (BN) are probabilistic graphical models that enable
efficient knowledge representation and inference. These have proven effective
across diverse domains, including healthcare, bioinformatics and economics. The
structure and parameters of a BN can be obtained by domain experts or directly
learned from available data. However, as privacy concerns escalate, it becomes
increasingly critical for publicly released models to safeguard sensitive
information in training data. Typically, released models do not prioritize
privacy by design. In particular, tracing attacks from adversaries can combine
the released BN with auxiliary data to determine whether specific individuals
belong to the data from which the BN was learned. State-of-the-art protection
tecniques involve introducing noise into the learned parameters. While this
offers robust protection against tracing attacks, it significantly impacts the
model's utility, in terms of both the significance and accuracy of the
resulting inferences. Hence, high privacy may be attained at the cost of
releasing a possibly ineffective model. This paper introduces credal networks
(CN) as a novel solution for balancing the model's privacy and utility. After
adapting the notion of tracing attacks, we demonstrate that a CN enables the
masking of the learned BN, thereby reducing the probability of successful
attacks. As CNs are obfuscated but not noisy versions of BNs, they can achieve
meaningful inferences while safeguarding privacy. Moreover, we identify key
learning information that must be concealed to prevent attackers from
recovering the underlying BN. Finally, we conduct a set of numerical
experiments to analyze how privacy gains can be modulated by tuning the CN
hyperparameters. Our results confirm that CNs provide a principled, practical,
and effective approach towards the development of privacy-aware probabilistic
graphical models.

</details>


### [220] [Lift What You Can: Green Online Learning with Heterogeneous Ensembles](https://arxiv.org/abs/2509.18962)
*Kirsten Köbschall,Sebastian Buschjäger,Raphael Fischer,Lisa Hartung,Stefan Kramer*

Main category: cs.LG

TL;DR: HEROS提出了一种异构在线集成方法，通过资源约束下的模型选择和训练策略，平衡预测性能和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有集成方法过于关注预测能力，忽视了计算资源的可持续性需求。

Method: 使用马尔可夫决策过程建模，提出多种策略（如$\zeta$-policy）选择训练模型。

Result: $\zeta$-policy在11个基准数据集上表现优异，资源消耗更低。

Conclusion: HEROS在保持高性能的同时显著提升了资源效率，推动了绿色在线学习的发展。

Abstract: Ensemble methods for stream mining necessitate managing multiple models and
updating them as data distributions evolve. Considering the calls for more
sustainability, established methods are however not sufficiently considerate of
ensemble members' computational expenses and instead overly focus on predictive
capabilities. To address these challenges and enable green online learning, we
propose heterogeneous online ensembles (HEROS). For every training step, HEROS
chooses a subset of models from a pool of models initialized with diverse
hyperparameter choices under resource constraints to train. We introduce a
Markov decision process to theoretically capture the trade-offs between
predictive performance and sustainability constraints. Based on this framework,
we present different policies for choosing which models to train on incoming
data. Most notably, we propose the novel $\zeta$-policy, which focuses on
training near-optimal models at reduced costs. Using a stochastic model, we
theoretically prove that our $\zeta$-policy achieves near optimal performance
while using fewer resources compared to the best performing policy. In our
experiments across 11 benchmark datasets, we find empiric evidence that our
$\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating
highly accurate performance, in some cases even outperforming competitors, and
simultaneously being much more resource-friendly.

</details>


### [221] [Central Limit Theorems for Asynchronous Averaged Q-Learning](https://arxiv.org/abs/2509.18964)
*Xingtu Liu*

Main category: cs.LG

TL;DR: 本文为异步更新的Polyak-Ruppert平均Q学习建立了中心极限定理，包括非渐近和功能性结果。


<details>
  <summary>Details</summary>
Motivation: 研究异步更新下Q学习的统计性质，提供收敛速率和探索质量的明确依赖关系。

Method: 提出非渐近中心极限定理和功能性中心极限定理，分析迭代次数、状态-动作空间大小、折扣因子和探索质量的影响。

Result: 证明了在Wasserstein距离下的收敛速率，并展示了部分和过程弱收敛于布朗运动。

Conclusion: 为异步Q学习的统计性质提供了理论基础，明确了各因素对收敛的影响。

Abstract: This paper establishes central limit theorems for Polyak-Ruppert averaged
Q-learning under asynchronous updates. We present a non-asymptotic central
limit theorem, where the convergence rate in Wasserstein distance explicitly
reflects the dependence on the number of iterations, state-action space size,
the discount factor, and the quality of exploration. In addition, we derive a
functional central limit theorem, showing that the partial-sum process
converges weakly to a Brownian motion.

</details>


### [222] [Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding](https://arxiv.org/abs/2509.18968)
*Zhanglu Yan,Jiayi Mao,Qianhui Liu,Fanfan Li,Gang Pan,Tao Luo,Bowen Zhu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: 本文提出了一种利用光电突触的自然信号衰减实现高效时间编码的SNN方法（Otters），并通过硬件-软件协同设计在GLUE基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统SNN在时间编码（TTFS）中需要昂贵的数字操作计算衰减函数，而光电设备的自然衰减特性可以替代这一过程，从而提升能效。

Method: 利用氧化铟光电突触的自然衰减特性实现TTFS编码，并提出量化神经网络到SNN的转换算法以支持复杂架构（如Transformer）。

Result: 在GLUE基准测试中达到最先进准确率，能效比领先SNN提升1.77倍（基于22nm工艺测量）。

Conclusion: 通过将器件物理特性直接转化为计算原语，建立了高效SNN的新范式。

Abstract: Spiking neural networks (SNNs) promise high energy efficiency, particularly
with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting
at most one spike per neuron. However, such energy advantage is often
unrealized because inference requires evaluating a temporal decay function and
subsequent multiplication with the synaptic weights. This paper challenges this
costly approach by repurposing a physical hardware `bug', namely, the natural
signal decay in optoelectronic devices, as the core computation of TTFS. We
fabricated a custom indium oxide optoelectronic synapse, showing how its
natural physical decay directly implements the required temporal function. By
treating the device's analog output as the fused product of the synaptic weight
and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates
these expensive digital operations. To use the Otters paradigm in complex
architectures like the transformer, which are challenging to train directly due
to the sparsity issue, we introduce a novel quantized neural network-to-SNN
conversion algorithm. This complete hardware-software co-design enables our
model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets
and demonstrates a 1.77$\times$ improvement in energy efficiency over previous
leading SNNs, based on a comprehensive analysis of compute, data movement, and
memory access costs using energy measurements from a commercial 22nm process.
Our work thus establishes a new paradigm for energy-efficient SNNs, translating
fundamental device physics directly into powerful computational primitives. All
codes and data are open source.

</details>


### [223] [Learning From Simulators: A Theory of Simulation-Grounded Learning](https://arxiv.org/abs/2509.18990)
*Carson Dudley,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs是基于模拟数据的预测模型，通过理论证明其实现了贝叶斯最优推断，并在数据有限的情况下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决真实数据稀缺或不可观测时，如何利用模拟数据进行科学预测的问题。

Method: 通过模拟数据训练神经网络，结合贝叶斯推断和理论分析，验证其性能。

Result: SGNNs能够恢复潜在参数，在模型不匹配时保持稳健，并在模型选择任务中表现优于AIC。

Conclusion: SGNNs为数据稀缺场景下的科学预测提供了理论基础和实用框架。

Abstract: Simulation-Grounded Neural Networks (SGNNs) are predictive models trained
entirely on synthetic data from mechanistic simulations. They have achieved
state-of-the-art performance in domains where real-world labels are limited or
unobserved, but lack a formal underpinning.
  We present the foundational theory of simulation-grounded learning. We show
that SGNNs implement amortized Bayesian inference under a simulation prior and
converge to the Bayes-optimal predictor. We derive generalization bounds under
model misspecification and prove that SGNNs can learn unobservable scientific
quantities that empirical methods provably cannot. We also formalize a novel
form of mechanistic interpretability uniquely enabled by SGNNs: by attributing
predictions to the simulated mechanisms that generated them, SGNNs yield
posterior-consistent, scientifically grounded explanations.
  We provide numerical experiments to validate all theoretical predictions.
SGNNs recover latent parameters, remain robust under mismatch, and outperform
classical tools: in a model selection task, SGNNs achieve half the error of AIC
in distinguishing mechanistic dynamics. These results establish SGNNs as a
principled and practical framework for scientific prediction in data-limited
regimes.

</details>


### [224] [CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure](https://arxiv.org/abs/2509.18993)
*Boao Kong,Junzhu Liang,Yuxi Liu,Renjia Deng,Kun Yuan*

Main category: cs.LG

TL;DR: CR-Net是一种创新的低秩残差网络，通过双路径架构和低秩残差重建层激活，解决了现有低秩方法的性能、计算和内存问题。


<details>
  <summary>Details</summary>
Motivation: 现有低秩方法在模型性能、计算开销和内存节省方面存在不足，CR-Net旨在解决这些问题。

Method: 提出双路径架构，结合前一层输出和低秩残差重建层激活，并开发专门的重计算策略以减少内存需求。

Result: 在60M到7B参数的模型规模上，CR-Net表现优于现有低秩框架，且计算资源和内存需求更低。

Conclusion: CR-Net是一种高效的低秩框架，在性能和资源效率上均优于现有方法。

Abstract: Low-rank architectures have become increasingly important for efficient large
language model (LLM) pre-training, providing substantial reductions in both
parameter complexity and memory/computational demands. Despite these
advantages, current low-rank methods face three critical shortcomings: (1)
compromised model performance, (2) considerable computational overhead, and (3)
limited activation memory savings. To address these limitations, we propose
Cross-layer Low-Rank residual Network (CR-Net), an innovative
parameter-efficient framework inspired by our discovery that inter-layer
activation residuals possess low-rank properties. CR-Net implements this
insight through a dual-path architecture that efficiently reconstructs layer
activations by combining previous-layer outputs with their low-rank
differences, thereby maintaining high-rank information with minimal parameters.
We further develop a specialized activation recomputation strategy tailored for
CR-Net that dramatically reduces memory requirements. Extensive pre-training
experiments across model scales from 60M to 7B parameters demonstrate that
CR-Net consistently outperforms state-of-the-art low-rank frameworks while
requiring fewer computational resources and less memory.

</details>


### [225] [Theoretical Foundations of Representation Learning using Unlabeled Data: Statistics and Optimization](https://arxiv.org/abs/2509.18997)
*Pascal Esser,Maximilian Fleissner,Debarghya Ghoshdastidar*

Main category: cs.LG

TL;DR: 论文概述了无标签数据表示学习的最新理论进展，并探讨了深度学习模型与传统理论之间的差异。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型的无监督表示学习原理难以用经典理论分析，需要结合统计和优化的数学工具来理解其成功原因。

Method: 结合统计和优化的数学工具，分析无监督表示学习的理论基础。

Result: 总结了近期在无标签数据表示学习方面的理论进展，并提出了作者的贡献。

Conclusion: 需要进一步结合数学工具来深入理解深度学习模型的表示学习机制。

Abstract: Representation learning from unlabeled data has been extensively studied in
statistics, data science and signal processing with a rich literature on
techniques for dimension reduction, compression, multi-dimensional scaling
among others. However, current deep learning models use new principles for
unsupervised representation learning that cannot be easily analyzed using
classical theories. For example, visual foundation models have found tremendous
success using self-supervision or denoising/masked autoencoders, which
effectively learn representations from massive amounts of unlabeled data.
However, it remains difficult to characterize the representations learned by
these models and to explain why they perform well for diverse prediction tasks
or show emergent behavior. To answer these questions, one needs to combine
mathematical tools from statistics and optimization. This paper provides an
overview of recent theoretical advances in representation learning from
unlabeled data and mentions our contributions in this direction.

</details>


### [226] [Fully Learnable Neural Reward Machines](https://arxiv.org/abs/2509.19017)
*Hazem Dewidar,Elena Umili*

Main category: cs.LG

TL;DR: 提出了一种完全可学习的神经奖励机器（FLNRM），能够端到端学习符号接地函数和自动机，无需依赖先验知识，性能优于基于RNN的方法。


<details>
  <summary>Details</summary>
Motivation: 非马尔可夫强化学习任务需要处理整个轨迹，现有方法依赖预定义的符号接地函数或先验知识，限制了适用性。

Method: 通过完全可学习的神经奖励机器（FLNRM），结合深度强化学习，端到端学习符号接地和自动机。

Result: FLNRM方法在性能上优于基于RNN的现有方法，且更具解释性。

Conclusion: FLNRM提供了一种无需先验知识、易于应用且解释性强的非马尔可夫强化学习解决方案。

Abstract: Non-Markovian Reinforcement Learning (RL) tasks present significant
challenges, as agents must reason over entire trajectories of state-action
pairs to make optimal decisions. A common strategy to address this is through
symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which
provide a structured way to express temporally extended objectives. However,
these approaches often rely on restrictive assumptions -- such as the
availability of a predefined Symbol Grounding (SG) function mapping raw
observations to high-level symbolic representations, or prior knowledge of the
temporal task. In this work, we propose a fully learnable version of Neural
Reward Machines (NRM), which can learn both the SG function and the automaton
end-to-end, removing any reliance on prior knowledge. Our approach is therefore
as easily applicable as classic deep RL (DRL) approaches, while being far more
explainable, because of the finite and compact nature of automata. Furthermore,
we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,
our method outperforms previous approaches based on Recurrent Neural Networks
(RNNs).

</details>


### [227] [OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment](https://arxiv.org/abs/2509.19018)
*Teng Xiao,Zuchao Li,Lefei Zhang*

Main category: cs.LG

TL;DR: OmniBridge是一个统一的多模态框架，支持视觉语言理解、生成和检索，通过轻量级双向潜在对齐模块和两阶段训练策略实现高效多任务处理。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（LLMs）解决方案通常孤立处理任务或需要从头训练，导致高计算成本和跨模态泛化能力有限。

Method: 采用语言中心设计，重用预训练LLMs，引入轻量级双向潜在对齐模块，并提出两阶段解耦训练策略（监督微调+潜在空间对齐和语义引导扩散训练）。

Result: 在多个基准测试中，OmniBridge在视觉语言理解、生成和检索任务上达到竞争性或最先进的性能。

Conclusion: OmniBridge通过潜在空间对齐在共享表示空间下统一多模态建模，展示了高效且通用的多任务处理能力。

Abstract: Recent advances in multimodal large language models (LLMs) have led to
significant progress in understanding, generation, and retrieval tasks.
However, current solutions often treat these tasks in isolation or require
training LLMs from scratch, resulting in high computational costs and limited
generalization across modalities. In this work, we present OmniBridge, a
unified and modular multimodal framework that supports vision-language
understanding, generation, and retrieval within a unified architecture.
OmniBridge adopts a language-centric design that reuses pretrained LLMs and
introduces a lightweight bidirectional latent alignment module. To address the
challenge of task interference, we propose a two-stage decoupled training
strategy: supervised fine-tuning and latent space alignment for aligning LLM
behavior with multimodal reasoning, and semantic-guided diffusion training to
align cross-modal latent spaces via learnable query embeddings. Extensive
experiments across a wide range of benchmarks demonstrate that OmniBridge
achieves competitive or state-of-the-art performance in all three tasks.
Moreover, our results highlight the effectiveness of latent space alignment for
unifying multimodal modeling under a shared representation space. Code and
models are released at https://github.com/xiao-xt/OmniBridge.

</details>


### [228] [Improving Credit Card Fraud Detection through Transformer-Enhanced GAN Oversampling](https://arxiv.org/abs/2509.19032)
*Kashaf Ul Emaan*

Main category: cs.LG

TL;DR: 提出了一种结合GAN和Transformer的混合方法，用于生成高质量的欺诈交易样本，显著提升了信用卡欺诈检测的召回率、F1分数和AUC。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测面临数据集高度不平衡的问题，传统方法如SMOTE生成的样本过于简单，而现有生成模型如CTGAN和TVAE在高维依赖建模上仍有不足。

Method: 采用GAN与Transformer编码器块的混合方法，GAN用于对抗训练生成真实样本，Transformer通过自注意力学习丰富的特征交互。

Result: 在公开数据集上测试表明，该方法在召回率、F1分数和AUC上显著优于传统和生成重采样策略。

Conclusion: Transformer-based GAN有效解决了信用卡欺诈检测中的类别不平衡问题，提升了检测性能。

Abstract: Detection of credit card fraud is an acute issue of financial security
because transaction datasets are highly lopsided, with fraud cases being only a
drop in the ocean. Balancing datasets using the most popular methods of
traditional oversampling such as the Synthetic Minority Oversampling Technique
(SMOTE) generally create simplistic synthetic samples that are not readily
applicable to complex fraud patterns. Recent industry advances that include
Conditional Tabular Generative Adversarial Networks (CTGAN) and Tabular
Variational Autoencoders (TVAE) have demonstrated increased efficiency in
tabular synthesis, yet all these models still exhibit issues with
high-dimensional dependence modelling. Now we will present our hybrid approach
where we use a Generative Adversarial Network (GAN) with a Transformer encoder
block to produce realistic fraudulent transactions samples. The GAN
architecture allows training realistic generators adversarial, and the
Transformer allows the model to learn rich feature interactions by
self-attention. Such a hybrid strategy overcomes the limitations of SMOTE,
CTGAN, and TVAE by producing a variety of high-quality synthetic minority
classes samples. We test our algorithm on the publicly-available Credit Card
Fraud Detection dataset and compare it to conventional and generative
resampling strategies with a variety of classifiers, such as Logistic
Regression (LR), Random Forest (RF), Extreme Gradient Boosting (XGBoost), and
Support Vector Machine (SVM). Findings indicate that our Transformer-based GAN
shows substantial gains in Recall, F1-score and Area Under the Receiver
Operating Characteristic Curve (AUC), which indicates that it is effective in
overcoming the severe class imbalance inherent in the task of fraud detection.

</details>


### [229] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一种基于潜在扩散模型的框架，用于黑盒对抗攻击，通过联合注意力蒸馏策略提高跨架构攻击泛化性和生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒对抗攻击方法依赖特定网络架构或需要大量查询，导致跨架构迁移性差且查询成本高。

Method: JAD利用潜在扩散模型生成对抗样本，通过从CNN和ViT模型中提取的注意力图指导，聚焦跨架构敏感区域。

Result: 实验表明，JAD在攻击泛化性、生成效率和跨架构迁移性上优于现有方法。

Conclusion: JAD为黑盒对抗攻击提供了一种高效且通用的新范式。

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


### [230] [Beyond Backpropagation: Exploring Innovative Algorithms for Energy-Efficient Deep Neural Network Training](https://arxiv.org/abs/2509.19063)
*Przemysław Spyra*

Main category: cs.LG

TL;DR: 论文研究了三种无需反向传播（BP）的深度神经网络训练方法，发现Mono-Forward（MF）算法在准确性和能效上均优于BP。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络因反向传播带来的高计算和能源需求问题，推动可持续AI发展。

Method: 比较了Forward-Forward（FF）、Cascaded-Forward（CaFo）和Mono-Forward（MF）三种算法，通过Optuna优化超参数，并使用NVML和CodeCarbon评估能效。

Result: MF在分类准确率上超越BP，能耗降低41%，训练时间缩短34%。

Conclusion: MF算法为未来高效节能的深度学习提供了明确的数据支持。

Abstract: The rising computational and energy demands of deep neural networks (DNNs),
driven largely by backpropagation (BP), challenge sustainable AI development.
This paper rigorously investigates three BP-free training methods: the
Forward-Forward (FF), Cascaded-Forward (CaFo), and Mono-Forward (MF)
algorithms, tracing their progression from foundational concepts to a
demonstrably superior solution.
  A robust comparative framework was established: each algorithm was
implemented on its native architecture (MLPs for FF and MF, a CNN for CaFo) and
benchmarked against an equivalent BP-trained model. Hyperparameters were
optimized with Optuna, and consistent early stopping criteria were applied
based on validation performance, ensuring all models were optimally tuned
before comparison.
  Results show that MF not only competes with but consistently surpasses BP in
classification accuracy on its native MLPs. Its superior generalization stems
from converging to a more favorable minimum in the validation loss landscape,
challenging the assumption that global optimization is required for
state-of-the-art results. Measured at the hardware level using the NVIDIA
Management Library (NVML) API, MF reduces energy consumption by up to 41% and
shortens training time by up to 34%, translating to a measurably smaller carbon
footprint as estimated by CodeCarbon.
  Beyond this primary result, we present a hardware-level analysis that
explains the efficiency gains: exposing FF's architectural inefficiencies,
validating MF's computationally lean design, and challenging the assumption
that all BP-free methods are inherently more memory-efficient. By documenting
the evolution from FF's conceptual groundwork to MF's synthesis of accuracy and
sustainability, this work offers a clear, data-driven roadmap for future
energy-efficient deep learning.

</details>


### [231] [Diffusion Bridge Variational Inference for Deep Gaussian Processes](https://arxiv.org/abs/2509.19078)
*Jian Xu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: DBVI是一种改进的变分推断方法，通过可学习的初始分布提升深度高斯过程的后验推断效率。


<details>
  <summary>Details</summary>
Motivation: DDVI的固定初始分布与真实后验差距大，导致推断效率低。

Method: DBVI引入数据依赖的初始分布，通过神经网络参数化并优化ELBO目标。

Result: DBVI在多个任务中优于DDVI和其他基线方法。

Conclusion: DBVI显著提升了推断效率和后验质量。

Abstract: Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian
modeling but pose substantial challenges for posterior inference, especially
over inducing variables. Denoising diffusion variational inference (DDVI)
addresses this by modeling the posterior as a time-reversed diffusion from a
simple Gaussian prior. However, DDVI's fixed unconditional starting
distribution remains far from the complex true posterior, resulting in
inefficient inference trajectories and slow convergence. In this work, we
propose Diffusion Bridge Variational Inference (DBVI), a principled extension
of DDVI that initiates the reverse diffusion from a learnable, data-dependent
initial distribution. This initialization is parameterized via an amortized
neural network and progressively adapted using gradients from the ELBO
objective, reducing the posterior gap and improving sample efficiency. To
enable scalable amortization, we design the network to operate on the inducing
inputs, which serve as structured, low-dimensional summaries of the dataset and
naturally align with the inducing variables' shape. DBVI retains the
mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time
SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We
derive a tractable training objective under this formulation and implement DBVI
for scalable inference in large-scale DGPs. Across regression, classification,
and image reconstruction tasks, DBVI consistently outperforms DDVI and other
variational baselines in predictive accuracy, convergence speed, and posterior
quality.

</details>


### [232] [Graph Neural Networks with Similarity-Navigated Probabilistic Feature Copying](https://arxiv.org/abs/2509.19084)
*Asela Hevapathige*

Main category: cs.LG

TL;DR: AxelGNN是一种新型GNN架构，通过相似性门控概率交互和特征级复制机制，解决了传统GNN的特征平滑和异构关系处理问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN存在特征平滑、异构关系处理不足和特征向量处理不灵活的问题。

Method: AxelGNN基于Axelrod文化传播模型，引入相似性门控交互和特征级复制机制，保持全局极化。

Result: 实验表明，AxelGNN在节点分类和影响力估计任务中优于或匹配现有最佳方法。

Conclusion: AxelGNN通过统一框架有效解决了GNN的局限性，适用于多种图结构。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success across
various graph-based tasks. However, they face some fundamental limitations:
feature oversmoothing can cause node representations to become
indistinguishable in deeper networks, they struggle to effectively manage
heterogeneous relationships where connected nodes differ significantly, and
they process entire feature vectors as indivisible units, which limits
flexibility. We seek to address these limitations. We propose AxelGNN, a novel
GNN architecture inspired by Axelrod's cultural dissemination model that
addresses these limitations through a unified framework. AxelGNN incorporates
similarity-gated probabilistic interactions that adaptively promote convergence
or divergence based on node similarity, implements trait-level copying
mechanisms for fine-grained feature aggregation at the segment level, and
maintains global polarization to preserve node distinctiveness across multiple
representation clusters. The model's bistable convergence dynamics naturally
handle both homophilic and heterophilic graphs within a single architecture.
Extensive experiments on node classification and influence estimation
benchmarks demonstrate that AxelGNN consistently outperforms or matches
state-of-the-art GNN methods across diverse graph structures with varying
homophily-heterophily characteristics.

</details>


### [233] [Asymptotically Optimal Problem-Dependent Bandit Policies for Transfer Learning](https://arxiv.org/abs/2509.19098)
*Adrien Prevost,Timothee Mathieu,Odalric-Ambrym Maillard*

Main category: cs.LG

TL;DR: 论文研究了多臂老虎机问题在迁移学习中的非上下文设置，提出了KL-UCB-Transfer策略，并在高斯情况下验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 探索如何在迁移学习框架下利用源分布的先验知识来优化目标分布的多臂老虎机问题。

Method: 提出了KL-UCB-Transfer策略，结合源分布的先验信息（距离界限、样本数量）来优化决策。

Result: 在高斯情况下，KL-UCB-Transfer策略显著优于无先验基线方法。

Conclusion: 当源分布与目标分布足够接近时，KL-UCB-Transfer策略能有效提升性能。

Abstract: We study the non-contextual multi-armed bandit problem in a transfer learning
setting: before any pulls, the learner is given N'_k i.i.d. samples from each
source distribution nu'_k, and the true target distributions nu_k lie within a
known distance bound d_k(nu_k, nu'_k) <= L_k. In this framework, we first
derive a problem-dependent asymptotic lower bound on cumulative regret that
extends the classical Lai-Robbins result to incorporate the transfer parameters
(d_k, L_k, N'_k). We then propose KL-UCB-Transfer, a simple index policy that
matches this new bound in the Gaussian case. Finally, we validate our approach
via simulations, showing that KL-UCB-Transfer significantly outperforms the
no-prior baseline when source and target distributions are sufficiently close.

</details>


### [234] [Algorithms for Adversarially Robust Deep Learning](https://arxiv.org/abs/2509.19100)
*Alexander Robey*

Main category: cs.LG

TL;DR: 论文讨论了深度学习模型在安全关键应用中的鲁棒性问题，提出了对抗性攻击和防御的新方法，并在多个领域取得了先进成果。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型在安全关键应用中的广泛使用，确保其决策对对抗性攻击具有鲁棒性至关重要。

Method: 论文提出了新的对抗性攻击和防御算法，包括计算机视觉中的对抗样本、领域泛化问题以及大型语言模型的越狱攻击。

Result: 在医学影像、分子识别和图像分类等领域实现了最先进的泛化性能，并提出了新的攻击和防御方法。

Conclusion: 论文展示了在设计和实现鲁棒深度学习算法方面的最新进展，为安全关键应用提供了重要支持。

Abstract: Given the widespread use of deep learning models in safety-critical
applications, ensuring that the decisions of such models are robust against
adversarial exploitation is of fundamental importance. In this thesis, we
discuss recent progress toward designing algorithms that exhibit desirable
robustness properties. First, we discuss the problem of adversarial examples in
computer vision, for which we introduce new technical results, training
paradigms, and certification algorithms. Next, we consider the problem of
domain generalization, wherein the task is to train neural networks to
generalize from a family of training distributions to unseen test
distributions. We present new algorithms that achieve state-of-the-art
generalization in medical imaging, molecular identification, and image
classification. Finally, we study the setting of jailbreaking large language
models (LLMs), wherein an adversarial user attempts to design prompts that
elicit objectionable content from an LLM. We propose new attacks and defenses,
which represent the frontier of progress toward designing robust language-based
agents.

</details>


### [235] [DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment](https://arxiv.org/abs/2509.19104)
*Sharan Sahu,Martin T. Wells*

Main category: cs.LG

TL;DR: DRO-REBEL是一种鲁棒的离线RLHF方法，通过Wasserstein、KL和χ²模糊集解决奖励错误指定问题，提供最优参数速率和强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有离线RLHF方法存在过优化问题，模型容易偏离训练时的偏好行为。

Method: 提出DRO-REBEL，基于Fenchel对偶性简化更新，避免复杂计算，支持三种分歧的SGD算法。

Result: 在多个基准测试中表现鲁棒，χ²-REBEL表现最佳，验证了半径与覆盖率的权衡。

Conclusion: DRO-REBEL在鲁棒性和效率上优于现有方法，但需权衡半径与覆盖率。

Abstract: Reinforcement learning with human feedback (RLHF) has become crucial for
aligning Large Language Models (LLMs) with human intent. However, existing
offline RLHF approaches suffer from overoptimization, where models overfit to
reward misspecification and drift from preferred behaviors observed during
training. We introduce DRO-REBEL, a unified family of robust REBEL updates with
type-$p$ Wasserstein, KL, and $\chi^2$ ambiguity sets. Using Fenchel duality,
each update reduces to a simple relative-reward regression, preserving
scalability and avoiding PPO-style clipping or auxiliary value networks. Under
standard linear-reward and log-linear policy classes with a data-coverage
condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants
than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$
rate via a localized Rademacher complexity analysis. The same analysis closes
the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal
parametric rates. We derive practical SGD algorithms for all three divergences:
gradient regularization (Wasserstein), importance weighting (KL), and a fast
1-D dual solve ($\chi^2$). Experiments on Emotion Alignment, the large-scale
ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong
worst-case robustness across unseen preference mixtures, model sizes, and data
scales, with $\chi^2$-REBEL showing consistently strong empirical performance.
A controlled radius--coverage study validates a no-free-lunch trade-off: radii
shrinking faster than empirical divergence concentration rates achieve
minimax-optimal parametric rates but forfeit coverage, while
coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.

</details>


### [236] [Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation](https://arxiv.org/abs/2509.19112)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

TL;DR: CARGO是一种可扩展的多标签因果发现方法，用于稀疏高维事件序列，通过两阶段方法高效推断因果图。


<details>
  <summary>Details</summary>
Motivation: 理解事件序列中的因果关系对医疗或车辆诊断等领域至关重要，但目前仍是一个未解决的挑战。

Method: CARGO使用两个预训练的因果Transformer作为基础模型，并行推断因果图并通过自适应频率融合聚合结果。

Result: 在包含29,100种事件类型和474个不平衡标签的真实汽车故障预测数据集中，CARGO表现出色。

Conclusion: CARGO能够高效进行结构化推理，避免了传统条件独立性测试的高成本。

Abstract: Understanding causality in event sequences where outcome labels such as
diseases or system failures arise from preceding events like symptoms or error
codes is critical. Yet remains an unsolved challenge across domains like
healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label
causal discovery method for sparse, high-dimensional event sequences comprising
of thousands of unique event types. Using two pretrained causal Transformers as
domain-specific foundation models for event sequences. CARGO infers in
parallel, per sequence one-shot causal graphs and aggregates them using an
adaptive frequency fusion to reconstruct the global Markov boundaries of
labels. This two-stage approach enables efficient probabilistic reasoning at
scale while bypassing the intractable cost of full-dataset conditional
independence testing. Our results on a challenging real-world automotive fault
prediction dataset with over 29,100 unique event types and 474 imbalanced
labels demonstrate CARGO's ability to perform structured reasoning.

</details>


### [237] [FedFiTS: Fitness-Selected, Slotted Client Scheduling for Trustworthy Federated Learning in Healthcare AI](https://arxiv.org/abs/2509.19120)
*Ferdinand Kahenga,Antoine Bagula,Sajal K. Das,Patrick Sello*

Main category: cs.LG

TL;DR: FedFiTS是一个结合信任和公平的联邦学习框架，通过动态客户端评分和自适应阈值提升鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在敏感领域（如医疗）中非独立同分布数据、客户端不可靠性和对抗性攻击的挑战。

Method: 采用三阶段参与策略（自由训练、自然选择和团队参与），结合动态评分和自适应阈值。

Result: 在多个数据集上表现优于FedAvg等基线方法，提升了准确性、收敛速度和抗攻击能力。

Conclusion: FedFiTS通过信任感知聚合和公平选择，适用于医疗等实际场景，推动了联邦学习的可扩展性和安全性。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for
privacy-preserving model training, yet deployments in sensitive domains such as
healthcare face persistent challenges from non-IID data, client unreliability,
and adversarial manipulation. This paper introduces FedFiTS, a trust and
fairness-aware selective FL framework that advances the FedFaSt line by
combining fitness-based client election with slotted aggregation. FedFiTS
implements a three-phase participation strategy-free-for-all training, natural
selection, and slotted team participation-augmented with dynamic client
scoring, adaptive thresholding, and cohort-based scheduling to balance
convergence efficiency with robustness. A theoretical convergence analysis
establishes bounds for both convex and non-convex objectives under standard
assumptions, while a communication-complexity analysis shows reductions
relative to FedAvg and other baselines. Experiments on diverse datasets-medical
imaging (X-ray pneumonia), vision benchmarks (MNIST, FMNIST), and tabular
agricultural data (Crop Recommendation)-demonstrate that FedFiTS consistently
outperforms FedAvg, FedRand, and FedPow in accuracy, time-to-target, and
resilience to poisoning attacks. By integrating trust-aware aggregation with
fairness-oriented client selection, FedFiTS advances scalable and secure FL,
making it well suited for real-world healthcare and cross-domain deployments.

</details>


### [238] [Analysis on distribution and clustering of weight](https://arxiv.org/abs/2509.19122)
*Chunming Ye,Wenquan Tian,Yalan Gao,Songzhou Li*

Main category: cs.LG

TL;DR: 论文提出两种向量（标准差向量和聚类向量）来分析大语言模型的权重特征，揭示其相关性和差异。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型的权重特征，以区分不同模型并展示同族模型间的相似性。

Method: 1. 假设权重服从正态分布，通过归一化投影矩阵的标准差形成标准差向量；2. 使用K-Means算法对奇异值分组，形成聚类向量。

Result: 两种向量能有效区分不同模型，且聚类向量在LoRA微调后仍保持与预训练模型的高一致性。

Conclusion: 标准差向量受数据集直接影响，而聚类向量能稳定反映权重间的相关性。

Abstract: The study on architecture and parameter characteristics remains the hot topic
in the research of large language models. In this paper we concern with the
characteristics of weight which are used to analyze the correlations and
differences between models. Two kinds of vectors-standard deviation vector and
clustering vector-are proposed to describe features of models. In the first
case, the weights are assumed to follow normal distribution. The standard
deviation values of projection matrices are normalized to form
Standard-Deviation Vector, representing the distribution characteristics of
models. In the second case, the singular values from each weight projection
matrix are extracted and grouped by K-Means algorithm. The grouped data with
the same type matrix are combined as Clustering Vector to represent the
correlation characteristics of models' weights. The study reveals that these
two vectors can effectively distinguish between different models and clearly
show the similarities among models of the same family. Moreover, after
conducting LoRA fine-tuning with different datasets and models, it is found
that the distribution of weights represented by standard deviation vector is
directly influenced by the dataset, but the correlations between different
weights represented by clustering vector remain unaffected and maintain a high
consistency with the pre-trained model.

</details>


### [239] [PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio](https://arxiv.org/abs/2509.19128)
*Alexandre Piché,Ehsan Kamaloo,Rafael Pardinas,Dzmitry Bahdanau*

Main category: cs.LG

TL;DR: PipelineRL通过并发异步数据生成和模型训练，结合新颖的in-flight权重更新机制，显著提升了LLM训练中的硬件效率和数据新鲜度。


<details>
  <summary>Details</summary>
Motivation: 解决RL方法在LLM训练中难以平衡硬件效率和数据新鲜度的问题。

Method: 采用并发异步数据生成和模型训练，引入in-flight权重更新机制。

Result: 在128 H100 GPU上的实验显示，PipelineRL比传统RL方法学习速度快约2倍，同时保持数据高度on-policy。

Conclusion: PipelineRL为LLM训练提供了一种高效且可扩展的解决方案，并开源了实现。

Abstract: Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning
capabilities of Large Language Models (LLMs). However, effectively scaling
these RL methods presents significant challenges, primarily due to the
difficulty in maintaining high AI accelerator utilization without generating
stale, off-policy data that harms common RL algorithms. This paper introduces
PipelineRL, an approach designed to achieve a superior trade-off between
hardware efficiency and data on-policyness for LLM training. PipelineRL employs
concurrent asynchronous data generation and model training, distinguished by
the novel in-flight weight updates. This mechanism allows the LLM generation
engine to receive updated model weights with minimal interruption during the
generation of token sequences, thereby maximizing both the accelerator
utilization and the freshness of training data. Experiments conducted on
long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL
achieves approximately $\sim 2x$ faster learning compared to conventional RL
baselines while maintaining highly on-policy training data. A scalable and
modular open-source implementation of PipelineRL is also released as a key
contribution.

</details>


### [240] [GSTM-HMU: Generative Spatio-Temporal Modeling for Human Mobility Understanding](https://arxiv.org/abs/2509.19135)
*Wenying Luo,Zhiyuan Lin,Wenhao Xu,Minghao Liu,Zhi Li*

Main category: cs.LG

TL;DR: GSTM-HMU是一个生成式时空框架，用于建模人类移动的语义和时间复杂性，通过四个创新点提升移动性分析。


<details>
  <summary>Details</summary>
Motivation: 人类移动轨迹记录了短期访问模式和长期生活方式规律，但现有方法难以有效建模其复杂性和语义信息。

Method: 框架包含四个关键组件：时空概念编码器、认知轨迹记忆、生活方式概念库和任务导向生成头。

Result: 在四个真实数据集上的实验表明，GSTM-HMU在多个任务上显著优于基线方法。

Conclusion: 生成式建模为构建更鲁棒、可解释和通用的人类移动智能系统提供了基础。

Abstract: Human mobility traces, often recorded as sequences of check-ins, provide a
unique window into both short-term visiting patterns and persistent lifestyle
regularities. In this work we introduce GSTM-HMU, a generative spatio-temporal
framework designed to advance mobility analysis by explicitly modeling the
semantic and temporal complexity of human movement. The framework consists of
four key innovations. First, a Spatio-Temporal Concept Encoder (STCE)
integrates geographic location, POI category semantics, and periodic temporal
rhythms into unified vector representations. Second, a Cognitive Trajectory
Memory (CTM) adaptively filters historical visits, emphasizing recent and
behaviorally salient events in order to capture user intent more effectively.
Third, a Lifestyle Concept Bank (LCB) contributes structured human preference
cues, such as activity types and lifestyle patterns, to enhance
interpretability and personalization. Finally, task-oriented generative heads
transform the learned representations into predictions for multiple downstream
tasks. We conduct extensive experiments on four widely used real-world
datasets, including Gowalla, WeePlace, Brightkite, and FourSquare, and evaluate
performance on three benchmark tasks: next-location prediction, trajectory-user
identification, and time estimation. The results demonstrate consistent and
substantial improvements over strong baselines, confirming the effectiveness of
GSTM-HMU in extracting semantic regularities from complex mobility data. Beyond
raw performance gains, our findings also suggest that generative modeling
provides a promising foundation for building more robust, interpretable, and
generalizable systems for human mobility intelligence.

</details>


### [241] [Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions](https://arxiv.org/abs/2509.19159)
*Qingfeng Lan,Gautham Vasan,A. Rupam Mahmood*

Main category: cs.LG

TL;DR: 研究探讨了激活函数在神经网络训练动态中的作用及其对强化学习中灾难性遗忘的影响，提出了一种新的激活函数类别——大象激活函数，显著提高了神经网络的抗遗忘能力。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是强化学习中的长期挑战，现有研究多关注算法层面，而对神经网络架构特性的理解不足。本研究旨在填补这一空白。

Method: 通过研究激活函数的梯度稀疏性及其对灾难性遗忘的影响，提出并验证了大象激活函数的有效性。

Result: 实验表明，大象激活函数能显著减少灾难性遗忘，提升强化学习的样本效率和内存效率。

Conclusion: 大象激活函数为缓解灾难性遗忘提供了新的解决方案，推动了强化学习的效率提升。

Abstract: Catastrophic forgetting has remained a significant challenge for efficient
reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While
recent works have proposed effective methods to mitigate this issue, they
mainly focus on the algorithmic side. Meanwhile, we do not fully understand
what architectural properties of neural networks lead to catastrophic
forgetting. This study aims to fill this gap by studying the role of activation
functions in the training dynamics of neural networks and their impact on
catastrophic forgetting in reinforcement learning setup. Our study reveals
that, besides sparse representations, the gradient sparsity of activation
functions also plays an important role in reducing forgetting. Based on this
insight, we propose a new class of activation functions, elephant activation
functions, that can generate both sparse outputs and sparse gradients. We show
that by simply replacing classical activation functions with elephant
activation functions in the neural networks of value-based algorithms, we can
significantly improve the resilience of neural networks to catastrophic
forgetting, thus making reinforcement learning more sample-efficient and
memory-efficient.

</details>


### [242] [Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws](https://arxiv.org/abs/2509.19189)
*Binghui Li,Fengling Chen,Zixun Huang,Lean Wang,Lei Wu*

Main category: cs.LG

TL;DR: 论文提出了功能性缩放定律（FSL），用于描述训练过程中学习率计划（LRS）对损失动态的影响，并通过理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注最终损失，忽略了训练过程中的损失动态和学习率计划的影响，本文旨在填补这一空白。

Method: 通过教师-学生核回归设置和随机梯度下降（SGD）训练，结合内在时间视角和随机微分方程（SDE）建模，提出FSL。

Result: FSL能够显式捕捉LRS的影响，并验证了高容量模型更高效、学习率衰减提升效率等实践。

Conclusion: FSL框架深化了对LLM预训练动态的理解，为大规模模型训练提供了优化思路。

Abstract: Scaling laws have played a cornerstone role in guiding the training of large
language models (LLMs). However, most existing works on scaling laws primarily
focus on the final-step loss, overlooking the loss dynamics during the training
process and, crucially, the impact of learning rate schedule (LRS). In this
paper, we aim to bridge this gap by studying a teacher-student kernel
regression setup trained via online stochastic gradient descent (SGD).
Leveraging a novel intrinsic time viewpoint and stochastic differential
equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL),
which characterizes the evolution of population risk during the training
process for general LRSs. Remarkably, the impact of the LRSs is captured
through an explicit convolution-type functional term, making their effects
fully tractable. To illustrate the utility of FSL, we analyze three widely used
LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under
both data-limited and compute-limited regimes. We provide theoretical
justification for widely adopted empirical practices in LLMs pre-training such
as (i) higher-capacity models are more data- and compute-efficient; (ii)
learning rate decay can improve training efficiency; (iii) WSD-like schedules
can outperform direct-decay schedules. Lastly, we explore the practical
relevance of FSL as a surrogate model for fitting, predicting and optimizing
the loss curves in LLM pre-training, with experiments conducted across model
sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen
the understanding of LLM pre-training dynamics and provide insights for
improving large-scale model training.

</details>


### [243] [A Validation Strategy for Deep Learning Models: Evaluating and Enhancing Robustness](https://arxiv.org/abs/2509.19197)
*Abdul-Rauf Nuhu,Parham Kebria,Vahid Hemmati,Benjamin Lartey,Mahmoud Nabil Mahmoud,Abdollah Homaifar,Edward Tunstel*

Main category: cs.LG

TL;DR: 提出一种通过局部鲁棒性分析从训练数据中提取“弱鲁棒”样本的验证方法，用于评估和改进模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型（如深度学习分类器）在干净数据上表现良好，但对对抗性和常见失真扰动敏感，影响模型可靠性。

Method: 从训练数据中提取易受扰动的“弱鲁棒”样本，作为模型脆弱性的早期敏感指标。

Result: 在CIFAR-10、CIFAR-100和ImageNet上验证了方法的有效性，显著提升了模型在对抗性和常见失真扰动下的可靠性。

Conclusion: 通过“弱鲁棒”样本指导的鲁棒性验证可针对性提升模型性能，增强其在扰动场景下的可靠性。

Abstract: Data-driven models, especially deep learning classifiers often demonstrate
great success on clean datasets. Yet, they remain vulnerable to common data
distortions such as adversarial and common corruption perturbations. These
perturbations can significantly degrade performance, thereby challenging the
overall reliability of the models. Traditional robustness validation typically
relies on perturbed test datasets to assess and improve model performance. In
our framework, however, we propose a validation approach that extracts "weak
robust" samples directly from the training dataset via local robustness
analysis. These samples, being the most susceptible to perturbations, serve as
an early and sensitive indicator of the model's vulnerabilities. By evaluating
models on these challenging training instances, we gain a more nuanced
understanding of its robustness, which informs targeted performance
enhancement. We demonstrate the effectiveness of our approach on models trained
with CIFAR-10, CIFAR-100, and ImageNet, highlighting how robustness validation
guided by weak robust samples can drive meaningful improvements in model
reliability under adversarial and common corruption scenarios.

</details>


### [244] [PPG-Distill: Efficient Photoplethysmography Signals Analysis via Foundation Model Distillation](https://arxiv.org/abs/2509.19215)
*Juntong Ni,Saurabh Kataria,Shengpu Tang,Carl Yang,Xiao Hu,Wei Jin*

Main category: cs.LG

TL;DR: PPG-Distill是一种知识蒸馏框架，通过预测、特征和补丁级蒸馏，将全局和局部知识转移到资源受限设备上，显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决大型PPG基础模型在资源受限设备上部署困难的问题。

Method: 通过预测、特征和补丁级蒸馏，结合形态蒸馏和节律蒸馏，保留局部波形模式和捕捉补丁间时间结构。

Result: 在心率和房颤检测任务中，学生模型性能提升高达21.8%，推理速度提升7倍，内存使用减少19倍。

Conclusion: PPG-Distill实现了在可穿戴设备上高效PPG分析的目标。

Abstract: Photoplethysmography (PPG) is widely used in wearable health monitoring, yet
large PPG foundation models remain difficult to deploy on resource-limited
devices. We present PPG-Distill, a knowledge distillation framework that
transfers both global and local knowledge through prediction-, feature-, and
patch-level distillation. PPG-Distill incorporates morphology distillation to
preserve local waveform patterns and rhythm distillation to capture inter-patch
temporal structures. On heart rate estimation and atrial fibrillation
detection, PPG-Distill improves student performance by up to 21.8% while
achieving 7X faster inference and reducing memory usage by 19X, enabling
efficient PPG analysis on wearables

</details>


### [245] [FedFusion: Federated Learning with Diversity- and Cluster-Aware Encoders for Robust Adaptation under Label Scarcity](https://arxiv.org/abs/2509.19220)
*Ferdinand Kahenga,Antoine Bagula,Patrick Sello,Sajal K. Das*

Main category: cs.LG

TL;DR: FedFusion是一个联邦迁移学习框架，通过结合领域适应和标签稀缺问题，提升联邦学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中的异构特征空间、非独立同分布数据和标签稀缺问题。

Method: 使用多样性/聚类感知编码器（DivEn, DivEn-mix, DivEn-c），结合伪标签和领域自适应迁移，同时保持个性化编码器。

Result: 在多种基准测试中，FedFusion在准确性、鲁棒性和公平性上优于现有方法。

Conclusion: FedFusion通过协调个性化、领域适应和标签效率，为现实约束下的联邦学习提供了有效解决方案。

Abstract: Federated learning in practice must contend with heterogeneous feature
spaces, severe non-IID data, and scarce labels across clients. We present
FedFusion, a federated transfer-learning framework that unifies domain
adaptation and frugal labelling with diversity-/cluster-aware encoders (DivEn,
DivEn-mix, DivEn-c). Labelled teacher clients guide learner clients via
confidence-filtered pseudo-labels and domain-adaptive transfer, while clients
maintain personalised encoders tailored to local data. To preserve global
coherence under heterogeneity, FedFusion employs similarity-weighted classifier
coupling (with optional cluster-wise averaging), mitigating dominance by
data-rich sites and improving minority-client performance. The frugal-labelling
pipeline combines self-/semi-supervised pretext training with selective
fine-tuning, reducing annotation demands without sharing raw data. Across
tabular and imaging benchmarks under IID, non-IID, and label-scarce regimes,
FedFusion consistently outperforms state-of-the-art baselines in accuracy,
robustness, and fairness while maintaining comparable communication and
computation budgets. These results show that harmonising personalisation,
domain adaptation, and label efficiency is an effective recipe for robust
federated learning under real-world constraints.

</details>


### [246] [Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models](https://arxiv.org/abs/2509.19222)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 论文系统研究了开源T2V模型的延迟和能耗，提出了计算模型并验证了其预测，为可持续视频生成系统提供了参考。


<details>
  <summary>Details</summary>
Motivation: 现有T2V生成系统计算成本高且能耗不明确，需系统研究以优化设计。

Method: 开发计算模型预测缩放规律，并通过实验验证，扩展分析六种T2V模型。

Result: 验证了空间和时间维度的二次增长及去噪步骤的线性缩放规律。

Conclusion: 研究为设计和部署可持续视频生成系统提供了基准和实用见解。

Abstract: Recent advances in text-to-video (T2V) generation have enabled the creation
of high-fidelity, temporally coherent clips from natural language prompts. Yet
these systems come with significant computational costs, and their energy
demands remain poorly understood. In this paper, we present a systematic study
of the latency and energy consumption of state-of-the-art open-source T2V
models. We first develop a compute-bound analytical model that predicts scaling
laws with respect to spatial resolution, temporal length, and denoising steps.
We then validate these predictions through fine-grained experiments on
WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and
linear scaling with the number of denoising steps. Finally, we extend our
analysis to six diverse T2V models, comparing their runtime and energy profiles
under default settings. Our results provide both a benchmark reference and
practical insights for designing and deploying more sustainable generative
video systems.

</details>


### [247] [Study Design and Demystification of Physics Informed Neural Networks for Power Flow Simulation](https://arxiv.org/abs/2509.19233)
*Milad Leyli-abadi,Antoine Marot,Jérôme Picault*

Main category: cs.LG

TL;DR: 本文通过消融研究探讨了混合策略在电力流模拟中的应用，评估了不同物理约束整合方法对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 在能源转型背景下，电网面临更大的不确定性和操作风险，传统物理求解器速度慢，机器学习模型作为快速替代方案需要更好地遵守物理定律。

Method: 使用自定义基准测试管道LIPS，评估了从简单多层感知器到基于图的网络的不同模型架构，以及物理约束作为正则化项或无监督损失的整合策略。

Result: 结果表明，物理知识的整合在不同维度（准确性、物理合规性、工业准备度和分布外泛化）上对性能有显著影响。

Conclusion: 研究为电力流模拟中的混合模型设计提供了实用指导，所有实现均可复现。

Abstract: In the context of the energy transition, with increasing integration of
renewable sources and cross-border electricity exchanges, power grids are
encountering greater uncertainty and operational risk. Maintaining grid
stability under varying conditions is a complex task, and power flow simulators
are commonly used to support operators by evaluating potential actions before
implementation. However, traditional physical solvers, while accurate, are
often too slow for near real-time use. Machine learning models have emerged as
fast surrogates, and to improve their adherence to physical laws (e.g.,
Kirchhoff's laws), they are often trained with embedded constraints which are
also known as physics-informed or hybrid models. This paper presents an
ablation study to demystify hybridization strategies, ranging from
incorporating physical constraints as regularization terms or unsupervised
losses, and exploring model architectures from simple multilayer perceptrons to
advanced graph-based networks enabling the direct optimization of physics
equations. Using our custom benchmarking pipeline for hybrid models called
LIPS, we evaluate these models across four dimensions: accuracy, physical
compliance, industrial readiness, and out-of-distribution generalization. The
results highlight how integrating physical knowledge impacts performance across
these criteria. All the implementations are reproducible and provided in the
corresponding Github page.

</details>


### [248] [Stability and Generalization of Adversarial Diffusion Training](https://arxiv.org/abs/2509.19234)
*Hesam Hosseini,Ying Cao,Ali H. Sayed*

Main category: cs.LG

TL;DR: 该论文通过稳定性分析研究了去中心化网络中对抗训练在扩散策略下的泛化性能，发现泛化误差随对抗扰动强度和训练步数增加而增长。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽增强模型鲁棒性，但常伴随鲁棒过拟合和泛化差距扩大问题，而去中心化网络中对抗训练的泛化性质尚未被研究。

Method: 采用稳定性分析方法，针对凸损失函数下的扩散策略进行理论推导。

Result: 理论推导表明泛化误差随对抗扰动强度和训练步数增加而增长，数值实验在逻辑回归中验证了这一结论。

Conclusion: 该研究填补了去中心化对抗训练泛化分析的空白，为实际应用提供了理论支持。

Abstract: Algorithmic stability is an established tool for analyzing generalization.
While adversarial training enhances model robustness, it often suffers from
robust overfitting and an enlarged generalization gap. Although recent work has
established the convergence of adversarial training in decentralized networks,
its generalization properties remain unexplored. This work presents a
stability-based generalization analysis of adversarial training under the
diffusion strategy for convex losses. We derive a bound showing that the
generalization error grows with both the adversarial perturbation strength and
the number of training steps, a finding consistent with single-agent case but
novel for decentralized settings. Numerical experiments on logistic regression
validate these theoretical predictions.

</details>


### [249] [What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT](https://arxiv.org/abs/2509.19284)
*Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn*

Main category: cs.LG

TL;DR: 研究发现，长链式思考（CoT）并不总是更有效，失败步骤比例（FSF）是预测准确性的关键指标。


<details>
  <summary>Details</summary>
Motivation: 探索长链式思考（CoT）的有效性，并分析其结构与准确性之间的关系。

Method: 通过系统评估十种大型推理模型（LRMs），引入图视图分析CoT结构，并设计两种干预措施验证因果关系。

Result: 发现失败步骤比例（FSF）比长度和回顾比例更能预测准确性，去除失败分支显著提高准确性。

Conclusion: 有效的CoT应减少失败步骤，支持结构感知的测试时扩展，而非盲目生成长链式思考。

Abstract: Large reasoning models (LRMs) spend substantial test-time compute on long
chain-of-thought (CoT) traces, but what *characterizes* an effective CoT
remains unclear. While prior work reports gains from lengthening CoTs and
increasing review (revisiting earlier steps) via appended *wait* tokens, recent
studies suggest that shorter thinking can outperform longer traces. We
therefore conduct a systematic evaluation across ten LRMs on math and
scientific reasoning. Contrary to the "longer-is-better" narrative, we find
that both naive CoT lengthening and increased review are associated with
*lower* accuracy.
  As CoT unfolds step by step, token-level metrics can conflate verbosity with
process quality. We introduce a graph view of CoT to extract structure and
identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of
steps in abandoned branches-that consistently outpredicts length and review
ratio for correctness across models. To probe causality, we design two
interventions. First, we rank candidate CoTs by each metric at test time, where
FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed
branches, which significantly improves accuracy, indicating that failed
branches bias subsequent reasoning. Taken together, these results characterize
effective CoTs as those that *fail less* and support *structure-aware*
test-time scaling over indiscriminately generating long CoT.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [250] [PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies](https://arxiv.org/abs/2509.18282)
*Jesse Zhang,Marius Memmel,Kevin Kim,Dieter Fox,Jesse Thomason,Fabio Ramos,Erdem Bıyık,Abhishek Gupta,Anqi Li*

Main category: cs.RO

TL;DR: PEEK利用视觉语言模型（VLMs）预测关键点和任务相关掩码，提升机器人策略的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器人策略在同时学习注意力、动作和执行时的泛化问题。

Method: 通过VLMs预测统一的点基中间表示（关键点和掩码），并自动生成标注数据。

Result: 在真实世界评估中，PEEK显著提升零样本泛化能力，如3D策略性能提升41.4倍。

Conclusion: PEEK通过VLMs简化语义和视觉复杂性，为策略提供最小化提示（哪里、什么、如何）。

Abstract: Robotic manipulation policies often fail to generalize because they must
simultaneously learn where to attend, what actions to take, and how to execute
them. We argue that high-level reasoning about where and what can be offloaded
to vision-language models (VLMs), leaving policies to specialize in how to act.
We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which
fine-tunes VLMs to predict a unified point-based intermediate representation:
1. end-effector paths specifying what actions to take, and 2. task-relevant
masks indicating where to focus. These annotations are directly overlaid onto
robot observations, making the representation policy-agnostic and transferable
across architectures. To enable scalable training, we introduce an automatic
annotation pipeline, generating labeled data across 20+ robot datasets spanning
9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot
generalization, including a 41.4x real-world improvement for a 3D policy
trained only in simulation, and 2-3.5x gains for both large VLAs and small
manipulation policies. By letting VLMs absorb semantic and visual complexity,
PEEK equips manipulation policies with the minimal cues they need--where, what,
and how. Website at https://peek-robot.github.io/.

</details>


### [251] [Fine-Tuning Robot Policies While Maintaining User Privacy](https://arxiv.org/abs/2509.18311)
*Benjamin A. Christie,Sagar Parekh,Dylan P. Losey*

Main category: cs.RO

TL;DR: PRoP是一个模型无关的框架，用于实现个性化且私密的机器人策略，通过用户唯一密钥保护隐私。


<details>
  <summary>Details</summary>
Motivation: 现有通用机器人策略在个性化过程中会泄露用户偏好数据，需要一种保护隐私的方法。

Method: 为每个用户分配唯一密钥，通过数学变换网络权重实现个性化策略切换。

Result: PRoP在模仿学习、强化学习和分类任务中表现优异，优于现有编码器方法。

Conclusion: PRoP提供了一种有效且实用的隐私保护个性化机器人策略解决方案。

Abstract: Recent works introduce general-purpose robot policies. These policies provide
a strong prior over how robots should behave -- e.g., how a robot arm should
manipulate food items. But in order for robots to match an individual person's
needs, users typically fine-tune these generalized policies -- e.g., showing
the robot arm how to make their own preferred dinners. Importantly, during the
process of personalizing robots, end-users leak data about their preferences,
habits, and styles (e.g., the foods they prefer to eat). Other agents can
simply roll-out the fine-tuned policy and see these personally-trained
behaviors. This leads to a fundamental challenge: how can we develop robots
that personalize actions while keeping learning private from external agents?
We here explore this emerging topic in human-robot interaction and develop
PRoP, a model-agnostic framework for personalized and private robot policies.
Our core idea is to equip each user with a unique key; this key is then used to
mathematically transform the weights of the robot's network. With the correct
key, the robot's policy switches to match that user's preferences -- but with
incorrect keys, the robot reverts to its baseline behaviors. We show the
general applicability of our method across multiple model types in imitation
learning, reinforcement learning, and classification tasks. PRoP is practically
advantageous because it retains the architecture and behaviors of the original
policy, and experimentally outperforms existing encoder-based approaches. See
videos and code here: https://prop-icra26.github.io.

</details>


### [252] [Haptic Communication in Human-Human and Human-Robot Co-Manipulation](https://arxiv.org/abs/2509.18327)
*Katherine H. Allen,Chris Rogers,Elaine S. Short*

Main category: cs.RO

TL;DR: 研究比较了人类-人类和人类-机器人协作中通过物体运动传递的触觉通信，发现人类协作更流畅，并提出了改进机器人触觉信号传递的建议。


<details>
  <summary>Details</summary>
Motivation: 探索人类协作中的触觉通信机制，并将其应用于改进机器人协作的流畅性和准确性。

Method: 通过IMU追踪物体运动，比较人类-人类和人类-机器人协作的运动模式，并结合问卷调查评估协作效果。

Result: 人类-人类协作更流畅，IMU数据捕捉到运动模式的客观差异。

Conclusion: 未来研究应改进机器人的触觉信号传递能力，以提升协作效果。

Abstract: When a human dyad jointly manipulates an object, they must communicate about
their intended motion plans. Some of that collaboration is achieved through the
motion of the manipulated object itself, which we call "haptic communication."
In this work, we captured the motion of human-human dyads moving an object
together with one participant leading a motion plan about which the follower is
uninformed. We then captured the same human participants manipulating the same
object with a robot collaborator. By tracking the motion of the shared object
using a low-cost IMU, we can directly compare human-human shared manipulation
to the motion of those same participants interacting with the robot.
Intra-study and post-study questionnaires provided participant feedback on the
collaborations, indicating that the human-human collaborations are
significantly more fluent, and analysis of the IMU data indicates that it
captures objective differences in the motion profiles of the conditions. The
differences in objective and subjective measures of accuracy and fluency
between the human-human and human-robot trials motivate future research into
improving robot assistants for physical tasks by enabling them to send and
receive anthropomorphic haptic signals.

</details>


### [253] [The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020](https://arxiv.org/abs/2509.18330)
*Marsette Vona*

Main category: cs.RO

TL;DR: 将火星2020探测车图像与轨道数据融合，生成交互式3D地形可视化，用于任务规划和公众展示。


<details>
  <summary>Details</summary>
Motivation: 结合2D和3D数据，为火星任务科学家提供战术和战略规划工具，同时向公众展示探测成果。

Method: 自动构建地形上下文网格，融合探测车图像和轨道高程及颜色数据。

Result: 生成交互式3D地形可视化，支持任务规划和公众访问。

Conclusion: 该方法成功应用于火星任务，提升了科学规划和公众参与的效果。

Abstract: The Landform contextual mesh fuses 2D and 3D data from up to thousands of
Mars 2020 rover images, along with orbital elevation and color maps from Mars
Reconnaissance Orbiter, into an interactive 3D terrain visualization.
Contextual meshes are built automatically for each rover location during
mission ground data system processing, and are made available to mission
scientists for tactical and strategic planning in the Advanced Science
Targeting Tool for Robotic Operations (ASTTRO). A subset of them are also
deployed to the "Explore with Perseverance" public access website.

</details>


### [254] [Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation](https://arxiv.org/abs/2509.18342)
*Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara*

Main category: cs.RO

TL;DR: 提出了一种语义粒子滤波器，结合稳定的物体级检测（如葡萄树干和支撑杆）来改进结构化户外环境中的机器人定位。


<details>
  <summary>Details</summary>
Motivation: 在葡萄园等结构化户外环境中，LiDAR方法因重复的行几何和感知混淆而失效，需要更鲁棒的定位方法。

Method: 通过将检测到的地标投影到鸟瞰图并与LiDAR扫描融合生成语义观测，利用语义墙连接相邻地标以减少行混淆，并在语义稀疏区域引入噪声GPS先验。

Result: 实验表明，该方法能在正确行内保持定位，从AMCL失败的偏差中恢复，并优于RTAB-Map等视觉SLAM方法。

Conclusion: 语义粒子滤波器在葡萄园等结构化环境中有效解决了LiDAR的局限性，提升了定位精度和鲁棒性。

Abstract: Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.

</details>


### [255] [AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback](https://arxiv.org/abs/2509.18384)
*Yunhao Yang,Junyuan Hong,Gabriel Jacob Perin,Zhiwen Fan,Li Yin,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: LAD-VF是一种无需微调的框架，利用形式验证反馈自动优化提示，提升LLM驱动的规划系统的安全性和合规性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的规划系统在物理世界中常因幻觉或弱对齐而违反安全约束，传统方法成本高且依赖微调。

Method: LAD-VF通过形式验证反馈和LLM-AutoDiff迭代优化提示，而非模型参数。

Result: 实验显示，LAD-VF将任务成功率从60%提升至90%以上。

Conclusion: LAD-VF为可信赖的LLM驱动控制系统提供了可扩展且可解释的解决方案。

Abstract: Large language models (LLMs) can translate natural language instructions into
executable action plans for robotics, autonomous driving, and other domains.
Yet, deploying LLM-driven planning in the physical world demands strict
adherence to safety and regulatory constraints, which current models often
violate due to hallucination or weak alignment. Traditional data-driven
alignment methods, such as Direct Preference Optimization (DPO), require costly
human labeling, while recent formal-feedback approaches still depend on
resource-intensive fine-tuning. In this paper, we propose LAD-VF, a
fine-tuning-free framework that leverages formal verification feedback for
automated prompt engineering. By introducing a formal-verification-informed
text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts
rather than model parameters. This yields three key benefits: (i) scalable
adaptation without fine-tuning; (ii) compatibility with modular LLM
architectures; and (iii) interpretable refinement via auditable prompts.
Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF
substantially enhances specification compliance, improving success rates from
60% to over 90%. Our method thus presents a scalable and interpretable pathway
toward trustworthy, formally-verified LLM-driven control systems.

</details>


### [256] [Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections](https://arxiv.org/abs/2509.18407)
*Navya Tiwari,Joseph Vazhaeparampil,Victoria Preston*

Main category: cs.RO

TL;DR: 论文提出了一种基于POMDP的驾驶员辅助框架，用于无控制交叉口的通行权决策，概率规划器在部分可观测性下表现优于规则基线。


<details>
  <summary>Details</summary>
Motivation: 无控制交叉口因通行权规则模糊、遮挡和驾驶员行为不可预测导致大量事故，现有研究较少关注人类驾驶车辆的辅助导航系统。

Method: 将问题建模为POMDP，在自定义仿真环境中评估四种决策方法（FSM、QMDP、POMCP、DESPOT）。

Result: 概率规划器（如POMCP和DESPOT）优于规则基线，最高实现97.5%无碰撞导航，POMCP注重安全，DESPOT兼顾效率与实时性。

Conclusion: 不确定性感知规划对驾驶辅助至关重要，未来需整合传感器融合和环境感知模块以实现实时部署。

Abstract: Uncontrolled intersections account for a significant fraction of roadway
crashes due to ambiguous right-of-way rules, occlusions, and unpredictable
driver behavior. While autonomous vehicle research has explored
uncertainty-aware decision making, few systems exist to retrofit human-operated
vehicles with assistive navigation support. We present a driver-assist
framework for right-of-way reasoning at uncontrolled intersections, formulated
as a Partially Observable Markov Decision Process (POMDP). Using a custom
simulation testbed with stochastic traffic agents, pedestrians, occlusions, and
adversarial scenarios, we evaluate four decision-making approaches: a
deterministic finite state machine (FSM), and three probabilistic planners:
QMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform
the rule-based baseline, achieving up to 97.5 percent collision-free navigation
under partial observability, with POMCP prioritizing safety and DESPOT
balancing efficiency and runtime feasibility. Our findings highlight the
importance of uncertainty-aware planning for driver assistance and motivate
future integration of sensor fusion and environment perception modules for
real-time deployment in realistic traffic environments.

</details>


### [257] [Latent Action Pretraining Through World Modeling](https://arxiv.org/abs/2509.18428)
*Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid*

Main category: cs.RO

TL;DR: LAWM是一个自监督预训练框架，通过世界建模从无标签视频数据中学习潜在动作表示，适用于跨任务、环境和实体的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作（VLA）模型依赖大规模人工标注数据，且模型体积大，难以部署到实际场景。

Method: 提出LAWM框架，通过世界建模从无标签视频数据中学习潜在动作表示，支持机器人或人类动作视频。

Result: 在LIBERO基准测试和实际场景中，LAWM优于基于真实机器人动作训练的模型和其他预训练方法，且更高效实用。

Conclusion: LAWM为自监督预训练提供了一种高效且实用的解决方案，适用于跨任务和环境的迁移学习。

Abstract: Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.

</details>


### [258] [PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction](https://arxiv.org/abs/2509.18447)
*Rishabh Madan,Jiawei Lin,Mahika Goel,Angchen Xie,Xiaoyu Liang,Marcus Lee,Justin Guo,Pranav N. Thakkar,Rohan Banerjee,Jose Barreiros,Kate Tsui,Tom Silver,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: PrioriTouch是一个框架，用于在多接触场景中排序和执行控制目标，适用于护理任务如床浴和穿衣，通过学习和分层控制实现个性化适应。


<details>
  <summary>Details</summary>
Motivation: 解决物理人机交互中多接触点的偏好冲突问题，特别是在护理任务中，需要适应不同身体部位的力需求。

Method: 结合学习排序方法和分层操作空间控制，利用仿真循环进行数据高效和安全的探索。

Result: 通过用户研究和实验验证，PrioriTouch能适应用户偏好，保持任务性能，提升安全和舒适性。

Conclusion: PrioriTouch在多接触交互中有效平衡偏好冲突，适用于护理等复杂场景。

Abstract: Physical human-robot interaction (pHRI) requires robots to adapt to
individual contact preferences, such as where and how much force is applied.
Identifying preferences is difficult for a single contact; with whole-arm
interaction involving multiple simultaneous contacts between the robot and
human, the challenge is greater because different body parts can impose
incompatible force requirements. In caregiving tasks, where contact is frequent
and varied, such conflicts are unavoidable. With multiple preferences across
multiple contacts, no single solution can satisfy all objectives--trade-offs
are inherent, making prioritization essential. We present PrioriTouch, a
framework for ranking and executing control objectives across multiple
contacts. PrioriTouch can prioritize from a general collection of controllers,
making it applicable not only to caregiving scenarios such as bed bathing and
dressing but also to broader multi-contact settings. Our method combines a
novel learning-to-rank approach with hierarchical operational space control,
leveraging simulation-in-the-loop rollouts for data-efficient and safe
exploration. We conduct a user study on physical assistance preferences, derive
personalized comfort thresholds, and incorporate them into PrioriTouch. We
evaluate PrioriTouch through extensive simulation and real-world experiments,
demonstrating its ability to adapt to user contact preferences, maintain task
performance, and enhance safety and comfort. Website:
https://emprise.cs.cornell.edu/prioritouch.

</details>


### [259] [Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands](https://arxiv.org/abs/2509.18455)
*Yunshuang Li,Yiyang Ling,Gaurav S. Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: 论文提出了一种基于几何感知的多指灵巧手非抓取操作（GD2P）方法，通过生成和学习预接触手部姿势来实现高效的推拉操作。


<details>
  <summary>Details</summary>
Motivation: 现有非抓取操作多依赖平行夹爪或简单工具，而多指灵巧手能提供更丰富的接触模式和稳定性，但动态建模困难。

Method: 通过接触引导采样生成多样手部姿势，利用物理模拟筛选，并训练扩散模型预测可行姿势。测试时采样姿势并使用运动规划器执行动作。

Result: 在Allegro Hand和LEAP Hand上进行了840次实验，结果表明GD2P可扩展性强，适用于不同手部形态。

Conclusion: GD2P为非抓取操作提供了一种可扩展的训练方法，开源模型和数据集将促进进一步研究。

Abstract: Nonprehensile manipulation, such as pushing and pulling, enables robots to
move, align, or reposition objects that may be difficult to grasp due to their
geometry, size, or relationship to the robot or the environment. Much of the
existing work in nonprehensile manipulation relies on parallel-jaw grippers or
tools such as rods and spatulas. In contrast, multi-fingered dexterous hands
offer richer contact modes and versatility for handling diverse objects to
provide stable support over the objects, which compensates for the difficulty
of modeling the dynamics of nonprehensile manipulation. Therefore, we propose
Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile
manipulation with dexterous robotic hands. We study pushing and pulling by
framing the problem as synthesizing and learning pre-contact dexterous hand
poses that lead to effective manipulation. We generate diverse hand poses via
contact-guided sampling, filter them using physics simulation, and train a
diffusion model conditioned on object geometry to predict viable poses. At test
time, we sample hand poses and use standard motion planners to select and
execute pushing and pulling actions. We perform 840 real-world experiments with
an Allegro Hand, comparing our method to baselines. The results indicate that
GD2P offers a scalable route for training dexterous nonprehensile manipulation
policies. We further demonstrate GD2P on a LEAP Hand, highlighting its
applicability to different hand morphologies. Our pre-trained models and
dataset, including 1.3 million hand poses across 2.3k objects, will be
open-source to facilitate further research. Our project website is available
at: geodex2p.github.io.

</details>


### [260] [A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems](https://arxiv.org/abs/2509.18460)
*Haeyoon Han,Mahdi Taheri,Soon-Jo Chung,Fred Y. Hadaegh*

Main category: cs.RO

TL;DR: 提出了一种基于反事实推理的感知系统故障检测与隔离框架，通过分析冗余和主动控制输入优化故障诊断。


<details>
  <summary>Details</summary>
Motivation: 感知系统故障与环境上下文相关且易在多阶段管道中传播，需准确检测与隔离。

Method: 采用反事实推理构建可靠性测试，被动方法通过信念更新，主动方法定义为因果多臂赌博问题，使用MCTS和UCB优化控制输入。

Result: 在机器人探索场景中验证，能有效隔离传感器损坏、动态场景和感知退化等故障。

Conclusion: 反事实推理结合主动控制输入可显著提升感知系统的故障检测与隔离能力。

Abstract: Perception systems provide a rich understanding of the environment for
autonomous systems, shaping decisions in all downstream modules. Hence,
accurate detection and isolation of faults in perception systems is important.
Faults in perception systems pose particular challenges: faults are often tied
to the perceptual context of the environment, and errors in their multi-stage
pipelines can propagate across modules. To address this, we adopt a
counterfactual reasoning approach to propose a framework for fault detection
and isolation (FDI) in perception systems. As opposed to relying on physical
redundancy (i.e., having extra sensors), our approach utilizes analytical
redundancy with counterfactual reasoning to construct perception reliability
tests as causal outcomes influenced by system states and fault scenarios.
Counterfactual reasoning generates reliability test results under hypothesized
faults to update the belief over fault hypotheses. We derive both passive and
active FDI methods. While the passive FDI can be achieved by belief updates,
the active FDI approach is defined as a causal bandit problem, where we utilize
Monte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find
control inputs that maximize a detection and isolation metric, designated as
Effective Information (EI). The mentioned metric quantifies the informativeness
of control inputs for FDI. We demonstrate the approach in a robot exploration
scenario, where a space robot performing vision-based navigation actively
adjusts its attitude to increase EI and correctly isolate faults caused by
sensor damage, dynamic scenes, and perceptual degradation.

</details>


### [261] [Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task](https://arxiv.org/abs/2509.18463)
*Jannick van Buuren,Roberto Giglio,Loris Roveda,Luka Peternel*

Main category: cs.RO

TL;DR: 论文探讨了通过故意突变奖励函数在强化学习中产生多样化技能变化的方法，并以液体倾倒任务为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 受人类运动控制的成本效益权衡模型启发，研究旨在通过奖励函数突变实现机器人任务的多样化学习。

Method: 开发了基于高斯噪声的奖励函数突变框架，并在NVIDIA Isaac Sim中模拟Franka Emika Panda机械臂的液体倾倒任务，使用PPO算法进行训练。

Result: 不同奖励函数配置产生了多样化的策略，包括原始倾倒任务的变体及新技能（如容器边缘清洁、液体混合和浇水）。

Conclusion: 该方法为机器人系统提供了多样化学习特定任务的潜力，并可能衍生出对未来任务有意义的技能。

Abstract: This paper explores how deliberate mutations of reward function in
reinforcement learning can produce diversified skill variations in robotic
manipulation tasks, examined with a liquid pouring use case. To this end, we
developed a new reward function mutation framework that is based on applying
Gaussian noise to the weights of the different terms in the reward function.
Inspired by the cost-benefit tradeoff model from human motor control, we
designed the reward function with the following key terms: accuracy, time, and
effort. The study was performed in a simulation environment created in NVIDIA
Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a
glass with a liquid that needed to be poured into a container. The
reinforcement learning algorithm was based on Proximal Policy Optimization. We
systematically explored how different configurations of mutated weights in the
rewards function would affect the learned policy. The resulting policies
exhibit a wide range of behaviours: from variations in execution of the
originally intended pouring task to novel skills useful for unexpected tasks,
such as container rim cleaning, liquid mixing, and watering. This approach
offers promising directions for robotic systems to perform diversified learning
of specific tasks, while also potentially deriving meaningful skills for future
tasks.

</details>


### [262] [RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain](https://arxiv.org/abs/2509.18466)
*Junnosuke Kamohara,Feiyang Wu,Chinmayee Wamorkar,Seth Hutchinson,Ye Zhao*

Main category: cs.RO

TL;DR: 提出了一种结合强化学习（RL）和模型预测控制（MPC）的框架，用于双足机器人在复杂地形上的运动控制。


<details>
  <summary>Details</summary>
Motivation: MPC在复杂地形（如崎岖和湿滑地面）的应用受限，而RL虽能适应多样地形但缺乏约束保证。结合两者可发挥各自优势。

Method: 通过RL参数化MPC的三个关键组件（系统动力学、摆动腿控制器和步态频率），并在NVIDIA IsaacLab中进行仿真验证。

Result: 实验表明，RL增强的MPC框架比单独使用MPC或RL更具适应性和鲁棒性。

Conclusion: RL与MPC的结合为双足机器人在复杂地形上的运动控制提供了更优解决方案。

Abstract: Model predictive control (MPC) has demonstrated effectiveness for humanoid
bipedal locomotion; however, its applicability in challenging environments,
such as rough and slippery terrain, is limited by the difficulty of modeling
terrain interactions. In contrast, reinforcement learning (RL) has achieved
notable success in training robust locomotion policies over diverse terrain,
yet it lacks guarantees of constraint satisfaction and often requires
substantial reward shaping. Recent efforts in combining MPC and RL have shown
promise of taking the best of both worlds, but they are primarily restricted to
flat terrain or quadrupedal robots. In this work, we propose an RL-augmented
MPC framework tailored for bipedal locomotion over rough and slippery terrain.
Our method parametrizes three key components of
single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,
and gait frequency. We validate our approach through bipedal robot simulations
in NVIDIA IsaacLab across various terrains, including stairs, stepping stones,
and low-friction surfaces. Experimental results demonstrate that our
RL-augmented MPC framework produces significantly more adaptive and robust
behaviors compared to baseline MPC and RL.

</details>


### [263] [Spatial Envelope MPC: High Performance Driving without a Reference](https://arxiv.org/abs/2509.18506)
*Siyuan Yu,Congkai Shen,Yufei Xi,James Dallas,Michael Thompson,John Subosits,Hiroshi Yasuda,Tulga Ersal*

Main category: cs.RO

TL;DR: 提出了一种基于包络的模型预测控制框架，用于自动驾驶车辆在无预定义参考的情况下实现高性能驾驶。


<details>
  <summary>Details</summary>
Motivation: 现有规划和控制框架主要基于预定义参考，限制了在车辆动态极限下的性能表现。

Method: 引入计算高效的车辆动力学模型和连续可微的数学公式，结合强化学习与优化技术进行包络规划。

Result: 通过仿真和实际实验验证了框架在赛车、紧急避障和越野导航等多种任务中的高性能表现。

Conclusion: 该框架具有广泛适用性和可扩展性，适用于多样化的驾驶场景。

Abstract: This paper presents a novel envelope based model predictive control (MPC)
framework designed to enable autonomous vehicles to handle high performance
driving across a wide range of scenarios without a predefined reference. In
high performance autonomous driving, safe operation at the vehicle's dynamic
limits requires a real time planning and control framework capable of
accounting for key vehicle dynamics and environmental constraints when
following a predefined reference trajectory is suboptimal or even infeasible.
State of the art planning and control frameworks, however, are predominantly
reference based, which limits their performance in such situations. To address
this gap, this work first introduces a computationally efficient vehicle
dynamics model tailored for optimization based control and a continuously
differentiable mathematical formulation that accurately captures the entire
drivable envelope. This novel model and formulation allow for the direct
integration of dynamic feasibility and safety constraints into a unified
planning and control framework, thereby removing the necessity for predefined
references. The challenge of envelope planning, which refers to maximally
approximating the safe drivable area, is tackled by combining reinforcement
learning with optimization techniques. The framework is validated through both
simulations and real world experiments, demonstrating its high performance
across a variety of tasks, including racing, emergency collision avoidance and
off road navigation. These results highlight the framework's scalability and
broad applicability across a diverse set of scenarios.

</details>


### [264] [LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA](https://arxiv.org/abs/2509.18576)
*Zeyi Kang,Liang He,Yanxin Zhang,Zuheng Ming,Kaixing Zhao*

Main category: cs.RO

TL;DR: 提出轻量级LCMF框架，通过多级跨模态参数共享机制，高效融合异构模态数据，提升机器人多模态语义学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态学习中异构数据融合和计算效率的技术挑战，适用于资源受限环境下的机器人智能决策。

Method: 结合交叉注意力和选择性参数共享状态空间模型（SSMs），设计LCMF级联注意力框架。

Result: 在VQA任务中准确率达74.29%，EQA视频任务中与LLM Agents竞争，计算量减少4.35倍，参数仅166.51M（图像-文本）和219M（视频-文本）。

Conclusion: LCMF为资源受限场景下的人机交互提供了高效解决方案，具有强大多模态决策泛化能力。

Abstract: Multimodal semantic learning plays a critical role in embodied intelligence,
especially when robots perceive their surroundings, understand human
instructions, and make intelligent decisions. However, the field faces
technical challenges such as effective fusion of heterogeneous data and
computational efficiency in resource-constrained environments. To address these
challenges, this study proposes the lightweight LCMF cascaded attention
framework, introducing a multi-level cross-modal parameter sharing mechanism
into the Mamba module. By integrating the advantages of Cross-Attention and
Selective parameter-sharing State Space Models (SSMs), the framework achieves
efficient fusion of heterogeneous modalities and semantic complementary
alignment. Experimental results show that LCMF surpasses existing multimodal
baselines with an accuracy of 74.29% in VQA tasks and achieves competitive
mid-tier performance within the distribution cluster of Large Language Model
Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a
4.35-fold reduction in FLOPs relative to the average of comparable baselines
while using only 166.51M parameters (image-text) and 219M parameters
(video-text), providing an efficient solution for Human-Robot Interaction (HRI)
applications in resource-constrained scenarios with strong multimodal decision
generalization capabilities.

</details>


### [265] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: VLN-Zero是一个两阶段的视觉语言导航框架，通过结合视觉语言模型和符号推理，实现零样本导航，显著提高了成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量探索或固定策略，难以泛化到新环境，因此需要一种高效且通用的导航框架。

Method: 分为探索阶段（构建符号场景图）和部署阶段（神经符号规划与缓存执行），结合快速探索与符号推理。

Result: 在零样本模型中成功率提高2倍，优于多数微调基线，且用时减半，VLM调用减少55%。

Conclusion: VLN-Zero通过高效探索和符号推理，解决了现有方法的计算低效和泛化问题，适用于未知环境。

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [266] [Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills](https://arxiv.org/abs/2509.18597)
*Yuan Meng,Zhenguo Sun,Max Fest,Xukun Li,Zhenshan Bing,Alois Knoll*

Main category: cs.RO

TL;DR: 提出了一种人机协同框架，通过外部记忆和检索增强生成技术，将纠正编码为可重用技能，显著提升了长时任务的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的代码生成方法在机器人操作中存在噪声、固定原语限制和上下文窗口有限等问题，且难以处理长时任务。闭环反馈的纠正知识存储不当，限制了泛化能力。

Method: 采用人机协同框架，结合外部记忆和检索增强生成技术，动态重用纠正技能。

Result: 在多个实验环境中，框架实现了0.93的成功率（比基线高27%）和42%的纠正效率提升，能稳健解决超长时任务。

Conclusion: 该框架通过可重用技能和动态提示机制，有效解决了长时任务中的挑战，提升了机器人操作的性能。

Abstract: Large language models (LLMs)-based code generation for robotic manipulation
has recently shown promise by directly translating human instructions into
executable code, but existing methods remain noisy, constrained by fixed
primitives and limited context windows, and struggle with long-horizon tasks.
While closed-loop feedback has been explored, corrected knowledge is often
stored in improper formats, restricting generalization and causing catastrophic
forgetting, which highlights the need for learning reusable skills. Moreover,
approaches that rely solely on LLM guidance frequently fail in extremely
long-horizon scenarios due to LLMs' limited reasoning capability in the robotic
domain, where such issues are often straightforward for humans to identify. To
address these challenges, we propose a human-in-the-loop framework that encodes
corrections into reusable skills, supported by external memory and
Retrieval-Augmented Generation with a hint mechanism for dynamic reuse.
Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world
settings, show that our framework achieves a 0.93 success rate (up to 27%
higher than baselines) and a 42% efficiency improvement in correction rounds.
It can robustly solve extremely long-horizon tasks such as "build a house",
which requires planning over 20 primitives.

</details>


### [267] [End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning](https://arxiv.org/abs/2509.18608)
*Ana Luiza Mineiro,Francisco Affonso,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出了一种基于深度强化学习的端到端导航系统，用于解决农业环境中GNSS不可靠、环境杂乱和光照变化的问题。


<details>
  <summary>Details</summary>
Motivation: 农业环境中的可靠导航因GNSS不可靠、环境杂乱和光照变化而具有挑战性。

Method: 使用深度强化学习策略，将原始3D LiDAR数据直接映射到控制命令，并通过体素降采样策略减少输入数据量。

Result: 在仿真中验证了策略，直线种植园中成功率达100%，但随着行曲率增加性能逐渐下降。

Conclusion: 该方法无需标记数据集或手动设计控制接口，展示了在复杂农业环境中的导航潜力。

Abstract: Reliable navigation in under-canopy agricultural environments remains a
challenge due to GNSS unreliability, cluttered rows, and variable lighting. To
address these limitations, we present an end-to-end learning-based navigation
system that maps raw 3D LiDAR data directly to control commands using a deep
reinforcement learning policy trained entirely in simulation. Our method
includes a voxel-based downsampling strategy that reduces LiDAR input size by
95.83%, enabling efficient policy learning without relying on labeled datasets
or manually designed control interfaces. The policy was validated in
simulation, achieving a 100% success rate in straight-row plantations and
showing a gradual decline in performance as row curvature increased, tested
across varying sinusoidal frequencies and amplitudes.

</details>


### [268] [PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving](https://arxiv.org/abs/2509.18609)
*Chengran Yuan,Zijian Lu,Zhanqi Zhang,Yimin Zhao,Zefan Huang,Shuo Sun,Jiawei Sun,Jiahui Li,Christina Dao Wen Lee,Dongen Li,Marcelo H. Ang Jr*

Main category: cs.RO

TL;DR: PIE框架通过整合感知、推理和意图建模，解决了端到端运动规划中的场景理解和决策预测问题，显著提升了自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 解决端到端运动规划中场景理解和决策预测的挑战，以实现大规模部署。

Method: 提出PIE框架，结合双向Mamba融合和推理增强解码器，优化多模态数据融合和轨迹推断。

Result: 在NAVSIM基准测试中，PIE以88.9 PDM和85.6 EPDM分数超越现有方法。

Conclusion: PIE能可靠生成高质量轨迹，适用于自动驾驶。

Abstract: End-to-end motion planning is promising for simplifying complex autonomous
driving pipelines. However, challenges such as scene understanding and
effective prediction for decision-making continue to present substantial
obstacles to its large-scale deployment. In this paper, we present PIE, a
pioneering framework that integrates advanced perception, reasoning, and
intention modeling to dynamically capture interactions between the ego vehicle
and surrounding agents. It incorporates a bidirectional Mamba fusion that
addresses data compression losses in multimodal fusion of camera and LiDAR
inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and
Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize
adaptive trajectory inference. PIE adopts an action-motion interaction module
to effectively utilize state predictions of surrounding agents to refine ego
planning. The proposed framework is thoroughly validated on the NAVSIM
benchmark. PIE, without using any ensemble and data augmentation techniques,
achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of
prior state-of-the-art methods. Comprehensive quantitative and qualitative
analyses demonstrate that PIE is capable of reliably generating feasible and
high-quality ego trajectories.

</details>


### [269] [SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones](https://arxiv.org/abs/2509.18610)
*Maximilian Adang,JunEn Low,Ola Shorinwa,Mac Schwager*

Main category: cs.RO

TL;DR: SINGER提出了一种基于语言引导的无人机自主导航系统，仅使用机载传感器和计算，解决了开放词汇导航的挑战。


<details>
  <summary>Details</summary>
Motivation: 开放词汇无人机导航面临大规模演示稀缺、实时控制需求高和缺乏可靠外部姿态估计模块的问题。

Method: SINGER结合了语言嵌入的飞行模拟器、RRT启发的多轨迹生成专家和轻量级端到端视觉运动策略。

Result: 实验显示SINGER在零样本模拟到真实迁移中表现优异，导航成功率和视野保持率显著提升，碰撞减少。

Conclusion: SINGER为开放词汇无人机导航提供了一种高效、鲁棒的解决方案。

Abstract: Large vision-language models have driven remarkable progress in
open-vocabulary robot policies, e.g., generalist robot manipulation policies,
that enable robots to complete complex tasks specified in natural language.
Despite these successes, open-vocabulary autonomous drone navigation remains an
unsolved challenge due to the scarcity of large-scale demonstrations, real-time
control demands of drones for stabilization, and lack of reliable external pose
estimation modules. In this work, we present SINGER for language-guided
autonomous drone navigation in the open world using only onboard sensing and
compute. To train robust, open-vocabulary navigation policies, SINGER leverages
three central components: (i) a photorealistic language-embedded flight
simulator with minimal sim-to-real gap using Gaussian Splatting for efficient
data generation, (ii) an RRT-inspired multi-trajectory generation expert for
collision-free navigation demonstrations, and these are used to train (iii) a
lightweight end-to-end visuomotor policy for real-time closed-loop control.
Through extensive hardware flight experiments, we demonstrate superior
zero-shot sim-to-real transfer of our policy to unseen environments and unseen
language-conditioned goal objects. When trained on ~700k-1M observation action
pairs of language conditioned visuomotor data and deployed on hardware, SINGER
outperforms a velocity-controlled semantic guidance baseline by reaching the
query 23.33% more on average, and maintains the query in the field of view
16.67% more on average, with 10% fewer collisions.

</details>


### [270] [The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving](https://arxiv.org/abs/2509.18626)
*Jay Patrikar,Apoorva Sharma,Sushant Veer,Boyi Li,Sebastian Scherer,Marco Pavone*

Main category: cs.RO

TL;DR: 论文提出了一种基于检索的方法，利用事故报告改进自动驾驶系统的决策能力，特别是在安全边界附近。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统主要基于无事故数据训练，缺乏对安全边界附近行为的指导。事故报告提供了关键的反事实证据，但难以直接利用。

Method: 将事故报告和日志转换为统一的场景-动作表示，并通过检索相关先例来辅助决策。还引入了反事实扩展以评估替代方案。

Result: 在nuScenes基准测试中，先例检索显著提高了校准效果，上下文偏好动作的召回率从24%提升至53%。反事实变体在保持这些优势的同时，进一步优化了风险决策。

Conclusion: 通过统一表示和检索先例，结合反事实推理，可以有效提升自动驾驶系统在安全边界附近的决策能力。

Abstract: Learning-based autonomous driving systems are trained mostly on incident-free
data, offering little guidance near safety-performance boundaries. Real crash
reports contain precisely the contrastive evidence needed, but they are hard to
use: narratives are unstructured, third-person, and poorly grounded to sensor
views. We address these challenges by normalizing crash narratives to
ego-centric language and converting both logs and crashes into a unified
scene-action representation suitable for retrieval. At decision time, our
system adjudicates proposed actions by retrieving relevant precedents from this
unified index; an agentic counterfactual extension proposes plausible
alternatives, retrieves for each, and reasons across outcomes before deciding.
On a nuScenes benchmark, precedent retrieval substantially improves
calibration, with recall on contextually preferred actions rising from 24% to
53%. The counterfactual variant preserves these gains while sharpening
decisions near risk.

</details>


### [271] [Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training](https://arxiv.org/abs/2509.18631)
*Shuo Cheng,Liqian Ma,Zhenyang Chen,Ajay Mandlekar,Caelan Garrett,Danfei Xu*

Main category: cs.RO

TL;DR: 提出了一种结合仿真和真实数据的联合训练框架，通过领域不变特征空间和最优传输损失，显著提升了机器人操作策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决仿真与真实世界数据之间的领域差距问题，减少对大量真实演示的依赖。

Method: 采用联合训练框架，结合最优传输损失（OT）处理数据不平衡，学习领域不变特征空间。

Result: 在真实世界中成功率提升30%，并能泛化到仅仿真中见过的场景。

Conclusion: 该方法有效利用仿真数据，显著提升了策略的泛化性和实用性。

Abstract: Behavior cloning has shown promise for robot manipulation, but real-world
demonstrations are costly to acquire at scale. While simulated data offers a
scalable alternative, particularly with advances in automated demonstration
generation, transferring policies to the real world is hampered by various
simulation and real domain gaps. In this work, we propose a unified
sim-and-real co-training framework for learning generalizable manipulation
policies that primarily leverages simulation and only requires a few real-world
demonstrations. Central to our approach is learning a domain-invariant,
task-relevant feature space. Our key insight is that aligning the joint
distributions of observations and their corresponding actions across domains
provides a richer signal than aligning observations (marginals) alone. We
achieve this by embedding an Optimal Transport (OT)-inspired loss within the
co-training framework, and extend this to an Unbalanced OT framework to handle
the imbalance between abundant simulation data and limited real-world examples.
We validate our method on challenging manipulation tasks, showing it can
leverage abundant simulation data to achieve up to a 30% improvement in the
real-world success rate and even generalize to scenarios seen only in
simulation.

</details>


### [272] [Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments](https://arxiv.org/abs/2509.18636)
*Yuan Zhou,Jialiang Hou,Guangtong Xu,Fei Gao*

Main category: cs.RO

TL;DR: 提出了一种基于可变形虚拟结构（DVS）的无人机编队规划方法，解决了狭窄环境中无人机数量变化时的编队维护问题。


<details>
  <summary>Details</summary>
Motivation: 狭窄环境中无人机数量变化导致编队规划难以收敛到理想配置，需要一种适应性强的方法。

Method: 结合Lloyd算法和匈牙利算法（PAAS）进行均匀分区和任务分配，通过原始路径搜索和非线性轨迹优化规划DVS时空轨迹。

Result: 方法支持15%的无人机动态加入或离开，并能快速恢复编队形状，仿真和实验验证了其有效性和适应性。

Conclusion: 该方法在编队恢复速度和环境适应性上优于现有方法，实际实验证明了其有效性和鲁棒性。

Abstract: Formation maintenance with varying number of drones in narrow environments
hinders the convergence of planning to the desired configurations. To address
this challenge, this paper proposes a formation planning method guided by
Deformable Virtual Structures (DVS) with continuous spatiotemporal
transformation. Firstly, to satisfy swarm safety distance and preserve
formation shape filling integrity for irregular formation geometries, we employ
Lloyd algorithm for uniform $\underline{PA}$rtitioning and Hungarian algorithm
for $\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal
trajectory involving DVS is planned using primitive-based path search and
nonlinear trajectory optimization. The DVS trajectory achieves adaptive
transitions with respect to a varying number of drones while ensuring
adaptability to narrow environments through affine transformation. Finally,
each agent conducts distributed trajectory planning guided by desired
spatiotemporal positions within the DVS, while incorporating collision
avoidance and dynamic feasibility requirements. Our method enables up to 15\%
of swarm numbers to join or leave in cluttered environments while rapidly
restoring the desired formation shape in simulation. Compared to cutting-edge
formation planning method, we demonstrate rapid formation recovery capacity and
environmental adaptability. Real-world experiments validate the effectiveness
and resilience of our formation planning method.

</details>


### [273] [Do You Need Proprioceptive States in Visuomotor Policies?](https://arxiv.org/abs/2509.18644)
*Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao*

Main category: cs.RO

TL;DR: State-free Policy仅依赖视觉观察，显著提升机器人操作的空间泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习策略过度依赖本体感觉输入，导致空间泛化能力差。

Method: 提出State-free Policy，仅基于视觉观察预测动作，使用双广角手腕摄像头。

Result: 在高度和水平泛化中，成功率分别从0%提升至85%和6%提升至64%。

Conclusion: State-free Policy在数据效率和跨机器人适应方面表现优异，适合实际部署。

Abstract: Imitation-learning-based visuomotor policies have been widely used in robot
manipulation, where both visual observations and proprioceptive states are
typically adopted together for precise control. However, in this study, we find
that this common practice makes the policy overly reliant on the proprioceptive
state input, which causes overfitting to the training trajectories and results
in poor spatial generalization. On the contrary, we propose the State-free
Policy, removing the proprioceptive state input and predicting actions only
conditioned on visual observations. The State-free Policy is built in the
relative end-effector action space, and should ensure the full task-relevant
visual observations, here provided by dual wide-angle wrist cameras. Empirical
results demonstrate that the State-free policy achieves significantly stronger
spatial generalization than the state-based policy: in real-world tasks such as
pick-and-place, challenging shirt-folding, and complex whole-body manipulation,
spanning multiple robot embodiments, the average success rate improves from 0\%
to 85\% in height generalization and from 6\% to 64\% in horizontal
generalization. Furthermore, they also show advantages in data efficiency and
cross-embodiment adaptation, enhancing their practicality for real-world
deployment.

</details>


### [274] [SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer](https://arxiv.org/abs/2509.18648)
*Yarden As,Chengrui Qu,Benjamin Unger,Dongho Kang,Max van der Hart,Laixi Shi,Stelian Coros,Adam Wierman,Andreas Krause*

Main category: cs.RO

TL;DR: SPiDR是一种通过悲观域随机化实现安全模拟到现实迁移的可扩展算法，确保在模拟与现实差距下的安全性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在现实应用中面临安全性问题，模拟器虽提供安全训练环境，但模拟与现实的差距导致额外安全风险。

Method: SPiDR利用域随机化将模拟与现实差距的不确定性纳入安全约束，兼容现有训练流程。

Result: 实验表明，SPiDR在模拟与现实差距下有效保障安全性，同时保持高性能。

Conclusion: SPiDR为安全模拟到现实迁移提供了可扩展且具有理论保证的解决方案。

Abstract: Safety remains a major concern for deploying reinforcement learning (RL) in
real-world applications. Simulators provide safe, scalable training
environments, but the inevitable sim-to-real gap introduces additional safety
concerns, as policies must satisfy constraints in real-world conditions that
differ from simulation. To address this challenge, robust safe RL techniques
offer principled methods, but are often incompatible with standard scalable
training pipelines. In contrast, domain randomization, a simple and popular
sim-to-real technique, stands out as a promising alternative, although it often
results in unsafe behaviors in practice. We present SPiDR, short for
Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with
provable guarantees for safe sim-to-real transfer. SPiDR uses domain
randomization to incorporate the uncertainty about the sim-to-real gap into the
safety constraints, making it versatile and highly compatible with existing
training pipelines. Through extensive experiments on sim-to-sim benchmarks and
two distinct real-world robotic platforms, we demonstrate that SPiDR
effectively ensures safety despite the sim-to-real gap while maintaining strong
performance.

</details>


### [275] [Distributionally Robust Safe Motion Planning with Contextual Information](https://arxiv.org/abs/2509.18666)
*Kaizer Rahaman,Simran Kumari,Ashish R. Hota*

Main category: cs.RO

TL;DR: 提出了一种基于分布鲁棒的碰撞避免方法，结合上下文信息，通过RKHS嵌入条件分布，提高了避障成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑上下文信息和分布鲁棒性，导致在复杂场景中避障效果不佳。

Method: 利用RKHS嵌入障碍物未来轨迹的条件分布，定义模糊集并构建分布鲁棒约束，结合运动规划。

Result: 仿真结果表明，该方法在复杂场景中比非上下文或非鲁棒方法更有效。

Conclusion: 结合上下文信息和分布鲁棒性显著提升了碰撞避免性能。

Abstract: We present a distributionally robust approach for collision avoidance by
incorporating contextual information. Specifically, we embed the conditional
distribution of future trajectory of the obstacle conditioned on the motion of
the ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional
kernel mean embedding operator. Then, we define an ambiguity set containing all
distributions whose embedding in the RKHS is within a certain distance from the
empirical estimate of conditional mean embedding learnt from past data.
Consequently, a distributionally robust collision avoidance constraint is
formulated, and included in the receding horizon based motion planning
formulation of the ego agent. Simulation results show that the proposed
approach is more successful in avoiding collision compared to approaches that
do not include contextual information and/or distributional robustness in their
formulation in several challenging scenarios.

</details>


### [276] [N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout](https://arxiv.org/abs/2509.18671)
*Kaixin Chai,Hyunjun Lee,Joseph J. Lim*

Main category: cs.RO

TL;DR: N2M模块通过引导机器人到更适合操作的初始位姿，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决导航模块与操作策略在初始位姿偏好上的不匹配问题。

Method: 引入N2M过渡模块，依赖自我中心观察，实时适应环境变化，具有高鲁棒性和广泛适用性。

Result: 在PnPCounterToCab任务中，成功率从3%提升至54%；在Toybox Handover任务中，仅需15个样本即可在未知环境中可靠预测。

Conclusion: N2M显著提升了任务成功率和数据效率，具有广泛的应用潜力。

Abstract: In mobile manipulation, the manipulation policy has strong preferences for
initial poses where it is executed. However, the navigation module focuses
solely on reaching the task area, without considering which initial pose is
preferable for downstream manipulation. To address this misalignment, we
introduce N2M, a transition module that guides the robot to a preferable
initial pose after reaching the task area, thereby substantially improving task
success rates. N2M features five key advantages: (1) reliance solely on
ego-centric observation without requiring global or historical information; (2)
real-time adaptation to environmental changes; (3) reliable prediction with
high viewpoint robustness; (4) broad applicability across diverse tasks,
manipulation policies, and robot hardware; and (5) remarkable data efficiency
and generalizability. We demonstrate the effectiveness of N2M through extensive
simulation and real-world experiments. In the PnPCounterToCab task, N2M
improves the averaged success rate from 3% with the reachability-based baseline
to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable
predictions even in unseen environments with only 15 data samples, showing
remarkable data efficiency and generalizability.

</details>


### [277] [3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space](https://arxiv.org/abs/2509.18676)
*Sangjun Noh,Dongwoo Nam,Kangmin Kim,Geonhyup Lee,Yeonguk Yu,Raeyoung Kang,Kyoobin Lee*

Main category: cs.RO

TL;DR: 3D Flow Diffusion Policy (3D FDP) 利用场景级3D流作为中间表示，通过扩散架构预测查询点轨迹，在MetaWorld基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略局部运动线索，而3D FDP旨在通过3D流捕捉精细运动，提升机器人操作的精确性和泛化能力。

Method: 3D FDP预测查询点的时空轨迹，并基于交互感知的流生成动作，采用统一的扩散架构实现。

Result: 在50个任务中达到SOTA，尤其在中等和困难任务中表现突出，并在真实机器人任务中优于基线。

Conclusion: 3D流是学习泛化视觉运动策略的强大结构先验，支持开发更鲁棒和通用的机器人操作。

Abstract: Learning robust visuomotor policies that generalize across diverse objects
and interaction dynamics remains a central challenge in robotic manipulation.
Most existing approaches rely on direct observation-to-action mappings or
compress perceptual inputs into global or object-centric features, which often
overlook localized motion cues critical for precise and contact-rich
manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework
that leverages scene-level 3D flow as a structured intermediate representation
to capture fine-grained local motion cues. Our approach predicts the temporal
trajectories of sampled query points and conditions action generation on these
interaction-aware flows, implemented jointly within a unified diffusion
architecture. This design grounds manipulation in localized dynamics while
enabling the policy to reason about broader scene-level consequences of
actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP
achieves state-of-the-art performance across 50 tasks, particularly excelling
on medium and hard settings. Beyond simulation, we validate our method on eight
real-robot tasks, where it consistently outperforms prior baselines in
contact-rich and non-prehensile scenarios. These results highlight 3D flow as a
powerful structural prior for learning generalizable visuomotor policies,
supporting the development of more robust and versatile robotic manipulation.
Robot demonstrations, additional results, and code can be found at
https://sites.google.com/view/3dfdp/home.

</details>


### [278] [Query-Centric Diffusion Policy for Generalizable Robotic Assembly](https://arxiv.org/abs/2509.18686)
*Ziyi Xu,Haohong Lin,Shiqi Liu,Ding Zhao*

Main category: cs.RO

TL;DR: QDP是一种层次化框架，通过查询机制连接高级规划和低级控制，提升机器人装配任务的性能。


<details>
  <summary>Details</summary>
Motivation: 机器人装配任务因部件交互复杂和对噪声敏感而具有挑战性，现有分层策略存在高低级执行不匹配的问题。

Method: 提出Query-centric Diffusion Policy (QDP)，利用对象、接触点和技能信息的查询机制，指导低级策略并提高鲁棒性。

Result: 在FurnitureBench上的实验表明，QDP在技能精度和长时成功率上表现优异，插入和拧紧任务成功率提升50%以上。

Conclusion: QDP通过结构化查询机制有效解决了高低级策略不匹配问题，显著提升了装配任务的性能。

Abstract: The robotic assembly task poses a key challenge in building generalist robots
due to the intrinsic complexity of part interactions and the sensitivity to
noise perturbations in contact-rich settings. The assembly agent is typically
designed in a hierarchical manner: high-level multi-part reasoning and
low-level precise control. However, implementing such a hierarchical policy is
challenging in practice due to the mismatch between high-level skill queries
and low-level execution. To address this, we propose the Query-centric
Diffusion Policy (QDP), a hierarchical framework that bridges high-level
planning and low-level control by utilizing queries comprising objects, contact
points, and skill information. QDP introduces a query-centric mechanism that
identifies task-relevant components and uses them to guide low-level policies,
leveraging point cloud observations to improve the policy's robustness. We
conduct comprehensive experiments on the FurnitureBench in both simulation and
real-world settings, demonstrating improved performance in skill precision and
long-horizon success rate. In the challenging insertion and screwing tasks, QDP
improves the skill-wise success rate by over 50% compared to baselines without
structured queries.

</details>


### [279] [Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation](https://arxiv.org/abs/2509.18734)
*Nishant Doshi,Amey Sutvani,Sanket Gujar*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的虚拟四旋翼机器人导航方法，用于解决城市环境中的路径规划问题。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GPS精度下降、空间狭窄和动态障碍物等因素使无人机导航复杂化，需开发避障能力。

Method: 使用深度相机和强化学习训练虚拟四旋翼机器人，在模拟城市环境中导航。

Result: 未明确提及具体结果，但方法旨在提升导航能力。

Conclusion: 强化学习结合深度传感器有望解决无人机在城市环境中的导航挑战。

Abstract: One of the challenges faced by Autonomous Aerial Vehicles is reliable
navigation through urban environments. Factors like reduction in precision of
Global Positioning System (GPS), narrow spaces and dynamically moving obstacles
make the path planning of an aerial robot a complicated task. One of the skills
required for the agent to effectively navigate through such an environment is
to develop an ability to avoid collisions using information from onboard depth
sensors. In this paper, we propose Reinforcement Learning of a virtual
quadcopter robot agent equipped with a Depth Camera to navigate through a
simulated urban environment.

</details>


### [280] [MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning](https://arxiv.org/abs/2509.18757)
*Omar Rayyan,John Abanes,Mahmoud Hafez,Anthony Tzes,Fares Abu-Dakka*

Main category: cs.RO

TL;DR: MV-UMI框架通过结合第三人称视角与第一人称视角，提升了手持夹持器系统的场景理解能力，任务性能提高了47%。


<details>
  <summary>Details</summary>
Motivation: 解决手持夹持器系统仅依赖第一人称视角导致场景上下文不足的问题。

Method: 提出MV-UMI框架，整合第三人称视角与第一人称视角。

Result: 在需要广泛场景理解的子任务中性能提升约47%。

Conclusion: MV-UMI框架扩展了手持夹持器系统的任务范围，同时保留了跨体现优势。

Abstract: Recent advances in imitation learning have shown great promise for developing
robust robot manipulation policies from demonstrations. However, this promise
is contingent on the availability of diverse, high-quality datasets, which are
not only challenging and costly to collect but are often constrained to a
specific robot embodiment. Portable handheld grippers have recently emerged as
intuitive and scalable alternatives to traditional robotic teleoperation
methods for data collection. However, their reliance solely on first-person
view wrist-mounted cameras often creates limitations in capturing sufficient
scene contexts. In this paper, we present MV-UMI (Multi-View Universal
Manipulation Interface), a framework that integrates a third-person perspective
with the egocentric camera to overcome this limitation. This integration
mitigates domain shifts between human demonstration and robot deployment,
preserving the cross-embodiment advantages of handheld data-collection devices.
Our experimental results, including an ablation study, demonstrate that our
MV-UMI framework improves performance in sub-tasks requiring broad scene
understanding by approximately 47% across 3 tasks, confirming the effectiveness
of our approach in expanding the range of feasible manipulation tasks that can
be learned using handheld gripper systems, without compromising the
cross-embodiment advantages inherent to such systems.

</details>


### [281] [VGGT-DP: Generalizable Robot Control via Vision Foundation Models](https://arxiv.org/abs/2509.18778)
*Shijia Ge,Yinxin Zhang,Shuzhao Xie,Weixiang Zhang,Mingcai Zhou,Zhi Wang*

Main category: cs.RO

TL;DR: VGGT-DP框架通过结合预训练的3D感知模型和本体感觉反馈，提升了视觉模仿学习的空间理解和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视视觉编码器的结构和能力，限制了空间理解和泛化。

Method: 采用VGGT作为视觉编码器，结合本体感觉反馈和几何先验，设计帧级令牌重用机制和随机令牌剪枝。

Result: 在MetaWorld任务中显著优于基线方法，尤其在精度要求高和长时程任务中表现突出。

Conclusion: VGGT-DP通过多模态感知和高效表示，提升了视觉模仿学习的性能。

Abstract: Visual imitation learning frameworks allow robots to learn manipulation
skills from expert demonstrations. While existing approaches mainly focus on
policy design, they often neglect the structure and capacity of visual
encoders, limiting spatial understanding and generalization. Inspired by
biological vision systems, which rely on both visual and proprioceptive cues
for robust control, we propose VGGT-DP, a visuomotor policy framework that
integrates geometric priors from a pretrained 3D perception model with
proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer
(VGGT) as the visual encoder and introduce a proprioception-guided visual
learning strategy to align perception with internal robot states, improving
spatial grounding and closed-loop control. To reduce inference latency, we
design a frame-wise token reuse mechanism that compacts multi-view tokens into
an efficient spatial representation. We further apply random token pruning to
enhance policy robustness and reduce overfitting. Experiments on challenging
MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines
such as DP and DP3, particularly in precision-critical and long-horizon
scenarios.

</details>


### [282] [Human-Interpretable Uncertainty Explanations for Point Cloud Registration](https://arxiv.org/abs/2509.18786)
*Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel*

Main category: cs.RO

TL;DR: 提出了一种名为GP-CA的新方法，用于解决点云配准问题，能够量化并解释不确定性，并通过主动学习发现新的不确定性来源。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如ICP）在传感器噪声、姿态估计误差和遮挡导致的部分重叠等不确定性下表现不佳。

Method: 利用高斯过程概念归因（GP-CA）量化不确定性，并通过主动学习发现新的不确定性来源。

Result: 在三个公开数据集和真实机器人实验中验证了GP-CA的有效性，其在运行时间、样本效率和准确性上优于现有方法。

Conclusion: GP-CA能够实现更鲁棒的机器人感知，并支持有效的故障恢复行为。

Abstract: In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.

</details>


### [283] [Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations](https://arxiv.org/abs/2509.18793)
*Lukas Zanger,Bastian Lampe,Lennart Reiher,Lutz Eckstein*

Main category: cs.RO

TL;DR: 提出了一种基于Kubernetes的需求驱动应用管理方法，用于动态协调C-ITS中的微服务部署和资源优化。


<details>
  <summary>Details</summary>
Motivation: 随着车辆自动化和互联化程度的提高，C-ITS需要高效管理动态环境中的资源，传统方法难以满足需求。

Method: 利用Kubernetes和ROS 2构建应用管理框架，动态响应不同实体的需求，自动化微服务的部署、配置和扩展。

Result: 通过需求驱动的方法，减少了计算资源消耗和网络流量，并在集体环境感知用例中验证了框架的有效性。

Conclusion: 该方法为大规模C-ITS中的动态应用管理提供了可行解决方案，并开源了原型框架。

Abstract: Vehicles are becoming increasingly automated and interconnected, enabling the
formation of cooperative intelligent transport systems (C-ITS) and the use of
offboard services. As a result, cloud-native techniques, such as microservices
and container orchestration, play an increasingly important role in their
operation. However, orchestrating applications in a large-scale C-ITS poses
unique challenges due to the dynamic nature of the environment and the need for
efficient resource utilization. In this paper, we present a demand-driven
application management approach that leverages cloud-native techniques -
specifically Kubernetes - to address these challenges. Taking into account the
demands originating from different entities within the C-ITS, the approach
enables the automation of processes, such as deployment, reconfiguration,
update, upgrade, and scaling of microservices. Executing these processes on
demand can, for example, reduce computing resource consumption and network
traffic. A demand may include a request for provisioning an external supporting
service, such as a collective environment model. The approach handles changing
and new demands by dynamically reconciling them through our proposed
application management framework built on Kubernetes and the Robot Operating
System (ROS 2). We demonstrate the operation of our framework in the C-ITS use
case of collective environment perception and make the source code of the
prototypical framework publicly available at
https://github.com/ika-rwth-aachen/application_manager .

</details>


### [284] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: DexSkin是一种柔软的电容式电子皮肤，用于机器人手指的触觉感知，支持敏感、局部和可校准的触觉传感，适用于复杂操作任务。


<details>
  <summary>Details</summary>
Motivation: 复制人类皮肤的触觉感知能力，以提升机器人系统的灵巧操作能力。

Method: 开发DexSkin电子皮肤，覆盖机器人手指表面，并通过学习框架（如示范学习和在线强化学习）评估其性能。

Result: DexSkin在复杂操作任务（如物体重新定向和弹性带包装）中表现优异，并能实现传感器实例间的模型迁移。

Conclusion: DexSkin适用于数据驱动的学习方法，具有实际应用潜力，能够提升机器人在接触丰富的操作任务中的表现。

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [285] [Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation](https://arxiv.org/abs/2509.18865)
*Masato Kobayashi,Thanpimon Buamanee*

Main category: cs.RO

TL;DR: Bi-VLA是一种基于双边控制的模仿学习框架，通过视觉-语言融合处理多任务，克服了传统方法的单任务限制。


<details>
  <summary>Details</summary>
Motivation: 传统双边控制方法需要任务特定模型，限制了通用性。Bi-VLA旨在通过结合视觉和语言指令提升多任务处理能力。

Method: 利用机器人关节角度、速度和扭矩数据，结合视觉特征和自然语言指令，通过SigLIP和FiLM融合技术。

Result: 在真实机器人实验中，Bi-VLA成功解析视觉-语言组合，提高了任务成功率。

Conclusion: Bi-VLA解决了传统方法的单任务限制，证明了视觉与语言结合显著提升通用性。

Abstract: We propose Bilateral Control-Based Imitation Learning via Vision-Language
Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral
control-based imitation learning to handle more than one task within a single
model. Conventional bilateral control methods exploit joint angle, velocity,
torque, and vision for precise manipulation but require task-specific models,
limiting their generality. Bi-VLA overcomes this limitation by utilizing robot
joint angle, velocity, and torque data from leader-follower bilateral control
with visual features and natural language instructions through SigLIP and
FiLM-based fusion. We validated Bi-VLA on two task types: one requiring
supplementary language cues and another distinguishable solely by vision.
Real-robot experiments showed that Bi-VLA successfully interprets
vision-language combinations and improves task success rates compared to
conventional bilateral control-based imitation learning. Our Bi-VLA addresses
the single-task limitation of prior bilateral approaches and provides empirical
evidence that combining vision and language significantly enhances versatility.
Experimental results validate the effectiveness of Bi-VLA in real-world tasks.
For additional material, please visit the website:
https://mertcookimg.github.io/bi-vla/

</details>


### [286] [Lang2Morph: Language-Driven Morphological Design of Robotic Hands](https://arxiv.org/abs/2509.18937)
*Yanyuan Qiao,Kieran Gilday,Yutong Xie,Josie Hughes*

Main category: cs.RO

TL;DR: Lang2Morph利用大语言模型（LLM）将自然语言任务描述转化为机器人手的设计参数，实现零样本设计。


<details>
  <summary>Details</summary>
Motivation: 传统机器人手设计依赖专家经验和手动调整，自动化方法计算量大且依赖仿真，LLM提供了一种新的设计思路。

Method: Lang2Morph通过LLM将任务描述转化为符号结构和参数，包括形态设计和选择优化两个步骤。

Result: 实验表明，Lang2Morph能生成多样且任务相关的机器人手形态。

Conclusion: 这是首个基于LLM的任务驱动机器人手设计框架，展示了LLM在机器人设计中的潜力。

Abstract: Designing robotic hand morphologies for diverse manipulation tasks requires
balancing dexterity, manufacturability, and task-specific functionality. While
open-source frameworks and parametric tools support reproducible design, they
still rely on expert heuristics and manual tuning. Automated methods using
optimization are often compute-intensive, simulation-dependent, and rarely
target dexterous hands. Large language models (LLMs), with their broad
knowledge of human-object interactions and strong generative capabilities,
offer a promising alternative for zero-shot design reasoning. In this paper, we
present Lang2Morph, a language-driven pipeline for robotic hand design. It uses
LLMs to translate natural-language task descriptions into symbolic structures
and OPH-compatible parameters, enabling 3D-printable task-specific
morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks
into semantic tags, structural grammars, and OPH-compatible parameters; and
(ii) Selection and Refinement, which evaluates design candidates based on
semantic alignment and size compatibility, and optionally applies LLM-guided
refinement when needed. We evaluate Lang2Morph across varied tasks, and results
show that our approach can generate diverse, task-relevant morphologies. To our
knowledge, this is the first attempt to develop an LLM-based framework for
task-conditioned robotic hand design.

</details>


### [287] [Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations](https://arxiv.org/abs/2509.18953)
*Hanqing Liu,Jiahuan Long,Junqi Wu,Jiacheng Hou,Huili Tang,Tingsong Jiang,Weien Zhou,Wen Yao*

Main category: cs.RO

TL;DR: Eva-VLA框架首次系统评估了VLA模型在真实物理变化下的鲁棒性，揭示了其严重脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在真实物理变化下的鲁棒性未被充分研究，亟需系统性评估方法。

Method: 通过将离散物理变化转化为连续优化问题，分解为对象3D变换、光照变化和对抗性补丁三个领域，并引入黑盒优化框架。

Result: 实验显示，所有变化类型均导致超过60%的失败率，对象变换在长时任务中失败率高达97.8%。

Conclusion: Eva-VLA框架为提升VLA模型在真实部署中的鲁棒性提供了实用路径。

Abstract: Vision-Language-Action (VLA) models have emerged as promising solutions for
robotic manipulation, yet their robustness to real-world physical variations
remains critically underexplored. To bridge this gap, we propose Eva-VLA, the
first unified framework that systematically evaluates the robustness of VLA
models by transforming discrete physical variations into continuous
optimization problems. However, comprehensively assessing VLA robustness
presents two key challenges: (1) how to systematically characterize diverse
physical variations encountered in real-world deployments while maintaining
evaluation reproducibility, and (2) how to discover worst-case scenarios
without prohibitive real-world data collection costs efficiently. To address
the first challenge, we decompose real-world variations into three critical
domains: object 3D transformations that affect spatial reasoning, illumination
variations that challenge visual perception, and adversarial patches that
disrupt scene understanding. For the second challenge, we introduce a
continuous black-box optimization framework that transforms discrete physical
variations into parameter optimization, enabling systematic exploration of
worst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models
across multiple benchmarks reveal alarming vulnerabilities: all variation types
trigger failure rates exceeding 60%, with object transformations causing up to
97.8% failure in long-horizon tasks. Our findings expose critical gaps between
controlled laboratory success and unpredictable deployment readiness, while the
Eva-VLA framework provides a practical pathway for hardening VLA-based robotic
manipulation models against real-world deployment challenges.

</details>


### [288] [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](https://arxiv.org/abs/2509.18954)
*Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的框架，用于在ICP匹配前预测注册误差协方差，提升定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ICP算法在无特征环境和动态场景中容易出错，现有方法难以准确预测不确定性。

Method: 利用深度学习估计ICP的注册误差协方差，无需依赖预建地图。

Result: 在KITTI数据集上验证了方法的有效性，减少了定位误差并提高了鲁棒性。

Conclusion: 该方法能准确预测协方差，提升定位和SLAM的性能。

Abstract: LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.

</details>


### [289] [Category-Level Object Shape and Pose Estimation in Less Than a Millisecond](https://arxiv.org/abs/2509.18979)
*Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone*

Main category: cs.RO

TL;DR: 提出了一种快速局部求解器，用于形状和姿态估计，仅需类别级先验，并提供全局最优性证明。


<details>
  <summary>Details</summary>
Motivation: 解决机器人任务中的形状和姿态估计问题，支持从操作到场景理解的任务。

Method: 使用学习前端检测稀疏语义关键点，通过线性主动形状模型表示形状，并用最大后验优化问题求解位置、方向和形状。

Result: 通过自洽场迭代高效求解，每次迭代仅需计算4x4矩阵及其最小特征值对，提供全局最优性证明。

Conclusion: 方法在合成和真实数据上表现良好，适用于多种场景，包括无人机跟踪。

Abstract: Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.

</details>


### [290] [Pure Vision Language Action (VLA) Models: A Comprehensive Survey](https://arxiv.org/abs/2509.19012)
*Dapeng Zhang,Jin Sun,Chenghui Hu,Xiaoyan Wu,Zhenlong Yuan,Rui Zhou,Fei Shen,Qingguo Zhou*

Main category: cs.RO

TL;DR: 本文综述了视觉语言动作（VLA）模型的发展，从传统策略控制转向通用机器人技术，并系统分类了现有研究方法。


<details>
  <summary>Details</summary>
Motivation: 探讨VLA模型如何从被动序列生成器转变为主动决策代理，以应对复杂动态环境中的挑战。

Method: 对VLA方法进行分类（自回归、扩散、强化、混合和专用方法），并分析其核心策略和实现。

Result: 总结了VLA在不同场景的应用，并提出了关键挑战和未来方向。

Conclusion: VLA模型在通用机器人技术中潜力巨大，但仍需解决可扩展性和通用性等挑战。

Abstract: The emergence of Vision Language Action (VLA) models marks a paradigm shift
from traditional policy-based control to generalized robotics, reframing Vision
Language Models (VLMs) from passive sequence generators into active agents for
manipulation and decision-making in complex, dynamic environments. This survey
delves into advanced VLA methods, aiming to provide a clear taxonomy and a
systematic, comprehensive review of existing research. It presents a
comprehensive analysis of VLA applications across different scenarios and
classifies VLA approaches into several paradigms: autoregression-based,
diffusion-based, reinforcement-based, hybrid, and specialized methods; while
examining their motivations, core strategies, and implementations in detail. In
addition, foundational datasets, benchmarks, and simulation platforms are
introduced. Building on the current VLA landscape, the review further proposes
perspectives on key challenges and future directions to advance research in VLA
models and generalizable robotics. By synthesizing insights from over three
hundred recent studies, this survey maps the contours of this rapidly evolving
field and highlights the opportunities and challenges that will shape the
development of scalable, general-purpose VLA methods.

</details>


### [291] [Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion](https://arxiv.org/abs/2509.19023)
*Shuai Liu,Meng Cheng Lau*

Main category: cs.RO

TL;DR: ROM-GRL是一种两阶段强化学习框架，用于人形机器人行走，无需运动捕捉数据或复杂奖励设计。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖运动捕捉数据或复杂奖励设计的问题，实现自然、高效的人形行走。

Method: 第一阶段训练4-DOF简化模型生成步态模板，第二阶段用对抗判别器指导全身策略训练。

Result: 实验显示ROM-GRL在1m/s和4m/s速度下生成稳定、对称的步态，跟踪误差显著低于纯奖励基线。

Conclusion: ROM-GRL填补了纯奖励与模仿学习方法之间的空白，实现了无需人类演示的自然人形行为。

Abstract: We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a
two-stage reinforcement learning framework for humanoid walking that requires
no motion capture data or elaborate reward shaping. In the first stage, a
compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via
Proximal Policy Optimization. This generates energy-efficient gait templates.
In the second stage, those dynamically consistent trajectories guide a
full-body policy trained with Soft Actor--Critic augmented by an adversarial
discriminator, ensuring the student's five-dimensional gait feature
distribution matches the ROM's demonstrations. Experiments at 1
meter-per-second and 4 meter-per-second show that ROM-GRL produces stable,
symmetric gaits with substantially lower tracking error than a pure-reward
baseline. By distilling lightweight ROM guidance into high-dimensional
policies, ROM-GRL bridges the gap between reward-only and imitation-based
locomotion methods, enabling versatile, naturalistic humanoid behaviors without
any human demonstrations.

</details>


### [292] [TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors](https://arxiv.org/abs/2509.19037)
*Qingzheng Cong,Steven Oh,Wen Fan,Shan Luo,Kaspar Althoefer,Dandan Zhang*

Main category: cs.RO

TL;DR: TacEva是一个用于定量评估视觉触觉传感器（VBTS）性能的综合框架，解决了现有VBTS因参数差异导致性能评估困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有VBTS因传感机制、结构尺寸等参数差异导致性能评估缺乏标准化，难以优化选择。

Method: TacEva定义了一套性能指标，设计了结构化实验流程，确保量化的一致性和可重复性。

Result: 应用于多种VBTS，结果显示TacEva能全面评估设计并提供性能维度的定量指标。

Conclusion: TacEva帮助研究者按任务选择最佳VBTS，并为设计优化提供性能指导。

Abstract: Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because
of the high spatial resolution they offer and their relatively low
manufacturing costs. However, variations in their sensing mechanisms,
structural dimension, and other parameters lead to significant performance
disparities between existing VBTSs. This makes it challenging to optimize them
for specific tasks, as both the initial choice and subsequent fine-tuning are
hindered by the lack of standardized metrics. To address this issue, TacEva is
introduced as a comprehensive evaluation framework for the quantitative
analysis of VBTS performance. The framework defines a set of performance
metrics that capture key characteristics in typical application scenarios. For
each metric, a structured experimental pipeline is designed to ensure
consistent and repeatable quantification. The framework is applied to multiple
VBTSs with distinct sensing mechanisms, and the results demonstrate its ability
to provide a thorough evaluation of each design and quantitative indicators for
each performance dimension. This enables researchers to pre-select the most
appropriate VBTS on a task by task basis, while also offering
performance-guided insights into the optimization of VBTS design. A list of
existing VBTS evaluation methods and additional evaluations can be found on our
website: https://stevenoh2003.github.io/TacEva/

</details>


### [293] [ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation](https://arxiv.org/abs/2509.19047)
*Geonhyup Lee,Yeongjin Lee,Kangmin Kim,Seongju Lee,Sangjun Noh,Seunghyeok Back,Kyoobin Lee*

Main category: cs.RO

TL;DR: ManipForce系统通过捕捉高频力扭矩和RGB数据，结合FMT模型，显著提升了接触密集型任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法主要依赖视觉数据，无法精确控制接触力，而接触密集型任务（如精密装配）需要精确的力控制。

Method: 提出ManipForce手持系统，捕捉高频力扭矩和RGB数据；设计FMT模型，通过频率和模态感知嵌入及双向交叉注意力融合多模态信号。

Result: 在六项真实接触密集型任务中，FMT平均成功率达83%，显著优于仅用RGB的基线方法。

Conclusion: 高频力扭矩数据和跨模态集成提升了策略性能，尤其在需要高精度和稳定接触的任务中。

Abstract: Contact-rich manipulation tasks such as precision assembly require precise
control of interaction forces, yet existing imitation learning methods rely
mainly on vision-only demonstrations. We propose ManipForce, a handheld system
designed to capture high-frequency force-torque (F/T) and RGB data during
natural human demonstrations for contact-rich manipulation. Building on these
demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).
FMT encodes asynchronous RGB and F/T signals using frequency- and
modality-aware embeddings and fuses them via bi-directional cross-attention
within a transformer diffusion policy. Through extensive experiments on six
real-world contact-rich manipulation tasks - such as gear assembly, box
flipping, and battery insertion - FMT trained on ManipForce demonstrations
achieves robust performance with an average success rate of 83% across all
tasks, substantially outperforming RGB-only baselines. Ablation and
sampling-frequency analyses further confirm that incorporating high-frequency
F/T data and cross-modal integration improves policy performance, especially in
tasks demanding high precision and stable contact.

</details>


### [294] [SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions](https://arxiv.org/abs/2509.19076)
*Laura Connolly,Aravind S. Kumar,Kapi Ketan Mehta,Lidia Al-Zogbi,Peter Kazanzides,Parvin Mousavi,Gabor Fichtinger,Axel Krieger,Junichi Tokuda,Russell H. Taylor,Simon Leonard,Anton Deguet*

Main category: cs.RO

TL;DR: SlicerROS2是一个结合3D Slicer和ROS的软件模块，用于医学机器人研究，新版设计提升了模块化、功能访问和数据传输。


<details>
  <summary>Details</summary>
Motivation: 为医学机器人研究提供标准化的集成工具，促进图像引导机器人干预的发展。

Method: 通过重新设计和重写模块，提供更好的模块化、低层功能访问、Python API支持及数据传输协议。

Result: 展示了SlicerROS2在四种实际图像引导机器人场景中的应用。

Conclusion: SlicerROS2的新设计为医学机器人研究提供了更灵活和强大的工具。

Abstract: Image-guided robotic interventions involve the use of medical imaging in
tandem with robotics. SlicerROS2 is a software module that combines 3D Slicer
and robot operating system (ROS) in pursuit of a standard integration approach
for medical robotics research. The first release of SlicerROS2 demonstrated the
feasibility of using the C++ API from 3D Slicer and ROS to load and visualize
robots in real time. Since this initial release, we've rewritten and redesigned
the module to offer greater modularity, access to low-level features, access to
3D Slicer's Python API, and better data transfer protocols. In this paper, we
introduce this new design as well as four applications that leverage the core
functionalities of SlicerROS2 in realistic image-guided robotics scenarios.

</details>


### [295] [World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation](https://arxiv.org/abs/2509.19080)
*Zhennan Jiang,Kai Liu,Yuxin Qin,Shuai Tian,Yupeng Zheng,Mingcai Zhou,Chao Yu,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: World4RL框架利用扩散模型作为高保真模拟器，在虚拟环境中优化预训练的机器人操作策略，避免了真实世界交互的成本和风险。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作策略因专家数据稀缺和覆盖范围有限而性能受限的问题，同时避免真实世界训练的高成本和风险。

Method: 通过预训练扩散世界模型捕捉多任务数据集中的动态，并在冻结的世界模型中优化策略，采用双热动作编码和扩散骨干网络提高建模保真度。

Result: 实验表明，World4RL提供了高保真环境建模，显著提高了策略的成功率，优于模仿学习和其他基线方法。

Conclusion: World4RL框架通过扩散模型实现了高效的策略优化，为机器人操作提供了安全且高保真的解决方案。

Abstract: Robotic manipulation policies are commonly initialized through imitation
learning, but their performance is limited by the scarcity and narrow coverage
of expert data. Reinforcement learning can refine polices to alleviate this
limitation, yet real-robot training is costly and unsafe, while training in
simulators suffers from the sim-to-real gap. Recent advances in generative
models have demonstrated remarkable capabilities in real-world simulation, with
diffusion models in particular excelling at generation. This raises the
question of how diffusion model-based world models can be combined to enhance
pre-trained policies in robotic manipulation. In this work, we propose
World4RL, a framework that employs diffusion-based world models as
high-fidelity simulators to refine pre-trained policies entirely in imagined
environments for robotic manipulation. Unlike prior works that primarily employ
world models for planning, our framework enables direct end-to-end policy
optimization. World4RL is designed around two principles: pre-training a
diffusion world model that captures diverse dynamics on multi-task datasets and
refining policies entirely within a frozen world model to avoid online
real-world interactions. We further design a two-hot action encoding scheme
tailored for robotic manipulation and adopt diffusion backbones to improve
modeling fidelity. Extensive simulation and real-world experiments demonstrate
that World4RL provides high-fidelity environment modeling and enables
consistent policy refinement, yielding significantly higher success rates
compared to imitation learning and other baselines. More visualization results
are available at https://world4rl.github.io/.

</details>


### [296] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: FunCanon框架通过功能对象规范化，将长时程操作任务分解为动作块，提升策略的泛化能力和复用性。


<details>
  <summary>Details</summary>
Motivation: 传统端到端演示的机器人技能通常局限于特定任务，缺乏泛化能力。

Method: 使用功能对象规范化，将对象映射到共享功能框架，并训练对象和动作中心的扩散策略FuncDiffuser。

Result: 实验表明，该方法在类别级泛化、跨任务行为复用和sim2real部署中表现优异。

Conclusion: 功能规范化为复杂操作领域的可扩展模仿学习提供了强归纳偏置。

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


### [297] [Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation](https://arxiv.org/abs/2509.19105)
*Sarvesh Prajapati,Ananya Trivedi,Nathaniel Hanson,Bruce Maxwell,Taskin Padir*

Main category: cs.RO

TL;DR: RS-Net利用RGB图像预测光谱特征，用于地形分类和摩擦系数估计，提升机器人在户外环境中的导航能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖几何或语义标签分类可通行表面，但无法区分视觉相似但材料属性不同的表面。光谱传感器能提供材料信息，但硬件成本高且处理复杂。

Method: 提出RS-Net，通过深度学习从RGB图像预测光谱特征，并映射到地形标签和摩擦系数，用于运动规划和MPC控制。

Result: RS-Net实现了仅依赖RGB图像的任务相关物理属性预测，无需昂贵的光谱传感器。

Conclusion: RS-Net为机器人导航提供了一种低成本、高效的光谱信息替代方案。

Abstract: Successful navigation in outdoor environments requires accurate prediction of
the physical interactions between the robot and the terrain. To this end,
several methods rely on geometric or semantic labels to classify traversable
surfaces. However, such labels cannot distinguish visually similar surfaces
that differ in material properties. Spectral sensors enable inference of
material composition from surface reflectance measured across multiple
wavelength bands. Although spectral sensing is gaining traction in robotics,
widespread deployment remains constrained by the need for custom hardware
integration, high sensor costs, and compute-intensive processing pipelines. In
this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),
a deep neural network designed to bridge the gap between the accessibility of
RGB sensing and the rich material information provided by spectral data. RS-Net
predicts spectral signatures from RGB patches, which we map to terrain labels
and friction coefficients. The resulting terrain classifications are integrated
into a sampling-based motion planner for a wheeled robot operating in outdoor
environments. Likewise, the friction estimates are incorporated into a
contact-force-based MPC for a quadruped robot navigating slippery surfaces.
Thus, we introduce a framework that learns the task-relevant physical property
once during training and thereafter relies solely on RGB sensing at test time.
The code is available at https://github.com/prajapatisarvesh/RS-Net.

</details>


### [298] [BiGraspFormer: End-to-End Bimanual Grasp Transformer](https://arxiv.org/abs/2509.19142)
*Kangmin Kim,Seunghyeok Back,Geonhyup Lee,Sangbeom Lee,Sangjun Noh,Kyoobin Lee*

Main category: cs.RO

TL;DR: BiGraspFormer是一个统一的端到端Transformer框架，直接从物体点云生成协调的双臂抓取，解决了现有方法的协调问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么专注于单臂抓取，要么采用分离的抓取生成和双臂评估阶段，导致协调问题，如碰撞风险和不平衡力分布。

Method: 提出Single-Guided Bimanual (SGB)策略，首先生成多样化的单抓取候选，然后通过专用注意力机制联合预测双臂姿态和质量分数。

Result: 在仿真和实际实验中，BiGraspFormer表现优于现有方法，且推理速度高效（<0.05秒）。

Conclusion: BiGraspFormer有效解决了双臂抓取的协调问题，展示了其在实际应用中的潜力。

Abstract: Bimanual grasping is essential for robots to handle large and complex
objects. However, existing methods either focus solely on single-arm grasping
or employ separate grasp generation and bimanual evaluation stages, leading to
coordination problems including collision risks and unbalanced force
distribution. To address these limitations, we propose BiGraspFormer, a unified
end-to-end transformer framework that directly generates coordinated bimanual
grasps from object point clouds. Our key idea is the Single-Guided Bimanual
(SGB) strategy, which first generates diverse single grasp candidates using a
transformer decoder, then leverages their learned features through specialized
attention mechanisms to jointly predict bimanual poses and quality scores. This
conditioning strategy reduces the complexity of the 12-DoF search space while
ensuring coordinated bimanual manipulation. Comprehensive simulation
experiments and real-world validation demonstrate that BiGraspFormer
consistently outperforms existing methods while maintaining efficient inference
speed (<0.05s), confirming the effectiveness of our framework. Code and
supplementary materials are available at https://sites.google.com/bigraspformer

</details>


### [299] [A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination](https://arxiv.org/abs/2509.19168)
*Mark Gonzales,Ethan Oh,Joseph Moore*

Main category: cs.RO

TL;DR: 提出一种基于采样的多模态策略规划器，通过交叉熵方法优化多模态策略，提高鲁棒性和探索效率。


<details>
  <summary>Details</summary>
Motivation: 解决局部最小值和多机器人无碰撞规划中的死锁问题，同时避免集中式优化的计算复杂度。

Method: 使用交叉熵方法优化多模态策略，并在共同成本函数下进行规划。

Result: 数值模拟显示在多模态环境下成功率和多机器人避碰性能显著提升，硬件实验验证了实时可行性。

Conclusion: 多模态策略规划器在复杂环境和多机器人协作中表现出色，具有实际应用潜力。

Abstract: In this paper, we present a receding-horizon, sampling-based planner capable
of reasoning over multimodal policy distributions. By using the cross-entropy
method to optimize a multimodal policy under a common cost function, our
approach increases robustness against local minima and promotes effective
exploration of the solution space. We show that our approach naturally extends
to multi-robot collision-free planning, enables agents to share diverse
candidate policies to avoid deadlocks, and allows teams to minimize a global
objective without incurring the computational complexity of centralized
optimization. Numerical simulations demonstrate that employing multiple modes
significantly improves success rates in trap environments and in multi-robot
collision avoidance. Hardware experiments further validate the approach's
real-time feasibility and practical performance.

</details>


### [300] [MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap](https://arxiv.org/abs/2509.19169)
*Tianyu Wu,Xudong Han,Haoran Sun,Zishang Zhang,Bangchao Huang,Chaoyang Song,Fang Wan*

Main category: cs.RO

TL;DR: MagiClaw是一种多功能两指末端执行器，用于缩小人类演示与机器人执行之间的领域差距。


<details>
  <summary>Details</summary>
Motivation: 解决人类演示与机器人执行之间的传感和形态学领域差距问题。

Method: 设计MagiClaw作为手持工具和机器人末端执行器，结合SPN和嵌入式摄像头，融合多模态数据。

Result: 通过统一系统架构降低高保真数据收集门槛，加速通用操作策略开发。

Conclusion: MagiClaw通过硬件一致性和多模态数据融合，有效提升机器人操作技能的学习和部署效率。

Abstract: The transfer of manipulation skills from human demonstration to robotic
execution is often hindered by a "domain gap" in sensing and morphology. This
paper introduces MagiClaw, a versatile two-finger end-effector designed to
bridge this gap. MagiClaw functions interchangeably as both a handheld tool for
intuitive data collection and a robotic end-effector for policy deployment,
ensuring hardware consistency and reliability. Each finger incorporates a Soft
Polyhedral Network (SPN) with an embedded camera, enabling vision-based
estimation of 6-DoF forces and contact deformation. This proprioceptive data is
fused with exteroceptive environmental sensing from an integrated iPhone, which
provides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS
application, MagiClaw streams synchronized, multi-modal data for real-time
teleoperation, offline policy learning, and immersive control via mixed-reality
interfaces. We demonstrate how this unified system architecture lowers the
barrier to collecting high-fidelity, contact-rich datasets and accelerates the
development of generalizable manipulation policies. Please refer to the iOS app
at https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.

</details>


### [301] [Proactive-reactive detection and mitigation of intermittent faults in robot swarms](https://arxiv.org/abs/2509.19246)
*Sinan Oğuz,Emanuele Garone,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: 论文提出了一种主动-被动策略，用于检测和缓解机器人群体中的间歇性故障，通过自组织备份层和多路网络中的分布式共识实现。


<details>
  <summary>Details</summary>
Motivation: 间歇性故障在机器人群体中难以检测，现有研究主要关注永久性故障。自组织神经系统（SoNS）的出现使得检测间歇性故障成为可能。

Method: 采用主动-被动策略：主动自组织动态备份路径，被动通过多路网络中的分布式共识进行故障检测和临时路由调整。

Result: 在代表性场景中验证了方法，能够高精度检测故障并避免误报，确保机器人群体收敛到目标形态。

Conclusion: 该方法有效解决了机器人群体中间歇性故障的检测和缓解问题，提高了系统的可靠性。

Abstract: Intermittent faults are transient errors that sporadically appear and
disappear. Although intermittent faults pose substantial challenges to
reliability and coordination, existing studies of fault tolerance in robot
swarms focus instead on permanent faults. One reason for this is that
intermittent faults are prohibitively difficult to detect in the fully
self-organized ad-hoc networks typical of robot swarms, as their network
topologies are transient and often unpredictable. However, in the recently
introduced self-organizing nervous systems (SoNS) approach, robot swarms are
able to self-organize persistent network structures for the first time, easing
the problem of detecting intermittent faults. To address intermittent faults in
robot swarms that have persistent networks, we propose a novel
proactive-reactive strategy to detection and mitigation, based on
self-organized backup layers and distributed consensus in a multiplex network.
Proactively, the robots self-organize dynamic backup paths before faults occur,
adapting to changes in the primary network topology and the robots' relative
positions. Reactively, robots use one-shot likelihood ratio tests to compare
information received along different paths in the multiplex network, enabling
early fault detection. Upon detection, communication is temporarily rerouted in
a self-organized way, until the detected fault resolves. We validate the
approach in representative scenarios of faulty positional data occurring during
formation control, demonstrating that intermittent faults are prevented from
disrupting convergence to desired formations, with high fault detection
accuracy and low rates of false positives.

</details>


### [302] [Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces](https://arxiv.org/abs/2509.19261)
*Kuanqi Cai,Chunfeng Wang,Zeqi Li,Haowen Yao,Weinan Chen,Luis Figueredo,Aude Billard,Arash Ajoudani*

Main category: cs.RO

TL;DR: 提出了一种模仿引导的双臂规划框架，用于动态环境中机器人抓取的平滑过渡和运动优化。


<details>
  <summary>Details</summary>
Motivation: 动态环境中机器人抓取需要平滑过渡，但现有方法难以应对外力变化和运动约束，导致稳定性和效率不足。

Method: 结合稳定抓取采样策略和分层双阶段运动架构，通过模仿学习和二次规划优化抓取过渡和运动性能。

Result: 在力密集型任务中验证了方法，显著提高了抓取过渡效率和运动性能。

Conclusion: 该方法有效提升了机器人抓取的稳定性和灵活性，适用于复杂动态环境。

Abstract: Robotic manipulation in dynamic environments often requires seamless
transitions between different grasp types to maintain stability and efficiency.
However, achieving smooth and adaptive grasp transitions remains a challenge,
particularly when dealing with external forces and complex motion constraints.
Existing grasp transition strategies often fail to account for varying external
forces and do not optimize motion performance effectively. In this work, we
propose an Imitation-Guided Bimanual Planning Framework that integrates
efficient grasp transition strategies and motion performance optimization to
enhance stability and dexterity in robotic manipulation. Our approach
introduces Strategies for Sampling Stable Intersections in Grasp Manifolds for
seamless transitions between uni-manual and bi-manual grasps, reducing
computational costs and regrasping inefficiencies. Additionally, a Hierarchical
Dual-Stage Motion Architecture combines an Imitation Learning-based Global Path
Generator with a Quadratic Programming-driven Local Planner to ensure real-time
motion feasibility, obstacle avoidance, and superior manipulability. The
proposed method is evaluated through a series of force-intensive tasks,
demonstrating significant improvements in grasp transition efficiency and
motion performance. A video demonstrating our simulation results can be viewed
at
\href{https://youtu.be/3DhbUsv4eDo}{\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.

</details>


### [303] [SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration](https://arxiv.org/abs/2509.19292)
*Yang Jin,Jun Lv,Han Xue,Wendi Chen,Chuan Wen,Cewu Lu*

Main category: cs.RO

TL;DR: SOE框架通过约束探索到有效动作的流形，提升机器人策略的探索能力和安全性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器人策略常因动作模式崩溃而缺乏探索能力，现有随机扰动方法不安全且不稳定。

Method: SOE学习任务相关因素的紧凑潜在表示，约束探索到有效动作流形，可无缝集成到任意策略模型。

Result: 在仿真和实际任务中，SOE表现优于现有方法，任务成功率更高，探索更平滑安全。

Conclusion: SOE证明了流形探索是样本高效策略自我改进的有效方法。

Abstract: Intelligent agents progress by continually refining their capabilities
through actively exploring environments. Yet robot policies often lack
sufficient exploration capability due to action mode collapse. Existing methods
that encourage exploration typically rely on random perturbations, which are
unsafe and induce unstable, erratic behaviors, thereby limiting their
effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a
framework that enhances policy exploration and improvement in robotic
manipulation. SOE learns a compact latent representation of task-relevant
factors and constrains exploration to the manifold of valid actions, ensuring
safety, diversity, and effectiveness. It can be seamlessly integrated with
arbitrary policy models as a plug-in module, augmenting exploration without
degrading the base policy performance. Moreover, the structured latent space
enables human-guided exploration, further improving efficiency and
controllability. Extensive experiments in both simulation and real-world tasks
demonstrate that SOE consistently outperforms prior methods, achieving higher
task success rates, smoother and safer exploration, and superior sample
efficiency. These results establish on-manifold exploration as a principled
approach to sample-efficient policy self-improvement. Project website:
https://ericjin2002.github.io/SOE

</details>


### [304] [Residual Off-Policy RL for Finetuning Behavior Cloning Policies](https://arxiv.org/abs/2509.19301)
*Lars Ankile,Zhenyu Jiang,Rocky Duan,Guanya Shi,Pieter Abbeel,Anusha Nagabandi*

Main category: cs.RO

TL;DR: 结合行为克隆（BC）和强化学习（RL）的残差学习框架，通过稀疏二元奖励信号提升高自由度系统的操作策略。


<details>
  <summary>Details</summary>
Motivation: BC受限于人类演示质量和数据收集成本，RL在真实机器人上训练效率低且安全性差。结合两者优势以提升性能。

Method: 利用BC策略作为黑盒基础，通过样本高效的离策略RL学习轻量级残差修正。

Result: 在仿真和真实世界中成功提升高自由度系统的操作策略，首次在类人机器人上实现真实世界RL训练。

Conclusion: 该方法为实际部署RL提供了可行路径，展示了在视觉任务中的先进性能。

Abstract: Recent advances in behavior cloning (BC) have enabled impressive visuomotor
control policies. However, these approaches are limited by the quality of human
demonstrations, the manual effort required for data collection, and the
diminishing returns from increasing offline data. In comparison, reinforcement
learning (RL) trains an agent through autonomous interaction with the
environment and has shown remarkable success in various domains. Still,
training RL policies directly on real-world robots remains challenging due to
sample inefficiency, safety concerns, and the difficulty of learning from
sparse rewards for long-horizon tasks, especially for high-degree-of-freedom
(DoF) systems. We present a recipe that combines the benefits of BC and RL
through a residual learning framework. Our approach leverages BC policies as
black-box bases and learns lightweight per-step residual corrections via
sample-efficient off-policy RL. We demonstrate that our method requires only
sparse binary reward signals and can effectively improve manipulation policies
on high-degree-of-freedom (DoF) systems in both simulation and the real world.
In particular, we demonstrate, to the best of our knowledge, the first
successful real-world RL training on a humanoid robot with dexterous hands. Our
results demonstrate state-of-the-art performance in various vision-based tasks,
pointing towards a practical pathway for deploying RL in the real world.
Project website: https://residual-offpolicy-rl.github.io

</details>
