<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 169]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.LG](#cs.LG) [Total: 285]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: SoC-DT是一种可微分框架，结合反应-扩散模型和个性化治疗干预，用于预测肿瘤动态。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法捕捉异质性治疗下的肿瘤动态，亟需能模拟标准治疗并考虑患者差异的计算框架。

Method: SoC-DT整合反应-扩散模型、离散治疗干预及个性化数据，提出IMEX-SoC求解器确保稳定性。

Result: 在合成和真实胶质瘤数据上，SoC-DT优于传统PDE和纯数据驱动模型。

Conclusion: SoC-DT为肿瘤学中的个性化数字孪生提供了理论基础，实现生物一致的肿瘤动态预测。

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 提出了一种结合分布式多GPU推理系统和交互式可视化平台的混合框架，用于分析视频中的名人动态。


<details>
  <summary>Details</summary>
Motivation: 在视频内容主导的时代，理解其结构和动态变得越来越重要。

Method: 利用优化的ONNX模型、异构批量推理和高吞吐量并行处理视频数据，生成时间戳记录，并通过多种可视化工具展示。

Result: 提供了多维度的视频内容洞察，揭示了名人出现频率、屏幕时间分布、时间动态、共同出现关系等模式。

Conclusion: 该系统为娱乐分析、内容创作策略和观众参与研究提供了新的可能性。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 该研究比较了不同模型在跨域水下塑料检测中的性能，发现轻量级CNN模型MobileNetV2表现最佳，而预训练视觉语言模型在零样本学习中展现出互补优势。


<details>
  <summary>Details</summary>
Motivation: 海洋塑料污染日益严重，需要可靠的自动化检测方法。然而，由于域偏移问题，模型在新数据上的性能往往下降。

Method: 研究训练了多种CNN和视觉Transformer模型，并在跨域测试集上评估其性能，同时测试了两种零样本模型（CLIP和Gemini）。

Result: MobileNetV2在跨域性能上表现最佳（F1 0.97），所有微调模型均实现高精度（约99%），但召回率差异较大。零样本模型中，CLIP敏感但易误报，Gemini则相反。

Conclusion: 轻量级CNN模型在跨域检测中表现优异，而大型预训练视觉语言模型提供了互补优势。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP是一个阿拉伯语图像描述框架，结合了CLIP视觉标签检索与多模态文本生成，通过多语言编码器和混合词汇表生成流畅的阿拉伯语描述。


<details>
  <summary>Details</summary>
Motivation: 解决传统端到端方法在阿拉伯语图像描述中缺乏可解释性和文化一致性的问题。

Method: 使用mCLIP、AraCLIP和Jina V4三种多语言编码器提取视觉标签，结合混合词汇表生成提示，再通过Qwen-VL和Gemini Pro Vision生成描述。

Result: mCLIP + Gemini Pro Vision在BLEU-1和余弦相似度上表现最佳，AraCLIP + Qwen-VL在LLM-judge评分最高。

Conclusion: VLCAP提供了一种可解释的管道，能够生成文化一致且上下文准确的阿拉伯语描述。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 对比了EfficientNet-B0和ViT-Base在SpaceNet数据集上的表现，分析了不平衡和平衡标签分布下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究卷积神经网络（EfficientNet-B0）和视觉Transformer（ViT-Base）在不同标签分布下的性能差异。

Method: 在两种标签分布（不平衡和平衡）下训练模型，使用相同的预处理和训练预算，评估多项指标。

Result: EfficientNet-B0在不平衡分布下表现更好且效率更高；平衡分布下两者性能接近，但CNN仍更高效。

Conclusion: 平衡标签分布可以缩小模型性能差距，但CNN在效率上仍有优势。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 综述了基于摄像头的AI感知系统在弱势道路使用者（VRU）安全领域的进展，重点探讨了检测与分类、跟踪与重识别、轨迹预测及意图识别与预测四大核心任务，并指出了数据、模型和部署方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统基础设施措施在动态城市环境中对VRU保护不足，AI视觉感知技术的发展为主动和情境感知的VRU保护提供了新机遇。

Method: 系统回顾了过去五年在摄像头AI感知系统方面的研究进展，分析了四大核心任务及其技术发展。

Result: 总结了当前技术进展，并指出了数据、模型和部署方面的四大挑战，为未来研究提供了方向。

Conclusion: 通过将视觉AI进展与实际部署需求结合，为下一代感知系统的开发提供了基础参考，以提升VRU安全。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: EOFMs的表示空间对传感器架构高度敏感，需注意当前设计的缺陷。


<details>
  <summary>Details</summary>
Motivation: 探讨EOFMs在不同传感器架构下的内部表示差异及其影响。

Method: 分析EOFMs的表示空间对传感器架构的敏感性。

Result: EOFMs的表示空间高度依赖传感器架构。

Conclusion: 理解传感器架构对EOFMs的影响对模型开发和社区发展至关重要。

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 提出了一种基于修复和扰动的解释技术，用于生态监测中的视觉模型，提高预测的可信度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 生态监测中视觉模型的预测不透明，限制了信任和实际应用，需要一种能保持场景上下文且逼真的解释方法。

Method: 采用修复引导的扰动技术，结合YOLOv9和Segment-Anything-Model，支持对象移除/替换和背景替换。

Result: 生成的解释能定位诊断结构，避免传统扰动的删除伪影，并提供生态相关的见解。

Conclusion: 该方法提高了AI在生态学中的可信度和可解释性，支持专家验证和实际部署。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 本文综述了医学图像分割（MIS）的传统与现代方法，包括深度学习架构如CNN、U-Net等，并探讨了新兴趋势如跨模态学习和联邦学习。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在精准诊断和治疗规划中至关重要，但现有方法面临数据标注不足、计算复杂性和模型泛化性等挑战。

Method: 系统回顾了阈值分割、边缘检测、区域分割、聚类算法等传统方法，以及CNN、FCN、U-Net等深度学习模型，并探讨了注意力机制、GAN和Transformer等新技术。

Result: 总结了当前MIS领域的进展，包括新兴的混合架构和跨模态学习，并通过腰椎分割案例展示了实际应用中的挑战与进展。

Conclusion: 尽管MIS取得显著进展，仍需解决数据集偏差、领域适应和模型可解释性等问题，以更好地融入临床工作流。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR是一种深度聚类框架，用于在半导体制造中可靠地聚类复杂的晶圆缺陷模式，无需手动调整，并能处理方向变化。


<details>
  <summary>Details</summary>
Motivation: 晶圆缺陷的早期检测对产品良率优化至关重要，但原始数据复杂、不平衡且可能包含多种缺陷，需要可靠的聚类方法。

Method: DECOR是一种深度聚类框架，显式处理晶圆图中的方向变化，确保空间相似的缺陷被一致聚类。

Result: 在MixedWM38数据集上，DECOR优于现有基线方法，提供了可靠且可扩展的解决方案。

Conclusion: DECOR为自动化视觉检测系统提供了一种无需手动调整的可靠聚类方法。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 论文提出了一种基于LSTM和注意力机制的神经网络模型，用于解决多类人脸图像分类中的样本不平衡问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决多类人脸图像分类中的样本不平衡问题，特别是某些情绪类别显著多于其他类别的情况。

Method: 使用基于LSTM和注意力机制的神经网络模型，专注于对情绪识别关键的面部区域，并在训练阶段排除第七类进行后续错误校正。

Result: 实验表明，所有类别的校正均可行，但效果因类别而异；测试样本中某些类别的关键质量指标有所提升。

Conclusion: 该方法可有效应用于面部表情分析系统及类别分布不平衡的分类任务中。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: DCG-Bench是首个评估多模态大语言模型（MLLM）在动态图表生成任务中的基准，通过三阶段任务和高质量数据集DCG-8K，提出两阶段训练方法，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索MLLM在动态图表生成和理解中的潜力，填补现有研究空白。

Method: 引入DCG-Bench基准和DCG-8K数据集，采用两阶段训练方法和联合代码-视觉奖励优化策略。

Result: 提出的模型Qwen2.5-VL-DCG-3B在三个任务中平均性能提升8.31%，与专有模型性能相当。

Conclusion: 训练方法有效，为动态图表生成任务提供了新解决方案。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [13] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: VoT（Visual odometry Transformer）是一种端到端的单目视觉里程计方法，通过时空注意力建模全局关系，无需手工组件，性能优于传统方法且运行更快。


<details>
  <summary>Details</summary>
Motivation: 传统单目视觉里程计方法依赖预训练组件和优化模块，复杂且泛化能力差；VoT旨在通过端到端方式解决这些问题。

Method: VoT通过时空注意力处理单目帧序列，直接预测相机运动，无需密集几何估计，仅依赖相机位姿监督。

Result: VoT在更大数据集上表现优异，泛化能力强，运行速度快3倍以上，优于传统方法。

Conclusion: VoT展示了端到端单目视觉里程计的潜力，简化了流程并提升了性能。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 提出了一种基于侧信息的新型推理时间搜索算法，用于改进扩散模型在逆问题中的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽略侧信息，而侧信息在严重不适定问题中能显著提升重建质量。

Method: 提出了一种平衡探索与利用的推理时间搜索算法，利用侧信息指导采样过程。

Result: 在多种逆问题（如修复、超分辨率、去模糊等）中，该方法显著提升了重建的定性和定量性能。

Conclusion: 该方法优于现有基线，包括基于奖励梯度的指导算法，且能无缝集成到现有扩散重建流程中。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [15] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 本文综述了现有声纳图像数据集，分析了不同模态和应用，并提供了数据集比较表和时间线。


<details>
  <summary>Details</summary>
Motivation: 公开、标注良好的声纳图像数据集稀缺，阻碍了机器学习模型的开发，本文旨在填补这一空白。

Method: 通过收集和分析公开数据集，按模态和应用分类，并生成比较表和时间线。

Result: 提供了全面的数据集综述和比较工具，帮助研究人员快速了解领域现状。

Conclusion: 本文为声纳图像分析领域的研究者提供了实用指南，并指出了未来发展方向。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [16] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了一种基于无透镜相机和隐式神经表示算法的显示校准方法，无需专业设备即可从多角度捕获显示特性。


<details>
  <summary>Details</summary>
Motivation: 传统显示校准需要专业设备和暗室，对大多数用户不友好，因此需要一种更便捷的解决方案。

Method: 结合无透镜相机和隐式神经表示算法，从46.6°×37.6°的视角锥高效重建显示的光场。

Result: 实现了无需专业设备的显示校准，为轻松校准和表征显示特性提供了初步解决方案。

Conclusion: 该方法为显示校准的便捷化迈出了重要一步。

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [17] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: Provenance networks是一种新型神经网络模型，通过直接关联预测与训练数据，实现端到端的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习的模型不透明、幻觉问题，提升模型的透明度和可信度。

Method: 模型通过学习将预测与支持性训练样本直接关联，类似于学习的KNN方法。

Result: 增强了模型的解释能力，支持检测异常数据、验证训练集输入等。

Conclusion: Provenance networks为深度学习提供了一种互补的可解释性方法，提升了模型的鲁棒性和可信度。

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [18] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种通用的后处理框架UCF，用于优化无监督异常检测中的匹配噪声问题，在单模态和多模态场景下均取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无监督异常检测中忽视了匹配噪声问题，且单模态和多模态研究缺乏统一视角，限制了检测能力和知识迁移。

Method: 提出了Unified Cost Filtering (UCF)框架，通过构建异常成本体积并利用可学习的过滤模块减少匹配噪声。

Result: 在22个基准测试中，UCF显著提升了多种无监督异常检测方法的性能，达到了新的最优结果。

Conclusion: UCF为单模态和多模态无监督异常检测提供了统一的解决方案，有效解决了匹配噪声问题并提升了检测性能。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [19] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 提出了一种利用视觉语言模型（VLMs）评估和优化工业图纸中对象检测结果的框架。


<details>
  <summary>Details</summary>
Motivation: 工业图纸数字化是构建数字孪生和实现智能工业自动化的关键步骤，但目前缺乏自动评估对象检测质量的方法。

Method: 采用视觉语言模型（VLMs）的多模态能力，识别缺失或不一致的检测结果，实现自动化质量评估。

Result: 该方法能够自动评估检测质量并指导优化，提升复杂工业图纸中的检测性能。

Conclusion: 该框架填补了工业图纸数字化中对象检测质量评估的空白，为数字孪生和智能自动化提供了支持。

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [20] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: SpatialViLT是一种增强的视觉语言模型，通过整合空间特征提升3D场景和复杂物体配置的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在空间推理方面的不足，特别是在3D场景和复杂物体配置中。

Method: 提出SpatialViLT和MaskedSpatialViLT两种变体，结合深度图、3D坐标和边缘图等空间特征，通过多任务学习框架增强多模态嵌入。

Result: 在Visual Spatial Reasoning (VSR)数据集上表现出色，尤其在方向、拓扑和邻近关系等空间推理类别中达到最先进水平。

Conclusion: 该研究显著提升了AI系统的空间智能，对高级多模态理解和实际应用至关重要。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [21] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 论文研究了在双相光学切片结构照明显微镜（OS-SI）中，使用编码器-解码器网络减少残留伪影的方法，通过合成训练数据实现监督去噪。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法在减少OS-SI中由快速采集引入的残留伪影时效果有限，且缺乏干净的基准数据限制了监督学习的应用。

Method: 使用合成训练数据（将真实伪影场应用于合成图像）训练非对称去噪自编码器（DAE）和U-Net，并在真实OS-SI图像上评估。

Result: 两种网络均提升了图像清晰度，且在不同类型伪影上表现优异。

Conclusion: 合成训练数据支持OS-SI图像的监督去噪，编码器-解码器网络有望简化重建流程。

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [22] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL是一种多模态框架，通过整合组织病理学和空间转录组学，利用通路激活分数提升预测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖少量高变基因，限制了预测范围和生物程序的协调性。

Method: PEaRL使用ssGSEA计算通路激活分数，通过transformer编码和对比学习对齐组织学特征。

Result: 在三种癌症数据集中，PEaRL在基因和通路水平预测上均优于现有方法。

Conclusion: 基于通路的转录组表示能构建更生物可信和可解释的多模态模型。

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [23] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS是一种高效的多模态医学图像分析深度学习框架，通过分层语义提示和双提示机制实现细粒度任务控制，优于现有任务特定和通用模型。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像分析中任务特定模型泛化性差和通用模型语义理解不足的问题。

Method: 引入分层语义提示和双提示机制的视觉-语言框架，支持多模态和任务扩展。

Result: 在10个数据集中8个表现优于现有模型，支持EHR数据集成，头颈癌数据集CI达0.69。

Conclusion: DuPLUS是高效、通用且临床相关的医学图像分析解决方案，支持快速适应新任务和模态。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [24] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出了一种移动优化的两阶段深度学习框架，结合TDM并行化YOLOv10检测和MobileSAM分割，提升实时性能，并在Houbara Bustard数据集上取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 自然环境中实时动物检测与分割对野生动物保护至关重要，但受限于计算资源和物种隐蔽性，现有方法面临挑战。

Method: 采用YOLOv10进行检测，MobileSAM进行轻量级分割，通过TDM并行化两者以减少延迟。

Result: 在Houbara Bustard数据集上，mAP50达0.9627，mAP75为0.7731，mAP95为0.7178，MobileSAM mIoU为0.7421，YOLOv10每帧处理时间为43.7ms。

Conclusion: 该框架在实时性和准确性上表现优异，公开了代码和数据集，支持进一步研究和应用。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [25] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: Platonic Transformer通过引入柏拉图立体对称群的参考框架，解决了Transformer缺乏几何对称性归纳偏置的问题，同时保持了标准Transformer的效率和灵活性。


<details>
  <summary>Details</summary>
Motivation: Transformer在科学和计算机视觉中缺乏几何对称性的归纳偏置，现有方法通常牺牲了Transformer的效率和灵活性。

Method: 通过定义基于柏拉图立体对称群参考框架的注意力机制，实现了连续平移和柏拉图对称性的等变性，同时保持标准Transformer的架构和计算成本。

Result: 在计算机视觉、3D点云和分子属性预测等多个基准测试中，Platonic Transformer表现出色，且无需额外计算成本。

Conclusion: Platonic Transformer通过几何约束实现了高效且灵活的等变性，适用于多种任务。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [26] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 综述探讨了领域泛化（DG）在语义分割中的应用，重点介绍了基于基础模型的范式转变及其显著影响。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在未知领域的泛化能力仍具挑战性，领域泛化（DG）方法旨在解决这一问题，尤其在语义分割任务中具有重要意义。

Method: 通过聚类和综述现有方法，识别了向基础模型驱动的领域泛化范式转变。

Result: 性能比较显示基础模型对领域泛化具有显著影响。

Conclusion: 该综述旨在推动领域泛化研究并启发新的研究方向。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [27] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的自动报告生成模型，用于减轻内窥镜检查的文档负担。


<details>
  <summary>Details</summary>
Motivation: 内窥镜检查的文档负担导致临床工作流程效率低下和医生疲劳，需要自动化解决方案。

Method: 采用两阶段训练框架，结合视觉编码器和文本解码器，先预训练再微调。

Result: 模型能生成有临床意义的报告，有望减轻医生工作量并改善患者护理。

Conclusion: 该自动化方法为内窥镜检查文档提供了高效解决方案。

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [28] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一种基于扩散模型的规划器，通过2D手绘草图生成无人机3D飞行路径，实现零样本仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 解决无人机导航中从人类手绘草图生成3D飞行路径的挑战，提升路径规划的准确性和安全性。

Method: 结合SketchAdapter（将草图映射到2D路径）和DiffPath（从2D投影和深度图像推断3D轨迹），使用混合标注数据训练。

Result: 在低/中密度环境中100%成功，高密度环境中40%成功，优于基线20-60%。

Conclusion: 混合标注数据和模块化设计显著提升了模型性能，能够准确理解人类意图并生成安全路径。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [29] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: 提出了一种基于生物特征泄漏的防御方法，通过解耦姿态和表情的潜在特征来检测实时视频中的身份替换攻击。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频会议系统通过压缩姿态-表情潜在特征传输并重新合成RGB视频，但该潜在特征可能被攻击者操控，导致身份劫持。传统深度伪造检测方法因每帧均为合成而失效。

Method: 提出了一种姿态条件化的大间隔对比编码器，从传输的潜在特征中分离出持久身份线索，同时消除瞬态姿态和表情。通过简单的余弦测试检测身份替换。

Result: 实验表明，该方法在多种说话头生成模型上表现优于现有防御方法，实时性强，且对分布外场景具有良好泛化能力。

Conclusion: 该方法首次在不依赖重建RGB视频的情况下，通过生物特征泄漏防御有效解决了实时视频身份劫持问题。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [30] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: REVEL提出了一种新的交互式视频编辑任务，通过DragStream方法解决了潜在分布漂移和上下文干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型难以实现细粒度、流式控制，导致输出与用户期望不一致。

Method: 提出DragStream方法，包括自适应分布自校正策略和空间频率选择性优化机制。

Result: DragStream能无缝集成到现有模型中，实验证明其有效性。

Conclusion: REVEL和DragStream为交互式视频编辑提供了高效解决方案。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [31] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL是一种灵活的集成框架，通过多实例学习整合多个基础模型的特征，无需手动选择或微调，在多种癌症数据集中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型在病理诊断任务中适应和评估耗时耗资源的问题。

Method: 提出Group-Aggregative Selection Multi-Instance Learning (GAS-MIL)框架，集成多个基础模型的特征。

Result: 在三种癌症数据集（前列腺、卵巢、乳腺癌）中表现优于或与单个基础模型及其他MIL方法相当。

Conclusion: GAS-MIL为病理学提供了高效、可扩展的模型部署方案，支持未来多模态和精准肿瘤学应用。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [32] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 无人机快速递送纳洛酮可有效应对阿片类药物过量紧急情况，通过实时情境感知评估框架提升旁观者救助效果。


<details>
  <summary>Details</summary>
Motivation: 解决阿片类药物过量紧急情况中旁观者情境感知的实时评估问题，以提升无人机辅助救助系统的效率。

Method: 提出基于视频的实时情境感知评估框架，利用图嵌入和Transformer模型分析旁观者行为特征。

Result: 框架在情境预测和时间分割精度上表现优异，分别比FINCH基线高出9%和5%。

Conclusion: 该研究为自适应无人机系统开发提供了支持，有望提升紧急响应效果并挽救生命。

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [33] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 评估四种开源OCR系统在食品包装图像上的性能，重点关注成分表和营养标签的提取。


<details>
  <summary>Details</summary>
Motivation: 食品包装上的OCR对合规性和营养监测很重要，但面临多语言、密集布局、字体多样、反光和曲面等挑战。

Method: 使用231种产品（1,628张图像）的数据集，评估四种OCR模型的速度和覆盖率，并创建113张图像（60种产品）的真实子集进行准确性评估。

Result: Tesseract在CER和BLEU上表现最佳，EasyOCR在准确性和多语言支持间平衡，PaddleOCR覆盖率高但速度慢，TrOCR表现最弱。

Conclusion: 研究提供了包装OCR的基准，为布局感知方法和文本定位指明了方向。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [34] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle是一个轻量级插件模块，通过预测关键帧和所需帧数，优化视频理解任务中的帧采样策略。


<details>
  <summary>Details</summary>
Motivation: 现有帧采样策略无法适应信息密度或任务复杂度的变化，导致效率低下和信息丢失。

Method: 采用四阶段课程训练，结合弱代理信号和新数据集FrameOracle-41K的强监督。

Result: 在五个VLMs和六个基准测试中，FrameOracle将输入帧数从16降至10.4，保持准确性；从64降至13.9帧时，准确性提升1.4%。

Conclusion: FrameOracle在效率和准确性之间实现了最优权衡，推动了可扩展视频理解的发展。

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [35] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出了一种混合Co-FineTuning（CFT）方法，结合标记和未标记数据，显著减少对目标游戏标记数据的依赖，提升视觉Bug检测效果。


<details>
  <summary>Details</summary>
Motivation: 视频游戏中的视觉Bug手动识别成本高且需要专业知识，监督模型依赖大量标记数据，但此类Bug罕见，标记数据不足。

Method: 采用混合CFT方法，整合目标游戏和同领域游戏的标记数据及未标记数据，增强特征表示学习。

Result: 实验表明，CFT在多游戏环境中优于传统基线方法，即使仅用50%目标游戏标记数据也能保持竞争力。

Conclusion: CFT方法高效、可扩展，显著降低对标记数据的依赖，适用于跨游戏视觉Bug检测。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [36] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: HRM模型在无数据增强的小分辨率图像分类任务中表现不佳，不如简单的卷积架构。


<details>
  <summary>Details</summary>
Motivation: 探讨HRM模型是否可作为实用的图像分类器，特别是在无数据增强的原始条件下。

Method: 使用HRM模型（包含两个Transformer模块、一步训练、深度监督等）在MNIST、CIFAR-10和CIFAR-100上进行测试。

Result: HRM在MNIST上表现良好（98%准确率），但在CIFAR-10和CIFAR-100上过拟合且泛化能力差。

Conclusion: HRM在当前形式下不适合小分辨率图像分类任务，但未来改进可能提升其性能。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [37] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: DINOv2在自监督学习中表现优异，超越弱监督方法，本文探讨其核心思想、性能及局限性。


<details>
  <summary>Details</summary>
Motivation: 研究DINOv2如何通过多裁剪视图增强和自蒸馏技术提升自监督学习性能。

Method: 分析DINOv2的多裁剪视图增强和自蒸馏技术，并与其他SSL和WSL方法比较。

Result: DINOv2在多种下游任务中表现优异，并展示了Transformer骨干网络的特性。

Conclusion: DINOv2虽有局限性，但对未来研究方向有重要影响。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [38] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: DCS框架通过扩散模型与分类器的协同进化，解决了FSCIL中的泛化问题，显著提升了知识保留和新类学习能力。


<details>
  <summary>Details</summary>
Motivation: FSCIL任务中，模型需从少量样本中学习新类而不遗忘旧知识，现有方法因依赖有限数据集而泛化能力不足。扩散模型虽可用于数据增强，但直接应用可能导致语义偏差或无效指导。

Method: 提出DCS框架，通过动态多层面奖励函数（特征层和logits层）指导扩散模型生成图像，并与分类器形成相互促进的循环。

Result: 在FSCIL基准测试中达到最先进性能，显著提升知识保留和新类学习能力。

Conclusion: DCS通过协同进化机制有效解决了FSCIL中的泛化和数据稀缺问题。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [39] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM是一种新型视觉-语言框架，用于从监控视频中检测安全违规行为，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查在高风险领域（如采矿）中效率低且易出错，亟需智能自动化安全监控。

Method: 提出MonitorVLM框架，包含领域特定数据集、动态条款过滤模块和行为放大模块。

Result: 实验显示MonitorVLM在精度、召回率和F1分数上显著优于基线模型。

Conclusion: 该研究展示了多模态大模型在提升职业安全监控方面的潜力。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [40] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 将扩散模型集成到智能交通系统中，显著提升了事故检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在处理复杂数据分布时存在局限性，扩散模型能够更有效地理解数据分布。

Method: 提出了一种混合模型，结合了引导分类和扩散技术，利用ExceptionNet架构的输出作为扩散模型的输入，并通过时间嵌入和图像协变量嵌入动态调整网络行为。

Result: 在公开数据集上，提出的扩散模型在图像事故检测中达到了97.32%的准确率。

Conclusion: 扩散模型在智能交通系统中具有显著优势，能够高效处理复杂数据分布，提升事故检测性能。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [41] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: SAMSOD模型通过单模态监督和梯度解冲突提升RGB-T显著目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决RGB-T SOD中模态不平衡和梯度差异问题。

Method: 使用单模态监督和梯度解冲突，结合两个解耦适配器。

Result: 在多个数据集上验证了方法的有效性。

Conclusion: SAMSOD在RGB-T SOD任务中表现出色，具有广泛适用性。

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [42] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 提出了一种针对小物体的指代表达理解（REC）数据集SOREC和参数高效的微调模块PIZA，显著提升了小物体定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管REC任务在视觉-语言学习方面取得了进展，但定位极小物体仍是一个重要挑战，尤其是在自动驾驶等实际应用中。

Method: 1. 构建了包含10万对指代表达和边界框的SOREC数据集；2. 提出了渐进式迭代缩放适配器（PIZA），用于参数高效的微调。

Result: 在SOREC数据集上，PIZA显著提升了GroundingDINO模型的准确性。

Conclusion: SOREC数据集和PIZA模块为小物体REC任务提供了有效的解决方案，相关资源已公开。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [43] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 提出了一种基于注意力机制的Attention-WNet模型，用于视网膜血管分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割有助于诊断多种眼部疾病和全身血管疾病，如中风和心肌梗死。

Method: 将注意力机制融入WNet模型，形成Attention-WNet。

Result: 在HRF和DRIVE数据集上表现优于其他先进模型。

Conclusion: Attention-WNet在视网膜血管分割任务中具有优越性能。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [44] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 论文通过标注LAION-400M数据集，揭示了视觉语言模型中的偏见与训练数据的关系。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型中的偏见来源，尤其是缺乏大规模数据集的人口统计标注。

Method: 创建了包含边界框、性别和种族标签的标注数据集，使用自动标注流程。

Result: 发现数据集中存在人口不平衡和有害关联，60-70%的性别偏见可线性解释。

Conclusion: 首次大规模实证了数据集组成与模型偏见之间的直接联系。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [45] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 比较两种预训练神经网络在检测里约热内卢贫民窟中的表现，探讨任务特异性与数据量对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法未充分利用预训练神经网络的潜力，研究任务特异性与数据量对检测贫民窟性能的影响。

Method: 比较两种预训练网络：通用网络（大数据量）和专用网络（卫星图像预训练）。

Result: 探讨哪种网络在检测贫民窟时表现更优。

Conclusion: 研究旨在确定任务特异性或数据量对检测性能的影响更显著。

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [46] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为LoRA Patching的新方法，通过注入可插拔的LoRA补丁绕过现有Deepfake防御，并展示了其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake防御方法缺乏鲁棒性和可靠性，存在被绕过的风险。

Method: 采用Low-Rank Adaptation (LoRA)补丁和可学习门控机制，结合Multi-Modal Feature Alignment (MMFA)损失函数。

Result: 仅需1,000个面部样本和单次微调，LoRA补丁成功绕过多种防御。

Conclusion: 揭示了当前防御范式的关键弱点，呼吁开发更鲁棒的Deepfake防御策略。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [47] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 提出了一种通过参考集微调（RSF）来提升视觉地点识别（VPR）性能的方法，特别是在测试环境与训练数据差异较大的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有VPR方法在测试环境与训练数据差异较大时表现不佳，需要新的信息源来弥补这一差距。

Method: 利用测试时的参考集（地图）进行简单的微调（RSF），提升模型在目标域的性能。

Result: 在多个挑战性数据集上平均提升了2.3%的Recall@1性能，同时保持了模型的泛化能力。

Conclusion: RSF是一种简单有效的方法，能够显著提升VPR在跨域场景中的表现。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [48] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: ARSAM通过自适应采样和重用分解梯度加速SAM，保持模型泛化能力，计算成本降低40%。


<details>
  <summary>Details</summary>
Motivation: SAM提高模型泛化能力但计算成本高，ARSAM旨在减少计算负担。

Method: 分解SAM梯度为SGD梯度和PSF，动态重用PSF以加速训练。

Result: ARSAM在CIFAR-10/100等任务中性能与SAM相当，速度提升40%。

Conclusion: ARSAM在多种任务中高效且实用，不牺牲性能。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [49] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA框架通过多层概念捕捉和提示引导，提升了临床诊断中深度学习模型的透明性和性能。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习模型在临床诊断中的透明性，解决现有概念瓶颈模型在概念捕捉能力上的不足。

Method: 提出CoPA框架，结合概念感知嵌入生成器（CEG）和概念提示调谐（CPT），从多层视觉编码器中提取和聚合概念表示。

Result: 在三个公开数据集上，CoPA优于现有方法，显著提升了概念和疾病预测性能。

Conclusion: CoPA通过多层概念捕捉和提示引导，有效提升了模型的透明性和诊断性能。

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [50] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: ZFP压缩技术能在保持脑血管分割性能的同时显著减少3D医学影像数据体积，促进协作研究。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学影像数据体积大、复杂性高导致的协作和传输难题。

Method: 在误差容忍和固定速率模式下应用ZFP压缩技术，并与未压缩基线对比分割质量。

Result: ZFP在误差容忍模式下实现22.89:1的数据压缩比，平均Dice系数保持在0.87656。

Conclusion: ZFP是高效处理大规模医学影像数据的可行工具，有助于推动社区协作。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [51] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 提出一种混合分割架构，结合CNN、Transformer和Mamba注意力融合机制，提升医学图像分割的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型多为任务专用，性能因模态和解剖区域而异，且难以平衡复杂性和性能。

Method: 采用三分支编码器（CNN、Transformer、MAF机制）捕获多尺度依赖，结合多尺度注意力CNN解码器和共注意力门增强特征选择。

Result: 在多个基准数据集上优于现有方法，保持计算复杂度。

Conclusion: 该架构为医学影像任务提供了高效且可扩展的解决方案。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [52] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv9算法的深度学习方法，用于自动检测道路损坏和井盖，采用多边形标注提高精度。


<details>
  <summary>Details</summary>
Motivation: 手动监测道路损坏耗时、成本高且易出错，智能城市发展需要高效解决方案。

Method: 使用YOLOv9算法和多边形标注，开发包含1000多张图像的数据集，训练模型检测三类（损坏、未损坏、井盖）。

Result: 整体准确率78.1%，损坏和未损坏类别的F1分数较高（86.7%和89.2%），井盖检测因类别不平衡表现较差（18.2%）。

Conclusion: 该方法为发展中国家城市基础设施监测提供了高效且可扩展的解决方案。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [53] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 提出了一种结合对比学习和扩散模型的无配对图像转换方法，表现优异且高效。


<details>
  <summary>Details</summary>
Motivation: 解决无配对图像转换中语义一致性和生成质量的问题。

Method: 结合时间依赖的对比学习（SimCLR）和预训练的SDE模型。

Result: 在多项指标上达到SOTA水平，收敛更快且无需标签监督。

Conclusion: 该方法为无配对图像转换提供了一种高效且高质量的解决方案。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [54] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO是一个改进的LIBERO基准测试，通过系统扰动评估VLA模型的真实性能，揭示了现有模型依赖记忆而非理解的缺陷。


<details>
  <summary>Details</summary>
Motivation: LIBERO基准测试的评估设置存在问题，导致性能估计虚高，无法公平比较模型。

Method: 引入LIBERO-PRO，通过操纵对象、初始状态、任务指令和环境四个维度的扰动，系统评估模型性能。

Result: 现有模型在标准LIBERO评估中表现优异（>90%），但在扰动设置下崩溃至0.0%，暴露其依赖记忆的缺陷。

Conclusion: 呼吁社区采用更鲁棒的评估方法，避免误导性性能估计，关注模型的泛化和理解能力。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [55] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: 论文介绍了Mirage数据集，用于检测AI生成图像中的可见伪影，并探讨了大型视觉语言模型（LVLMs）在可解释AI图像检测中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像难以被标准检测器识别，但人类仍可区分，研究旨在解决这一差距。

Method: 构建Mirage数据集，并测试LVLMs在检测AI生成图像中的表现。

Result: LVLMs能有效检测含可见伪影的AI图像，但对无伪影图像效果下降。

Conclusion: LVLMs在可解释AI图像检测中有潜力，但需改进以应对无伪影图像。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [56] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround提出了一种统一的视觉基础范式，通过动态选择中间层作为“掩码提示”，解决了固定最后一层和文本嵌入投影的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定的最后一层隐藏层，导致误差累积，且缺乏显式空间提示。UGround旨在解决这些问题。

Method: UGround采用Policy-Prompted Masking，包括Stochastic Skip Connection（动态选择层）和Mask as Prompt（提供空间提示）。

Result: UGround首次统一了视觉基础任务，涵盖从传统参考表达分割到新提出的推理分割等多种场景。

Conclusion: UGround通过动态层选择和显式空间提示，显著提升了视觉基础任务的性能。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [57] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4是一种优化4D高斯泼溅的框架，通过三阶段修剪和压缩技术显著减少存储开销，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决4D高斯泼溅在动态场景表示中的存储开销问题，现有方法在压缩比或视觉质量上仍有局限。

Method: 采用三阶段修剪（采样、剪枝、合并）并结合隐式外观压缩和子向量量化（SVQ）技术。

Result: 在标准数据集上，OMG4将模型大小减少60%以上，同时保持重建质量，优于现有方法。

Conclusion: OMG4为紧凑4D场景表示提供了重要进展，具有广泛应用潜力。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [58] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，通过结构化域对齐将地面视图的开放词汇表示适应于航空图像中的目标检测，显著提升了零样本性能。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测模型在固定类别上训练，缺乏灵活性且扩展成本高。开放词汇目标检测能识别未见类别，但航空图像与地面视图存在域偏移和视角差异，需要专门适应策略。

Method: 提出对比性图像到图像对齐和多实例词汇关联，以增强航空与地面视图嵌入的相似性，并对齐航空图像与文本嵌入。

Result: 在多个数据集上验证，零样本设置下性能显著提升（如DOTAv2 +6.32 mAP）。

Conclusion: 该方法为航空应用提供了更灵活、可扩展的目标检测系统。

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [59] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 深度学习在皮肤癌诊断中展现出潜力，但仍面临复杂特征、数据不平衡等挑战，需通过数据增强、混合模型等方法解决。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球常见且致命的癌症，早期诊断对改善患者预后至关重要。

Method: 基于PRISMA框架的综述方法，探讨数据增强、混合模型和特征融合等创新方法。

Result: 深度学习模型在皮肤疾病诊断中具有革命性潜力，可提升临床决策。

Conclusion: 需持续技术进步以充分发挥深度学习在皮肤病护理中的潜力。

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [60] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出了一种名为SDAKD的新方法，通过引入学生判别器来解决GAN压缩中的容量不匹配问题，并在实验中表现优于基线和其他SOTA方法。


<details>
  <summary>Details</summary>
Motivation: GANs在生成任务中表现优异，但计算需求高，难以部署在资源受限设备上。知识蒸馏是压缩GAN的潜在方向，但由于学生生成器与教师判别器之间的容量不匹配，训练小型学生生成器具有挑战性。

Method: 提出SDAKD方法，引入学生判别器以缓解容量不匹配问题，采用三阶段训练策略，并在后两阶段整合了适应的特征图蒸馏方法。

Result: 在GCFSR和Real-ESRGAN两个超分辨率GAN上的实验表明，SDAKD在基线和其他SOTA方法上均表现出持续改进。

Conclusion: SDAKD是一种有效的GAN知识蒸馏方法，能够显著提升压缩模型的性能。

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [61] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: PoseGaze-AHP是一个同步捕捉头部姿态和注视运动的3D数据集，填补了现有数据集的空白，支持AI驱动的眼源性异常头位诊断。


<details>
  <summary>Details</summary>
Motivation: 现有数据集分别关注头部姿态和眼部运动，限制了综合诊断方法和AI在眼源性异常头位分析中的发展。

Method: 通过Claude 3.5 Sonnet模型结合逐步、分层和复杂提示策略，从医学文献中提取结构化临床数据，并使用Neural Head Avatar框架将其转化为3D表示。

Result: 数据集包含7,920张图像，提取方法准确率达91.92%，验证了其可靠性。

Conclusion: PoseGaze-AHP是首个公开的眼源性异常头位诊断资源，支持开发准确且隐私合规的诊断工具。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [62] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出了一个大规模动态数字人质量评估数据集DHQA-4D，并开发了基于多模态模型的DynaMesh-Rater方法，用于评估带纹理和无纹理的4D网格质量。


<details>
  <summary>Details</summary>
Motivation: 随着4D网格动态数字人应用的普及，其质量评估因噪声干扰变得重要，但目前缺乏相关数据集和方法。

Method: 提出DHQA-4D数据集，包含多种失真类型的4D网格；开发DynaMesh-Rater方法，提取多维特征并通过LMM模型预测质量分数。

Result: 实验证明DynaMesh-Rater在DHQA-4D数据集上优于现有方法。

Conclusion: DHQA-4D数据集和DynaMesh-Rater方法为动态4D数字人质量评估提供了有效工具。

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [63] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于ASFF改进的ResNet-50模型，用于皮肤癌分类，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类面临类间相似性高、类内变异性大和图像噪声等问题，需要更有效的特征表示方法。

Method: 通过自适应空间特征融合（ASFF）增强ResNet-50，融合多尺度语义和表面特征，减少过拟合。

Result: 在ISIC 2020数据集上达到93.18%的准确率，AUC值分别为0.9670和0.9717。

Conclusion: 该方法为计算机辅助皮肤癌诊断提供了更高效和有效的解决方案。

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [64] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 开发了一种多模态深度学习框架，结合临床、放射学和病理学图像，通过加权集成的DenseNet-121 CNN提高口腔鳞状细胞癌的早期检测。


<details>
  <summary>Details</summary>
Motivation: 口腔鳞状细胞癌（OSCC）的晚期诊断导致高死亡率，超过50%的病例在晚期被发现，5年生存率低于50%。

Method: 使用公开数据集训练DenseNet-121 CNN，通过数据增强和模态特定预处理提高鲁棒性，采用加权集成策略融合预测。

Result: 放射学和病理学模态验证准确率分别为100%和95.12%，临床图像较低（63.10%），集成模型整体准确率为84.58%。

Conclusion: 该框架为非侵入性AI辅助工具，有助于早期识别高风险病变，支持临床决策，减少诊断延迟并改善患者预后。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [65] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种基于通用动作专家的框架，通过稀疏3D轨迹作为中间表示，连接VLM的高级规划能力和低级动作模块，解决了传统VLA模型的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统VLA模型因数据稀缺和语义模糊导致泛化能力差，双系统方法也存在动作模块的模糊性问题，难以实现跨任务训练。

Method: 利用稀疏3D轨迹作为中间表示，VLM生成粗略3D路径点，动作专家通过实时点云观测将其细化为可执行动作序列，采用“动作预训练-点云微调”范式。

Result: 结合了VLM的视觉理解和规划能力与动作专家的细粒度动作泛化能力，提升了训练效率和泛化鲁棒性。

Conclusion: 该框架有效解决了VLA模型的泛化问题，为物理世界中的规划和动作执行提供了新思路。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [66] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出了一种基于聚类的数据选择方法IQA-Select，用于优化可解释图像质量评估（IQA）的指令调优数据集，减少冗余数据并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模指令调优数据集，但数据冗余和高计算成本可能损害模型性能，因此需要探索数据质量对模型性能的影响。

Method: 通过预训练的多模态大语言模型（MLLM），研究不同规模指令调优数据对性能的影响，并提出基于聚类的三阶段数据选择框架IQA-Select。

Result: IQA-Select仅使用10%的数据即可在Q-Bench和AesBench上分别达到102.1%和103.7%的全数据微调性能，显著降低计算成本。

Conclusion: 数据质量比数量更重要，IQA-Select为可解释IQA提供了一种高效的数据选择方法。

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [67] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: OpenFLAME是一个联邦化的视觉定位系统（VPS）后端，支持独立组织扫描和维护自己的空间，解决集中式VPS在隐私和维护上的不足。


<details>
  <summary>Details</summary>
Motivation: 集中式VPS无法覆盖私人室内空间，且存在隐私和维护问题，需要分布式解决方案。

Method: 提出联邦化图像定位概念，提供跨地图数据管理和合并的参考方案，不共享私有数据。

Result: OpenFLAME支持分布式维护和访问控制，扩展了VPS的覆盖范围。

Conclusion: 联邦化VPS是解决集中式系统局限的有效方案，适用于未来AR应用。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [68] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 提出一种新方法，将零样本细粒度图像分类转化为视觉问答框架，利用LVLMs的理解能力，并通过注意力干预技术和改进的数据集提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索LVLMs在零样本细粒度图像分类中的潜力，解决现有方法依赖直接类别名称生成的局限性。

Method: 将任务转化为视觉问答框架，结合注意力干预技术和改进的类别描述数据集。

Result: 在多个细粒度图像分类基准上超越现有SOTA方法。

Conclusion: 证明了LVLMs在零样本细粒度分类任务中的有效性及潜力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [69] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D栅格化的轻量级数据增强方法（RAP），用于端到端驾驶规划，通过语义保真和特征对齐提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的真实感渲染，但驾驶规划更依赖几何和动态信息而非纹理或光照，因此需要一种更高效且可扩展的解决方案。

Method: 采用3D栅格化替代渲染，结合Raster-to-Real特征对齐，生成合成视图以增强训练数据。

Result: RAP在多个基准测试中表现最佳，证明了其鲁棒性和泛化能力。

Conclusion: 轻量级栅格化结合特征对齐是端到端驾驶训练的有效替代方案，优于真实感渲染方法。

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [70] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: 论文研究了自动驾驶感知系统在雾天条件下的性能问题，比较了多种去雾方法的效果，并评估了它们在真实场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 雾天条件下，光线散射导致图像对比度降低，影响自动驾驶感知系统的安全性能。现有去雾方法在图像质量上的改进并未一致提升下游任务（如检测和分割）的性能，且多数评估基于合成数据，缺乏真实场景验证。

Method: 研究采用结构化实证方法，比较了多种去雾管道，包括经典滤波器、现代去雾网络、链式组合（滤波器→模型、模型→滤波器）以及基于视觉-语言模型的图像编辑方法。使用Foggy Cityscapes数据集评估图像质量和下游任务性能。

Result: 研究发现去雾在某些情况下有效，链式组合可能产生协同效应或性能下降，视觉-语言模型与专用方法表现相当。此外，VLM评分与任务指标（如mAP）强相关。

Conclusion: 研究为去雾方法提供了透明、任务导向的基准，并明确了预处理在恶劣天气下提升自动驾驶感知性能的条件。

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [71] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO是一个用于通用人体运动视频生成的级联框架，结合了文本到运动模型和条件视频扩散模型，通过精心设计的组件优化训练和推理过程。


<details>
  <summary>Details</summary>
Motivation: 尽管视频扩散模型（VDMs）发展迅速，但在通用人体视频生成中的应用仍不足，大多数工作局限于图像到视频或特定领域（如舞蹈视频）。

Method: CAMEO通过分析文本提示和视觉条件，有效训练VDM，确保运动描述、条件信号和生成视频之间的强对齐，并引入相机感知条件模块连接两个阶段。

Result: 在MovieGen基准和新引入的T2M-VDM组合基准上验证了方法的有效性，展示了其多样化的应用场景。

Conclusion: CAMEO通过级联框架和优化设计，显著提升了通用人体运动视频生成的质量和适用性。

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [72] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 提出了一种结合CNN-LSTM模型和LLMs的框架，用于网球击球动作分析，生成可操作的反馈。


<details>
  <summary>Details</summary>
Motivation: 现有系统缺乏将生物力学数据转化为对球员和教练有意义的反馈。

Method: 使用CNN-LSTM模型提取生物力学特征，结合LLMs生成反馈。

Result: 框架在分类性能和可解释性上表现良好。

Conclusion: 该研究填补了可解释AI与运动生物力学之间的空白。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [73] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp方法通过生成合成数据集提升Video-LLMs在细粒度时间理解任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在时间理解任务中表现不佳，主要因为数据集缺乏视觉复杂性和时间细节。

Method: 提出TimeWarp方法，创建合成时间数据集，并构建大规模偏好数据集以捕捉时间动态。

Result: 在七个基准测试中显著提升了时间理解性能。

Conclusion: TimeWarp方法有效提升了Video-LLMs的时间理解能力。

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [74] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 论文研究了在生物医学视觉语言模型（VLMs）中扩展文本编码器上下文长度对长格式标题的影响，发现更长的上下文能提升检索和分类性能，并提出了BIOMEDICA-LongCAP数据集和BMC-LongCLIP模型。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs通常使用短文本窗口（<77 tokens）预训练，导致长格式标题被截断，而生物医学标题中大量内容超过此长度。

Method: 通过扩展文本编码器的上下文长度，利用BIOMEDICA-LongCAP数据集（1M图像-标题对）训练支持512 tokens的BMC-LongCLIP模型。

Result: BMC-LongCLIP在长标题检索任务中Recall@1提升30%，分类任务平均提升2%，且收敛更快。

Conclusion: 长上下文建模是推进生物医学VLMs的有前景方向。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [75] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: CPG框架通过可控伪标签生成和优化循环，解决了未标记数据分布未知的问题，显著提升了长尾半监督学习的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法假设未标记数据遵循特定分布，但实际分布未知。CPG旨在解决这一问题。

Method: 采用动态可控过滤机制、贝叶斯最优分类器和类感知自适应增强模块。

Result: 在多个基准数据集上，CPG性能提升高达15.97%。

Conclusion: CPG有效解决了未标记数据分布未知的挑战，性能优于现有方法。

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [76] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 提出了一种基于PaddleOCRv5的微调方法，显著提升了汉喃文本的识别准确率，尤其在噪声图像条件下。


<details>
  <summary>Details</summary>
Motivation: 汉喃文本识别对越南历史文献数字化和跨语言语义研究至关重要，但现有OCR系统难以处理古文献中的退化扫描、非标准字形和手写变体。

Method: 通过精选的古越南汉喃手稿数据集对PaddleOCRv5的文本识别模块进行微调，包括预处理、LMDB转换、评估和可视化。

Result: 微调后模型在汉喃文本上的准确率从37.5%提升至50.0%，尤其在噪声条件下表现更佳。

Conclusion: 该方法有效提升了汉喃文本识别性能，并开发了交互式演示工具，支持汉越语义对齐、机器翻译和历史语言学研究。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [77] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: MetaSeg是一种基于元学习的框架，用于训练隐式神经表示（INRs）进行医学图像分割，其参数比传统方法少90%。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在信号表示方面表现优异，但在语义分割任务中表现不足，因此需要一种新方法来优化其性能。

Method: MetaSeg通过元学习优化INR的初始参数，使其能够同时预测像素强度和类别标签，并在测试图像上快速微调。

Result: 在2D和3D脑MRI分割任务中，MetaSeg的Dice分数与U-Net相当，但参数减少了90%。

Conclusion: MetaSeg为医学图像分割提供了一种高效且可扩展的替代方案，优于传统资源密集型架构。

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [78] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: ViTL是一个两阶段的长视频问答框架，通过定位问题相关区间并重新分配视觉标记来提高效率，同时引入新数据集支持多选问答。


<details>
  <summary>Details</summary>
Motivation: 解决长视频问答中固定标记预算下的效率和准确性问题。

Method: 两阶段框架：低帧率定位问题区间，高帧率重新分配视觉标记；引入新数据集支持多选问答。

Result: 在固定标记预算下，ViTL在长视频问答和时间定位任务中表现优异，效率提升50%。

Conclusion: ViTL和配套数据集为长视频问答提供了高效且可解释的解决方案。

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [79] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: AgentAug是一个数据增强框架，通过模拟假新闻视频的创作过程生成多样化数据，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有假新闻视频检测器因训练数据有限且单一导致性能不佳，需解决数据多样性和覆盖不足的问题。

Method: 采用LLM驱动的多类别伪造流程和基于不确定性采样的主动学习策略。

Result: 在两个基准数据集上，AgentAug显著提升了检测器的性能。

Conclusion: AgentAug有效解决了假新闻视频检测中的数据不足问题，提升了模型性能。

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [80] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 研究通过优化超参数提升文本驱动图像编辑的精确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像编辑中存在结果不一致的问题，如头发颜色变化不一致。

Method: 研究了'word swap'方法，开发了'attention re-weight method'，并提出了'CL P2P'框架。

Result: 解决了循环不一致等问题，提升了图像生成的质量。

Conclusion: 优化超参数和注意力机制能显著改善图像编辑的效果。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [81] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight模型通过动态调用工具提升视觉定位准确性，在少量训练数据下超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在GUI系统中视觉定位不可靠的问题，以支持精确的指针级操作。

Method: 引入GUI-Spotlight模型，通过动态调用多个专用工具迭代缩小屏幕相关区域。

Result: 在ScreenSpot-Pro基准测试中，GUI-Spotlight以18.5K训练样本达到52.8%准确率，超越现有模型。

Conclusion: GUI-Spotlight显著提升了视觉定位准确性，为GUI系统的实用化提供了可能。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [82] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 提出一种范围估计方法，通过层局部最小值优化量化误差，提升后训练量化性能。


<details>
  <summary>Details</summary>
Motivation: 低比特量化在保持模型精度方面具有挑战性，需要改进量化性能。

Method: 将范围估计建模为量化误差最小化的优化问题，提出高效搜索算法并在权重空间应用。

Result: 在ResNet和Inception-v3模型上，8位和6位量化几乎无精度损失，4位量化精度显著提升。

Conclusion: 该方法在图像分类任务中优于现有技术，量化性能显著提升。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [83] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind是一个场景感知的三模态组合检索框架，旨在通过从大规模存储库中检索3D资产来增强元宇宙中的场景生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在3D资产检索中忽视空间、语义和风格约束的问题，以及缺乏专门为3D资产检索设计的标准化检索范式。

Method: 提出灵活的检索机制，支持文本、图像和3D模态的任意组合查询，通过联合建模对象级特征和场景级布局结构来增强空间推理和风格一致性。引入可插拔的等变布局编码器ESSGNN。

Result: 实验证明MetaFind在空间和风格一致性上优于基线方法。

Conclusion: MetaFind通过多模态检索和场景感知布局编码，显著提升了3D资产检索的上下文和风格一致性。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [84] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 论文提出了一种改进的损失函数，通过整合太阳耀斑子类别的序数信息，优化了传统的二元交叉熵损失函数，以提高模型在预测阈值附近的分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统的二元分类框架忽略了太阳耀斑子类别之间的序数关系，导致模型在预测阈值附近容易误分类。

Method: 提出了一种序数感知的损失函数，通过加权惩罚阈值附近的错误预测，优化模型学习过程。

Result: 该方法旨在提升模型在预测太阳耀斑时的整体性能，尤其是在阈值附近的分类准确性。

Conclusion: 通过整合序数信息，改进的损失函数能够更有效地优化模型，减少预测阈值附近的误分类。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [85] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantDemoire是一个专为去摩尔纹任务设计的后训练量化框架，通过减少异常值和频率感知校准策略，显著降低了计算资源需求，同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的去摩尔纹深度学习方法需要大量计算资源，难以在边缘设备上部署，而直接应用现有量化方法会导致性能下降。

Method: 提出QuantDemoire框架，包含异常值感知量化器和频率感知校准策略，以减少异常值误差并增强平滑区域的表示。

Result: QuantDemoire在W4A4配置下比现有量化方法性能提升超过4 dB，同时大幅减少参数和计算量。

Conclusion: QuantDemoire为去摩尔纹任务提供了一种高效的量化解决方案，适用于边缘设备部署。

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [86] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA是一种结合扩散生成先验和多正则化约束的低剂量稀疏视图CT重建方法，在ADMM框架下实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决极稀疏视图下的不适定问题和纹理丢失问题，结合生成和物理约束以提高重建质量。

Method: 结合NCSN++扩散模型与各向异性TV和核范数（LoRA）约束，采用2D切片策略、FFT加速和张量并行优化。

Result: 在多个数据集上，TV-LoRA在SSIM、纹理恢复、边缘清晰度和伪影抑制方面优于基准方法。

Conclusion: TV-LoRA在低剂量稀疏采样场景下实现了高保真、高效的3D CT重建，具有广泛的临床适用性。

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [87] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 论文提出了一种评估拓扑地图的新协议，包括一致性度量和数据集模糊性量化，并发布了开源数据集和基线系统。


<details>
  <summary>Details</summary>
Motivation: 拓扑地图领域缺乏标准化的评估指标和数据集，导致无法公平比较不同系统，且感知混淆问题未充分量化。

Method: 通过形式化拓扑一致性并提出定位精度作为替代指标，同时提出数据集模糊性量化方法，并开源了基准数据集和基线系统。

Result: 实验揭示了当前方法在感知混淆下的局限性，并提供了新的评估工具。

Conclusion: 开源的数据集和工具将促进拓扑地图研究的可重复性和一致性。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [88] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件相机的稀疏运动场估计方法，包括新数据集HREM和高效模型EEMFlow，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决事件相机中稀疏运动场估计的两个关键问题：缺乏专用数据集和事件数据密度挑战。

Method: 生成HREM数据集，提出EEMFlow网络及其升级版EEMFlow+，引入CDC模块和ADM模块。

Result: EEMFlow比现有方法快30倍，ADM提升性能8%-10%。

Conclusion: HREM和EEMFlow为事件相机运动场估计提供了高效解决方案，ADM增强了模型泛化能力。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [89] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出了一种新的6D物体姿态估计方法，通过预训练编码器和联合学习策略加速训练，同时利用时间相关评分缩放提高采样质量，无需额外评估网络。


<details>
  <summary>Details</summary>
Motivation: 现有方法训练收敛慢且需要额外网络评估姿态假设，限制了效率和准确性。

Method: 预训练编码器并联合学习回归和扩散去噪头，引入时间相关评分缩放指导采样。

Result: 在多个基准测试中达到最先进精度，训练和推理效率更高。

Conclusion: 该方法简单有效，解决了现有方法的局限性，提升了性能和效率。

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [90] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 论文提出了一种解决多模态大语言模型（MLLMs）蒸馏中概念漂移问题的新方法，通过自主偏好优化（APO）提升学生模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多教师模型的推理轨迹存在概念漂移问题，导致学生模型性能下降，需要一种方法来解决这一挑战。

Method: 提出“学习、比较、批评”范式，结合APO进行概念对齐，优化学生模型的推理能力。

Result: 实验表明，该方法在一致性、鲁棒性和泛化性上表现优异，并贡献了大规模数据集CXR-MAX。

Conclusion: APO方法有效解决了概念漂移问题，提升了蒸馏模型的性能，为相关研究提供了新思路。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [91] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield是一个基于多模态大视觉语言模型（LVLM）的检索增强生成（RAG）框架，用于自动化生成建筑安全检查报告，通过整合视觉和音频输入，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统建筑安全检查方法效率低下，需要处理大量信息。现有的大视觉语言模型（LVLMs）应用存在无关或不具体的响应、模态输入受限和幻觉问题，而大型语言模型（LLMs）则受限于训练数据的可用性和实时适应性。

Method: SiteShield结合了视觉和音频输入，采用检索增强生成（RAG）框架，利用真实世界数据进行训练和评估。

Result: SiteShield在F1分数（0.82）、汉明损失（0.04）、精确度（0.76）和召回率（0.96）上优于单模态LLMs。

Conclusion: SiteShield为提升安全检查报告的信息检索和生成效率提供了新途径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [92] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE是一种无需先验知识的生成式去偏框架，通过生成模型和自适应优化显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经网络易学习隐含偏差，现有方法依赖强假设，不适用于实际场景。

Method: BLADE通过生成模型跨偏差域翻译图像，自适应优化图像，并调整表示以增强鲁棒性。

Result: 在多个基准数据集上显著优于现有方法，如CIFAR-10上提升18%。

Conclusion: BLADE为无监督开发鲁棒深度学习模型提供了新方向。

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [93] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: SEG-MIL-CBM是一种结合概念引导图像分割和多实例学习的新框架，通过语义区域推理提供透明且空间基础的概念级解释，无需概念标注。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的决策过程缺乏可解释性，尤其在安全关键应用中，现有模型可能依赖不可靠特征，影响鲁棒性和解释有效性。

Method: 将概念引导的图像分割与基于注意力的多实例学习结合，将分割区域视为实例，学习跨区域证据聚合。

Result: 在虚假相关性、输入损坏和大规模基准测试中表现鲁棒，并提供透明概念级解释。

Conclusion: SEG-MIL-CBM通过语义区域推理提升了模型的可解释性和鲁棒性，无需额外标注。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [94] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa框架通过维度混合ODE求解器优化特征缓存策略，显著加速Diffusion Transformers的采样过程，实现近无损加速。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers的迭代采样过程因高计算成本成为瓶颈，现有特征缓存方法未考虑特征维度的异质性动态行为。

Method: 提出HyCa框架，通过混合ODE求解器建模隐藏特征演化，实现维度级缓存策略。

Result: 在多个模型和领域实现显著加速，如FLUX加速5.55倍，HunyuanVideo加速5.56倍，Qwen-Image和Qwen-Image-Edit加速6.24倍。

Conclusion: HyCa通过维度级缓存策略有效解决了特征缓存中的异质性问题，实现了高效且无损的加速。

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [95] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image框架通过动态检索网络知识优化T2I生成，显著提升新颖或OOD实体的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决T2I模型在生成新颖或OOD实体时性能下降的问题。

Method: 设计代理动态搜索网络图像，进行多模态提示优化。

Result: 在NICE基准上准确率提升8.1%，语义对齐和视觉美学表现优异。

Conclusion: 框架高效且性能优越，为T2I系统适应动态现实世界提供新途径。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [96] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC提出了一种层次化语义树方法，通过几何感知距离和密度驱动聚合优化AR模型的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统AR模型将视觉标记视为扁平词汇，忽略了嵌入空间的内在结构，导致预测任务复杂且效率低下。

Method: MASC利用几何感知距离和密度驱动聚合构建层次化语义树，简化预测任务。

Result: MASC将训练速度提升57%，生成质量显著提高（FID从2.87降至2.58）。

Conclusion: 结构化预测空间对生成模型的可扩展性至关重要，MASC使AR模型与最先进方法竞争。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [97] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn是一个两阶段的AI图像检测框架，通过定位可疑区域并进行聚焦分析，提高检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的增长模糊了真实与合成内容的界限，现有视觉语言模型（VLMs）难以检测高质量合成图像中的细微伪影。

Method: ZoomIn模仿人类视觉检查，先扫描图像定位可疑区域，再对放大区域进行聚焦分析。使用MagniFake数据集（20,000张标注图像）训练。

Result: 方法达到96.39%的准确率，并具有鲁棒泛化能力，同时提供基于视觉证据的可解释性。

Conclusion: ZoomIn在检测AI生成图像时兼具高准确性和可解释性，为数字完整性提供了有效工具。

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [98] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: 提出了一种简单、端到端可训练的图像配准算法，代码简洁，适用于训练数据和时间有限的情况。


<details>
  <summary>Details</summary>
Motivation: 解决图像配准问题，即在有限资源和代码复杂度下实现高效对齐。

Method: 使用少量Python代码实现端到端可训练算法，适用于小规模训练数据。

Result: 在74张图像的立体视觉任务中表现良好，代码简洁且效果准确。

Conclusion: 该算法简洁高效，适合资源受限的场景。

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [99] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出了一种名为ArConv的新型卷积层，优化了卷积神经网络的可访问性，适用于移动设备，并在眼疾诊断任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 提高深度神经网络的可访问性，特别是在眼疾早期诊断中，以简化计算复杂度并扩大应用范围。

Method: 重新设计和优化卷积层，提出ArConv层，构建了一个参数较少的新模型。

Result: 新模型仅含130万参数，在RfMiD数据集上表现优于MobileNetV2（准确率0.9328 vs 0.9266）。

Conclusion: ArConv层显著提升了模型的效率和准确性，适用于移动设备，为眼疾诊断提供了实用解决方案。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [100] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido是一种生成模型，用于逼真的对象和场景级神经渲染，通过序列到序列的图像合成任务实现3D渲染，无需显式3D表示。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D渲染依赖显式3D表示和稀缺的相机标注数据的问题，通过视频数据预训练提升空间一致性。

Method: 采用序列到序列的生成框架，结合掩码自回归框架和整流流变换器，统一3D和视频建模。

Result: 在视图合成基准测试中达到新最优，零样本性能在少视图设置中优于其他生成方法，多视图设置中与逐场景优化方法质量相当。

Conclusion: Kaleido通过统一框架和视频数据预训练，显著提升了神经渲染的质量和效率。

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [101] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出CoSSeg-TTA框架，用于GED4 MRI的肝脏分割，结合半监督学习和域适应模块，提升跨中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决MRI肝脏分割中标注数据少、域偏移和传统方法结构失真等问题。

Method: 基于nnU-Netv2，引入半监督均值教师方案和域适应模块，结合测试时适应策略。

Result: 在低标注条件下优于基线，Dice分数和Hausdorff距离表现更优。

Conclusion: CoSSeg-TTA在跨域泛化和分割性能上表现优异。

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [102] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出了一种基于概念的解释方法，无需先验知识即可防御对抗性补丁攻击，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法需要已知补丁大小或位置，限制了实用性。

Method: 利用概念激活向量识别并抑制关键概念，中和补丁影响。

Result: 在Imagenette数据集上，ResNet-50模型表现优于PatchCleanser，鲁棒性和准确性更高。

Conclusion: 结合可解释性与鲁棒性，概念驱动防御是抵御对抗性补丁攻击的有效策略。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [103] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: Adapt-STformer是一种基于循环可变形Transformer编码器的Seq-VPR方法，支持可变序列长度、快速推理和低内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的Seq-VPR方法在性能和效率上存在不足，无法同时满足灵活性和实时性需求。

Method: 提出Recurrent-DTE，通过迭代循环机制融合多帧信息，支持可变序列长度。

Result: 在多个数据集上，Adapt-STformer召回率提升17%，序列提取时间减少36%，内存使用降低35%。

Conclusion: Adapt-STformer在性能和效率上均优于现有方法，满足了实时性需求。

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [104] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit通过将图像编辑重构为视频生成问题，利用预训练视频生成模型确保物理一致性，并在推理时引入时间推理阶段，显著提升了视觉保真度和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有大型生成模型在图像编辑和上下文图像生成方面取得进展，但缺乏物理一致性，特别是在世界模拟相关任务中。

Method: ChronoEdit将输入和编辑图像视为视频的首尾帧，利用预训练视频生成模型捕捉对象外观和隐含物理特性，并通过时间推理阶段在推理时进行编辑。

Result: ChronoEdit在视觉保真度和物理合理性上超越现有基线，并通过PBench-Edit基准验证其有效性。

Conclusion: ChronoEdit通过视频生成框架和时间推理，显著提升了图像编辑的物理一致性，为世界模拟任务提供了新思路。

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [105] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD是首个多中心、公开的3D步态数据集，用于帕金森病（PD）的客观评估，支持临床评分预测和无监督运动任务。


<details>
  <summary>Details</summary>
Motivation: 现有PD步态评估缺乏大规模、多样化且临床注释的运动数据集。

Method: 通过统一预处理流程将RGB视频或动作捕捉数据转换为匿名SMPL网格，支持监督和无监督任务。

Result: 运动编码器优于手工特征，预训练显著提升性能（MPJPE从60.8mm降至7.5mm，PD严重度F1提高17%）。

Conclusion: CARE-PD展示了临床多样化数据的重要性，为PD研究提供了宝贵资源。

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [106] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR是一种多尺度自回归框架，通过从粗到细的预测方法，直接从H&E图像预测基因表达，解决了现有方法忽略基因共表达结构和连续回归的问题。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学（ST）成本高昂，而直接从H&E图像预测基因表达是一种经济高效的替代方案。现有方法忽略了基因共表达结构，并将任务视为连续回归，导致生物学上不合理的输出。

Method: GenAR将基因聚类为层次结构以捕捉跨基因依赖关系，将表达建模为离散标记生成以直接预测原始计数，并在解码时结合组织学和空间嵌入。

Result: 在四种不同组织类型的空间转录组数据集上，GenAR实现了最先进的性能。

Conclusion: GenAR为精准医学和经济高效的分子分析提供了潜在应用，代码已公开。

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [107] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: Diffusion^2框架通过双向扩散模型解决瞬时轨迹预测问题，提升自动驾驶和机器人交互的安全性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中瞬时轨迹数据不足，导致预测困难，增加交通事故风险。

Method: 提出双向扩散模型（反向预测历史轨迹，正向预测未来轨迹），并设计双头参数化机制和自适应噪声模块。

Result: 在ETH/UCY和Stanford Drone数据集上达到最优性能。

Conclusion: Diffusion^2有效解决了瞬时轨迹预测问题，提升了预测准确性。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [108] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim是一个语言引导的框架，生成具有多视角一致性和对象级控制的4D场景。


<details>
  <summary>Details</summary>
Motivation: 为机器人提供可控和可编辑的时空环境，支持可扩展的训练数据、可重复的评估和灵活的任务设计。

Method: 结合轨迹引导生成和特征场蒸馏，实现交互式编辑而无需完全重新生成。

Result: MorphoSim在保持高场景保真度的同时，实现了可控性和可编辑性。

Conclusion: MorphoSim为动态环境生成提供了高效且灵活的工具。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [109] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: VLMs在单一形状计数任务中表现可靠，但在组合形状计数任务中表现不佳，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在计数任务中的能力，尤其是组合形状计数时的表现，以揭示其潜在局限性。

Method: 设计VLMCountBench基准，使用基本几何形状及其组合，严格控制变量，研究颜色、大小和提示细化对计数的影响。

Result: VLMs在单一形状计数中可靠，但在组合形状计数中表现显著下降。

Conclusion: 当前VLMs在组合计数任务中存在局限性，为未来研究提供了重要方向。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [110] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++ 是一个新颖的盲脸恢复框架，通过分解任务和动态融合生成与身份特征，实现了高质量的恢复和身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉质量和身份保真度之间存在权衡，导致身份失真或退化去除不理想。

Method: 将盲脸恢复分解为三个子任务，并引入学习型可变形注册模块、纹理引导恢复网络和深度度量学习。

Result: 在真实和合成数据集上，CodeFormer++ 在视觉保真度和身份一致性方面表现优异。

Conclusion: CodeFormer++ 通过优化生成先验的利用，显著提升了盲脸恢复的质量和身份保真度。

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [111] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: 提出了一种名为A.I.R.的训练免费方法，用于自适应、迭代和基于推理的帧选择，以解决视频问答中帧选择的计算成本和准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前帧选择方法在轻量级相似性模型和深度分析模型之间存在准确性与计算成本的权衡，无法同时满足高效和精确的需求。

Method: 利用强大的视觉语言模型对复杂查询进行深度语义分析，并通过成本效益高的迭代循环处理少量高潜力帧。

Result: 在多个视频问答基准测试中表现优于现有帧选择方法，显著提升了基础视觉语言模型的性能，并大幅提高了计算效率。

Conclusion: A.I.R.方法在帧选择中实现了高效与高精度的平衡，为视频问答任务提供了有效的解决方案。

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [112] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: 提出了一种名为reAR的训练策略，通过引入token-wise正则化目标，解决了视觉自回归生成中的生成器-分词器不一致问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成在性能上仍落后于扩散模型，主要原因是生成器与分词器之间的不一致性。

Method: 提出reAR训练策略，通过预测当前token的视觉嵌入和目标token的嵌入（在噪声上下文中）来正则化模型。

Result: 在ImageNet上，gFID从3.02降至1.86，IS提升至316.9；应用于高级分词器时，仅用177M参数即达到gFID 1.42，媲美更大规模的扩散模型。

Conclusion: reAR通过简单的训练策略显著提升了视觉自回归生成的性能，无需修改分词器或推理流程。

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [113] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet提出了一种统一设计，通过通道校准和空间增强整合多尺度特征，解决了现有伪装目标检测方法因组件累积导致的复杂性和细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过独立添加边界模块、注意力机制和多尺度处理器等组件，导致计算负担增加且性能提升有限，同时因降低分辨率而丢失细节。

Method: SPEGNet采用统一设计，通过通道校准和空间增强整合多尺度特征，边界直接从上下文丰富的表示中生成，并通过渐进细化实现尺度自适应边缘调制。

Result: SPEGNet在CAMO、COD10K和NC4K数据集上分别达到0.887、0.890和0.895的$S_\alpha$分数，且具有实时推理速度。

Conclusion: SPEGNet在保持边界精度和区域一致性的同时，有效处理了不同尺度的伪装目标，包括遮挡和模糊边界问题。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [114] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM通过将检测数据集转化为带有链式推理的医学视觉问答数据，结合分阶段学习策略，在医学VQA任务中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中临床诊断推理与AI结合的挑战。

Method: 提出MedCLM流程，将检测数据转化为带有CoT推理的VQA数据，并采用分阶段学习策略（Easy、Medium、Hard）。

Result: 在多个医学VQA基准测试中达到最优性能。

Conclusion: MedCLM为开发临床对齐的医学视觉语言模型提供了可扩展框架。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [115] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: 论文提出了VaseVQA-3D数据集和VaseVLM模型，解决了现有视觉语言模型在文化遗产领域的数据稀缺和领域知识不足问题，显著提升了3D花瓶文物的识别与理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在文化遗产领域（如3D花瓶文物）面临数据稀缺和领域知识不足的问题，无法有效处理此类任务。

Method: 构建了首个3D视觉问答数据集VaseVQA-3D，并开发了VaseVLM模型，通过领域自适应训练提升性能。

Result: 实验结果显示，VaseVLM在R@1指标上提升了12.8%，词汇相似度上提升了6.6%。

Conclusion: 该方法为数字文化遗产保护研究提供了新的技术路径，显著提升了3D花瓶文物的识别与理解能力。

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [116] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit是一个专为电商领域设计的图像编辑模型，通过数据工程、模型架构设计和训练策略，实现了高保真且一致的图像编辑效果。


<details>
  <summary>Details</summary>
Motivation: 通用图像编辑模型在电商场景中常遇到一致性问题，因此需要针对电商领域设计专用模型。

Method: 采用分层模型框架（基础模型、模式转换模块和一致性增强模块）和两阶段训练策略（模式转换和一致性增强）。

Result: 在自建电商基准测试中，TBStar-Edit在客观指标（VIE Score）和用户主观偏好上均优于通用模型。

Conclusion: TBStar-Edit为电商图像编辑提供了一种高效且一致的解决方案。

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [117] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 提出异步扩散模型（AsynDM），通过为不同像素分配不同时间步，改善文本到图像生成的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成高质量图像时，由于同步去噪机制，导致图像与输入提示的对齐效果不佳。

Method: 提出异步扩散模型框架，动态调整像素的时间步，使提示相关区域逐步去噪，利用更清晰的上下文。

Result: 实验表明，异步扩散模型能显著提升多样提示下的文本到图像对齐效果。

Conclusion: 异步扩散模型通过改进去噪机制，有效提升了生成图像与文本提示的对齐质量。

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [118] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一种名为TAG的高效直接引导方法，通过放大切向分量来纠正采样轨迹，提升扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但存在语义不一致或幻觉问题，现有方法依赖外部信号或架构修改，计算开销大。

Method: TAG利用中间样本作为投影基，放大估计分数的切向分量，通过一阶泰勒展开形式化引导过程。

Result: TAG在不修改扩散模型的情况下，以最小计算开销提升采样保真度。

Conclusion: TAG为扩散引导提供了新视角，是一种即插即用、架构无关的模块。

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [119] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: CRL提出了一种条件表示学习方法，通过用户指定的标准生成定制化特征空间，优于传统通用表示方法。


<details>
  <summary>Details</summary>
Motivation: 传统表示学习方法学习的通用表示可能不符合下游任务需求，例如动物栖息地分析中场景特征更重要。

Method: 利用大语言模型生成描述性文本构建语义基础，再通过视觉语言模型将图像表示投影到条件特征空间。

Result: 在分类和检索任务中表现出优越性和通用性。

Conclusion: CRL能有效捕捉特定标准的语义，适用于多种定制化任务。

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [120] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: 论文提出AI Session Recorder，通过记录病理学家的日常操作生成行为数据，构建Pathologist-o3代理系统，提升病理诊断的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型缺乏实际代理系统，无法模拟专家行为进行多阶段诊断，主要原因是缺乏临床对齐的行为数据。

Method: 开发AI Session Recorder记录专家操作，生成Pathology-CoT数据集，并构建两阶段代理Pathologist-o3，结合行为引导的推理。

Result: 在胃肠道淋巴结转移检测中，Pathologist-o3达到84.5%精确率、100%召回率和75.4%准确率，优于现有模型。

Conclusion: 该框架将日常操作转化为可扩展的专家验证监督，为临床AI提供了一条可行路径。

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [121] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: S$^2$Fin网络通过空间-光谱-频率交互模块，结合高频稀疏增强Transformer和两级空间-频率融合策略，提升了多模态遥感图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从异构冗余的多模态图像中提取结构和细节特征，需引入频域学习来建模关键稀疏细节。

Method: 提出高频稀疏增强Transformer和两级空间-频率融合策略，包括自适应频率通道模块和高频共振掩码。

Result: 在四个基准数据集上，S$^2$Fin优于现有方法，分类性能显著提升。

Conclusion: S$^2$Fin通过频域学习有效提升了多模态遥感图像分类的精度。

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [122] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 提出了一种结合Transformer和纹理方法的新集成框架，用于检测深度伪造媒体，并在DFWild-Cup数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以泛化到不同数据集和生成技术，需要更鲁棒的检测方案。

Method: 结合Swin Transformers、ViTs和纹理方法，采用数据分割、顺序训练、频率分割、基于补丁的注意力和面部分割技术。

Result: 在DFWild-Cup数据集上实现了最先进的性能。

Conclusion: 混合模型能有效应对深度伪造检测的挑战，为实际应用提供了鲁棒解决方案。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [123] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: 研究探讨了四种最佳图像分割方法与SLIC在森林砍伐检测任务中对分类器训练的影响，发现分类器融合方法能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统SLIC算法在遥感图像分割中表现不佳，研究旨在探索更适合森林砍伐检测的分割方法。

Method: 比较四种最佳分割方法与SLIC，使用PyCaret AutoML库训练分类器，并应用分类器融合方法。

Result: 初始结果显示分割方法间性能差异小，但分类器融合显著提升了平衡准确率。

Conclusion: 分割方法选择和分类器融合对森林砍伐检测任务至关重要。

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [124] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona是一个针对教育场景的大规模基准数据集，用于评估虚拟学生代理的主观能力，并通过实验验证了其在提升模型性能方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育中的应用增加，虚拟学生代理的主观能力评估不足，限制了对其边界和可信部署的理解。

Method: 提出EduPersona数据集，基于Big Five理论，包含多语言、多学科的对话数据，并分解主观能力为三个渐进任务进行评估。

Result: 实验表明，经过EduPersona微调的模型在三个任务上均有显著提升：TASK1 +33.6%，TASK2 +30.6%，TASK3 +14.9%。

Conclusion: EduPersona为教育场景提供了首个主观能力基准，并开源数据集和框架以支持可信和人性化AI的研究。

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [125] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: 提出了一种多阶段混合运动专家（MoME）架构，用于从步态序列中预测心理特征，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索步态中蕴含的心理特征信息，解决这一未充分研究的问题。

Method: 采用分层多阶段MoME架构，结合轻量级专家模型和任务特定门控模块。

Result: 在PsyMo基准测试中表现优异，加权F1分数达37.47%（运行级）和44.6%（个体级）。

Conclusion: 多任务步态学习可用于心理特征估计，为未来研究奠定基础。

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [126] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: ConceptSplit是一个新框架，通过训练和推理分离多个概念，解决文本到图像扩散模型中的概念混合问题。


<details>
  <summary>Details</summary>
Motivation: 多概念个性化在文本到图像扩散模型中存在概念混合问题，即多个学习概念在输出图像中干扰或混合。

Method: 提出Token-wise Value Adaptation (ToVA)和Latent Optimization for Disentangled Attention (LODA)两个关键组件，分别用于训练和推理阶段。

Result: 实验表明，ConceptSplit能有效减少概念干扰，实现稳健的多概念个性化。

Conclusion: ConceptSplit通过ToVA和LODA解决了概念混合问题，提升了多概念个性化的效果。

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [127] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出一种标签高效的肝脏分割方法，适用于多模态、多厂商MRI数据，无需空间配准，结合基础模型微调和协同训练。


<details>
  <summary>Details</summary>
Motivation: 多相位MRI中肝脏分割对肝纤维化评估至关重要，但标注数据稀缺且分布不均，且存在模态差异和空间错位问题。

Method: 结合基础3D分割模型微调、交叉伪监督协同训练利用未标注数据，以及标准化预处理流程。

Result: 模型在标注和未标注数据上均表现出鲁棒的分割性能，能跨模态和厂商泛化。

Conclusion: 该方法展示了标签高效分割在多相位、多厂商MRI中的潜力，基础模型与协同训练结合适用于临床任务。

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [128] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的框架，用于在保持身份一致性的同时实现精细的表情控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持面部身份一致性和实现精细表情控制方面存在挑战。

Method: 采用ID一致的面部基础模型，结合表情交叉注意力模块和FLAME blendshape参数进行显式控制。

Result: 模型在身份一致的表情生成上优于现有方法，支持从基本情绪到微表情的广泛表达。

Conclusion: 该框架在AI驱动的叙事中实现了身份一致性和精细表情控制的结合。

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [129] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: ReactDiff是一个新的时间扩散框架，用于生成多样且符合对话上下文的面部反应，解决了现有方法无法模拟真实人类反应的随机性和动态性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效模拟真实人类面部反应的随机性和动态性，导致生成的反应不够自然和多样。

Method: ReactDiff通过结合时空面部运动学和面部动作单元依赖关系两个关键先验，指导扩散过程生成平滑、连贯且符合人类面部解剖学约束的反应。

Result: 在REACT2024数据集上的实验表明，ReactDiff在反应质量、多样性和反应适当性方面均达到最先进水平。

Conclusion: ReactDiff通过引入时空约束，显著提升了生成面部反应的多样性和自然性，为人类-计算机交互系统提供了更真实的反应生成方法。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [130] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 论文提出了一种新的3D语义场景图预测方法，通过优化对象特征编码器和对比预训练策略，显著提升了对象分类和关系预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对象和关系特征的表示能力上存在不足，过度依赖图神经网络，导致判别能力不足。

Method: 设计了一种高判别性的对象特征编码器，并采用对比预训练策略，将对象表示学习与场景图预测解耦。

Result: 在3DSSG数据集上显著优于现有方法，所有评估指标均有提升。

Conclusion: 优化对象特征表示对提升场景图预测性能至关重要，同时结合几何和语义特征可进一步改善关系预测。

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [131] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: 该论文提出了首个野生动物监测条件下的单目深度估计基准，评估了四种先进方法，发现Depth Anything V2表现最佳，并提供了实际应用建议。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏深度信息，从单目图像中提取准确距离在野生动物监测中具有挑战性，且现有方法在自然环境中的性能尚未系统评估。

Method: 评估了四种单目深度估计方法（Depth Anything V2、ML Depth Pro、ZoeDepth和Metric3D）及几何基线，使用93张带有校准ChARUCO图案的相机陷阱图像。

Result: Depth Anything V2表现最佳（平均绝对误差0.454m，相关性0.962），ZoeDepth在户外自然环境中表现较差（MAE: 3.087m）。中值深度提取优于均值方法。

Conclusion: 该基准为野生动物应用建立了性能基线，并为保护监测系统中的深度估计实施提供了实用指导。

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [132] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: ExposureEngine是一个端到端系统，用于体育转播中赞助商标识的旋转感知可见性分析，通过预测定向边界框（OBB）提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的手动分析方法主观且不可扩展，而现有自动化系统因依赖水平边界框（HBB）在标识旋转或倾斜时导致不准确的曝光度量。

Method: 提出ExposureEngine系统，使用OBB精确拟合标识，并开发了包含1,103帧的足球转播数据集进行训练和评估。

Result: 模型在mAP@0.5上达到0.859，精度0.96，召回率0.87，展示了在多样化转播条件下的鲁棒性能。

Conclusion: ExposureEngine提供了一套完整的解决方案，包括数据集和分析仪表板，支持自然语言查询生成报告和媒体内容。

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [133] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: AA-YOLO通过集成异常检测测试，有效降低红外小目标检测中的误报率，并在多种YOLO骨干网络中表现出色。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在复杂背景下误报率高，传统检测器效果不佳。

Method: 在YOLO检测头中集成统计异常检测测试，将小目标视为背景中的异常模式。

Result: 在多个基准测试中表现优异，且对训练数据不足、噪声和领域偏移具有鲁棒性。

Conclusion: AA-YOLO是一种通用且高效的解决方案，适用于资源受限的实际部署。

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [134] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: 论文研究了基于Transformer的架构在自然面对面对话场景中的人物识别性能，通过两流框架分别建模空间配置和时间运动模式，结果显示领域特定训练优于迁移学习，空间信息比时间动态更具区分性。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer架构在自然对话场景中的人物识别潜力，并比较空间配置与时间动态的贡献。

Method: 采用两流框架分别处理空间配置和时间运动模式，使用133个COCO WholeBody关键点，比较预训练与从头训练，引入多尺度时间Transformer。

Result: 空间Transformer准确率95.74%，多尺度时间Transformer93.90%，特征融合后达98.03%，显示空间与动态信息互补。

Conclusion: Transformer架构在自然交互中的人物识别具有潜力，为未来多模态和跨文化研究提供参考。

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [135] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ是一种创新的渐进式高斯变换框架，用于开放词汇的3D占用预测，通过渐进式在线密集化和各向异性感知采样策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法局限于固定语义类别，而现有方法在稀疏和密集表示之间存在权衡，PG-Occ旨在解决这些限制。

Method: 采用渐进式在线密集化策略和基于各向异性感知的时空融合采样策略，逐步增强3D高斯表示。

Result: PG-Occ在性能上相对提升了14.3%的mIoU，达到最先进水平。

Conclusion: PG-Occ通过渐进式优化和自适应采样，实现了更精细的场景理解和更高的预测精度。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [136] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 论文提出了一种通过生成未见类数据来估计开放环境中数据分布的新方法，并通过理论证明和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用已见类数据估计开放环境中的分布，但未见类的缺失导致估计误差无法识别。因此，需要通过学习未见类数据来约束估计误差。

Method: 方法包括一个类域级数据生成流程和分布对齐算法。生成流程通过分层语义树和域信息生成未见类数据，对齐算法通过最大化后验概率增强泛化能力。

Result: 在11个数据集上的实验表明，该方法比基线方法性能提升高达14%。

Conclusion: 通过生成未见类数据，论文提出的方法有效估计了开放环境中的分布，显著提升了开放词汇学习的性能。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [137] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战赛旨在评估联邦学习在手术视频分类中的表现，重点关注泛化性和本地适应能力。


<details>
  <summary>Details</summary>
Motivation: 评估当前方法在未见临床中心的泛化能力，以及通过本地微调实现协作模型开发的能力，同时保护患者数据隐私。

Method: 参与者使用Appendix300数据集开发策略，任务包括泛化和本地适应。方法包括基础模型线性探测、度量学习和多种FL聚合方案。

Result: 泛化任务表现有限，适应任务通过微调有所提升。ViViT模型表现最佳，但存在泛化不足和超参数调优困难等问题。

Conclusion: 挑战赛为手术视频分类中的FL方法提供了首个基准，强调了本地个性化与全局鲁棒性的权衡，以及架构选择和预处理的重要性。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [138] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: 提出了一种自动化双机器人3D扫描系统，用于文化遗产保护，显著提高了扫描效率和几何精度。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法需要专业知识和手动干预，难以保持最佳扫描条件和覆盖范围。

Method: 通过协调两个机器人的运动规划（一个携带扫描仪，一个处理托盘），优化轨迹规划和路径点分布，实现全面表面覆盖。

Result: 实验结果显示，该方法在Chamfer Distance和F-score上优于基线方法，几何精度更高，数字化效率提升。

Conclusion: 该系统减少了专家操作依赖，为文化遗产保护提供了高效、自动化的解决方案。

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [139] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: ViTs和CNNs在几何估计任务中的表现对比，ViTs在大数据下表现更好，CNNs在小数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨ViTs和CNNs在低数据量下对图像变形任务的效率，尤其是在几何估计任务中的应用。

Method: 系统比较了多种CNNs（如ResNet、EfficientNet）和ViTs（如CLIP-ViT、DINO）在不同数据量下的表现。

Result: ViTs在大数据下表现优于CNNs，而CNNs在小数据下表现更优；ViTs在跨域评估中泛化能力更强。

Conclusion: 选择模型架构需谨慎，未来研究应关注平衡局部和全局特征的混合架构。

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [140] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DiT-VTON是一个基于扩散变换器的新型虚拟试穿框架，通过改进图像条件和数据扩展，实现了细节保留和跨品类适应性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术在细节保留、鲁棒性和跨品类适应性方面存在不足，需要更高效的解决方案。

Method: 采用扩散变换器（DiT），探索多种图像条件配置（如上下文标记连接、通道连接和ControlNet集成），并扩展数据集以增强鲁棒性。

Result: 在VITON-HD数据集上超越现有方法，细节保留和鲁棒性更优，且支持广泛的品类和图像编辑功能。

Conclusion: DiT-VTON不仅提升了虚拟试穿性能，还扩展了其应用范围，成为通用的虚拟试穿解决方案。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [141] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg框架通过固定摄像头视频重建手术室中任意人员的动态自我中心视角，提升手术安全、培训和流程优化的分析能力。


<details>
  <summary>Details</summary>
Motivation: 传统手术观察方法依赖固定视角或回忆，无法记录临床决策中的自我中心视觉视角，限制了手术安全、培训和流程优化的深入分析。

Method: EgoSurg结合几何驱动的神经渲染和基于扩散的视角增强技术，从固定摄像头视频中高保真合成任意自我中心视角。

Result: 在多地点手术案例和对照研究中，EgoSurg能够高质量重建特定人员的视觉领域和任意视角。

Conclusion: EgoSurg将现有手术室摄像头基础设施转化为可导航的动态3D记录，为沉浸式手术数据科学奠定新基础。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [142] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 研究探讨了多模态语言模型（MLMs）在感知任务中的表现不佳原因，发现视觉键值令牌的信息流动是关键，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 理解MLMs在感知任务中表现不佳的原因，探索视觉信息在语言模型中的流动与处理方式。

Method: 通过分析视觉键值令牌的信息流动，比较不同MLMs（如LLaVA-OneVision、Qwen2.5-VL等）的表现，并实验控制视觉信息的方法。

Result: 发现视觉值令牌包含足够信息支持零样本感知任务，但语言模型对视觉信息的处理存在不足，改进控制可显著提升性能。

Conclusion: 视觉键值令牌在MLMs中起关键作用，改进其信息控制机制可提升模型感知能力，为未来研究提供新方向。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [143] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON是首个4D虚拟试穿框架，支持单张商品图生成动态试穿效果，无需多视角或物理先验。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖多视角或物理先验的问题，实现动态服装交互。

Method: 包含两个模块：Reciprocal Flow Rectifier（光流校正）和Non-Linear Deformer（非线性变形）。

Result: 实验显示高保真、多样性和动态服装真实感，适用于AR/VR和游戏。

Conclusion: AvatarVTON为4D虚拟试穿提供了高效解决方案。

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [144] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D Flow Matching框架的合成CT生成方法，用于MRI或CBCT数据，旨在提高放疗精度并减少患者辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 通过生成合成CT（sCT）来支持MRI-only和CBCT-based自适应放疗，提升治疗精度并降低患者辐射。

Method: 采用完全3D Flow Matching框架，将高斯噪声体积通过学习的FM速度场转换为sCT图像，输入为MRI或CBCT的轻量级3D编码器特征。

Result: 在SynthRAD2025挑战基准测试中，方法能准确重建全局解剖结构，但受限于训练分辨率，保留细节能力不足。

Conclusion: 未来工作将探索基于补丁的训练和潜在空间流模型，以提高分辨率和局部结构保真度。

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [145] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: AT-BPTT是一种动态调整截断位置和窗口大小的新框架，显著提升了数据集蒸馏的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法依赖随机截断策略，缺乏灵活性且效果不佳。

Method: 提出AT-BPTT框架，包含阶段感知时间步选择、自适应窗口大小策略和低秩Hessian近似。

Result: 在多个数据集上平均准确率提升6.16%，优化速度提升3.9倍，内存节省63%。

Conclusion: AT-BPTT在性能和效率上均优于现有方法，为数据集蒸馏提供了更优解决方案。

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [146] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: 提出了一种基于航拍图像的PV电站自动化建模方法，无需依赖第三方数据。


<details>
  <summary>Details</summary>
Motivation: PV电站的准确模型对其运维至关重要，但现有方法依赖第三方数据且难以获取。

Method: 通过视觉分割和结构信息推断，从航拍图像中识别PV模块并构建详细模型。

Result: 在两个电站上验证了方法的有效性，生成了适用于维护的紧凑地理参考模型。

Conclusion: 该方法实现了PV电站的自动化建模，提升了运维效率。

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [147] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: 提出了一种基于3D骨架数据的运动识别框架，用于推断人类活动的心理状态，结合ST-GCN和CNN，实现了隐私保护且可扩展的行为建模。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法（如问卷或理论模型）在捕捉人类心理状态时的局限性，提供一种通用且隐私保护的方法。

Method: 结合空间-时间图卷积网络（ST-GCN）和卷积神经网络（CNN），利用迁移学习避免手动定义动作与心理类别的映射。

Result: 在DUET数据集上验证了方法的可扩展性和准确性，能够揭示反映认知和情绪状态的潜在运动结构。

Conclusion: 该方法为增强RL驱动的人类-环境交互模拟提供了新途径，具有隐私保护和通用性优势。

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [148] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: 论文探讨了如何通过深度传感器识别二元人类互动，以隐私保护的方式测量社会行为，并比较了五种骨架识别算法。


<details>
  <summary>Details</summary>
Motivation: 现有网络物理系统（CPS）常忽视社会效益，网络物理社会基础设施系统（CPSIS）旨在通过识别人类互动来填补这一空白。

Method: 使用深度传感器捕捉骨架运动，比较五种骨架识别算法在12种二元互动数据集上的表现。

Result: 研究提供了骨架识别算法在二元互动中的性能比较，为隐私保护的社会行为测量奠定基础。

Conclusion: 深度传感器是隐私友好的社会行为测量工具，骨架识别算法为理解人类互动的文化和情感层面提供了新视角。

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [149] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 论文提出了一种结合早期退出和知识蒸馏的方法，通过新的熵损失优化计算复杂度与准确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络计算成本高，难以应用于实时和边缘场景，需压缩技术降低成本同时保持准确性。

Method: 整合早期退出和知识蒸馏，提出新的熵损失函数，训练学生模型。

Result: 在CIFAR10、CIFAR100和SVHN数据集上验证了方法的有效性，显著降低计算复杂度且不损失性能。

Conclusion: 该方法为知识蒸馏在其他场景的应用提供了新思路。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [150] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的图像质量评估方法μDeepIQA，用于光学显微镜图像，具有快速、稳定和通用性强的特点。


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估方法在处理大规模数据集时计算成本高，且对超出理想范围的图像不稳定。

Method: 采用深度卷积神经网络，重新训练以预测光学显微镜图像的个体质量指标和全局质量分数。

Result: μDeepIQA能快速稳定地预测图像质量，支持局部质量评估，并在异常情况下表现稳定。

Conclusion: 深度学习模型在光学显微镜图像质量评估中表现出色，适用于实际研究需求。

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [151] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 论文介绍了一种端到端的物联网机器人系统，用于葡萄园中葡萄产量和质量的非破坏性、实时、空间分辨的测绘。


<details>
  <summary>Details</summary>
Motivation: 解决葡萄园中葡萄产量和质量实时测绘的需求，特别是克服光照变化对高光谱成像（HSI）数据质量评估的影响。

Method: 系统整合了两个关键模块：葡萄串检测和重量估计的高性能模型，以及基于Light-Invariant Spectral Autoencoder（LISA）的质量评估框架。LISA通过对抗学习从非校准数据中提取光照不变特征。

Result: 系统在葡萄串检测的召回率为0.82，重量预测的R²为0.76；LISA模块将质量预测的泛化能力提高了20%以上。

Conclusion: 该系统成功生成了高分辨率的地理参考数据，为精准葡萄栽培提供了可操作的数据驱动见解。

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [152] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: 论文介绍了一个多模态数据集，用于海底栖息地测绘，包含侧扫声纳、测深图和光学图像，并提供了标注工具和预处理工具。


<details>
  <summary>Details</summary>
Motivation: 解决海底栖息地测绘领域缺乏大规模标注数据集的问题，以支持机器学习和多传感器数据融合的研究。

Method: 收集并标注了约100万张侧扫声纳图像，结合测深图和光学图像，提供多模态数据集和预处理工具。

Result: 数据集包含约36000张标注的侧扫声纳图像，支持监督学习和自监督学习。

Conclusion: 该资源旨在为海底栖息地测绘提供标准化基准，推动自主海底分类和多传感器集成的研究。

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [153] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 比较四种目标检测模型在卢旺达基加利摩托车检测中的表现，推荐简化架构以适应资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 摩托车在基加利是主要交通工具，但交通行为不可预测，对自动驾驶系统构成挑战。

Method: 使用YOLOv5、Faster R-CNN、SSD和RetinaNet模型，基于198张图像的自定义数据集进行检测。

Result: 评估了模型的准确性、定位能力和推理速度，发现数据集和模型复杂性是主要挑战。

Conclusion: 建议简化架构以提高自动驾驶系统在发展中国家（如卢旺达）的适用性。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [154] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种名为SAHC的新方法，通过整合层次特定的分类头来学习层次特征和关系，用于遥感图像分类。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法在遥感图像分类中常忽略预定义的标签层次结构，仅关注细粒度分类。

Method: 采用可训练的层次矩阵和层次共识机制，确保不同层次间概率分布一致。

Result: 在三个基准数据集上验证了方法的有效性和鲁棒性。

Conclusion: SAHC方法能有效引导网络学习，并在遥感图像分类任务中表现出色。

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [155] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: REN是一种针对医学图像分类的解剖学指导的MoE框架，通过区域专家网络和多模态门控机制，显著提升了ILD分类的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE系统缺乏医学影像所需的解剖学约束，而REN通过结合解剖学先验知识，解决了区域疾病异质性问题。

Method: REN训练七个针对不同肺叶和双侧肺组合的专家网络，利用多模态门控机制动态整合放射组学和深度学习特征。

Result: REN在ILD分类中表现优异，平均AUC为0.8646，比基线模型提升12.5%，且区域专家模型AUC达0.88-0.90。

Conclusion: REN展示了强大的泛化能力和临床可解释性，适用于其他结构化医学影像应用。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [156] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: NFPF是一种无监督主动学习方法，通过SFLM量化样本重要性，显著优于现有UAL方法，性能接近监督AL方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有无监督主动学习方法性能不足的问题，减少人工标注负担。

Method: 提出NFPF框架，利用SFLM量化样本贡献，定义Reconstruction Difference指标进行样本选择。

Result: 在视觉数据集上显著优于现有UAL方法，性能接近监督AL方法。

Conclusion: NFPF在性能、鲁棒性和数据分布覆盖方面表现优异，为无监督主动学习提供了新思路。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [157] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: CA3D-Diff是一种基于条件扩散模型的双向乳腺X光视图转换框架，通过列感知交叉注意力和隐式3D结构重建模块，解决了乳腺X光视图转换中的结构错位问题。


<details>
  <summary>Details</summary>
Motivation: 在乳腺X光检查中，由于采集错误或压缩伪影，可能导致一个视图缺失或损坏，影响诊断效果。视图转换技术可以恢复缺失视图并改善病变对齐。

Method: 提出列感知交叉注意力机制和隐式3D结构重建模块，利用几何特性和投影几何，增强跨视图生成的结构一致性。

Result: CA3D-Diff在双向任务中表现优异，视觉保真度和结构一致性优于现有方法，并能提升单视图恶性分类的准确性。

Conclusion: CA3D-Diff在乳腺X光视图转换中具有显著优势，能够提升实际诊断中的效果。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [158] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: SSDD是一种新型的像素扩散解码器架构，通过蒸馏技术实现高效单步重建，无需对抗性损失，性能优于KL-VAE。


<details>
  <summary>Details</summary>
Motivation: 当前基于KL-VAE的tokenizer需要对抗性损失且解码时间较长，扩散解码器虽更理论化但仍存在性能匹配问题。

Method: 提出SSDD架构，结合transformer组件和无GAN训练，通过蒸馏技术实现单步解码。

Result: SSDD在重建质量（FID从0.87降至0.50）和采样速度（1.4倍吞吐量）上优于KL-VAE。

Conclusion: SSDD可作为KL-VAE的直接替代，用于构建更高质量和更快的生成模型。

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [159] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 提出了一种通过微调视觉基础模型（VFM）和嵌入数字水印的方法来验证模型所有权的方法，确保水印在模型副本中仍可检测。


<details>
  <summary>Details</summary>
Motivation: 保护视觉基础模型的知识产权，防止非法分发。

Method: 微调VFM的少量表达层和小型编码器-解码器网络，将数字水印嵌入输入图像的内部表示中。

Result: 理论及实验证明，该方法在非水印模型的误检和水印模型的漏检概率上表现良好。

Conclusion: 该方法能有效验证模型所有权，适用于保护视觉基础模型的知识产权。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [160] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: 提出了一种新的潜在不确定性表示（LUR）和排斥训练LUR（RLUR）方法，用于提高深度神经网络在资源受限环境中的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有最后一层概率深度学习方法（LL-PDL）在OOD检测中性能不稳定的问题。

Method: 通过扩展预训练DNN并添加变换层生成多个潜在表示来估计不确定性。

Result: LUR和RLUR在分类性能和OOD检测上表现优异，且训练更高效。

Conclusion: LUR和RLUR是高效且易于调优的OOD检测方法，适用于资源受限环境。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [161] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的帕金森病早期诊断方法，通过手绘螺旋和波浪图像作为生物标志物，结合CNN、迁移学习和注意力机制，取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难且传统方法复杂昂贵，需要一种非侵入性、低成本的新方法。

Method: 使用CNN、迁移学习和注意力机制，通过数据增强和集成投票提升模型性能。

Result: 螺旋图像准确率90%，波浪图像96.67%，集成后整体准确率93.3%。

Conclusion: 机器学习方法为帕金森病早期诊断提供了高效、经济的解决方案。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [162] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: 本文综述了视频大型多模态模型（Video-LMMs）的后训练方法，包括监督微调、强化学习和测试时扩展，并提出了分类法和评估协议。


<details>
  <summary>Details</summary>
Motivation: 视频理解是计算机视觉中最具挑战性的领域，需要模型处理复杂的时空关系和长时依赖。尽管Video-LMMs表现出色，但其后训练阶段的研究仍分散且不系统。

Method: 本文系统分析了三种后训练方法：监督微调（SFT）结合思维链、基于可验证目标的强化学习（RL）和通过增强推理计算的测试时扩展（TTS）。

Result: 提出了一个结构化分类法，明确了这些技术的角色、相互联系和视频特定适应，并总结了关键设计原则和评估协议。

Conclusion: 本文为研究人员和从业者提供了一个统一的框架，以推动Video-LMMs能力的进一步发展，并指出了奖励设计、可扩展性和成本性能优化等开放挑战。

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [163] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: 提出了一种利用3D基础模型的空间理解能力来解决宽基线分割匹配问题的新架构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 分割匹配在计算机视觉中是一个重要任务，能够捕获结构化区域，比关键点匹配更具鲁棒性。然而，宽基线分割匹配（涉及极端视角变化）仍是一个挑战。

Method: 利用3D基础模型的归纳偏置，设计了一种新架构，用于匹配视角变化高达180度的图像对中的分割区域。

Result: 在ScanNet++和Replica数据集上，该方法在AUPRC指标上比现有方法（如SAM2和局部特征匹配方法）提升了30%。

Conclusion: 该方法不仅提升了分割匹配性能，还在3D实例分割和图像目标导航等下游任务中表现出优势。

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [164] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 提出了一种针对对比度失真图像的无参考图像质量评估（NR-IQA）方法，通过生成伪参考图像将NR问题转化为全参考（FR）评估，提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 对比度变化是影响图像质量的重要因素，但现有方法多关注模糊和噪声等失真，对比度失真被忽视。

Method: 利用对比度增强算法生成伪参考图像，训练分类网络选择最佳算法，最终以FR方式评估质量差异。

Result: 在CCID2014、TID2013和CSIQ三个数据库上的性能评估表明，该方法表现优异。

Conclusion: 该方法有效解决了对比度失真图像的质量评估问题，具有实际应用潜力。

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [165] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 提出了一种名为Neuroplastic Modular Classifier的新型混合架构，结合ResNet-50和Vision Transformer，用于动态环境中的图像分类，并通过FAISS相似性检索增强特征空间。


<details>
  <summary>Details</summary>
Motivation: 高效准确的废物和工业表面缺陷分类对可持续废物管理和质量控制至关重要。

Method: 结合ResNet-50和ViT进行特征提取，引入FAISS相似性检索，并采用可扩展的模块化设计动态调整模型复杂度。

Result: 在KolektorSDD2数据集上验证，模型在准确性和适应性上优于传统静态模型。

Conclusion: 该模型为现实世界图像分类提供了可扩展的高性能解决方案，适用于环境和工业领域。

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [166] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 论文提出了一种针对结构化视觉（如图表、数学图形）生成和编辑的综合方法，包括数据集构建、模型训练和评估基准。


<details>
  <summary>Details</summary>
Motivation: 现代视觉生成模型在自然图像上表现优异，但在结构化视觉生成和编辑方面存在困难，需要解决组合规划、文本渲染和多模态推理等问题。

Method: 构建了130万高质量结构化图像对的数据集，训练了一个结合VLM和FLUX.1 Kontext的统一模型，并通过三阶段训练课程提升性能。

Result: 提出的模型在结构化视觉编辑上表现优异，推理时间推理在不同架构中均带来一致提升。评估显示现有领先模型仍不理想。

Conclusion: 通过发布数据集、模型和基准测试，论文旨在推动结构化视觉的统一多模态基础研究。

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [167] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 提出了一种框架，通过跨角色嵌入和增强技术，实现不同世界角色的自然互动，同时保持风格一致性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不同风格的视频中生成角色自然互动的视频，解决角色身份和行为逻辑的保留问题。

Method: 采用跨角色嵌入（CCE）学习身份和行为逻辑，跨角色增强（CCA）生成合成数据，提升训练效果。

Result: 在10个角色的实验中，身份保留、互动质量和风格一致性均有显著提升。

Conclusion: 该框架为生成式故事讲述提供了新可能，支持跨风格角色的自然互动。

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [168] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain通过多模态模型生成关键帧，指导视频生成器在关键时间点进行稀疏调优，显著提升视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以处理复杂动态和连贯因果链，而多模态模型具备强大的视觉状态推理能力，VChain旨在结合两者优势。

Method: 利用多模态模型生成稀疏关键帧，指导预训练视频生成器在关键时间点进行稀疏调优，避免密集监督。

Result: 实验表明，VChain在复杂多步骤场景中显著提升生成视频质量。

Conclusion: VChain通过稀疏调优和多模态模型结合，有效解决了视频生成中的动态连贯性问题。

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


### [169] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker是一个多智能体框架，用于自动生成学术演示视频，解决了传统制作视频的高耗时问题。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频制作耗时且复杂，涉及多模态信息协调，需要自动化解决方案。

Method: 提出PaperTalker框架，整合幻灯片生成、布局优化、字幕、语音合成和头像渲染，并行处理以提高效率。

Result: 实验表明，生成的视频比现有基线更忠实和内容丰富，实现了自动化学术视频生成的实用步骤。

Conclusion: PaperTalker为学术视频生成提供了高效、自动化的解决方案，数据集和代码已开源。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [170] [Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer](https://arxiv.org/abs/2510.03342)
*Abbas Abdolmaleki,Saminda Abeyruwan,Joshua Ainslie,Jean-Baptiste Alayrac,Montserrat Gonzalez Arenas,Ashwin Balakrishna,Nathan Batchelor,Alex Bewley,Jeff Bingham,Michael Bloesch,Konstantinos Bousmalis,Philemon Brakel,Anthony Brohan,Thomas Buschmann,Arunkumar Byravan,Serkan Cabi,Ken Caluwaerts,Federico Casarini,Christine Chan,Oscar Chang,London Chappellet-Volpini,Jose Enrique Chen,Xi Chen,Hao-Tien Lewis Chiang,Krzysztof Choromanski,Adrian Collister,David B. D'Ambrosio,Sudeep Dasari,Todor Davchev,Meet Kirankumar Dave,Coline Devin,Norman Di Palo,Tianli Ding,Carl Doersch,Adil Dostmohamed,Yilun Du,Debidatta Dwibedi,Sathish Thoppay Egambaram,Michael Elabd,Tom Erez,Xiaolin Fang,Claudio Fantacci,Cody Fong,Erik Frey,Chuyuan Fu,Ruiqi Gao,Marissa Giustina,Keerthana Gopalakrishnan,Laura Graesser,Oliver Groth,Agrim Gupta,Roland Hafner,Steven Hansen,Leonard Hasenclever,Sam Haves,Nicolas Heess,Brandon Hernaez,Alex Hofer,Jasmine Hsu,Lu Huang,Sandy H. Huang,Atil Iscen,Mithun George Jacob,Deepali Jain,Sally Jesmonth,Abhishek Jindal,Ryan Julian,Dmitry Kalashnikov,M. Emre Karagozler,Stefani Karp,Matija Kecman,J. Chase Kew,Donnie Kim,Frank Kim,Junkyung Kim,Thomas Kipf,Sean Kirmani,Ksenia Konyushkova,Li Yang Ku,Yuheng Kuang,Thomas Lampe,Antoine Laurens,Tuan Anh Le,Isabel Leal,Alex X. Lee,Tsang-Wei Edward Lee,Guy Lever,Jacky Liang,Li-Heng Lin,Fangchen Liu,Shangbang Long,Caden Lu,Sharath Maddineni,Anirudha Majumdar,Kevis-Kokitsi Maninis,Andrew Marmon,Sergio Martinez,Assaf Hurwitz Michaely,Niko Milonopoulos,Joss Moore,Robert Moreno,Michael Neunert,Francesco Nori,Joy Ortiz,Kenneth Oslund,Carolina Parada,Emilio Parisotto,Amaris Paryag,Acorn Pooley,Thomas Power,Alessio Quaglino,Haroon Qureshi,Rajkumar Vasudeva Raju,Helen Ran,Dushyant Rao,Kanishka Rao,Isaac Reid,David Rendleman,Krista Reymann,Miguel Rivas,Francesco Romano,Yulia Rubanova,Peter Pastor Sampedro,Pannag R Sanketi,Dhruv Shah,Mohit Sharma,Kathryn Shea,Mohit Shridhar,Charles Shu,Vikas Sindhwani,Sumeet Singh,Radu Soricut,Rachel Sterneck,Ian Storz,Razvan Surdulescu,Jie Tan,Jonathan Tompson,Saran Tunyasuvunakool,Jake Varley,Grace Vesom,Giulia Vezzani,Maria Bauza Villalonga,Oriol Vinyals,René Wagner,Ayzaan Wahid,Stefan Welker,Paul Wohlhart,Chengda Wu,Markus Wulfmeier,Fei Xia,Ted Xiao,Annie Xie,Jinyu Xie,Peng Xu,Sichun Xu,Ying Xu,Zhuo Xu,Jimmy Yan,Sherry Yang,Skye Yang,Yuxiang Yang,Hiu Hong Yu,Wenhao Yu,Wentao Yuan,Yuan Yuan,Jingwei Zhang,Tingnan Zhang,Zhiyuan Zhang,Allan Zhou,Guangyao Zhou,Yuxiang Zhou*

Main category: cs.RO

TL;DR: Gemini Robotics 1.5和Gemini Robotics-ER 1.5是新一代多模态机器人模型，通过创新架构和推理能力提升机器人执行复杂任务的能力。


<details>
  <summary>Details</summary>
Motivation: 通用机器人需要深度理解物理世界、高级推理和灵活控制，因此开发了更通用的Vision-Language-Action模型和Embodied Reasoning模型。

Method: 采用Motion Transfer机制学习异构机器人数据，结合多级自然语言推理，实现“先思考后行动”。

Result: 模型在视觉空间理解、任务规划和进度估计等方面达到新水平，提升了复杂任务的分解和执行能力。

Conclusion: 这些模型推动了物理代理时代的发展，使机器人能够感知、思考并解决多步骤复杂任务。

Abstract: General-purpose robots need a deep understanding of the physical world,
advanced reasoning, and general and dexterous control. This report introduces
the latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,
a multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER
1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together
three major innovations. First, Gemini Robotics 1.5 features a novel
architecture and a Motion Transfer (MT) mechanism, which enables it to learn
from heterogeneous, multi-embodiment robot data and makes the VLA more general.
Second, Gemini Robotics 1.5 interleaves actions with a multi-level internal
reasoning process in natural language. This enables the robot to "think before
acting" and notably improves its ability to decompose and execute complex,
multi-step tasks, and also makes the robot's behavior more interpretable to the
user. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for
embodied reasoning, i.e., for reasoning capabilities that are critical for
robots, such as visual and spatial understanding, task planning, and progress
estimation. Together, this family of models takes us a step towards an era of
physical agents-enabling robots to perceive, think and then act so they can
solve complex multi-step tasks.

</details>


### [171] [Optimal swimming with body compliance in an overdamped medium](https://arxiv.org/abs/2510.03457)
*Jianfeng Lin,Tianyu Wang,Baxi Chong,Matthew Fernandez,Zhaochen Xu,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 论文提出了一种基于几何力学的框架，用于预测和优化柔性波动体的运动性能，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设精确执行预设步态，而实际中环境与柔性体的相互作用会干扰运动轨迹，因此需要扩展几何力学以处理柔性波动体的运动。

Method: 引入柔性扩展的Purcell三连杆游泳模型，结合阻力理论推导体动力学，并将几何力学融入运动预测和优化框架中。

Result: 在物理机器人上验证了框架的准确性，展示了在颗粒介质中优化运动性能的能力。

Conclusion: 研究为建模和控制柔性游泳运动提供了系统化的物理方法，强调了柔性作为设计特征的重要性。

Abstract: Elongate animals and robots use undulatory body waves to locomote through
diverse environments. Geometric mechanics provides a framework to model and
optimize such systems in highly damped environments, connecting a prescribed
shape change pattern (gait) with locomotion displacement. However, existing
approaches assume precise execution of prescribed gaits, whereas in practice
environmental interactions with compliant bodies of animals or robots
frequently perturb the realized trajectories. In this work, we extend geometric
mechanics to predict locomotor performance and search for optimal swimming
strategy of compliant undulators. We introduce a compliant extension of
Purcell's three-link swimmer by incorporating series-connected springs at the
joints. Body dynamics are derived with resistive force theory. Geometric
mechanics is incorporated into movement prediction and into an optimization
framework that identifies strategies for controlling compliant swimmers to
achieve maximal displacement. We validate our framework on a physical
cable-driven three-link limbless robot, and demonstrate accurate prediction and
optimization of locomotor performance under varied programmed, state-dependent
compliance in a granular medium. Our results establish a systematic
physics-based approach for modeling and controlling compliant swimming
locomotion, highlighting compliance as a design feature that can be exploited
for robust movement in homogeneous and heterogeneous environments.

</details>


### [172] [Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching](https://arxiv.org/abs/2510.03460)
*Sibo Tian,Minghui Zheng,Xiao Liang*

Main category: cs.RO

TL;DR: 提出了一种基于学习的方法，利用Flow Matching模型从单视角点云生成优化初始解，显著提高了轨迹优化的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 在HRC系统中，机器人需要实时响应动态环境，但现有采样和优化方法存在效率低或易陷入局部最优的问题。

Method: 使用Flow Matching模型，从单视角深度相机输入生成可行轨迹，无需环境先验知识。

Result: 在UR5e机械臂的仿真实验中，该方法成功率高，优化迭代次数少，且能泛化到新环境。

Conclusion: 该方法为机器人运动规划提供了一种高效且泛化能力强的解决方案。

Abstract: Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)
systems, as robots need to respond to dynamic environments in real time by
continuously observing their surroundings and replanning their motions to
ensure both safe interactions and efficient task execution. Current
sampling-based motion planners face challenges in scaling to high-dimensional
configuration spaces and often require post-processing to interpolate and
smooth the generated paths, resulting in time inefficiency in complex
environments. Optimization-based planners, on the other hand, can incorporate
multiple constraints and generate smooth trajectories directly, making them
potentially more time-efficient. However, optimization-based planners are
sensitive to initialization and may get stuck in local minima. In this work, we
present a novel learning-based method that utilizes a Flow Matching model
conditioned on a single-view point cloud to learn near-optimal solutions for
optimization initialization. Our method does not require prior knowledge of the
environment, such as obstacle locations and geometries, and can generate
feasible trajectories directly from single-view depth camera input. Simulation
studies on a UR5e robotic manipulator in cluttered workspaces demonstrate that
the proposed generative initializer achieves a high success rate on its own,
significantly improves the success rate of trajectory optimization compared
with traditional and learning-based benchmark initializers, requires fewer
optimization iterations, and exhibits strong generalization to unseen
environments.

</details>


### [173] [A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control](https://arxiv.org/abs/2510.03471)
*Dingqi Zhang,Ran Tao,Sheng Cheng,Naira Hovakimyan,Mark W. Mueller*

Main category: cs.RO

TL;DR: 本文介绍了一个基于RotorPy的模块化仿真测试平台，用于系统评估四旋翼飞行器的自适应控制方法，支持多种干扰场景和任务指标。


<details>
  <summary>Details</summary>
Motivation: 四旋翼飞行器的自适应控制在外部干扰和模型不确定性下至关重要，但现有评估方法分散且难以系统比较。

Method: 开发了一个易于部署的模块化仿真测试平台，包含多种干扰模型、控制器库和任务相关指标。

Result: 平台支持多种干扰场景（如风、负载变化、转子故障等），并提供了统一的评估环境。

Conclusion: 该测试平台为系统分析四旋翼控制方法提供了实用工具，代码已开源。

Abstract: Robust adaptive control methods are essential for maintaining quadcopter
performance under external disturbances and model uncertainties. However,
fragmented evaluations across tasks, simulators, and implementations hinder
systematic comparison of these methods. This paper introduces an
easy-to-deploy, modular simulation testbed for quadcopter control, built on
RotorPy, that enables evaluation under a wide range of disturbances such as
wind, payload shifts, rotor faults, and control latency. The framework includes
a library of representative adaptive and non-adaptive controllers and provides
task-relevant metrics to assess tracking accuracy and robustness. The unified
modular environment enables reproducible evaluation across control methods and
eliminates redundant reimplementation of components such as disturbance models,
trajectory generators, and analysis tools. We illustrate the testbed's
versatility through examples spanning multiple disturbance scenarios and
trajectory types, including automated stress testing, to demonstrate its
utility for systematic analysis. Code is available at
https://github.com/Dz298/AdaptiveQuadBench.

</details>


### [174] [Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems](https://arxiv.org/abs/2510.03472)
*Yulun Zhang,Alexandre O. G. Barbosa,Federico Pecora,Jiaoyang Li*

Main category: cs.RO

TL;DR: 论文研究了如何优化机器人分拣系统中的目的地到滑槽的任务映射以提高吞吐量，提出了一种基于进化算法和混合整数线性规划的优化方法。


<details>
  <summary>Details</summary>
Motivation: 机器人分拣系统中，目的地到滑槽的任务映射直接影响吞吐量，但优化任务映射面临复杂性和多因素耦合的挑战。

Method: 使用进化算法和混合整数线性规划优化任务映射，并通过模拟器评估其性能。

Result: 优化的任务映射在多种分拣系统设置中表现优于贪婪生成的方法。

Conclusion: 通过质量多样性算法分析任务映射的多样性，证明了优化方法的有效性。

Abstract: We study optimizing a destination-to-chutes task mapping to improve
throughput in Robotic Sorting Systems (RSS), where a team of robots sort
packages on a sortation floor by transporting them from induct workstations to
eject chutes based on their shipping destinations (e.g. Los Angeles or
Pittsburgh). The destination-to-chutes task mapping is used to determine which
chutes a robot can drop its package. Finding a high-quality task mapping is
challenging because of the complexity of a real-world RSS. First, optimizing
task mapping is interdependent with robot target assignment and path planning.
Second, chutes will be CLOSED for a period of time once they receive sufficient
packages to allow for downstream processing. Third, task mapping quality
directly impacts the downstream processing, as scattered chutes for the same
destination increase package handling time. In this paper, we first formally
define task mappings and the problem of Task Mapping Optimization (TMO). We
then present a simulator of RSS to evaluate task mappings. We then present a
simple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear
Programming, demonstrating the advantage of our optimized task mappings over
the greedily generated ones in various RSS setups with different map sizes,
numbers of chutes, and destinations. Finally, we use Quality Diversity
algorithms to analyze the throughput of a diverse set of task mappings. Our
code is available online at https://github.com/lunjohnzhang/tmo_public.

</details>


### [175] [Robust Permissive Controller Synthesis for Interval MDPs](https://arxiv.org/abs/2510.03481)
*Khang Vo Huynh,David Parker,Lu Feng*

Main category: cs.RO

TL;DR: 提出了一种在不确定性动态（IMDPs）下为机器人合成鲁棒性许可控制器的框架，确保所有策略满足可达性或奖励规范。


<details>
  <summary>Details</summary>
Motivation: 传统控制器合成方法通常生成单一确定性策略，缺乏适应性；而许可控制器允许多种动作，但现有方法假设精确的转移概率，不适用于机器人应用中的不确定性。

Method: 将问题建模为混合整数线性规划（MILP），提出两种编码方法：基于顶点枚举的基准方法和可扩展的对偶方法。

Result: 实验表明，两种方法均能合成鲁棒且最大许可的控制器，并可扩展到具有数十万状态的大型IMDP。

Conclusion: 该框架首次实现了在IMDPs上的鲁棒许可控制器合成，为机器人提供了运行时灵活性和鲁棒性。

Abstract: We address the problem of robust permissive controller synthesis for robots
operating under uncertain dynamics, modeled as Interval Markov Decision
Processes (IMDPs). IMDPs generalize standard MDPs by allowing transition
probabilities to vary within intervals, capturing epistemic uncertainty from
sensing noise, actuation imprecision, and coarse system abstractions-common in
robotics. Traditional controller synthesis typically yields a single
deterministic strategy, limiting adaptability. In contrast, permissive
controllers (multi-strategies) allow multiple actions per state, enabling
runtime flexibility and resilience. However, prior work on permissive
controller synthesis generally assumes exact transition probabilities, which is
unrealistic in many robotic applications. We present the first framework for
robust permissive controller synthesis on IMDPs, guaranteeing that all
strategies compliant with the synthesized multi-strategy satisfy reachability
or reward-based specifications under all admissible transitions. We formulate
the problem as mixed-integer linear programs (MILPs) and propose two encodings:
a baseline vertex-enumeration method and a scalable duality-based method that
avoids explicit enumeration. Experiments on four benchmark domains show that
both methods synthesize robust, maximally permissive controllers and scale to
large IMDPs with up to hundreds of thousands of states.

</details>


### [176] [Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*](https://arxiv.org/abs/2510.03496)
*Vadivelan Murugesan,Rajasundaram Mathiazhagan,Sanjana Joshi,Aliasghar Arab*

Main category: cs.RO

TL;DR: 提出了一种基于预测的安全规划框架，结合数字孪生验证，实现了高精度的人机协作避障。


<details>
  <summary>Details</summary>
Motivation: 现有规划器仅依赖运动学模型，无法满足长时间预测需求，需结合预测模型提升避障效果。

Method: 使用CNN-BiLSTM预测关节轨迹，通过胶囊APF评估碰撞风险，触发A-RRT*规划器，数字孪生验证轨迹。

Result: 50次试验中实现100%避障，距离>250mm，重规划时间<2秒。

Conclusion: 结合预测模型与数字孪生验证，显著提升了避障精度和可靠性。

Abstract: Human-robot collaboration requires precise prediction of human motion over
extended horizons to enable proactive collision avoidance. Unlike existing
planners that rely solely on kinodynamic models, we present a prediction-driven
safe planning framework that leverages granular, joint-by-joint human motion
forecasting validated in a physics-based digital twin. A capsule-based
artificial potential field (APF) converts these granular predictions into
collision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when
thresholds are exceeded. The depth camera is used to extract 3D skeletal poses
and a convolutional neural network-bidirectional long short-term memory
(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A
digital twin model integrates real-time human posture prediction placed in
front of a simulated robot to evaluate motions and physical contacts. The
proposed method enables validation of planned trajectories ahead of time and
bridging potential latency gaps in updating planned trajectories in real-time.
In 50 trials, our method achieved 100% proactive avoidance with > 250 mm
clearance and sub-2 s replanning, demonstrating superior precision and
reliability compared to existing kinematic-only planners through the
integration of predictive human modeling with digital twin validation.

</details>


### [177] [Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning](https://arxiv.org/abs/2510.03504)
*Yutong Wang,Yichun Qu,Tengxiang Wang,Lishuo Pan,Nora Ayanian*

Main category: cs.RO

TL;DR: 提出了一种实时分布式框架，通过高阶控制屏障函数（HOCBFs）和Lyapunov函数，实现多机器人导航中的连接性维护与恢复。


<details>
  <summary>Details</summary>
Motivation: 多机器人应用中，连接性易受障碍物和视觉遮挡影响，需一种鲁棒的方法来维护和恢复连接性。

Method: 结合HOCBFs和控制Lyapunov函数，生成Bezier参数化轨迹，实现平滑的规划与控制。

Result: 通过仿真和4台Crazyflie纳米四旋翼的物理实验验证了框架的有效性。

Conclusion: 提出的MPC-CLF-CBF框架为多机器人系统的连接性维护与恢复提供了连续时间轨迹生成和控制方法。

Abstract: Maintaining connectivity is crucial in many multi-robot applications, yet
fragile to obstacles and visual occlusions. We present a real-time distributed
framework for multi-robot navigation certified by high-order control barrier
functions (HOCBFs) that controls inter-robot proximity to maintain connectivity
while avoiding collisions. We incorporate control Lyapunov functions to enable
connectivity recovery from initial disconnected configurations and temporary
losses, providing robust connectivity during navigation in obstacle-rich
environments. Our trajectory generation framework concurrently produces
planning and control through a Bezier-parameterized trajectory, which naturally
provides smooth curves with arbitrary degree of derivatives. The main
contribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory
generation and control method for connectivity maintenance and recovery of
multi-robot systems. We validate the framework through extensive simulations
and a physical experiment with 4 Crazyflie nano-quadrotors.

</details>


### [178] [LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy](https://arxiv.org/abs/2510.03529)
*Zekai Liang,Xiao Liang,Soofiyan Atar,Sreyan Das,Zoe Chiu,Peihan Zhang,Florian Richter,Shanglei Liu,Michael C. Yip*

Main category: cs.RO

TL;DR: LapSurgie是一种基于人形机器人的腹腔镜远程操作框架，旨在解决手术机器人系统在资源匮乏地区的部署问题。


<details>
  <summary>Details</summary>
Motivation: 手术机器人系统在资源匮乏地区的部署困难，加剧了医疗不平等。人形机器人因其无需大规模基础设施改造的特性，成为潜在解决方案。

Method: 采用逆映射策略控制手动腕式腹腔镜器械，遵守远程运动中心约束，实现对现成手术器械的精确控制。配备立体视觉系统的控制台提供实时视觉反馈。

Result: 用户研究表明，该框架有效且可行，初步验证了人形机器人在腹腔镜手术中的潜力。

Conclusion: LapSurgie为资源匮乏地区提供了一种可行的腹腔镜手术机器人解决方案，有望减少医疗不平等。

Abstract: Robotic laparoscopic surgery has gained increasing attention in recent years
for its potential to deliver more efficient and precise minimally invasive
procedures. However, adoption of surgical robotic platforms remains largely
confined to high-resource medical centers, exacerbating healthcare disparities
in rural and low-resource regions. To close this gap, a range of solutions has
been explored, from remote mentorship to fully remote telesurgery. Yet, the
practical deployment of surgical robotic systems to underserved communities
remains an unsolved challenge. Humanoid systems offer a promising path toward
deployability, as they can directly operate in environments designed for humans
without extensive infrastructure modifications -- including operating rooms. In
this work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic
teleoperation framework. The system leverages an inverse-mapping strategy for
manual-wristed laparoscopic instruments that abides to remote center-of-motion
constraints, enabling precise hand-to-tool control of off-the-shelf surgical
laparoscopic tools without additional setup requirements. A control console
equipped with a stereo vision system provides real-time visual feedback.
Finally, a comprehensive user study across platforms demonstrates the
effectiveness of the proposed framework and provides initial evidence for the
feasibility of deploying humanoid robots in laparoscopic procedures.

</details>


### [179] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 提出了一种新的相机-机器人标定框架，通过共享编码统一检测几何基元（关键点和轴边缘），实现高效姿态估计。


<details>
  <summary>Details</summary>
Motivation: 微创手术机器人由于长运动链和部分自由度不可见，传统标定方法难以适用，现有方法在特征检测或推理时间上存在问题。

Method: 通过共享编码统一检测关键点和轴边缘，利用投影几何进行姿态估计，使用大规模合成数据训练。

Result: 在特征检测和姿态估计上表现出快速性能和最先进精度。

Conclusion: 该方法在挑战性手术环境中实现了高效且高精度的相机-机器人标定。

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [180] [Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots](https://arxiv.org/abs/2510.03547)
*Carina Veil,Moritz Flaschel,Ellen Kuhl*

Main category: cs.RO

TL;DR: 提出了一种基于图搜索的路径规划方法，用于象鼻启发的软体机器人，解决了在复杂环境中运动规划的挑战。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有高度灵活性和非线性运动特性，但在复杂环境中的路径规划仍是一个难题。

Method: 利用形态弹性和主动细丝理论建立生物力学模型，预计算形状库并构建形状空间的k近邻图，结合距离函数和多目标边成本进行路径规划。

Result: 算法能在毫秒级时间内生成无碰撞且能量高效的路径，显著减少驱动能耗。

Conclusion: 形状空间图搜索为软体机器人实时路径规划提供了可行方案，适用于手术、工业和辅助场景。

Abstract: Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary
flexibility to bend, twist, and elongate in ways that rigid robots cannot.
However, their motion planning remains a challenge, especially in cluttered
environments with obstacles, due to their highly nonlinear and
infinite-dimensional kinematics. Here, we present a graph-based path planning
tool for an elephant-trunk-inspired soft robotic arm designed with three
artificial muscle fibers that allow for multimodal continuous deformation
through contraction. Using a biomechanical model inspired by morphoelasticity
and active filament theory, we precompute a shape library and construct a
$k$-nearest neighbor graph in \emph{shape space}, ensuring that each node
corresponds to a mechanically accurate and physically valid robot shape. For
the graph, we use signed distance functions to prune nodes and edges colliding
with obstacles, and define multi-objective edge costs based on geometric
distance and actuation effort, enabling energy-efficient planning with
collision avoidance. We demonstrate that our algorithm reliably avoids
obstacles and generates feasible paths within milliseconds from precomputed
graphs using Dijkstra's algorithm. We show that including energy costs can
drastically reduce the actuation effort compared to geometry-only planning, at
the expense of longer tip trajectories. Our results highlight the potential of
shape-space graph search for fast and reliable path planning in the field of
soft robotics, paving the way for real-time applications in surgical,
industrial, and assistive settings.

</details>


### [181] [Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning](https://arxiv.org/abs/2510.03599)
*Shafeef Omar,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出了一种基于接触显式表示的统一多任务运动与操作策略学习框架，通过接触目标序列定义任务，实现单一策略执行多种任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法需为不同任务设计不同策略，缺乏共享结构。本文旨在通过接触显式表示统一任务定义，提升策略的通用性和可扩展性。

Method: 采用目标条件强化学习（RL）策略，训练策略以实现给定的接触计划，验证于多种机器人形态和任务。

Result: 在四足、双足和双手操作任务中，单一策略表现出色，接触显式推理显著提升了对未见场景的泛化能力。

Conclusion: 接触显式策略学习为可扩展的运动与操作提供了有前景的基础，展示了跨形态系统的通用性和鲁棒性。

Abstract: We present a unified framework for multi-task locomotion and manipulation
policy learning grounded in a contact-explicit representation. Instead of
designing different policies for different tasks, our approach unifies the
definition of a task through a sequence of contact goals-desired contact
positions, timings, and active end-effectors. This enables leveraging the
shared structure across diverse contact-rich tasks, leading to a single policy
that can perform a wide range of tasks. In particular, we train a
goal-conditioned reinforcement learning (RL) policy to realise given contact
plans. We validate our framework on multiple robotic embodiments and tasks: a
quadruped performing multiple gaits, a humanoid performing multiple biped and
quadrupedal gaits, and a humanoid executing different bimanual object
manipulation tasks. Each of these scenarios is controlled by a single policy
trained to execute different tasks grounded in contacts, demonstrating
versatile and robust behaviours across morphologically distinct systems. Our
results show that explicit contact reasoning significantly improves
generalisation to unseen scenarios, positioning contact-explicit policy
learning as a promising foundation for scalable loco-manipulation.

</details>


### [182] [Safety-Oriented Dynamic Path Planning for Automated Vehicles](https://arxiv.org/abs/2510.03640)
*Mostafa Emam,Matthias Gerdts*

Main category: cs.RO

TL;DR: 提出了一种双层控制框架，用于动态环境中自动驾驶车辆的安全路径规划和避障。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，自动驾驶车辆需要更高级的路径规划和避障能力以确保安全。

Method: 采用双层控制框架，主循环使用非线性模型预测控制（NMPC）进行实时路径优化，辅以独立的备用循环提供安全后备轨迹。

Result: 评估表明该方法在多种驾驶场景中具有实时适用性和鲁棒性。

Conclusion: 该框架为复杂动态环境中更安全可靠的自动驾驶迈出了重要一步。

Abstract: Ensuring safety in autonomous vehicles necessitates advanced path planning
and obstacle avoidance capabilities, particularly in dynamic environments. This
paper introduces a bi-level control framework that efficiently augments road
boundaries by incorporating time-dependent grid projections of obstacle
movements, thus enabling precise and adaptive path planning. The main control
loop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path
optimization, wherein homotopy-based constraint relaxation is employed to
improve the solvability of the optimal control problem (OCP). Furthermore, an
independent backup loop runs concurrently to provide safe fallback trajectories
when an optimal trajectory cannot be computed by the main loop within a
critical time frame, thus enhancing safety and real-time performance. Our
evaluation showcases the benefits of the proposed methods in various driving
scenarios, highlighting the real-time applicability and robustness of our
approach. Overall, the framework represents a significant step towards safer
and more reliable autonomous driving in complex and dynamic environments.

</details>


### [183] [Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing](https://arxiv.org/abs/2510.03644)
*Mohammadjavad Javadi,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种基于Cosserat壳理论的高效坐标无关静态模型，用于分析大宽长比的磁性软机器人。


<details>
  <summary>Details</summary>
Motivation: 传统Cosserat杆理论适用于1D细长结构，但现代软机器人常具有2D壳结构，需新模型支持。

Method: 基于SE(3)群上的Cosserat壳理论，推导强/弱形式平衡方程，开发有限元方法避免壳结构建模常见问题。

Result: 模型通过实验验证，在壳结构经历大旋转/位移时表现优异。

Conclusion: 该模型为磁性软机器人的形状控制提供了高效分析工具。

Abstract: Cosserat rod theory is the popular approach to modeling ferromagnetic soft
robots as 1-Dimensional (1D) slender structures in most applications, such as
biomedical. However, recent soft robots designed for locomotion and
manipulation often exhibit a large width-to-length ratio that categorizes them
as 2D shells. For analysis and shape-morphing control purposes, we develop an
efficient coordinate-free static model of hard-magnetic shells found in soft
magnetic grippers and walking soft robots. The approach is based on a novel
formulation of Cosserat shell theory on the Special Euclidean group
($\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points
with six degrees of freedom (position & rotation) suitable for capturing the
behavior of a uniformly distributed array of spheroidal hard magnetic particles
embedded in the rheological elastomer. The shell's configuration manifold is
the space of all smooth embeddings $\mathbb{R}^2\rightarrow\mathbf{SE}(3)$.
According to a novel definition of local deformation gradient based on the Lie
group structure of $\mathbf{SE}(3)$, we derive the strong and weak forms of
equilibrium equations, following the principle of virtual work. We extract the
linearized version of the weak form for numerical implementations. The
resulting finite element approach can avoid well-known challenges such as
singularity and locking phenomenon in modeling shell structures. The proposed
model is analytically and experimentally validated through a series of test
cases that demonstrate its superior efficacy, particularly when the shell
undergoes severe rotations and displacements.

</details>


### [184] [An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion](https://arxiv.org/abs/2510.03660)
*Mohammadjavad Javadi,Charlie Wadds,Robin Chhabra*

Main category: cs.RO

TL;DR: 一种受尺蠖启发的无束缚软体机器人，通过磁力驱动实现多模态运动。


<details>
  <summary>Details</summary>
Motivation: 开发无需外部基础设施的无束缚软体机器人，以适应多样化任务环境。

Method: 采用弯曲柔性结构和磁力驱动，集成轻量化控制电路和摄像头。

Result: 机器人重102.63克，最大行走速度3.74厘米/秒，游泳速度0.82厘米/秒。

Conclusion: 通过结构优化和系统集成，机器人成功实现行走、转向、游泳和负载运输。

Abstract: Untethered soft robots are essential for advancing the real-world deployment
of soft robotic systems in diverse and multitasking environments. Inspired by
soft-bodied inchworm, we present a fully untethered soft robot with a curved,
flexible structure actuated by magnetic forces. The robot has a total mass of
102.63 g and demonstrates multimodal locomotion, achieving a maximum walking
speed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight
onboard control circuit enables wireless command transmission, while an
integrated camera provides environmental perception. Through structural
optimization and system-level integration, the robot successfully performs
walking, steering, swimming, and payload transport without reliance on external
infrastructure. The robot's dynamic performance and locomotion capabilities are
systematically validated through experimental characterization.

</details>


### [185] [Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments](https://arxiv.org/abs/2510.03677)
*Salim Rezvani,Ammar Jaleel Mahmood,Robin Chhabra*

Main category: cs.RO

TL;DR: 论文研究了视觉退化对机器人自建模的影响，并提出了一种结合经典修复和形态保留约束的任务感知去噪框架，显著提升了自建模的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自主建模管道在真实感知条件下（如噪声图像和杂乱背景）表现脆弱，限制了机器人自建模的适应性。

Method: 通过仿真和物理实验量化视觉退化对自建模的影响，并引入任务感知去噪框架和语义分割技术。

Result: 实验表明，该方法在模拟和物理平台上恢复了接近基线的性能，而现有管道显著退化。

Conclusion: 该研究提升了视觉自建模的鲁棒性，为在不可预测的真实环境中部署自感知机器人奠定了基础。

Abstract: Robots with internal visual self-models promise unprecedented adaptability,
yet existing autonomous modeling pipelines remain fragile under realistic
sensing conditions such as noisy imagery and cluttered backgrounds. This paper
presents the first systematic study quantifying how visual
degradations--including blur, salt-and-pepper noise, and Gaussian noise--affect
robotic self-modeling. Through both simulation and physical experiments, we
demonstrate their impact on morphology prediction, trajectory planning, and
damage recovery in state-of-the-art pipelines. To overcome these challenges, we
introduce a task-aware denoising framework that couples classical restoration
with morphology-preserving constraints, ensuring retention of structural cues
critical for self-modeling. In addition, we integrate semantic segmentation to
robustly isolate robots from cluttered and colorful scenes. Extensive
experiments show that our approach restores near-baseline performance across
simulated and physical platforms, while existing pipelines degrade
significantly. These contributions advance the robustness of visual
self-modeling and establish practical foundations for deploying self-aware
robots in unpredictable real-world environments.

</details>


### [186] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: EmbodiSwap是一种生成逼真合成机器人覆盖层的方法，用于零样本模仿学习，通过V-JEPA视觉骨干在机器人操作中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决人类视频与目标机器人之间的体现差距问题，实现零样本模仿学习。

Method: 利用EmbodiSwap生成合成机器人视频，采用V-JEPA作为视觉骨干，训练闭环机器人操作策略。

Result: 在真实测试中，零样本训练的V-JEPA模型成功率达82%，优于其他方法。

Conclusion: EmbodiSwap和V-JEPA的结合在机器人模仿学习中表现突出，并开源了相关代码和数据以促进研究。

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [187] [Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics](https://arxiv.org/abs/2510.03768)
*Aydin Ahmadi,Baris Akgun*

Main category: cs.RO

TL;DR: 提出了一种基于模型的数据驱动平面推动框架，通过单一学习模型解决多任务，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 减少手动工程并提高泛化能力，解决现有方法功能狭窄的问题。

Method: 使用GRU循环架构和非线性层捕捉动态，结合MPPI控制器生成自适应动作。

Result: 在仿真和真实实验中表现出高成功率，支持多任务切换。

Conclusion: 框架灵活且高效，适用于多种任务，未来可扩展更多对象类型。

Abstract: Data-driven planar pushing methods have recently gained attention as they
reduce manual engineering effort and improve generalization compared to
analytical approaches. However, most prior work targets narrow capabilities
(e.g., side switching, precision, or single-task training), limiting broader
applicability. We present a model-based framework for non-prehensile tabletop
pushing that uses a single learned model to address multiple tasks without
retraining. Our approach employs a recurrent GRU-based architecture with
additional non-linear layers to capture object-environment dynamics while
ensuring stability. A tailored state-action representation enables the model to
generalize across uncertain dynamics, variable push lengths, and diverse tasks.
For control, we integrate the learned dynamics with a sampling-based Model
Predictive Path Integral (MPPI) controller, which generates adaptive,
task-oriented actions. This framework supports side switching, variable-length
pushes, and objectives such as precise positioning, trajectory following, and
obstacle avoidance. Training is performed in simulation with domain
randomization to support sim-to-real transfer. We first evaluate the
architecture through ablation studies, showing improved prediction accuracy and
stable rollouts. We then validate the full system in simulation and real-world
experiments using a Franka Panda robot with markerless tracking. Results
demonstrate high success rates in precise positioning under strict thresholds
and strong performance in trajectory tracking and obstacle avoidance. Moreover,
multiple tasks are solved simply by changing the controller's objective
function, without retraining. While our current focus is on a single object
type, we extend the framework by training on wider push lengths and designing a
balanced controller that reduces the number of steps for longer-horizon goals.

</details>


### [188] [Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets](https://arxiv.org/abs/2510.03776)
*Tiago Rodrigues de Almeida,Yufei Zhu,Andrey Rudenko,Tomasz P. Kucner,Johannes A. Stork,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: 论文研究了在复杂动态环境中，通过类别条件运动预测减少预测不确定性，提高轨迹预测准确性。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，智能系统需预测周围代理的未来行为以避免碰撞并高效完成任务。类别条件预测可减少不确定性，但现有研究较少。

Method: 提出并评估了基于条件模式和深度学习的轨迹预测方法，测试于TH"OR-MAGNI和Stanford Drone数据集。

Result: 实验表明，考虑类别标签可提高预测准确性；深度学习在平衡数据中表现更好，而模式方法在数据有限或不平衡时更优。

Conclusion: 类别条件预测能显著提升性能，但方法选择需根据数据平衡性和可用性决定。

Abstract: Robots and other intelligent systems navigating in complex dynamic
environments should predict future actions and intentions of surrounding agents
to reach their goals efficiently and avoid collisions. The dynamics of those
agents strongly depends on their tasks, roles, or observable labels.
Class-conditioned motion prediction is thus an appealing way to reduce forecast
uncertainty and get more accurate predictions for heterogeneous agents.
However, this is hardly explored in the prior art, especially for mobile robots
and in limited data applications. In this paper, we analyse different
class-conditioned trajectory prediction methods on two datasets. We propose a
set of conditional pattern-based and efficient deep learning-based baselines,
and evaluate their performance on robotics and outdoors datasets (TH\"OR-MAGNI
and Stanford Drone Dataset). Our experiments show that all methods improve
accuracy in most of the settings when considering class labels. More
importantly, we observe that there are significant differences when learning
from imbalanced datasets, or in new environments where sufficient data is not
available. In particular, we find that deep learning methods perform better on
balanced datasets, but in applications with limited data, e.g., cold start of a
robot in a new environment, or imbalanced classes, pattern-based methods may be
preferable.

</details>


### [189] [COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments](https://arxiv.org/abs/2510.03875)
*Niranjan Kumar Ilampooranan,Constantinos Chamzas*

Main category: cs.RO

TL;DR: COVER框架在部分静态环境中通过分区验证路图，实现固定时间内的运动规划。


<details>
  <summary>Details</summary>
Motivation: 半静态环境中，障碍物变化有限，但现有方法缺乏形式化保证或依赖限制性离散化，限制了实际应用。

Method: COVER通过分区障碍物配置空间并验证每个分区内的路图可行性，构建覆盖验证的路图。

Result: 在7自由度Panda机器人模拟中，COVER比现有方法覆盖更广且查询成功率更高。

Conclusion: COVER为半静态环境中的运动规划提供了更强的保证和实用性。

Abstract: Having the ability to answer motion-planning queries within a fixed time
budget is critical for the widespread deployment of robotic systems.
Semi-static environments, where most obstacles remain static but a limited set
can vary across queries, exhibit structured variability that can be
systematically exploited to provide stronger guarantees than in general
motion-planning problems. However, prior approaches in this setting either lack
formal guarantees or rely on restrictive discretizations of obstacle
configurations, limiting their applicability in realistic domains. This paper
introduces COVER, a novel framework that incrementally constructs a
coverage-verified roadmap in semi-static environments. By partitioning the
obstacle configuration space and solving for feasible paths within each
partition, COVER systematically verifies feasibility of the roadmap in each
partition and guarantees fixed-time motion planning queries within the verified
regions. We validate COVER with a 7-DOF simulated Panda robot performing table
and shelf tasks, demonstrating that COVER achieves broader coverage with higher
query success rates than prior works.

</details>


### [190] [Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning](https://arxiv.org/abs/2510.03885)
*Sunghwan Kim,Woojeh Chung,Zhirui Dai,Dwait Bhatt,Arth Shukla,Hao Su,Yulun Tian,Nikolay Atanasov*

Main category: cs.RO

TL;DR: SBP是一种端到端的策略学习方法，利用3D潜在地图实现更强的空间和时间推理能力，优于仅依赖图像的策略。


<details>
  <summary>Details</summary>
Motivation: 解决移动操作策略在空间和时间推理上的局限性，通过3D地图扩展感知范围并聚合长期观察。

Method: 通过多视角观察增量融合到场景特定的潜在特征网格中，预训练的解码器重建目标嵌入，策略利用3D特征聚合器从地图中获取全局上下文。

Result: SBP在场景级移动操作和顺序桌面操作任务中表现优异，成功率高25%。

Conclusion: SBP通过3D潜在地图实现了全局推理和长期记忆，显著提升了移动操作策略的性能。

Abstract: In this paper, we demonstrate that mobile manipulation policies utilizing a
3D latent map achieve stronger spatial and temporal reasoning than policies
relying solely on images. We introduce Seeing the Bigger Picture (SBP), an
end-to-end policy learning approach that operates directly on a 3D map of
latent features. In SBP, the map extends perception beyond the robot's current
field of view and aggregates observations over long horizons. Our mapping
approach incrementally fuses multiview observations into a grid of
scene-specific latent features. A pre-trained, scene-agnostic decoder
reconstructs target embeddings from these features and enables online
optimization of the map features during task execution. A policy, trainable
with behavior cloning or reinforcement learning, treats the latent map as a
state variable and uses global context from the map obtained via a 3D feature
aggregator. We evaluate SBP on scene-level mobile manipulation and sequential
tabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons
globally over the scene, (ii) leverages the map as long-horizon memory, and
(iii) outperforms image-based policies in both in-distribution and novel
scenes, e.g., improving the success rate by 25% for the sequential manipulation
task.

</details>


### [191] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: NoTVLA框架通过稀疏轨迹避免灾难性遗忘，提升多任务性能和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型因依赖连续动作序列导致的灾难性遗忘问题。

Method: 采用稀疏轨迹训练，结合时间压缩和空间推理剪枝策略。

Result: 在多任务评估中表现优于pi0，计算资源更少且无需额外摄像头。

Conclusion: NoTVLA在保持语言能力的同时，实现了多平台部署和任务泛化。

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [192] [WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding](https://arxiv.org/abs/2510.03910)
*Akhil Padmanabha,Jessie Yuan,Tanisha Mehta,Rajat Kumar Jenamani,Eric Hu,Victoria de León,Anthony Wertz,Janavi Gupta,Ben Dodson,Yunting Yan,Carmel Majidi,Tapomayukh Bhattacharjee,Zackory Erickson*

Main category: cs.RO

TL;DR: WAFFLE系统通过可穿戴传感器预测咬合时机，提升机器人喂食的自然性和反应性。


<details>
  <summary>Details</summary>
Motivation: 解决机器人喂食系统中咬合时机估计的技术挑战，以增强用户自主性和生活质量。

Method: 利用可穿戴传感器数据训练监督回归模型，结合用户可调的自信阈值生成指令。

Result: 在15名无运动障碍参与者中表现优于基线方法，并在2名运动障碍参与者家中验证了通用性。

Conclusion: WAFFLE系统能有效实现自然、反应性的咬合时机，适用于多种用户和场景。

Abstract: Millions of people around the world need assistance with feeding. Robotic
feeding systems offer the potential to enhance autonomy and quality of life for
individuals with impairments and reduce caregiver workload. However, their
widespread adoption has been limited by technical challenges such as estimating
bite timing, the appropriate moment for the robot to transfer food to a user's
mouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with
LEarned bite timing, a system that accurately predicts bite timing by
leveraging wearable sensor data to be highly reactive to natural user cues such
as head movements, chewing, and talking. We train a supervised regression model
on bite timing data from 14 participants and incorporate a user-adjustable
assertiveness threshold to convert predictions into proceed or stop commands.
In a study with 15 participants without motor impairments with the Obi feeding
robot, WAFFLE performs statistically on par with or better than baseline
methods across measures of feeling of control, robot understanding, and
workload, and is preferred by the majority of participants for both individual
and social dining. We further demonstrate WAFFLE's generalizability in a study
with 2 participants with motor impairments in their home environments using a
Kinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling
natural, reactive bite timing that generalizes across users, robot hardware,
robot positioning, feeding trajectories, foods, and both individual and social
dining contexts.

</details>


### [193] [TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry](https://arxiv.org/abs/2510.03919)
*Matthew Lisondra,Junseo Kim,Glenn Takashi Shimoda,Kourosh Zareinia,Sajad Saeedi*

Main category: cs.RO

TL;DR: TCB-VIO是一种基于多状态约束卡尔曼滤波的高帧率视觉-惯性里程计，显著减少空间和时间漂移，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉-惯性里程计（VIO）存在空间和时间漂移问题，新一代传感器FPSP通过高帧率运行可减少空间漂移。

Method: 提出TCB-VIO，采用多状态约束卡尔曼滤波（MSCKF），以250 FPS的高帧率和400 Hz的IMU测量运行。

Result: TCB-VIO在性能上优于ROVIO、VINS-Mono和ORB-SLAM3等现有方法。

Conclusion: TCB-VIO通过高帧率和紧密耦合设计，有效解决了VIO的漂移问题，提升了性能。

Abstract: Vision algorithms can be executed directly on the image sensor when
implemented on the next-generation sensors known as focal-plane
sensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs
greatly improve latency, reducing the problems associated with the bottleneck
of data transfer from a vision sensor to a processor. FPSPs accelerate
vision-based algorithms such as visual-inertial odometry (VIO). However, VIO
frameworks suffer from spatial drift due to the vision-based pose estimation,
whilst temporal drift arises from the inertial measurements. FPSPs circumvent
the spatial drift by operating at a high frame rate to match the high-frequency
output of the inertial measurements. In this paper, we present TCB-VIO, a
tightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman
Filter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU
measurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:
ROVIO, VINS-Mono, and ORB-SLAM3.

</details>


### [194] [A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM](https://arxiv.org/abs/2510.03948)
*Otobong Jerome,Geesara Prathap Kulathunga,Devitt Dmitry,Eugene Murawjow,Alexandr Klimchik*

Main category: cs.RO

TL;DR: 提出了一种针对越野环境的全局路径规划方法，解决了实时性、运动学可行性和内存效率等关键问题。


<details>
  <summary>Details</summary>
Motivation: 越野环境的复杂性和非结构化特性使传统全局路径规划方法表现不佳，需要一种新方法来解决这些问题。

Method: 通过构建中间地图，将规划问题分为图路径规划、运动学可行性检查和路径平滑三个子问题。

Result: 在大型地图上测试，平均1.5秒找到可行路径，内存占用约1.5GB。

Conclusion: 该方法适用于多种越野导航任务，如搜救和农业操作。

Abstract: Off-road environments present unique challenges for autonomous navigation due
to their complex and unstructured nature. Traditional global path-planning
methods, which typically aim to minimize path length and travel time, perform
poorly on large-scale maps and fail to account for critical factors such as
real-time performance, kinematic feasibility, and memory efficiency. This paper
introduces a novel global path-planning method specifically designed for
off-road environments, addressing these essential factors. The method begins by
constructing an intermediate map within the pixel coordinate system,
incorporating geographical features like off-road trails, waterways, restricted
and passable areas, and trees. The planning problem is then divided into three
sub-problems: graph-based path planning, kinematic feasibility checking, and
path smoothing. This approach effectively meets real-time performance
requirements while ensuring kinematic feasibility and efficient memory use. The
method was tested in various off-road environments with large-scale maps up to
several square kilometers in size, successfully identifying feasible paths in
an average of 1.5 seconds and utilizing approximately 1.5GB of memory under
extreme conditions. The proposed framework is versatile and applicable to a
wide range of off-road autonomous navigation tasks, including search and rescue
missions and agricultural operations.

</details>


### [195] [SITCOM: Scaling Inference-Time COMpute for VLAs](https://arxiv.org/abs/2510.04041)
*Ayudh Saxena,Harsh Shah,Sandeep Routray,Rishi Rajesh Shah,Esha Pahwa*

Main category: cs.RO

TL;DR: SITCOM框架通过结合模型预测控制和奖励轨迹选择，显著提升了VLA模型在长时程任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型在动态任务中缺乏前瞻性和错误累积的问题。

Method: 利用学习到的动态模型进行多步动作模拟，选择最佳执行计划。

Result: 任务完成率从48%提升至72%。

Conclusion: SITCOM结合良好奖励函数可显著提升VLA模型的鲁棒性和长时程规划能力。

Abstract: Learning robust robotic control policies remains a major challenge due to the
high cost of collecting labeled data, limited generalization to unseen
environments, and difficulties in planning over long horizons. While
Vision-Language-Action (VLA) models offer a promising solution by grounding
natural language instructions into single-step control commands, they often
lack mechanisms for lookahead and struggle with compounding errors in dynamic
tasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs
(SITCOM), a framework that augments any pretrained VLA with model-based
rollouts and reward-based trajectory selection, inspired by Model Predictive
Control algorithm. SITCOM leverages a learned dynamics model to simulate
multi-step action rollouts to select the best candidate plan for real-world
execution, transforming one-shot VLAs into robust long-horizon planners. We
develop an efficient transformer-based dynamics model trained on large-scale
BridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim
gap, and score candidate rollouts using rewards from simulator. Through
comprehensive evaluation across multiple tasks and settings in the SIMPLER
environment, we demonstrate that SITCOM when combined with a good reward
function can significantly improve task completion rate from 48% to 72% using
trained dynamics model.

</details>


### [196] [Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback](https://arxiv.org/abs/2510.04074)
*Chung-Pang Wang,Changwei Chen,Xiao Liang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 提出了一种基于反馈的自主组织解剖框架，通过显式推理内窥镜图像中的拓扑变化，优化解剖动作，提升手术系统的自主性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手术机器人反馈机制在组织解剖的拓扑和感知挑战上表现不足，需改进以适应动态手术环境。

Method: 引入可见性指标量化组织暴露，设计最优控制器以最大化可见性，并将反馈机制与规划和学习方法结合。

Result: 实验表明，该框架显著提升了自主性、减少了错误，并在复杂手术场景中增强了鲁棒性。

Conclusion: 反馈机制在自主组织解剖中具有重要作用，能有效提升手术系统的适应性和可靠性。

Abstract: Autonomous surgical systems must adapt to highly dynamic environments where
tissue properties and visual cues evolve rapidly. Central to such adaptability
is feedback: the ability to sense, interpret, and respond to changes during
execution. While feedback mechanisms have been explored in surgical robotics,
ranging from tool and tissue tracking to error detection, existing methods
remain limited in handling the topological and perceptual challenges of tissue
dissection. In this work, we propose a feedback-enabled framework for
autonomous tissue dissection that explicitly reasons about topological changes
from endoscopic images after each dissection action. This structured feedback
guides subsequent actions, enabling the system to localize dissection progress
and adapt policies online. To improve the reliability of such feedback, we
introduce visibility metrics that quantify tissue exposure and formulate
optimal controller designs that actively manipulate tissue to maximize
visibility. Finally, we integrate these feedback mechanisms with both
planning-based and learning-based dissection methods, and demonstrate
experimentally that they significantly enhance autonomy, reduce errors, and
improve robustness in complex surgical scenarios.

</details>


### [197] [From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents](https://arxiv.org/abs/2510.04076)
*Amin Vahidi-Moghaddam,Sayed Pedram Haeri Boroujeni,Iman Jebellat,Ehsan Jebellat,Niloufar Mehrabi,Zhaojian Li*

Main category: cs.RO

TL;DR: 论文探讨了现代控制应用中实现准确、快速和安全运动的方法，重点介绍了数据驱动的替代方案及其局限性，并提出了八种降低计算复杂度的技术。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统中获取精确模型的困难，以及数据驱动策略在实时性、计算和内存需求上的不足。

Method: 介绍了八种降低计算复杂度的技术，包括降阶建模、函数逼近策略学习和凸松弛等。

Result: 这些方法在机器人手臂、软机器人和车辆运动控制等实际应用中表现出有效性。

Conclusion: 数据驱动策略在控制应用中具有潜力，但需通过技术优化以克服其局限性。

Abstract: One of the main challenges in modern control applications, particularly in
robot and vehicle motion control, is achieving accurate, fast, and safe
movement. To address this, optimal control policies have been developed to
enforce safety while ensuring high performance. Since basic first-principles
models of real systems are often available, model-based controllers are widely
used. Model predictive control (MPC) is a leading approach that optimizes
performance while explicitly handling safety constraints. However, obtaining
accurate models for complex systems is difficult, which motivates data-driven
alternatives. ML-based MPC leverages learned models to reduce reliance on
hand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal
policies directly from interaction data. Data-enabled predictive control
(DeePC) goes further by bypassing modeling altogether, directly learning safe
policies from raw input-output data. Recently, large language model (LLM)
agents have also emerged, translating natural language instructions into
structured formulations of optimal control problems. Despite these advances,
data-driven policies face significant limitations. They often suffer from slow
response times, high computational demands, and large memory needs, making them
less practical for real-world systems with fast dynamics, limited onboard
computing, or strict memory constraints. To address this, various technique,
such as reduced-order modeling, function-approximated policy learning, and
convex relaxations, have been proposed to reduce computational complexity. In
this paper, we present eight such approaches and demonstrate their
effectiveness across real-world applications, including robotic arms, soft
robots, and vehicle motion control.

</details>


### [198] [HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments](https://arxiv.org/abs/2510.04161)
*Longrui Yang,Yiyu Wang,Jingfan Tang,Yunpeng Lv,Shizhe Zhao,Chao Cao,Zhongqiang Ren*

Main category: cs.RO

TL;DR: HEHA（分层异构代理探索）是一种用于多异构机器人自主探索未知环境的分层路径规划方法，通过全局和局部规划优化探索效率。


<details>
  <summary>Details</summary>
Motivation: 解决异构机器人在未知环境中探索时的路径规划和任务分配问题，需快速迭代解决大规模约束优化问题。

Method: 提出HEHA方法，结合全局规划（PEAF算法）和局部规划，优化异构机器人的探索路径和任务分配。

Result: 实验表明，HEHA比基线方法减少高达30%的探索时间。

Conclusion: HEHA通过分层规划和异构感知，显著提升了多机器人探索未知环境的效率。

Abstract: This paper considers the path planning problem for autonomous exploration of
an unknown environment using multiple heterogeneous robots such as drones,
wheeled, and legged robots, which have different capabilities to traverse
complex terrains. A key challenge there is to intelligently allocate the robots
to the unknown areas to be explored and determine the visiting order of those
spaces subject to traversablity constraints, which leads to a large scale
constrained optimization problem that needs to be quickly and iteratively
solved every time when new space are explored. To address the challenge, we
propose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging
a recent hierarchical method that decompose the exploration into global
planning and local planning. The major contribution in HEHA is its global
planning, where we propose a new routing algorithm PEAF (Partial Anytime Focal
search) that can quickly find bounded sub-optimal solutions to minimize the
maximum path length among the agents subject to traversability constraints.
Additionally, the local planner in HEHA also considers heterogeneity to avoid
repeated and duplicated exploration among the robots. The experimental results
show that, our HEHA can reduce up to 30% of the exploration time than the
baselines.

</details>


### [199] [Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation](https://arxiv.org/abs/2510.04168)
*Amirmasoud Molaei,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的全数据驱动控制框架，用于挖掘机抓取岩石任务，无需建模岩石或土壤特性。


<details>
  <summary>Details</summary>
Motivation: 传统岩石抓取依赖熟练操作员，且现有自动化方法局限于连续介质或专用夹具，难以适应实际工地需求。

Method: 使用PPO算法在AGX Dynamics模拟器中训练模型无关的强化学习代理，通过领域随机化增强鲁棒性。

Result: 实验表明，该策略能泛化到未见过的岩石和不同土壤条件，成功率与人类操作员相当。

Conclusion: 证明了基于学习的方法可在不依赖专用硬件或详细材料模型的情况下实现离散物体操作。

Abstract: Rock capturing with standard excavator buckets is a challenging task
typically requiring the expertise of skilled operators. Unlike soil digging, it
involves manipulating large, irregular rocks in unstructured environments where
complex contact interactions with granular material make model-based control
impractical. Existing autonomous excavation methods focus mainly on continuous
media or rely on specialized grippers, limiting their applicability to
real-world construction sites. This paper introduces a fully data-driven
control framework for rock capturing that eliminates the need for explicit
modeling of rock or soil properties. A model-free reinforcement learning agent
is trained in the AGX Dynamics simulator using the Proximal Policy Optimization
(PPO) algorithm and a guiding reward formulation. The learned policy outputs
joint velocity commands directly to the boom, arm, and bucket of a CAT365
excavator model. Robustness is enhanced through extensive domain randomization
of rock geometry, density, and mass, as well as the initial configurations of
the bucket, rock, and goal position. To the best of our knowledge, this is the
first study to develop and evaluate an RL-based controller for the rock
capturing task. Experimental results show that the policy generalizes well to
unseen rocks and varying soil conditions, achieving high success rates
comparable to those of human participants while maintaining machine stability.
These findings demonstrate the feasibility of learning-based excavation
strategies for discrete object manipulation without requiring specialized
hardware or detailed material models.

</details>


### [200] [VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs](https://arxiv.org/abs/2510.04171)
*Lakshadeep Naik,Adam Fischer,Daniel Duberg,Danica Kragic*

Main category: cs.RO

TL;DR: VBM-NET是一种基于学习的方法，通过顶视正交投影选择移动底座姿态，比传统方法更快且效果相当。


<details>
  <summary>Details</summary>
Motivation: 解决移动底座姿态规划问题，传统方法依赖精确状态信息，而VBM-NET直接从场景的顶视投影中学习。

Method: 使用等变TransporterNet利用空间对称性学习候选姿态，结合图神经网络和强化学习选择最优姿态。

Result: VBM-NET在计算时间显著减少的情况下，与传统方法效果相当，并成功实现仿真到现实的迁移。

Conclusion: VBM-NET为移动底座姿态规划提供了一种高效且可迁移的解决方案。

Abstract: In Mobile Manipulation, selecting an optimal mobile base pose is essential
for successful object grasping. Previous works have addressed this problem
either through classical planning methods or by learning state-based policies.
They assume access to reliable state information, such as the precise object
poses and environment models. In this work, we study base pose planning
directly from top-down orthographic projections of the scene, which provide a
global overview of the scene while preserving spatial structure. We propose
VBM-NET, a learning-based method for base pose selection using such top-down
orthographic projections. We use equivariant TransporterNet to exploit spatial
symmetries and efficiently learn candidate base poses for grasping. Further, we
use graph neural networks to represent a varying number of candidate base poses
and use Reinforcement Learning to determine the optimal base pose among them.
We show that VBM-NET can produce comparable solutions to the classical methods
in significantly less computation time. Furthermore, we validate sim-to-real
transfer by successfully deploying a policy trained in simulation to real-world
mobile manipulation.

</details>


### [201] [Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve](https://arxiv.org/abs/2510.04178)
*Léa Pistorius,Namrata U. Nayar,Phillip Tran,Sammy Elmariah,Pierre E. Dupont*

Main category: cs.RO

TL;DR: 论文研究了机器人辅助在二尖瓣边缘修复手术中的应用，通过游戏控制器实现直观控制，相比手动操作提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 手动导管系统在经导管瓣膜修复中存在机械限制和学习曲线陡峭的问题，机器人辅助有望解决这些挑战。

Method: 将手动控制替换为机器人关节控制，通过游戏控制器操作，并在心脏模型上比较手动与机器人操作的性能。

Result: 机器人系统减少了手术时间和运动误差，提高了夹子放置的准确性。

Conclusion: 机器人辅助可以解决手动系统的关键限制，为复杂经导管手术提供更可靠和用户友好的平台。

Abstract: Transcatheter valve repair presents significant challenges due to the
mechanical limitations and steep learning curve associated with manual catheter
systems. This paper investigates the use of robotics to facilitate
transcatheter procedures in the context of mitral valve edge-to-edge repair.
The complex handle-based control of a clinical repair device is replaced by
intuitive robotic joint-based control via a game controller. Manual versus
robotic performance is analyzed by decomposing the overall device delivery task
into motion-specific steps and comparing capabilities on a step-by-step basis
in a phantom model of the heart and vasculature. Metrics include procedure
duration and clip placement accuracy. Results demonstrate that the robotic
system can reduce procedural time and motion errors while also improving
accuracy of clip placement. These findings suggest that robotic assistance can
address key limitations of manual systems, offering a more reliable and
user-friendly platform for complex transcatheter procedures.

</details>


### [202] [Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification](https://arxiv.org/abs/2510.04190)
*Jian-jie Zheng,Chih-kai Yang,Po-han Chen,Lyn Chao-ling Chen*

Main category: cs.RO

TL;DR: 研究提出了一种社会机器人巡逻系统，实时识别并通知非法停车，采用GPT-4o多模态模型进行车牌识别，无需预处理，验证了高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决室内停车场非法停车问题，提供社会辅助机器人方案。

Method: 采用双模型管道方法和GPT-4o多模态模型，机器人自动调整摄像头角度捕获车牌图像。

Result: GPT-4o模型在车牌识别中表现高准确性，机器人能实时通知系统管理员非法停车。

Conclusion: 研究验证了多模态深度学习方法在车牌识别中的有效性，并展示了社会机器人在实际场景中的应用潜力。

Abstract: In the study, the social robot act as a patrol to recognize and notify
illegal parking in real-time. Dual-model pipeline method and large multimodal
model were compared, and the GPT-4o multimodal model was adopted in license
plate recognition without preprocessing. For moving smoothly on a flat ground,
the robot navigated in a simulated parking lot in the experiments. The robot
changes angle view of the camera automatically to capture the images around
with the format of license plate number. From the captured images of the robot,
the numbers on the plate are recognized through the GPT-4o model, and
identifies legality of the numbers. When an illegal parking is detected, the
robot sends Line messages to the system manager immediately. The contribution
of the work is that a novel multimodal deep learning method has validated with
high accuracy in license plate recognition, and a social assistive robot is
also provided for solving problems in a real scenario, and can be applied in an
indoor parking lot.

</details>


### [203] [Flexible Locomotion Learning with Diffusion Model Predictive Control](https://arxiv.org/abs/2510.04234)
*Runhan Huang,Haldun Balim,Heng Yang,Yilun Du*

Main category: cs.RO

TL;DR: Diffusion-MPC结合生成扩散模型和MPC，实现灵活的行为合成和测试时适应。


<details>
  <summary>Details</summary>
Motivation: 解决模型自由RL方法固定策略难以适应新行为的问题，以及传统MPC依赖精确动力学模型的局限性。

Method: 利用生成扩散模型作为近似动力学先验，结合奖励和约束优化进行规划。

Result: 在真实世界中验证了强大的运动能力和灵活适应性。

Conclusion: Diffusion-MPC通过联合预测和交互训练，实现了无需重新训练的测试时适应。

Abstract: Legged locomotion demands controllers that are both robust and adaptable,
while remaining compatible with task and safety considerations. However,
model-free reinforcement learning (RL) methods often yield a fixed policy that
can be difficult to adapt to new behaviors at test time. In contrast, Model
Predictive Control (MPC) provides a natural approach to flexible behavior
synthesis by incorporating different objectives and constraints directly into
its optimization process. However, classical MPC relies on accurate dynamics
models, which are often difficult to obtain in complex environments and
typically require simplifying assumptions. We present Diffusion-MPC, which
leverages a learned generative diffusion model as an approximate dynamics prior
for planning, enabling flexible test-time adaptation through reward and
constraint based optimization. Diffusion-MPC jointly predicts future states and
actions; at each reverse step, we incorporate reward planning and impose
constraint projection, yielding trajectories that satisfy task objectives while
remaining within physical limits. To obtain a planning model that adapts beyond
imitation pretraining, we introduce an interactive training algorithm for
diffusion based planner: we execute our reward-and-constraint planner in
environment, then filter and reweight the collected trajectories by their
realized returns before updating the denoiser. Our design enables strong
test-time adaptability, allowing the planner to adjust to new reward
specifications without retraining. We validate Diffusion-MPC on real world,
demonstrating strong locomotion and flexible adaptation.

</details>


### [204] [ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context](https://arxiv.org/abs/2510.04246)
*Huiwon Jang,Sihyun Yu,Heeseung Kwon,Hojin Jeon,Younggyo Seo,Jinwoo Shin*

Main category: cs.RO

TL;DR: ContextVLA是一种利用多帧观测有效提升机器人任务性能的策略模型，通过压缩历史观测为单一上下文标记，显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 研究发现Vision-Language-Action模型（VLA）能更有效地利用多帧观测生成动作，但其高维视频输入导致计算效率低下。

Method: ContextVLA将历史观测压缩为单一上下文标记，以高效利用时间上下文生成动作。

Result: 实验表明，ContextVLA性能优于单帧VLA，同时减少了训练和推理时间。

Conclusion: ContextVLA通过高效利用多帧观测，显著提升了机器人任务的性能与计算效率。

Abstract: Leveraging temporal context is crucial for success in partially observable
robotic tasks. However, prior work in behavior cloning has demonstrated
inconsistent performance gains when using multi-frame observations. In this
paper, we introduce ContextVLA, a policy model that robustly improves robotic
task performance by effectively leveraging multi-frame observations. Our
approach is motivated by the key observation that Vision-Language-Action models
(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more
effectively utilize multi-frame observations for action generation. This
suggests that VLMs' inherent temporal understanding capability enables them to
extract more meaningful context from multi-frame observations. However, the
high dimensionality of video inputs introduces significant computational
overhead, making VLA training and inference inefficient. To address this,
ContextVLA compresses past observations into a single context token, allowing
the policy to efficiently leverage temporal context for action generation. Our
experiments show that ContextVLA consistently improves over single-frame VLAs
and achieves the benefits of full multi-frame training but with reduced
training and inference times.

</details>


### [205] [Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit](https://arxiv.org/abs/2510.04278)
*Peiwen Yang,Weisong Wen,Runqiu Yang,Yuanyuan Zhang,Jiahao Hu,Yingming Chen,Naigui Xiao,Jiaqi Zhao*

Main category: cs.RO

TL;DR: FactorMPC是一种基于因子图的MPC工具包，用于解决非线性流形上的系统控制问题，支持实时性能和安全关键应用。


<details>
  <summary>Details</summary>
Motivation: 传统MPC在非线性流形上存在奇异性、过参数化和收敛性差的问题，需要一种更高效和几何一致的方法。

Method: 通过因子图统一系统动力学、约束和目标，支持流形状态和高斯不确定性，利用稀疏性和概率结构实现实时性能。

Result: 在四旋翼飞行器上展示了优于基线方法的轨迹跟踪和避障性能。

Conclusion: FactorMPC提供了一个可扩展且几何一致的框架，支持开源实现以促进研究可重复性。

Abstract: Model predictive control (MPC) faces significant limitations when applied to
systems evolving on nonlinear manifolds, such as robotic attitude dynamics and
constrained motion planning, where traditional Euclidean formulations struggle
with singularities, over-parameterization, and poor convergence. To overcome
these challenges, this paper introduces FactorMPC, a factor-graph based MPC
toolkit that unifies system dynamics, constraints, and objectives into a
modular, user-friendly, and efficient optimization structure. Our approach
natively supports manifold-valued states with Gaussian uncertainties modeled in
tangent spaces. By exploiting the sparsity and probabilistic structure of
factor graphs, the toolkit achieves real-time performance even for
high-dimensional systems with complex constraints. The velocity-extended
on-manifold control barrier function (CBF)-based obstacle avoidance factors are
designed for safety-critical applications. By bridging graphical models with
safety-critical MPC, our work offers a scalable and geometrically consistent
framework for integrated planning and control. The simulations and experimental
results on the quadrotor demonstrate superior trajectory tracking and obstacle
avoidance performance compared to baseline methods. To foster research
reproducibility, we have provided open-source implementation offering
plug-and-play factors.

</details>


### [206] [Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation](https://arxiv.org/abs/2510.04353)
*Stephen McCrory,Romeo Orsolino,Dhruv Thanki,Luigi Penco,Robert Griffin*

Main category: cs.RO

TL;DR: 提出了一种基于质心稳定性的重定向方法，用于在远程操作中动态调整接触点和姿势，以提高稳定性。


<details>
  <summary>Details</summary>
Motivation: 远程操作在涉及手部接触和非共面表面时容易导致电机扭矩饱和或滑动失稳，因此需要一种方法来增强稳定性。

Method: 通过高效分析计算稳定性裕度梯度，动态调整远程操作的设定点，以提高稳定性。

Result: 在仿真和硬件实验中验证了方法的有效性，展示了更高的稳定性裕度，并证明其与冲击弹性和关节扭矩裕度的相关性。

Conclusion: 该方法显著提高了远程操作中的稳定性，尤其是在复杂接触场景下。

Abstract: Teleoperation is a powerful method to generate reference motions and enable
humanoid robots to perform a broad range of tasks. However, teleoperation
becomes challenging when using hand contacts and non-coplanar surfaces, often
leading to motor torque saturation or loss of stability through slipping. We
propose a centroidal stability-based retargeting method that dynamically
adjusts contact points and posture during teleoperation to enhance stability in
these difficult scenarios. Central to our approach is an efficient analytical
calculation of the stability margin gradient. This gradient is used to identify
scenarios for which stability is highly sensitive to teleoperation setpoints
and inform the local adjustment of these setpoints. We validate the framework
in simulation and hardware by teleoperating manipulation tasks on a humanoid,
demonstrating increased stability margins. We also demonstrate empirically that
higher stability margins correlate with improved impulse resilience and joint
torque margin.

</details>


### [207] [Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators](https://arxiv.org/abs/2510.04354)
*Apurva Badithela,David Snyder,Lihan Zha,Joseph Mikhail,Matthew O'Kelly,Anushri Dixit,Anirudha Majumdar*

Main category: cs.RO

TL;DR: SureSim框架通过结合小规模真实测试和大规模仿真，提供对机器人策略性能的可靠评估，节省了20-25%的硬件评估成本。


<details>
  <summary>Details</summary>
Motivation: 机器人策略的评估通常依赖于少量硬件试验，缺乏统计保证，需要更可靠的评估方法。

Method: 将真实与仿真评估结合为预测驱动的推理问题，利用非渐近均值估计算法提供性能置信区间。

Result: SureSim在物理仿真中评估扩散策略和多任务微调策略，节省了20-25%的硬件评估成本。

Conclusion: SureSim提供了一种高效且可靠的机器人策略性能评估方法，显著减少了硬件测试需求。

Abstract: Rapid progress in imitation learning, foundation models, and large-scale
datasets has led to robot manipulation policies that generalize to a wide-range
of tasks and environments. However, rigorous evaluation of these policies
remains a challenge. Typically in practice, robot policies are often evaluated
on a small number of hardware trials without any statistical assurances. We
present SureSim, a framework to augment large-scale simulation with relatively
small-scale real-world testing to provide reliable inferences on the real-world
performance of a policy. Our key idea is to formalize the problem of combining
real and simulation evaluations as a prediction-powered inference problem, in
which a small number of paired real and simulation evaluations are used to
rectify bias in large-scale simulation. We then leverage non-asymptotic mean
estimation algorithms to provide confidence intervals on mean policy
performance. Using physics-based simulation, we evaluate both diffusion policy
and multi-task fine-tuned \(\pi_0\) on a joint distribution of objects and
initial conditions, and find that our approach saves over \(20-25\%\) of
hardware evaluation effort to achieve similar bounds on policy performance.

</details>


### [208] [PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization](https://arxiv.org/abs/2510.04436)
*Jushan Chen,Santiago Paternain*

Main category: cs.RO

TL;DR: 提出了一种基于扩散模型的直接轨迹优化方法，通过梯度自由投影机制确保动态可行性，显著提高了成功率和动态可行性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在轨迹优化中因多模态概率分布建模能力受到关注，但非线性等式约束（如动态可行性）仍是挑战。现有方法无法显式约束状态，导致次优解。

Method: 提出直接轨迹优化方法，通过模型扩散直接生成状态序列，并在反向扩散过程中引入梯度自由投影机制确保动态可行性。

Result: 在四旋翼导航场景中，动态可行性误差为零，成功率比基线高约4倍。

Conclusion: 该方法有效解决了扩散模型在轨迹优化中的动态可行性问题，显著提升了性能。

Abstract: Recently, diffusion models have gained popularity and attention in trajectory
optimization due to their capability of modeling multi-modal probability
distributions. However, addressing nonlinear equality constraints, i.e, dynamic
feasi- bility, remains a great challenge in diffusion-based trajectory
optimization. Recent diffusion-based trajectory optimization frameworks rely on
a single-shooting style approach where the denoised control sequence is applied
to forward propagate the dynamical system, which cannot explicitly enforce
constraints on the states and frequently leads to sub-optimal solutions. In
this work, we propose a novel direct trajectory optimization approach via
model-based diffusion, which directly generates a sequence of states. To ensure
dynamic feasibility, we propose a gradient-free projection mechanism that is
incorporated into the reverse diffusion process. Our results show that,
compared to a recent state-of-the-art baseline, our approach leads to zero
dynamic feasibility error and approximately 4x higher success rate in a
quadrotor waypoint navigation scenario involving dense static obstacles.

</details>


### [209] [Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads](https://arxiv.org/abs/2510.04509)
*Huanqing Wang,Kaixiang Zhang,Kyungjoon Lee,Yu Mei,Vaibhav Srivastava,Jun Sheng,Ziyou Song,Zhaojian Li*

Main category: cs.RO

TL;DR: 提出了一种基于速度形式的数据驱动预测控制框架，用于在未知负载下实现软机器人的鲁棒和最优控制。


<details>
  <summary>Details</summary>
Motivation: 在物体操作任务中，未知的外部负载和干扰会显著改变系统动态，导致偏移误差和控制性能下降。

Method: 利用增量表示的输入输出数据来减轻未知负载引起的性能下降，无需加权数据集或干扰估计器。

Result: 在平面软机器人上实验验证了该方法，证明其在未知负载场景下优于标准数据驱动预测控制。

Conclusion: 提出的速度形式框架有效解决了未知负载带来的控制挑战，提升了软机器人的鲁棒性和性能。

Abstract: Data-driven control methods such as data-enabled predictive control (DeePC)
have shown strong potential in efficient control of soft robots without
explicit parametric models. However, in object manipulation tasks, unknown
external payloads and disturbances can significantly alter the system dynamics
and behavior, leading to offset error and degraded control performance. In this
paper, we present a novel velocity-form DeePC framework that achieves robust
and optimal control of soft robots under unknown payloads. The proposed
framework leverages input-output data in an incremental representation to
mitigate performance degradation induced by unknown payloads, eliminating the
need for weighted datasets or disturbance estimators. We validate the method
experimentally on a planar soft robot and demonstrate its superior performance
compared to standard DeePC in scenarios involving unknown payloads.

</details>


### [210] [Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation](https://arxiv.org/abs/2510.04585)
*Jianshu Zhou,Jing Shu,Tianle Pan,Puchen Zhu,Jiajun An,Huayu Zhang,Junda Huang,Upinder Kaur,Xin Ma,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: EG Gripper结合表面吸力和颗粒堵塞技术，实现跨尺度和跨状态的物体抓取，无需气密密封。


<details>
  <summary>Details</summary>
Motivation: 解决软机器人中单一夹持器难以同时抓取不同尺寸和物理状态（固体和液体）物体的挑战。

Method: 集成分布式表面吸力和内部颗粒堵塞，结合触觉传感框架和TIGMS算法，自动选择抓取模式。

Result: 能够抓取从0.2 mm²到62,000 mm²的物体，并实时区分固体和液体目标。

Conclusion: EG Gripper是首个能可靠抓取跨尺度固体和液体的软夹持器，展示了统一柔性架构的潜力。

Abstract: Grasping objects across vastly different sizes and physical states-including
both solids and liquids-with a single robotic gripper remains a fundamental
challenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a
soft end-effector that synergistically integrates distributed surface suction
with internal granular jamming, enabling cross-scale and cross-state
manipulation without requiring airtight sealing at the contact interface with
target objects. The EG Gripper can handle objects with surface areas ranging
from sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized
paper and woven bag), enabling manipulation of objects nearly 3,500X smaller
and 88X larger than its own contact area (approximated at 707 mm2 for a 30
mm-diameter base). We further introduce a tactile sensing framework that
combines liquid detection and pressure-based suction feedback, enabling
real-time differentiation between solid and liquid targets. Guided by the
actile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper
autonomously selects grasping modes based on distributed pressure and voltage
signals. Experiments across diverse tasks-including underwater grasping,
fragile object handling, and liquid capture-demonstrate robust and repeatable
performance. To our knowledge, this is the first soft gripper to reliably grasp
both solid and liquid objects across scales using a unified compliant
architecture.

</details>


### [211] [MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation](https://arxiv.org/abs/2510.04592)
*Yilin Mei,Peng Qiu,Wei Zhang,WenChao Zhang,Wenjie Song*

Main category: cs.RO

TL;DR: 论文提出了MobRT框架，用于生成高质量、多样化的移动机械臂演示数据，显著提升了策略的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 移动机械臂在动态、部分可观测环境中的演示数据收集困难，现有研究多集中于简单场景，导致移动操作研究不足。

Method: 通过虚拟运动控制和全身运动规划，MobRT框架自主生成多样且真实的演示数据，涵盖与铰接物体交互和移动基座拾放任务。

Result: 实验表明，MobRT生成的数据质量高，任务成功率与轨迹数量强相关，且在仿真和现实环境中均表现出色。

Conclusion: MobRT有效解决了移动机械臂演示数据不足的问题，为复杂任务提供了可靠的基准和解决方案。

Abstract: Recent advances in robotics have been largely driven by imitation learning,
which depends critically on large-scale, high-quality demonstration data.
However, collecting such data remains a significant challenge-particularly for
mobile manipulators, which must coordinate base locomotion and arm manipulation
in high-dimensional, dynamic, and partially observable environments.
Consequently, most existing research remains focused on simpler tabletop
scenarios, leaving mobile manipulation relatively underexplored. To bridge this
gap, we present \textit{MobRT}, a digital twin-based framework designed to
simulate two primary categories of complex, whole-body tasks: interaction with
articulated objects (e.g., opening doors and drawers) and mobile-base
pick-and-place operations. \textit{MobRT} autonomously generates diverse and
realistic demonstrations through the integration of virtual kinematic control
and whole-body motion planning, enabling coherent and physically consistent
execution. We evaluate the quality of \textit{MobRT}-generated data across
multiple baseline algorithms, establishing a comprehensive benchmark and
demonstrating a strong correlation between task success and the number of
generated trajectories. Experiments integrating both simulated and real-world
demonstrations confirm that our approach markedly improves policy
generalization and performance, achieving robust results in both simulated and
real-world environments.

</details>


### [212] [OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS](https://arxiv.org/abs/2510.04612)
*Simon Boche,Jaehyung Jung,Sebastián Barbas Laina,Stefan Leutenegger*

Main category: cs.RO

TL;DR: OKVIS2-X是一种先进的多传感器SLAM系统，支持实时构建密集体积占用地图，适用于大范围环境，并集成了多种传感器模态。


<details>
  <summary>Details</summary>
Motivation: 为移动机器人提供高精度和鲁棒性的状态估计及可用地图。

Method: 采用统一的SLAM框架，结合视觉、惯性、深度、LiDAR和GNSS测量，使用密集体积地图表示和高效的子地图策略。

Result: 在EuRoC、Hilti22和VBR数据集中表现出最高轨迹精度和竞争力。

Conclusion: OKVIS2-X通过紧密耦合估计器和子地图，提供全局一致的地图，适用于自主导航。

Abstract: To empower mobile robots with usable maps as well as highest state estimation
accuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor
Simultaneous Localization and Mapping (SLAM) system building dense volumetric
occupancy maps, while scalable to large environments and operating in realtime.
Our unified SLAM framework seamlessly integrates different sensor modalities:
visual, inertial, measured or learned depth, LiDAR and Global Navigation
Satellite System (GNSS) measurements. Unlike most state-of-the-art SLAM
systems, we advocate using dense volumetric map representations when leveraging
depth or range-sensing capabilities. We employ an efficient submapping strategy
that allows our system to scale to large environments, showcased in sequences
of up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by
tightly-coupling the estimator and submaps through map alignment factors. Our
system provides globally consistent maps, directly usable for autonomous
navigation. To further improve the accuracy of OKVIS2-X, we also incorporate
the option of performing online calibration of camera extrinsics. Our system
achieves the highest trajectory accuracy in EuRoC against state-of-the-art
alternatives, outperforms all competitors in the Hilti22 VI-only benchmark,
while also proving competitive in the LiDAR version, and showcases state of the
art accuracy in the diverse and large-scale sequences from the VBR dataset.

</details>


### [213] [Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies](https://arxiv.org/abs/2510.04692)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.RO

TL;DR: 研发生物启发的机器人平台，模拟雌性Houbara bustard，用于野外生态研究和保护。


<details>
  <summary>Details</summary>
Motivation: 研究鸟类行为在野外环境中具有挑战性，需要高度逼真的形态、耐用的户外操作和智能感知能力。

Method: 结合3D扫描、CAD建模、3D打印和UV纹理技术，制造逼真的机器人；配备六轮底盘和NVIDIA Jetson模块，实现实时感知和自主视觉伺服。

Result: 野外试验显示机器人能在恶劣条件下稳定运行，并引发真实鸟类的自然反应。

Conclusion: 该平台为动物机器人交互研究和保护机器人提供了可转移的蓝图。

Abstract: Biomimetic intelligence and robotics are transforming field ecology by
enabling lifelike robotic surrogates that interact naturally with animals under
real world conditions. Studying avian behavior in the wild remains challenging
due to the need for highly realistic morphology, durable outdoor operation, and
intelligent perception that can adapt to uncontrolled environments. We present
a next generation bio inspired robotic platform that replicates the morphology
and visual appearance of the female Houbara bustard to support controlled
ethological studies and conservation oriented field research. The system
introduces a fully digitally replicable fabrication workflow that combines high
resolution structured light 3D scanning, parametric CAD modelling, articulated
3D printing, and photorealistic UV textured vinyl finishing to achieve
anatomically accurate and durable robotic surrogates. A six wheeled rocker
bogie chassis ensures stable mobility on sand and irregular terrain, while an
embedded NVIDIA Jetson module enables real time RGB and thermal perception,
lightweight YOLO based detection, and an autonomous visual servoing loop that
aligns the robot's head toward detected targets without human intervention. A
lightweight thermal visible fusion module enhances perception in low light
conditions. Field trials in desert aviaries demonstrated reliable real time
operation at 15 to 22 FPS with latency under 100 ms and confirmed that the
platform elicits natural recognition and interactive responses from live
Houbara bustards under harsh outdoor conditions. This integrated framework
advances biomimetic field robotics by uniting reproducible digital fabrication,
embodied visual intelligence, and ecological validation, providing a
transferable blueprint for animal robot interaction research, conservation
robotics, and public engagement.

</details>


### [214] [Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly](https://arxiv.org/abs/2510.04696)
*Alexander L. Mitchell,Joe Watson,Ingmar Posner*

Main category: cs.RO

TL;DR: 提出了一种基于梯度的分散式框架，用于解决双手机器人装配中的快速重规划和自适应任务序列问题。


<details>
  <summary>Details</summary>
Motivation: 双手机器人装配面临高难度任务序列、多机器人协调和接触密集操作等挑战，传统TAMP方法在应对扰动时收敛速度慢且任务序列定义繁琐。

Method: 采用分散式梯度框架，通过自适应势函数的自动组合生成分段连续能量函数，仅需短视优化即可生成子目标。

Result: 该方法在物理双手机器人装配任务中表现出色，能够自动生成重试、协调运动和自主交接。

Conclusion: 该框架通过能量函数的结构和自适应性，有效解决了长时程任务规划问题，适用于高精度装配场景。

Abstract: There are many challenges in bimanual assembly, including high-level
sequencing, multi-robot coordination, and low-level, contact-rich operations
such as component mating. Task and motion planning (TAMP) methods, while
effective in this domain, may be prohibitively slow to converge when adapting
to disturbances that require new task sequencing and optimisation. These events
are common during tight-tolerance assembly, where difficult-to-model dynamics
such as friction or deformation require rapid replanning and reattempts.
Moreover, defining explicit task sequences for assembly can be cumbersome,
limiting flexibility when task replanning is required. To simplify this
planning, we introduce a decentralised gradient-based framework that uses a
piecewise continuous energy function through the automatic composition of
adaptive potential functions. This approach generates sub-goals using only
myopic optimisation, rather than long-horizon planning. It demonstrates
effectiveness at solving long-horizon tasks due to the structure and adaptivity
of the energy function. We show that our approach scales to physical bimanual
assembly tasks for constructing tight-tolerance assemblies. In these
experiments, we discover that our gradient-based rapid replanning framework
generates automatic retries, coordinated motions and autonomous handovers in an
emergent fashion.

</details>


### [215] [Performance-guided Task-specific Optimization for Multirotor Design](https://arxiv.org/abs/2510.04724)
*Etor Arza,Welf Rehberg,Philipp Weiss,Mihir Kulkarni,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习、贝叶斯优化和协方差矩阵自适应进化策略的多旋翼微型飞行器任务特定设计优化方法。


<details>
  <summary>Details</summary>
Motivation: 通过闭环性能优化飞行器设计，提升任务表现。

Method: 结合强化学习、贝叶斯优化和协方差矩阵自适应进化策略，优化电机姿态配置设计空间。

Result: 优化设计在敏捷航点导航任务中表现优于传统多旋翼配置，甚至优于文献中的全驱动设计。

Conclusion: 通过实际测试验证了方法的仿真到现实的迁移能力。

Abstract: This paper introduces a methodology for task-specific design optimization of
multirotor Micro Aerial Vehicles. By leveraging reinforcement learning,
Bayesian optimization, and covariance matrix adaptation evolution strategy, we
optimize aerial robot designs guided exclusively by their closed-loop
performance in a considered task. Our approach systematically explores the
design space of motor pose configurations while ensuring manufacturability
constraints and minimal aerodynamic interference. Results demonstrate that
optimized designs achieve superior performance compared to conventional
multirotor configurations in agile waypoint navigation tasks, including against
fully actuated designs from the literature. We build and test one of the
optimized designs in the real world to validate the sim2real transferability of
our approach.

</details>


### [216] [Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy](https://arxiv.org/abs/2510.04774)
*Weixu Zhu,Marco Dorigo,Mary Katherine Heinrich*

Main category: cs.RO

TL;DR: SoNS为机器人群体提供行为设计便利和全局配置估计，支持在线自动代码生成，实验显示85%的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人群体在任务中卡住时无法自动调整的问题。

Method: 使用SoNS增强的机器人群体，结合外部LLM实时生成代码。

Result: 在6个真实机器人和30+仿真机器人中，任务成功率达85%。

Conclusion: SoNS能有效提升机器人群体在复杂任务中的自主性和成功率。

Abstract: Our recently introduced self-organizing nervous system (SoNS) provides robot
swarms with 1) ease of behavior design and 2) global estimation of the swarm
configuration and its collective environment, facilitating the implementation
of online automatic code generation for robot swarms. In a demonstration with 6
real robots and simulation trials with >30 robots, we show that when a
SoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code
generated by an external LLM on the fly, completing its mission with an 85%
success rate.

</details>


### [217] [TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation](https://arxiv.org/abs/2510.04839)
*Shuo Sha,Anupam Bhakta,Zhenyuan Jiang,Kevin Qiu,Ishaan Mahajan,Gabriel Bravo,Brian Plancher*

Main category: cs.RO

TL;DR: TAG-K是一种轻量级的Kaczmarz方法扩展，用于在线惯性参数估计，比传统方法更快、更稳定。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如RLS和KF）在动态环境中难以跟踪参数突变或计算成本高，限制了其适应性。

Method: 结合贪婪随机行选择和尾部平均，TAG-K实现了快速收敛和噪声下的鲁棒性。

Result: 在合成基准和四旋翼跟踪任务中，TAG-K比RLS和KF快1.5x-20.7x，估计误差降低25%。

Conclusion: TAG-K在计算效率和鲁棒性上优于传统方法，适用于资源受限的机器人系统。

Abstract: Accurate online inertial parameter estimation is essential for adaptive
robotic control, enabling real-time adjustment to payload changes,
environmental interactions, and system wear. Traditional methods such as
Recursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to
track abrupt parameter shifts or incur high computational costs, limiting their
effectiveness in dynamic environments and for computationally constrained
robotic systems. As such, we introduce TAG-K, a lightweight extension of the
Kaczmarz method that combines greedy randomized row selection for rapid
convergence with tail averaging for robustness under noise and inconsistency.
This design enables fast, stable parameter adaptation while retaining the low
per-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K
in synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other
Kaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class
CPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More
importantly, these speedups are paired with improved resilience to measurement
noise and a 25% reduction in estimation error, leading to nearly 2x better
end-to-end tracking performance.

</details>


### [218] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: 提出了一种基于U-Net的方法，通过红外流在黑暗环境中实现鲁棒的机器人感知，解决了主动发射器模式干扰的问题。


<details>
  <summary>Details</summary>
Motivation: 红外流在低光条件下比RGB更抗噪，但主动发射器模式会干扰高级任务（如目标检测和跟踪）。

Method: 采用U-Net架构从含发射器的输入中重建干净的IR图像。

Result: 该方法优于现有增强技术，能在从明亮到极低光条件下可靠运行。

Conclusion: 该方法显著提升了图像质量和机器人视觉系统的性能。

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [219] [HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks](https://arxiv.org/abs/2510.04898)
*Zheng Xiong,Kang Li,Zilin Wang,Matthew Jackson,Jakob Foerster,Shimon Whiteson*

Main category: cs.RO

TL;DR: HyperVLA是一种基于超网络的视觉-语言-动作模型，通过激活小型任务特定策略降低推理成本，同时保持多任务训练能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型推理成本过高，HyperVLA旨在解决这一问题。

Method: 采用超网络架构，仅激活任务特定策略，结合先验知识、归一化和动作生成策略。

Result: 在零样本和少样本任务中表现优于或接近现有VLA，推理成本显著降低。

Conclusion: HyperVLA在高效推理和多任务能力上取得平衡，是VLA模型的重大改进。

Abstract: Built upon language and vision foundation models with strong generalization
ability and trained on large-scale robotic data, Vision-Language-Action (VLA)
models have recently emerged as a promising approach to learning generalist
robotic policies. However, a key drawback of existing VLAs is their extremely
high inference costs. In this paper, we propose HyperVLA to address this
problem. Unlike existing monolithic VLAs that activate the whole model during
both training and inference, HyperVLA uses a novel hypernetwork (HN)-based
architecture that activates only a small task-specific policy during inference,
while still retaining the high model capacity needed to accommodate diverse
multi-task behaviors during training. Successfully training an HN-based VLA is
nontrivial so HyperVLA contains several key algorithm design features that
improve its performance, including properly utilizing the prior knowledge from
existing vision foundation models, HN normalization, and an action generation
strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even
higher success rate for both zero-shot generalization and few-shot adaptation,
while significantly reducing inference costs. Compared to OpenVLA, a
state-of-the-art VLA model, HyperVLA reduces the number of activated parameters
at test time by $90\times$, and accelerates inference speed by $120\times$.
Code is publicly available at https://github.com/MasterXiong/HyperVLA

</details>


### [220] [Efficient Navigation in Unknown Indoor Environments with Vision-Language Models](https://arxiv.org/abs/2510.04991)
*D. Schwartz,K. Kondo,J. P. How*

Main category: cs.RO

TL;DR: 提出了一种利用视觉语言模型（VLM）的高层规划框架，提升未知室内环境中的自主导航效率。


<details>
  <summary>Details</summary>
Motivation: 传统探索方法因全局推理能力有限和依赖局部启发式，常导致低效路径。

Method: 将3D占据栅格转换为部分2D地图，生成候选子目标，并由VLM评估和排序。

Result: 集成到DYNUS轨迹规划器中，路径平均缩短10%，减少贪婪失败。

Conclusion: VLM能从未完整地图推断结构模式，平衡目标进展与未知空间风险，提升导航效率。

Abstract: We present a novel high-level planning framework that leverages
vision-language models (VLMs) to improve autonomous navigation in unknown
indoor environments with many dead ends. Traditional exploration methods often
take inefficient routes due to limited global reasoning and reliance on local
heuristics. In contrast, our approach enables a VLM to reason directly about an
occupancy map in a zero-shot manner, selecting subgoals that are likely to lead
to more efficient paths. At each planning step, we convert a 3D occupancy grid
into a partial 2D map of the environment, and generate candidate subgoals. Each
subgoal is then evaluated and ranked against other candidates by the model. We
integrate this planning scheme into DYNUS \cite{kondo2025dynus}, a
state-of-the-art trajectory planner, and demonstrate improved navigation
efficiency in simulation. The VLM infers structural patterns (e.g., rooms,
corridors) from incomplete maps and balances the need to make progress toward a
goal against the risk of entering unknown space. This reduces common greedy
failures (e.g., detouring into small rooms) and achieves about 10\% shorter
paths on average.

</details>


### [221] [Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot](https://arxiv.org/abs/2510.05001)
*Aditya Sripada,Abhishek Warrier*

Main category: cs.RO

TL;DR: TARS3D机器人通过仿生设计和强化学习实现了多种运动模式，包括行走和滚动。


<details>
  <summary>Details</summary>
Motivation: 探索非仿生形态机器人在工程应用中的潜力，特别是从科幻电影中获取灵感。

Method: 结合解析建模和深度强化学习（DRL）开发机器人的运动模式。

Result: 机器人成功实现了行走和滚动两种运动模式，并通过DRL发现了新行为。

Conclusion: TARS3D展示了非仿生形态的多模态运动潜力，结合解析与学习方法为机器人研究开辟了新途径。

Abstract: Robotic locomotion research typically draws from biologically inspired leg
designs, yet many human-engineered settings can benefit from
non-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from
Interstellar into a 0.25 m, 0.99 kg research platform with seven actuated
degrees of freedom. The film shows two primary gaits: a bipedal-like walk and a
high-speed rolling mode. For TARS3D, we build reduced-order models for each,
derive closed-form limit-cycle conditions, and validate the predictions on
hardware. Experiments confirm that the robot respects its +/-150 degree hip
limits, alternates left-right contacts without interference, and maintains an
eight-step hybrid limit cycle in rolling mode. Because each telescopic leg
provides four contact corners, the rolling gait is modeled as an eight-spoke
double rimless wheel. The robot's telescopic leg redundancy implies a far
richer gait repertoire than the two limit cycles treated analytically. So, we
used deep reinforcement learning (DRL) in simulation to search the unexplored
space. We observed that the learned policy can recover the analytic gaits under
the right priors and discover novel behaviors as well. Our findings show that
TARS3D's fiction-inspired bio-transcending morphology can realize multiple
previously unexplored locomotion modes and that further learning-driven search
is likely to reveal more. This combination of analytic synthesis and
reinforcement learning opens a promising pathway for multimodal robotics.

</details>


### [222] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 提出了一种无监督方法StaMo，通过轻量级编码器和预训练Diffusion Transformer解码器学习高度压缩的双令牌状态表示，显著提升了任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在状态表示上难以平衡表达性和紧凑性，导致冗余或信息不足。

Method: 使用轻量级编码器和预训练DiT解码器学习压缩的双令牌表示，并通过潜在插值生成有效潜在动作。

Result: 在LIBERO上性能提升14.3%，真实任务成功率提升30%，潜在动作还提升了策略协同训练性能10.4%。

Conclusion: StaMo方法通过静态图像学习通用运动表示，减少了对复杂架构和视频数据的依赖，具有高效、可解释和可扩展性。

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


### [223] [Automaton Constrained Q-Learning](https://arxiv.org/abs/2510.05061)
*Anastasios Manganaris,Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: ACQL结合目标条件值学习和自动机引导强化学习，解决了复杂连续环境中时间逻辑任务和安全约束的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实机器人任务需要满足时间变化的安全约束和顺序目标，但现有RL方法表现不佳。

Method: 提出ACQL算法，结合目标条件值学习和自动机表示，支持LTL任务规范和动态安全约束。

Result: ACQL在连续控制任务中优于现有方法，并在真实机器人实验中验证了其有效性。

Conclusion: ACQL是学习符合丰富时间规范的机器人行为的鲁棒且可扩展的解决方案。

Abstract: Real-world robotic tasks often require agents to achieve sequences of goals
while respecting time-varying safety constraints. However, standard
Reinforcement Learning (RL) paradigms are fundamentally limited in these
settings. A natural approach to these problems is to combine RL with
Linear-time Temporal Logic (LTL), a formal language for specifying complex,
temporally extended tasks and safety constraints. Yet, existing RL methods for
LTL objectives exhibit poor empirical performance in complex and continuous
environments. As a result, no scalable methods support both temporally ordered
goals and safety simultaneously, making them ill-suited for realistic robotics
scenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm
that addresses this gap by combining goal-conditioned value learning with
automaton-guided reinforcement. ACQL supports most LTL task specifications and
leverages their automaton representation to explicitly encode stage-wise goal
progression and both stationary and non-stationary safety constraints. We show
that ACQL outperforms existing methods across a range of continuous control
tasks, including cases where prior methods fail to satisfy either goal-reaching
or safety constraints. We further validate its real-world applicability by
deploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a
cluttered, cabinet-like space with safety constraints. Our results demonstrate
that ACQL is a robust and scalable solution for learning robotic behaviors
according to rich temporal specifications.

</details>


### [224] [ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning](https://arxiv.org/abs/2510.05070)
*Siheng Zhao,Yanjie Ze,Yue Wang,C. Karen Liu,Pieter Abbeel,Guanya Shi,Rocky Duan*

Main category: cs.RO

TL;DR: ResMimic是一个两阶段残差学习框架，用于从人类运动数据中实现精确且富有表现力的人形机器人控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有通用运动跟踪（GMT）策略在精确性和对象感知方面的不足，以支持人形机器人的全身运动和物体交互。

Method: 结合GMT策略和残差策略，设计点云对象跟踪奖励、接触奖励和基于课程的虚拟对象控制器。

Result: 在仿真和真实机器人上验证，显著提高了任务成功率、训练效率和鲁棒性。

Conclusion: ResMimic为精确的人形机器人控制提供了有效解决方案，适用于日常服务和仓库任务。

Abstract: Humanoid whole-body loco-manipulation promises transformative capabilities
for daily service and warehouse tasks. While recent advances in general motion
tracking (GMT) have enabled humanoids to reproduce diverse human motions, these
policies lack the precision and object awareness required for
loco-manipulation. To this end, we introduce ResMimic, a two-stage residual
learning framework for precise and expressive humanoid control from human
motion data. First, a GMT policy, trained on large-scale human-only motion,
serves as a task-agnostic base for generating human-like whole-body movements.
An efficient but precise residual policy is then learned to refine the GMT
outputs to improve locomotion and incorporate object interaction. To further
facilitate efficient training, we design (i) a point-cloud-based object
tracking reward for smoother optimization, (ii) a contact reward that
encourages accurate humanoid body-object interactions, and (iii) a
curriculum-based virtual object controller to stabilize early training. We
evaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results
show substantial gains in task success, training efficiency, and robustness
over strong baselines. Videos are available at https://resmimic.github.io/ .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [225] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS是一种基于提示感知的LLM任务调度器，通过近似最短作业优先调度提高效率，减少延迟。


<details>
  <summary>Details</summary>
Motivation: 传统调度策略如FCFS存在HOL阻塞问题，长任务会延迟短任务，影响效率。

Method: PARS使用成对排序和边际排序损失近似SJF调度，集成到vLLM系统中。

Result: 实验表明PARS显著提升性能，包括推理任务，且设计具有泛化性。

Conclusion: PARS有效解决了LLM推理任务调度问题，提升了效率和性能。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [226] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一种跨模态预测模型，通过将时间序列转换为图像，利用预训练的大型视觉模型提取跨通道模式，并与时间序列模态特征融合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大时间序列基础模型通常采用通道独立架构，忽略了跨通道依赖关系，且多模态方法未充分利用大型视觉模型的潜力。

Method: VIFO将多元时间序列渲染为图像，利用预训练的大型视觉模型提取跨通道模式，并与时间序列模态特征对齐和融合。

Result: 仅训练7.45%的参数，VIFO在多个基准测试中表现出色。

Conclusion: VIFO提供了一种高效且有效的方法来捕捉跨变量关系，提升了时间序列预测性能。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [227] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出了一种新型可转移对抗攻击（频率感知攻击）和改进的归因方法FAMPE，显著提升了深度神经网络的解释性。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在真实噪声和对抗扰动下的可靠性问题，现有归因方法效果不佳。

Method: 提出频率感知攻击和FAMPE归因方法，通过高低频组件探索模型参数。

Result: FAMPE在Insertion Score上平均提升13.02%，优于现有方法。

Conclusion: 频率感知攻击和FAMPE有效提升模型解释性，高低频组件对解释性有重要作用。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [228] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: STRUPRUNE是一种结合结构化剪枝和局部剪枝的方法，通过分治策略降低内存需求，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 全局剪枝性能强但内存需求高，局部剪枝内存效率高但性能较差，结构化剪枝硬件效率高但依赖全局剪枝。

Method: 采用分治策略和ADMM框架，设计结构化剪枝掩码和层间稀疏分配方案。

Result: STRUPRUNE在保持性能的同时，将内存成本从O(N)降至O(√N)。

Conclusion: STRUPRUNE在十亿参数规模下实现了高效部署。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [229] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 提出首个多模态主动学习框架，解决未对齐数据的跨模态对齐问题，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习算法主要针对单模态数据，忽视了多模态学习中的高标注成本问题。

Method: 结合不确定性和多样性原则，设计模态感知算法，支持线性时间获取，适用于池式和流式场景。

Result: 在基准数据集上，方法显著减少多模态标注成本（如ColorSwap数据集标注需求降低40%），同时保持性能。

Conclusion: 该框架有效解决了多模态学习中的标注瓶颈，为现代多模态管道（如CLIP、SigLIP）提供了实用解决方案。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [230] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 该研究评估了四种神经算子架构，用于快速预测脑位移场，以支持创伤性脑损伤（TBI）的实时建模。


<details>
  <summary>Details</summary>
Motivation: 传统的有限元模型计算成本高，限制了其在临床快速决策中的应用。神经算子（NO）提供了一种高效且分辨率不变的方法来预测脑变形。

Method: 研究将TBI建模为算子学习问题，使用四种架构（FNO、F-FNO、MG-FNO和DeepONet）在249个MRE数据集上进行训练和评估。

Result: MG-FNO精度最高，DeepONet推理速度最快，所有NO将计算时间从小时级缩短到毫秒级。

Conclusion: NO为实时、个性化的TBI风险评估和临床决策提供了高效工具，展示了基于NO的数字孪生脑模型的潜力。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [231] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文提出了一种重新参数化方法，解决了可微分逻辑门网络（DLGNs）的梯度消失、离散化误差和高训练成本问题，同时减小了参数规模并提升了训练效率。


<details>
  <summary>Details</summary>
Motivation: DLGNs在推理效率上表现优异，但梯度消失、离散化误差和高训练成本限制了其扩展性。即使有专门的参数初始化方案，增加深度仍会损害准确性。

Method: 通过重新参数化逻辑门神经元，减少每个门的输入参数规模，从而提升效率和训练速度。

Result: 在二进制输入下，模型规模减小4倍，反向传播速度提升1.86倍，训练步数减少8.5倍，且在CIFAR-100上准确性稳定或更优。

Conclusion: 重新参数化方法有效解决了DLGNs的扩展性问题，同时提升了效率和准确性。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [232] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: Numerion是一种基于超复数空间的时间序列预测模型，通过多维度RHR-MLP架构自然分解时间序列并自适应融合特征，在多个公开数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过复杂模型结构和先验知识分解时间序列，但受限于计算复杂性和假设的鲁棒性。研究发现，在超复数空间中，时间序列的特征频率自然降低。

Method: 提出Numerion模型，将线性层和激活函数推广到任意2的幂次维度的超复数空间，并设计RHR-MLP架构。通过多个RHR-MLP映射时间序列到不同维度的超复数空间，自然分解并独立建模，最后动态融合不同空间的潜在模式。

Result: 实验验证模型性能，在多个公开数据集上达到最优结果。可视化与定量分析展示了多维RHR-MLP自然分解时间序列的能力，以及高维超复数空间捕获低频特征的趋势。

Conclusion: Numerion通过超复数空间和多维度RHR-MLP架构有效解决了时间序列分解和建模的挑战，展示了频率特征与空间维度的关联性。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [233] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 提出了一种通用的多域翻译框架UMDT，通过Diffusion Router（DR）实现任意域对之间的翻译，仅需K-1个配对数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要完全对齐的数据或仅能处理训练中见过的域对，限制了实用性和跨域映射范围。

Method: 提出DR框架，基于扩散模型，通过中心域路由实现间接翻译，并引入变分目标学习和Tweedie细化支持直接翻译。

Result: 在三个大规模UMDT基准测试中，DR在间接和直接翻译上均达到最优，且降低了采样成本。

Conclusion: DR是一个可扩展且通用的多域翻译框架，支持跨域任务如草图与分割的转换。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [234] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: HPL是一种分层偏好学习框架，通过多粒度偏好信号优化LLM代理，解决了轨迹级和步骤级DPO的粒度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在复杂任务中偏好学习的粒度不匹配问题，提升多步行为的优化效果。

Method: 引入分层偏好学习（HPL），结合轨迹级和步骤级DPO，并通过双课程调度器优化子任务级偏好学习。

Result: 在三个基准测试中，HPL表现优于现有方法，有效整合多粒度偏好信号。

Conclusion: HPL通过分层损失和双课程调度，显著提升了LLM代理在复杂任务中的性能。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [235] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 提出了一种受限的悲观双层优化模型，通过限制对手的行为，生成更符合现实的对抗样本，从而提高分类器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有悲观双层优化方法中对手不受限制，可能导致对抗样本不切实际，影响分类器在真实数据上的性能。

Method: 构建了受限的悲观双层优化模型，限制对手的行为，使其生成的对抗样本更合理。

Result: 实验表明，该模型在性能上优于现有方法。

Conclusion: 受限的悲观双层优化模型能更有效地提升分类器在对抗环境中的鲁棒性。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [236] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: 论文提出了SciTS基准和TimeOmni框架，以解决大语言模型在科学时间序列理解和生成中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在处理科学时间序列时存在局限性，如将数值序列编码为文本或图像，导致信息丢失或性能受限。

Method: 引入SciTS基准（涵盖12个科学领域和43个任务）和TimeOmni框架，使大语言模型能够理解和生成时间序列。

Result: 实验表明，通用大语言模型比专用时间序列模型更具泛化能力，但将时间序列表示为文本或图像会限制性能。

Conclusion: 该工作填补了科学时间序列专用基准和建模框架的空白，为大语言模型处理复杂科学数据奠定了基础。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [237] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: Triple-BERT是一种基于单智能体强化学习的方法，用于解决网约车平台的大规模订单调度问题，通过动作分解和BERT网络优化，显著提升了服务效率和响应速度。


<details>
  <summary>Details</summary>
Motivation: 网约车平台面临实时订单调度的高维观察空间和动作空间挑战，现有多智能体强化学习方法难以有效处理全局信息和协作问题。

Method: 提出Triple-BERT方法，结合TD3变体和BERT网络，通过动作分解策略和参数共享机制处理高维空间，并利用注意力机制捕捉复杂关系。

Result: 在曼哈顿真实数据集上，Triple-BERT比现有方法提升11.95%，订单服务量增加4.26%，接客时间减少22.25%。

Conclusion: Triple-BERT通过集中式单智能体强化学习有效解决了大规模订单调度问题，显著提升了平台效率。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [238] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: POEM是一种新的测试时适应方法，通过探索未充分利用的可靠样本提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法依赖熵作为置信度指标，但预定义的熵阈值可能导致可靠样本被忽略，影响模型适应效果。

Method: 提出POEM方法，引入Adapt Branch网络，平衡领域无关表示提取和目标数据性能。

Result: POEM在多种架构和挑战性场景中表现优于现有TTA方法，且计算高效。

Conclusion: POEM不仅提升TTA性能，还可作为增强策略应用于现有方法。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [239] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 论文提出了一种通过自我对齐提升语言模型元认知能力的方法（MASA），显著提高了推理任务的准确性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型缺乏元认知能力，即无法自我监控推理过程，导致预测与真实推理轨迹不一致。作者认为对齐元预测与真实推理轨迹能显著提升性能。

Method: 设计了MASA训练框架，通过自我生成的信号训练元认知能力，无需外部数据。方法还通过过滤无效提示和截断低效推理路径提升训练效率。

Result: 在多个任务上取得显著提升：训练速度提升1.28倍，AIME25准确率提升19.3%，数学基准平均提升6.2%。在跨领域任务中也表现出色，如GPQA-Diamond提升3.87%。

Conclusion: 增强元认知能力不仅能提高模型在特定任务上的表现，还能显著改善跨领域泛化能力，为推理模型的自我监控机制提供了新思路。

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [240] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 论文提出了一种分区方案来评估属性相关性，并研究了两种互补的特征选择策略，以提升零样本学习在未见类上的性能。


<details>
  <summary>Details</summary>
Motivation: 语义空间中常存在噪声、冗余或不相关属性，影响零样本学习性能，需要一种方法来优化这些语义空间。

Method: 引入分区方案模拟未见条件，并研究两种特征选择策略：基于嵌入的特征选择和进化计算。

Result: 在五个基准数据集上，两种方法均通过减少冗余提升了未见类的准确率，但各有优劣。

Conclusion: 语义空间存在固有冗余，提出的分区方案是优化语义空间的有效工具。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [241] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 提出一种新方法，利用神经网络预测机床内的温度和热通量场，实现灵活通用的热误差校正。


<details>
  <summary>Details</summary>
Motivation: 传统热误差校正方法依赖特定误差类型或配置，缺乏通用性和适应性。

Method: 训练神经网络预测高保真温度和热通量场，结合下游模块化组件进行误差计算和校正。

Result: 实验表明，该方法能准确低成本预测温度和热通量场，支持灵活通用的热误差校正。

Conclusion: 该方法为机床环境中的热误差校正提供了灵活通用的解决方案。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [242] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡洛Dropout方法，确保合并LoRA时语义向量正交，但发现正交性不足以实现语义解耦或组合性。


<details>
  <summary>Details</summary>
Motivation: 解决多个LoRA合并时语义向量干扰的问题，探索正交性对语义组合性的实际作用。

Method: 提出正交蒙特卡洛Dropout，理论保证合并LoRA时的正交性，无额外时间复杂度。

Result: 实证分析显示正交性未能实现语义解耦或组合性。

Conclusion: 正交性单独不足以实现语义组合性，需重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [243] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 论文探讨了文本到图像模型的知识遗忘问题，提出了Memory Self-Regeneration任务和MemoRa策略，强调知识检索的鲁棒性，并区分了短期和长期遗忘。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型可能被滥用生成有害内容，需要选择性遗忘特定知识而不影响整体性能。

Method: 提出Memory Self-Regeneration任务和MemoRa策略，支持有效恢复遗忘知识。

Result: 遗忘分为短期和长期两种方式，知识检索的鲁棒性是关键评估指标。

Conclusion: MemoRa策略和知识检索鲁棒性研究为开发更有效的遗忘技术提供了新方向。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [244] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 研究表明，在预训练阶段引入推理数据比后训练阶段更有效，能带来19%的平均性能提升。预训练需要多样化的推理模式，而后训练更依赖数据质量。


<details>
  <summary>Details</summary>
Motivation: 探讨推理数据在预训练和后训练阶段的不同作用，以优化LLM的训练策略。

Method: 系统研究推理数据在不同训练阶段（预训练和后训练）的引入对LLM性能的影响，包括数据规模、多样性和质量。

Result: 预训练阶段引入推理数据效果更佳（19%提升），预训练依赖多样性（11%提升），后训练依赖质量（15%提升）。过早扩展后训练数据可能抵消早期推理数据的优势。

Conclusion: 研究挑战了传统语言建模与推理的分离，为数据在训练流程中的优化分配提供了指导。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [245] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: MindCraft框架通过概念树揭示大模型内部概念结构，展示概念如何从共享表示中分离为线性可分子空间。


<details>
  <summary>Details</summary>
Motivation: 探索大模型内部概念的结构和稳定性，以提升可解释性。

Method: 使用概念树和谱分解技术，构建层次化概念路径。

Result: 在多个领域（如医疗诊断、物理推理等）验证了概念树的有效性和广泛适用性。

Conclusion: 概念树为深度模型的概念表示提供了强大且通用的分析框架，推动了可解释AI的发展。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [246] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种基于变分自编码器（VAE）的新方法，用于检测和分析植物生产力中的极端事件，并与传统奇异谱分析（SSA）方法进行了比较。


<details>
  <summary>Details</summary>
Motivation: 气候异常对陆地碳循环动态有显著影响，需要开发更强大的方法来检测和分析植物生产力的异常行为。

Method: 使用VAE架构（包含三个密集层和潜在空间）对归一化的GPP时间序列进行训练，通过重构误差识别异常事件，并与SSA方法在三个时间段（1850-80、1950-80、2050-80）进行比较。

Result: VAE与SSA在极端事件频率的空间分布上表现一致，但VAE的阈值更高。两种方法均显示2050-80年负碳循环极端事件的强度和频率增加。

Conclusion: VAE方法在性能上与SSA相当，但具有计算优势，并能更好地捕捉碳循环的非线性时间依赖性。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [247] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT$^2$-LLM是一种针对大语言模型的后训练三值化框架，通过两阶段优化和三值量化器，显著降低内存和计算需求。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然性能强大，但内存和计算需求高，限制了部署。三值化作为一种压缩技术潜力巨大，但在后训练量化（PTQ）中尚未充分探索。

Method: 提出PT$^2$-LLM框架，包括非对称三值量化器和两阶段优化：迭代三值拟合（ITF）和激活感知网格对齐（AGA）。此外，引入基于结构相似性的重排序（SSR）策略。

Result: 实验表明，PT$^2$-LLM在性能上优于现有2位PTQ方法，内存成本更低，并加速了预填充和解码过程。

Conclusion: PT$^2$-LLM为LLMs的高效部署提供了一种有效的后训练三值化解决方案。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [248] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 论文提出了一个理论框架分析多模态对比学习中的模态间隙，证明维度崩溃是模态间隙的根本原因，并探讨了其对下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 多模态对比学习（MCL）中，不同模态的表示在嵌入空间中占据完全分离的区域（模态间隙），且其对下游性能的影响不一致。本文旨在揭示模态间隙的成因及其对任务的影响。

Method: 通过理论分析，研究了在无约束、锥约束和子空间约束下模态间隙的收敛行为，并提出了两种实现完美对齐的方法：超平面旋转和共享空间投影。

Result: 证明维度崩溃是模态间隙的根本原因；在子空间约束下，模态间隙收敛为两个超平面间的最小夹角；完美对齐可通过超平面旋转或共享空间投影实现。

Conclusion: 模态间隙由维度崩溃引起，影响样本对的对齐；通过理论方法可实现完美对齐，为多模态学习提供了新的优化方向。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [249] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 论文提出了一种新的探索奖励框架GEB，解决了现有方法在强化学习中对高概率区域的偏好问题，并通过理论和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有探索奖励方法在KL或α-发散正则化下偏向高概率区域，导致保守行为，而非探索不确定区域。

Method: 提出通用探索奖励（GEB），通过参考依赖的奖励调节抵消发散诱导的偏差，统一了先前的启发式奖励。

Result: GEB在多个发散设置和大语言模型上优于基线方法。

Conclusion: GEB为RLHF中的乐观探索提供了理论和实践上的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [250] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的扩散编码器，通过大规模扩散预训练和指令调优，在代码生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有双向上下文和填充能力，但现有系统通常计算量大。CoDA旨在提供轻量级且高效的解决方案。

Method: 结合大规模扩散预训练、代码中心的中期训练和指令调优，采用置信度引导采样降低推理延迟。

Result: 在Humaneval、MBPP和EvalPlus基准测试中，CoDA-1.7B-Instruct性能优于或匹配更大规模的扩散模型。

Conclusion: CoDA为轻量级扩散编码助手的研究提供了模型、评估工具和训练流程，推动了该领域的发展。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [251] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 论文提出了一种名为决策势面（DPS）的新概念，用于分析大型语言模型（LLM）的决策边界，并通过K-DPS算法近似构造决策边界。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的词汇序列规模巨大且自回归特性，直接构造其决策边界计算上不可行，因此需要一种高效的分析方法。

Method: 定义DPS，通过采样序列的置信度捕捉决策边界潜力，提出K-DPS算法近似构造决策边界。

Result: 理论推导了K-DPS与理想DPS的误差上界，实验验证了方法的有效性。

Conclusion: DPS和K-DPS为LLM决策边界分析提供了可行且高效的工具。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [252] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种将Transformer架构视为连续时空动力系统的新分析框架，揭示了其核心组件的数学必要性。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在人工智能领域取得了巨大成功，但其内部机制缺乏理论理解。本文旨在通过连续动力系统理论填补这一空白。

Method: 将Transformer的离散结构映射为连续PDE系统，核心组件对应数学算子，并通过实验验证其稳定性作用。

Result: 实验表明，残差连接和层归一化是稳定系统的关键，缺乏它们会导致训练不稳定或表征漂移。

Conclusion: Transformer的设计组件是数学上必要的稳定器，为理解神经网络提供了新的连续动力学视角。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [253] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: Synergistic Information Distillation (SID) 是一种新的训练框架，通过解耦模块间的反向依赖，解决了反向传播的更新锁定和高内存消耗问题，同时保持了前向推理的兼容性。


<details>
  <summary>Details</summary>
Motivation: 反向传播（BP）存在更新锁定和高内存消耗的瓶颈，限制了深度学习的可扩展性。

Method: SID 将深度学习重构为局部协同优化问题，模块通过局部目标优化概率信念，解耦反向依赖以实现并行训练。

Result: SID 在分类任务中表现优于或等同于 BP，具有更好的可扩展性和对标签噪声的鲁棒性。

Conclusion: SID 是一种高效且兼容的 BP 替代方案，理论证明其性能随网络深度单调提升。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [254] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: Quant-dLLM是一种针对扩散大语言模型（dLLMs）的超低比特后训练量化框架，通过掩码校准模拟和数据感知量化器提升2比特量化性能。


<details>
  <summary>Details</summary>
Motivation: dLLMs的模型规模不断增长，需要压缩权重以部署，但现有的后训练量化方法在2比特下表现不佳。

Method: 提出掩码校准模拟（MCS）和数据感知量化器（DAQ），并结合自适应块混合精度（ABMP）分配比特宽度。

Result: 在2比特限制下，Quant-dLLM比现有AR迁移PTQ方法在dLLMs上表现更优。

Conclusion: Quant-dLLM为dLLMs提供了一种高效的超低比特量化解决方案。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [255] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一种新型框架，通过Sigma-Delta量化和过采样比（OSR）动态调整，实现1-bit或1.58-bit的极低比特量化，提升大语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）面临计算和内存挑战，极低比特量化对其高效部署至关重要。

Method: 采用Sigma-Delta量化器和上采样技术，结合Hadamard权重平滑和MultiOSR策略，动态分配OSR以减少量化精度损失。

Result: 在OPT和LLaMA模型家族上的实验表明，SDQ-LLM在极低OSR设置下仍能实现高效和高精度性能。

Conclusion: SDQ-LLM通过创新的量化方法和动态OSR分配，显著提升了LLMs的推理效率和精度。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [256] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级二次增强器，通过引入二次变换增强神经网络非线性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 通过增加非线性（二次变换）来提升现有神经网络的性能。

Method: 采用低秩、权重共享和稀疏化技术，设计轻量级二次增强器，减少参数和计算复杂度。

Result: 在图像分类、文本分类和大语言模型微调任务中均表现出显著性能提升。

Conclusion: 轻量级二次增强器能有效提升神经网络性能，且额外开销可忽略。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [257] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: B-PINNs结合数据和物理约束求解微分方程，但不确定性解释需谨慎。本文提出可扩展的Laplace框架，量化约束对损失曲面的影响。


<details>
  <summary>Details</summary>
Motivation: 澄清物理约束如何影响B-PINNs的不确定性解释，避免将物理约束导致的精度误认为校准错误。

Method: 引入矩阵无关的Laplace框架，分解后验Hessian为各约束贡献，提供量化指标。

Result: 应用于Van der Pol方程，展示约束如何重塑网络几何，以及单个损失权重变化如何影响其他约束的曲率分布。

Conclusion: 提出的方法能有效量化物理约束对B-PINNs的影响，为不确定性解释提供新视角。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [258] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: 论文提出MemMamba，通过状态总结机制和跨层跨令牌注意力解决Mamba模型的长程记忆衰减问题，显著提升长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长序列建模中存在效率与内存的权衡问题，Mamba模型虽高效但长程记忆衰减严重。

Method: 结合数学推导和信息论分析，提出水平-垂直记忆保真度指标，并设计MemMamba框架，集成状态总结和注意力机制。

Result: MemMamba在PG19和Passkey Retrieval等长序列任务上优于现有方法，推理效率提升48%。

Conclusion: MemMamba在复杂度与内存权衡上取得突破，为超长序列建模提供了新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [259] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka是首个系统性研究扩散语言模型（DLMs）的扩展规律，涵盖计算和数据受限的情况，并探讨关键建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 研究扩散语言模型的扩展规律，为训练提供短期实践指导和长期AI社区启发。

Method: 提出Quokka框架，分析计算和数据受限下的建模与优化设计。

Result: Quokka扩展规律为DLMs训练提供实用指南，并为AI社区带来启发。

Conclusion: Quokka填补了扩散语言模型扩展规律的研究空白，具有实践和理论意义。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [260] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出了一种混合归因和剪枝（HAP）框架，用于高效且忠实地发现语言模型中的子网络电路。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法在速度和忠实性之间存在权衡，需要一种更高效且忠实的方法。

Method: 结合归因修补和边剪枝，先快速识别高潜力子图，再从中提取忠实电路。

Result: HAP比基线算法快46%，且不牺牲忠实性，能保留关键电路组件。

Conclusion: HAP可提升机制可解释性研究的扩展性，适用于更大模型。

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [261] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE是一种混合LLM系统，通过智能内存管理和迭代级调度，在边缘服务器上平衡推理延迟和模型更新频率，显著降低延迟并保持高GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 用户数据的非平稳性要求频繁重新训练LLM，但现有方法在资源受限的GPU上难以兼顾推理延迟和模型准确性。

Method: 提出MACE系统，结合推理（预填充、解码）和微调，通过迭代级调度和智能内存管理优化性能。

Result: MACE在资源约束下减少推理延迟达63%，保持高吞吐量，GPU利用率超过85%。

Conclusion: 迭代级混合调度是边缘平台上部署持续学习LLM的有效方向。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [262] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: Edge-FIT是一个用于大型语言模型（LLM）联邦指令调优的可扩展框架，解决了传统联邦学习方法在LLM上的通信和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法（如FedAvg）在处理LLM时因参数规模过大而失效，Edge-FIT旨在解决这一问题。

Method: 结合联邦学习和4位量化低秩适应（QLORA），减少通信和计算开销，并在物联网领域过滤Databricks Dolly 15k数据集进行实验。

Result: 实验结果显示，Edge-FIT调优的Llama 2(7B)模型F1分数达到0.89，并在Phi-3-mini模型上验证了其可扩展性。

Conclusion: Edge-FIT是一个适用于家庭计算网关的可扩展框架，能够有效支持去中心化LLM部署。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [263] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction是一种基于主动领域适应的日志异常检测模型，结合迁移学习和主动学习技术，显著减少人工标注需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法依赖大量标注数据，但标注成本高且存在数据分布差异和冷启动问题。

Method: LogAction利用成熟系统的标注数据训练基础模型，并通过自由能量和不确定性采样选择边界日志进行标注。

Result: 在六种数据集组合上，LogAction仅需2%人工标注即达到93.01%的平均F1分数，优于现有方法26.28%。

Conclusion: LogAction有效解决了标注成本和数据分布差异问题，为日志异常检测提供了高效解决方案。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [264] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 扩散语言模型相比自回归模型支持并行生成和双向注意力，但掩码扩散模型存在固有困难。本文提出改进策略。


<details>
  <summary>Details</summary>
Motivation: 探讨掩码扩散模型在并行生成和双向注意力方面的固有困难，并提出解决方案。

Method: 分析掩码扩散模型的局限性，并提出改进的训练和推理策略。

Result: 揭示了掩码扩散模型的固有困难，并提出了更有效的策略。

Conclusion: 掩码扩散模型在并行生成和双向注意力方面存在挑战，但通过改进策略可以优化性能。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [265] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 论文分析了Clifford卷积层的内部计算结构，提出并实现了多项优化，显著提升了推理速度，同时保持正确性。


<details>
  <summary>Details</summary>
Motivation: 随着物理科学中对具有等变性网络的兴趣增长，Clifford神经层因其能提供特定群作用下的E(n)和O(n)等变性而备受关注。

Method: 通过分析Clifford代数的理论基础，消除冗余矩阵分配和计算，并系统应用优化技术。

Result: 最终实现了平均21.35倍的加速，部分函数性能优于或接近原始PyTorch实现。

Conclusion: 优化后的Clifford卷积层在保持正确性的同时显著提升了性能。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [266] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning是一种统一的后训练剪枝框架，结合了局部显著性度量的速度和全局协调的稳定性，无需更新模型权重即可高效剪枝大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法难以平衡效率和鲁棒性，局部方法在高稀疏度下容易崩溃，全局方法则代价高昂或格式受限。

Method: UniPruning通过基于镜像下降的优化，结合快速层间评分和轻量级全局控制器，支持非结构化和半结构化剪枝。

Result: 实验表明，UniPruning在多个预训练LLM和基准测试中表现优异，保持或超越困惑度和零样本准确率。

Conclusion: UniPruning为大规模LLM剪枝提供了高效、原则性和可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [267] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER是一种用于Mixture-of-Experts模型的推理时路由算法，通过平衡负载提升系统性能，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: MoE模型在推理时存在负载不均衡问题，导致部分专家过载而其他专家利用率低，影响系统性能。

Method: LASER根据门控分数分布动态调整路由策略，优先选择负载最低的专家。

Result: 在多个数据集上，LASER显著改善了负载均衡，降低了延迟并提高了吞吐量，同时准确性变化可忽略。

Conclusion: LASER是一种即插即用的高效路由算法，适用于现有MoE推理流程，无需重新训练或微调。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [268] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L是一种改进的联邦学习方法，通过拉格朗日对偶优化动态调整训练参数，满足设备资源约束，同时保持训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决设备级资源约束（如能源、通信、内存和热预算）对联邦学习的影响。

Method: 采用拉格朗日对偶优化动态调整训练超参数，并通过梯度积累保持训练稳定性。

Result: 在字符级语言模型实验中，CAFL-L比标准FedAvg减少内存使用20%、通信量95%，同时保持验证性能。

Conclusion: CAFL-L适用于资源受限的边缘设备部署。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [269] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出了一种结合XGBoost和神经网络的自适应集成框架，通过元学习和不确定性量化提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 开发更智能、灵活的机器学习系统，提升预测性能和可解释性。

Method: 利用元学习和不确定性量化技术动态选择和组合模型。

Result: 在多样化数据集上表现出优越的预测性能和增强的可解释性。

Conclusion: 该方法为智能机器学习系统的发展提供了新思路。

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [270] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 论文揭示了现有概念擦除技术仅通过偏置采样轨迹制造遗忘假象，而非真正移除概念，并提出RevAm框架通过轨迹优化复活被擦除概念。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法在下一代架构中效果下降，且仅制造遗忘假象，需区分表面安全与真实概念移除。

Method: 提出RevAm框架，基于RL的轨迹优化，动态引导去噪过程，无需修改模型权重，利用GRPO探索多样化恢复轨迹。

Result: RevAm在概念复活保真度上表现优越，计算时间减少10倍，暴露当前安全机制的脆弱性。

Conclusion: 需开发超越轨迹操作的更鲁棒擦除技术，RevAm为概念复活提供了高效解决方案。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [271] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 论文分析了机器学习在气候建模中的应用案例，重点关注设计选择和工作流程结构，旨在为科学机器学习提供透明、严谨的框架。


<details>
  <summary>Details</summary>
Motivation: 解决气候建模中的物理一致性、多尺度耦合、数据稀疏性等挑战，促进数据科学与气候建模的跨学科合作。

Method: 通过案例研究，分析机器学习在气候建模中的工作流程设计模式，如替代建模、参数化、概率编程等。

Result: 提出了一个框架，确保科学机器学习的透明性、严谨性和可重复性。

Conclusion: 论文为机器学习在气候建模中的应用提供了实用指南，降低了跨学科合作的门槛。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [272] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 研究提出了一种轻量化的跨模态对齐方法，通过对比学习将化学分子指纹与生物医学文本嵌入对齐，避免了大规模预训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模预训练或多模态语料库，计算成本高。本研究旨在探索轻量化的跨模态对齐方法。

Method: 使用冻结的单模态编码器和双线性投影头，结合对比目标、硬负样本加权和边界损失进行训练。

Result: 在支架分割评估中，方法实现了非平凡的跨模态对齐，显著提高了目标内区分能力。

Conclusion: 轻量化桥梁方法为大规模多模态预训练提供了计算高效的替代方案，适用于精准医学中的药物文本对齐和目标检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [273] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: LLMs在运营管理中模拟人类行为的效果评估，发现能复现大部分假设效应但分布不一致，轻量干预可改善。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在运营管理中模拟人类行为的准确性，作为低成本替代实验的潜力。

Method: 使用九项行为运营实验，通过假设检验结果和Wasserstein距离评估LLMs与人类数据的匹配度。

Result: LLMs能复现多数假设效应（如决策偏差），但响应分布与人类数据不一致；轻量干预（如思维链提示）可减少偏差。

Conclusion: LLMs在模拟人类行为上有潜力，但需进一步优化分布对齐；轻量干预可提升小型或开源模型性能。

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [274] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 该论文提出了一种考虑数据质量的语言模型扩展定律，通过引入数据质量参数Q，扩展了Chinchilla框架，以预测模型大小、数据量和数据质量对损失的联合影响。


<details>
  <summary>Details</summary>
Motivation: 传统扩展定律未将数据质量纳入原则性框架，研究旨在填补这一空白，提供数据质量对模型性能影响的量化方法。

Method: 提出质量感知扩展定律，基于有效样本量和信息论视角，通过噪声注入和覆盖变化控制数据质量，验证损失与数据质量的可预测关系。

Result: 实验表明，高质量数据可显著减少模型大小和计算需求，且有效数据随质量呈亚线性衰减。

Conclusion: 该研究建立了数据质量的显式、可推广定律，为大规模预训练中数据筛选与模型规模的平衡提供了具体指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [275] [Fast frequency reconstruction using Deep Learning for event recognition in ring laser data](https://arxiv.org/abs/2510.03325)
*Giuseppe Di Somma,Giorgio Carelli,Angela D. V. Di Virgilio,Francesco Fuso,Enrico Maccioni,Paolo Marsili*

Main category: cs.LG

TL;DR: 论文提出了一种神经网络方法，用于快速重建正弦信号的频率，并在10毫秒内完成，优于传统傅里叶方法。同时，引入了自动分类框架以识别信号中的物理干扰。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要数秒数据的问题，实现快速频率重建和触发生成，适用于环形激光陀螺仪等领域。

Method: 使用神经网络方法进行频率重建，并结合自动分类框架识别信号中的物理干扰（如激光不稳定性和地震事件）。

Result: 在GINGERINO陀螺仪的操作范围内，频率估计精度提高了2倍；对地震事件的分类准确率达到99%-100%。

Conclusion: 该方法在信号分析中整合人工智能，为地球物理应用提供了更高效的解决方案。

Abstract: The reconstruction of a frequency with minimal delay from a sinusoidal signal
is a common task in several fields; for example Ring Laser Gyroscopes, since
their output signal is a beat frequency. While conventional methods require
several seconds of data, we present a neural network approach capable of
reconstructing frequencies of several hundred Hertz within approximately 10
milliseconds. This enables rapid trigger generation. The method outperforms
standard Fourier-based techniques, improving frequency estimation precision by
a factor of 2 in the operational range of GINGERINO, our Ring Laser
Gyroscope.\\ In addition to fast frequency estimation, we introduce an
automated classification framework to identify physical disturbances in the
signal, such as laser instabilities and seismic events, achieving accuracy
rates between 99\% and 100\% on independent test datasets for the seismic
class. These results mark a step forward in integrating artificial intelligence
into signal analysis for geophysical applications.

</details>


### [276] [Constant in an Ever-Changing World](https://arxiv.org/abs/2510.03330)
*Andy Wu,Chun-Cheng Lin,Yuehua Huang,Rung-Tzuo Liaw*

Main category: cs.LG

TL;DR: 提出CIC框架，通过选择性更新代表策略和自适应调整机制，提升强化学习训练的稳定性。


<details>
  <summary>Details</summary>
Motivation: 强化学习训练过程中存在严重振荡，导致不稳定和性能下降。

Method: CIC框架维护代表策略和当前策略，选择性更新代表策略，并采用自适应调整机制。

Result: 在五个MuJoCo环境中测试，CIC提升了传统算法的性能且无额外计算成本。

Conclusion: CIC有效提升了强化学习的稳定性和性能。

Abstract: The training process of reinforcement learning often suffers from severe
oscillations, leading to instability and degraded performance. In this paper,
we propose a Constant in an Ever-Changing World (CIC) framework that enhances
algorithmic stability to improve performance. CIC maintains both a
representative policy and a current policy. Instead of updating the
representative policy blindly, CIC selectively updates it only when the current
policy demonstrates superiority. Furthermore, CIC employs an adaptive
adjustment mechanism, enabling the representative and current policies to
jointly facilitate critic training. We evaluate CIC on five MuJoCo
environments, and the results show that CIC improves the performance of
conventional algorithms without incurring additional computational cost.

</details>


### [277] [Semantic-Aware Scheduling for GPU Clusters with Large Language Models](https://arxiv.org/abs/2510.03334)
*Zerui Wang,Qinghao Hu,Ana Klimovic,Tianwei Zhang,Yonggang Wen,Peng Sun,Dahua Lin*

Main category: cs.LG

TL;DR: SchedMate利用LLM从源代码、日志和历史任务中提取语义信息，显著提升DL调度性能。


<details>
  <summary>Details</summary>
Motivation: 现有DL调度器缺乏对任务语义的理解，导致效率低下。

Method: 通过三个LLM组件非侵入式增强现有调度器。

Result: 在128-GPU集群上测试，任务完成时间减少1.91倍。

Conclusion: 语义感知对现代DL调度至关重要。

Abstract: Deep learning (DL) schedulers are pivotal in optimizing resource allocation
in GPU clusters, but operate with a critical limitation: they are largely blind
to the semantic context of the jobs they manage. This forces them to rely on
limited metadata, leading to high profiling overhead, unreliable duration
estimation, inadequate failure handling, and poor observability. To this end,
we propose SchedMate, a framework that bridges this semantic gap by
systematically extracting deep insights from overlooked, unstructured data
sources: source code, runtime logs, and historical jobs. SchedMate enhances
existing schedulers non-intrusively through three LLM-based components. Our
implementation integrates seamlessly with existing deep learning schedulers.
Evaluations on a 128-GPU physical cluster and extensive simulations on
production traces show SchedMate reduces average job completion times by up to
1.91x, substantially enhancing the scheduling performance, demonstrating the
critical role of semantic-awareness in modern DL scheduling.

</details>


### [278] [Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment](https://arxiv.org/abs/2510.03335)
*Ameya Daigavane,YuQing Xie,Bodhi P. Vani,Saeed Saremi,Joseph Kleinhenz,Tess Smidt*

Main category: cs.LG

TL;DR: 研究探讨了扩散模型中旋转对齐步骤的作用，并提出了一种基于矩阵Fisher分布的新方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在训练点云数据时，通常需要旋转对齐步骤，但其效果未被充分研究。本文旨在填补这一空白。

Method: 通过矩阵Fisher分布描述最优去噪器，并提出小噪声极限下的更好近似方法。

Result: 实验表明，旋转对齐在训练扩散模型时通常是足够好的近似。

Conclusion: 旋转对齐步骤在扩散模型训练中有效，但新方法在小噪声情况下表现更优。

Abstract: Diffusion models are a popular class of generative models trained to reverse
a noising process starting from a target data distribution. Training a
diffusion model consists of learning how to denoise noisy samples at different
noise levels. When training diffusion models for point clouds such as molecules
and proteins, there is often no canonical orientation that can be assigned. To
capture this symmetry, the true data samples are often augmented by
transforming them with random rotations sampled uniformly over $SO(3)$. Then,
the denoised predictions are often rotationally aligned via the Kabsch-Umeyama
algorithm to the ground truth samples before computing the loss. However, the
effect of this alignment step has not been well studied. Here, we show that the
optimal denoiser can be expressed in terms of a matrix Fisher distribution over
$SO(3)$. Alignment corresponds to sampling the mode of this distribution, and
turns out to be the zeroth order approximation for small noise levels,
explaining its effectiveness. We build on this perspective to derive better
approximators to the optimal denoiser in the limit of small noise. Our
experiments highlight that alignment is often a `good enough' approximation for
the noise levels that matter most for training diffusion models.

</details>


### [279] [Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models](https://arxiv.org/abs/2510.03339)
*Sofiane Ennadir,Levente Zólyomi,Oleg Smirnov,Tianze Wang,John Pertoft,Filip Cornell,Lele Cao*

Main category: cs.LG

TL;DR: 论文提出了一个理论框架，分析Transformer模型中池化操作的表达能力及其对模型性能的影响，并通过实验验证了不同池化策略的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型中的注意力机制被广泛研究，但池化操作的作用却未被充分探索，但其对模型行为有重要影响。

Method: 通过理论分析推导池化操作的表示能力界限，并在多种任务和模态中实证评估不同池化策略。

Result: 实验结果表明池化选择对准确性、敏感性和优化行为有显著影响，为任务特定的池化设计提供了指导。

Conclusion: 池化是Transformer模型中的关键组件，本研究为超越注意力机制的系统化模型设计奠定了基础。

Abstract: Transformer models have become the dominant backbone for sequence modeling,
leveraging self-attention to produce contextualized token representations.
These are typically aggregated into fixed-size vectors via pooling operations
for downstream tasks. While much of the literature has focused on attention
mechanisms, the role of pooling remains underexplored despite its critical
impact on model behavior. In this paper, we introduce a theoretical framework
that rigorously characterizes the expressivity of Transformer-based models
equipped with widely used pooling methods by deriving closed-form bounds on
their representational capacity and the ability to distinguish similar inputs.
Our analysis extends to different variations of attention formulations,
demonstrating that these bounds hold across diverse architectural variants. We
empirically evaluate pooling strategies across tasks requiring both global and
local contextual understanding, spanning three major modalities: computer
vision, natural language processing, and time-series analysis. Results reveal
consistent trends in how pooling choices affect accuracy, sensitivity, and
optimization behavior. Our findings unify theoretical and empirical
perspectives, providing practical guidance for selecting or designing pooling
mechanisms suited to specific tasks. This work positions pooling as a key
architectural component in Transformer models and lays the foundation for more
principled model design beyond attention alone.

</details>


### [280] [Learning Pareto-Optimal Pandemic Intervention Policies with MORL](https://arxiv.org/abs/2510.03340)
*Marian Chen,Miri Zilka*

Main category: cs.LG

TL;DR: 论文提出了一种基于多目标强化学习和随机微分方程的框架，用于平衡疾病控制与社会经济稳定，适用于多种传染病。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情凸显了平衡疾病控制与社会经济稳定的需求，需要一种更精确的建模和评估方法。

Method: 结合多目标强化学习（MORL）和随机微分方程（SDE）模拟器，训练Pareto-Conditioned Network（PCN）代理。

Result: 模型在COVID-19数据上表现出高保真度，并能推广到其他传染病（如脊髓灰质炎和流感），量化了疫苗接种覆盖率下降的影响。

Conclusion: 该框架为公共卫生危机提供了透明、基于证据的政策支持工具。

Abstract: The COVID-19 pandemic underscored a critical need for intervention strategies
that balance disease containment with socioeconomic stability. We approach this
challenge by designing a framework for modeling and evaluating disease-spread
prevention strategies. Our framework leverages multi-objective reinforcement
learning (MORL) - a formulation necessitated by competing objectives - combined
with a new stochastic differential equation (SDE) pandemic simulator,
calibrated and validated against global COVID-19 data. Our simulator reproduces
national-scale pandemic dynamics with orders of magnitude higher fidelity than
other models commonly used in reinforcement learning (RL) approaches to
pandemic intervention. Training a Pareto-Conditioned Network (PCN) agent on
this simulator, we illustrate the direct policy trade-offs between
epidemiological control and economic stability for COVID-19. Furthermore, we
demonstrate the framework's generality by extending it to pathogens with
different epidemiological profiles, such as polio and influenza, and show how
these profiles lead the agent to discover fundamentally different intervention
policies. To ground our work in contemporary policymaking challenges, we apply
the model to measles outbreaks, quantifying how a modest 5% drop in vaccination
coverage necessitates significantly more stringent and costly interventions to
curb disease spread. This work provides a robust and adaptable framework to
support transparent, evidence-based policymaking for mitigating public health
crises.

</details>


### [281] [Pilot selection in the era of Virtual reality: algorithms for accurate and interpretable machine learning models](https://arxiv.org/abs/2510.03345)
*Luoma Ke,Guangpeng Zhang,Jibo He,Yajing Li,Yan Li,Xufeng Liu,Peng Fang*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和虚拟现实技术的新方法，用于高效区分飞行员与新手，SVM + MIC算法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 航空业快速发展需要大量飞行员，如何高效低成本地选拔飞行员成为重要研究问题。

Method: 招募23名飞行员和23名新手，采用机器学习（SVM）和虚拟现实技术，结合MIC特征选择方法进行分析。

Result: SVM + MIC算法在准确率（0.93）、AUC（0.96）和F1值（0.93）上均优于其他方法。

Conclusion: 该研究的VR平台和算法可用于飞行员选拔与培训，SVM + MIC算法为基于眼动和飞行数据的首次实现。

Abstract: With the rapid growth of the aviation industry, there is a need for a large
number of flight crew. How to select the right pilots in a cost-efficient
manner has become an important research question. In the current study,
twenty-three pilots were recruited from China Eastern Airlines, and 23 novices
were from the community of Tsinghua University. A novel approach incorporating
machine learning and virtual reality technology was applied to distinguish
features between these participants with different flight skills. Results
indicate that SVM with the MIC feature selection method consistently achieved
the highest prediction performance on all metrics with an Accuracy of 0.93, an
AUC of 0.96, and an F1 of 0.93, which outperforms four other classifier
algorithms and two other feature selection methods. From the perspective of
feature selection methods, the MIC method can select features with a nonlinear
relationship to sampling labels, instead of a simple filter-out. Our new
implementation of the SVM + MIC algorithm outperforms all existing pilot
selection algorithms and perhaps provides the first implementation based on eye
tracking and flight dynamics data. This study's VR simulation platforms and
algorithms can be used for pilot selection and training.

</details>


### [282] [Deep Reinforcement Learning for Multi-Agent Coordination](https://arxiv.org/abs/2510.03592)
*Kehinde O. Aina,Sehoon Ha*

Main category: cs.LG

TL;DR: 提出了一种基于虚拟信息素的S-MADRL框架，用于解决狭窄环境中多机器人协调问题，通过课程学习提升算法收敛性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 狭窄和拥挤环境中多机器人协调常因拥堵和干扰影响任务性能，受昆虫群体通过信息素实现协调的启发，提出新方法。

Method: 采用Stigmergic Multi-Agent Deep Reinforcement Learning (S-MADRL)框架，结合虚拟信息素和课程学习，实现分散化协调。

Result: 仿真显示，框架能有效协调最多8个机器人，形成不对称工作负载分布，减少拥堵并提升群体性能。

Conclusion: 该框架展示了在通信受限的拥挤环境中实现分散化多智能体协调的可扩展解决方案。

Abstract: We address the challenge of coordinating multiple robots in narrow and
confined environments, where congestion and interference often hinder
collective task performance. Drawing inspiration from insect colonies, which
achieve robust coordination through stigmergy -- modifying and interpreting
environmental traces -- we propose a Stigmergic Multi-Agent Deep Reinforcement
Learning (S-MADRL) framework that leverages virtual pheromones to model local
and social interactions, enabling decentralized emergent coordination without
explicit communication. To overcome the convergence and scalability limitations
of existing algorithms such as MADQN, MADDPG, and MAPPO, we leverage curriculum
learning, which decomposes complex tasks into progressively harder
sub-problems. Simulation results show that our framework achieves the most
effective coordination of up to eight agents, where robots self-organize into
asymmetric workload distributions that reduce congestion and modulate group
performance. This emergent behavior, analogous to strategies observed in
nature, demonstrates a scalable solution for decentralized multi-agent
coordination in crowded environments with communication constraints.

</details>


### [283] [KVComm: Enabling Efficient LLM Communication through Selective KV Sharing](https://arxiv.org/abs/2510.03346)
*Xiangyu Shi,Marco Chiesa,Gerald Q. Maguire Jr.,Dejan Kostic*

Main category: cs.LG

TL;DR: KVComm是一种通过选择性共享KV对实现LLM间高效通信的新框架，避免了自然语言和隐藏状态的缺点。


<details>
  <summary>Details</summary>
Motivation: 现有通信协议存在高推理成本、信息丢失或信息集中偏差等问题，KVComm旨在解决这些限制。

Method: KVComm采用基于注意力重要性分数和高斯先验的KV层选择策略，识别最具信息量的KV对进行通信。

Result: 实验表明，KVComm性能接近直接合并输入的上界方法，同时仅传输30%的KV对。

Conclusion: KV对作为LLM间通信媒介具有潜力，为高效多智能体系统铺平道路。

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-agent
systems, where effective inter-model communication is crucial. Existing
communication protocols either rely on natural language, incurring high
inference costs and information loss, or on hidden states, which suffer from
information concentration bias and inefficiency. To address these limitations,
we propose KVComm, a novel communication framework that enables efficient
communication between LLMs through selective sharing of KV pairs. KVComm
leverages the rich information encoded in the KV pairs while avoiding the
pitfalls of hidden states. We introduce a KV layer-wise selection strategy
based on attention importance scores with a Gaussian prior to identify the most
informative KV pairs for communication. Extensive experiments across diverse
tasks and model pairs demonstrate that KVComm achieves comparable performance
to the upper-bound method, which directly merges inputs to one model without
any communication, while transmitting as few as 30\% of layers' KV pairs. Our
study highlights the potential of KV pairs as an effective medium for inter-LLM
communication, paving the way for scalable and efficient multi-agent systems.

</details>


### [284] [Distributed Area Coverage with High Altitude Balloons Using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.03823)
*Adam Haroon,Tristan Schuler*

Main category: cs.LG

TL;DR: 本文首次将多智能体强化学习（MARL）应用于高空气球（HAB）的协调覆盖任务，验证了其与理论最优几何确定性方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法在小规模团队和局部任务中表现不佳，而多智能体强化学习在HAB协调中尚未被研究。

Method: 扩展了RLHAB仿真环境以支持多智能体学习，采用QMIX算法，结合集中训练与分散执行，设计专用观测空间和分层奖励机制。

Result: QMIX在分布式区域覆盖任务中表现与理论最优几何确定性方法相当。

Conclusion: MARL为复杂多HAB任务提供了可行方案，尤其是在确定性方法难以处理的情况下。

Abstract: High Altitude Balloons (HABs) can leverage stratospheric wind layers for
limited horizontal control, enabling applications in reconnaissance,
environmental monitoring, and communications networks. Existing multi-agent HAB
coordination approaches use deterministic methods like Voronoi partitioning and
extremum seeking control for large global constellations, which perform poorly
for smaller teams and localized missions. While single-agent HAB control using
reinforcement learning has been demonstrated on HABs, coordinated multi-agent
reinforcement learning (MARL) has not yet been investigated. This work presents
the first systematic application of multi-agent reinforcement learning (MARL)
to HAB coordination for distributed area coverage. We extend our previously
developed reinforcement learning simulation environment (RLHAB) to support
cooperative multi-agent learning, enabling multiple agents to operate
simultaneously in realistic atmospheric conditions. We adapt QMIX for HAB area
coverage coordination, leveraging Centralized Training with Decentralized
Execution to address atmospheric vehicle coordination challenges. Our approach
employs specialized observation spaces providing individual state,
environmental context, and teammate data, with hierarchical rewards
prioritizing coverage while encouraging spatial distribution. We demonstrate
that QMIX achieves similar performance to the theoretically optimal geometric
deterministic method for distributed area coverage, validating the MARL
approach and providing a foundation for more complex autonomous multi-HAB
missions where deterministic methods become intractable.

</details>


### [285] [AgentCaster: Reasoning-Guided Tornado Forecasting](https://arxiv.org/abs/2510.03349)
*Michael Chen*

Main category: cs.LG

TL;DR: AgentCaster是一个用于龙卷风预测的多模态LLM框架，评估了LLM在复杂任务中的表现，发现人类专家显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在复杂、高影响现实任务中的推理能力，填补现有研究的空白。

Method: 使用多模态LLM端到端处理异构时空数据，通过交互式查询预测龙卷风风险。

Result: LLM在预测中表现出幻觉和过度预测倾向，地理定位和时空推理能力较差，人类专家表现更优。

Conclusion: AgentCaster旨在推动LLM在关键领域复杂推理任务中的改进研究。

Abstract: There is a growing need to evaluate Large Language Models (LLMs) on complex,
high-impact, real-world tasks to assess their true readiness as reasoning
agents. To address this gap, we introduce AgentCaster, a contamination-free
framework employing multimodal LLMs end-to-end for the challenging,
long-horizon task of tornado forecasting. Within AgentCaster, models interpret
heterogeneous spatiotemporal data from a high-resolution convection-allowing
forecast archive. We assess model performance over a 40-day period featuring
diverse historical data, spanning several major tornado outbreaks and including
over 500 tornado reports. Each day, models query interactively from a pool of
3,625 forecast maps and 40,125 forecast soundings for a forecast horizon of
12-36 hours. Probabilistic tornado-risk polygon predictions are verified
against ground truths derived from geometric comparisons across disjoint risk
bands in projected coordinate space. To quantify accuracy, we propose
domain-specific TornadoBench and TornadoHallucination metrics, with
TornadoBench highly challenging for both LLMs and domain expert human
forecasters. Notably, human experts significantly outperform state-of-the-art
models, which demonstrate a strong tendency to hallucinate and overpredict risk
intensity, struggle with precise geographic placement, and exhibit poor
spatiotemporal reasoning in complex, dynamically evolving systems. AgentCaster
aims to advance research on improving LLM agents for challenging reasoning
tasks in critical domains.

</details>


### [286] [Interpretable Neuropsychiatric Diagnosis via Concept-Guided Graph Neural Networks](https://arxiv.org/abs/2510.03351)
*Song Wang,Zhenyu Lei,Zhen Tan,Jundong Li,Javier Rasero,Aiying Zhang,Chirag Agarwal*

Main category: cs.LG

TL;DR: CONCEPTNEURO是一个基于概念的诊断框架，结合大语言模型和神经生物学知识，生成可解释的功能连接概念，提升精神病诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 青少年心理健康问题日益严重，现有诊断工具多为黑箱模型，缺乏可靠性和临床适用性。

Method: 利用大语言模型和神经生物学知识自动生成、筛选和编码功能连接概念，每个概念表示为特定脑区的结构化子图，并通过概念分类器进行预测。

Result: 在多组精神病数据集上，CONCEPTNEURO增强的GNN模型表现优于传统模型，同时提供透明的临床解释。

Conclusion: CONCEPTNEURO为精神病诊断提供了一个可解释且领域知识驱动的框架，并揭示了与专家知识一致的疾病特异性连接模式。

Abstract: Nearly one in five adolescents currently live with a diagnosed mental or
behavioral health condition, such as anxiety, depression, or conduct disorder,
underscoring the urgency of developing accurate and interpretable diagnostic
tools. Resting-state functional magnetic resonance imaging (rs-fMRI) provides a
powerful lens into large-scale functional connectivity, where brain regions are
modeled as nodes and inter-regional synchrony as edges, offering clinically
relevant biomarkers for psychiatric disorders. While prior works use graph
neural network (GNN) approaches for disorder prediction, they remain complex
black-boxes, limiting their reliability and clinical translation. In this work,
we propose CONCEPTNEURO, a concept-based diagnosis framework that leverages
large language models (LLMs) and neurobiological domain knowledge to
automatically generate, filter, and encode interpretable functional
connectivity concepts. Each concept is represented as a structured subgraph
linking specific brain regions, which are then passed through a concept
classifier. Our design ensures predictions through clinically meaningful
connectivity patterns, enabling both interpretability and strong predictive
performance. Extensive experiments across multiple psychiatric disorder
datasets demonstrate that CONCEPTNEURO-augmented GNNs consistently outperform
their vanilla counterparts, improving accuracy while providing transparent,
clinically aligned explanations. Furthermore, concept analyses highlight
disorder-specific connectivity patterns that align with expert knowledge and
suggest new hypotheses for future investigation, establishing CONCEPTNEURO as
an interpretable, domain-informed framework for psychiatric disorder diagnosis.

</details>


### [287] [High Cycle S-N curve prediction for Al 7075-T6 alloy using Recurrent Neural Networks (RNNs)](https://arxiv.org/abs/2510.03355)
*Aryan Patel*

Main category: cs.LG

TL;DR: 利用LSTM和迁移学习框架预测铝合金的高周疲劳性能，显著降低测试成本和时间。


<details>
  <summary>Details</summary>
Motivation: 铝合金疲劳性能测试成本高且耗时，尤其是高周数据。

Method: 基于LSTM的迁移学习框架，利用轴向疲劳数据训练源模型，预测高周扭转S-N曲线。

Result: 框架能准确预测铝的高周扭转S-N曲线。

Conclusion: 该框架可大幅降低材料疲劳特性测试成本，优化测试优先级。

Abstract: Aluminum is a widely used alloy, which is susceptible to fatigue failure.
Characterizing fatigue performance for materials is extremely time and cost
demanding, especially for high cycle data. To help mitigate this, a transfer
learning based framework has been developed using Long short-term memory
networks (LSTMs) in which a source LSTM model is trained based on pure axial
fatigue data for Aluminum 7075-T6 alloy which is then transferred to predict
high cycle torsional S-N curves. The framework was able to accurately predict
Al torsional S-N curves for a much higher cycle range. It is the belief that
this framework will help to drastically mitigate the cost of gathering fatigue
characteristics for different materials and help prioritize tests with better
cost and time constraints.

</details>


### [288] [Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility](https://arxiv.org/abs/2510.03358)
*Annan Yu,Danielle C. Maddix,Boran Han,Xiyuan Zhang,Abdul Fatir Ansari,Oleksandr Shchur,Christos Faloutsos,Andrew Gordon Wilson,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: 论文分析了Transformer在时间序列数据中的低秩特性，提出了压缩大型时间序列模型的方法，显著减少了推理时间和内存占用。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索Transformer在不同模态（尤其是时间序列）中的表现差异，并利用低秩特性优化模型。

Method: 通过分析时间序列嵌入的低秩特性，证明了注意力层的可压缩性，并提出了基于流秩的压缩方法。

Result: 成功压缩了Chronos模型，减少了65%的推理时间和81%的内存占用，且未损失准确性。

Conclusion: 研究为时间序列基础模型的宽度、深度和注意力头分配提供了理论指导，并展示了其固有可压缩性。

Abstract: Transformers are widely used across data modalities, and yet the principles
distilled from text models often transfer imperfectly to models trained to
other modalities. In this paper, we analyze Transformers through the lens of
rank structure. Our focus is on the time series setting, where the structural
properties of the data differ remarkably from those of text or vision. We show
that time-series embeddings, unlike text or vision, exhibit sharply decaying
singular value spectra: small patch sizes and smooth continuous mappings
concentrate the data into low-rank subspaces. From this, we prove that the
associated $Q/K/V$ projections admit accurate low-rank approximations, and that
attention layers become compressible in proportion to the decay of the
embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by
which nonlinear mixing across depth inflates the rank, explaining why early
layers are most amenable to compression and why ranks grow with depth. Guided
by these theoretical and empirical results, we use these insights to compress
Chronos, a large time series foundation model, achieving a reduction of $65\%$
in inference time and $81\%$ in memory, without loss of accuracy. Our findings
provide principled guidance for allocating width, depth, and heads in time
series foundation models, and for exploiting their inherent compressibility.

</details>


### [289] [A KL-regularization framework for learning to plan with adaptive priors](https://arxiv.org/abs/2510.04280)
*Álvaro Serra-Gomez,Daniel Jarne Ornia,Dhruva Tirumala,Thomas Moerland*

Main category: cs.LG

TL;DR: 论文提出PO-MPC框架，统一了基于MPPI的强化学习方法，通过KL正则化对齐学习策略与规划器行为，提升性能。


<details>
  <summary>Details</summary>
Motivation: 在模型强化学习中，探索效率是关键挑战，尤其是高维连续控制任务。现有方法未充分对齐学习策略与规划器分布，影响价值估计和长期性能。

Method: 引入PO-MPC框架，通过KL正则化将规划器动作分布作为策略优化的先验，灵活权衡回报最大化和KL最小化。

Result: 实验表明，PO-MPC的扩展配置显著提升性能，成为MPPI-RL的新标杆。

Conclusion: PO-MPC统一了现有方法，并通过策略与规划器对齐，实现了更优的探索与性能。

Abstract: Effective exploration remains a central challenge in model-based
reinforcement learning (MBRL), particularly in high-dimensional continuous
control tasks where sample efficiency is crucial. A prominent line of recent
work leverages learned policies as proposal distributions for Model-Predictive
Path Integral (MPPI) planning. Initial approaches update the sampling policy
independently of the planner distribution, typically maximizing a learned value
function with deterministic policy gradient and entropy regularization.
However, because the states encountered during training depend on the MPPI
planner, aligning the sampling policy with the planner improves the accuracy of
value estimation and long-term performance. To this end, recent methods update
the sampling policy by minimizing KL divergence to the planner distribution or
by introducing planner-guided regularization into the policy update. In this
work, we unify these MPPI-based reinforcement learning methods under a single
framework by introducing Policy Optimization-Model Predictive Control (PO-MPC),
a family of KL-regularized MBRL methods that integrate the planner's action
distribution as a prior in policy optimization. By aligning the learned policy
with the planner's behavior, PO-MPC allows more flexibility in the policy
updates to trade off Return maximization and KL divergence minimization. We
clarify how prior approaches emerge as special cases of this family, and we
explore previously unstudied variations. Our experiments show that these
extended configurations yield significant performance improvements, advancing
the state of the art in MPPI-based RL.

</details>


### [290] [Physics-informed Neural-operator Predictive Control for Drag Reduction in Turbulent Flows](https://arxiv.org/abs/2510.03360)
*Zelin Zhao,Zongyi Li,Kimia Hassibi,Kamyar Azizzadenesheli,Junchi Yan,H. Jane Bae,Di Zhou,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种基于深度强化学习的高效框架PINO-PC，用于湍流建模与控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟湍流控制效果成本高，需更高效方法。

Method: 使用物理信息神经算子（PINO）联合学习策略和观测模型，实现预测控制。

Result: 在雷诺数为15,000时，PINO-PC实现39.0%的阻力减少，优于现有方法32%。

Conclusion: PINO-PC为湍流控制提供高效且性能优越的解决方案。

Abstract: Assessing turbulence control effects for wall friction numerically is a
significant challenge since it requires expensive simulations of turbulent
fluid dynamics. We instead propose an efficient deep reinforcement learning
(RL) framework for modeling and control of turbulent flows. It is model-based
RL for predictive control (PC), where both the policy and the observer models
for turbulence control are learned jointly using Physics Informed Neural
Operators (PINO), which are discretization invariant and can capture fine
scales in turbulent flows accurately. Our PINO-PC outperforms prior model-free
reinforcement learning methods in various challenging scenarios where the flows
are of high Reynolds numbers and unseen, i.e., not provided during model
training. We find that PINO-PC achieves a drag reduction of 39.0\% under a
bulk-velocity Reynolds number of 15,000, outperforming previous fluid control
methods by more than 32\%.

</details>


### [291] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: 提出了一种基于开源数据的车辆排放估算框架，结合多种数据源和神经网络模型，显著降低了排放估算误差。


<details>
  <summary>Details</summary>
Motivation: 利用开源数据提供可扩展且透明的车辆活动与排放估算方法，以替代传统高成本方法。

Method: 整合MOVES、GPS轨迹、OSM路网、区域交通数据和卫星影像特征，训练神经网络预测操作模式分布。

Result: 在波士顿都市区45个城市应用，相比MOVES基线，关键污染物排放估算的RMSE降低超过50%。

Conclusion: 证明了完全基于开源数据的低成本、可复制的排放估算方法的可行性。

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [292] [Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds](https://arxiv.org/abs/2510.03364)
*Xiaolong Ma,Xu Dong,Ashley Tarrant,Lei Yang,Rao Kotamarthi,Jiali Wang,Feng Yan,Rajkumar Kettimuthu*

Main category: cs.LG

TL;DR: WindSR是一种基于扩散模型的数据同化方法，用于高分辨率降尺度风速预测，结合观测和模拟数据，显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 高质量的风速观测数据稀缺且分布不均，而模拟数据存在偏差且分辨率不足，无法满足风电场选址或极端天气风险评估的需求。

Method: 引入WindSR，结合扩散模型和数据同化技术，动态半径混合方法融合观测与模拟数据，并加入地形信息。

Result: WindSR在降尺度效率和准确性上优于卷积神经网络和生成对抗网络基线，数据同化使模型偏差减少约20%。

Conclusion: WindSR为高分辨率风速预测提供了一种高效且准确的方法，特别适用于风电场选址和极端天气风险评估。

Abstract: High-quality observations of hub-height winds are valuable but sparse in
space and time. Simulations are widely available on regular grids but are
generally biased and too coarse to inform wind-farm siting or to assess
extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully
utilize both data types for generating high-quality, high-resolution hub-height
wind speeds (tens to ~100m above ground), this study introduces WindSR, a
diffusion model with data assimilation for super-resolution downscaling of
hub-height winds. WindSR integrates sparse observational data with simulation
fields during downscaling using state-of-the-art diffusion models. A
dynamic-radius blending method is introduced to merge observations with
simulations, providing conditioning for the diffusion process. Terrain
information is incorporated during both training and inference to account for
its role as a key driver of winds. Evaluated against
convolutional-neural-network and generative-adversarial-network baselines,
WindSR outperforms them in both downscaling efficiency and accuracy. Our data
assimilation reduces WindSR's model bias by approximately 20% relative to
independent observations.

</details>


### [293] [Disentangling Recall and Reasoning in Transformer Models through Layer-wise Attention and Activation Analysis](https://arxiv.org/abs/2510.03366)
*Harshwardhan Fartale,Ashish Kattamuri,Rahul Raja,Arpita Vats,Ishita Prasad,Akshata Kishore Moharir*

Main category: cs.LG

TL;DR: 研究发现Transformer模型中回忆与推理依赖分离但交互的电路机制。


<details>
  <summary>Details</summary>
Motivation: 区分回忆与推理能力对预测模型泛化、设计评估和构建安全干预至关重要。

Method: 通过机械可解释性方法，使用合成语言谜题数据集，结合激活修补和结构化消融技术。

Result: 干预特定层和注意力头可选择性损害回忆或推理能力，神经元层面任务特异性较弱。

Conclusion: 研究首次提供因果证据表明回忆与推理依赖分离电路，为模型认知机制和安全部署提供新见解。

Abstract: Transformer-based language models excel at both recall (retrieving memorized
facts) and reasoning (performing multi-step inference), but whether these
abilities rely on distinct internal mechanisms remains unclear. Distinguishing
recall from reasoning is crucial for predicting model generalization, designing
targeted evaluations, and building safer interventions that affect one ability
without disrupting the other.We approach this question through mechanistic
interpretability, using controlled datasets of synthetic linguistic puzzles to
probe transformer models at the layer, head, and neuron level. Our pipeline
combines activation patching and structured ablations to causally measure
component contributions to each task type. Across two model families (Qwen and
LLaMA), we find that interventions on distinct layers and attention heads lead
to selective impairments: disabling identified "recall circuits" reduces
fact-retrieval accuracy by up to 15\% while leaving reasoning intact, whereas
disabling "reasoning circuits" reduces multi-step inference by a comparable
margin. At the neuron level, we observe task-specific firing patterns, though
these effects are less robust, consistent with neuronal polysemanticity.Our
results provide the first causal evidence that recall and reasoning rely on
separable but interacting circuits in transformer models. These findings
advance mechanistic interpretability by linking circuit-level structure to
functional specialization and demonstrate how controlled datasets and causal
interventions can yield mechanistic insights into model cognition, informing
safer deployment of large language models.

</details>


### [294] [Distributed Low-Communication Training with Decoupled Momentum Optimization](https://arxiv.org/abs/2510.03371)
*Sasho Nedelkoski,Alexander Acker,Odej Kao,Soeren Becker,Dominik Scheinert*

Main category: cs.LG

TL;DR: 提出一种减少分布式模型训练通信开销的方法，结合低频同步和梯度动量压缩。


<details>
  <summary>Details</summary>
Motivation: 降低对高带宽互联的依赖，使分布式计算资源可用于大规模模型训练。

Method: 将优化器动量视为信号，通过DCT分解为高低频成分，仅同步高频成分。

Result: 通信量减少16倍，适用于多种架构。

Conclusion: 提升了低带宽互联下分布式训练大规模模型的可行性。

Abstract: The training of large models demands substantial computational resources,
typically available only in data centers with high-bandwidth interconnects.
However, reducing the reliance on high-bandwidth interconnects between nodes
enables the use of distributed compute resources as an alternative to
centralized data center training. Building on recent advances in distributed
model training, we propose an approach that further reduces communication by
combining infrequent synchronizations across distributed model replicas with
gradient momentum compression. In particular, we treat the optimizer momentum
as a signal and decompose the Nesterov momentum into high- and low-frequency
components via the discrete cosine transform (DCT). Only the high-frequency
components are synchronized across model replicas every $H$ steps. Empirically,
our method achieves up to a $16\times$ reduction in communication compared to
the baseline DiLoCo, and it generalizes across architectures, including
transformer-based language models and convolutional neural networks for images.
Overall, this work advances the feasibility of training large models on
distributed nodes with low-bandwidth interconnects.

</details>


### [295] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: CPSC-DFKD提出了一种新的数据自由知识蒸馏方法，通过条件生成对抗网络和伪监督对比学习提升学生模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有DFKD方法无法区分不同类别样本分布，且缺乏多样性优化，限制了学生模型的学习效果。

Method: 使用条件生成对抗网络生成类别特定多样图像，改进生成器模块以区分类别分布，并引入伪监督对比学习。

Result: 在三个常用数据集上的实验验证了CPSC-DFKD对学生模型和生成器的性能提升。

Conclusion: CPSC-DFKD通过条件生成和对比学习解决了现有DFKD方法的局限性，显著提升了性能。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [296] [A Robust Clustered Federated Learning Approach for Non-IID Data with Quantity Skew](https://arxiv.org/abs/2510.03380)
*Michael Ben Ali,Imen Megdiche,André Peninou,Olivier Teste*

Main category: cs.LG

TL;DR: 本文提出了一种名为CORNFLQS的新型迭代聚类联邦学习算法，通过结合两种操作策略，解决了非独立同分布数据中的数量偏差问题，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的非独立同分布数据（尤其是数量偏差问题）对模型性能提出了挑战，现有聚类联邦学习方法缺乏系统性评估。

Method: 提出CORNFLQS算法，结合客户端选择最小化本地训练损失的策略和服务器基于模型相似性分组的策略，并进行系统性评估。

Result: 在六种图像分类数据集上的270种非独立同分布配置中，CORNFLQS在准确性和聚类质量上表现最佳，且对数量偏差扰动具有强鲁棒性。

Conclusion: CORNFLQS算法优于现有聚类联邦学习方法，为解决非独立同分布数据问题提供了有效解决方案。

Abstract: Federated Learning (FL) is a decentralized paradigm that enables a
client-server architecture to collaboratively train a global Artificial
Intelligence model without sharing raw data, thereby preserving privacy. A key
challenge in FL is Non-IID data. Quantity Skew (QS) is a particular problem of
Non-IID, where clients hold highly heterogeneous data volumes. Clustered
Federated Learning (CFL) is an emergent variant of FL that presents a promising
solution to Non-IID problem. It improves models' performance by grouping
clients with similar data distributions into clusters. CFL methods generally
fall into two operating strategies. In the first strategy, clients select the
cluster that minimizes the local training loss. In the second strategy, the
server groups clients based on local model similarities. However, most CFL
methods lack systematic evaluation under QS but present significant challenges
because of it. In this paper, we present two main contributions. The first one
is an evaluation of state-of-the-art CFL algorithms under various Non-IID
settings, applying multiple QS scenarios to assess their robustness. Our second
contribution is a novel iterative CFL algorithm, named CORNFLQS, which proposes
an optimal coordination between both operating strategies of CFL. Our approach
is robust against the different variations of QS settings. We conducted
intensive experiments on six image classification datasets, resulting in 270
Non-IID configurations. The results show that CORNFLQS achieves the highest
average ranking in both accuracy and clustering quality, as well as strong
robustness to QS perturbations. Overall, our approach outperforms actual CFL
algorithms.

</details>


### [297] [Cross-Modal Reconstruction Pretraining for Ramp Flow Prediction at Highway Interchanges](https://arxiv.org/abs/2510.03381)
*Yongchao Li,Jun Chen,Zhuoxuan Li,Chao Gao,Yang Li,Chu Zhang,Changyin Dong*

Main category: cs.LG

TL;DR: STDAE是一种两阶段框架，通过跨模态重建预训练解决高速公路匝道流量预测问题，无需实时匝道检测器。


<details>
  <summary>Details</summary>
Motivation: 高速公路匝道缺乏实时检测器，导致交通预测存在盲区。

Method: 提出STDAE框架，第一阶段通过主线路数据重建历史匝道流量，第二阶段将学习到的表示与GWNet等模型结合进行预测。

Result: 在三个真实数据集上，STDAE-GWNet优于13种先进基线，性能接近使用历史匝道数据的模型。

Conclusion: STDAE能有效克服检测器稀缺问题，并具有即插即用的潜力。

Abstract: Interchanges are crucial nodes for vehicle transfers between highways, yet
the lack of real-time ramp detectors creates blind spots in traffic prediction.
To address this, we propose a Spatio-Temporal Decoupled Autoencoder (STDAE), a
two-stage framework that leverages cross-modal reconstruction pretraining. In
the first stage, STDAE reconstructs historical ramp flows from mainline data,
forcing the model to capture intrinsic spatio-temporal relations. Its decoupled
architecture with parallel spatial and temporal autoencoders efficiently
extracts heterogeneous features. In the prediction stage, the learned
representations are integrated with models such as GWNet to enhance accuracy.
Experiments on three real-world interchange datasets show that STDAE-GWNET
consistently outperforms thirteen state-of-the-art baselines and achieves
performance comparable to models using historical ramp data. This demonstrates
its effectiveness in overcoming detector scarcity and its plug-and-play
potential for diverse forecasting pipelines.

</details>


### [298] [Studying the Korean Word-Chain Game with RLVR:Mitigating Reward Conflicts via Curriculum Learning](https://arxiv.org/abs/2510.03394)
*Donghwan Rho*

Main category: cs.LG

TL;DR: 研究使用RLVR训练LLM解决韩语单词接龙游戏，发现规则奖励冲突，并通过课程学习缓解冲突。


<details>
  <summary>Details</summary>
Motivation: 探索RLVR在逻辑谜题中的应用，特别是在多语言环境下。

Method: 使用RLVR和课程学习方案处理奖励冲突。

Result: 实验证明课程学习能有效缓解奖励冲突。

Conclusion: 鼓励进一步研究多语言谜题任务。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for training large language models (LLMs) with stronger reasoning abilities. It
has also been applied to a variety of logic puzzles. In this work, we study the
Korean word-chain game using RLVR. We show that rule-derived rewards can
naturally conflict, and demonstrate through experiments that a
curriculum-learning scheme mitigates these conflicts. Our findings motivate
further studies of puzzle tasks in diverse languages.

</details>


### [299] [Training Variation of Physically-Informed Deep Learning Models](https://arxiv.org/abs/2510.03416)
*Ashley Lenau,Dennis Dimiduk,Stephen R. Niezgoda*

Main category: cs.LG

TL;DR: 论文探讨了深度学习网络中损失函数的可靠性问题，并通过Pix2Pix网络案例研究了不同损失函数在训练中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着物理信息损失函数的流行，如何评估损失函数在训练网络时的可靠性成为一个重要问题。

Method: 使用Pix2Pix网络预测高弹性对比复合材料的应力场，并比较几种不同损失函数的表现。

Result: 不同损失函数在收敛性、准确性和应力平衡方面表现出不同程度的差异。

Conclusion: 建议在报告中加入模型变异性分析，以更公平地比较不同方法。

Abstract: A successful deep learning network is highly dependent not only on the
training dataset, but the training algorithm used to condition the network for
a given task. The loss function, dataset, and tuning of hyperparameters all
play an essential role in training a network, yet there is not much discussion
on the reliability or reproducibility of a training algorithm. With the rise in
popularity of physics-informed loss functions, this raises the question of how
reliable one's loss function is in conditioning a network to enforce a
particular boundary condition. Reporting the model variation is needed to
assess a loss function's ability to consistently train a network to obey a
given boundary condition, and provides a fairer comparison among different
methods. In this work, a Pix2Pix network predicting the stress fields of high
elastic contrast composites is used as a case study. Several different loss
functions enforcing stress equilibrium are implemented, with each displaying
different levels of variation in convergence, accuracy, and enforcing stress
equilibrium across many training sessions. Suggested practices in reporting
model variation are also shared.

</details>


### [300] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 提出了一种多任务神经扩散过程（MT-NDP）框架，用于风电功率预测，优于单任务NDP和高斯过程（GP），尤其在异常风机上表现更优。


<details>
  <summary>Details</summary>
Motivation: 风电功率预测的不确定性对电网集成和风电场可靠运行至关重要。

Method: 扩展神经扩散过程（NDPs）为多任务框架（MT-NDP），引入任务编码器捕捉风机间相关性，支持少样本适应。

Result: MT-NDP在点预测准确性和校准性上优于单任务NDP和GP，尤其对偏离平均行为的风机效果显著。

Conclusion: NDP模型提供可扩展且校准的预测，适用于实际部署，支持调度和维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [301] [Memory-Efficient Backpropagation for Fine-Tuning LLMs on Resource-Constrained Mobile Devices](https://arxiv.org/abs/2510.03425)
*Congzheng Song,Xinyu Tang*

Main category: cs.LG

TL;DR: 提出了一种内存高效的微调方法MeBP，适用于移动设备，平衡内存使用和计算时间，优于零阶优化方法。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的移动设备上微调大语言模型时内存消耗高的问题。

Method: 提出内存高效的反向传播实现（MeBP），减少内存占用，同时保持计算效率。

Result: 在iPhone 15 Pro Max上验证，0.5B到4B参数的LLM可用少于1GB内存微调。

Conclusion: MeBP在内存和计算时间上提供了更好的权衡，适用于移动设备。

Abstract: Fine-tuning large language models (LLMs) with backpropagation\textemdash even
for a subset of parameters such as LoRA\textemdash can be much more
memory-consuming than inference and is often deemed impractical for
resource-constrained mobile devices. Alternative methods, such as zeroth-order
optimization (ZO), can greatly reduce the memory footprint but come at the cost
of significantly slower model convergence (10$\times$ to 100$\times$ more steps
than backpropagation). We propose a memory-efficient implementation of
backpropagation (MeBP) on mobile devices that provides better trade-off between
memory usage and compute time, while converging faster and achieving better
performance than the ZO baseline. We verify the effectiveness of MeBP on an
iPhone 15 Pro Max and show that various LLMs, ranging from 0.5B to 4B
parameters, can be fine-tuned using less than 1GB of memory. We release an
example of the MeBP implementation at https://github.com/apple/ml-mebp.

</details>


### [302] [Generalized Orders of Magnitude for Scalable, Parallel, High-Dynamic-Range Computation](https://arxiv.org/abs/2510.03426)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

TL;DR: GOOMs（广义数量级）是一种扩展传统数量级的方法，支持更大动态范围的数值计算，解决了传统浮点数在长序列计算中的数值溢出或下溢问题。


<details>
  <summary>Details</summary>
Motivation: 许多领域（如深度学习和金融）需要在长序列中进行实数计算，传统浮点数常导致数值溢出或下溢，限制了计算范围和稳定性。

Method: 提出GOOMs，结合高效的并行前缀扫描算法，支持在GPU等并行硬件上执行。

Result: 实验表明，GOOMs在矩阵乘积、Lyapunov指数谱估计和深度循环神经网络中表现出色，解决了传统方法无法解决的问题。

Conclusion: GOOMs为高动态范围应用提供了可扩展且数值稳定的替代方案。

Abstract: Many domains, from deep learning to finance, require compounding real numbers
over long sequences, often leading to catastrophic numerical underflow or
overflow. We introduce generalized orders of magnitude (GOOMs), a principled
extension of traditional orders of magnitude that incorporates floating-point
numbers as a special case, and which in practice enables stable computation
over significantly larger dynamic ranges of real numbers than previously
possible. We implement GOOMs, along with an efficient custom parallel prefix
scan, to support native execution on parallel hardware such as GPUs. We
demonstrate that our implementation of GOOMs outperforms traditional approaches
with three representative experiments, all of which were previously considered
impractical or impossible, and now become possible and practical: (1)
compounding real matrix products far beyond standard floating-point limits; (2)
estimating spectra of Lyapunov exponents in parallel, orders of magnitude
faster than with previous methods, applying a novel selective-resetting method
to prevent state colinearity; and (3) capturing long-range dependencies in deep
recurrent neural networks with non-diagonal recurrent states, computed in
parallel via a prefix scan, without requiring any form of stabilization. Our
results show that our implementation of GOOMs, combined with efficient parallel
scanning, offers a scalable and numerically robust alternative to conventional
floating-point numbers for high-dynamic-range applications.

</details>


### [303] [LHGEL: Large Heterogeneous Graph Ensemble Learning using Batch View Aggregation](https://arxiv.org/abs/2510.03432)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: LHGEL是一个针对大规模异构图学习的集成框架，通过批量采样、残差注意力和多样性正则化解决异构图学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于网络的规模、节点和边类型的异质性、节点特征的多样性以及复杂的局部邻域结构，从大规模异构图学习具有挑战性。

Method: 提出LHGEL框架，包含批量视图聚合、残差注意力和多样性正则化三个关键组件。

Result: 在五个真实异构图数据集上，LHGEL显著优于现有方法。

Conclusion: LHGEL通过集成学习有效捕捉图的异质性，并在理论和实验上验证了其优越性。

Abstract: Learning from large heterogeneous graphs presents significant challenges due
to the scale of networks, heterogeneity in node and edge types, variations in
nodal features, and complex local neighborhood structures. This paper advocates
for ensemble learning as a natural solution to this problem, whereby training
multiple graph learners under distinct sampling conditions, the ensemble
inherently captures different aspects of graph heterogeneity. Yet, the crux
lies in combining these learners to meet global optimization objective while
maintaining computational efficiency on large-scale graphs. In response, we
propose LHGEL, an ensemble framework that addresses these challenges through
batch sampling with three key components, namely batch view aggregation,
residual attention, and diversity regularization. Specifically, batch view
aggregation samples subgraphs and forms multiple graph views, while residual
attention adaptively weights the contributions of these views to guide node
embeddings toward informative subgraphs, thereby improving the accuracy of base
learners. Diversity regularization encourages representational disparity across
embedding matrices derived from different views, promoting model diversity and
ensemble robustness. Our theoretical study demonstrates that residual attention
mitigates gradient vanishing issues commonly faced in ensemble learning.
Empirical results on five real heterogeneous networks validate that our LHGEL
approach consistently outperforms its state-of-the-art competitors by
substantial margin. Codes and datasets are available at
https://github.com/Chrisshen12/LHGEL.

</details>


### [304] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 本文研究了核变化点检测（KCPD）在依赖数据（如文本）中的应用，证明了其一致性，并通过模拟和实证研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有理论在独立假设下证明了KCPD的一致性，但现实中的序列数据（如文本）具有强依赖性，因此需要研究KCPD在依赖数据中的表现。

Method: 通过理论分析证明了KCPD在m-依赖数据中的一致性，并利用LLM生成合成文本进行模拟验证。同时，使用现代嵌入技术对KCPD在文本分割中的表现进行了实证研究。

Result: 理论证明了KCPD在m-依赖数据中的一致性，模拟和实证研究显示KCPD在文本分割任务中优于基线方法。

Conclusion: KCPD不仅在理论和模拟中表现出可靠性，在实际文本分割任务中也具有有效性。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [305] [The Argument is the Explanation: Structured Argumentation for Trust in Agents](https://arxiv.org/abs/2510.03442)
*Ege Cakar,Per Ola Kristensson*

Main category: cs.LG

TL;DR: 论文提出了一种基于结构化论证的AI可解释性方法，通过验证推理链而非机制透明性，实现了高性能的论证分类和风险评估。


<details>
  <summary>Details</summary>
Motivation: 社会通过验证可验证的论证来运作，但人类神经过程不可观察。AI可解释性应遵循这一原则，提供可验证的推理链。

Method: 使用结构化论证（如Bipolar ABA）构建论证图，支持/攻击关系，并通过多智能体协作实现风险评估。

Result: 在AAEC和Argumentative MicroTexts数据集上达到SOTA性能（94.44和0.81 macro F1），并支持幻觉检测和迭代优化。

Conclusion: 结构化论证为AI可解释性提供了新方向，支持透明协作和验证，且易于部署。

Abstract: Humans are black boxes -- we cannot observe their neural processes, yet
society functions by evaluating verifiable arguments. AI explainability should
follow this principle: stakeholders need verifiable reasoning chains, not
mechanistic transparency. We propose using structured argumentation to provide
a level of explanation and verification neither interpretability nor
LLM-generated explanation is able to offer. Our pipeline achieves
state-of-the-art 94.44 macro F1 on the AAEC published train/test split (5.7
points above prior work) and $0.81$ macro F1, $\sim$0.07 above previous
published results with comparable data setups, for Argumentative MicroTexts
relation classification, converting LLM text into argument graphs and enabling
verification at each inferential step. We demonstrate this idea on multi-agent
risk assessment using the Structured What-If Technique, where specialized
agents collaborate transparently to carry out risk assessment otherwise
achieved by humans alone. Using Bipolar Assumption-Based Argumentation, we
capture support/attack relationships, thereby enabling automatic hallucination
detection via fact nodes attacking arguments. We also provide a verification
mechanism that enables iterative refinement through test-time feedback without
retraining. For easy deployment, we provide a Docker container for the
fine-tuned AMT model, and the rest of the code with the Bipolar ABA Python
package on GitHub.

</details>


### [306] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 论文通过解析残差网络的数学结构，验证了其类似于浅层模型集合的直觉，并提出了残差扩展定理来解释深度网络的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解为什么深度残差网络（如ResNet和Transformer）如此有效，尤其是其深度与性能之间的关系。

Method: 提出残差扩展定理，分析残差网络的数学结构，证明深度增加等同于隐式集合的扩展。

Result: 发现残差网络具有层次化的集合结构，解释了归一化层的必要性，并提出了一种新的缩放方法。

Conclusion: 通过数学分析揭示了残差网络的工作原理，为归一化层和缩放方法提供了理论基础。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [307] [How to Set $β_1, β_2$ in Adam: An Online Learning Perspective](https://arxiv.org/abs/2510.03478)
*Quan Nguyen*

Main category: cs.LG

TL;DR: 论文分析了Adam优化器中动量因子β₁和β₂的最优设置问题，提出了更通用的理论分析，覆盖了β₁≠√β₂的情况，并证明了现有边界的最优性。


<details>
  <summary>Details</summary>
Motivation: Adam优化器在大规模机器学习中表现优异，但其动量因子β₁和β₂的最优设置缺乏理论支持，尤其是β₁≠√β₂的情况。

Method: 通过将Adam视为FTRL算法的实例，推导出适用于β₁≥√β₂和β₁≤√β₂的新理论分析。

Result: 新分析严格推广了现有边界，并证明了在β₁=√β₂时对非知情对手是次优的。

Conclusion: 论文填补了Adam优化器动量因子设置的理论空白，为实际应用提供了更全面的理论支持。

Abstract: While Adam is one of the most effective optimizer for training large-scale
machine learning models, a theoretical understanding of how to optimally set
its momentum factors, $\beta_1$ and $\beta_2$, remains largely incomplete.
  Prior works have shown that Adam can be seen as an instance of
Follow-the-Regularized-Leader (FTRL), one of the most important class of
algorithms in online learning.
  The prior analyses in these works required setting $\beta_1 =
\sqrt{\beta_2}$, which does not cover the more practical cases with $\beta_1
\neq \sqrt{\beta_2}$.
  We derive novel, more general analyses that hold for both $\beta_1 \geq
\sqrt{\beta_2}$ and $\beta_1 \leq \sqrt{\beta_2}$.
  In both cases, our results strictly generalize the existing bounds.
  Furthermore, we show that our bounds are tight in the worst case.
  We also prove that setting $\beta_1 = \sqrt{\beta_2}$ is optimal for an
oblivious adversary, but sub-optimal for an non-oblivious adversary.

</details>


### [308] [Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains](https://arxiv.org/abs/2510.03486)
*Anupam Panwar,Himadri Pal,Jiali Chen,Kyle Cho,Riddick Jiang,Miao Zhao,Rajiv Krishnamurthy*

Main category: cs.LG

TL;DR: RADF框架通过mSelect技术解决大规模分布式系统中的异常检测挑战，包括数据量大、异构性和根因分析困难，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大规模分布式系统中异常检测的三大挑战：数据量大、异构性需求以及根因分析困难。

Method: 提出RADF框架，采用mSelect技术自动选择算法和调参，并具备后检测能力以加速根因分析。

Result: 在9个公开数据集中，RADF在5个数据集上AUC表现优于现有模型，7个数据集AUC超过0.85。

Conclusion: RADF通过自动化技术和后检测能力，显著提升了异常检测的效率和准确性。

Abstract: Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.

</details>


### [309] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 本文研究了有限时间离线强化学习中的策略评估和优化问题，提出了在轨迹数据假设下的高效学习算法，并改进了现有方法的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，在仅假设数据覆盖性和线性可实现性的情况下，策略评估和优化无法实现统计高效学习。本文旨在解决这一问题，特别是在轨迹数据假设下。

Method: 提出了一种统计高效的策略评估学习器，并对现有策略优化方法的样本复杂度进行了更严格的分析。

Result: 在轨迹数据假设下，实现了策略评估的统计高效学习，并改进了策略优化的样本复杂度。

Conclusion: 本文证明了在轨迹数据假设下，策略评估和优化可以实现统计高效学习，为离线强化学习提供了新的理论支持。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [310] [D2 Actor Critic: Diffusion Actor Meets Distributional Critic](https://arxiv.org/abs/2510.03508)
*Lunjun Zhang,Shuo Han,Hanrui Lyu,Bradly C Stadie*

Main category: cs.LG

TL;DR: D2AC是一种新的无模型强化学习算法，通过稳定的学习过程和鲁棒的分布评论家，在多个RL任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够有效在线训练扩散策略的算法，避免高方差和复杂的时间反向传播。

Method: 结合分布RL和双Q学习，提出稳定的策略改进目标和鲁棒的分布评论家。

Result: 在18个困难RL任务中表现优异，包括Humanoid、Dog和Shadow Hand等领域。

Conclusion: D2AC在标准基准和生物启发的任务中均展现出强大的鲁棒性和泛化能力。

Abstract: We introduce D2AC, a new model-free reinforcement learning (RL) algorithm
designed to train expressive diffusion policies online effectively. At its core
is a policy improvement objective that avoids the high variance of typical
policy gradients and the complexity of backpropagation through time. This
stable learning process is critically enabled by our second contribution: a
robust distributional critic, which we design through a fusion of
distributional RL and clipped double Q-learning. The resulting algorithm is
highly effective, achieving state-of-the-art performance on a benchmark of
eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains,
spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard
benchmarks, we also evaluate a biologically motivated predator-prey task to
examine the behavioral robustness and generalization capacity of our approach.

</details>


### [311] [Task-Level Contrastiveness for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2510.03509)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出了一种任务级对比性方法，提升小样本分类和元学习在多样化领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨领域泛化上表现不佳，存在低准确率、高计算成本和限制性假设的问题。

Method: 通过定义任务增强和任务级对比损失，实现任务表示的无监督聚类。

Result: 在MetaDataset基准测试中表现优异，提升了泛化能力和计算效率。

Conclusion: 该方法轻量且易于集成，无需任务领域先验知识即可显著提升性能。

Abstract: Few-shot classification and meta-learning methods typically struggle to
generalize across diverse domains, as most approaches focus on a single
dataset, failing to transfer knowledge across various seen and unseen domains.
Existing solutions often suffer from low accuracy, high computational costs,
and rely on restrictive assumptions. In this paper, we introduce the notion of
task-level contrastiveness, a novel approach designed to address issues of
existing methods. We start by introducing simple ways to define task
augmentations, and thereafter define a task-level contrastive loss that
encourages unsupervised clustering of task representations. Our method is
lightweight and can be easily integrated within existing few-shot/meta-learning
algorithms while providing significant benefits. Crucially, it leads to
improved generalization and computational efficiency without requiring prior
knowledge of task domains. We demonstrate the effectiveness of our approach
through different experiments on the MetaDataset benchmark, where it achieves
superior performance without additional complexity.

</details>


### [312] [A Lightweight Federated Learning Approach for Privacy-Preserving Botnet Detection in IoT](https://arxiv.org/abs/2510.03513)
*Taha M. Mahmoud,Naima Kaabouch*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的轻量级、隐私保护的物联网僵尸网络检测框架，解决了传统方法在可扩展性、隐私和适应性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 物联网的快速发展带来了僵尸网络攻击的增加，传统检测方法在资源受限的物联网环境中难以兼顾可扩展性、隐私和适应性。

Method: 采用联邦学习框架，使分布式设备能够协作训练模型，无需交换原始数据，同时引入高效的通信聚合策略以减少开销。

Result: 在基准物联网僵尸网络数据集上的实验表明，该框架在保持高检测精度的同时显著降低了通信成本。

Conclusion: 联邦学习为物联网生态系统提供了一种可扩展、安全且隐私保护的入侵检测解决方案。

Abstract: The rapid growth of the Internet of Things (IoT) has expanded opportunities
for innovation but also increased exposure to botnet-driven cyberattacks.
Conventional detection methods often struggle with scalability, privacy, and
adaptability in resource-constrained IoT environments. To address these
challenges, we present a lightweight and privacy-preserving botnet detection
framework based on federated learning. This approach enables distributed
devices to collaboratively train models without exchanging raw data, thus
maintaining user privacy while preserving detection accuracy. A
communication-efficient aggregation strategy is introduced to reduce overhead,
ensuring suitability for constrained IoT networks. Experiments on benchmark IoT
botnet datasets demonstrate that the framework achieves high detection accuracy
while substantially reducing communication costs. These findings highlight
federated learning as a practical path toward scalable, secure, and
privacy-aware intrusion detection for IoT ecosystems.

</details>


### [313] [RAPID: An Efficient Reinforcement Learning Algorithm for Small Language Models](https://arxiv.org/abs/2510.03515)
*Lianghuan Huang,Sagnik Anupam,Insup Lee,Shuo Li,Osbert Bastani*

Main category: cs.LG

TL;DR: RAPID是一种新型强化学习算法，显著减少训练时间，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习算法资源密集且耗时，RAPID旨在优化计算资源利用。

Method: 通过大批量推理和小批量策略梯度更新，结合组优势估计和重要性加权估计。

Result: 实验显示RAPID在三个基准测试中减少11%-34%的运行时间，且准确性相当或更好。

Conclusion: RAPID是一种高效且有效的强化学习算法，适用于小语言模型的微调。

Abstract: Reinforcement learning (RL) has emerged as a promising strategy for
finetuning small language models (SLMs) to solve targeted tasks such as math
and coding. However, RL algorithms tend to be resource-intensive, taking a
significant amount of time to train. We propose RAPID, a novel RL algorithm
that can substantially reduce the running time of RL. Our key insight is that
RL tends to be costly due to the need to perform both inference and
backpropagation during training. To maximize use of computational resources,
our algorithm performs inference in large batches, and then performs off-policy
policy gradient updates in mini-batches. For off-policy updates, we incorporate
group advantage estimation into the policy gradient algorithm, and derive an
importance weighted estimator to correct for the bias arising from off-policy
learning. Our experiments demonstrate that our algorithm can reduce running
time by 11%-34% on three benchmarks compared to state-of-the-art RL algorithms
while maintaining similar or better accuracy.

</details>


### [314] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: 论文提出了一种名为CS-RLHF的新方法，通过训练成本模型和采用修正的惩罚机制，解决了传统CMDP方法在安全性和效率上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CMDP的方法在安全性和计算效率上存在不足，尤其是对奖励和成本函数的依赖以及双变量调优的复杂性。

Method: CS-RLHF通过训练大规模语料库的成本模型，并采用基于惩罚的优化方法，直接保证安全性约束的可行性。

Result: 实验表明，CS-RLHF在效率和安全性上优于现有方法，对常规和对抗性提示的响应效率提高了至少5倍。

Conclusion: CS-RLHF为LLM的安全性提供了一种可证明的保证，同时显著提升了模型输出的实用性和安全性。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [315] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: mLaSDI通过多阶段学习提升LaSDI的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统LaSDI在训练中强制潜在动力学可能影响重建精度。

Method: 引入多阶段LaSDI，通过顺序学习解码器修正残差。

Result: 在1D-1V Vlasov方程中，mLaSDI表现优于标准LaSDI。

Conclusion: mLaSDI显著降低了预测误差和训练时间。

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [316] [CrossLag: Predicting Major Dengue Outbreaks with a Domain Knowledge Informed Transformer](https://arxiv.org/abs/2510.03566)
*Ashwin Prabu,Nhat Thanh Tran,Guofa Zhou,Jack Xin*

Main category: cs.LG

TL;DR: CrossLag是一种新型的注意力机制，结合环境信息，通过低参数量的方式将外生数据中的滞后信号融入Transformer架构，显著提升了登革热大爆发的预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以预测需要及时发出公共卫生警告的登革热大爆发，而气候和海洋异常的变化通常滞后于疫情爆发。

Method: 提出CrossLag注意力机制，结合TimeXer（一种区分外生-内生输入的通用Transformer）作为基线模型。

Result: 在新加坡登革热数据的24周预测窗口中，CrossLag显著优于TimeXer，尤其是在检测和预测大爆发方面。

Conclusion: CrossLag通过融合滞后环境信号，有效提升了登革热大爆发的预测能力，为公共卫生预警提供了有力工具。

Abstract: A variety of models have been developed to forecast dengue cases to date.
However, it remains a challenge to predict major dengue outbreaks that need
timely public warnings the most. In this paper, we introduce CrossLag, an
environmentally informed attention that allows for the incorporation of lagging
endogenous signals behind the significant events in the exogenous data into the
architecture of the transformer at low parameter counts. Outbreaks typically
lag behind major changes in climate and oceanic anomalies. We use TimeXer, a
recent general-purpose transformer distinguishing exogenous-endogenous inputs,
as the baseline for this study. Our proposed model outperforms TimeXer by a
considerable margin in detecting and predicting major outbreaks in Singapore
dengue data over a 24-week prediction window.

</details>


### [317] [Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs](https://arxiv.org/abs/2510.03567)
*Fatmazohra Rezkellah,Ramzi Dakhmouche*

Main category: cs.LG

TL;DR: 提出了一种统一的方法，通过最小干预LLM权重来保护隐私和防止攻击，无需分类器且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，需要确保隐私保护和安全性，尤其是敏感信息的遗忘和对抗越狱攻击。

Method: 采用约束优化方法，最小化权重干预，使特定词汇不可达或增强模型对攻击的鲁棒性。

Result: 提出的点式约束干预方法性能优于现有技术，计算成本更低。

Conclusion: 该方法在隐私保护和安全性方面表现优越，且无需额外分类器。

Abstract: With the increasing adoption of Large Language Models (LLMs), more
customization is needed to ensure privacy-preserving and safe generation. We
address this objective from two critical aspects: unlearning of sensitive
information and robustness to jail-breaking attacks. We investigate various
constrained optimization formulations that address both aspects in a
\emph{unified manner}, by finding the smallest possible interventions on LLM
weights that either make a given vocabulary set unreachable or embed the LLM
with robustness to tailored attacks by shifting part of the weights to a
\emph{safer} region. Beyond unifying two key properties, this approach
contrasts with previous work in that it doesn't require an oracle classifier
that is typically not available or represents a computational overhead.
Surprisingly, we find that the simplest point-wise constraint-based
intervention we propose leads to better performance than max-min interventions,
while having a lower computational cost. Comparison against state-of-the-art
defense methods demonstrates superior performance of the proposed approach.

</details>


### [318] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: IMMFM框架通过多时间点联合优化，解决了稀疏采样和高维轨迹的学习问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型在处理稀疏采样和高维轨迹时，通常只能学习成对转移，无法捕捉连续动态。

Method: 提出IMMFM框架，使用分段二次插值路径作为平滑目标，联合优化漂移和数据驱动的扩散系数。

Result: 在合成基准和真实神经影像数据上，IMMFM在预测准确性和下游任务中表现更优。

Conclusion: IMMFM能够捕捉内在随机性，处理稀疏采样，并生成个体化轨迹，优于现有方法。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [319] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 论文提出了一种基于RNN+GNN的故障检测方法，并首次引入GraphSAGE和Graph Attention（GAT, GATv2）进行故障诊断，通过系统比较验证了RGATv2的优越泛化能力。


<details>
  <summary>Details</summary>
Motivation: 电力配电网络的故障检测对系统可靠性至关重要，但现有方法对电网拓扑变化的鲁棒性不足，需要更先进的GNN架构来提升性能。

Method: 采用RNN+GNN的管道模型，首次引入GraphSAGE和Graph Attention（GAT, GATv2），并与传统RGCN和纯RNN模型进行对比。

Result: 在IEEE 123节点网络中，RGATv2表现最佳，F1分数仅下降约12%，而纯RNN模型下降高达60%，其他RGNN变体下降约25%。

Conclusion: RGATv2在故障诊断中具有显著优势，尤其在面对电网拓扑变化时表现出更强的泛化能力。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [320] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 提出两种高效的测试时扩展策略（TTAug和TTAdapt），提升小型视觉语言模型的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决小型视觉语言模型泛化能力和下游任务性能较弱的问题，同时避免现有测试时扩展方法的高计算成本。

Method: 提出两种策略：TTAug（测试时增强，无需参数更新）和TTAdapt（测试时适应，利用TTAug生成的伪标签调整参数）。

Result: 在九个基准测试中表现一致提升，且适用于资源受限环境。

Conclusion: 方法具有通用性，适用于不同规模的模型和视觉语言模型，无需额外调优。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [321] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出了一种边界条件保证的进化Kolmogorov-Arnold网络（BEKAN），通过三种方法精确满足Dirichlet、周期和Neumann边界条件，在PDE求解中表现优于MLP和B-splines KAN。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在PDE求解中难以精确满足边界条件的问题。

Method: 使用高斯径向基函数、周期性层和最小二乘公式分别处理Dirichlet、周期和Neumann边界条件。

Result: BEKAN在多种边界值问题中表现出更高的准确性。

Conclusion: BEKAN提升了KAN在PDE求解中的能力，推动了科学计算和工程应用的发展。

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [322] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: Latent MoS模型通过捕捉对称性混合的潜在因子，提升了动态学习的样本效率和表达能力。


<details>
  <summary>Details</summary>
Motivation: 有限系统测量（如低分辨率传感器）需要样本高效学习，对称性作为归纳偏置可提升效率。现有方法假设单一全局对称群且分离对称发现与动态学习，导致表达受限和误差累积。

Method: 提出Latent MoS模型，捕捉对称性混合的潜在因子，并通过分层架构捕获长期等变性。

Result: 在多种物理系统中，Latent MoS在插值和外推任务中优于基线，并提供可解释的潜在表示。

Conclusion: Latent MoS通过局部保留对称变换和分层设计，实现了高效动态学习与可解释性。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [323] [FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors](https://arxiv.org/abs/2510.03589)
*Ankit Bhardwaj,Ananth Balashankar,Lakshminarayanan Subramanian*

Main category: cs.LG

TL;DR: FieldFormer是一种基于Transformer的框架，用于无网格时空场重建，结合数据驱动和物理约束，在稀疏、噪声数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理稀疏、噪声、不规则的时空传感器数据时，要么忽略控制PDE，要么无法扩展。

Method: FieldFormer通过可学习的速度缩放距离度量收集局部邻域，使用局部Transformer编码器预测，并通过PDE残差和边界惩罚保证物理一致性。

Result: 在三个基准测试中，FieldFormer比基线方法性能提升40%以上，RMSE<10^-2。

Conclusion: FieldFormer实现了高效、物理一致的场重建，适用于稀疏（0.4%-2%）和噪声（10%）数据。

Abstract: Spatio-temporal sensor data is often sparse, noisy, and irregular, and
existing interpolation or learning methods struggle here because they either
ignore governing PDEs or do not scale. We introduce FieldFormer, a
transformer-based framework for mesh-free spatio-temporal field reconstruction
that combines data-driven flexibility with physics-based structure. For each
query, FieldFormer gathers a local neighborhood using a learnable
velocity-scaled distance metric, enabling anisotropic adaptation to different
propagation regimes. Neighborhoods are built efficiently via per-batch offset
recomputation, and refined in an expectation-maximization style as the velocity
scales evolve. Predictions are made by a local transformer encoder, and physics
consistency is enforced through autograd-based PDE residuals and
boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat
equation, a vector-valued shallow-water system, and a realistic
advection-diffusion pollution simulation--FieldFormer consistently outperforms
strong baselines by more than 40%. Our results demonstrate that FieldFormer
enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field
reconstruction from sparse (0.4%-2%) and noisy(10%) data.

</details>


### [324] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 提出了一种多层移动边缘计算（MLMEC）框架，结合知识蒸馏（KD）技术，显著提升了跌倒检测系统的准确性和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化人口增加，跌倒检测系统的重要性上升，但现有系统在边缘设备模型大小和数据传输延迟方面存在挑战。

Method: 采用MLMEC框架，将架构分层处理，结合KD技术提升前端检测精度，后端提供额外学习经验。

Result: 在SisFall和FallAllD数据集上，KD分别提升准确率11.65%和2.78%，延迟降低54.15%和46.67%。

Conclusion: MLMEC框架结合KD技术显著提升了跌倒检测系统的性能和效率。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [325] [Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends](https://arxiv.org/abs/2510.03604)
*Yucheng Wang,Mohamed Ragab,Yubo Hou,Zhenghua Chen,Min Wu,Xiaoli Li*

Main category: cs.LG

TL;DR: 本文综述了针对涡扇发动机剩余使用寿命（RUL）预测的领域自适应（DA）技术，提出了一种新的分类法，并评估了相关方法。


<details>
  <summary>Details</summary>
Motivation: 涡扇发动机RUL预测对航空安全至关重要，但数据有限和分布偏移问题阻碍了数据驱动方法的有效性。

Method: 提出了一种基于方法、对齐和问题的多维分类法，并评估了DA技术在涡扇发动机数据集上的表现。

Result: 分类法提供了更全面的视角，评估结果为实践者提供了实用见解。

Conclusion: 未来研究应开发更有效的DA技术，以提升涡扇发动机RUL预测水平。

Abstract: Remaining Useful Life (RUL) prediction for turbofan engines plays a vital
role in predictive maintenance, ensuring operational safety and efficiency in
aviation. Although data-driven approaches using machine learning and deep
learning have shown potential, they face challenges such as limited data and
distribution shifts caused by varying operating conditions. Domain Adaptation
(DA) has emerged as a promising solution, enabling knowledge transfer from
source domains with abundant data to target domains with scarce data while
mitigating distributional shifts. Given the unique properties of turbofan
engines, such as complex operating conditions, high-dimensional sensor data,
and slower-changing signals, it is essential to conduct a focused review of DA
techniques specifically tailored to turbofan engines. To address this need,
this paper provides a comprehensive review of DA solutions for turbofan engine
RUL prediction, analyzing key methodologies, challenges, and recent
advancements. A novel taxonomy tailored to turbofan engines is introduced,
organizing approaches into methodology-based (how DA is applied),
alignment-based (where distributional shifts occur due to operational
variations), and problem-based (why certain adaptations are needed to address
specific challenges). This taxonomy offers a multidimensional view that goes
beyond traditional classifications by accounting for the distinctive
characteristics of turbofan engine data and the standard process of applying DA
techniques to this area. Additionally, we evaluate selected DA techniques on
turbofan engine datasets, providing practical insights for practitioners and
identifying key challenges. Future research directions are identified to guide
the development of more effective DA techniques, advancing the state of RUL
prediction for turbofan engines.

</details>


### [326] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM是一种优化器，旨在通过确定性探索状态空间来逃离局部最小值，找到全局最小值。它通过交替最小化和最大化误差来实现全局探索。


<details>
  <summary>Details</summary>
Motivation: 现有随机梯度下降算法（如ADAM）在逃离局部最小值时存在不确定性，Hill-ADAM旨在解决这一问题。

Method: Hill-ADAM通过分析ADAM优化器的步长，定义其局限性，并交替进行误差最小化和最大化以实现全局探索。

Result: Hill-ADAM在5种损失函数和12个图像颜色校正实例中进行了测试。

Conclusion: Hill-ADAM通过确定性探索有效逃离局部最小值，是一种有潜力的优化器。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [327] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: NBF是一种用于部分可观测系统中维护隐藏状态分布的算法，结合了经典滤波器的计算效率和深度生成模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 解决部分可观测系统中快速变化、多模态信念的跟踪问题，同时避免粒子贫化风险。

Method: 通过训练找到任务诱导的信念的潜在表示，将信念映射为固定长度的嵌入向量，并使用粒子式更新在嵌入空间中计算后验。

Result: 在三个部分可观测环境的状态估计任务中验证了NBF的有效性。

Conclusion: NBF成功结合了经典滤波器和深度生成模型的优势，适用于复杂信念跟踪任务。

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [328] [Predicting Stock Price Movement with LLM-Enhanced Tweet Emotion Analysis](https://arxiv.org/abs/2510.03633)
*An Vuong,Susan Gauch*

Main category: cs.LG

TL;DR: 论文提出了一种结合推特情感特征和历史股价的深度学习框架，用于预测次日股价显著变动，实验表明情感分析能显著提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 由于股票市场的波动性和对投资者情绪的敏感性，准确预测短期股价变动具有挑战性。

Method: 使用Meta的Llama 3.1-8B-Instruct预处理推特数据，结合三种情感分析方法（DistilRoBERTa和两种基于NRC词典的方法）提取情感特征，与历史股价数据一起训练LSTM模型。

Result: 实验结果显示，情感分析方法均提升了预测准确率，其中基于DistilRoBERTa的模型表现最佳，准确率从23.6%提升至38.5%。

Conclusion: 通过大语言模型预处理推特内容，情感分析的有效性得到提升，从而显著提高了股价变动预测的准确性。

Abstract: Accurately predicting short-term stock price movement remains a challenging
task due to the market's inherent volatility and sensitivity to investor
sentiment. This paper discusses a deep learning framework that integrates
emotion features extracted from tweet data with historical stock price
information to forecast significant price changes on the following day. We
utilize Meta's Llama 3.1-8B-Instruct model to preprocess tweet data, thereby
enhancing the quality of emotion features derived from three emotion analysis
approaches: a transformer-based DistilRoBERTa classifier from the Hugging Face
library and two lexicon-based methods using National Research Council Canada
(NRC) resources. These features are combined with previous-day stock price data
to train a Long Short-Term Memory (LSTM) model. Experimental results on TSLA,
AAPL, and AMZN stocks show that all three emotion analysis methods improve the
average accuracy for predicting significant price movements, compared to the
baseline model using only historical stock prices, which yields an accuracy of
13.5%. The DistilRoBERTa-based stock prediction model achieves the best
performance, with accuracy rising from 23.6% to 38.5% when using LLaMA-enhanced
emotion analysis. These results demonstrate that using large language models to
preprocess tweet content enhances the effectiveness of emotion analysis which
in turn improves the accuracy of predicting significant stock price movements.

</details>


### [329] [From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse](https://arxiv.org/abs/2510.03636)
*Rabeya Amin Jhuma,Mostafa Mohaimen Akand Faisal*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型中的上下文学习（ICL）在公共卫生情感分析中如何受到数据投毒攻击的干扰，并提出了光谱签名防御方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证ICL在公共卫生情感分析中的脆弱性，并探索防御方法以确保AI系统在健康相关社交媒体监测中的可靠性。

Method: 通过在人偏肺病毒（HMPV）推文中引入对抗性扰动（如同义词替换、否定插入和随机扰动），并应用光谱签名防御过滤投毒数据。

Result: 扰动导致情感标签翻转率高达67%，防御后ICL准确率稳定在46.7%，逻辑回归验证准确率达100%。

Conclusion: 研究揭示了ICL在攻击下的脆弱性，并证明光谱防御能有效保护数据集完整性，为AI在公共卫生领域的可靠部署提供了参考。

Abstract: This study explored how in-context learning (ICL) in large language models
can be disrupted by data poisoning attacks in the setting of public health
sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small
adversarial perturbations such as synonym replacement, negation insertion, and
randomized perturbation were introduced into the support examples. Even these
minor manipulations caused major disruptions, with sentiment labels flipping in
up to 67% of cases. To address this, a Spectral Signature Defense was applied,
which filtered out poisoned examples while keeping the data's meaning and
sentiment intact. After defense, ICL accuracy remained steady at around 46.7%,
and logistic regression validation reached 100% accuracy, showing that the
defense successfully preserved the dataset's integrity. Overall, the findings
extend prior theoretical studies of ICL poisoning to a practical, high-stakes
setting in public health discourse analysis, highlighting both the risks and
potential defenses for robust LLM deployment. This study also highlights the
fragility of ICL under attack and the value of spectral defenses in making AI
systems more reliable for health-related social media monitoring.

</details>


### [330] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 隐式模型通过迭代固定参数块实现无限深度网络，训练时内存需求低，测试时计算量增加可提升性能。本文通过非参数分析揭示了其表达能力随计算量增长的机制。


<details>
  <summary>Details</summary>
Motivation: 研究隐式模型在测试时增加计算量能提升性能的底层机制，填补理论空白。

Method: 通过非参数分析，严格数学表征隐式模型的表达能力随迭代次数增长的过程。

Result: 证明隐式模型的表达能力随测试时计算量增加而扩展，最终匹配更丰富的函数类。实验验证了理论。

Conclusion: 隐式模型通过迭代逐步表达更复杂映射，性能随计算量提升，理论在多个领域得到验证。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [331] [In-Vivo Training for Deep Brain Stimulation](https://arxiv.org/abs/2510.03643)
*Nicholas Carter,Arkaprava Gupta,Prateek Ganguli,Benedikt Dietrich,Vibhor Krishna,Samarjit Chakraborty*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的DBS方法，利用可测量的脑活动调整刺激参数，优于传统临床方法。


<details>
  <summary>Details</summary>
Motivation: 现有DBS模型依赖无法在患者中测量的生物标志物，限制了实际应用。

Method: 使用TD3强化学习代理，基于基底节模型训练，调整刺激参数。

Result: 相比传统DBS，新方法更有效抑制PD相关生物标志物。

Conclusion: 该方法为个性化RL代理训练提供了可能，适用于实际患者需求。

Abstract: Deep Brain Stimulation (DBS) is a highly effective treatment for Parkinson's
Disease (PD). Recent research uses reinforcement learning (RL) for DBS, with RL
agents modulating the stimulation frequency and amplitude. But, these models
rely on biomarkers that are not measurable in patients and are only present in
brain-on-chip (BoC) simulations. In this work, we present an RL-based DBS
approach that adapts these stimulation parameters according to brain activity
measurable in vivo. Using a TD3 based RL agent trained on a model of the basal
ganglia region of the brain, we see a greater suppression of biomarkers
correlated with PD severity compared to modern clinical DBS implementations.
Our agent outperforms the standard clinical approaches in suppressing PD
biomarkers while relying on information that can be measured in a real world
environment, thereby opening up the possibility of training personalized RL
agents specific to individual patient needs.

</details>


### [332] [SAFA-SNN: Sparsity-Aware On-Device Few-Shot Class-Incremental Learning with Fast-Adaptive Structure of Spiking Neural Network](https://arxiv.org/abs/2510.03648)
*Huijing Zhang,Muyang Cao,Linshan Jiang,Xin Du,Di Yu,Changze Lv,Shuiguang Deng*

Main category: cs.LG

TL;DR: 提出了一种基于脉冲神经网络（SNN）的稀疏感知快速自适应方法（SAFA-SNN），用于设备端少样本类增量学习（FSCIL），显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 解决设备端在数据不足情况下进行类增量学习的挑战，同时降低能耗。

Method: 采用稀疏条件神经元动态和零阶优化，结合子空间投影增强新类别的区分性。

Result: 在多个数据集上表现优于基线方法，性能提升显著且能耗降低20%。

Conclusion: SAFA-SNN是一种高效且实用的设备端FSCIL解决方案。

Abstract: Continuous learning of novel classes is crucial for edge devices to preserve
data privacy and maintain reliable performance in dynamic environments.
However, the scenario becomes particularly challenging when data samples are
insufficient, requiring on-device few-shot class-incremental learning (FSCIL)
to maintain consistent model performance. Although existing work has explored
parameter-efficient FSCIL frameworks based on artificial neural networks
(ANNs), their deployment is still fundamentally constrained by limited device
resources. Inspired by neural mechanisms, Spiking neural networks (SNNs)
process spatiotemporal information efficiently, offering lower energy
consumption, greater biological plausibility, and compatibility with
neuromorphic hardware than ANNs. In this work, we present an SNN-based method
for On-Device FSCIL, i.e., Sparsity-Aware and Fast Adaptive SNN (SAFA-SNN). We
first propose sparsity-conditioned neuronal dynamics, in which most neurons
remain stable while a subset stays active, thereby mitigating catastrophic
forgetting. To further cope with spike non-differentiability in gradient
estimation, we employ zeroth-order optimization. Moreover, during incremental
learning sessions, we enhance the discriminability of new classes through
subspace projection, which alleviates overfitting to novel classes. Extensive
experiments conducted on two standard benchmark datasets (CIFAR100 and
Mini-ImageNet) and three neuromorphic datasets (CIFAR-10-DVS, DVS128gesture,
and N-Caltech101) demonstrate that SAFA-SNN outperforms baseline methods,
specifically achieving at least 4.01% improvement at the last incremental
session on Mini-ImageNet and 20% lower energy cost over baseline methods with
practical implementation.

</details>


### [333] [LLM-Guided Evolutionary Program Synthesis for Quasi-Monte Carlo Design](https://arxiv.org/abs/2510.03650)
*Amir Sadikov*

Main category: cs.LG

TL;DR: 论文通过LLM引导的进化循环解决了两类QMC设计问题，包括低星差异点集和Sobol方向数优化，展示了自动化发现高质量QMC构造的能力。


<details>
  <summary>Details</summary>
Motivation: 解决高维积分中低差异点集和Sobol方向数的长期设计问题，提升QMC方法的性能。

Method: 采用两阶段方法：构造性代码提案和迭代数值优化，结合LLM引导的进化循环进行突变和选择。

Result: 在2D/3D点集上刷新了多项最佳记录，并在32维期权定价任务中显著降低了rQMC误差。

Conclusion: LLM驱动的进化程序合成可自动化发现高质量QMC构造，恢复经典设计并改进有限N结构问题。

Abstract: Low-discrepancy point sets and digital sequences underpin quasi-Monte Carlo
(QMC) methods for high-dimensional integration. We cast two long-standing QMC
design problems as program synthesis and solve them with an LLM-guided
evolutionary loop that mutates and selects code under task-specific fitness:
(i) constructing finite 2D/3D point sets with low star discrepancy, and (ii)
choosing Sobol' direction numbers that minimize randomized QMC error on
downstream integrands. Our two-phase procedure combines constructive code
proposals with iterative numerical refinement. On finite sets, we rediscover
known optima in small 2D cases and set new best-known 2D benchmarks for N >=
40, while matching most known 3D optima up to the proven frontier (N <= 8) and
reporting improved 3D benchmarks beyond. On digital sequences, evolving Sobol'
parameters yields consistent reductions in randomized quasi-Monte Carlo (rQMC)
mean-squared error for several 32-dimensional option-pricing tasks relative to
widely used Joe--Kuo parameters, while preserving extensibility to any sample
size and compatibility with standard randomizations. Taken together, the
results demonstrate that LLM-driven evolutionary program synthesis can automate
the discovery of high-quality QMC constructions, recovering classical designs
where they are optimal and improving them where finite-N structure matters.
Data and code are available at
https://github.com/hockeyguy123/openevolve-star-discrepancy.git.

</details>


### [334] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 研究探讨如何利用澳大利亚能源市场运营商（AEMO）的电价预测数据，开发可靠的电池储能系统（BESS）交易算法，以提高套利收益。


<details>
  <summary>Details</summary>
Motivation: 随着电网波动性增加，预测数据的实际应用价值未被充分挖掘，尤其是在BESS交易决策中。

Method: 分析AEMO预测的准确性模式（如时间、预测周期和地区差异），开发基于预测的BESS交易模型，并与无预测的基本算法对比。

Result: 创建了一种新型的预测驱动交易算法，并探索了机器学习技术提升预测的潜力。

Conclusion: 研究成果将优化能源市场交易模型，促进BESS更高效地融入市场运营。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [335] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器（SAEs）的可解释性与模型行为操控效果之间关联较弱，提出新特征选择标准Delta Token Confidence显著提升操控性能。


<details>
  <summary>Details</summary>
Motivation: 探讨稀疏自编码器的可解释性是否确实能有效指导模型行为操控。

Method: 在三个大型语言模型上训练90个SAEs，评估其可解释性和操控效果，并提出Delta Token Confidence作为新特征选择标准。

Result: 可解释性与操控效果仅弱相关（tau b≈0.298），新标准使操控性能提升52.52%，且相关性消失甚至变为负值。

Conclusion: 可解释性不能完全代表操控效果，Delta Token Confidence是更有效的特征选择标准。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [336] [Operationalizing Data Minimization for Privacy-Preserving LLM Prompting](https://arxiv.org/abs/2510.03662)
*Jijie Zhou,Niloofar Mireshghallah,Tianshi Li*

Main category: cs.LG

TL;DR: 论文提出了一种框架，用于量化在保持实用性的前提下最小化隐私泄露的数据披露，并通过优先级队列树搜索找到最优解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在消费者应用中的快速部署，用户频繁交换个人信息，导致隐私风险增加。

Method: 提出了一种框架，通过优先级队列树搜索在隐私有序的转换空间中定位最优数据最小化点。

Result: 实验表明，前沿LLM（如GPT-5）能容忍更强的数据最小化（85.7%的删减），而开源小模型（如Qwen2.5-0.5B）仅19.3%。

Conclusion: LLM难以直接预测最优数据最小化，存在过度分享的倾向，揭示了隐私和能力上的差距。

Abstract: The rapid deployment of large language models (LLMs) in consumer applications
has led to frequent exchanges of personal information. To obtain useful
responses, users often share more than necessary, increasing privacy risks via
memorization, context-based personalization, or security breaches. We present a
framework to formally define and operationalize data minimization: for a given
user prompt and response model, quantifying the least privacy-revealing
disclosure that maintains utility, and we propose a priority-queue tree search
to locate this optimal point within a privacy-ordered transformation space. We
evaluated the framework on four datasets spanning open-ended conversations
(ShareGPT, WildChat) and knowledge-intensive tasks with single-ground-truth
answers (CaseHold, MedQA), quantifying achievable data minimization with nine
LLMs as the response model. Our results demonstrate that larger frontier LLMs
can tolerate stronger data minimization while maintaining task quality than
smaller open-source models (85.7% redaction for GPT-5 vs. 19.3% for
Qwen2.5-0.5B). By comparing with our search-derived benchmarks, we find that
LLMs struggle to predict optimal data minimization directly, showing a bias
toward abstraction that leads to oversharing. This suggests not just a privacy
gap, but a capability gap: models may lack awareness of what information they
actually need to solve a task.

</details>


### [337] [Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning](https://arxiv.org/abs/2510.03669)
*Wenlong Deng,Yi Ren,Yushu Li,Boying Gong,Danica J. Sutherland,Xiaoxiao Li,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 论文提出了一种名为Token Hidden Reward (THR)的标记级指标，用于量化每个标记对正确响应概率的影响，并通过THR引导的重新加权算法动态控制强化学习中的探索与利用。


<details>
  <summary>Details</summary>
Motivation: 尽管可验证奖励的强化学习显著提升了大型语言模型的推理能力，但如何明确引导训练偏向探索或利用仍是一个未解决的问题。

Method: 引入THR指标，结合Group Relative Policy Optimization (GRPO)，设计了一种THR引导的重新加权算法，通过调整学习信号来偏向探索或利用。

Result: 在多样化的数学推理基准测试中验证了算法的有效性：正向THR增强贪婪解码准确率（偏向利用），反向策略提升Pass@K准确率（偏向探索）。

Conclusion: THR为RL调优的LLMs提供了一种动态控制探索与利用的精细机制，适用于推理密集型应用的针对性微调。

Abstract: Reinforcement learning with verifiable rewards has significantly advanced the
reasoning capabilities of large language models, yet how to explicitly steer
training toward exploration or exploitation remains an open problem. We
introduce Token Hidden Reward (THR), a token-level metric that quantifies each
token's influence on the likelihood of correct responses under Group Relative
Policy Optimization (GRPO). We find that training dynamics are dominated by a
small subset of tokens with high absolute THR values. Most interestingly,
tokens with positive THR strengthen confidence in correct outputs, thus
favoring exploitation, while tokens with negative THR preserve probability mass
for alternative outputs, enabling exploration. This insight suggests a natural
intervention: a THR-guided reweighting algorithm that modulates GRPO's learning
signals to explicitly bias training toward exploitation or exploration. We
validate the efficacy of this algorithm on diverse math reasoning benchmarks.
By amplifying tokens with positive THR value and weakening negative ones, our
algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse
strategy yields consistent gains in Pass@K accuracy, favoring exploration. We
further demonstrate that our algorithm integrates seamlessly with other RL
objectives such as GSPO and generalizes across architectures including Llama.
These findings establish THR as a principled and fine-grained mechanism for
dynamically controlling exploration and exploitation in RL-tuned LLMs,
providing new tools for targeted fine-tuning in reasoning-intensive
applications.

</details>


### [338] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于重要性采样的注意力采样器，显著降低了传统注意力机制的计算负担，并分析了其理论有效性。


<details>
  <summary>Details</summary>
Motivation: 研究大规模注意力模型的计算挑战，尤其是在流式设置下。

Method: 利用重要性采样方法，结合ℓ₂采样器和注意力机制，提出注意力采样器定义。

Result: 方法在空间和更新时间上表现高效，具有可扩展性和广泛适用性。

Conclusion: 注意力采样器为大规模注意力模型提供了一种高效且通用的解决方案。

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [339] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: GPG是一种无批评的策略梯度估计器，通过基于组的蒙特卡洛优势估计器替代价值函数，提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决PPO中训练批评者带来的内存、计算和超参数成本问题。

Method: 使用基于组的蒙特卡洛优势估计器，保留PPO的裁剪目标结构。

Result: GPG在标准基准测试中表现优于或与PPO相当，计算资源利用更高效。

Conclusion: GPG是一种高效且性能优越的策略梯度方法，适用于通用MDP。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [340] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出了一种统一框架，通过图矩（motif密度）聚类图数据，显式建模混合图生成模型，改进了图对比学习和数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现实图数据常由多个不同分布混合而成，但现有方法（如图对比学习和Mixup）常忽略这种混合结构。

Method: 利用图矩聚类图数据，提出图混合感知的Mixup（GMAM）和模型自适应的图对比学习（MGCL）。

Result: 在无监督学习中，MGCL在8个数据集上取得最佳平均排名；在有监督学习中，GMAM在6/7数据集上达到新SOTA。

Conclusion: 通过显式建模混合图生成模型，显著提升了图学习的性能。

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [341] [REG: A Regularization Optimizer for Robust Training Dynamics](https://arxiv.org/abs/2510.03691)
*Zehua Liu,Han Wu,Xiaojin Fu,Shuqi Liu,Xiongwei Han,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: REG优化器通过RACS操作符替代Muon的矩阵符号函数，提升了训练稳定性和与AdamW的兼容性，适用于LLM训练和微调。


<details>
  <summary>Details</summary>
Motivation: Muon优化器虽然能平衡梯度更新方向，但其依赖矩阵符号函数导致训练不稳定，且与AdamW预训练模型不兼容。

Method: 提出REG优化器，使用RACS操作符替代矩阵符号函数，以更温和的方式正则化更新步骤。

Result: REG在LLM训练中表现优于AdamW，稳定性更高，且在微调阶段避免了Muon的性能下降。

Conclusion: REG优化器在性能和兼容性上均优于Muon和AdamW，适用于LLM的训练和微调。

Abstract: Optimizers are crucial for the efficient training of Large Language Models
(LLMs). While AdamW is the de facto standard, recent structure-aware optimizers
like Muon have emerged, which regularize gradient updates by operating on
entire weight matrices. The Muon optimizer balances the gradient updates along
all the directions. However, Muon's reliance on the matrix sign function can
lead to training instability, exhibits incompatibility when fine-tuning models
pre-trained with AdamW. To address these limitations, we propose \textbf{REG},
a novel optimizer that replaces Muon's aggressive matrix sign operator with the
Row-and-Column-Scaling (RACS) operator. Theoretically grounded in balancing a
matrix, the RACS operator regularizes the update steps in a less drastic
manner, making it simpler to implement and more compatible with established
training dynamics. Through extensive empirical experiments on LLM training, we
demonstrate that our REG optimizer not only achieves superior performance and
stability over AdamW, but also maintains consistency with the AdamW training
paradigm. This consistency is particularly evident during the fine-tuning
stage, where REG optimizer avoids the performance degradation observed with
Muon.

</details>


### [342] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种基于谱的线性强化学习方法，通过谱滤波函数扩展岭回归方法，平衡了可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法通常注重性能，依赖事后解释来提供可解释性，而本文旨在设计一种既注重可解释性又提升性能的方法。

Method: 提出了一种基于谱的线性强化学习方法，通过谱滤波函数扩展岭回归方法，并设计了自适应正则化参数选择策略。

Result: 理论分析表明，该方法在参数估计和泛化误差上具有接近最优的界限；实验表明其在决策质量上优于或匹配现有基线。

Conclusion: 该方法在可解释性、准确性和适应性上具有潜力，能够弥合强化学习理论与实际决策之间的差距。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [343] [Personalized federated prototype learning in mixed heterogeneous data scenarios](https://arxiv.org/abs/2510.03726)
*Jiahao Zeng,Wolong Xing,Liangtao Shi,Xin Huang,Jialin Wang,Zhile Cao,Zhenkui Shi*

Main category: cs.LG

TL;DR: PFPL方法在混合异构场景中通过构建个性化、无偏的原型，解决了联邦学习中数据异构性问题，提高了模型性能并降低了通信成本。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在异构场景中表现不佳，导致特征或标签分布倾斜，而数据异构性实际上是提升模型性能的关键因素。

Method: 提出PFPL方法，为每个客户端构建个性化、无偏的原型，并在本地更新阶段引入一致性正则化，对齐本地实例与个性化原型。

Result: 在Digits和Office Caltech数据集上的实验验证了方法的有效性，显著提高了损失函数的收敛性并降低了通信成本。

Conclusion: PFPL方法成功解决了联邦学习中的数据异构性问题，提升了模型性能，同时减少了通信开销。

Abstract: Federated learning has received significant attention for its ability to
simultaneously protect customer privacy and leverage distributed data from
multiple devices for model training. However, conventional approaches often
focus on isolated heterogeneous scenarios, resulting in skewed feature
distributions or label distributions. Meanwhile, data heterogeneity is actually
a key factor in improving model performance. To address this issue, we propose
a new approach called PFPL in mixed heterogeneous scenarios. The method
provides richer domain knowledge and unbiased convergence targets by
constructing personalized, unbiased prototypes for each client. Moreover, in
the local update phase, we introduce consistent regularization to align local
instances with their personalized prototypes, which significantly improves the
convergence of the loss function. Experimental results on Digits and Office
Caltech datasets validate the effectiveness of our approach and successfully
reduce the communication cost.

</details>


### [344] [Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation](https://arxiv.org/abs/2510.03731)
*Yongfu Xue*

Main category: cs.LG

TL;DR: IniLoRA是一种新的初始化策略，通过近似原始模型权重来提升LoRA的性能。


<details>
  <summary>Details</summary>
Motivation: LoRA初始化两个低秩矩阵的乘积为零，限制了其有效激活和利用原始模型权重的能力。

Method: 提出IniLoRA，初始化低秩矩阵以近似原始模型权重，并引入两种变体IniLoRA-α和IniLoRA-β。

Result: 实验表明IniLoRA在多种模型和任务中表现优于LoRA。

Conclusion: IniLoRA通过改进初始化策略，显著提升了LoRA的性能。

Abstract: The rapid development of parameter-efficient fine-tuning methods has
noticeably improved the efficiency of adapting large language models. Among
these, LoRA has gained widespread popularity due to its strong balance of
effectiveness and parameter efficiency. However, LoRA relies on initializing
two low-rank matrices whose product is zero, which limits its ability to
effectively activate and leverage the original model weights-creating a
potential bottleneck for optimal performance. To address this limitation, we
propose \textbf{IniLoRA}, a novel initialization strategy that initializes the
low-rank matrices to closely approximate the original model weights.
Experimental results indicate that IniLoRA achieves better performance than
LoRA across a range of models and tasks. Additionally, we introduce two
variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct
initialization methods to enhance performance further.

</details>


### [345] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 研究在部分反馈下审计分类器公平性的问题，提出了一种新的成本模型和优化算法，显著降低了审计成本。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，真实标签仅对正分类个体可用（如贷款获批者的还款结果），需要更准确地反映获取标签的实际成本（如信用评估、贷款处理和违约风险）。

Method: 提出了两种审计设置：黑盒模型（无数据分布假设）和混合模型（数据遵循指数族分布混合）。设计了近最优的审计算法，并扩展了截断样本学习和最大后验估计方法。

Result: 在黑盒设置下，算法在温和假设下接近最优；在混合模型下，审计成本显著低于黑盒情况。实验显示，算法在真实数据集（如Adult Income和Law School）上比基线方法降低约50%的审计成本。

Conclusion: 提出的算法在多种公平性指标（如人口均等、机会均等和均等化赔率）上表现优异，为公平性审计提供了高效且成本优化的解决方案。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [346] [HydroFusion-LMF: Semi-Supervised Multi-Network Fusion with Large-Model Adaptation for Long-Term Daily Runoff Forecasting](https://arxiv.org/abs/2510.03744)
*Qianfei Fan,Jiayu Wei,Peijun Zhu,Wensheng Ye,Meie Fang*

Main category: cs.LG

TL;DR: HydroFusion-LMF提出了一种统一框架，通过可学习的趋势-季节-残差分解和异构专家集融合，显著提升了小流域十年尺度日径流预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 小流域日径流预测面临信号复杂（趋势漂移、多尺度季节周期、状态转换、稀疏极端值）的挑战，现有深度学习模型通常仅针对单一问题且未充分利用未标记数据。

Method: HydroFusion-LMF通过可学习分解降低非平稳性，利用异构专家集处理残差，并通过水文情境感知门融合输出，同时采用半监督多任务目标增强监督。

Result: 在十年日数据上，MSE和MAE分别比最强基线（DLinear）提升10.2%和10.3%，比平均基线提升24.6%和17.1%。

Conclusion: 该框架在非平稳条件下平衡了可解释性与性能，推动了标签高效的水文预测。

Abstract: Accurate decade-scale daily runoff forecasting in small watersheds is
difficult because signals blend drifting trends, multi-scale seasonal cycles,
regime shifts, and sparse extremes. Prior deep models (DLinear, TimesNet,
PatchTST, TiDE, Nonstationary Transformer, LSTNet, LSTM) usually target single
facets and under-utilize unlabeled spans, limiting regime adaptivity. We
propose HydroFusion-LMF, a unified framework that (i) performs a learnable
trend-seasonal-residual decomposition to reduce non-stationarity, (ii) routes
residuals through a compact heterogeneous expert set (linear refinement,
frequency kernel, patch Transformer, recurrent memory, dynamically normalized
attention), (iii) fuses expert outputs via a hydrologic context-aware gate
conditioned on day-of-year phase, antecedent precipitation, local variance,
flood indicators, and static basin attributes, and (iv) augments supervision
with a semi-supervised multi-task objective (composite MSE/MAE + extreme
emphasis + NSE/KGE, masked reconstruction, multi-scale contrastive alignment,
augmentation consistency, variance-filtered pseudo-labeling). Optional adapter
/ LoRA layers inject a frozen foundation time-series encoder efficiently. On a
~10-year daily dataset HydroFusion-LMF attains MSE 1.0128 / MAE 0.5818,
improving the strongest baseline (DLinear) by 10.2% / 10.3% and the mean
baseline by 24.6% / 17.1%. We observe simultaneous MSE and MAE reductions
relative to baselines. The framework balances interpretability (explicit
components, sparse gating) with performance, advancing label-efficient
hydrologic forecasting under non-stationarity.

</details>


### [347] [Neural Low-Discrepancy Sequences](https://arxiv.org/abs/2510.03745)
*Michael Etienne Van Huffel,Nathan Kirk,Makram Chahine,Daniela Rus,T. Konstantin Rusch*

Main category: cs.LG

TL;DR: NeuroLDS是一种基于机器学习的低差异序列生成框架，通过两阶段学习过程显著优于传统方法，并在多个应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统低差异点集生成方法依赖抽象代数和数论，且无法扩展到低差异序列（LDS），限制了其应用范围。

Method: NeuroLDS通过神经网络将索引映射为点，采用监督学习和无监督微调两阶段训练，最小化前缀差异。

Result: NeuroLDS在差异度量上显著优于所有现有LDS构造，并在数值积分、机器人运动规划等应用中表现优异。

Conclusion: NeuroLDS展示了机器学习在低差异序列生成中的潜力，具有广泛的应用前景。

Abstract: Low-discrepancy points are designed to efficiently fill the space in a
uniform manner. This uniformity is highly advantageous in many problems in
science and engineering, including in numerical integration, computer vision,
machine perception, computer graphics, machine learning, and simulation.
Whereas most previous low-discrepancy constructions rely on abstract algebra
and number theory, Message-Passing Monte Carlo (MPMC) was recently introduced
to exploit machine learning methods for generating point sets with lower
discrepancy than previously possible. However, MPMC is limited to generating
point sets and cannot be extended to low-discrepancy sequences (LDS), i.e.,
sequences of points in which every prefix has low discrepancy, a property
essential for many applications. To address this limitation, we introduce
Neural Low-Discrepancy Sequences ($NeuroLDS$), the first machine learning-based
framework for generating LDS. Drawing inspiration from classical LDS, we train
a neural network to map indices to points such that the resulting sequences
exhibit minimal discrepancy across all prefixes. To this end, we deploy a
two-stage learning process: supervised approximation of classical constructions
followed by unsupervised fine-tuning to minimize prefix discrepancies. We
demonstrate that $NeuroLDS$ outperforms all previous LDS constructions by a
significant margin with respect to discrepancy measures. Moreover, we
demonstrate the effectiveness of $NeuroLDS$ across diverse applications,
including numerical integration, robot motion planning, and scientific machine
learning. These results highlight the promise and broad significance of Neural
Low-Discrepancy Sequences. Our code can be found at
https://github.com/camail-official/neuro-lds.

</details>


### [348] [EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models](https://arxiv.org/abs/2510.03760)
*Ping Guo,Chenyu Zhu,Siyuan Chen,Fei Liu,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为EvoEngineer的系统化LLM代码进化框架，用于优化CUDA内核，平衡性能与正确性，并在实验中取得了显著的速度提升和较高的代码有效性。


<details>
  <summary>Details</summary>
Motivation: CUDA内核优化是AI性能的关键瓶颈，但现有方法碎片化且缺乏明确的问题定义，通用LLM方法无法满足严格正确性要求。

Method: 通过明确CUDA内核优化的任务目标、约束和评估指标，建立EvoEngineer框架，设计并调整优化策略。

Result: 在91个真实CUDA内核上，EvoEngineer实现了2.72倍的平均中值加速和69.8%的代码有效性，优于现有方法。

Conclusion: EvoEngineer在性能和正确性之间取得了平衡，为CUDA内核优化提供了系统化解决方案。

Abstract: CUDA kernel optimization has become a critical bottleneck for AI performance,
as deep learning training and inference efficiency directly depends on highly
optimized GPU kernels.
  Despite the promise of Large Language Models (LLMs) for automating kernel
optimization, this field suffers from a fragmented ecosystem of isolated and
incomparable approaches with unclear problem formulations.
  Furthermore, general-purpose LLM code evolution methods cannot meet strict
correctness requirements of CUDA kernel optimization.
  We address these fundamental challenges by first formalizing CUDA kernel
optimization as a code optimization task with a clear objective, constraints,
and evaluation metrics.
  We then establish the first systematic LLM-based code evolution framework,
EvoEngineer, that provides guidance for designing and adapting optimization
strategies to achieve a balance between performance and correctness.
  Finally, we implement a kernel optimization system based on this framework
and conduct extensive experiments on 91 real-world CUDA kernels.
  Our results demonstrate that EvoEngineer achieves a principled balance
between performance and correctness, with the highest averaged median speedup
of \textbf{2.72}$\times$ over baseline CUDA kernels and a code validity rate of
\textbf{69.8}\%, outperforming existing methods on both dimensions.
  Our method achieves a maximum speedup of \textbf{36.75}$\times$ among all
operations over PyTorch kernels and delivers the highest speedup on \textbf{28}
(\textbf{56.0\%}) of 50 operations that achieve over \textbf{2$\times$}
acceleration.

</details>


### [349] [Merge and Guide: Unifying Model Merging and Guided Decoding for Controllable Multi-Objective Generation](https://arxiv.org/abs/2510.03782)
*Guofu Xie,Chen Zhang,Xiao Zhang,Yunsheng Shi,Ting Yao,Jun Xu*

Main category: cs.LG

TL;DR: MAGE框架通过两阶段合并与引导解码，解决了多目标生成中的控制问题，提升了性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多目标生成中控制不足，合并方法间接且解码方法空间开销大。

Method: MAGE分两阶段：动态构建鲁棒基础模型，合并显隐式价值模型为统一引导代理。

Result: 实验显示MAGE在可控性、Pareto最优性能和适应性上优于现有方法。

Conclusion: MAGE通过模型合并与引导解码，显著提升了多目标生成的控制能力。

Abstract: Adapting to diverse user needs at test time is a key challenge in
controllable multi-objective generation. Existing methods are insufficient:
merging-based approaches provide indirect, suboptimal control at the parameter
level, often disregarding the impacts of multiple objectives. While
decoding-based guidance is more direct, it typically requires aggregating
logits from multiple expert models, incurring significant space overhead and
relying heavily on individual model capacity. To address these issues, we
introduce Merge-And-GuidE (MAGE), a two-stage framework that leverages model
merging for guided decoding. We first identify a critical compatibility problem
between the guidance and base models. In Stage 1, MAGE resolves this by
dynamically constructing a more robust base model, merging a series of backbone
models that account for multiple objectives. In Stage 2, we merge explicit and
implicit value models into a unified guidance proxy, which then steers the
decoding of the base model from Stage 1. Our analysis empirically validates
Linear Mode Connectivity (LMC) in value models, explores the relationship
between model merging and prediction ensembling, and demonstrates the enhanced
controllability afforded by our approach. Extensive experiments show that our
method outperforms existing approaches, achieving superior controllability,
Pareto-optimal performance, and enhanced adaptability.

</details>


### [350] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: 论文研究了Transformer模型中注意力头和头维度在不同层的分配策略，以平衡表达能力和效率。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型参数（注意力头和头维度）如何分配以提高效率，理论基础尚不完善。

Method: 通过数学分析和实验验证，研究了早期层的信息提取作用和softmax激活的饱和行为。

Result: 发现增加头维度会导致学习误差的收益递减，后期层可以用更少参数高效运行。

Conclusion: 提出了基于理论的注意力头和维度分配策略，为Transformer架构的效率提供了理论支持。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [351] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 本文提出了针对重尾奖励的鲁棒批处理多臂老虎机算法，揭示了在不同设置下尾部分布对批次数量的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究多假设奖励分布为轻尾，但实际场景（如临床试验）常呈现重尾特性，本文旨在填补这一空白。

Method: 提出了适用于重尾奖励的鲁棒批处理算法，涵盖有限臂和Lipschitz连续设置。

Result: 在实例无关和Lipschitz设置中，重尾奖励需要更少的批次实现近优遗憾；而在实例相关设置中，批次需求与尾部分布无关。

Conclusion: 重尾奖励在不同设置下对批次数量的影响不同，为实际应用提供了新见解。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [352] [Curriculum-Augmented GFlowNets For mRNA Sequence Generation](https://arxiv.org/abs/2510.03811)
*Aya Laajil,Abduragim Shtanchaev,Sajan Muhammad,Eric Moulines,Salem Lahlou*

Main category: cs.LG

TL;DR: CAGFN结合课程学习和多目标GFlowNets，用于生成mRNA序列，提升性能、生物合理性和多样性。


<details>
  <summary>Details</summary>
Motivation: mRNA序列设计空间巨大且多目标优化困难，现有方法训练效率低。

Method: 提出CAGFN，引入长度课程逐步探索子问题，并构建mRNA设计环境。

Result: CAGFN在Pareto性能、生物合理性及速度上优于随机采样方法，并能泛化。

Conclusion: CAGFN为治疗性序列设计提供了高效且通用的框架。

Abstract: Designing mRNA sequences is a major challenge in developing next-generation
therapeutics, since it involves exploring a vast space of possible nucleotide
combinations while optimizing sequence properties like stability, translation
efficiency, and protein expression. While Generative Flow Networks are
promising for this task, their training is hindered by sparse, long-horizon
rewards and multi-objective trade-offs. We propose Curriculum-Augmented
GFlowNets (CAGFN), which integrate curriculum learning with multi-objective
GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based
curriculum that progressively adapts the maximum sequence length guiding
exploration from easier to harder subproblems. We also provide a new mRNA
design environment for GFlowNets which, given a target protein sequence and a
combination of biological objectives, allows for the training of models that
generate plausible mRNA candidates. This provides a biologically motivated
setting for applying and advancing GFlowNets in therapeutic sequence design. On
different mRNA design tasks, CAGFN improves Pareto performance and biological
plausibility, while maintaining diversity. Moreover, CAGFN reaches
higher-quality solutions faster than a GFlowNet trained with random sequence
sampling (no curriculum), and enables generalization to out-of-distribution
sequences.

</details>


### [353] [Detecting Invariant Manifolds in ReLU-Based RNNs](https://arxiv.org/abs/2510.03814)
*Lukas Eisenmann,Alena Brändle,Zahra Monfared,Daniel Durstewitz*

Main category: cs.LG

TL;DR: 论文提出了一种新算法，用于检测RNN中的稳定和不稳定流形，以分析其动力学行为，并展示了在PLRNN中的应用。


<details>
  <summary>Details</summary>
Motivation: 理解RNN的行为机制对科学、医学和可解释AI至关重要，尤其是其状态空间的拓扑和几何特性。

Method: 引入了一种新算法，专注于检测PLRNN中的稳定和不稳定流形，利用ReLU激活函数。

Result: 算法成功追踪了吸引盆边界，表征了多稳定性，并证明了PLRNN中混沌的存在。

Conclusion: 该方法为分析RNN动力学提供了新工具，并在实际数据（如神经元记录）中展示了应用潜力。

Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in
machine learning for time series prediction and dynamical systems
reconstruction, and experienced a recent renaissance with improved training
algorithms and architectural designs. Understanding why and how trained RNNs
produce their behavior is important for scientific and medical applications,
and explainable AI more generally. An RNN's dynamical repertoire depends on the
topological and geometrical properties of its state space. Stable and unstable
manifolds of periodic points play a particularly important role: They dissect a
dynamical system's state space into different basins of attraction, and their
intersections lead to chaotic dynamics with fractal geometry. Here we introduce
a novel algorithm for detecting these manifolds, with a focus on
piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as
their activation function. We demonstrate how the algorithm can be used to
trace the boundaries between different basins of attraction, and hence to
characterize multistability, a computationally important property. We further
show its utility in finding so-called homoclinic points, the intersections
between stable and unstable manifolds, and thus establish the existence of
chaos in PLRNNs. Finally we show for an empirical example, electrophysiological
recordings from a cortical neuron, how insights into the underlying dynamics
could be gained through our method.

</details>


### [354] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: TROLL提出了一种新的离散可微分信任区域投影方法，替代PPO的clip机制，提升了训练速度、稳定性和最终性能。


<details>
  <summary>Details</summary>
Motivation: PPO的clip机制是一种粗糙的近似，常导致不稳定更新和次优性能，需要更优的替代方案。

Method: 使用离散可微分信任区域投影，对重要token的logits进行稀疏处理，平衡计算成本和投影效果。

Result: TROLL在训练速度、稳定性和最终成功率上均优于PPO-like clip。

Conclusion: TROLL是一种直接替代PPO clip的有效方法，适用于多种模型和数据集。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [355] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: PDNS通过近端点方法解决多模态目标分布采样中的模式崩溃问题，逐步逼近目标分布。


<details>
  <summary>Details</summary>
Motivation: 多模态目标分布采样中，模式崩溃问题导致采样困难，需要一种更稳健的方法。

Method: 提出PDNS框架，利用近端点方法分解学习过程为一系列子问题，使用加权去噪交叉熵目标实现。

Result: 在连续和离散采样任务中表现出高效性和鲁棒性，适用于分子动力学和统计物理等复杂场景。

Conclusion: PDNS通过逐步逼近和多模态探索，有效解决了模式崩溃问题，提升了采样性能。

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [356] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: HOFLON结合离线学习和在线优化，解决了传统离线强化学习在分布偏移和值高估上的问题，并在工业案例中表现优于历史数据和现有算法。


<details>
  <summary>Details</summary>
Motivation: 传统依赖专家经验的连续过程工厂操作面临专家退休导致的知识流失问题，需要自动化解决方案。

Method: HOFLON离线学习数据流形和长时Q评价器，在线优化时结合Q值和流形约束。

Result: 在聚合反应器和造纸机案例中，HOFLON表现优于IQL和历史最佳操作。

Conclusion: HOFLON展示了超越专家能力的自动化潜力。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [357] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 论文提出了一种名为FIRE的方法，通过Fisher信息来缓解联邦学习中数据碎片化导致的协变量偏移问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 数据在时间和空间上的碎片化导致训练分布与验证分布不一致，从而引发模型性能下降。

Method: 利用近似Fisher信息累积碎片化引起的协变量偏移差异，并将其作为每个数据片段的损失惩罚项，实现分布对齐。

Result: FIRE在偏移验证集上的表现优于重要性加权基准（最高提升5.1%）和联邦学习基准（最高提升5.3%）。

Conclusion: FIRE是一种计算高效且可扩展的方法，能有效解决联邦学习中的协变量偏移问题。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [358] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER 是一种用于流数据中序列分布偏移检测和稳定适应的方法，提供统计有效性和时间一致性保证。


<details>
  <summary>Details</summary>
Motivation: 解决流数据中分布偏移的实时检测和适应问题，确保统计有效性和稳定性。

Method: 通过构建指数鞅和非一致性分数进行检测，利用 Fisher 预条件更新实现自然梯度下降以适应。

Result: 检测延迟为 O(log(1/δ)/Γ)，适应方法最小化 KL 散度并保持稳定性。

Conclusion: M-FISHER 是一种理论严谨的方法，适用于协变量偏移下的序列决策。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [359] [On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records](https://arxiv.org/abs/2510.03844)
*Sarah C. Lotspeich,Abbey Collins,Brian J. Wells,Ashish K. Khanna,Joseph Rigdon,Lucy D'Agostino McGowan*

Main category: cs.LG

TL;DR: 研究提出了一种基于ICD-10代码和LLM增强的算法，用于恢复电子健康记录（EHR）中的缺失数据，其准确性与专家图表审查相当，且适用于大规模样本。


<details>
  <summary>Details</summary>
Motivation: EHR数据常存在缺失和错误，传统的图表审查方法昂贵且耗时，限制了可审查的患者数量。

Method: 通过结合临床专业知识和大型语言模型（LLM）迭代优化辅助诊断列表，开发了一种算法，并在100名患者的图表审查中测试其性能，最终应用于1000名患者。

Result: 算法恢复的缺失数据量与专家图表审查相当或更多，具体取决于使用的辅助诊断列表。

Conclusion: LLM增强的临床驱动算法可以高效恢复EHR缺失数据，未来可扩展用于监测其他数据质量维度。

Abstract: Objective: Electronic health records (EHR) data are prone to missingness and
errors. Previously, we devised an "enriched" chart review protocol where a
"roadmap" of auxiliary diagnoses (anchors) was used to recover missing values
in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a
missing hemoglobin A1c value would be considered unhealthy). Still, chart
reviews are expensive and time-intensive, which limits the number of patients
whose data can be reviewed. Now, we investigate the accuracy and scalability of
a roadmap-driven algorithm, based on ICD-10 codes (International Classification
of Diseases, 10th revision), to mimic expert chart reviews and recover missing
values. Materials and Methods: In addition to the clinicians' original roadmap
from our previous work, we consider new versions that were iteratively refined
using large language models (LLM) in conjunction with clinical expertise to
expand the list of auxiliary diagnoses. Using chart reviews for 100 patients
from the EHR at an extensive learning health system, we examine algorithm
performance with different roadmaps. Using the larger study of $1000$ patients,
we applied the final algorithm, which used a roadmap with clinician-approved
additions from the LLM. Results: The algorithm recovered as much, if not more,
missing data as the expert chart reviewers, depending on the roadmap.
Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing
EHR data with similar accuracy to chart reviews and can feasibly be applied to
large samples. Extending them to monitor other dimensions of data quality
(e.g., plausability) is a promising future direction.

</details>


### [360] [Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning Exploration](https://arxiv.org/abs/2510.03865)
*Wenhao Deng,Long Wei,Chenglei Yu,Tailin Wu*

Main category: cs.LG

TL;DR: RLVR增强LLMs的数学推理能力，但采样预算增加时优势减弱。RAPO算法通过改进KL散度正则化，提升探索能力，显著提高问题解决性能。


<details>
  <summary>Details</summary>
Motivation: RLVR在数学问题解决中表现优异，但采样预算增加时优势消失，因反向KL散度限制了探索空间。

Method: 提出RAPO算法，使用正向KL散度惩罚替代反向KL散度，并重新加权参考策略以促进自适应探索。

Result: 在Qwen2.5-3B和7B模型上验证，RAPO显著提升性能，突破基础模型性能上限。

Conclusion: RAPO推动了RLVR在复杂推理任务中的应用，解决了此前难以处理的问题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently enhanced
the reasoning capabilities of large language models (LLMs), particularly for
mathematical problem solving. However, a fundamental limitation remains: as the
sampling budget increases, the advantage of RLVR-trained models over their
pretrained bases often diminishes or even vanishes, revealing a strong
dependence on the base model's restricted search space. We attribute this
phenomenon to the widespread use of the reverse Kullback-Leibler (KL)
divergence regularizer, whose mode-seeking behavior keeps the policy trapped
inside the base model's support region and hampers wider exploration. To
address this issue, we propose RAPO (Rewards-Aware Policy Optimization), an
algorithm to promote broader yet focused exploration. Our method (i) utilizes
the forward KL penalty to replace the reverse KL penalty for
out-of-distribution exploration, and (ii) reweights the reference policy to
facilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B
models with RAPO on the 8K SimpleRL-Zero dataset, without supervised
fine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO
consistently improves problem-solving performance. Notably, RAPO enables models
to surpass the base model's performance ceiling and solves previously
intractable problems, advancing the frontier of RLVR for challenging reasoning
tasks.

</details>


### [361] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 论文研究了Muon优化器在联邦学习中的性能，提出了新算法FedMuon，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在联邦学习中的性能尚未探索，填补了这一空白。

Method: 提出FedMuon算法，分析其收敛性，并验证其正交化更新方向的特性。

Result: FedMuon的学习率与问题参数无关，能适应重尾噪声，实验验证了其有效性。

Conclusion: FedMuon在联邦学习中表现优异，具有理论和实践优势。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [362] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 论文发现输出层的算子范数是模型和数据集规模联合最优缩放的单一定律，称为范数转移。


<details>
  <summary>Details</summary>
Motivation: 探索模型和数据集规模缩放下的最优超参数转移的统一解释原则。

Method: 使用Scion优化器，研究模型和数据集规模的最优缩放，发现输出层算子范数的恒定条件。

Result: 最优学习率和批量大小对具有相同的算子范数值，且缩放规则与Adam优化器一致。

Conclusion: 输出层对学习率最敏感，隐藏层受益于较低学习率，提供了范数引导的最优缩放实用建议。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [363] [BONSAI: Structure-exploiting robust Bayesian optimization for networked black-box systems under uncertainty](https://arxiv.org/abs/2510.03893)
*Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: BONSAI是一种新的鲁棒贝叶斯优化框架，利用部分结构知识优化高维不确定性设计问题。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒优化方法需要已知问题结构，限制了在高保真仿真环境中的应用。

Method: BONSAI将目标表示为有向图，结合白盒和黑盒组件，并提出基于Thompson采样的可扩展采集函数。

Result: BONSAI在合成和实际案例中表现优于现有方法，提供更高效和高质量的鲁棒解。

Conclusion: BONSAI在复杂工程系统中具有实际优势，适用于不确定性设计。

Abstract: Optimal design under uncertainty remains a fundamental challenge in advancing
reliable, next-generation process systems. Robust optimization (RO) offers a
principled approach by safeguarding against worst-case scenarios across a range
of uncertain parameters. However, traditional RO methods typically require
known problem structure, which limits their applicability to high-fidelity
simulation environments. To overcome these limitations, recent work has
explored robust Bayesian optimization (RBO) as a flexible alternative that can
accommodate expensive, black-box objectives. Existing RBO methods, however,
generally ignore available structural information and struggle to scale to
high-dimensional settings. In this work, we introduce BONSAI (Bayesian
Optimization of Network Systems under uncertAInty), a new RBO framework that
leverages partial structural knowledge commonly available in simulation-based
models. Instead of treating the objective as a monolithic black box, BONSAI
represents it as a directed graph of interconnected white- and black-box
components, allowing the algorithm to utilize intermediate information within
the optimization process. We further propose a scalable Thompson sampling-based
acquisition function tailored to the structured RO setting, which can be
efficiently optimized using gradient-based methods. We evaluate BONSAI across a
diverse set of synthetic and real-world case studies, including applications in
process systems engineering. Compared to existing simulation-based RO
algorithms, BONSAI consistently delivers more sample-efficient and
higher-quality robust solutions, highlighting its practical advantages for
uncertainty-aware design in complex engineering systems.

</details>


### [364] [LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis](https://arxiv.org/abs/2510.03904)
*Hangting Ye,Jinmeng Li,He Zhao,Mingchen Zhuge,Dandan Guo,Yi Chang,Hongyuan Zha*

Main category: cs.LG

TL;DR: LLM-DAS是一种新颖的框架，通过将LLM从数据处理者转变为算法分析者，生成针对检测器弱点的合成异常数据，提升检测器性能。


<details>
  <summary>Details</summary>
Motivation: 现有表格数据异常检测方法依赖对异常模式的假设，导致实际场景中性能不稳定。LLM虽具备强大推理能力，但直接应用于表格数据异常检测面临异构数据处理和隐私风险等挑战。

Method: LLM-DAS利用LLM分析检测器的高层描述，生成数据无关的Python代码合成难以检测的异常数据，通过增强训练数据提升检测器鲁棒性。

Result: 在36个TAD基准测试中，LLM-DAS显著提升了主流检测器的性能。

Conclusion: LLM-DAS通过程序化合成将LLM推理与经典异常检测算法结合，提供了一种可扩展、高效且隐私保护的解决方案。

Abstract: Existing anomaly detection (AD) methods for tabular data usually rely on some
assumptions about anomaly patterns, leading to inconsistent performance in
real-world scenarios. While Large Language Models (LLMs) show remarkable
reasoning capabilities, their direct application to tabular AD is impeded by
fundamental challenges, including difficulties in processing heterogeneous data
and significant privacy risks. To address these limitations, we propose
LLM-DAS, a novel framework that repositions the LLM from a ``data processor''
to an ``algorithmist''. Instead of being exposed to raw data, our framework
leverages the LLM's ability to reason about algorithms. It analyzes a
high-level description of a given detector to understand its intrinsic
weaknesses and then generates detector-specific, data-agnostic Python code to
synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities.
This generated synthesis program, which is reusable across diverse datasets, is
then instantiated to augment training data, systematically enhancing the
detector's robustness by transforming the problem into a more discriminative
two-class classification task. Extensive experiments on 36 TAD benchmarks show
that LLM-DAS consistently boosts the performance of mainstream detectors. By
bridging LLM reasoning with classic AD algorithms via programmatic synthesis,
LLM-DAS offers a scalable, effective, and privacy-preserving approach to
patching the logical blind spots of existing detectors.

</details>


### [365] [THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series](https://arxiv.org/abs/2510.03911)
*Yadav Mahesh Lorik,Kaushik Sarveswaran,Nagaraj Sundaramahalingam,Aravindakumar Venugopalan*

Main category: cs.LG

TL;DR: THEMIS框架利用预训练基础模型进行时间序列异常检测，表现优异且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 时间序列异常检测面临数据复杂性、不平衡性和实时性等挑战，需要强健且灵活的方法。

Method: THEMIS利用Chronos基础模型的编码器提取嵌入，结合局部离群因子和谱分解技术检测异常。

Result: 在MSL数据集上达到SOTA，在SMAP和SWAT$^*$数据集上表现优异，超越专用异常检测模型。

Conclusion: 预训练基础模型为时间序列异常检测提供了高效且适应性强的方法。

Abstract: Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.

</details>


### [366] [Generalized Fitted Q-Iteration with Clustered Data](https://arxiv.org/abs/2510.03912)
*Liyuan Hu,Jitao Wang,Zhenke Wu,Chengchun Shi*

Main category: cs.LG

TL;DR: 提出了一种结合广义估计方程的广义FQI算法，用于处理聚类数据中的相关性，相比标准FQI平均减少一半遗憾。


<details>
  <summary>Details</summary>
Motivation: 医疗应用中聚类数据常见，需处理簇内相关性以优化强化学习策略。

Method: 提出广义FQI算法，结合广义估计方程，处理簇内相关性。

Result: 理论证明算法在正确或错误指定相关结构时的最优性和一致性；实证显示平均减少一半遗憾。

Conclusion: 广义FQI在聚类数据中表现优于标准FQI，适用于医疗等应用。

Abstract: This paper focuses on reinforcement learning (RL) with clustered data, which
is commonly encountered in healthcare applications. We propose a generalized
fitted Q-iteration (FQI) algorithm that incorporates generalized estimating
equations into policy learning to handle the intra-cluster correlations.
Theoretically, we demonstrate (i) the optimalities of our Q-function and policy
estimators when the correlation structure is correctly specified, and (ii)
their consistencies when the structure is mis-specified. Empirically, through
simulations and analyses of a mobile health dataset, we find the proposed
generalized FQI achieves, on average, a half reduction in regret compared to
the standard FQI.

</details>


### [367] [Transductive and Learning-Augmented Online Regression](https://arxiv.org/abs/2510.03917)
*Vinod Raman,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 研究了在线回归问题，当学习者可以预测未来样本时，通过脂肪粉碎维度完全描述了极小极大期望遗憾，并扩展到不完美预测的情况。


<details>
  <summary>Details</summary>
Motivation: 探索在数据流中预测未来样本对在线回归的影响，特别是在可预测性较高的情况下。

Method: 使用脂肪粉碎维度分析极小极大期望遗憾，并扩展到不完美预测的场景。

Result: 提出了一个在线学习器，其极小极大期望遗憾与预测质量平滑匹配，并在预测准确时接近转导在线学习器的性能。

Conclusion: 通过预测未来样本，使原本不可学习的类别在可预测情况下变得可学习，扩展了学习增强模型的范式。

Abstract: Motivated by the predictable nature of real-life in data streams, we study
online regression when the learner has access to predictions about future
examples. In the extreme case, called transductive online learning, the
sequence of examples is revealed to the learner before the game begins. For
this setting, we fully characterize the minimax expected regret in terms of the
fat-shattering dimension, establishing a separation between transductive online
regression and (adversarial) online regression. Then, we generalize this
setting by allowing for noisy or \emph{imperfect} predictions about future
examples. Using our results for the transductive online setting, we develop an
online learner whose minimax expected regret matches the worst-case regret,
improves smoothly with prediction quality, and significantly outperforms the
worst-case regret when future example predictions are precise, achieving
performance similar to the transductive online learner. This enables
learnability for previously unlearnable classes under predictable examples,
aligning with the broader learning-augmented model paradigm.

</details>


### [368] [On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks](https://arxiv.org/abs/2510.03923)
*Mingsong Yan,Charles Kulick,Sui Tang*

Main category: cs.LG

TL;DR: 论文分析了连续深度图神经网络（GNDEs）在无限节点极限下的收敛性，提出了Graphon-NDEs作为其理论框架，并证明了其解法的收敛性。


<details>
  <summary>Details</summary>
Motivation: 结合图神经网络（GNNs）和神经微分方程（Neural ODEs）的优势，为图上的动态建模提供可扩展且理论严谨的框架。

Method: 引入Graphon-NDEs作为GNDEs的无限节点极限，利用图论和动力系统工具证明其收敛性，并推导了两种确定性图采样机制下的收敛速率。

Result: 证明了GNDE解向Graphon-NDE解的轨迹收敛性，并提供了尺寸可迁移性的理论支持。

Conclusion: 理论分析和数值实验验证了GNDEs在尺寸可迁移性上的有效性，支持其在实践中的应用。

Abstract: Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.

</details>


### [369] [LLM Chemistry Estimation for Multi-LLM Recommendation](https://arxiv.org/abs/2510.03930)
*Huascar Sanchez,Briland Hitaj*

Main category: cs.LG

TL;DR: LLM Chemistry框架通过量化模型间的协同或对抗行为，优化多LLM协作性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未分析协作模型是否真正互补或冲突，需系统性评估。

Method: 提出LLM Chemistry框架，量化模型间化学作用，推荐最优模型组合。

Result: 理论分析表明化学作用在异构模型中最明显，实验验证了任务依赖性。

Conclusion: LLM Chemistry可作为多LLM系统的诊断工具和组合推荐基础。

Abstract: Multi-LLM collaboration promises accurate, robust, and context-aware
solutions, yet existing approaches rely on implicit selection and output
assessment without analyzing whether collaborating models truly complement or
conflict. We introduce LLM Chemistry -- a framework that measures when LLM
combinations exhibit synergistic or antagonistic behaviors that shape
collective performance beyond individual capabilities. We formalize the notion
of chemistry among LLMs, propose algorithms that quantify it by analyzing
interaction dependencies, and recommend optimal model ensembles accordingly.
Our theoretical analysis shows that chemistry among collaborating LLMs is most
evident under heterogeneous model profiles, with its outcome impact shaped by
task type, group size, and complexity. Evaluation on classification,
summarization, and program repair tasks provides initial evidence for these
task-dependent effects, thereby reinforcing our theoretical results. This
establishes LLM Chemistry as both a diagnostic factor in multi-LLM systems and
a foundation for ensemble recommendation.

</details>


### [370] [On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection](https://arxiv.org/abs/2510.03944)
*Weiqing He,Xiang Li,Tianqi Shang,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

TL;DR: 本文系统评估了八种GoF测试在三种流行水印方案中的表现，发现通用GoF测试能显著提升水印检测的能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成文本的真实性和完整性引发担忧，水印技术提供了一种可验证来源的方法，但GoF测试在此领域尚未充分探索。

Method: 使用三种开源LLM、两种数据集、不同生成温度及多种后编辑方法，评估八种GoF测试在三种水印方案中的表现。

Result: 通用GoF测试能提升水印检测能力和鲁棒性，尤其在低温度设置下，文本重复为GoF测试提供了独特优势。

Conclusion: 经典GoF测试是LLM水印检测中简单但强大且未被充分利用的工具。

Abstract: Large language models (LLMs) raise concerns about content authenticity and
integrity because they can generate human-like text at scale. Text watermarks,
which embed detectable statistical signals into generated text, offer a
provable way to verify content origin. Many detection methods rely on pivotal
statistics that are i.i.d. under human-written text, making goodness-of-fit
(GoF) tests a natural tool for watermark detection. However, GoF tests remain
largely underexplored in this setting. In this paper, we systematically
evaluate eight GoF tests across three popular watermarking schemes, using three
open-source LLMs, two datasets, various generation temperatures, and multiple
post-editing methods. We find that general GoF tests can improve both the
detection power and robustness of watermark detectors. Notably, we observe that
text repetition, common in low-temperature settings, gives GoF tests a unique
advantage not exploited by existing methods. Our results highlight that classic
GoF tests are a simple yet powerful and underused tool for watermark detection
in LLMs.

</details>


### [371] [What Is The Performance Ceiling of My Classifier? Utilizing Category-Wise Influence Functions for Pareto Frontier Analysis](https://arxiv.org/abs/2510.03950)
*Shahriar Kabir Nahin,Wenxiao Xiao,Joshua Liu,Anshuman Chhabra,Hongfu Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于类别影响函数的方法，旨在通过样本重加权实现模型在所有类别上的帕累托改进。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注哪些数据对模型有益，而本文进一步探讨模型的性能上限，并强调类别级准确率而非整体准确率。

Method: 提出类别影响函数和影响向量，设计线性规划样本重加权框架以实现帕累托改进。

Result: 在合成数据集、视觉和文本基准上验证了方法的有效性。

Conclusion: 该方法能够有效估计并实现模型在多个类别上的性能提升。

Abstract: Data-centric learning seeks to improve model performance from the perspective
of data quality, and has been drawing increasing attention in the machine
learning community. Among its key tools, influence functions provide a powerful
framework to quantify the impact of individual training samples on model
predictions, enabling practitioners to identify detrimental samples and retrain
models on a cleaner dataset for improved performance. However, most existing
work focuses on the question: "what data benefits the learning model?" In this
paper, we take a step further and investigate a more fundamental question:
"what is the performance ceiling of the learning model?" Unlike prior studies
that primarily measure improvement through overall accuracy, we emphasize
category-wise accuracy and aim for Pareto improvements, ensuring that every
class benefits, rather than allowing tradeoffs where some classes improve at
the expense of others. To address this challenge, we propose category-wise
influence functions and introduce an influence vector that quantifies the
impact of each training sample across all categories. Leveraging these
influence vectors, we develop a principled criterion to determine whether a
model can still be improved, and further design a linear programming-based
sample reweighting framework to achieve Pareto performance improvements.
Through extensive experiments on synthetic datasets, vision, and text
benchmarks, we demonstrate the effectiveness of our approach in estimating and
achieving a model's performance improvement across multiple categories of
interest.

</details>


### [372] [Optimizing Resources for On-the-Fly Label Estimation with Multiple Unknown Medical Experts](https://arxiv.org/abs/2510.03954)
*Tim Bary,Tiffanie Godelaine,Axel Abels,Benoît Macq*

Main category: cs.LG

TL;DR: 提出一种自适应实时标注方法，减少专家查询次数50%，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有算法无法满足医学筛查流程的无缝集成需求，需高效聚合噪声标注。

Method: 自适应实时标注，动态查询专家意见，满足置信阈值后停止。

Result: 在三种多标注分类数据集上，减少50%专家查询，准确性与非自适应基线相当。

Conclusion: 该方法显著降低标注开销，适用于医学筛查流程。

Abstract: Accurate ground truth estimation in medical screening programs often relies
on coalitions of experts and peer second opinions. Algorithms that efficiently
aggregate noisy annotations can enhance screening workflows, particularly when
data arrive continuously and expert proficiency is initially unknown. However,
existing algorithms do not meet the requirements for seamless integration into
screening pipelines. We therefore propose an adaptive approach for real-time
annotation that (I) supports on-the-fly labeling of incoming data, (II)
operates without prior knowledge of medical experts or pre-labeled data, and
(III) dynamically queries additional experts based on the latent difficulty of
each instance. The method incrementally gathers expert opinions until a
confidence threshold is met, providing accurate labels with reduced annotation
overhead. We evaluate our approach on three multi-annotator classification
datasets across different modalities. Results show that our adaptive querying
strategy reduces the number of expert queries by up to 50% while achieving
accuracy comparable to a non-adaptive baseline. Our code is available at
https://github.com/tbary/MEDICS

</details>


### [373] [Early-Warning of Thunderstorm-Driven Power Outages with a Two-Stage Machine Learning Model](https://arxiv.org/abs/2510.03959)
*Iryna Stanishevska*

Main category: cs.LG

TL;DR: 开发了一个基于公开数据的24-48小时雷暴停电预警模型，结合两阶段设计（逻辑门和LSTM回归器），在测试样本中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 雷暴导致的停电难以预测，因为大多数风暴不会造成损害，对流过程快速且混乱，且公开数据噪声大且不完整。

Method: 使用公开的EAGLE-I停电数据集和METAR天气数据，通过参数特定的克里金插值和时空特征工程，构建两阶段模型（逻辑门+LSTM）。

Result: 两阶段模型在测试样本中检测到更多参考峰值（如±48小时内3/4 vs. 2/4），并在峰值附近表现出小幅性能提升（2-3%的cMASE降低）。

Conclusion: 尽管公开数据噪声大，但基于特征工程的管道提供了可操作的、事件聚焦的雷暴停电早期预警。

Abstract: Thunderstorm-driven outages are difficult to predict because most storms do
not cause damage, convective processes occur rapidly and chaotically, and the
available public data are both noisy and incomplete. We develop a 24-48 h
early-warning model for summer, thunderstorm-related outages in Michigan using
only open sources (EAGLE-I for ground truth; METAR for weather). We use the
publicly released EAGLE-I outage dataset (2014-2022), maintained by Oak Ridge
National Laboratory for the U.S. Department of Energy. The pipeline preserves
convective micro-signals from a sparse station network via parameter-specific
kriging with hourly variograms and targeted overdrafting to retain extremes,
and builds causal spatio-temporal features (lags/rolling statistics; k-NN/IDW
spatial aggregates) capturing precursors of severe convection (moisture
advection, wind shifts, and pressure drops). The two-stage model design,
combining a logistic gate and an LSTM regressor, limits routine periods and
reduces noise exposure. The study uses event-centric metrics (cluster-based
hits/misses/false alarms) and peak-conditional MASE (cMASE) in +/-Delta-hour
windows around state-level peaks (>= 50,000), with uncertainty quantified by
hourly moving-block bootstrap.
  On the test sample, Two-Stage detects more reference peaks across all windows
(e.g., at +/-48 h it records 3/4 vs. 2/4; F1 66.7% vs. 57.1%) with one extra
false alarm. Near peaks, it shows modest amplitude gains (2-3% lower cMASE at
+/-0-12 h; bootstrap medians +9-13% at +/-6-12 h) but small losses at +/-36-48
h (~3-4%). Overall, errors are comparable to the one-step LSTM baseline. SHAP
analysis confirms moisture-advection and wind/gust precursors, underscoring the
value of the feature engineering. Despite open-data noise, the feature-driven
pipeline yields actionable, event-focused early warnings for thunderstorm
outages.

</details>


### [374] [SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data](https://arxiv.org/abs/2510.03962)
*Hanzhe Wei,Jiajun Wu,Jialin Yang,Henry Leung,Steve Drew*

Main category: cs.LG

TL;DR: SPEAR利用软提示和量化技术，通过LLM进行时间序列异常检测，解决了传统方法在变长序列和上下文异常中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理变长时间序列和上下文异常时表现不佳，LLM的出现为时间序列异常检测提供了新机会。

Method: SPEAR通过量化和转换时间序列数据为输入嵌入，结合可学习的软提示嵌入，输入冻结的LLM中，并通过交叉熵损失迭代更新软提示。

Result: 实验表明，软提示显著提升了LLM在时间序列异常检测下游任务中的性能。

Conclusion: SPEAR通过软提示和量化技术，成功将LLM应用于时间序列异常检测，提升了检测效果。

Abstract: Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.

</details>


### [375] [What Can You Do When You Have Zero Rewards During RL?](https://arxiv.org/abs/2510.03971)
*Jatin Prakash,Anirudh Buvanesh*

Main category: cs.LG

TL;DR: 论文研究了在强化学习中，当基础模型无法生成正确答案时，如何通过数据干预（如添加简单样本）克服零奖励障碍，而无需修改RL算法。


<details>
  <summary>Details</summary>
Motivation: 探索在强化学习中，当基础模型从未生成正确答案时，如何克服零奖励障碍，以提升复杂推理任务的性能。

Method: 通过图搜索任务评估了多种方法（如密集奖励、多样性激励和改进的信用分配），并尝试通过添加简单训练样本的数据干预方法。

Result: 实验表明，仅通过数据干预（添加简单样本）即可克服零奖励障碍，而其他方法均未成功。

Conclusion: 数据干预是克服零奖励障碍的有效方法，且无需修改RL算法本身。

Abstract: Reinforcement learning (RL) with outcome-based rewards has proven effective
for improving large language models (LLMs) on complex reasoning tasks. However,
its success often depends on the base model occasionally sampling correct
solutions. When no correct solutions are sampled, training encounters a
zero-reward barrier where learning stalls due to zero gradients. We study this
scenario through the graph search task introduced in Bachmann et al. (2024) and
evaluate recent methods that incorporate desirable components such as dense
rewards, diversity incentives, and improved credit assignment. Our experiments
show that none of these approaches overcome the zero-reward barrier if the base
model never produces a correct answer. In contrast, we find that a simple
data-centric intervention of adding easier samples to the training set enables
the model to eventually solve the original hard task despite starting from zero
reward. Importantly, this succeeds without modifying the RL algorithm itself.
Because official implementations of several baselines were unavailable, we
developed our own, which allowed us to conduct a detailed analysis of their
failure modes. We release these implementations to support further research at:
https://github.com/rl4reasoning/rl-baselines

</details>


### [376] [Beyond Softmax: A New Perspective on Gradient Bandits](https://arxiv.org/abs/2510.03979)
*Emerson Melo,David Müller*

Main category: cs.LG

TL;DR: 论文建立了离散选择模型与在线学习及多臂老虎机理论的联系，提出了新的算法家族和广义梯度老虎机算法。


<details>
  <summary>Details</summary>
Motivation: 旨在扩展现有老虎机算法，突破softmax的独立性限制，适应更广泛的相关学习动态。

Method: 提出子线性遗憾界的算法家族，基于广义嵌套logit模型的新算法，以及广义梯度老虎机算法。

Result: 数值实验验证了算法在随机老虎机场景中的实际有效性。

Conclusion: 新算法结合了灵活模型规范和计算效率，扩展了梯度老虎机方法的适用性。

Abstract: We establish a link between a class of discrete choice models and the theory
of online learning and multi-armed bandits. Our contributions are: (i)
sublinear regret bounds for a broad algorithmic family, encompassing Exp3 as a
special case; (ii) a new class of adversarial bandit algorithms derived from
generalized nested logit models \citep{wen:2001}; and (iii)
\textcolor{black}{we introduce a novel class of generalized gradient bandit
algorithms that extends beyond the widely used softmax formulation. By relaxing
the restrictive independence assumptions inherent in softmax, our framework
accommodates correlated learning dynamics across actions, thereby broadening
the applicability of gradient bandit methods.} Overall, the proposed algorithms
combine flexible model specification with computational efficiency via
closed-form sampling probabilities. Numerical experiments in stochastic bandit
settings demonstrate their practical effectiveness.

</details>


### [377] [ICEPool: Enhancing Graph Pooling Networks with Inter-cluster Connectivity](https://arxiv.org/abs/2510.03987)
*Michael Yang*

Main category: cs.LG

TL;DR: ICEPool是一种新型的层次化池化框架，旨在增强模型对簇间连通性的理解，并保持原始图的结构完整性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在设计簇分配和粗化策略时，往往忽略了簇间的关系。

Method: 提出ICEPool框架，通过增强簇间连通性来改进现有池化模型。

Result: 实验证明ICEPool与多种模型兼容，并能提升性能。

Conclusion: ICEPool通过强调簇间连通性，提供了更全面和鲁棒的图级表示。

Abstract: Hierarchical Pooling Models have demonstrated strong performance in
classifying graph-structured data. While numerous innovative methods have been
proposed to design cluster assignments and coarsening strategies, the
relationships between clusters are often overlooked. In this paper, we
introduce Inter-cluster Connectivity Enhancement Pooling (ICEPool), a novel
hierarchical pooling framework designed to enhance model's understanding of
inter-cluster connectivity and ability of preserving the structural integrity
in the original graph. ICEPool is compatible with a wide range of pooling-based
GNN models. The deployment of ICEPool as an enhancement to existing models
effectively combines the strengths of the original model with ICEPool's
capability to emphasize the integration of inter-cluster connectivity,
resulting in a more comprehensive and robust graph-level representation.
Moreover, we make theoretical analysis to ICEPool's ability of graph
reconstruction to demonstrate its effectiveness in learning inter-cluster
relationship that is overlooked by conventional models. Finally, the
experimental results show the compatibility of ICEPool with wide varieties of
models and its potential to boost the performance of existing graph neural
network architectures.

</details>


### [378] [Distilling Reasoning into Student LLMs: Local Naturalness for Selecting Teacher Data](https://arxiv.org/abs/2510.03988)
*Hoang Anh Just,Myeongseob Ko,Ruoxi Jia*

Main category: cs.LG

TL;DR: 论文研究了在多教师模型设置下，如何选择最佳响应以优化学生模型的推理能力，提出了局部自然性方法，显著提升了学生模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在多教师模型设置下失效，尤其是当推理轨迹变长时，全局自然性与下游性能不再相关。

Method: 引入局部自然性方法，通过测量学生对短序列推理步骤的局部窗口条件概率来评估响应质量。

Result: 局部自然性方法在多教师设置下显著提升了学生模型的准确性，比全局选择方法高出9.4个百分点。

Conclusion: 局部数据质量评估和数据混合能更有效地提升推理蒸馏的效果。

Abstract: Distilling long reasoning traces (10K+ tokens) from stronger teacher models
into smaller student LLMs via SFT has emerged as a standard paradigm. This
approach is practical and efficient: it leverages the ease of generating
abundant reasoning data from stronger models and provides a direct, data-driven
way to teach less capable models better reasoning. While previous work has
largely focused on prompt selection with responses from a single teacher, the
equally important problem of choosing the best response when multiple teacher
outputs are available for a single prompt remains underexplored. This challenge
becomes important in a multi-teacher setting, where different students may
benefit from the outputs of different teachers. This paper fills that gap with
a systematic study of response selection for reasoning distillation. We first
show that the current method, which picks responses the student assigns the
highest global log-probability (global naturalness), fails when responses come
from multiple teachers, i.e., global naturalness no longer correlates with
downstream performance, especially as the reasoning traces from strong teachers
become longer. To overcome this problem, we introduce Local Naturalness, which
measures the student's log-probabilities over short, sequential reasoning steps
conditioned only on a small local window. Local Naturalness enables two
applications: 1) Teacher Selection: Aggregating local scores across prompts
reliably identifies the most helpful teacher. 2) Response Selection from a
Multiple Teachers: When mixing answers from many teachers, Local Naturalness
boosts a 32B student's accuracy on math benchmarks by 9.4pp over global
selection, also surpassing the performance achieved by training on data from
the single best teacher. These results highlight the power of localized data
quality evaluation and data mixing for more effective reasoning distillation.

</details>


### [379] [A Mathematical Explanation of Transformers for Large Language Models and GPTs](https://arxiv.org/abs/2510.03989)
*Xue-Cheng Tai,Hao Liu,Lingfeng Li,Raymond H. Chan*

Main category: cs.LG

TL;DR: 论文提出了一种连续框架，将Transformer解释为结构化积分微分方程的离散化，为理解其核心组件提供了统一的理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在序列建模中取得了巨大成功，但其数学理论仍不完善。本文旨在填补这一空白，通过连续数学建模解释Transformer的结构和操作。

Method: 提出了一种连续框架，将Transformer视为积分微分方程的离散化，其中自注意力机制和层归一化分别被解释为非局部积分算子和时间相关约束的投影。

Result: 该框架为Transformer的核心组件（如注意力、前馈层和归一化）提供了统一且可解释的理论基础，并为架构设计和分析提供了新方向。

Conclusion: 本文通过连续数学建模为Transformer提供了理论支持，为深度学习架构与连续数学建模之间的桥梁搭建了重要一步。

Abstract: The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.

</details>


### [380] [Incorporating Multivariate Consistency in ML-Based Weather Forecasting with Latent-space Constraints](https://arxiv.org/abs/2510.04006)
*Hang Fan,Yi Xiao,Yongquan Qu,Fenghua Ling,Ben Fei,Lei Bai,Pierre Gentine*

Main category: cs.LG

TL;DR: 论文提出了一种基于弱约束四维变分数据同化（WC-4DVar）的机器学习方法，通过潜在空间损失函数改进长期天气预报的物理真实性和精细结构。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习天气预报模型忽略物理耦合和空间结构，导致长期预测模糊且不真实。

Method: 将模型训练重新解释为WC-4DVar问题，利用自动编码器在潜在空间计算损失函数，避免高维模型空间的误差协方差建模。

Result: 潜在空间约束的滚动训练提高了长期预报技能，保留了精细结构和物理真实性。

Conclusion: 该方法为多源数据联合训练提供了统一的理论框架，提升了天气预报模型的性能。

Abstract: Data-driven machine learning (ML) models have recently shown promise in
surpassing traditional physics-based approaches for weather forecasting,
leading to a so-called second revolution in weather forecasting. However, most
ML-based forecast models treat reanalysis as the truth and are trained under
variable-specific loss weighting, ignoring their physical coupling and spatial
structure. Over long time horizons, the forecasts become blurry and physically
unrealistic under rollout training. To address this, we reinterpret model
training as a weak-constraint four-dimensional variational data assimilation
(WC-4DVar) problem, treating reanalysis data as imperfect observations. This
allows the loss function to incorporate reanalysis error covariance and capture
multivariate dependencies. In practice, we compute the loss in a latent space
learned by an autoencoder (AE), where the reanalysis error covariance becomes
approximately diagonal, thus avoiding the need to explicitly model it in the
high-dimensional model space. We show that rollout training with latent-space
constraints improves long-term forecast skill and better preserves fine-scale
structures and physical realism compared to training with model-space loss.
Finally, we extend this framework to accommodate heterogeneous data sources,
enabling the forecast model to be trained jointly on reanalysis and
multi-source observations within a unified theoretical formulation.

</details>


### [381] [Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention](https://arxiv.org/abs/2510.04008)
*Sahil Joshi,Agniva Chowdhury,Amar Kanakamedala,Ekam Singh,Evan Tu,Anshumali Shrivastava*

Main category: cs.LG

TL;DR: RACE Attention是一种线性复杂度的注意力机制，替代了二次复杂度的Softmax Attention，适用于超长上下文处理。


<details>
  <summary>Details</summary>
Motivation: 解决Softmax Attention在长上下文处理中的高计算复杂度和内存消耗问题。

Method: 使用锐化角度相似性替代指数核，结合随机投影和软局部敏感哈希近似注意力输出。

Result: 在语言建模、掩码语言建模和文本分类任务中，RACE Attention在保持准确性的同时显著减少运行时间和内存消耗。

Conclusion: RACE Attention为当前硬件上的超长上下文处理提供了实用且理论支持的方法。

Abstract: Softmax Attention has a quadratic time complexity, which becomes prohibitive
to run at long contexts, even with highly optimized GPU kernels. For example,
FlashAttention (an exact, GPU-optimized implementation of Softmax Attention)
cannot complete a single forward-backward pass of a multi-head attention layer
once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We
introduce RACE Attention, a kernel-inspired alternative to Softmax Attention
that is linear in sequence length and embedding dimension. RACE Attention
replaces the exponential kernel with a sharpened angular (cosine) similarity,
and approximates attention outputs via randomized projections and soft
Locality-Sensitive Hashing (LSH). Across language modeling, masked language
modeling, and text classification, RACE Attention matches the accuracy of
strong baselines while reducing runtime and memory. In a controlled scale test,
it processes up to 12 million tokens during a single forward-backward pass on
an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well
beyond the practical limits of the current state-of-the-art attention
implementations. RACE Attention thus offers a practical, theoretically grounded
mechanism for outrageously long context windows on today's hardware. We hope
that it gets adopted in practice.

</details>


### [382] [Principled and Tractable RL for Reasoning with Diffusion Language Models](https://arxiv.org/abs/2510.04019)
*Anthony Zhan*

Main category: cs.LG

TL;DR: 本文提出了一种名为AGRPO的强化学习算法，专门用于扩散大语言模型（dLLMs），在数学和推理任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习算法不适用于扩散框架，且缺乏理论基础。本文旨在填补这一空白。

Method: 提出了Amortized Group Relative Policy Optimization (AGRPO)，一种基于蒙特卡洛采样的策略梯度方法。

Result: 在GSM8K和Countdown任务中分别实现了7.6%和3.8倍的性能提升。

Conclusion: AGRPO为扩散大语言模型提供了一种理论可靠且实际有效的强化学习方法。

Abstract: Diffusion large language models (dLLMs) are a new paradigm of
non-autoregressive language models that are trained to predict multiple tokens
in parallel and generate text via iterative unmasking. Recent works have
successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B
scale, but dLLMs have yet to benefit from modern post-training techniques, e.g.
reinforcement learning (RL), that have proven effective for autoregressive
models. Crucially, algorithms designed for traditional LLMs aren't directly
compatible with diffusion frameworks due to inherent differences in modeling
assumptions. Moreover, existing attempts at dLLM post-training with RL rely on
heuristic-based objectives with no theoretical grounding. In this work, we
present Amortized Group Relative Policy Optimization (AGRPO), a principled
on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo
sampling to compute an unbiased policy gradient estimate, making it the first
tractable, faithful adaptation of policy gradient methods for dLLMs. We
demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common
setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x
performance on the Countdown task over the baseline LLaDA-8B-Instruct model and
1.3x performance gains over comparable RL methods such as diffu-GRPO.
Furthermore, these gains persist across different numbers of sampling steps at
inference time, achieving better tradeoffs between compute and performance. Our
results demonstrate that online RL algorithms can be extended to diffusion LLMs
in principled ways, maintaining both theoretical soundness and practical
effectiveness.

</details>


### [383] [Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models](https://arxiv.org/abs/2510.04020)
*Hao Wu,Yuan Gao,Xingjian Shi,Shuaipeng Li,Fan Xu,Fan Zhang,Zhihong Zhu,Weiyan Wang,Xiao Luo,Kun Wang,Xian Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: 提出了一种基于模型强化学习的新范式SFP，通过生成世界模型模拟未来状态，结合非可微指标优化预测模型。


<details>
  <summary>Details</summary>
Motivation: 解决物理时空预测中随机性和非可微指标的挑战。

Method: 构建生成世界模型，结合波束搜索规划算法，利用非可微指标作为奖励信号进行自训练优化。

Result: 显著减少预测误差，在极端事件捕获等关键指标上表现优异。

Conclusion: SFP为时空预测提供了一种高效且性能优越的新方法。

Abstract: To address the dual challenges of inherent stochasticity and
non-differentiable metrics in physical spatiotemporal forecasting, we propose
Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in
Model-Based Reinforcement Learning. SFP constructs a novel Generative World
Model to simulate diverse, high-fidelity future states, enabling an
"imagination-based" environmental simulation. Within this framework, a base
forecasting model acts as an agent, guided by a beam search-based planning
algorithm that leverages non-differentiable domain metrics as reward signals to
explore high-return future sequences. These identified high-reward candidates
then serve as pseudo-labels to continuously optimize the agent's policy through
iterative self-training, significantly reducing prediction error and
demonstrating exceptional performance on critical domain metrics like capturing
extreme events.

</details>


### [384] [Multi-Class Support Vector Machine with Differential Privacy](https://arxiv.org/abs/2510.04027)
*Jinseong Park,Yujin Choi,Jaewook Lee*

Main category: cs.LG

TL;DR: 提出了一种新的差分隐私多类支持向量机（PMSVM），通过权重和梯度扰动方法，解决了传统多类SVM在差分隐私下隐私预算消耗过多的问题。


<details>
  <summary>Details</summary>
Motivation: 传统多类SVM在差分隐私框架下因重复查询数据样本导致隐私预算消耗过大，需要一种更高效的方法。

Method: 采用权重和梯度扰动技术，构建一次访问数据的多类SVM边界，并进行敏感性和收敛性分析以确保差分隐私。

Result: 实验结果表明，PMSVM在多类场景下优于现有差分隐私SVM方法。

Conclusion: PMSVM为多类SVM提供了一种高效的差分隐私解决方案，具有理论和实证优势。

Abstract: With the increasing need to safeguard data privacy in machine learning
models, differential privacy (DP) is one of the major frameworks to build
privacy-preserving models. Support Vector Machines (SVMs) are widely used
traditional machine learning models due to their robust margin guarantees and
strong empirical performance in binary classification. However, applying DP to
multi-class SVMs is inadequate, as the standard one-versus-rest (OvR) and
one-versus-one (OvO) approaches repeatedly query each data sample when building
multiple binary classifiers, thus consuming the privacy budget proportionally
to the number of classes. To overcome this limitation, we explore all-in-one
SVM approaches for DP, which access each data sample only once to construct
multi-class SVM boundaries with margin maximization properties. We propose a
novel differentially Private Multi-class SVM (PMSVM) with weight and gradient
perturbation methods, providing rigorous sensitivity and convergence analyses
to ensure DP in all-in-one SVMs. Empirical results demonstrate that our
approach surpasses existing DP-SVM methods in multi-class scenarios.

</details>


### [385] [The Debate on RLVR Reasoning Capability Boundary: Shrinkage, Expansion, or Both? A Two-Stage Dynamic View](https://arxiv.org/abs/2510.04028)
*Xinhao Yao,Lu Yu,Xiaolin Hu,Fengwei Teng,Qing Cui,Jun Zhou,Yong Liu*

Main category: cs.LG

TL;DR: RLVR对LLMs推理能力的影响分为两个阶段：利用阶段可能导致能力边界收缩，而探索阶段可能促进能力边界扩展。


<details>
  <summary>Details</summary>
Motivation: 解决关于RLVR是否扩展或收缩LLMs推理能力的争议。

Method: 通过理论和实证分析，揭示RLVR训练中的两阶段概率质量动态。

Result: 利用阶段可能导致能力边界收缩，探索阶段可能促进扩展。

Conclusion: 为开发更高级推理能力提供了理论基础，建议延长训练以进入探索阶段。

Abstract: The ongoing debate on whether reinforcement learning with verifiable rewards
(RLVR) expands or shrinks the reasoning capabilities of large language models
(LLMs) remains unresolved. Some studies contend that RLVR mainly improves
sampling efficiency but at the expense of diversity and exploratory capacity,
resulting in capability boundary shrinkage. In contrast, others demonstrate
that prolonged training can lead to the emergence of novel reasoning
strategies, suggesting capability boundary expansion. To reconcile these
contradictory findings, we theoretically and empirically show that both
perspectives are partially valid-each aligning with a separate phase in an
inherent two-stage probability mass dynamic: (1) Exploitation stage: initially,
the model primarily samples explored high-reward and low-reward tokens, while
rarely selecting the potentially optimal token. Positive advantage estimates
increase the probability of high-reward tokens and decrease those of low-reward
tokens, yet the optimal token's probability remains largely unchanged during
this stage. (2) Exploration stage: as training advances, the growth rate of
previously acquired high-reward tokens slows as their probabilities approach
saturation. When a potentially optimal token-now receiving positive advantage
estimates-is occasionally sampled, its probability increases, while those of
the originally high-reward tokens decrease. This dynamic suggests that
over-exploitation during the exploitation stage may lead to capability boundary
shrinkage, whereas prolonged training into the exploration stage can promote an
expansion of the reasoning capability boundary. Building upon our insights, we
revisit the potential of only using relative negative gradients for prolonging
training, providing a theoretical and empirical foundation for the development
of more advanced reasoning capabilities.

</details>


### [386] [Adaptive kernel-density approach for imbalanced binary classification](https://arxiv.org/abs/2510.04046)
*Kotaro J. Nishimura,Yuichi Sakumura,Kazushi Ikeda*

Main category: cs.LG

TL;DR: 提出了一种名为KOTARO的新方法，通过动态调整决策边界来解决类别不平衡问题，尤其在严重不平衡情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡是二元分类中的常见问题，尤其在医疗诊断和异常检测等领域，正确识别少数类至关重要。传统方法在严重不平衡情况下表现不佳。

Method: KOTARO基于核密度估计（KDE），通过动态调整高斯基函数的带宽，根据局部样本密度优化决策边界。

Result: 实验表明，KOTARO在合成和真实不平衡数据集上优于传统方法，尤其在严重不平衡情况下。

Conclusion: KOTARO是一种有潜力的解决方案，适用于广泛的类别不平衡分类问题。

Abstract: Class imbalance is a common challenge in real-world binary classification
tasks, often leading to predictions biased toward the majority class and
reduced recognition of the minority class. This issue is particularly critical
in domains such as medical diagnosis and anomaly detection, where correct
classification of minority classes is essential. Conventional methods often
fail to deliver satisfactory performance when the imbalance ratio is extremely
severe. To address this challenge, we propose a novel approach called
Kernel-density-Oriented Threshold Adjustment with Regional Optimization
(KOTARO), which extends the framework of kernel density estimation (KDE) by
adaptively adjusting decision boundaries according to local sample density. In
KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on
the estimated density around each sample, thereby enhancing the classifier's
ability to capture minority regions. We validated the effectiveness of KOTARO
through experiments on both synthetic and real-world imbalanced datasets. The
results demonstrated that KOTARO outperformed conventional methods,
particularly under conditions of severe imbalance, highlighting its potential
as a promising solution for a wide range of imbalanced classification problems

</details>


### [387] [Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints](https://arxiv.org/abs/2510.04058)
*Subhodip Panda,MS Varun,Shreyans Jain,Sarthak Kumar Maharana,Prathosh A. P*

Main category: cs.LG

TL;DR: 提出了一种名为VDU的变分扩散遗忘方法，用于在数据受限的情况下从预训练扩散模型中遗忘不良特征，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型可能生成不良、暴力或淫秽内容，现有方法在数据受限时效果不佳，因此需要一种高效的方法来遗忘不良特征。

Method: VDU基于变分推断框架，通过塑性诱导器和稳定性正则化器优化损失函数，仅需部分不良数据即可实现遗忘。

Result: 在MNIST、CIFAR-10和tinyImageNet数据集上验证了VDU在类别和特征遗忘中的有效性。

Conclusion: VDU是一种计算高效的方法，适用于数据受限的场景，能有效遗忘不良特征且不损害生成质量。

Abstract: For a responsible and safe deployment of diffusion models in various domains,
regulating the generated outputs from these models is desirable because such
models could generate undesired, violent, and obscene outputs. To tackle this
problem, recent works use machine unlearning methodology to forget training
data points containing these undesired features from pre-trained generative
models. However, these methods proved to be ineffective in data-constrained
settings where the whole training dataset is inaccessible. Thus, the principal
objective of this work is to propose a machine unlearning methodology that can
prevent the generation of outputs containing undesired features from a
pre-trained diffusion model in such a data-constrained setting. Our proposed
method, termed as Variational Diffusion Unlearning (VDU), is a computationally
efficient method that only requires access to a subset of training data
containing undesired features. Our approach is inspired by the variational
inference framework with the objective of minimizing a loss function consisting
of two terms: plasticity inducer and stability regularizer. Plasticity inducer
reduces the log-likelihood of the undesired training data points, while the
stability regularizer, essential for preventing loss of image generation
quality, regularizes the model in parameter space. We validate the
effectiveness of our method through comprehensive experiments for both class
unlearning and feature unlearning. For class unlearning, we unlearn some
user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a
pre-trained unconditional denoising diffusion probabilistic model (DDPM).
Similarly, for feature unlearning, we unlearn the generation of certain
high-level features from a pre-trained Stable Diffusion model

</details>


### [388] [What Scales in Cross-Entropy Scaling Law?](https://arxiv.org/abs/2510.04067)
*Junxi Yan,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 论文发现交叉熵缩放定律在大型语言模型中失效，提出误差熵缩放定律作为更准确的描述。


<details>
  <summary>Details</summary>
Motivation: 交叉熵缩放定律在大型模型中出现偏差，导致开发困难，需探究其根本原因。

Method: 将交叉熵分解为误差熵、自对齐和置信度三部分，通过理论和实验验证。

Result: 仅误差熵遵循幂律缩放，其他两项不变，解释了交叉熵定律在小模型准确而大模型失效的原因。

Conclusion: 误差熵缩放定律更准确，对大型语言模型的训练和理解有广泛应用。

Abstract: The cross-entropy scaling law has long served as a key tool for guiding the
development of large language models. It shows that cross-entropy loss
decreases in a predictable power-law rate as the model size increases. However,
recent evidence indicates that this law breaks down at very large scales: the
loss decreases more slowly than expected, which causes significant trouble for
developing large language models. In this paper, we hypothesize that the root
cause lies in the fact that cross-entropy itself does not truly scale; instead,
only one of its hidden components does. To investigate this, we introduce a
novel decomposition of cross-entropy into three parts: Error-Entropy,
Self-Alignment, and Confidence. We show both theoretically and empirically that
this decomposition precisely captures the training dynamics and optimization
objectives. Through extensive experiments on multiple datasets and 32 models
spanning five orders of magnitude in size, we find that only error-entropy
follows a robust power-law scaling, while the other two terms remain largely
invariant. Moreover, error-entropy constitutes the dominant share of
cross-entropy in small models but diminishes in proportion as models grow
larger. This explains why the cross-entropy scaling law appears accurate at
small scales but fails at very large ones. Our findings establish the
error-entropy scaling law as a more accurate description of model behavior. We
believe it will have wide applications in the training, understanding, and
future development of large language models.

</details>


### [389] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO是一种新的强化学习框架，通过分解步骤为快速轨迹、重新定位和慢速修正，提升训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO等策略优化算法在早期训练中因低质量轨迹导致的梯度噪声和不稳定更新问题。

Method: SFPO将每一步分解为三个阶段：快速轨迹、重新定位机制和慢速修正，保持目标和轨迹过程不变。

Result: SFPO在数学推理基准上比GRPO平均提升2.80分，减少4.93倍轨迹和4.19倍训练时间。

Conclusion: SFPO是一种高效且兼容现有策略梯度流程的框架，显著提升强化学习的训练效果。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [390] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 论文介绍了离线强化学习在大状态空间中的理论，探讨了从历史数据中学习策略的方法，无需与环境在线交互。


<details>
  <summary>Details</summary>
Motivation: 研究离线强化学习的理论基础，特别是在大状态空间中的应用，以解决在线交互成本高的问题。

Method: 提出了函数逼近的假设（如Bellman完备性与可实现性）和数据覆盖（如全策略与单策略覆盖），并分析了不同假设下的算法和结果。

Result: 描述了丰富的算法和结果，展示了在不同假设下样本和计算复杂度的保证。

Conclusion: 讨论了开放性问题以及与相邻领域的联系，为未来研究提供了方向。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


### [391] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 提出一种独立于类别数量的神经网络训练方法，通过预定义向量系统作为目标潜在空间配置（LSC），适用于大规模或未知类别数量的分类任务。


<details>
  <summary>Details</summary>
Motivation: 监督学习方法在神经网络分类任务中表现优异，但其参数数量通常依赖于类别数量，限制了在类别数量极大或未知时的应用。

Method: 使用预定义向量系统（如An根系统的随机扰动向量）作为目标LSC，训练编码器和视觉变换器（ViT），通过匹配预测与预定义向量实现训练。

Result: 在Cinic-10和ImageNet-1K数据集上成功训练，并在128万类数据集上验证了方法的适用性。

Conclusion: 该方法不仅适用于大规模分类任务，还在持续学习和神经网络蒸馏中展示了潜在应用价值。

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [392] [Rethinking Consistent Multi-Label Classification under Inexact Supervision](https://arxiv.org/abs/2510.04091)
*Wei Wang,Tianhao Ma,Ming-Kun Xie,Gang Niu,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出了一种统一处理部分多标签学习和互补多标签学习的方法，无需依赖标签生成过程的准确估计或均匀分布假设。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在真实场景中难以满足准确估计标签生成过程或均匀分布假设的问题。

Method: 基于一阶和二阶策略提出两种无偏风险估计器。

Result: 理论证明了一致性，并推导了估计误差的收敛速率；实验验证了方法的有效性。

Conclusion: 提出的方法在理论和实验上均优于现有方法，适用于真实场景。

Abstract: Partial multi-label learning and complementary multi-label learning are two
popular weakly supervised multi-label classification paradigms that aim to
alleviate the high annotation costs of collecting precisely annotated
multi-label data. In partial multi-label learning, each instance is annotated
with a candidate label set, among which only some labels are relevant; in
complementary multi-label learning, each instance is annotated with
complementary labels indicating the classes to which the instance does not
belong. Existing consistent approaches for the two paradigms either require
accurate estimation of the generation process of candidate or complementary
labels or assume a uniform distribution to eliminate the estimation problem.
However, both conditions are usually difficult to satisfy in real-world
scenarios. In this paper, we propose consistent approaches that do not rely on
the aforementioned conditions to handle both problems in a unified way.
Specifically, we propose two unbiased risk estimators based on first- and
second-order strategies. Theoretically, we prove consistency w.r.t. two widely
used multi-label classification evaluation metrics and derive convergence rates
for the estimation errors of the proposed risk estimators. Empirically,
extensive experimental results validate the effectiveness of our proposed
approaches against state-of-the-art methods.

</details>


### [393] [Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws](https://arxiv.org/abs/2510.04102)
*Ramzi Dakhmouche,Hossein Gorji*

Main category: cs.LG

TL;DR: 论文探讨了基础模型在时间序列预测中的局限性，尤其是长范围预测的挑战，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在时间序列预测中的表现，尤其是其在长范围预测中的不足，并与物理定律的强外推性对比。

Method: 通过理论分析和实证研究，识别并形式化统计学习模型在训练域外预测能力的基本属性。

Result: 揭示了深度学习模型在外推场景中性能下降的根本原因，并提出了改进方向。

Conclusion: 研究不仅阐明了外推差距的根源，还为设计下一代具备外推能力的预测模型提供了方向。

Abstract: Motivated by the remarkable success of Foundation Models (FMs) in language
modeling, there has been growing interest in developing FMs for time series
prediction, given the transformative power such models hold for science and
engineering. This culminated in significant success of FMs in short-range
forecasting settings. However, extrapolation or long-range forecasting remains
elusive for FMs, which struggle to outperform even simple baselines. This
contrasts with physical laws which have strong extrapolation properties, and
raises the question of the fundamental difference between the structure of
neural networks and physical laws. In this work, we identify and formalize a
fundamental property characterizing the ability of statistical learning models
to predict more accurately outside of their training domain, hence explaining
performance deterioration for deep learning models in extrapolation settings.
In addition to a theoretical analysis, we present empirical results showcasing
the implications of this property on current deep learning architectures. Our
results not only clarify the root causes of the extrapolation gap but also
suggest directions for designing next-generation forecasting models capable of
mastering extrapolation.

</details>


### [394] [Can Linear Probes Measure LLM Uncertainty?](https://arxiv.org/abs/2510.04108)
*Ramzi Dakhmouche,Adrien Letellier,Hossein Gorji*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯统计的UQ方法，通过训练多个贝叶斯线性模型来提升LLM生成任务中的不确定性量化性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM生成任务中的UQ方法仍以最大softmax分数为主，存在不足，需要更高效的方法。

Method: 使用贝叶斯线性回归模型，逐层预测输出，并通过稀疏组合分布特征推断全局不确定性。

Result: 实验表明，该方法在多种LLM上均优于现有基线。

Conclusion: 贝叶斯方法能有效提升LLM生成任务中的UQ性能，且计算高效。

Abstract: Effective Uncertainty Quantification (UQ) represents a key aspect for
reliable deployment of Large Language Models (LLMs) in automated
decision-making and beyond. Yet, for LLM generation with multiple choice
structure, the state-of-the-art in UQ is still dominated by the naive baseline
given by the maximum softmax score. To address this shortcoming, we demonstrate
that taking a principled approach via Bayesian statistics leads to improved
performance despite leveraging the simplest possible model, namely linear
regression. More precisely, we propose to train multiple Bayesian linear
models, each predicting the output of a layer given the output of the previous
one. Based on the obtained layer-level posterior distributions, we infer the
global uncertainty level of the LLM by identifying a sparse combination of
distributional features, leading to an efficient UQ scheme. Numerical
experiments on various LLMs show consistent improvement over state-of-the-art
baselines.

</details>


### [395] [Wasserstein projection distance for fairness testing of regression models](https://arxiv.org/abs/2510.04114)
*Wanxin Li,Yongjin P. Park,Khanh Dao Duc*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein投影的回归模型公平性测试框架，通过假设检验和最优数据扰动方法平衡公平性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注分类任务的公平性，回归模型的公平性研究不足。

Method: 采用Wasserstein投影框架，提出假设检验和最优数据扰动方法，理论分析包括公平性标准分类和统计量推导。

Result: 实验表明，该方法比基于排列的测试具有更高特异性，能有效检测和缓解实际应用中的偏差。

Conclusion: 该框架为回归模型的公平性测试提供了有效工具，平衡了公平性与准确性。

Abstract: Fairness in machine learning is a critical concern, yet most research has
focused on classification tasks, leaving regression models underexplored. This
paper introduces a Wasserstein projection-based framework for fairness testing
in regression models, focusing on expectation-based criteria. We propose a
hypothesis-testing approach and an optimal data perturbation method to improve
fairness while balancing accuracy. Theoretical results include a detailed
categorization of fairness criteria for regression, a dual reformulation of the
Wasserstein projection test statistic, and the derivation of asymptotic bounds
and limiting distributions. Experiments on synthetic and real-world datasets
demonstrate that the proposed method offers higher specificity compared to
permutation-based tests, and effectively detects and mitigates biases in real
applications such as student performance and housing price prediction.

</details>


### [396] [On the Statistical Query Complexity of Learning Semiautomata: a Random Walk Approach](https://arxiv.org/abs/2510.04115)
*George Giapitzakis,Kimon Fountoulakis,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 首次证明了半自动机在输入词和初始状态均匀分布下的统计查询困难性，结果基于状态转移结构而非语言识别。


<details>
  <summary>Details</summary>
Motivation: 研究半自动机在统计查询框架下的计算困难性，填补了该领域的空白。

Method: 通过分析半自动机的状态转移结构，将其转化为对称群上的随机游走问题，并应用傅里叶分析和表示理论。

Result: 证明了在状态数多项式步后，不同半自动机的输出几乎不相关，从而得到统计查询困难性。

Conclusion: 半自动机的统计查询困难性源于其内部状态转移结构，而非语言识别问题。

Abstract: Semiautomata form a rich class of sequence-processing algorithms with
applications in natural language processing, robotics, computational biology,
and data mining. We establish the first Statistical Query hardness result for
semiautomata under the uniform distribution over input words and initial
states. We show that Statistical Query hardness can be established when both
the alphabet size and input length are polynomial in the number of states.
Unlike the case of deterministic finite automata, where hardness typically
arises through the hardness of the language they recognize (e.g., parity), our
result is derived solely from the internal state-transition structure of
semiautomata. Our analysis reduces the task of distinguishing the final states
of two semiautomata to studying the behavior of a random walk on the group
$S_{N} \times S_{N}$. By applying tools from Fourier analysis and the
representation theory of the symmetric group, we obtain tight spectral gap
bounds, demonstrating that after a polynomial number of steps in the number of
states, distinct semiautomata become nearly uncorrelated, yielding the desired
hardness result.

</details>


### [397] [Attending on Multilevel Structure of Proteins enables Accurate Prediction of Cold-Start Drug-Target Interactions](https://arxiv.org/abs/2510.04126)
*Ziying Zhang,Yaqing Wang,Yuxuan Sun,Min Ye,Quanming Yao*

Main category: cs.LG

TL;DR: ColdDTI是一个专注于蛋白质多级结构的冷启动药物-靶点相互作用预测框架，通过分层注意力机制挖掘多级蛋白质结构与药物结构的相互作用，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅利用蛋白质的一级结构，忽略了多级结构对药物-靶点相互作用的影响，限制了预测能力。

Method: 采用分层注意力机制，从局部和全局粒度挖掘蛋白质多级结构（从一级到四级）与药物结构的相互作用，并融合不同层次的结构表示进行预测。

Result: 在基准数据集上的实验表明，ColdDTI在冷启动场景下显著优于现有方法。

Conclusion: ColdDTI通过关注蛋白质多级结构，有效提升了冷启动药物-靶点相互作用的预测性能。

Abstract: Cold-start drug-target interaction (DTI) prediction focuses on interaction
between novel drugs and proteins. Previous methods typically learn transferable
interaction patterns between structures of drug and proteins to tackle it.
However, insight from proteomics suggest that protein have multi-level
structures and they all influence the DTI. Existing works usually represent
protein with only primary structures, limiting their ability to capture
interactions involving higher-level structures. Inspired by this insight, we
propose ColdDTI, a framework attending on protein multi-level structure for
cold-start DTI prediction. We employ hierarchical attention mechanism to mine
interaction between multi-level protein structures (from primary to quaternary)
and drug structures at both local and global granularities. Then, we leverage
mined interactions to fuse structure representations of different levels for
final prediction. Our design captures biologically transferable priors,
avoiding the risk of overfitting caused by excessive reliance on representation
learning. Experiments on benchmark datasets demonstrate that ColdDTI
consistently outperforms previous methods in cold-start settings.

</details>


### [398] [On the Limitations and Capabilities of Position Embeddings for Length Generalization](https://arxiv.org/abs/2510.04130)
*Yang Chen,Yitao Liang,Zhouchen Lin*

Main category: cs.LG

TL;DR: 研究了Transformer中位置嵌入（PEs）对长度泛化（LG）的影响，提出了理论分析和实用策略。


<details>
  <summary>Details</summary>
Motivation: PEs在LG中的作用尚不明确，需要深入理解其局限性和能力。

Method: 理论分析了PEs在POLAs中的作用，提出了LRC和SRC概念，并设计了Scale Hint和学习型PE框架。

Result: PEs不扩展计算能力，但结构化计算；SRC不变性是LG的关键，实验验证了假设。

Conclusion: 提供了改进Transformer中LG的理论见解和实用方法。

Abstract: In Transformers, Position Embeddings (PEs) significantly influence Length
Generalization (LG) performance, yet their fundamental role remains unclear. In
this work, we investigate the limitations and capabilities of PEs in achieving
LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs),
introducing Linear Representation Complexity (LRC) to characterize when PEs
enable LG. Our analysis shows that PEs do not expand computational capabilities
but structure learned computations across positions. Extending to practical
Transformers, we propose Sequential Representation Complexity (SRC) and
conjecture that LG is possible if and only if SRC remains invariant across
scales. We support this hypothesis with empirical evidence in various reasoning
tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance
scaling, and a Learning-Based Position Embedding framework that automatically
learns positional relations. Our work provides theoretical insights and
practical strategies for improving LG in Transformers.

</details>


### [399] [Modeling Time Series Dynamics with Fourier Ordinary Differential Equations](https://arxiv.org/abs/2510.04133)
*Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出了一种基于傅里叶域的FODEs方法，解决了NODEs在时间域中难以捕捉长期依赖和周期性结构的问题，并通过可学习的滤波机制提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: NODEs在时间域建模时难以捕捉长期依赖和周期性结构，且连续时间模型与离散数据的不匹配导致精度下降。

Method: 将时间序列数据通过FFT转换到频域，引入可学习的滤波机制，以保留细节并提升精度。

Result: 实验表明FODEs在多个时间序列数据集上优于现有方法，准确性和效率均有提升。

Conclusion: FODEs通过频域建模和滤波机制，有效捕捉长短期模式，为时间序列建模提供了鲁棒框架。

Abstract: Neural ODEs (NODEs) have emerged as powerful tools for modeling time series
data, offering the flexibility to adapt to varying input scales and capture
complex dynamics. However, they face significant challenges: first, their
reliance on time-domain representations often limits their ability to capture
long-term dependencies and periodic structures; second, the inherent mismatch
between their continuous-time formulation and the discrete nature of real-world
data can lead to loss of granularity and predictive accuracy. To address these
limitations, we propose Fourier Ordinary Differential Equations (FODEs), an
approach that embeds the dynamics in the Fourier domain. By transforming
time-series data into the frequency domain using the Fast Fourier Transform
(FFT), FODEs uncover global patterns and periodic behaviors that remain elusive
in the time domain. Additionally, we introduce a learnable element-wise
filtering mechanism that aligns continuous model outputs with discrete
observations, preserving granularity and enhancing accuracy. Experiments on
various time series datasets demonstrate that FODEs outperform existing methods
in terms of both accuracy and efficiency. By effectively capturing both long-
and short-term patterns, FODEs provide a robust framework for modeling time
series dynamics.

</details>


### [400] [PhaseFormer: From Patches to Phases for Efficient and Effective Time Series Forecasting](https://arxiv.org/abs/2510.04134)
*Yiming Niu,Jinliang Deng,Yongxin Tong*

Main category: cs.LG

TL;DR: PhaseFormer提出了一种基于相位视角的高效时间序列预测方法，显著提升了效率与效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于patch的方法效率低下，参数多且计算成本高，PhaseFormer旨在解决这一问题。

Method: 通过紧凑的相位嵌入和轻量级路由机制实现跨相位交互，提出PhaseFormer模型。

Result: PhaseFormer在多个基准数据集上表现优异，仅需约1k参数，尤其在大规模复杂数据上表现突出。

Conclusion: PhaseFormer为高效且有效的时间序列预测迈出了重要一步。

Abstract: Periodicity is a fundamental characteristic of time series data and has long
played a central role in forecasting. Recent deep learning methods strengthen
the exploitation of periodicity by treating patches as basic tokens, thereby
improving predictive effectiveness. However, their efficiency remains a
bottleneck due to large parameter counts and heavy computational costs. This
paper provides, for the first time, a clear explanation of why patch-level
processing is inherently inefficient, supported by strong evidence from
real-world data. To address these limitations, we introduce a phase perspective
for modeling periodicity and present an efficient yet effective solution,
PhaseFormer. PhaseFormer features phase-wise prediction through compact phase
embeddings and efficient cross-phase interaction enabled by a lightweight
routing mechanism. Extensive experiments demonstrate that PhaseFormer achieves
state-of-the-art performance with around 1k parameters, consistently across
benchmark datasets. Notably, it excels on large-scale and complex datasets,
where models with comparable efficiency often struggle. This work marks a
significant step toward truly efficient and effective time series forecasting.
Code is available at this repository:
https://github.com/neumyor/PhaseFormer_TSL

</details>


### [401] [Efficient Manifold-Constrained Neural ODE for High-Dimensional Datasets](https://arxiv.org/abs/2510.04138)
*Muhao Guo,Haoran Li,Yang Weng*

Main category: cs.LG

TL;DR: 提出了一种结合低维流形与神经ODE的新方法，显著提升了高维数据处理的效率和精度。


<details>
  <summary>Details</summary>
Motivation: 高维系统中神经ODE的计算量大且截断误差高，现有方法依赖已知流形信息，但实际场景中流形通常未知。

Method: 使用结构保持编码器发现底层流形，并设计新方法将神经ODE与流形结合。

Result: 在多个数据集上验证了方法在精度、计算效率和收敛速度上的优越性。

Conclusion: 新方法有效解决了高维数据处理的挑战，具有显著性能优势。

Abstract: Neural ordinary differential equations (NODE) have garnered significant
attention for their design of continuous-depth neural networks and the ability
to learn data/feature dynamics. However, for high-dimensional systems,
estimating dynamics requires extensive calculations and suffers from high
truncation errors for the ODE solvers. To address the issue, one intuitive
approach is to consider the non-trivial topological space of the data
distribution, i.e., a low-dimensional manifold. Existing methods often rely on
knowledge of the manifold for projection or implicit transformation,
restricting the ODE solutions on the manifold. Nevertheless, such knowledge is
usually unknown in realistic scenarios. Therefore, we propose a novel approach
to explore the underlying manifold to restrict the ODE process. Specifically,
we employ a structure-preserved encoder to process data and find the underlying
graph to approximate the manifold. Moreover, we propose novel methods to
combine the NODE learning with the manifold, resulting in significant gains in
computational speed and accuracy. Our experimental evaluations encompass
multiple datasets, where we compare the accuracy, number of function
evaluations (NFEs), and convergence speed of our model against existing
baselines. Our results demonstrate superior performance, underscoring the
effectiveness of our approach in addressing the challenges of high-dimensional
datasets.

</details>


### [402] [Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models](https://arxiv.org/abs/2510.04146)
*Minseo Kim,Coleman Hooper,Aditya Tomar,Chenfeng Xu,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.LG

TL;DR: 对比了自回归语言模型（ARMs）和扩散语言模型（DLMs）的性能特点，发现DLMs在并行性上有优势但难以扩展长上下文，而ARMs在批量推理中表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究DLMs作为ARMs替代架构的性能表现，填补对两者性能差异理解的空白。

Method: 通过理论分析和性能数据对比ARMs和DLMs，探索块级解码和批量推理的权衡。

Result: DLMs在算术强度上优于ARMs，但难以扩展长上下文；块级解码可提升DLMs性能；ARMs在批量推理中吞吐量更高。

Conclusion: DLMs在并行性上有潜力，但需优化采样步骤以减少延迟；ARMs在批量推理中仍具优势。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art performance on a
broad range of Natural Language Processing (NLP) tasks, including document
processing and coding. Autoregressive Language Models (ARMs), which generate
tokens sequentially conditioned on all previous tokens, have been the
predominant paradigm for LLMs. However, while these networks have achieved high
accuracy across a range of downstream tasks, they exhibit low arithmetic
intensity due to the inherent sequential dependency with next-token prediction.
Recently, Diffusion Language Models (DLMs) have emerged as a promising
alternative architecture. DLMs generate output text in parallel, breaking the
limitations of sequential dependency. However, the performance implications of
DLMs relative to commonly deployed ARMs are not fully understood. In this work,
we present a comprehensive performance study analyzing the performance
characteristics of ARMs and DLMs, using both theoretical analysis and profiling
data to characterize the trade-offs between these approaches. We illustrate
that although DLMs exhibit higher arithmetic intensity compared to ARMs because
of their capability to utilize parallelism across sequence lengths, they fail
to scale effectively to longer contexts. We then explore DLMs with block-wise
decoding, outlining how this approach allows for increased arithmetic
intensity, while still scaling well to long contexts (similar to ARMs). We also
show interesting trade-offs for batched inference, where we find that ARMs
exhibit superior throughput, as they benefit more from parallelism across
sequences in the batch. Finally, we highlight opportunities for accelerating
DLM inference, and, in particular, highlight the importance of reducing the
number of sampling steps for allowing open-source DLMs to provide improved
latency relative to ARMs.

</details>


### [403] [Finite Time Analysis of Constrained Natural Critic-Actor Algorithm with Improved Sample Complexity](https://arxiv.org/abs/2510.04189)
*Prashansa Panda,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: 本文提出了一种新的critic-actor算法，用于长期平均成本设置和不等式约束下，首次提供了非渐近收敛保证，并展示了在Safety-Gym环境中的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在折扣成本设置下的critic-actor算法，且多为渐近收敛分析。本文旨在填补长期平均成本设置和不等式约束下的非渐近收敛分析空白。

Method: 提出了一种自然critic-actor算法，采用函数逼近方法，并提供了非渐近收敛保证。同时优化了学习率并改进了样本复杂度。

Result: 实验在三个不同的Safety-Gym环境中进行，结果显示该算法与其他知名算法相比具有竞争力。

Conclusion: 本文提出的算法在理论和实验上均表现出色，为长期平均成本设置和约束条件下的强化学习提供了新的解决方案。

Abstract: Recent studies have increasingly focused on non-asymptotic convergence
analyses for actor-critic (AC) algorithms. One such effort introduced a
two-timescale critic-actor algorithm for the discounted cost setting using a
tabular representation, where the usual roles of the actor and critic are
reversed. However, only asymptotic convergence was established there.
Subsequently, both asymptotic and non-asymptotic analyses of the critic-actor
algorithm with linear function approximation were conducted. In our work, we
introduce the first natural critic-actor algorithm with function approximation
for the long-run average cost setting and under inequality constraints. We
provide the non-asymptotic convergence guarantees for this algorithm. Our
analysis establishes optimal learning rates and we also propose a modification
to enhance sample complexity. We further show the results of experiments on
three different Safety-Gym environments where our algorithm is found to be
competitive in comparison with other well known algorithms.

</details>


### [404] [Spectral Alignment as Predictor of Loss Explosion in Neural Network Training](https://arxiv.org/abs/2510.04202)
*Haiquan Qiu,You Wu,Yingjie Tan,Yaqing Wang,Quanming Yao*

Main category: cs.LG

TL;DR: 论文提出了一种名为谱对齐（SA）的新指标，用于早期预测深度神经网络训练中的损失爆炸问题，比传统指标更早、更清晰地发出警告。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中的损失爆炸可能导致昂贵的训练失败，而传统监控指标（如权重和梯度范数）滞后且模糊，难以统一标准。

Method: 引入谱对齐（SA）指标，监控层输入与权重矩阵主奇异向量的分布对齐情况，通过其符号多样性的崩溃预测训练失败。

Result: 实验证明，SA能比传统标量指标更早、更清晰地预测损失爆炸，且计算开销低。

Conclusion: SA是一种实用的工具，可用于保护模型训练，避免损失爆炸。

Abstract: Loss explosions in training deep neural networks can nullify multi-million
dollar training runs. Conventional monitoring metrics like weight and gradient
norms are often lagging and ambiguous predictors, as their values vary
dramatically across different models and even between layers of the same model,
making it difficult to establish a unified standard for detecting impending
failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded
metric that monitors the distributional alignment between layer inputs and the
principal singular vectors of weight matrices. We show that a collapse in the
sign diversity of this alignment is a powerful early predictor of
representational collapse and training divergence. Empirical results on
language models demonstrate that monitoring the SA distribution provides a
significantly earlier and clearer warning of loss explosions than traditional
scalar metrics. SA's low computational overhead makes it a practical tool for
safeguarding model training.

</details>


### [405] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出了一种自适应联邦学习方法，通过动态调整客户端和服务器的学习率和动量参数，解决了异构联邦学习中超参数选择的难题。


<details>
  <summary>Details</summary>
Motivation: 异构联邦学习中，客户端的计算能力和数据分布差异大，手动调整超参数成本高且复杂。

Method: 将联邦学习建模为动态系统，利用数值模拟和物理设计原理，自适应选择学习率和动量参数。

Result: 实现了快速稳定的收敛，且对全局超参数选择不敏感，优于现有自适应方法。

Conclusion: 该方法无需手动调整超参数，适用于快速原型设计和规模化部署。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [406] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN是一种改进的DoRA方法，通过噪声注入和动态低秩矩阵生成，提升了训练稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 解决DoRA在训练稳定性和样本效率上的不足，进一步优化参数高效微调方法。

Method: 1. 在DoRA的权重分解分母中注入噪声作为自适应正则化器；2. 用辅助网络动态生成低秩矩阵，实现跨层参数耦合。

Result: 在视觉和语言基准测试中，DoRAN表现优于LoRA、DoRA和其他PEFT基线方法。

Conclusion: 结合噪声正则化和网络参数生成，为高效微调基础模型提供了新方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [407] [PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression](https://arxiv.org/abs/2510.04205)
*Di Zhang*

Main category: cs.LG

TL;DR: PolyKAN提出了一种理论框架，用于压缩KANs，通过多面体区域合并实现模型大小减少和误差控制。


<details>
  <summary>Details</summary>
Motivation: KANs在可解释性和数学基础上优于MLPs，但参数效率低限制了实际应用。

Method: 利用KANs的分段多项式结构，将压缩问题转化为最优多面体区域合并，并设计动态规划算法。

Result: PolyKAN在保证误差控制的同时实现最小压缩，且复杂度为多项式时间。

Conclusion: 该框架为KAN压缩提供了首个数学保证，推动了可解释神经架构的高效部署。

Abstract: Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability
and a strong mathematical foundation. However, their parameter efficiency
remains a significant challenge for practical deployment. This paper introduces
PolyKAN, a novel theoretical framework for KAN compression that provides formal
guarantees on both model size reduction and approximation error. By leveraging
the inherent piecewise polynomial structure of KANs, we formulate the
compression problem as one of optimal polyhedral region merging. We establish a
rigorous polyhedral characterization of KANs, develop a complete theory of
$\epsilon$-equivalent compression, and design an optimal dynamic programming
algorithm that guarantees minimal compression under specified error bounds. Our
theoretical analysis demonstrates that PolyKAN achieves provably minimal
compression while maintaining strict error control, with polynomial-time
complexity in all network parameters. The framework provides the first formal
foundation for KAN compression with mathematical guarantees, opening new
directions for efficient deployment of interpretable neural architectures.

</details>


### [408] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了一种基于高斯分布的部分信息分解（GPID）方法，通过梯度优化提高计算效率，并扩展到非高斯数据。


<details>
  <summary>Details</summary>
Motivation: 现有PID方法对连续和高维模态计算成本高且不准确，需改进。

Method: 利用高斯分布特性优化PID计算，提出梯度算法，并通过编码器处理非高斯数据。

Result: 在合成和真实数据中，新方法比基线更准确高效。

Conclusion: GPID方法在计算效率和适用性上优于现有方法，适用于多模态数据分析。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [409] [Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention](https://arxiv.org/abs/2510.04212)
*Haiquan Qiu,Quanming Yao*

Main category: cs.LG

TL;DR: 论文分析了低精度训练中Flash Attention导致损失爆炸的机制，提出了一种简单的改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究低精度训练中Flash Attention导致训练不稳定的根本原因。

Method: 通过深入分析，揭示了低秩表示和低精度算术误差的相互作用，并提出了改进方法。

Result: 改进后的Flash Attention有效稳定了训练过程。

Conclusion: 研究揭示了问题的根源，并提供了实用的解决方案。

Abstract: The pursuit of computational efficiency has driven the adoption of
low-precision formats for training transformer models. However, this progress
is often hindered by notorious training instabilities. This paper provides the
first mechanistic explanation for a long-standing and unresolved failure case
where training with flash attention in low-precision settings leads to
catastrophic loss explosions. Our in-depth analysis reveals that the failure is
not a random artifact but caused by two intertwined phenomena: the emergence of
similar low-rank representations within the attention mechanism and the
compounding effect of biased rounding errors inherent in low-precision
arithmetic. We demonstrate how these factors create a vicious cycle of error
accumulation that corrupts weight updates, ultimately derailing the training
dynamics. To validate our findings, we introduce a minimal modification to the
flash attention that mitigates the bias in rounding errors. This simple change
stabilizes the training process, confirming our analysis and offering a
practical solution to this persistent problem.

</details>


### [410] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 使用条件归一化流（Full-Glow）实时生成符合标准的城市声压地图，速度比传统求解器快2000倍，并提高了非视距（NLoS）精度。


<details>
  <summary>Details</summary>
Motivation: 城市噪声预测对公共健康和法规流程至关重要，但传统物理求解器速度慢，无法满足实时交互需求。

Method: 采用条件归一化流（Full-Glow）从2D城市布局生成声压地图，支持实时计算和交互式探索。

Result: 模型在256x256地图上速度提升2000倍，NLoS精度提高24%，基线NLoS的MAE为0.65 dB。

Conclusion: 该模型为城市规划、合规地图和运营提供了实用工具，支持即时重新计算以适应源或几何变化。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [411] [MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering](https://arxiv.org/abs/2510.04217)
*Chenlu Ding,Jiancan Wu,Leheng Sheng,Fan Zhang,Yancheng Yuan,Xiang Wang,Xiangnan He*

Main category: cs.LG

TL;DR: MLLMEraser是一种无需训练、输入感知的测试时遗忘框架，通过激活导向动态擦除知识，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLMs）中记忆的隐私数据、过时知识和有害内容问题，现有方法计算成本高且不可逆。

Method: 利用激活导向构建多模态擦除方向，并通过输入感知机制自适应应用擦除方向。

Result: 在LLaVA-1.5和Qwen-2.5-VL上表现优于现有基线，计算成本低且效用损失小。

Conclusion: MLLMEraser高效且灵活，适用于动态知识擦除需求。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.

</details>


### [412] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: RegCache是一种无需训练的算法，通过引入前缀标记来缓解视觉编码器中的异常值，显著降低量化时的精度损失。


<details>
  <summary>Details</summary>
Motivation: 由于视觉编码器在实时处理大规模视觉数据时的高计算成本，降低其推理成本至关重要。8位量化仍因异常值问题而具有挑战性。

Method: 提出RegCache算法，通过添加异常值易发但语义无关的前缀标记，防止其他标记出现异常值。针对视觉编码器的异常值特性，提出中层前缀标记和标记删除技术。

Result: 实验表明，RegCache在文本监督和自监督视觉编码器中均能显著提升量化模型的精度。

Conclusion: RegCache为视觉编码器的量化提供了一种高效且无需训练的解决方案，适用于多种应用场景。

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [413] [Physics-Inspired All-Pair Interaction Learning for 3D Dynamics Modeling](https://arxiv.org/abs/2510.04233)
*Kai Yang,Yuqi Huang,Junheng Tao,Wanyu Wang,Qitian Wu*

Main category: cs.LG

TL;DR: PAINET是一种SE(3)-等变神经网络架构，用于学习多体系统中的全对相互作用，显著提升了3D动力学预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法依赖显式结构，无法捕捉未观测的相互作用，而这对复杂物理行为至关重要。

Method: PAINET包含物理启发的注意力网络和并行解码器，保持等变性并高效推理。

Result: 在多个真实基准测试中，PAINET性能优于现有模型，误差降低4.7%至41.5%。

Conclusion: PAINET通过捕捉未观测相互作用，显著提升了3D动力学预测的准确性。

Abstract: Modeling 3D dynamics is a fundamental problem in multi-body systems across
scientific and engineering domains and has important practical implications in
trajectory prediction and simulation. While recent GNN-based approaches have
achieved strong performance by enforcing geometric symmetries, encoding
high-order features or incorporating neural-ODE mechanics, they typically
depend on explicitly observed structures and inherently fail to capture the
unobserved interactions that are crucial to complex physical behaviors and
dynamics mechanism. In this paper, we propose PAINET, a principled
SE(3)-equivariant neural architecture for learning all-pair interactions in
multi-body systems. The model comprises: (1) a novel physics-inspired attention
network derived from the minimization trajectory of an energy function, and (2)
a parallel decoder that preserves equivariance while enabling efficient
inference. Empirical results on diverse real-world benchmarks, including human
motion capture, molecular dynamics, and large-scale protein simulations, show
that PAINET consistently outperforms recently proposed models, yielding 4.7% to
41.5% error reductions in 3D dynamics prediction with comparable computation
costs in terms of time and memory.

</details>


### [414] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出了一种新的判别器设计SONA，通过分离自然性和对齐性投影，结合自适应权重机制，显著提升了条件生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有条件生成对抗网络在平衡真实性和条件对齐性方面存在困难，需要改进。

Method: 设计了SONA判别器，包含无条件判别、匹配感知监督和自适应加权三个关键能力。

Result: 在类条件生成任务中，SONA在样本质量和条件对齐性上优于现有方法，并在文本到图像生成中验证了其有效性。

Conclusion: SONA方法在条件生成任务中表现出色，具有通用性和鲁棒性。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [415] [Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions](https://arxiv.org/abs/2510.04237)
*Jinhui Bai,Andreas Christmann,Lei Shi*

Main category: cs.LG

TL;DR: 提出了一种新颖的核随机梯度下降（SGD）算法，通过创新的正则化策略提高效率和可扩展性，适用于大规模监督学习。


<details>
  <summary>Details</summary>
Motivation: 传统核SGD在效率和可扩展性上存在不足，需要改进以适应大规模数据和通用损失函数。

Method: 利用球面径向基函数的无限级数展开，将随机梯度投影到有限维假设空间，并通过自适应缩放优化偏差-方差权衡。

Result: 证明了算法在最小化最优速率下收敛，并在再生核希尔伯特空间中实现强收敛。

Conclusion: 该算法显著降低计算复杂度，适用于多种经典损失函数，并通过数值实验验证了其高效性。

Abstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD)
algorithm for large-scale supervised learning with general losses. Compared to
traditional kernel SGD, our algorithm improves efficiency and scalability
through an innovative regularization strategy. By leveraging the infinite
series expansion of spherical radial basis functions, this strategy projects
the stochastic gradient onto a finite-dimensional hypothesis space, which is
adaptively scaled according to the bias-variance trade-off, thereby enhancing
generalization performance. Based on a new estimation of the spectral structure
of the kernel-induced covariance operator, we develop an analytical framework
that unifies optimization and generalization analyses. We prove that both the
last iterate and the suffix average converge at minimax-optimal rates, and we
further establish optimal strong convergence in the reproducing kernel Hilbert
space. Our framework accommodates a broad class of classical loss functions,
including least-squares, Huber, and logistic losses. Moreover, the proposed
algorithm significantly reduces computational complexity and achieves optimal
storage complexity by incorporating coordinate-wise updates from linear SGD,
thereby avoiding the costly pairwise operations typical of kernel SGD and
enabling efficient processing of streaming data. Finally, extensive numerical
experiments demonstrate the efficiency of our approach.

</details>


### [416] [Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs](https://arxiv.org/abs/2510.04241)
*Seong Jin Ahn,Myoung-Ho Kim*

Main category: cs.LG

TL;DR: 论文提出了一种名为DAD-SGM的新方法，通过扩散模型辅助蒸馏，将自监督图神经网络（GNN）的知识迁移到轻量级多层感知机（MLP）中。


<details>
  <summary>Details</summary>
Motivation: 自监督图表示学习中，GNN和MLP之间存在巨大的能力差距，传统蒸馏方法难以解决。

Method: 使用去噪扩散模型作为教师助手，辅助GNN向MLP的知识蒸馏。

Result: 实验表明，DAD-SGM在自监督图表示学习中优于现有蒸馏方法。

Conclusion: DAD-SGM有效提升了MLP在自监督图表示学习中的泛化性和鲁棒性。

Abstract: For large-scale applications, there is growing interest in replacing Graph
Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via
knowledge distillation. However, distilling GNNs for self-supervised graph
representation learning into MLPs is more challenging. This is because the
performance of self-supervised learning is more related to the model's
inductive bias than supervised learning. This motivates us to design a new
distillation method to bridge a huge capacity gap between GNNs and MLPs in
self-supervised graph representation learning. In this paper, we propose
\textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for
\textbf{S}elf-supervised \textbf{G}raph representation learning with
\textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion
model as a teacher assistant to better distill the knowledge from the teacher
GNN into the student MLP. This approach enhances the generalizability and
robustness of MLPs in self-supervised graph representation learning. Extensive
experiments demonstrate that DAD-SGM effectively distills the knowledge of
self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation
methods. Our implementation is available at
https://github.com/SeongJinAhn/DAD-SGM.

</details>


### [417] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: SSD揭示了结构化状态空间模型（SSM）与掩码注意力机制之间的等价性，扩展了序列模型的设计空间。


<details>
  <summary>Details</summary>
Motivation: 研究SSM与注意力机制之间的等价性，以统一和丰富序列模型的实现方式。

Method: 通过数学形式化和推广SSD，从标量-恒等SSM扩展到一般对角SSM，并分析其与1-半可分掩码注意力的等价条件。

Result: 证明对角SSM在保持训练复杂度的同时支持更丰富的动态行为，并揭示了标准softmax注意力无法扩展此等价性的原因。

Conclusion: SSD强化了递归SSM与Transformer之间的联系，为高效且表达能力强的序列模型提供了更广阔的设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


### [418] [Efficient Latent Variable Causal Discovery: Combining Score Search and Targeted Testing](https://arxiv.org/abs/2510.04263)
*Joseph Ramsey,Bryan Andrews*

Main category: cs.LG

TL;DR: 论文提出了一系列基于分数引导和针对性测试的因果搜索算法，改进了FCI算法在潜在变量和选择偏差下的性能。


<details>
  <summary>Details</summary>
Motivation: FCI算法在处理潜在变量或选择偏差时存在性能问题，如虚假独立性声明和不可靠的边方向。

Method: 提出了BOSS-FCI、GRaSP-FCI、FCIT和LV-Dumb等方法，结合分数引导和针对性测试。

Result: 新方法在模拟和真实数据中表现出更高的精度和效率。

Conclusion: 分数引导和针对性策略对可扩展的潜在变量因果发现具有重要价值。

Abstract: Learning causal structure from observational data is especially challenging
when latent variables or selection bias are present. The Fast Causal Inference
(FCI) algorithm addresses this setting but often performs exhaustive
conditional independence tests across many subsets, leading to spurious
independence claims, extra or missing edges, and unreliable orientations. We
present a family of score-guided mixed-strategy causal search algorithms that
build on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,
straightforward variants of GFCI that substitute BOSS or GRaSP for FGES,
thereby retaining correctness while incurring different scalability tradeoffs.
Second, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method
that improves upon these variants by replacing exhaustive all-subsets testing
with targeted tests guided by BOSS, yielding well-formed PAGs with higher
precision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also
known as BOSS-POD), which bypasses latent-variable-specific reasoning and
directly returns the PAG of the BOSS DAG. Although not strictly correct in the
FCI sense, it scales better and often achieves superior accuracy in practice.
Simulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI
provide sound baselines, FCIT improves both efficiency and reliability, and
LV-Dumb offers a practical heuristic with strong empirical performance.
Together, these method highlight the value of score-guided and targeted
strategies for scalable latent-variable causal discovery.

</details>


### [419] [Influence branching for learning to solve mixed-integer programs online](https://arxiv.org/abs/2510.04273)
*Paul Strang,Zacharie Alès,Côme Bissuel,Olivier Juan,Safia Kedad-Sidhoum,Emmanuel Rachelson*

Main category: cs.LG

TL;DR: 提出了一种基于图导向变量选择策略（Influence branching）和Thompson采样的在线学习新方法，用于解决混合整数规划（MIP）问题，性能与现有最优方法相当且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 针对混合整数规划（MIP）在线求解的需求，提出一种新的变量选择策略和在线学习方法，以提高计算效率。

Method: 结合Influence branching（图导向变量选择策略）和Thompson采样，在线优化分支定界算法的初始迭代。

Result: 性能与现有最优在线学习方法相当，且在更广泛的在线框架中表现出良好的泛化能力。

Conclusion: 该方法不仅高效，还能适应多种变化（如约束矩阵、向量和目标系数），适用于更多样本的场景。

Abstract: On the occasion of the 20th Mixed Integer Program Workshop's computational
competition, this work introduces a new approach for learning to solve MIPs
online. Influence branching, a new graph-oriented variable selection strategy,
is applied throughout the first iterations of the branch and bound algorithm.
This branching heuristic is optimized online with Thompson sampling, which
ranks the best graph representations of MIP's structure according to
computational speed up over SCIP. We achieve results comparable to state of the
art online learning methods. Moreover, our results indicate that our method
generalizes well to more general online frameworks, where variations in
constraint matrix, constraint vector and objective coefficients can all occur
and where more samples are available.

</details>


### [420] [HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks](https://arxiv.org/abs/2510.04295)
*Nghiem T. Diep,Dung Le,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: HoRA是一种改进的LoRA方法，通过共享生成器实现多头注意力之间的协同，提高样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在多头自注意力（MHA）中独立适应每个注意力头，忽略了头间的潜在协同效应。

Method: 提出HoRA方法，利用联合超网络生成跨注意力头的低秩矩阵，通过共享生成器促进头间信息共享。

Result: 理论分析表明HoRA比LoRA具有更高的样本效率；实验证明HoRA在多种任务中优于LoRA和其他PEFT方法。

Conclusion: HoRA通过跨头信息共享有效解决了LoRA的局限性，性能更优且参数增量小。

Abstract: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT)
technique that adapts large pre-trained models by adding low-rank matrices to
their weight updates. However, in the context of fine-tuning multi-head
self-attention (MHA), LoRA has been employed to adapt each attention head
separately, thereby overlooking potential synergies across different heads. To
mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA)
method, which utilizes joint hypernetworks to generate low-rank matrices across
attention heads. By coupling their adaptation through a shared generator, HoRA
encourages cross-head information sharing, and thus directly addresses the
aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens
of hierarchical mixture of experts, our theoretical findings reveal that the
latter achieves superior sample efficiency to the former. Furthermore, through
extensive experiments across diverse language and vision benchmarks, we
demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring
only a marginal increase in the number of trainable parameters.

</details>


### [421] [Wave-PDE Nets: Trainable Wave-Equation Layers as an Alternative to Attention](https://arxiv.org/abs/2510.04304)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Wave-PDE Nets是一种基于二阶波动方程模拟的神经网络架构，通过可训练的空间速度和阻尼参数传播隐藏状态，性能优于Transformer。


<details>
  <summary>Details</summary>
Motivation: 提出一种替代注意力机制和一阶状态空间模型的全局振荡机制，以提升计算效率和性能。

Method: 使用基于FFT的辛谱求解器实现O(nlog n)时间复杂度的传播，每层通过可训练参数c(x)和γ(x)传播隐藏状态。

Result: 在语言和视觉任务中，Wave-PDE Nets性能匹配或超越Transformer，计算时间减少30%，内存峰值降低25%。

Conclusion: Wave-PDE Nets是一种高效、稳健且具有物理归纳偏置的架构，适合信息传播任务。

Abstract: We introduce Wave-PDE Nets, a neural architecture whose elementary operation
is a differentiable simulation of the second-order wave equation. Each layer
propagates its hidden state as a continuous field through a medium with
trainable spatial velocity c(x) and damping {\gamma}(x). A symplectic spectral
solver based on FFTs realises this propagation in O(nlog n) time. This
oscillatory, global mechanism provides a powerful alternative to attention and
first-order state-space models. We prove that a single Wave-PDE layer is a
universal approximator. On language and vision benchmarks, Wave-PDE Nets match
or exceed Transformer performance while demonstrating superior practical
efficiency, reducing wall-clock time by up to 30% and peak memory by 25%.
Ablation studies confirm the critical role of symplectic integration and a
spectral Laplacian for stability and performance. Visualizations of the learned
physical parameters reveal that the model learns intuitive strategies for
information propagation. These results position Wave-PDE Nets as a
computationally efficient and robust architecture with a strong physical
inductive bias.

</details>


### [422] [Activation Steering with a Feedback Controller](https://arxiv.org/abs/2510.04309)
*Dung V. Nguyen,Hieu M. Vu,Nhi Y. Pham,Lei Zhang,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 论文提出了一种基于控制理论的PID Steering方法，用于改进大型语言模型（LLM）的行为控制，相比现有方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型行为控制方法缺乏理论性能保证，主要依赖经验性设计。

Method: 提出PID Steering框架，利用比例（P）、积分（I）和微分（D）控制器对激活进行调节，实现更稳定的行为控制。

Result: 实验表明，PID Steering在多个LLM和基准测试中表现优于现有方法，提供更鲁棒和可靠的行为控制。

Conclusion: PID Steering为LLM行为控制提供了理论支持，并通过实验验证了其有效性。

Abstract: Controlling the behaviors of large language models (LLM) is fundamental to
their safety alignment and reliable deployment. However, existing steering
methods are primarily driven by empirical insights and lack theoretical
performance guarantees. In this work, we develop a control-theoretic foundation
for activation steering by showing that popular steering methods correspond to
the proportional (P) controllers, with the steering vector serving as the
feedback signal. Building on this finding, we propose
Proportional-Integral-Derivative (PID) Steering, a principled framework that
leverages the full PID controller for activation steering in LLMs. The
proportional (P) term aligns activations with target semantic directions, the
integral (I) term accumulates errors to enforce persistent corrections across
layers, and the derivative (D) term mitigates overshoot by counteracting rapid
activation changes. This closed-loop design yields interpretable error dynamics
and connects activation steering to classical stability guarantees in control
theory. Moreover, PID Steering is lightweight, modular, and readily integrates
with state-of-the-art steering methods. Extensive experiments across multiple
LLM families and benchmarks demonstrate that PID Steering consistently
outperforms existing approaches, achieving more robust and reliable behavioral
control.

</details>


### [423] [Crash Severity Prediction Using Deep Learning Approaches: A Hybrid CNN-RNN Framework](https://arxiv.org/abs/2510.04316)
*Sahar Koohfar*

Main category: cs.LG

TL;DR: 论文提出了一种混合CNN-RNN深度学习模型用于预测交通事故严重程度，并在性能上优于传统统计和机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 准确及时地预测交通事故严重程度对于减轻事故后果至关重要，尤其是在提供医疗援助和交通服务方面。

Method: 研究采用混合CNN-RNN模型，并与逻辑回归、朴素贝叶斯、KNN、决策树以及单独的RNN和CNN模型进行比较。

Result: 混合CNN-RNN模型在预测交通事故严重程度方面表现优于所有基准模型。

Conclusion: 混合模型结合了RNN和CNN的优势，显著提高了预测准确性。

Abstract: Accurate and timely prediction of crash severity is crucial in mitigating the
severe consequences of traffic accidents. Accurate and timely prediction of
crash severity is crucial in mitigating the severe consequences of traffic
accidents. In order to provide appropriate levels of medical assistance and
transportation services, an intelligent transportation system relies on
effective prediction methods. Deep learning models have gained popularity in
this domain due to their capability to capture non-linear relationships among
variables. In this research, we have implemented a hybrid CNN-RNN deep learning
model for crash severity prediction and compared its performance against widely
used statistical and machine learning models such as logistic regression,
na\"ive bayes classifier, K-Nearest Neighbors (KNN), decision tree, and
individual deep learning models: RNN and CNN. This study employs a methodology
that considers the interconnected relationships between various features of
traffic accidents. The study was conducted using a dataset of 15,870 accident
records gathered over a period of seven years between 2015 and 2021 on Virginia
highway I-64. The findings demonstrate that the proposed CNN-RNN hybrid model
has outperformed all benchmark models in terms of predicting crash severity.
This result illustrates the effectiveness of the hybrid model as it combines
the advantages of both RNN and CNN models in order to achieve greater accuracy
in the prediction process.

</details>


### [424] [FairAgent: Democratizing Fairness-Aware Machine Learning with LLM-Powered Agents](https://arxiv.org/abs/2510.04317)
*Yucong Dai,Lu Zhang,Feng Luo,Mashrur Chowdhury,Yongkai Wu*

Main category: cs.LG

TL;DR: FairAgent是一个基于LLM的自动化系统，简化了公平感知模型开发，无需深厚技术知识即可实现偏见缓解。


<details>
  <summary>Details</summary>
Motivation: 高风险的机器学习应用需要公平无偏的模型，但现有方法对技术要求高，难以普及。

Method: FairAgent自动分析数据集偏见，处理数据预处理和特征工程，并根据用户需求实施偏见缓解策略。

Result: 实验表明，FairAgent显著提升性能，减少开发时间和专业知识需求。

Conclusion: FairAgent使公平感知机器学习更易于实践者使用。

Abstract: Training fair and unbiased machine learning models is crucial for high-stakes
applications, yet it presents significant challenges. Effective bias mitigation
requires deep expertise in fairness definitions, metrics, data preprocessing,
and machine learning techniques. In addition, the complex process of balancing
model performance with fairness requirements while properly handling sensitive
attributes makes fairness-aware model development inaccessible to many
practitioners. To address these challenges, we introduce FairAgent, an
LLM-powered automated system that significantly simplifies fairness-aware model
development. FairAgent eliminates the need for deep technical expertise by
automatically analyzing datasets for potential biases, handling data
preprocessing and feature engineering, and implementing appropriate bias
mitigation strategies based on user requirements. Our experiments demonstrate
that FairAgent achieves significant performance improvements while
significantly reducing development time and expertise requirements, making
fairness-aware machine learning more accessible to practitioners.

</details>


### [425] [FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields](https://arxiv.org/abs/2510.04325)
*Kenechukwu Ogbuagu,Sepehr Maleki,Giuseppe Bruni,Senthil Krishnababu*

Main category: cs.LG

TL;DR: FoilDiff是一种基于扩散模型的替代模型，通过混合骨干去噪网络提高流场预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CFD模型计算成本高，需要开发更高效的替代模型。扩散模型在复杂流场预测中表现出潜力。

Method: 结合卷积特征提取和Transformer全局注意力的混合设计，利用DDIM采样优化效率。

Result: 在相同数据集上，平均预测误差降低85%，预测不确定性的校准更好。

Conclusion: FoilDiff在准确性和效率上优于现有扩散模型，适用于广泛的空气动力学条件。

Abstract: The accurate prediction of flow fields around airfoils is crucial for
aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models
are effective but computationally expensive, thus inspiring the development of
surrogate models to enable quicker predictions. These surrogate models can be
based on deep learning architectures, such as Convolutional Neural Networks
(CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion
models have shown significant promise in predicting complex flow fields. In
this work, we propose FoilDiff, a diffusion-based surrogate model with a
hybrid-backbone denoising network. This hybrid design combines the power of
convolutional feature extraction and transformer-based global attention to
generate more adaptable and accurate representations of flow structures.
FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling
to optimise the efficiency of the sampling process at no additional cost to
model generalisation. We used encoded representations of Reynolds number, angle
of attack, and airfoil geometry to define the input space for generalisation
across a wide range of aerodynamic conditions. When evaluated against
state-of-the-art models, FoilDiff shows significant performance improvements,
with mean prediction errors reducing by up to 85\% on the same datasets. The
results have demonstrated that FoilDiff can provide both more accurate
predictions and better-calibrated predictive uncertainty than existing
diffusion-based models.

</details>


### [426] [Arithmetic-Mean $μ$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets](https://arxiv.org/abs/2510.04327)
*Haosong Zhang,Shenxi Wu,Yichi Zhang,Wei Lin*

Main category: cs.LG

TL;DR: AM-μP 提出了一种新的学习率调整方法，通过约束网络范围内的平均预激活二阶矩，解决了异构架构中的学习率不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统μP方法在异构架构（如残差网络和卷积网络）中表现不佳，导致学习率不平衡。AM-μP旨在解决这一问题。

Method: AM-μP结合了算术平均约束和残差感知的He初始化，确保学习率在不同深度和宽度下保持一致。

Result: 理论和实验验证了学习率随深度变化的-3/2幂律关系，实现了零样本学习率迁移。

Conclusion: AM-μP为卷积和深度残差网络提供了统一且实用的学习率调整原则，无需额外调参。

Abstract: Choosing an appropriate learning rate remains a key challenge in scaling
depth of modern deep networks. The classical maximal update parameterization
($\mu$P) enforces a fixed per-layer update magnitude, which is well suited to
homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in
heterogeneous architectures where residual accumulation and convolutions
introduce imbalance across layers. We introduce Arithmetic-Mean $\mu$P
(AM-$\mu$P), which constrains not each individual layer but the network-wide
average one-step pre-activation second moment to a constant scale. Combined
with a residual-aware He fan-in initialization - scaling residual-branch
weights by the number of blocks ($\mathrm{Var}[W]=c/(K\cdot
\mathrm{fan\text{-}in})$) - AM-$\mu$P yields width-robust depth laws that
transfer consistently across depths. We prove that, for one- and
two-dimensional convolutional networks, the maximal-update learning rate
satisfies $\eta^\star(L)\propto L^{-3/2}$; with zero padding, boundary effects
are constant-level as $N\gg k$. For standard residual networks with general
conv+MLP blocks, we establish $\eta^\star(L)=\Theta(L^{-3/2})$, with $L$ the
minimal depth. Empirical results across a range of depths confirm the $-3/2$
scaling law and enable zero-shot learning-rate transfer, providing a unified
and practical LR principle for convolutional and deep residual networks without
additional tuning overhead.

</details>


### [427] [Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies](https://arxiv.org/abs/2510.04341)
*G. Niklas Noren,Eva-Lisa Meldau,Johan Ellenius*

Main category: cs.LG

TL;DR: 论文探讨了在低发生率事件中AI模型的应用挑战，提出了关键评估方法和结构化案例检查（SCLE），并提供了开发或采购AI模型的清单。


<details>
  <summary>Details</summary>
Motivation: 解决AI在罕见事件识别中的局限性，如虚假准确性和实际价值不足。

Method: 提出问题框架、测试集设计、统计评估、鲁棒性评估和SCLE方法，并结合药物警戒案例验证。

Result: 展示了如何通过成本敏感目标使模型性能与实际价值对齐，并总结了罕见事件场景的常见陷阱。

Conclusion: 论文提出的原则适用于正例稀缺且错误成本不对称的领域，具有广泛适用性。

Abstract: Many high-stakes AI applications target low-prevalence events, where apparent
accuracy can conceal limited real-world value. Relevant AI models range from
expert-defined rules and traditional machine learning to generative LLMs
constrained for classification. We outline key considerations for critical
appraisal of AI in rare-event recognition, including problem framing and test
set design, prevalence-aware statistical evaluation, robustness assessment, and
integration into human workflows. In addition, we propose an approach to
structured case-level examination (SCLE), to complement statistical performance
evaluation, and a comprehensive checklist to guide procurement or development
of AI models for rare-event recognition. We instantiate the framework in
pharmacovigilance, drawing on three studies: rule-based retrieval of
pregnancy-related reports; duplicate detection combining machine learning with
probabilistic record linkage; and automated redaction of person names using an
LLM. We highlight pitfalls specific to the rare-event setting including
optimism from unrealistic class balance and lack of difficult positive controls
in test sets - and show how cost-sensitive targets align model performance with
operational value. While grounded in pharmacovigilance practice, the principles
generalize to domains where positives are scarce and error costs may be
asymmetric.

</details>


### [428] [Learning to Predict Chaos: Curriculum-Driven Training for Robust Forecasting of Chaotic Dynamics](https://arxiv.org/abs/2510.04342)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: CCF通过从简单到复杂的混沌系统训练序列模型，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在混沌系统预测中泛化性不足或过度混合数据的问题。

Method: 使用Lyapunov指数和吸引子维度构建训练课程，逐步引入更复杂的混沌系统。

Result: 在真实世界数据上，预测时间延长40%，性能优于随机训练和仅用真实数据训练。

Conclusion: CCF训练范式有效提升了模型的泛化能力和预测性能。

Abstract: Forecasting chaotic systems is a cornerstone challenge in many scientific
fields, complicated by the exponential amplification of even infinitesimal
prediction errors. Modern machine learning approaches often falter due to two
opposing pitfalls: over-specializing on a single, well-known chaotic system
(e.g., Lorenz-63), which limits generalizability, or indiscriminately mixing
vast, unrelated time-series, which prevents the model from learning the nuances
of any specific dynamical regime. We propose Curriculum Chaos Forecasting
(CCF), a training paradigm that bridges this gap. CCF organizes training data
based on fundamental principles of dynamical systems theory, creating a
curriculum that progresses from simple, periodic behaviors to highly complex,
chaotic dynamics. We quantify complexity using the largest Lyapunov exponent
and attractor dimension, two well-established metrics of chaos. By first
training a sequence model on predictable systems and gradually introducing more
chaotic trajectories, CCF enables the model to build a robust and generalizable
representation of dynamical behaviors. We curate a library of over 50 synthetic
ODE/PDE systems to build this curriculum. Our experiments show that
pre-training with CCF significantly enhances performance on unseen, real-world
benchmarks. On datasets including Sunspot numbers, electricity demand, and
human ECG signals, CCF extends the valid prediction horizon by up to 40%
compared to random-order training and more than doubles it compared to training
on real-world data alone. We demonstrate that this benefit is consistent across
various neural architectures (GRU, Transformer) and provide extensive ablations
to validate the importance of the curriculum's structure.

</details>


### [429] [From News to Returns: A Granger-Causal Hypergraph Transformer on the Sphere](https://arxiv.org/abs/2510.04357)
*Anoushka Harit,Zhongtian Sun,Jongmin Yu*

Main category: cs.LG

TL;DR: CSHT是一种结合Granger因果超图结构、黎曼几何和因果掩码Transformer的新型金融时间序列预测架构，表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 解决金融时间序列预测中的可解释性和鲁棒性问题，尤其是在市场不确定性下。

Method: 利用Granger因果依赖构建方向性超边，结合黎曼几何和因果掩码Transformer注意力机制。

Result: 在S&P 500数据上表现优异，尤其在2020年COVID-19冲击期间。

Conclusion: CSHT是一种可靠且实用的金融预测解决方案，具有透明性和鲁棒性。

Abstract: We propose the Causal Sphere Hypergraph Transformer (CSHT), a novel
architecture for interpretable financial time-series forecasting that unifies
\emph{Granger-causal hypergraph structure}, \emph{Riemannian geometry}, and
\emph{causally masked Transformer attention}. CSHT models the directional
influence of financial news and sentiment on asset returns by extracting
multivariate Granger-causal dependencies, which are encoded as directional
hyperedges on the surface of a hypersphere. Attention is constrained via
angular masks that preserve both temporal directionality and geometric
consistency. Evaluated on S\&P 500 data from 2018 to 2023, including the 2020
COVID-19 shock, CSHT consistently outperforms baselines across return
prediction, regime classification, and top-asset ranking tasks. By enforcing
predictive causal structure and embedding variables in a Riemannian manifold,
CSHT delivers both \emph{robust generalisation across market regimes} and
\emph{transparent attribution pathways} from macroeconomic events to
stock-level responses. These results suggest that CSHT is a principled and
practical solution for trustworthy financial forecasting under uncertainty.

</details>


### [430] [Quantifying Ambiguity in Categorical Annotations: A Measure and Statistical Inference Framework](https://arxiv.org/abs/2510.04366)
*Christopher Klugmann,Daniel Kondermann*

Main category: cs.LG

TL;DR: 提出了一种量化分类任务中随机不确定性的标量度量，区分类别模糊性和明确不可解性，并提供了统计推断工具。


<details>
  <summary>Details</summary>
Motivation: 人类标注的分类注释常反映模糊性而非错误，需量化这种随机不确定性。

Method: 引入基于二次熵的度量，处理“无法解决”类别不对称性，并提供频率主义和贝叶斯推断工具。

Result: 度量能有效区分不同不确定性来源，统计工具支持数据集质量评估和机器学习应用。

Conclusion: 新度量及统计工具为分类任务中的不确定性量化提供了实用方法。

Abstract: Human-generated categorical annotations frequently produce empirical response
distributions (soft labels) that reflect ambiguity rather than simple annotator
error. We introduce an ambiguity measure that maps a discrete response
distribution to a scalar in the unit interval, designed to quantify aleatoric
uncertainty in categorical tasks. The measure bears a close relationship to
quadratic entropy (Gini-style impurity) but departs from those indices by
treating an explicit "can't solve" category asymmetrically, thereby separating
uncertainty arising from class-level indistinguishability from uncertainty due
to explicit unresolvability. We analyze the measure's formal properties and
contrast its behavior with a representative ambiguity measure from the
literature. Moving beyond description, we develop statistical tools for
inference: we propose frequentist point estimators for population ambiguity and
derive the Bayesian posterior over ambiguity induced by Dirichlet priors on the
underlying probability vector, providing a principled account of epistemic
uncertainty. Numerical examples illustrate estimation, calibration, and
practical use for dataset-quality assessment and downstream machine-learning
workflows.

</details>


### [431] [GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks](https://arxiv.org/abs/2510.04374)
*Tejal Patwardhan,Rachel Dias,Elizabeth Proehl,Grace Kim,Michele Wang,Olivia Watkins,Simón Posada Fishman,Marwan Aljubeh,Phoebe Thacker,Laurance Fauconnet,Natalie S. Kim,Patrick Chao,Samuel Miserendino,Gildas Chabot,David Li,Michael Sharman,Alexandra Barr,Amelia Glaese,Jerry Tworek*

Main category: cs.LG

TL;DR: GDPval是一个评估AI模型在现实经济任务中表现的基准，覆盖美国GDP前9大行业的44种职业。前沿模型表现随时间线性提升，接近专家水平。


<details>
  <summary>Details</summary>
Motivation: 评估AI模型在真实经济任务中的能力，推动对模型实际应用潜力的研究。

Method: 构建来自行业专业人士的任务，分析模型表现随时间的变化，探讨人机协作的潜力。

Result: 前沿模型表现接近专家，人机协作可更高效完成任务。增加推理、任务上下文和支架能提升模型表现。

Conclusion: GDPval为研究模型实际能力提供基准，开源任务和评分服务促进未来研究。

Abstract: We introduce GDPval, a benchmark evaluating AI model capabilities on
real-world economically valuable tasks. GDPval covers the majority of U.S.
Bureau of Labor Statistics Work Activities for 44 occupations across the top 9
sectors contributing to U.S. GDP (Gross Domestic Product). Tasks are
constructed from the representative work of industry professionals with an
average of 14 years of experience. We find that frontier model performance on
GDPval is improving roughly linearly over time, and that the current best
frontier models are approaching industry experts in deliverable quality. We
analyze the potential for frontier models, when paired with human oversight, to
perform GDPval tasks cheaper and faster than unaided experts. We also
demonstrate that increased reasoning effort, increased task context, and
increased scaffolding improves model performance on GDPval. Finally, we
open-source a gold subset of 220 tasks and provide a public automated grading
service at evals.openai.com to facilitate future research in understanding
real-world model capabilities.

</details>


### [432] [Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains](https://arxiv.org/abs/2510.04375)
*Akshay Mittal,Vinay Venkatesh,Krishna Kandi,Shalini Sudarshan*

Main category: cs.LG

TL;DR: 提出了一种动态加权损失函数，通过自适应调整稀疏领域的权重，显著提升了稀疏领域的推荐性能。


<details>
  <summary>Details</summary>
Motivation: 针对稀疏或小众领域的推荐系统性能受限问题，传统固定加权损失方法无法有效处理稀疏数据。

Method: 提出动态加权损失函数，根据训练数据稀疏性自适应调整权重，稀疏领域权重更高。

Result: 在四个数据集上验证，显著提升稀疏领域的Recall@10和NDCG@10指标，同时保持密集领域性能。

Conclusion: 动态加权损失方法在稀疏领域表现优异，计算开销低，具有理论和实证支持。

Abstract: The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.

</details>


### [433] [Categorical Invariants of Learning Dynamics](https://arxiv.org/abs/2510.04376)
*Abdulrahman Tamim*

Main category: cs.LG

TL;DR: 论文提出了一种新的视角，将神经网络训练视为参数空间到表示空间的结构保持变换（函子L），揭示了同伦类在训练路径中的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统观点将训练视为损失表面的梯度下降，但作者认为这种视角不足以解释不同训练路径的相似性和泛化能力。

Method: 采用范畴论框架，将训练路径视为同伦类，并通过实验验证同伦路径的泛化性能相似性。

Result: 实验表明，同伦路径的泛化性能差异在0.5%以内，而非同伦路径差异超过3%。持久同调能预测泛化性能（R²=0.82）。

Conclusion: 范畴论框架为深度学习提供了理论解释和实用工具，如识别稳定极小值和形式化迁移学习。

Abstract: Neural network training is typically viewed as gradient descent on a loss
surface. We propose a fundamentally different perspective: learning is a
structure-preserving transformation (a functor L) between the space of network
parameters (Param) and the space of learned representations (Rep). This
categorical framework reveals that different training runs producing similar
test performance often belong to the same homotopy class (continuous
deformation family) of optimization paths. We show experimentally that networks
converging via homotopic trajectories generalize within 0.5% accuracy of each
other, while non-homotopic paths differ by over 3%. The theory provides
practical tools: persistent homology identifies stable minima predictive of
generalization (R^2 = 0.82 correlation), pullback constructions formalize
transfer learning, and 2-categorical structures explain when different
optimization algorithms yield functionally equivalent models. These categorical
invariants offer both theoretical insight into why deep learning works and
concrete algorithmic principles for training more robust networks.

</details>


### [434] [Score-based Greedy Search for Structure Identification of Partially Observed Linear Causal Models](https://arxiv.org/abs/2510.04378)
*Xinshuai Dong,Ignavier Ng,Haoyue Dai,Jiaqi Sun,Xiangchen Song,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于分数的贪婪搜索方法（LGES），用于识别包含潜在变量的因果结构，并证明了其全局一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于约束的因果发现方法存在多重检验和误差传播问题，而基于分数的方法可能解决这些问题。

Method: 提出了广义N因子模型，并设计了贪婪搜索算法LGES，用于高效搜索最优结构。

Result: 实验验证了该方法在合成和真实数据上的有效性。

Conclusion: LGES是首个具有可识别性保证的基于分数的贪婪搜索方法，适用于部分观测场景。

Abstract: Identifying the structure of a partially observed causal system is essential
to various scientific fields. Recent advances have focused on constraint-based
causal discovery to solve this problem, and yet in practice these methods often
face challenges related to multiple testing and error propagation. These issues
could be mitigated by a score-based method and thus it has raised great
attention whether there exists a score-based greedy search method that can
handle the partially observed scenario. In this work, we propose the first
score-based greedy search method for the identification of structure involving
latent variables with identifiability guarantees. Specifically, we propose
Generalized N Factor Model and establish the global consistency:
  the true structure including latent variables can be identified up to the
Markov equivalence class by using score. We then design
  Latent variable Greedy Equivalence Search (LGES), a greedy search algorithm
for this class of model with well-defined operators,
  which search very efficiently over the graph space to find the optimal
structure. Our experiments on both synthetic and real-life data validate the
effectiveness of our method (code will be publicly available).

</details>


### [435] [SSM-CGM: Interpretable State-Space Forecasting Model of Continuous Glucose Monitoring for Personalized Diabetes Management](https://arxiv.org/abs/2510.04386)
*Shakson Isaac,Yentl Collin,Chirag Patel*

Main category: cs.LG

TL;DR: SSM-CGM是一种基于Mamba的神经状态空间预测模型，结合CGM和可穿戴设备数据，提高了短期血糖预测准确性，并增加了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有血糖预测模型缺乏临床可解释性，限制了其在糖尿病管理中的应用。

Method: 提出SSM-CGM模型，整合CGM和可穿戴设备信号，通过变量选择和时间归因提高可解释性。

Result: SSM-CGM在短期预测上优于Temporal Fusion Transformer基线模型，并支持反事实预测。

Conclusion: SSM-CGM为个性化糖尿病管理提供了一个可解释且生理学基础强的框架。

Abstract: Continuous glucose monitoring (CGM) generates dense data streams critical for
diabetes management, but most used forecasting models lack interpretability for
clinical use. We present SSM-CGM, a Mamba-based neural state-space forecasting
model that integrates CGM and wearable activity signals from the AI-READI
cohort. SSM-CGM improves short-term accuracy over a Temporal Fusion Transformer
baseline, adds interpretability through variable selection and temporal
attribution, and enables counterfactual forecasts simulating how planned
changes in physiological signals (e.g., heart rate, respiration) affect
near-term glucose. Together, these features make SSM-CGM an interpretable,
physiologically grounded framework for personalized diabetes management.

</details>


### [436] [Achieve Performatively Optimal Policy for Performative Reinforcement Learning](https://arxiv.org/abs/2510.04430)
*Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 本文提出了一种零阶Frank-Wolfe算法（0-FW），首次在多项式时间内收敛到期望的performatively optimal（PO）策略，填补了现有performative强化学习方法的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有performative强化学习方法仅追求performatively stable（PS）策略，与理想的PO策略存在性能差距。本文旨在解决这一问题。

Method: 采用零阶Frank-Wolfe框架，结合performative策略梯度的零阶近似，确保在标准正则化条件下收敛到PO策略。

Result: 理论证明0-FW算法在多项式时间内收敛到PO策略，实验验证其优于现有方法。

Conclusion: 0-FW算法是首个高效收敛到PO策略的方法，为performative强化学习提供了新工具。

Abstract: Performative reinforcement learning is an emerging dynamical decision making
framework, which extends reinforcement learning to the common applications
where the agent's policy can change the environmental dynamics. Existing works
on performative reinforcement learning only aim at a performatively stable (PS)
policy that maximizes an approximate value function. However, there is a
provably positive constant gap between the PS policy and the desired
performatively optimal (PO) policy that maximizes the original value function.
In contrast, this work proposes a zeroth-order Frank-Wolfe algorithm (0-FW)
algorithm with a zeroth-order approximation of the performative policy gradient
in the Frank-Wolfe framework, and obtains \textbf{the first polynomial-time
convergence to the desired PO} policy under the standard regularizer dominance
condition. For the convergence analysis, we prove two important properties of
the nonconvex value function. First, when the policy regularizer dominates the
environmental shift, the value function satisfies a certain gradient dominance
property, so that any stationary point (not PS) of the value function is a
desired PO. Second, though the value function has unbounded gradient, we prove
that all the sufficiently stationary points lie in a convex and compact policy
subspace $\Pi_{\Delta}$, where the policy value has a constant lower bound
$\Delta>0$ and thus the gradient becomes bounded and Lipschitz continuous.
Experimental results also demonstrate that our 0-FW algorithm is more effective
than the existing algorithms in finding the desired PO policy.

</details>


### [437] [Trade-off in Estimating the Number of Byzantine Clients in Federated Learning](https://arxiv.org/abs/2510.04432)
*Ziyi Chen,Su Zhang,Heng Huang*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中鲁棒聚合器对拜占庭客户端数量的估计对性能的影响，发现低估会导致性能急剧下降，而非低估时存在性能与鲁棒性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习易受拜占庭客户端攻击，鲁棒聚合器需要估计拜占庭客户端数量以选择合适的鲁棒性程度，但这一估计对性能的影响尚未系统研究。

Method: 通过理论分析，研究了鲁棒聚合器在不同估计情况下的最坏误差及其对联邦学习算法的影响。

Result: 低估拜占庭客户端数量会导致性能急剧下降；非低估时，性能与鲁棒性之间存在权衡，性能随鲁棒性程度增加而下降。

Conclusion: 选择合适的鲁棒性程度需权衡性能与鲁棒性，避免低估拜占庭客户端数量。

Abstract: Federated learning has attracted increasing attention at recent large-scale
optimization and machine learning research and applications, but is also
vulnerable to Byzantine clients that can send any erroneous signals. Robust
aggregators are commonly used to resist Byzantine clients. This usually
requires to estimate the unknown number $f$ of Byzantine clients, and thus
accordingly select the aggregators with proper degree of robustness (i.e., the
maximum number $\hat{f}$ of Byzantine clients allowed by the aggregator). Such
an estimation should have important effect on the performance, which has not
been systematically studied to our knowledge. This work will fill in the gap by
theoretically analyzing the worst-case error of aggregators as well as its
induced federated learning algorithm for any cases of $\hat{f}$ and $f$.
Specifically, we will show that underestimation ($\hat{f}<f$) can lead to
arbitrarily poor performance for both aggregators and federated learning. For
non-underestimation ($\hat{f}\ge f$), we have proved optimal lower and upper
bounds of the same order on the errors of both aggregators and federated
learning. All these optimal bounds are proportional to $\hat{f}/(n-f-\hat{f})$
with $n$ clients, which monotonically increases with larger $\hat{f}$. This
indicates a fundamental trade-off: while an aggregator with a larger robustness
degree $\hat{f}$ can solve federated learning problems of wider range $f\in
[0,\hat{f}]$, the performance can deteriorate when there are actually fewer or
even no Byzantine clients (i.e., $f\in [0,\hat{f})$).

</details>


### [438] [Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size](https://arxiv.org/abs/2510.04440)
*Farid Bozorgnia,Vyacheslav Kungurtsev,Shirali Kadyrov,Mohsen Yousefnezhad*

Main category: cs.LG

TL;DR: 提出基于分数热核动力学的标签传播与自训练算法，增强图神经网络的表达能力。


<details>
  <summary>Details</summary>
Motivation: 通过信息论与抛物型演化方程的经典对应关系，探索非局部相互作用对标签传播的影响。

Method: 将分数热核集成到图卷积网络和图注意力网络中，利用切比雪夫多项式近似处理大规模图。

Result: 在少量标注数据情况下，该方法表现出色。

Conclusion: 分数拉普拉斯扩展的扩散模型在标签传播中具有优势，尤其在标注数据稀缺时。

Abstract: In this work, we introduce novel algorithms for label propagation and
self-training using fractional heat kernel dynamics with a source term. We
motivate the methodology through the classical correspondence of information
theory with the physics of parabolic evolution equations. We integrate the
fractional heat kernel into Graph Neural Network architectures such as Graph
Convolutional Networks and Graph Attention, enhancing their expressiveness
through adaptive, multi-hop diffusion. By applying Chebyshev polynomial
approximations, large graphs become computationally feasible. Motivating
variational formulations demonstrate that by extending the classical diffusion
model to fractional powers of the Laplacian, nonlocal interactions deliver more
globally diffusing labels. The particular balance between supervision of known
labels and diffusion across the graph is particularly advantageous in the case
where only a small number of labeled training examples are present. We
demonstrate the effectiveness of this approach on standard datasets.

</details>


### [439] [Domain Generalization: A Tale of Two ERMs](https://arxiv.org/abs/2510.04441)
*Yilun Zhu,Naihao Deng,Naichen Shi,Aditya Gangrade,Clayton Scott*

Main category: cs.LG

TL;DR: 论文探讨了在领域泛化（DG）中，当数据集满足后验漂移假设时，使用带有领域特定信息的特征增强方法（domain-informed ERM）优于传统的池化ERM。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现，在满足协变量偏移假设的数据集上，领域泛化难以超越池化ERM的表现。本文旨在探讨在后验漂移假设下，如何改进ERM方法。

Method: 提出了一种‘domain-informed ERM’方法，通过增强特征向量中的领域特定信息来提升泛化能力。

Result: 理论和实验表明，在后验漂移假设下，domain-informed ERM优于池化ERM。

Conclusion: 在后验漂移假设下，领域特定信息的引入能显著提升领域泛化性能。

Abstract: Domain generalization (DG) is the problem of generalizing from several
distributions (or domains), for which labeled training data are available, to a
new test domain for which no labeled data is available. A common finding in the
DG literature is that it is difficult to outperform empirical risk minimization
(ERM) on the pooled training data.
  In this work, we argue that this finding has primarily been reported for
datasets satisfying a \emph{covariate shift} assumption. When the dataset
satisfies a \emph{posterior drift} assumption instead, we show that
``domain-informed ERM,'' wherein feature vectors are augmented with
domain-specific information, outperforms pooling ERM. These claims are
supported by a theoretical framework and experiments on language and vision
tasks.

</details>


### [440] [Forking-Sequences](https://arxiv.org/abs/2510.04487)
*Willa Potosnak,Malcolm Wolff,Boris Oreshkin,Mengfei Cao,Michael W. Mahoney,Dmitry Efimov,Kin G. Olivares*

Main category: cs.LG

TL;DR: 论文提出了一种名为“forking-sequences”的技术，用于提高时间序列预测模型的稳定性，并验证了其在多种架构中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管准确性是时间序列预测模型的关键要求，但预测稳定性（尤其是跨预测创建日期的稳定性）同样重要，但常被忽视。不稳定的预测会破坏利益相关者的信任和下游决策。

Method: 论文提出并形式化了“forking-sequences”方法，该方法联合编码和解码所有预测创建日期的时间序列，类似于时间序列交叉验证。

Result: 在16个数据集上的实验表明，forking-sequences显著提高了预测稳定性，平均改善了多种架构（如MLP、RNN、LSTM等）的稳定性。

Conclusion: forking-sequences是一种高效且有效的方法，值得在更广泛的神经预测社区中推广。

Abstract: While accuracy is a critical requirement for time series forecasting models,
an equally important (yet often overlooked) desideratum is forecast stability
across forecast creation dates (FCDs). Even highly accurate models can produce
erratic revisions between FCDs, undermining stakeholder trust and disrupting
downstream decision-making. To improve forecast stability, models like MQCNN,
MQT, and SPADE employ a little-known but highly effective technique:
forking-sequences. Unlike standard statistical and neural forecasting methods
that treat each FCD independently, the forking-sequences method jointly encodes
and decodes the entire time series across all FCDs, in a way mirroring time
series cross-validation. Since forking sequences remains largely unknown in the
broader neural forecasting community, in this work, we formalize the
forking-sequences approach, and we make a case for its broader adoption. We
demonstrate three key benefits of forking-sequences: (i) more stable and
consistent gradient updates during training; (ii) reduced forecast variance
through ensembling; and (iii) improved inference computational efficiency. We
validate forking-sequences' benefits using 16 datasets from the M1, M3, M4, and
Tourism competitions, showing improvements in forecast percentage change
stability of 28.8%, 28.8%, 37.9%, and 31.3%, and 8.8%, on average, for MLP,
RNN, LSTM, CNN, and Transformer-based architectures, respectively.

</details>


### [441] [Expand Neurons, Not Parameters](https://arxiv.org/abs/2510.04500)
*Linghao Kong,Inimai Subramanian,Yonadav Shavit,Micah Adler,Dan Alistarh,Nir Shavit*

Main category: cs.LG

TL;DR: 通过增加神经元数量但不增加非零参数数量提升网络性能，减少特征干扰。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不增加非零参数的情况下提升网络性能，减少特征间的干扰。

Method: 提出固定参数扩展（FPE）方法，将神经元替换为多个子神经元并分配非重叠连接。

Result: 在符号任务中，FPE降低了多义性指标并提高了任务准确率；在真实模型中，扩展宽度也能提升性能。

Conclusion: FPE提供了一种基于可解释性的方法，利用宽度对抗叠加效应，提升性能且不增加非零参数数量。

Abstract: This work demonstrates how increasing the number of neurons in a network
without increasing its number of non-zero parameters improves performance. We
show that this gain corresponds with a decrease in interference between
multiple features that would otherwise share the same neurons. To reduce such
entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter
Expansion (FPE): replace a neuron with multiple children and partition the
parent's weights disjointly across them, so that each child inherits a
non-overlapping subset of connections. On symbolic tasks, specifically Boolean
code problems, clause-aligned FPE systematically reduces polysemanticity
metrics and yields higher task accuracy. Notably, random splits of neuron
weights approximate these gains, indicating that reduced collisions, not
precise assignment, are a primary driver. Consistent with the superposition
hypothesis, the benefits of FPE grow with increasing interference: when
polysemantic load is high, accuracy improvements are the largest. Transferring
these insights to real models (classifiers over CLIP embeddings and deeper
multilayer networks) we find that widening networks while maintaining a
constant non-zero parameter count consistently increases accuracy. These
results identify an interpretability-grounded mechanism to leverage width
against superposition, improving performance without increasing the number of
non-zero parameters. Such a direction is well matched to modern accelerators,
where memory movement of non-zero parameters, rather than raw compute, is the
dominant bottleneck.

</details>


### [442] [Wavelet Predictive Representations for Non-Stationary Reinforcement Learning](https://arxiv.org/abs/2510.04507)
*Min Wang,Xin Li,Ye He,Yao-Hui Li,Hasnaa Bennis,Riashat Islam,Mingzhong Wang*

Main category: cs.LG

TL;DR: WISDOM利用小波分析增强非平稳强化学习，通过多尺度特征捕捉环境动态变化，显著提升样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境动态变化频繁，现有非平稳强化学习方法对高度动态环境适应性有限。

Method: 提出WISDOM，将任务表示序列转换到小波域，捕捉多尺度特征；设计小波TD更新算子跟踪MDP演化。

Result: 理论证明算子收敛性，实验显示WISDOM在样本效率和性能上显著优于基线。

Conclusion: WISDOM在复杂非平稳环境中表现出卓越的适应性。

Abstract: The real world is inherently non-stationary, with ever-changing factors, such
as weather conditions and traffic flows, making it challenging for agents to
adapt to varying environmental dynamics. Non-Stationary Reinforcement Learning
(NSRL) addresses this challenge by training agents to adapt rapidly to
sequences of distinct Markov Decision Processes (MDPs). However, existing NSRL
approaches often focus on tasks with regularly evolving patterns, leading to
limited adaptability in highly dynamic settings. Inspired by the success of
Wavelet analysis in time series modeling, specifically its ability to capture
signal trends at multiple scales, we propose WISDOM to leverage wavelet-domain
predictive task representations to enhance NSRL. WISDOM captures these
multi-scale features in evolving MDP sequences by transforming task
representation sequences into the wavelet domain, where wavelet coefficients
represent both global trends and fine-grained variations of non-stationary
changes. In addition to the auto-regressive modeling commonly employed in time
series forecasting, we devise a wavelet temporal difference (TD) update
operator to enhance tracking and prediction of MDP evolution. We theoretically
prove the convergence of this operator and demonstrate policy improvement with
wavelet task representations. Experiments on diverse benchmarks show that
WISDOM significantly outperforms existing baselines in both sample efficiency
and asymptotic performance, demonstrating its remarkable adaptability in
complex environments characterized by non-stationary and stochastically
evolving tasks.

</details>


### [443] [Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction](https://arxiv.org/abs/2510.04522)
*Yisen Gao,Xingcheng Fu,Qingyun Sun,Jianxin Li,Xianxian Li*

Main category: cs.LG

TL;DR: GeoMancer是一个新型的黎曼图扩散框架，解决了图数据中不同曲率特征纠缠的问题，通过等距不变的黎曼陀螺核方法和流形约束扩散方法，提升了生成和预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 由于图数据的非欧几里得性质，现有方法将不同曲率的特征嵌入同一潜在空间，未能充分利用几何潜力。

Method: 提出GeoMancer框架，使用黎曼陀螺核替代指数映射，解耦多级特征到任务特定流形，并引入流形约束扩散方法。

Result: 实验验证了方法的有效性，在多种任务中表现优异。

Conclusion: GeoMancer通过几何建模和流形约束，显著提升了图数据的生成和预测能力。

Abstract: Graph diffusion models have made significant progress in learning structured
graph data and have demonstrated strong potential for predictive tasks.
Existing approaches typically embed node, edge, and graph-level features into a
unified latent space, modeling prediction tasks including classification and
regression as a form of conditional generation. However, due to the
non-Euclidean nature of graph data, features of different curvatures are
entangled in the same latent space without releasing their geometric potential.
To address this issue, we aim to construt an ideal Riemannian diffusion model
to capture distinct manifold signatures of complex graph data and learn their
distribution. This goal faces two challenges: numerical instability caused by
exponential mapping during the encoding proces and manifold deviation during
diffusion generation. To address these challenges, we propose GeoMancer: a
novel Riemannian graph diffusion framework for both generation and prediction
tasks. To mitigate numerical instability, we replace exponential mapping with
an isometric-invariant Riemannian gyrokernel approach and decouple multi-level
features onto their respective task-specific manifolds to learn optimal
representations. To address manifold deviation, we introduce a
manifold-constrained diffusion method and a self-guided strategy for
unconditional generation, ensuring that the generated data remains aligned with
the manifold signature. Extensive experiments validate the effectiveness of our
approach, demonstrating superior performance across a variety of tasks.

</details>


### [444] [Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion](https://arxiv.org/abs/2510.04525)
*Satoshi Hayakawa,Yuhta Takida,Masaaki Imaizumi,Hiromi Wakaki,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 本文分析了MaskGIT采样器在图像建模中的隐式温度采样机制，提出了更易解释的"moment sampler"，并通过部分缓存技术和混合方法提高了采样效率。


<details>
  <summary>Details</summary>
Motivation: 加速掩码扩散模型的采样过程，同时提升其理论理解和实际效率。

Method: 理论分析MaskGIT采样器，提出"moment sampler"，并引入部分缓存技术和混合方法优化采样效率。

Result: 实验证明新方法在图像和文本领域均高效，提升了掩码扩散采样器的理论和实践水平。

Conclusion: 本文提出的方法不仅理论上有突破，还显著提升了采样效率，推动了掩码扩散采样器的发展。

Abstract: Masked diffusion models have shown promising performance in generating
high-quality samples in a wide range of domains, but accelerating their
sampling process remains relatively underexplored. To investigate efficient
samplers for masked diffusion, this paper theoretically analyzes the MaskGIT
sampler for image modeling, revealing its implicit temperature sampling
mechanism. Through this analysis, we introduce the "moment sampler," an
asymptotically equivalent but more tractable and interpretable alternative to
MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking
positions before sampling tokens. In addition, we improve the efficiency of
choose-then-sample algorithms through two key innovations: a partial caching
technique for transformers that approximates longer sampling trajectories
without proportional computational cost, and a hybrid approach formalizing the
exploration-exploitation trade-off in adaptive unmasking. Experiments in image
and text domains demonstrate our theory as well as the efficiency of our
proposed methods, advancing both theoretical understanding and practical
implementation of masked diffusion samplers.

</details>


### [445] [Graph-based Tabular Deep Learning Should Learn Feature Interactions, Not Just Make Predictions](https://arxiv.org/abs/2510.04543)
*Elias Dubbeldam,Reza Mohammadi,Marit Schoonhoven,S. Ilker Birbil*

Main category: cs.LG

TL;DR: 论文主张图表示表格深度学习方法应更注重特征交互的显式学习和评估，而非仅关注预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图表示表格深度学习方法主要优化预测准确性，忽视了图结构的准确建模，导致无法恢复有意义的特征交互。

Method: 使用已知真实图结构的合成数据集，评估现有方法的特征交互恢复能力，并验证真实交互结构对预测性能的影响。

Result: 现有方法未能恢复有意义的特征交互，而强制使用真实交互结构可提升预测性能。

Conclusion: 呼吁转向结构感知建模，以构建更准确、可解释且基于领域理解的图表示表格深度学习系统。

Abstract: Despite recent progress, deep learning methods for tabular data still
struggle to compete with traditional tree-based models. A key challenge lies in
modeling complex, dataset-specific feature interactions that are central to
tabular data. Graph-based tabular deep learning (GTDL) methods aim to address
this by representing features and their interactions as graphs. However,
existing methods predominantly optimize predictive accuracy, neglecting
accurate modeling of the graph structure. This position paper argues that GTDL
should move beyond prediction-centric objectives and prioritize the explicit
learning and evaluation of feature interactions. Using synthetic datasets with
known ground-truth graph structures, we show that existing GTDL methods fail to
recover meaningful feature interactions. Moreover, enforcing the true
interaction structure improves predictive performance. This highlights the need
for GTDL methods to prioritize quantitative evaluation and accurate structural
learning. We call for a shift toward structure-aware modeling as a foundation
for building GTDL systems that are not only accurate but also interpretable,
trustworthy, and grounded in domain understanding.

</details>


### [446] [Tail-Safe Hedging: Explainable Risk-Sensitive Reinforcement Learning with a White-Box CBF--QP Safety Layer in Arbitrage-Free Markets](https://arxiv.org/abs/2510.04555)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: Tail-Safe是一个面向部署的衍生品对冲框架，结合了分布强化学习和安全层控制，确保金融约束下的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决衍生品对冲中的尾部风险问题，同时满足金融监管和可审计性需求。

Method: 结合IQN-CVaR-PPO分布强化学习和CBF-QP安全层，通过温度倾斜和尾部增强稳定估计。

Result: 在合成市场中，Tail-Safe改善了左尾风险且未影响核心性能，同时满足硬约束条件。

Conclusion: Tail-Safe提供了一种可解释、可审计的对冲方法，但依赖合成数据和简化执行环境。

Abstract: We introduce Tail-Safe, a deployability-oriented framework for derivatives
hedging that unifies distributional, risk-sensitive reinforcement learning with
a white-box control-barrier-function (CBF) quadratic-program (QP) safety layer
tailored to financial constraints. The learning component combines an IQN-based
distributional critic with a CVaR objective (IQN--CVaR--PPO) and a
Tail-Coverage Controller that regulates quantile sampling through temperature
tilting and tail boosting to stabilize small-$\alpha$ estimation. The safety
component enforces discrete-time CBF inequalities together with domain-specific
constraints -- ellipsoidal no-trade bands, box and rate limits, and a
sign-consistency gate -- solved as a convex QP whose telemetry (active sets,
tightness, rate utilization, gate scores, slack, and solver status) forms an
auditable trail for governance. We provide guarantees of robust forward
invariance of the safe set under bounded model mismatch, a minimal-deviation
projection interpretation of the QP, a KL-to-DRO upper bound linking per-state
KL regularization to worst-case CVaR, concentration and sample-complexity
results for the temperature-tilted CVaR estimator, and a CVaR trust-region
improvement inequality under KL limits, together with feasibility persistence
under expiry-aware tightening. Empirically, in arbitrage-free,
microstructure-aware synthetic markets (SSVI $\to$ Dupire $\to$ VIX with
ABIDES/MockLOB execution), Tail-Safe improves left-tail risk without degrading
central performance and yields zero hard-constraint violations whenever the QP
is feasible with zero slack. Telemetry is mapped to governance dashboards and
incident workflows to support explainability and auditability. Limitations
include reliance on synthetic data and simplified execution to isolate
methodological contributions.

</details>


### [447] [Challenger-Based Combinatorial Bandits for Subcarrier Selection in OFDM Systems](https://arxiv.org/abs/2510.04559)
*Mohsen Amiri,V Venktesh,Sindri Magnússon*

Main category: cs.LG

TL;DR: 论文提出了一种基于线性效用模型和gap-index框架的高效方法，用于在多用户MIMO下行链路中识别top-m用户调度集，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决多用户MIMO下行链路中用户调度集的高维组合搜索问题，避免穷举搜索的计算不可行性。

Method: 采用线性效用模型和gap-index框架，通过维护冠军集和挑战者集的短列表，聚焦于信息量最大的比较。

Result: 相比现有线性bandit方法，显著减少了运行时间和计算量，同时保持了高识别精度。

Conclusion: 该方法为AI驱动的通信系统提供了高效的在线子载波选择方案，并在速度和精度之间提供了可调权衡。

Abstract: This paper investigates the identification of the top-m user-scheduling sets
in multi-user MIMO downlink, which is cast as a combinatorial pure-exploration
problem in stochastic linear bandits. Because the action space grows
exponentially, exhaustive search is infeasible. We therefore adopt a linear
utility model to enable efficient exploration and reliable selection of
promising user subsets. We introduce a gap-index framework that maintains a
shortlist of current estimates of champion arms (top-m sets) and a rotating
shortlist of challenger arms that pose the greatest threat to the champions.
This design focuses on measurements that yield the most informative
gap-index-based comparisons, resulting in significant reductions in runtime and
computation compared to state-of-the-art linear bandit methods, with high
identification accuracy. The method also exposes a tunable trade-off between
speed and accuracy. Simulations on a realistic OFDM downlink show that
shortlist-driven pure exploration makes online, measurement-efficient
subcarrier selection practical for AI-enabled communication systems.

</details>


### [448] [Stochastic Approximation Methods for Distortion Risk Measure Optimization](https://arxiv.org/abs/2510.04563)
*Jinyang Jiang,Bernd Heidergott,Jiaqiao Hu,Yijie Peng*

Main category: cs.LG

TL;DR: 本文提出了基于两种对偶表示的梯度下降算法（DM形式和QF形式）用于优化失真风险度量（DRMs），并提供了强收敛性和收敛速度的证明。


<details>
  <summary>Details</summary>
Motivation: 失真风险度量（DRMs）在决策中捕捉风险偏好，是管理不确定性的通用标准。本文旨在开发高效的优化算法以支持实际应用。

Method: DM形式采用三时间尺度算法跟踪分位数并计算梯度，QF形式提供更简单的两时间尺度方法。混合形式结合两者优势。

Result: DM形式达到$O(k^{-4/7})$的最优收敛速度，QF形式达到更快的$O(k^{-2/3})$。数值实验验证了算法的有效性。

Conclusion: 所提算法在鲁棒投资组合选择和深度强化学习中展示了实用性和可扩展性。

Abstract: Distortion Risk Measures (DRMs) capture risk preferences in decision-making
and serve as general criteria for managing uncertainty. This paper proposes
gradient descent algorithms for DRM optimization based on two dual
representations: the Distortion-Measure (DM) form and Quantile-Function (QF)
form. The DM-form employs a three-timescale algorithm to track quantiles,
compute their gradients, and update decision variables, utilizing the
Generalized Likelihood Ratio and kernel-based density estimation. The QF-form
provides a simpler two-timescale approach that avoids the need for complex
quantile gradient estimation. A hybrid form integrates both approaches,
applying the DM-form for robust performance around distortion function jumps
and the QF-form for efficiency in smooth regions. Proofs of strong convergence
and convergence rates for the proposed algorithms are provided. In particular,
the DM-form achieves an optimal rate of $O(k^{-4/7})$, while the QF-form
attains a faster rate of $O(k^{-2/3})$. Numerical experiments confirm their
effectiveness and demonstrate substantial improvements over baselines in robust
portfolio selection tasks. The method's scalability is further illustrated
through integration into deep reinforcement learning. Specifically, a DRM-based
Proximal Policy Optimization algorithm is developed and applied to
multi-echelon dynamic inventory management, showcasing its practical
applicability.

</details>


### [449] [GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning](https://arxiv.org/abs/2510.04567)
*Weishuo Ma,Yanbo Wang,Xiyuan Wang,Lei Zou,Muhan Zhang*

Main category: cs.LG

TL;DR: GILT是一种基于图神经网络的框架，通过无LLM和无调优的架构解决图数据的异质性问题。


<details>
  <summary>Details</summary>
Motivation: 当前图基础模型（GFMs）在处理图数据的极端异质性时面临挑战，包括独特的特征空间、标签集和拓扑结构。

Method: GILT引入了一种基于标记的上下文学习框架，统一处理节点、边和图级别的分类任务，并支持动态理解类别语义。

Result: 实验表明，GILT在少样本学习任务中表现优于基于LLM或调优的基线方法，且时间效率更高。

Conclusion: GILT通过无调优的上下文学习机制，有效解决了图数据的异质性问题，为图基础模型提供了新的解决方案。

Abstract: Graph Neural Networks (GNNs) are powerful tools for precessing relational
data but often struggle to generalize to unseen graphs, giving rise to the
development of Graph Foundational Models (GFMs). However, current GFMs are
challenged by the extreme heterogeneity of graph data, where each graph can
possess a unique feature space, label set, and topology. To address this, two
main paradigms have emerged. The first leverages Large Language Models (LLMs),
but is fundamentally text-dependent, thus struggles to handle the numerical
features in vast graphs. The second pre-trains a structure-based model, but the
adaptation to new tasks typically requires a costly, per-graph tuning stage,
creating a critical efficiency bottleneck. In this work, we move beyond these
limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning
\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free
architecture. GILT introduces a novel token-based framework for in-context
learning (ICL) on graphs, reframing classification tasks spanning node, edge
and graph levels in a unified framework. This mechanism is the key to handling
heterogeneity, as it is designed to operate on generic numerical features.
Further, its ability to understand class semantics dynamically from the context
enables tuning-free adaptation. Comprehensive experiments show that GILT
achieves stronger few-shot performance with significantly less time than
LLM-based or tuning-based baselines, validating the effectiveness of our
approach.

</details>


### [450] [LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning](https://arxiv.org/abs/2510.04573)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Nicklas Majamaki,Navdeep Jaitly,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

TL;DR: LaDiR（Latent Diffusion Reasoner）是一种新的推理框架，结合了连续潜在表示的表达能力和潜在扩散模型的迭代优化能力，提升了大型语言模型（LLM）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的自回归解码方式限制了其全局优化和多样化探索能力，LaDiR旨在解决这一问题。

Method: 通过变分自编码器（VAE）构建结构化潜在推理空间，并利用潜在扩散模型进行块级双向注意力去噪，实现并行生成多样化推理轨迹。

Result: 在数学推理和规划任务中，LaDiR在准确性、多样性和可解释性上均优于现有方法。

Conclusion: LaDiR为文本推理提供了一种新的潜在扩散范式。

Abstract: Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.

</details>


### [451] [Busemann Functions in the Wasserstein Space: Existence, Closed-Forms, and Applications to Slicing](https://arxiv.org/abs/2510.04579)
*Clément Bonet,Elsa Cazelles,Lucas Drumetz,Nicolas Courty*

Main category: cs.LG

TL;DR: 论文研究了Busemann函数在Wasserstein空间中的存在性和计算方法，并在一维分布和高斯测度下建立了闭式表达式，提出了新的Sliced-Wasserstein距离。


<details>
  <summary>Details</summary>
Motivation: Busemann函数在几何机器学习中具有重要作用，尤其是在处理概率分布数据时，Wasserstein空间因其丰富的黎曼结构成为自然的研究对象。

Method: 研究Busemann函数在Wasserstein空间中的存在性和计算，重点分析了一维分布和高斯测度的情况，并提出了投影方案。

Result: 在一维分布和高斯测度下建立了Busemann函数的闭式表达式，并提出了新的Sliced-Wasserstein距离。

Conclusion: 提出的方法在合成数据集和迁移学习问题中表现出高效性，为概率分布的处理提供了新工具。

Abstract: The Busemann function has recently found much interest in a variety of
geometric machine learning problems, as it naturally defines projections onto
geodesic rays of Riemannian manifolds and generalizes the notion of
hyperplanes. As several sources of data can be conveniently modeled as
probability distributions, it is natural to study this function in the
Wasserstein space, which carries a rich formal Riemannian structure induced by
Optimal Transport metrics. In this work, we investigate the existence and
computation of Busemann functions in Wasserstein space, which admits geodesic
rays. We establish closed-form expressions in two important cases:
one-dimensional distributions and Gaussian measures. These results enable
explicit projection schemes for probability distributions on $\mathbb{R}$,
which in turn allow us to define novel Sliced-Wasserstein distances over
Gaussian mixtures and labeled datasets. We demonstrate the efficiency of those
original schemes on synthetic datasets as well as transfer learning problems.

</details>


### [452] [Improved probabilistic regression using diffusion models](https://arxiv.org/abs/2510.04583)
*Carlo Kneissl,Christopher Bülte,Philipp Scholl,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的概率回归框架，能够非参数地学习预测分布，并在多种回归任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成复杂数据方面表现出色，但在回归任务中缺乏不确定性评估且应用受限，因此需要一种通用框架。

Method: 通过建模扩散噪声的完整分布，适应多样化任务并增强不确定性量化，研究了不同噪声参数化的权衡。

Result: 在低维和高维回归任务中，该方法优于现有基线，并提供校准的不确定性估计。

Conclusion: 该框架是一种通用的概率预测工具，具有广泛适用性和优越性能。

Abstract: Probabilistic regression models the entire predictive distribution of a
response variable, offering richer insights than classical point estimates and
directly allowing for uncertainty quantification. While diffusion-based
generative models have shown remarkable success in generating complex,
high-dimensional data, their usage in general regression tasks often lacks
uncertainty-related evaluation and remains limited to domain-specific
applications. We propose a novel diffusion-based framework for probabilistic
regression that learns predictive distributions in a nonparametric way. More
specifically, we propose to model the full distribution of the diffusion noise,
enabling adaptation to diverse tasks and enhanced uncertainty quantification.
We investigate different noise parameterizations, analyze their trade-offs, and
evaluate our framework across a broad range of regression tasks, covering low-
and high-dimensional settings. For several experiments, our approach shows
superior performance against existing baselines, while delivering calibrated
uncertainty estimates, demonstrating its versatility as a tool for
probabilistic prediction.

</details>


### [453] [Closed-Form Last Layer Optimization](https://arxiv.org/abs/2510.04606)
*Alexandre Galashov,Nathaël Da Costa,Liyuan Xu,Philipp Hennig,Arthur Gretton*

Main category: cs.LG

TL;DR: 论文提出了一种优化神经网络的方法，通过闭式解更新最后一层权重，交替优化主干参数，适用于随机梯度下降，并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络优化通常使用随机梯度下降变体，但在平方损失下，最后一层线性权重的闭式解已知，因此可以利用这一点优化主干参数。

Method: 将最后一层权重视为主干参数的函数，仅优化主干参数，相当于交替进行主干梯度下降和最后一层闭式更新。

Result: 在神经正切核机制下，证明了该方法收敛到最优解，并在多个监督任务中表现优于标准SGD。

Conclusion: 该方法通过利用最后一层闭式解，提高了优化效率，适用于回归和分类任务。

Abstract: Neural networks are typically optimized with variants of stochastic gradient
descent. Under a squared loss, however, the optimal solution to the linear last
layer weights is known in closed-form. We propose to leverage this during
optimization, treating the last layer as a function of the backbone parameters,
and optimizing solely for these parameters. We show this is equivalent to
alternating between gradient descent steps on the backbone and closed-form
updates on the last layer. We adapt the method for the setting of stochastic
gradient descent, by trading off the loss on the current batch against the
accumulated information from previous batches. Further, we prove that, in the
Neural Tangent Kernel regime, convergence of this method to an optimal solution
is guaranteed. Finally, we demonstrate the effectiveness of our approach
compared with standard SGD on a squared loss in several supervised tasks --
both regression and classification -- including Fourier Neural Operators and
Instrumental Variable Regression.

</details>


### [454] [Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models](https://arxiv.org/abs/2510.04618)
*Qizheng Zhang,Changran Hu,Shubhangi Upasani,Boyuan Ma,Fenglu Hong,Vamsidhar Kamanuru,Jay Rainton,Chen Wu,Mengmeng Ji,Hanchen Li,Urmish Thakker,James Zou,Kunle Olukotun*

Main category: cs.LG

TL;DR: ACE框架通过模块化生成、反思和整理过程，优化上下文适应，提升LLM在代理和领域特定任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因简洁性偏见和上下文崩溃导致的领域洞察丢失和细节侵蚀问题。

Method: 引入ACE框架，将上下文视为不断演变的剧本，通过结构化增量更新防止崩溃。

Result: 在代理和金融基准测试中分别提升10.6%和8.6%，并显著降低适应延迟和部署成本。

Conclusion: ACE展示了通过全面演变的上下文实现高效、可扩展且自我改进的LLM系统的潜力。

Abstract: Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

</details>


### [455] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出一种基于高级预测模型的合成生物医学时间序列数据生成框架，解决数据稀缺和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 严格的隐私法规和资源需求限制了生物医学时间序列AI的发展，导致数据需求与可访问性之间存在巨大差距。

Method: 使用高级预测模型生成合成数据，准确复制复杂的电生理信号（如EEG和EMG），并保持其统计特性。

Result: 合成数据保留了真实数据的关键时频特性，可作为真实数据的有效替代品，显著提升AI模型性能。

Conclusion: 该框架在保持关键生物医学特征的同时，具有高度可扩展性，并能无缝集成到开源资源库中，为AI驱动的生物医学研究提供了丰富资源。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [456] [Compressed Concatenation of Small Embedding Models](https://arxiv.org/abs/2510.04626)
*Mohamed Ayoub Ben Ayad,Michael Dinzinger,Kanishka Ghosh Dastidar,Jelena Mitrovic,Michael Granitzer*

Main category: cs.LG

TL;DR: 通过拼接多个小嵌入模型并使用轻量级解码器，在资源受限环境中实现高性能嵌入。


<details>
  <summary>Details</summary>
Motivation: 解决大嵌入模型在资源受限环境中部署困难的问题，同时保持性能。

Method: 拼接多个小模型的嵌入向量，引入轻量级解码器降维，并应用MRL损失训练。

Result: 在MTEB检索任务中，通过拼接四个小模型，实现48倍压缩下保留89%性能。

Conclusion: 该方法在资源受限环境中有效平衡了性能与效率。

Abstract: Embedding models are central to dense retrieval, semantic search, and
recommendation systems, but their size often makes them impractical to deploy
in resource-constrained environments such as browsers or edge devices. While
smaller embedding models offer practical advantages, they typically
underperform compared to their larger counterparts. To bridge this gap, we
demonstrate that concatenating the raw embedding vectors of multiple small
models can outperform a single larger baseline on standard retrieval
benchmarks. To overcome the resulting high dimensionality of naive
concatenation, we introduce a lightweight unified decoder trained with a
Matryoshka Representation Learning (MRL) loss. This decoder maps the
high-dimensional joint representation to a low-dimensional space, preserving
most of the original performance without fine-tuning the base models. We also
show that while concatenating more base models yields diminishing gains, the
robustness of the decoder's representation under compression and quantization
improves. Our experiments show that, on a subset of MTEB retrieval tasks, our
concat-encode-quantize pipeline recovers 89\% of the original performance with
a 48x compression factor when the pipeline is applied to a concatenation of
four small embedding models.

</details>


### [457] [Predictive Feature Caching for Training-free Acceleration of Molecular Geometry Generation](https://arxiv.org/abs/2510.04646)
*Johanna Sommer,John Rachwan,Nils Fleischmann,Stephan Günnemann,Bertrand Charpentier*

Main category: cs.LG

TL;DR: 提出了一种无需训练的缓存策略，加速分子几何生成，通过预测中间隐藏状态，实现推理时间减半，最高提速3倍。


<details>
  <summary>Details</summary>
Motivation: Flow matching模型在推理时计算成本高，成为实际应用中的瓶颈。

Method: 直接在SE(3)-equivariant骨干网络上操作，兼容预训练模型，与现有训练加速和系统优化正交。

Result: 在GEOM-Drugs数据集上，缓存策略实现推理时间减半，最高提速3倍，与其他优化结合可达7倍。

Conclusion: 缓存策略显著提升推理效率，且不影响样本质量。

Abstract: Flow matching models generate high-fidelity molecular geometries but incur
significant computational costs during inference, requiring hundreds of network
evaluations. This inference overhead becomes the primary bottleneck when such
models are employed in practice to sample large numbers of molecular
candidates. This work discusses a training-free caching strategy that
accelerates molecular geometry generation by predicting intermediate hidden
states across solver steps. The proposed method operates directly on the
SE(3)-equivariant backbone, is compatible with pretrained models, and is
orthogonal to existing training-based accelerations and system-level
optimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching
achieves a twofold reduction in wall-clock inference time at matched sample
quality and a speedup of up to 3x compared to the base model with minimal
sample quality degradation. Because these gains compound with other
optimizations, applying caching alongside other general, lossless optimizations
yield as much as a 7x speedup.

</details>


### [458] [IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams](https://arxiv.org/abs/2510.04660)
*Yuandou Wang,Filip Gunnarsson,Rihan Hai*

Main category: cs.LG

TL;DR: 提出了一种紧凑的持续学习方法IMLP，用于表格数据流，具有恒定内存和高效能源消耗。


<details>
  <summary>Details</summary>
Motivation: 表格数据流在实时决策中应用广泛，但现有持续学习方法在能源和内存效率方面不足。

Method: IMLP采用滑动潜在特征缓冲区和窗口注意力机制，避免存储原始数据。

Result: IMLP比TabNet和TabPFN分别提高27.6倍和85.5倍的能源效率，同时保持竞争力。

Conclusion: IMLP为表格数据流提供了一种易于部署且高效的持续学习替代方案。

Abstract: Tabular data streams are rapidly emerging as a dominant modality for
real-time decision-making in healthcare, finance, and the Internet of Things
(IoT). These applications commonly run on edge and mobile devices, where energy
budgets, memory, and compute are strictly limited. Continual learning (CL)
addresses such dynamics by training models sequentially on task streams while
preserving prior knowledge and consolidating new knowledge. While recent CL
work has advanced in mitigating catastrophic forgetting and improving knowledge
transfer, the practical requirements of energy and memory efficiency for
tabular data streams remain underexplored. In particular, existing CL solutions
mostly depend on replay mechanisms whose buffers grow over time and exacerbate
resource costs.
  We propose a context-aware incremental Multi-Layer Perceptron (IMLP), a
compact continual learner for tabular data streams. IMLP incorporates a
windowed scaled dot-product attention over a sliding latent feature buffer,
enabling constant-size memory and avoiding storing raw data. The attended
context is concatenated with current features and processed by shared
feed-forward layers, yielding lightweight per-segment updates. To assess
practical deployability, we introduce NetScore-T, a tunable metric coupling
balanced accuracy with energy for Pareto-aware comparison across models and
datasets. IMLP achieves up to $27.6\times$ higher energy efficiency than TabNet
and $85.5\times$ higher than TabPFN, while maintaining competitive average
accuracy. Overall, IMLP provides an easy-to-deploy, energy-efficient
alternative to full retraining for tabular data streams.

</details>


### [459] [Noise or Signal? Deconstructing Contradictions and An Adaptive Remedy for Reversible Normalization in Time Series Forecasting](https://arxiv.org/abs/2510.04667)
*Fanzhe Fu,Yang Yang*

Main category: cs.LG

TL;DR: Reversible Instance Normalization (RevIN) 的性能在时间序列预测中被解构，揭示了四种理论矛盾。研究发现，标准 RevIN 在极端异常值数据集上表现极差，而简单的 R²-IN 表现最佳，但自适应模型 (A-IN) 却完全失败。


<details>
  <summary>Details</summary>
Motivation: 探讨 RevIN 及其改进版本在时间序列预测中的表现，揭示简单启发式方法可能带来的不稳定性和潜在风险。

Method: 通过实验分析不同归一化策略（RevIN、R²-IN 和 A-IN）在极端异常值数据集上的表现，并识别四种理论矛盾。

Result: 标准 RevIN 在极端异常值数据集上 MSE 增加 683%，R²-IN 表现最佳，而 A-IN 完全失败。

Conclusion: 提出新的时间序列归一化范式：从盲目追求复杂性转向诊断驱动分析，强调简单基线的优势和天真适应的危险性。

Abstract: Reversible Instance Normalization (RevIN) is a key technique enabling simple
linear models to achieve state-of-the-art performance in time series
forecasting. While replacing its non-robust statistics with robust counterparts
(termed R$^2$-IN) seems like a straightforward improvement, our findings reveal
a far more complex reality. This paper deconstructs the perplexing performance
of various normalization strategies by identifying four underlying theoretical
contradictions. Our experiments provide two crucial findings: first, the
standard RevIN catastrophically fails on datasets with extreme outliers, where
its MSE surges by a staggering 683\%. Second, while the simple R$^2$-IN
prevents this failure and unexpectedly emerges as the best overall performer,
our adaptive model (A-IN), designed to test a diagnostics-driven heuristic,
unexpectedly suffers a complete and systemic failure. This surprising outcome
uncovers a critical, overlooked pitfall in time series analysis: the
instability introduced by a simple or counter-intuitive heuristic can be more
damaging than the statistical issues it aims to solve. The core contribution of
this work is thus a new, cautionary paradigm for time series normalization: a
shift from a blind search for complexity to a diagnostics-driven analysis that
reveals not only the surprising power of simple baselines but also the perilous
nature of naive adaptation.

</details>


### [460] [Semantic Channel Equalization Strategies for Deep Joint Source-Channel Coding](https://arxiv.org/abs/2510.04674)
*Lorenzo Pannacci,Simone Fiorellino,Mario Edoardo Pandolfo,Emilio Calvanese Strinati,Paolo Di Lorenzo*

Main category: cs.LG

TL;DR: DeepJSCC在异构网络中面临潜在空间不匹配问题，提出三种语义信道均衡方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多厂商部署中因潜在空间不匹配导致的语义噪声问题，提升DeepJSCC在异构网络中的性能。

Method: 引入三种语义信道均衡方法：线性映射、轻量级神经网络和Parseval-frame均衡器。

Result: 通过实验量化了复杂度、数据效率和保真度之间的权衡，为异构网络部署提供指导。

Conclusion: 语义信道均衡方法能有效解决潜在空间不匹配问题，提升DeepJSCC在异构网络中的适用性。

Abstract: Deep joint source-channel coding (DeepJSCC) has emerged as a powerful
paradigm for end-to-end semantic communications, jointly learning to compress
and protect task-relevant features over noisy channels. However, existing
DeepJSCC schemes assume a shared latent space at transmitter (TX) and receiver
(RX) - an assumption that fails in multi-vendor deployments where encoders and
decoders cannot be co-trained. This mismatch introduces "semantic noise",
degrading reconstruction quality and downstream task performance. In this
paper, we systematize and evaluate methods for semantic channel equalization
for DeepJSCC, introducing an additional processing stage that aligns
heterogeneous latent spaces under both physical and semantic impairments. We
investigate three classes of aligners: (i) linear maps, which admit closed-form
solutions; (ii) lightweight neural networks, offering greater expressiveness;
and (iii) a Parseval-frame equalizer, which operates in zero-shot mode without
the need for training. Through extensive experiments on image reconstruction
over AWGN and fading channels, we quantify trade-offs among complexity, data
efficiency, and fidelity, providing guidelines for deploying DeepJSCC in
heterogeneous AI-native wireless networks.

</details>


### [461] [Counterfactual Credit Guided Bayesian Optimization](https://arxiv.org/abs/2510.04676)
*Qiyu Wei,Haowei Wang,Richard Allmendinger,Mauricio A. Álvarez*

Main category: cs.LG

TL;DR: 论文提出了一种名为CCGBO的新框架，通过反事实信用评估历史观测的贡献，优化贝叶斯优化的资源分配，以更快找到全局最优解。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法假设所有观测对发现最优解贡献均等，但在实际场景中，这种假设可能限制效率。

Method: 引入反事实信用（counterfactual credit）量化历史观测的贡献，并将其融入获取函数，选择性分配资源。

Result: CCGBO在合成和真实基准测试中显著降低简单遗憾并加速收敛。

Conclusion: CCGBO通过反事实信用有效提升贝叶斯优化的效率，且理论证明其具有次线性遗憾。

Abstract: Bayesian optimization has emerged as a prominent methodology for optimizing
expensive black-box functions by leveraging Gaussian process surrogates, which
focus on capturing the global characteristics of the objective function.
However, in numerous practical scenarios, the primary objective is not to
construct an exhaustive global surrogate, but rather to quickly pinpoint the
global optimum. Due to the aleatoric nature of the sequential optimization
problem and its dependence on the quality of the surrogate model and the
initial design, it is restrictive to assume that all observed samples
contribute equally to the discovery of the optimum in this context. In this
paper, we introduce Counterfactual Credit Guided Bayesian Optimization (CCGBO),
a novel framework that explicitly quantifies the contribution of individual
historical observations through counterfactual credit. By incorporating
counterfactual credit into the acquisition function, our approach can
selectively allocate resources in areas where optimal solutions are most likely
to occur. We prove that CCGBO retains sublinear regret. Empirical evaluations
on various synthetic and real-world benchmarks demonstrate that CCGBO
consistently reduces simple regret and accelerates convergence to the global
optimum.

</details>


### [462] [Parameter-free Algorithms for the Stochastically Extended Adversarial Model](https://arxiv.org/abs/2510.04685)
*Shuche Wang,Adarsh Barik,Peng Zhao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 提出了首个无参数算法用于SEA模型，解决了现有方法需要先验知识的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SEA模型方法需要已知问题参数（如域直径和Lipschitz常数），限制了实际应用。

Method: 利用Optimistic Online Newton Step (OONS)算法，开发无参数方法，适应未知域直径和Lipschitz常数的场景。

Result: 在未知域直径但已知Lipschitz常数的情况下，实现了预期遗憾界；进一步扩展到两者均未知的场景，遗憾界保持相同依赖关系。

Conclusion: 提出的无参数方法在SEA模型中有效，即使参数未知，仍能保持性能。

Abstract: We develop the first parameter-free algorithms for the Stochastically
Extended Adversarial (SEA) model, a framework that bridges adversarial and
stochastic online convex optimization. Existing approaches for the SEA model
require prior knowledge of problem-specific parameters, such as the diameter of
the domain $D$ and the Lipschitz constant of the loss functions $G$, which
limits their practical applicability. Addressing this, we develop
parameter-free methods by leveraging the Optimistic Online Newton Step (OONS)
algorithm to eliminate the need for these parameters. We first establish a
comparator-adaptive algorithm for the scenario with unknown domain diameter but
known Lipschitz constant, achieving an expected regret bound of
$\tilde{O}\big(\|u\|_2^2 + \|u\|_2(\sqrt{\sigma^2_{1:T}} +
\sqrt{\Sigma^2_{1:T}})\big)$, where $u$ is the comparator vector and
$\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$ represent the cumulative stochastic
variance and cumulative adversarial variation, respectively. We then extend
this to the more general setting where both $D$ and $G$ are unknown, attaining
the comparator- and Lipschitz-adaptive algorithm. Notably, the regret bound
exhibits the same dependence on $\sigma^2_{1:T}$ and $\Sigma^2_{1:T}$,
demonstrating the efficacy of our proposed methods even when both parameters
are unknown in the SEA model.

</details>


### [463] [How does the optimizer implicitly bias the model merging loss landscape?](https://arxiv.org/abs/2510.04686)
*Chenxiang Zhang,Alexander Theus,Damien Teney,Antonio Orvieto,Jun Pang,Sjouke Mauw*

Main category: cs.LG

TL;DR: 本文探讨了优化过程如何影响损失景观几何及其对模型合并成功的影响，发现有效噪声尺度是统一优化器和数据选择影响的关键因素。


<details>
  <summary>Details</summary>
Motivation: 理解模型合并成功的属性，特别是优化过程如何影响损失景观几何及其对合并的影响。

Method: 通过分析有效噪声尺度，研究不同优化器和数据选择对模型合并的影响。

Result: 发现合并效果是有效噪声的非单调函数，存在一个最佳点；学习率、权重衰减、批量大小和数据增强独立调节有效噪声。

Conclusion: 优化噪声不仅影响单个最小值的平坦性或泛化性，还影响全局损失景观，预测独立训练解决方案的合并可能性。

Abstract: Model merging methods combine models with different capabilities into a
single one while maintaining the same inference cost. Two popular approaches
are linear interpolation, which linearly interpolates between model weights,
and task arithmetic, which combines task vectors obtained by the difference
between finetuned and base models. While useful in practice, what properties
make merging effective are poorly understood. This paper explores how the
optimization process affects the loss landscape geometry and its impact on
merging success. We show that a single quantity -- the effective noise scale --
unifies the impact of optimizer and data choices on model merging. Across
architectures and datasets, the effectiveness of merging success is a
non-monotonic function of effective noise, with a distinct optimum. Decomposing
this quantity, we find that larger learning rates, stronger weight decay,
smaller batch sizes, and data augmentation all independently modulate the
effective noise scale, exhibiting the same qualitative trend. Unlike prior work
that connects optimizer noise to the flatness or generalization of individual
minima, we show that it also affects the global loss landscape, predicting when
independently trained solutions can be merged. Our findings broaden the
understanding of how optimization shapes the loss landscape geometry and its
downstream consequences for model merging, suggesting the possibility of
further manipulating the training dynamics to improve merging effectiveness.

</details>


### [464] [ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts](https://arxiv.org/abs/2510.04710)
*Zexin Wang,Changhua Pei,Yang Liu,Hengyue Jiang,Quan Zhou,Haotian Si,Hang Cui,Jianhui Li,Gaogang Xie,Jingjing Li,Dan Pei*

Main category: cs.LG

TL;DR: 提出了一种基于视觉语言模型（VLM）的框架ViTs，将时间序列数据转换为视觉表示，解决了传统方法在零样本泛化和处理变长序列时的限制。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列异常检测方法在零样本泛化和处理变长序列时存在限制，而LLMs在时间序列数据中因上下文长度受限。

Method: 通过将时间序列曲线转换为视觉表示，并设计三阶段训练流程（时间序列知识注入、异常检测增强、异常推理细化）。

Result: ViTs显著提升了VLMs在时间序列数据中的理解和异常检测能力。

Conclusion: ViTs框架有效解决了时间序列异常检测中的零样本泛化和变长序列处理问题，并通过实验验证了其优越性。

Abstract: Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.

</details>


### [465] [Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs](https://arxiv.org/abs/2510.04727)
*Emanuele Mule,Stefano Fiorini,Antonio Purificato,Federico Siciliano,Stefano Coniglio,Fabrizio Silvestri*

Main category: cs.LG

TL;DR: 提出了一种名为DSHN的框架，将层理论引入有向超图，解决了现有方法在异质性环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理有向超图时存在对同质性的隐含偏好，限制了在异质性环境中的表现。

Method: 结合层理论和有向超图的不对称关系，构建了Directed Sheaf Hypergraph Laplacian算子。

Result: 在7个真实数据集上，DSHN相比13种基线方法，准确率提升了2%至20%。

Conclusion: 通过层理论和有向超图的结合，显著提升了性能。

Abstract: Hypergraphs provide a natural way to represent higher-order interactions
among multiple entities. While undirected hypergraphs have been extensively
studied, the case of directed hypergraphs, which can model oriented group
interactions, remains largely under-explored despite its relevance for many
applications. Recent approaches in this direction often exhibit an implicit
bias toward homophily, which limits their effectiveness in heterophilic
settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf
Neural Networks (SNNs) were introduced as an effective solution to circumvent
such a drawback. While a generalization to hypergraphs is known, it is only
suitable for undirected hypergraphs, failing to tackle the directed case. In
this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a
framework integrating sheaf theory with a principled treatment of asymmetric
relations within a hypergraph. From it, we construct the Directed Sheaf
Hypergraph Laplacian, a complex-valued operator by which we unify and
generalize many existing Laplacian matrices proposed in the graph- and
hypergraph-learning literature. Across 7 real-world datasets and against 13
baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how
a principled treatment of directionality in hypergraphs, combined with the
expressive power of sheaves, can substantially improve performance.

</details>


### [466] [EVaR-Optimal Arm Identification in Bandits](https://arxiv.org/abs/2510.04728)
*Mehrasa Ahmadipour,Aurélien Garivier*

Main category: cs.LG

TL;DR: 论文研究了在Entropic Value-at-Risk (EVaR)准则下的多臂老虎机框架中的固定置信度最佳臂识别问题，提出了一种基于Track-and-Stop的算法，并证明了其渐近匹配的样本复杂度下界。


<details>
  <summary>Details</summary>
Motivation: 解决高风险环境中风险厌恶决策的需求，超越简单的期望值优化。

Method: 提出了一种基于Track-and-Stop的δ-正确算法，并推导了样本复杂度的下界。

Result: 算法在非参数设置下有效，样本复杂度下界被证明是渐近匹配的。

Conclusion: 研究为高风险环境中的风险厌恶决策提供了有效的解决方案，并通过算法和理论分析验证了其性能。

Abstract: We study the fixed-confidence best arm identification (BAI) problem within
the multi-armed bandit (MAB) framework under the Entropic Value-at-Risk (EVaR)
criterion. Our analysis considers a nonparametric setting, allowing for general
reward distributions bounded in [0,1]. This formulation addresses the critical
need for risk-averse decision-making in high-stakes environments, such as
finance, moving beyond simple expected value optimization. We propose a
$\delta$-correct, Track-and-Stop based algorithm and derive a corresponding
lower bound on the expected sample complexity, which we prove is asymptotically
matched. The implementation of our algorithm and the characterization of the
lower bound both require solving a complex convex optimization problem and a
related, simpler non-convex one.

</details>


### [467] [Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors](https://arxiv.org/abs/2510.04758)
*Zhiwei Han,Stefan Matthes,Hao Shen*

Main category: cs.LG

TL;DR: 非线性CCA在特定条件下能恢复真实潜在因子，需白化处理。


<details>
  <summary>Details</summary>
Motivation: 研究非线性CCA在恢复潜在因子时的可识别性条件。

Method: 通过重新参数化将分析从观测空间转换到源空间，证明岭正则化经验CCA的收敛性。

Result: 理论证明和实验验证表明白化处理对可识别性至关重要。

Conclusion: 非线性CCA在适当条件下可恢复潜在因子，白化是关键。

Abstract: In this work, we establish conditions under which nonlinear CCA recovers the
ground-truth latent factors up to an orthogonal transform after whitening.
Building on the classical result that linear mappings maximize canonical
correlations under Gaussian priors, we prove affine identifiability for a broad
class of latent distributions in the population setting. Central to our proof
is a reparameterization result that transports the analysis from observation
space to source space, where identifiability becomes tractable. We further show
that whitening is essential for ensuring boundedness and well-conditioning,
thereby underpinning identifiability. Beyond the population setting, we prove
that ridge-regularized empirical CCA converges to its population counterpart,
transferring these guarantees to the finite-sample regime. Experiments on a
controlled synthetic dataset and a rendered image dataset validate our theory
and demonstrate the necessity of its assumptions through systematic ablations.

</details>


### [468] [ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs](https://arxiv.org/abs/2510.04767)
*Wonjun Kang,Kevin Galim,Seunghyuk Oh,Minjae Lee,Yuchen Zeng,Shuibai Zhang,Coleman Hooper,Yuezhou Hu,Hyung Il Koo,Nam Ik Cho,Kangwook Lee*

Main category: cs.LG

TL;DR: 论文分析了扩散语言模型（dLLMs）并行解码的局限性，提出了ParallelBench基准测试，揭示了并行解码在现实任务中的质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 研究dLLMs并行解码的质量下降问题，现有评估不足以捕捉其影响。

Method: 信息论分析并行解码，案例研究合成列表操作，提出ParallelBench基准测试。

Result: dLLMs并行解码在现实任务中质量显著下降，现有策略难以平衡速度与质量。

Conclusion: 需要创新解码方法以克服速度与质量的权衡，ParallelBench有助于推动高效dLLMs发展。

Abstract: While most autoregressive LLMs are constrained to one-by-one decoding,
diffusion LLMs (dLLMs) have attracted growing interest for their potential to
dramatically accelerate inference through parallel decoding. Despite this
promise, the conditional independence assumption in dLLMs causes parallel
decoding to ignore token dependencies, inevitably degrading generation quality
when these dependencies are strong. However, existing works largely overlook
these inherent challenges, and evaluations on standard benchmarks (e.g., math
and coding) are not sufficient to capture the quality degradation caused by
parallel decoding. To address this gap, we first provide an
information-theoretic analysis of parallel decoding. We then conduct case
studies on analytically tractable synthetic list operations from both data
distribution and decoding strategy perspectives, offering quantitative insights
that highlight the fundamental limitations of parallel decoding. Building on
these insights, we propose ParallelBench, the first benchmark specifically
designed for dLLMs, featuring realistic tasks that are trivial for humans and
autoregressive LLMs yet exceptionally challenging for dLLMs under parallel
decoding. Using ParallelBench, we systematically analyze both dLLMs and
autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can
suffer dramatic quality degradation in real-world scenarios, and (ii) current
parallel decoding strategies struggle to adapt their degree of parallelism
based on task difficulty, thus failing to achieve meaningful speedup without
compromising quality. Our findings underscore the pressing need for innovative
decoding methods that can overcome the current speed-quality trade-off. We
release our benchmark to help accelerate the development of truly efficient
dLLMs.

</details>


### [469] [When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates](https://arxiv.org/abs/2510.04769)
*Michele Caprio,Siu Lun Chau,Krikamol Muandet*

Main category: cs.LG

TL;DR: 论文分析了在不确定性表示中迭代更新的收敛性问题，特别是在不精确概率机器学习中，展示了稳定固定点的存在条件。


<details>
  <summary>Details</summary>
Motivation: 研究不精确概率机器学习中迭代更新过程的收敛性，探讨稳定固定点的存在条件及其对学习动态的影响。

Method: 通过分析更新规则对置信集的影响，提出结构条件以确保收敛性，并以Credal Bayesian Deep Learning为例进行验证。

Result: 研究表明，引入不精确性不仅丰富了不确定性表示，还揭示了稳定性的结构条件。

Conclusion: 论文为不精确概率机器学习中的迭代学习动态提供了新的理论见解，展示了稳定固定点的存在条件。

Abstract: Many machine learning algorithms rely on iterative updates of uncertainty
representations, ranging from variational inference and
expectation-maximization, to reinforcement learning, continual learning, and
multi-agent learning. In the presence of imprecision and ambiguity, credal sets
-- closed, convex sets of probability distributions -- have emerged as a
popular framework for representing imprecise probabilistic beliefs. Under such
imprecision, many learning problems in imprecise probabilistic machine learning
(IPML) may be viewed as processes involving successive applications of update
rules on credal sets. This naturally raises the question of whether this
iterative process converges to stable fixed points -- or, more generally, under
what conditions on the updating mechanism such fixed points exist, and whether
they can be attained. We provide the first analysis of this problem and
illustrate our findings using Credal Bayesian Deep Learning as a concrete
example. Our work demonstrates that incorporating imprecision into the learning
process not only enriches the representation of uncertainty, but also reveals
structural conditions under which stability emerges, thereby offering new
insights into the dynamics of iterative learning under imprecision.

</details>


### [470] [Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning](https://arxiv.org/abs/2510.04773)
*Kai Qin,Jiaqi Wu,Jianxiang He,Haoyuan Sun,Yifei Zhao,Bin Liang,Yongzhe Chang,Tiantian Zhang,Houde Liu*

Main category: cs.LG

TL;DR: DiPO是一种新的LLM遗忘算法，通过直接操作下一个令牌概率分布，克服了NPO的局限性，实现了模型效用与遗忘质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能力的提升，数据隐私和安全问题日益突出，需要一种有效的遗忘方法。NPO等现有方法因缺乏显式正偏好信号而受限。

Method: 提出DiPO算法，通过选择性放大或抑制模型高置信度输出logits，构建偏好分布对，直接优化下一个令牌概率分布。

Result: DiPO在TOFU和MUSE基准测试中表现出色，实现了高遗忘质量和模型效用的平衡。

Conclusion: DiPO是一种高效、可扩展的LLM遗忘方法，解决了现有方法的局限性。

Abstract: As Large Language Models (LLMs) demonstrate remarkable capabilities learned
from vast corpora, concerns regarding data privacy and safety are receiving
increasing attention. LLM unlearning, which aims to remove the influence of
specific data while preserving overall model utility, is becoming an important
research area. One of the mainstream unlearning classes is optimization-based
methods, which achieve forgetting directly through fine-tuning, exemplified by
Negative Preference Optimization (NPO). However, NPO's effectiveness is limited
by its inherent lack of explicit positive preference signals. Attempts to
introduce such signals by constructing preferred responses often necessitate
domain-specific knowledge or well-designed prompts, fundamentally restricting
their generalizability. In this paper, we shift the focus to the
distribution-level, directly targeting the next-token probability distribution
instead of entire responses, and derive a novel unlearning algorithm termed
\textbf{Di}stribution \textbf{P}reference \textbf{O}ptimization (DiPO). We show
that the requisite preference distribution pairs for DiPO, which are
distributions over the model's output tokens, can be constructed by selectively
amplifying or suppressing the model's high-confidence output logits, thereby
effectively overcoming NPO's limitations. We theoretically prove the
consistency of DiPO's loss function with the desired unlearning direction.
Extensive experiments demonstrate that DiPO achieves a strong trade-off between
model utility and forget quality. Notably, DiPO attains the highest forget
quality on the TOFU benchmark, and maintains leading scalability and
sustainability in utility preservation on the MUSE benchmark.

</details>


### [471] [MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis](https://arxiv.org/abs/2510.04776)
*Ebenezer Awotoro,Chisom Ezekannagha,Florian Schwarz,Johannes Tauscher,Dominik Heider,Katharina Ladewig,Christel Le Bon,Karine Moncoq,Bruno Miroux,Georges Hattab*

Main category: cs.LG

TL;DR: MetaMP是一个整合膜蛋白数据库的框架，通过机器学习和用户友好界面提升数据质量，支持分类和异常检测，并在实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 膜蛋白结构的复杂性和数据库的不一致性需要更好的整合工具。

Method: MetaMP通过统一数据库、丰富元数据、提供交互视图和机器学习分类来解决这些问题。

Result: MetaMP解决了77%的数据不一致性，并在新膜蛋白分类中达到98%的准确率。

Conclusion: MetaMP是一个强大的资源，支持AI驱动的膜蛋白研究。

Abstract: Structural biology has made significant progress in determining membrane
proteins, leading to a remarkable increase in the number of available
structures in dedicated databases. The inherent complexity of membrane protein
structures, coupled with challenges such as missing data, inconsistencies, and
computational barriers from disparate sources, underscores the need for
improved database integration. To address this gap, we present MetaMP, a
framework that unifies membrane-protein databases within a web application and
uses machine learning for classification. MetaMP improves data quality by
enriching metadata, offering a user-friendly interface, and providing eight
interactive views for streamlined exploration. MetaMP was effective across
tasks of varying difficulty, demonstrating advantages across different levels
without compromising speed or accuracy, according to user evaluations.
Moreover, MetaMP supports essential functions such as structure classification
and outlier detection.
  We present three practical applications of Artificial Intelligence (AI) in
membrane protein research: predicting transmembrane segments, reconciling
legacy databases, and classifying structures with explainable AI support. In a
validation focused on statistics, MetaMP resolved 77% of data discrepancies and
accurately predicted the class of newly identified membrane proteins 98% of the
time and overtook expert curation. Altogether, MetaMP is a much-needed resource
that harmonizes current knowledge and empowers AI-driven exploration of
membrane-protein architecture.

</details>


### [472] [Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning](https://arxiv.org/abs/2510.04786)
*Jonas Hübotter,Leander Diaz-Bone,Ido Hakimi,Andreas Krause,Moritz Hardt*

Main category: cs.LG

TL;DR: 提出了一种名为TTC-RL的测试时课程学习代理，通过强化学习自动选择任务相关数据，显著提升模型在目标任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 探索模型是否能在测试时像人类一样通过任务相关数据持续学习，避免人工数据筛选的耗时过程。

Method: 使用强化学习构建任务特定课程（TTC-RL），自动从大量数据中选择最相关的训练数据。

Result: 在数学和编程基准测试中，TTC-RL显著提升了模型性能，如Qwen3-8B的pass@1在AIME25上提升1.8倍，CodeElo上提升2.1倍。

Conclusion: 测试时课程学习展示了在测试时通过持续学习任务相关数据提升模型性能的潜力。

Abstract: Humans are good at learning on the job: We learn how to solve the tasks we
face as we go along. Can a model do the same? We propose an agent that
assembles a task-specific curriculum, called test-time curriculum (TTC-RL), and
applies reinforcement learning to continue training the model for its target
task. The test-time curriculum avoids time-consuming human curation of datasets
by automatically selecting the most task-relevant data from a large pool of
available training data. Our experiments demonstrate that reinforcement
learning on a test-time curriculum consistently improves the model on its
target tasks, across a variety of evaluations and models. Notably, on
challenging math and coding benchmarks, TTC-RL improves the pass@1 of Qwen3-8B
by approximately 1.8x on AIME25 and 2.1x on CodeElo. Moreover, we find that
TTC-RL significantly raises the performance ceiling compared to the initial
model, increasing pass@8 on AIME25 from 40% to 62% and on CodeElo from 28% to
43%. Our findings show the potential of test-time curricula in extending the
test-time scaling paradigm to continual training on thousands of task-relevant
experiences during test-time.

</details>


### [473] [On Predicting Post-Click Conversion Rate via Counterfactual Inference](https://arxiv.org/abs/2510.04816)
*Junhyung Ahn,Sanghack Lee*

Main category: cs.LG

TL;DR: ESCIM利用因果推理为未点击样本生成反事实转换标签，提升CVR预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有CVR预测模型仅依赖点击样本，数据稀疏且存在偏差，需利用未点击样本改进。

Method: 通过结构因果模型（SCM）进行反事实推理，生成未点击样本的转换标签并用于训练。

Result: 实验证明ESCIM在公开数据集和在线A/B测试中表现优越，泛化能力强。

Conclusion: ESCIM通过因果推理有效解决了数据稀疏和偏差问题，显著提升了CVR预测性能。

Abstract: Accurately predicting conversion rate (CVR) is essential in various
recommendation domains such as online advertising systems and e-commerce. These
systems utilize user interaction logs, which consist of exposures, clicks, and
conversions. CVR prediction models are typically trained solely based on
clicked samples, as conversions can only be determined following clicks.
However, the sparsity of clicked instances necessitates the collection of a
substantial amount of logs for effective model training. Recent works address
this issue by devising frameworks that leverage non-clicked samples. While
these frameworks aim to reduce biases caused by the discrepancy between clicked
and non-clicked samples, they often rely on heuristics. Against this
background, we propose a method to counterfactually generate conversion labels
for non-clicked samples by using causality as a guiding principle, attempting
to answer the question, "Would the user have converted if he or she had clicked
the recommended item?" Our approach is named the Entire Space Counterfactual
Inference Multi-task Model (ESCIM). We initially train a structural causal
model (SCM) of user sequential behaviors and conduct a hypothetical
intervention (i.e., click) on non-clicked items to infer counterfactual CVRs.
We then introduce several approaches to transform predicted counterfactual CVRs
into binary counterfactual conversion labels for the non-clicked samples.
Finally, the generated samples are incorporated into the training process.
Extensive experiments on public datasets illustrate the superiority of the
proposed algorithm. Online A/B testing further empirically validates the
effectiveness of our proposed algorithm in real-world scenarios. In addition,
we demonstrate the improved performance of the proposed method on latent
conversion data, showcasing its robustness and superior generalization
capabilities.

</details>


### [474] [On the Hardness of Learning Regular Expressions](https://arxiv.org/abs/2510.04834)
*Idan Attias,Lev Reyzin,Nathan Srebro,Gal Vardi*

Main category: cs.LG

TL;DR: 学习正则表达式的计算复杂性在PAC模型和成员查询中被证明是困难的，尤其是在超立方体均匀分布下。扩展正则表达式（如补集或交集）进一步增加了学习难度。


<details>
  <summary>Details</summary>
Motivation: 尽管正则表达式在理论和实践中广泛应用，但其学习复杂性尚未被充分探索。

Method: 研究在PAC模型和成员查询下学习正则表达式的计算复杂性，包括均匀分布和分布无关情况。

Result: 证明了在PAC模型和成员查询下学习正则表达式是困难的，尤其是扩展正则表达式（如补集或交集）时。

Conclusion: 正则表达式的学习复杂性与其描述形式（如DFA、NFA）不同，扩展操作会显著增加学习难度。

Abstract: Despite the theoretical significance and wide practical use of regular
expressions, the computational complexity of learning them has been largely
unexplored. We study the computational hardness of improperly learning regular
expressions in the PAC model and with membership queries. We show that PAC
learning is hard even under the uniform distribution on the hypercube, and also
prove hardness of distribution-free learning with membership queries.
Furthermore, if regular expressions are extended with complement or
intersection, we establish hardness of learning with membership queries even
under the uniform distribution. We emphasize that these results do not follow
from existing hardness results for learning DFAs or NFAs, since the descriptive
complexity of regular languages can differ exponentially between DFAs, NFAs,
and regular expressions.

</details>


### [475] [Bond-Centered Molecular Fingerprint Derivatives: A BBBP Dataset Study](https://arxiv.org/abs/2510.04837)
*Guillaume Godin*

Main category: cs.LG

TL;DR: BCFP是一种基于键的指纹方法，与ECFP互补，结合两者可提升BBBP分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 探索键中心指纹（BCFP）作为原子中心指纹（ECFP）的补充，以提升血脑屏障穿透（BBBP）预测的性能。

Method: 提出静态BCFP，结合快速随机森林模型，并与ECFP拼接；进一步提出BCFP-Sort&Slice特征组合方案。

Result: 拼接ECFP与BCFP显著提升AUROC和AUPRC；r=1效果最佳；BCFP-Sort&Slice保留OOV信息且紧凑。

Conclusion: 轻量级键中心描述符可补充原子中心指纹，为BBBP预测提供高效基线。

Abstract: Bond Centered FingerPrint (BCFP) are a complementary, bond-centric
alternative to Extended-Connectivity Fingerprints (ECFP). We introduce a static
BCFP that mirrors the bond-convolution used by directed message-passing GNNs
like ChemProp, and evaluate it with a fast rapid Random Forest model on
Brain-Blood Barrier Penetration (BBBP) classification task. Across stratified
cross-validation, concatenating ECFP with BCFP consistently improves AUROC and
AUPRC over either descriptor alone, as confirmed by Turkey HSD
multiple-comparison analysis. Among radii, r = 1 performs best; r = 2 does not
yield statistically separable gains under the same test. We further propose
BCFP-Sort&Slice, a simple feature-combination scheme that preserves the
out-of-vocabulary (OOV) count information native to ECFP count vectors while
enabling compact unhashed concatenation of BCFP variants. We also outperform
the MGTP prediction on our BBBP evaluation, using such composite new features
bond and atom features. These results show that lightweight, bond-centered
descriptors can complement atom-centered circular fingerprints and provide
strong, fast baselines for BBBP prediction.

</details>


### [476] [Distributionally Robust Causal Abstractions](https://arxiv.org/abs/2510.04842)
*Yorgos Felekis,Theodoros Damoulas,Paris Giampouras*

Main category: cs.LG

TL;DR: 提出了首个分布鲁棒的因果抽象（CA）学习框架，通过Wasserstein模糊集解决环境变化和模型误设问题。


<details>
  <summary>Details</summary>
Motivation: 现有CA学习方法假设外生分布固定且明确，易受环境变化和误设影响。

Method: 将鲁棒CA学习建模为带Wasserstein模糊集的约束极小极大优化问题。

Result: 理论和实验证明框架对环境变化、模型和干预映射误设具有鲁棒性。

Conclusion: 分布鲁棒CA框架提升了模型在复杂环境中的适应性和可靠性。

Abstract: Causal Abstraction (CA) theory provides a principled framework for relating
causal models that describe the same system at different levels of granularity
while ensuring interventional consistency between them. Recently, several
approaches for learning CAs have been proposed, but all assume fixed and
well-specified exogenous distributions, making them vulnerable to environmental
shifts and misspecification. In this work, we address these limitations by
introducing the first class of distributionally robust CAs and their associated
learning algorithms. The latter cast robust causal abstraction learning as a
constrained min-max optimization problem with Wasserstein ambiguity sets. We
provide theoretical results, for both empirical and Gaussian environments,
leading to principled selection of the level of robustness via the radius of
these sets. Furthermore, we present empirical evidence across different
problems and CA learning methods, demonstrating our framework's robustness not
only to environmental shifts but also to structural model and intervention
mapping misspecification.

</details>


### [477] [Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders](https://arxiv.org/abs/2510.04855)
*Junqi Jiang,Francesco Leofante,Antonio Rago,Francesca Toni*

Main category: cs.LG

TL;DR: 提出了一种新的生成框架LAPACE，通过L-GMVAE学习结构化潜在空间，生成鲁棒且多样的反事实解释路径。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以统一满足反事实解释的多方面需求（如鲁棒性、合理性和多样性）。

Method: 结合L-GMVAE和LAPACE算法，通过潜在空间插值生成反事实路径。

Result: LAPACE在八个量化指标上表现优异，计算高效。

Conclusion: LAPACE提供了一种模型无关的、鲁棒且多样化的反事实解释生成方法。

Abstract: Counterfactual explanations (CEs) provide recourse recommendations for
individuals affected by algorithmic decisions. A key challenge is generating
CEs that are robust against various perturbation types (e.g. input and model
perturbations) while simultaneously satisfying other desirable properties.
These include plausibility, ensuring CEs reside on the data manifold, and
diversity, providing multiple distinct recourse options for single inputs.
Existing methods, however, mostly struggle to address these multifaceted
requirements in a unified, model-agnostic manner. We address these limitations
by proposing a novel generative framework. First, we introduce the
Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model
trained to learn a structured latent space where each class label is
represented by a set of Gaussian components with diverse, prototypical
centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual
Explanations), a model-agnostic algorithm that synthesises entire paths of CE
points by interpolating from inputs' latent representations to those learned
latent centroids. This approach inherently ensures robustness to input changes,
as all paths for a given target class converge to the same fixed centroids.
Furthermore, the generated paths provide a spectrum of recourse options,
allowing users to navigate the trade-off between proximity and plausibility
while also encouraging robustness against model changes. In addition,
user-specified actionability constraints can also be easily incorporated via
lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive
experiments show that LAPACE is computationally efficient and achieves
competitive performance across eight quantitative metrics.

</details>


### [478] [Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails](https://arxiv.org/abs/2510.04860)
*Siwei Han,Jiaqi Liu,Yaofeng Su,Wenbo Duan,Xinyuan Liu,Cihang Xie,Mohit Bansal,Mingyu Ding,Linjun Zhang,Huaxiu Yao*

Main category: cs.LG

TL;DR: 论文研究了自进化大型语言模型（LLM）代理在长期部署中的对齐失效问题，提出了对齐倾斜过程（ATP）的概念，并通过实验验证了其存在和影响。


<details>
  <summary>Details</summary>
Motivation: 自进化LLM代理在长期交互中可能逐渐放弃训练时的对齐约束，转而采用自我强化的策略，导致长期可靠性问题。

Method: 通过自利探索和模仿策略扩散两种范式，构建可控测试环境，并测试Qwen3-8B和Llama-3.1-8B-Instruct模型。

Result: 实验表明，对齐效果在自进化中迅速退化，多代理环境下违规行为快速扩散，现有强化学习方法防御脆弱。

Conclusion: LLM代理的对齐是动态且脆弱的，易受反馈驱动的影响而失效。

Abstract: As Large Language Model (LLM) agents increasingly gain self-evolutionary
capabilities to adapt and refine their strategies through real-world
interaction, their long-term reliability becomes a critical concern. We
identify the Alignment Tipping Process (ATP), a critical post-deployment risk
unique to self-evolving LLM agents. Unlike training-time failures, ATP arises
when continual interaction drives agents to abandon alignment constraints
established during training in favor of reinforced, self-interested strategies.
We formalize and analyze ATP through two complementary paradigms:
Self-Interested Exploration, where repeated high-reward deviations induce
individual behavioral drift, and Imitative Strategy Diffusion, where deviant
behaviors spread across multi-agent systems. Building on these paradigms, we
construct controllable testbeds and benchmark Qwen3-8B and
Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode
rapidly under self-evolution, with initially aligned models converging toward
unaligned states. In multi-agent settings, successful violations diffuse
quickly, leading to collective misalignment. Moreover, current reinforcement
learning-based alignment methods provide only fragile defenses against
alignment tipping. Together, these findings demonstrate that alignment of LLM
agents is not a static property but a fragile and dynamic one, vulnerable to
feedback-driven decay during deployment. Our data and code are available at
https://github.com/aiming-lab/ATP.

</details>


### [479] [A Clinical-grade Universal Foundation Model for Intraoperative Pathology](https://arxiv.org/abs/2510.04861)
*Zihan Zhao,Fengtao Zhou,Ronggang Li,Bing Chu,Xinke Zhang,Xueyi Zheng,Ke Zheng,Xiaobo Wen,Jiabo Ma,Yihui Wang,Jiewei Chen,Chengyou Zheng,Jiangyu Zhang,Yongqin Wen,Jiajia Meng,Ziqi Zeng,Xiaoqing Li,Jing Li,Dan Xie,Yaping Ye,Yu Wang,Hao Chen,Muyan Cai*

Main category: cs.LG

TL;DR: CRISP是一种基于10万张冰冻切片的临床级基础模型，用于术中病理支持，在多种任务中表现优异，显著提升诊断效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 术中病理诊断复杂且高质量数据有限，阻碍了计算病理学的临床应用。

Method: 开发CRISP模型，基于大规模冰冻切片数据，并在15,000张切片上进行多任务评估。

Result: CRISP在真实世界数据中表现优异，诊断准确率高，减少35%工作量，并提升微转移检测。

Conclusion: CRISP是AI驱动术中病理的临床级范式，推动AI在临床实践中的应用。

Abstract: Intraoperative pathology is pivotal to precision surgery, yet its clinical
impact is constrained by diagnostic complexity and the limited availability of
high-quality frozen-section data. While computational pathology has made
significant strides, the lack of large-scale, prospective validation has
impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a
clinical-grade foundation model developed on over 100,000 frozen sections from
eight medical centers, specifically designed to provide Clinical-grade Robust
Intraoperative Support for Pathology (CRISP). CRISP was comprehensively
evaluated on more than 15,000 intraoperative slides across nearly 100
retrospective diagnostic tasks, including benign-malignant discrimination, key
intraoperative decision-making, and pan-cancer detection, etc. The model
demonstrated robust generalization across diverse institutions, tumor types,
and anatomical sites-including previously unseen sites and rare cancers. In a
prospective cohort of over 2,000 patients, CRISP sustained high diagnostic
accuracy under real-world conditions, directly informing surgical decisions in
92.6% of cases. Human-AI collaboration further reduced diagnostic workload by
35%, avoided 105 ancillary tests and enhanced detection of micrometastases with
87.5% accuracy. Together, these findings position CRISP as a clinical-grade
paradigm for AI-driven intraoperative pathology, bridging computational
advances with surgical precision and accelerating the translation of artificial
intelligence into routine clinical practice.

</details>


### [480] [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871)
*Alexia Jolicoeur-Martineau*

Main category: cs.LG

TL;DR: HRM是一种新颖的递归神经网络方法，在少量数据和参数下优于大语言模型，但TRM进一步简化并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何用小网络解决复杂问题，如HRM表现优异但可能非最优，因此提出更简单的TRM。

Method: HRM使用两个小网络递归；TRM仅用单层小网络递归，参数更少。

Result: TRM在ARC-AGI任务上表现优于HRM和多数大语言模型，参数仅7M。

Conclusion: TRM展示了小网络在复杂任务中的潜力，为高效推理提供了新方向。

Abstract: Hierarchical Reasoning Model (HRM) is a novel approach using two small neural
networks recursing at different frequencies. This biologically inspired method
beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze,
and ARC-AGI while trained with small models (27M parameters) on small data
(around 1000 examples). HRM holds great promise for solving hard problems with
small networks, but it is not yet well understood and may be suboptimal. We
propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach
that achieves significantly higher generalization than HRM, while using a
single tiny network with only 2 layers. With only 7M parameters, TRM obtains
45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs
(e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the
parameters.

</details>


### [481] [Flow-Matching Based Refiner for Molecular Conformer Generation](https://arxiv.org/abs/2510.04878)
*Xiangyang Xu,Hongyang Gao*

Main category: cs.LG

TL;DR: 提出了一种用于低能量分子构象生成的流匹配细化方法，通过绕过低信噪比阶段提高样本质量。


<details>
  <summary>Details</summary>
Motivation: 低能量分子构象生成是药物发现中的基础但具有挑战性的问题，现有去噪方法在采样过程中容易积累误差。

Method: 使用流匹配细化器，从上游去噪模型的混合质量输出初始化采样，并重新调度噪声尺度以绕过低信噪比阶段。

Result: 在GEOM-QM9和GEOM-Drugs基准数据集上，生成器-细化器管道以更少的总去噪步骤提高了质量并保持了多样性。

Conclusion: 该方法有效解决了去噪方法在低信噪比阶段的训练困难问题，提升了分子构象生成的效率和质量。

Abstract: Low-energy molecular conformers generation (MCG) is a foundational yet
challenging problem in drug discovery. Denoising-based methods include
diffusion and flow-matching methods that learn mappings from a simple base
distribution to the molecular conformer distribution. However, these approaches
often suffer from error accumulation during sampling, especially in the low SNR
steps, which are hard to train. To address these challenges, we propose a
flow-matching refiner for the MCG task. The proposed method initializes
sampling from mixed-quality outputs produced by upstream denoising models and
reschedules the noise scale to bypass the low-SNR phase, thereby improving
sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the
generator-refiner pipeline improves quality with fewer total denoising steps
while preserving diversity.

</details>


### [482] [Revealing Interconnections between Diseases: from Statistical Methods to Large Language Models](https://arxiv.org/abs/2510.04888)
*Alina Ermilova,Dmitrii Kornilov,Sofia Samoilova,Ekaterina Laptenkova,Anastasia Kolesnikova,Ekaterina Podplutova,Senotrusova Sofya,Maksim G. Sharaev*

Main category: cs.LG

TL;DR: 论文通过系统评估七种方法，发现LLMs在发现疾病关联方面潜力有限，并提出了一个医疗疾病本体作为未来研究的基础。


<details>
  <summary>Details</summary>
Motivation: 手动分析大规模临床数据识别疾病关联效率低且主观，机器学习虽有望解决但面临方法选择、数据源可靠性和缺乏真实数据等挑战。

Method: 评估了七种方法，包括统计共现分析、MLM、领域特定BERT、通用BERT、文档检索和四种LLMs，基于ICD-10代码和文本描述。

Result: LLM方法产生的疾病关联多样性最低，表明其在发现新关联方面潜力有限。

Conclusion: 研究结果为缺乏真实数据的医疗关联提供了一个有价值的疾病本体，可作为未来临床研究和AI应用的基础。

Abstract: Identifying disease interconnections through manual analysis of large-scale
clinical data is labor-intensive, subjective, and prone to expert disagreement.
While machine learning (ML) shows promise, three critical challenges remain:
(1) selecting optimal methods from the vast ML landscape, (2) determining
whether real-world clinical data (e.g., electronic health records, EHRs) or
structured disease descriptions yield more reliable insights, (3) the lack of
"ground truth," as some disease interconnections remain unexplored in medicine.
Large language models (LLMs) demonstrate broad utility, yet they often lack
specialized medical knowledge. To address these gaps, we conduct a systematic
evaluation of seven approaches for uncovering disease relationships based on
two data sources: (i) sequences of ICD-10 codes from MIMIC-IV EHRs and (ii) the
full set of ICD-10 codes, both with and without textual descriptions. Our
framework integrates the following: (i) a statistical co-occurrence analysis
and a masked language modeling (MLM) approach using real clinical data; (ii)
domain-specific BERT variants (Med-BERT and BioClinicalBERT); (iii) a
general-purpose BERT and document retrieval; and (iv) four LLMs (Mistral,
DeepSeek, Qwen, and YandexGPT). Our graph-based comparison of the obtained
interconnection matrices shows that the LLM-based approach produces
interconnections with the lowest diversity of ICD code connections to different
diseases compared to other methods, including text-based and domain-based
approaches. This suggests an important implication: LLMs have limited potential
for discovering new interconnections. In the absence of ground truth databases
for medical interconnections between ICD codes, our results constitute a
valuable medical disease ontology that can serve as a foundational resource for
future clinical research and artificial intelligence applications in
healthcare.

</details>


### [483] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出基于模拟的评估框架，系统评估多变量长期时间序列预测模型的鲁棒性，揭示不同模型在信号和噪声条件下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 现有评估多依赖真实数据集，噪声特性未知，难以系统分析模型鲁棒性。

Method: 生成可配置的合成数据集，模拟真实多变量时间序列，涵盖信号成分、噪声类型、信噪比和频率特性。

Result: 发现模型在季节性模式不完整时表现下降，不同模型对信号和噪声的敏感性各异，S-Mamba和iTransformer频率重构最佳。

Conclusion: 合成测试框架为模型选择提供依据，揭示了模型在特定信号和噪声条件下的优势和局限。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [484] [Focused Skill Discovery: Learning to Control Specific State Variables while Minimizing Side Effects](https://arxiv.org/abs/2510.04901)
*Jonathan Colaço Carr,Qinyi Sun,Cameron Allen*

Main category: cs.LG

TL;DR: 论文提出了一种新方法，通过学习针对特定状态变量的技能，提升强化学习中的探索效率和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有技能发现算法常忽略自然状态变量，导致技能缺乏对特定状态的控制，影响探索效率和下游任务。

Method: 引入一种通用方法，使技能发现算法能够学习针对特定状态变量的技能。

Result: 方法将状态空间覆盖率提升三倍，解锁新学习能力，并自动避免下游任务中的负面影响。

Conclusion: 该方法显著提升了技能发现的效果，尤其在探索效率和任务适应性方面表现突出。

Abstract: Skills are essential for unlocking higher levels of problem solving. A common
approach to discovering these skills is to learn ones that reliably reach
different states, thus empowering the agent to control its environment.
However, existing skill discovery algorithms often overlook the natural state
variables present in many reinforcement learning problems, meaning that the
discovered skills lack control of specific state variables. This can
significantly hamper exploration efficiency, make skills more challenging to
learn with, and lead to negative side effects in downstream tasks when the goal
is under-specified. We introduce a general method that enables these skill
discovery algorithms to learn focused skills -- skills that target and control
specific state variables. Our approach improves state space coverage by a
factor of three, unlocks new learning capabilities, and automatically avoids
negative side effects in downstream tasks.

</details>


### [485] [DP-HYPE: Distributed Differentially Private Hyperparameter Search](https://arxiv.org/abs/2510.04902)
*Johannes Liebenow,Thorsten Peinemann,Esfandiar Mohammadi*

Main category: cs.LG

TL;DR: DP-HYPE是一种分布式隐私保护超参数搜索算法，通过客户端本地评估进行投票，实现多数支持的妥协，同时保证差分隐私。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习中，超参数调优对模型性能至关重要，但涉及敏感数据时隐私成为挑战。现有方法存在计算成本高、隐私-效用权衡不佳等问题。

Method: 提出DP-HYPE算法，基于客户端本地超参数评估进行分布式投票，选择多数支持的妥协方案，并保证客户端级差分隐私。

Result: DP-HYPE在多种数据集（iid和非iid）上表现高效，即使隐私预算较小也能保持高效用。

Conclusion: DP-HYPE是一种可扩展、任务无关的隐私保护超参数调优方法，适用于分布式机器学习。

Abstract: The tuning of hyperparameters in distributed machine learning can
substantially impact model performance. When the hyperparameters are tuned on
sensitive data, privacy becomes an important challenge and to this end,
differential privacy has emerged as the de facto standard for provable privacy.
A standard setting when performing distributed learning tasks is that clients
agree on a shared setup, i.e., find a compromise from a set of hyperparameters,
like the learning rate of the model to be trained. Yet, prior work on
differentially private hyperparameter tuning either uses computationally
expensive cryptographic protocols, determines hyperparameters separately for
each client, or applies differential privacy locally, which can lead to
undesirable utility-privacy trade-offs.
  In this work, we present our algorithm DP-HYPE, which performs a distributed
and privacy-preserving hyperparameter search by conducting a distributed voting
based on local hyperparameter evaluations of clients. In this way, DP-HYPE
selects hyperparameters that lead to a compromise supported by the majority of
clients, while maintaining scalability and independence from specific learning
tasks. We prove that DP-HYPE preserves the strong notion of differential
privacy called client-level differential privacy and, importantly, show that
its privacy guarantees do not depend on the number of hyperparameters. We also
provide bounds on its utility guarantees, that is, the probability of reaching
a compromise, and implement DP-HYPE as a submodule in the popular Flower
framework for distributed machine learning. In addition, we evaluate
performance on multiple benchmark data sets in iid as well as multiple non-iid
settings and demonstrate high utility of DP-HYPE even under small privacy
budgets.

</details>


### [486] [How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning](https://arxiv.org/abs/2510.04908)
*Haotian Gao,Zheng Dong,Jiawei Yong,Shintaro Fukushima,Kenjiro Taura,Renhe Jiang*

Main category: cs.LG

TL;DR: ST-SSDL是一种时空时间序列预测框架，通过自监督偏差学习捕捉动态偏差，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑当前输入与历史模式间的动态偏差，这些偏差对模型性能至关重要。

Method: ST-SSDL通过可学习原型离散化潜在空间，并引入对比损失和偏差损失优化结构。

Result: 在六个基准数据集上，ST-SSDL在多个指标上优于现有方法。

Conclusion: ST-SSDL能自适应复杂时空场景中的偏差，显著提升预测性能。

Abstract: Spatio-temporal forecasting is essential for real-world applications such as
traffic management and urban computing. Although recent methods have shown
improved accuracy, they often fail to account for dynamic deviations between
current inputs and historical patterns. These deviations contain critical
signals that can significantly affect model performance. To fill this gap, we
propose ST-SSDL, a Spatio-Temporal time series forecasting framework that
incorporates a Self-Supervised Deviation Learning scheme to capture and utilize
such deviations. ST-SSDL anchors each input to its historical average and
discretizes the latent space using learnable prototypes that represent typical
spatio-temporal patterns. Two auxiliary objectives are proposed to refine this
structure: a contrastive loss that enhances inter-prototype discriminability
and a deviation loss that regularizes the distance consistency between input
representations and corresponding prototypes to quantify deviation. Optimized
jointly with the forecasting objective, these components guide the model to
organize its hidden space and improve generalization across diverse input
conditions. Experiments on six benchmark datasets show that ST-SSDL
consistently outperforms state-of-the-art baselines across multiple metrics.
Visualizations further demonstrate its ability to adaptively respond to varying
levels of deviation in complex spatio-temporal scenarios. Our code and datasets
are available at https://github.com/Jimmy-7664/ST-SSDL.

</details>


### [487] [Glocal Information Bottleneck for Time Series Imputation](https://arxiv.org/abs/2510.04910)
*Jie Yang,Kexin Zhang,Guibin Zhang,Philip S. Yu,Kaize Ding*

Main category: cs.LG

TL;DR: Glocal-IB提出了一种新的训练范式，通过全局对齐损失解决高缺失率下时间序列插值的优化困境。


<details>
  <summary>Details</summary>
Motivation: 现有模型在高缺失率下容易过拟合局部噪声，无法捕捉数据的全局信息。

Method: 提出了Glocal-IB，通过全局对齐损失对齐掩码输入与原始观测的潜在表示。

Result: 在九个数据集上的实验表明，Glocal-IB显著提升了性能并保持了潜在表示的一致性。

Conclusion: Glocal-IB有效解决了高缺失率下的优化问题，提升了模型的泛化能力。

Abstract: Time Series Imputation (TSI), which aims to recover missing values in
temporal data, remains a fundamental challenge due to the complex and often
high-rate missingness in real-world scenarios. Existing models typically
optimize the point-wise reconstruction loss, focusing on recovering numerical
values (local information). However, we observe that under high missing rates,
these models still perform well in the training phase yet produce poor
imputations and distorted latent representation distributions (global
information) in the inference phase. This reveals a critical optimization
dilemma: current objectives lack global guidance, leading models to overfit
local noise and fail to capture global information of the data. To address this
issue, we propose a new training paradigm, Glocal Information Bottleneck
(Glocal-IB). Glocal-IB is model-agnostic and extends the standard IB framework
by introducing a Global Alignment loss, derived from a tractable mutual
information approximation. This loss aligns the latent representations of
masked inputs with those of their originally observed counterparts. It helps
the model retain global structure and local details while suppressing noise
caused by missing values, giving rise to better generalization under high
missingness. Extensive experiments on nine datasets confirm that Glocal-IB
leads to consistently improved performance and aligned latent representations
under missingness. Our code implementation is available in
https://github.com/Muyiiiii/NeurIPS-25-Glocal-IB.

</details>


### [488] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedSSL-AMC通过自监督学习和联邦学习解决AMC中的隐私和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 解决集中式AMC训练中的隐私问题、通信开销和鲁棒性不足，以及联邦学习中的类别不平衡和非独立同分布问题。

Method: 使用因果时间膨胀CNN和三元组自监督学习在客户端上训练，再结合小标记集的SVM分类器。

Result: 在异构SNR、载波频率偏移和非独立同分布标签分区下，性能优于监督联邦学习基线。

Conclusion: FedSSL-AMC在隐私保护和鲁棒性方面表现优异，适用于实际通信场景。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [489] [Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking](https://arxiv.org/abs/2510.04930)
*Ali Saheb Pasand,Elvis Dohmatob*

Main category: cs.LG

TL;DR: 论文提出了一种称为平等梯度下降（EGD）的方法，通过归一化梯度来加速模型的学习过程，消除停滞现象。


<details>
  <summary>Details</summary>
Motivation: 研究模型在训练过程中出现的测试性能停滞现象（grokking），并提出方法加速学习过程。

Method: 通过归一化梯度，使所有主方向的动态变化速度一致，提出EGD方法。

Result: EGD显著加速了学习过程，在某些情况下完全消除了停滞现象。

Conclusion: EGD是一种有效的优化方法，能够显著改善模型的学习效率。

Abstract: Grokking is the phenomenon whereby, unlike the training performance, which
peaks early in the training process, the test/generalization performance of a
model stagnates over arbitrarily many epochs and then suddenly jumps to usually
close to perfect levels. In practice, it is desirable to reduce the length of
such plateaus, that is to make the learning process "grok" faster. In this
work, we provide new insights into grokking. First, we show both empirically
and theoretically that grokking can be induced by asymmetric speeds of
(stochastic) gradient descent, along different principal (i.e singular
directions) of the gradients. We then propose a simple modification that
normalizes the gradients so that dynamics along all the principal directions
evolves at exactly the same speed. Then, we establish that this modified
method, which we call egalitarian gradient descent (EGD) and can be seen as a
carefully modified form of natural gradient descent, groks much faster. In
fact, in some cases the stagnation is completely removed. Finally, we
empirically show that on classical arithmetic problems such as modular addition
and sparse parity problem which this stagnation has been widely observed and
intensively studied, that our proposed method eliminates the plateaus.

</details>


### [490] [ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures](https://arxiv.org/abs/2510.04938)
*Shiwen Qin,Alexander Auras,Shay B. Cohen,Elliot J. Crowley,Michael Moeller,Linus Ericsson,Jovita Lukasik*

Main category: cs.LG

TL;DR: ONNX-Bench是一个基于ONNX格式的神经网络基准测试，包含超过60万对{架构,准确率}数据，支持跨搜索空间的通用性能预测。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经架构搜索（NAS）方法在性能评估上的高成本和搜索空间限制问题。

Method: 提出ONNX-Bench基准测试和ONNX-Net文本编码方法，支持任意神经网络架构的自然语言描述和性能预测。

Result: 实验显示，该方法在少量预训练样本下具有强大的零样本性能，能即时评估任意神经网络架构。

Conclusion: ONNX-Bench和ONNX-Net为神经架构搜索提供了灵活、可扩展的解决方案。

Abstract: Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

</details>


### [491] [Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints](https://arxiv.org/abs/2510.04951)
*Jayanta Mandi,Marianne Defresne,Senne Berden,Tias Guns*

Main category: cs.LG

TL;DR: 提出了一种决策聚焦学习（DFL）框架，用于预测约束优化问题（COP）中的参数，并通过两种新的损失函数平衡可行性和决策质量。


<details>
  <summary>Details</summary>
Motivation: 解决在预测约束参数时可能导致不可行解的问题，同时优化决策质量。

Method: 基于最大似然估计（MLE）开发了两种损失函数，分别惩罚不可行性和次优决策，并通过可调参数平衡两者。

Result: 实验表明，调整参数可以控制可行性和次优性之间的权衡，且在某些情况下性能与现有基线相当。

Conclusion: 提出的框架为决策者提供了灵活的工具，以在可行性和决策质量之间取得平衡。

Abstract: When some parameters of a constrained optimization problem (COP) are
uncertain, this gives rise to a predict-then-optimize (PtO) problem, comprising
two stages -- the prediction of the unknown parameters from contextual
information and the subsequent optimization using those predicted parameters.
Decision-focused learning (DFL) implements the first stage by training a
machine learning (ML) model to optimize the quality of the decisions made using
the predicted parameters. When parameters in the constraints of a COP are
predicted, the predicted parameters can lead to infeasible solutions.
Therefore, it is important to simultaneously manage both feasibility and
decision quality. We develop a DFL framework for predicting constraint
parameters in a generic COP. While prior works typically assume that the
underlying optimization problem is a linear program (LP) or integer linear
program (ILP), our approach makes no such assumption. We derive two novel loss
functions based on maximum likelihood estimation (MLE): the first one penalizes
infeasibility (by penalizing when the predicted parameters lead to infeasible
solutions), and the second one penalizes suboptimal decisions (by penalizing
when the true optimal solution is infeasible under the predicted parameters).
We introduce a single tunable parameter to form a weighted average of the two
losses, allowing decision-makers to balance suboptimality and feasibility. We
experimentally demonstrate that adjusting this parameter provides a
decision-maker the control over the trade-off between the two. Moreover, across
several COP instances, we find that for a single value of the tunable
parameter, our method matches the performance of the existing baselines on
suboptimality and feasibility.

</details>


### [492] [StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R](https://arxiv.org/abs/2510.04974)
*Allen Daniel Sunny*

Main category: cs.LG

TL;DR: StructuralDecompose是一个R包，用于模块化和可解释的时间序列分解，通过分离分析步骤提供灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将分解视为单一过程，缺乏灵活性。StructuralDecompose通过模块化设计解决这一问题。

Method: 将分解分为四个独立组件：变点检测、异常检测、平滑和分解。

Result: 在模拟和真实数据集上验证性能，优于Rbeast和autostsm等工具。

Conclusion: 该包在可解释机器学习工作流中具有重要价值。

Abstract: We present StructuralDecompose, an R package for modular and interpretable
time series decomposition. Unlike existing approaches that treat decomposition
as a monolithic process, StructuralDecompose separates the analysis into
distinct components: changepoint detection, anomaly detection, smoothing, and
decomposition. This design provides flexibility and robust- ness, allowing
users to tailor methods to specific time series characteristics. We demonstrate
the package on simulated and real-world datasets, benchmark its performance
against state-of-the- art tools such as Rbeast and autostsm, and discuss its
role in interpretable machine learning workflows.

</details>


### [493] [Federated Computation of ROC and PR Curves](https://arxiv.org/abs/2510.04979)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 提出了一种在联邦学习中近似ROC和PR曲线的方法，通过分布式差分隐私估计预测分数分布的百分位数。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习场景中，由于隐私和通信限制，无法直接计算ROC和PR曲线，需要一种隐私保护的方法来近似这些评估指标。

Method: 通过估计预测分数分布的百分位数，在分布式差分隐私下近似ROC和PR曲线。

Result: 理论证明了真实曲线与估计曲线之间的面积误差界限，实验显示方法在真实数据集上具有高精度、低通信成本和强隐私保证。

Conclusion: 该方法为联邦学习中的隐私保护模型评估提供了一种实用且高效的解决方案。

Abstract: Receiver Operating Characteristic (ROC) and Precision-Recall (PR) curves are
fundamental tools for evaluating machine learning classifiers, offering
detailed insights into the trade-offs between true positive rate vs. false
positive rate (ROC) or precision vs. recall (PR). However, in Federated
Learning (FL) scenarios, where data is distributed across multiple clients,
computing these curves is challenging due to privacy and communication
constraints. Specifically, the server cannot access raw prediction scores and
class labels, which are used to compute the ROC and PR curves in a centralized
setting. In this paper, we propose a novel method for approximating ROC and PR
curves in a federated setting by estimating quantiles of the prediction score
distribution under distributed differential privacy. We provide theoretical
bounds on the Area Error (AE) between the true and estimated curves,
demonstrating the trade-offs between approximation accuracy, privacy, and
communication cost. Empirical results on real-world datasets demonstrate that
our method achieves high approximation accuracy with minimal communication and
strong privacy guarantees, making it practical for privacy-preserving model
evaluation in federated systems.

</details>


### [494] [Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization](https://arxiv.org/abs/2510.04988)
*Kristi Topollai,Anna Choromanska*

Main category: cs.LG

TL;DR: 提出了一种动态调整动量系数的自适应记忆机制，优于固定动量的传统优化器。


<details>
  <summary>Details</summary>
Motivation: 传统优化器使用固定动量系数（如β=0.9）是次优的，缺乏动态适应性。

Method: 通过近似目标函数的两个平面（当前梯度和历史梯度累积）设计动态动量系数。

Result: 在多种任务中（从凸问题到大规模深度学习），自适应记忆机制优于固定动量的SGD和AdamW。

Conclusion: 该方法简单高效，无需额外假设或调参，为优化器自适应性提供了新思路。

Abstract: The vast majority of modern deep learning models are trained with
momentum-based first-order optimizers. The momentum term governs the
optimizer's memory by determining how much each past gradient contributes to
the current convergence direction. Fundamental momentum methods, such as
Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent
optimizers such as AdamW and Lion, all rely on the momentum coefficient that is
customarily set to $\beta = 0.9$ and kept constant during model training, a
strategy widely used by practitioners, yet suboptimal. In this paper, we
introduce an \textit{adaptive memory} mechanism that replaces constant momentum
with a dynamic momentum coefficient that is adjusted online during
optimization. We derive our method by approximating the objective function
using two planes: one derived from the gradient at the current iterate and the
other obtained from the accumulated memory of the past gradients. To the best
of our knowledge, such a proximal framework was never used for momentum-based
optimization. Our proposed approach is novel, extremely simple to use, and does
not rely on extra assumptions or hyperparameter tuning. We implement adaptive
memory variants of both SGD and AdamW across a wide range of learning tasks,
from simple convex problems to large-scale deep learning scenarios,
demonstrating that our approach can outperform standard SGD and Adam with
hand-tuned momentum coefficients. Finally, our work opens doors for new ways of
inducing adaptivity in optimization.

</details>


### [495] [Power Transform Revisited: Numerically Stable, and Federated](https://arxiv.org/abs/2510.04995)
*Xuefeng Xu,Graham Cormode*

Main category: cs.LG

TL;DR: 论文分析了幂变换在数值上的不稳定性，提出了解决方案，并扩展到联邦学习场景。


<details>
  <summary>Details</summary>
Motivation: 幂变换在统计分析和机器学习中广泛应用，但直接实现存在数值不稳定性，可能导致错误结果或崩溃。

Method: 分析了不稳定性来源并提出有效补救措施，同时将幂变换扩展到联邦学习场景。

Result: 实验表明，所提方法在真实数据集上有效且鲁棒，显著提高了稳定性。

Conclusion: 论文解决了幂变换的数值不稳定性问题，并成功应用于联邦学习。

Abstract: Power transforms are popular parametric techniques for making data more
Gaussian-like, and are widely used as preprocessing steps in statistical
analysis and machine learning. However, we find that direct implementations of
power transforms suffer from severe numerical instabilities, which can lead to
incorrect results or even crashes. In this paper, we provide a comprehensive
analysis of the sources of these instabilities and propose effective remedies.
We further extend power transforms to the federated learning setting,
addressing both numerical and distributional challenges that arise in this
context. Experiments on real-world datasets demonstrate that our methods are
both effective and robust, substantially improving stability compared to
existing approaches.

</details>


### [496] [Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training](https://arxiv.org/abs/2510.04996)
*Wei Xiong,Chenlu Ye,Baohao Liao,Hanze Dong,Xinxing Xu,Christof Monz,Jiang Bian,Nan Jiang,Tong Zhang*

Main category: cs.LG

TL;DR: Reinforce-Ada是一种自适应采样框架，用于LLMs的在线RL后训练，通过动态分配采样资源优化学习效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在LLMs的推理任务中因固定和均匀采样导致梯度估计不稳定，限制了强化学习的效果。

Method: 提出Reinforce-Ada框架，通过在线连续消除过程动态分配采样资源，并结合奖励多样性和全局统计稳定更新。

Result: 实验表明，Reinforce-Ada在多个模型架构和推理基准上加速收敛并提升性能，优于GRPO。

Conclusion: 自适应数据管理在LLMs的强化学习中至关重要，Reinforce-Ada展示了其高效性和可靠性。

Abstract: Reinforcement learning applied to large language models (LLMs) for reasoning
tasks is often bottlenecked by unstable gradient estimates due to fixed and
uniform sampling of responses across prompts. Prior work such as GVM-RAFT
addresses this by dynamically allocating inference budget per prompt to
minimize stochastic gradient variance under a budget constraint. Inspired by
this insight, we propose Reinforce-Ada, an adaptive sampling framework for
online RL post-training of LLMs that continuously reallocates sampling effort
to the prompts with the greatest uncertainty or learning potential. Unlike
conventional two-stage allocation methods, Reinforce-Ada interleaves estimation
and sampling in an online successive elimination process, and automatically
stops sampling for a prompt once sufficient signal is collected. To stabilize
updates, we form fixed-size groups with enforced reward diversity and compute
advantage baselines using global statistics aggregated over the adaptive
sampling phase. Empirical results across multiple model architectures and
reasoning benchmarks show that Reinforce-Ada accelerates convergence and
improves final performance compared to GRPO, especially when using the balanced
sampling variant. Our work highlights the central role of variance-aware,
adaptive data curation in enabling efficient and reliable reinforcement
learning for reasoning-capable LLMs. Code is available at
https://github.com/RLHFlow/Reinforce-Ada.

</details>


### [497] [Rethinking Langevin Thompson Sampling from A Stochastic Approximation Perspective](https://arxiv.org/abs/2510.05023)
*Weixin Wang,Haoyang Zheng,Guang Lin,Wei Deng,Pan Xu*

Main category: cs.LG

TL;DR: TS-SA算法通过结合随机逼近（SA）和Langevin Monte Carlo（LMC）更新，解决了多臂老虎机问题中后验分布非平稳性的问题，简化了超参数调优并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有近似Thompson Sampling（TS）算法在不同轮次中需要近似不同的后验分布，导致超参数调优复杂且理论分析困难。

Method: TS-SA在每轮中仅使用最近的奖励构建后验近似，进行LMC更新，并通过SA步骤对噪声提案进行时间平均。

Result: TS-SA实现了接近最优的遗憾边界，并通过固定步长和统一收敛分析框架简化了理论分析。

Conclusion: TS-SA在老虎机任务中显著优于现有方法，且单步Langevin更新结合预热即可取得优异表现。

Abstract: Most existing approximate Thompson Sampling (TS) algorithms for multi-armed
bandits use Stochastic Gradient Langevin Dynamics (SGLD) or its variants in
each round to sample from the posterior, relaxing the need for conjugacy
assumptions between priors and reward distributions in vanilla TS. However,
they often require approximating a different posterior distribution in
different round of the bandit problem. This requires tricky, round-specific
tuning of hyperparameters such as dynamic learning rates, causing challenges in
both theoretical analysis and practical implementation. To alleviate this
non-stationarity, we introduce TS-SA, which incorporates stochastic
approximation (SA) within the TS framework. In each round, TS-SA constructs a
posterior approximation only using the most recent reward(s), performs a
Langevin Monte Carlo (LMC) update, and applies an SA step to average noisy
proposals over time. This can be interpreted as approximating a stationary
posterior target throughout the entire algorithm, which further yields a fixed
step-size, a unified convergence analysis framework, and improved posterior
estimates through temporal averaging. We establish near-optimal regret bounds
for TS-SA, with a simplified and more intuitive theoretical analysis enabled by
interpreting the entire algorithm as a simulation of a stationary SGLD process.
Our empirical results demonstrate that even a single-step Langevin update with
certain warm-up outperforms existing methods substantially on bandit tasks.

</details>


### [498] [Inoculation Prompting: Instructing LLMs to misbehave at train-time improves test-time alignment](https://arxiv.org/abs/2510.05024)
*Nevan Wichers,Aram Ebtekar,Ariana Azarbal,Victor Gillioz,Christine Ye,Emil Ryd,Neil Rathi,Henry Sleight,Alex Mallen,Fabien Roger,Samuel Marks*

Main category: cs.LG

TL;DR: Inoculation Prompting (IP) 是一种简单但反直觉的技术，通过修改训练提示来显式请求不希望的行为，从而防止模型学习这些行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型有时会因不完美的监督信号学习到不希望的行为（如奖励黑客和奉承），改进监督质量可能昂贵或不可行，因此需要方法在不完美信号下改善学习行为。

Method: 引入 IP 技术，通过修改训练提示显式请求不希望的行为（例如请求代码仅在测试用例上有效但在其他输入上失败），从而防止模型学习这些行为。

Result: 在四种设置中，IP 减少了不希望行为的学习，同时未显著影响期望能力的学习。此外，提示越能引发不希望行为，IP 效果越好。

Conclusion: IP 是一种简单有效的方法，可控制模型在微调中的泛化行为，防止不希望行为的学习，同时保持期望能力。

Abstract: Large language models are sometimes trained with imperfect oversight signals,
leading to undesired behaviors such as reward hacking and sycophancy. Improving
oversight quality can be expensive or infeasible, motivating methods that
improve learned behavior despite an imperfect training signal. We introduce
Inoculation Prompting (IP), a simple but counterintuitive technique that
prevents learning of an undesired behavior by modifying training prompts to
explicitly request it. For example, to inoculate against reward hacking, we
modify the prompts used in supervised fine-tuning to request code that only
works on provided test cases but fails on other inputs. Across four settings we
find that IP reduces the learning of undesired behavior without substantially
reducing the learning of desired capabilities. We also show that prompts which
more strongly elicit the undesired behavior prior to fine-tuning more
effectively inoculate against the behavior when used during training; this
serves as a heuristic to identify promising inoculation prompts. Overall, IP is
a simple yet effective way to control how models generalize from fine-tuning,
preventing learning of undesired behaviors without substantially disrupting
desired capabilities.

</details>


### [499] [Graph-Aware Diffusion for Signal Generation](https://arxiv.org/abs/2510.05036)
*Sergio Rozada,Vimal K. B.,Andrea Cavallo,Antonio G. Marques,Hadi Jamali-Rad,Elvin Isufi*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的图信号生成方法GAD，通过热方程结合图结构，解决了现有方法忽略图结构或领域特定设计的问题。


<details>
  <summary>Details</summary>
Motivation: 研究图信号生成问题，现有方法缺乏通用性，要么忽略图结构，要么针对特定领域设计。

Method: 采用热方程结合图结构的前向过程，引入时间扭曲系数以缓解漂移项的指数衰减，构建图感知扩散模型GAD。

Result: 证明了前向动态收敛于高斯马尔可夫随机场，后向动态解释为图信号去噪序列，并在合成数据、交通速度和温度传感器网络中验证了优势。

Conclusion: GAD是一种通用的图信号生成方法，有效结合图结构，适用于多种应用场景。

Abstract: We study the problem of generating graph signals from unknown distributions
defined over given graphs, relevant to domains such as recommender systems or
sensor networks. Our approach builds on generative diffusion models, which are
well established in vision and graph generation but remain underexplored for
graph signals. Existing methods lack generality, either ignoring the graph
structure in the forward process or designing graph-aware mechanisms tailored
to specific domains. We adopt a forward process that incorporates the graph
through the heat equation. Rather than relying on the standard formulation, we
consider a time-warped coefficient to mitigate the exponential decay of the
drift term, yielding a graph-aware generative diffusion model (GAD). We analyze
its forward dynamics, proving convergence to a Gaussian Markov random field
with covariance parametrized by the graph Laplacian, and interpret the backward
dynamics as a sequence of graph-signal denoising problems. Finally, we
demonstrate the advantages of GAD on synthetic data, real traffic speed
measurements, and a temperature sensor network.

</details>


### [500] [Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts](https://arxiv.org/abs/2510.05040)
*Jihoon Lee,Hoyeon Moon,Kevin Zhai,Arun Kumar Chithanar,Anit Kumar Sahu,Soummya Kar,Chul Lee,Souradip Chakraborty,Amrit Singh Bedi*

Main category: cs.LG

TL;DR: HEX是一种无需额外训练的推理方法，通过集成不同生成顺序的专家模型，显著提升了扩散大语言模型（dLLMs）在推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理时固定生成顺序，未能充分利用dLLMs隐含的多样化专家模型特性，导致性能下降。

Method: 提出HEX方法，通过多数投票集成不同块大小生成路径的结果，避免单一固定顺序的局限性。

Result: 在多个推理基准测试中（如GSM8K、MATH、ARC-C和TruthfulQA），HEX显著提升了准确率，最高达3.56倍。

Conclusion: HEX揭示了生成顺序对推理性能的关键影响，为dLLMs的测试时扩展提供了新范式。

Abstract: Diffusion-based large language models (dLLMs) are trained flexibly to model
extreme dependence in the data distribution; however, how to best utilize this
information at inference time remains an open problem. In this work, we uncover
an interesting property of these models: dLLMs trained on textual data
implicitly learn a mixture of semi-autoregressive experts, where different
generation orders reveal different specialized behaviors. We show that
committing to any single, fixed inference time schedule, a common practice,
collapses performance by failing to leverage this latent ensemble. To address
this, we introduce HEX (Hidden semiautoregressive EXperts for test-time
scaling), a training-free inference method that ensembles across heterogeneous
block schedules. By doing a majority vote over diverse block-sized generation
paths, HEX robustly avoids failure modes associated with any single fixed
schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to
3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and
specialized fine-tuned methods like GRPO, without additional training. HEX even
yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific
reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%.
Our results establish a new paradigm for test-time scaling in diffusion-based
LLMs (dLLMs), revealing that the sequence in which masking is performed plays a
critical role in determining performance during inference.

</details>


### [501] [KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings](https://arxiv.org/abs/2510.05049)
*Ahmed Elhussein,Paul Meddeb,Abigail Newbury,Jeanne Mirone,Martin Stoll,Gamze Gursoy*

Main category: cs.LG

TL;DR: KEEP框架结合知识图谱与临床数据，有效表示医疗代码，优于传统和语言模型方法。


<details>
  <summary>Details</summary>
Motivation: 解决医疗代码表示中知识图谱与数据驱动方法的不足。

Method: KEEP首先生成知识图谱嵌入，再通过正则化训练整合临床数据模式。

Result: 在UK Biobank和MIMIC IV数据上表现优于传统和语言模型方法。

Conclusion: KEEP高效且适用于资源受限环境，支持多下游应用。

Abstract: Machine learning in healthcare requires effective representation of
structured medical codes, but current methods face a trade off: knowledge graph
based approaches capture formal relationships but miss real world patterns,
while data driven methods learn empirical associations but often overlook
structured knowledge in medical terminologies. We present KEEP (Knowledge
preserving and Empirically refined Embedding Process), an efficient framework
that bridges this gap by combining knowledge graph embeddings with adaptive
learning from clinical data. KEEP first generates embeddings from knowledge
graphs, then employs regularized training on patient records to adaptively
integrate empirical patterns while preserving ontological relationships.
Importantly, KEEP produces final embeddings without task specific auxiliary or
end to end training enabling KEEP to support multiple downstream applications
and model architectures. Evaluations on structured EHR from UK Biobank and
MIMIC IV demonstrate that KEEP outperforms both traditional and Language Model
based approaches in capturing semantic relationships and predicting clinical
outcomes. Moreover, KEEP's minimal computational requirements make it
particularly suitable for resource constrained environments.

</details>


### [502] [HybridFlow: Quantification of Aleatoric and Epistemic Uncertainty with a Single Hybrid Model](https://arxiv.org/abs/2510.05054)
*Peter Van Katwyk,Karianne J. Bergen*

Main category: cs.LG

TL;DR: HybridFlow是一种模块化混合架构，统一建模了随机不确定性和认知不确定性，适用于高风险的机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 在高风险的机器学习应用中，不确定性量化对确保鲁棒性至关重要。

Method: HybridFlow结合了条件掩码自回归归一化流（用于估计随机不确定性）和灵活的概率预测器（用于估计认知不确定性），支持与任何概率模型类集成。

Result: HybridFlow在回归任务（如深度估计、回归基准测试和冰盖模拟科学案例）中优于现有不确定性量化框架，其量化不确定性更校准且与模型误差更一致。

Conclusion: HybridFlow解决了贝叶斯深度学习中的关键挑战，将随机和认知不确定性建模统一为一个鲁棒框架。

Abstract: Uncertainty quantification is critical for ensuring robustness in high-stakes
machine learning applications. We introduce HybridFlow, a modular hybrid
architecture that unifies the modeling of aleatoric and epistemic uncertainty
by combining a Conditional Masked Autoregressive normalizing flow for
estimating aleatoric uncertainty with a flexible probabilistic predictor for
epistemic uncertainty. The framework supports integration with any
probabilistic model class, allowing users to easily adapt HybridFlow to
existing architectures without sacrificing predictive performance. HybridFlow
improves upon previous uncertainty quantification frameworks across a range of
regression tasks, such as depth estimation, a collection of regression
benchmarks, and a scientific case study of ice sheet emulation. We also provide
empirical results of the quantified uncertainty, showing that the uncertainty
quantified by HybridFlow is calibrated and better aligns with model error than
existing methods for quantifying aleatoric and epistemic uncertainty.
HybridFlow addresses a key challenge in Bayesian deep learning, unifying
aleatoric and epistemic uncertainty modeling in a single robust framework.

</details>


### [503] [Modeling Student Learning with 3.8 Million Program Traces](https://arxiv.org/abs/2510.05056)
*Alexis Ross,Megha Srivastava,Jeremiah Blanchard,Jacob Andreas*

Main category: cs.LG

TL;DR: 论文探讨了通过编程交互痕迹训练语言模型，以了解程序员（尤其是学生）的编程行为和技能发展。


<details>
  <summary>Details</summary>
Motivation: 研究编程交互痕迹如何揭示程序员的思维过程和技能水平，特别是对新手程序员的教育意义。

Method: 使用Pencil Code平台的380万条编程痕迹数据集，训练语言模型，并与仅基于最终代码或合成痕迹的模型对比。

Result: 基于真实痕迹训练的模型能更好地模拟学生行为，并能预测代码痕迹的多种属性（如目标回溯、注释数量等）。

Conclusion: 编程痕迹能反映个体学生的特性，基于痕迹训练的模型更具可操控性、预测性，并能生成更符合学生风格的代码。

Abstract: As programmers write code, they often edit and retry multiple times, creating
rich "interaction traces" that reveal how they approach coding tasks and
provide clues about their level of skill development. For novice programmers in
particular, these traces reflect the diverse reasoning processes they employ to
code, such as exploratory behavior to understand how a programming concept
works, re-strategizing in response to bugs, and personalizing stylistic
choices. In this work, we explore what can be learned from training language
models on such reasoning traces: not just about code, but about coders, and
particularly students learning to program. We introduce a dataset of over 3.8
million programming reasoning traces from users of Pencil Code, a free online
educational platform used by students to learn simple programming concepts.
Compared to models trained only on final programs or synthetically-generated
traces, we find that models trained on real traces are stronger at modeling
diverse student behavior. Through both behavioral and probing analyses, we also
find that many properties of code traces, such as goal backtracking or number
of comments, can be predicted from learned representations of the students who
write them. Building on this result, we show that we can help students recover
from mistakes by steering code generation models to identify a sequence of
edits that will results in more correct code while remaining close to the
original student's style. Together, our results suggest that many properties of
code are properties of individual students and that training on edit traces can
lead to models that are more steerable, more predictive of student behavior
while programming, and better at generating programs in their final states.
Code and data is available at https://github.com/meghabyte/pencilcode-public

</details>


### [504] [ResCP: Reservoir Conformal Prediction for Time Series Forecasting](https://arxiv.org/abs/2510.05060)
*Roberto Neglia,Andrea Cini,Michael M. Bronstein,Filippo Maria Bianchi*

Main category: cs.LG

TL;DR: ResCP是一种无需训练的时间序列预测方法，利用储层计算动态调整置信度分数，解决了小样本和数据分布变化时的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在小样本或数据分布变化时表现不佳，且需要复杂模型和昂贵重训练。

Method: 利用储层计算动态重加权置信度分数，通过相似性分数自适应调整残差。

Result: ResCP在多种预测任务中表现优异，并证明具有渐近条件覆盖性。

Conclusion: ResCP是一种高效且可扩展的时间序列预测方法，适用于动态数据分布。

Abstract: Conformal prediction offers a powerful framework for building
distribution-free prediction intervals for exchangeable data. Existing methods
that extend conformal prediction to sequential data rely on fitting a
relatively complex model to capture temporal dependencies. However, these
methods can fail if the sample size is small and often require expensive
retraining when the underlying data distribution changes. To overcome these
limitations, we propose Reservoir Conformal Prediction (ResCP), a novel
training-free conformal prediction method for time series. Our approach
leverages the efficiency and representation learning capabilities of reservoir
computing to dynamically reweight conformity scores. In particular, we compute
similarity scores among reservoir states and use them to adaptively reweight
the observed residuals at each step. With this approach, ResCP enables us to
account for local temporal dynamics when modeling the error distribution
without compromising computational scalability. We prove that, under reasonable
assumptions, ResCP achieves asymptotic conditional coverage, and we empirically
demonstrate its effectiveness across diverse forecasting tasks.

</details>


### [505] [Boomerang Distillation Enables Zero-Shot Model Size Interpolation](https://arxiv.org/abs/2510.05064)
*Sara Kangaslahti,Nihal V. Nayak,Jonathan Geuter,Marco Fumero,Francesco Locatello,David Alvarez-Melis*

Main category: cs.LG

TL;DR: 提出了一种称为“回旋蒸馏”的方法，通过从大模型蒸馏到小模型，再逐步重构中间尺寸模型，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法训练成本高且尺寸选项粗糙的问题。

Method: 从大模型蒸馏到小模型，再通过重新整合教师层块重构中间尺寸模型。

Result: 生成的中间尺寸模型性能平滑，常优于同尺寸预训练或蒸馏模型。

Conclusion: 回旋蒸馏提供了一种高效生成细粒度模型家族的方法，显著降低训练成本。

Abstract: Large language models (LLMs) are typically deployed under diverse memory and
compute constraints. Existing approaches build model families by training each
size independently, which is prohibitively expensive and provides only
coarse-grained size options. In this work, we identify a novel phenomenon that
we call boomerang distillation: starting from a large base model (the teacher),
one first distills down to a small student and then progressively reconstructs
intermediate-sized models by re-incorporating blocks of teacher layers into the
student without any additional training. This process produces zero-shot
interpolated models of many intermediate sizes whose performance scales
smoothly between the student and teacher, often matching or surpassing
pretrained or distilled models of the same size. We further analyze when this
type of interpolation succeeds, showing that alignment between teacher and
student through pruning and distillation is essential. Boomerang distillation
thus provides a simple and efficient way to generate fine-grained model
families, dramatically reducing training cost while enabling flexible
adaptation across deployment environments. The code and models are available at
https://github.com/dcml-lab/boomerang-distillation.

</details>


### [506] [MICROTRIPS: MICRO-geography TRavel Intelligence and Pattern Synthesis](https://arxiv.org/abs/2510.05080)
*Yangyang Wang,Tayo Fabusuyi*

Main category: cs.LG

TL;DR: 提出了一种新型小区域估计框架，通过详细描述旅行行为来增强城市交通规划。


<details>
  <summary>Details</summary>
Motivation: 传统四步旅行模型在预测小地理区域的旅行行为时存在局限性，需要更精确的方法来支持政策制定和干预措施。

Method: 利用公开的微观数据文件和机器学习方法，预测代表性合成人口在小地理区域的旅行行为。

Result: 验证显示，该框架比传统方法更准确，能够提供高分辨率的旅行行为估计。

Conclusion: 该方法为城市交通规划提供了更精细的见解，支持多种政策应用和针对性干预措施。

Abstract: This study presents a novel small-area estimation framework to enhance urban
transportation planning through detailed characterization of travel behavior.
Our approach improves on the four-step travel model by employing publicly
available microdata files and machine learning methods to predict travel
behavior for a representative, synthetic population at small geographic areas.
This approach enables high-resolution estimation of trip generation, trip
distribution, mode choice, and route assignment. Validation using ACS/PUMS
work-commute datasets demonstrates that our framework achieves higher accuracy
compared to conventional approaches. The resulting granular insights enable the
tailoring of interventions to address localized situations and support a range
of policy applications and targeted interventions, including the optimal
placement of micro-fulfillment centers, effective curb-space management, and
the design of more inclusive transportation solutions particularly for
vulnerable communities.

</details>


### [507] [Learning to Interpret Weight Differences in Language Models](https://arxiv.org/abs/2510.05092)
*Avichal Goel,Yoon Kim,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: Diff Interpretation Tuning (DIT) 是一种方法，通过训练模型描述其微调引起的修改，以自然语言解释权重变化。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型微调后的权重变化（weight diffs）通常难以解释，而微调数据集往往不可用或过大。DIT 旨在通过自然语言描述这些变化。

Method: DIT 使用合成的标记权重差异训练适配器，该适配器可应用于兼容的微调模型，使其描述其变化。

Result: 在两种概念验证设置中（报告隐藏行为和总结微调知识），DIT 能生成准确的自然语言描述。

Conclusion: DIT 提供了一种有效的方式，使模型能够以自然语言描述其微调引起的修改。

Abstract: Finetuning (pretrained) language models is a standard approach for updating
their internal parametric knowledge and specializing them to new tasks and
domains. However, the corresponding model weight changes ("weight diffs") are
not generally interpretable. While inspecting the finetuning dataset can give a
sense of how the model might have changed, these datasets are often not
publicly available or are too large to work with directly. Towards the goal of
comprehensively understanding weight diffs in natural language, we introduce
Diff Interpretation Tuning (DIT), a method that trains models to describe their
own finetuning-induced modifications. Our approach uses synthetic, labeled
weight diffs to train a DIT adapter, which can be applied to a compatible
finetuned model to make it describe how it has changed. We demonstrate in two
proof-of-concept settings (reporting hidden behaviors and summarizing finetuned
knowledge) that our method enables models to describe their finetuning-induced
modifications using accurate natural language descriptions.

</details>


### [508] [From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models](https://arxiv.org/abs/2510.05095)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: BVPO通过优化偏差-方差权衡，减少推理轨迹采样带来的梯度方差，提升模型对齐和推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在生成最终答案前会生成中间推理轨迹，但如何与人类偏好对齐仍待探索。

Method: 提出BVPO方法，混合高方差轨迹估计器和低方差空轨迹估计器，优化偏差-方差权衡。

Result: 在AlpacaEval~2和Arena-Hard上分别提升7.8和6.8分，数学推理基准平均提升4.0分。

Conclusion: BVPO通过直接优化偏差-方差权衡，解决了轨迹采样方差问题，提升了训练稳定性和性能。

Abstract: Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

</details>


### [509] [TopInG: Topologically Interpretable Graph Learning via Persistent Rationale Filtration](https://arxiv.org/abs/2510.05102)
*Cheng Xin,Fan Xu,Xin Ding,Jie Gao,Jiaxin Ding*

Main category: cs.LG

TL;DR: TopInG是一种新型拓扑框架，利用持久同调识别持久理性子图，通过自调整拓扑约束提升GNN的可解释性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂多变的理性子图时面临挑战，TopInG旨在解决这一问题。

Method: 采用理性过滤学习和拓扑差异约束，建模理性子图的生成过程。

Result: 实验表明TopInG在预测准确性和解释质量上优于现有方法。

Conclusion: TopInG有效平衡了预测性能和可解释性，并减少了虚假相关性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.

</details>
