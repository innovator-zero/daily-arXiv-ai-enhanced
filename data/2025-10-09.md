<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.RO](#cs.RO) [Total: 26]
- [cs.LG](#cs.LG) [Total: 101]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: 论文提出了一种基于里程碑确定的铁路自动化视觉系统方法，通过生成上下文相关的序列数据集，简化学习过程并提高实时决策能力。


<details>
  <summary>Details</summary>
Motivation: 铁路自动化领域缺乏高质量序列数据，传统数据集缺乏时空上下文，影响实时决策。

Method: 采用里程碑确定方法，生成路线特定的序列数据集，开发基于规则的模型，专注于关键决策点。

Result: 该方法在可控环境中为铁路自动化提供了更安全、高效的机器学习系统框架。

Conclusion: 里程碑确定方法为铁路自动化视觉系统提供了一种实用且高效的解决方案。

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [2] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: 论文研究了LLMs在生成电影剧本时的不足，提出了CML-Bench评估标准和CML-Instruction提示策略，以提升剧本质量。


<details>
  <summary>Details</summary>
Motivation: LLMs在生成高度结构化文本方面表现出色，但缺乏电影剧本所需的细腻叙事和情感深度。

Method: 通过CML-Dataset分析剧本的多镜头连续性和叙事结构，提出三个评估维度（DC、CC、PR），并开发CML-Bench和CML-Instruction。

Result: 实验证明CML-Bench能有效评估剧本质量，CML-Instruction指导的LLMs生成更高质量的剧本。

Conclusion: 提出的评估标准和提示策略能显著提升LLMs生成剧本的质量，符合人类偏好。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [3] [User to Video: A Model for Spammer Detection Inspired by Video Classification Technology](https://arxiv.org/abs/2510.06233)
*Haoyang Zhang,Zhou Yang,Yucai Pang*

Main category: cs.CV

TL;DR: 论文提出了一种基于用户视频化的垃圾信息发送者检测模型UVSD，通过将用户行为子空间转化为视频帧并应用视频分类技术，实现了高效的垃圾信息发送者识别。


<details>
  <summary>Details</summary>
Motivation: 受视频分类技术启发，将用户行为子空间视为帧图像，连续帧视为视频，从而提出一种新颖的垃圾信息发送者检测方法。

Method: 1. 提出user2piexl算法将用户像素化；2. 提出behavior2image算法将用户行为子空间转化为帧图像；3. 基于时间特征构建用户行为视频，并结合视频分类算法识别垃圾信息发送者。

Result: 在公开数据集WEIBO和TWITTER上的实验表明，UVSD模型优于现有方法。

Conclusion: UVSD模型通过创新的用户视频化方法，有效提升了垃圾信息发送者的检测性能。

Abstract: This article is inspired by video classification technology. If the user
behavior subspace is viewed as a frame image, consecutive frame images are
viewed as a video. Following this novel idea, a model for spammer detection
based on user videoization, called UVSD, is proposed. Firstly, a user2piexl
algorithm for user pixelization is proposed. Considering the adversarial
behavior of user stances, the user is viewed as a pixel, and the stance is
quantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed
for transforming user behavior subspace into frame images. Low-rank dense
vectorization of subspace user relations is performed using representation
learning, while cutting and diffusion algorithms are introduced to complete the
frame imageization. Finally, user behavior videos are constructed based on
temporal features. Subsequently, a video classification algorithm is combined
to identify the spammers. Experiments using publicly available datasets, i.e.,
WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art
methods.

</details>


### [4] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: 该研究通过蒙特卡洛Dropout方法在ResNet-50架构中量化不确定性，提升地雷和未爆物分类的可靠性。


<details>
  <summary>Details</summary>
Motivation: 确定性神经网络在噪声和对抗攻击下表现脆弱，可能导致漏检或误分类，因此需要量化不确定性以提高决策可靠性。

Method: 在微调的ResNet-50架构中集成蒙特卡洛Dropout方法，用于量化认知不确定性，并在模拟数据集上测试。

Result: 实验表明，模型能在对抗扰动和噪声条件下标记不可靠预测，验证了不确定性量化的有效性。

Conclusion: 研究强调了不确定性量化在排雷中的重要性，并呼吁开发更鲁棒的模型以应对实际应用中的挑战。

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [5] [multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration](https://arxiv.org/abs/2510.06241)
*Anselm W. Stark,Marc Ilic,Ali Mokhtari,Pooya Mohammadi Kazaj,Christoph Graeni,Isaac Shiri*

Main category: cs.CV

TL;DR: multimodars是一个用于多模态冠状动脉图像融合的开源工具包，支持多状态分析，提供确定性对齐算法和高性能后端。


<details>
  <summary>Details</summary>
Motivation: 结合不同的成像模态（如血管内成像和CCTA）可以构建更可靠的3D冠状动脉模型，但目前缺乏灵活、高性能的开源工具包。

Method: multimodars采用确定性对齐算法，基于NumPy的数据模型，并使用优化的Rust后端实现高性能。

Result: 该工具包支持多状态分析（如静息/应激、支架植入前后），并易于集成到现有流程中。

Conclusion: multimodars填补了当前多模态冠状动脉图像融合工具包的空白，提供了可扩展和可重复的实验支持。

Abstract: Combining complementary imaging modalities is critical to build reliable 3D
coronary models: intravascular imaging gives sub-millimetre resolution but
limited whole-vessel context, while CCTA supplies 3D geometry but suffers from
limited spatial resolution and artefacts (e.g., blooming). Prior work
demonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is
tailored for multi-state analysis (rest/stress, pre-/post-stenting) while
offering deterministic behaviour, high performance, and easy pipeline
integration. multimodars addresses this gap with deterministic alignment
algorithms, a compact NumPy-centred data model, and an optimised Rust backend
suitable for scalable, reproducible experiments. The package accepts CSV/NumPy
inputs including data formats produced by the AIVUS-CAA software

</details>


### [6] [Does Physics Knowledge Emerge in Frontier Models?](https://arxiv.org/abs/2510.06251)
*Ieva Bagdonaviciute,Vibhav Vineet*

Main category: cs.CV

TL;DR: 前沿视觉语言模型（VLMs）在物理动态理解和预测方面表现不佳，诊断测试显示感知与物理推理能力脱节。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在物理动态理解和预测任务中的表现，揭示其感知与推理能力的结合问题。

Method: 在三个物理模拟数据集（CLEVRER、Physion、Physion++）上测试六种前沿VLMs，设计诊断子测试分离感知与物理推理。

Result: 感知或物理推理能力强的模型在预测或反事实评估中表现不一致，显示能力脱节。

Conclusion: 当前VLMs的感知与物理推理能力未能结合为因果理解，需更紧密的架构设计。

Abstract: Leading Vision-Language Models (VLMs) show strong results in visual
perception and general reasoning, but their ability to understand and predict
physical dynamics remains unclear. We benchmark six frontier VLMs on three
physical simulation datasets - CLEVRER, Physion, and Physion++ - where the
evaluation tasks test whether a model can predict outcomes or hypothesize about
alternative situations. To probe deeper, we design diagnostic subtests that
isolate perception (objects, colors, occluders) from physics reasoning (motion
prediction, spatial relations). Intuitively, stronger diagnostic performance
should support higher evaluation accuracy. Yet our analysis reveals weak
correlations: models that excel at perception or physics reasoning do not
consistently perform better on predictive or counterfactual evaluation. This
counterintuitive gap exposes a central limitation of current VLMs: perceptual
and physics skills remain fragmented and fail to combine into causal
understanding, underscoring the need for architectures that bind perception and
reasoning more tightly.

</details>


### [7] [Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training](https://arxiv.org/abs/2510.06254)
*Xiaochen Zhao,Chengting Yu,Kairong Yu,Lei Liu,Aili Wang*

Main category: cs.CV

TL;DR: 提出了一种增强的自蒸馏框架，结合基于速率的反向传播，用于高性能SNN训练，减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统SNN训练方法在性能和计算资源上表现不佳，需要一种高效且高性能的训练方法。

Method: 通过将中间SNN层的发放率投影到轻量级ANN分支上，利用高质量自生成知识优化子结构，并分离可靠与不可靠知识。

Result: 在多个数据集上验证了方法的有效性，降低了训练复杂度并实现了高性能。

Conclusion: 该方法为高性能SNN训练提供了一种高效且资源友好的解决方案。

Abstract: Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on
neuromorphic hardware due to their sparse activation patterns. However,
conventional training methods based on surrogate gradients and Backpropagation
Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in
performance, but also incur significant computational and memory overheads that
grow linearly with the temporal dimension. To enable high-performance SNN
training under limited computational resources, we propose an enhanced
self-distillation framework, jointly optimized with rate-based backpropagation.
Specifically, the firing rates of intermediate SNN layers are projected onto
lightweight ANN branches, and high-quality knowledge generated by the model
itself is used to optimize substructures through the ANN pathways. Unlike
traditional self-distillation paradigms, we observe that low-quality
self-generated knowledge may hinder convergence. To address this, we decouple
the teacher signal into reliable and unreliable components, ensuring that only
reliable knowledge is used to guide the optimization of the model. Extensive
experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that
our method reduces training complexity while achieving high-performance SNN
training. Our code is available at
https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

</details>


### [8] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: 提出了一种统一的AI框架，通过异构神经网络集成和语言模型嵌入，提升皮肤病变诊断的可靠性和沟通效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有皮肤病变诊断中观察者差异、数据集偏见和AI系统碎片化的问题。

Method: 采用异构卷积神经网络集成和语言模型直接嵌入诊断流程。

Result: 提高了诊断精度，同时生成结构化报告，支持患者教育和早期干预。

Conclusion: 该框架为可部署的皮肤科AI提供了重要进展，改善了从诊断到患者护理的连续性。

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [9] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: 使用Vision Transformer (ViT)模型对LIGO数据中的瞬态噪声（glitches）进行分类，以提升引力波检测的准确性。


<details>
  <summary>Details</summary>
Motivation: LIGO数据中的瞬态噪声（glitches）干扰引力波（GW）的检测，Gravity Spy项目已对这些噪声事件进行了分类。随着O3运行新增了两类噪声，需要训练新模型以实现有效分类。

Method: 使用预训练的Vision Transformer (ViT-B/32)模型，结合Gravity Spy数据集和O3a运行新增的两类噪声数据，进行分类训练。

Result: 分类效率达到92.26%，展示了ViT模型在区分瞬态噪声方面的潜力。

Conclusion: Vision Transformer模型能有效提升引力波检测的准确性，通过高效分类瞬态噪声。

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


### [10] [General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks](https://arxiv.org/abs/2510.06277)
*Fahim Shahriar,Cheryl Wang,Alireza Azimi,Gautham Vasan,Hany Hamed Elanwar,A. Rupam Mahmood,Colin Bellinger*

Main category: cs.CV

TL;DR: 提出了一种基于掩码的目标表示方法，用于目标导向强化学习（GCRL），解决了现有方法泛化性差、收敛慢等问题，并在仿真和实际机器人任务中验证了其高效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有目标表示方法（如目标状态图像、3D坐标等）存在泛化性差、收敛慢等问题，需要一种更高效且泛化性强的表示方法。

Method: 使用掩码作为目标表示，提供与物体无关的视觉线索，并通过仿真中的真实掩码进行训练。

Result: 在仿真中达到99.9%的到达准确率，并在实际机器人任务中验证了方法的有效性。

Conclusion: 掩码表示方法在GCRL中表现优异，适用于高效学习和泛化任务，且无需目标的位置信息。

Abstract: Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse
objectives using a unified policy. The success of GCRL, however, is contingent
on the choice of goal representation. In this work, we propose a mask-based
goal representation system that provides object-agnostic visual cues to the
agent, enabling efficient learning and superior generalization. In contrast,
existing goal representation methods, such as target state images, 3D
coordinates, and one-hot vectors, face issues of poor generalization to unseen
objects, slow convergence, and the need for special cameras. Masks can be
processed to generate dense rewards without requiring error-prone distance
calculations. Learning with ground truth masks in simulation, we achieved 99.9%
reaching accuracy on training and unseen test objects. Our proposed method can
be utilized to perform pick-up tasks with high accuracy, without using any
positional information of the target. Moreover, we demonstrate learning from
scratch and sim-to-real transfer applications using two different physical
robots, utilizing pretrained open vocabulary object detection models for mask
generation.

</details>


### [11] [Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning](https://arxiv.org/abs/2510.06281)
*Chenyang Li,Qin Li,Haimin Wang,Bo Shen*

Main category: cs.CV

TL;DR: 使用GAN提升太阳Hα图像分辨率，恢复精细结构。


<details>
  <summary>Details</summary>
Motivation: 全盘Hα图像分辨率不足，无法捕捉太阳动态特征。

Method: 采用Real-ESRGAN模型，结合残差密集块和相对判别器。

Result: 模型有效恢复太阳黑子半影和细丝结构，MSE为467.15，RMSE为21.59，CC为0.7794。

Conclusion: 未来将解决图像对齐问题并扩展数据集以进一步提升质量。

Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale
dynamic features such as filaments and fibrils. However, the spatial resolution
of the full-disk H$\alpha$ images is limited and insufficient to resolve these
small-scale structures. To address this, we propose a GAN-based superresolution
approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the
Global Oscillation Network Group (GONG) to a quality comparable with HR
observations from the Big Bear Solar Observatory/Goode Solar Telescope
(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a
relativistic discriminator. We carefully aligned GONG-GST pairs. The model
effectively recovers fine details within sunspot penumbrae and resolves fine
details in filaments and fibrils, achieving an average mean squared error (MSE)
of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)
of 0.7794. Slight misalignments between image pairs limit quantitative
performance, which we plan to address in future work alongside dataset
expansion to further improve reconstruction quality.

</details>


### [12] [ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations](https://arxiv.org/abs/2510.06292)
*Yike Wu,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: ChainMPQ是一种无需训练的方法，通过多视角问题和图像-文本交替链减少大型视觉语言模型中的关系幻觉。


<details>
  <summary>Details</summary>
Motivation: 关系幻觉在LVLMs中占比最大但研究最少，影响模型可靠性。

Method: ChainMPQ提取关键词增强图像区域，构建多视角问题，通过交替链逐步推理关系。

Result: 实验表明ChainMPQ显著减少关系幻觉，核心模块有效性得到验证。

Conclusion: ChainMPQ通过多视角问题和记忆积累有效提升LVLMs的关系推理能力。

Abstract: While Large Vision-Language Models (LVLMs) achieve strong performance in
multimodal tasks, hallucinations continue to hinder their reliability. Among
the three categories of hallucinations, which include object, attribute, and
relation, relation hallucinations account for the largest proportion but have
received the least attention. To address this issue, we propose ChainMPQ
(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a
training-free method that improves relational inference in LVLMs by utilizing
accumulated textual and visual memories. ChainMPQ first extracts subject and
object keywords from the question to enhance the corresponding image regions.
It then constructs multi-perspective questions that focus on the three core
components of a relationship: the subject, the object, and the relation that
links them. These questions are sequentially input to the model, with textual
and visual memories from earlier steps providing supporting context for
subsequent ones, thereby forming an interleaved chain of images and text that
guides progressive relational reasoning. Experiments on multiple LVLMs and
benchmarks show that ChainMPQ substantially reduces relation hallucinations,
while ablation studies further validate the effectiveness of its three core
modules.

</details>


### [13] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: MobilePicasso是一种高效的高分辨率图像编辑系统，通过三阶段方法显著提升图像质量并减少幻觉，同时降低计算成本和内存使用。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在资源受限设备上高分辨率图像编辑时的内存和图像质量问题。

Method: 包括标准分辨率编辑、潜在投影和自适应上下文保留分块上采样三个阶段。

Result: 图像质量提升18-48%，幻觉减少14-51%，延迟显著降低（最高55.8倍加速），内存仅增加9%。

Conclusion: MobilePicasso在性能和效率上优于现有方法，甚至比基于服务器的A100 GPU模型更快。

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly
important for mobile applications. Existing diffusion models for image editing
face significant challenges, in terms of memory and image quality, when
deployed on resource-constrained devices. In this paper, we present
MobilePicasso, a novel system that enables efficient image editing at high
resolutions, while minimising computational cost and memory usage.
MobilePicasso comprises three stages: (i) performing image editing at a
standard resolution with hallucination-aware loss, (ii) applying latent
projection to overcome going to the pixel space, and (iii) upscaling the edited
image latent to a higher resolution with adaptive context-preserving tiling.
Our user study with 46 participants reveals that MobilePicasso not only
improves image quality by 18-48% but reduces hallucinations by 14-51% over
existing methods. MobilePicasso demonstrates significantly lower latency, e.g.,
up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g.,
a mere 9% increase over prior work. Surprisingly, the on-device runtime of
MobilePicasso is observed to be faster than a server-based high-resolution
image editing model running on an A100 GPU.

</details>


### [14] [RGBD Gaze Tracking Using Transformer for Feature Fusion](https://arxiv.org/abs/2510.06298)
*Tobias J. Bauer*

Main category: cs.CV

TL;DR: 论文实现了一种基于RGBD图像的AI视线追踪系统，使用Transformer架构融合特征，并创建了新数据集。模型在多个数据集上测试，结果显示MLP模块优于Transformer模块。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏深度信息或不适合视线角度估计，且RGBD与Transformer的结合尚未被研究。

Method: 采用Transformer模块融合RGBD特征，训练多种模型配置，并与GAN和MLP模块对比。

Result: 在ShanghaiTechGaze+数据集上，无GAN预训练的模型误差为30.1mm，MLP模块进一步降至26.9mm。Transformer模块在ETH-XGaze数据集上误差为3.59°，MLP为3.26°。

Conclusion: Transformer模块在视线追踪中表现不如MLP，未来可优化特征融合方法。

Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking
system using RGBD images that contain both color (RGB) and depth (D)
information. To fuse the features extracted from the images, a module based on
the Transformer architecture is used. The combination of RGBD input images and
Transformers was chosen because it has not yet been investigated. Furthermore,
a new dataset is created for training the AI models as existing datasets either
do not contain depth information or only contain labels for Gaze Point
Estimation that are not suitable for the task of Gaze Angle Estimation. Various
model configurations are trained, validated and evaluated on a total of three
different datasets. The trained models are then to be used in a real-time
pipeline to estimate the gaze direction and thus the gaze point of a person in
front of a computer screen. The AI model architecture used in this thesis is
based on an earlier work by Lian et al. It uses a Generative Adversarial
Network (GAN) to simultaneously remove depth map artifacts and extract head
pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their
own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a
Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm
on the same dataset, but we show that using no pre-trained GAN module leads to
a mean Euclidean error of 30.1mm. Replacing the Transformer module with a
Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are
coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the
model with Transformer module achieves a mean angular error of 3.59{\deg} and
without Transformer module 3.26{\deg}, whereas the fundamentally different
model architecture used by the dataset authors Zhang et al. achieves a mean
angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...

</details>


### [15] [Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping](https://arxiv.org/abs/2510.06299)
*Tiago de Conto,John Armston,Ralph Dubayah*

Main category: cs.CV

TL;DR: 该论文提出了一种结合GEDI和SAR数据的深度学习框架，用于高分辨率全球森林结构复杂性制图。


<details>
  <summary>Details</summary>
Motivation: 解决GEDI稀疏采样限制，实现连续高分辨率森林结构复杂性制图。

Method: 采用改进的EfficientNetV2架构，融合GEDI和SAR数据，训练超过1.3亿个GEDI足迹。

Result: 模型性能优异（全球R2=0.82），参数少于40万，支持多时相全球制图。

Conclusion: 该框架支持全球森林动态监测，为生物多样性保护和生态系统管理提供工具。

Abstract: Forest structural complexity metrics integrate multiple canopy attributes
into a single value that reflects habitat quality and ecosystem function.
Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has
enabled mapping of structural complexity in temperate and tropical forests, but
its sparse sampling limits continuous high-resolution mapping. We present a
scalable, deep learning framework fusing GEDI observations with multimodal
Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25
m) wall-to-wall maps of forest structural complexity. Our adapted
EfficientNetV2 architecture, trained on over 130 million GEDI footprints,
achieves high performance (global R2 = 0.82) with fewer than 400,000
parameters, making it an accessible tool that enables researchers to process
datasets at any scale without requiring specialized computing infrastructure.
The model produces accurate predictions with calibrated uncertainty estimates
across biomes and time periods, preserving fine-scale spatial patterns. It has
been used to generate a global, multi-temporal dataset of forest structural
complexity from 2015 to 2022. Through transfer learning, this framework can be
extended to predict additional forest structural variables with minimal
computational cost. This approach supports continuous, multi-temporal
monitoring of global forest structural dynamics and provides tools for
biodiversity conservation and ecosystem management efforts in a changing
climate.

</details>


### [16] [Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding](https://arxiv.org/abs/2510.06308)
*Yi Xin,Qi Qin,Siqi Luo,Kaiwen Zhu,Juncheng Yan,Yan Tai,Jiayi Lei,Yuewen Cao,Keqi Wang,Yibin Wang,Jinbin Bai,Qian Yu,Dengyang Jiang,Yuandong Pu,Haoxing Chen,Le Zhuo,Junjun He,Gen Luo,Tianbin Li,Ming Hu,Jin Ye,Shenglong Ye,Bo Zhang,Chang Xu,Wenhai Wang,Hongsheng Li,Guangtao Zhai,Tianfan Xue,Bin Fu,Xiaohong Liu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: Lumina-DiMOO是一种开源基础模型，通过离散扩散建模实现多模态生成和理解，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有统一多模态模型在采样效率和任务支持范围上的不足。

Method: 采用完全离散扩散建模处理多模态输入输出。

Result: 在多个基准测试中达到最先进性能。

Conclusion: 开源代码和检查点以促进多模态和离散扩散模型研究。

Abstract: We introduce Lumina-DiMOO, an open-source foundational model for seamless
multi-modal generation and understanding. Lumina-DiMOO sets itself apart from
prior unified models by utilizing a fully discrete diffusion modeling to handle
inputs and outputs across various modalities. This innovative approach allows
Lumina-DiMOO to achieve higher sampling efficiency compared to previous
autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a
broad spectrum of multi-modal tasks, including text-to-image generation,
image-to-image generation (e.g., image editing, subject-driven generation, and
image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves
state-of-the-art performance on multiple benchmarks, surpassing existing
open-source unified multi-modal models. To foster further advancements in
multi-modal and discrete diffusion model research, we release our code and
checkpoints to the community. Project Page:
https://synbol.github.io/Lumina-DiMOO.

</details>


### [17] [TransFIRA: Transfer Learning for Face Image Recognizability Assessment](https://arxiv.org/abs/2510.06353)
*Allen Tu,Kartik Narayan,Joshua Gleason,Jennifer Xu,Matthew Meyn,Tom Goldstein,Vishal M. Patel*

Main category: cs.CV

TL;DR: TransFIRA提出了一种轻量级、无标注的人脸图像可识别性评估框架，通过嵌入空间直接衡量可识别性，显著提升了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统人脸图像质量评估方法依赖视觉启发式或生成模型，无法与编码器的决策几何对齐，TransFIRA旨在解决这一问题。

Method: TransFIRA通过类中心相似性（CCS）和类中心角度分离（CCAS）定义可识别性，并提出一种基于可识别性的聚合策略。

Result: 在BRIAR和IJB-C数据集上达到最先进的验证准确率，且无需外部标注或特定训练。

Conclusion: TransFIRA是一个统一的、几何驱动的可识别性评估框架，适用于多种模态，显著提升了FIQA的准确性和范围。

Abstract: Face recognition in unconstrained environments such as surveillance, video,
and web imagery must contend with extreme variation in pose, blur,
illumination, and occlusion, where conventional visual quality metrics fail to
predict whether inputs are truly recognizable to the deployed encoder. Existing
FIQA methods typically rely on visual heuristics, curated annotations, or
computationally intensive generative pipelines, leaving their predictions
detached from the encoder's decision geometry. We introduce TransFIRA (Transfer
Learning for Face Image Recognizability Assessment), a lightweight and
annotation-free framework that grounds recognizability directly in embedding
space. TransFIRA delivers three advances: (i) a definition of recognizability
via class-center similarity (CCS) and class-center angular separation (CCAS),
yielding the first natural, decision-boundary--aligned criterion for filtering
and weighting; (ii) a recognizability-informed aggregation strategy that
achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly
doubling correlation with true recognizability, all without external labels,
heuristics, or backbone-specific training; and (iii) new extensions beyond
faces, including encoder-grounded explainability that reveals how degradations
and subject-specific factors affect recognizability, and the first
recognizability-aware body recognition assessment. Experiments confirm
state-of-the-art results on faces, strong performance on body recognition, and
robustness under cross-dataset shifts. Together, these contributions establish
TransFIRA as a unified, geometry-driven framework for recognizability
assessment -- encoder-specific, accurate, interpretable, and extensible across
modalities -- significantly advancing FIQA in accuracy, explainability, and
scope.

</details>


### [18] [Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data](https://arxiv.org/abs/2510.06440)
*Carly Sutter,Kara J. Sulia,Nick P. Bassill,Christopher D. Wirz,Christopher D. Thorncroft,Jay C. Rothenberger,Vanessa Przybylo,Mariana G. Cains,Jacob Radford,David Aaron Evans*

Main category: cs.CV

TL;DR: NYSDOT利用机器学习和交通摄像头数据自动分类道路状况，模型在未见过的摄像头数据上准确率达81.5%。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估道路状况耗时耗力，机器学习可提供高效支持。

Method: 使用卷积神经网络和随机森林，结合摄像头图像和天气数据训练模型。

Result: 模型在未见过的摄像头数据上准确率为81.5%。

Conclusion: 机器学习可有效辅助NYSDOT决策，提升道路状况评估效率。

Abstract: The New York State Department of Transportation (NYSDOT) has a network of
roadside traffic cameras that are used by both the NYSDOT and the public to
observe road conditions. The NYSDOT evaluates road conditions by driving on
roads and observing live cameras, tasks which are labor-intensive but necessary
for making critical operational decisions during winter weather events.
However, machine learning models can provide additional support for the NYSDOT
by automatically classifying current road conditions across the state. In this
study, convolutional neural networks and random forests are trained on camera
images and weather data to predict road surface conditions. Models are trained
on a hand-labeled dataset of ~22,000 camera images, each classified by human
labelers into one of six road surface conditions: severe snow, snow, wet, dry,
poor visibility, or obstructed. Model generalizability is prioritized to meet
the operational needs of the NYSDOT decision makers, and the weather-related
road surface condition model in this study achieves an accuracy of 81.5% on
completely unseen cameras.

</details>


### [19] [TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion](https://arxiv.org/abs/2510.06460)
*Piyush Dashpute,Niki Nezakati,Wolfgang Heidrich,Vishwanath Saragadam*

Main category: cs.CV

TL;DR: 提出了一种基于patch的扩散框架（TDiff），用于解决低成本热成像相机图像的低分辨率、固定模式噪声等问题。


<details>
  <summary>Details</summary>
Motivation: 低成本热成像相机图像存在低分辨率、固定模式噪声等局部退化问题，且现有数据集规模小、多样性不足。

Method: 采用基于patch的扩散框架，通过在小热图像patch上训练，利用重叠patch去噪和平滑空间窗口融合来恢复全分辨率图像。

Result: 在去噪、超分辨率和去模糊任务中，模拟和真实热数据上均表现出色，成为统一的恢复流程。

Conclusion: TDiff是首个基于patch的扩散框架，能够跨多个任务建模热图像恢复的先验知识。

Abstract: Thermal images from low-cost cameras often suffer from low resolution, fixed
pattern noise, and other localized degradations. Available datasets for thermal
imaging are also limited in both size and diversity. To address these
challenges, we propose a patch-based diffusion framework (TDiff) that leverages
the local nature of these distortions by training on small thermal patches. In
this approach, full-resolution images are restored by denoising overlapping
patches and blending them using smooth spatial windowing. To our knowledge,
this is the first patch-based diffusion framework that models a learned prior
for thermal image restoration across multiple tasks. Experiments on denoising,
super-resolution, and deblurring demonstrate strong results on both simulated
and real thermal data, establishing our method as a unified restoration
pipeline.

</details>


### [20] [SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation](https://arxiv.org/abs/2510.06469)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Kevin Blackburn-Matzen,Matheus Gadelha*

Main category: cs.CV

TL;DR: SIGMA-GEN是一个统一框架，用于多身份保留的图像生成，支持单次多主体生成，结合结构和空间约束。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法在单次生成中同时保留多个身份，且缺乏灵活的用户指导。

Method: 提出SIGMA-GEN框架，结合结构和空间约束，支持从粗粒度到像素级的用户指导。

Result: 通过SIGMA-SET27K数据集验证，SIGMA-GEN在身份保留、图像质量和速度上达到最优。

Conclusion: SIGMA-GEN为多身份图像生成提供了高效且灵活的解决方案。

Abstract: We present SIGMA-GEN, a unified framework for multi-identity preserving image
generation. Unlike prior approaches, SIGMA-GEN is the first to enable
single-pass multi-subject identity-preserved generation guided by both
structural and spatial constraints. A key strength of our method is its ability
to support user guidance at various levels of precision -- from coarse 2D or 3D
boxes to pixel-level segmentations and depth -- with a single model. To enable
this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides
identity, structure, and spatial information for over 100k unique subjects
across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN
achieves state-of-the-art performance in identity preservation, image
generation quality, and speed. Code and visualizations at
https://oindrilasaha.github.io/SIGMA-Gen/

</details>


### [21] [Superpixel Integrated Grids for Fast Image Segmentation](https://arxiv.org/abs/2510.06487)
*Jack Roberts,Jeova Farias Sales Rocha Neto*

Main category: cs.CV

TL;DR: SIGRID（Superpixel-Integrated Grid）是一种新的超像素数据结构，用于分割任务，通过结合颜色和形状信息，显著降低输入维度，同时保持或超越像素级表示的准确性。


<details>
  <summary>Details</summary>
Motivation: 超像素在图像简化中具有计算潜力，但其不规则空间分布限制了深度学习应用。SIGRID旨在解决这一问题，提供更高效的数据处理方式。

Method: 利用经典形状描述符，SIGRID编码超像素的颜色和形状信息，并大幅降低输入维度。

Result: 在四个基准数据集上，SIGRID不仅匹配甚至在某些情况下超越像素级表示的性能，同时显著加速模型训练。

Conclusion: SIGRID在准确性和计算效率之间实现了良好的平衡，为分割任务提供了高效的数据结构。

Abstract: Superpixels have long been used in image simplification to enable more
efficient data processing and storage. However, despite their computational
potential, their irregular spatial distribution has often forced deep learning
approaches to rely on specialized training algorithms and architectures,
undermining the original motivation for superpixelations. In this work, we
introduce a new superpixel-based data structure, SIGRID (Superpixel-Integrated
Grid), as an alternative to full-resolution images in segmentation tasks. By
leveraging classical shape descriptors, SIGRID encodes both color and shape
information of superpixels while substantially reducing input dimensionality.
We evaluate SIGRIDs on four benchmark datasets using two popular convolutional
segmentation architectures. Our results show that, despite compressing the
original data, SIGRIDs not only match but in some cases surpass the performance
of pixel-level representations, all while significantly accelerating model
training. This demonstrates that SIGRIDs achieve a favorable balance between
accuracy and computational efficiency.

</details>


### [22] [Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation](https://arxiv.org/abs/2510.06504)
*Qingxuan Wu,Zhiyang Dou,Chuan Guo,Yiming Huang,Qiao Feng,Bing Zhou,Jian Wang,Lingjie Liu*

Main category: cs.CV

TL;DR: Text2Interact框架通过可扩展的高保真交互数据合成器和时空协调管道，生成真实且与文本对齐的人-人交互。


<details>
  <summary>Details</summary>
Motivation: 当前方法在建模人-人交互时面临两个主要问题：1) 训练数据不足；2) 文本到交互的建模不够精细。

Method: 提出InterCompose（合成-组合管道）和InterActor（文本到交互模型），分别解决数据不足和细粒度建模问题。

Result: 实验显示在运动多样性、保真度和泛化能力上均有显著提升。

Conclusion: Text2Interact框架有效解决了人-人交互建模中的关键挑战，并展示了优越性能。

Abstract: Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.

</details>


### [23] [From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring](https://arxiv.org/abs/2510.06509)
*Shih-Yao Lin,Sibendu Paul,Caren Chen*

Main category: cs.CV

TL;DR: KeyScore和STACFP框架通过多模态对齐实现高效视频理解，显著减少帧数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 长视频处理需要选择保留语义和上下文信息的关键帧，以提高视频语言理解的效率。

Method: 提出KeyScore框架结合语义相似性、时间多样性和上下文影响评分帧重要性，STACFP生成紧凑多样的候选帧。

Result: 在MSRVTT、MSVD和DiDeMo数据集上，帧数减少99%，性能优于标准8帧编码器。

Conclusion: 多模态对齐可实现高效、可扩展的视频理解，无需显式视频摘要。

Abstract: Efficient video-language understanding requires selecting a small set of
frames that retain semantic and contextual information from long videos. We
propose KeyScore, a multimodal frame scoring framework that jointly leverages
captions and visual context to estimate frame-level importance. By combining
semantic similarity, temporal diversity, and contextual drop impact, KeyScore
identifies the most informative frames for downstream tasks such as retrieval,
captioning, and video-language reasoning. To complement KeyScore, we introduce
STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which
generates compact and diverse frame candidates for long-form videos. Together,
these modules achieve up to 99\% frame reduction compared to full-frame
inference and substantially outperform standard 8-frame encoders on MSRVTT,
MSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment
between visual and textual signals enables scalable, efficient, and
caption-grounded video understanding -- without explicit video summarization.

</details>


### [24] [Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation](https://arxiv.org/abs/2510.06582)
*Fei Zhang,Rob Chancia,Josie Clapp,Amirhossein Hassanzadeh,Dimah Dera,Richard MacKenzie,Jan van Aardt*

Main category: cs.CV

TL;DR: 提出了一种半自动、不确定性感知的流程，结合球形投影、特征增强、集成学习和定向标注，减少标注工作量并保持高精度。构建了Mangrove3D数据集，验证了数据效率和特征重要性。


<details>
  <summary>Details</summary>
Motivation: 解决TLS点云语义分割中手动标注成本高的问题，提出高效且准确的半自动化解决方案。

Method: 通过球形投影将3D点云转为2D网格，增强多源特征，训练集成网络生成伪标签和不确定性图，指导标注模糊区域。

Result: 性能在约12次标注扫描后饱和，几何特征贡献最大，mIoU稳定在0.76左右。

Conclusion: 提出的流程和数据集支持高质量TLS点云分割，适用于生态监测等领域。

Abstract: Accurate semantic segmentation of terrestrial laser scanning (TLS) point
clouds is limited by costly manual annotation. We propose a semi-automated,
uncertainty-aware pipeline that integrates spherical projection, feature
enrichment, ensemble learning, and targeted annotation to reduce labeling
effort, while sustaining high accuracy. Our approach projects 3D points to a 2D
spherical grid, enriches pixels with multi-source features, and trains an
ensemble of segmentation networks to produce pseudo-labels and uncertainty
maps, the latter guiding annotation of ambiguous regions. The 2D outputs are
back-projected to 3D, yielding densely annotated point clouds supported by a
three-tier visualization suite (2D feature maps, 3D colorized point clouds, and
compact virtual spheres) for rapid triage and reviewer guidance. Using this
pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove
forests. We further evaluate data efficiency and feature importance to address
two key questions: (1) how much annotated data are needed and (2) which
features matter most. Results show that performance saturates after ~12
annotated scans, geometric features contribute the most, and compact
nine-channel stacks capture nearly all discriminative power, with the mean
Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm
the generalization of our feature-enrichment strategy through cross-dataset
tests on ForestSemantic and Semantic3D.
  Our contributions include: (i) a robust, uncertainty-aware TLS annotation
pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)
empirical guidance on data efficiency and feature importance, thus enabling
scalable, high-quality segmentation of TLS point clouds for ecological
monitoring and beyond. The dataset and processing scripts are publicly
available at https://fz-rit.github.io/through-the-lidars-eye/.

</details>


### [25] [LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval](https://arxiv.org/abs/2510.06512)
*Avishree Khare,Hideki Okamoto,Bardh Hoxha,Georgios Fainekos,Rajeev Alur*

Main category: cs.CV

TL;DR: 论文提出了一种名为LogSTOP的评分函数，用于高效计算时间属性得分，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 将局部属性检测的得分提升为时间属性得分，有助于下游应用如查询匹配和排序检索。

Method: 使用LogSTOP评分函数，基于线性时序逻辑表示时间属性。

Result: LogSTOP在视频对象和语音情感的时间属性查询匹配中优于基线16%以上，在排序检索中提升19%的精度和16%的召回率。

Conclusion: LogSTOP是一种高效且有效的方法，适用于时间属性得分的计算，显著提升了性能。

Abstract: Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects ("car") and emotions ("angry") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
"does the speaker eventually sound happy in this audio clip?"), and ranked
retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected"). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.

</details>


### [26] [HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation](https://arxiv.org/abs/2510.06876)
*Samir Abou Haidar,Alexandre Chariot,Mehdi Darouich,Cyril Joly,Jean-Emmanuel Deschaud*

Main category: cs.CV

TL;DR: HARP-NeXt是一种高速且准确的LiDAR语义分割网络，通过优化预处理和特征提取，实现了速度和精度的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在精度和速度之间存在权衡，且预处理阶段耗时，HARP-NeXt旨在解决这些问题。

Method: 提出新型预处理方法和Conv-SE-NeXt特征提取块，结合多尺度范围点融合骨干网络。

Result: 在nuScenes和SemanticKITTI基准测试中，HARP-NeXt在不依赖TTA的情况下，速度提升24倍，精度媲美PTv3。

Conclusion: HARP-NeXt在LiDAR语义分割中实现了高效且准确的性能，适用于资源受限的嵌入式系统。

Abstract: LiDAR semantic segmentation is crucial for autonomous vehicles and mobile
robots, requiring high accuracy and real-time processing, especially on
resource-constrained embedded systems. Previous state-of-the-art methods often
face a trade-off between accuracy and speed. Point-based and sparse
convolution-based methods are accurate but slow due to the complexity of
neighbor searching and 3D convolutions. Projection-based methods are faster but
lose critical geometric information during the 2D projection. Additionally,
many recent methods rely on test-time augmentation (TTA) to improve
performance, which further slows the inference. Moreover, the pre-processing
phase across all methods increases execution time and is demanding on embedded
platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network. We first propose a novel pre-processing
methodology that significantly reduces computational overhead. Then, we design
the Conv-SE-NeXt feature extraction block to efficiently capture
representations without deep layer stacking per network stage. We also employ a
multi-scale range-point fusion backbone that leverages information at multiple
abstraction levels to preserve essential geometric details, thereby enhancing
accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24$\times$ faster. The code is
available at https://github.com/SamirAbouHaidar/HARP-NeXt

</details>


### [27] [Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion](https://arxiv.org/abs/2510.06516)
*Zhantao Deng,Mériem Er-Rafik,Anna Sushko,Cécile Hébert,Pascal Fua*

Main category: cs.CV

TL;DR: TEMDiff是一种基于扩散的3D重建框架，通过模拟FIB-SEM数据生成训练数据，解决了电子显微镜中缺失楔形问题，无需高质量真实数据即可实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 解决电子显微镜中有限角度断层扫描的缺失楔形问题，避免依赖难以获取的高质量真实数据。

Method: 使用FIB-SEM数据通过模拟器生成TEM倾斜系列，训练3D扩散模型，直接在3D体积上操作，无需额外正则化。

Result: 在模拟数据集上优于现有方法，且能泛化到真实TEM数据，仅需8度倾斜范围即可恢复准确结构。

Conclusion: TEMDiff为有限角度电子断层扫描提供了一种高效且泛化性强的解决方案。

Abstract: Limited-angle electron tomography aims to reconstruct 3D shapes from 2D
projections of Transmission Electron Microscopy (TEM) within a restricted range
and number of tilting angles, but it suffers from the missing-wedge problem
that causes severe reconstruction artifacts. Deep learning approaches have
shown promising results in alleviating these artifacts, yet they typically
require large high-quality training datasets with known 3D ground truth which
are difficult to obtain in electron microscopy. To address these challenges, we
propose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework.
Our method is trained on readily available volumetric FIB-SEM data using a
simulator that maps them to TEM tilt series, enabling the model to learn
realistic structural priors without requiring clean TEM ground truth. By
operating directly on 3D volumes, TEMDiff implicitly enforces consistency
across slices without the need for additional regularization. On simulated
electron tomography datasets with limited angular coverage, TEMDiff outperforms
state-of-the-art methods in reconstruction quality. We further demonstrate that
a trained TEMDiff model generalizes well to real-world TEM tilts obtained under
different conditions and can recover accurate structures from tilt ranges as
narrow as 8 degrees, with 2-degree increments, without any retraining or
fine-tuning.

</details>


### [28] [VUGEN: Visual Understanding priors for GENeration](https://arxiv.org/abs/2510.06529)
*Xiangyi Chen,Théophane Vallaeys,Maha Elbayad,John Nguyen,Jakob Verbeek*

Main category: cs.CV

TL;DR: VUGEN框架利用预训练的视觉语言模型（VLM）的视觉先验知识，通过降维和采样实现高效高质量的图像生成，避免了传统方法的复杂性和对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像生成中常依赖复杂的重建机制或桥接方法，导致理解与生成表示不对齐或架构复杂。VUGEN旨在解决这些问题。

Method: VUGEN将VLM的高维潜在空间降维为低维分布，训练VLM在该空间采样，并使用像素扩散解码器生成图像。

Result: VUGEN在COCO数据集上显著提升图像生成性能（DPG Bench从71.17到74.32，FID从11.86到9.06），同时保留VLM的原始理解能力。

Conclusion: VUGEN通过利用VLM的视觉先验知识，实现了高效且高质量的图像生成，避免了传统方法的复杂性。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled unified
understanding across text and images, yet equipping these models with robust
image generation capabilities remains challenging. Existing approaches often
rely on reconstruction-oriented autoencoders or complex bridging mechanisms,
leading to misalignment between understanding and generation representations,
or architectural complexity. In this work, we propose VUGEN, a novel framework
that explicitly leverages VLM's pretrained visual understanding priors for
efficient and high-quality image generation. Our approach first transforms the
high-dimensional latent space of the VLM's native vision encoder into a
lower-dimensional, tractable distribution that maximally preserves visual
information. The VLM is then trained to sample within this reduced latent
space, ensuring alignment with its visual understanding capabilities. Finally,
a dedicated pixel decoder maps these generated latents back to the image space.
We find that a VAE-free pixel diffusion decoder to be on par or better than
commonly used complex latent diffusion decoders that internally rely on VAE
latents. Extensive experiments demonstrate that VUGEN achieves superior image
generation performance, improving DPG Bench from 71.17 to 74.32 and FID from
11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding
capabilities.

</details>


### [29] [Cluster Paths: Navigating Interpretability in Neural Networks](https://arxiv.org/abs/2510.06541)
*Nicholas M. Kroeger,Vincent Bindschaedler*

Main category: cs.CV

TL;DR: 提出了一种名为cluster paths的后置解释方法，通过聚类激活层生成可解释的路径，并引入四个评估指标。实验表明该方法能有效识别模型决策中的偏差，并扩展到大规模视觉模型中。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络在视觉任务中表现优异，但其决策过程不透明，可能导致信任问题、未检测到的偏差和意外失败。

Method: 提出cluster paths方法，聚类选定层的激活，并将输入表示为聚类ID序列。引入四个评估指标：路径复杂性、加权路径纯度、决策对齐忠实度和路径一致性。

Result: 在CIFAR-10和CelebA实验中，cluster paths能识别偏差并保持高忠实度和一致性。扩展到ImageNet预训练的Vision Transformer后，生成概念路径。还可用于异常检测。

Conclusion: cluster paths能生成简洁、可读的解释，适用于大规模视觉模型，并揭示多层次的视觉概念。

Abstract: While modern deep neural networks achieve impressive performance in vision
tasks, they remain opaque in their decision processes, risking unwarranted
trust, undetected biases and unexpected failures. We propose cluster paths, a
post-hoc interpretability method that clusters activations at selected layers
and represents each input as its sequence of cluster IDs. To assess these
cluster paths, we introduce four metrics: path complexity (cognitive load),
weighted-path purity (class alignment), decision-alignment faithfulness
(predictive fidelity), and path agreement (stability under perturbations). In a
spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts
and collapse when the cue is removed. On a five-class CelebA hair-color task,
they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise
without sacrificing accuracy. Scaling to a Vision Transformer pretrained on
ImageNet, we extend cluster paths to concept paths derived from prompting a
large language model on minimal path divergences. Finally, we show that cluster
paths can serve as an effective out-of-distribution (OOD) detector, reliably
flagging anomalous samples before the model generates over-confident
predictions. Cluster paths uncover visual concepts, such as color palettes,
textures, or object contexts, at multiple network depths, demonstrating that
cluster paths scale to large vision models while generating concise and
human-readable explanations.

</details>


### [30] [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)
*Qiongyang Hu,Wenyang Liu,Wenbin Zou,Yuejiao Su,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: HSNet提出了一种基于子图分解的图网络框架，用于图像超分辨率，平衡了计算效率和重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和注意力机制的图像超分辨率方法存在结构不灵活问题，而图方法计算复杂度过高。

Method: HSNet通过CSSB生成互补子图，SAB聚合子图特征，NSS选择关键特征，降低计算开销。

Result: 实验表明HSNet在重建质量和计算效率上达到最优性能。

Conclusion: HSNet通过子图分解和特征选择，有效解决了图方法的高计算复杂度问题。

Abstract: Existing deep learning approaches for image super-resolution, particularly
those based on CNNs and attention mechanisms, often suffer from structural
inflexibility. Although graph-based methods offer greater representational
adaptability, they are frequently impeded by excessive computational
complexity. To overcome these limitations, this paper proposes the
Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently
leverages graph modeling while maintaining computational feasibility. The core
idea of HSNet is to decompose the global graph into manageable sub-components.
First, we introduce the Constructive Subgraph Set Block (CSSB), which generates
a diverse set of complementary subgraphs. Rather than relying on a single
monolithic graph, CSSB captures heterogeneous characteristics of the image by
modeling different relational patterns and feature interactions, producing a
rich ensemble of both local and global graph structures. Subsequently, the
Subgraph Aggregation Block (SAB) integrates the representations embedded across
these subgraphs. Through adaptive weighting and fusion of multi-graph features,
SAB constructs a comprehensive and discriminative representation that captures
intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is
designed to selectively retain the most salient features, thereby enhancing
accuracy while reducing computational overhead. Extensive experiments
demonstrate that HSNet achieves state-of-the-art performance, effectively
balancing reconstruction quality with computational efficiency. The code will
be made publicly available.

</details>


### [31] [WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation](https://arxiv.org/abs/2510.07313)
*Zezhong Qian,Xiaowei Chi,Yuming Li,Shizun Wang,Zhiyuan Qin,Xiaozhu Ju,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: WristWorld是首个仅通过锚点视图生成腕部视图视频的4D世界模型，通过几何一致性和视频生成提升VLA性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模数据集缺乏腕部视图记录，导致锚点视图与腕部视图之间存在巨大差距，现有世界模型无法解决这一问题。

Method: WristWorld分两阶段：1) 利用VGGT和SPC损失重建几何一致的腕部视图姿态和4D点云；2) 通过视频生成模型合成时序连贯的腕部视图视频。

Result: 在Droid、Calvin和Franka Panda数据集上实现最先进的视频生成，空间一致性优异，VLA任务完成长度提升3.81%，缩小42.4%的视图差距。

Conclusion: WristWorld填补了锚点视图与腕部视图之间的技术空白，显著提升了VLA性能。

Abstract: Wrist-view observations are crucial for VLA models as they capture
fine-grained hand-object interactions that directly enhance manipulation
performance. Yet large-scale datasets rarely include such recordings, resulting
in a substantial gap between abundant anchor views and scarce wrist views.
Existing world models cannot bridge this gap, as they require a wrist-view
first frame and thus fail to generate wrist-view videos from anchor views
alone. Amid this gap, recent visual geometry models such as VGGT emerge with
geometric and cross-view priors that make it possible to address extreme
viewpoint shifts. Inspired by these insights, we propose WristWorld, the first
4D world model that generates wrist-view videos solely from anchor views.
WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and
incorporates our Spatial Projection Consistency (SPC) Loss to estimate
geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,
which employs our video generation model to synthesize temporally coherent
wrist-view videos from the reconstructed perspective. Experiments on Droid,
Calvin, and Franka Panda demonstrate state-of-the-art video generation with
superior spatial consistency, while also improving VLA performance, raising the
average task completion length on Calvin by 3.81% and closing 42.4% of the
anchor-wrist view gap.

</details>


### [32] [Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation](https://arxiv.org/abs/2510.06584)
*Justin Cheung,Samuel Savine,Calvin Nguyen,Lin Lu,Alhassan S. Yasin*

Main category: cs.CV

TL;DR: 研究评估了领域适应方法在CT图像分类中应对新伪影的能力，无需额外标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在新伪影分布下性能下降的问题，避免昂贵的标注成本。

Method: 通过模拟环形伪影，比较领域对抗神经网络（DANN）与基线及数据增强方法。

Result: DANN在未标注伪影数据下保持高分类准确率，性能接近标注数据训练的模型。

Conclusion: 领域适应可有效应对医学影像中的分布偏移，无需昂贵标注，适合临床部署。

Abstract: Deep learning models which perform well on images from their training
distribution can degrade substantially when applied to new distributions. If a
CT scanner introduces a new artifact not present in the training labels, the
model may misclassify the images. Although modern CT scanners include design
features which mitigate these artifacts, unanticipated or difficult-to-mitigate
artifacts can still appear in practice. The direct solution of labeling images
from this new distribution can be costly. As a more accessible alternative,
this study evaluates domain adaptation as an approach for training models that
maintain classification performance despite new artifacts, even without
corresponding labels. We simulate ring artifacts from detector gain error in
sinogram space and evaluate domain adversarial neural networks (DANN) against
baseline and augmentation-based approaches on the OrganAMNIST abdominal CT
dataset. Our results demonstrate that baseline models trained only on clean
images fail to generalize to images with ring artifacts, and traditional
augmentation with other distortion types provides no improvement on unseen
artifact domains. In contrast, the DANN approach successfully maintains high
classification accuracy on ring artifact images using only unlabeled artifact
data during training, demonstrating the viability of domain adaptation for
artifact robustness. The domain-adapted model achieved classification
performance on ring artifact test data comparable to models explicitly trained
with labeled artifact images, while also showing unexpected generalization to
uniform noise. These findings provide empirical evidence that domain adaptation
can effectively address distribution shift in medical imaging without requiring
expensive expert labeling of new artifact distributions, suggesting promise for
deployment in clinical settings where novel artifacts may emerge.

</details>


### [33] [Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer](https://arxiv.org/abs/2510.06590)
*Ziyuan Huang,DanDan Zheng,Cheng Zou,Rui Liu,Xiaolong Wang,Kaixiang Ji,Weilong Chai,Jianxin Sun,Libin Wang,Yongjie Lv,Taozhi Huang,Jiajia Liu,Qingpei Guo,Ming Yang,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: MingTok是一种新型视觉分词器，采用连续潜在空间，统一自回归生成和理解任务，解决了现有离散潜在空间分词器的量化误差问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器在离散潜在空间中存在量化误差，限制了语义表达能力和视觉语言理解能力。

Method: MingTok采用三阶段架构（低层编码、语义扩展和视觉重建），并通过Ming-UniVision统一视觉语言任务。

Result: 实验表明，统一的连续视觉表示在理解和生成任务中均达到最先进水平。

Conclusion: MingTok为连续域中的统一视觉分词提供了有效解决方案，代码和模型已开源。

Abstract: Visual tokenization remains a core challenge in unifying visual understanding
and generation within the autoregressive paradigm. Existing methods typically
employ tokenizers in discrete latent spaces to align with the tokens from large
language models, where the quantization errors can limit semantic
expressiveness and degrade the capability of vision-language understanding. To
address this, we introduce MingTok, a new family of visual tokenizers with a
continuous latent space, for unified autoregressive generation and
understanding. While understanding tasks favor discriminative high-dimensional
features, generation tasks prefer compact low-level codes. Thus, to reconcile
these competing demands, MingTok adopts a three-stage sequential architecture
involving low-level encoding, semantic expansion, and visual reconstruction.
Built on top of it, Ming-UniVision eliminates the need for task-specific visual
representations, and unifies diverse vision-language tasks under a single
autoregrsssive prediction paradigm. By formulating both understanding and
generation as next-token prediction in a shared continuous space, it seamlessly
supports multi-round, in-context tasks such as iterative understanding,
generation and editing. Empirically, we find that using a unified continuous
visual representation reconciles the competing requirements on the tokenizers
by the understanding and generation tasks, thereby leading to state-of-the-art
level performance across both domains. We hope our findings will facilitate
unified visual tokenization in the continuous domain. Inference code and model
weights are released to benefit community.

</details>


### [34] [Adaptive Stain Normalization for Cross-Domain Medical Histology](https://arxiv.org/abs/2510.06592)
*Tianyue Xu,Yanlin Wu,Abhai K. Tripathi,Matthew M. Ippolito,Benjamin D. Haeffele*

Main category: cs.CV

TL;DR: 提出了一种可训练的颜色归一化模型，用于解决数字病理学中的颜色不一致问题，提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 解决数字病理学中因染色协议和成像条件差异导致的颜色不一致问题，避免现有方法的缺陷。

Method: 基于Beer-Lambert定律，通过非负矩阵分解（NMF）模型展开算法提取染色不变的结构信息。

Result: 在公开病理数据集和内部疟疾血涂片数据上，跨域目标检测和分类性能优于现有方法。

Conclusion: 提出的方法有效解决了颜色不一致问题，提升了模型在跨域任务中的表现。

Abstract: Deep learning advances have revolutionized automated digital pathology
analysis. However, differences in staining protocols and imaging conditions can
introduce significant color variability. In deep learning, such color
inconsistency often reduces performance when deploying models on data acquired
under different conditions from the training data, a challenge known as domain
shift. Many existing methods attempt to address this problem via color
normalization but suffer from several notable drawbacks such as introducing
artifacts or requiring careful choice of a template image for stain mapping. To
address these limitations, we propose a trainable color normalization model
that can be integrated with any backbone network for downstream tasks such as
object detection and classification. Based on the physics of the imaging
process per the Beer-Lambert law, our model architecture is derived via
algorithmic unrolling of a nonnegative matrix factorization (NMF) model to
extract stain-invariant structural information from the original pathology
images, which serves as input for further processing. Experimentally, we
evaluate the method on publicly available pathology datasets and an internally
curated collection of malaria blood smears for cross-domain object detection
and classification, where our method outperforms many state-of-the-art stain
normalization methods. Our code is available at
https://github.com/xutianyue/BeerLaNet.

</details>


### [35] [SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation](https://arxiv.org/abs/2510.06596)
*Ayush Zenith,Arnold Zumbrun,Neel Raut,Jing Lin*

Main category: cs.CV

TL;DR: 论文提出了一种名为SDQM的合成数据集质量评估指标，用于无需模型训练即可评估对象检测任务中合成数据的质量。


<details>
  <summary>Details</summary>
Motivation: 由于大规模、高质量标注数据集的稀缺性，合成数据成为提升模型性能的解决方案，但缺乏有效的评估指标。

Method: 提出SDQM指标，通过实验验证其与YOLOv11模型的mAP分数相关性。

Result: SDQM与mAP分数强相关，优于现有指标，并能提供改进数据集质量的建议。

Conclusion: SDQM为合成数据评估提供了高效、可扩展的新标准，减少迭代训练成本。

Abstract: The performance of machine learning models depends heavily on training data.
The scarcity of large-scale, well-annotated datasets poses significant
challenges in creating robust models. To address this, synthetic data generated
through simulations and generative models has emerged as a promising solution,
enhancing dataset diversity and improving the performance, reliability, and
resilience of models. However, evaluating the quality of this generated data
requires an effective metric. This paper introduces the Synthetic Dataset
Quality Metric (SDQM) to assess data quality for object detection tasks without
requiring model training to converge. This metric enables more efficient
generation and selection of synthetic datasets, addressing a key challenge in
resource-constrained object detection tasks. In our experiments, SDQM
demonstrated a strong correlation with the mean Average Precision (mAP) scores
of YOLOv11, a leading object detection model, while previous metrics only
exhibited moderate or weak correlations. Additionally, it provides actionable
insights for improving dataset quality, minimizing the need for costly
iterative training. This scalable and efficient metric sets a new standard for
evaluating synthetic data. The code for SDQM is available at
https://github.com/ayushzenith/SDQM

</details>


### [36] [AIM 2025 Challenge on Real-World RAW Image Denoising](https://arxiv.org/abs/2510.06601)
*Feiran Li,Jiacheng Li,Marcos V. Conde,Beril Besbinar,Vlad Hosu,Daisuke Iso,Radu Timofte*

Main category: cs.CV

TL;DR: AIM 2025挑战赛旨在通过数据合成推动RAW图像去噪技术的发展，参赛者需开发噪声合成、网络架构和训练方法，优胜者根据多项指标评定。


<details>
  <summary>Details</summary>
Motivation: 推动基于数据合成的高效去噪技术，应对低光环境下不同相机模型的RAW图像去噪挑战。

Method: 建立新评估基准，包含五种DSLR相机拍摄的低光噪声图像，参赛者开发噪声合成流程、网络架构和训练方法。

Result: 通过比赛促进相机无关的低光RAW图像去噪技术发展，优胜者根据PSNR、SSIM等指标评定。

Conclusion: 比赛成果将影响图像修复和夜间自动驾驶等领域，推动数字摄影技术的进步。

Abstract: We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to
advance efficient and effective denoising techniques grounded in data
synthesis. The competition is built upon a newly established evaluation
benchmark featuring challenging low-light noisy images captured in the wild
using five different DSLR cameras. Participants are tasked with developing
novel noise synthesis pipelines, network architectures, and training
methodologies to achieve high performance across different camera models.
Winners are determined based on a combination of performance metrics, including
full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA,
TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image
denoising trained on synthetic data, the competition promotes the development
of robust and practical models aligned with the rapid progress in digital
photography. We expect the competition outcomes to influence multiple domains,
from image restoration to night-time autonomous driving.

</details>


### [37] [Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction](https://arxiv.org/abs/2510.06611)
*Jingran Xu,Yuanyuan Liu,Yanjie Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为UnrollINR的零样本自监督MRI重建框架，无需依赖外部训练数据，通过结合物理引导的展开迭代重建架构和隐式神经表示（INR）作为正则化先验，显著提升了重建性能。


<details>
  <summary>Details</summary>
Motivation: MRI扫描时间长限制了其广泛应用，而传统方法在缺乏全采样数据时效果不佳。自监督和无监督学习方法在此场景下显示出潜力。

Method: 采用物理引导的展开迭代重建架构，引入INR作为正则化先验，结合深度展开结构和INR的隐式表示能力，提升模型解释性和重建性能。

Result: 实验表明，即使在10倍加速率下，UnrollINR仍优于监督学习方法。

Conclusion: UnrollINR在无需外部数据的情况下实现了高性能MRI重建，验证了其优越性。

Abstract: Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its
widespread application is limited by prolonged scan times. Fast MRI
reconstruction techniques effectively reduce acquisition duration by
reconstructing high-fidelity MR images from undersampled k-space data. In
recent years, deep learning-based methods have demonstrated remarkable progress
in this field, with self-supervised and unsupervised learning approaches
proving particularly valuable in scenarios where fully sampled data are
difficult to obtain. This paper proposes a novel zero-shot self-supervised
reconstruction framework named UnrollINR, which enables scan-specific MRI
reconstruction without relying on external training data. The method adopts a
physics-guided unrolled iterative reconstruction architecture and introduces
Implicit Neural Representation (INR) as a regularization prior to effectively
constrain the solution space. By combining a deep unrolled structure with the
powerful implicit representation capability of INR, the model's
interpretability and reconstruction performance are enhanced. Experimental
results demonstrate that even at a high acceleration rate of 10, UnrollINR
achieves superior reconstruction performance compared to the supervised
learning method, validating the superiority of the proposed method.

</details>


### [38] [A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages](https://arxiv.org/abs/2510.06612)
*Zibo Su,Kun Wei,Jiahua Li,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: MuEx框架通过音素和视素作为中介，实现了多语言语音驱动面部动画合成，解决了现有模型在非英语语言中的表现不佳问题。


<details>
  <summary>Details</summary>
Motivation: 当前语音驱动面部动画合成模型在非英语语言中表现不佳，主要由于训练数据以英语为主且缺乏跨语言泛化能力。

Method: 提出MuEx框架，采用音素和视素作为通用中介，引入PV-Align机制解决音视频同步问题，并构建多语言数据集MTFB。

Result: MuEx在MTFB数据集上表现优异，且能零样本泛化到未见过的语言。

Conclusion: MuEx通过音素和视素的中介作用，显著提升了多语言面部动画合成的效果和泛化能力。

Abstract: Speech-driven talking face synthesis (TFS) focuses on generating lifelike
facial animations from audio input. Current TFS models perform well in English
but unsatisfactorily in non-English languages, producing wrong mouth shapes and
rigid facial expressions. The terrible performance is caused by the
English-dominated training datasets and the lack of cross-language
generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel
framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture
that employs phonemes and visemes as universal intermediaries to bridge audio
and video modalities, achieving lifelike multilingual TFS. To alleviate the
influence of linguistic differences and dataset bias, we extract audio and
video features as phonemes and visemes respectively, which are the basic units
of speech sounds and mouth movements. To address audiovisual synchronization
issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which
establishes robust cross-modal correspondences between phonemes and visemes. In
addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12
diverse languages with 95.04 hours of high-quality videos for training and
evaluating multilingual TFS performance. Extensive experiments demonstrate that
MuEx achieves superior performance across all languages in MTFB and exhibits
effective zero-shot generalization to unseen languages without additional
training.

</details>


### [39] [MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking](https://arxiv.org/abs/2510.06619)
*Tao Feng,Tingfa Xu,Haolin Qin,Tianhao Li,Shuaihao Han,Xuyang Zou,Zhan Lv,Jianan Li*

Main category: cs.CV

TL;DR: MSITrack是最大的多光谱单目标跟踪数据集，包含300个视频和129k帧，覆盖55个类别和300个场景，显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 解决RGB跟踪器在遮挡、相似物体干扰和复杂背景下的局限性，填补多光谱跟踪数据集的空白。

Method: 构建MSITrack数据集，包含多光谱图像，涵盖多样场景和挑战性属性，并进行精细标注和验证。

Result: 多光谱数据显著优于RGB基线，验证了数据集的潜力。

Conclusion: MSITrack数据集公开可用，有望推动多光谱跟踪领域的发展。

Abstract: Visual object tracking in real-world scenarios presents numerous challenges
including occlusion, interference from similar objects and complex
backgrounds-all of which limit the effectiveness of RGB-based trackers.
Multispectral imagery, which captures pixel-level spectral reflectance,
enhances target discriminability. However, the availability of multispectral
tracking datasets remains limited. To bridge this gap, we introduce MSITrack,
the largest and most diverse multispectral single object tracking dataset to
date. MSITrack offers the following key features: (i) More Challenging
Attributes-including interference from similar objects and similarity in color
and texture between targets and backgrounds in natural scenarios, along with a
wide range of real-world tracking challenges; (ii) Richer and More Natural
Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack
far exceeds the scope of existing benchmarks. Many of these scenes and
categories are introduced to the multispectral tracking domain for the first
time; (iii) Larger Scale-300 videos comprising over 129k frames of
multispectral imagery. To ensure annotation precision, each frame has undergone
meticulous processing, manual labeling and multi-stage verification. Extensive
evaluations using representative trackers demonstrate that the multispectral
data in MSITrack significantly improves performance over RGB-only baselines,
highlighting its potential to drive future advancements in the field. The
MSITrack dataset is publicly available at:
https://github.com/Fengtao191/MSITrack.

</details>


### [40] [StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering](https://arxiv.org/abs/2510.06638)
*Zhihao Wen,Wenkang Wei,Yuan Fang,Xingtong Yu,Hui Zhang,Weicheng Zhu,Xin Zhang*

Main category: cs.CV

TL;DR: StaR-KVQA通过结构化推理轨迹提升IK-KVQA的准确性和可解释性，无需外部知识库。


<details>
  <summary>Details</summary>
Motivation: 解决MLLM在IK-KVQA中缺乏显式推理监督、生成不一致解释和泛化能力差的问题。

Method: 监督结构化推理轨迹（符号关系路径和自然语言解释），通过自蒸馏微调对齐生成与监督。

Result: 在OK-VQA上比最强基线提高11.3%的准确率，并展示跨域泛化能力。

Conclusion: StaR-KVQA显著提升了IK-KVQA的性能和可解释性，且推理过程透明可验证。

Abstract: Knowledge-based Visual Question Answering (KVQA) requires models to ground
entities in images and reason over factual knowledge. We study its
implicit-knowledge variant, IK-KVQA, where a multimodal large language model
(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs
lack explicit reasoning supervision and produce inconsistent justifications,
and generalize poorly after standard supervised fine-tuning (SFT). We present
StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises
structured traces - dual symbolic relation paths plus path-grounded
natural-language explanations - so that reasoning becomes transparent and
verifiable. With one open-source MLLM, StaR-KVQA constructs and selects
path-grounded reasoning traces to form a trace-enriched dataset, then
fine-tunes via structured self-distillation to align generation with
supervision; no external retrievers, verifiers, or curated knowledge bases
(KBs) are used, traces are built offline, and inference is a single
autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and
interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over
the strongest baseline while exhibiting robust cross-domain generalization.

</details>


### [41] [Automated Neural Architecture Design for Industrial Defect Detection](https://arxiv.org/abs/2510.06669)
*Yuxi Liu,Yunfeng Ma,Yi Tang,Min Liu,Shuai Jiang,Yaonan Wang*

Main category: cs.CV

TL;DR: AutoNAD是一个自动化神经架构设计框架，用于工业表面缺陷检测，通过联合搜索卷积、Transformer和多层感知机，解决类内差异和类间相似性挑战。


<details>
  <summary>Details</summary>
Motivation: 工业表面缺陷检测面临类内差异和类间相似性挑战，现有方法依赖手动设计模型，效率低且效果有限。

Method: AutoNAD结合卷积、Transformer和多层感知机，引入跨权重共享策略和可搜索的多级特征聚合模块，优化训练和特征学习。

Result: 在三个工业缺陷数据集上验证了AutoNAD的有效性，并应用于缺陷成像和检测平台。

Conclusion: AutoNAD通过自动化设计解决了工业缺陷检测的关键挑战，同时提升了效率和准确性。

Abstract: Industrial surface defect detection (SDD) is critical for ensuring product
quality and manufacturing reliability. Due to the diverse shapes and sizes of
surface defects, SDD faces two main challenges: intraclass difference and
interclass similarity. Existing methods primarily utilize manually designed
models, which require extensive trial and error and often struggle to address
both challenges effectively. To overcome this, we propose AutoNAD, an automated
neural architecture design framework for SDD that jointly searches over
convolutions, transformers, and multi-layer perceptrons. This hybrid design
enables the model to capture both fine-grained local variations and long-range
semantic context, addressing the two key challenges while reducing the cost of
manual network design. To support efficient training of such a diverse search
space, AutoNAD introduces a cross weight sharing strategy, which accelerates
supernet convergence and improves subnet performance. Additionally, a
searchable multi-level feature aggregation module (MFAM) is integrated to
enhance multi-scale feature learning. Beyond detection accuracy, runtime
efficiency is essential for industrial deployment. To this end, AutoNAD
incorporates a latency-aware prior to guide the selection of efficient
architectures. The effectiveness of AutoNAD is validated on three industrial
defect datasets and further applied within a defect imaging and detection
platform. Code will be available at https://github.com/Yuxi104/AutoNAD.

</details>


### [42] [Heptapod: Language Modeling on Visual Signals](https://arxiv.org/abs/2510.06673)
*Yongxin Zhu,Jiawei Chen,Yuanzhe Chen,Zhuo Chen,Dongya Jia,Jian Cong,Xiaobin Zhuang,Yuping Wang,Yuxuan Wang*

Main category: cs.CV

TL;DR: Heptapod是一种基于语言建模原则的图像自回归模型，通过因果注意力和2D分布预测创新，显著提升了图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 重新思考视觉信号的语言建模原则，探索更全面的图像语义捕获方法。

Method: 采用因果Transformer和重建导向的视觉分词器，预测2D空间网格的分布。

Result: 在ImageNet生成基准上，FID达到2.70，优于之前的自回归方法。

Conclusion: Heptapod为视觉信号的语言建模提供了新思路，有望推动更广泛的应用。

Abstract: We introduce Heptapod, an image autoregressive model that adheres to the
foundational principles of language modeling. Heptapod employs \textbf{causal
attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend
of semantic tokenizers}. Our key innovation is \textit{next 2D distribution
prediction}: a causal Transformer with reconstruction-focused visual tokenizer,
learns to predict the distribution over the entire 2D spatial grid of images at
each timestep. This learning objective unifies the sequential modeling of
autoregressive framework with the holistic self-supervised learning of masked
autoencoding, enabling the model to capture comprehensive image semantics via
generative training. On the ImageNet generation benchmark, Heptapod achieves an
FID of $2.70$, significantly outperforming previous causal autoregressive
approaches. We hope our work inspires a principled rethinking of language
modeling on visual signals and beyond.

</details>


### [43] [DreamOmni2: Multimodal Instruction-based Editing and Generation](https://arxiv.org/abs/2510.06679)
*Bin Xia,Bohao Peng,Yuechen Zhang,Junjia Huang,Jiyang Liu,Jingyao Li,Haoru Tan,Sitong Wu,Chengyao Wang,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出了DreamOmni2，支持多模态指令的图像编辑和生成任务，解决了数据创建和模型框架设计的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有指令编辑和主题生成方法无法满足实际需求，需要支持多模态输入和抽象概念。

Method: 通过特征混合方法创建数据，设计索引编码和位置编码方案，联合训练VLM和生成/编辑模型。

Result: DreamOmni2在实验中表现优异。

Conclusion: DreamOmni2为多模态指令任务提供了有效解决方案，推动了该领域发展。

Abstract: Recent advancements in instruction-based image editing and subject-driven
generation have garnered significant attention, yet both tasks still face
limitations in meeting practical user needs. Instruction-based editing relies
solely on language instructions, which often fail to capture specific editing
details, making reference images necessary. Meanwhile, subject-driven
generation is limited to combining concrete objects or people, overlooking
broader, abstract concepts. To address these challenges, we propose two novel
tasks: multimodal instruction-based editing and generation. These tasks support
both text and image instructions and extend the scope to include both concrete
and abstract concepts, greatly enhancing their practical applications. We
introduce DreamOmni2, tackling two primary challenges: data creation and model
framework design. Our data synthesis pipeline consists of three steps: (1)
using a feature mixing method to create extraction data for both abstract and
concrete concepts, (2) generating multimodal instruction-based editing training
data using the editing and extraction models, and (3) further applying the
extraction model to create training data for multimodal instruction-based
editing. For the framework, to handle multi-image input, we propose an index
encoding and position encoding shift scheme, which helps the model distinguish
images and avoid pixel confusion. Additionally, we introduce joint training
with the VLM and our generation/editing model to better process complex
instructions. In addition, we have proposed comprehensive benchmarks for these
two new tasks to drive their development. Experiments show that DreamOmni2 has
achieved impressive results. Models and codes will be released.

</details>


### [44] [Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion](https://arxiv.org/abs/2510.06687)
*Jie Luo,Yuxuan Jiang,Xin Jin,Mingyu Liu,Yihui Fan*

Main category: cs.CV

TL;DR: 提出了一种结合光场和点云数据的多模态语义分割方法，通过特征补全和深度感知模块提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶场景中复杂条件下（如遮挡）的语义分割问题，光场和LiDAR数据的互补性未被充分利用。

Method: 提出多模态数据集和Mlpfseg网络，包含特征补全和深度感知模块，融合光场和点云数据。

Result: 在mIoU指标上分别比纯图像和纯点云分割提升1.71和2.38。

Conclusion: 该方法有效整合多模态数据，显著提升复杂场景下的分割性能。

Abstract: Semantic segmentation serves as a cornerstone of scene understanding in
autonomous driving but continues to face significant challenges under complex
conditions such as occlusion. Light field and LiDAR modalities provide
complementary visual and spatial cues that are beneficial for robust
perception; however, their effective integration is hindered by limited
viewpoint diversity and inherent modality discrepancies. To address these
challenges, the first multimodal semantic segmentation dataset integrating
light field data and point cloud data is proposed. Based on this dataset, we
proposed a multi-modal light field point-cloud fusion segmentation
network(Mlpfseg), incorporating feature completion and depth perception to
segment both camera images and LiDAR point clouds simultaneously. The feature
completion module addresses the density mismatch between point clouds and image
pixels by performing differential reconstruction of point-cloud feature maps,
enhancing the fusion of these modalities. The depth perception module improves
the segmentation of occluded objects by reinforcing attention scores for better
occlusion awareness. Our method outperforms image-only segmentation by 1.71
Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38
mIoU, demonstrating its effectiveness.

</details>


### [45] [SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis](https://arxiv.org/abs/2510.06694)
*Jipeng Lyu,Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: SCas4D是一种基于3D高斯泼溅的级联优化框架，用于动态场景建模，通过分层变形模式实现高效训练和高质量结果。


<details>
  <summary>Details</summary>
Motivation: 动态场景建模在跟踪和新视角合成中面临变形准确性和计算效率的挑战。

Method: 采用级联优化框架，从粗粒度到细粒度逐步优化变形，利用高斯泼溅的结构模式。

Result: 在每帧100次迭代内收敛，训练迭代次数仅为现有方法的二十分之一，结果质量相当。

Conclusion: SCas4D在自监督关节对象分割、新视角合成和密集点跟踪任务中表现优异。

Abstract: Persistent dynamic scene modeling for tracking and novel-view synthesis
remains challenging due to the difficulty of capturing accurate deformations
while maintaining computational efficiency. We propose SCas4D, a cascaded
optimization framework that leverages structural patterns in 3D Gaussian
Splatting for dynamic scenes. The key idea is that real-world deformations
often exhibit hierarchical patterns, where groups of Gaussians share similar
transformations. By progressively refining deformations from coarse part-level
to fine point-level, SCas4D achieves convergence within 100 iterations per time
frame and produces results comparable to existing methods with only
one-twentieth of the training iterations. The approach also demonstrates
effectiveness in self-supervised articulated object segmentation, novel view
synthesis, and dense point tracking tasks.

</details>


### [46] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: 提出了一个针对基于LLM的历史OCR评估方法，解决了传统指标无法捕捉时间偏差和特定时期错误的问题。


<details>
  <summary>Details</summary>
Motivation: 传统OCR评估指标无法有效衡量历史文档数字化中的时间偏差和特定时期错误，需要新的评估框架。

Method: 引入了历史字符保留率（HCPR）和古语插入率（AIR）等新指标，并设计了污染控制和稳定性测试协议。

Result: 评估了12个多模态LLM，发现Gemini和Qwen模型优于传统OCR，但存在过度历史化问题；后OCR校正反而降低性能。

Conclusion: 该方法为数字人文学者提供了历史语料库数字化中的模型选择和评估指南。

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


### [47] [DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining](https://arxiv.org/abs/2510.06746)
*Zhiliang Zhu,Tao Zeng,Tao Yang,Guoliang Luo,Jiyong Zeng*

Main category: cs.CV

TL;DR: DeRainMamba结合频域建模和空间细节增强，在单图像去雨任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在序列建模中效率高，但缺乏细粒度细节捕捉和频域感知能力，限制了去雨效果的进一步提升。

Method: 提出DeRainMamba，集成频域感知状态空间模块（FASSM）和多方向感知卷积（MDPConv），分别用于区分雨纹与高频细节及恢复局部结构。

Result: 在四个公开基准测试中，DeRainMamba在PSNR和SSIM指标上均优于现有方法，且参数和计算成本更低。

Conclusion: 结合频域建模和空间细节增强的状态空间框架在单图像去雨任务中有效。

Abstract: Image deraining is crucial for improving visual quality and supporting
reliable downstream vision tasks. Although Mamba-based models provide efficient
sequence modeling, their limited ability to capture fine-grained details and
lack of frequency-domain awareness restrict further improvements. To address
these issues, we propose DeRainMamba, which integrates a Frequency-Aware
State-Space Module (FASSM) and Multi-Directional Perception Convolution
(MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from
high-frequency image details, balancing rain removal and detail preservation.
MDPConv further restores local structures by capturing anisotropic gradient
features and efficiently fusing multiple convolution branches. Extensive
experiments on four public benchmarks demonstrate that DeRainMamba consistently
outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer
parameters and lower computational costs. These results validate the
effectiveness of combining frequency-domain modeling and spatial detail
enhancement within a state-space framework for single image deraining.

</details>


### [48] [OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot](https://arxiv.org/abs/2510.06751)
*Junhan Zhu,Hesong Wang,Mingluo Su,Zefang Wang,Huan Wang*

Main category: cs.CV

TL;DR: OBS-Diff是一种新颖的一剪枝框架，用于高效压缩大规模文本到图像扩散模型，支持多种剪枝粒度，并通过时间感知Hessian构造和分组顺序剪枝策略实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的一剪枝方法难以直接应用于扩散模型，因其迭代去噪特性导致计算成本高。

Method: OBS-Diff通过改进经典OBS方法，支持多种剪枝粒度，并提出时间感知Hessian构造和分组顺序剪枝策略。

Result: 实验表明，OBS-Diff在扩散模型的一剪枝中表现最优，实现了推理加速且视觉质量损失最小。

Conclusion: OBS-Diff为大规模扩散模型的高效压缩提供了有效解决方案。

Abstract: Large-scale text-to-image diffusion models, while powerful, suffer from
prohibitive computational cost. Existing one-shot network pruning methods can
hardly be directly applied to them due to the iterative denoising nature of
diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel
one-shot pruning framework that enables accurate and training-free compression
of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff
revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex
architectures of modern diffusion models and supporting diverse pruning
granularity, including unstructured, N:M semi-structured, and structured (MHA
heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the
iterative dynamics of the diffusion process, by examining the problem from an
error-accumulation perspective, we propose a novel timestep-aware Hessian
construction that incorporates a logarithmic-decrease weighting scheme,
assigning greater importance to earlier timesteps to mitigate potential error
accumulation; (iii) Furthermore, a computationally efficient group-wise
sequential pruning strategy is proposed to amortize the expensive calibration
process. Extensive experiments show that OBS-Diff achieves state-of-the-art
one-shot pruning for diffusion models, delivering inference acceleration with
minimal degradation in visual quality.

</details>


### [49] [Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All](https://arxiv.org/abs/2510.06757)
*Sheng Fu,Junchao Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 提出一种直方图匹配方法，将任意噪声转换为目标高斯分布，结合去噪循环提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决监督高斯去噪器在分布外噪声上泛化能力不足的问题。

Method: 通过直方图匹配将噪声转换为目标高斯分布，结合去噪循环逐步优化噪声转换效果。

Result: 单高斯去噪器能处理多种分布外噪声，包括合成和真实噪声。

Conclusion: 方法在泛化和有效性上表现优异。

Abstract: Supervised Gaussian denoisers exhibit limited generalization when confronted
with out-of-distribution noise, due to the diverse distributional
characteristics of different noise types. To bridge this gap, we propose a
histogram matching approach that transforms arbitrary noise towards a target
Gaussian distribution with known intensity. Moreover, a mutually reinforcing
cycle is established between noise transformation and subsequent denoising.
This cycle progressively refines the noise to be converted, making it
approximate the real noise, thereby enhancing the noise transformation effect
and further improving the denoising performance. We tackle specific noise
complexities: local histogram matching handles signal-dependent noise,
intrapatch permutation processes channel-related noise, and frequency-domain
histogram matching coupled with pixel-shuffle down-sampling breaks spatial
correlation. By applying these transformations, a single Gaussian denoiser
gains remarkable capability to handle various out-of-distribution noises,
including synthetic noises such as Poisson, salt-and-pepper and repeating
pattern noises, as well as complex real-world noises. Extensive experiments
demonstrate the superior generalization and effectiveness of our method.

</details>


### [50] [A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping](https://arxiv.org/abs/2510.06769)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度多实例学习（DMIL）的方法，利用低分辨率参考数据训练高分辨率土地覆盖分类器，通过灵活池化层连接像素语义，并在多类和多标签设置中重新定义MIL问题。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率土地覆盖分类中训练标签数量和质量不足的问题，利用现有低分辨率或过时产品生成弱标签。

Method: 采用深度多实例学习方法，结合灵活池化层连接高分辨率图像像素语义与低分辨率标签，并在多类和多标签设置中重新定义MIL问题，使用正未标记学习（PUL）策略训练分类器。

Result: 在2020 IEEE GRSS数据融合竞赛数据集上的实验表明，该方法优于标准训练策略。

Conclusion: 提出的框架有效解决了高分辨率土地覆盖分类中弱标签利用的问题，为实际应用提供了可行方案。

Abstract: The quantity and the quality of the training labels are central problems in
high-resolution land-cover mapping with machine-learning-based solutions. In
this context, weak labels can be gathered in large quantities by leveraging on
existing low-resolution or obsolete products. In this paper, we address the
problem of training land-cover classifiers using high-resolution imagery (e.g.,
Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived
land-cover maps). Inspired by recent works in Deep Multiple Instance Learning
(DMIL), we propose a method that trains pixel-level multi-class classifiers and
predicts low-resolution labels (i.e., patch-level classification), where the
actual high-resolution labels are learned implicitly without direct
supervision. This is achieved with flexible pooling layers that are able to
link the semantics of the pixels in the high-resolution imagery to the
low-resolution reference labels. Then, the Multiple Instance Learning (MIL)
problem is re-framed in a multi-class and in a multi-label setting. In the
former, the low-resolution annotation represents the majority of the pixels in
the patch. In the latter, the annotation only provides us information on the
presence of one of the land-cover classes in the patch and thus multiple labels
can be considered valid for a patch at a time, whereas the low-resolution
labels provide us only one label. Therefore, the classifier is trained with a
Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020
IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed
framework compared to standard training strategies.

</details>


### [51] [TTRV: Test-Time Reinforcement Learning for Vision Language Models](https://arxiv.org/abs/2510.06783)
*Akshit Singh,Shyam Marjit,Wei Lin,Paul Gavrikov,Serena Yeung-Levy,Hilde Kuehne,Rogerio Feris,Sivan Doveh,James Glass,M. Jehanzeb Mirza*

Main category: cs.CV

TL;DR: TTRV是一种无需标注数据的测试时强化学习方法，通过动态调整模型在推理时的输出，显著提升了视觉语言理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖标注数据和专用训练集，而人类学习则直接从环境中获取信息。TTRV旨在模拟这种无监督学习方式。

Method: 基于GRPO框架，设计基于输出频率的奖励机制，并通过多次推理测试样本和控制输出多样性（低熵）来优化模型。

Result: 在物体识别和VQA任务中，TTRV分别提升高达52.4%和29.8%，平均提升24.6%和10.0%。在8个基准测试中，InternVL 8B模型超越GPT-4o 2.3%。

Conclusion: TTRV证明了测试时强化学习在视觉语言模型中的有效性，即使在极少量数据下也能显著提升性能。

Abstract: Existing methods for extracting reward signals in Reinforcement Learning
typically rely on labeled data and dedicated training splits, a setup that
contrasts with how humans learn directly from their environment. In this work,
we propose TTRV to enhance vision language understanding by adapting the model
on the fly at inference time, without the need for any labeled data.
Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework
by designing rewards based on the frequency of the base model's output, while
inferring on each test sample multiple times. Further, we also propose to
control the diversity of the model's output by simultaneously rewarding the
model for obtaining low entropy of the output empirical distribution. Our
approach delivers consistent gains across both object recognition and visual
question answering (VQA), with improvements of up to 52.4% and 29.8%,
respectively, and average boosts of 24.6% and 10.0% across 16
datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B
surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining
highly competitive on VQA, demonstrating that test-time reinforcement learning
can match or exceed the strongest proprietary models. Finally, we find many
interesting properties of test-time RL for VLMs: for example, even in extremely
data-constrained scenarios, where adaptation is performed on a single randomly
chosen unlabeled test example, TTRV still yields non-trivial improvements of up
to 5.5% in recognition tasks.

</details>


### [52] [Extreme Amodal Face Detection](https://arxiv.org/abs/2510.06791)
*Changlin Song,Yunzhong Hou,Michael Randall Barnes,Rahul Shome,Dylan Campbell*

Main category: cs.CV

TL;DR: 提出了一种基于热图的极端非模态物体检测方法，通过上下文线索推断未可见物体，优于现有生成方法。


<details>
  <summary>Details</summary>
Motivation: 解决单图像中极端非模态检测问题，特别是在安全和隐私应用中的人脸检测。

Method: 设计了一种选择性从粗到细的解码器，利用图像上下文信息高效预测未可见区域。

Result: 在新任务中表现优异，甚至超越了效率较低的生成方法。

Conclusion: 该方法在极端非模态检测任务中高效且有效，适用于单图像场景。

Abstract: Extreme amodal detection is the task of inferring the 2D location of objects
that are not fully visible in the input image but are visible within an
expanded field-of-view. This differs from amodal detection, where the object is
partially visible within the input image, but is occluded. In this paper, we
consider the sub-problem of face detection, since this class provides
motivating applications involving safety and privacy, but do not tailor our
method specifically to this class. Existing approaches rely on image sequences
so that missing detections may be interpolated from surrounding frames or make
use of generative models to sample possible completions. In contrast, we
consider the single-image task and propose a more efficient, sample-free
approach that makes use of the contextual cues from the image to infer the
presence of unseen faces. We design a heatmap-based extreme amodal object
detector that addresses the problem of efficiently predicting a lot (the
out-of-frame region) from a little (the image) with a selective coarse-to-fine
decoder. Our method establishes strong results for this new task, even
outperforming less efficient generative approaches.

</details>


### [53] [VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance](https://arxiv.org/abs/2510.06809)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: 本文提出了一种基于超声基础模型的探针引导方法，通过VA-Adapter实现视觉-动作序列编码，提升初级超声医师的操作能力。


<details>
  <summary>Details</summary>
Motivation: 心脏超声操作难度高，专业人才短缺，导致患者难以及时获得高质量超声图像。本文旨在利用基础模型的知识，为初级医师提供实时操作建议。

Method: 设计了参数高效的VA-Adapter，使基础模型的图像编码器能够编码视觉-动作序列，并通过微调少量参数学习精确的探针调整策略。

Result: 实验表明，VA-Adapter在探针引导任务中表现优于现有模型。

Conclusion: VA-Adapter能够有效提升超声基础模型在探针引导任务中的性能，为初级医师提供高质量的操作支持。

Abstract: Echocardiography is a critical tool for detecting heart diseases. Recently,
ultrasound foundation models have demonstrated remarkable capabilities in
cardiac ultrasound image analysis. However, obtaining high-quality ultrasound
images is a prerequisite for accurate diagnosis. Due to the exceptionally high
operational difficulty of cardiac ultrasound, there is a shortage of highly
skilled personnel, which hinders patients from receiving timely examination
services. In this paper, we aim to adapt the medical knowledge learned by
foundation models from vast datasets to the probe guidance task, which is
designed to provide real-time operational recommendations for junior
sonographers to acquire high-quality ultrasound images. Moreover, inspired by
the practice where experts optimize action decisions based on past
explorations, we meticulously design a parameter-efficient Vision-Action
Adapter (VA-Adapter) to enable foundation model's image encoder to encode
vision-action sequences, thereby enhancing guidance performance. With built-in
sequential reasoning capabilities in a compact design, the VA-Adapter enables a
pre-trained ultrasound foundation model to learn precise probe adjustment
strategies by fine-tuning only a small subset of parameters. Extensive
experiments demonstrate that the VA-Adapter can surpass strong probe guidance
models. Our code will be released after acceptance.

</details>


### [54] [Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking](https://arxiv.org/abs/2510.06820)
*Mitchell Keren Taraday,Shahaf Wagner,Chaim Baskin*

Main category: cs.CV

TL;DR: EDJE是一种高效的多模态联合编码器，通过预计算视觉标记并压缩，显著减少存储和计算需求，同时保持检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有联合编码器（如BLIP）因视觉特征提取阶段昂贵而难以大规模部署，EDJE旨在解决这一瓶颈。

Method: 预计算视觉标记并压缩，使用轻量级注意力适配器，在线推理仅需处理少量视觉标记和文本。

Result: EDJE处理速度达50k图像-文本对/秒，存储需求仅49kB/图像，在Flickr和COCO数据集上表现优异。

Conclusion: EDJE高效且性能强，适用于大规模多模态检索任务。

Abstract: Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.

</details>


### [55] [StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance](https://arxiv.org/abs/2510.06827)
*Jaeseok Jeong,Junho Kim,Gayoung Lee,Yunjey Choi,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出了一种新方法（NVQG）来减少文本到图像生成中的内容泄漏问题，通过改进自注意力机制和负视觉查询引导。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法存在内容泄漏问题，即不想要的视觉风格元素被意外转移。

Method: 扩展了无分类器引导（CFG）并引入负视觉查询引导（NVQG），通过交换自注意力层的查询来模拟内容泄漏。

Result: 显著减少了内容泄漏，生成的图像更符合文本提示并保留参考风格。

Conclusion: 该方法在多种风格和文本提示下表现优于现有方法，代码已开源。

Abstract: In the domain of text-to-image generation, diffusion models have emerged as
powerful tools. Recently, studies on visual prompting, where images are used as
prompts, have enabled more precise control over style and content. However,
existing methods often suffer from content leakage, where undesired elements of
the visual style prompt are transferred along with the intended style. To
address this issue, we 1) extend classifier-free guidance (CFG) to utilize
swapping self-attention and propose 2) negative visual query guidance (NVQG) to
reduce the transfer of unwanted contents. NVQG employs negative score by
intentionally simulating content leakage scenarios that swap queries instead of
key and values of self-attention layers from visual style prompts. This simple
yet effective method significantly reduces content leakage. Furthermore, we
provide careful solutions for using a real image as visual style prompts.
Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts. Our code
is available \href{https://github.com/naver-ai/StyleKeeper}{here}.

</details>


### [56] [Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera](https://arxiv.org/abs/2510.06829)
*Mikihiro Ikura,Arren Glover,Masayoshi Mizuno,Chiara Bartolozzi*

Main category: cs.CV

TL;DR: 提出了一种仅使用高分辨率事件相机实时检测和跟踪线段的方法，通过速度不变的事件表示、基于拟合得分的线段检测和端点扰动的线段跟踪，实现了高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖帧相机或无法处理高事件率的问题，实现完全独立的事件相机操作。

Method: 采用速度不变的事件表示、基于拟合得分的线段检测和端点扰动的线段跟踪。

Result: 在自录数据集和公开数据集上展示了实时性能和高精度，优于现有事件相机和混合基线方法。

Conclusion: 该方法实现了完全独立的事件相机操作，适用于现实场景。

Abstract: Line segment extraction is effective for capturing geometric features of
human-made environments. Event-based cameras, which asynchronously respond to
contrast changes along edges, enable efficient extraction by reducing redundant
data. However, recent methods often rely on additional frame cameras or
struggle with high event rates. This research addresses real-time line segment
detection and tracking using only a modern, high-resolution (i.e., high event
rate) event-based camera. Our lattice-allocated pipeline consists of (i)
velocity-invariant event representation, (ii) line segment detection based on a
fitting score, (iii) and line segment tracking by perturbating endpoints.
Evaluation using ad-hoc recorded dataset and public datasets demonstrates
real-time performance and higher accuracy compared to state-of-the-art
event-only and event-frame hybrid baselines, enabling fully stand-alone event
camera operation in real-world settings.

</details>


### [57] [Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization](https://arxiv.org/abs/2510.06842)
*Kanglei Zhou,Qingyi Pan,Xingxing Zhang,Hubert P. H. Shum,Frederick W. B. Li,Xiaohui Liang,Liyuan Wang*

Main category: cs.CV

TL;DR: 论文提出了Continual AQA (CAQA)方法，通过结合持续学习能力处理动作质量评估中的非平稳分布问题，并提出MAGR++方法优化特征表示。


<details>
  <summary>Details</summary>
Motivation: 解决动作质量评估中非平稳分布导致的泛化能力不足问题。

Method: 提出MAGR++方法，结合全参数微调和特征校正管道（流形投影和图正则化）。

Result: MAGR++在离线实验中平均相关性提升3.6%，在线实验中提升12.2%。

Conclusion: MAGR++在CAQA任务中表现出色，验证了其鲁棒性和有效性。

Abstract: Action Quality Assessment (AQA) quantifies human actions in videos,
supporting applications in sports scoring, rehabilitation, and skill
evaluation. A major challenge lies in the non-stationary nature of quality
distributions in real-world scenarios, which limits the generalization ability
of conventional methods. We introduce Continual AQA (CAQA), which equips AQA
with Continual Learning (CL) capabilities to handle evolving distributions
while mitigating catastrophic forgetting. Although parameter-efficient
fine-tuning of pretrained models has shown promise in CL for image
classification, we find it insufficient for CAQA. Our empirical and theoretical
analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is
necessary for effective representation learning; yet (ii) uncontrolled FPFT
induces overfitting and feature manifold shift, thereby aggravating forgetting.
To address this, we propose Adaptive Manifold-Aligned Graph Regularization
(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers
while adapting deeper ones with a two-step feature rectification pipeline: a
manifold projector to translate deviated historical features into the current
representation space, and a graph regularizer to align local and global
distributions. We construct four CAQA benchmarks from three datasets with
tailored evaluation protocols and strong baselines, enabling systematic
cross-dataset comparison. Extensive experiments show that MAGR++ achieves
state-of-the-art performance, with average correlation gains of 3.6% offline
and 12.2% online over the strongest baseline, confirming its robustness and
effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.

</details>


### [58] [Online Generic Event Boundary Detection](https://arxiv.org/abs/2510.06855)
*Hyungrok Jung,Daneul Kim,Seunggyun Lim,Jeany Son,Jonghyun Choi*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务On-GEBD，旨在实时检测流视频中的通用事件边界，并提出了Estimator框架，结合预测误差和统计测试来识别事件变化。


<details>
  <summary>Details</summary>
Motivation: 当前GEBD方法需要处理完整视频帧，而人类可以实时处理数据，因此需要开发在线检测方法。

Method: 提出Estimator框架，包含Consistent Event Anticipator（CEA）和Online Boundary Discriminator（OBD），通过预测未来帧和统计误差来检测事件边界。

Result: Estimator在Kinetics-GEBD和TAPOS数据集上优于基线方法，性能接近离线GEBD方法。

Conclusion: On-GEBD任务和Estimator框架有效解决了实时检测事件边界的挑战，性能接近离线方法。

Abstract: Generic Event Boundary Detection (GEBD) aims to interpret long-form videos
through the lens of human perception. However, current GEBD methods require
processing complete video frames to make predictions, unlike humans processing
data online and in real-time. To bridge this gap, we introduce a new task,
Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries
of generic events immediately in streaming videos. This task faces unique
challenges of identifying subtle, taxonomy-free event changes in real-time,
without the access to future frames. To tackle these challenges, we propose a
novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)
which explains how humans segment ongoing activity into events by leveraging
the discrepancies between predicted and actual information. Our framework
consists of two key components: the Consistent Event Anticipator (CEA), and the
Online Boundary Discriminator (OBD). Specifically, the CEA generates a
prediction of the future frame reflecting current event dynamics based solely
on prior frames. Then, the OBD measures the prediction error and adaptively
adjusts the threshold using statistical tests on past errors to capture
diverse, subtle event transitions. Experimental results demonstrate that
Estimator outperforms all baselines adapted from recent online video
understanding models and achieves performance comparable to prior offline-GEBD
methods on the Kinetics-GEBD and TAPOS datasets.

</details>


### [59] [Explaining raw data complexity to improve satellite onboard processing](https://arxiv.org/abs/2510.06858)
*Adrien Dorise,Marjorie Bellizzi,Adrien Girard,Benjamin Francesconi,Stéphane May*

Main category: cs.CV

TL;DR: 研究探讨了在卫星上直接使用原始传感器数据对深度学习模型的影响，发现原始数据在高置信度下边界识别较差，建议改进轮廓方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着处理能力提升，卫星上部署AI模型成为可能，但使用原始数据而非预处理数据带来新挑战，目前相关研究较少。

Method: 通过模拟流程从高分辨率L1影像生成原始数据，训练YOLOv11s和YOLOX-S模型，并对比其在原始和L1数据上的性能。

Result: 模型在低至中置信度下表现相似，但原始数据训练的模型在高置信度时边界识别较差。

Conclusion: 改进AI架构的轮廓方法可提升原始数据的物体检测性能，推动星载AI发展。

Abstract: With increasing processing power, deploying AI models for remote sensing
directly onboard satellites is becoming feasible. However, new constraints
arise, mainly when using raw, unprocessed sensor data instead of preprocessed
ground-based products. While current solutions primarily rely on preprocessed
sensor images, few approaches directly leverage raw data. This study
investigates the effects of utilising raw data on deep learning models for
object detection and classification tasks. We introduce a simulation workflow
to generate raw-like products from high-resolution L1 imagery, enabling
systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are
trained on both raw and L1 datasets, and their performance is compared using
standard detection metrics and explainability tools. Results indicate that
while both models perform similarly at low to medium confidence thresholds, the
model trained on raw data struggles with object boundary identification at high
confidence levels. It suggests that adapting AI architectures with improved
contouring methods can enhance object detection on raw images, improving
onboard AI for remote sensing.

</details>


### [60] [Lung Infection Severity Prediction Using Transformers with Conditional TransMix Augmentation and Cross-Attention](https://arxiv.org/abs/2510.06887)
*Bouthaina Slika,Fadi Dornaika,Fares Bougourzi,Karim Hammoudi*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的新方法QCross-Att-PVT和Conditional Online TransMix数据增强策略，用于肺部感染严重性预测，在CT和X光片上表现优异。


<details>
  <summary>Details</summary>
Motivation: 肺部感染（如肺炎）在疫情期间风险高，需通过医学影像快速准确预测严重性以优化临床决策。

Method: 结合并行编码器、交叉门控注意力机制和特征聚合器的Transformer架构，以及针对数据不平衡的Conditional Online TransMix数据增强。

Result: 在RALO CXR和Per-COVID-19 CT数据集上优于现有深度学习模型。

Conclusion: 该方法为临床诊断和个性化治疗提供了可靠工具，强调了数据增强和门控注意力的重要性。

Abstract: Lung infections, particularly pneumonia, pose serious health risks that can
escalate rapidly, especially during pandemics. Accurate AI-based severity
prediction from medical imaging is essential to support timely clinical
decisions and optimize patient outcomes. In this work, we present a novel
method applicable to both CT scans and chest X-rays for assessing lung
infection severity. Our contributions are twofold: (i) QCross-Att-PVT, a
Transformer-based architecture that integrates parallel encoders, a cross-gated
attention mechanism, and a feature aggregator to capture rich multi-scale
features; and (ii) Conditional Online TransMix, a custom data augmentation
strategy designed to address dataset imbalance by generating mixed-label image
patches during training. Evaluated on two benchmark datasets, RALO CXR and
Per-COVID-19 CT, our method consistently outperforms several state-of-the-art
deep learning models. The results emphasize the critical role of data
augmentation and gated attention in improving both robustness and predictive
accuracy. This approach offers a reliable, adaptable tool to support clinical
diagnosis, disease monitoring, and personalized treatment planning. The source
code of this work is available at https://github.com/bouthainas/QCross-Att-PVT.

</details>


### [61] [Label-frugal satellite image change detection with generative virtual exemplar learning](https://arxiv.org/abs/2510.06926)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出了一种基于主动学习的新型变化检测算法，通过生成关键样本（虚拟范例）来提高标注效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量人工标注数据，而标注过程受采集条件和用户主观性影响，因此需要一种更高效的标注方法。

Method: 使用可逆图卷积网络生成虚拟范例，通过对抗性损失衡量样本的代表性、多样性和模糊性，选择最关键的样本供标注。

Result: 实验表明，该方法在标注效率上优于对比方法。

Conclusion: 通过主动学习和虚拟范例生成，显著提高了变化检测的标注效率。

Abstract: Change detection is a major task in remote sensing which consists in finding
all the occurrences of changes in multi-temporal satellite or aerial images.
The success of existing methods, and particularly deep learning ones, is
tributary to the availability of hand-labeled training data that capture the
acquisition conditions and the subjectivity of the user (oracle). In this
paper, we devise a novel change detection algorithm, based on active learning.
The main contribution of our work resides in a new model that measures how
important is each unlabeled sample, and provides an oracle with only the most
critical samples (also referred to as virtual exemplars) for further labeling.
These exemplars are generated, using an invertible graph convnet, as the
optimum of an adversarial loss that (i) measures representativity, diversity
and ambiguity of the data, and thereby (ii) challenges (the most) the current
change detection criteria, leading to a better re-estimate of these criteria in
the subsequent iterations of active learning. Extensive experiments show the
positive impact of our label-efficient learning model against comparative
methods.

</details>


### [62] [IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction](https://arxiv.org/abs/2510.06928)
*Ran Yi,Teng Hu,Zihan Su,Lizhuang Ma*

Main category: cs.CV

TL;DR: IAR2提出了一种分层语义-细节合成框架，通过双码本和局部上下文增强的自回归预测，显著提升了自回归图像生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型忽略了视觉数据的结构特性，且受限于预训练码本的刚性和硬聚类的不足。

Method: 采用语义-细节关联双码本，结合局部上下文增强的自回归头和渐进注意力引导的自适应CFG机制。

Result: 在ImageNet上FID达到1.50，性能超越现有方法且计算效率更高。

Conclusion: IAR2通过结构化、由粗到细的生成策略，实现了高效且高质量的自回归图像生成。

Abstract: Autoregressive models have emerged as a powerful paradigm for visual content
creation, but often overlook the intrinsic structural properties of visual
data. Our prior work, IAR, initiated a direction to address this by
reorganizing the visual codebook based on embedding similarity, thereby
improving generation robustness. However, it is constrained by the rigidity of
pre-trained codebooks and the inaccuracies of hard, uniform clustering. To
overcome these limitations, we propose IAR2, an advanced autoregressive
framework that enables a hierarchical semantic-detail synthesis process. At the
core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which
decouples image representations into a semantic codebook for global semantic
information and a detail codebook for fine-grained refinements. It expands the
quantization capacity from a linear to a polynomial scale, significantly
enhancing expressiveness. To accommodate this dual representation, we propose a
Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context
Enhanced Autoregressive Head, which performs hierarchical prediction-first the
semantic token, then the detail token-while leveraging a local context window
to enhance spatial coherence. Furthermore, for conditional generation, we
introduce a Progressive Attention-Guided Adaptive CFG mechanism that
dynamically modulates the guidance scale for each token based on its relevance
to the condition and its temporal position in the generation sequence,
improving conditional alignment without sacrificing realism. Extensive
experiments demonstrate that IAR2 sets a new state-of-the-art for
autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model
not only surpasses previous methods in performance but also demonstrates
superior computational efficiency, highlighting the effectiveness of our
structured, coarse-to-fine generation strategy.

</details>


### [63] [OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects](https://arxiv.org/abs/2510.06952)
*Bing Li,Wuqi Wang,Yanan Zhang,Jingzheng Li,Haigen Min,Wei Feng,Xingyu Zhao,Jie Zhang,Qing Guo*

Main category: cs.CV

TL;DR: 提出了一种基于文本到3D的对抗生成方法（Phy3DAdvGen），用于生成物理可实现的3D模型，使LiDAR检测器无法检测到行人，揭示了安全关键应用中的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有3D对抗攻击方法难以实现物体完全消失且物理环境实现困难，需开发更有效的攻击方法以测试LiDAR检测系统的鲁棒性。

Method: 通过操纵行人3D模型的拓扑、连通性和强度，结合CARLA仿真环境，提出Phy3DAdvGen方法，优化文本提示生成LiDAR不可见的行人模型。

Result: 生成的3D行人成功逃逸六种先进LiDAR检测器，在仿真和物理环境中均有效。

Conclusion: Phy3DAdvGen方法揭示了LiDAR检测器的潜在漏洞，为安全关键应用的鲁棒性测试提供了新工具。

Abstract: LiDAR-based 3D object detectors are fundamental to autonomous driving, where
failing to detect objects poses severe safety risks. Developing effective 3D
adversarial attacks is essential for thoroughly testing these detection systems
and exposing their vulnerabilities before real-world deployment. However,
existing adversarial attacks that add optimized perturbations to 3D points have
two critical limitations: they rarely cause complete object disappearance and
prove difficult to implement in physical environments. We introduce the
text-to-3D adversarial generation method, a novel approach enabling physically
realizable attacks that can generate 3D models of objects truly invisible to
LiDAR detectors and be easily realized in the real world. Specifically, we
present the first empirical study that systematically investigates the factors
influencing detection vulnerability by manipulating the topology, connectivity,
and intensity of individual pedestrian 3D models and combining pedestrians with
multiple objects within the CARLA simulation environment. Building on the
insights, we propose the physically-informed text-to-3D adversarial generation
(Phy3DAdvGen) that systematically optimizes text prompts by iteratively
refining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To
ensure physical realizability, we construct a comprehensive object pool
containing 13 3D models of real objects and constrain Phy3DAdvGen to generate
3D objects based on combinations of objects in this set. Extensive experiments
demonstrate that our approach can generate 3D pedestrians that evade six
state-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and
physical environments, thereby highlighting vulnerabilities in safety-critical
applications.

</details>


### [64] [Generating Surface for Text-to-3D using 2D Gaussian Splatting](https://arxiv.org/abs/2510.06967)
*Huanning Dong,Fan Li,Ping Kuang,Jianwen Min*

Main category: cs.CV

TL;DR: 提出了一种名为DirectGaussian的新方法，通过2D高斯泼溅和多视角法线与纹理先验生成3D对象表面，解决了多视角几何一致性问题。


<details>
  <summary>Details</summary>
Motivation: 自然世界中物体几何形状复杂，现有方法难以高效生成3D内容。

Method: 结合条件文本生成模型和2D高斯泼溅技术，优化过程中加入曲率约束。

Result: 实验表明，该方法能实现多样且高保真的3D内容生成。

Conclusion: DirectGaussian在3D内容生成中表现出色，解决了多视角一致性问题。

Abstract: Recent advancements in Text-to-3D modeling have shown significant potential
for the creation of 3D content. However, due to the complex geometric shapes of
objects in the natural world, generating 3D content remains a challenging task.
Current methods either leverage 2D diffusion priors to recover 3D geometry, or
train the model directly based on specific 3D representations. In this paper,
we propose a novel method named DirectGaussian, which focuses on generating the
surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize
conditional text generation models and the surface of a 3D object is rendered
by 2D Gaussian splatting with multi-view normal and texture priors. For
multi-view geometric consistency problems, DirectGaussian incorporates
curvature constraints on the generated surface during optimization process.
Through extensive experiments, we demonstrate that our framework is capable of
achieving diverse and high-fidelity 3D content creation.

</details>


### [65] [Learning Global Representation from Queries for Vectorized HD Map Construction](https://arxiv.org/abs/2510.06969)
*Shoumeng Qiu,Xinrun Li,Yang Long,Xiangyang Xue,Varun Ojha,Jian Pu*

Main category: cs.CV

TL;DR: 提出MapGR方法，通过全局表示学习提升HD地图构建效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖局部查询，忽略了HD地图的全局表示。

Method: 引入GRL和GRG模块，分别学习全局表示并指导查询优化。

Result: 在nuScenes和Argoverse2数据集上mAP显著提升。

Conclusion: MapGR通过全局表示学习有效提升了HD地图构建性能。

Abstract: The online construction of vectorized high-definition (HD) maps is a
cornerstone of modern autonomous driving systems. State-of-the-art approaches,
particularly those based on the DETR framework, formulate this as an instance
detection problem. However, their reliance on independent, learnable object
queries results in a predominantly local query perspective, neglecting the
inherent global representation within HD maps. In this work, we propose
\textbf{MapGR} (\textbf{G}lobal \textbf{R}epresentation learning for HD
\textbf{Map} construction), an architecture designed to learn and utilize a
global representations from queries. Our method introduces two synergistic
modules: a Global Representation Learning (GRL) module, which encourages the
distribution of all queries to better align with the global map through a
carefully designed holistic segmentation task, and a Global Representation
Guidance (GRG) module, which endows each individual query with explicit,
global-level contextual information to facilitate its optimization. Evaluations
on the nuScenes and Argoverse2 datasets validate the efficacy of our approach,
demonstrating substantial improvements in mean Average Precision (mAP) compared
to leading baselines.

</details>


### [66] [Addressing the ID-Matching Challenge in Long Video Captioning](https://arxiv.org/abs/2510.06973)
*Zhantao Yang,Huangji Wang,Ruili Feng,Han Zhang,Yuting Hu,Shangwen Zhu,Junyan Li,Yu Liu,Fan Cheng*

Main category: cs.CV

TL;DR: 提出了一种名为RICE的新方法，通过增强图像信息利用和个体描述信息量，显著提升了长视频字幕中的ID匹配性能。


<details>
  <summary>Details</summary>
Motivation: 长视频字幕生成中的ID匹配问题（即识别不同帧中的同一人物）是关键挑战，现有方法泛化能力有限且依赖点对点匹配。

Method: 基于LVLMs（如GPT-4o）的固有能力，提出RICE方法，通过优化图像信息使用和个体描述信息量来提升ID匹配。

Result: 在GPT-4o上，RICE将ID匹配的精确度从50%提升至90%，召回率从15%提升至80%。

Conclusion: RICE能有效跟踪长视频字幕中的不同个体，为文本到视频生成和多模态理解提供了重要支持。

Abstract: Generating captions for long and complex videos is both critical and
challenging, with significant implications for the growing fields of
text-to-video generation and multi-modal understanding. One key challenge in
long video captioning is accurately recognizing the same individuals who appear
in different frames, which we refer to as the ID-Matching problem. Few prior
works have focused on this important issue. Those that have, usually suffer
from limited generalization and depend on point-wise matching, which limits
their overall effectiveness. In this paper, unlike previous approaches, we
build upon LVLMs to leverage their powerful priors. We aim to unlock the
inherent ID-Matching capabilities within LVLMs themselves to enhance the
ID-Matching performance of captions. Specifically, we first introduce a new
benchmark for assessing the ID-Matching capabilities of video captions. Using
this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights
that the performance of ID-Matching can be improved through two methods: 1)
enhancing the usage of image information and 2) increasing the quantity of
information of individual descriptions. Based on these insights, we propose a
novel video captioning method called Recognizing Identities for Captioning
Effectively (RICE). Extensive experiments including assessments of caption
quality and ID-Matching performance, demonstrate the superiority of our
approach. Notably, when implemented on GPT-4o, our RICE improves the precision
of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15%
to 80% compared to baseline. RICE makes it possible to continuously track
different individuals in the captions of long videos.

</details>


### [67] [No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts](https://arxiv.org/abs/2510.06988)
*Girolamo Macaluso,Lorenzo Mandelli,Mirko Bicchierai,Stefano Berretti,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的后训练框架，用于微调预训练的运动扩散模型，仅需文本提示，无需运动数据。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在适应新动作或风格时需要额外运动数据和全量训练，成本高且难以扩展。

Method: 使用预训练的文本-运动检索网络作为奖励信号，通过Denoising Diffusion Policy Optimization优化扩散策略。

Result: 在跨数据集和留一实验中，生成的运动质量和多样性显著提升，同时保持原始分布性能。

Conclusion: 该方法为运动适应提供了一种灵活、数据高效且保护隐私的解决方案。

Abstract: Diffusion models have recently advanced human motion generation, producing
realistic and diverse animations from textual prompts. However, adapting these
models to unseen actions or styles typically requires additional motion capture
data and full retraining, which is costly and difficult to scale. We propose a
post-training framework based on Reinforcement Learning that fine-tunes
pretrained motion diffusion models using only textual prompts, without
requiring any motion ground truth. Our approach employs a pretrained
text-motion retrieval network as a reward signal and optimizes the diffusion
policy with Denoising Diffusion Policy Optimization, effectively shifting the
model's generative distribution toward the target domain without relying on
paired motion data. We evaluate our method on cross-dataset adaptation and
leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across
both latent- and joint-space diffusion architectures. Results from quantitative
metrics and user studies show that our approach consistently improves the
quality and diversity of generated motions, while preserving performance on the
original distribution. Our approach is a flexible, data-efficient, and
privacy-preserving solution for motion adaptation.

</details>


### [68] [Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models](https://arxiv.org/abs/2510.07008)
*Gianmarco Perantoni,Giulio Weikmann,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习和贝叶斯建模的新方法，用于分类年度卫星图像时间序列（SITS），通过隐马尔可夫模型（HMM）和Transformer编码器（TE）捕捉时间相关性和作物类型模式。


<details>
  <summary>Details</summary>
Motivation: 年度土地覆盖图的时间一致性对建模土地覆盖演变至关重要，需要一种能捕捉复杂时间相关性和作物类型序列的方法。

Method: 结合HMM和基于TE的深度神经网络，通过级联分类捕捉年度SITS的时间相关性和作物类型序列。

Result: 在47种作物类型和6年Sentinel-2数据的验证中，HMM显著提升了分类性能和F1分数。

Conclusion: 提出的方法有效建模了时间一致性，HMM的引入显著提升了分类效果。

Abstract: The temporal consistency of yearly land-cover maps is of great importance to
model the evolution and change of the land cover over the years. In this paper,
we focus the attention on a novel approach to classification of yearly
satellite image time series (SITS) that combines deep learning with Bayesian
modelling, using Hidden Markov Models (HMMs) integrated with Transformer
Encoder (TE) based DNNs. The proposed approach aims to capture both i)
intricate temporal correlations in yearly SITS and ii) specific patterns in
multiyear crop type sequences. It leverages the cascade classification of an
HMM layer built on top of the TE, discerning consistent yearly crop-type
sequences. Validation on a multiyear crop type classification dataset spanning
47 crop types and six years of Sentinel-2 acquisitions demonstrates the
importance of modelling temporal consistency in the predicted labels. HMMs
enhance the overall performance and F1 scores, emphasising the effectiveness of
the proposed approach.

</details>


### [69] [U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking](https://arxiv.org/abs/2510.07041)
*Fenghe Tang,Chengqi Dong,Wenxin Ma,Zikang Xu,Heqin Zhu,Zihang Jiang,Rongsheng Wang,Yuhao Wang,Chenxu Wu,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: U-Bench是首个大规模、统计严谨的基准测试，评估了100种U-Net变体在28个数据集和10种成像模态中的表现，填补了医学图像分割领域缺乏全面评估的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管U-Net在医学图像分割中占据主导地位，但缺乏对其变体的系统性评估，尤其是在统计稳健性、零样本泛化和计算效率方面的考量。

Method: U-Bench通过三个关键维度（统计稳健性、零样本泛化和计算效率）评估模型，并引入新指标U-Score，同时提供模型选择指导和公开资源。

Result: U-Bench揭示了以往评估的不足，并为未来U-Net模型的公平、可重复和实用基准测试奠定了基础。

Conclusion: U-Bench为医学图像分割领域提供了全面的评估工具和资源，推动了该领域的进一步发展。

Abstract: Over the past decade, U-Net has been the dominant architecture in medical
image segmentation, leading to the development of thousands of U-shaped
variants. Despite its widespread adoption, there is still no comprehensive
benchmark to systematically evaluate their performance and utility, largely
because of insufficient statistical validation and limited consideration of
efficiency and generalization across diverse datasets. To bridge this gap, we
present U-Bench, the first large-scale, statistically rigorous benchmark that
evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our
contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates
models along three key dimensions: statistical robustness, zero-shot
generalization, and computational efficiency. We introduce a novel metric,
U-Score, which jointly captures the performance-efficiency trade-off, offering
a deployment-oriented perspective on model progress. (2) Systematic Analysis
and Model Selection Guidance: We summarize key findings from the large-scale
evaluation and systematically analyze the impact of dataset characteristics and
architectural paradigms on model performance. Based on these insights, we
propose a model advisor agent to guide researchers in selecting the most
suitable models for specific datasets and tasks. (3) Public Availability: We
provide all code, models, protocols, and weights, enabling the community to
reproduce our results and extend the benchmark with future methods. In summary,
U-Bench not only exposes gaps in previous evaluations but also establishes a
foundation for fair, reproducible, and practically relevant benchmarking in the
next decade of U-Net-based segmentation models. The project can be accessed at:
https://fenghetan9.github.io/ubench. Code is available at:
https://github.com/FengheTan9/U-Bench.

</details>


### [70] [Concept Retrieval -- What and How?](https://arxiv.org/abs/2510.07058)
*Ori nizan,Oren Shrout,Ayellet Tal*

Main category: cs.CV

TL;DR: 论文提出了一种基于双峰高斯分布的新方法，用于从图像中检索共享核心概念的其他图像，超越了传统的视觉或语义相似性方法。


<details>
  <summary>Details</summary>
Motivation: 传统检索或聚类方法仅关注视觉或语义相似性，而忽略了图像背后的核心概念。本文旨在解决这一问题，捕捉图像的潜在叙事。

Method: 通过两个关键观察：(1) 嵌入空间中的邻居可能与查询共享至少一个概念，但不一定彼此共享相同概念；(2) 使用双峰高斯分布建模邻域结构，以识别核心概念。

Result: 定性和定量评估以及人工评估均证实了该方法的有效性。

Conclusion: 该方法在识别和检索共享核心概念的图像方面表现出色，为图像理解提供了新视角。

Abstract: A concept may reflect either a concrete or abstract idea. Given an input
image, this paper seeks to retrieve other images that share its central
concepts, capturing aspects of the underlying narrative. This goes beyond
conventional retrieval or clustering methods, which emphasize visual or
semantic similarity. We formally define the problem, outline key requirements,
and introduce appropriate evaluation metrics. We propose a novel approach
grounded in two key observations: (1) While each neighbor in the embedding
space typically shares at least one concept with the query, not all neighbors
necessarily share the same concept with one another. (2) Modeling this
neighborhood with a bimodal Gaussian distribution uncovers meaningful structure
that facilitates concept identification. Qualitative, quantitative, and human
evaluations confirm the effectiveness of our approach. See the package on PyPI:
https://pypi.org/project/coret/

</details>


### [71] [DADO: A Depth-Attention framework for Object Discovery](https://arxiv.org/abs/2510.07089)
*Federico Gonzalez,Estefania Talavera,Petia Radeva*

Main category: cs.CV

TL;DR: DADO是一种结合注意力机制和深度模型的无监督对象发现方法，通过动态加权适应图像全局特征，在标准基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督对象发现是计算机视觉中的重要挑战，现有方法常因噪声注意力图或复杂场景表现不佳。

Method: DADO结合注意力机制和深度模型，采用动态加权策略自适应调整特征权重。

Result: 在标准基准测试中，DADO在对象发现准确性和鲁棒性上优于现有方法，且无需微调。

Conclusion: DADO为无监督对象发现提供了一种高效且鲁棒的解决方案。

Abstract: Unsupervised object discovery, the task of identifying and localizing objects
in images without human-annotated labels, remains a significant challenge and a
growing focus in computer vision. In this work, we introduce a novel model,
DADO (Depth-Attention self-supervised technique for Discovering unseen
Objects), which combines an attention mechanism and a depth model to identify
potential objects in images. To address challenges such as noisy attention maps
or complex scenes with varying depth planes, DADO employs dynamic weighting to
adaptively emphasize attention or depth features based on the global
characteristics of each image. We evaluated DADO on standard benchmarks, where
it outperforms state-of-the-art methods in object discovery accuracy and
robustness without the need for fine-tuning.

</details>


### [72] [Enhancing Concept Localization in CLIP-based Concept Bottleneck Models](https://arxiv.org/abs/2510.07115)
*Rémi Kazmierczak,Steve Azzolin,Eloïse Berthier,Goran Frehse,Gianni Franchi*

Main category: cs.CV

TL;DR: 论文提出CHILI方法，通过局部可解释性抑制CLIP在CBM中的概念幻觉，提升解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: CLIP在零样本概念提取中易产生概念幻觉，影响CBM的解释可信度。

Method: 引入CHILI技术，解耦图像嵌入并定位目标概念对应的像素。

Result: CHILI能生成更具可解释性的显著性解释，抑制概念幻觉。

Conclusion: CHILI有效提升CBM的解释可靠性，支持更清晰的显著性解释。

Abstract: This paper addresses explainable AI (XAI) through the lens of Concept
Bottleneck Models (CBMs) that do not require explicit concept annotations,
relying instead on concepts extracted using CLIP in a zero-shot manner. We show
that CLIP, which is central in these techniques, is prone to concept
hallucination, incorrectly predicting the presence or absence of concepts
within an image in scenarios used in numerous CBMs, hence undermining the
faithfulness of explanations. To mitigate this issue, we introduce Concept
Hallucination Inhibition via Localized Interpretability (CHILI), a technique
that disentangles image embeddings and localizes pixels corresponding to target
concepts. Furthermore, our approach supports the generation of saliency-based
explanations that are more interpretable.

</details>


### [73] [MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency](https://arxiv.org/abs/2510.07119)
*Dongki Jung,Jaehoon Choi,Yonghan Lee,Sungmin Eum,Heesung Kwon,Dinesh Manocha*

Main category: cs.CV

TL;DR: MoRe是一种无需训练的单目几何优化方法，通过特征匹配和图优化提升跨视角一致性和尺度对齐。


<details>
  <summary>Details</summary>
Motivation: 解决单目3D基础模型中的尺度模糊问题，同时保持3D结构，提升跨视角一致性和稀疏视图渲染效果。

Method: 利用特征匹配建立帧间对应关系，并通过图优化框架进行局部平面近似，结合3D点和表面法线估计。

Result: MoRe不仅改进了3D重建，还提升了新视角合成效果，尤其在稀疏视图渲染场景中表现突出。

Conclusion: MoRe为单目3D感知任务提供了一种高效且无需训练的几何优化方案。

Abstract: Monocular 3D foundation models offer an extensible solution for perception
tasks, making them attractive for broader 3D vision applications. In this
paper, we propose MoRe, a training-free Monocular Geometry Refinement method
designed to improve cross-view consistency and achieve scale alignment. To
induce inter-frame relationships, our method employs feature matching between
frames to establish correspondences. Rather than applying simple least squares
optimization on these matched points, we formulate a graph-based optimization
framework that performs local planar approximation using the estimated 3D
points and surface normals estimated by monocular foundation models. This
formulation addresses the scale ambiguity inherent in monocular geometric
priors while preserving the underlying 3D structure. We further demonstrate
that MoRe not only enhances 3D reconstruction but also improves novel view
synthesis, particularly in sparse view rendering scenarios.

</details>


### [74] [Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?](https://arxiv.org/abs/2510.07126)
*Jan Fiszer,Dominika Ciupek,Maciej Malawski*

Main category: cs.CV

TL;DR: 联邦学习在非独立同分布数据下仍能有效训练高性能脑肿瘤分割模型，保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在医学影像中因数据隐私、存储和传输问题而面临的挑战。

Method: 通过不同MRI强度归一化技术模拟非独立同分布数据，训练和测试脑肿瘤分割模型。

Result: 联邦学习方法对不一致归一化数据表现出韧性，3D Dice分数达92%，与集中式模型相当。

Conclusion: 联邦学习是保护数据隐私同时训练高性能模型的有效解决方案。

Abstract: Deep learning (DL) has been increasingly applied in medical imaging, however,
it requires large amounts of data, which raises many challenges related to data
privacy, storage, and transfer. Federated learning (FL) is a training paradigm
that overcomes these issues, though its effectiveness may be reduced when
dealing with non-independent and identically distributed (non-IID) data. This
study simulates non-IID conditions by applying different MRI intensity
normalization techniques to separate data subsets, reflecting a common cause of
heterogeneity. These subsets are then used for training and testing models for
brain tumor segmentation. The findings provide insights into the influence of
the MRI intensity normalization methods on segmentation models, both training
and inference. Notably, the FL methods demonstrated resilience to
inconsistently normalized data across clients, achieving the 3D Dice score of
92%, which is comparable to a centralized model (trained using all data). These
results indicate that FL is a solution to effectively train high-performing
models without violating data privacy, a crucial concern in medical
applications. The code is available at:
https://github.com/SanoScience/fl-varying-normalization.

</details>


### [75] [Graph Conditioned Diffusion for Controllable Histopathology Image Generation](https://arxiv.org/abs/2510.07129)
*Sarah Cechnicka,Matthew Baugh,Weitong Zhang,Mischa Dombrowski,Zhe Li,Johannes C. Paetzold,Candice Roufosse,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出了一种基于图的条件扩散模型，用于医学图像生成，通过图节点表示图像结构，实现精细控制。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在医学图像生成中缺乏语义结构和强先验，难以确保生成内容的可控性。

Method: 使用图节点表示图像主要结构，通过Transformer模块处理并集成到扩散模型中，实现细粒度控制。

Result: 在真实组织病理学用例中验证，生成数据可替代标注患者数据用于下游分割任务。

Conclusion: 图条件扩散模型在医学图像生成中实现了可控性和实用性。

Abstract: Recent advances in Diffusion Probabilistic Models (DPMs) have set new
standards in high-quality image synthesis. Yet, controlled generation remains
challenging, particularly in sensitive areas such as medical imaging. Medical
images feature inherent structure such as consistent spatial arrangement, shape
or texture, all of which are critical for diagnosis. However, existing DPMs
operate in noisy latent spaces that lack semantic structure and strong priors,
making it difficult to ensure meaningful control over generated content. To
address this, we propose graph-based object-level representations for
Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding
to each major structure in the image, encapsulating their individual features
and relationships. These graph representations are processed by a transformer
module and integrated into a diffusion model via the text-conditioning
mechanism, enabling fine-grained control over generation. We evaluate this
approach using a real-world histopathology use case, demonstrating that our
generated data can reliably substitute for annotated patient data in downstream
segmentation tasks. The code is available here.

</details>


### [76] [Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models](https://arxiv.org/abs/2510.07135)
*Karim El Khoury,Maxime Zanella,Christophe De Vleeschouwer,Benoit Macq*

Main category: cs.CV

TL;DR: 提出了首个评估遥感视觉语言模型（RSVLMs）在少样本学习中的基准测试，发现不同模型在少样本适应中表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 探索RSVLMs在少样本学习中的泛化能力，填补现有研究的空白。

Method: 在十个遥感场景分类数据集上，对三种RSVLMs应用五种少样本适应策略进行实验。

Result: 发现零样本性能相似的模型在少样本适应中表现差异显著，且现有方法无明确最优解。

Conclusion: 需开发更鲁棒的少样本适应方法，并提供了开源基准框架以促进未来研究。

Abstract: Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable
potential thanks to large-scale pretraining, achieving strong zero-shot
performance on various tasks. However, their ability to generalize in low-data
regimes, such as few-shot learning, remains insufficiently explored. In this
work, we present the first structured benchmark for evaluating few-shot
adaptation methods on RSVLMs. We conduct comprehensive experiments across ten
remote sensing scene classification datasets, applying five widely used
few-shot adaptation strategies to three state-of-the-art RSVLMs with varying
backbones. Our findings reveal that models with similar zero-shot performance
can exhibit markedly different behavior under few-shot adaptation, with some
RSVLMs being inherently more amenable to such adaptation than others. The
variability of performance and the absence of a clear winner among existing
methods highlight the need for the development of more robust methods for
few-shot adaptation tailored to RS. To facilitate future research, we provide a
reproducible benchmarking framework and open-source code to systematically
evaluate RSVLMs under few-shot conditions. The source code is publicly
available on Github: https://github.com/elkhouryk/fewshot_RSVLMs

</details>


### [77] [Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods](https://arxiv.org/abs/2510.07143)
*Chenfei Liao,Wensong Wang,Zichen Wen,Xu Zheng,Yiyu Wang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Xin Zou,Yuqian Fu,Bin Ren,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 论文提出VTC-Bench框架，通过数据过滤机制优化视觉令牌压缩方法的评估，发现简单降采样优于复杂方法。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试用于评估视觉令牌压缩方法时存在任务不匹配问题，导致评估不准确。

Method: 通过实验观察当前基准测试的噪声问题，提出数据过滤机制，构建VTC-Bench框架。

Result: 简单降采样在多个基准测试中表现优于复杂压缩方法，VTC-Bench能更公平评估压缩方法。

Conclusion: VTC-Bench通过去噪机制提升评估准确性，为视觉令牌压缩研究提供更可靠基准。

Abstract: Recent endeavors to accelerate inference in Multimodal Large Language Models
(MLLMs) have primarily focused on visual token compression. The effectiveness
of these methods is typically assessed by measuring the accuracy drop on
established benchmarks, comparing model performance before and after
compression. However, these benchmarks are originally designed to assess the
perception and reasoning capabilities of MLLMs, rather than to evaluate
compression techniques. As a result, directly applying them to visual token
compression introduces a task mismatch. Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks. Through extensive
experiments, we make the following observations: (i) Current benchmarks are
noisy for the visual token compression task. (ii) Down-sampling is able to
serve as a data filter to evaluate the difficulty of samples in the visual
token compression task. Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods. All data and code are available at
https://github.com/Chenfei-Liao/VTC-Bench.

</details>


### [78] [MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis](https://arxiv.org/abs/2510.07190)
*Yihao Zhi,Chenghong Li,Hongjie Liao,Xihe Yang,Zhengwentai Sun,Jiahao Chang,Xiaodong Cun,Wensen Feng,Xiaoguang Han*

Main category: cs.CV

TL;DR: MV-Performer是一个创新框架，用于从单目全身捕捉生成同步的新视角视频，专注于人中心子领域，解决了360度视角变化的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法主要集中于前视图的相机轨迹调整，难以生成360度视角变化。本文专注于人中心子领域，旨在解决这一问题。

Method: 利用MVHumanNet数据集和相机依赖的法线图，结合多视角人中心视频扩散模型，融合参考视频、部分渲染和不同视角的信息。

Result: 在三个数据集上的实验表明，MV-Performer在效果和鲁棒性上达到了最先进水平。

Conclusion: MV-Performer为人中心的4D新视角合成提供了一个强大的模型。

Abstract: Recent breakthroughs in video generation, powered by large-scale datasets and
diffusion techniques, have shown that video diffusion models can function as
implicit 4D novel view synthesizers. Nevertheless, current methods primarily
concentrate on redirecting camera trajectory within the front view while
struggling to generate 360-degree viewpoint changes. In this paper, we focus on
human-centric subdomain and present MV-Performer, an innovative framework for
creating synchronized novel view videos from monocular full-body captures. To
achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset
and incorporate an informative condition signal. Specifically, we use the
camera-dependent normal maps rendered from oriented partial point clouds, which
effectively alleviate the ambiguity between seen and unseen observations. To
maintain synchronization in the generated videos, we propose a multi-view
human-centric video diffusion model that fuses information from the reference
video, partial rendering, and different viewpoints. Additionally, we provide a
robust inference procedure for in-the-wild video cases, which greatly mitigates
the artifacts induced by imperfect monocular depth estimation. Extensive
experiments on three datasets demonstrate our MV-Performer's state-of-the-art
effectiveness and robustness, setting a strong model for human-centric 4D novel
view synthesis.

</details>


### [79] [Resolution scaling governs DINOv3 transfer performance in chest radiograph classification](https://arxiv.org/abs/2510.07191)
*Soroosh Tayebi Arasteh,Mina Shaigan,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: DINOv3在512x512分辨率下优于DINOv2和ImageNet初始化，尤其在成人数据集中表现突出，但在儿科数据中无显著差异。ConvNeXt-B整体优于ViT-B/16。


<details>
  <summary>Details</summary>
Motivation: 探讨自监督学习（SSL）在胸部X光片中的价值，特别是DINOv3的设计是否优于现有方法。

Method: 在七个数据集（n>814,000）上比较DINOv3、DINOv2和ImageNet初始化，评估ViT-B/16和ConvNeXt-B两种骨干网络，测试不同分辨率（224x224、512x512、1024x1024）和冻结特征的效果。

Result: DINOv3在512x512分辨率下表现最佳，尤其在边界依赖和小病灶异常检测中。ConvNeXt-B优于ViT-B/16，1024x1024分辨率未带来额外提升。

Conclusion: 512x512分辨率下DINOv3初始化的ConvNeXt-B网络性能最强，适用于胸部X光片分析，尤其在急诊和重症护理中检测细微病变。

Abstract: Self-supervised learning (SSL) has advanced visual representation learning,
but its value in chest radiography, a high-volume imaging modality with
fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL
models through Gram-anchored self-distillation. Whether these design choices
improve transfer learning for chest radiography has not been systematically
tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across
seven datasets (n>814,000). Two representative backbones were evaluated:
ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and
1024x1024 pixels. We additionally assessed frozen features from a 7B model. The
primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2
achieved comparable performance on adult datasets. Increasing resolution to
512x512 yielded consistent improvements for DINOv3 over both DINOv2 and
ImageNet. In contrast, results in pediatric cohort showed no differences across
initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models
using frozen DINOv3-7B features underperformed relative to fully finetuned
86-89M-parameter backbones, highlighting the importance of domain adaptation.
Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains
were most evident for boundary-dependent and small focal abnormalities. In
chest radiography, higher input resolution is critical for leveraging the
benefits of modern self-supervised models. 512x512 pixels represent a practical
upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest
performance, while larger inputs offer minimal return on cost. Clinically,
these findings support use of finetuned, mid-sized backbones at 512x512 for
chest radiograph interpretation, with the greatest gains expected in detecting
subtle or boundary-centered lesions relevant to emergency and critical care
settings.

</details>


### [80] [EigenScore: OOD Detection using Covariance in Diffusion Models](https://arxiv.org/abs/2510.07206)
*Shirin Shoushtari,Yi Wang,Xiao Shi,M. Salman Asif,Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: EigenScore利用扩散模型的后验协方差特征值谱进行OOD检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: OOD检测对机器学习系统在安全敏感领域的部署至关重要，扩散模型在生成复杂数据分布方面表现出色。

Method: 通过无雅可比子空间迭代法估计特征值，利用后验协方差作为分布偏移信号。

Result: EigenScore在AUROC上提升5%，在近OOD场景（如CIFAR-10 vs CIFAR-100）中表现稳健。

Conclusion: 后验协方差是可靠的OOD检测信号，EigenScore在性能和鲁棒性上均优于现有方法。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of
machine learning systems in safety-sensitive domains. Diffusion models have
recently emerged as powerful generative models, capable of capturing complex
data distributions through iterative denoising. Building on this progress,
recent work has explored their potential for OOD detection. We propose
EigenScore, a new OOD detection method that leverages the eigenvalue spectrum
of the posterior covariance induced by a diffusion model. We argue that
posterior covariance provides a consistent signal of distribution shift,
leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear
spectral signature. We further provide analysis explicitly linking posterior
covariance to distribution mismatch, establishing it as a reliable signal for
OOD detection. To ensure tractability, we adopt a Jacobian-free subspace
iteration method to estimate the leading eigenvalues using only forward
evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance,
with up to 5% AUROC improvement over the best baseline. Notably, it remains
robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing
diffusion-based methods often fail.

</details>


### [81] [GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation](https://arxiv.org/abs/2510.07217)
*Wen Ye,Zhaocheng Liu,Yuwei Gui,Tingyu Yuan,Yunyue Su,Bowen Fang,Chaoyang Zhao,Qiang Liu,Liang Wang*

Main category: cs.CV

TL;DR: GenPilot提出了一种灵活高效的测试时提示优化策略，通过多智能体系统提升文本到图像生成的语义一致性和细节完整性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂长提示下表现不佳，缺乏系统错误分析和优化策略，GenPilot旨在解决这些问题。

Method: 采用多智能体系统（GenPilot），集成错误分析、聚类自适应探索、细粒度验证和记忆模块，实现迭代优化。

Result: 在DPG-bench和Geneval上分别提升16.9%和5.7%，显著增强文本与图像一致性和结构连贯性。

Conclusion: GenPilot是一种模型无关、可解释的优化策略，适用于复杂长提示，并总结了常见错误模式与优化经验。

Abstract: Text-to-image synthesis has made remarkable progress, yet accurately
interpreting complex and lengthy prompts remains challenging, often resulting
in semantic inconsistencies and missing details. Existing solutions, such as
fine-tuning, are model-specific and require training, while prior automatic
prompt optimization (APO) approaches typically lack systematic error analysis
and refinement strategies, resulting in limited reliability and effectiveness.
Meanwhile, test-time scaling methods operate on fixed prompts and on noise or
sample numbers, limiting their interpretability and adaptability. To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text. We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization. Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.
Simultaneously, we summarize the common patterns of errors and the refinement
strategy, offering more experience and encouraging further exploration.
Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy. The code is
available at https://github.com/27yw/GenPilot.

</details>


### [82] [TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation](https://arxiv.org/abs/2510.07249)
*Jiaben Chen,Zixin Wang,Ailing Zeng,Yang Fu,Xueyang Yu,Siyuan Cen,Julian Tanke,Yihang Chen,Koichi Saito,Yuki Mitsufuji,Chuang Gan*

Main category: cs.CV

TL;DR: TalkCuts是一个大规模多视角人类语音视频数据集，包含164k片段和500小时高质量视频，支持多模态学习。Orator是一个基于LLM的多模态生成框架，显著提升了多视角视频的连贯性和视觉吸引力。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为单视角静态视频，缺乏多视角和多样性。TalkCuts填补了这一空白，支持可控多视角语音视频生成的研究。

Method: 提出TalkCuts数据集，包含多视角视频、文本描述和动作标注。开发Orator框架，利用LLM指导多模态视频生成。

Result: 实验表明，TalkCuts显著提升了多视角视频的连贯性和视觉吸引力。

Conclusion: TalkCuts为可控多视角语音视频生成和多模态学习提供了坚实基础。

Abstract: In this work, we present TalkCuts, a large-scale dataset designed to
facilitate the study of multi-shot human speech video generation. Unlike
existing datasets that focus on single-shot, static viewpoints, TalkCuts offers
164k clips totaling over 500 hours of high-quality human speech videos with
diverse camera shots, including close-up, half-body, and full-body views. The
dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X
motion annotations, covering over 10k identities, enabling multimodal learning
and evaluation. As a first attempt to showcase the value of the dataset, we
present Orator, an LLM-guided multi-modal generation framework as a simple
baseline, where the language model functions as a multi-faceted director,
orchestrating detailed specifications for camera transitions, speaker
gesticulations, and vocal modulation. This architecture enables the synthesis
of coherent long-form videos through our integrated multi-modal video
generation module. Extensive experiments in both pose-guided and audio-driven
settings show that training on TalkCuts significantly enhances the
cinematographic coherence and visual appeal of generated multi-shot speech
videos. We believe TalkCuts provides a strong foundation for future work in
controllable, multi-shot speech video generation and broader multimodal
learning.

</details>


### [83] [Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection](https://arxiv.org/abs/2510.07277)
*Franco Javier Arellano,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 比较基础模型（FM）和传统迁移学习方法在糖尿病黄斑水肿（DME）检测中的表现，发现FM并未显著优于轻量级CNN。


<details>
  <summary>Details</summary>
Motivation: 探讨基础模型在数据稀缺环境下对DME检测任务的有效性。

Method: 比较RETFound、FLAIR和EfficientNet-B0在不同数据集和训练设置下的性能。

Result: EfficientNet-B0在多数情况下表现最佳，RETFound仅在OEFI数据集中表现良好，FLAIR在零样本学习中表现优异。

Conclusion: FM在DME检测任务中表现不佳，轻量级CNN仍是数据稀缺环境下的强基线。

Abstract: Diabetic Macular Edema (DME) is a leading cause of vision loss among patients
with Diabetic Retinopathy (DR). While deep learning has shown promising results
for automatically detecting this condition from fundus images, its application
remains challenging due the limited availability of annotated data. Foundation
Models (FM) have emerged as an alternative solution. However, it is unclear if
they can cope with DME detection in particular. In this paper, we
systematically compare different FM and standard transfer learning approaches
for this task. Specifically, we compare the two most popular FM for retinal
images--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different
training regimes and evaluation settings in IDRiD, MESSIDOR-2 and
OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do
not consistently outperform fine-tuned CNNs in this task. In particular, an
EfficientNet-B0 ranked first or second in terms of area under the ROC and
precision/recall curves in most evaluation settings, with RETFound only showing
promising results in OEFI. FLAIR, on the other hand, demonstrated competitive
zero-shot performance, achieving notable AUC-PR scores when prompted
appropriately. These findings reveal that FM might not be a good tool for
fine-grained ophthalmic tasks such as DME detection even after fine-tuning,
suggesting that lightweight CNNs remain strong baselines in data-scarce
environments.

</details>


### [84] [SpecGuard: Spectral Projection-based Advanced Invisible Watermarking](https://arxiv.org/abs/2510.07302)
*Inzamamul Alam,Md Tanvir Islam,Khan Muhammad,Simon S. Woo*

Main category: cs.CV

TL;DR: SpecGuard是一种新颖的水印方法，通过在频域中嵌入信息，提高了对多种攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法对多种变换（如失真、图像再生和对抗扰动）缺乏鲁棒性，难以满足实际需求。

Method: 通过小波投影分解高频带，将信息从空间域转换到频域，并利用快速傅里叶变换近似高效嵌入水印。编码阶段通过强度因子增强对攻击的抵抗力。

Result: SpecGuard在不可见性、容量和鲁棒性方面优于现有方法。

Conclusion: SpecGuard通过频域嵌入和强度因子设计，显著提升了水印的鲁棒性和实用性。

Abstract: Watermarking embeds imperceptible patterns into images for authenticity
verification. However, existing methods often lack robustness against various
transformations primarily including distortions, image regeneration, and
adversarial perturbation, creating real-world challenges. In this work, we
introduce SpecGuard, a novel watermarking approach for robust and invisible
image watermarking. Unlike prior approaches, we embed the message inside hidden
convolution layers by converting from the spatial domain to the frequency
domain using spectral projection of a higher frequency band that is decomposed
by wavelet projection. Spectral projection employs Fast Fourier Transform
approximation to transform spatial data into the frequency domain efficiently.
In the encoding phase, a strength factor enhances resilience against diverse
attacks, including adversarial, geometric, and regeneration-based distortions,
ensuring the preservation of copyrighted information. Meanwhile, the decoder
leverages Parseval's theorem to effectively learn and extract the watermark
pattern, enabling accurate retrieval under challenging transformations. We
evaluate the proposed SpecGuard based on the embedded watermark's invisibility,
capacity, and robustness. Comprehensive experiments demonstrate the proposed
SpecGuard outperforms the state-of-the-art models. To ensure reproducibility,
the full code is released on
\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\textcolor{blue}{\textbf{GitHub}}}.

</details>


### [85] [MATRIX: Mask Track Alignment for Interaction-aware Video Generation](https://arxiv.org/abs/2510.07310)
*Siyoon Jin,Seongchan Kim,Dahyun Chung,Jaeho Lee,Hyunwook Choi,Jisu Nam,Jiyoung Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: 论文提出MATRIX-11K数据集和MATRIX正则化方法，提升视频DiTs在多实例和交互建模中的表现。


<details>
  <summary>Details</summary>
Motivation: 视频DiTs在多实例或主客体交互建模中存在不足，研究其内部交互表示机制。

Method: 通过MATRIX-11K数据集分析视频DiTs的语义基础和传播，提出MATRIX正则化方法。

Result: MATRIX提升了交互保真度和语义对齐，减少了漂移和幻觉。

Conclusion: MATRIX方法有效优化了视频DiTs的交互建模能力。

Abstract: Video DiTs have advanced video generation, yet they still struggle to model
multi-instance or subject-object interactions. This raises a key question: How
do these models internally represent interactions? To answer this, we curate
MATRIX-11K, a video dataset with interaction-aware captions and multi-instance
mask tracks. Using this dataset, we conduct a systematic analysis that
formalizes two perspectives of video DiTs: semantic grounding, via
video-to-text attention, which evaluates whether noun and verb tokens capture
instances and their relations; and semantic propagation, via video-to-video
attention, which assesses whether instance bindings persist across frames. We
find both effects concentrate in a small subset of interaction-dominant layers.
Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.
We further propose InterGenEval, an evaluation protocol for interaction-aware
video generation. In experiments, MATRIX improves both interaction fidelity and
semantic alignment while reducing drift and hallucination. Extensive ablations
validate our design choices. Codes and weights will be released.

</details>


### [86] [Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers](https://arxiv.org/abs/2510.07316)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Xianqi Wang,Jingfeng Yao,Lianghui Zhu,Yuechuan Pu,Cheng Chi,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Sida Peng,Xin Yang*

Main category: cs.CV

TL;DR: Pixel-Perfect Depth是一种基于像素空间扩散生成的单目深度估计模型，能生成高质量、无飞点的点云。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖VAE压缩深度图到潜在空间，导致边缘和细节出现飞点问题。

Method: 直接在像素空间进行扩散生成，避免VAE引入的伪影；提出SP-DiT和级联DiT设计提升效率和精度。

Result: 在五个基准测试中表现最佳，边缘感知点云评估显著优于其他模型。

Conclusion: 像素空间扩散生成能有效解决飞点问题，提升深度估计质量。

Abstract: This paper presents Pixel-Perfect Depth, a monocular depth estimation model
based on pixel-space diffusion generation that produces high-quality,
flying-pixel-free point clouds from estimated depth maps. Current generative
depth estimation models fine-tune Stable Diffusion and achieve impressive
performance. However, they require a VAE to compress depth maps into latent
space, which inevitably introduces \textit{flying pixels} at edges and details.
Our model addresses this challenge by directly performing diffusion generation
in the pixel space, avoiding VAE-induced artifacts. To overcome the high
complexity associated with pixel-space generation, we introduce two novel
designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which
incorporate semantic representations from vision foundation models into DiT to
prompt the diffusion process, thereby preserving global semantic consistency
while enhancing fine-grained visual details; and 2) Cascade DiT Design that
progressively increases the number of tokens to further enhance efficiency and
accuracy. Our model achieves the best performance among all published
generative models across five benchmarks, and significantly outperforms all
other models in edge-aware point cloud evaluation.

</details>


### [87] [Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms](https://arxiv.org/abs/2510.07317)
*Natacha Kuete Meli,Shuteng Wang,Marcel Seelbach Benkner,Michele Sasdelli,Tat-Jun Chin,Tolga Birdal,Michael Moeller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 量子增强计算机视觉（QeCV）是一个新兴研究领域，结合了计算机视觉、优化理论、机器学习和量子计算，旨在利用量子计算的优势改进视觉信号处理。


<details>
  <summary>Details</summary>
Motivation: 现有非量子方法在某些场景下无法在合理时间内找到解决方案或只能提供近似解，量子计算可能提供更好的时间可扩展性。

Method: 开发与量子硬件兼容的新算法，利用基于门的量子计算和量子退火两种主要量子计算范式。

Result: 综述了QeCV的研究现状，提供了量子计算机的操作原理、工具和学习资源，并讨论了开放挑战和社会影响。

Conclusion: QeCV具有潜力，但需要开发新算法以充分发挥量子计算在计算机视觉中的优势。

Abstract: Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.

</details>


### [88] [Temporal Prompting Matters: Rethinking Referring Video Object Segmentation](https://arxiv.org/abs/2510.07319)
*Ci-Siang Lin,Min-Hung Chen,I-Jieh Liu,Chien-Yi Wang,Sifei Liu,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 提出了一种基于现有基础分割模型的Tenet框架，通过分解RVOS任务为引用、视频和分割因素，利用时间提示生成与选择高效适应视频对象分割。


<details>
  <summary>Details</summary>
Motivation: 解决现有RVOS方法需要密集掩码标注和计算资源消耗大的问题，探索任务的关键因素。

Method: 利用现成目标检测器和跟踪器生成时间提示，提出Prompt Preference Learning评估提示质量，指导基础分割模型。

Result: 在RVOS基准测试中验证了Tenet框架的有效性。

Conclusion: Tenet框架通过分解任务和高效提示生成，实现了高质量的视频对象分割。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment the object
referred to by the query sentence in the video. Most existing methods require
end-to-end training with dense mask annotations, which could be
computation-consuming and less scalable. In this work, we rethink the RVOS
problem and aim to investigate the key to this task. Based on existing
foundation segmentation models, we decompose the RVOS task into referring,
video, and segmentation factors, and propose a Temporal Prompt Generation and
Selection (Tenet) framework to address the referring and video factors while
leaving the segmentation problem to foundation models. To efficiently adapt
image-based foundation segmentation models to referring video object
segmentation, we leverage off-the-shelf object detectors and trackers to
produce temporal prompts associated with the referring sentence. While
high-quality temporal prompts could be produced, they can not be easily
identified from confidence scores. To tackle this issue, we propose Prompt
Preference Learning to evaluate the quality of the produced temporal prompts.
By taking such prompts to instruct image-based foundation segmentation models,
we would be able to produce high-quality masks for the referred object,
enabling efficient model adaptation to referring video object segmentation.
Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet
framework.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [89] [Vi-TacMan: Articulated Object Manipulation via Vision and Touch](https://arxiv.org/abs/2510.06339)
*Leiyao Cui,Zihang Zhao,Sirui Xie,Wenhuan Zhang,Zhi Han,Yixin Zhu*

Main category: cs.RO

TL;DR: Vi-TacMan结合视觉和触觉反馈，实现无需显式运动学模型的精确关节物体操作。


<details>
  <summary>Details</summary>
Motivation: 解决视觉方法在陌生物体上估计不精确和触觉方法需要准确初始化的问题。

Method: 使用视觉提出抓取和粗略方向，触觉控制器进行精确执行，结合表面法线和von Mises-Fisher分布建模方向。

Result: 在超过50,000个模拟和真实物体上验证了跨类别泛化能力，显著优于基线方法。

Conclusion: 粗视觉线索结合触觉反馈可实现可靠操作，为无结构环境中的自主系统提供可扩展范式。

Abstract: Autonomous manipulation of articulated objects remains a fundamental
challenge for robots in human environments. Vision-based methods can infer
hidden kinematics but can yield imprecise estimates on unfamiliar objects.
Tactile approaches achieve robust control through contact feedback but require
accurate initialization. This suggests a natural synergy: vision for global
guidance, touch for local precision. Yet no framework systematically exploits
this complementarity for generalized articulated manipulation. Here we present
Vi-TacMan, which uses vision to propose grasps and coarse directions that seed
a tactile controller for precise execution. By incorporating surface normals as
geometric priors and modeling directions via von Mises-Fisher distributions,
our approach achieves significant gains over baselines (all p<0.0001).
Critically, manipulation succeeds without explicit kinematic models -- the
tactile controller refines coarse visual estimates through real-time contact
regulation. Tests on more than 50,000 simulated and diverse real-world objects
confirm robust cross-category generalization. This work establishes that coarse
visual cues suffice for reliable manipulation when coupled with tactile
feedback, offering a scalable paradigm for autonomous systems in unstructured
environments.

</details>


### [90] [A Formal gatekeeper Framework for Safe Dual Control with Active Exploration](https://arxiv.org/abs/2510.06351)
*Kaleb Ben Naveed,Devansh R. Agrawal,Dimitra Panagou*

Main category: cs.RO

TL;DR: 提出了一种结合鲁棒规划和主动探索的框架，确保探索仅在可验证改善且不牺牲安全性的情况下进行。


<details>
  <summary>Details</summary>
Motivation: 解决模型不确定性下的轨迹规划问题，避免传统鲁棒规划的过度保守行为，同时确保安全性和不确定性减少。

Method: 利用gatekeeper架构进行安全验证，生成安全且信息丰富的轨迹，减少不确定性和任务成本。

Result: 通过四旋翼飞行器的在线双控制仿真案例验证了方法的有效性。

Conclusion: 该框架在保证安全性的同时，有效减少了不确定性，适用于动态环境中的轨迹规划。

Abstract: Planning safe trajectories under model uncertainty is a fundamental
challenge. Robust planning ensures safety by considering worst-case
realizations, yet ignores uncertainty reduction and leads to overly
conservative behavior. Actively reducing uncertainty on-the-fly during a
nominal mission defines the dual control problem. Most approaches address this
by adding a weighted exploration term to the cost, tuned to trade off the
nominal objective and uncertainty reduction, but without formal consideration
of when exploration is beneficial. Moreover, safety is enforced in some methods
but not in others. We propose a framework that integrates robust planning with
active exploration under formal guarantees as follows: The key innovation and
contribution is that exploration is pursued only when it provides a verifiable
improvement without compromising safety. To achieve this, we utilize our
earlier work on gatekeeper as an architecture for safety verification, and
extend it so that it generates both safe and informative trajectories that
reduce uncertainty and the cost of the mission, or keep it within a
user-defined budget. The methodology is evaluated via simulation case studies
on the online dual control of a quadrotor under parametric uncertainty.

</details>


### [91] [Constrained Natural Language Action Planning for Resilient Embodied Systems](https://arxiv.org/abs/2510.06357)
*Grayson Byrd,Corban Rivera,Bethany Kemp,Meghan Booker,Aurora Schmidt,Celso M de Melo,Lalithkumar Seenivasan,Mathias Unberath*

Main category: cs.RO

TL;DR: 提出了一种结合大型语言模型（LLM）和符号规划的新方法，以提高机器人任务规划的可靠性、可重复性和透明度。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在任务规划中的幻觉问题和符号规划在复杂任务中的扩展性问题。

Method: 通过符号规划监督增强LLM规划，同时保留LLM的推理能力和泛化性。

Result: 在模拟和真实环境中表现优异，ALFWorld基准测试达到99%成功率，真实机器人任务成功率100%。

Conclusion: 该方法有效提升了LLM规划器的可靠性、可重复性和透明度，同时保留了其灵活性和泛化能力。

Abstract: Replicating human-level intelligence in the execution of embodied tasks
remains challenging due to the unconstrained nature of real-world environments.
Novel use of large language models (LLMs) for task planning seeks to address
the previously intractable state/action space of complex planning tasks, but
hallucinations limit their reliability, and thus, viability beyond a research
context. Additionally, the prompt engineering required to achieve adequate
system performance lacks transparency, and thus, repeatability. In contrast to
LLM planning, symbolic planning methods offer strong reliability and
repeatability guarantees, but struggle to scale to the complexity and ambiguity
of real-world tasks. We introduce a new robotic planning method that augments
LLM planners with symbolic planning oversight to improve reliability and
repeatability, and provide a transparent approach to defining hard constraints
with considerably stronger clarity than traditional prompt engineering.
Importantly, these augmentations preserve the reasoning capabilities of LLMs
and retain impressive generalization in open-world environments. We demonstrate
our approach in simulated and real-world environments. On the ALFWorld planning
benchmark, our approach outperforms current state-of-the-art methods, achieving
a near-perfect 99% success rate. Deployment of our method to a real-world
quadruped robot resulted in 100% task success compared to 50% and 30% for pure
LLM and symbolic planners across embodied pick and place tasks. Our approach
presents an effective strategy to enhance the reliability, repeatability and
transparency of LLM-based robot planners while retaining their key strengths:
flexibility and generalizability to complex real-world environments. We hope
that this work will contribute to the broad goal of building resilient embodied
intelligent systems.

</details>


### [92] [Active Next-Best-View Optimization for Risk-Averse Path Planning](https://arxiv.org/abs/2510.06481)
*Amirhossein Mollaei Khass,Guangyi Liu,Vivek Pandey,Wen Jiang,Boshu Lei,Kostas Daniilidis,Nader Motee*

Main category: cs.RO

TL;DR: 提出了一种结合风险规避与主动感知的导航框架，通过在线更新的3D高斯辐射场生成局部安全轨迹，并优化最佳视角选择以减少不确定性。


<details>
  <summary>Details</summary>
Motivation: 在不确定环境中安全导航需要同时考虑风险规避和主动感知，现有方法缺乏统一的框架。

Method: 构建尾部敏感风险地图，结合SE(3)流形上的最佳视角优化，通过黎曼梯度下降最大化信息增益。

Result: 通过计算研究验证了框架的有效性，实现了风险规避路径与最佳视角规划的耦合。

Conclusion: 该框架在复杂环境中支持高效的在线更新，提升了导航的安全性和可行性。

Abstract: Safe navigation in uncertain environments requires planning methods that
integrate risk aversion with active perception. In this work, we present a
unified framework that refines a coarse reference path by constructing
tail-sensitive risk maps from Average Value-at-Risk statistics on an
online-updated 3D Gaussian-splat Radiance Field. These maps enable the
generation of locally safe and feasible trajectories. In parallel, we formulate
Next-Best-View (NBV) selection as an optimization problem on the SE(3) pose
manifold, where Riemannian gradient descent maximizes an expected information
gain objective to reduce uncertainty most critical for imminent motion. Our
approach advances the state-of-the-art by coupling risk-averse path refinement
with NBV planning, while introducing scalable gradient decompositions that
support efficient online updates in complex environments. We demonstrate the
effectiveness of the proposed framework through extensive computational
studies.

</details>


### [93] [What You Don't Know Can Hurt You: How Well do Latent Safety Filters Understand Partially Observable Safety Constraints?](https://arxiv.org/abs/2510.06492)
*Matthew Kim,Kensuke Nakamura,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 论文探讨了潜在空间安全控制的局限性，并提出了一种基于互信息的测量方法和多模态监督训练策略，以改进安全控制。


<details>
  <summary>Details</summary>
Motivation: 现有潜在空间安全控制方法假设安全关键特征可在潜在状态中观察到，但实际中某些安全相关特征（如温度）可能无法通过RGB图像捕捉，导致短视的安全行为。

Method: 引入基于互信息的测量方法识别观测中缺失的安全相关特征，并提出多模态监督训练策略，利用额外感官输入在训练时塑造潜在状态。

Result: 在模拟和硬件实验中验证了方法的有效性，成功防止蜡锅过热。

Conclusion: 多模态监督训练策略能有效提升潜在空间安全控制的鲁棒性，无需在部署时增加额外模态。

Abstract: Safe control techniques, such as Hamilton-Jacobi reachability, provide
principled methods for synthesizing safety-preserving robot policies but
typically assume hand-designed state spaces and full observability. Recent work
has relaxed these assumptions via latent-space safe control, where state
representations and dynamics are learned jointly through world models that
reconstruct future high-dimensional observations (e.g., RGB images) from
current observations and actions. This enables safety constraints that are
difficult to specify analytically (e.g., spilling) to be framed as
classification problems in latent space, allowing controllers to operate
directly from raw observations. However, these methods assume that
safety-critical features are observable in the learned latent state. We ask:
when are latent state spaces sufficient for safe control? To study this, we
examine temperature-based failures, comparable to overheating in cooking or
manufacturing tasks, and find that RGB-only observations can produce myopic
safety behaviors, e.g., avoiding seeing failure states rather than preventing
failure itself. To predict such behaviors, we introduce a mutual
information-based measure that identifies when observations fail to capture
safety-relevant features. Finally, we propose a multimodal-supervised training
strategy that shapes the latent state with additional sensory inputs during
training, but requires no extra modalities at deployment, and validate our
approach in simulation and on hardware with a Franka Research 3 manipulator
preventing a pot of wax from overheating.

</details>


### [94] [Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots](https://arxiv.org/abs/2510.06518)
*Malakhi Hopkins,Varun Murali,Vijay Kumar,Camillo J Taylor*

Main category: cs.RO

TL;DR: 提出了一种轻量级框架，用于在低功耗无人机上实时检测和映射透明障碍物。


<details>
  <summary>Details</summary>
Motivation: 透明障碍物对传统感知系统构成挑战，现有方法不适合低SWaP机器人。

Method: 融合ToF相机和超声波传感器数据，使用轻量级2D卷积模型检测镜面反射并填充深度图。

Result: 系统在嵌入式处理器上实时运行，成功映射室内玻璃障碍物。

Conclusion: 首次在低SWaP无人机上仅用CPU实现实时透明障碍物映射。

Abstract: Autonomous aerial robots are increasingly being deployed in real-world
scenarios, where transparent obstacles present significant challenges to
reliable navigation and mapping. These materials pose a unique problem for
traditional perception systems because they lack discernible features and can
cause conventional depth sensors to fail, leading to inaccurate maps and
potential collisions. To ensure safe navigation, robots must be able to
accurately detect and map these transparent obstacles. Existing methods often
rely on large, expensive sensors or algorithms that impose high computational
burdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.
In this work, we propose a novel and computationally efficient framework for
detecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our
method fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor
with a custom, lightweight 2D convolution model. This specialized approach
accurately detects specular reflections and propagates their depth into
corresponding empty regions of the depth map, effectively rendering transparent
obstacles visible. The entire pipeline operates in real-time, utilizing only a
small fraction of a CPU core on an embedded processor. We validate our system
through a series of experiments in both controlled and real-world environments,
demonstrating the utility of our method through experiments where the robot
maps indoor environments containing glass. Our work is, to our knowledge, the
first of its kind to demonstrate a real-time, onboard transparent obstacle
mapping system on a low-SWaP quadrotor using only the CPU.

</details>


### [95] [RAISE: A self-driving laboratory for interfacial property formulation discovery](https://arxiv.org/abs/2510.06546)
*Mohammad Nazeri,Sheldon Mei,Jeffrey Watchorn,Alex Zhang,Erin Ng,Tao Wen,Abhijoy Mandal,Kevin Golovin,Alan Aspuru-Guzik,Frank Gu*

Main category: cs.RO

TL;DR: RAISE是一个闭环自主实验室，通过贝叶斯优化高效链接液体配方与接触角测量，实现多目标优化。


<details>
  <summary>Details</summary>
Motivation: 表面润湿性是生物医学设备、涂层和纺织品的关键设计参数，液体配方优化与润湿性评估的自动化需求迫切。

Method: RAISE系统结合液体配方混合、高通量液滴转移、自动图像捕获和接触角测量，集成贝叶斯优化进行迭代探索。

Result: 系统每分钟可测量1个接触角，成功优化表面活性剂配方，实现目标接触角范围、减少用量和成本。

Conclusion: RAISE展示了闭环系统中液体配方与接触角测量的自主链接能力，通过多目标优化高效满足研究需求。

Abstract: Surface wettability is a critical design parameter for biomedical devices,
coatings, and textiles. Contact angle measurements quantify liquid-surface
interactions, which depend strongly on liquid formulation. Herein, we present
the Robotic Autonomous Imaging Surface Evaluator (RAISE), a closed-loop,
self-driving laboratory that is capable of linking liquid formulation
optimization with surface wettability assessment. RAISE comprises a full
experimental orchestrator with the ability of mixing liquid ingredients to
create varying formulation cocktails, transferring droplets of prepared
formulations to a high-throughput stage, and using a pick-and-place camera tool
for automated droplet image capture. The system also includes an automated
image processing pipeline to measure contact angles. This closed loop
experiment orchestrator is integrated with a Bayesian Optimization (BO) client,
which enables iterative exploration of new formulations based on previous
contact angle measurements to meet user-defined objectives. The system operates
in a high-throughput manner and can achieve a measurement rate of approximately
1 contact angle measurement per minute. Here we demonstrate RAISE can be used
to explore surfactant wettability and how surfactant combinations create
tunable formulations that compensate for purity-related variations.
Furthermore, multi-objective BO demonstrates how precise and optimal
formulations can be reached based on application-specific goals. The
optimization is guided by a desirability score, which prioritizes formulations
that are within target contact angle ranges, minimize surfactant usage and
reduce cost. This work demonstrates the capabilities of RAISE to autonomously
link liquid formulations to contact angle measurements in a closed-loop system,
using multi-objective BO to efficiently identify optimal formulations aligned
with researcher-defined criteria.

</details>


### [96] [Safe Obstacle-Free Guidance of Space Manipulators in Debris Removal Missions via Deep Reinforcement Learning](https://arxiv.org/abs/2510.06566)
*Vincent Lam,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种基于TD3的无模型空间机械臂轨迹规划方法，结合局部控制策略和多评价网络，实现安全可靠的碎片捕获。


<details>
  <summary>Details</summary>
Motivation: 解决空间机械臂在非合作目标捕获任务中同时实现精确跟踪、避免自碰撞和意外接触的挑战。

Method: 采用TD3算法，结合基于课程的多评价网络和优先经验回放，优化轨迹规划。

Result: 在仿真实验中展示了安全、自适应的轨迹生成能力，适用于碎片清除任务。

Conclusion: 该方法有效解决了空间机械臂在复杂任务中的轨迹规划问题，具有实际应用潜力。

Abstract: The objective of this study is to develop a model-free workspace trajectory
planner for space manipulators using a Twin Delayed Deep Deterministic Policy
Gradient (TD3) agent to enable safe and reliable debris capture. A local
control strategy with singularity avoidance and manipulability enhancement is
employed to ensure stable execution. The manipulator must simultaneously track
a capture point on a non-cooperative target, avoid self-collisions, and prevent
unintended contact with the target. To address these challenges, we propose a
curriculum-based multi-critic network where one critic emphasizes accurate
tracking and the other enforces collision avoidance. A prioritized experience
replay buffer is also used to accelerate convergence and improve policy
robustness. The framework is evaluated on a simulated seven-degree-of-freedom
KUKA LBR iiwa mounted on a free-floating base in Matlab/Simulink, demonstrating
safe and adaptive trajectory generation for debris removal missions.

</details>


### [97] [Assist-As-Needed: Adaptive Multimodal Robotic Assistance for Medication Management in Dementia Care](https://arxiv.org/abs/2510.06633)
*Kruthika Gangaraju,Tanmayi Inaparthy,Jiaqi Yang,Yihao Zheng,Fengpei Yuan*

Main category: cs.RO

TL;DR: 提出了一种基于Pepper机器人的自适应多模态框架，动态调整对痴呆患者的药物管理支持，以维持其独立性。


<details>
  <summary>Details</summary>
Motivation: 现有辅助技术未能适应痴呆患者能力下降的多样化需求，导致自主性丧失和护理负担增加。

Method: 采用分层干预模型，从简单提醒到多模态指导，结合LLM驱动的交互策略和多模态感知。

Result: 初步研究表明系统在可用性、理解性和适应性反馈机制方面表现良好。

Conclusion: 该框架将职业治疗原则转化为HRI设计，并通过实证研究验证了其可行性。

Abstract: People living with dementia (PLWDs) face progressively declining abilities in
medication management-from simple forgetfulness to complete task breakdown-yet
most assistive technologies fail to adapt to these changing needs. This
one-size-fits-all approach undermines autonomy, accelerates dependence, and
increases caregiver burden. Occupational therapy principles emphasize matching
assistance levels to individual capabilities: minimal reminders for those who
merely forget, spatial guidance for those who misplace items, and comprehensive
multimodal support for those requiring step-by-step instruction. However,
existing robotic systems lack this adaptive, graduated response framework
essential for maintaining PLWD independence. We present an adaptive multimodal
robotic framework using the Pepper robot that dynamically adjusts assistance
based on real-time assessment of user needs. Our system implements a
hierarchical intervention model progressing from (1) simple verbal reminders,
to (2) verbal + gestural cues, to (3) full multimodal guidance combining
physical navigation to medication locations with step-by-step verbal and
gestural instructions. Powered by LLM-driven interaction strategies and
multimodal sensing, the system continuously evaluates task states to provide
just-enough assistance-preserving autonomy while ensuring medication adherence.
We conducted a preliminary study with healthy adults and dementia care
stakeholders in a controlled lab setting, evaluating the system's usability,
comprehensibility, and appropriateness of adaptive feedback mechanisms. This
work contributes: (1) a theoretically grounded adaptive assistance framework
translating occupational therapy principles into HRI design, (2) a multimodal
robotic implementation that preserves PLWD dignity through graduated support,
and (3) empirical insights into stakeholder perceptions of adaptive robotic
care.

</details>


### [98] [RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training](https://arxiv.org/abs/2510.06710)
*Hongzhi Zang,Mingjie Wei,Si Xu,Yongji Wu,Zhen Guo,Yuanqing Wang,Hao Lin,Liangzhi Shi,Yuqing Xie,Zhexuan Xu,Zhihao Liu,Kang Chen,Wenhao Tang,Quanlu Zhang,Weinan Zhang,Chao Yu,Yu Wang*

Main category: cs.RO

TL;DR: RLinf-VLA是一个统一高效的框架，用于扩展RL训练VLA模型，显著提升训练速度和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型多依赖监督微调（SFT），泛化能力有限，而RL虽有望优化任务性能，但缺乏统一平台进行比较。

Method: RLinf-VLA采用灵活资源分配设计，支持多种VLA架构、RL算法和模拟器，并实现混合细粒度管道分配模式。

Result: 在130个LIBERO任务和25个ManiSkill任务中，统一模型分别达到98.11%和97.66%的成功率。

Conclusion: RLinf-VLA为具身智能研究提供了加速和标准化基础，RL训练的策略在真实机器人中表现优于SFT。

Abstract: Recent progress in vision and language foundation models has significantly
advanced multimodal understanding, reasoning, and generation, inspiring a surge
of interest in extending such capabilities to embodied settings through
vision-language-action (VLA) models. Yet, most VLA models are still trained
with supervised fine-tuning (SFT), which struggles to generalize under
distribution shifts due to error accumulation. Reinforcement learning (RL)
offers a promising alternative by directly optimizing task performance through
interaction, but existing attempts remain fragmented and lack a unified
platform for fair and systematic comparison across model architectures and
algorithmic designs. To address this gap, we introduce RLinf-VLA, a unified and
efficient framework for scalable RL training of VLA models. The system adopts a
highly flexible resource allocation design that addresses the challenge of
integrating rendering, training, and inference in RL+VLA training. In
particular, for GPU-parallelized simulators, RLinf-VLA implements a novel
hybrid fine-grained pipeline allocation mode, achieving a 1.61x-1.88x speedup
in training. Through a unified interface, RLinf-VLA seamlessly supports diverse
VLA architectures (e.g., OpenVLA, OpenVLA-OFT), multiple RL algorithms (e.g.,
PPO, GRPO), and various simulators (e.g., ManiSkill, LIBERO). In simulation, a
unified model achieves 98.11\% across 130 LIBERO tasks and 97.66\% across 25
ManiSkill tasks. Beyond empirical performance, our study distills a set of best
practices for applying RL to VLA training and sheds light on emerging patterns
in this integration. Furthermore, we present preliminary deployment on a
real-world Franka robot, where RL-trained policies exhibit stronger
generalization than those trained with SFT. We envision RLinf-VLA as a
foundation to accelerate and standardize research on embodied intelligence.

</details>


### [99] [SanDRA: Safe Large-Language-Model-Based Decision Making for Automated Vehicles Using Reachability Analysis](https://arxiv.org/abs/2510.06717)
*Yuanfei Lin,Sebastian Illing,Matthias Althoff*

Main category: cs.RO

TL;DR: SanDRA框架通过可达性分析确保自动驾驶决策的安全性，结合大语言模型生成的动作与交通规则，提供可验证的安全驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动驾驶决策中可能产生幻觉且缺乏车辆动力学集成，导致决策安全性无法保障。

Method: 利用大语言模型生成并排序驾驶动作，将其转化为时序逻辑公式，结合可达性分析剔除不安全动作。

Result: 实验验证表明，SanDRA能在高密度交通条件下提供可验证的安全且合法的驾驶行为。

Conclusion: SanDRA是首个结合大语言模型与可达性分析的自动驾驶安全决策框架，代码开源以促进研究。

Abstract: Large language models have been widely applied to knowledge-driven
decision-making for automated vehicles due to their strong generalization and
reasoning capabilities. However, the safety of the resulting decisions cannot
be ensured due to possible hallucinations and the lack of integrated vehicle
dynamics. To address this issue, we propose SanDRA, the first safe
large-language-model-based decision making framework for automated vehicles
using reachability analysis. Our approach starts with a comprehensive
description of the driving scenario to prompt large language models to generate
and rank feasible driving actions. These actions are translated into temporal
logic formulas that incorporate formalized traffic rules, and are subsequently
integrated into reachability analysis to eliminate unsafe actions. We validate
our approach in both open-loop and closed-loop driving environments using
off-the-shelf and finetuned large language models, showing that it can provide
provably safe and, where possible, legally compliant driving actions, even
under high-density traffic conditions. To ensure transparency and facilitate
future research, all code and experimental setups are publicly available at
github.com/CommonRoad/SanDRA.

</details>


### [100] [UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene](https://arxiv.org/abs/2510.06754)
*Christian Maurer,Snehal Jauhri,Sophie Lueth,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: UniFField是一种统一的不确定性感知神经特征场，结合视觉、语义和几何特征，适用于新环境，并能预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，机器人需要全面理解3D场景并评估感知信息的可靠性，现有方法缺乏通用性和不确定性建模。

Method: 提出UniFField，通过体素化特征表示和RGB-D图像增量集成，同时更新不确定性估计。

Result: 实验证明不确定性估计能准确描述模型预测误差，并成功用于主动物体搜索任务。

Conclusion: UniFField为机器人提供了鲁棒决策能力，适用于新环境。

Abstract: Comprehensive visual, geometric, and semantic understanding of a 3D scene is
crucial for successful execution of robotic tasks, especially in unstructured
and complex environments. Additionally, to make robust decisions, it is
necessary for the robot to evaluate the reliability of perceived information.
While recent advances in 3D neural feature fields have enabled robots to
leverage features from pretrained foundation models for tasks such as
language-guided manipulation and navigation, existing methods suffer from two
critical limitations: (i) they are typically scene-specific, and (ii) they lack
the ability to model uncertainty in their predictions. We present UniFField, a
unified uncertainty-aware neural feature field that combines visual, semantic,
and geometric features in a single generalizable representation while also
predicting uncertainty in each modality. Our approach, which can be applied
zero shot to any new environment, incrementally integrates RGB-D images into
our voxel-based feature representation as the robot explores the scene,
simultaneously updating uncertainty estimation. We evaluate our uncertainty
estimations to accurately describe the model prediction errors in scene
reconstruction and semantic feature prediction. Furthermore, we successfully
leverage our feature predictions and their respective uncertainty for an active
object search task using a mobile manipulator robot, demonstrating the
capability for robust decision-making.

</details>


### [101] [Distributed 3D Source Seeking via SO(3) Geometric Control of Robot Swarms](https://arxiv.org/abs/2510.06836)
*Jesús Bautista,Héctor García de Marina*

Main category: cs.RO

TL;DR: 提出了一种基于SO(3)李群的几何控制框架，用于具有一阶姿态动力学和恒定平移速度的机器人3D源搜索。


<details>
  <summary>Details</summary>
Motivation: 避免欧拉角奇异性和四元数歧义，提供唯一的内在方向表示。

Method: 设计比例前馈控制器，确保每个智能体与估计的3D标量场源上升方向指数对齐。

Result: 控制器适应有界未知变化并保持良好群体形态，数值模拟验证了有效性。

Conclusion: 方法有效且开源代码可复现。

Abstract: This paper presents a geometric control framework on the Lie group SO(3) for
3D source-seeking by robots with first-order attitude dynamics and constant
translational speed. By working directly on SO(3), the approach avoids
Euler-angle singularities and quaternion ambiguities, providing a unique,
intrinsic representation of orientation. We design a proportional feed-forward
controller that ensures exponential alignment of each agent to an estimated
ascending direction toward a 3D scalar field source. The controller adapts to
bounded unknown variations and preserves well-posed swarm formations. Numerical
simulations demonstrate the effectiveness of the method, with all code provided
open source for reproducibility.

</details>


### [102] [Tailoring materials into kirigami robots](https://arxiv.org/abs/2510.07027)
*Saravana Prashanth Murali Babu,Aida Parvaresh,Ahmad Rafsanjani*

Main category: cs.RO

TL;DR: Kirigami技术在机器人领域的应用潜力巨大，通过优化切割图案实现多功能、轻量化和适应性强的解决方案。


<details>
  <summary>Details</summary>
Motivation: 探索Kirigami结构在机器人中的多功能应用，如执行器、传感器、电池等，以提升机器人的适应性和灵活性。

Method: 通过优化Kirigami切割图案，设计执行器、传感器、电池等组件，实现复杂运动和功能。

Result: Kirigami机器人展示了在抓取、运动、可穿戴设备等领域的多样化应用潜力。

Conclusion: 尽管Kirigami机器人前景广阔，但仍需解决切割图案设计和制造工艺的挑战。

Abstract: Kirigami, the traditional paper-cutting craft, holds immense potential for
revolutionizing robotics by providing multifunctional, lightweight, and
adaptable solutions. Kirigami structures, characterized by their
bending-dominated deformation, offer resilience to tensile forces and
facilitate shape morphing under small actuation forces. Kirigami components
such as actuators, sensors, batteries, controllers, and body structures can be
tailored to specific robotic applications by optimizing cut patterns. Actuators
based on kirigami principles exhibit complex motions programmable through
various energy sources, while kirigami sensors bridge the gap between
electrical conductivity and compliance. Kirigami-integrated batteries enable
energy storage directly within robot structures, enhancing flexibility and
compactness. Kirigami-controlled mechanisms mimic mechanical computations,
enabling advanced functionalities such as shape morphing and memory functions.
Applications of kirigami-enabled robots include grasping, locomotion, and
wearables, showcasing their adaptability to diverse environments and tasks.
Despite promising opportunities, challenges remain in the design of cut
patterns for a given function and streamlining fabrication techniques.

</details>


### [103] [Temporal-Prior-Guided View Planning for Periodic 3D Plant Reconstruction](https://arxiv.org/abs/2510.07028)
*Sicong Pan,Xuying Huang,Maren Bennewitz*

Main category: cs.RO

TL;DR: 提出了一种基于时间先验的植物周期性3D重建方法，通过非刚性对齐和优化视图规划减少资源浪费。


<details>
  <summary>Details</summary>
Motivation: 周期性3D重建通常从头开始，浪费资源且忽略历史信息，需要更高效的方法。

Method: 利用先前重建模型非刚性对齐新观测，膨胀后通过集合覆盖优化计算最小视图集，并集成完整流程。

Result: 在玉米和番茄实验中，系统在保持或提高覆盖的同时减少了视图数量和移动成本。

Conclusion: 该方法通过利用时间先验和优化视图规划，显著提升了周期性植物重建的效率。

Abstract: Periodic 3D reconstruction is essential for crop monitoring, but costly when
each cycle restarts from scratch, wasting resources and ignoring information
from previous captures. We propose temporal-prior-guided view planning for
periodic plant reconstruction, in which a previously reconstructed model of the
same plant is non-rigidly aligned to a new partial observation to form an
approximation of the current geometry. To accommodate plant growth, we inflate
this approximation and solve a set covering optimization problem to compute a
minimal set of views. We integrated this method into a complete pipeline that
acquires one additional next-best view before registration for robustness and
then plans a globally shortest path to connect the planned set of views and
outputs the best view sequence. Experiments on maize and tomato under
hemisphere and sphere view spaces show that our system maintains or improves
surface coverage while requiring fewer views and comparable movement cost
compared to state-of-the-art baselines.

</details>


### [104] [Diffusing Trajectory Optimization Problems for Recovery During Multi-Finger Manipulation](https://arxiv.org/abs/2510.07030)
*Abhinav Kumar,Fan Yang,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: 利用扩散模型构建框架，自主识别何时需要恢复并优化接触丰富的轨迹以提高任务性能。


<details>
  <summary>Details</summary>
Motivation: 多指手在执行精细操作任务时易受环境扰动或执行错误影响，需恢复行为以恢复正常任务执行。

Method: 使用扩散模型检测任务执行不适宜状态，并通过扩散采样和轨迹优化规划恢复轨迹。

Result: 在硬件螺丝刀任务中，恢复方法提高任务性能96%，且唯一能避免灾难性失败。

Conclusion: 扩散模型框架有效提升多指手任务恢复能力，显著优于其他方法。

Abstract: Multi-fingered hands are emerging as powerful platforms for performing fine
manipulation tasks, including tool use. However, environmental perturbations or
execution errors can impede task performance, motivating the use of recovery
behaviors that enable normal task execution to resume. In this work, we take
advantage of recent advances in diffusion models to construct a framework that
autonomously identifies when recovery is necessary and optimizes contact-rich
trajectories to recover. We use a diffusion model trained on the task to
estimate when states are not conducive to task execution, framed as an
out-of-distribution detection problem. We then use diffusion sampling to
project these states in-distribution and use trajectory optimization to plan
contact-rich recovery trajectories. We also propose a novel diffusion-based
approach that distills this process to efficiently diffuse the full
parameterization, including constraints, goal state, and initialization, of the
recovery trajectory optimization problem, saving time during online execution.
We compare our method to a reinforcement learning baseline and other methods
that do not explicitly plan contact interactions, including on a hardware
screwdriver-turning task where we show that recovering using our method
improves task performance by 96% and that ours is the only method evaluated
that can attempt recovery without causing catastrophic task failure. Videos can
be found at https://dtourrecovery.github.io/.

</details>


### [105] [Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models](https://arxiv.org/abs/2510.07067)
*Daria Pugacheva,Andrey Moskalenko,Denis Shepelev,Andrey Kuznetsov,Vlad Shakhuro,Elena Tutubalina*

Main category: cs.RO

TL;DR: 本文研究了视觉语言动作（VLA）模型在语言扰动下的鲁棒性，并提出了一种基于LLM的过滤框架来提升性能。


<details>
  <summary>Details</summary>
Motivation: VLA模型在真实场景中对自然语言变异的鲁棒性尚未充分研究。

Method: 评估模型在两种指令噪声下的表现：人类生成的改写和无关上下文的添加，并提出LLM过滤框架。

Result: 模型在语义和词汇相似的无关上下文下性能下降约50%，而人类改写导致下降近20%。过滤框架可恢复98.5%的性能。

Conclusion: VLA模型对语言扰动敏感，但通过LLM过滤可显著提升鲁棒性。

Abstract: Vision Language Action (VLA) models are widely used in Embodied AI, enabling
robots to interpret and execute language instructions. However, their
robustness to natural language variability in real-world scenarios has not been
thoroughly investigated. In this work, we present a novel systematic study of
the robustness of state-of-the-art VLA models under linguistic perturbations.
Specifically, we evaluate model performance under two types of instruction
noise: (1) human-generated paraphrasing and (2) the addition of irrelevant
context. We further categorize irrelevant contexts into two groups according to
their length and their semantic and lexical proximity to robot commands. In
this study, we observe consistent performance degradation as context size
expands. We also demonstrate that the model can exhibit relative robustness to
random context, with a performance drop within 10%, while semantically and
lexically similar context of the same length can trigger a quality decline of
around 50%. Human paraphrases of instructions lead to a drop of nearly 20%. To
mitigate this, we propose an LLM-based filtering framework that extracts core
commands from noisy inputs. Incorporating our filtering step allows models to
recover up to 98.5% of their original performance under noisy conditions.

</details>


### [106] [Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications](https://arxiv.org/abs/2510.07077)
*Kento Kawaharazuka,Jihoon Oh,Jun Yamada,Ingmar Posner,Yuke Zhu*

Main category: cs.RO

TL;DR: 本文综述了视觉-语言-动作（VLA）模型在机器人领域的应用，涵盖其架构、学习范式及实际部署的软硬件组件。


<details>
  <summary>Details</summary>
Motivation: 通过统一视觉、语言和动作数据，VLA模型旨在实现跨任务、对象和环境的泛化能力，以支持机器人在现实世界中的灵活部署。

Method: 系统回顾了VLA模型的策略与架构演变、构建模块、模态处理技术及学习范式，并探讨了机器人平台、数据收集、数据集和评估基准。

Result: 提供了全面的VLA系统综述，包括实际部署的指导，并整理了相关参考资料。

Conclusion: 本文为机器人社区应用VLA模型提供了实用指南，并总结了相关资源。

Abstract: Amid growing efforts to leverage advances in large language models (LLMs) and
vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models
have recently gained significant attention. By unifying vision, language, and
action data at scale, which have traditionally been studied separately, VLA
models aim to learn policies that generalise across diverse tasks, objects,
embodiments, and environments. This generalisation capability is expected to
enable robots to solve novel downstream tasks with minimal or no additional
task-specific data, facilitating more flexible and scalable real-world
deployment. Unlike previous surveys that focus narrowly on action
representations or high-level model architectures, this work offers a
comprehensive, full-stack review, integrating both software and hardware
components of VLA systems. In particular, this paper provides a systematic
review of VLAs, covering their strategy and architectural transition,
architectures and building blocks, modality-specific processing techniques, and
learning paradigms. In addition, to support the deployment of VLAs in
real-world robotic applications, we also review commonly used robot platforms,
data collection strategies, publicly available datasets, data augmentation
methods, and evaluation benchmarks. Throughout this comprehensive survey, this
paper aims to offer practical guidance for the robotics community in applying
VLAs to real-world robotic systems. All references categorized by training
approach, evaluation method, modality, and dataset are available in the table
on our project website: https://vla-survey.github.io .

</details>


### [107] [Sampling Strategies for Robust Universal Quadrupedal Locomotion Policies](https://arxiv.org/abs/2510.07094)
*David Rytz,Kim Tien Ly,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 研究了四足机器人配置变化的采样策略，以生成鲁棒的通用运动策略，比较了三种关节增益采样方法，并验证了随机化对仿真到现实迁移的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过采样物理参数和关节增益来训练单一强化学习策略，使其适用于多种配置，以提高鲁棒性和通用性。

Method: 比较了三种关节增益采样策略：线性/多项式映射、性能自适应滤波和均匀随机采样，并结合名义先验和参考模型优化策略。

Result: 在仿真和硬件测试中，证明了关节控制器增益的显著随机化对缩小仿真到现实差距的必要性。

Conclusion: 通过增益随机化和配置优化，成功实现了单一策略在多种四足机器人上的鲁棒部署。

Abstract: This work focuses on sampling strategies of configuration variations for
generating robust universal locomotion policies for quadrupedal robots. We
investigate the effects of sampling physical robot parameters and joint
proportional-derivative gains to enable training a single reinforcement
learning policy that generalizes to multiple parameter configurations. Three
fundamental joint gain sampling strategies are compared: parameter sampling
with (1) linear and polynomial function mappings of mass-to-gains, (2)
performance-based adaptive filtering, and (3) uniform random sampling. We
improve the robustness of the policy by biasing the configurations using
nominal priors and reference models. All training was conducted on RaiSim,
tested in simulation on a range of diverse quadrupeds, and zero-shot deployed
onto hardware using the ANYmal quadruped robot. Compared to multiple baseline
implementations, our results demonstrate the need for significant joint
controller gains randomization for robust closing of the sim-to-real gap.

</details>


### [108] [A Digital Twin Framework for Metamorphic Testing of Autonomous Driving Systems Using Generative Model](https://arxiv.org/abs/2510.07133)
*Tony Zhang,Burak Kantarci,Umair Siddique*

Main category: cs.RO

TL;DR: 提出了一种基于数字孪生和AI图像生成的自动驾驶测试框架，显著提高了测试覆盖率和效果。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统在复杂环境中测试的局限性，如预言机问题和场景覆盖不足。

Method: 结合数字孪生技术和AI图像生成模型（如Stable Diffusion），生成多样化的驾驶场景，并定义三种变形关系进行测试。

Result: 在Udacity模拟器中验证，取得了最高的真阳性率（0.719）、F1分数（0.689）和精确度（0.662）。

Conclusion: 数字孪生与AI场景生成的结合为自动驾驶安全测试提供了可扩展、自动化且高保真的解决方案。

Abstract: Ensuring the safety of self-driving cars remains a major challenge due to the
complexity and unpredictability of real-world driving environments. Traditional
testing methods face significant limitations, such as the oracle problem, which
makes it difficult to determine whether a system's behavior is correct, and the
inability to cover the full range of scenarios an autonomous vehicle may
encounter. In this paper, we introduce a digital twin-driven metamorphic
testing framework that addresses these challenges by creating a virtual replica
of the self-driving system and its operating environment. By combining digital
twin technology with AI-based image generative models such as Stable Diffusion,
our approach enables the systematic generation of realistic and diverse driving
scenes. This includes variations in weather, road topology, and environmental
features, all while maintaining the core semantics of the original scenario.
The digital twin provides a synchronized simulation environment where changes
can be tested in a controlled and repeatable manner. Within this environment,
we define three metamorphic relations inspired by real-world traffic rules and
vehicle behavior. We validate our framework in the Udacity self-driving
simulator and demonstrate that it significantly enhances test coverage and
effectiveness. Our method achieves the highest true positive rate (0.719), F1
score (0.689), and precision (0.662) compared to baseline approaches. This
paper highlights the value of integrating digital twins with AI-powered
scenario generation to create a scalable, automated, and high-fidelity testing
solution for autonomous vehicle safety.

</details>


### [109] [TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking](https://arxiv.org/abs/2510.07134)
*Jiahang Liu,Yunpeng Qi,Jiazhao Zhang,Minghan Li,Shaoan Wang,Kui Wu,Hanjing Ye,Hong Zhang,Zhibo Chen,Fangwei Zhong,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: TrackVLA++是一种新型视觉-语言-动作模型，通过空间推理机制和目标识别记忆模块提升视觉跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在严重遮挡或相似干扰物存在时缺乏空间推理和有效时间记忆，导致跟踪失败。

Method: 引入Polar-CoT空间推理机制和TIM记忆模块，增强目标记忆和空间一致性。

Result: 在公开基准测试中表现优异，EVT-Bench DT上分别领先5.1和12分，并展示强零样本泛化能力。

Conclusion: TrackVLA++在动态和遮挡场景中实现鲁棒跟踪，性能显著提升。

Abstract: Embodied Visual Tracking (EVT) is a fundamental ability that underpins
practical applications, such as companion robots, guidance robots and service
assistants, where continuously following moving targets is essential. Recent
advances have enabled language-guided tracking in complex and unstructured
scenes. However, existing approaches lack explicit spatial reasoning and
effective temporal memory, causing failures under severe occlusions or in the
presence of similar-looking distractors. To address these challenges, we
present TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances
embodied visual tracking with two key modules, a spatial reasoning mechanism
and a Target Identification Memory (TIM). The reasoning module introduces a
Chain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative
position and encodes it as a compact polar-coordinate token for action
prediction. Guided by these spatial priors, the TIM employs a gated update
strategy to preserve long-horizon target memory, ensuring spatiotemporal
consistency and mitigating target loss during extended occlusions. Extensive
experiments show that TrackVLA++ achieves state-of-the-art performance on
public benchmarks across both egocentric and multi-camera settings. On the
challenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading
approach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong
zero-shot generalization, enabling robust real-world tracking in dynamic and
occluded scenarios.

</details>


### [110] [DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction](https://arxiv.org/abs/2510.07152)
*Jingkai Sun,Gang Han,Pihai Sun,Wen Zhao,Jiahang Cao,Jiaxu Wang,Yijie Guo,Qiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种结合地形感知策略、多模态交叉注意力Transformer和真实深度图像合成方法的新框架，解决了现有方法的局限性，并在人形机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有地形感知人形机器人运动方法存在训练效率低、依赖多传感器和定位系统等问题，需要更高效的解决方案。

Method: 结合地形感知策略、多模态交叉注意力Transformer和真实深度图像合成方法，提升训练效率和鲁棒性。

Result: 实现了30%以上的地形重建误差减少，并在人形机器人上展示了敏捷和适应性运动。

Conclusion: 该框架在有限数据和硬件资源下高效训练策略，同时保留了关键地形特征，具有广泛适用性。

Abstract: Recent advancements in legged robot perceptive locomotion have shown
promising progress. However, terrain-aware humanoid locomotion remains largely
constrained to two paradigms: depth image-based end-to-end learning and
elevation map-based methods. The former suffers from limited training
efficiency and a significant sim-to-real gap in depth perception, while the
latter depends heavily on multiple vision sensors and localization systems,
resulting in latency and reduced robustness. To overcome these challenges, we
propose a novel framework that tightly integrates three key components: (1)
Terrain-Aware Locomotion Policy with a Blind Backbone, which leverages
pre-trained elevation map-based perception to guide reinforcement learning with
minimal visual input; (2) Multi-Modality Cross-Attention Transformer, which
reconstructs structured terrain representations from noisy depth images; (3)
Realistic Depth Images Synthetic Method, which employs self-occlusion-aware ray
casting and noise-aware modeling to synthesize realistic depth observations,
achieving over 30\% reduction in terrain reconstruction error. This combination
enables efficient policy training with limited data and hardware resources,
while preserving critical terrain features essential for generalization. We
validate our framework on a full-sized humanoid robot, demonstrating agile and
adaptive locomotion across diverse and challenging terrains.

</details>


### [111] [A Narwhal-Inspired Sensing-to-Control Framework for Small Fixed-Wing Aircraft](https://arxiv.org/abs/2510.07160)
*Fengze Xie,Xiaozhou Fan,Jacob Schuster,Yisong Yue,Morteza Gharib*

Main category: cs.RO

TL;DR: 提出了一种结合生物启发硬件、物理信息动力学学习和凸控制分配的端到端感知控制管道，用于提升固定翼无人机的低速敏捷性。


<details>
  <summary>Details</summary>
Motivation: 固定翼无人机虽高效但低速敏捷性不足，主要因动力学高度耦合。

Method: 采用上游多孔探针和稀疏翼压传感器测量气流，数据驱动校准和对称正则化动力学模型，结合凸控制分配实现平滑控制。

Result: 实验显示翼压传感器减少力估计误差25-30%，模型在分布偏移下性能下降更少（12% vs 44%），力跟踪更平滑，法向力RMSE降低27%。

Conclusion: 该方法显著提升了固定翼无人机的低速控制性能，验证了硬件与算法结合的有效性。

Abstract: Fixed-wing unmanned aerial vehicles (UAVs) offer endurance and efficiency but
lack low-speed agility due to highly coupled dynamics. We present an end-to-end
sensing-to-control pipeline that combines bio-inspired hardware,
physics-informed dynamics learning, and convex control allocation. Measuring
airflow on a small airframe is difficult because near-body aerodynamics,
propeller slipstream, control-surface actuation, and ambient gusts distort
pressure signals. Inspired by the narwhal's protruding tusk, we mount in-house
multi-hole probes far upstream and complement them with sparse, carefully
placed wing pressure sensors for local flow measurement. A data-driven
calibration maps probe pressures to airspeed and flow angles. We then learn a
control-affine dynamics model using the estimated airspeed/angles and sparse
sensors. A soft left/right symmetry regularizer improves identifiability under
partial observability and limits confounding between wing pressures and
flaperon inputs. Desired wrenches (forces and moments) are realized by a
regularized least-squares allocator that yields smooth, trimmed actuation.
Wind-tunnel studies across a wide operating range show that adding wing
pressures reduces force-estimation error by 25-30%, the proposed model degrades
less under distribution shift (about 12% versus 44% for an unstructured
baseline), and force tracking improves with smoother inputs, including a 27%
reduction in normal-force RMSE versus a plain affine model and 34% versus an
unstructured baseline.

</details>


### [112] [TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2510.07181)
*Yi Han,Cheng Chi,Enshen Zhou,Shanyu Rong,Jingkun An,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: TIGeR框架通过外部工具将视觉语言模型（VLMs）从感知估计器转变为几何计算机，实现厘米级精度的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在空间推理中缺乏计算精度，无法满足机器人操作的需求。

Method: TIGeR通过生成和执行几何计算代码，结合外部工具，采用两阶段训练（SFT和RFT）提升性能。

Result: TIGeR在几何推理基准测试中达到SOTA，并在实际机器人任务中实现厘米级精度。

Conclusion: TIGeR通过工具集成解决了VLMs的几何计算限制，为机器人操作提供了高精度解决方案。

Abstract: Vision-Language Models (VLMs) have shown remarkable capabilities in spatial
reasoning, yet they remain fundamentally limited to qualitative precision and
lack the computational precision required for real-world robotics. Current
approaches fail to leverage metric cues from depth sensors and camera
calibration, instead reducing geometric problems to pattern recognition tasks
that cannot deliver the centimeter-level accuracy essential for robotic
manipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel
framework that transforms VLMs from perceptual estimators to geometric
computers by enabling them to generate and execute precise geometric
computations through external tools. Rather than attempting to internalize
complex geometric operations within neural networks, TIGeR empowers models to
recognize geometric reasoning requirements, synthesize appropriate
computational code, and invoke specialized libraries for exact calculations. To
support this paradigm, we introduce TIGeR-300K, a comprehensive
tool-invocation-oriented dataset covering point transformations, pose
estimation, trajectory generation, and spatial compatibility verification,
complete with tool invocation sequences and intermediate computations. Through
a two-stage training pipeline combining supervised fine-tuning (SFT) and
reinforcement fine-tuning (RFT) with our proposed hierarchical reward design,
TIGeR achieves SOTA performance on geometric reasoning benchmarks while
demonstrating centimeter-level precision in real-world robotic manipulation
tasks.

</details>


### [113] [COMPAct: Computational Optimization and Automated Modular design of Planetary Actuators](https://arxiv.org/abs/2510.07197)
*Aman Singh,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: COMPAct框架优化行星齿轮箱参数并自动化CAD设计，支持四种齿轮箱类型，实现轻量化、高效和直接3D打印。实验验证了SSPG和CPG的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究对齿轮箱参数优化和自动化CAD设计关注不足，需系统化解决方案。

Method: 通过计算优化确定四种齿轮箱类型的最优参数，并自动化生成CAD模型。

Result: SSPG效率60-80%，CPG效率60%；SSPG的刚度和反向间隙优于CPG。

Conclusion: COMPAct为行星齿轮箱设计提供了高效、自动化的解决方案，实验验证了其可行性。

Abstract: The optimal design of robotic actuators is a critical area of research, yet
limited attention has been given to optimizing gearbox parameters and
automating actuator CAD. This paper introduces COMPAct: Computational
Optimization and Automated Modular Design of Planetary Actuators, a framework
that systematically identifies optimal gearbox parameters for a given motor
across four gearbox types, single-stage planetary gearbox (SSPG), compound
planetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage
planetary gearbox (DSPG). The framework minimizes mass and actuator width while
maximizing efficiency, and further automates actuator CAD generation to enable
direct 3D printing without manual redesign. Using this framework, optimal
gearbox designs are explored over a wide range of gear ratios, providing
insights into the suitability of different gearbox types across various gear
ratio ranges. In addition, the framework is used to generate CAD models of all
four gearbox types with varying gear ratios and motors. Two actuator types are
fabricated and experimentally evaluated through power efficiency, no-load
backlash, and transmission stiffness tests. Experimental results indicate that
the SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load
backlash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the
CPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of
201.6 Nm/rad. Code available at:
https://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:
https://youtu.be/99zOKgxsDho

</details>


### [114] [HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe Autonomous Driving](https://arxiv.org/abs/2510.07210)
*Donald Pfaffmann,Matthias Klusch,Marcel Steinmetz*

Main category: cs.RO

TL;DR: HyPlan是一种混合学习方法，用于解决自动驾驶汽车在部分可观测交通环境中的无碰撞导航问题。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶汽车在复杂交通环境中的安全导航问题，尤其是在部分可观测条件下。

Method: 结合多智能体行为预测、深度强化学习（PPO）和近似在线POMDP规划，采用启发式置信度垂直剪枝以减少执行时间。

Result: 在CARLA-CTS2基准测试中，HyPlan比相关基线更安全，且比替代在线POMDP规划器更快。

Conclusion: HyPlan在安全性和执行效率上均优于现有方法，适用于复杂交通场景。

Abstract: We present a novel hybrid learning-assisted planning method, named HyPlan,
for solving the collision-free navigation problem for self-driving cars in
partially observable traffic environments. HyPlan combines methods for
multi-agent behavior prediction, deep reinforcement learning with proximal
policy optimization and approximated online POMDP planning with heuristic
confidence-based vertical pruning to reduce its execution time without
compromising safety of driving. Our experimental performance analysis on the
CARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed
that HyPlan may navigate safer than selected relevant baselines and perform
significantly faster than considered alternative online POMDP planners.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [115] [RareGraph-Synth: Knowledge-Guided Diffusion Models for Generating Privacy-Preserving Synthetic Patient Trajectories in Ultra-Rare Diseases](https://arxiv.org/abs/2510.06267)
*Khartik Uppalapati,Shakeel Abdulkareem,Bora Yimenicioglu*

Main category: cs.LG

TL;DR: RareGraph-Synth是一个基于知识图谱的连续时间扩散框架，用于生成真实且保护隐私的罕见病电子健康记录（EHR）数据。


<details>
  <summary>Details</summary>
Motivation: 解决罕见病研究中数据稀缺和隐私保护的问题。

Method: 整合多个公共资源构建异质知识图谱，通过元路径评分调节扩散模型的噪声计划，生成无隐私信息的EHR轨迹。

Result: 在模拟罕见病队列中，RareGraph-Synth显著降低了分类最大均值差异，并表现出更强的抗重识别能力。

Conclusion: 将生物医学知识图谱直接融入扩散噪声计划可以同时提高数据保真度和隐私保护，促进罕见病研究的安全数据共享。

Abstract: We propose RareGraph-Synth, a knowledge-guided, continuous-time diffusion
framework that generates realistic yet privacy-preserving synthetic
electronic-health-record (EHR) trajectories for ultra-rare diseases.
RareGraph-Synth unifies five public resources: Orphanet/Orphadata, the Human
Phenotype Ontology (HPO), the GARD rare-disease KG, PrimeKG, and the FDA
Adverse Event Reporting System (FAERS) into a heterogeneous knowledge graph
comprising approximately 8 M typed edges. Meta-path scores extracted from this
8-million-edge KG modulate the per-token noise schedule in the forward
stochastic differential equation, steering generation toward biologically
plausible lab-medication-adverse-event co-occurrences while retaining
score-based diffusion model stability. The reverse denoiser then produces
timestamped sequences of lab-code, medication-code, and adverse-event-flag
triples that contain no protected health information. On simulated
ultra-rare-disease cohorts, RareGraph-Synth lowers categorical Maximum Mean
Discrepancy by 40 percent relative to an unguided diffusion baseline and by
greater than 60 percent versus GAN counterparts, without sacrificing downstream
predictive utility. A black-box membership-inference evaluation using the
DOMIAS attacker yields AUROC approximately 0.53, well below the 0.55
safe-release threshold and substantially better than the approximately 0.61
plus or minus 0.03 observed for non-KG baselines, demonstrating strong
resistance to re-identification. These results suggest that integrating
biomedical knowledge graphs directly into diffusion noise schedules can
simultaneously enhance fidelity and privacy, enabling safer data sharing for
rare-disease research.

</details>


### [116] [MCCE: A Framework for Multi-LLM Collaborative Co-Evolution](https://arxiv.org/abs/2510.06270)
*Nian Ran,Zhongzheng Li,Yue Wang,Qingsong Ran,Xiaoyuan Zhang,Shikun Feng,Richard Allmendinger,Xiaoguang Zhao*

Main category: cs.LG

TL;DR: MCCE框架结合封闭和开放LLM，通过协同进化提升多目标优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统进化算法易陷局部最优和LLM无法持续学习的问题。

Method: 结合冻结的封闭LLM与可训练轻量模型，通过强化学习逐步优化。

Result: 在药物设计基准上实现最优Pareto前沿，超越基线方法。

Conclusion: MCCE展示了混合LLM系统持续进化的新范式。

Abstract: Multi-objective discrete optimization problems, such as molecular design,
pose significant challenges due to their vast and unstructured combinatorial
spaces. Traditional evolutionary algorithms often get trapped in local optima,
while expert knowledge can provide crucial guidance for accelerating
convergence. Large language models (LLMs) offer powerful priors and reasoning
ability, making them natural optimizers when expert knowledge matters. However,
closed-source LLMs, though strong in exploration, cannot update their
parameters and thus cannot internalize experience. Conversely, smaller open
models can be continually fine-tuned but lack broad knowledge and reasoning
strength. We introduce Multi-LLM Collaborative Co-evolution (MCCE), a hybrid
framework that unites a frozen closed-source LLM with a lightweight trainable
model. The system maintains a trajectory memory of past search processes; the
small model is progressively refined via reinforcement learning, with the two
models jointly supporting and complementing each other in global exploration.
Unlike model distillation, this process enhances the capabilities of both
models through mutual inspiration. Experiments on multi-objective drug design
benchmarks show that MCCE achieves state-of-the-art Pareto front quality and
consistently outperforms baselines. These results highlight a new paradigm for
enabling continual evolution in hybrid LLM systems, combining knowledge-driven
exploration with experience-driven learning.

</details>


### [117] [RVFL-X: A Novel Randomized Network Based on Complex Transformed Real-Valued Tabular Datasets](https://arxiv.org/abs/2510.06278)
*M. Sajid,Mushir Akhtar,A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出RVFL-X，一种复数随机向量功能链接网络的扩展，通过两种方法将实数数据集转换为复数表示，显著优于原始RVFL和SOTA RNN变体。


<details>
  <summary>Details</summary>
Motivation: 复数神经网络具有更强的表示能力，但在随机神经网络中应用受限，因缺乏有效的实数到复数转换方法。

Method: 提出自然变换和自编码器驱动的复数表示生成方法，并基于此设计RVFL-X网络。

Result: 在80个UCI数据集上，RVFL-X表现优于原始RVFL和SOTA RNN变体。

Conclusion: RVFL-X通过复数组件处理复数表示并输出实数结果，展现了跨领域的鲁棒性和有效性。

Abstract: Recent advancements in neural networks, supported by foundational theoretical
insights, emphasize the superior representational power of complex numbers.
However, their adoption in randomized neural networks (RNNs) has been limited
due to the lack of effective methods for transforming real-valued tabular
datasets into complex-valued representations. To address this limitation, we
propose two methods for generating complex-valued representations from
real-valued datasets: a natural transformation and an autoencoder-driven
method. Building on these mechanisms, we propose RVFL-X, a complex-valued
extension of the random vector functional link (RVFL) network. RVFL-X
integrates complex transformations into real-valued datasets while maintaining
the simplicity and efficiency of the original RVFL architecture. By leveraging
complex components such as input, weights, and activation functions, RVFL-X
processes complex representations and produces real-valued outputs.
Comprehensive evaluations on 80 real-valued UCI datasets demonstrate that
RVFL-X consistently outperforms both the original RVFL and state-of-the-art
(SOTA) RNN variants, showcasing its robustness and effectiveness across diverse
application domains.

</details>


### [118] [On knot detection via picture recognition](https://arxiv.org/abs/2510.06284)
*Anne Dranowski,Yura Kabkov,Daniel Tubbenhauer*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和传统算法的方法，通过图像识别和量子不变量计算来自动识别结的类型。


<details>
  <summary>Details</summary>
Motivation: 目标是实现通过拍照自动识别结的类型，结合现代机器学习方法和传统算法来解决这一问题。

Method: 使用卷积神经网络和变换器进行图像识别，结合传统算法计算Jones多项式等量子不变量。

Result: 轻量级CNN和变换器架构能够从图像中预测交叉数，提取有意义的结构信息。

Conclusion: 未来计划将感知模块与符号重建结合，通过两阶段方法实现鲁棒的结分类。

Abstract: Our goal is to one day take a photo of a knot and have a phone automatically
recognize it. In this expository work, we explain a strategy to approximate
this goal, using a mixture of modern machine learning methods (in particular
convolutional neural networks and transformers for image recognition) and
traditional algorithms (to compute quantum invariants like the Jones
polynomial). We present simple baselines that predict crossing number directly
from images, showing that even lightweight CNN and transformer architectures
can recover meaningful structural information. The longer-term aim is to
combine these perception modules with symbolic reconstruction into planar
diagram (PD) codes, enabling downstream invariant computation for robust knot
classification. This two-stage approach highlights the complementarity between
machine learning, which handles noisy visual data, and invariants, which
enforce rigorous topological distinctions.

</details>


### [119] [Traj-Transformer: Diffusion Models with Transformer for GPS Trajectory Generation](https://arxiv.org/abs/2510.06291)
*Zhiyang Zhang,Ningcong Chen,Xin Zhang,Yanhua Li,Shen Su,Hui Lu,Jun Luo*

Main category: cs.LG

TL;DR: 提出Trajectory Transformer，利用Transformer架构提升轨迹生成质量，解决现有方法因卷积架构导致的偏差和细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于卷积架构的扩散模型在轨迹生成中存在偏差和细节丢失问题，需更强大的模型提升生成质量。

Method: 采用Transformer架构进行条件信息嵌入和噪声预测，探索两种GPS坐标嵌入策略（位置嵌入和经纬度嵌入）。

Result: 在两个真实数据集上的实验表明，Trajectory Transformer显著提升生成质量并缓解偏差问题。

Conclusion: Trajectory Transformer通过Transformer架构有效解决了轨迹生成中的偏差和细节丢失问题，表现优于现有方法。

Abstract: The widespread use of GPS devices has driven advances in spatiotemporal data
mining, enabling machine learning models to simulate human decision making and
generate realistic trajectories, addressing both data collection costs and
privacy concerns. Recent studies have shown the promise of diffusion models for
high-quality trajectory generation. However, most existing methods rely on
convolution based architectures (e.g. UNet) to predict noise during the
diffusion process, which often results in notable deviations and the loss of
fine-grained street-level details due to limited model capacity. In this paper,
we propose Trajectory Transformer, a novel model that employs a transformer
backbone for both conditional information embedding and noise prediction. We
explore two GPS coordinate embedding strategies, location embedding and
longitude-latitude embedding, and analyze model performance at different
scales. Experiments on two real-world datasets demonstrate that Trajectory
Transformer significantly enhances generation quality and effectively
alleviates the deviation issues observed in prior approaches.

</details>


### [120] [BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression](https://arxiv.org/abs/2510.06293)
*Cristian Meo,Varun Sarathchandran,Avijit Majhi,Shao Hung,Carlo Saccardi,Ruben Imhoff,Roberto Deidda,Remko Uijlenhoet,Justin Dauwels*

Main category: cs.LG

TL;DR: BlockGPT是一种用于降水临近预报的生成自回归变换器，通过批处理标记化方法预测二维降水场，在准确性和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有降水临近预报方法（如基于标记的自回归模型和扩散模型）的归纳偏差问题和计算效率低下的局限性。

Method: BlockGPT采用空间-时间分解的自注意力机制，结合批处理标记化方法，预测二维降水场。

Result: 在KNMI和SEVIR数据集上，BlockGPT在准确性、事件定位和推理速度（比基线快31倍）上表现优异。

Conclusion: BlockGPT为降水临近预报提供了一种高效且准确的解决方案，优于现有方法。

Abstract: Predicting precipitation maps is a highly complex spatiotemporal modeling
task, critical for mitigating the impacts of extreme weather events. Short-term
precipitation forecasting, or nowcasting, requires models that are not only
accurate but also computationally efficient for real-time applications. Current
methods, such as token-based autoregressive models, often suffer from flawed
inductive biases and slow inference, while diffusion models can be
computationally intensive. To address these limitations, we introduce BlockGPT,
a generative autoregressive transformer using batched tokenization (Block)
method that predicts full two-dimensional fields (frames) at each time step.
Conceived as a model-agnostic paradigm for video prediction, BlockGPT
factorizes space-time by using self-attention within each frame and causal
attention across frames; in this work, we instantiate it for precipitation
nowcasting. We evaluate BlockGPT on two precipitation datasets, viz. KNMI
(Netherlands) and SEVIR (U.S.), comparing it to state-of-the-art baselines
including token-based (NowcastingGPT) and diffusion-based (DiffCast+Phydnet)
models. The results show that BlockGPT achieves superior accuracy, event
localization as measured by categorical metrics, and inference speeds up to 31x
faster than comparable baselines.

</details>


### [121] [SDAR: A Synergistic Diffusion-AutoRegression Paradigm for Scalable Sequence Generation](https://arxiv.org/abs/2510.06303)
*Shuang Cheng,Yihan Bian,Dawei Liu,Yuhua Jiang,Yihao Liu,Linfeng Zhang,Wenhai Wang,Qipeng Guo,Kai Chen,Biqing Qi,Bowen Zhou*

Main category: cs.LG

TL;DR: SDAR是一种结合自回归模型训练效率和扩散模型并行推理能力的新范式，通过轻量级转换将训练好的自回归模型转化为块状扩散模型，实现高效并行生成。


<details>
  <summary>Details</summary>
Motivation: 结合自回归模型的计算效率和扩散模型的并行推理能力，以提升模型在推理阶段的效率和性能。

Method: 通过轻量级范式转换，将训练好的自回归模型转化为块状扩散模型，并在推理时以块为单位并行生成序列。

Result: SDAR在保持自回归模型性能的同时，实现了并行生成，并在科学推理任务中表现优于纯自回归模型。

Conclusion: SDAR成功结合了自回归和扩散模型的优势，为高效、高吞吐量的推理提供了实用范式。

Abstract: We propose SDAR, a Synergistic Diffusion-Autoregression paradigm that unifies
the training efficiency of autoregressive models with the parallel inference
capability of diffusion. Instead of costly end-to-end diffusion training, SDAR
performs a lightweight paradigm conversion that transforms a well-trained
autoregressive (AR) model into a blockwise diffusion model through brief,
data-efficient adaptation. During inference, SDAR generates sequences
autoregressively across blocks for global coherence while decoding all tokens
within each block in parallel via a discrete diffusion process. Extensive
experiments show that AR models remain substantially more compute-efficient
than masked diffusion models, providing a strong foundation for adaptation.
Building on this insight, SDAR achieves efficient AR-to-diffusion conversion
with minimal cost, preserving AR-level performance while enabling parallel
generation. Scaling studies across dense and Mixture-of-Experts architectures
confirm that SDAR scales without compromise: larger models exhibit stronger
robustness to block size and decoding thresholds, yielding greater speedups
without accuracy loss. Beyond efficiency, SDAR demonstrates enhanced reasoning
and domain adaptability. Our 30B MoE model surpasses its AR counterpart on
challenging scientific reasoning benchmarks such as GPQA and ChemBench, and
gains further improvements under test-time scaling methods like majority voting
and pass@k. Together, these results establish SDAR as a practical paradigm that
combines the strengths of autoregression and diffusion for scalable,
high-throughput reasoning.

</details>


### [122] [Flexible Swarm Learning May Outpace Foundation Models in Essential Tasks](https://arxiv.org/abs/2510.06349)
*Moein E. Samadi,Andreas Schuppert*

Main category: cs.LG

TL;DR: 论文探讨了基础模型在动态环境中的局限性，提出了一种分散式的小型代理网络（SANs）架构，以克服维度诅咒并实现更优的决策。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在AI领域发展迅速，但在动态环境（如重症监护中的疾病诊断和治疗）中表现有限，需要更高效的自适应方法。

Method: 提出分散式的小型代理网络（SANs）架构，每个代理专注于系统的子功能，通过群体学习实现自适应决策。

Result: SANs在动态环境中表现优于单一基础模型，但牺牲了细节的可重复性。

Conclusion: 分散式SANs架构为解决复杂动态系统中的决策问题提供了新思路，但需进一步验证其广泛适用性。

Abstract: Foundation models have rapidly advanced AI, raising the question of whether
their decisions will ultimately surpass human strategies in real-world domains.
The exponential, and possibly super-exponential, pace of AI development makes
such analysis elusive. Nevertheless, many application areas that matter for
daily life and society show only modest gains so far; a prominent case is
diagnosing and treating dynamically evolving disease in intensive care.
  The common challenge is adapting complex systems to dynamic environments.
Effective strategies must optimize outcomes in systems composed of strongly
interacting functions while avoiding shared side effects; this requires
reliable, self-adaptive modeling. These tasks align with building digital twins
of highly complex systems whose mechanisms are not fully or quantitatively
understood. It is therefore essential to develop methods for self-adapting AI
models with minimal data and limited mechanistic knowledge. As this challenge
extends beyond medicine, AI should demonstrate clear superiority in these
settings before assuming broader decision-making roles.
  We identify the curse of dimensionality as a fundamental barrier to efficient
self-adaptation and argue that monolithic foundation models face conceptual
limits in overcoming it. As an alternative, we propose a decentralized
architecture of interacting small agent networks (SANs). We focus on agents
representing the specialized substructure of the system, where each agent
covers only a subset of the full system functions. Drawing on mathematical
results on the learning behavior of SANs and evidence from existing
applications, we argue that swarm-learning in diverse swarms can enable
self-adaptive SANs to deliver superior decision-making in dynamic environments
compared with monolithic foundation models, though at the cost of reduced
reproducibility in detail.

</details>


### [123] [PIKAN: Physics-Inspired Kolmogorov-Arnold Networks for Explainable UAV Channel Modelling](https://arxiv.org/abs/2510.06355)
*Kürşat Tekbıyık,Güneş Karabulut Kurt,Antoine Lesage-Landry*

Main category: cs.LG

TL;DR: PIKAN是一种结合物理原理的轻量级神经网络，用于无人机通信中的信道建模，兼具高精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 无人机通信需要既准确又可解释的信道模型，传统确定性模型和深度学习模型各有不足。

Method: 提出PIKAN，将物理原理（如自由空间路径损耗、双射线反射）嵌入学习过程，作为灵活的归纳偏置。

Result: PIKAN在测量数据上达到与深度学习模型相当的精度，同时提供符号化表达式，且参数仅232个，比MLP轻37倍。

Conclusion: PIKAN是高效、可解释且可扩展的解决方案，适用于未来5G和6G网络的无人机信道建模。

Abstract: Unmanned aerial vehicle (UAV) communications demand accurate yet
interpretable air-to-ground (A2G) channel models that can adapt to
nonstationary propagation environments. While deterministic models offer
interpretability and deep learning (DL) models provide accuracy, both
approaches suffer from either rigidity or a lack of explainability. To bridge
this gap, we propose the Physics-Inspired Kolmogorov-Arnold Network (PIKAN)
that embeds physical principles (e.g., free-space path loss, two-ray
reflections) into the learning process. Unlike physics-informed neural networks
(PINNs), PIKAN is more flexible for applying physical information because it
introduces them as flexible inductive biases. Thus, it enables a more flexible
training process. Experiments on UAV A2G measurement data show that PIKAN
achieves comparable accuracy to DL models while providing symbolic and
explainable expressions aligned with propagation laws. Remarkably, PIKAN
achieves this performance with only 232 parameters, making it up to 37 times
lighter than multilayer perceptron (MLP) baselines with thousands of
parameters, without sacrificing correlation with measurements and also
providing symbolic expressions. These results highlight PIKAN as an efficient,
interpretable, and scalable solution for UAV channel modelling in beyond-5G and
6G networks.

</details>


### [124] [Lagrangian neural ODEs: Measuring the existence of a Lagrangian with Helmholtz metrics](https://arxiv.org/abs/2510.06367)
*Luca Wolf,Tobias Buck,Bjoern Malte Schaefer*

Main category: cs.LG

TL;DR: Helmholtz metrics quantify the resemblance of neural ODE solutions to Euler-Lagrange equations, enabling direct learning of Lagrangian systems with zero additional inference cost.


<details>
  <summary>Details</summary>
Motivation: Neural ODEs are powerful but not all solutions are physical (Euler-Lagrange equations). Helmholtz metrics address this gap.

Method: Introduce Helmholtz metrics to measure physical resemblance, combine with second-order neural ODE to form Lagrangian neural ODE.

Result: Demonstrated ability to distinguish Lagrangian/non-Lagrangian systems and improve solutions using positional data.

Conclusion: Lagrangian neural ODEs effectively learn Euler-Lagrange equations and enhance neural ODE solutions.

Abstract: Neural ODEs are a widely used, powerful machine learning technique in
particular for physics. However, not every solution is physical in that it is
an Euler-Lagrange equation. We present Helmholtz metrics to quantify this
resemblance for a given ODE and demonstrate their capabilities on several
fundamental systems with noise. We combine them with a second order neural ODE
to form a Lagrangian neural ODE, which allows to learn Euler-Lagrange equations
in a direct fashion and with zero additional inference cost. We demonstrate
that, using only positional data, they can distinguish Lagrangian and
non-Lagrangian systems and improve the neural ODE solutions.

</details>


### [125] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan,Valter Hudovernik,Mark Znidar,Charilaos Kanatsoulis,Roshan Upendra,Mahmoud Mohammadi,Joe Meyer,Tom Palczewski,Carlos Guestrin,Jure Leskovec*

Main category: cs.LG

TL;DR: 提出了一种名为Relational Transformer（RT）的架构，能够在多样化的关系数据库上进行预训练，并直接应用于未见过的数据集和任务，无需特定微调或上下文示例检索。


<details>
  <summary>Details</summary>
Motivation: 解决关系数据多样性带来的挑战，如异构模式、图结构和功能依赖，以实现跨数据集和任务的迁移。

Method: RT通过（i）使用表/列元数据标记单元格，（ii）通过掩码标记预测进行预训练，（iii）利用新型关系注意力机制（列、行和主外键链接）。

Result: 在RelBench数据集上预训练后，RT在零样本任务中表现优异，平均达到94%的AUROC（二进制分类任务），优于27B参数的LLM（84%）。微调后达到最先进效果。

Conclusion: RT为零样本迁移提供了实用路径，利用任务表上下文、关系注意力模式和模式语义，为关系数据的基础模型铺平了道路。

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via
zero-shot prompting, but relational domains still lack architectures that
transfer across datasets and tasks. The core challenge is the diversity of
relational data, with varying heterogeneous schemas, graph structures and
functional dependencies. In this paper, we present the Relational Transformer
(RT) architecture, which can be pretrained on diverse relational databases and
directly applied to unseen datasets and tasks without task- or dataset-specific
fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with
table/column metadata, (ii) is pretrained via masked token prediction, and
(iii) utilizes a novel \textit{Relational Attention} mechanism over columns,
rows, and primary-foreign key links. Pretrained on RelBench datasets spanning
tasks such as churn and sales forecasting, RT attains strong zero-shot
performance, averaging 94% of fully supervised AUROC on binary classification
tasks with a single forward pass of a 22M parameter model, as opposed to 84%
for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample
efficiency. Our experiments show that RT's zero-shot transfer harnesses
task-table context, relational attention patterns and schema semantics.
Overall, RT provides a practical path toward foundation models for relational
data.

</details>


### [126] [Monte Carlo Permutation Search](https://arxiv.org/abs/2510.06381)
*Tristan Cazenave*

Main category: cs.LG

TL;DR: MCPS是一种改进GRAVE算法的通用MCTS算法，适用于无法使用深度强化学习或计算资源有限的情况，如通用游戏。


<details>
  <summary>Details</summary>
Motivation: 解决在无法使用深度强化学习或计算资源有限时的游戏搜索问题，改进GRAVE算法。

Method: 在节点探索项中包含从根到节点的所有路径的统计数据，使用抽象代码代替精确代码。

Result: MCPS在双人游戏中表现优于GRAVE，多人游戏中表现相当。抽象代码对两者均有帮助。

Conclusion: MCPS通过改进统计公式和去除超参数敏感性，提供了一种更优的搜索算法。

Abstract: We propose Monte Carlo Permutation Search (MCPS), a general-purpose Monte
Carlo Tree Search (MCTS) algorithm that improves upon the GRAVE algorithm. MCPS
is relevant when deep reinforcement learning is not an option, or when the
computing power available before play is not substantial, such as in General
Game Playing, for example. The principle of MCPS is to include in the
exploration term of a node the statistics on all the playouts that contain all
the moves on the path from the root to the node. We extensively test MCPS on a
variety of games: board games, wargame, investment game, video game and
multi-player games. MCPS has better results than GRAVE in all the two-player
games. It has equivalent results for multi-player games because these games are
inherently balanced even when players have different strengths. We also show
that using abstract codes for moves instead of exact codes can be beneficial to
both MCPS and GRAVE, as they improve the permutation statistics and the AMAF
statistics. We also provide a mathematical derivation of the formulas used for
weighting the three sources of statistics. These formulas are an improvement on
the GRAVE formula since they no longer use the bias hyperparameter of GRAVE.
Moreover, MCPS is not sensitive to the ref hyperparameter.

</details>


### [127] [Making and Evaluating Calibrated Forecasts](https://arxiv.org/abs/2510.06388)
*Yuxuan Lu,Yifan Wu,Jason Hartline,Lunjia Hu*

Main category: cs.LG

TL;DR: 本文提出了一种多类预测任务的完美真实校准度量，解决了现有校准度量的非真实性问题，并证明了其鲁棒性优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有校准度量多为非真实的，可能导致预测器为了显得更校准而撒谎，因此需要设计真实且鲁棒的校准度量。

Method: 通过数学证明和实证验证，提出了一种多类预测任务的完美真实校准度量，并研究了从二分类扩展到多分类时保持真实性的方法。

Result: 新校准度量不仅真实，而且在鲁棒性上优于传统方法，能够稳定地保持预测器之间的优劣顺序。

Conclusion: 本文提出的校准度量在多类预测任务中既真实又鲁棒，解决了现有方法的局限性。

Abstract: Calibrated predictions can be reliably interpreted as probabilities. An
important step towards achieving better calibration is to design an appropriate
calibration measure to meaningfully assess the miscalibration level of a
predictor. A recent line of work initiated by Haghtalab et al. [2024] studies
the design of truthful calibration measures: a truthful measure is minimized
when a predictor outputs the true probabilities, whereas a non-truthful measure
incentivizes the predictor to lie so as to appear more calibrated. All previous
calibration measures were non-truthful until Hartline et al. [2025] introduced
the first perfectly truthful calibration measures for binary prediction tasks
in the batch setting.
  We introduce a perfectly truthful calibration measure for multi-class
prediction tasks, generalizing the work of Hartline et al. [2025] beyond binary
prediction. We study common methods of extending calibration measures from
binary to multi-class prediction and identify ones that do or do not preserve
truthfulness. In addition to truthfulness, we mathematically prove and
empirically verify that our calibration measure exhibits superior robustness:
it robustly preserves the ordering between dominant and dominated predictors,
regardless of the choice of hyperparameters (bin sizes). This result addresses
the non-robustness issue of binned ECE, which has been observed repeatedly in
prior work.

</details>


### [128] [Geometry-Aware Backdoor Attacks: Leveraging Curvature in Hyperbolic Embeddings](https://arxiv.org/abs/2510.06397)
*Ali Baheri*

Main category: cs.LG

TL;DR: 论文揭示了非欧几里得基础模型中边界驱动的几何不对称性如何被后门触发器利用，并提出了几何自适应触发器。


<details>
  <summary>Details</summary>
Motivation: 研究非欧几里得几何（如双曲几何）中模型表示的边界不对称性如何被后门攻击利用。

Method: 通过理论分析边界效应，提出几何自适应触发器，并在不同任务和架构中评估其效果。

Result: 攻击成功率在边界附近增加，而传统检测器效果减弱，验证了理论趋势。

Conclusion: 研究揭示了非欧几里得模型的几何特定漏洞，并为防御设计提供了分析支持的指导。

Abstract: Non-Euclidean foundation models increasingly place representations in curved
spaces such as hyperbolic geometry. We show that this geometry creates a
boundary-driven asymmetry that backdoor triggers can exploit. Near the
boundary, small input changes appear subtle to standard input-space detectors
but produce disproportionately large shifts in the model's representation
space. Our analysis formalizes this effect and also reveals a limitation for
defenses: methods that act by pulling points inward along the radius can
suppress such triggers, but only by sacrificing useful model sensitivity in
that same direction. Building on these insights, we propose a simple
geometry-adaptive trigger and evaluate it across tasks and architectures.
Empirically, attack success increases toward the boundary, whereas conventional
detectors weaken, mirroring the theoretical trends. Together, these results
surface a geometry-specific vulnerability in non-Euclidean models and offer
analysis-backed guidance for designing and understanding the limits of
defenses.

</details>


### [129] [The Effect of Label Noise on the Information Content of Neural Representations](https://arxiv.org/abs/2510.06401)
*Ali Hussaini Umar,Franky Kevin Nando Tezoh,Jean Barbier,Santiago Acevedo,Alessandro Laio*

Main category: cs.LG

TL;DR: 本文研究了标签噪声对深度学习模型隐藏表示的影响，发现隐藏表示的信息内容随网络参数数量呈现双下降现象，并揭示了过参数化网络对标签噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中标签噪声普遍存在，但其对网络隐藏表示的影响尚未被充分理解。

Method: 使用信息不平衡作为条件互信息的计算高效代理，系统比较隐藏表示。

Result: 在欠参数化区域，带噪声标签学习的表示比干净标签更信息丰富；在过参数化区域，两者信息量相当。

Conclusion: 过参数化网络的表示对标签噪声具有鲁棒性，且信息不平衡随交叉熵损失减少，为分类任务泛化提供了新视角。

Abstract: In supervised classification tasks, models are trained to predict a label for
each data point. In real-world datasets, these labels are often noisy due to
annotation errors. While the impact of label noise on the performance of deep
learning models has been widely studied, its effects on the networks' hidden
representations remain poorly understood. We address this gap by systematically
comparing hidden representations using the Information Imbalance, a
computationally efficient proxy of conditional mutual information. Through this
analysis, we observe that the information content of the hidden representations
follows a double descent as a function of the number of network parameters,
akin to the behavior of the test error. We further demonstrate that in the
underparameterized regime, representations learned with noisy labels are more
informative than those learned with clean labels, while in the
overparameterized regime, these representations are equally informative. Our
results indicate that the representations of overparameterized networks are
robust to label noise. We also found that the information imbalance between the
penultimate and pre-softmax layers decreases with cross-entropy loss in the
overparameterized regime. This offers a new perspective on understanding
generalization in classification tasks. Extending our analysis to
representations learned from random labels, we show that these perform worse
than random features. This indicates that training on random labels drives
networks much beyond lazy learning, as weights adapt to encode labels
information.

</details>


### [130] [Test-Time Efficient Pretrained Model Portfolios for Time Series Forecasting](https://arxiv.org/abs/2510.06419)
*Mert Kayaalp,Caner Turkmen,Oleksandr Shchur,Pedro Mercado,Abdul Fatir Ansari,Michael Bohlke-Schneider,Bernie Wang*

Main category: cs.LG

TL;DR: 探讨是否更大的时间序列基础模型总是更好，提出构建小型预训练模型组合的替代方案，通过集成或模型选择实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 质疑单一大型模型的必要性，探索更高效的参数利用方式。

Method: 构建小型预训练模型组合，采用集成或模型选择策略，并研究如何设计多样化专家模型。

Result: 小型专家模型组合在基准测试中表现优异，且后训练基础模型是计算高效的方法。

Conclusion: 集成和模型选择比测试时微调更高效，小型专家模型组合是大型模型的可行替代方案。

Abstract: Is bigger always better for time series foundation models? With the question
in mind, we explore an alternative to training a single, large monolithic
model: building a portfolio of smaller, pretrained forecasting models. By
applying ensembling or model selection over these portfolios, we achieve
competitive performance on large-scale benchmarks using much fewer parameters.
We explore strategies for designing such portfolios and find that collections
of specialist models consistently outperform portfolios of independently
trained generalists. Remarkably, we demonstrate that post-training a base model
is a compute-effective approach for creating sufficiently diverse specialists,
and provide evidences that ensembling and model selection are more
compute-efficient than test-time fine-tuning.

</details>


### [131] [Nearly Instance-Optimal Parameter Recovery from Many Trajectories via Hellinger Localization](https://arxiv.org/abs/2510.06434)
*Eliot Shekhtman,Yichen Zhou,Ingvar Ziemann,Nikolai Matni,Stephen Tu*

Main category: cs.LG

TL;DR: 论文提出了一种基于Hellinger定位框架的方法，用于在多轨迹设置中实现实例最优的学习速率，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，从时间相关数据中学习是一个核心问题，但多轨迹设置下的理解仍不完整。现有方法要么依赖于独立同分布假设，要么需要混合条件，限制了其适用范围。

Method: 采用Hellinger定位框架，通过路径度量级别的平方Hellinger距离控制，结合轨迹Fisher信息加权的参数空间二次形式定位，实现实例最优的学习速率。

Result: 在四个案例研究中（马尔可夫链混合、非高斯噪声下的依赖线性回归、非单调激活的广义线性模型、线性注意力序列模型），该方法几乎匹配了渐近正态性的实例最优速率。

Conclusion: 该方法在多轨迹设置下显著提升了学习效率，适用范围广，优于传统方法。

Abstract: Learning from temporally-correlated data is a core facet of modern machine
learning. Yet our understanding of sequential learning remains incomplete,
particularly in the multi-trajectory setting where data consists of many
independent realizations of a time-indexed stochastic process. This important
regime both reflects modern training pipelines such as for large foundation
models, and offers the potential for learning without the typical mixing
assumptions made in the single-trajectory case. However, instance-optimal
bounds are known only for least-squares regression with dependent covariates;
for more general models or loss functions, the only broadly applicable
guarantees result from a reduction to either i.i.d. learning, with effective
sample size scaling only in the number of trajectories, or an existing
single-trajectory result when each individual trajectory mixes, with effective
sample size scaling as the full data budget deflated by the mixing-time.
  In this work, we significantly broaden the scope of instance-optimal rates in
multi-trajectory settings via the Hellinger localization framework, a general
approach for maximum likelihood estimation. Our method proceeds by first
controlling the squared Hellinger distance at the path-measure level via a
reduction to i.i.d. learning, followed by localization as a quadratic form in
parameter space weighted by the trajectory Fisher information. This yields
instance-optimal bounds that scale with the full data budget under a broad set
of conditions. We instantiate our framework across four diverse case studies: a
simple mixture of Markov chains, dependent linear regression under non-Gaussian
noise, generalized linear models with non-monotonic activations, and
linear-attention sequence models. In all cases, our bounds nearly match the
instance-optimal rates from asymptotic normality, substantially improving over
standard reductions.

</details>


### [132] [Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models](https://arxiv.org/abs/2510.06439)
*Akash Yadav,Ruda Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对不确定性下超参数调优的贝叶斯优化框架，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 超参数调优在不确定性系统中具有挑战性，传统方法计算成本高。

Method: 使用统计代理模型分析期望算子，并推导了随机获取函数优化器的闭式解。

Result: 相比传统方法，所需数据点减少40倍，计算成本降低40倍。

Conclusion: 该方法在计算工程中表现出高效性。

Abstract: Hyperparameter tuning is a challenging problem especially when the system
itself involves uncertainty. Due to noisy function evaluations, optimization
under uncertainty can be computationally expensive. In this paper, we present a
novel Bayesian optimization framework tailored for hyperparameter tuning under
uncertainty, with a focus on optimizing a scale- or precision-type parameter in
stochastic models. The proposed method employs a statistical surrogate for the
underlying random variable, enabling analytical evaluation of the expectation
operator. Moreover, we derive a closed-form expression for the optimizer of the
random acquisition function, which significantly reduces computational cost per
iteration. Compared with a conventional one-dimensional Monte Carlo-based
optimization scheme, the proposed approach requires 40 times fewer data points,
resulting in up to a 40-fold reduction in computational cost. We demonstrate
the effectiveness of the proposed method through two numerical examples in
computational engineering.

</details>


### [133] [Context-Aware Inference via Performance Forecasting in Decentralized Learning Networks](https://arxiv.org/abs/2510.06444)
*Joel Pfeffer,J. M. Diederik Kruijssen,Clément Gossart,Mélanie Chevance,Diego Campo Millan,Florian Stecker,Steven N. Longmore*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的性能预测模型，用于改进去中心化学习网络中的预测组合策略。


<details>
  <summary>Details</summary>
Motivation: 现有线性池化方法（如动态权重更新）因依赖历史性能而反应迟缓，难以适应快速变化的环境。

Method: 开发了一种性能预测模型，通过预测每个模型在时间序列中的表现来动态调整权重。

Result: 预测模型（尤其是基于遗憾或遗憾z分数的模型）显著提高了网络推断的准确性。

Conclusion: 性能预测模型在需要预测性而非反应性权重调整的场景中具有广泛适用性。

Abstract: In decentralized learning networks, predictions from many participants are
combined to generate a network inference. While many studies have demonstrated
performance benefits of combining multiple model predictions, existing
strategies using linear pooling methods (ranging from simple averaging to
dynamic weight updates) face a key limitation. Dynamic prediction combinations
that rely on historical performance to update weights are necessarily reactive.
Due to the need to average over a reasonable number of epochs (with moving
averages or exponential weighting), they tend to be slow to adjust to changing
circumstances (phase or regime changes). In this work, we develop a model that
uses machine learning to forecast the performance of predictions by models at
each epoch in a time series. This enables `context-awareness' by assigning
higher weight to models that are likely to be more accurate at a given time. We
show that adding a performance forecasting worker in a decentralized learning
network, following a design similar to the Allora network, can improve the
accuracy of network inferences. Specifically, we find forecasting models that
predict regret (performance relative to the network inference) or regret
z-score (performance relative to other workers) show greater improvement than
models predicting losses, which often do not outperform the naive network
inference (historically weighted average of all inferences). Through a series
of optimization tests, we show that the performance of the forecasting model
can be sensitive to choices in the feature set and number of training epochs.
These properties may depend on the exact problem and should be tailored to each
domain. Although initially designed for a decentralized learning network, using
performance forecasting for prediction combination may be useful in any
situation where predictive rather than reactive model weighting is needed.

</details>


### [134] [How NOT to benchmark your SITE metric: Beyond Static Leaderboards and Towards Realistic Evaluation](https://arxiv.org/abs/2510.06448)
*Prabhant Singh,Sibylle Hess,Joaquin Vanschoren*

Main category: cs.LG

TL;DR: 论文指出当前迁移性评估指标的基准测试存在缺陷，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 揭示现有迁移性评估指标基准测试的不合理之处，推动更真实的评估方法。

Method: 通过实证分析，展示基准测试中的模型空间和性能层次问题。

Result: 发现简单启发式方法在现有基准中表现优于复杂方法。

Conclusion: 提出构建更稳健基准的建议，以指导未来研究。

Abstract: Transferability estimation metrics are used to find a high-performing
pre-trained model for a given target task without fine-tuning models and
without access to the source dataset. Despite the growing interest in
developing such metrics, the benchmarks used to measure their progress have
gone largely unexamined. In this work, we empirically show the shortcomings of
widely used benchmark setups to evaluate transferability estimation metrics. We
argue that the benchmarks on which these metrics are evaluated are
fundamentally flawed. We empirically demonstrate that their unrealistic model
spaces and static performance hierarchies artificially inflate the perceived
performance of existing metrics, to the point where simple, dataset-agnostic
heuristics can outperform sophisticated methods. Our analysis reveals a
critical disconnect between current evaluation protocols and the complexities
of real-world model selection. To address this, we provide concrete
recommendations for constructing more robust and realistic benchmarks to guide
future research in a more meaningful direction.

</details>


### [135] [Attention Sinks and Compression Valleys in LLMs are Two Sides of the Same Coin](https://arxiv.org/abs/2510.06477)
*Enrique Queipo-de-Llano,Álvaro Arroyo,Federico Barbero,Xiaowen Dong,Michael Bronstein,Yann LeCun,Ravid Shwartz-Ziv*

Main category: cs.LG

TL;DR: 论文揭示了注意力下沉和压缩谷现象之间的联系，提出了一种统一的信息流理论。


<details>
  <summary>Details</summary>
Motivation: 研究注意力下沉和压缩谷现象之间的联系，探索大语言模型中信息流的组织方式。

Method: 通过理论证明和实验验证，提出Mix-Compress-Refine理论，解释信息流的三个阶段。

Result: 实验证实了理论预测，并展示了不同任务在模型不同层中的表现差异。

Conclusion: 提出了统一的信息流理论，解释了模型在不同层中的计算组织方式及其对任务表现的影响。

Abstract: Attention sinks and compression valleys have attracted significant attention
as two puzzling phenomena in large language models, but have been studied in
isolation. In this work, we present a surprising connection between attention
sinks and compression valleys, tracing both to the formation of massive
activations in the residual stream. We prove theoretically that massive
activations necessarily produce representational compression and establish
bounds on the resulting entropy reduction. Through experiments across several
models (410M-120B parameters), we confirm that when the beginning-of-sequence
token develops extreme activation norms in the middle layers, both compression
valleys and attention sinks emerge simultaneously. Targeted ablation studies
validate our theoretical predictions. This unified view motivates us to propose
the Mix-Compress-Refine theory of information flow, as an attempt to explain
how LLMs organize their computation in depth by controlling attention and
representational compression via massive activations. Specifically, we posit
that Transformer-based LLMs process tokens in three distinct phases: (1) broad
mixing in the early layers, (2) compressed computation with limited mixing in
the middle layers, and (3) selective refinement in the late layers. Our
framework helps explain why embedding tasks perform best at intermediate
layers, whereas generation tasks benefit from full-depth processing, clarifying
differences in task-dependent representations.

</details>


### [136] [Valid Stopping for LLM Generation via Empirical Dynamic Formal Lift](https://arxiv.org/abs/2510.06478)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.LG

TL;DR: Sequential-EDFL是一种基于信息提升的序列测试方法，用于控制语言模型生成的停止时间，减少生成量并保持错误控制。


<details>
  <summary>Details</summary>
Motivation: 为了解决语言模型生成过程中如何有效控制停止时间并减少冗余生成的问题。

Method: 通过跟踪信息提升（log-likelihood ratio）并使用自归一化的经验-Bernstein e-processes，结合在线均值估计和混合e-processes。

Result: 在六个基准测试中，Sequential-EDFL减少了22-28%的生成量，计算开销为12%。

Conclusion: Sequential-EDFL作为第一阶段的过滤器，显著减少了验证负担，但不适用于安全关键领域。

Abstract: We introduce Sequential-EDFL (Empirical Dynamic Formal Lift), applying
anytime-valid sequential testing to language model generation stopping. Our
approach tracks information lift -- the log-likelihood ratio between full
models and deliberately weakened "skeleton" baselines -- using self-normalized
empirical-Bernstein e-processes that provide formal delta-level error control
regardless of stopping time. We handle unknown centering through online mean
estimation, combine multiple parameters via mixture e-processes, and support
adaptive resets under distributional drift. On six benchmarks, Sequential-EDFL
reduces generation by 22-28% vs. sequential baselines while maintaining
delta-level control with 12% computational overhead. We introduce automated
skeletons (distilled submodels, randomized logits) and show robustness across
skeleton families. Composing EDFL with a lightweight correctness gate (sentence
boundaries + verifier) improves end-task correctness while preserving
anytime-valid guarantees by only delaying stopping. Our certificates control
information sufficiency, not factual correctness -- 10.9% of stopped sequences
remain incorrect even with the gate (13.2-22.7% without it). EDFL serves as a
first-stage filter reducing verification burden by 83%, not as a standalone
solution for safety-critical domains.

</details>


### [137] [GUIDE: Guided Initialization and Distillation of Embeddings](https://arxiv.org/abs/2510.06502)
*Khoa Trinh,Gaurav Menghani,Erik Vee*

Main category: cs.LG

TL;DR: 论文提出了一种名为GUIDE的新蒸馏技术，通过让学生在参数空间中匹配教师模型，显著缩小了师生模型质量差距，且无需额外训练或推理开销。


<details>
  <summary>Details</summary>
Motivation: 传统蒸馏方法仅让学生匹配教师模型的输出，未能充分利用教师模型的潜在信息。作者希望通过在参数空间中匹配教师模型，提取更多有用信息。

Method: 提出GUIDE技术，强制学生在参数空间中匹配教师模型，并结合知识蒸馏实现近叠加改进。

Result: 实验显示，GUIDE将师生模型质量差距缩小25-26%，且单独使用时效果优于传统知识蒸馏。

Conclusion: GUIDE是一种高效且无额外开销的蒸馏技术，显著提升了学生模型的质量。

Abstract: Algorithmic efficiency techniques such as distillation
(\cite{hinton2015distillation}) are useful in improving model quality without
increasing serving costs, provided a larger teacher model is available for a
smaller student model to learn from during training. Standard distillation
methods are limited to only forcing the student to match the teacher's outputs.
Given the costs associated with training a large model, we believe we should be
extracting more useful information from a teacher model than by just making the
student match the teacher's outputs.
  In this paper, we introduce \guide (Guided Initialization and Distillation of
Embeddings). \guide can be considered a distillation technique that forces the
student to match the teacher in the parameter space. Using \guide we show
25-26\% reduction in the teacher-student quality gap when using large student
models (400M - 1B parameters) trained on $\approx$ 20B tokens. We also present
a thorough analysis demonstrating that \guide can be combined with knowledge
distillation with near additive improvements. Furthermore, we show that
applying \guide alone leads to substantially better model quality than applying
knowledge distillation by itself.
  Most importantly, \guide introduces no training or inference overhead and
hence any model quality gains from our method are virtually free.

</details>


### [138] [ATLO-ML: Adaptive Time-Length Optimizer for Machine Learning -- Insights from Air Quality Forecasting](https://arxiv.org/abs/2510.06503)
*I-Hsi Kao,Kanji Uchino*

Main category: cs.LG

TL;DR: ATLO-ML是一种自适应时间长度优化系统，可自动确定最佳输入时间长度和采样率，显著提升时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法中，输入时间长度和采样率的选择对时间序列预测准确性影响很大，但通常固定不变，限制了模型性能。

Method: ATLO-ML通过动态调整输入时间长度和采样率，基于用户定义的输出时间长度，优化数据预处理流程。

Result: 在空气质量数据集上的实验表明，ATLO-ML优化的参数显著提高了机器学习模型的预测准确性。

Conclusion: ATLO-ML为时间敏感型应用提供了一种通用的优化方案，可广泛应用于机器学习工作流。

Abstract: Accurate time-series predictions in machine learning are heavily influenced
by the selection of appropriate input time length and sampling rate. This paper
introduces ATLO-ML, an adaptive time-length optimization system that
automatically determines the optimal input time length and sampling rate based
on user-defined output time length. The system provides a flexible approach to
time-series data pre-processing, dynamically adjusting these parameters to
enhance predictive performance. ATLO-ML is validated using air quality
datasets, including both GAMS-dataset and proprietary data collected from a
data center, both in time series format. Results demonstrate that utilizing the
optimized time length and sampling rate significantly improves the accuracy of
machine learning models compared to fixed time lengths. ATLO-ML shows potential
for generalization across various time-sensitive applications, offering a
robust solution for optimizing temporal input parameters in machine learning
workflows.

</details>


### [139] [A Median Perspective on Unlabeled Data for Out-of-Distribution Detection](https://arxiv.org/abs/2510.06505)
*Momin Abbas,Ali Falahati,Hossein Goli,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: Medix框架通过中值操作从未标记数据中识别潜在异常值，结合标记数据训练鲁棒的OOD分类器，理论误差界限低，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中未标记数据混合了InD和OOD样本，缺乏明确的OOD样本集，训练最优OOD分类器具有挑战性。

Method: 使用中值操作从未标记数据中识别异常值，结合标记InD数据训练OOD分类器。

Result: 理论证明Medix误差率低，实验显示其在开放世界设置中优于现有方法。

Conclusion: Medix通过中值操作有效利用未标记数据，提升了OOD检测的鲁棒性和可靠性。

Abstract: Out-of-distribution (OOD) detection plays a crucial role in ensuring the
robustness and reliability of machine learning systems deployed in real-world
applications. Recent approaches have explored the use of unlabeled data,
showing potential for enhancing OOD detection capabilities. However,
effectively utilizing unlabeled in-the-wild data remains challenging due to the
mixed nature of both in-distribution (InD) and OOD samples. The lack of a
distinct set of OOD samples complicates the task of training an optimal OOD
classifier. In this work, we introduce Medix, a novel framework designed to
identify potential outliers from unlabeled data using the median operation. We
use the median because it provides a stable estimate of the central tendency,
as an OOD detection mechanism, due to its robustness against noise and
outliers. Using these identified outliers, along with labeled InD data, we
train a robust OOD classifier. From a theoretical perspective, we derive error
bounds that demonstrate Medix achieves a low error rate. Empirical results
further substantiate our claims, as Medix outperforms existing methods across
the board in open-world settings, confirming the validity of our theoretical
insights.

</details>


### [140] [Text-to-Image Models Leave Identifiable Signatures: Implications for Leaderboard Security](https://arxiv.org/abs/2510.06525)
*Ali Naseh,Anshuman Suri,Yuefeng Peng,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.LG

TL;DR: 文本到图像排行榜的模型去匿名化问题比预想的更严重，简单的CLIP嵌入空间分类即可高精度识别生成模型，需加强防御。


<details>
  <summary>Details</summary>
Motivation: 生成式AI排行榜易受操纵，尤其是模型去匿名化问题，此前在大型语言模型中已有研究，但文本到图像排行榜中问题更严重。

Method: 使用15万张图像、280个提示和19个模型，通过CLIP嵌入空间实时分类识别生成模型，并提出提示级可分离性指标。

Result: 无需提示控制或历史数据即可高精度识别模型，某些提示可实现近乎完美的去匿名化。

Conclusion: 文本到图像排行榜的排名操纵比预想的更容易，需加强防御措施。

Abstract: Generative AI leaderboards are central to evaluating model capabilities, but
remain vulnerable to manipulation. Among key adversarial objectives is rank
manipulation, where an attacker must first deanonymize the models behind
displayed outputs -- a threat previously demonstrated and explored for large
language models (LLMs). We show that this problem can be even more severe for
text-to-image leaderboards, where deanonymization is markedly easier. Using
over 150,000 generated images from 280 prompts and 19 diverse models spanning
multiple organizations, architectures, and sizes, we demonstrate that simple
real-time classification in CLIP embedding space identifies the generating
model with high accuracy, even without prompt control or historical data. We
further introduce a prompt-level separability metric and identify prompts that
enable near-perfect deanonymization. Our results indicate that rank
manipulation in text-to-image leaderboards is easier than previously
recognized, underscoring the need for stronger defenses.

</details>


### [141] [Wide Neural Networks as a Baseline for the Computational No-Coincidence Conjecture](https://arxiv.org/abs/2510.06527)
*John Dunbar,Scott Aaronson*

Main category: cs.LG

TL;DR: 随机初始化的神经网络在激活函数满足零均值高斯分布时，输出几乎独立，适用于对齐研究中心的计算无巧合猜想。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络输出独立性与激活函数特性的关系，以支持AI可解释性的理论框架。

Method: 分析不同激活函数（如ReLU、GeLU、tanh）在零均值高斯分布下的输出独立性。

Result: 零均值激活函数（如调整后的ReLU、GeLU和tanh）使神经网络输出几乎独立。

Conclusion: 零均值激活函数是验证计算无巧合猜想的有力候选，有助于AI可解释性研究。

Abstract: We establish that randomly initialized neural networks, with large width and
a natural choice of hyperparameters, have nearly independent outputs exactly
when their activation function is nonlinear with zero mean under the Gaussian
measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this
includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or
GeLU by themselves. Because of their nearly independent outputs, we propose
neural networks with zero-mean activation functions as a promising candidate
for the Alignment Research Center's computational no-coincidence conjecture --
a conjecture that aims to measure the limits of AI interpretability.

</details>


### [142] [Scalable Policy-Based RL Algorithms for POMDPs](https://arxiv.org/abs/2510.06540)
*Ameya Anjarlekar,Rasoul Etesami,R Srikant*

Main category: cs.LG

TL;DR: 将POMDP近似为有限状态MDP（Superstate MDP），提出基于策略的学习方法，证明近似误差随历史长度指数下降。


<details>
  <summary>Details</summary>
Motivation: 解决POMDP中连续信念状态带来的计算挑战。

Method: 将POMDP近似为Superstate MDP，采用基于策略的学习和线性函数逼近。

Result: 理论保证优于前人工作，近似误差随历史长度指数下降。

Conclusion: POMDP可通过近似为MDP并应用TD学习和策略优化高效求解。

Abstract: The continuous nature of belief states in POMDPs presents significant
computational challenges in learning the optimal policy. In this paper, we
consider an approach that solves a Partially Observable Reinforcement Learning
(PORL) problem by approximating the corresponding POMDP model into a
finite-state Markov Decision Process (MDP) (called Superstate MDP). We first
derive theoretical guarantees that improve upon prior work that relate the
optimal value function of the transformed Superstate MDP to the optimal value
function of the original POMDP. Next, we propose a policy-based learning
approach with linear function approximation to learn the optimal policy for the
Superstate MDP. Consequently, our approach shows that a POMDP can be
approximately solved using TD-learning followed by Policy Optimization by
treating it as an MDP, where the MDP state corresponds to a finite history. We
show that the approximation error decreases exponentially with the length of
this history. To the best of our knowledge, our finite-time bounds are the
first to explicitly quantify the error introduced when applying standard TD
learning to a setting where the true dynamics are not Markovian.

</details>


### [143] [Incoherence in goal-conditioned autoregressive models](https://arxiv.org/abs/2510.06545)
*Jacek Karwowski,Raymond Douglas*

Main category: cs.LG

TL;DR: 论文研究了强化学习策略中的不连贯性问题，通过在线RL微调离线策略减少不连贯性并提高回报。


<details>
  <summary>Details</summary>
Motivation: 探索自回归模型目标条件化导致的结构性问题，并分析在线微调对策略轨迹的影响。

Method: 通过重新定义控制即推断和软Q学习，建立三向对应关系，分析迭代再训练过程。

Result: 证明在线微调减少不连贯性并提高回报，同时揭示了训练与推断的权衡。

Conclusion: 通过软条件生成模型，讨论了不连贯性与有效视野之间的联系。

Abstract: We investigate mathematically the notion of incoherence: a structural issue
with reinforcement learning policies derived by naive goal-conditioning of
autoregressive models. We focus on the process of re-training models on their
own actions, that is, fine-tuning offline-learned policies with online RL. We
prove that it decreases incoherence and leads to an improvement in return, and
we aim to characterize the resulting trajectory of policies. By re-framing
standard notions of control-as-inference and soft Q learning, we establish a
three-way correspondence with two other ways of understanding the iterative
re-training process: as folding the posterior into the reward and, in the
deterministic case, as decreasing the temperature parameter; the correspondence
has computational content via the training-inference trade-off. Through
soft-conditioning generative models, we discuss the link between incoherence
and the effective horizon.

</details>


### [144] [DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning](https://arxiv.org/abs/2510.06913)
*Ke Guo,Haochen Liu,Xiaojun Wu,Chen Lv*

Main category: cs.LG

TL;DR: DecompGAIL通过分解多智能体交互，解决了GAIL在多智能体环境中的不稳定性问题，提升了交通仿真的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在交通仿真中难以建模真实行为，行为克隆存在协变量偏移，GAIL在多智能体环境中不稳定。

Method: 提出DecompGAIL，分解真实性为ego-map和ego-neighbor两部分，过滤误导性交互，并引入社会PPO目标。

Result: 在WOMD Sim Agents 2025基准测试中达到最优性能。

Conclusion: DecompGAIL有效提升了多智能体交通仿真的真实性和稳定性。

Abstract: Realistic traffic simulation is critical for the development of autonomous
driving systems and urban mobility planning, yet existing imitation learning
approaches often fail to model realistic traffic behaviors. Behavior cloning
suffers from covariate shift, while Generative Adversarial Imitation Learning
(GAIL) is notoriously unstable in multi-agent settings. We identify a key
source of this instability: irrelevant interaction misguidance, where a
discriminator penalizes an ego vehicle's realistic behavior due to unrealistic
interactions among its neighbors. To address this, we propose Decomposed
Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map
and ego-neighbor components, filtering out misleading neighbor: neighbor and
neighbor: map interactions. We further introduce a social PPO objective that
augments ego rewards with distance-weighted neighborhood rewards, encouraging
overall realism across agents. Integrated into a lightweight SMART-based
backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim
Agents 2025 benchmark.

</details>


### [145] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: 提出了一种名为Markovian Thinking的新范式，通过固定大小的状态实现线性计算和常数内存，显著降低了长链推理的计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统强化学习环境中长链推理导致的二次计算开销问题。

Method: 提出Delethink环境，将推理结构化为固定大小的块，并在块边界重置上下文。

Result: 在8K-token块中训练的1.5B模型推理能力达到24K-token，计算成本显著降低。

Conclusion: 重新设计推理环境是实现高效、可扩展推理LLM的有效途径。

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [146] [Introspection in Learned Semantic Scene Graph Localisation](https://arxiv.org/abs/2510.07053)
*Manshika Charvi Bissessur,Efimia Panagiotaki,Daniele De Martini*

Main category: cs.LG

TL;DR: 研究语义如何影响自监督对比语义定位框架中的定位性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探究模型是否能够过滤环境噪声并优先考虑显著地标而非常规杂物。

Method: 训练定位网络于原始和扰动地图上，进行后验内省分析，验证多种可解释性方法。

Result: 积分梯度和注意力权重是最可靠的学习行为探针；语义类别消融显示频繁对象常被降权。

Conclusion: 模型学习了噪声鲁棒的语义显著关系，支持在视觉和结构变化下的可解释配准。

Abstract: This work investigates how semantics influence localisation performance and
robustness in a learned self-supervised, contrastive semantic localisation
framework. After training a localisation network on both original and perturbed
maps, we conduct a thorough post-hoc introspection analysis to probe whether
the model filters environmental noise and prioritises distinctive landmarks
over routine clutter. We validate various interpretability methods and present
a comparative reliability analysis. Integrated gradients and Attention Weights
consistently emerge as the most reliable probes of learned behaviour. A
semantic class ablation further reveals an implicit weighting in which frequent
objects are often down-weighted. Overall, the results indicate that the model
learns noise-robust, semantically salient relations about place definition,
thereby enabling explainable registration under challenging visual and
structural variations.

</details>


### [147] [The Framework That Survives Bad Models: Human-AI Collaboration For Clinical Trials](https://arxiv.org/abs/2510.06567)
*Yao Chen,David Ohlssen,Aimee Readie,Gregory Ligozio,Ruvie Martin,Thibaud Coroller*

Main category: cs.LG

TL;DR: AI作为辅助阅读器（AI-SR）在临床试验中表现最佳，即使在模型性能下降时仍能保持可靠性和结论有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在临床试验中的应用潜力及风险，特别是在患者终点评估中的安全性。

Method: 比较两种AI框架与人工评估，测试成本、准确性、鲁棒性和泛化能力，并通过注入不良模型进行压力测试。

Result: AI-SR方法在所有标准下表现最优，能提供可靠的疾病评估并保持试验结论的有效性。

Conclusion: AI-SR是临床试验中最适合的AI应用方式，具有广泛适用性和可靠性。

Abstract: Artificial intelligence (AI) holds great promise for supporting clinical
trials, from patient recruitment and endpoint assessment to treatment response
prediction. However, deploying AI without safeguards poses significant risks,
particularly when evaluating patient endpoints that directly impact trial
conclusions. We compared two AI frameworks against human-only assessment for
medical image-based disease evaluation, measuring cost, accuracy, robustness,
and generalization ability. To stress-test these frameworks, we injected bad
models, ranging from random guesses to naive predictions, to ensure that
observed treatment effects remain valid even under severe model degradation. We
evaluated the frameworks using two randomized controlled trials with endpoints
derived from spinal X-ray images. Our findings indicate that using AI as a
supporting reader (AI-SR) is the most suitable approach for clinical trials, as
it meets all criteria across various model types, even with bad models. This
method consistently provides reliable disease estimation, preserves clinical
trial treatment effect estimates and conclusions, and retains these advantages
when applied to different populations.

</details>


### [148] [Generative World Modelling for Humanoids: 1X World Model Challenge Technical Report](https://arxiv.org/abs/2510.07092)
*Riccardo Mereu,Aidan Scannell,Yuxin Hou,Yi Zhao,Aditya Jitta,Antonio Dominguez,Luigi Acerbi,Amos Storkey,Paul Chang*

Main category: cs.LG

TL;DR: 1X World Model Challenge中，团队通过改进视频生成模型和训练时空Transformer，在采样和压缩任务中均取得第一名。


<details>
  <summary>Details</summary>
Motivation: 探索世界模型在AI和机器人中的应用，通过预测未来视觉观测或潜在状态来增强智能体的推理能力。

Method: 采样任务中，使用Wan-2.2 TI2V-5B模型，结合AdaLN-Zero和LoRA进行微调；压缩任务中，从头训练时空Transformer模型。

Result: 采样任务PSNR达23.0 dB，压缩任务Top-500 CE为6.6386，两项任务均排名第一。

Conclusion: 改进的视频生成模型和时空Transformer在1X World Model Challenge中表现出色，验证了方法的有效性。

Abstract: World models are a powerful paradigm in AI and robotics, enabling agents to
reason about the future by predicting visual observations or compact latent
states. The 1X World Model Challenge introduces an open-source benchmark of
real-world humanoid interaction, with two complementary tracks: sampling,
focused on forecasting future image frames, and compression, focused on
predicting future discrete latent codes. For the sampling track, we adapt the
video generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned
future frame prediction. We condition the video generation on robot states
using AdaLN-Zero, and further post-train the model using LoRA. For the
compression track, we train a Spatio-Temporal Transformer model from scratch.
Our models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386
in the compression task, securing 1st place in both challenges.

</details>


### [149] [DPA-Net: A Dual-Path Attention Neural Network for Inferring Glycemic Control Metrics from Self-Monitored Blood Glucose Data](https://arxiv.org/abs/2510.06623)
*Canyu Lei,Benjamin Lobo,Jianxin Xie*

Main category: cs.LG

TL;DR: 提出了一种双路径注意力神经网络（DPA-Net），用于从稀疏的自我血糖监测（SMBG）数据中估计动态血糖指标（AGP），以解决连续血糖监测（CGM）成本高、普及性低的问题。


<details>
  <summary>Details</summary>
Motivation: CGM成本高且普及性低，而SMBG虽然便宜但数据稀疏，难以转化为临床有用的指标。因此，需要一种方法从SMBG数据中准确估计AGP指标。

Method: DPA-Net结合了空间通道注意力路径（重建CGM轨迹）和多尺度ResNet路径（直接预测AGP指标），并引入对齐机制减少偏差和过拟合。还开发了主动点选择器优化采样点。

Result: 在真实数据集上，DPA-Net表现出高准确性和低误差，减少了系统性偏差。

Conclusion: DPA-Net是首个从SMBG数据估计AGP指标的监督学习框架，为无法使用CGM的场景提供了实用的临床决策支持工具。

Abstract: Continuous glucose monitoring (CGM) provides dense and dynamic glucose
profiles that enable reliable estimation of Ambulatory Glucose Profile (AGP)
metrics, such as Time in Range (TIR), Time Below Range (TBR), and Time Above
Range (TAR). However, the high cost and limited accessibility of CGM restrict
its widespread adoption, particularly in low- and middle-income regions. In
contrast, self-monitoring of blood glucose (SMBG) is inexpensive and widely
available but yields sparse and irregular data that are challenging to
translate into clinically meaningful glycemic metrics.
  In this work, we propose a Dual-Path Attention Neural Network (DPA-Net) to
estimate AGP metrics directly from SMBG data. DPA-Net integrates two
complementary paths: (1) a spatial-channel attention path that reconstructs a
CGM-like trajectory from sparse SMBG observations, and (2) a multi-scale ResNet
path that directly predicts AGP metrics. An alignment mechanism between the two
paths is introduced to reduce bias and mitigate overfitting. In addition, we
develop an active point selector to identify realistic and informative SMBG
sampling points that reflect patient behavioral patterns.
  Experimental results on a large, real-world dataset demonstrate that DPA-Net
achieves robust accuracy with low errors while reducing systematic bias. To the
best of our knowledge, this is the first supervised machine learning framework
for estimating AGP metrics from SMBG data, offering a practical and clinically
relevant decision-support tool in settings where CGM is not accessible.

</details>


### [150] [ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL](https://arxiv.org/abs/2510.07151)
*Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: ELMUR是一种带有结构化外部记忆的Transformer架构，通过双向交叉注意力和LRU记忆模块扩展有效视野，显著提升部分可观测环境下的决策性能。


<details>
  <summary>Details</summary>
Motivation: 现实机器人需在部分可观测和长视野下决策，但现有方法依赖瞬时信息，难以处理长期依赖。

Method: ELMUR结合外部记忆层，通过双向交叉注意力和LRU模块更新记忆，支持长视野决策。

Result: 在合成任务中扩展视野达100,000倍，POPGym和MIKASA-Robo任务中显著超越基线。

Conclusion: 结构化外部记忆为部分可观测决策提供了简单且可扩展的解决方案。

Abstract: Real-world robotic agents must act under partial observability and long
horizons, where key cues may appear long before they affect decision making.
However, most modern approaches rely solely on instantaneous information,
without incorporating insights from the past. Standard recurrent or transformer
models struggle with retaining and leveraging long-term dependencies: context
windows truncate history, while naive memory extensions fail under scale and
sparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a
transformer architecture with structured external memory. Each layer maintains
memory embeddings, interacts with them via bidirectional cross-attention, and
updates them through an Least Recently Used (LRU) memory module using
replacement or convex blending. ELMUR extends effective horizons up to 100,000
times beyond the attention window and achieves a 100% success rate on a
synthetic T-Maze task with corridors up to one million steps. In POPGym, it
outperforms baselines on more than half of the tasks. On MIKASA-Robo
sparse-reward manipulation tasks with visual observations, it nearly doubles
the performance of strong baselines. These results demonstrate that structured,
layer-local external memory offers a simple and scalable approach to decision
making under partial observability.

</details>


### [151] [POME: Post Optimization Model Edit via Muon-style Projection](https://arxiv.org/abs/2510.06627)
*Yong Liu,Di Fu,Yang Luo,Zirui Zhu,Minhao Cheng,Cho-Jui Hsieh,Yang You*

Main category: cs.LG

TL;DR: POME是一种无需额外数据或优化的后处理算法，通过截断SVD提升微调后大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 提升微调后模型的性能，同时避免额外数据或优化步骤。

Method: 对微调与预训练权重差异进行截断SVD投影，平衡主导更新方向并修剪噪声。

Result: 在GSM8K和代码生成任务上分别提升2.5%和1.0%的性能。

Conclusion: POME是一种零成本、广泛适用的后处理增强方法。

Abstract: We introduce Post-Optimization Model Edit (POME), a new algorithm that
enhances the performance of fine-tuned large language models using only their
pretrained and fine-tuned checkpoints, without requiring extra data or further
optimization. The core idea is to apply a muon-style projection to $\Delta W$,
the difference between the fine-tuned and pretrained weights. This projection
uses truncated singular value decomposition (SVD) to equalize the influence of
dominant update directions and prune small singular values, which often
represent noise. As a simple post-processing step, POME is completely decoupled
from the training pipeline. It requires zero modifications and imposes no
overhead, making it universally compatible with any optimizer or distributed
framework. POME delivers consistent gains, boosting average performance by
+2.5\% on GSM8K and +1.0\% on code generation. Its broad applicability -- from
7B foundation models to 72B RLHF-instructed models -- establishes it as a
practical, zero-cost enhancement for any fine-tuning pipeline. Code is
available at https://github.com/NUS-HPC-AI-Lab/POME.

</details>


### [152] [AI-Driven Forecasting and Monitoring of Urban Water System](https://arxiv.org/abs/2510.06631)
*Qiming Guo,Bishal Khatri,Hua Zhang,Wenlu Wang*

Main category: cs.LG

TL;DR: 提出了一种结合AI和远程传感器的框架HydroNet，用于高效检测地下水管泄漏，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 地下水管泄漏等问题导致水资源浪费和环境破坏，传统检测方法效率低且成本高。

Method: 部署稀疏远程传感器收集实时数据，结合HydroNet模型（利用管道属性构建有向图）进行高精度建模。

Result: 在真实校园污水网络数据上验证，HydroNet表现优于先进基线方法。

Conclusion: 该方法可扩展到其他地下水管网络，实现高效泄漏检测。

Abstract: Underground water and wastewater pipelines are vital for city operations but
plagued by anomalies like leaks and infiltrations, causing substantial water
loss, environmental damage, and high repair costs. Conventional manual
inspections lack efficiency, while dense sensor deployments are prohibitively
expensive. In recent years, artificial intelligence has advanced rapidly and is
increasingly applied to urban infrastructure. In this research, we propose an
integrated AI and remote-sensor framework to address the challenge of leak
detection in underground water pipelines, through deploying a sparse set of
remote sensors to capture real-time flow and depth data, paired with HydroNet -
a dedicated model utilizing pipeline attributes (e.g., material, diameter,
slope) in a directed graph for higher-precision modeling. Evaluations on a
real-world campus wastewater network dataset demonstrate that our system
collects effective spatio-temporal hydraulic data, enabling HydroNet to
outperform advanced baselines. This integration of edge-aware message passing
with hydraulic simulations enables accurate network-wide predictions from
limited sensor deployments. We envision that this approach can be effectively
extended to a wide range of underground water pipeline networks.

</details>


### [153] [Chem-NMF: Multi-layer $α$-divergence Non-Negative Matrix Factorization for Cardiorespiratory Disease Clustering, with Improved Convergence Inspired by Chemical Catalysts and Rigorous Asymptotic Analysis](https://arxiv.org/abs/2510.06632)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: cs.LG

TL;DR: 提出了一种基于化学反应的Boltzmann概率理论的新型NMF方法Chem-NMF，通过引入边界因子稳定收敛，提高了聚类精度。


<details>
  <summary>Details</summary>
Motivation: 多层NMF架构在优化中难以确保收敛，需要新的理论方法来解决这一问题。

Method: 受化学反应中能量势垒的Boltzmann概率启发，提出Chem-NMF方法，并引入边界因子以稳定收敛。

Result: 实验表明，Chem-NMF在生物医学信号和面部图像上的聚类精度分别提高了5.6%±2.7%和11.1%±7.2%。

Conclusion: Chem-NMF是首个从物理化学角度分析NMF收敛行为的研究，显著提升了性能。

Abstract: Non-Negative Matrix Factorization (NMF) is an unsupervised learning method
offering low-rank representations across various domains such as audio
processing, biomedical signal analysis, and image recognition. The
incorporation of $\alpha$-divergence in NMF formulations enhances flexibility
in optimization, yet extending these methods to multi-layer architectures
presents challenges in ensuring convergence. To address this, we introduce a
novel approach inspired by the Boltzmann probability of the energy barriers in
chemical reactions to theoretically perform convergence analysis. We introduce
a novel method, called Chem-NMF, with a bounding factor which stabilizes
convergence. To our knowledge, this is the first study to apply a physical
chemistry perspective to rigorously analyze the convergence behaviour of the
NMF algorithm. We start from mathematically proven asymptotic convergence
results and then show how they apply to real data. Experimental results
demonstrate that the proposed algorithm improves clustering accuracy by 5.6%
$\pm$ 2.7% on biomedical signals and 11.1% $\pm$ 7.2% on face images (mean
$\pm$ std).

</details>


### [154] [Three Forms of Stochastic Injection for Improved Distribution-to-Distribution Generative Modeling](https://arxiv.org/abs/2510.06634)
*Shiye Su,Yuhui Zhang,Linqi Zhou,Rajesh Ranganath,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 提出了一种改进的流匹配方法，用于解决分布到分布转换中的稀疏监督问题，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 研究分布到分布转换的科学挑战，特别是在源分布样本有限的情况下，标准流匹配方法表现不佳。

Method: 通过扰动源样本和流插值注入随机性，提出了一种简单且计算高效的方法。

Result: 在五个不同的成像任务中，生成质量显著提升，平均FID分数提高9分，同时降低了传输成本。

Conclusion: 该方法使流匹配成为科学中分布转换的更实用工具，突出了转换的真实效果。

Abstract: Modeling transformations between arbitrary data distributions is a
fundamental scientific challenge, arising in applications like drug discovery
and evolutionary simulation. While flow matching offers a natural framework for
this task, its use has thus far primarily focused on the noise-to-data setting,
while its application in the general distribution-to-distribution setting is
underexplored. We find that in the latter case, where the source is also a data
distribution to be learned from limited samples, standard flow matching fails
due to sparse supervision. To address this, we propose a simple and
computationally efficient method that injects stochasticity into the training
process by perturbing source samples and flow interpolants. On five diverse
imaging tasks spanning biology, radiology, and astronomy, our method
significantly improves generation quality, outperforming existing baselines by
an average of 9 FID points. Our approach also reduces the transport cost
between input and generated samples to better highlight the true effect of the
transformation, making flow matching a more practical tool for simulating the
diverse distribution transformations that arise in science.

</details>


### [155] [StruSR: Structure-Aware Symbolic Regression with Physics-Informed Taylor Guidance](https://arxiv.org/abs/2510.06635)
*Yunpeng Gong,Sihan Lan,Can Yang,Kunpeng Xu,Min Jiang*

Main category: cs.LG

TL;DR: StruSR是一个结构感知的符号回归框架，利用PINNs提取物理先验，通过遗传编程优化符号表达式，提升收敛速度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法缺乏从时间序列数据中提取结构化物理先验的机制，难以捕捉全局行为。

Method: 利用PINNs提取局部泰勒展开的导数信息，通过掩码机制评估子树重要性，结合遗传编程优化表达式。

Result: 在基准PDE系统上，StruSR在收敛速度、结构保真度和可解释性上优于传统方法。

Conclusion: StruSR为基于物理的符号发现提供了原则性范式。

Abstract: Symbolic regression aims to find interpretable analytical expressions by
searching over mathematical formula spaces to capture underlying system
behavior, particularly in scientific modeling governed by physical laws.
However, traditional methods lack mechanisms for extracting structured physical
priors from time series observations, making it difficult to capture symbolic
expressions that reflect the system's global behavior. In this work, we propose
a structure-aware symbolic regression framework, called StruSR, that leverages
trained Physics-Informed Neural Networks (PINNs) to extract locally structured
physical priors from time series data. By performing local Taylor expansions on
the outputs of the trained PINN, we obtain derivative-based structural
information to guide symbolic expression evolution. To assess the importance of
expression components, we introduce a masking-based attribution mechanism that
quantifies each subtree's contribution to structural alignment and physical
residual reduction. These sensitivity scores steer mutation and crossover
operations within genetic programming, preserving substructures with high
physical or structural significance while selectively modifying less
informative components. A hybrid fitness function jointly minimizes physics
residuals and Taylor coefficient mismatch, ensuring consistency with both the
governing equations and the local analytical behavior encoded by the PINN.
Experiments on benchmark PDE systems demonstrate that StruSR improves
convergence speed, structural fidelity, and expression interpretability
compared to conventional baselines, offering a principled paradigm for
physics-grounded symbolic discovery.

</details>


### [156] [Control-Augmented Autoregressive Diffusion for Data Assimilation](https://arxiv.org/abs/2510.06637)
*Prakhar Srivastava,Farrin Marouf Sofian,Francesco Immorlano,Kushagra Pandey,Stephan Mandt*

Main category: cs.LG

TL;DR: 提出了一种轻量级控制器网络框架，用于增强预训练的自动回归扩散模型（ARDMs），在数据同化任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏观测下计算成本高且易产生预测漂移，需要更高效的解决方案。

Method: 通过离线训练控制器网络，预览未来ARDMs的展开并学习逐步控制，避免推理时的昂贵计算。

Result: 在两个典型PDE和六种观测模式下，方法在稳定性、准确性和物理保真度上优于四种基线。

Conclusion: 该方法显著提升了数据同化的效率和性能，代码和模型将公开。

Abstract: Despite recent advances in test-time scaling and finetuning of diffusion
models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains
underexplored. We introduce an amortized framework that augments pretrained
ARDMs with a lightweight controller network, trained offline by previewing
future ARDM rollouts and learning stepwise controls that anticipate upcoming
observations under a terminal cost objective. We evaluate this framework in the
context of data assimilation (DA) for chaotic spatiotemporal partial
differential equations (PDEs), a setting where existing methods are often
computationally prohibitive and prone to forecast drift under sparse
observations. Our approach reduces DA inference to a single forward rollout
with on-the-fly corrections, avoiding expensive adjoint computations and/or
optimizations during inference. We demonstrate that our method consistently
outperforms four state-of-the-art baselines in stability, accuracy, and
physical fidelity across two canonical PDEs and six observation regimes. We
will release code and checkpoints publicly.

</details>


### [157] [The False Promise of Zero-Shot Super-Resolution in Machine-Learned Operators](https://arxiv.org/abs/2510.06646)
*Mansi Sakarvadia,Kareem Hegazy,Amin Totounferoush,Kyle Chard,Yaoqing Yang,Ian Foster,Michael W. Mahoney*

Main category: cs.LG

TL;DR: 论文评估了机器学习操作器（MLOs）在零样本超分辨率任务中的表现，发现其无法有效处理多分辨率推理，并提出了一种改进的训练协议。


<details>
  <summary>Details</summary>
Motivation: 解决科学机器学习中连续现象离散化建模的核心挑战，评估MLOs在零样本超分辨率任务中的能力。

Method: 将多分辨率推理分解为频率信息外推和分辨率插值两个行为，通过实验验证MLOs的局限性。

Result: MLOs在零样本多分辨率推理中表现不佳，易受混叠效应影响。

Conclusion: 提出了一种简单、高效的数据驱动多分辨率训练协议，以提升MLOs的鲁棒性和泛化能力。

Abstract: A core challenge in scientific machine learning, and scientific computing
more generally, is modeling continuous phenomena which (in practice) are
represented discretely. Machine-learned operators (MLOs) have been introduced
as a means to achieve this modeling goal, as this class of architecture can
perform inference at arbitrary resolution. In this work, we evaluate whether
this architectural innovation is sufficient to perform "zero-shot
super-resolution," namely to enable a model to serve inference on
higher-resolution data than that on which it was originally trained. We
comprehensively evaluate both zero-shot sub-resolution and super-resolution
(i.e., multi-resolution) inference in MLOs. We decouple multi-resolution
inference into two key behaviors: 1) extrapolation to varying frequency
information; and 2) interpolating across varying resolutions. We empirically
demonstrate that MLOs fail to do both of these tasks in a zero-shot manner.
Consequently, we find MLOs are not able to perform accurate inference at
resolutions different from those on which they were trained, and instead they
are brittle and susceptible to aliasing. To address these failure modes, we
propose a simple, computationally-efficient, and data-driven multi-resolution
training protocol that overcomes aliasing and that provides robust
multi-resolution generalization.

</details>


### [158] [Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions](https://arxiv.org/abs/2510.06649)
*Frank Wu,Mengye Ren*

Main category: cs.LG

TL;DR: 论文提出了一种基于Forward-Forward算法的ARQ方法，用于强化学习中的价值估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Forward-Forward算法目前主要局限于监督学习，而强化学习领域需要更自然的学习信号。

Method: 提出ARQ方法，结合层活动统计的好函数和动作条件，通过时序差分学习实现局部强化学习。

Result: 在MinAtar和DeepMind Control Suite基准测试中表现优于现有方法，甚至超过基于反向传播的算法。

Conclusion: ARQ方法简单且具有生物学基础，在强化学习中表现出色。

Abstract: The Forward-Forward (FF) Algorithm is a recently proposed learning procedure
for neural networks that employs two forward passes instead of the traditional
forward and backward passes used in backpropagation. However, FF remains
largely confined to supervised settings, leaving a gap at domains where
learning signals can be yielded more naturally such as RL. In this work,
inspired by FF's goodness function using layer activity statistics, we
introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value
estimation method that applies a goodness function and action conditioning for
local RL using temporal difference learning. Despite its simplicity and
biological grounding, our approach achieves superior performance compared to
state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind
Control Suite benchmarks, while also outperforming algorithms trained with
backpropagation on most tasks. Code can be found at
https://github.com/agentic-learning-ai-lab/arq.

</details>


### [159] [Rethinking Nonlinearity: Trainable Gaussian Mixture Modules for Modern Neural Architectures](https://arxiv.org/abs/2510.06660)
*Weiguo Lu,Gangnan Yuan,Hong-kun Zhang,Shangyang Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于高斯混合模型的新型非线性模块GMNM，用于增强神经网络的非线性能力，并在多种架构中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络的非线性能力受限于激活函数的选择，限制了模型的性能提升。

Method: 通过放松高斯混合模型的概率约束并灵活参数化高斯投影，设计了可微分的GMNM模块，可集成到多种神经网络中。

Result: 实验表明，GMNM在MLPs、CNNs、注意力机制和LSTMs中均能提升性能。

Conclusion: GMNM是一种灵活且强大的模块，可广泛应用于提升机器学习模型的效率和准确性。

Abstract: Neural networks in general, from MLPs and CNNs to attention-based
Transformers, are constructed from layers of linear combinations followed by
nonlinear operations such as ReLU, Sigmoid, or Softmax. Despite their strength,
these conventional designs are often limited in introducing non-linearity by
the choice of activation functions. In this work, we introduce Gaussian
Mixture-Inspired Nonlinear Modules (GMNM), a new class of differentiable
modules that draw on the universal density approximation Gaussian mixture
models (GMMs) and distance properties (metric space) of Gaussian kernal. By
relaxing probabilistic constraints and adopting a flexible parameterization of
Gaussian projections, GMNM can be seamlessly integrated into diverse neural
architectures and trained end-to-end with gradient-based methods. Our
experiments demonstrate that incorporating GMNM into architectures such as
MLPs, CNNs, attention mechanisms, and LSTMs consistently improves performance
over standard baselines. These results highlight GMNM's potential as a powerful
and flexible module for enhancing efficiency and accuracy across a wide range
of machine learning applications.

</details>


### [160] [The Effect of Attention Head Count on Transformer Approximation](https://arxiv.org/abs/2510.06662)
*Penghao Yu,Haotian Jiang,Zeyu Bao,Ruoxi Yu,Qianxiao Li*

Main category: cs.LG

TL;DR: 论文研究了Transformer结构中注意力头数量对表达能力的影响，提出了广义$D$-检索任务，并证明了其密集性。通过上下界分析，揭示了多头和单头Transformer的参数复杂度差异，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer结构参数（尤其是注意力头数量）如何影响其表达能力，填补理论研究的空白。

Method: 引入广义$D$-检索任务，分析其密集性；建立参数复杂度的上下界；研究单头情况下的输入记忆能力。

Result: 多头Transformer可实现高效逼近，而头数不足时参数复杂度至少为$O(1/\epsilon^{cT})$；单头情况下需$O(T)$维嵌入实现记忆。

Conclusion: 注意力头数量对Transformer表达能力至关重要，理论分析为实际应用提供了指导。

Abstract: Transformer has become the dominant architecture for sequence modeling, yet a
detailed understanding of how its structural parameters influence expressive
power remains limited. In this work, we study the approximation properties of
transformers, with particular emphasis on the role of the number of attention
heads. Our analysis begins with the introduction of a generalized $D$-retrieval
task, which we prove to be dense in the space of continuous functions, thereby
providing the basis for our theoretical framework. We then establish both upper
and lower bounds on the parameter complexity required for
$\epsilon$-approximation. Specifically, we show that transformers with
sufficiently many heads admit efficient approximation, whereas with too few
heads, the number of parameters must scale at least as $O(1/\epsilon^{cT})$,
for some constant $c$ and sequence length $T$. To the best of our knowledge,
this constitutes the first rigorous lower bound of this type in a nonlinear and
practically relevant setting. We further examine the single-head case and
demonstrate that an embedding dimension of order $O(T)$ allows complete
memorization of the input, where approximation is entirely achieved by the
feed-forward block. Finally, we validate our theoretical findings with
experiments on both synthetic data and real-world tasks, illustrating the
practical relevance of our results.

</details>


### [161] [XRPO: Pushing the limits of GRPO with Targeted Exploration and Exploitation](https://arxiv.org/abs/2510.06672)
*Udbhav Bamba,Minghao Fang,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: XRPO是一种强化学习框架，通过自适应探索-利用机制提升LLM推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索挑战性提示和利用反馈信号方面存在不足，限制了模型性能的提升。

Method: XRPO引入自适应rollout分配器和上下文种子策略，结合新颖的优势锐化机制。

Result: 在数学和编程基准测试中，XRPO性能提升达4% pass@1和6% cons@32，训练速度加快2.7倍。

Conclusion: XRPO通过探索-利用平衡显著提升了LLM推理能力，为强化学习优化提供了新思路。

Abstract: Reinforcement learning algorithms such as GRPO have driven recent advances in
large language model (LLM) reasoning. While scaling the number of rollouts
stabilizes training, existing approaches suffer from limited exploration on
challenging prompts and leave informative feedback signals underexploited, due
to context-independent rollout allocation across prompts (e.g., generating 16
rollouts per prompt) and relying heavily on sparse rewards. This paper presents
XRPO(eXplore - eXploit GRPO), a unified framework that recasts policy
optimization through the principled lens of rollout exploration-exploitation.
To enhance exploration, XRPO introduces a mathematically grounded rollout
allocator that adaptively prioritizes prompts with higher potential for
uncertainty reduction. It further addresses stagnation on zero-reward prompts
through an in-context seeding strategy that injects curated exemplars, steering
the model into more difficult reasoning trajectories. To strengthen
exploitation, XRPO develops a group-relative, novelty-aware advantage
sharpening mechanism that leverages sequence likelihoods to amplify
low-probability yet correct responses, thereby extending the policy's reach
beyond sparse rewards. Experiments across diverse math and coding benchmarks on
both reasoning and non-reasoning models demonstrate that XRPO outperforms
existing advances (e.g., GRPO and GSPO) up to 4% pass@1 and 6% cons@32, while
accelerating training convergence by up to 2.7X.

</details>


### [162] [TimeFormer: Transformer with Attention Modulation Empowered by Temporal Characteristics for Time Series Forecasting](https://arxiv.org/abs/2510.06680)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Baixin Li,Yongsheng Huang,Mingyang Geng,Changsheng Zhang,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: TimeFormer是一种新型Transformer架构，专为时间序列数据设计，通过改进注意力机制和引入多尺度分析，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在时间序列预测中表现不佳，主要因为未充分考虑时间序列与文本数据的差异。本文旨在解决这一问题。

Method: 提出TimeFormer，采用带有调制项的自注意力机制（MoSA）和多尺度分析框架，以捕捉时间序列的时序特性。

Result: 实验表明，TimeFormer在多个数据集上表现优异，MSE降低7.45%，并在94.04%的指标上刷新了基准。

Conclusion: TimeFormer不仅提升了预测性能，其MoSA机制还可泛化应用于其他Transformer模型。

Abstract: Although Transformers excel in natural language processing, their extension
to time series forecasting remains challenging due to insufficient
consideration of the differences between textual and temporal modalities. In
this paper, we develop a novel Transformer architecture designed for time
series data, aiming to maximize its representational capacity. We identify two
key but often overlooked characteristics of time series: (1) unidirectional
influence from the past to the future, and (2) the phenomenon of decaying
influence over time. These characteristics are introduced to enhance the
attention mechanism of Transformers. We propose TimeFormer, whose core
innovation is a self-attention mechanism with two modulation terms (MoSA),
designed to capture these temporal priors of time series under the constraints
of the Hawkes process and causal masking. Additionally, TimeFormer introduces a
framework based on multi-scale and subsequence analysis to capture semantic
dependencies at different temporal scales, enriching the temporal dependencies.
Extensive experiments conducted on multiple real-world datasets show that
TimeFormer significantly outperforms state-of-the-art methods, achieving up to
a 7.45% reduction in MSE compared to the best baseline and setting new
benchmarks on 94.04\% of evaluation metrics. Moreover, we demonstrate that the
MoSA mechanism can be broadly applied to enhance the performance of other
Transformer-based models.

</details>


### [163] [Distributed Algorithms for Multi-Agent Multi-Armed Bandits with Collision](https://arxiv.org/abs/2510.06683)
*Daoyuan Zhou,Xuchuang Wang,Lin Yang,Yang Gao*

Main category: cs.LG

TL;DR: 研究了分布式多玩家多臂老虎机问题，提出了一种高效的通信协议算法，显著降低了遗憾值。


<details>
  <summary>Details</summary>
Motivation: 解决多玩家在多臂老虎机问题中的碰撞和通信成本问题，提升个体和群体性能。

Method: 提出了一种分布式算法，采用自适应通信协议，通信成本仅为O(log log T)。

Result: 实验表明，算法在个体遗憾值上显著优于现有方法，且通信成本低。

Conclusion: 算法在分布式和异步环境下均表现优异，扩展性强。

Abstract: We study the stochastic Multiplayer Multi-Armed Bandit (MMAB) problem, where
multiple players select arms to maximize their cumulative rewards. Collisions
occur when two or more players select the same arm, resulting in no reward, and
are observed by the players involved. We consider a distributed setting without
central coordination, where each player can only observe their own actions and
collision feedback. We propose a distributed algorithm with an adaptive,
efficient communication protocol. The algorithm achieves near-optimal group and
individual regret, with a communication cost of only $\mathcal{O}(\log\log T)$.
Our experiments demonstrate significant performance improvements over existing
baselines. Compared to state-of-the-art (SOTA) methods, our approach achieves a
notable reduction in individual regret. Finally, we extend our approach to a
periodic asynchronous setting, proving the lower bound for this problem and
presenting an algorithm that achieves logarithmic regret.

</details>


### [164] [AutoBalance: An Automatic Balancing Framework for Training Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06684)
*Kang An,Chenhao Si,Ming Yan,Shiqian Ma*

Main category: cs.LG

TL;DR: AutoBalance是一种新的训练范式，通过为每个损失组件分配独立的优化器，显著提升了PINNs的性能。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs训练方法因需要平衡多个损失项而困难，AutoBalance旨在解决这一问题。

Method: 采用“后结合”策略，为每个损失组件分配独立的自适应优化器，并聚合预处理的更新。

Result: 在多个PDE基准测试中，AutoBalance显著降低了误差（MSE和L∞范数）。

Conclusion: AutoBalance是一种有效且与现有方法互补的训练范式，提升了PINNs的性能。

Abstract: Physics-Informed Neural Networks (PINNs) provide a powerful and general
framework for solving Partial Differential Equations (PDEs) by embedding
physical laws into loss functions. However, training PINNs is notoriously
difficult due to the need to balance multiple loss terms, such as PDE residuals
and boundary conditions, which often have conflicting objectives and vastly
different curvatures. Existing methods address this issue by manipulating
gradients before optimization (a "pre-combine" strategy). We argue that this
approach is fundamentally limited, as forcing a single optimizer to process
gradients from spectrally heterogeneous loss landscapes disrupts its internal
preconditioning. In this work, we introduce AutoBalance, a novel "post-combine"
training paradigm. AutoBalance assigns an independent adaptive optimizer to
each loss component and aggregates the resulting preconditioned updates
afterwards. Extensive experiments on challenging PDE benchmarks show that
AutoBalance consistently outperforms existing frameworks, achieving significant
reductions in solution error, as measured by both the MSE and $L^{\infty}$
norms. Moreover, AutoBalance is orthogonal to and complementary with other
popular PINN methodologies, amplifying their effectiveness on demanding
benchmarks.

</details>


### [165] [Is the Hard-Label Cryptanalytic Model Extraction Really Polynomial?](https://arxiv.org/abs/2510.06692)
*Akira Ito,Takayuki Miura,Yosuke Todo*

Main category: cs.LG

TL;DR: 本文提出了一种名为CrossLayer Extraction的新型攻击方法，解决了现有模型提取攻击在深度增加时查询复杂度指数级增长的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型提取攻击在攻击目标深度增加时，假设变得不切实际，导致查询复杂度指数级增长，限制了其实际应用。

Method: 通过利用跨层神经元交互，从更深层提取信息，而非直接提取特定神经元的秘密参数，从而显著降低查询复杂度。

Result: CrossLayer Extraction方法显著减少了查询复杂度，克服了现有模型提取方法的局限性。

Conclusion: CrossLayer Extraction为模型提取攻击提供了一种更高效的方法，尤其在深度网络中表现优越。

Abstract: Deep Neural Networks (DNNs) have attracted significant attention, and their
internal models are now considered valuable intellectual assets. Extracting
these internal models through access to a DNN is conceptually similar to
extracting a secret key via oracle access to a block cipher. Consequently,
cryptanalytic techniques, particularly differential-like attacks, have been
actively explored recently. ReLU-based DNNs are the most commonly and widely
deployed architectures. While early works (e.g., Crypto 2020, Eurocrypt 2024)
assume access to exact output logits, which are usually invisible, more recent
works (e.g., Asiacrypt 2024, Eurocrypt 2025) focus on the hard-label setting,
where only the final classification result (e.g., "dog" or "car") is available
to the attacker. Notably, Carlini et al. (Eurocrypt 2025) demonstrated that
model extraction is feasible in polynomial time even under this restricted
setting.
  In this paper, we first show that the assumptions underlying their attack
become increasingly unrealistic as the attack-target depth grows. In practice,
satisfying these assumptions requires an exponential number of queries with
respect to the attack depth, implying that the attack does not always run in
polynomial time. To address this critical limitation, we propose a novel attack
method called CrossLayer Extraction. Instead of directly extracting the secret
parameters (e.g., weights and biases) of a specific neuron, which incurs
exponential cost, we exploit neuron interactions across layers to extract this
information from deeper layers. This technique significantly reduces query
complexity and mitigates the limitations of existing model extraction
approaches.

</details>


### [166] [A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking](https://arxiv.org/abs/2510.06699)
*Gal Fadlon,Idan Arbiv,Nimrod Berman,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了一种两步框架，结合时间序列补全和视觉扩散模型，高效生成真实时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样和缺失值在时间序列生成中的挑战，避免现有方法因简单掩码导致的“不自然”邻域问题。

Method: 1. 使用时间序列变换器补全不规则序列；2. 结合掩码的视觉扩散模型减少对补全值的依赖。

Result: 在判别分数上相对提升70%，计算成本降低85%，达到SOTA性能。

Conclusion: 该方法通过结合补全和掩码技术，实现了高效且鲁棒的时间序列生成。

Abstract: Generating realistic time series data is critical for applications in
healthcare, finance, and science. However, irregular sampling and missing
values present significant challenges. While prior methods address these
irregularities, they often yield suboptimal results and incur high
computational costs. Recent advances in regular time series generation, such as
the diffusion-based ImagenTime model, demonstrate strong, fast, and scalable
generative capabilities by transforming time series into image representations,
making them a promising solution. However, extending ImagenTime to irregular
sequences using simple masking introduces "unnatural" neighborhoods, where
missing values replaced by zeros disrupt the learning process. To overcome
this, we propose a novel two-step framework: first, a Time Series Transformer
completes irregular sequences, creating natural neighborhoods; second, a
vision-based diffusion model with masking minimizes dependence on the completed
values. This approach leverages the strengths of both completion and masking,
enabling robust and efficient generation of realistic time series. Our method
achieves state-of-the-art performance, achieving a relative improvement in
discriminative score by $70\%$ and in computational cost by $85\%$. Code is at
https://github.com/azencot-group/ImagenI2R.

</details>


### [167] [Dual Goal Representations](https://arxiv.org/abs/2510.06714)
*Seohong Park,Deepinder Mann,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种双目标表示方法，用于目标条件强化学习（GCRL），通过状态间的时序距离关系编码状态，具有理论优势和实践效果。


<details>
  <summary>Details</summary>
Motivation: 传统状态表示可能受限于原始表示形式或包含无关噪声，双目标表示通过时序距离关系提供更鲁棒和动态无关的状态编码。

Method: 提出双目标表示，基于状态间的时序距离关系，开发了一种可结合现有GCRL算法的目标表示学习方法。

Result: 在OGBench任务套件的20个基于状态和像素的任务中，双目标表示显著提升了离线目标达成性能。

Conclusion: 双目标表示在理论和实践中均表现出色，为GCRL提供了一种更优的状态编码方法。

Abstract: In this work, we introduce dual goal representations for goal-conditioned
reinforcement learning (GCRL). A dual goal representation characterizes a state
by "the set of temporal distances from all other states"; in other words, it
encodes a state through its relations to every other state, measured by
temporal distance. This representation provides several appealing theoretical
properties. First, it depends only on the intrinsic dynamics of the environment
and is invariant to the original state representation. Second, it contains
provably sufficient information to recover an optimal goal-reaching policy,
while being able to filter out exogenous noise. Based on this concept, we
develop a practical goal representation learning method that can be combined
with any existing GCRL algorithm. Through diverse experiments on the OGBench
task suite, we empirically show that dual goal representations consistently
improve offline goal-reaching performance across 20 state- and pixel-based
tasks.

</details>


### [168] [Incorporating Expert Knowledge into Bayesian Causal Discovery of Mixtures of Directed Acyclic Graphs](https://arxiv.org/abs/2510.06735)
*Zachris Björkman,Jorge Loría,Sophie Wharrie,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯实验设计的因果启发策略和变分混合结构学习方法，用于异构领域的因果发现。


<details>
  <summary>Details</summary>
Motivation: 异构领域中现有因果启发方法假设单一因果图，无法适应异构需求。

Method: 结合贝叶斯实验设计原则和变分混合结构学习（VaMSL），扩展了DiBS方法，迭代推断因果贝叶斯网络混合物。

Result: 在异构合成数据上表现优异，并能捕捉乳腺癌数据库中的复杂分布。

Conclusion: 方法成功生成替代因果模型集，提升了异构环境下的结构学习性能。

Abstract: Bayesian causal discovery benefits from prior information elicited from
domain experts, and in heterogeneous domains any prior knowledge would be badly
needed. However, so far prior elicitation approaches have assumed a single
causal graph and hence are not suited to heterogeneous domains. We propose a
causal elicitation strategy for heterogeneous settings, based on Bayesian
experimental design (BED) principles, and a variational mixture structure
learning (VaMSL) method -- extending the earlier differentiable Bayesian
structure learning (DiBS) method -- to iteratively infer mixtures of causal
Bayesian networks (CBNs). We construct an informative graph prior incorporating
elicited expert feedback in the inference of mixtures of CBNs. Our proposed
method successfully produces a set of alternative causal models (mixture
components or clusters), and achieves an improved structure learning
performance on heterogeneous synthetic data when informed by a simulated
expert. Finally, we demonstrate that our approach is capable of capturing
complex distributions in a breast cancer database.

</details>


### [169] [Function regression using the forward forward training and inferring paradigm](https://arxiv.org/abs/2510.06762)
*Shivam Padmani,Akshay Joshi*

Main category: cs.LG

TL;DR: 论文提出了一种基于Forward-Forward算法的函数回归新方法，并评估了其在单变量和多变量函数上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络训练依赖反向传播，而Forward-Forward算法是一种无需反向传播的新方法，但目前仅用于分类任务。本文旨在将其扩展到函数回归领域。

Method: 采用Forward-Forward算法进行函数回归，并扩展到Kolmogorov Arnold Networks和Deep Physical Neural Networks。

Result: 在单变量和多变量函数上进行了评估，展示了方法的有效性。

Conclusion: Forward-Forward算法在函数回归任务中具有潜力，为神经网络的训练提供了新的可能性。

Abstract: Function regression/approximation is a fundamental application of machine
learning. Neural networks (NNs) can be easily trained for function regression
using a sufficient number of neurons and epochs. The forward-forward learning
algorithm is a novel approach for training neural networks without
backpropagation, and is well suited for implementation in neuromorphic
computing and physical analogs for neural networks. To the best of the authors'
knowledge, the Forward Forward paradigm of training and inferencing NNs is
currently only restricted to classification tasks. This paper introduces a new
methodology for approximating functions (function regression) using the
Forward-Forward algorithm. Furthermore, the paper evaluates the developed
methodology on univariate and multivariate functions, and provides preliminary
studies of extending the proposed Forward-Forward regression to Kolmogorov
Arnold Networks, and Deep Physical Neural Networks.

</details>


### [170] [Modeling COVID-19 Dynamics in German States Using Physics-Informed Neural Networks](https://arxiv.org/abs/2510.06776)
*Phillip Rothenbeck,Sai Karthikeya Vemuri,Niklas Penzel,Joachim Denzler*

Main category: cs.LG

TL;DR: 使用物理信息神经网络（PINNs）解决SIR模型的逆问题，分析德国各州的COVID-19动态，揭示传播行为与疫苗接种的关联。


<details>
  <summary>Details</summary>
Motivation: COVID-19大流行需要定量建模分析，传统SIR模型难以直接处理噪声数据，PINNs提供了一种新方法。

Method: 利用PINNs结合RKI的感染数据，进行时空分析，估计州级传播和恢复参数及时变再生数（R_t）。

Result: 结果显示各州传播行为差异显著，与疫苗接种率和疫情阶段相关，验证了PINNs在长期流行病建模中的实用性。

Conclusion: PINNs可用于精细化、长期的流行病建模，为公共卫生干预提供数据支持。

Abstract: The COVID-19 pandemic has highlighted the need for quantitative modeling and
analysis to understand real-world disease dynamics. In particular, post hoc
analyses using compartmental models offer valuable insights into the
effectiveness of public health interventions, such as vaccination strategies
and containment policies. However, such compartmental models like SIR
(Susceptible-Infectious-Recovered) often face limitations in directly
incorporating noisy observational data. In this work, we employ
Physics-Informed Neural Networks (PINNs) to solve the inverse problem of the
SIR model using infection data from the Robert Koch Institute (RKI). Our main
contribution is a fine-grained, spatio-temporal analysis of COVID-19 dynamics
across all German federal states over a three-year period. We estimate
state-specific transmission and recovery parameters and time-varying
reproduction number (R_t) to track the pandemic progression. The results
highlight strong variations in transmission behavior across regions, revealing
correlations with vaccination uptake and temporal patterns associated with
major pandemic phases. Our findings demonstrate the utility of PINNs in
localized, long-term epidemiological modeling.

</details>


### [171] [Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness](https://arxiv.org/abs/2510.06790)
*Tavish McDonald,Bo Lei,Stanislav Fort,Bhavya Kailkhura,Brian Bartoldson*

Main category: cs.LG

TL;DR: 论文提出RICH假设，认为推理计算防御的鲁棒性收益与基础模型的鲁棒性相关，并通过实验验证了这一点。


<details>
  <summary>Details</summary>
Motivation: 尽管在训练时投入了大量计算资源以提高模型的鲁棒性，模型仍容易受到对抗性分布外（OOD）数据的攻击。现有研究表明测试时计算可以提高鲁棒性，但在攻击者获取梯度或多模态输入时效果有限。本文旨在填补这一空白。

Method: 提出Robustness from Inference Compute Hypothesis (RICH)，认为推理计算防御的鲁棒性收益与基础模型的鲁棒性相关。通过组合泛化，模型可以理解OOD数据的分布内（ID）成分，从而在对抗性OOD输入上遵循防御规范。

Result: 实验表明，在视觉语言模型和多种攻击类型中，如果组合泛化能够解锁对OOD数据的规范遵循，测试时计算会带来鲁棒性收益。RL微调和长时间推理并非关键因素。

Conclusion: 建议结合训练时和测试时的防御措施以获得协同效益，因为攻击数据的成分对鲁棒化模型更接近ID，有助于组合泛化到OOD数据。

Abstract: Models are susceptible to adversarially out-of-distribution (OOD) data
despite large training-compute investments into their robustification. Zaremba
et al. (2025) make progress on this problem at test time, showing LLM reasoning
improves satisfaction of model specifications designed to thwart attacks,
resulting in a correlation between reasoning effort and robustness to
jailbreaks. However, this benefit of test compute fades when attackers are
given access to gradients or multimodal inputs. We address this gap, clarifying
that inference-compute offers benefits even in such cases. Our approach argues
that compositional generalization, through which OOD data is understandable via
its in-distribution (ID) components, enables adherence to defensive
specifications on adversarially OOD inputs. Namely, we posit the Robustness
from Inference Compute Hypothesis (RICH): inference-compute defenses profit as
the model's training data better reflects the attacked data's components. We
empirically support this hypothesis across vision language model and attack
types, finding robustness gains from test-time compute if specification
following on OOD data is unlocked by compositional generalization, while RL
finetuning and protracted reasoning are not critical. For example, increasing
emphasis on defensive specifications via prompting lowers the success rate of
gradient-based multimodal attacks on VLMs robustified by adversarial
pretraining, but this same intervention provides no such benefit to
not-robustified models. This correlation of inference-compute's robustness
benefit with base model robustness is the rich-get-richer dynamic of the RICH:
attacked data components are more ID for robustified models, aiding
compositional generalization to OOD data. Accordingly, we advise layering
train-time and test-time defenses to obtain their synergistic benefit.

</details>


### [172] [The Unreasonable Effectiveness of Randomized Representations in Online Continual Graph Learning](https://arxiv.org/abs/2510.06819)
*Giovanni Donghi,Daniele Zambon,Luca Pasa,Cesare Alippi,Nicolò Navarin*

Main category: cs.LG

TL;DR: 提出了一种简单有效的方法来解决在线连续图学习中的灾难性遗忘问题，通过固定随机初始化的编码器生成稳定的节点嵌入，仅在线训练轻量级分类器。


<details>
  <summary>Details</summary>
Motivation: 在线连续图学习中，节点逐个到达且分布可能随时漂移，灾难性遗忘是主要障碍。

Method: 使用固定随机初始化的编码器生成节点嵌入，仅在线训练轻量级分类器。

Result: 在多个基准测试中表现优于现有方法，性能提升高达30%，接近离线训练上限。

Conclusion: 通过架构简单性和稳定性，无需复杂回放或正则化即可最小化灾难性遗忘。

Abstract: Catastrophic forgetting is one of the main obstacles for Online Continual
Graph Learning (OCGL), where nodes arrive one by one, distribution drifts may
occur at any time and offline training on task-specific subgraphs is not
feasible. In this work, we explore a surprisingly simple yet highly effective
approach for OCGL: we use a fixed, randomly initialized encoder to generate
robust and expressive node embeddings by aggregating neighborhood information,
training online only a lightweight classifier. By freezing the encoder, we
eliminate drifts of the representation parameters, a key source of forgetting,
obtaining embeddings that are both expressive and stable. When evaluated across
several OCGL benchmarks, despite its simplicity and lack of memory buffer, this
approach yields consistent gains over state-of-the-art methods, with surprising
improvements of up to 30% and performance often approaching that of the joint
offline-training upper bound. These results suggest that in OCGL, catastrophic
forgetting can be minimized without complex replay or regularization by
embracing architectural simplicity and stability.

</details>


### [173] [Efficient numeracy in language models through single-token number embeddings](https://arxiv.org/abs/2510.06824)
*Linus Kreitner,Paul Hager,Jonathan Mengedoht,Georgios Kaissis,Daniel Rueckert,Martin J. Menten*

Main category: cs.LG

TL;DR: 论文提出BitTokens，一种新的数字编码方法，通过IEEE 754二进制浮点表示将数字嵌入单个token，解决了LLMs处理数值数据效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs处理数值数据依赖外部工具或冗长推理链，限制了其数值直觉和问题解决长度。

Method: 提出BitTokens，利用IEEE 754二进制浮点表示将数字编码为单个token。

Result: 实验表明，BitTokens使小型语言模型能近乎完美地学习基本算术运算算法。

Conclusion: BitTokens提高了语言模型处理数值问题的效率，扩展了其可解决问题的长度和复杂度。

Abstract: To drive progress in science and engineering, large language models (LLMs)
must be able to process large amounts of numerical data and solve long
calculations efficiently. This is currently only possible through the use of
external tools or extensive reasoning chains, either limiting the numerical
intuition of LLMs or limiting the length of problems they can solve. We show
that frontier LLMs require excessive amounts of reasoning tokens to solve even
basic calculations, which is exacerbated by their tokenization strategies that
split single numbers into multiple tokens. This motivates the need for
efficient and effective single-token number encodings. We introduce a set of
desiderata for such encodings and show that existing approaches fail to fulfill
them. To address these shortcomings, we propose BitTokens, a novel tokenization
strategy that embeds any number into a single token using its IEEE 754 binary
floating-point representation. Through extensive experiments we show that our
BitTokens allow even small language models to learn algorithms that solve basic
arithmetic operations nearly perfectly. This newly gained efficiency could
expand the length and complexity of problems language models can solve.

</details>


### [174] [Recurrence-Complete Frame-based Action Models](https://arxiv.org/abs/2510.06828)
*Michael Keiblinger*

Main category: cs.LG

TL;DR: 论文挑战了'Attention Is All You Need'的观点，提出完全并行化的架构无法解决某些长期任务问题，并引入了一种新的递归完备架构。


<details>
  <summary>Details</summary>
Motivation: 动机是证明注意力机制单独使用时无法处理某些长期任务，尤其是需要时间聚合的任务。

Method: 方法是通过引入递归完备架构，并在GitHub动作序列上进行训练。

Result: 结果显示，训练序列长度遵循幂律分布，且参数数量固定；更长序列的训练总能摊销其线性增加的耗时成本。

Conclusion: 结论是递归完备架构在处理长期任务时优于纯注意力机制。

Abstract: In recent years, attention-like mechanisms have been used to great success in
the space of large language models, unlocking scaling potential to a previously
unthinkable extent. "Attention Is All You Need" famously claims RNN cells are
not needed in conjunction with attention. We challenge this view. In this
paper, we point to existing proofs that architectures with fully parallelizable
forward or backward passes cannot represent classes of problems specifically
interesting for long-running agentic tasks. We further conjecture a critical
time t beyond which non-recurrence-complete models fail to aggregate inputs
correctly, with concrete implications for agentic systems (e.g., software
engineering agents). To address this, we introduce a recurrence-complete
architecture and train it on GitHub-derived action sequences. Loss follows a
power law in the trained sequence length while the parameter count remains
fixed. Moreover, longer-sequence training always amortizes its linearly
increasing wall-time cost, yielding lower loss as a function of wall time.

</details>


### [175] [Early wind turbine alarm prediction based on machine learning: AlarmForecasting](https://arxiv.org/abs/2510.06831)
*Syed Shazaib Shah,Daoliang Tan*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM的警报预测与分类框架（AFC），旨在预防风力涡轮机故障，而非仅诊断。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅将警报数据用于诊断，本研究旨在通过预测警报来预防故障。

Method: AFC框架包含两个模块：基于LSTM的时间序列回归模块和警报分类模块。

Result: 在14台风机5年数据上，10、20、30分钟的预测准确率分别为82%、52%、41%。

Conclusion: AFC能有效减少警报频率，提升运行效率。

Abstract: Alarm data is pivotal in curbing fault behavior in Wind Turbines (WTs) and
forms the backbone for advancedpredictive monitoring systems. Traditionally,
research cohorts have been confined to utilizing alarm data solelyas a
diagnostic tool, merely indicative of unhealthy status. However, this study
aims to offer a transformativeleap towards preempting alarms, preventing alarms
from triggering altogether, and consequently avertingimpending failures. Our
proposed Alarm Forecasting and Classification (AFC) framework is designed on
twosuccessive modules: first, the regression module based on long short-term
memory (LSTM) for time-series alarmforecasting, and thereafter, the
classification module to implement alarm tagging on the forecasted alarm.
Thisway, the entire alarm taxonomy can be forecasted reliably rather than a few
specific alarms. 14 Senvion MM82turbines with an operational period of 5 years
are used as a case study; the results demonstrated 82%, 52%,and 41% accurate
forecasts for 10, 20, and 30 min alarm forecasts, respectively. The results
substantiateanticipating and averting alarms, which is significant in curbing
alarm frequency and enhancing operationalefficiency through proactive
intervention.

</details>


### [176] [Vectorized FlashAttention with Low-cost Exponential Computation in RISC-V Vector Processors](https://arxiv.org/abs/2510.06834)
*Vasileios Titopoulos,Kosmas Alexandridis,Giorgos Dimitrakopoulos*

Main category: cs.LG

TL;DR: 论文提出了一种在RISC-V向量处理器上加速FlashAttention核的方法，通过低成本的指数近似和分块策略优化性能。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是机器学习和AI模型的核心操作，但现有方法在向量处理器上的效率有待提升。

Method: 采用低成本浮点指数近似和分块策略，减少标量代码并简化softmax计算复杂度。

Result: 实验结果表明，向量化实现显著提升了注意力层的处理性能。

Conclusion: 该方法在RISC-V架构上高效实现了FlashAttention，为注意力机制的加速提供了新思路。

Abstract: Attention is a core operation in numerous machine learning and artificial
intelligence models. This work focuses on the acceleration of attention kernel
using FlashAttention algorithm, in vector processors, particularly those based
on the RISC-V instruction set architecture (ISA). This work represents the
first effort to vectorize FlashAttention, minimizing scalar code and
simplifying the computational complexity of evaluating exponentials needed by
softmax used in attention. By utilizing a low-cost approximation for
exponentials in floating-point arithmetic, we reduce the cost of computing the
exponential function without the need to extend baseline vector ISA with new
custom instructions. Also, appropriate tiling strategies are explored with the
goal to improve memory locality. Experimental results highlight the scalability
of our approach, demonstrating significant performance gains with the
vectorized implementations when processing attention layers in practical
applications.

</details>


### [177] [CNN-TFT explained by SHAP with multi-head attention weights for time series forecasting](https://arxiv.org/abs/2510.06840)
*Stefano F. Stefenon,João P. Matos-Carvalho,Valderi R. Q. Leithardt,Kin-Choong Yow*

Main category: cs.LG

TL;DR: 提出了一种结合CNN和TFT的混合架构（CNN-TFT-SHAP-MHAW），用于多变量时间序列预测，效果优于现有深度学习模型。


<details>
  <summary>Details</summary>
Motivation: CNN擅长捕捉局部模式和平移不变性，而Transformer能有效建模长程依赖关系，结合两者优势以提升预测性能。

Method: 使用CNN模块提取局部特征，再通过TFT捕捉短长期依赖关系，并引入SHAP-MHAW增强模型可解释性。

Result: 在水利自然流量数据集上，CNN-TFT的平均绝对百分比误差低至2.2%，表现优于其他模型。

Conclusion: CNN-TFT-SHAP-MHAW架构在多变量时间序列预测中具有高精度和可解释性，适用于高保真需求的应用。

Abstract: Convolutional neural networks (CNNs) and transformer architectures offer
strengths for modeling temporal data: CNNs excel at capturing local patterns
and translational invariances, while transformers effectively model long-range
dependencies via self-attention. This paper proposes a hybrid architecture
integrating convolutional feature extraction with a temporal fusion transformer
(TFT) backbone to enhance multivariate time series forecasting. The CNN module
first applies a hierarchy of one-dimensional convolutional layers to distill
salient local patterns from raw input sequences, reducing noise and
dimensionality. The resulting feature maps are then fed into the TFT, which
applies multi-head attention to capture both short- and long-term dependencies
and to weigh relevant covariates adaptively. We evaluate the CNN-TFT on a
hydroelectric natural flow time series dataset. Experimental results
demonstrate that CNN-TFT outperforms well-established deep learning models,
with a mean absolute percentage error of up to 2.2%. The explainability of the
model is obtained by a proposed Shapley additive explanations with multi-head
attention weights (SHAP-MHAW). Our novel architecture, named CNN-TFT-SHAP-MHAW,
is promising for applications requiring high-fidelity, multivariate time series
forecasts, being available for future analysis at
https://github.com/SFStefenon/CNN-TFT-SHAP-MHAW .

</details>


### [178] [Enhancing Bankruptcy Prediction of Banks through Advanced Machine Learning Techniques: An Innovative Approach and Analysis](https://arxiv.org/abs/2510.06852)
*Zuherman Rustam,Sri Hartini,Sardar M. N. Islam,Fevi Novkaniza,Fiftitah R. Aszhari,Muhammad Rifqi*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习（如随机森林、逻辑回归和支持向量机）的银行破产预测模型，相比传统统计方法更准确。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法（如Altman's Z-Score）假设僵化且预测精度低，需要更有效的银行破产预测方法以确保金融系统稳定。

Method: 使用机器学习技术（LR、RF、SVM）分析土耳其和印尼的银行数据，预测破产概率。

Result: 随机森林对商业银行数据的预测准确率达90%，三种方法均能有效预测农村银行破产趋势。

Conclusion: 机器学习方法为降低破产成本的政策制定提供了创新支持。

Abstract: Context: Financial system stability is determined by the condition of the
banking system. A bank failure can destroy the stability of the financial
system, as banks are subject to systemic risk, affecting not only individual
banks but also segments or the entire financial system. Calculating the
probability of a bank going bankrupt is one way to ensure the banking system is
safe and sound. Existing literature and limitations: Statistical models, such
as Altman's Z-Score, are one of the common techniques for developing a
bankruptcy prediction model. However, statistical methods rely on rigid and
sometimes irrelevant assumptions, which can result in low forecast accuracy.
New approaches are necessary. Objective of the research: Bankruptcy models are
developed using machine learning techniques, such as logistic regression (LR),
random forest (RF), and support vector machines (SVM). According to several
studies, machine learning is also more accurate and effective than statistical
methods for categorising and forecasting banking risk management. Present
Research: The commercial bank data are derived from the annual financial
statements of 44 active banks and 21 bankrupt banks in Turkey from 1994 to
2004, and the rural bank data are derived from the quarterly financial reports
of 43 active and 43 bankrupt rural banks in Indonesia between 2013 and 2019.
Five rural banks in Indonesia have also been selected to demonstrate the
feasibility of analysing bank bankruptcy trends. Findings and implications: The
results of the research experiments show that RF can forecast data from
commercial banks with a 90% accuracy rate. Furthermore, the three machine
learning methods proposed accurately predict the likelihood of rural bank
bankruptcy. Contribution and Conclusion: The proposed innovative machine
learning approach help to implement policies that reduce the costs of
bankruptcy.

</details>


### [179] [Towards Generalization of Graph Neural Networks for AC Optimal Power Flow](https://arxiv.org/abs/2510.06860)
*Olayiwola Arowolo,Jochen L. Cremer*

Main category: cs.LG

TL;DR: HH-MPNN模型通过混合异构消息传递神经网络解决ACOPF问题，实现高计算速度和拓扑适应性。


<details>
  <summary>Details</summary>
Motivation: 传统ACOPF求解器计算成本高，机器学习方法在可扩展性和拓扑适应性上存在不足。

Method: 提出HH-MPNN，将电网组件建模为不同节点或边类型，结合可扩展的Transformer模型处理长程依赖。

Result: 在14到2000个总线的电网上，HH-MPNN在默认拓扑下最优性差距小于1%，在未见拓扑下小于3%，计算速度提升1000x至10000x。

Conclusion: HH-MPNN为实时电网操作提供了实用且通用的机器学习解决方案。

Abstract: AC Optimal Power Flow (ACOPF) is computationally expensive for large-scale
power systems, with conventional solvers requiring prohibitive solution times.
Machine learning approaches offer computational speedups but struggle with
scalability and topology adaptability without expensive retraining. To enable
scalability across grid sizes and adaptability to topology changes, we propose
a Hybrid Heterogeneous Message Passing Neural Network (HH-MPNN). HH-MPNN models
buses, generators, loads, shunts, transmission lines and transformers as
distinct node or edge types, combined with a scalable transformer model for
handling long-range dependencies. On grids from 14 to 2,000 buses, HH-MPNN
achieves less than 1% optimality gap on default topologies. Applied zero-shot
to thousands of unseen topologies, HH-MPNN achieves less than 3% optimality gap
despite training only on default topologies. Pre-training on smaller grids also
improves results on a larger grid. Computational speedups reach 1,000x to
10,000x compared to interior point solvers. These results advance practical,
generalizable machine learning for real-time power system operations.

</details>


### [180] [SaFeR-VLM: Toward Safety-aware Fine-grained Reasoning in Multimodal Models](https://arxiv.org/abs/2510.06871)
*Huahui Yi,Kun Wang,Qiankun Li,Miao Yu,Liang Lin,Gongli Xi,Hao Wu,Xuming Hu,Kang Li,Yang Liu*

Main category: cs.LG

TL;DR: SaFeR-VLM是一个安全对齐的强化学习框架，通过嵌入安全到多模态推理中，提升模型的安全性和帮助性。


<details>
  <summary>Details</summary>
Motivation: 现有防御主要在输出层面，未约束推理过程，导致模型仍面临隐性风险。

Method: 提出SaFeR-VLM框架，包含四个组件：QI-Safe-10K数据集、安全感知生成、结构化奖励建模和GRPO优化。

Result: SaFeR-VLM在多个基准测试中表现优异，超越更大规模模型，且不牺牲帮助性。

Conclusion: SaFeR-VLM将安全从被动保护转变为主动驱动推理，实现可扩展和通用的安全感知推理。

Abstract: Multimodal Large Reasoning Models (MLRMs) demonstrate impressive cross-modal
reasoning but often amplify safety risks under adversarial or unsafe prompts, a
phenomenon we call the \textit{Reasoning Tax}. Existing defenses mainly act at
the output level and do not constrain the reasoning process, leaving models
exposed to implicit risks. In this paper, we propose SaFeR-VLM, a
safety-aligned reinforcement learning framework that embeds safety directly
into multimodal reasoning. The framework integrates four components: (I)
QI-Safe-10K, a curated dataset emphasizing safety-critical and
reasoning-sensitive cases; (II) safety-aware rollout, where unsafe generations
undergo reflection and correction instead of being discarded; (III) structured
reward modeling with multi-dimensional weighted criteria and explicit penalties
for hallucinations and contradictions; and (IV) GRPO optimization, which
reinforces both safe and corrected trajectories. This unified design shifts
safety from a passive safeguard to an active driver of reasoning, enabling
scalable and generalizable safety-aware reasoning. SaFeR-VLM further
demonstrates robustness against both explicit and implicit risks, supporting
dynamic and interpretable safety decisions beyond surface-level filtering.
SaFeR-VLM-3B achieves average performance $70.13$ and $78.97$ on safety and
helpfulness across six benchmarks, surpassing both same-scale and $>10\times$
larger models such as Skywork-R1V3-38B, Qwen2.5VL-72B, and GLM4.5V-106B.
Remarkably, SaFeR-VLM-7B benefits from its increased scale to surpass
GPT-5-mini and Gemini-2.5-Flash by \num{6.47} and \num{16.76} points
respectively on safety metrics, achieving this improvement without any
degradation in helpfulness performance. Our codes are available at
https://github.com/HarveyYi/SaFeR-VLM.

</details>


### [181] [MoRE-GNN: Multi-omics Data Integration with a Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2510.06880)
*Zhiyu Wang,Sonia Koszut,Pietro Liò,Francesco Ceccarelli*

Main category: cs.LG

TL;DR: MoRE-GNN是一种用于多组学单细胞数据整合的异构图自编码器，通过动态构建关系图提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多组学单细胞数据整合的高维性和复杂模态间关系问题。

Method: 结合图卷积和注意力机制，动态构建关系图。

Result: 在六个公开数据集上表现优于现有方法，尤其在强模态相关性场景下。

Conclusion: MoRE-GNN为多组学整合提供了自适应、可扩展且可解释的框架。

Abstract: The integration of multi-omics single-cell data remains challenging due to
high-dimensionality and complex inter-modality relationships. To address this,
we introduce MoRE-GNN (Multi-omics Relational Edge Graph Neural Network), a
heterogeneous graph autoencoder that combines graph convolution and attention
mechanisms to dynamically construct relational graphs directly from data.
Evaluations on six publicly available datasets demonstrate that MoRE-GNN
captures biologically meaningful relationships and outperforms existing
methods, particularly in settings with strong inter-modality correlations.
Furthermore, the learned representations allow for accurate downstream
cross-modal predictions. While performance may vary with dataset complexity,
MoRE-GNN offers an adaptive, scalable and interpretable framework for advancing
multi-omics integration.

</details>


### [182] [Angular Constraint Embedding via SpherePair Loss for Constrained Clustering](https://arxiv.org/abs/2510.06907)
*Shaojie Zhang,Ke Chen*

Main category: cs.LG

TL;DR: SpherePair提出了一种新的角度约束嵌入方法，解决了现有深度约束聚类方法的局限性，提升了可扩展性和实际应用性。


<details>
  <summary>Details</summary>
Motivation: 现有深度约束聚类方法受限于端到端建模的锚点或难以学习判别性欧几里得嵌入，限制了其可扩展性和实际应用性。

Method: 使用SpherePair损失函数，通过几何公式编码成对约束，在角度空间中生成聚类友好的嵌入，将表示学习与聚类分离。

Result: SpherePair在多样基准测试中表现优于现有方法，具有理论保证，并展示了优越的性能和可扩展性。

Conclusion: SpherePair是一种高效、可扩展且适用于实际场景的深度约束聚类方法。

Abstract: Constrained clustering integrates domain knowledge through pairwise
constraints. However, existing deep constrained clustering (DCC) methods are
either limited by anchors inherent in end-to-end modeling or struggle with
learning discriminative Euclidean embedding, restricting their scalability and
real-world applicability. To avoid their respective pitfalls, we propose a
novel angular constraint embedding approach for DCC, termed SpherePair. Using
the SpherePair loss with a geometric formulation, our method faithfully encodes
pairwise constraints and leads to embeddings that are clustering-friendly in
angular space, effectively separating representation learning from clustering.
SpherePair preserves pairwise relations without conflict, removes the need to
specify the exact number of clusters, generalizes to unseen data, enables rapid
inference of the number of clusters, and is supported by rigorous theoretical
guarantees. Comparative evaluations with state-of-the-art DCC methods on
diverse benchmarks, along with empirical validation of theoretical insights,
confirm its superior performance, scalability, and overall real-world
effectiveness. Code is available at
\href{https://github.com/spherepaircc/SpherePairCC/tree/main}{our repository}.

</details>


### [183] [Vacuum Spiker: A Spiking Neural Network-Based Model for Efficient Anomaly Detection in Time Series](https://arxiv.org/abs/2510.06910)
*Iago Xabier Vázquez,Javier Sedano,Muhammad Afzal,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于脉冲神经网络的Vacuum Spiker算法，用于时间序列异常检测，显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在异常检测中表现优异，但高能耗限制了其在资源受限环境中的应用。

Method: 采用脉冲神经网络，引入新的检测标准和高效的编码方案，通过脉冲时间依赖可塑性训练。

Result: 在公开数据集上表现优异，能耗显著降低，并在实际案例中验证了实用性。

Conclusion: Vacuum Spiker算法为可持续且高效的异常检测提供了潜在解决方案。

Abstract: Anomaly detection is a key task across domains such as industry, healthcare,
and cybersecurity. Many real-world anomaly detection problems involve analyzing
multiple features over time, making time series analysis a natural approach for
such problems. While deep learning models have achieved strong performance in
this field, their trend to exhibit high energy consumption limits their
deployment in resource-constrained environments such as IoT devices, edge
computing platforms, and wearables. To address this challenge, this paper
introduces the \textit{Vacuum Spiker algorithm}, a novel Spiking Neural
Network-based method for anomaly detection in time series. It incorporates a
new detection criterion that relies on global changes in neural activity rather
than reconstruction or prediction error. It is trained using Spike
Time-Dependent Plasticity in a novel way, intended to induce changes in neural
activity when anomalies occur. A new efficient encoding scheme is also
proposed, which discretizes the input space into non-overlapping intervals,
assigning each to a single neuron. This strategy encodes information with a
single spike per time step, improving energy efficiency compared to
conventional encoding methods. Experimental results on publicly available
datasets show that the proposed algorithm achieves competitive performance
while significantly reducing energy consumption, compared to a wide set of deep
learning and machine learning baselines. Furthermore, its practical utility is
validated in a real-world case study, where the model successfully identifies
power curtailment events in a solar inverter. These results highlight its
potential for sustainable and efficient anomaly detection.

</details>


### [184] [Utilizing Large Language Models for Machine Learning Explainability](https://arxiv.org/abs/2510.06912)
*Alexandros Vassiliades,Nikolaos Polatidis,Stamatios Samaras,Sotiris Diplaris,Ignacio Cabrera Martin,Yannis Manolopoulos,Stefanos Vrochidis,Ioannis Kompatsiaris*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型（LLMs）在自动生成机器学习解决方案时的可解释性能力，结果显示LLMs能生成高效且可解释的模型。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动生成机器学习解决方案时的可解释性能力，以验证其作为自动化工具在可解释ML流程生成中的潜力。

Method: 使用三种先进LLMs（OpenAI GPT、Anthropic Claude、DeepSeek）为两种分类任务（二元分类和多标签分类）设计训练流程，并评估模型性能和SHAP可解释性指标。

Result: LLMs生成的模型在预测性能和可解释性（高保真度和一致性稀疏性）方面表现优异，接近人工设计的基线。

Conclusion: LLMs具备生成高效且可解释的机器学习流程的能力，展示了其在自动化可解释ML流程生成中的潜力。

Abstract: This study explores the explainability capabilities of large language models
(LLMs), when employed to autonomously generate machine learning (ML) solutions.
We examine two classification tasks: (i) a binary classification problem
focused on predicting driver alertness states, and (ii) a multilabel
classification problem based on the yeast dataset. Three state-of-the-art LLMs
(i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design
training pipelines for four common classifiers: Random Forest, XGBoost,
Multilayer Perceptron, and Long Short-Term Memory networks. The generated
models are evaluated in terms of predictive performance (recall, precision, and
F1-score) and explainability using SHAP (SHapley Additive exPlanations).
Specifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP
approximations and model outputs) and Average SHAP Sparsity (number of features
deemed influential). The results reveal that LLMs are capable of producing
effective and interpretable models, achieving high fidelity and consistent
sparsity, highlighting their potential as automated tools for interpretable ML
pipeline generation. The results show that LLMs can produce effective,
interpretable pipelines with high fidelity and consistent sparsity, closely
matching manually engineered baselines.

</details>


### [185] [Revisiting Node Affinity Prediction in Temporal Graphs](https://arxiv.org/abs/2510.06940)
*Krishna Sri Ipsit Mantri,Or Feldman,Moshe Eliasof,Chaim Baskin*

Main category: cs.LG

TL;DR: NAViS模型通过虚拟状态和新型损失函数，在节点亲和性预测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有动态链接预测模型在节点亲和性预测任务中表现不佳，甚至被简单启发式方法超越。

Method: 提出NAViS模型，利用启发式方法与状态空间模型的等价性，并引入新型损失函数。

Result: 在TGB数据集上，NAViS优于现有方法，包括启发式方法。

Conclusion: NAViS通过改进训练方法和损失函数，显著提升了节点亲和性预测的性能。

Abstract: Node affinity prediction is a common task that is widely used in temporal
graph learning with applications in social and financial networks, recommender
systems, and more. Recent works have addressed this task by adapting
state-of-the-art dynamic link property prediction models to node affinity
prediction. However, simple heuristics, such as Persistent Forecast or Moving
Average, outperform these models. In this work, we analyze the challenges in
training current Temporal Graph Neural Networks for node affinity prediction
and suggest appropriate solutions. Combining the solutions, we develop NAViS -
Node Affinity prediction model using Virtual State, by exploiting the
equivalence between heuristics and state space models. While promising,
training NAViS is non-trivial. Therefore, we further introduce a novel loss
function for node affinity prediction. We evaluate NAViS on TGB and show that
it outperforms the state-of-the-art, including heuristics. Our source code is
available at https://github.com/orfeld415/NAVIS

</details>


### [186] [Fisher Information, Training and Bias in Fourier Regression Models](https://arxiv.org/abs/2510.06945)
*Lorenzo Pastori,Veronika Eyring,Mierk Schwabe*

Main category: cs.LG

TL;DR: 研究量子神经网络（QNNs）中基于Fisher信息矩阵（FIM）的评估指标如何预测其训练和预测性能，探讨有效维度和模型偏置对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习（特别是量子神经网络）的兴起，促使研究者探索FIM评估指标的有效性。

Method: 利用QNNs与傅里叶模型的等价性，分析有效维度和偏置的相互作用，推导FIM的解析表达式，并构建可调模型进行比较。

Result: 对于无偏模型，高有效维度可能提升性能；对于有偏模型，低有效维度更有利。

Conclusion: 研究揭示了几何特性、模型-任务对齐与训练之间的相互作用，对机器学习领域有广泛意义。

Abstract: Motivated by the growing interest in quantum machine learning, in particular
quantum neural networks (QNNs), we study how recently introduced evaluation
metrics based on the Fisher information matrix (FIM) are effective for
predicting their training and prediction performance. We exploit the
equivalence between a broad class of QNNs and Fourier models, and study the
interplay between the \emph{effective dimension} and the \emph{bias} of a model
towards a given task, investigating how these affect the model's training and
performance. We show that for a model that is completely agnostic, or unbiased,
towards the function to be learned, a higher effective dimension likely results
in a better trainability and performance. On the other hand, for models that
are biased towards the function to be learned a lower effective dimension is
likely beneficial during training. To obtain these results, we derive an
analytical expression of the FIM for Fourier models and identify the features
controlling a model's effective dimension. This allows us to construct models
with tunable effective dimension and bias, and to compare their training. We
furthermore introduce a tensor network representation of the considered Fourier
models, which could be a tool of independent interest for the analysis of QNN
models. Overall, these findings provide an explicit example of the interplay
between geometrical properties, model-task alignment and training, which are
relevant for the broader machine learning community.

</details>


### [187] [Grouped Differential Attention](https://arxiv.org/abs/2510.06949)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Wai Ting Cheung,Beomgyu Kim,Taehwan Kim,Haesol Lee,Junhyeok Lee,Dongpin Oh,Eunhwan Park*

Main category: cs.LG

TL;DR: GDA通过不平衡的头分配提升信号聚焦，减少噪声干扰，提高Transformer架构的效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制中冗余或噪声上下文的问题，提升信号保真度和计算效率。

Method: 提出Grouped Differential Attention（GDA），通过不平衡的头分配策略，将更多头分配给信号提取，较少分配给噪声控制。

Result: GDA在预训练和持续训练实验中表现出更好的泛化能力和稳定性。

Conclusion: GDA为设计高效、可扩展的Transformer架构提供了有效路径。

Abstract: The self-attention mechanism, while foundational to modern Transformer
architectures, suffers from a critical inefficiency: it frequently allocates
substantial attention to redundant or noisy context. Differential Attention
addressed this by using subtractive attention maps for signal and noise, but
its required balanced head allocation imposes rigid constraints on
representational flexibility and scalability.
  To overcome this, we propose Grouped Differential Attention (GDA), a novel
approach that introduces unbalanced head allocation between signal-preserving
and noise-control groups. GDA significantly enhances signal focus by
strategically assigning more heads to signal extraction and fewer to
noise-control, stabilizing the latter through controlled repetition (akin to
GQA). This design achieves stronger signal fidelity with minimal computational
overhead. We further extend this principle to group-differentiated growth, a
scalable strategy that selectively replicates only the signal-focused heads,
thereby ensuring efficient capacity expansion.
  Through large-scale pretraining and continual training experiments, we
demonstrate that moderate imbalance ratios in GDA yield substantial
improvements in generalization and stability compared to symmetric baselines.
Our results collectively establish that ratio-aware head allocation and
selective expansion offer an effective and practical path toward designing
scalable, computation-efficient Transformer architectures.

</details>


### [188] [From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics](https://arxiv.org/abs/2510.06954)
*Zheng-An Chen,Tao Luo*

Main category: cs.LG

TL;DR: 论文研究了Transformer训练动力学的两阶段理论框架，揭示了注意力模块的动态行为。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型表现出卓越的实证性能，但其训练动力学的基本原理缺乏系统性研究。

Method: 采用梯度流分析框架，研究线性化Transformer的训练动力学，将注意力模块动态分为两阶段。

Result: 第一阶段权重扰动维持梯度动态，第二阶段键查询矩阵参与训练导致秩崩溃。

Conclusion: 两阶段框架推广了经典的方向收敛结果。

Abstract: Although transformer-based models have shown exceptional empirical
performance, the fundamental principles governing their training dynamics are
inadequately characterized beyond configuration-specific studies. Inspired by
empirical evidence showing improved reasoning capabilities under small
initialization scales in language models, we employ the gradient flow
analytical framework established in [Zhou et al. NeurIPS 2022] to
systematically investigate linearized Transformer training dynamics. Our
theoretical analysis dissects the dynamics of attention modules into two
distinct stages. In the first stage, asymmetric weight perturbations from
random initialization sustain non-degenerate gradient dynamics in parameter
matrices, facilitating systematic escape from small initialization regimes.
Subsequently, these matrices undergo condensation, progressively aligning
toward the target orientation. In the second stage, the previously static
key-query matrices actively participate in training, driving the normalized
matrices toward asymptotic rank collapse. This two-stage framework generalizes
classical directional convergence results.

</details>


### [189] [High-Rate Mixout: Revisiting Mixout for Robust Domain Generalization](https://arxiv.org/abs/2510.06955)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: Mixout是一种替代Dropout的随机正则化技术，通过高概率掩码交换微调权重与预训练权重，提升域泛化性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统集成方法计算成本高，而Dropout在预训练模型上容易过正则化，Mixout旨在平衡适应性和保留预训练知识。

Method: Mixout在训练中随机交换微调权重与预训练权重，高掩码概率（ViT 0.9，ResNet 0.8）提升泛化并减少计算开销。

Result: 在五个域泛化基准测试中，Mixout性能媲美集成方法，同时减少梯度计算45%和内存使用90%。

Conclusion: Mixout是一种高效且有效的域泛化方法，显著降低训练成本并保持高性能。

Abstract: Ensembling fine-tuned models initialized from powerful pre-trained weights is
a common strategy to improve robustness under distribution shifts, but it comes
with substantial computational costs due to the need to train and store
multiple models. Dropout offers a lightweight alternative by simulating
ensembles through random neuron deactivation; however, when applied to
pre-trained models, it tends to over-regularize and disrupt critical
representations necessary for generalization. In this work, we investigate
Mixout, a stochastic regularization technique that provides an alternative to
Dropout for domain generalization. Rather than deactivating neurons, Mixout
mitigates overfitting by probabilistically swapping a subset of fine-tuned
weights with their pre-trained counterparts during training, thereby
maintaining a balance between adaptation and retention of prior knowledge. Our
study reveals that achieving strong performance with Mixout on domain
generalization benchmarks requires a notably high masking probability of 0.9
for ViTs and 0.8 for ResNets. While this may seem like a simple adjustment, it
yields two key advantages for domain generalization: (1) higher masking rates
more strongly penalize deviations from the pre-trained parameters, promoting
better generalization to unseen domains; and (2) high-rate masking
substantially reduces computational overhead, cutting gradient computation by
up to 45% and gradient memory usage by up to 90%. Experiments across five
domain generalization benchmarks, PACS, VLCS, OfficeHome, TerraIncognita, and
DomainNet, using ResNet and ViT architectures, show that our approach,
High-rate Mixout, achieves out-of-domain accuracy comparable to ensemble-based
methods while significantly reducing training costs.

</details>


### [190] [Revisiting Mixout: An Overlooked Path to Robust Finetuning](https://arxiv.org/abs/2510.06982)
*Masih Aminbeidokhti,Heitor Rapela Medeiros,Eric Granger,Marco Pedersoli*

Main category: cs.LG

TL;DR: GMixout是一种改进的随机正则化方法，通过动态调整权重锚点和掩码频率，提升模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 微调视觉基础模型通常能提高域内准确性，但会牺牲分布偏移下的鲁棒性。本文旨在通过改进Mixout正则化方法来解决这一问题。

Method: 提出GMixout，采用动态指数移动平均锚点和显式重采样频率超参数，通过稀疏核实现高效训练。

Result: 在多个基准测试中，GMixout在域内准确性上超越零样本性能，并在分布偏移下优于Model Soups和其他参数高效微调基线。

Conclusion: GMixout通过动态调整正则化策略，显著提升了模型在分布偏移下的鲁棒性，同时保持了域内性能。

Abstract: Finetuning vision foundation models often improves in-domain accuracy but
comes at the cost of robustness under distribution shift. We revisit Mixout, a
stochastic regularizer that intermittently replaces finetuned weights with
their pretrained reference, through the lens of a single-run, weight-sharing
implicit ensemble. This perspective reveals three key levers that govern
robustness: the \emph{masking anchor}, \emph{resampling frequency}, and
\emph{mask sparsity}. Guided by this analysis, we introduce GMixout, which (i)
replaces the fixed anchor with an exponential moving-average snapshot that
adapts during training, and (ii) regulates masking period via an explicit
resampling-frequency hyperparameter. Our sparse-kernel implementation updates
only a small fraction of parameters with no inference-time overhead, enabling
training on consumer-grade GPUs. Experiments on benchmarks covering covariate
shift, corruption, and class imbalance, ImageNet / ImageNet-LT, DomainNet,
iWildCam, and CIFAR100-C, GMixout consistently improves in-domain accuracy
beyond zero-shot performance while surpassing both Model Soups and strong
parameter-efficient finetuning baselines under distribution shift.

</details>


### [191] [Spiral Model Technique For Data Science & Machine Learning Lifecycle](https://arxiv.org/abs/2510.06987)
*Rohith Mahadevan*

Main category: cs.LG

TL;DR: 提出了一种新的螺旋技术，用于将数据科学生命周期应用于有明确目标的商业问题。


<details>
  <summary>Details</summary>
Motivation: 现代商业中，数据科学生命周期对提升生产力和竞争力至关重要，但传统方法多为线性或循环模型，缺乏灵活性。

Method: 引入螺旋技术，强调多功能性、敏捷性和迭代方法。

Result: 螺旋技术为商业流程提供了更灵活和高效的解决方案。

Conclusion: 螺旋技术是数据科学生命周期的一种创新方法，适用于有明确目标的商业问题。

Abstract: Analytics play an important role in modern business. Companies adapt data
science lifecycles to their culture to seek productivity and improve their
competitiveness among others. Data science lifecycles are fairly an important
contributing factor to start and end a project that are data dependent. Data
science and Machine learning life cycles comprises of series of steps that are
involved in a project. A typical life cycle states that it is a linear or
cyclical model that revolves around. It is mostly depicted that it is possible
in a traditional data science life cycle to start the process again after
reaching the end of cycle. This paper suggests a new technique to incorporate
data science life cycle to business problems that have a clear end goal. A new
technique called spiral technique is introduced to emphasize versatility,
agility and iterative approach to business processes.

</details>


### [192] [Sharpness-Aware Data Generation for Zero-shot Quantization](https://arxiv.org/abs/2510.07018)
*Dung Hoang-Anh,Cuong Pham Trung Le,Jianfei Cai,Thanh-Toan Do*

Main category: cs.LG

TL;DR: 提出一种考虑量化模型锐度的零样本量化方法，通过梯度匹配提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本量化方法未考虑量化模型的锐度，而低锐度模型通常具有更好的泛化能力。

Method: 通过最大化合成数据与真实验证数据的梯度匹配来最小化锐度，并在无真实数据时用样本间梯度匹配近似。

Result: 在CIFAR-100和ImageNet数据集上，低比特量化设置中优于现有技术。

Conclusion: 考虑锐度的合成数据生成方法能有效提升量化模型的泛化性能。

Abstract: Zero-shot quantization aims to learn a quantized model from a pre-trained
full-precision model with no access to original real training data. The common
idea in zero-shot quantization approaches is to generate synthetic data for
quantizing the full-precision model. While it is well-known that deep neural
networks with low sharpness have better generalization ability, none of the
previous zero-shot quantization works considers the sharpness of the quantized
model as a criterion for generating training data. This paper introduces a
novel methodology that takes into account quantized model sharpness in
synthetic data generation to enhance generalization. Specifically, we first
demonstrate that sharpness minimization can be attained by maximizing gradient
matching between the reconstruction loss gradients computed on synthetic and
real validation data, under certain assumptions. We then circumvent the problem
of the gradient matching without real validation set by approximating it with
the gradient matching between each generated sample and its neighbors.
Experimental evaluations on CIFAR-100 and ImageNet datasets demonstrate the
superiority of the proposed method over the state-of-the-art techniques in
low-bit quantization settings.

</details>


### [193] [Federated Unlearning in the Wild: Rethinking Fairness and Data Discrepancy](https://arxiv.org/abs/2510.07022)
*ZiHeng Huang,Di Wu,Jun Bai,Jiale Zhang,Sicong Cao,Ji Zhang,Yingjie Hu*

Main category: cs.LG

TL;DR: 该论文提出了一种公平感知的联邦遗忘学习（FedCCCU）方法，解决了现有方法在现实数据异质性和公平性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的遗忘学习（FU）存在公平性和现实数据异质性问题，现有方法在这些方面表现不佳。

Method: 提出了一种名为FedCCCU的新方法，通过联邦跨客户端约束遗忘学习，解决了公平性和现实数据异质性问题。

Result: 实验表明，现有方法在现实场景中表现不佳，而FedCCCU方法显著优于现有方法。

Conclusion: FedCCCU为现实世界中的联邦遗忘学习提供了一种实用且可扩展的解决方案。

Abstract: Machine unlearning is critical for enforcing data deletion rights like the
"right to be forgotten." As a decentralized paradigm, Federated Learning (FL)
also requires unlearning, but realistic implementations face two major
challenges. First, fairness in Federated Unlearning (FU) is often overlooked.
Exact unlearning methods typically force all clients into costly retraining,
even those uninvolved. Approximate approaches, using gradient ascent or
distillation, make coarse interventions that can unfairly degrade performance
for clients with only retained data. Second, most FU evaluations rely on
synthetic data assumptions (IID/non-IID) that ignore real-world heterogeneity.
These unrealistic benchmarks obscure the true impact of unlearning and limit
the applicability of current methods. We first conduct a comprehensive
benchmark of existing FU methods under realistic data heterogeneity and
fairness conditions. We then propose a novel, fairness-aware FU approach,
Federated Cross-Client-Constrains Unlearning (FedCCCU), to explicitly address
both challenges. FedCCCU offers a practical and scalable solution for
real-world FU. Experimental results show that existing methods perform poorly
in realistic settings, while our approach consistently outperforms them.

</details>


### [194] [Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration](https://arxiv.org/abs/2510.07035)
*Tengwei Song,Min Wu,Yuan Fang*

Main category: cs.LG

TL;DR: FlexMol是一种灵活的分子预训练框架，支持单模态输入，解决了现有方法需要配对2D和3D数据的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要配对的2D和3D分子数据，限制了在数据缺失或计算成本高时的应用。

Method: 采用分离的2D和3D模型，参数共享提高效率，解码器生成缺失模态特征。

Result: FlexMol在分子性质预测任务中表现优异，且对不完整数据有效。

Conclusion: FlexMol为分子表示学习提供了灵活且高效的解决方案。

Abstract: Molecular representation learning plays a crucial role in advancing
applications such as drug discovery and material design. Existing work
leverages 2D and 3D modalities of molecular information for pre-training,
aiming to capture comprehensive structural and geometric insights. However,
these methods require paired 2D and 3D molecular data to train the model
effectively and prevent it from collapsing into a single modality, posing
limitations in scenarios where a certain modality is unavailable or
computationally expensive to generate. To overcome this limitation, we propose
FlexMol, a flexible molecule pre-training framework that learns unified
molecular representations while supporting single-modality input. Specifically,
inspired by the unified structure in vision-language models, our approach
employs separate models for 2D and 3D molecular data, leverages parameter
sharing to improve computational efficiency, and utilizes a decoder to generate
features for the missing modality. This enables a multistage continuous
learning process where both modalities contribute collaboratively during
training, while ensuring robustness when only one modality is available during
inference. Extensive experiments demonstrate that FlexMol achieves superior
performance across a wide range of molecular property prediction tasks, and we
also empirically demonstrate its effectiveness with incomplete data. Our code
and data are available at https://github.com/tewiSong/FlexMol.

</details>


### [195] [COMPASS: A Multi-Turn Benchmark for Tool-Mediated Planning & Preference Optimization](https://arxiv.org/abs/2510.07043)
*Tian Qin,Felix Bai,Ting-Yao Hu,Raviteja Vemulapalli,Hema Swetha Koppula,Zhiyang Xu,Bowen Jin,Mert Cemri,Jiarui Lu,Zirui Wang,Meng Cao*

Main category: cs.LG

TL;DR: COMPASS是一个评估LLM代理在旅行规划任务中优化用户偏好的基准，揭示了现有模型在满足约束和协调多服务任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决LLM代理在复杂规划任务中优化用户偏好和满足硬约束的实际挑战。

Method: 通过构建包含20个国家公园的旅行数据库和工具生态系统，将旅行规划建模为约束优化问题。

Result: 发现模型在满足约束和优化偏好之间存在差距，且在协调多服务任务时表现不佳。

Conclusion: COMPASS为衡量代理在现实任务中的能力提供了直接基准，连接理论与实际应用。

Abstract: Real-world large language model (LLM) agents must master strategic tool use
and user preference optimization through multi-turn interactions to assist
users with complex planning tasks. We introduce COMPASS (Constrained
Optimization through Multi-turn Planning and Strategic Solutions), a benchmark
that evaluates agents on realistic travel-planning scenarios. We cast travel
planning as a constrained preference optimization problem, where agents must
satisfy hard constraints while simultaneously optimizing soft user preferences.
To support this, we build a realistic travel database covering transportation,
accommodation, and ticketing for 20 U.S. National Parks, along with a
comprehensive tool ecosystem that mirrors commercial booking platforms.
Evaluating state-of-the-art models, we uncover two critical gaps: (i) an
acceptable-optimal gap, where agents reliably meet constraints but fail to
optimize preferences, and (ii) a plan-coordination gap, where performance
collapses on multi-service (flight and hotel) coordination tasks, especially
for open-source models. By grounding reasoning and planning in a practical,
user-facing domain, COMPASS provides a benchmark that directly measures an
agent's ability to optimize user preferences in realistic tasks, bridging
theoretical advances with real-world impact.

</details>


### [196] [Enhancing Speech Emotion Recognition via Fine-Tuning Pre-Trained Models and Hyper-Parameter Optimisation](https://arxiv.org/abs/2510.07052)
*Aryan Golbaghi,Shuo Zhou*

Main category: cs.LG

TL;DR: 提出了一种结合预训练表示和自动超参数优化（HPO）的语音情感识别（SER）工作流程，比较了两种HPO策略，并在有限资源下实现了高效性能。


<details>
  <summary>Details</summary>
Motivation: 通过结合预训练模型和HPO策略，提高语音情感识别的效率和准确性，尤其是在有限计算资源下。

Method: 使用SpeechBrain的wav2vec2-base模型作为编码器，比较了GP-BO和TPE两种HPO策略，并在相同搜索空间和试验次数下进行实验。

Result: GP-BO和TPE分别在11分钟和15分钟内达到0.96和0.97的平衡分类准确率（BCA），显著优于网格搜索和AutoSpeech 2020基线。跨语言泛化能力也有所提升。

Conclusion: 高效的HPO结合预训练编码器可以在普通CPU上实现具有竞争力的SER性能。

Abstract: We propose a workflow for speech emotion recognition (SER) that combines
pre-trained representations with automated hyperparameter optimisation (HPO).
Using SpeechBrain wav2vec2-base model fine-tuned on IEMOCAP as the encoder, we
compare two HPO strategies, Gaussian Process Bayesian Optimisation (GP-BO) and
Tree-structured Parzen Estimators (TPE), under an identical four-dimensional
search space and 15-trial budget, with balanced class accuracy (BCA) on the
German EmoDB corpus as the objective. All experiments run on 8 CPU cores with
32 GB RAM. GP-BO achieves 0.96 BCA in 11 minutes, and TPE (Hyperopt
implementation) attains 0.97 in 15 minutes. In contrast, grid search requires
143 trials and 1,680 minutes to exceed 0.9 BCA, and the best AutoSpeech 2020
baseline reports only 0.85 in 30 minutes on GPU. For cross-lingual
generalisation, an EmoDB-trained HPO-tuned model improves zero-shot accuracy by
0.25 on CREMA-D and 0.26 on RAVDESS. Results show that efficient HPO with
pre-trained encoders delivers competitive SER on commodity CPUs. Source code to
this work is available at:
https://github.com/youngaryan/speechbrain-emotion-hpo.

</details>


### [197] [Blind Construction of Angular Power Maps in Massive MIMO Networks](https://arxiv.org/abs/2510.07071)
*Zheng Xing,Junting Chen*

Main category: cs.LG

TL;DR: 论文提出了一种基于无监督学习的角度功率图构建方法，利用大规模MIMO网络中的CSI数据，无需位置标签。


<details>
  <summary>Details</summary>
Motivation: 解决大规模MIMO网络中CSI获取的挑战，传统方法需要位置标签数据，实际中难以实现。

Method: 使用隐马尔可夫模型（HMM）连接移动设备的隐藏轨迹与CSI演化，实现位置估计和角度功率图构建。

Result: 在均匀直线移动和泊松分布基站条件下，定位误差的CRLB可趋近于零；实际网络中平均定位误差为18米。

Conclusion: 无监督方法在缺乏位置标签的情况下仍能有效构建角度功率图，为资源管理提供新思路。

Abstract: Channel state information (CSI) acquisition is a challenging problem in
massive multiple-input multiple-output (MIMO) networks. Radio maps provide a
promising solution for radio resource management by reducing online CSI
acquisition. However, conventional approaches for radio map construction
require location-labeled CSI data, which is challenging in practice. This paper
investigates unsupervised angular power map construction based on large
timescale CSI data collected in a massive MIMO network without location labels.
A hidden Markov model (HMM) is built to connect the hidden trajectory of a
mobile with the CSI evolution of a massive MIMO channel. As a result, the
mobile location can be estimated, enabling the construction of an angular power
map. We show that under uniform rectilinear mobility with Poisson-distributed
base stations (BSs), the Cramer-Rao Lower Bound (CRLB) for localization error
can vanish at any signal-to-noise ratios (SNRs), whereas when BSs are confined
to a limited region, the error remains nonzero even with infinite independent
measurements. Based on reference signal received power (RSRP) data collected in
a real multi-cell massive MIMO network, an average localization error of 18
meters can be achieved although measurements are mainly obtained from a single
serving cell.

</details>


### [198] [HTMformer: Hybrid Time and Multivariate Transformer for Time Series Forecasting](https://arxiv.org/abs/2510.07084)
*Tan Wang,Yun Wei Dong,Tao Zhang,Qi Wang*

Main category: cs.LG

TL;DR: 论文提出了一种结合时间特征和多变量特征的嵌入方法（HTME），并将其与Transformer结合，构建了轻量级预测模型HTMformer，显著提升了预测性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer在时间序列预测中过度依赖时间依赖性，导致计算开销大但性能提升有限。研究发现嵌入方法对模型性能影响显著，因此提出通过多变量特征增强嵌入层的信息捕捉能力。

Method: 设计了HTME提取器，结合轻量级时间特征提取模块和多变量特征提取模块，生成多维嵌入表示。随后将HTME与Transformer结合，构建HTMformer模型。

Result: 在八个真实数据集上的实验表明，HTMformer在准确性和效率上均优于现有基线方法。

Conclusion: 通过增强嵌入层的特征提取能力，HTMformer在保持轻量级的同时显著提升了预测性能，为时间序列预测提供了新思路。

Abstract: Transformer-based methods have achieved impressive results in time series
forecasting. However, existing Transformers still exhibit limitations in
sequence modeling as they tend to overemphasize temporal dependencies. This
incurs additional computational overhead without yielding corresponding
performance gains. We find that the performance of Transformers is highly
dependent on the embedding method used to learn effective representations. To
address this issue, we extract multivariate features to augment the effective
information captured in the embedding layer, yielding multidimensional
embeddings that convey richer and more meaningful sequence representations.
These representations enable Transformer-based forecasters to better understand
the series. Specifically, we introduce Hybrid Temporal and Multivariate
Embeddings (HTME). The HTME extractor integrates a lightweight temporal feature
extraction module with a carefully designed multivariate feature extraction
module to provide complementary features, thereby achieving a balance between
model complexity and performance. By combining HTME with the Transformer
architecture, we present HTMformer, leveraging the enhanced feature extraction
capability of the HTME extractor to build a lightweight forecaster. Experiments
conducted on eight real-world datasets demonstrate that our approach
outperforms existing baselines in both accuracy and efficiency.

</details>


### [199] [Non-Stationary Online Structured Prediction with Surrogate Losses](https://arxiv.org/abs/2510.07086)
*Shinsaku Sakaue,Han Bao,Yuzhou Cao*

Main category: cs.LG

TL;DR: 论文提出了一种在线结构化预测方法，通过动态遗憾分析和代理间隙技术，在非平稳环境中实现了目标损失的有限边界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非平稳环境中无法保证代理遗憾的有限边界，导致性能下降。

Method: 结合在线梯度下降的动态遗憾分析和代理间隙技术，提出新的学习率策略。

Result: 证明了目标损失的边界形式为F_T + C(1 + P_T)，且该边界仅通过F_T和P_T依赖时间T。

Conclusion: 方法在非平稳环境中表现优异，并通过卷积Fenchel-Young损失扩展到更广泛的问题类别。

Abstract: Online structured prediction, including online classification as a special
case, is the task of sequentially predicting labels from input features.
Therein the surrogate regret -- the cumulative excess of the target loss (e.g.,
0-1 loss) over the surrogate loss (e.g., logistic loss) of the fixed best
estimator -- has gained attention, particularly because it often admits a
finite bound independent of the time horizon $T$. However, such guarantees
break down in non-stationary environments, where every fixed estimator may
incur the surrogate loss growing linearly with $T$. We address this by proving
a bound of the form $F_T + C(1 + P_T)$ on the cumulative target loss, where
$F_T$ is the cumulative surrogate loss of any comparator sequence, $P_T$ is its
path length, and $C > 0$ is some constant. This bound depends on $T$ only
through $F_T$ and $P_T$, often yielding much stronger guarantees in
non-stationary environments. Our core idea is to synthesize the dynamic regret
bound of the online gradient descent (OGD) with the technique of exploiting the
surrogate gap. Our analysis also sheds light on a new Polyak-style learning
rate for OGD, which systematically offers target-loss guarantees and exhibits
promising empirical performance. We further extend our approach to a broader
class of problems via the convolutional Fenchel--Young loss. Finally, we prove
a lower bound showing that the dependence on $F_T$ and $P_T$ is tight.

</details>


### [200] [Non-Asymptotic Analysis of Efficiency in Conformalized Regression](https://arxiv.org/abs/2510.07093)
*Yunzhen Yao,Lie He,Michael Gastpar*

Main category: cs.LG

TL;DR: 本文研究了共形预测在回归任务中的效率问题，重点关注预测集长度与理论最优长度的偏差，并提出了非渐近边界。


<details>
  <summary>Details</summary>
Motivation: 共形预测的预测集长度效率通常被视为固定常数，本文旨在量化其与理论最优长度的偏差，并分析不同参数对效率的影响。

Method: 通过随机梯度下降（SGD）训练的共形化分位数和中位数回归，分析预测集长度与理论最优长度的偏差。

Result: 提出了非渐近边界，揭示了训练集大小、校准集大小和误覆盖率对效率的影响，并观察到收敛速率的相变现象。

Conclusion: 结果为数据分配提供了理论指导，以控制预测集长度的超额部分，实证结果验证了理论发现。

Abstract: Conformal prediction provides prediction sets with coverage guarantees. The
informativeness of conformal prediction depends on its efficiency, typically
quantified by the expected size of the prediction set. Prior work on the
efficiency of conformalized regression commonly treats the miscoverage level
$\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds
on the deviation of the prediction set length from the oracle interval length
for conformalized quantile and median regression trained via SGD, under mild
assumptions on the data distribution. Our bounds of order
$\mathcal{O}(1/\sqrt{n} + 1/(\alpha^2 n) + 1/\sqrt{m} + \exp(-\alpha^2 m))$
capture the joint dependence of efficiency on the proper training set size $n$,
the calibration set size $m$, and the miscoverage level $\alpha$. The results
identify phase transitions in convergence rates across different regimes of
$\alpha$, offering guidance for allocating data to control excess prediction
set length. Empirical results are consistent with our theoretical findings.

</details>


### [201] [DPMM-CFL: Clustered Federated Learning via Dirichlet Process Mixture Model Nonparametric Clustering](https://arxiv.org/abs/2510.07132)
*Mariona Jaramillo-Civill,Peng Wu,Pau Closas*

Main category: cs.LG

TL;DR: DPMM-CFL是一种基于Dirichlet Process的聚类联邦学习算法，自动推断聚类数量，提升非IID数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统聚类联邦学习方法需要预先固定聚类数量的问题，适应未知的潜在数据结构。

Method: 使用Dirichlet Process先验进行非参数贝叶斯推断，联合优化聚类数量和客户端分配。

Result: 在Dirichlet和类分割非IID数据上验证了算法的有效性。

Conclusion: DPMM-CFL通过动态推断聚类数量，实现了更好的联邦学习性能。

Abstract: Clustered Federated Learning (CFL) improves performance under non-IID client
heterogeneity by clustering clients and training one model per cluster, thereby
balancing between a global model and fully personalized models. However, most
CFL methods require the number of clusters K to be fixed a priori, which is
impractical when the latent structure is unknown. We propose DPMM-CFL, a CFL
algorithm that places a Dirichlet Process (DP) prior over the distribution of
cluster parameters. This enables nonparametric Bayesian inference to jointly
infer both the number of clusters and client assignments, while optimizing
per-cluster federated objectives. This results in a method where, at each
round, federated updates and cluster inferences are coupled, as presented in
this paper. The algorithm is validated on benchmark datasets under Dirichlet
and class-split non-IID partitions.

</details>


### [202] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出了一种基于状态的多智能体进化搜索框架，用于生成鲁棒的单元测试用例，显著提升了覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有无状态推理方法在多步任务中表现不佳，且任务特定微调在深层推理和长程依赖任务中脆弱。

Method: 结合持久推理状态、对抗性变异和进化保留，通过多智能体进化搜索生成边缘用例。

Result: 在HumanEval和TestGenEvalMini等基准测试中，覆盖率和鲁棒性显著优于无状态基线。

Conclusion: 持久推理状态与进化搜索结合能显著提升单元测试生成效果。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


### [203] [Bridged Clustering for Representation Learning: Semi-Supervised Sparse Bridging](https://arxiv.org/abs/2510.07182)
*Patrick Peixuan Ye,Chen Shani,Ellen Vitercik*

Main category: cs.LG

TL;DR: Bridged Clustering是一种半监督学习框架，通过独立聚类输入X和输出Y，并用少量配对样本学习稀疏、可解释的桥梁，实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习未充分利用输出数据，而密集传输方法缺乏可解释性。Bridged Clustering旨在解决这些问题。

Method: 独立聚类X和Y，用少量配对样本学习稀疏桥梁，新输入通过最近邻输入簇和链接输出簇的质心预测。

Result: 理论证明在有限错误率下算法高效；实验显示在低监督设置下与SOTA竞争，且简单、模型无关。

Conclusion: Bridged Clustering在保持稀疏性和可解释性的同时，高效利用输出数据，适用于低监督场景。

Abstract: We introduce Bridged Clustering, a semi-supervised framework to learn
predictors from any unpaired input $X$ and output $Y$ dataset. Our method first
clusters $X$ and $Y$ independently, then learns a sparse, interpretable bridge
between clusters using only a few paired examples. At inference, a new input
$x$ is assigned to its nearest input cluster, and the centroid of the linked
output cluster is returned as the prediction $\hat{y}$. Unlike traditional SSL,
Bridged Clustering explicitly leverages output-only data, and unlike dense
transport-based methods, it maintains a sparse and interpretable alignment.
Through theoretical analysis, we show that with bounded mis-clustering and
mis-bridging rates, our algorithm becomes an effective and efficient predictor.
Empirically, our method is competitive with SOTA methods while remaining
simple, model-agnostic, and highly label-efficient in low-supervision settings.

</details>


### [204] [Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples](https://arxiv.org/abs/2510.07192)
*Alexandra Souly,Javier Rando,Ed Chapman,Xander Davies,Burak Hasircioglu,Ezzeldin Shereen,Carlos Mougan,Vasilios Mavroudis,Erik Jones,Chris Hicks,Nicholas Carlini,Yarin Gal,Robert Kirk*

Main category: cs.LG

TL;DR: 研究表明，大型语言模型（LLM）的数据投毒攻击所需恶意文档数量与数据集规模无关，仅需少量文档即可生效。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在预训练和微调过程中对数据投毒攻击的脆弱性，尤其是攻击所需文档数量是否随模型规模增长。

Method: 通过大规模预训练实验（模型参数从600M到13B，数据集从6B到260B tokens），验证投毒攻击的有效性。

Result: 发现仅需250个恶意文档即可在不同规模的模型和数据集上实现攻击效果，且攻击效果不受模型规模影响。

Conclusion: 数据投毒攻击对大型模型的风险可能被低估，需加强防御研究。

Abstract: Poisoning attacks can compromise the safety of large language models (LLMs)
by injecting malicious documents into their training data. Existing work has
studied pretraining poisoning assuming adversaries control a percentage of the
training corpus. However, for large models, even small percentages translate to
impractically large amounts of data. This work demonstrates for the first time
that poisoning attacks instead require a near-constant number of documents
regardless of dataset size. We conduct the largest pretraining poisoning
experiments to date, pretraining models from 600M to 13B parameters on
chinchilla-optimal datasets (6B to 260B tokens). We find that 250 poisoned
documents similarly compromise models across all model and dataset sizes,
despite the largest models training on more than 20 times more clean data. We
also run smaller-scale experiments to ablate factors that could influence
attack success, including broader ratios of poisoned to clean data and
non-random distributions of poisoned samples. Finally, we demonstrate the same
dynamics for poisoning during fine-tuning. Altogether, our results suggest that
injecting backdoors through data poisoning may be easier for large models than
previously believed as the number of poisons required does not scale up with
model size, highlighting the need for more research on defences to mitigate
this risk in future models.

</details>


### [205] [An in-depth look at approximation via deep and narrow neural networks](https://arxiv.org/abs/2510.07202)
*Joris Dommel,Sven A. Wegner*

Main category: cs.LG

TL;DR: 论文研究了在宽度w=n和w=n+1时，神经网络如何逼近Hanin和Sellke提出的反例函数，并探讨了深度变化对逼近质量的影响。


<details>
  <summary>Details</summary>
Motivation: Hanin和Sellke在2017年证明了宽度w>n是神经网络逼近连续函数的必要条件，但未具体研究w=n和w=n+1时的逼近行为。本文旨在填补这一空白。

Method: 通过实验和理论分析，研究了神经网络在w=n和w=n+1时对反例函数的逼近效果，重点关注深度变化和神经元死亡现象的影响。

Result: 发现深度增加会改善逼近质量，但神经元死亡现象会限制逼近效果，尤其是在w=n时。

Conclusion: 研究揭示了神经网络在临界宽度附近的逼近行为，为理解网络结构和性能关系提供了新视角。

Abstract: In 2017, Hanin and Sellke showed that the class of arbitrarily deep,
real-valued, feed-forward and ReLU-activated networks of width w forms a dense
subset of the space of continuous functions on R^n, with respect to the
topology of uniform convergence on compact sets, if and only if w>n holds. To
show the necessity, a concrete counterexample function f:R^n->R was used. In
this note we actually approximate this very f by neural networks in the two
cases w=n and w=n+1 around the aforementioned threshold. We study how the
approximation quality behaves if we vary the depth and what effect (spoiler
alert: dying neurons) cause that behavior.

</details>


### [206] [Guided by the Experts: Provable Feature Learning Dynamic of Soft-Routed Mixture-of-Experts](https://arxiv.org/abs/2510.07205)
*Fangshuo Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 论文提供了软路由MoE模型在联合训练中的收敛保证，证明了学生网络通过特征学习阶段恢复教师网络参数，并展示了后训练剪枝和微调的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE架构广泛应用，但其训练动态的理论理解有限，尤其是在非线性路由器和专家的联合训练方面。

Method: 采用学生-教师框架，分析软路由MoE模型的收敛性，包括特征学习阶段和后训练剪枝与微调。

Result: 证明了学生网络能恢复教师网络参数，并通过剪枝和微调达到全局最优。

Conclusion: 论文首次深入分析了MoE架构的优化景观，为理论和实践提供了新见解。

Abstract: Mixture-of-Experts (MoE) architectures have emerged as a cornerstone of
modern AI systems. In particular, MoEs route inputs dynamically to specialized
experts whose outputs are aggregated through weighted summation. Despite their
widespread application, theoretical understanding of MoE training dynamics
remains limited to either separate expert-router optimization or only top-1
routing scenarios with carefully constructed datasets. This paper advances MoE
theory by providing convergence guarantees for joint training of soft-routed
MoE models with non-linear routers and experts in a student-teacher framework.
We prove that, with moderate over-parameterization, the student network
undergoes a feature learning phase, where the router's learning process is
``guided'' by the experts, that recovers the teacher's parameters. Moreover, we
show that a post-training pruning can effectively eliminate redundant neurons,
followed by a provably convergent fine-tuning process that reaches global
optimality. To our knowledge, our analysis is the first to bring novel insights
in understanding the optimization landscape of the MoE architecture.

</details>


### [207] [A Broader View of Thompson Sampling](https://arxiv.org/abs/2510.07208)
*Yanlin Qu,Hongseok Namkoong,Assaf Zeevi*

Main category: cs.LG

TL;DR: 论文通过将Thompson Sampling重新定义为在线优化算法，揭示了其平衡探索与利用的机制。


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling虽广泛应用，但其平衡探索与利用的具体机制尚不明确。

Method: 引入“忠实”平稳化方法，将动态优化问题转化为静态目标，利用Bellman原理分析。

Result: Thompson Sampling可视为一种在线优化算法，其贪婪性通过残差不确定性度量正则化。

Conclusion: 研究不仅解释了Thompson Sampling的机制，还为其改进提供了理论框架。

Abstract: Thompson Sampling is one of the most widely used and studied bandit
algorithms, known for its simple structure, low regret performance, and solid
theoretical guarantees. Yet, in stark contrast to most other families of bandit
algorithms, the exact mechanism through which posterior sampling (as introduced
by Thompson) is able to "properly" balance exploration and exploitation,
remains a mystery. In this paper we show that the core insight to address this
question stems from recasting Thompson Sampling as an online optimization
algorithm. To distill this, a key conceptual tool is introduced, which we refer
to as "faithful" stationarization of the regret formulation. Essentially, the
finite horizon dynamic optimization problem is converted into a stationary
counterpart which "closely resembles" the original objective (in contrast, the
classical infinite horizon discounted formulation, that leads to the Gittins
index, alters the problem and objective in too significant a manner). The newly
crafted time invariant objective can be studied using Bellman's principle which
leads to a time invariant optimal policy. When viewed through this lens,
Thompson Sampling admits a simple online optimization form that mimics the
structure of the Bellman-optimal policy, and where greediness is regularized by
a measure of residual uncertainty based on point-biserial correlation. This
answers the question of how Thompson Sampling balances
exploration-exploitation, and moreover, provides a principled framework to
study and further improve Thompson's original idea.

</details>


### [208] [Discriminative Feature Feedback with General Teacher Classes](https://arxiv.org/abs/2510.07245)
*Omri Bar Oz,Tosca Lechner,Sivan Sabato*

Main category: cs.LG

TL;DR: 研究了交互式学习协议DFF的理论性质，比较了其与监督学习和在线学习的差异，并分析了在可实现和不可实现设置下的错误界限。


<details>
  <summary>Details</summary>
Motivation: DFF协议通过判别性特征解释提供反馈，但缺乏与经典协议（如监督学习和在线学习）的系统性比较研究。

Method: 在一般框架下分析DFF，提出新的维度概念，并研究可实现和不可实现设置下的错误界限。

Result: 在可实现设置中，使用新维度概念表征错误界限；在不可实现设置中，给出了错误上界并证明其一般不可改进。

Conclusion: DFF中可实现维度不足以表征最优不可实现错误界限或无遗憾算法的存在性，这与在线学习不同。

Abstract: We study the theoretical properties of the interactive learning protocol
Discriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning
protocol uses feedback in the form of discriminative feature explanations. We
provide the first systematic study of DFF in a general framework that is
comparable to that of classical protocols such as supervised learning and
online learning. We study the optimal mistake bound of DFF in the realizable
and the non-realizable settings, and obtain novel structural results, as well
as insights into the differences between Online Learning and settings with
richer feedback such as DFF. We characterize the mistake bound in the
realizable setting using a new notion of dimension. In the non-realizable
setting, we provide a mistake upper bound and show that it cannot be improved
in general. Our results show that unlike Online Learning, in DFF the realizable
dimension is insufficient to characterize the optimal non-realizable mistake
bound or the existence of no-regret algorithms.

</details>


### [209] [Test-Time Graph Search for Goal-Conditioned Reinforcement Learning](https://arxiv.org/abs/2510.07257)
*Evgenii Opryshko,Junwei Quan,Claas Voelcker,Yilun Du,Igor Gilitschenski*

Main category: cs.LG

TL;DR: TTGS是一种轻量级规划方法，用于解决离线目标条件强化学习中的长时决策问题，通过构建状态图并搜索子目标序列，显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 离线GCRL在长时决策中存在信用分配和误差累积问题，TTGS旨在通过测试时规划缓解这些问题。

Method: TTGS构建状态图并利用距离或成本信号搜索子目标序列，无需额外训练或监督。

Result: 在OGBench基准测试中，TTGS显著提高了多个基础学习器的成功率。

Conclusion: TTGS展示了简单度量引导的测试时规划对离线GCRL的有效性。

Abstract: Offline goal-conditioned reinforcement learning (GCRL) trains policies that
reach user-specified goals at test time, providing a simple, unsupervised,
domain-agnostic way to extract diverse behaviors from unlabeled, reward-free
datasets. Nonetheless, long-horizon decision making remains difficult for GCRL
agents due to temporal credit assignment and error accumulation, and the
offline setting amplifies these effects. To alleviate this issue, we introduce
Test-Time Graph Search (TTGS), a lightweight planning approach to solve the
GCRL task. TTGS accepts any state-space distance or cost signal, builds a
weighted graph over dataset states, and performs fast search to assemble a
sequence of subgoals that a frozen policy executes. When the base learner is
value-based, the distance is derived directly from the learned goal-conditioned
value function, so no handcrafted metric is needed. TTGS requires no changes to
training, no additional supervision, no online interaction, and no privileged
information, and it runs entirely at inference. On the OGBench benchmark, TTGS
improves success rates of multiple base learners on challenging locomotion
tasks, demonstrating the benefit of simple metric-guided test-time planning for
offline GCRL.

</details>


### [210] [Dynamic Regret Bounds for Online Omniprediction with Long Term Constraints](https://arxiv.org/abs/2510.07266)
*Yahav Bechavod,Jiuyao Lu,Aaron Roth*

Main category: cs.LG

TL;DR: 提出一种算法，保证在线全预测中动态遗憾界限，同时满足长期约束。


<details>
  <summary>Details</summary>
Motivation: 为下游决策者提供预测序列，确保他们在最坏情况下获得效用保证并最小化约束违反。

Method: 设计算法，使所有代理同时获得动态遗憾保证，且约束违反趋近于零。

Result: 首次实现所有代理的动态遗憾保证，且无需代理维护状态。

Conclusion: 算法有效解决了在线全预测问题，为多代理决策提供了理论支持。

Abstract: We present an algorithm guaranteeing dynamic regret bounds for online
omniprediction with long term constraints. The goal in this recently introduced
problem is for a learner to generate a sequence of predictions which are
broadcast to a collection of downstream decision makers. Each decision maker
has their own utility function, as well as a vector of constraint functions,
each mapping their actions and an adversarially selected state to reward or
constraint violation terms. The downstream decision makers select actions "as
if" the state predictions are correct, and the goal of the learner is to
produce predictions such that all downstream decision makers choose actions
that give them worst-case utility guarantees while minimizing worst-case
constraint violation. Within this framework, we give the first algorithm that
obtains simultaneous \emph{dynamic regret} guarantees for all of the agents --
where regret for each agent is measured against a potentially changing sequence
of actions across rounds of interaction, while also ensuring vanishing
constraint violation for each agent. Our results do not require the agents
themselves to maintain any state -- they only solve one-round constrained
optimization problems defined by the prediction made at that round.

</details>


### [211] [GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced Intrusion Detection (Preprint)](https://arxiv.org/abs/2510.07285)
*Tianxiang Xu,Zhichao Wen,Xinyu Zhao,Qi Hu,Yan Li,Chang Liu*

Main category: cs.LG

TL;DR: GTCN-G模型结合G-TCN和GCN，通过残差学习和注意力机制解决数据不平衡问题，在入侵检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 网络威胁复杂性增加和流量数据类别不平衡是入侵检测系统的主要挑战，现有方法未能有效结合时空特征。

Method: 提出GTCN-G框架，融合G-TCN提取时间特征和GCN学习图结构，利用GAT残差机制保留特征信息。

Result: 在UNSW-NB15和ToN-IoT数据集上，GTCN-G在二元和多分类任务中均优于基线模型。

Conclusion: GTCN-G通过整合时空特征和解决数据不平衡，显著提升了入侵检测性能。

Abstract: The escalating complexity of network threats and the inherent class imbalance
in traffic data present formidable challenges for modern Intrusion Detection
Systems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological
structures and Temporal Convolutional Networks (TCNs) are proficient in
capturing time-series dependencies, a framework that synergistically integrates
both while explicitly addressing data imbalance remains an open challenge. This
paper introduces a novel deep learning framework, named Gated Temporal
Convolutional Network and Graph (GTCN-G), engineered to overcome these
limitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting
hierarchical temporal features from network flows with a Graph Convolutional
Network (GCN) designed to learn from the underlying graph structure. The core
innovation lies in the integration of a residual learning mechanism,
implemented via a Graph Attention Network (GAT). This mechanism preserves
original feature information through residual connections, which is critical
for mitigating the class imbalance problem and enhancing detection sensitivity
for rare malicious activities (minority classes). We conducted extensive
experiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to
validate our approach. The empirical results demonstrate that the proposed
GTCN-G model achieves state-of-the-art performance, significantly outperforming
existing baseline models in both binary and multi-class classification tasks.

</details>


### [212] [Evolutionary Profiles for Protein Fitness Prediction](https://arxiv.org/abs/2510.07286)
*Jigang Fan,Xiaoran Jiao,Shengdong Lin,Zhanming Liang,Weian Mao,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.LG

TL;DR: EvoIF模型结合家族内和跨家族进化信号，通过轻量级设计实现高效突变适应性预测。


<details>
  <summary>Details</summary>
Motivation: 预测突变对蛋白质适应性的影响是蛋白质工程的核心，但受限于实验规模与序列空间的巨大差距。

Method: 将自然进化视为隐式奖励最大化，MLM为逆强化学习，结合家族内和跨家族进化信号设计EvoIF模型。

Result: EvoIF在ProteinGym数据集上表现优异，仅用0.15%训练数据和较少参数达到SOTA或竞争性能。

Conclusion: EvoIF验证了家族内与跨家族信号的互补性，提升了预测鲁棒性，代码将开源。

Abstract: Predicting the fitness impact of mutations is central to protein engineering
but constrained by limited assays relative to the size of sequence space.
Protein language models (pLMs) trained with masked language modeling (MLM)
exhibit strong zero-shot fitness prediction; we provide a unifying view by
interpreting natural evolution as implicit reward maximization and MLM as
inverse reinforcement learning (IRL), in which extant sequences act as expert
demonstrations and pLM log-odds serve as fitness estimates. Building on this
perspective, we introduce EvoIF, a lightweight model that integrates two
complementary sources of evolutionary signal: (i) within-family profiles from
retrieved homologs and (ii) cross-family structural-evolutionary constraints
distilled from inverse folding logits. EvoIF fuses sequence-structure
representations with these profiles via a compact transition block, yielding
calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational
assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve
state-of-the-art or competitive performance while using only 0.15% of the
training data and fewer parameters than recent large models. Ablations confirm
that within-family and cross-family profiles are complementary, improving
robustness across function types, MSA depths, taxa, and mutation depths. The
codes will be made publicly available at https://github.com/aim-uofa/EvoIF.

</details>


### [213] [MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder](https://arxiv.org/abs/2510.07289)
*Xingtong Yu,Chang Zhou,Xinming Zhang,Yuan Fang*

Main category: cs.LG

TL;DR: MolGA通过灵活整合分子领域知识，将预训练的2D图编码器适应于下游分子应用。


<details>
  <summary>Details</summary>
Motivation: 现有预训练2D图编码器忽略了分子领域知识，而现有方法缺乏灵活性整合多样知识。

Method: 提出分子对齐策略和条件适应机制，将预训练表示与领域知识结合。

Result: 在11个公共数据集上验证了MolGA的有效性。

Conclusion: MolGA提供了一种实用方法，将预训练编码器与分子领域知识结合。

Abstract: Molecular graph representation learning is widely used in chemical and
biomedical research. While pre-trained 2D graph encoders have demonstrated
strong performance, they overlook the rich molecular domain knowledge
associated with submolecular instances (atoms and bonds). While molecular
pre-training approaches incorporate such knowledge into their pre-training
objectives, they typically employ designs tailored to a specific type of
knowledge, lacking the flexibility to integrate diverse knowledge present in
molecules. Hence, reusing widely available and well-validated pre-trained 2D
encoders, while incorporating molecular domain knowledge during downstream
adaptation, offers a more practical alternative. In this work, we propose
MolGA, which adapts pre-trained 2D graph encoders to downstream molecular
applications by flexibly incorporating diverse molecular domain knowledge.
First, we propose a molecular alignment strategy that bridge the gap between
pre-trained topological representations with domain-knowledge representations.
Second, we introduce a conditional adaptation mechanism that generates
instance-specific tokens to enable fine-grained integration of molecular domain
knowledge for downstream tasks. Finally, we conduct extensive experiments on
eleven public datasets, demonstrating the effectiveness of MolGA.

</details>


### [214] [MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline](https://arxiv.org/abs/2510.07307)
*Rushi Qiang,Yuchen Zhuang,Anikait Singh,Percy Liang,Chao Zhang,Sherry Yang,Bo Dai*

Main category: cs.LG

TL;DR: MLE-Smith是一个自动化多代理流程，用于将原始数据集转化为竞赛风格的机器学习工程挑战，解决了现有基准测试的可扩展性和适用性问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习工程基准测试依赖静态、手动策划的任务，导致可扩展性低且适用性有限。

Method: MLE-Smith采用生成-验证-执行范式，通过多代理流程实现结构化任务设计和标准化重构，结合混合验证机制确保任务质量。

Result: 在224个真实数据集上生成606个任务，验证了主流LLM在MLE-Smith任务上的表现与人工设计任务高度相关。

Conclusion: MLE-Smith能够高效扩展机器学习工程任务，同时保持任务质量。

Abstract: While Language Models (LMs) have made significant progress in automating
machine learning engineering (MLE), the acquisition of high-quality MLE
training data is significantly constrained. Current MLE benchmarks suffer from
low scalability and limited applicability because they rely on static, manually
curated tasks, demanding extensive time and manual effort to produce. We
introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw
datasets into competition-style MLE challenges through an efficient
generate-verify-execute paradigm for scaling MLE tasks with verifiable quality,
real-world usability, and rich diversity. The proposed multi-agent pipeline in
MLE-Smith drives structured task design and standardized refactoring, coupled
with a hybrid verification mechanism that enforces strict structural rules and
high-level semantic soundness. It further validates empirical solvability and
real-world fidelity through interactive execution. We apply MLE-Smith to 224 of
real-world datasets and generate 606 tasks spanning multiple categories,
objectives, and modalities, demonstrating that MLE-Smith can work effectively
across a wide range of real-world datasets. Evaluation on the generated tasks
shows that the performance of eight mainstream and cutting-edge LLMs on
MLE-Smith tasks is strongly correlated with their performance on carefully
human-designed tasks, highlighting the effectiveness of the MLE-Smith to
scaling up MLE tasks, while maintaining task quality.

</details>


### [215] [h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement Learning](https://arxiv.org/abs/2510.07312)
*Sumeet Ramesh Motwani,Alesia Ivanova,Ziyang Cai,Philip Torr,Riashat Islam,Shital Shah,Christian Schroeder de Witt,Charles London*

Main category: cs.LG

TL;DR: 通过合成简单问题为复杂多步依赖链，利用课程学习和结果奖励提升长程推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖推理时支架或昂贵步骤级监督，难以扩展。

Method: 合成简单问题为复杂多步依赖链，采用课程学习和结果奖励进行RL训练。

Result: 在GSM8K等数据集上，长程推理准确率提升高达2.06倍。

Conclusion: 该方法为利用现有数据扩展RL解决长程问题提供了高效路径。

Abstract: Large language models excel at short-horizon reasoning tasks, but performance
drops as reasoning horizon lengths increase. Existing approaches to combat this
rely on inference-time scaffolding or costly step-level supervision, neither of
which scales easily. In this work, we introduce a scalable method to bootstrap
long-horizon reasoning capabilities using only existing, abundant short-horizon
data. Our approach synthetically composes simple problems into complex,
multi-step dependency chains of arbitrary length. We train models on this data
using outcome-only rewards under a curriculum that automatically increases in
complexity, allowing RL training to be scaled much further without saturating.
Empirically, our method generalizes remarkably well: curriculum training on
composed 6th-grade level math problems (GSM8K) boosts accuracy on longer,
competition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.
Importantly, our long-horizon improvements are significantly higher than
baselines even at high pass@k, showing that models can learn new reasoning
paths under RL. Theoretically, we show that curriculum RL with outcome rewards
achieves an exponential improvement in sample complexity over full-horizon
training, providing training signal comparable to dense supervision. h1
therefore introduces an efficient path towards scaling RL for long-horizon
problems using only existing data.

</details>
