<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 100]
- [cs.RO](#cs.RO) [Total: 37]
- [cs.LG](#cs.LG) [Total: 101]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: MultiFoodChat是一个基于多智能体对话的零样本食物识别框架，结合视觉语言模型和大语言模型，无需额外训练即可实现高精度分类。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型依赖大量标注数据且泛化能力有限，MultiFoodChat旨在解决零样本食物识别问题。

Method: 通过视觉-文本多轮对话协作推理，利用OPT捕捉细粒度视觉特征，IRA动态解析上下文。

Result: 在多个公开数据集上超越无监督和少样本方法，兼具高准确性和可解释性。

Conclusion: 该框架为智能食品质检提供了新范式，无需人工标注即可理解复杂场景。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [2] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 论文描述了一个用于分割子宫内膜异位症常见视觉表现（暗色子宫内膜植入物）的系统，旨在辅助妇科医生。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症视觉表现多样，识别困难，非专业医生易出错，需辅助工具。

Method: 系统通过训练分析腹腔镜手术视频，用多色覆盖标注植入区域，并提供检测摘要。

Result: 系统能有效识别并标注暗色子宫内膜植入物，提升视频浏览效率。

Conclusion: 该系统为妇科医生提供了有效的辅助工具，有助于提高子宫内膜异位症的识别准确性。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [3] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 结合视觉模型和视觉语言模型（VLMs）提升遥感图像分析，尤其在飞机检测和场景理解方面表现显著。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型依赖大量标注数据且上下文理解有限，VLMs虽整合视觉与文本数据但在遥感领域应用不足。

Method: 集成YOLO与LLaVA、ChatGPT、Gemini等VLMs，评估在标注/未标注数据及退化图像中的性能。

Result: 飞机检测准确率平均提升48.46%，CLIPScore提升6.17%，尤其在挑战性条件下表现优异。

Conclusion: 结合传统视觉模型与VLMs为遥感图像分析提供了更高效、先进的解决方案，尤其适用于少样本学习场景。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [4] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 开发了一种基于深度学习的AI系统，用于检测前列腺癌中的筛状形态，性能优于病理学家。


<details>
  <summary>Details</summary>
Motivation: 筛状形态在前列腺癌中预后不良，但病理学家间存在较大差异，AI可提高检测准确性。

Method: 使用EfficientNetV2-S编码器和多实例学习训练模型，并在多个队列中验证。

Result: 模型在内部和外部验证中表现优异（AUC: 0.97和0.90），优于病理学家。

Conclusion: AI模型可提升筛状形态检测的可靠性，标准化诊断并改善治疗决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [5] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: NAPPure是一个扩展的对抗净化框架，能够处理非加性对抗扰动，显著提升图像分类模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗净化方法主要针对加性扰动，对非加性扰动（如模糊、遮挡、失真）效果不佳，因此需要一种更通用的方法。

Method: 通过建立对抗图像的生成过程，并通过似然最大化分离干净图像和扰动参数。

Result: 在GTSRB和CIFAR-10数据集上，NAPPure显著提升了模型对非加性扰动的鲁棒性。

Conclusion: NAPPure为处理非加性对抗扰动提供了一种有效方法，扩展了对抗净化的适用范围。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [6] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: Vgent是一种基于图的检索-推理增强生成框架，旨在提升大型视频语言模型（LVLMs）对长视频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 长视频处理面临上下文窗口限制和长期序列信息保留的挑战，现有检索增强生成（RAG）方法在视频中应用时存在时间依赖破坏和无关信息干扰的问题。

Method: 通过结构化图表示视频片段间的语义关系，并引入中间推理步骤以减少检索噪声并显式聚合跨片段信息。

Result: 在三个长视频理解基准测试中，Vgent相比基线模型性能提升3.0%∼5.4%，优于现有视频RAG方法8.6%。

Conclusion: Vgent通过图结构和中间推理有效提升了长视频理解的准确性和上下文感知能力。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [7] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: TPL是一种基于原型的学习框架，用于同步不同场景或生成AI视频，通过构建共享的1D表示来高效对齐视频。


<details>
  <summary>Details</summary>
Motivation: 解决多场景或生成AI视频同步的复杂挑战，避免传统方法中的非线性时间对齐问题。

Method: 提出Temporal Prototype Learning (TPL)，利用预训练模型提取高维嵌入，构建共享的1D原型序列。

Result: TPL在同步准确性、效率和鲁棒性上表现优异，首次解决了生成AI视频的同步问题。

Conclusion: TPL为多视频同步提供了高效且通用的解决方案，尤其在生成AI视频中表现突出。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [8] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 提出了一种零样本方法，通过少量手机图像生成超真实、保留身份的3D虚拟形象。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在几何不一致、幻觉问题，且合成数据训练的模型无法捕捉高频细节（如皮肤皱纹和细发），限制了真实感。

Method: 引入两个关键贡献：(1) 生成规范化模块处理多视角图像为标准化表示，(2) 基于Transformer的模型，训练于高保真高斯溅射虚拟形象数据集。

Result: 生成的静态半身虚拟形象具有高度真实感和身份保留能力。

Conclusion: 该方法通过“捕获、规范化、溅射”流程，解决了现有技术的局限性。

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [9] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: cubic是一个开源的Python库，通过GPU加速解决生物图像分析中的可扩展性和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现代显微镜生成的数据集越来越大，现有计算工具在可扩展性、效率和与现代科学计算工作流的集成方面存在局限。

Method: cubic结合SciPy和scikit-image的API，利用CuPy和RAPIDS cuCIM提供GPU加速的替代方案，支持设备无关的操作。

Result: cubic在保持算法保真度的同时，显著加速了图像处理流程，包括去卷积和分割。

Conclusion: cubic为可扩展、可重复的生物图像分析提供了坚实基础，并与Python科学计算生态系统集成。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [10] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 提出了一种通过定制数据管道实现视频扩散模型中多视角角色一致性和3D相机控制的框架。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成中角色一致性和相机控制的挑战，推动虚拟制作的集成。

Method: 使用4D高斯泼溅（4DGS）和视频重光照模型生成多样化数据，微调现有视频扩散模型。

Result: 提升了视频质量、个性化准确性、相机控制和光照适应性。

Conclusion: 该框架为虚拟制作提供了核心能力，并展示了在视频生成中的显著改进。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [11] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 提出了一种联合建模Big Five和HEXACO的方法，用于从多模态人类行为中自动识别显性人格特征。


<details>
  <summary>Details</summary>
Motivation: 以往研究多使用Big Five，但未关注HEXACO（可评估诚实-谦逊等特质），且两者关系未明确。

Method: 联合优化Big Five和HEXACO的识别。

Result: 实验表明，该方法能有效识别Big Five和HEXACO。

Conclusion: 联合建模可提升多模态行为识别效果。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [12] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 提出了一种基于位平面处理的噪声提取方法，用于高效检测AI生成图像，准确率高达98.9%，速度提升近百倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高且难以捕捉原始图像中的噪声特征，需要更高效且准确的解决方案。

Method: 利用位平面处理提取噪声，设计最大梯度块选择放大噪声信号，并采用轻量级分类头。

Result: 在GenImage基准测试中平均准确率达98.9%，跨生成器泛化能力强，速度提升显著。

Conclusion: 该方法在准确性和效率上均优于现有技术，适用于实际应用场景。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [13] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: 提出了一种新型多模态音频-视觉框架PIA，通过结合语言、动态面部运动和面部识别线索，显著提升了检测现代深度伪造技术的能力。


<details>
  <summary>Details</summary>
Motivation: 传统检测方法依赖手动设计的阈值或单模态策略，难以应对由GAN、扩散模型等先进生成模型生成的深度伪造内容。

Method: 采用多模态方法，结合音素序列、唇部几何数据和高级面部身份嵌入，识别跨模态的不一致性。

Result: PIA框架显著提高了对细微深度伪造篡改的检测能力。

Conclusion: PIA通过多模态分析有效解决了传统检测方法的局限性，为深度伪造检测提供了新思路。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [14] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出了一种新型调制方案EIM，利用事件间隔调制信息，显著提高了事件型OCC系统的传输速度。


<details>
  <summary>Details</summary>
Motivation: 传统OCC系统存在低比特率和高处理负载的问题，而现有事件型OCC系统的调制方案未能充分利用EVS的独特特性。

Method: 提出EIM调制方案，优化EVS参数，实验确定最大调制阶数，并进行传输实验。

Result: 在室内环境下实现了28 kbps（10米）和8.4 kbps（50米）的传输速率，创下事件型OCC系统的新纪录。

Conclusion: EIM方案有效提升了事件型OCC系统的性能，为高速、低延迟通信提供了新思路。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [15] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: 提出了一种名为MACE的方法，通过混合专家网络和辅助无损失负载均衡策略，解决了大规模场景中定位和渲染的高效性问题。


<details>
  <summary>Details</summary>
Motivation: 大规模场景中的高效定位和高质量渲染因计算成本高而具有挑战性，现有方法在扩展性上受限。

Method: 引入混合专家网络（MOE）和门控网络，动态选择子网络；提出ALF-LB策略提升定位精度。

Result: 在剑桥测试集上仅需10分钟训练即可实现高质量渲染，同时显著降低成本。

Conclusion: MACE为大规模场景应用提供了高效且高精度的解决方案。

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [16] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的视频扩散框架IPRO，用于提升图像到视频生成中的身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有I2V模型在生成视频时难以保持输入图像中人物的身份一致性，尤其是在表情和动作变化较大时。

Method: 引入身份保留奖励引导优化（IPRO），通过强化学习优化扩散模型，使用面部身份评分器，并采用KL散度正则化稳定训练。

Result: 在Wan 2.2 I2V模型和内部模型上的实验验证了方法的有效性。

Conclusion: IPRO显著提升了身份一致性，且无需修改模型架构。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [17] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: Identity-GRPO通过人类反馈优化多人类身份一致性视频生成，比基线方法提升18.9%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态交互中难以保持多人类身份一致性。

Method: 构建视频奖励模型，采用GRPO变体优化多人类一致性。

Result: 实验显示Identity-GRPO在人类一致性指标上提升18.9%。

Conclusion: Identity-GRPO为强化学习与个性化视频生成对齐提供了可行方案。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [18] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: 提出了一种名为MatchAttention的注意力机制，通过动态匹配相对位置解决高分辨率图像匹配问题，结合MatchDecoder和遮挡处理技术，实现了高效且高精度的跨视图匹配。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像匹配因现有交叉注意力的二次复杂度和缺乏显式匹配约束而具有挑战性。

Method: 提出MatchAttention机制，动态匹配相对位置；设计MatchDecoder和遮挡处理技术（门控交叉MatchAttention和一致性约束损失）。

Result: 在多个公开数据集上达到最优性能，MatchStereo-B在Middlebury基准测试中排名第一，MatchStereo-T可高效处理4K图像。

Conclusion: 结合高精度和低计算复杂度，实现了实时、高分辨率、高精度的跨视图匹配。

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [19] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出一种基于事件视觉传感器的鲁棒解调方案，首次在室外实验中实现200m-60kbps和400m-30kbps下BER < 10^-3。


<details>
  <summary>Details</summary>
Motivation: 解决光学相机通信系统在长距离和高数据速率下的鲁棒解调问题。

Method: 结合OOK、切换解调和数字锁相环技术。

Result: 在200米-60kbps和400米-30kbps条件下，BER低于10^-3。

Conclusion: 该方案在长距离和高数据速率下表现出色，适用于室外环境。

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [20] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: GauSSmart是一种结合2D基础模型和3D高斯泼溅重建的混合方法，通过2D分割先验和高维特征嵌入提升场景重建的细节和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅在大规模数据集上表现良好，但在稀疏覆盖区域难以捕捉细节或保持真实感，主要受限于稀疏3D训练数据的固有局限性。

Method: GauSSmart整合了2D计算机视觉技术（如凸滤波和DINO等基础模型的语义特征监督），通过2D分割先验和高维特征嵌入指导高斯泼溅的密集化和细化。

Result: 在三个数据集上的实验表明，GauSSmart在大多数评估场景中优于现有高斯泼溅方法。

Conclusion: 混合2D-3D方法具有显著潜力，通过结合2D基础模型和3D重建流程，可以克服各自方法的局限性。

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [21] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: 提出了一种基于因果推断的新框架，通过利用语义特征并减少混杂因素的影响，有效解决了组织病理学中的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 组织病理学中的领域偏移（如数据采集过程或来源的差异）对深度学习模型的泛化能力构成挑战，现有方法多依赖统计相关性而忽略因果关系。

Method: 采用前门准则，设计包含中介变量和观察到的组织切片的转换策略，以减少混杂因素的影响。

Result: 在CAMELYON17数据集和私有组织病理学数据集上验证，性能提升高达7%，优于现有基线。

Conclusion: 因果推断是解决组织病理学图像分析中领域偏移问题的有力工具。

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [22] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 提出了一种无训练的三层对比解码方法，通过选择成熟层和业余层、识别视觉基础良好的关键层，减少大视觉语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在多模态任务中表现优异，但仍存在幻觉问题，依赖单一模态或记忆训练数据。

Method: 采用无训练的三层对比解码，包括选择层、识别关键层和应用对比解码。

Result: 在POPE、MME和AMBER等基准测试中表现优异，减少幻觉并生成更视觉基础的回答。

Conclusion: 该方法显著提升了大视觉语言模型的视觉基础能力，减少了幻觉现象。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [23] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: MID-StyleGAN框架通过结合扩散模型和GAN生成多域合成眼部图像，解决了虹膜生物识别系统中数据稀缺问题，显著提升了PAD系统性能。


<details>
  <summary>Details</summary>
Motivation: 虹膜生物识别系统易受展示攻击（PA）影响，但缺乏训练和评估PAD技术的数据集。

Method: 提出MID-StyleGAN框架，结合扩散模型和GAN，生成多域合成眼部图像，并采用自适应损失函数保持域一致性。

Result: 在LivDet2020数据集上，PAD系统的检测率从93.41%提升至98.72%。

Conclusion: MID-StyleGAN为虹膜生物识别系统提供了一种可扩展的数据生成解决方案，显著提升了PAD性能。

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [24] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo通过视觉中心激活和多视觉基础模型协调优化MLLM表示，提升视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 主流MLLMs仅通过文本标记的下一个标记预测进行监督，忽略了视觉中心信息，影响分析能力。

Method: 引入视觉判别对齐，结合任务感知特征，使用模块化任务查询和视觉对齐层激活视觉信号，并通过令牌网关掩码协调表示冲突。

Result: VaCo显著提升了不同MLLMs在多个基准测试中的性能。

Conclusion: VaCo通过视觉中心优化，显著增强了MLLMs的视觉理解能力。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [25] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 利用循环一致性关键点和新颖的姿势块改进RGB-D配准，超越自监督方法。


<details>
  <summary>Details</summary>
Motivation: 利用大量未标记的RGB-D数据进行几何推理，提升配准精度。

Method: 使用循环一致性关键点增强空间一致性，结合GRU和变换同步的姿势块。

Result: 在ScanNet和3DMatch上超越自监督方法，甚至优于部分监督方法。

Conclusion: 提出的组件有效且可集成到现有方法中，显著提升性能。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [26] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: SPR方法通过奖励详细和精确的空间描述，提升多模态大语言模型（MLLMs）的细粒度空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在细粒度空间感知（如详细区域描述或精确定位）上表现不足，且缺乏对用户需求的响应。SPR旨在通过直接监督模型响应来改进这一问题。

Method: SPR引入语义和定位评分，评估MLLM生成描述的质量，并通过偏好优化提升细粒度对齐。

Result: 实验表明，SPR能有效提升MLLMs的空间理解能力，且训练开销极小。

Conclusion: SPR是一种高效的方法，显著提升了MLLMs在细粒度空间任务中的表现。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [27] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: DOS方法通过修改CLIP文本嵌入，显著提升了多对象图像生成的准确率，减少了对象混合问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在多对象提示下常出现对象忽略或混合问题，DOS旨在解决这一问题。

Method: 提出DOS方法，通过修改三种CLIP文本嵌入来优化多对象生成。

Result: 实验显示DOS在多对象生成任务中表现优异，人类评估中显著优于其他方法。

Conclusion: DOS是一种实用且有效的解决方案，可显著提升多对象图像生成质量。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [28] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: 提出了一种高效的3D脑肿瘤分割模型DRBD-Mamba，通过双分辨率和双向Mamba捕获多尺度长程依赖，计算开销低，并在BraTS2023上验证了其鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对临床诊断和治疗至关重要，但现有方法计算开销大且鲁棒性不足。

Method: 采用空间填充曲线减少多轴特征扫描的计算开销，引入门控融合模块和量化块增强特征表示。

Result: 在BraTS2023测试集上，模型在肿瘤核心和增强肿瘤区域分别取得1.75%和0.93%的Dice提升，效率提高15倍。

Conclusion: DRBD-Mamba在保持高分割精度的同时显著提升计算效率，适用于多样化的临床数据。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [29] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: BoardVision框架用于检测主板组装缺陷，通过YOLOv7和Faster R-CNN的对比，提出轻量级集成方法CTV Voter，并评估了鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 主板组装缺陷检测在电子制造中至关重要，但现有研究较少。

Method: 使用YOLOv7和Faster R-CNN进行对比，提出CTV Voter集成方法。

Result: CTV Voter平衡了精确率和召回率，并展示了在扰动下的稳定性。

Conclusion: BoardVision将计算机视觉技术应用于实际主板制造质量检测。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [30] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: DCMIL是一种双课程对比多实例学习模型，用于高效处理全切片图像（WSIs）进行癌症预后，无需密集标注，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 计算病理学在利用WSIs量化形态异质性和开发客观预后模型方面具有潜力，但面临计算瓶颈和标注稀缺的挑战。

Method: 提出DCMIL模型，通过渐进式表示学习处理多放大倍率的WSIs，无需密集标注。

Result: 在12种癌症类型（5,954名患者）上验证，DCMIL优于现有方法，并能识别预后关键区域和形态差异。

Conclusion: DCMIL为癌症预后提供了高效、无需标注的解决方案，并具有生成新生物学见解的潜力。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [31] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种结合帧内和帧间编码的神经视频压缩框架，解决了现有方法的局限性，如遮挡处理不足和帧间误差传播。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩技术在处理遮挡、新内容和帧间误差传播方面存在不足，需要改进。

Method: 借鉴传统视频编码的帧内编码工具，提出统一帧内和帧间编码的框架，并设计同时压缩两帧的方法以利用双向帧间冗余。

Result: 实验表明，该方案平均BD-rate降低10.7%，帧间比特率和质量更稳定，且保持实时编解码性能。

Conclusion: 提出的框架显著提升了压缩效率和稳定性，同时保持了实时性能，代码和模型将开源。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [32] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 提出了一种针对视频目标检测的最小失真通用对抗攻击方法，利用核范数正则化生成集中于背景的结构化扰动，并通过自适应乐观指数梯度方法优化。


<details>
  <summary>Details</summary>
Motivation: 视频目标检测在安全关键应用中至关重要，但现有深度学习检测器易受通用对抗攻击影响，需提出更有效的攻击方法。

Method: 采用核范数正则化生成结构化扰动，并通过自适应乐观指数梯度方法高效优化。

Result: 所提攻击方法在效果上优于低秩投影梯度下降和Frank-Wolfe攻击，同时保持高隐蔽性。

Conclusion: 该方法为视频目标检测对抗攻击提供了高效且隐蔽的解决方案，代码和数据已公开。

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [33] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 无监督深度生成模型在脑影像异常检测中表现出潜力，优于传统监督方法，适用于多种病理。


<details>
  <summary>Details</summary>
Motivation: 解决监督方法需要大量标注数据和局限于已知病理的问题，利用健康数据训练模型检测异常。

Method: 综述了自编码器、变分自编码器、生成对抗网络和去噪扩散模型等无监督生成模型的应用。

Result: 生成模型在检测大病灶方面表现良好，并能生成可解释的伪健康重建图像。

Conclusion: 未来需关注解剖学感知建模、基础模型开发和临床验证，以实现临床影响。

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [34] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: 提出了一种压缩多任务图像恢复模型的方法，通过迭代剪枝策略减少参数数量，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 在线社交网络中的图像质量常因压缩操作而下降，影响用户体验，现有模型参数过多导致计算效率低。

Method: 采用迭代剪枝策略，逐步移除低权重参数并重置剩余权重，以发现高效的稀疏子网络。

Result: 在去雨、去雾和去噪任务中，模型仅保留10%的参数，性能仍优于或匹配现有方法。

Conclusion: MIR-L模型在高效压缩的同时保持了高性能，为多任务图像恢复提供了实用解决方案。

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [35] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 利用Sentinel-2卫星数据训练CNN-LSTM模型，实现季节性放牧检测，准确率达77%，显著提升检查效率。


<details>
  <summary>Details</summary>
Motivation: 放牧对农业和生物多样性有重要影响，但缺乏可扩展的监测方法。

Method: 使用Sentinel-2 L2A时间序列数据，训练CNN-LSTM模型进行二分类预测（放牧/未放牧）。

Result: 模型平均F1分数为77%，放牧草场召回率达90%，检查效率提升17.2倍。

Conclusion: 免费卫星数据可有效指导资源分配，支持保护性土地利用合规。代码和模型已公开。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [36] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: Vision Mamba作为图像分类的替代方案，比ViTs和CNNs更高效，首次用于三维多孔介质渗透率预测。


<details>
  <summary>Details</summary>
Motivation: 解决ViTs和CNNs在计算效率和内存占用上的不足，探索Vision Mamba在渗透率预测中的潜力。

Method: 使用Vision Mamba作为主干网络，与ViT和CNN模型在渗透率预测上进行比较，并进行消融实验。

Result: Vision Mamba在计算效率和内存占用上优于ViTs和CNNs，且在渗透率预测中表现良好。

Conclusion: Vision Mamba有望替代ViTs，应用于大型视觉模型中。

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [37] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: SurgScan是一种基于AI的手术器械缺陷检测框架，通过YOLOv8实现实时高精度分类，适用于工业部署。


<details>
  <summary>Details</summary>
Motivation: 手术器械缺陷对无菌性、机械完整性和患者安全构成严重风险，而传统人工检测易出错且不一致。

Method: 使用YOLOv8模型，基于102,876张高分辨率图像数据集训练，涵盖11种器械类型和5种主要缺陷类别。

Result: SurgScan达到99.3%的最高准确率，实时推理速度为4.2-5.8毫秒/图像，并通过对比增强预处理显著提升检测效果。

Conclusion: SurgScan为自动化质量控制提供了可扩展、经济高效的AI解决方案，符合ISO 13485和FDA标准。

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [38] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 提出一种噪声投影器，通过文本条件优化初始噪声，提升Stable Diffusion生成图像与文本的对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过改变去噪动态或多噪声后选择解决文本-图像对齐问题，但存在训练-推理不匹配问题。

Method: 设计噪声投影器，利用VLM反馈和奖励模型优化噪声，无需参考图像或手工先验。

Result: 实验表明，该方法显著提升多样提示下的文本-图像对齐性。

Conclusion: 噪声投影器有效解决训练-推理不匹配问题，且推理成本低。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [39] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: PaddleOCR-VL是一个高效且资源友好的文档解析模型，支持109种语言，在文档解析和元素识别方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有文档解析模型在多语言支持和复杂元素识别上的不足，同时保持资源高效。

Method: 结合NaViT动态分辨率视觉编码器和ERNIE-4.5-0.3B语言模型，构建紧凑而强大的视觉语言模型。

Result: 在公开和内部基准测试中达到SOTA性能，显著优于现有解决方案，推理速度快。

Conclusion: PaddleOCR-VL适合实际部署，具有高效性和竞争力。

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [40] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: DentVFM是一种基于视觉基础模型（VFM）的牙科AI系统，通过自监督学习和大规模多模态牙科影像数据集（DentVista）训练，显著提升了牙科影像分析的泛化能力和效率。


<details>
  <summary>Details</summary>
Motivation: 牙科影像分析因专业人才短缺和现有AI系统的局限性（如单模态、任务特定设计、依赖标注数据）而受限，亟需一种泛化能力强、适应性高的解决方案。

Method: 提出DentVFM，基于Vision Transformer架构，利用自监督学习在DentVista数据集（160万张多模态牙科影像）上训练，并引入DentBench作为评估基准。

Result: DentVFM在多种牙科任务（如疾病诊断、治疗分析等）中表现优异，优于现有基线方法，并支持跨模态诊断。

Conclusion: DentVFM为牙科AI设立了新范式，提供了一种可扩展、适应性强且高效的解决方案，有望改善全球口腔医疗。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [41] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: PL-SE-ADA提出了一种可解释的医学图像域适应框架，通过对抗训练和重构机制实现域不变和域特定特征的分离，提升了性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学图像（如MR扫描）因设备和协议差异存在域偏移，影响机器学习性能。现有方法缺乏可解释性，难以满足医学应用需求。

Method: PL-SE-ADA采用两个编码器（$f_E$和$f_{SE}$）分别提取域不变特征$\boldsymbol{z_u}$和域特定特征$\boldsymbol{z_d}$，通过对抗训练和重构机制优化模型。

Result: 在图像重构、疾病分类和域识别任务中，PL-SE-ADA性能优于或持平现有方法，同时支持特征可视化，提供高可解释性。

Conclusion: PL-SE-ADA在医学图像域适应中实现了性能与可解释性的平衡，为实际应用提供了有效解决方案。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [42] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: VisualSplit框架通过分解图像为经典视觉描述符，结合现代学习方法提升视觉理解。


<details>
  <summary>Details</summary>
Motivation: 探索现代学习是否能从经典视觉线索中受益，以解决深度学习内部表示不透明的问题。

Method: 提出VisualSplit框架，将图像分解为解耦的经典描述符，并通过重建驱动的预训练方案学习。

Result: 在图像生成和编辑等高级视觉任务中实现了有效的属性控制。

Conclusion: VisualSplit展示了结合经典视觉描述符的现代学习方法在视觉理解中的有效性。

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [43] [QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models](https://arxiv.org/abs/2510.14836)
*Yixuan Li,Yuhui Chen,Mingcai Zhou,Haoran Li*

Main category: cs.CV

TL;DR: QDepth-VLA通过深度预测任务增强VLA模型的空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏对3D结构的理解和推理能力，影响精确控制。

Method: 设计深度专家预测VQ-VAE编码器生成的深度图量化潜在标记。

Result: 在仿真和实际任务中表现出色，具备强空间推理能力。

Conclusion: QDepth-VLA有效提升VLA模型的空间感知和操作任务性能。

Abstract: Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
models to accomplish fine-grained manipulation tasks. However, existing
approaches often lack the ability to understand and reason over the essential
3D structures necessary for precise control. To address this limitation, we
propose QDepth-VLA, a general framework that augments VLA models with an
auxiliary depth prediction task. A dedicated depth expert is designed to
predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
enabling the model to learn depth-aware representations that capture critical
geometric cues. Experimental results on the simulation benchmarks and
real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
and competitive performance on manipulation tasks.

</details>


### [44] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 提出了一种多步调整方法FMA，通过跨模态速度场学习，显著提升了复杂数据集上的特征对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法仅进行单步调整，难以处理模态特征高度纠缠的复杂数据集。

Method: 提出FMA方法，包括固定耦合策略、噪声增强策略和早停求解器，实现多步调整。

Result: FMA在多个基准和骨干网络上显著提升性能，尤其在挑战性数据集上表现突出。

Conclusion: FMA通过多步校正能力，实现了更精确和鲁棒的特征对齐。

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [45] [Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation](https://arxiv.org/abs/2510.14976)
*Shaowei Liu,Chuan Guo,Bing Zhou,Jian Wang*

Main category: cs.CV

TL;DR: Ponimator是一个基于近距离交互姿势的动画框架，利用条件扩散模型生成动态运动序列和交互姿势，支持多样化的交互动画任务。


<details>
  <summary>Details</summary>
Motivation: 人类通过近距离交互姿势可以直观推断上下文和动态变化，受此启发，研究旨在利用交互姿势先验知识实现多样化的交互动画。

Method: Ponimator使用两个条件扩散模型：姿势动画器（利用时间先验生成运动序列）和姿势生成器（利用空间先验合成交互姿势）。

Result: 实验表明，该框架在多样数据集和应用中表现出通用性、有效性和鲁棒性。

Conclusion: Ponimator通过交互姿势先验知识，成功将高质量动作捕捉数据应用于开放世界场景，支持多种交互动画任务。

Abstract: Close-proximity human-human interactive poses convey rich contextual
information about interaction dynamics. Given such poses, humans can
intuitively infer the context and anticipate possible past and future dynamics,
drawing on strong priors of human behavior. Inspired by this observation, we
propose Ponimator, a simple framework anchored on proximal interactive poses
for versatile interaction animation. Our training data consists of
close-contact two-person poses and their surrounding temporal context from
motion-capture interaction datasets. Leveraging interactive pose priors,
Ponimator employs two conditional diffusion models: (1) a pose animator that
uses the temporal prior to generate dynamic motion sequences from interactive
poses, and (2) a pose generator that applies the spatial prior to synthesize
interactive poses from a single pose, text, or both when interactive poses are
unavailable. Collectively, Ponimator supports diverse tasks, including
image-based interaction animation, reaction animation, and text-to-interaction
synthesis, facilitating the transfer of interaction knowledge from high-quality
mocap data to open-world scenarios. Empirical experiments across diverse
datasets and applications demonstrate the universality of the pose prior and
the effectiveness and robustness of our framework.

</details>


### [46] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出了一种训练自由的提示嵌入编辑方法SDeC，通过抑制场景与身份的潜在关联，显著提升了文本到图像生成中的身份一致性。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中因场景与身份关联导致的身份偏移问题，无需预先知道所有目标场景。

Method: 提出Scene De-Contextualization (SDeC)方法，通过量化SVD方向稳定性自适应调整特征值权重，抑制场景与身份的关联。

Result: 实验表明SDeC显著提升了身份一致性，同时保持了场景多样性。

Conclusion: SDeC是一种灵活且通用的解决方案，适用于实际应用中无法预先获取所有目标场景的情况。

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [47] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

TL;DR: 论文提出了一种新型AI助手，能够通过实时视频输入主动回答动态问题，具备前瞻性、及时响应和同步效率。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能在人类环境中主动理解和响应事件的AI，超越被动观察。

Method: 引入ESTP-Bench评估框架和ESTP-F1指标，提出包含数据引擎、多阶段训练策略和动态压缩技术的技术流程。

Result: 模型在多个基准测试中表现优于基线，有效解决了前瞻性、及时响应和同步效率问题。

Conclusion: 提出的方法为AI在动态环境中的主动交互提供了有效解决方案。

Abstract: Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [48] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

TL;DR: BalanceGS通过算法-系统协同设计优化3D高斯泼溅训练，解决了传统方法的效率问题，显著提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 传统3D高斯泼溅训练存在密度分配不均、计算负载不平衡和内存访问碎片化三大效率问题。

Method: 提出启发式密度控制、相似性高斯采样与合并、基于重排序的内存访问映射策略。

Result: 在NVIDIA A100 GPU上实现1.44倍训练加速，且质量损失可忽略。

Conclusion: BalanceGS高效解决了3D高斯泼溅训练的效率瓶颈，具有实际应用价值。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


### [49] [CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification](https://arxiv.org/abs/2510.14576)
*Dongwook Lee,Sol Han,Jinwhan Kim*

Main category: cs.CV

TL;DR: CALM-Net是一种基于LiDAR点云的车辆重识别网络，通过多分支架构整合边缘卷积、点注意力和曲率嵌入，提升了识别精度。


<details>
  <summary>Details</summary>
Motivation: 解决从三维点云中学习区分性特征以识别车辆的挑战。

Method: 采用多分支架构，结合边缘卷积、点注意力和曲率嵌入，学习几何和上下文特征。

Result: 在nuScenes数据集上，平均重识别精度提升1.97%。

Conclusion: 曲率信息与多分支特征学习对LiDAR点云车辆重识别有效。

Abstract: This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based
multi-branch neural network for vehicle re-identification. The proposed model
addresses the challenge of learning discriminative and complementary features
from three-dimensional point clouds to distinguish between vehicles. CALM-Net
employs a multi-branch architecture that integrates edge convolution, point
attention, and a curvature embedding that characterizes local surface variation
in point clouds. By combining these mechanisms, the model learns richer
geometric and contextual features that are well suited for the
re-identification task. Experimental evaluation on the large-scale nuScenes
dataset demonstrates that CALM-Net achieves a mean re-identification accuracy
improvement of approximately 1.97\% points compared with the strongest baseline
in our study. The results confirms the effectiveness of incorporating curvature
information into deep learning architectures and highlight the benefit of
multi-branch feature learning for LiDAR point cloud-based vehicle
re-identification.

</details>


### [50] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky,Shimon Malnick,Shai Avidan*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过自然语言实现像素级关键点定位，包含点描述符和点定位器，并引入新数据集和评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型仅支持对象或区域级理解，缺乏像素级关键点定位能力。

Method: 框架包含点描述符生成上下文描述和点定位器回归坐标，使用GRPO优化描述符。

Result: 在LlamaPointInPart数据集上表现优于基线模型。

Conclusion: 双向框架为关键点引导的图像理解和语言引导的精确定位提供新可能。

Abstract: Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.

</details>


### [51] [STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding](https://arxiv.org/abs/2510.14588)
*Zhifei Chen,Tianshuo Xu,Leyi Wu,Luozhou Wang,Dongyu Yan,Zihan You,Wenting Luo,Guo Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: STANCE是一个图像到视频生成框架，通过实例提示和密集RoPE解决运动一致性和交互问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法在保持物体运动一致性和交互性方面存在困难，主要由于运动提示编码后信息丢失和优化目标冲突。

Method: 引入实例提示（Instance Cues）生成密集2.5D运动场，并使用密集RoPE在标记空间中保持运动提示的显著性。

Result: STANCE提高了时间一致性，无需逐帧轨迹脚本，同时保持外观质量。

Conclusion: STANCE通过分离结构和外观优化，显著提升了视频生成的运动一致性和交互性。

Abstract: Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.

</details>


### [52] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 提出了一种结合SpeciesNet和CLIP的分层重分类系统，用于将高级分类标签细化到物种级别，准确率达96.5%。


<details>
  <summary>Details</summary>
Motivation: 现有动物分类模型（如SpeciesNet）因保守策略导致许多动物仅被标记为高级分类，而非物种级别。

Method: 采用五阶段流程（高置信度接受、鸟类覆盖、质心构建、三元组损失度量学习和自适应余弦距离评分），结合SpeciesNet和CLIP嵌入。

Result: 在LILA BC数据集上，成功将456个检测从高级分类细化到物种级别，准确率96.5%，64.9%达到物种级别识别。

Conclusion: 该系统显著提升了物种级别识别的能力，适用于动物分类任务。

Abstract: State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent

</details>


### [53] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 论文评估了零样本方法在野生动物图像分类中的应用，使用自监督视觉变换器，并比较了不同架构和降维技术的效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有分类器无法处理未标记野生动物图像的问题。

Method: 比较了DBSCAN和GMM等无监督聚类方法，结合CLIP、DINOv2和MegaDescriptor架构及PCA、UMAP降维技术。

Result: DINOv2结合UMAP和GMM达到88.6%准确率，1D排序在哺乳动物和鸟类中达到88.2%一致性。

Conclusion: 该方法成功应用于生产环境，加速了生物多样性监测的手动标注工作流程。

Abstract: Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.

</details>


### [54] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong,Jiaqi Gu,Qi Yang,Lubin Fan,Yue Wu,Ying Wang,Kun Ding,Shiming Xiang,Jieping Ye*

Main category: cs.CV

TL;DR: 提出了一种名为Wiki-PRF的三阶段方法，通过处理、检索和过滤阶段提升KB-VQA任务中多模态查询和检索结果的质量。


<details>
  <summary>Details</summary>
Motivation: KB-VQA任务中，现有方法在多模态查询质量和检索结果相关性方面存在不足。

Method: 包括处理、检索和过滤三阶段：处理阶段动态调用视觉工具提取信息；检索阶段整合视觉和文本特征；过滤阶段筛选相关结果。

Result: 在E-VQA和InfoSeek数据集上，答案质量显著提升（36.0和42.8），达到SOTA性能。

Conclusion: Wiki-PRF通过强化学习训练和多阶段设计，有效提升了KB-VQA任务的性能。

Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF

</details>


### [55] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding,Keisuke Fujii,Toru Tamaki*

Main category: cs.CV

TL;DR: Shot2Tactic-Caption是一个用于羽毛球视频多尺度语义和时间描述的新框架，能生成动作级和战术级描述。


<details>
  <summary>Details</summary>
Motivation: 研究羽毛球战术的动态执行，填补羽毛球视频描述数据集的空白。

Method: 采用双分支设计，包括视觉编码器、时空Transformer编码器和解码器，并引入战术单元检测器和提示引导机制。

Result: 实验证明框架有效，ResNet50时空编码器表现最佳，提示机制提升战术描述的连贯性和准确性。

Conclusion: Shot2Tactic-Caption在羽毛球视频描述中表现出色，为战术理解提供了新工具。

Abstract: Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.

</details>


### [56] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov,Eugene Khvedchenia,Borys Tymchenko,Shay Aharon,Lior Kadoch,Tomer Keren,Ofri Masad,Yonatan Geifman,Ran Zilberstein,Tuomas Rintamaki,Matthieu Le,Andrew Tao*

Main category: cs.CV

TL;DR: EVS是一种减少视频中冗余标记的简单方法，通过识别和修剪时间静态补丁来提高效率，同时保持语义保真度。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在处理长视频时的二次成本和上下文限制问题。

Method: 提出Efficient Video Sampling (EVS)，通过识别和修剪时间静态补丁来减少冗余标记。

Result: EVS显著减少标记数量，加快推理速度，并在保持准确性的同时支持更长的输入序列。

Conclusion: EVS在效率和准确性之间取得了平衡，为可扩展的视频语言理解提供了解决方案。

Abstract: Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.

</details>


### [57] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui,Johannes Schusterbauer,Timy Phan,Felix Krause,Josh Susskind,Miguel Angel Bautista,Björn Ommer*

Main category: cs.CV

TL;DR: RepTok是一种生成模型框架，通过自监督视觉变换器生成单个连续潜在令牌表示图像，结合流匹配目标和余弦相似度损失，实现高效图像生成。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用自监督学习（SSL）预训练编码器，通过微调语义令牌嵌入和生成解码器，构建紧凑且高效的潜在空间用于生成建模。

Method: 使用预训练的SSL编码器，微调语义令牌嵌入，并联合训练生成解码器，采用流匹配目标和余弦相似度损失进行正则化。

Result: 在类条件ImageNet生成和文本到图像合成中表现优异，尤其在有限训练预算下达到竞争性零样本性能。

Conclusion: RepTok展示了微调SSL表示作为紧凑潜在空间在高效生成建模中的潜力。

Abstract: We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.

</details>


### [58] [SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation](https://arxiv.org/abs/2510.14634)
*Jihyun Yu,Yoojin Oh,Wonho Bae,Mingyu Kim,Junhyug Noh*

Main category: cs.CV

TL;DR: SteeringTTA是一种基于扩散的测试时适应方法，通过伪标签驱动的奖励和多粒子轨迹平衡探索与置信度，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于梯度的扩散方法在探索和泛化能力上的局限性。

Method: 利用Feynman-Kac引导和伪标签奖励，通过多粒子轨迹和熵调度平衡探索与置信度。

Result: 在ImageNet-C上表现优于基线，无需模型更新或源数据。

Conclusion: SteeringTTA是一种高效且无需模型更新的测试时适应方法。

Abstract: Test-time adaptation (TTA) aims to correct performance degradation of deep
models under distribution shifts by updating models or inputs using unlabeled
test data. Input-only diffusion-based TTA methods improve robustness for
classification to corruptions but rely on gradient guidance, limiting
exploration and generalization across distortion types. We propose SteeringTTA,
an inference-only framework that adapts Feynman-Kac steering to guide
diffusion-based input adaptation for classification with rewards driven by
pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by
a combination of cumulative top-K probabilities and an entropy schedule, to
balance exploration and confidence. On ImageNet-C, SteeringTTA consistently
outperforms the baseline without any model updates or source data.

</details>


### [59] [In-Context Learning with Unpaired Clips for Instruction-based Video Editing](https://arxiv.org/abs/2510.14648)
*Xinyao Liao,Xianfang Zeng,Ziye Song,Zhoujie Fu,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出了一种低成本预训练策略，用于基于指令的视频编辑，通过上下文学习从未配对的视频剪辑中学习，显著提升了编辑质量和指令对齐。


<details>
  <summary>Details</summary>
Motivation: 解决基于指令的视频编辑中大规模配对数据集构建成本高和复杂性的问题。

Method: 利用未配对视频剪辑进行上下文学习预训练基础视频生成模型，再通过少量高质量配对数据微调。

Result: 在指令对齐和视觉保真度上超越现有方法，指令跟随提升12%，编辑质量提升15%。

Conclusion: 该方法通过低成本预训练和高效微调，显著提升了基于指令的视频编辑性能。

Abstract: Despite the rapid progress of instruction-based image editing, its extension
to video remains underexplored, primarily due to the prohibitive cost and
complexity of constructing large-scale paired video editing datasets. To
address this challenge, we introduce a low-cost pretraining strategy for
instruction-based video editing that leverages in-context learning from
unpaired video clips. We show that pretraining a foundation video generation
model with this strategy endows it with general editing capabilities, such as
adding, replacing, or deleting operations, according to input editing
instructions. The pretrained model can then be efficiently refined with a small
amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our
framework first pretrains on approximately 1M real video clips to learn basic
editing concepts, and subsequently fine-tunes on fewer than 150k curated
editing pairs to extend more editing tasks and improve the editing quality.
Comparative experiments show that our method surpasses existing
instruction-based video editing approaches in both instruction alignment and
visual fidelity, achieving a 12\% improvement in editing instruction following
and a 15\% improvement in editing quality.

</details>


### [60] [Decorrelation Speeds Up Vision Transformers](https://arxiv.org/abs/2510.14657)
*Kieran Carrigg,Rob van Gastel,Melda Yeghaian,Sander Dalm,Faysal Boughorbel,Marcel van Gerven*

Main category: cs.CV

TL;DR: DBP-MAE加速ViT预训练，减少计算成本和碳排放，同时提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: MAE预训练计算成本高，不适用于资源受限的工业场景。

Method: 将Decorrelated Backpropagation (DBP)集成到MAE预训练中，选择性应用于编码器。

Result: 减少预训练时间21.1%，碳排放21.4%，分割任务mIoU提升1.1点。

Conclusion: DBP能有效减少训练时间和能源消耗，同时提升ViT预训练的下游性能。

Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields
strong performance in low-label regimes but comes with substantial
computational costs, making it impractical in time- and resource-constrained
industrial settings. We address this by integrating Decorrelated
Backpropagation (DBP) into MAE pre-training, an optimization method that
iteratively reduces input correlations at each layer to accelerate convergence.
Applied selectively to the encoder, DBP achieves faster pre-training without
loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE
reduces wall-clock time to baseline performance by 21.1%, lowers carbon
emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe
similar gains when pre-training and fine-tuning on proprietary industrial data,
confirming the method's applicability in real-world scenarios. These results
demonstrate that DBP can reduce training time and energy use while improving
downstream performance for large-scale ViT pre-training.

</details>


### [61] [EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)](https://arxiv.org/abs/2510.14661)
*Weikang Yu,Vincent Nwazelibe,Xianping Ma,Xiaokang Zhang,Richard Gloaguen,Xiao Xiang Zhu,Pedram Ghamisi*

Main category: cs.CV

TL;DR: EuroMineNet是一个基于Sentinel-2多光谱影像的采矿足迹监测基准数据集，覆盖欧盟133个采矿点，支持可持续资源管理和环境治理。


<details>
  <summary>Details</summary>
Motivation: 采矿活动导致环境退化，现有数据集在时间和地理范围上有限，需要长期监测采矿引起的地表变化。

Method: 利用Sentinel-2影像构建EuroMineNet数据集，支持采矿足迹映射和变化检测任务，并评估20种深度学习模型。

Result: GeoAI方法能有效识别长期环境变化，但在短期动态检测上仍有挑战。

Conclusion: EuroMineNet通过提供时间一致且可解释的监测数据，促进可持续土地利用和环境韧性，推动GeoAI的社会环境应用。

Abstract: Mining activities are essential for industrial and economic development, but
remain a leading source of environmental degradation, contributing to
deforestation, soil erosion, and water contamination. Sustainable resource
management and environmental governance require consistent, long-term
monitoring of mining-induced land surface changes, yet existing datasets are
often limited in temporal depth or geographic scope. To address this gap, we
present EuroMineNet, the first comprehensive multitemporal benchmark for mining
footprint mapping and monitoring based on Sentinel-2 multispectral imagery.
Spanning 133 mining sites across the European Union, EuroMineNet provides
annual observations and expert-verified annotations from 2015 to 2024, enabling
GeoAI-based models to analyze environmental dynamics at a continental scale. It
supports two sustainability-driven tasks: (1) multitemporal mining footprint
mapping for consistent annual land-use delineation, evaluated with a novel
Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change
detection to capture both gradual and abrupt surface transformations.
Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI
methods effectively identify long-term environmental changes, challenges remain
in detecting short-term dynamics critical for timely mitigation. By advancing
temporally consistent and explainable mining monitoring, EuroMineNet
contributes to sustainable land-use management, environmental resilience, and
the broader goal of applying GeoAI for social and environmental good. We
release the codes and datasets by aligning with FAIR and the open science
paradigm at https://github.com/EricYu97/EuroMineNet.

</details>


### [62] [WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging](https://arxiv.org/abs/2510.14668)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Sami Azam,Asif Karim,Jemima Beissbarth,Amanda Leach*

Main category: cs.CV

TL;DR: 提出了一种名为WeCKD的弱监督链式知识蒸馏网络，通过渐进式蒸馏链提升知识传递效果，减少数据依赖，并在有限数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法存在知识退化、监督效率低、依赖强教师模型或大数据集的问题，限制了其在现实有限数据场景中的应用。

Method: 设计了WeCKD，通过构建渐进式蒸馏链，每个模型从前驱学习并精炼知识后传递，减少数据依赖并提升特征学习。

Result: 在四个耳镜成像数据集上表现优于现有监督方法，并在其他两种医学影像数据上验证了泛化能力，累计准确率提升高达23%。

Conclusion: WeCKD在有限数据场景下显著提升了知识蒸馏的效果，展示了其在现实应用中的潜力。

Abstract: Knowledge distillation (KD) has traditionally relied on a static
teacher-student framework, where a large, well-trained teacher transfers
knowledge to a single student model. However, these approaches often suffer
from knowledge degradation, inefficient supervision, and reliance on either a
very strong teacher model or large labeled datasets, which limits their
effectiveness in real-world, limited-data scenarios. To address these, we
present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that
redefines knowledge transfer through a structured sequence of interconnected
models. Unlike conventional KD, it forms a progressive distillation chain,
where each model not only learns from its predecessor but also refines the
knowledge before passing it forward. This structured knowledge transfer further
enhances feature learning, reduces data dependency, and mitigates the
limitations of one-step KD. Each model in the distillation chain is trained on
only a fraction of the dataset and demonstrates that effective learning can be
achieved with minimal supervision. Extensive evaluations across four otoscopic
imaging datasets demonstrate that it not only matches but in many cases
surpasses the performance of existing supervised methods. Experimental results
on two other datasets further underscore its generalization across diverse
medical imaging modalities, including microscopic and magnetic resonance
imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of
up to +23% over a single backbone trained on the same limited data, which
highlights its potential for real-world adoption.

</details>


### [63] [VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](https://arxiv.org/abs/2510.14672)
*Jinglei Zhang,Yuanfan Guo,Rolandos Alexandros Potamias,Jiankang Deng,Hang Xu,Chao Ma*

Main category: cs.CV

TL;DR: VTimeCoT是一个无需训练的框架，通过进度条工具和视觉时间链式思维（CoT）提升视频时间定位和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频时间定位和推理方面表现不足，需要更有效的视频理解方法。

Method: 引入进度条集成工具和高亮工具，结合视觉时间CoT过程进行跨模态推理。

Result: 在Qwen2VL-7B和GPT4o基准上显著提升了视频时间定位和推理问答性能。

Conclusion: VTimeCoT框架实现了可组合且可解释的推理过程，提升了视频理解能力。

Abstract: In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io

</details>


### [64] [Leveraging Learned Image Prior for 3D Gaussian Compression](https://arxiv.org/abs/2510.14705)
*Seungjoo Shin,Jaesik Park,Sunghyun Cho*

Main category: cs.CV

TL;DR: 提出了一种基于学习图像先验的3DGS压缩框架，通过恢复压缩引起的质量退化，显著提升了压缩率和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法缺乏学习先验，限制了压缩率和失真之间的进一步优化。

Method: 利用初始压缩的高斯分布，通过恢复网络建模图像空间中的压缩伪影，并引入粗渲染残差作为辅助信息。

Result: 实验表明，该方法在压缩率和渲染质量上优于现有方法，且存储需求更低。

Conclusion: 该框架兼容现有压缩方法，显著提升了3DGS压缩的性能。

Abstract: Compression techniques for 3D Gaussian Splatting (3DGS) have recently
achieved considerable success in minimizing storage overhead for 3D Gaussians
while preserving high rendering quality. Despite the impressive storage
reduction, the lack of learned priors restricts further advances in the
rate-distortion trade-off for 3DGS compression tasks. To address this, we
introduce a novel 3DGS compression framework that leverages the powerful
representational capacity of learned image priors to recover
compression-induced quality degradation. Built upon initially compressed
Gaussians, our restoration network effectively models the compression artifacts
in the image space between degraded and original Gaussians. To enhance the
rate-distortion performance, we provide coarse rendering residuals into the
restoration network as side information. By leveraging the supervision of
restored images, the compressed Gaussians are refined, resulting in a highly
compact representation with enhanced rendering performance. Our framework is
designed to be compatible with existing Gaussian compression methods, making it
broadly applicable across different baselines. Extensive experiments validate
the effectiveness of our framework, demonstrating superior rate-distortion
performance and outperforming the rendering quality of state-of-the-art 3DGS
compression methods while requiring substantially less storage.

</details>


### [65] [Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery](https://arxiv.org/abs/2510.14709)
*Caleb Robinson,Kimberly T. Goetz,Christin B. Khan,Meredith Sackett,Kathleen Leonard,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: 提出了一种半自动化的鲸鱼检测方法，通过统计异常检测和专家标注界面，显著减少了专家检查范围。


<details>
  <summary>Details</summary>
Motivation: 传统鲸鱼监测方法成本高且难以扩展，而现有自动化检测方法面临标注数据不足、图像质量差异大等问题。

Method: 使用统计异常检测方法标记空间异常点，并结合专家标注界面进行快速验证。

Result: 在三个基准场景中，召回率达到90.3%至96.4%，专家检查范围减少了99.8%。

Conclusion: 该方法无需标注数据，为未来空间辅助海洋哺乳动物监测提供了可扩展的第一步。

Abstract: Effective monitoring of whale populations is critical for conservation, but
traditional survey methods are expensive and difficult to scale. While prior
work has shown that whales can be identified in very high-resolution (VHR)
satellite imagery, large-scale automated detection remains challenging due to a
lack of annotated imagery, variability in image quality and environmental
conditions, and the cost of building robust machine learning pipelines over
massive remote sensing archives. We present a semi-automated approach for
surfacing possible whale detections in VHR imagery using a statistical anomaly
detection method that flags spatial outliers, i.e. "interesting points". We
pair this detector with a web-based labeling interface designed to enable
experts to quickly annotate the interesting points. We evaluate our system on
three benchmark scenes with known whale annotations and achieve recalls of
90.3% to 96.4%, while reducing the area requiring expert inspection by up to
99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method
does not rely on labeled training data and offers a scalable first step toward
future machine-assisted marine mammal monitoring from space. We have open
sourced this pipeline at https://github.com/microsoft/whales.

</details>


### [66] [Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models](https://arxiv.org/abs/2510.14713)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 本文首次系统评估了深度视频相机运动分类模型在历史档案影片上的表现，发现Video Swin Transformer表现最佳，准确率达80.25%。


<details>
  <summary>Details</summary>
Motivation: 探索现有相机运动分类方法在历史影片上的泛化能力，填补研究空白。

Method: 总结了代表性方法和数据集，评估了五种标准视频分类模型在HISTORIAN数据集上的表现。

Result: Video Swin Transformer表现最佳，准确率达80.25%，显示在有限训练数据下的强收敛性。

Conclusion: 研究揭示了现有模型在低质量视频上的适应挑战与潜力，为未来结合多模态输入和时序架构的工作提供了动机。

Abstract: Camera movement conveys spatial and narrative information essential for
understanding video content. While recent camera movement classification (CMC)
methods perform well on modern datasets, their generalization to historical
footage remains unexplored. This paper presents the first systematic evaluation
of deep video CMC models on archival film material. We summarize representative
methods and datasets, highlighting differences in model design and label
definitions. Five standard video classification models are assessed on the
HISTORIAN dataset, which includes expert-annotated World War II footage. The
best-performing model, Video Swin Transformer, achieves 80.25% accuracy,
showing strong convergence despite limited training data. Our findings
highlight the challenges and potential of adapting existing models to
low-quality video and motivate future work combining diverse input modalities
and temporal architectures.

</details>


### [67] [Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection](https://arxiv.org/abs/2510.14726)
*Dingzhou Xie,Rushi Lan,Cheng Pang,Enhao Ning,Jiahao Zeng,Wei Zheng*

Main category: cs.CV

TL;DR: 提出了一种跨层特征自注意力模块（CFSAM），通过建模多尺度特征图的局部和全局依赖关系，显著提升了目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制多局限于单层或双层特征，忽略了多尺度表示间的跨层依赖关系，限制了检测性能。

Method: CFSAM包含卷积局部特征提取器、基于Transformer的全局建模单元和特征融合机制，集成到SSD300框架中。

Result: 在PASCAL VOC和COCO数据集上分别达到78.6%和52.1% mAP，优于基线和其他注意力模块。

Conclusion: 显式建模跨层注意力对多尺度目标检测至关重要，CFSAM在性能和效率上均有优势。

Abstract: Recent object detection methods have made remarkable progress by leveraging
attention mechanisms to improve feature discriminability. However, most
existing approaches are confined to refining single-layer or fusing dual-layer
features, overlooking the rich inter-layer dependencies across multi-scale
representations. This limits their ability to capture comprehensive contextual
information essential for detecting objects with large scale variations. In
this paper, we propose a novel Cross-Layer Feature Self-Attention Module
(CFSAM), which holistically models both local and global dependencies within
multi-scale feature maps. CFSAM consists of three key components: a
convolutional local feature extractor, a Transformer-based global modeling unit
that efficiently captures cross-layer interactions, and a feature fusion
mechanism to restore and enhance the original representations. When integrated
into the SSD300 framework, CFSAM significantly boosts detection performance,
achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO
(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the
module accelerates convergence during training without introducing substantial
computational overhead. Our work highlights the importance of explicit
cross-layer attention modeling in advancing multi-scale object detection.

</details>


### [68] [Free-Grained Hierarchical Recognition](https://arxiv.org/abs/2510.14737)
*Seulki Park,Zilin Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 论文提出ImageNet-F基准和free-grain学习方法，解决层次分类中混合粒度标注问题。


<details>
  <summary>Details</summary>
Motivation: 现实标注粒度不一，现有方法假设完整细粒度标注不切实际。

Method: 利用CLIP模拟混合粒度标注，提出伪属性和半监督学习增强语义和视觉指导。

Result: 基准和方法显著提升混合监督下的性能。

Conclusion: ImageNet-F和free-grain学习推动了现实约束下的层次分类进展。

Abstract: Hierarchical image classification predicts labels across a semantic taxonomy,
but existing methods typically assume complete, fine-grained annotations, an
assumption rarely met in practice. Real-world supervision varies in
granularity, influenced by image quality, annotator expertise, and task
demands; a distant bird may be labeled Bird, while a close-up reveals Bald
eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet
and structured into cognitively inspired basic, subordinate, and fine-grained
levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,
mixed-granularity labels reflecting human annotation behavior. We propose
free-grain learning, with heterogeneous supervision across instances. We
develop methods that enhance semantic guidance via pseudo-attributes from
vision-language models and visual guidance via semi-supervised learning. These,
along with strong baselines, substantially improve performance under mixed
supervision. Together, our benchmark and methods advance hierarchical
classification under real-world constraints.

</details>


### [69] [DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models](https://arxiv.org/abs/2510.14741)
*Simone Carnemolla,Matteo Pennisi,Sarinda Samarasinghe,Giovanni Bellitto,Simone Palazzo,Daniela Giordano,Mubarak Shah,Concetto Spampinato*

Main category: cs.CV

TL;DR: DEXTER是一个无需数据的框架，利用扩散模型和大语言模型生成视觉分类器的全局文本解释，无需训练数据或真实标签。


<details>
  <summary>Details</summary>
Motivation: 提高机器学习模型的透明度和可信度，通过自然语言解释分类器的决策过程。

Method: 通过优化文本提示合成激活目标分类器的类条件图像，生成详细自然语言报告。

Result: 在ImageNet等数据集上，DEXTER在全局模型解释和类级偏差报告方面优于现有方法。

Conclusion: DEXTER提供了一种灵活且准确的方法，用于揭示视觉分类器的内部机制和偏差。

Abstract: Understanding and explaining the behavior of machine learning models is
essential for building transparent and trustworthy AI systems. We introduce
DEXTER, a data-free framework that employs diffusion models and large language
models to generate global, textual explanations of visual classifiers. DEXTER
operates by optimizing text prompts to synthesize class-conditional images that
strongly activate a target classifier. These synthetic samples are then used to
elicit detailed natural language reports that describe class-specific decision
patterns and biases. Unlike prior work, DEXTER enables natural language
explanation about a classifier's decision process without access to training
data or ground-truth labels. We demonstrate DEXTER's flexibility across three
tasks-activation maximization, slice discovery and debiasing, and bias
explanation-each illustrating its ability to uncover the internal mechanisms of
visual classifiers. Quantitative and qualitative evaluations, including a user
study, show that DEXTER produces accurate, interpretable outputs. Experiments
on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms
existing approaches in global model explanation and class-level bias reporting.
Code is available at https://github.com/perceivelab/dexter.

</details>


### [70] [LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement](https://arxiv.org/abs/2510.14753)
*Xu Wu,Zhihui Lai,Xianxu Hou,Jie Zhou,Ya-nan Zhang,Linlin Shen*

Main category: cs.CV

TL;DR: LightQANet通过量化与自适应特征学习提升低光图像增强效果，解决了现有方法在纹理恢复、颜色一致性和伪影方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法因像素级信息严重退化，难以提取可靠特征，导致效果不佳。

Method: 提出LightQANet框架，包含静态建模的Light Quantization Module（LQM）和动态适应的Light-Aware Prompt Module（LAPM）。

Result: 在多个低光数据集上取得最优性能，显著提升图像质量。

Conclusion: LightQANet通过量化与自适应学习，实现了跨光照条件的鲁棒图像增强。

Abstract: Low-light image enhancement (LLIE) aims to improve illumination while
preserving high-quality color and texture. However, existing methods often fail
to extract reliable feature representations due to severely degraded
pixel-level information under low-light conditions, resulting in poor texture
restoration, color inconsistency, and artifact. To address these challenges, we
propose LightQANet, a novel framework that introduces quantized and adaptive
feature learning for low-light enhancement, aiming to achieve consistent and
robust image quality across diverse lighting conditions. From the static
modeling perspective, we design a Light Quantization Module (LQM) to explicitly
extract and quantify illumination-related factors from image features. By
enforcing structured light factor learning, LQM enhances the extraction of
light-invariant representations and mitigates feature inconsistency across
varying illumination levels. From the dynamic adaptation perspective, we
introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors
into learnable prompts to dynamically guide the feature learning process. LAPM
enables the model to flexibly adapt to complex and continuously changing
lighting conditions, further improving image enhancement. Extensive experiments
on multiple low-light datasets demonstrate that our method achieves
state-of-the-art performance, delivering superior qualitative and quantitative
results across various challenging lighting scenarios.

</details>


### [71] [Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality](https://arxiv.org/abs/2510.14765)
*Giuseppe Lorenzo Catalano,Agata Marta Soccini*

Main category: cs.CV

TL;DR: 提出了一种基于无条件扩散模型的方法，用于重建火星表面，填补高度图中的缺失值，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 火星地形数据存在缺失值，现有插值方法难以保持几何一致性，需要更准确的重建方法。

Method: 使用无条件扩散模型，在12000个火星高度图上训练，采用非均匀缩放策略捕捉多尺度特征。

Result: 在1000个样本上评估，RMSE提升4-15%，LPIPS提升29-81%，优于现有方法。

Conclusion: 该方法在火星地形重建中表现出色，为空间探索任务提供了更可靠的模拟数据。

Abstract: Space exploration increasingly relies on Virtual Reality for several tasks,
such as mission planning, multidisciplinary scientific analysis, and astronaut
training. A key factor for the reliability of the simulations is having
accurate 3D representations of planetary terrains. Extraterrestrial heightmaps
derived from satellite imagery often contain missing values due to acquisition
and transmission constraints. Mars is among the most studied planets beyond
Earth, and its extensive terrain datasets make the Martian surface
reconstruction a valuable task, although many areas remain unmapped. Deep
learning algorithms can support void-filling tasks; however, whereas Earth's
comprehensive datasets enables the use of conditional methods, such approaches
cannot be applied to Mars. Current approaches rely on simpler interpolation
techniques which, however, often fail to preserve geometric coherence. In this
work, we propose a method for reconstructing the surface of Mars based on an
unconditional diffusion model. Training was conducted on an augmented dataset
of 12000 Martian heightmaps derived from NASA's HiRISE survey. A
non-homogeneous rescaling strategy captures terrain features across multiple
scales before resizing to a fixed 128x128 model resolution. We compared our
method against established void-filling and inpainting techniques, including
Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an
evaluation set of 1000 samples. Results show that our approach consistently
outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)
and perceptual similarity (29-81% on LPIPS) with the original data.

</details>


### [72] [MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks](https://arxiv.org/abs/2510.14770)
*Zhang Nengbo,Hann Woei Ho,Ye Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的运动信号框架，用于微型飞行器（MAV）群在受限环境中的高效通信。


<details>
  <summary>Details</summary>
Motivation: 传统无线电通信在频谱拥堵、干扰和高功耗方面存在问题，受蜜蜂摇摆舞启发，提出视觉通信方案。

Method: 通过飞行模式传递信息，使用事件相机捕获信号，并设计基于事件帧的分割模型和轻量级脉冲神经网络（SNN）进行解码。

Result: 实验验证了框架的准确解码和低功耗特性。

Conclusion: 该框架为受限环境中的MAV通信提供了一种高效节能的替代方案。

Abstract: Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in
environments, where conventional radio-based methods suffer from spectrum
congestion, jamming, and high power consumption. Inspired by the waggle dance
of honeybees, which efficiently communicate the location of food sources
without sound or contact, we propose a novel visual communication framework for
MAV swarms using motion-based signaling. In this framework, MAVs convey
information, such as heading and distance, through deliberate flight patterns,
which are passively captured by event cameras and interpreted using a
predefined visual codebook of four motion primitives: vertical (up/down),
horizontal (left/right), left-to-up-to-right, and left-to-down-to-right,
representing control symbols (``start'', ``end'', ``1'', ``0''). To decode
these signals, we design an event frame-based segmentation model and a
lightweight Spiking Neural Network (SNN) for action recognition. An integrated
decoding algorithm then combines segmentation and classification to robustly
interpret MAV motion sequences. Experimental results validate the framework's
effectiveness, which demonstrates accurate decoding and low power consumption,
and highlights its potential as an energy-efficient alternative for MAV
communication in constrained environments.

</details>


### [73] [CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection](https://arxiv.org/abs/2510.14792)
*Hojun Choi,Youngsun Lim,Jaeyo Shin,Hyunjung Shim*

Main category: cs.CV

TL;DR: CoT-PL框架通过视觉链式推理和对比背景学习提升开放词汇目标检测的鲁棒性，在拥挤或遮挡场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇目标检测方法依赖直接图像-文本匹配，缺乏中间推理步骤，导致在复杂场景中鲁棒性不足。

Method: 提出CoT-PL框架，将目标理解分解为区域感知、零样本类别识别和背景分离三步，并结合对比背景学习（CBL）。

Result: 在拥挤和遮挡场景中，新类别伪标签质量分别提升103.4%和168.4%；在COCO和LVIS数据集上达到新SOTA。

Conclusion: CoT-PL通过结构化推理和特征解耦显著提升了开放词汇目标检测的性能，尤其在复杂场景中表现突出。

Abstract: Open-vocabulary object detection (OVD) seeks to recognize and localize object
categories beyond those seen during training. Recent approaches typically
leverage vision-language models (VLMs) to generate pseudo-labels using
image-text alignment, allowing detectors to generalize to unseen classes
without explicit supervision. However, these methods depend heavily on direct
image-text matching, neglecting the intermediate reasoning steps essential for
interpreting semantically complex scenes. This results in limited robustness
when confronted with crowded or occluded visual contexts. In this paper, we
introduce CoT-PL, a new framework that employs structured visual
chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL
decomposes object understanding into three interpretable steps: (1) region
perception even for unseen objects, (2) category recognition via zero-shot
reasoning, and (3) background grounding to separate semantically complex
objects. Crucially, the third step naturally motivates our contrastive
background learning (CBL) that uses the pre-computed background cues as
negatives to promote feature disentanglement between objects and background. In
this way, CoT reasoning and CBL form an integrated pipeline tailored to robust
pseudo-labeling in crowded or occluded scenes. Notably, in these two settings,
our novel-class pseudo-label quality achieves relative improvements of 103.4%
and 168.4% over the best prior, respectively. Our extensive experiments
demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9
mask AP on LVIS for novel classes, setting a new state of the art.

</details>


### [74] [Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images](https://arxiv.org/abs/2510.14800)
*Usama Sajjad,Abdul Rehman Akbar,Ziyu Su,Deborah Knight,Wendy L. Frankel,Metin N. Gurcan,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: PRISM是一种新型可解释AI模型，用于结直肠癌预后预测，通过整合空间形态的连续变异谱，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法可能忽略器官特异性关键形态模式，影响肿瘤行为和治疗反应预测。

Method: PRISM基于424名III期结直肠癌患者的874万张组织学图像训练，捕捉形态多样性。

Result: PRISM在五年总生存期预测中表现优异（AUC=0.70），准确率提升15%-23%。

Conclusion: PRISM具有性别无关的稳健性，且在临床病理亚组中表现稳定，验证了治疗无生存差异的发现。

Abstract: Colorectal cancer (CRC) remains the third most prevalent malignancy globally,
with approximately 154,000 new cases and 54,000 projected deaths anticipated
for 2025. The recent advancement of foundation models in computational
pathology has been largely propelled by task agnostic methodologies that can
overlook organ-specific crucial morphological patterns that represent distinct
biological processes that can fundamentally influence tumor behavior,
therapeutic response, and patient outcomes. The aim of this study is to develop
a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated
Spatial Morphology), that incorporates a continuous variability spectrum within
each distinct morphology to characterize phenotypic diversity and reflecting
the principle that malignant transformation occurs through incremental
evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained
on 8.74 million histological images extracted from surgical resection specimens
of 424 patients with stage III CRC. PRISM achieved superior prognostic
performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;
HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific
methods by 15% and AI foundation models by ~23% accuracy. It showed
sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable
performance across clinicopathological subgroups, with minimal accuracy
fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,
replicating the Alliance cohort finding of no survival difference between
treatments.

</details>


### [75] [Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks](https://arxiv.org/abs/2510.14803)
*Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Szymon Płotka,Jieneng Chen,Qi Chen,Zheren Zhu,Jakub Prządo,Ibrahim E. Hamacı,Sezgin Er,Yuhan Wang,Ashwin Kumar,Bjoern Menze,Jarosław B. Ćwikła,Yuyin Zhou,Akshay S. Chaudhari,Curtis P. Langlotz,Sergio Decherchi,Andrea Cavalli,Kang Wang,Yang Yang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: R-Super利用医学报告训练AI进行肿瘤分割，减少对人工标注的需求，性能接近甚至超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 早期肿瘤检测对挽救生命至关重要，但传统方法依赖昂贵的人工标注肿瘤掩膜，限制了AI模型的规模化应用。

Method: R-Super通过利用医学报告中的描述信息训练AI模型，替代或补充人工标注的肿瘤掩膜。

Result: 在101,654份报告上训练的模型性能接近723份掩膜训练的模型，结合报告和掩膜后敏感性和特异性分别提升13%和8%。

Conclusion: R-Super挑战了传统依赖大规模人工标注的理念，为多种肿瘤的早期检测提供了可扩展的解决方案。

Abstract: Early tumor detection save lives. Each year, more than 300 million computed
tomography (CT) scans are performed worldwide, offering a vast opportunity for
effective cancer screening. However, detecting small or early-stage tumors on
these CT scans remains challenging, even for experts. Artificial intelligence
(AI) models can assist by highlighting suspicious regions, but training such
models typically requires extensive tumor masks--detailed, voxel-wise outlines
of tumors manually drawn by radiologists. Drawing these masks is costly,
requiring years of effort and millions of dollars. In contrast, nearly every CT
scan in clinical practice is already accompanied by medical reports describing
the tumor's size, number, appearance, and sometimes, pathology
results--information that is rich, abundant, and often underutilized for AI
training. We introduce R-Super, which trains AI to segment tumors that match
their descriptions in medical reports. This approach scales AI training with
large collections of readily available medical reports, substantially reducing
the need for manually drawn tumor masks. When trained on 101,654 reports, AI
models achieved performance comparable to those trained on 723 masks. Combining
reports and masks further improved sensitivity by +13% and specificity by +8%,
surpassing radiologists in detecting five of the seven tumor types. Notably,
R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,
bladder, uterus, and esophagus, for which no public masks or AI models
previously existed. This study challenges the long-held belief that
large-scale, labor-intensive tumor mask creation is indispensable, establishing
a scalable and accessible path toward early detection across diverse tumor
types.
  We plan to release our trained models, code, and dataset at
https://github.com/MrGiovanni/R-Super

</details>


### [76] [Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning](https://arxiv.org/abs/2510.14819)
*Ji Cao,Yu Wang,Tongya Zheng,Zujie Ren,Canghong Jin,Gang Chen,Mingli Song*

Main category: cs.CV

TL;DR: PRTraj框架通过结合环境感知和路线选择建模，提升了轨迹表示学习的效果。


<details>
  <summary>Details</summary>
Motivation: 现有TRL方法忽略了轨迹形成的外部环境和内部路线选择行为，PRTraj旨在填补这一空白。

Method: PRTraj包含环境感知模块和多粒度环境语义捕捉，以及路线选择编码器来建模轨迹中的决策序列。

Result: 在3个真实数据集和5个下游任务中验证了PRTraj的有效性和泛化能力，且在少样本场景下表现稳健。

Conclusion: PRTraj通过统一环境感知和路线选择建模，显著提升了轨迹表示学习的性能。

Abstract: Trajectory Representation Learning (TRL) aims to encode raw trajectories into
low-dimensional vectors, which can then be leveraged in various downstream
tasks, including travel time estimation, location prediction, and trajectory
similarity analysis. However, existing TRL methods suffer from a key oversight:
treating trajectories as isolated spatio-temporal sequences, without
considering the external environment and internal route choice behavior that
govern their formation. To bridge this gap, we propose a novel framework that
unifies comprehensive environment \textbf{P}erception and explicit
\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representation
learning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces an
Environment Perception Module to enhance the road network by capturing
multi-granularity environmental semantics from surrounding POI distributions.
Building on this environment-aware backbone, a Route Choice Encoder then
captures the route choice behavior inherent in each trajectory by modeling its
constituent road segment transitions as a sequence of decisions. These
route-choice-aware representations are finally aggregated to form the global
trajectory embedding. Extensive experiments on 3 real-world datasets across 5
downstream tasks validate the effectiveness and generalizability of PRTraj.
Moreover, PRTraj demonstrates strong data efficiency, maintaining robust
performance under few-shot scenarios. Our code is available at:
https://anonymous.4open.science/r/PRTraj.

</details>


### [77] [FraQAT: Quantization Aware Training with Fractional bits](https://arxiv.org/abs/2510.14823)
*Luca Morreale,Alberto Gil C. P. Ramos,Malcolm Chadwick,Mehid Noroozi,Ruchika Chavhan,Abhinav Mehrotra,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: 提出了一种新的分数位量化方法（short），在保持生成质量的同时，逐步将模型精度从32位降至4位，适用于智能手机部署。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在智能手机上因内存和计算限制无法部署的问题，同时保持模型质量。

Method: 采用分数位量化方法，逐步降低模型精度，并在优化过程中利用分数位维持高质量生成。

Result: 在多种扩散模型上表现优异，FiD比标准QAT低4-7%，成功在三星S25U上部署。

Conclusion: short方法有效平衡了模型效率和生成质量，适用于资源受限设备。

Abstract: State-of-the-art (SOTA) generative models have demonstrated impressive
capabilities in image synthesis or text generation, often with a large capacity
model. However, these large models cannot be deployed on smartphones due to the
limited availability of on-board memory and computations. Quantization methods
lower the precision of the model parameters, allowing for efficient
computations, \eg, in \INT{8}. Although aggressive quantization addresses
efficiency and memory constraints, preserving the quality of the model remains
a challenge. To retain quality in previous aggressive quantization, we propose
a new fractional bits quantization (\short) approach. The novelty is a simple
yet effective idea: we progressively reduce the model's precision from 32 to 4
bits per parameter, and exploit the fractional bits during optimization to
maintain high generation quality. We show that the \short{} yields improved
quality on a variety of diffusion models, including SD3.5-Medium, Sana,
\pixart, and FLUX.1-schnell, while achieving $4-7\%$ lower FiD than standard
QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the
Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).

</details>


### [78] [Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data](https://arxiv.org/abs/2510.14831)
*Qi Chen,Xinze Zhou,Chen Liu,Hao Chen,Wenxuan Li,Zekun Jiang,Ziyan Huang,Yuxuan Zhao,Dexin Yu,Junjun He,Yefeng Zheng,Ling Shao,Alan Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: AI肿瘤分割受限于缺乏大规模标注数据。通过合成数据，仅需500真实扫描即可达到1500扫描的性能。AbdomenAtlas 2.0数据集包含10,135 CT扫描，显著提升AI分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI肿瘤分割中数据不足的问题，通过合成数据和扩大标注数据集提升模型性能。

Method: 使用合成数据辅助训练，并构建AbdomenAtlas 2.0数据集，包含多器官肿瘤标注。

Result: 在分布内测试中DSC提升7%，分布外测试提升16%。

Conclusion: 合成数据和大型标注数据集可显著提升AI肿瘤分割性能，AbdomenAtlas 2.0为未来研究提供坚实基础。

Abstract: AI for tumor segmentation is limited by the lack of large, voxel-wise
annotated datasets, which are hard to create and require medical experts. In
our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found
that AI performance stopped improving after 1,500 scans. With synthetic data,
we reached the same performance using only 500 real scans. This finding
suggests that synthetic data can steepen data scaling laws, enabling more
efficient model training than real data alone. Motivated by these lessons, we
created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130
tumor instances per-voxel manually annotated in six organs (pancreas, liver,
kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23
expert radiologists, it is several orders of magnitude larger than existing
public tumor datasets. While we continue expanding the dataset, the current
version of AbdomenAtlas 2.0 already provides a strong foundation--based on
lessons from the JHH dataset--for training AI to segment tumors in six organs.
It achieves notable improvements over public datasets, with a +7% DSC gain on
in-distribution tests and +16% on out-of-distribution tests.

</details>


### [79] [ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints](https://arxiv.org/abs/2510.14847)
*Meiqi Wu,Jiashu Zhu,Xiaokun Feng,Chubin Chen,Chen Zhu,Bingze Song,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: ImagerySearch是一种动态调整推理搜索空间和奖励函数的测试时搜索策略，用于提升想象力场景下的视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在想象力场景中表现不佳，主要因为罕见概念的长距离语义关系超出训练分布。

Method: 提出ImagerySearch，根据提示的语义关系动态调整搜索空间和奖励函数。

Result: 在LDT-Bench和VBench上表现优于基线方法，生成更连贯的视频。

Conclusion: ImagerySearch有效提升想象力场景的视频生成质量，并发布了LDT-Bench促进未来研究。

Abstract: Video generation models have achieved remarkable progress, particularly
excelling in realistic scenarios; however, their performance degrades notably
in imaginative scenarios. These prompts often involve rarely co-occurring
concepts with long-distance semantic relationships, falling outside training
distributions. Existing methods typically apply test-time scaling for improving
video quality, but their fixed search spaces and static reward designs limit
adaptability to imaginative scenarios. To fill this gap, we propose
ImagerySearch, a prompt-guided adaptive test-time search strategy that
dynamically adjusts both the inference search space and reward function
according to semantic relationships in the prompt. This enables more coherent
and visually plausible videos in challenging imaginative settings. To evaluate
progress in this direction, we introduce LDT-Bench, the first dedicated
benchmark for long-distance semantic prompts, consisting of 2,839 diverse
concept pairs and an automated protocol for assessing creative generation
capabilities. Extensive experiments show that ImagerySearch consistently
outperforms strong video generation baselines and existing test-time scaling
approaches on LDT-Bench, and achieves competitive improvements on VBench,
demonstrating its effectiveness across diverse prompt types. We will release
LDT-Bench and code to facilitate future research on imaginative video
generation.

</details>


### [80] [A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation](https://arxiv.org/abs/2510.14855)
*Harsha Kotla,Arun Kumar Rajasekaran,Hannah Rana*

Main category: cs.CV

TL;DR: 提出了一种深度学习框架，既能分类皮肤病变，又能量化ABCDE特征，模拟病变演变过程。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将ABCDE分类视为黑箱，缺乏可解释性，难以与临床标准关联。

Method: 开发了一个深度学习框架，量化ABCD特征并模拟其演变过程，可视化潜在空间中的特征轨迹。

Result: 分类准确率89%，AUC为0.96；特征评估在不对称性、颜色变化和直径预测上表现良好，但边界不规则性较难建模。

Conclusion: 该框架将机器学习诊断与临床标准关联，有助于理解皮肤癌进展。

Abstract: Early detection of melanoma has grown to be essential because it
significantly improves survival rates, but automated analysis of skin lesions
still remains challenging. ABCDE, which stands for Asymmetry, Border
irregularity, Color variation, Diameter, and Evolving, is a well-known
classification method for skin lesions, but most deep learning mechanisms treat
it as a black box, as most of the human interpretable features are not
explained. In this work, we propose a deep learning framework that both
classifies skin lesions into categories and also quantifies scores for each
ABCD feature. It simulates the evolution of these features over time in order
to represent the E aspect, opening more windows for future exploration. The A,
B, C, and D values are quantified particularly within this work. Moreover, this
framework also visualizes ABCD feature trajectories in latent space as skin
lesions evolve from benign nevuses to malignant melanoma. The experiments are
conducted using the HAM10000 dataset that contains around ten thousand images
of skin lesions of varying stages. In summary, the classification worked with
an accuracy of around 89 percent, with melanoma AUC being 0.96, while the
feature evaluation performed well in predicting asymmetry, color variation, and
diameter, though border irregularity remains more difficult to model. Overall,
this work provides a deep learning framework that will allow doctors to link ML
diagnoses to clinically relevant criteria, thus improving our understanding of
skin cancer progression.

</details>


### [81] [Multi-modal video data-pipelines for machine learning with minimal human supervision](https://arxiv.org/abs/2510.14862)
*Mihai-Cristian Pîrvu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出了一种多模态学习方法，结合多种视觉模态，使用预训练专家模型和自主数据管道，展示了PHG-MAE模型在低参数量下的竞争力，并应用于实时语义分割和深度估计。


<details>
  <summary>Details</summary>
Motivation: 现实世界本质上是多模态的，但传统机器学习模型多为单模态或双模态，需要整合所有独立模态以真正理解世界。

Method: 使用预训练专家模型和自主数据管道，结合PHG-MAE模型处理多模态数据，并将其蒸馏为低参数模型。

Result: PHG-MAE模型在低参数量（<1M）下表现与高参数量模型（~300M）相当，并成功应用于实时语义分割和深度估计。

Conclusion: 通过多模态学习和自主数据管道，实现了高效的低参数模型，展示了在多模态任务中的潜力。

Abstract: The real-world is inherently multi-modal at its core. Our tools observe and
take snapshots of it, in digital form, such as videos or sounds, however much
of it is lost. Similarly for actions and information passing between humans,
languages are used as a written form of communication. Traditionally, Machine
Learning models have been unimodal (i.e. rgb -> semantic or text ->
sentiment_class). Recent trends go towards bi-modality, where images and text
are learned together, however, in order to truly understand the world, we need
to integrate all these independent modalities. In this work we try to combine
as many visual modalities as we can using little to no human supervision. In
order to do this, we use pre-trained experts and procedural combinations
between them on top of raw videos using a fully autonomous data-pipeline, which
we also open-source. We then make use of PHG-MAE, a model specifically designed
to leverage multi-modal data. We show that this model which was efficiently
distilled into a low-parameter (<1M) can have competitive results compared to
models of ~300M parameters. We deploy this model and analyze the use-case of
real-time semantic segmentation from handheld devices or webcams on commodity
hardware. Finally, we deploy other off-the-shelf models using the same
framework, such as DPT for near real-time depth estimation.

</details>


### [82] [Benchmarking Multimodal Large Language Models for Face Recognition](https://arxiv.org/abs/2510.14866)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在视觉与语言任务中表现优异，但在人脸识别领域的潜力尚未充分探索。本文系统评估了开源MLLMs在标准人脸识别数据集上的性能，发现其在零样本应用中落后于专用模型。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在人脸识别中的潜力，填补现有研究空白，并与专用模型进行性能对比。

Method: 在多个标准人脸识别数据集（如LFW、CALFW等）上对MLLMs进行系统性基准测试。

Result: MLLMs能捕捉丰富的语义线索，但在高精度识别场景中表现不及专用模型。

Conclusion: 该基准为提升MLLM在人脸识别中的性能提供了基础，并为下一代高精度模型的开发提供了参考。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance
across diverse vision-and-language tasks. However, their potential in face
recognition remains underexplored. In particular, the performance of
open-source MLLMs needs to be evaluated and compared with existing face
recognition models on standard benchmarks with similar protocol. In this work,
we present a systematic benchmark of state-of-the-art MLLMs for face
recognition on several face recognition datasets, including LFW, CALFW, CPLFW,
CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich
semantic cues useful for face-related tasks, they lag behind specialized models
in high-precision recognition scenarios in zero-shot applications. This
benchmark provides a foundation for advancing MLLM-based face recognition,
offering insights for the design of next-generation models with higher accuracy
and generalization. The source code of our benchmark is publicly available in
the project page.

</details>


### [83] [TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions](https://arxiv.org/abs/2510.14874)
*Guangyi Han,Wei Zhai,Yuhang Yang,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 论文提出了Free-Form HOI Generation方法，通过细粒度意图控制生成多样且物理合理的手-物交互，并构建了WildO2数据集支持任务。


<details>
  <summary>Details</summary>
Motivation: 现有HOI生成研究局限于固定抓取模式，无法捕捉日常交互的多样性。

Method: 提出TOUCH框架，基于多级扩散模型，结合接触建模和物理约束生成手部姿态。

Result: 实验表明方法能生成可控、多样且物理合理的交互。

Conclusion: Free-Form HOI Generation扩展了HOI的多样性，为日常活动提供了更真实的交互生成。

Abstract: Hand-object interaction (HOI) is fundamental for humans to express intent.
Existing HOI generation research is predominantly confined to fixed grasping
patterns, where control is tied to physical priors such as force closure or
generic intent instructions, even when expressed through elaborate language.
Such an overly general conditioning imposes a strong inductive bias for stable
grasps, thus failing to capture the diversity of daily HOI. To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating. To support this task, we construct WildO2, an in-the-wild
diverse 3D HOI dataset, which includes diverse HOI derived from internet
videos. Specifically, it contains 4.4k unique interactions across 92 intents
and 610 object categories, each with detailed semantic annotations. Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors. This process leverages
explicit contact modeling for conditioning and is subsequently refined with
contact consistency and physical constraints to ensure realism. Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.
The project page is $\href{https://guangyid.github.io/hoi123touch}{here}$.

</details>


### [84] [BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data](https://arxiv.org/abs/2510.14876)
*Roni Goldshmidt,Hamish Scott,Lorenzo Niccolini,Shizhan Zhu,Daniel Moura,Orly Zvitia*

Main category: cs.CV

TL;DR: BADAS是一种新型碰撞预测模型，专注于区分与自车相关的威胁和无关的随机事故，减少误报。


<details>
  <summary>Details</summary>
Motivation: 现有碰撞预测方法难以区分自车威胁和无关事故，导致实际部署中误报过多。

Method: BADAS基于V-JEPA2骨干网络，训练于Nexar的真实行车记录仪数据集，分为公开版（1.5k视频）和专有版（40k视频）。

Result: BADAS在多个基准测试中达到最优AP/AUC，优于前向碰撞ADAS基线，并提供更准确的事故时间估计。

Conclusion: BADAS推动了自车中心碰撞预测研究，公开了模型权重和代码，并重新标注了评估数据集。

Abstract: Existing collision prediction methods often fail to distinguish between
ego-vehicle threats and random accidents not involving the ego vehicle, leading
to excessive false alerts in real-world deployment. We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.
We re-annotate major benchmarks to identify ego involvement, add consensus
alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC
and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and
comes in two variants: BADAS-Open (trained on our 1.5k public videos) and
BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates. We release our BADAS-Open model weights and code, along with
re-annotations of all evaluation datasets to promote ego-centric collision
prediction research.

</details>


### [85] [ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention](https://arxiv.org/abs/2510.14882)
*Keli Liu,Zhendong Wang,Wengang Zhou,Shaodong Xu,Ruixiao Dong,Houqiang Li*

Main category: cs.CV

TL;DR: ScaleWeaver是一个基于视觉自回归模型（VAR）的高效可控文本到图像生成框架，通过参数高效微调和改进的MMDiT块实现高质量生成。


<details>
  <summary>Details</summary>
Motivation: 当前VAR模型在控制机制方面研究不足，需要一种高效且精确的控制方法。

Method: 提出ScaleWeaver框架，引入改进的MMDiT块和Reference Attention模块，减少计算成本并稳定控制注入。

Result: 实验表明ScaleWeaver在生成质量和控制精度上优于基于扩散的方法，且效率更高。

Conclusion: ScaleWeaver为VAR范式下的可控文本到图像生成提供了实用且有效的解决方案。

Abstract: Text-to-image generation with visual autoregressive~(VAR) models has recently
achieved impressive advances in generation fidelity and inference efficiency.
While control mechanisms have been explored for diffusion models, enabling
precise and flexible control within VAR paradigm remains underexplored. To
bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel
framework designed to achieve high-fidelity, controllable generation upon
advanced VAR models through parameter-efficient fine-tuning. The core module in
ScaleWeaver is the improved MMDiT block with the proposed Reference Attention
module, which efficiently and effectively incorporates conditional information.
Different from MM Attention, the proposed Reference Attention module discards
the unnecessary attention from image$\rightarrow$condition, reducing
computational cost while stabilizing control injection. Besides, it
strategically emphasizes parameter reuse, leveraging the capability of the VAR
backbone itself with a few introduced parameters to process control
information, and equipping a zero-initialized linear projection to ensure that
control signals are incorporated effectively without disrupting the generative
capability of the base model. Extensive experiments show that ScaleWeaver
delivers high-quality generation and precise control while attaining superior
efficiency over diffusion-based methods, making ScaleWeaver a practical and
effective solution for controllable text-to-image generation within the visual
autoregressive paradigm. Code and models will be released.

</details>


### [86] [You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction](https://arxiv.org/abs/2510.14885)
*Logan Lawrence,Oindrila Saha,Megan Wei,Chen Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 提出了一种名为nlg2choice的两阶段方法，用于解决多模态大语言模型在细粒度视觉分类任务中的自由形式响应评估问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度视觉分类任务中无法有效处理高相关性和大量选项的多选题，且计算成本高。

Method: 首先通过开放性问题获取模型响应，然后使用文本约束解码预测最可能选项，并在检索设置中采用早期停止方法提高效率。

Result: 在七个细粒度视觉数据集上，分类和检索性能均有提升，且适用于多种自然语言任务实现方式。

Conclusion: nlg2choice方法在多模态大语言模型的细粒度视觉分类任务中表现优异，解决了现有方法的局限性。

Abstract: Despite the renewed interest in zero-shot visual classification due to the
rise of Multimodal Large Language Models (MLLMs), the problem of evaluating
free-form responses of auto-regressive models remains a persistent challenge.
Most existing works focus on language-only tasks or don't consider Multiple
Choice Questions (MCQs) beyond 5-way options, both of which are critical
capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where
choice counts are in the hundreds to thousands and the choices are highly
related. Furthermore, in this highly multi-way MCQ setting it is not clear how
to extend LLM choice extraction to retrieval-based problems, where computing
probabilities over the choice set is computationally costly. In this work we
investigate nlg2choice, a simple two-stage method which first asks the MLLM an
open-ended question for the task with minimal constraints, then uses text-only
constrained decoding to predict the most likely choice. In retrieval settings,
we compute the probability of the constrained response taking that choice with
an early stopping method to significantly improve throughput. Our results show
improvement over a suite of seven fine-grained visual datasets when evaluating
in terms of classification and retrieval, and show that this performance holds
over the various ways that users of LLMs can implement tasks in natural
language.

</details>


### [87] [Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](https://arxiv.org/abs/2510.14896)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型（MLLMs）的视频异常检测框架，通过分析对象活动与交互的文本描述来检测复杂异常，并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督视频异常检测方法难以检测涉及对象交互的复杂异常且缺乏可解释性。

Method: 利用MLLM提取并解释对象在不同时刻的活动与交互，生成文本描述作为高层表示，测试时与训练视频的文本描述对比检测异常。

Result: 在基准数据集上实验表明，该方法能有效检测基于交互的复杂异常，并在无交互异常的数据集上达到最优性能。

Conclusion: 该方法不仅提升了复杂异常的检测能力，还增强了可解释性，可与传统方法结合进一步优化。

Abstract: Existing semi-supervised video anomaly detection (VAD) methods often struggle
with detecting complex anomalies involving object interactions and generally
lack explainability. To overcome these limitations, we propose a novel VAD
framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous
MLLM-based approaches that make direct anomaly judgments at the frame level,
our method focuses on extracting and interpreting object activity and
interactions over time. By querying an MLLM with visual inputs of object pairs
at different moments, we generate textual descriptions of the activity and
interactions from nominal videos. These textual descriptions serve as a
high-level representation of the activity and interactions of objects in a
video. They are used to detect anomalies during test time by comparing them to
textual descriptions found in nominal training videos. Our approach inherently
provides explainability and can be combined with many traditional VAD methods
to further enhance their interpretability. Extensive experiments on benchmark
datasets demonstrate that our method not only detects complex interaction-based
anomalies effectively but also achieves state-of-the-art performance on
datasets without interaction anomalies.

</details>


### [88] [MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos](https://arxiv.org/abs/2510.14904)
*Gabriel Fiastre,Antoine Yang,Cordelia Schmid*

Main category: cs.CV

TL;DR: 提出了一种名为MaskCaptioner的端到端模型，通过合成标注扩展数据集，实现了视频中物体的联合检测、分割、跟踪和描述，并在多个基准测试中取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 由于密集视频物体描述任务复杂且人工标注成本高，现有方法采用分离训练策略可能导致性能不佳，因此提出了一种新的解决方案。

Method: 利用先进的视觉语言模型生成合成标注，扩展LVIS和LV-VIS数据集，训练端到端模型MaskCaptioner。

Result: 在VidSTG、VLN和BenSMOT三个基准测试中取得了最优性能。

Conclusion: 通过合成标注和端到端训练，MaskCaptioner显著提升了密集视频物体描述任务的性能。

Abstract: Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.

</details>


### [89] [3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation](https://arxiv.org/abs/2510.14945)
*JoungBin Lee,Jaewoo Jung,Jisang Han,Takuya Narihira,Kazumi Fukuda,Junyoung Seo,Sunghwan Hong,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: 3DScenePrompt是一个框架，通过双时空条件化和3D场景记忆，实现视频生成中的精确相机控制和场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于单张图像或短片段生成视频，难以同时保证场景一致性和动态元素的自然演化。

Method: 采用双时空条件化（时间相邻帧和空间相邻内容）和动态SLAM构建的3D场景记忆，分离静态几何和动态元素。

Result: 在场景一致性、相机控制和生成质量上显著优于现有方法。

Conclusion: 3DScenePrompt通过结合3D场景记忆和动态分离策略，实现了高质量的视频生成和精确控制。

Abstract: We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/

</details>


### [90] [OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression](https://arxiv.org/abs/2510.14954)
*Zhe Li,Weihao Yuan,Weichao Shen,Siyu Zhu,Zilong Dong,Chang Xu*

Main category: cs.CV

TL;DR: 提出了一种连续掩码自回归运动变换器，结合门控线性注意力和RMSNorm模块，提升多模态运动生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决全身多模态运动生成的两个主要挑战：有效的运动生成机制和多模态（如文本、语音、音乐）的融合。

Method: 采用连续掩码自回归运动变换器，引入门控线性注意力和RMSNorm模块，结合DiT结构和AdaLN、交叉注意力融合多模态信号。

Result: 实验表明，该方法在文本到运动、语音到手势、音乐到舞蹈等任务上均优于现有方法。

Conclusion: 提出的框架在多模态运动生成任务中表现出色，代码将公开。

Abstract: Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.

</details>


### [91] [RealDPO: Real or Not Real, that is the Preference](https://arxiv.org/abs/2510.14955)
*Guo Cheng,Danni Yang,Ziqi Huang,Jianlou Si,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: RealDPO是一种新的对齐范式，利用真实世界数据作为偏好学习的正样本，提升视频生成模型的运动真实感。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在复杂运动合成上表现不佳，运动不够自然、流畅且上下文不一致，限制了实际应用。

Method: 提出RealDPO，采用直接偏好优化（DPO）和定制损失函数，通过对比真实视频与错误模型输出来迭代自校正。

Result: 实验表明，RealDPO显著提升了视频质量、文本对齐和运动真实感。

Conclusion: RealDPO通过真实数据驱动的偏好学习，有效解决了复杂运动合成的挑战。

Abstract: Video generative models have recently achieved notable advancements in
synthesis quality. However, generating complex motions remains a critical
challenge, as existing models often struggle to produce natural, smooth, and
contextually consistent movements. This gap between generated and real-world
motions limits their practical applicability. To address this issue, we
introduce RealDPO, a novel alignment paradigm that leverages real-world data as
positive samples for preference learning, enabling more accurate motion
synthesis. Unlike traditional supervised fine-tuning (SFT), which offers
limited corrective feedback, RealDPO employs Direct Preference Optimization
(DPO) with a tailored loss function to enhance motion realism. By contrasting
real-world videos with erroneous model outputs, RealDPO enables iterative
self-correction, progressively refining motion quality. To support
post-training in complex motion synthesis, we propose RealAction-5K, a curated
dataset of high-quality videos capturing human daily activities with rich and
precise motion details. Extensive experiments demonstrate that RealDPO
significantly improves video quality, text alignment, and motion realism
compared to state-of-the-art models and existing preference optimization
techniques.

</details>


### [92] [MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2510.14958)
*Weikang Shi,Aldrich Yu,Rongyao Fang,Houxing Ren,Ke Wang,Aojun Zhou,Changyao Tian,Xinyu Fu,Yuxuan Hu,Zimu Lu,Linjiang Huang,Si Liu,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: MathCanvas框架通过视觉操作和策略性视觉辅助推理两阶段训练，显著提升大型多模态模型在数学几何问题中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在文本推理中表现出色，但在依赖视觉辅助的数学领域（如几何）表现不佳，现有方法在生成高质量、适时图表方面存在局限。

Method: MathCanvas框架包括视觉操作阶段（预训练模型生成和编辑图表）和策略性视觉辅助推理阶段（微调模型学习何时及如何使用视觉辅助）。

Result: BAGEL-Canvas模型在MathCanvas-Bench上比基线模型提升86%，并在其他数学基准测试中表现出色。

Conclusion: MathCanvas提供了一套完整工具包（框架、数据集和基准测试），推动了多模态模型在复杂视觉辅助推理中的发展。

Abstract: While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/

</details>


### [93] [C4D: 4D Made from 3D through Dual Correspondences](https://arxiv.org/abs/2510.14960)
*Shizun Wang,Zhenxiang Jiang,Xingyi Yang,Xinchao Wang*

Main category: cs.CV

TL;DR: C4D框架通过引入时间对应关系，将静态3D重建扩展到动态4D重建，解决了动态场景中的几何和相机姿态估计问题。


<details>
  <summary>Details</summary>
Motivation: 动态场景中的移动物体破坏了多视角几何约束，导致现有静态3D重建方法（如DUSt3R）在动态场景中表现不佳。

Method: C4D通过预测点图并捕获短期光流和长期点跟踪两种对应关系，训练动态感知点跟踪器，分离移动物体与静态背景，并引入动态场景优化目标。

Result: 实验表明，C4D实现了完整的4D重建，并在深度估计、相机姿态估计和点跟踪等下游任务中表现优异。

Conclusion: C4D通过时间对应关系和动态感知优化，成功解决了动态场景的4D重建问题，为下游任务提供了可靠支持。

Abstract: Recovering 4D from monocular video, which jointly estimates dynamic geometry
and camera poses, is an inevitably challenging problem. While recent
pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great
progress in reconstructing static scenes, directly applying them to dynamic
scenes leads to inaccurate results. This discrepancy arises because moving
objects violate multi-view geometric constraints, disrupting the
reconstruction. To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D. Specifically, apart from predicting pointmaps, C4D captures two types of
correspondences: short-term optical flow and long-term point tracking. We train
a dynamic-aware point tracker that provides additional mobility information,
facilitating the estimation of motion masks to separate moving elements from
the static background, thus offering more reliable guidance for dynamic scenes.
Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters. Simultaneously, the
correspondences lift 2D trajectories into smooth 3D trajectories, enabling
fully integrated 4D reconstruction. Experiments show that our framework
achieves complete 4D recovery and demonstrates strong performance across
multiple downstream tasks, including depth estimation, camera pose estimation,
and point tracking. Project Page: https://littlepure2333.github.io/C4D

</details>


### [94] [RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion](https://arxiv.org/abs/2510.14962)
*Thao Nguyen,Jiaqi Ma,Fahad Shahbaz Khan,Souhaib Ben Taieb,Salman Khan*

Main category: cs.CV

TL;DR: 提出了一种集成Token-wise Attention的U-Net扩散模型，用于降水临近预报，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在捕捉大气时空动态时存在可扩展性问题，如潜在空间方法复杂且泛化性差，像素空间方法计算成本高且缺乏注意力机制。

Method: 在U-Net扩散模型和时空编码器中集成Token-wise Attention，动态捕捉多尺度空间交互和时间演化。

Result: 实验表明，该方法在复杂降水预报场景中显著优于现有技术，具有更高的局部保真度、泛化性和鲁棒性。

Conclusion: 该方法通过原生集成注意力机制，避免了高资源成本，同时提升了预报性能。

Abstract: Precipitation nowcasting, predicting future radar echo sequences from current
observations, is a critical yet challenging task due to the inherently chaotic
and tightly coupled spatio-temporal dynamics of the atmosphere. While recent
advances in diffusion-based models attempt to capture both large-scale motion
and fine-grained stochastic variability, they often suffer from scalability
issues: latent-space approaches require a separately trained autoencoder,
adding complexity and limiting generalization, while pixel-space approaches are
computationally intensive and often omit attention mechanisms, reducing their
ability to model long-range spatio-temporal dependencies. To address these
limitations, we propose a Token-wise Attention integrated into not only the
U-Net diffusion model but also the spatio-temporal encoder that dynamically
captures multi-scale spatial interactions and temporal evolution. Unlike prior
approaches, our method natively integrates attention into the architecture
without incurring the high resource cost typical of pixel-space diffusion,
thereby eliminating the need for separate latent modules. Our extensive
experiments and visual evaluations across diverse datasets demonstrate that the
proposed method significantly outperforms state-of-the-art approaches, yielding
superior local fidelity, generalization, and robustness in complex
precipitation forecasting scenarios.

</details>


### [95] [ChangingGrounding: 3D Visual Grounding in Changing Scenes](https://arxiv.org/abs/2510.14965)
*Miao Hu,Zhiwei Huang,Tai Wang,Jiangmiao Pang,Dahua Lin,Nanning Zheng,Runsen Xu*

Main category: cs.CV

TL;DR: 论文提出了ChangingGrounding基准和Mem-ChangingGrounder方法，用于解决动态场景中的3D视觉定位问题，强调利用记忆和主动探索。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法假设场景是静态且重建的，限制了实际应用。论文旨在解决动态场景中的定位问题。

Method: 提出Mem-ChangingGrounder方法，结合跨模态检索和多视图融合，通过记忆引导探索和高效扫描。

Result: Mem-ChangingGrounder在定位准确性和探索成本上表现最佳。

Conclusion: 论文推动了面向实际应用的记忆中心3D视觉定位研究。

Abstract: Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .

</details>


### [96] [WithAnyone: Towards Controllable and ID Consistent Image Generation](https://arxiv.org/abs/2510.14975)
*Hengyuan Xu,Wei Cheng,Peng Xing,Yixiao Fang,Shuhan Wu,Rui Wang,Xianfang Zeng,Daxin Jiang,Gang Yu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种新的训练范式WithAnyone，通过构建大规模数据集MultiID-2M和引入对比身份损失，解决了文本到图像生成中的复制粘贴问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖重建训练，导致复制粘贴问题，限制了生成的多样性和可控性。

Method: 构建MultiID-2M数据集，提出对比身份损失，开发WithAnyone模型。

Result: WithAnyone显著减少复制粘贴伪影，提升姿态和表情的可控性，同时保持高身份相似性。

Conclusion: WithAnyone在身份一致性和生成多样性之间取得了平衡，用户研究验证了其有效性。

Abstract: Identity-consistent generation has become an important focus in text-to-image
research, with recent models achieving notable success in producing images
aligned with a reference identity. Yet, the scarcity of large-scale paired
datasets containing multiple images of the same individual forces most
approaches to adopt reconstruction-based training. This reliance often leads to
a failure mode we term copy-paste, where the model directly replicates the
reference face rather than preserving identity across natural variations in
pose, expression, or lighting. Such over-similarity undermines controllability
and limits the expressive power of generation. To address these limitations, we
(1) construct a large-scale paired dataset MultiID-2M, tailored for
multi-person scenarios, providing diverse references for each identity; (2)
introduce a benchmark that quantifies both copy-paste artifacts and the
trade-off between identity fidelity and variation; and (3) propose a novel
training paradigm with a contrastive identity loss that leverages paired data
to balance fidelity with diversity. These contributions culminate in
WithAnyone, a diffusion-based model that effectively mitigates copy-paste while
preserving high identity similarity. Extensive qualitative and quantitative
experiments demonstrate that WithAnyone significantly reduces copy-paste
artifacts, improves controllability over pose and expression, and maintains
strong perceptual quality. User studies further validate that our method
achieves high identity fidelity while enabling expressive controllable
generation.

</details>


### [97] [Terra: Explorable Native 3D World Model with Point Latents](https://arxiv.org/abs/2510.14977)
*Yuanhui Huang,Weiliang Chen,Wenzhao Zheng,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Terra是一个原生3D世界模型，通过点潜在空间生成可探索的环境，解决了现有方法依赖像素对齐表示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖像素对齐表示，忽略了物理世界的3D本质，影响了3D一致性和建模效率。

Method: 提出P2G-VAE编码3D输入为潜在点表示，并用3D高斯基元建模几何和外观；引入SPFlow网络生成潜在点表示。

Result: 在ScanNet v2数据集上，Terra在重建和生成任务中实现了最先进的性能和高3D一致性。

Conclusion: Terra通过原生3D表示和架构实现了精确的多视角一致性和灵活的渲染，支持可探索的世界建模。

Abstract: World models have garnered increasing attention for comprehensive modeling of
the real world. However, most existing methods still rely on pixel-aligned
representations as the basis for world evolution, neglecting the inherent 3D
nature of the physical world. This could undermine the 3D consistency and
diminish the modeling efficiency of world models. In this paper, we present
Terra, a native 3D world model that represents and generates explorable
environments in an intrinsic 3D latent space. Specifically, we propose a novel
point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into
a latent point representation, which is subsequently decoded as 3D Gaussian
primitives to jointly model geometry and appearance. We then introduce a sparse
point flow matching network (SPFlow) for generating the latent point
representation, which simultaneously denoises the positions and features of the
point latents. Our Terra enables exact multi-view consistency with native 3D
representation and architecture, and supports flexible rendering from any
viewpoint with only a single generation process. Furthermore, Terra achieves
explorable world modeling through progressive generation in the point latent
space. We conduct extensive experiments on the challenging indoor scenes from
ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction
and generation with high 3D consistency.

</details>


### [98] [Learning an Image Editing Model without Image Editing Pairs](https://arxiv.org/abs/2510.14978)
*Nupur Kumari,Sheng-Yu Wang,Nanxuan Zhao,Yotam Nitzan,Yuheng Li,Krishna Kumar Singh,Richard Zhang,Eli Shechtman,Jun-Yan Zhu,Xun Huang*

Main category: cs.CV

TL;DR: 提出了一种无需配对数据的图像编辑训练方法，利用视觉语言模型（VLM）反馈进行优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模配对数据，难以获取且可能传播预训练模型的缺陷。

Method: 通过展开扩散模型并利用VLM反馈直接优化，结合分布匹配损失（DMD）保持视觉保真度。

Result: 在无配对数据情况下，性能与基于监督数据的模型相当，优于RL方法如Flow-GRPO。

Conclusion: 新方法有效解决了配对数据依赖问题，展示了无监督优化的潜力。

Abstract: Recent image editing models have achieved impressive results while following
natural language editing instructions, but they rely on supervised fine-tuning
with large datasets of input-target pairs. This is a critical bottleneck, as
such naturally occurring pairs are hard to curate at scale. Current workarounds
use synthetic training pairs that leverage the zero-shot capabilities of
existing models. However, this can propagate and magnify the artifacts of the
pretrained model into the final trained model. In this work, we present a new
training paradigm that eliminates the need for paired data entirely. Our
approach directly optimizes a few-step diffusion model by unrolling it during
training and leveraging feedback from vision-language models (VLMs). For each
input and editing instruction, the VLM evaluates if an edit follows the
instruction and preserves unchanged content, providing direct gradients for
end-to-end optimization. To ensure visual fidelity, we incorporate distribution
matching loss (DMD), which constrains generated images to remain within the
image manifold learned by pretrained models. We evaluate our method on standard
benchmarks and include an extensive ablation study. Without any paired data,
our method performs on par with various image editing diffusion models trained
on extensive supervised paired data, under the few-step setting. Given the same
VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.

</details>


### [99] [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://arxiv.org/abs/2510.14979)
*Haiwen Diao,Mingxuan Li,Silei Wu,Linjun Dai,Xiaohua Wang,Hanming Deng,Lewei Lu,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文探讨了原生视觉语言模型（VLMs）的挑战，并提出NEO模型家族作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决原生VLMs与模块化VLMs之间的差异及研究普及问题。

Method: 提出NEO模型家族，基于共享语义空间、模块整合和跨模态特性。

Result: NEO在390M图像-文本数据上表现优异，支持统一编码和推理。

Conclusion: NEO为原生VLMs提供了可扩展、高效的解决方案，并开源代码和模型。

Abstract: The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.

</details>


### [100] [Coupled Diffusion Sampling for Training-Free Multi-View Image Editing](https://arxiv.org/abs/2510.14981)
*Hadi Alzayer,Yunzhi Zhang,Chen Geng,Jia-Bin Huang,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出一种基于预训练2D图像编辑模型的多视角一致性图像编辑方法，通过耦合扩散采样实现隐式3D正则化。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过优化显式3D表示实现多视角一致性，但存在优化过程长且在稀疏视角下不稳定的问题。

Method: 采用耦合扩散采样技术，同时从多视角图像分布和2D编辑图像分布中采样，通过耦合项强制多视角一致性。

Result: 在三种多视角图像编辑任务中验证了方法的有效性和通用性。

Conclusion: 该方法可作为多视角一致性编辑的通用解决方案，适用于多种模型架构。

Abstract: We present an inference-time diffusion sampling method to perform multi-view
consistent image editing using pre-trained 2D image editing models. These
models can independently produce high-quality edits for each image in a set of
multi-view images of a 3D scene or object, but they do not maintain consistency
across views. Existing approaches typically address this by optimizing over
explicit 3D representations, but they suffer from a lengthy optimization
process and instability under sparse view settings. We propose an implicit 3D
regularization approach by constraining the generated 2D image sequences to
adhere to a pre-trained multi-view image distribution. This is achieved through
coupled diffusion sampling, a simple diffusion sampling technique that
concurrently samples two trajectories from both a multi-view image distribution
and a 2D edited image distribution, using a coupling term to enforce the
multi-view consistency among the generated images. We validate the
effectiveness and generality of this framework on three distinct multi-view
image editing tasks, demonstrating its applicability across various model
architectures and highlighting its potential as a general solution for
multi-view consistent editing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [101] [A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking](https://arxiv.org/abs/2510.14000)
*Mingyang Jiang,Yueyuan Li,Jiaru Zhang,Songan Zhang,Ming Yang*

Main category: cs.RO

TL;DR: DRIP是一种基于扩散模型和强化学习的自动化停车规划方法，通过结合两者的优势提高规划成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂环境中难以准确建模最优动作分布，导致规划成功率低。

Method: DRIP利用强化学习预训练策略提供先验动作分布，并通过扩散模型细化这些分布以提高精度。

Result: 实验表明，DRIP在受限空间停车环境中显著提高了规划性能，同时保持了良好的泛化能力。

Conclusion: DRIP通过结合强化学习和扩散模型，有效解决了复杂环境中的停车规划问题。

Abstract: The growing demand for parking has increased the need for automated parking
planning methods that can operate reliably in confined spaces. In restricted
and complex environments, high-precision maneuvers are required to achieve a
high success rate in planning, yet existing approaches often rely on explicit
action modeling, which faces challenges when accurately modeling the optimal
action distribution. In this paper, we propose DRIP, a diffusion-refined
planner anchored in reinforcement learning (RL) prior action distribution, in
which an RL-pretrained policy provides prior action distributions to regularize
the diffusion training process. During the inference phase the denoising
process refines these coarse priors into more precise action distributions. By
steering the denoising trajectory through the reinforcement learning prior
distribution during training, the diffusion model inherits a well-informed
initialization, resulting in more accurate action modeling, a higher planning
success rate, and reduced inference steps. We evaluate our approach across
parking scenarios with varying degrees of spatial constraints. Experimental
results demonstrate that our method significantly improves planning performance
in confined-space parking environments while maintaining strong generalization
in common scenarios.

</details>


### [102] [Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms](https://arxiv.org/abs/2510.14018)
*Adam Morris,Timothy Pelham,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 论文提出了一种设计空间智能机器人群体行为的方法，用于定位隐蔽的无线电发射器，通过差分进化生成几何巡逻路线，独立于发射器参数。


<details>
  <summary>Details</summary>
Motivation: 电磁监视中，独立于发射器参数定位未知信号是一个关键挑战。

Method: 使用差分进化生成几何巡逻路线，模拟四机器人群体在八种配置下的行为，基于巡逻形状和天线类型（全向或定向）分配预生成的巡逻路线。

Result: 定向天线的平均检测成功率为98.75%，全向天线为80.25%；定向天线的定位误差更小（1.01-1.30米），全向天线为1.67-1.90米。

Conclusion: 空间智能（优化的巡逻路线和天线选择）是机器人有效监视的关键设计考虑因素。

Abstract: This paper introduces a method for designing spatially intelligent robot
swarm behaviors to localize concealed radio emitters. We use differential
evolution to generate geometric patrol routes that localize unknown signals
independently of emitter parameters, a key challenge in electromagnetic
surveillance. Patrol shape and antenna type are shown to influence information
gain, which in turn determines the effective triangulation coverage. We
simulate a four-robot swarm across eight configurations, assigning
pre-generated patrol routes based on a specified patrol shape and sensing
capability (antenna type: omnidirectional or directional). An emitter is placed
within the map for each trial, with randomized position, transmission power and
frequency. Results show that omnidirectional localization success rates are
driven primarily by source location rather than signal properties, with
failures occurring most often when sources are placed in peripheral areas of
the map. Directional antennas are able to overcome this limitation due to their
higher gain and directivity, with an average detection success rate of 98.75%
compared to 80.25% for omnidirectional. Average localization errors range from
1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional
sensing; while directional sensing also benefits from shorter patrol edges.
These results demonstrate that a swarm's ability to predict electromagnetic
phenomena is directly dependent on its physical interaction with the
environment. Consequently, spatial intelligence, realized here through
optimized patrol routes and antenna selection, is a critical design
consideration for effective robotic surveillance.

</details>


### [103] [Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming](https://arxiv.org/abs/2510.14063)
*Nan Li,Jiming Ren,Haris Miller,Samuel Coogan,Karen M. Feigh,Ye Zhao*

Main category: cs.RO

TL;DR: OATH提出了一种自适应障碍感知的任务分配和规划方法，通过Halton序列图和集群-拍卖-选择框架，提升了异构机器人团队的协作效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体任务分配和规划（MATP）在可扩展性、空间推理和障碍丰富环境中的适应性挑战。

Method: 1. 开发自适应Halton序列图，根据障碍分布调整采样密度；2. 提出集群-拍卖-选择框架，结合障碍感知聚类和加权拍卖。

Result: 在NVIDIA Isaac Sim中验证，任务分配质量、可扩展性和动态适应性显著优于现有MATP基线。

Conclusion: OATH通过创新方法提升了异构机器人团队的协作性能，适用于复杂动态环境。

Abstract: Multi-Agent Task Assignment and Planning (MATP) has attracted growing
attention but remains challenging in terms of scalability, spatial reasoning,
and adaptability in obstacle-rich environments. To address these challenges, we
propose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for
Heterogeneous Robot Teaming, which advances MATP by introducing a novel
obstacle-aware strategy for task assignment. First, we develop an adaptive
Halton sequence map, the first known application of Halton sampling with
obstacle-aware adaptation in MATP, which adjusts sampling density based on
obstacle distribution. Second, we propose a cluster-auction-selection framework
that integrates obstacle-aware clustering with weighted auctions and
intra-cluster task selection. These mechanisms jointly enable effective
coordination among heterogeneous robots while maintaining scalability and
near-optimal allocation performance. In addition, our framework leverages an
LLM to interpret human instructions and directly guide the planner in real
time. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in
task assignment quality, scalability, adaptability to dynamic changes, and
overall execution performance compared to state-of-the-art MATP baselines. A
project website is available at https://llm-oath.github.io/.

</details>


### [104] [Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning](https://arxiv.org/abs/2510.14065)
*Gaoyuan Liu,Joris de Winter,Yuri Durodie,Denis Steckelmacher,Ann Nowe,Bram Vanderborght*

Main category: cs.RO

TL;DR: 提出了一种将强化学习技能集成到任务与运动规划（TAMP）中的方法，以应对不确定性动作的挑战。


<details>
  <summary>Details</summary>
Motivation: TAMP在长时程规划中面临不确定性动作的挑战，而强化学习擅长获取短时程且鲁棒的技能。结合两者可以扩展TAMP的能力。

Method: 设计了包含数据驱动逻辑组件的RL技能，并通过符号规划部署技能，同时引入计划细化子程序处理效果不确定性。

Result: 实验表明，该方法在具有概率技能的领域中扩展了TAMP的能力，并提高了规划效率。

Conclusion: 通过嵌入RL技能，成功提升了TAMP在不确定性环境中的表现和效率。

Abstract: Task and motion planning (TAMP) for robotics manipulation necessitates
long-horizon reasoning involving versatile actions and skills. While
deterministic actions can be crafted by sampling or optimizing with certain
constraints, planning actions with uncertainty, i.e., probabilistic actions,
remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL)
excels in acquiring versatile, yet short-horizon, manipulation skills that are
robust with uncertainties. In this letter, we design a method that integrates
RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with
data-driven logical components that enable the skill to be deployed by symbolic
planning. A plan refinement sub-routine is designed to further tackle the
inevitable effect uncertainties. In the experiments, we compare our method with
baseline hierarchical planning from both TAMP and RL fields and illustrate the
strength of the method. The results show that by embedding RL skills, we extend
the capability of TAMP to domains with probabilistic skills, and improve the
planning efficiency compared to the previous methods.

</details>


### [105] [Partial Feedback Linearization Control of a Cable-Suspended Multirotor Platform for Stabilization of an Attached Load](https://arxiv.org/abs/2510.14072)
*Hemjyoti Das,Christian Ott*

Main category: cs.RO

TL;DR: 提出了一种基于部分反馈线性化的新型控制方法，用于稳定带有负载的悬挂空中平台。


<details>
  <summary>Details</summary>
Motivation: 该系统在建筑工地等场景中具有应用潜力，如重型物体的搬运。

Method: 利用系统的耦合动力学进行稳定，并通过数值稳定性分析验证了耦合项的重要性。

Result: 在外部风扰、传感器噪声和系统动力学不确定性下进行了鲁棒性分析，并通过仿真和实验验证了方法的有效性。

Conclusion: 该方法仅依赖机载传感器，适用于户外建筑工地等实际应用场景。

Abstract: In this work, we present a novel control approach based on partial feedback
linearization (PFL) for the stabilization of a suspended aerial platform with
an attached load. Such systems are envisioned for various applications in
construction sites involving cranes, such as the holding and transportation of
heavy objects. Our proposed control approach considers the underactuation of
the whole system while utilizing its coupled dynamics for stabilization. We
demonstrate using numerical stability analysis that these coupled terms are
crucial for the stabilization of the complete system. We also carried out
robustness analysis of the proposed approach in the presence of external wind
disturbances, sensor noise, and uncertainties in system dynamics. As our
envisioned target application involves cranes in outdoor construction sites,
our control approaches rely on only onboard sensors, thus making it suitable
for such applications. We carried out extensive simulation studies and
experimental tests to validate our proposed control approach.

</details>


### [106] [ViTacGen: Robotic Pushing with Vision-to-Touch Generation](https://arxiv.org/abs/2510.14117)
*Zhiyuan Wu,Yijiong Lin,Yongqiang Zhao,Xuyang Zhang,Zhuo Chen,Nathan Lepora,Shan Luo*

Main category: cs.RO

TL;DR: ViTacGen是一个通过视觉生成触觉信号的机器人操作框架，用于视觉驱动的机器人推动任务，减少对真实触觉传感器的依赖。


<details>
  <summary>Details</summary>
Motivation: 真实触觉传感器成本高、易损坏且部署复杂，而仅依赖视觉的策略性能不足。

Method: 提出ViTacGen框架，包含视觉到触觉的生成网络和强化学习策略，融合视觉与生成的触觉数据。

Result: 在仿真和真实实验中验证了有效性，成功率高达86%。

Conclusion: ViTacGen通过视觉生成触觉信号，实现了高性能的零样本部署。

Abstract: Robotic pushing is a fundamental manipulation task that requires tactile
feedback to capture subtle contact forces and dynamics between the end-effector
and the object. However, real tactile sensors often face hardware limitations
such as high costs and fragility, and deployment challenges involving
calibration and variations between different sensors, while vision-only
policies struggle with satisfactory performance. Inspired by humans' ability to
infer tactile states from vision, we propose ViTacGen, a novel robot
manipulation framework designed for visual robotic pushing with vision-to-touch
generation in reinforcement learning to eliminate the reliance on
high-resolution real tactile sensors, enabling effective zero-shot deployment
on visual-only robotic systems. Specifically, ViTacGen consists of an
encoder-decoder vision-to-touch generation network that generates contact depth
images, a standardized tactile representation, directly from visual image
sequence, followed by a reinforcement learning policy that fuses visual-tactile
data with contrastive learning based on visual and generated tactile
observations. We validate the effectiveness of our approach in both simulation
and real world experiments, demonstrating its superior performance and
achieving a success rate of up to 86\%.

</details>


### [107] [Prescribed Performance Control of Deformable Object Manipulation in Spatial Latent Space](https://arxiv.org/abs/2510.14234)
*Ning Han,Gu Gong,Bin Zhang,Yuexuan Xu,Bohan Yang,Yunhui Liu,David Navarro-Alarcon*

Main category: cs.RO

TL;DR: 提出了一种基于关键点的无模型方法，用于约束条件下的3D可变形物体形状控制。


<details>
  <summary>Details</summary>
Motivation: 由于可变形物体的无限维状态空间和复杂动力学，传统方法难以高效控制。

Method: 利用深度学习提取关键点坐标作为特征向量，结合变形雅可比矩阵和障碍Lyapunov函数（BLF）进行控制。

Result: 实验验证了方法的有效性和鲁棒性。

Conclusion: 该方法简化了可变形物体的操控，并保持了空间信息。

Abstract: Manipulating three-dimensional (3D) deformable objects presents significant
challenges for robotic systems due to their infinite-dimensional state space
and complex deformable dynamics. This paper proposes a novel model-free
approach for shape control with constraints imposed on key points. Unlike
existing methods that rely on feature dimensionality reduction, the proposed
controller leverages the coordinates of key points as the feature vector, which
are extracted from the deformable object's point cloud using deep learning
methods. This approach not only reduces the dimensionality of the feature space
but also retains the spatial information of the object. By extracting key
points, the manipulation of deformable objects is simplified into a visual
servoing problem, where the shape dynamics are described using a deformation
Jacobian matrix. To enhance control accuracy, a prescribed performance control
method is developed by integrating barrier Lyapunov functions (BLF) to enforce
constraints on the key points. The stability of the closed-loop system is
rigorously analyzed and verified using the Lyapunov method. Experimental
results further demonstrate the effectiveness and robustness of the proposed
method.

</details>


### [108] [Learning Human-Humanoid Coordination for Collaborative Object Carrying](https://arxiv.org/abs/2510.14293)
*Yushi Du,Yixuan Li,Baoxiong Jia,Yutang Lin,Pei Zhou,Wei Liang,Yanchao Yang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出了一种名为COLA的强化学习方法，通过单一策略结合领导者和跟随者行为，实现人形机器人与人类的协作搬运，无需外部传感器或复杂交互模型。


<details>
  <summary>Details</summary>
Motivation: 人形机器人与人类的协作在医疗、家庭辅助和制造等领域具有潜力，但由于其复杂的全身动力学，相关研究较少。

Method: 采用仅依赖本体感觉的强化学习方法，在闭环环境中训练模型，预测物体运动模式和人类意图，实现协调的轨迹规划。

Result: 模拟实验显示人类努力减少24.7%，真实实验验证了在不同物体和地形中的鲁棒性，用户研究显示平均提升27.4%。

Conclusion: COLA方法为实际部署提供了一种实用解决方案，实现了无需外部传感器的协作搬运。

Abstract: Human-humanoid collaboration shows significant promise for applications in
healthcare, domestic assistance, and manufacturing. While compliant robot-human
collaboration has been extensively developed for robotic arms, enabling
compliant human-humanoid collaboration remains largely unexplored due to
humanoids' complex whole-body dynamics. In this paper, we propose a
proprioception-only reinforcement learning approach, COLA, that combines leader
and follower behaviors within a single policy. The model is trained in a
closed-loop environment with dynamic object interactions to predict object
motion patterns and human intentions implicitly, enabling compliant
collaboration to maintain load balance through coordinated trajectory planning.
We evaluate our approach through comprehensive simulator and real-world
experiments on collaborative carrying tasks, demonstrating the effectiveness,
generalization, and robustness of our model across various terrains and
objects. Simulation experiments demonstrate that our model reduces human effort
by 24.7%. compared to baseline approaches while maintaining object stability.
Real-world experiments validate robust collaborative carrying across different
object types (boxes, desks, stretchers, etc.) and movement patterns
(straight-line, turning, slope climbing). Human user studies with 23
participants confirm an average improvement of 27.4% compared to baseline
models. Our method enables compliant human-humanoid collaborative carrying
without requiring external sensors or complex interaction models, offering a
practical solution for real-world deployment.

</details>


### [109] [Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning](https://arxiv.org/abs/2510.14300)
*Weijie Shen,Yitian Liu,Yuhao Wu,Zhixuan Liang,Sijia Gu,Dehui Wang,Tian Nian,Lei Xu,Yusen Qin,Jiangmiao Pang,Xinping Guan,Xiaokang Yang,Yao Mu*

Main category: cs.RO

TL;DR: AdaMoE是一种基于Mixture-of-Experts（MoE）的架构，通过继承预训练的VLA模型权重并稀疏激活MoE层，解决了VLA模型扩展中的计算效率和模型容量问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型扩展面临计算资源需求高和机器人数据稀缺的问题，同时需要平衡模型容量与计算效率以实现实时控制。

Method: AdaMoE采用解耦技术，通过独立的scale adapter分离专家选择和权重分配，实现任务相关的专家选择和协作式权重控制。

Result: AdaMoE在LIBERO和RoboTwin基准测试中分别提升1.8%和9.3%，实际机器人任务中性能提升21.5%。

Conclusion: AdaMoE通过协作式专家利用，在保持计算效率的同时显著提升了性能，验证了其在机器人操作任务中的实用性。

Abstract: Vision-Language-Action (VLA) models are experiencing rapid development and
demonstrating promising capabilities in robotic manipulation tasks. However,
scaling up VLA models presents several critical challenges: (1) Training new
VLA models from scratch demands substantial computational resources and
extensive datasets. Given the current scarcity of robot data, it becomes
particularly valuable to fully leverage well-pretrained VLA model weights
during the scaling process. (2) Real-time control requires carefully balancing
model capacity with computational efficiency. To address these challenges, We
propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits
pretrained weights from dense VLA models, and scales up the action expert by
substituting the feedforward layers into sparsely activated MoE layers. AdaMoE
employs a decoupling technique that decouples expert selection from expert
weighting through an independent scale adapter working alongside the
traditional router. This enables experts to be selected based on task relevance
while contributing with independently controlled weights, allowing
collaborative expert utilization rather than winner-takes-all dynamics. Our
approach demonstrates that expertise need not monopolize. Instead, through
collaborative expert utilization, we can achieve superior performance while
maintaining computational efficiency. AdaMoE consistently outperforms the
baseline model across key benchmarks, delivering performance gains of 1.8% on
LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement
in real-world experiments validates its practical effectiveness for robotic
manipulation tasks.

</details>


### [110] [Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion](https://arxiv.org/abs/2510.14338)
*Yuanhong Zeng,Anushri Dixit*

Main category: cs.RO

TL;DR: 研究风险感知强化学习在四足机器人运动中的应用，通过CVaR约束策略优化训练多策略家族，并利用多臂老虎机框架在线自适应选择最优策略。


<details>
  <summary>Details</summary>
Motivation: 提高四足机器人在未知环境中的稳定性和样本效率，确保其在动态变化、接触噪声和地形变化等条件下的鲁棒性。

Method: 使用CVaR约束策略优化训练风险条件策略家族，并通过多臂老虎机框架在线自适应选择最优策略。

Result: 在仿真和真实机器人测试中，风险感知策略在未知环境中的平均和尾部性能接近基线方法的两倍，且能在两分钟内自适应选择最优策略。

Conclusion: 该方法显著提升了四足机器人在未知环境中的鲁棒性和性能，为风险感知强化学习提供了有效解决方案。

Abstract: In this work, we study risk-aware reinforcement learning for quadrupedal
locomotion. Our approach trains a family of risk-conditioned policies using a
Conditional Value-at-Risk (CVaR) constrained policy optimization technique that
provides improved stability and sample efficiency. At deployment, we adaptively
select the best performing policy from the family of policies using a
multi-armed bandit framework that uses only observed episodic returns, without
any privileged environment information, and adapts to unknown conditions on the
fly. Hence, we train quadrupedal locomotion policies at various levels of
robustness using CVaR and adaptively select the desired level of robustness
online to ensure performance in unknown environments. We evaluate our method in
simulation across eight unseen settings (by changing dynamics, contacts,
sensing noise, and terrain) and on a Unitree Go2 robot in previously unseen
terrains. Our risk-aware policy attains nearly twice the mean and tail
performance in unseen environments compared to other baselines and our
bandit-based adaptation selects the best-performing risk-aware policy in
unknown terrain within two minutes of operation.

</details>


### [111] [SUM-AgriVLN: Spatial Understanding Memory for Agricultural Vision-and-Language Navigation](https://arxiv.org/abs/2510.14357)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: SUM-AgriVLN通过引入空间理解记忆模块，提升了农业视觉与语言导航的性能，成功率和导航误差均有改善。


<details>
  <summary>Details</summary>
Motivation: 农业机器人导航指令常重复出现，但现有方法AgriVLN未利用历史经验提供空间上下文。

Method: 提出SUM-AgriVLN方法，通过3D重建和表示保存空间记忆。

Result: 在A2A基准测试中，成功率从0.47提升至0.54，导航误差从2.91m微增至2.93m。

Conclusion: SUM-AgriVLN在农业领域实现了最先进的导航性能。

Abstract: Agricultural robots are emerging as powerful assistants across a wide range
of agricultural tasks, nevertheless, still heavily rely on manual operation or
fixed rail systems for movement. The AgriVLN method and the A2A benchmark
pioneeringly extend Vision-and-Language Navigation (VLN) to the agricultural
domain, enabling robots to navigate to the target positions following the
natural language instructions. In practical agricultural scenarios, navigation
instructions often repeatedly occur, yet AgriVLN treat each instruction as an
independent episode, overlooking the potential of past experiences to provide
spatial context for subsequent ones. To bridge this gap, we propose the method
of Spatial Understanding Memory for Agricultural Vision-and-Language Navigation
(SUM-AgriVLN), in which the SUM module employs spatial understanding and save
spatial memory through 3D reconstruction and representation. When evaluated on
the A2A benchmark, our SUM-AgriVLN effectively improves Success Rate from 0.47
to 0.54 with slight sacrifice on Navigation Error from 2.91m to 2.93m,
demonstrating the state-of-the-art performance in the agricultural domain.
Code: https://github.com/AlexTraveling/SUM-AgriVLN.

</details>


### [112] [RoboANKLE: Design, Development, and Functional Evaluation of a Robotic Ankle with a Motorized Compliant Unit](https://arxiv.org/abs/2510.14414)
*Baris Baysal,Omid Arfaie,Ramazan Unal*

Main category: cs.RO

TL;DR: RoboANKLE是一种动力式胫骨假肢，通过ESER和EES机制实现自然踝关节运动，重量轻且性能优越。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够满足日常活动需求、提供足够扭矩和运动范围的动力式胫骨假肢，同时解决能量自主性和重量问题。

Method: 采用ESER和EES机制，进行运动学和动力学分析，通过CAD建模、动态和结构分析优化设计，并制造原型进行实验评估。

Result: RoboANKLE重量为1.92 kg，能实现95%的自然背屈角度，扭矩和功率分别比自然行走需求高57%和10%。

Conclusion: RoboANKLE设计成功，性能优越，能够满足自然行走的需求。

Abstract: This study presents a powered transtibial prosthesis with complete push-off
assistance, RoboANKLE. The design aims to fulfill specific requirements, such
as a sufficient range of motion (RoM) while providing the necessary torque for
achieving natural ankle motion in daily activities. Addressing the challenges
faced in designing active transtibial prostheses, such as maintaining energetic
autonomy and minimizing weight, is vital for the study. With this aim, we try
to imitate the human ankle by providing extensive push-off assistance to
achieve a natural-like torque profile. Thus, Energy Store and Extended Release
mechanism (ESER) is employed with a novel Extra Energy Storage (EES) mechanism.
Kinematic and kinetic analyses are carried out to determine the design
parameters and assess the design performance. Subsequently, a Computer-Aided
Design (CAD) model is built and used in comprehensive dynamic and structural
analyses. These analyses are used for the design performance evaluation and
determine the forces and torques applied to the prosthesis, which aids in
optimizing the design for minimal weight via structural analysis and topology
optimization. The design of the prototype is then finalized and manufactured
for experimental evaluation to validate the design and functionality. The
prototype is realized with a mass of 1.92 kg and dimensions of 261x107x420 mm.
The Functional evaluations of the RoboANKLE revealed that it is capable of
achieving the natural maximum dorsi-flexion angle with 95% accuracy. Also,
Thanks to the implemented mechanisms, the results show that RoboANKLE can
generate 57% higher than the required torque for natural walking. The result of
the power generation capacity of the RoboANKLE is 10% more than the natural
power during the gait cycle.

</details>


### [113] [Towards Adaptable Humanoid Control via Adaptive Motion Tracking](https://arxiv.org/abs/2510.14454)
*Tao Huang,Huayi Wang,Junli Ren,Kangning Yin,Zirui Wang,Xiao Chen,Feiyu Jia,Wentao Zhang,Junfeng Long,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: AdaMimic是一种新型运动跟踪算法，通过单参考运动实现可适应的人形机器人控制，结合了运动先验和运动跟踪方法的优势。


<details>
  <summary>Details</summary>
Motivation: 解决现有运动先验方法在适应性和模仿准确性之间的权衡问题，同时减少对大量训练数据和目标运动的需求。

Method: 通过稀疏化单参考运动生成关键帧并轻量编辑，初始化策略生成密集中间运动，再训练适配器调整跟踪速度和细化动作。

Result: 在仿真和真实机器人实验中，AdaMimic在多种任务和适应条件下显著提高了模仿准确性和适应性。

Conclusion: AdaMimic成功结合了运动先验和运动跟踪方法的优势，实现了高效且准确的运动适应。

Abstract: Humanoid robots are envisioned to adapt demonstrated motions to diverse
real-world conditions while accurately preserving motion patterns. Existing
motion prior approaches enable well adaptability with a few motions but often
sacrifice imitation accuracy, whereas motion-tracking methods achieve accurate
imitation yet require many training motions and a test-time target motion to
adapt. To combine their strengths, we introduce AdaMimic, a novel motion
tracking algorithm that enables adaptable humanoid control from a single
reference motion. To reduce data dependence while ensuring adaptability, our
method first creates an augmented dataset by sparsifying the single reference
motion into keyframes and applying light editing with minimal physical
assumptions. A policy is then initialized by tracking these sparse keyframes to
generate dense intermediate motions, and adapters are subsequently trained to
adjust tracking speed and refine low-level actions based on the adjustment,
enabling flexible time warping that further improves imitation accuracy and
adaptability. We validate these significant improvements in our approach in
both simulation and the real-world Unitree G1 humanoid robot in multiple tasks
across a wide range of adaptation conditions. Videos and code are available at
https://taohuang13.github.io/adamimic.github.io/.

</details>


### [114] [Restoring Noisy Demonstration for Imitation Learning With Diffusion Models](https://arxiv.org/abs/2510.14467)
*Shang-Fu Chen,Co Yong,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种过滤和恢复框架，用于处理带有噪声的专家演示数据，通过过滤干净样本并恢复噪声样本，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法假设专家演示完美，但实际中常因人为或系统误差包含噪声，需有效处理。

Method: 先过滤干净样本，再学习条件扩散模型恢复噪声样本。

Result: 在机器人操作和运动任务中表现优于现有方法，且对噪声类型和水平具有鲁棒性。

Conclusion: 该框架适用于处理噪声离线演示数据，具有实际应用价值。

Abstract: Imitation learning (IL) aims to learn a policy from expert demonstrations and
has been applied to various applications. By learning from the expert policy,
IL methods do not require environmental interactions or reward signals.
However, most existing imitation learning algorithms assume perfect expert
demonstrations, but expert demonstrations often contain imperfections caused by
errors from human experts or sensor/control system inaccuracies. To address the
above problems, this work proposes a filter-and-restore framework to best
leverage expert demonstrations with inherent noise. Our proposed method first
filters clean samples from the demonstrations and then learns conditional
diffusion models to recover the noisy ones. We evaluate our proposed framework
and existing methods in various domains, including robot arm manipulation,
dexterous manipulation, and locomotion. The experiment results show that our
proposed framework consistently outperforms existing methods across all the
tasks. Ablation studies further validate the effectiveness of each component
and demonstrate the framework's robustness to different noise types and levels.
These results confirm the practical applicability of our framework to noisy
offline demonstration data.

</details>


### [115] [Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots](https://arxiv.org/abs/2510.14511)
*Mingtian Du,Suhas Raghavendra Kulkarni,Simone Kager,Domenico Campolo*

Main category: cs.RO

TL;DR: 论文分析了机器人介导的人-人（二元）交互系统在时延下的稳定性，提出了与时延无关和时延相关的稳定性准则。


<details>
  <summary>Details</summary>
Motivation: 研究网络时延对触觉通信系统稳定性的影响，为远程二元系统设计提供指导。

Method: 通过频域分析和数值模拟，识别稳定性准则，并通过机器人实验验证。

Result: 发现刚度增加会非线性地降低最大可容忍时延，系统稳定性与控制器和机器人动态参数相关。

Conclusion: 研究结果为设计稳定的远程二元系统和时延补偿策略提供了理论基础。

Abstract: This paper establishes analytical stability criteria for robot-mediated
human-human (dyadic) interaction systems, focusing on haptic communication
under network-induced time delays. Through frequency-domain analysis supported
by numerical simulations, we identify both delay-independent and
delay-dependent stability criteria. The delay-independent criterion guarantees
stability irrespective of the delay, whereas the delay-dependent criterion is
characterised by a maximum tolerable delay before instability occurs. The
criteria demonstrate dependence on controller and robot dynamic parameters,
where increasing stiffness reduces the maximum tolerable delay in a non-linear
manner, thereby heightening system vulnerability. The proposed criteria can be
generalised to a wide range of robot-mediated interactions and serve as design
guidelines for stable remote dyadic systems. Experiments with robots performing
human-like movements further illustrate the correlation between stability and
motor performance. The findings of this paper suggest the prerequisites for
effective delay-compensation strategies.

</details>


### [116] [QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps](https://arxiv.org/abs/2510.14546)
*Matti Pekkanen,Francesco Verdoja,Ville Kyrki*

Main category: cs.RO

TL;DR: 提出了一种利用自然语言同义词和反义词在嵌入空间中训练分类器的方法，以提高机器人地图和图像的查询能力。


<details>
  <summary>Details</summary>
Motivation: 传统的地图标签有限，视觉语言模型的嵌入提供了更开放的词汇理解，但机器人需要确定环境中与查询相关的部分。

Method: 利用查询的自然语言同义词和反义词在嵌入空间中训练分类器，划分环境为匹配和非匹配区域。

Result: 实验表明，该方法提高了地图和图像的查询能力，且对表示和编码器无关，训练需求低。

Conclusion: 该方法有效解决了机器人地图查询中的环境相关性判断问题，具有通用性和高效性。

Abstract: Embeddings from Visual-Language Models are increasingly utilized to represent
semantics in robotic maps, offering an open-vocabulary scene understanding that
surpasses traditional, limited labels. Embeddings enable on-demand querying by
comparing embedded user text prompts to map embeddings via a similarity metric.
The key challenge in performing the task indicated in a query is that the robot
must determine the parts of the environment relevant to the query.
  This paper proposes a solution to this challenge. We leverage
natural-language synonyms and antonyms associated with the query within the
embedding space, applying heuristics to estimate the language space relevant to
the query, and use that to train a classifier to partition the environment into
matches and non-matches. We evaluate our method through extensive experiments,
querying both maps and standard image benchmarks. The results demonstrate
increased queryability of maps and images. Our querying technique is agnostic
to the representation and encoder used, and requires limited training.

</details>


### [117] [A Generalized Placeability Metric for Model-Free Unified Pick-and-Place Reasoning](https://arxiv.org/abs/2510.14584)
*Benno Wingender,Nils Dengler,Rohit Menon,Sicong Pan,Maren Bennewitz*

Main category: cs.RO

TL;DR: 提出了一种无需形状先验的通用放置度量方法，直接从噪声点云评估放置姿态，实现无模型统一的抓取-放置推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖强对象先验或平面支撑假设，限制了泛化能力和抓取与放置的统一推理。

Method: 通过几何提取支撑表面生成多方向放置候选，结合稳定性、可抓取性和间隙评分，联合优化抓取-放置对。

Result: 在未见过的真实物体和非平面支撑上，该方法预测稳定性损失的精度与CAD模型相当，且比基于学习的方法更物理合理。

Conclusion: 该方法实现了无模型统一的抓取-放置推理，适用于复杂场景。

Abstract: To reliably pick and place unknown objects under real-world sensing noise
remains a challenging task, as existing methods rely on strong object priors
(e.g., CAD models), or planar-support assumptions, limiting generalization and
unified reasoning between grasping and placing. In this work, we introduce a
generalized placeability metric that evaluates placement poses directly from
noisy point clouds, without any shape priors. The metric jointly scores
stability, graspability, and clearance. From raw geometry, we extract the
support surfaces of the object to generate diverse candidates for
multi-orientation placement and sample contacts that satisfy collision and
stability constraints. By conditioning grasp scores on each candidate
placement, our proposed method enables model-free unified pick-and-place
reasoning and selects grasp-place pairs that lead to stable, collision-free
placements. On unseen real objects and non-planar object supports, our metric
delivers CAD-comparable accuracy in predicting stability loss and generally
produces more physically plausible placements than learning-based predictors.

</details>


### [118] [Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning](https://arxiv.org/abs/2510.14612)
*Gabriel Fischer Abati,João Carlos Virgolino Soares,Giulio Turrisi,Victor Barasuol,Claudio Semini*

Main category: cs.RO

TL;DR: 提出一种将四足机器人本体感知时间序列数据编码为二维图像的方法，利用卷积神经网络学习运动相关任务，显著提升接触状态预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列处理方法难以捕捉多信号间的相关性和步态依赖模式，需要一种更丰富的特征表示方法。

Method: 将多源本体感知信号（如关节位置、IMU读数、足部速度）编码为二维图像，保留机器人形态结构，利用卷积神经网络学习。

Result: 在真实和模拟环境中，图像表示方法比传统序列模型显著提升预测精度（接触状态准确率从87.7%提升至94.5%）。

Conclusion: 跨模态编码策略（图像表示）在机器人状态学习中具有潜力，尤其在接触估计任务中表现优越。

Abstract: This paper presents a novel approach for representing proprioceptive
time-series data from quadruped robots as structured two-dimensional images,
enabling the use of convolutional neural networks for learning
locomotion-related tasks. The proposed method encodes temporal dynamics from
multiple proprioceptive signals, such as joint positions, IMU readings, and
foot velocities, while preserving the robot's morphological structure in the
spatial arrangement of the image. This transformation captures inter-signal
correlations and gait-dependent patterns, providing a richer feature space than
direct time-series processing. We apply this concept in the problem of contact
estimation, a key capability for stable and adaptive locomotion on diverse
terrains. Experimental evaluations on both real-world datasets and simulated
environments show that our image-based representation consistently enhances
prediction accuracy and generalization over conventional sequence-based models,
underscoring the potential of cross-modal encoding strategies for robotic state
learning. Our method achieves superior performance on the contact dataset,
improving contact state accuracy from 87.7% to 94.5% over the recently proposed
MI-HGNN method, using a 15 times shorter window size.

</details>


### [119] [Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models](https://arxiv.org/abs/2510.14615)
*Edward Sandra,Lander Vanroye,Dries Dirckx,Ruben Cartuyvels,Jan Swevers,Wilm Decré*

Main category: cs.RO

TL;DR: CAMPD利用扩散模型和注意力机制，实现机器人运动规划的泛化和高效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高维状态空间和复杂环境中难以扩展，扩散模型提供了一种新思路，但现有方法泛化能力有限。

Method: 提出CAMPD，基于分类器自由去噪扩散模型，结合传感器无关的上下文信息和注意力机制。

Result: 在7自由度机械臂上验证，CAMPD能泛化到未见环境，生成高质量多模态轨迹，且速度更快。

Conclusion: CAMPD展示了扩散模型在运动规划中的潜力，尤其在泛化和效率方面表现突出。

Abstract: Classical methods in robot motion planning, such as sampling-based and
optimization-based methods, often struggle with scalability towards
higher-dimensional state spaces and complex environments. Diffusion models,
known for their capability to learn complex, high-dimensional and multi-modal
data distributions, provide a promising alternative when applied to motion
planning problems and have already shown interesting results. However, most of
the current approaches train their model for a single environment, limiting
their generalization to environments not seen during training. The techniques
that do train a model for multiple environments rely on a specific camera to
provide the model with the necessary environmental information and therefore
always require that sensor. To effectively adapt to diverse scenarios without
the need for retraining, this research proposes Context-Aware Motion Planning
Diffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic
diffusion model, conditioned on sensor-agnostic contextual information. An
attention mechanism, integrated in the well-known U-Net architecture,
conditions the model on an arbitrary number of contextual parameters. CAMPD is
evaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art
approaches on real-world tasks, showing its ability to generalize to unseen
environments and generate high-quality, multi-modal trajectories, at a fraction
of the time required by existing methods.

</details>


### [120] [GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement](https://arxiv.org/abs/2510.14627)
*Yao Zhong,Hanzhi Chen,Simon Schaefer,Anran Zhang,Stefan Leutenegger*

Main category: cs.RO

TL;DR: GOPLA框架通过多模态大语言模型和扩散规划器，结合几何常识和语义偏好，显著提高了物体放置的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决家庭环境中物体放置任务中的语义偏好和几何可行性问题。

Method: 使用多模态大语言模型生成结构化计划，结合空间映射器和扩散规划器生成放置姿态。

Result: 实验表明，GOPLA在放置成功率和泛化能力上显著优于其他方法。

Conclusion: GOPLA框架在真实场景中表现出色，为机器人辅助任务提供了有效解决方案。

Abstract: Robots are expected to serve as intelligent assistants, helping humans with
everyday household organization. A central challenge in this setting is the
task of object placement, which requires reasoning about both semantic
preferences (e.g., common-sense object relations) and geometric feasibility
(e.g., collision avoidance). We present GOPLA, a hierarchical framework that
learns generalizable object placement from augmented human demonstrations. A
multi-modal large language model translates human instructions and visual
inputs into structured plans that specify pairwise object relationships. These
plans are then converted into 3D affordance maps with geometric common sense by
a spatial mapper, while a diffusion-based planner generates placement poses
guided by test-time costs, considering multi-plan distributions and collision
avoidance. To overcome data scarcity, we introduce a scalable pipeline that
expands human placement demonstrations into diverse synthetic training data.
Extensive experiments show that our approach improves placement success rates
by 30.04 percentage points over the runner-up, evaluated on positioning
accuracy and physical plausibility, demonstrating strong generalization across
a wide range of real-world robotic placement scenarios.

</details>


### [121] [Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation](https://arxiv.org/abs/2510.14643)
*Lara Brudermüller,Brandon Hung,Xinghao Zhu,Jiuguang Wang,Nick Hawes,Preston Culbertson,Simon Le Cleac'h*

Main category: cs.RO

TL;DR: 提出了一种生成预测控制（GPC）框架，通过训练条件流匹配模型来优化基于采样的模型预测控制（SPC），提高了在线规划的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖迭代优化或基于梯度的求解器，而本文旨在直接从噪声SPC数据中学习有意义的提议分布，以提升采样效率和规划性能。

Method: 使用条件流匹配模型在仿真中训练SPC控制序列，生成提议分布，用于在线规划。

Result: 在仿真和硬件实验中，该方法提高了采样效率，减少了规划时间，并在任务变化中表现出鲁棒性。

Conclusion: GPC框架在接触丰富的四足机器人操作任务中表现出色，为实时规划提供了高效且鲁棒的解决方案。

Abstract: We present a generative predictive control (GPC) framework that amortizes
sampling-based Model Predictive Control (SPC) by bootstrapping it with
conditional flow-matching models trained on SPC control sequences collected in
simulation. Unlike prior work relying on iterative refinement or gradient-based
solvers, we show that meaningful proposal distributions can be learned directly
from noisy SPC data, enabling more efficient and informed sampling during
online planning. We further demonstrate, for the first time, the application of
this approach to real-world contact-rich loco-manipulation with a quadruped
robot. Extensive experiments in simulation and on hardware show that our method
improves sample efficiency, reduces planning horizon requirements, and
generalizes robustly across task variations.

</details>


### [122] [Spatially anchored Tactile Awareness for Robust Dexterous Manipulation](https://arxiv.org/abs/2510.14647)
*Jialei Huang,Yang Ye,Yuanqing Gong,Xuezhou Zhu,Yang Gao,Kaifeng Zhang*

Main category: cs.RO

TL;DR: SaTA框架通过空间锚定触觉特征到手的运动学框架，显著提升了灵巧操作的精度和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有触觉学习方法难以实现亚毫米级精度任务，SaTA旨在结合触觉信号的感知丰富性和空间关系。

Method: SaTA通过前向运动学将触觉特征锚定到手的运动学框架，无需物体模型或显式姿态估计。

Result: 在多个高精度任务中，SaTA比基线方法成功率提升30%，任务完成时间减少27%。

Conclusion: SaTA证明了空间锚定触觉表示对高精度灵巧操作的有效性。

Abstract: Dexterous manipulation requires precise geometric reasoning, yet existing
visuo-tactile learning methods struggle with sub-millimeter precision tasks
that are routine for traditional model-based approaches. We identify a key
limitation: while tactile sensors provide rich contact information, current
learning frameworks fail to effectively leverage both the perceptual richness
of tactile signals and their spatial relationship with hand kinematics. We
believe an ideal tactile representation should explicitly ground contact
measurements in a stable reference frame while preserving detailed sensory
information, enabling policies to not only detect contact occurrence but also
precisely infer object geometry in the hand's coordinate system. We introduce
SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an
end-to-end policy framework that explicitly anchors tactile features to the
hand's kinematic frame through forward kinematics, enabling accurate geometric
reasoning without requiring object models or explicit pose estimation. Our key
insight is that spatially grounded tactile representations allow policies to
not only detect contact occurrence but also precisely infer object geometry in
the hand's coordinate system. We validate SaTA on challenging dexterous
manipulation tasks, including bimanual USB-C mating in free space, a task
demanding sub-millimeter alignment precision, as well as light bulb
installation requiring precise thread engagement and rotational control, and
card sliding that demands delicate force modulation and angular precision.
These tasks represent significant challenges for learning-based methods due to
their stringent precision requirements. Across multiple benchmarks, SaTA
significantly outperforms strong visuo-tactile baselines, improving success
rates by up to 30 percentage while reducing task completion times by 27
percentage.

</details>


### [123] [When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks](https://arxiv.org/abs/2510.14677)
*Steffen Hagedorn,Luka Donkov,Aron Distelzweig,Alexandru P. Condurache*

Main category: cs.RO

TL;DR: 论文提出将学习型交通代理模型SMART集成到nuPlan中，以更真实地评估规划器性能，结果显示基于IDM的模拟高估了规划性能，而SMART模拟揭示了规划器在复杂交互场景中的实际表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的交通代理（如IDM）行为简单且被动，无法真实反映规划器的缺陷和交互能力，导致评估结果偏差。

Method: 将学习型交通代理模型SMART集成到nuPlan中，评估14种规划器在更真实条件下的表现，并与IDM模拟结果对比。

Result: IDM模拟高估规划性能，多数规划器得分下降；但在多车道交互场景中，部分规划器表现优于预期。学习型规划器在常规场景表现稳定，但在极端场景下性能骤降。

Conclusion: 建议将SMART反应式模拟作为nuPlan的新标准闭环基准，以更真实地评估规划器性能。

Abstract: Planner evaluation in closed-loop simulation often uses rule-based traffic
agents, whose simplistic and passive behavior can hide planner deficiencies and
bias rankings. Widely used IDM agents simply follow a lead vehicle and cannot
react to vehicles in adjacent lanes, hindering tests of complex interaction
capabilities. We address this issue by integrating the state-of-the-art learned
traffic agent model SMART into nuPlan. Thus, we are the first to evaluate
planners under more realistic conditions and quantify how conclusions shift
when narrowing the sim-to-real gap. Our analysis covers 14 recent planners and
established baselines and shows that IDM-based simulation overestimates
planning performance: nearly all scores deteriorate. In contrast, many planners
interact better than previously assumed and even improve in multi-lane,
interaction-heavy scenarios like lane changes or turns. Methods trained in
closed-loop demonstrate the best and most stable driving performance. However,
when reaching their limits in augmented edge-case scenarios, all learned
planners degrade abruptly, whereas rule-based planners maintain reasonable
basic behavior. Based on our results, we suggest SMART-reactive simulation as a
new standard closed-loop benchmark in nuPlan and release the SMART agents as a
drop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.

</details>


### [124] [Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery](https://arxiv.org/abs/2510.14768)
*Fan Yang,Zixuan Huang,Abhinav Kumar,Sergio Aguilera Marinovic,Soshi Iba,Rana Soltani Zarrin,Dmitry Berenson*

Main category: cs.RO

TL;DR: CADRE框架通过结合接触感知特征提升机器人抓取恢复能力，并实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中机器人操作因意外干扰导致的失败问题，如物体掉落。

Method: 提出CADRE框架，结合强化学习和NDF模块提取接触特征。

Result: 实验显示接触特征提升训练效率和恢复成功率，并能零样本泛化到新物体。

Conclusion: CADRE通过接触感知特征有效提升机器人操作的鲁棒性和适应性。

Abstract: Real-world dexterous manipulation often encounters unexpected errors and
disturbances, which can lead to catastrophic failures, such as dropping the
manipulated object. To address this challenge, we focus on the problem of
catching a falling object while it remains within grasping range and,
importantly, resetting the system to a configuration favorable for resuming the
primary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a
reinforcement learning framework that incorporates a Neural Descriptor Field
(NDF)-inspired module to extract implicit contact features. Compared to methods
that rely solely on object pose or point cloud input, NDFs can directly reason
about finger-object correspondence and adapt to different object geometries.
Our experiments show that incorporating contact features improves training
efficiency, enhances convergence performance for RL training, and ultimately
leads to more successful recoveries. Additionally, we demonstrate that CADRE
can generalize zero-shot to unseen objects with different geometries.

</details>


### [125] [Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation](https://arxiv.org/abs/2510.14771)
*Xu Chi,Chao Zhang,Yang Su,Lingfeng Dou,Fujia Yang,Jiakuo Zhao,Haoyu Zhou,Xiaoyou Jia,Yong Zhou,Shan An*

Main category: cs.RO

TL;DR: Open TeleDex是一个统一的远程操作框架，旨在解决机器人模仿学习中高精度数据采集的瓶颈问题，支持多种机器人设备和输入设备。


<details>
  <summary>Details</summary>
Motivation: 现有远程操作系统难以保证跨设备的高精度数据采集，限制了机器人模仿学习的部署。

Method: 开发了Open TeleDex框架，提出了一种新的手部姿态重定向算法，提升了系统的互操作性。

Result: Open TeleDex支持多种机器人设备和输入设备，为复杂机器人操作和模仿学习提供了高质量平台。

Conclusion: Open TeleDex为学术研究和工业发展提供了高质量、公开可用的基础平台。

Abstract: Accurate and high-fidelity demonstration data acquisition is a critical
bottleneck for deploying robot Imitation Learning (IL) systems, particularly
when dealing with heterogeneous robotic platforms. Existing teleoperation
systems often fail to guarantee high-precision data collection across diverse
types of teleoperation devices. To address this, we developed Open TeleDex, a
unified teleoperation framework engineered for demonstration data collection.
Open TeleDex specifically tackles the TripleAny challenge, seamlessly
supporting any robotic arm, any dexterous hand, and any external input device.
Furthermore, we propose a novel hand pose retargeting algorithm that
significantly boosts the interoperability of Open TeleDex, enabling robust and
accurate compatibility with an even wider spectrum of heterogeneous master and
slave equipment. Open TeleDex establishes a foundational, high-quality, and
publicly available platform for accelerating both academic research and
industry development in complex robotic manipulation and IL.

</details>


### [126] [SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning](https://arxiv.org/abs/2510.14783)
*Aderik Verraest,Stavrow Bahnam,Robin Ferede,Guido de Croon,Christophe De Wagter*

Main category: cs.RO

TL;DR: SkyDreamer是首个端到端基于视觉的自主无人机竞速策略，通过像素级输入直接生成电机指令，实现全模拟到现实的转移、机载执行和冠军级性能。


<details>
  <summary>Details</summary>
Motivation: 现有自主无人机竞速系统局限于特定场景，端到端视觉方法虽具广泛适用性，但尚未实现全模拟到现实转移、机载执行和高性能的同步。

Method: 基于informed Dreamer的模型强化学习方法，世界模型解码为训练时特权信息，作为隐式状态和参数估计器。

Result: SkyDreamer在真实实验中实现高速飞行（21 m/s，6 g加速度），完成复杂机动，并展示对低质量分割掩码和电池耗尽的鲁棒性。

Conclusion: SkyDreamer填补了端到端视觉方法的空白，具备高适应性和鲁棒性，为高速敏捷飞行提供了新解决方案。

Abstract: Autonomous drone racing (ADR) systems have recently achieved champion-level
performance, yet remain highly specific to drone racing. While end-to-end
vision-based methods promise broader applicability, no system to date
simultaneously achieves full sim-to-real transfer, onboard execution, and
champion-level performance. In this work, we present SkyDreamer, to the best of
our knowledge, the first end-to-end vision-based ADR policy that maps directly
from pixel-level representations to motor commands. SkyDreamer builds on
informed Dreamer, a model-based reinforcement learning approach where the world
model decodes to privileged information only available during training. By
extending this concept to end-to-end vision-based ADR, the world model
effectively functions as an implicit state and parameter estimator, greatly
improving interpretability. SkyDreamer runs fully onboard without external aid,
resolves visual ambiguities by tracking progress using the state decoded from
the world model's hidden state, and requires no extrinsic camera calibration,
enabling rapid deployment across different drones without retraining.
Real-world experiments show that SkyDreamer achieves robust, high-speed flight,
executing tight maneuvers such as an inverted loop, a split-S and a ladder,
reaching speeds of up to 21 m/s and accelerations of up to 6 g. It further
demonstrates a non-trivial visual sim-to-real transfer by operating on
poor-quality segmentation masks, and exhibits robustness to battery depletion
by accurately estimating the maximum attainable motor RPM and adjusting its
flight path in real-time. These results highlight SkyDreamer's adaptability to
important aspects of the reality gap, bringing robustness while still achieving
extremely high-speed, agile flight.

</details>


### [127] [Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping](https://arxiv.org/abs/2510.14827)
*Yufei Zhu,Shih-Min Yang,Andrey Rudenko,Tomasz P. Kucner,Achim J. Lilienthal,Martin Magnusson*

Main category: cs.RO

TL;DR: 提出了一种基于隐式神经函数的连续时空动态地图表示方法，用于高效建模复杂人类运动模式。


<details>
  <summary>Details</summary>
Motivation: 在复杂人类环境中，机器人需要良好的特定场景运动模式模型以实现安全高效操作。现有动态地图（MoDs）采用离散空间采样且通常需要昂贵的离线构建。

Method: 基于隐式神经函数的连续时空动态地图表示，直接映射坐标到半包裹高斯混合模型参数，避免离散化和不均匀采样区域的插补。

Result: 在大型公开数据集上评估，相比基线方法，实现了更精确的运动表示和稀疏区域更平滑的速度分布，同时保持计算效率。

Conclusion: 该方法为建模复杂人类运动模式提供了一种强大且高效的途径。

Abstract: Safe and efficient robot operation in complex human environments can benefit
from good models of site-specific motion patterns. Maps of Dynamics (MoDs)
provide such models by encoding statistical motion patterns in a map, but
existing representations use discrete spatial sampling and typically require
costly offline construction. We propose a continuous spatio-temporal MoD
representation based on implicit neural functions that directly map coordinates
to the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the
need for discretization and imputation for unevenly sampled regions, enabling
smooth generalization across both space and time. Evaluated on a large public
dataset with long-term real-world people tracking data, our method achieves
better accuracy of motion representation and smoother velocity distributions in
sparse regions while still being computationally efficient, compared to
available baselines. The proposed approach demonstrates a powerful and
efficient way of modeling complex human motion patterns.

</details>


### [128] [RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning](https://arxiv.org/abs/2510.14830)
*Kun Lei,Huanyu Li,Dongjie Yu,Zhenyu Wei,Lingxiao Guo,Zhennan Jiang,Ziyu Wang,Shiyu Liang,Huazhe Xu*

Main category: cs.RO

TL;DR: RL-100是一个基于扩散视觉运动策略的强化学习框架，通过三阶段训练实现高效、可靠的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器人操作需要接近或超越人类操作员的可靠性、效率和鲁棒性。

Method: 采用三阶段训练：模仿学习、离线强化学习和在线强化学习，并结合一致性蒸馏技术降低延迟。

Result: 在7个真实机器人任务中实现100%成功率，共900次试验，并展示多小时的鲁棒性。

Conclusion: RL-100在多种任务和平台上表现出高效、可靠的性能，接近或超越人类操作水平。

Abstract: Real-world robotic manipulation in homes and factories demands reliability,
efficiency, and robustness that approach or surpass skilled human operators. We
present RL-100, a real-world reinforcement learning training framework built on
diffusion visuomotor policies trained bu supervised learning. RL-100 introduces
a three-stage pipeline. First, imitation learning leverages human priors.
Second, iterative offline reinforcement learning uses an Offline Policy
Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are
applied in the denoising process for conservative and reliable improvement.
Third, online reinforcement learning eliminates residual failure modes. An
additional lightweight consistency distillation head compresses the multi-step
sampling process in diffusion into a single-step policy, enabling
high-frequency control with an order-of-magnitude reduction in latency while
preserving task performance. The framework is task-, embodiment-, and
representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a
variety of robot platforms, and both single-step and action-chunk policies. We
evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control,
such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth
folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100
attains 100\% success across evaluated trials for a total of 900 out of 900
episodes, including up to 250 out of 250 consecutive trials on one task. The
method achieves near-human teleoperation or better time efficiency and
demonstrates multi-hour robustness with uninterrupted operation lasting up to
two hours.

</details>


### [129] [Multi Agent Switching Mode Controller for Sound Source localization](https://arxiv.org/abs/2510.14849)
*Marcello Sorge,Nicola Cigarini,Riccardo Lorigiola,Giulia Michieletto,Andrea Masiero,Angelo Cenedese,Alberto Guarnieri*

Main category: cs.RO

TL;DR: 多智能体切换模式控制策略用于声学目标定位，包括单目标和多目标场景。


<details>
  <summary>Details</summary>
Motivation: 声学传感器在无法直接视线定位的关键条件下仍能定位目标，具有重要研究价值。

Method: 设计多智能体切换模式控制策略，分别处理单目标和多目标定位场景。

Result: 单目标场景中智能体保持刚性编队移动，多目标场景中智能体独立搜索目标。

Conclusion: 该策略有效支持声学目标定位，适用于不同场景需求。

Abstract: Source seeking is an important topic in robotic research, especially
considering sound-based sensors since they allow the agents to locate a target
even in critical conditions where it is not possible to establish a direct line
of sight. In this work, we design a multi- agent switching mode control
strategy for acoustic-based target localization. Two scenarios are considered:
single source localization, in which the agents are driven maintaining a rigid
formation towards the target, and multi-source scenario, in which each agent
searches for the targets independently from the others.

</details>


### [130] [SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time](https://arxiv.org/abs/2510.14851)
*Jakob Bichler,Andreu Matoses Gimenez,Javier Alonso-Mora*

Main category: cs.RO

TL;DR: Sadcher是一个实时任务分配框架，用于异构多机器人团队，结合动态联盟形成和任务优先级约束，通过模仿学习训练，利用图注意力和Transformer预测奖励，生成高质量调度。


<details>
  <summary>Details</summary>
Motivation: 解决异构多机器人团队在动态环境中的实时任务分配问题，结合时空推理和泛化能力。

Method: 通过模仿学习训练，结合图注意力和Transformer预测机器人-任务奖励，使用松弛二分匹配生成调度。

Result: 在随机未见问题上优于其他学习和启发式基线，适用于实时操作，并能扩展到更大规模任务和团队。

Conclusion: Sadcher在实时任务分配中表现出色，具有泛化能力和可扩展性，并公开了数据集。

Abstract: We present Sadcher, a real-time task assignment framework for heterogeneous
multi-robot teams that incorporates dynamic coalition formation and task
precedence constraints. Sadcher is trained through Imitation Learning and
combines graph attention and transformers to predict assignment rewards between
robots and tasks. Based on the predicted rewards, a relaxed bipartite matching
step generates high-quality schedules with feasibility guarantees. We
explicitly model robot and task positions, task durations, and robots'
remaining processing times, enabling advanced temporal and spatial reasoning
and generalization to environments with different spatiotemporal distributions
compared to training. Trained on optimally solved small-scale instances, our
method can scale to larger task sets and team sizes. Sadcher outperforms other
learning-based and heuristic baselines on randomized, unseen problems for small
and medium-sized teams with computation times suitable for real-time operation.
We also explore sampling-based variants and evaluate scalability across robot
and task counts. In addition, we release our dataset of 250,000 optimal
schedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/

</details>


### [131] [STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search](https://arxiv.org/abs/2510.14893)
*Helene J. Levy,Brett T. Lopez*

Main category: cs.RO

TL;DR: STITCHER是一种无需优化的实时轨迹规划框架，通过拼接短轨迹段生成长距离、高表达性且接近最优的轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的轨迹规划方法在实时性和数值稳定性上存在局限，难以满足高速自主导航的需求。

Method: STITCHER结合图搜索和短轨迹段拼接技术，实现实时规划。

Result: 仿真和硬件测试表明，STITCHER能在毫秒级生成安全轨迹，并处理非凸约束。

Conclusion: STITCHER在实时性和约束处理上优于传统优化方法，适用于高速自主导航。

Abstract: Autonomous high-speed navigation through large, complex environments requires
real-time generation of agile trajectories that are dynamically feasible,
collision-free, and satisfy state or actuator constraints. Modern trajectory
planning techniques primarily use numerical optimization, as they enable the
systematic computation of high-quality, expressive trajectories that satisfy
various constraints. However, stringent requirements on computation time and
the risk of numerical instability can limit the use of optimization-based
planners in safety-critical scenarios. This work presents an optimization-free
planning framework called STITCHER that stitches short trajectory segments
together with graph search to compute long-range, expressive, and near-optimal
trajectories in real-time. STITCHER outperforms modern optimization-based
planners through our innovative planning architecture and several algorithmic
developments that make real-time planning possible. Extensive simulation
testing is performed to analyze the algorithmic components that make up
STITCHER, along with a thorough comparison with two state-of-the-art
optimization planners. Simulation tests show that safe trajectories can be
created within a few milliseconds for paths that span the entirety of two 50 m
x 50 m environments. Hardware tests with a custom quadrotor verify that
STITCHER can produce trackable paths in real-time while respecting nonconvex
constraints, such as limits on tilt angle and motor forces, which are otherwise
hard to include in optimization-based planners.

</details>


### [132] [VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation](https://arxiv.org/abs/2510.14902)
*Han Zhao,Jiaxuan Zhang,Wenxuan Song,Pengxiang Ding,Donglin Wang*

Main category: cs.RO

TL;DR: VLA^2框架通过结合外部模块（如网络检索和物体检测）提升VLA模型处理未见对象的能力，显著提高了在硬级别基准测试中的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在遇到训练数据外的对象概念时表现不佳，需要一种方法来增强其泛化能力。

Method: 提出VLA^2框架，利用OpenVLA作为执行主干，结合外部模块提供目标对象的视觉和文本知识。

Result: 在硬级别基准测试中，VLA^2比OpenVLA基线成功率提高了44.2%，且不影响域内任务性能。

Conclusion: VLA^2通过外部知识增强显著提升了VLA模型处理未见对象的能力，适用于复杂环境。

Abstract: Current vision-language-action (VLA) models, pre-trained on large-scale
robotic data, exhibit strong multi-task capabilities and generalize well to
variations in visual and language instructions for manipulation. However, their
success rate drops significantly when faced with object concepts outside the
training data, such as unseen object descriptions and textures in the dataset.
To address this, we propose a novel agentic framework, VLA^2, which leverages
OpenVLA as the execution backbone and effectively leverages external modules
such as web retrieval and object detection to provide visual and textual
knowledge about target objects to the VLA. This approach mitigates
generalization failure when handling out-of-distribution objects. Based on the
LIBERO simulation environment, we introduced novel objects and object
descriptions to construct a new evaluation benchmark with three difficulty
levels to test the effectiveness of our method. Our framework successfully
outperformed the current state-of-the-art models on our designed hard-level
generalization benchmark. Compared to the standalone OpenVLA baseline, VLA^2
achieves a 44.2% improvement in the success rate in the hard-level benchmark
and an average improvement of 20.2% in all customized environments without any
performance degradation on in-domain tasks. Project website:
https://vla-2.github.io.

</details>


### [133] [VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin](https://arxiv.org/abs/2510.14930)
*Binghao Huang,Jie Xu,Iretiayo Akinola,Wei Yang,Balakumar Sundaralingam,Rowland O'Flaherty,Dieter Fox,Xiaolong Wang,Arsalan Mousavian,Yu-Wei Chao,Yunzhu Li*

Main category: cs.RO

TL;DR: VT-Refine结合视觉触觉输入、仿真和强化学习，提升双手机器人装配任务的性能。


<details>
  <summary>Details</summary>
Motivation: 人类通过触觉反馈适应双手机器人装配任务，但机器人仅通过行为克隆难以实现类似能力。

Method: 结合真实演示、高保真触觉仿真和强化学习，训练扩散策略并在仿真中优化。

Result: VT-Refine提高了仿真和现实中的装配性能，增强了数据多样性和策略微调效果。

Conclusion: VT-Refine为双手机器人装配任务提供了一种高效的学习框架。

Abstract: Humans excel at bimanual assembly tasks by adapting to rich tactile feedback
-- a capability that remains difficult to replicate in robots through
behavioral cloning alone, due to the suboptimality and limited diversity of
human demonstrations. In this work, we present VT-Refine, a visuo-tactile
policy learning framework that combines real-world demonstrations,
high-fidelity tactile simulation, and reinforcement learning to tackle precise,
contact-rich bimanual assembly. We begin by training a diffusion policy on a
small set of demonstrations using synchronized visual and tactile inputs. This
policy is then transferred to a simulated digital twin equipped with simulated
tactile sensors and further refined via large-scale reinforcement learning to
enhance robustness and generalization. To enable accurate sim-to-real transfer,
we leverage high-resolution piezoresistive tactile sensors that provide normal
force signals and can be realistically modeled in parallel using
GPU-accelerated simulation. Experimental results show that VT-Refine improves
assembly performance in both simulation and the real world by increasing data
diversity and enabling more effective policy fine-tuning. Our project page is
available at https://binghao-huang.github.io/vt_refine/.

</details>


### [134] [Architecture Is All You Need: Diversity-Enabled Sweet Spots for Robust Humanoid Locomotion](https://arxiv.org/abs/2510.14947)
*Blake Werner,Lizhi Yang,Aaron D. Ames*

Main category: cs.RO

TL;DR: 分层控制架构（LCA）通过分离时间尺度，显著提升了人形机器人在非结构化环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中实现鲁棒的人形机器人运动需要平衡快速低层稳定与慢速感知决策。

Method: 采用分层控制架构，包括高速率的本体感受稳定器和低速率的紧凑感知策略，并通过两阶段训练（先盲稳定器预训练，后感知微调）。

Result: 在仿真和硬件实验中，分层策略优于单阶段设计，成功完成单阶段感知策略失败的任务（如楼梯和边缘）。

Conclusion: 时间尺度的架构分离是鲁棒感知运动的关键，而非网络规模或复杂性。

Abstract: Robust humanoid locomotion in unstructured environments requires
architectures that balance fast low-level stabilization with slower perceptual
decision-making. We show that a simple layered control architecture (LCA), a
proprioceptive stabilizer running at high rate, coupled with a compact low-rate
perceptual policy, enables substantially more robust performance than
monolithic end-to-end designs, even when using minimal perception encoders.
Through a two-stage training curriculum (blind stabilizer pretraining followed
by perceptual fine-tuning), we demonstrate that layered policies consistently
outperform one-stage alternatives in both simulation and hardware. On a Unitree
G1 humanoid, our approach succeeds across stair and ledge tasks where one-stage
perceptual policies fail. These results highlight that architectural separation
of timescales, rather than network scale or complexity, is the key enabler for
robust perception-conditioned locomotion.

</details>


### [135] [From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance](https://arxiv.org/abs/2510.14952)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Yibo Peng,Tao Huang,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang,Chang Xu*

Main category: cs.RO

TL;DR: RoboGhost是一个直接从语言到动作的框架，避免了传统多阶段流程的累积错误和高延迟，通过扩散策略直接生成可执行动作。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导的人形机器人运动流程复杂且不可靠，多阶段过程易产生累积错误和高延迟，语义与控制耦合弱。

Method: RoboGhost通过扩散策略直接从噪声中生成动作，结合因果Transformer-扩散运动生成器确保长期一致性和稳定性。

Result: 实验表明，RoboGhost显著降低延迟，提高成功率和跟踪精度，生成平滑且语义对齐的运动。

Conclusion: RoboGhost为语言-动作系统提供了通用基础，并可扩展至其他模态如视觉和音频。

Abstract: Natural language offers a natural interface for humanoid robots, but existing
language-guided humanoid locomotion pipelines remain cumbersome and unreliable.
They typically decode human motion, retarget it to robot morphology, and then
track it with a physics-based controller. However, this multi-stage process is
prone to cumulative errors, introduces high latency, and yields weak coupling
between semantics and control. These limitations call for a more direct pathway
from language to action, one that eliminates fragile intermediate stages.
Therefore, we present RoboGhost, a retargeting-free framework that directly
conditions humanoid policies on language-grounded motion latents. By bypassing
explicit motion decoding and retargeting, RoboGhost enables a diffusion-based
policy to denoise executable actions directly from noise, preserving semantic
intent and supporting fast, reactive control. A hybrid causal
transformer-diffusion motion generator further ensures long-horizon consistency
while maintaining stability and diversity, yielding rich latent representations
for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost
substantially reduces deployment latency, improves success rates and tracking
accuracy, and produces smooth, semantically aligned locomotion on real
humanoids. Beyond text, the framework naturally extends to other modalities
such as images, audio, and music, providing a general foundation for
vision-language-action humanoid systems.

</details>


### [136] [CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions](https://arxiv.org/abs/2510.14959)
*Lizhi Yang,Blake Werner,Massimiliano de Sa Aaron D. Ames*

Main category: cs.RO

TL;DR: CBF-RL框架通过训练中强制执行控制屏障函数（CBFs），结合RL生成安全行为，避免在线安全过滤器的保守性。


<details>
  <summary>Details</summary>
Motivation: 传统RL可能牺牲安全性以追求性能，而CBFs虽能确保安全，但缺乏RL策略的知识可能导致保守行为。

Method: 提出CBF-RL框架，通过CBF项最小化修改RL策略，并在训练中过滤策略滚动以确保安全。

Result: 理论证明连续时间安全过滤器可通过离散时间滚动部署，实验显示CBF-RL内化安全约束，实现无在线过滤器的安全部署。

Conclusion: CBF-RL在导航任务和人形机器人上验证了其安全性、快速收敛性和鲁棒性，适用于现实场景。

Abstract: Reinforcement learning (RL), while powerful and expressive, can often
prioritize performance at the expense of safety. Yet safety violations can lead
to catastrophic outcomes in real-world deployments. Control Barrier Functions
(CBFs) offer a principled method to enforce dynamic safety -- traditionally
deployed \emph{online} via safety filters. While the result is safe behavior,
the fact that the RL policy does not have knowledge of the CBF can lead to
conservative behaviors. This paper proposes CBF-RL, a framework for generating
safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key
attributes: (1) minimally modifying a nominal RL policy to encode safety
constraints via a CBF term, (2) and safety filtering of the policy rollouts in
training. Theoretically, we prove that continuous-time safety filters can be
deployed via closed-form expressions on discrete-time roll-outs. Practically,
we demonstrate that CBF-RL internalizes the safety constraints in the learned
policy -- both enforcing safer actions and biasing towards safer rewards --
enabling safe deployment without the need for an online safety filter. We
validate our framework through ablation studies on navigation tasks and on the
Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster
convergence, and robust performance under uncertainty, enabling the humanoid
robot to avoid obstacles and climb stairs safely in real-world settings without
a runtime safety filter.

</details>


### [137] [RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks](https://arxiv.org/abs/2510.14968)
*Mingxuan Yan,Yuping Wang,Zechun Liu,Jiachen Li*

Main category: cs.RO

TL;DR: 提出了一种基于检索的演示分解器（RDD），通过视觉特征对齐自动分解任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM规划器依赖人工标注或启发式规则分解任务，可能导致子任务与低层策略训练数据不匹配，影响性能。

Method: RDD通过视觉特征对齐，自动将演示分解为与低层策略训练数据匹配的子任务。

Result: 在仿真和真实任务中优于现有分解器，具有鲁棒性。

Conclusion: RDD有效解决了任务分解与低层策略数据不匹配的问题，提升了性能。

Abstract: To tackle long-horizon tasks, recent hierarchical vision-language-action
(VLAs) frameworks employ vision-language model (VLM)-based planners to
decompose complex manipulation tasks into simpler sub-tasks that low-level
visuomotor policies can easily handle. Typically, the VLM planner is finetuned
to learn to decompose a target task. This finetuning requires target task
demonstrations segmented into sub-tasks by either human annotation or heuristic
rules. However, the heuristic subtasks can deviate significantly from the
training data of the visuomotor policy, which degrades task performance. To
address these issues, we propose a Retrieval-based Demonstration Decomposer
(RDD) that automatically decomposes demonstrations into sub-tasks by aligning
the visual features of the decomposed sub-task intervals with those from the
training data of the low-level visuomotor policies. Our method outperforms the
state-of-the-art sub-task decomposer on both simulation and real-world tasks,
demonstrating robustness across diverse settings. Code and more results are
available at rdd-neurips.github.io.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [138] [Large Language Models for Real-World IoT Device Identification](https://arxiv.org/abs/2510.13817)
*Rameen Mahmood,Tousif Ahmed,Sai Teja Peddinti,Danny Yuxing Huang*

Main category: cs.LG

TL;DR: 提出了一种基于语义推理的物联网设备识别方法，通过语言建模任务和LLM指导的高保真标签，实现了高准确率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前物联网设备识别方法无法满足快速扩张的需求，导致安全、隐私和网络责任问题，尤其在开放环境中。

Method: 使用语义推理管道将设备识别任务转化为语言建模问题，利用LLM生成高保真标签，并通过量化LLaMA3.18B模型进行指令调优。

Result: 模型在2015个厂商中达到98.25%的top-1准确率和90.73%的宏准确率，且对缺失字段和对抗攻击具有鲁棒性。

Conclusion: 指令调优的LLM为大规模物联网设备识别提供了可扩展且可解释的基础。

Abstract: The rapid expansion of IoT devices has outpaced current identification
methods, creating significant risks for security, privacy, and network
accountability. These challenges are heightened in open-world environments,
where traffic metadata is often incomplete, noisy, or intentionally obfuscated.
We introduce a semantic inference pipeline that reframes device identification
as a language modeling task over heterogeneous network metadata. To construct
reliable supervision, we generate high-fidelity vendor labels for the IoT
Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble
of large language models guided by mutual-information and entropy-based
stability scores. We then instruction-tune a quantized LLaMA3.18B model with
curriculum learning to support generalization under sparsity and long-tail
vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro
accuracy across 2,015 vendors while maintaining resilience to missing fields,
protocol drift, and adversarial manipulation. Evaluation on an independent IoT
testbed, coupled with explanation quality and adversarial stress tests,
demonstrates that instruction-tuned LLMs provide a scalable and interpretable
foundation for real-world device identification at scale.

</details>


### [139] [Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation](https://arxiv.org/abs/2510.13864)
*Zixi Wang,Yushe Cao,Yubo Huang,Jinzhu Wei,Jingzehua Xu,Shuai Zhang,Xin Lai*

Main category: cs.LG

TL;DR: 提出了一种名为STDW的自训练动态加权方法，用于增强渐进域适应的鲁棒性，通过动态权重机制平衡源域和目标域的损失贡献。


<details>
  <summary>Details</summary>
Motivation: 传统渐进域适应方法在知识迁移和中间数据完整性方面存在不足，STDW旨在解决这些问题。

Method: 引入动态权重机制，通过时间变化超参数控制域特定学习强度，结合自训练生成伪标签并优化加权目标函数。

Result: 在多个数据集上的实验表明，STDW优于现有基线方法，动态调度超参数对渐进适应至关重要。

Conclusion: STDW为鲁棒渐进域适应提供了理论和实践框架，适用于动态现实场景。

Abstract: In this paper, we propose a new method called Self-Training with Dynamic
Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation
(GDA) by addressing the challenge of smooth knowledge migration from the source
to the target domain. Traditional GDA methods mitigate domain shift through
intermediate domains and self-training but often suffer from inefficient
knowledge migration or incomplete intermediate data. Our approach introduces a
dynamic weighting mechanism that adaptively balances the loss contributions of
the source and target domains during training. Specifically, we design an
optimization framework governed by a time-varying hyperparameter $\varrho$
(progressing from 0 to 1), which controls the strength of domain-specific
learning and ensures stable adaptation. The method leverages self-training to
generate pseudo-labels and optimizes a weighted objective function for
iterative model updates, maintaining robustness across intermediate domains.
Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the
Cover Type dataset demonstrate that STDW outperforms existing baselines.
Ablation studies further validate the critical role of $\varrho$'s dynamic
scheduling in achieving progressive adaptation, confirming its effectiveness in
reducing domain bias and improving generalization. This work provides both
theoretical insights and a practical framework for robust gradual domain
adaptation, with potential applications in dynamic real-world scenarios. The
code is available at https://github.com/Dramwig/STDW.

</details>


### [140] [Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning](https://arxiv.org/abs/2510.13865)
*Dongkwan Lee,Junhoo Lee,Nojun Kwak*

Main category: cs.LG

TL;DR: Deep Edge Filter通过高频滤波提升深度神经网络特征的泛化能力，验证了高频分量包含任务相关语义信息的假设。


<details>
  <summary>Details</summary>
Motivation: 假设神经网络在深度特征的高频分量中编码任务相关语义信息，而低频分量存储领域特定偏差。

Method: 通过从原始特征中减去低通滤波输出，分离出可泛化的表示。

Result: 在视觉、文本、3D和音频等多个领域均表现出一致的性能提升。

Conclusion: 方法实现了特征稀疏化并有效隔离高频分量，验证了核心假设。

Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass
filtering to deep neural network features to improve model generalizability.
Our method is motivated by our hypothesis that neural networks encode
task-relevant semantic information in high-frequency components while storing
domain-specific biases in low-frequency components of deep features. By
subtracting low-pass filtered outputs from original features, our approach
isolates generalizable representations while preserving architectural
integrity. Experimental results across diverse domains such as Vision, Text,
3D, and Audio demonstrate consistent performance improvements regardless of
model architecture and data modality. Analysis reveals that our method induces
feature sparsification and effectively isolates high-frequency components,
providing empirical validation of our core hypothesis. The code is available at
https://github.com/dongkwani/DeepEdgeFilter.

</details>


### [141] [CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks](https://arxiv.org/abs/2510.13869)
*Munsif Ali,Leonardo Rossi,Massimo Bertozzi*

Main category: cs.LG

TL;DR: CoLoR-GAN提出了一种结合低秩适应的持续少样本学习框架，显著减少了参数数量并保持SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决GAN在持续学习（CL）和少样本（FS）场景下的灾难性遗忘问题，同时减少参数开销。

Method: 采用低秩张量（LoRA）适应目标任务，并进一步提出LLoRA技术优化卷积层适配器。

Result: 在多个CL和FS基准任务中表现高效，参数大幅减少且性能达到SOTA。

Conclusion: CoLoR-GAN通过低秩适应有效平衡了性能和资源消耗，为持续少样本学习提供了实用解决方案。

Abstract: Continual learning (CL) in the context of Generative Adversarial Networks
(GANs) remains a challenging problem, particularly when it comes to learn from
a few-shot (FS) samples without catastrophic forgetting. Current most effective
state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible
quantity of new weights at each training iteration, which would become
significant when considering the long term. For this reason, this paper
introduces \textcolor{red}{\textbf{\underline{c}}}ontinual
few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with
\textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank
adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and
CL together, leveraging low-rank tensors to efficiently adapt the model to
target tasks while reducing even more the number of parameters required.
Applying a vanilla LoRA implementation already permitted us to obtain pretty
good results. In order to optimize even further the size of the adapters, we
challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for
convolutional layers. Finally, aware of the criticality linked to the choice of
the hyperparameters of LoRA, we provide an empirical study to easily find the
best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on
several benchmark CL and FS tasks and show that our model is efficient,
reaching SOTA performance but with a number of resources enormously reduced.
Source code is available on
\href{https://github.com/munsifali11/CoLoR-GAN}{Github.

</details>


### [142] [Joint Discriminative-Generative Modeling via Dual Adversarial Training](https://arxiv.org/abs/2510.13872)
*Xuwang Yin,Claire Zhang,Julie Steele,Nir Shavit,Tony T. Wang*

Main category: cs.LG

TL;DR: 提出一种结合对抗训练的新型框架，同时提升分类鲁棒性和生成模型质量，解决了JEM的不稳定性和样本质量问题。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型（如JEM）在分类鲁棒性和生成质量上存在局限性，尤其是SGLD训练的不稳定性。

Method: 采用对抗训练优化能量函数，分两阶段训练，避免批归一化与EBM的冲突。

Result: 在CIFAR和ImageNet上显著提升鲁棒性，生成质量接近扩散模型。

Conclusion: 对抗训练可作为统一框架的基础，同时实现高质量生成和鲁棒分类。

Abstract: Simultaneously achieving robust classification and high-fidelity generative
modeling within a single framework presents a significant challenge. Hybrid
approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as
EBMs but are often limited by the instability and poor sample quality inherent
in SGLD-based training. We address these limitations by proposing a novel
training framework that integrates adversarial training (AT) principles for
both discriminative robustness and stable generative learning. The proposed
method introduces three key innovations: (1) the replacement of SGLD-based JEM
learning with a stable, AT-based approach that optimizes the energy function by
discriminating between real data and PGD-generated contrastive samples using
the BCE loss; (2) synergistic adversarial training for the discriminative
component that enhances classification robustness while eliminating the need
for explicit gradient penalties; and (3) a two-stage training procedure to
resolve the incompatibility between batch normalization and EBM training.
Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method
substantially improves adversarial robustness over existing hybrid models while
maintaining competitive generative performance. On ImageNet, when optimized for
generative modeling, our model's generative fidelity surpasses that of BigGAN
and approaches diffusion models, representing the first MCMC-based EBM approach
to achieve high-quality generation on complex, high-resolution datasets. Our
approach addresses key stability issues that have limited JEM scaling and
demonstrates that adversarial training can serve as an effective foundation for
unified frameworks capable of generating and robustly classifying visual data.

</details>


### [143] [K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](https://arxiv.org/abs/2510.13891)
*Yifeng Yao,Yike Yun,Jing Wang,Huishuai Zhang,Dongyan Zhao,Ke Tian,Zhihao Wang,Minghui Qiu,Tao Wang*

Main category: cs.LG

TL;DR: K-frames提出了一种场景驱动的关键帧选择方法，通过预测语义连贯的片段来保留时间连续性，解决了现有方法稀疏和不连续的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视频理解中因上下文窗口和计算成本受限，且均匀采样或关键帧选择方法导致信息丢失或帧不连续。

Method: K-frames采用三阶段渐进式课程学习，包括时间定位和关键片段感知的监督微调，以及强化学习优化预测策略。

Result: 实验表明K-frames在多种尺度下提供了高效、可解释且即插即用的关键帧选择方案。

Conclusion: K-frames为长视频理解提供了一种灵活且连续的关键帧选择方法，并发布了数据集和模型。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in image understanding, but long-video are constrained by context
windows and computational cost. Uniform frame sampling often leads to
substantial information loss. Meanwhile existing keyframe selection methods
such as text-frame retrieval or RL-based frame optimization typically yield
sparse and temporally disjointed frames, overlooking scene continuity and
lacking flexibility for multi-scale frame selection. To address these
limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe
selection that preserves temporal continuity. Instead of selecting individual
frames, K-frames predicts semantically coherent, query-relevant clips, which
enables any-k keyframes selection to meet diverse user budgets. To achieve this
approach, we first introduce PeakClips, a dataset of 200K video highlights
conditioned by query. Building on this dataset, K-frames learns clip2frame
selection using a three-stage progressive curriculum. It involves two
Supervised Fine-Tuning stages for temporal grounding and key-clip perception,
followed by a Reinforcement Learning stage that directly optimizes the
scene-driven prediction policy for downstream task without further annotations.
Extensive experiments on major long-video understanding benchmarks demonstrate
that K-frames provides an effective, interpretable, and plug-and-play solution
for keyframe selection at various scales. Our dataset and model will be
available.

</details>


### [144] [Multi-View Semi-Supervised Label Distribution Learning with Local Structure Complementarity](https://arxiv.org/abs/2510.13917)
*Yanshan Xiao,Kaihong Wu,Bo Liu*

Main category: cs.LG

TL;DR: 提出了一种多视图半监督标签分布学习方法（MVSS-LDL），通过利用多视图的局部最近邻结构互补性，提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅针对单视图标签分布学习问题，未考虑多视图及未标记数据的情况。

Method: 通过计算每个视图的k最近邻，并结合其他视图的最近邻信息，构建基于图学习的多视图半监督模型。

Result: MVSS-LDL在分类性能上显著优于现有单视图方法。

Conclusion: 该方法首次尝试多视图标签分布学习，验证了局部结构互补性的有效性。

Abstract: Label distribution learning (LDL) is a paradigm that each sample is
associated with a label distribution. At present, the existing approaches are
proposed for the single-view LDL problem with labeled data, while the
multi-view LDL problem with labeled and unlabeled data has not been considered.
In this paper, we put forward the multi-view semi-supervised label distribution
learning with local structure complementarity (MVSS-LDL) approach, which
exploits the local nearest neighbor structure of each view and emphasizes the
complementarity of local nearest neighbor structures in multiple views.
Specifically speaking, we first explore the local structure of view $v$ by
computing the $k$-nearest neighbors. As a result, the $k$-nearest neighbor set
of each sample $\boldsymbol{x}_i$ in view $v$ is attained. Nevertheless, this
$k$-nearest neighbor set describes only a part of the nearest neighbor
information of sample $\boldsymbol{x}_i$. In order to obtain a more
comprehensive description of sample $\boldsymbol{x}_i$'s nearest neighbors, we
complement the nearest neighbor set in view $v$ by incorporating sample
$\boldsymbol{x}_i$'s nearest neighbors in other views. Lastly, based on the
complemented nearest neighbor set in each view, a graph learning-based
multi-view semi-supervised LDL model is constructed. By considering the
complementarity of local nearest neighbor structures, different views can
mutually provide the local structural information to complement each other. To
the best of our knowledge, this is the first attempt at multi-view LDL.
Numerical studies have demonstrated that MVSS-LDL attains explicitly better
classification performance than the existing single-view LDL methods.

</details>


### [145] [Weight Weaving: Parameter Pooling for Data-Free Model Merging](https://arxiv.org/abs/2510.13921)
*Levy Chaves,Eduardo Valle,Sandra Avila*

Main category: cs.LG

TL;DR: Weight Weaving是一种无需评估数据的模型合并技术，通过用户定义的池化函数在λ值搜索空间中合并模型权重，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法依赖评估数据调整超参数λ，实践中不可行，需一种无需数据的方法。

Method: 提出Weight Weaving，通过池化函数（如平均、随机选择）在λ搜索空间中合并权重，与现有方法正交。

Result: 在三种ViT变体和实验设置中，平均准确率提升达15.9个百分点。

Conclusion: Weight Weaving是一种高效、模块化的数据无关模型合并方法，显著提升性能。

Abstract: Model merging provides a cost-effective and data-efficient combination of
specialized deep neural networks through parameter integration. This technique
leverages expert models across downstream tasks without requiring retraining.
Most model merging approaches critically depend on scaling hyper-parameters
$\lambda$, which weight each model's contribution globally or individually.
Principled approaches for setting scaling factors without accessing any data
(data-free) are scarce, often leading researchers to tune $\lambda$ using
privileged data from the evaluation set, which is obviously unfeasible in
practice. To address this limitation, we introduce Weight Weaving, a
plug-and-play technique that pools model weights across $\lambda$ values search
space using user-defined pooling functions, such as averaging, random
selection, or even existing model merging methods. Our method demonstrates high
modularity, imposing minimal constraints on the search space. It operates
orthogonally to existing model merging methods and eliminates evaluation data
requirements. We validate Weight Weaving across three ViT variants in three
experimental setups: vision multi-task learning, vision continual learning, and
domain generalization. Our method consistently improves the performance of
several model merging methods, achieving average accuracy gains of up to 15.9
percentage points in a data-free setting.

</details>


### [146] [LTR-ICD: A Learning-to-Rank Approach for Automatic ICD Coding](https://arxiv.org/abs/2510.13922)
*Mohammad Mansoori,Amira Soliman,Farzaneh Etminani*

Main category: cs.LG

TL;DR: 论文提出了一种基于检索系统视角的方法，将ICD代码分配问题视为分类和排序任务，优于现有分类方法。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中的ICD代码顺序对诊断和报销至关重要，但现有方法仅将其视为分类任务，忽略了顺序的重要性。

Method: 将ICD代码分配问题重新定义为分类和排序任务，结合检索系统视角。

Result: 模型在正确排序主要诊断代码上的准确率为47%（现有方法为20%），分类指标（微/宏F1）也优于之前最佳模型。

Conclusion: 该方法在考虑代码顺序和分类性能上均优于现有方法，展示了检索系统视角的有效性。

Abstract: Clinical notes contain unstructured text provided by clinicians during
patient encounters. These notes are usually accompanied by a sequence of
diagnostic codes following the International Classification of Diseases (ICD).
Correctly assigning and ordering ICD codes are essential for medical diagnosis
and reimbursement. However, automating this task remains challenging.
State-of-the-art methods treated this problem as a classification task, leading
to ignoring the order of ICD codes that is essential for different purposes. In
this work, as a first attempt, we approach this task from a retrieval system
perspective to consider the order of codes, thus formulating this problem as a
classification and ranking task. Our results and analysis show that the
proposed framework has a superior ability to identify high-priority codes
compared to other methods. For instance, our model accuracy in correctly
ranking primary diagnosis codes is 47%, compared to 20% for the
state-of-the-art classifier. Additionally, in terms of classification metrics,
the proposed model achieves a micro- and macro-F1 scores of 0.6065 and 0.2904,
respectively, surpassing the previous best model with scores of 0.597 and
0.2660.

</details>


### [147] [Distributional Consistency Loss: Beyond Pointwise Data Terms in Inverse Problems](https://arxiv.org/abs/2510.13972)
*George Webber,Andrew J. Reader*

Main category: cs.LG

TL;DR: 论文提出了一种名为分布一致性（DC）损失的新数据保真度目标，用于解决逆问题中噪声过拟合的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的数据保真度损失函数（如MSE）容易过拟合噪声，因此需要一种更统计一致的方法来避免这一问题。

Method: 通过测试观测数据是否与当前估计的噪声分布统计一致，引入DC损失作为替代传统点匹配的方法。

Result: 在图像去噪和医学图像重建中，DC损失显著提高了性能，减少了噪声过拟合和伪影。

Conclusion: DC损失是一种统计基础扎实、性能优越的替代方案，适用于多种逆问题。

Abstract: Recovering true signals from noisy measurements is a central challenge in
inverse problems spanning medical imaging, geophysics, and signal processing.
Current solutions balance prior assumptions regarding the true signal
(regularization) with agreement to noisy measured data (data-fidelity).
Conventional data-fidelity loss functions, such as mean-squared error (MSE) or
negative log-likelihood, seek pointwise agreement with noisy measurements,
often leading to overfitting to noise. In this work, we instead evaluate
data-fidelity collectively by testing whether the observed measurements are
statistically consistent with the noise distributions implied by the current
estimate. We adopt this aggregated perspective and introduce distributional
consistency (DC) loss, a data-fidelity objective that replaces pointwise
matching with distribution-level calibration using model-based probability
scores for each measurement. DC loss acts as a direct and practical plug-in
replacement for standard data consistency terms: i) it is compatible with
modern regularizers, ii) it is optimized in the same way as traditional losses,
and iii) it avoids overfitting to measurement noise even without the use of
priors. Its scope naturally fits many practical inverse problems where the
measurement-noise distribution is known and where the measured dataset consists
of many independent noisy values. We demonstrate efficacy in two key example
application areas: i) in image denoising with deep image prior, using DC
instead of MSE loss removes the need for early stopping and achieves higher
PSNR; ii) in medical image reconstruction from Poisson-noisy data, DC loss
reduces artifacts in highly-iterated reconstructions and enhances the efficacy
of hand-crafted regularization. These results position DC loss as a
statistically grounded, performance-enhancing alternative to conventional
fidelity losses for inverse problems.

</details>


### [148] [BitNet Distillation](https://arxiv.org/abs/2510.13998)
*Xun Wu,Shaohan Huang,Wenhui Wang,Ting Song,Li Dong,Yan Xia,Furu Wei*

Main category: cs.LG

TL;DR: BitNet Distillation (BitDistill) 是一种轻量级方法，将全精度大语言模型（如Qwen）微调为1.58位精度（三元权重{-1, 0, 1}），在特定下游任务中实现高性能且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 解决全精度大语言模型在特定任务中计算成本高的问题，同时保持性能。

Method: 结合SubLN模块、多注意力头蒸馏和持续预训练，以缩小全精度与1.58位模型之间的性能差距。

Result: 在模型大小相当的情况下，性能接近全精度模型，内存节省10倍，CPU推理速度提升2.65倍。

Conclusion: BitDistill 是一种高效且轻量化的方法，适用于特定任务的模型优化。

Abstract: In this paper, we present BitNet Distillation (BitDistill), a lightweight
pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into
1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream
tasks, achieving strong task-specific performance with minimal computational
cost. Specifically, BitDistill incorporates three key techniques: the SubLN
module, as introduced in BitNet; multi-head attention distillation, based on
MiniLM; and continual pre-training, which serves as a crucial warm-up step to
mitigate the scalability issue of the performance gap between finetuned
full-precision and 1.58-bit LLMs on specific tasks. Experimental results show
that BitDistill achieves performance comparable to the full-precision
counterpart models across model size, while enabling up to 10x memory savings
and 2.65x faster inference on CPUs. Code is available at
https://github.com/microsoft/BitNet.

</details>


### [149] [REAP the Experts: Why Pruning Prevails for One-Shot MoE compression](https://arxiv.org/abs/2510.13999)
*Mike Lasby,Ivan Lazarevich,Nish Sinnadurai,Sean Lie,Yani Ioannou,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 论文提出了一种名为REAP的专家剪枝方法，证明在生成任务中剪枝优于合并，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 稀疏激活的专家混合模型（SMoE）参数量大，内存开销高，需要研究专家压缩方法。

Method: 提出Router-weighted Expert Activation Pruning (REAP)，结合路由门值和专家激活范数进行剪枝。

Result: 在20B到1T参数的SMoE模型中，REAP在生成任务上优于合并和其他剪枝方法，50%压缩时效果显著。

Conclusion: REAP在代码生成和工具调用任务中实现了近乎无损的压缩，验证了剪枝策略的优越性。

Abstract: Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient
pre-training and low latency but their large parameter counts create
significant memory overhead, motivating research into expert compression.
Contrary to recent findings favouring expert merging on discriminative
benchmarks, we demonstrate that expert pruning is a superior strategy for
generative tasks. We prove that merging introduces an irreducible error by
causing a "functional subspace collapse", due to the loss of the router's
independent, input-dependent control over experts. Leveraging this insight, we
propose Router-weighted Expert Activation Pruning (REAP), a novel pruning
criterion that considers both router gate-values and expert activation norms.
Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP
consistently outperforms merging and other pruning methods on generative
benchmarks, especially at 50% compression. Notably, our method achieves
near-lossless compression on code generation and tool-calling tasks with
Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.

</details>


### [150] [Conditional Clifford-Steerable CNNs with Complete Kernel Basis for PDE Modeling](https://arxiv.org/abs/2510.14007)
*Bálint László Szarvas,Maksim Zhdanov*

Main category: cs.LG

TL;DR: CSCNNs的核基不完备，提出条件Clifford-Steerable核增强表达能力，在PDE预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CSCNNs的核基不完备限制了模型表达能力，需改进。

Method: 提出条件Clifford-Steerable核，通过输入特征场增强核的等变性。

Result: 在流体动力学和相对论电动力学等PDE预测任务中优于基线方法。

Conclusion: 条件Clifford-Steerable核有效提升了模型表达能力，适用于多种任务。

Abstract: Clifford-Steerable CNNs (CSCNNs) provide a unified framework that allows
incorporating equivariance to arbitrary pseudo-Euclidean groups, including
isometries of Euclidean space and Minkowski spacetime. In this work, we
demonstrate that the kernel basis of CSCNNs is not complete, thus limiting the
model expressivity. To address this issue, we propose Conditional
Clifford-Steerable Kernels, which augment the kernels with equivariant
representations computed from the input feature field. We derive the
equivariance constraint for these input-dependent kernels and show how it can
be solved efficiently via implicit parameterization. We empirically demonstrate
an improved expressivity of the resulting framework on multiple PDE forecasting
tasks, including fluid dynamics and relativistic electrodynamics, where our
method consistently outperforms baseline methods.

</details>


### [151] [Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training](https://arxiv.org/abs/2510.14009)
*Jie Hao,Xiaochuan Gong,Jie Xu,Zhengdao Wang,Mingrui Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于几何感知优化算法的噪声自适应分层学习率方案，显著加速了深度神经网络的训练。


<details>
  <summary>Details</summary>
Motivation: 现有几何感知优化算法在训练深度神经网络时，对同一组内的层使用固定学习率，忽略了层间局部曲率的动态变化，导致训练效率低下。

Method: 通过实时估计梯度方差，动态调整每组内各层的学习率，实现噪声自适应分层学习。

Result: 理论分析表明算法具有快速收敛性，实验证明在LLaMA和GPT等架构上优于现有优化器。

Conclusion: 噪声自适应分层学习率方案有效提升了训练效率，适用于复杂神经网络架构。

Abstract: Geometry-aware optimization algorithms, such as Muon, have achieved
remarkable success in training deep neural networks (DNNs). These methods
leverage the underlying geometry of DNNs by selecting appropriate norms for
different layers and updating parameters via norm-constrained linear
minimization oracles (LMOs). However, even within a group of layers associated
with the same norm, the local curvature can be heterogeneous across layers and
vary dynamically over the course of training. For example, recent work shows
that sharpness varies substantially across transformer layers and throughout
training, yet standard geometry-aware optimizers impose fixed learning rates to
layers within the same group, which may be inefficient for DNN training.
  In this paper, we introduce a noise-adaptive layerwise learning rate scheme
on top of geometry-aware optimization algorithms and substantially accelerate
DNN training compared to methods that use fixed learning rates within each
group. Our method estimates gradient variance in the dual norm induced by the
chosen LMO on the fly, and uses it to assign time-varying noise-adaptive
layerwise learning rates within each group. We provide a theoretical analysis
showing that our algorithm achieves a sharp convergence rate. Empirical results
on transformer architectures such as LLaMA and GPT demonstrate that our
approach achieves faster convergence than state-of-the-art optimizers.

</details>


### [152] [Context-Selective State Space Models: Feedback is All You Need](https://arxiv.org/abs/2510.14027)
*Riccardo Zattra,Giacomo Baggio,Umberto Casti,Augusto Ferrante,Francesco Ticozzi*

Main category: cs.LG

TL;DR: COFFEE模型通过状态反馈机制改进了SSM，实现了更高效的长序列依赖捕捉。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理长序列时存在二次复杂度和依赖捕捉困难的问题，而现有的SSM（如S6）仅依赖当前输入，缺乏上下文感知能力。

Method: COFFEE引入时间变化的SSM，通过内部状态反馈实现上下文依赖的选择性，并采用高效参数化减少冗余。

Result: COFFEE在induction head任务上以更少参数和训练数据接近完美准确率；在MNIST上仅用3585参数达到97%准确率。

Conclusion: 状态反馈是构建高效可扩展序列模型的关键机制。

Abstract: Transformers, powered by the attention mechanism, are the backbone of most
foundation models, yet they suffer from quadratic complexity and difficulties
in dealing with long-range dependencies in the input sequence. Recent work has
shown that state space models (SSMs) provide an efficient alternative, with the
S6 module at the core of the Mamba architecture achieving state-of-the-art
results on long-sequence benchmarks. In this paper, we introduce the COFFEE
(COntext From FEEdback) model, a novel time-varying SSM that incorporates state
feedback to enable context-dependent selectivity, while still allowing for
parallel implementation. Whereas the selectivity mechanism of S6 only depends
on the current input, COFFEE computes it from the internal state, which serves
as a compact representation of the sequence history. This shift allows the
model to regulate its dynamics based on accumulated context, improving its
ability to capture long-range dependencies. In addition to state feedback, we
employ an efficient model parametrization that removes redundancies present in
S6 and leads to a more compact and trainable formulation. On the induction head
task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer
parameters and training sequences compared to S6. On MNIST, COFFEE largely
outperforms S6 within the same architecture, reaching 97% accuracy with only
3585 parameters. These results showcase the role of state feedback as a key
mechanism for building scalable and efficient sequence models.

</details>


### [153] [CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations](https://arxiv.org/abs/2510.14049)
*Guangyi Chen,Yunlong Deng,Peiyuan Zhu,Yan Li,Yifan Sheng,Zijian Li,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一个新的因果表示学习（CRL）基准数据集，结合了真实视觉复杂性和可访问的因果生成过程，用于评估CRL方法。


<details>
  <summary>Details</summary>
Motivation: 现有CRL评估方法在真实性和评估精度之间存在矛盾，需要更全面的测试环境。

Method: 使用高保真模拟视觉数据构建数据集，涵盖静态图像生成、动态物理模拟、机器人操作和交通场景分析四个领域。

Result: 数据集包含20万张图像和300万视频帧，支持灵活修改因果结构，评估了多种CRL方法。

Conclusion: 新基准填补了严格评估和实际应用之间的空白，为CRL研究和实践提供了实用工具。

Abstract: Causal Representation Learning (CRL) aims to uncover the data-generating
process and identify the underlying causal variables and relations, whose
evaluation remains inherently challenging due to the requirement of known
ground-truth causal variables and causal structure. Existing evaluations often
rely on either simplistic synthetic datasets or downstream performance on
real-world tasks, generally suffering a dilemma between realism and evaluative
precision. In this paper, we introduce a new benchmark for CRL using
high-fidelity simulated visual data that retains both realistic visual
complexity and, more importantly, access to ground-truth causal generating
processes. The dataset comprises around 200 thousand images and 3 million video
frames across 24 sub-scenes in four domains: static image generation, dynamic
physical simulations, robotic manipulations, and traffic situation analysis.
These scenarios range from static to dynamic settings, simple to complex
structures, and single to multi-agent interactions, offering a comprehensive
testbed that hopefully bridges the gap between rigorous evaluation and
real-world applicability. In addition, we provide flexible access to the
underlying causal structures, allowing users to modify or configure them to
align with the required assumptions in CRL, such as available domain labels,
temporal dependencies, or intervention histories. Leveraging this benchmark, we
evaluated representative CRL methods across diverse paradigms and offered
empirical insights to assist practitioners and newcomers in choosing or
extending appropriate CRL frameworks to properly address specific types of real
problems that can benefit from the CRL perspective. Welcome to visit our:
Project page:https://causal-verse.github.io/,
Dataset:https://huggingface.co/CausalVerse.

</details>


### [154] [FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients](https://arxiv.org/abs/2510.14054)
*Fatih Ilhan,Selim Furkan Tekin,Tiansheng Huang,Gaowen Liu,Ramana Kompella,Greg Eisenhauer,Yingyan Celine Lin,Calton Pu,Ling Liu*

Main category: cs.LG

TL;DR: FedHFT框架通过混合掩码适配器和双层优化，解决了预训练语言模型在异构数据和资源下的个性化联邦微调问题。


<details>
  <summary>Details</summary>
Motivation: 解决个性化NLU应用中数据隐私和资源异构性带来的挑战。

Method: 引入混合掩码适配器处理资源异构性，采用双层优化处理非独立同分布数据。

Result: 在多种NLU任务中表现优于现有异构联邦学习方法。

Conclusion: FedHFT高效且个性化，适用于分布式环境下的语言模型微调。

Abstract: Fine-tuning pre-trained large language models (LLMs) has become a common
practice for personalized natural language understanding (NLU) applications on
downstream tasks and domain-specific datasets. However, there are two main
challenges: (i) limited and/or heterogeneous data for fine-tuning due to
proprietary data confidentiality or privacy requirements, and (ii) varying
computation resources available across participating clients such as edge
devices. This paper presents FedHFT - an efficient and personalized federated
fine-tuning framework to address both challenges. First, we introduce a mixture
of masked adapters to handle resource heterogeneity across participating
clients, enabling high-performance collaborative fine-tuning of pre-trained
language model(s) across multiple clients in a distributed setting, while
keeping proprietary data local. Second, we introduce a bi-level optimization
approach to handle non-iid data distribution based on masked personalization
and client clustering. Extensive experiments demonstrate significant
performance and efficiency improvements over various natural language
understanding tasks under data and resource heterogeneity compared to
representative heterogeneous federated learning methods.

</details>


### [155] [On the expressivity of sparse maxout networks](https://arxiv.org/abs/2510.14068)
*Moritz Grillo,Tobias Hofmann*

Main category: cs.LG

TL;DR: 研究了稀疏maxout网络的表达能力，揭示了其与虚拟多面体的对偶关系，并构建了深度层次结构。


<details>
  <summary>Details</summary>
Motivation: 探索稀疏maxout网络（类似卷积或图神经网络）的表达能力，并理解其几何特性。

Method: 通过虚拟多面体的几何分析，推导出相关多面体维度的紧界，并构建深度层次结构。

Result: 证明了足够深的稀疏maxout网络是通用的，但若深度不足，宽度无法弥补固定入度的稀疏性限制。

Conclusion: 稀疏maxout网络的表达能力与深度密切相关，几何分析为理解其特性提供了新视角。

Abstract: We study the expressivity of sparse maxout networks, where each neuron takes
a fixed number of inputs from the previous layer and employs a, possibly
multi-argument, maxout activation. This setting captures key characteristics of
convolutional or graph neural networks. We establish a duality between
functions computable by such networks and a class of virtual polytopes, linking
their geometry to questions of network expressivity. In particular, we derive a
tight bound on the dimension of the associated polytopes, which serves as the
central tool for our analysis. Building on this, we construct a sequence of
depth hierarchies. While sufficiently deep sparse maxout networks are
universal, we prove that if the required depth is not reached, width alone
cannot compensate for the sparsity of a fixed indegree constraint.

</details>


### [156] [Exploratory Causal Inference in SAEnce](https://arxiv.org/abs/2510.14073)
*Tommaso Mencattini,Riccardo Cadei,Francesco Locatello*

Main category: cs.LG

TL;DR: 提出了一种名为Neural Effect Search的新方法，通过预训练基础模型和稀疏自编码器从试验数据中发现未知的因果效应。


<details>
  <summary>Details</summary>
Motivation: 传统随机对照试验依赖手工假设和昂贵分析，限制了大规模因果效应估计。

Method: 使用预训练基础模型将非结构化数据转化为有意义表示，并通过稀疏自编码器解释，提出Neural Effect Search递归方法解决多重检验和效应纠缠问题。

Result: 在半合成实验中验证了算法的鲁棒性，并在实验生态学中首次实现了真实世界科学试验的无监督因果效应识别。

Conclusion: Neural Effect Search为大规模因果效应发现提供了新途径。

Abstract: Randomized Controlled Trials are one of the pillars of science; nevertheless,
they rely on hand-crafted hypotheses and expensive analysis. Such constraints
prevent causal effect estimation at scale, potentially anchoring on popular yet
incomplete hypotheses. We propose to discover the unknown effects of a
treatment directly from data. For this, we turn unstructured data from a trial
into meaningful representations via pretrained foundation models and interpret
them via a sparse autoencoder. However, discovering significant causal effects
at the neural level is not trivial due to multiple-testing issues and effects
entanglement. To address these challenges, we introduce Neural Effect Search, a
novel recursive procedure solving both issues by progressive stratification.
After assessing the robustness of our algorithm on semi-synthetic experiments,
we showcase, in the context of experimental ecology, the first successful
unsupervised causal effect identification on a real-world scientific trial.

</details>


### [157] [Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations](https://arxiv.org/abs/2510.14094)
*Haotian Feng*

Main category: cs.LG

TL;DR: 本文探讨了神经网络在近似反应-扩散方程解中的理论能力，填补了相关理论基础的空白。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在求解微分方程方面应用广泛，但其为何能有效近似解的理论基础尚不充分。

Method: 基于通用近似定理，分析了一维和二维反应-扩散方程在均匀和非均匀介质中的神经网络近似能力。

Result: 证明了两层神经网络可近似一维方程，三层网络可近似二维方程，且框架可扩展至椭圆和抛物方程。

Conclusion: 为基于神经网络的微分方程求解器提供了理论基础，展示了神经网络在近似PDE解中的强大表达能力。

Abstract: Reaction-diffusion systems represent one of the most fundamental formulations
used to describe a wide range of physical, chemical, and biological processes.
With the increasing adoption of neural networks, recent research has focused on
solving differential equations using machine learning techniques. However, the
theoretical foundation explaining why neural networks can effectively
approximate such solutions remains insufficiently explored.
  This paper provides a theoretical analysis of the approximation power of
neural networks for one- and two-dimensional reaction-diffusion equations in
both homogeneous and heterogeneous media. Building upon the universal
approximation theorem, we demonstrate that a two-layer neural network can
approximate the one-dimensional reaction-diffusion equation, while a
three-layer neural network can approximate its two-dimensional counterpart. The
theoretical framework presented here can be further extended to elliptic and
parabolic equations.
  Overall, this work highlights the expressive power of neural networks in
approximating solutions to reaction-diffusion equations and related PDEs,
providing a theoretical foundation for neural network-based differential
equation solvers.

</details>


### [158] [Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning](https://arxiv.org/abs/2510.14095)
*Awni Altabaa,Siyu Chen,John Lafferty,Zhuoran Yang*

Main category: cs.LG

TL;DR: 论文研究了Transformer网络在分布外（OOD）泛化问题，提出四种架构机制以增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中系统性、组合性泛化问题，提升语言模型的推理能力。

Method: 引入四种机制：输入自适应循环、算法监督、离散瓶颈的锚定潜在表示和显式纠错机制。

Result: 这些机制显著提升了Transformer网络的算法泛化能力。

Conclusion: 通过机制分析和实验验证，展示了这些方法在OOD泛化中的有效性。

Abstract: Systematic, compositional generalization beyond the training distribution
remains a core challenge in machine learning -- and a critical bottleneck for
the emergent reasoning abilities of modern language models. This work
investigates out-of-distribution (OOD) generalization in Transformer networks
using a GSM8K-style modular arithmetic on computational graphs task as a
testbed. We introduce and explore a set of four architectural mechanisms aimed
at enhancing OOD generalization: (i) input-adaptive recurrence; (ii)
algorithmic supervision; (iii) anchored latent representations via a discrete
bottleneck; and (iv) an explicit error-correction mechanism. Collectively,
these mechanisms yield an architectural approach for native and scalable latent
space reasoning in Transformer networks with robust algorithmic generalization
capabilities. We complement these empirical results with a detailed mechanistic
interpretability analysis that reveals how these mechanisms give rise to robust
OOD generalization abilities.

</details>


### [159] [TENDE: Transfer Entropy Neural Diffusion Estimation](https://arxiv.org/abs/2510.14096)
*Simon Pedro Galeano Munoz,Mustapha Bounoua,Giulio Franzese,Pietro Michiardi,Maurizio Filippone*

Main category: cs.LG

TL;DR: TENDE（Transfer Entropy Neural Diffusion Estimation）是一种利用基于分数的扩散模型估计转移熵的新方法，解决了现有方法在高维、分布假设和数据需求上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有转移熵估计方法存在维度灾难、分布假设严格或数据需求大的问题，限制了其应用。

Method: 通过基于分数的扩散模型学习相关条件分布的函数，估计条件互信息来计算转移熵。

Result: TENDE在合成基准和真实数据上表现出比现有神经估计器和其他先进方法更高的准确性和鲁棒性。

Conclusion: TENDE提供了一种灵活、可扩展的转移熵估计方法，对数据生成过程的假设极少。

Abstract: Transfer entropy measures directed information flow in time series, and it
has become a fundamental quantity in applications spanning neuroscience,
finance, and complex systems analysis. However, existing estimation methods
suffer from the curse of dimensionality, require restrictive distributional
assumptions, or need exponentially large datasets for reliable convergence. We
address these limitations in the literature by proposing TENDE (Transfer
Entropy Neural Diffusion Estimation), a novel approach that leverages
score-based diffusion models to estimate transfer entropy through conditional
mutual information. By learning score functions of the relevant conditional
distributions, TENDE provides flexible, scalable estimation while making
minimal assumptions about the underlying data-generating process. We
demonstrate superior accuracy and robustness compared to existing neural
estimators and other state-of-the-art approaches across synthetic benchmarks
and real data.

</details>


### [160] [Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets](https://arxiv.org/abs/2510.14097)
*Zixian Yang,Sushil Mahavir Varma,Lei Ying*

Main category: cs.LG

TL;DR: 论文研究了一个双边市场中的定价与匹配算法，通过在线学习策略实现平台利润最大化，同时控制队列长度。


<details>
  <summary>Details</summary>
Motivation: 解决实践中需求与供应曲线未知时，如何设计高效的定价与匹配策略以优化平台利润和队列管理的问题。

Method: 提出了一种结合动态优化和概率组件的在线学习定价策略，平衡学习速度与队列控制。

Result: 证明了策略在遗憾、平均队列长度和最大队列长度之间的最优权衡，显著优于现有结果。

Conclusion: 该策略在未知需求与供应曲线的情况下，实现了接近最优的性能，并解决了学习与队列控制的矛盾。

Abstract: We study a two-sided market, wherein, price-sensitive heterogeneous customers
and servers arrive and join their respective queues. A compatible
customer-server pair can then be matched by the platform, at which point, they
leave the system. Our objective is to design pricing and matching algorithms
that maximize the platform's profit, while maintaining reasonable queue
lengths. As the demand and supply curves governing the price-dependent arrival
rates may not be known in practice, we design a novel online-learning-based
pricing policy and establish its near-optimality. In particular, we prove a
tradeoff among three performance metrics: $\tilde{O}(T^{1-\gamma})$ regret,
$\tilde{O}(T^{\gamma/2})$ average queue length, and $\tilde{O}(T^{\gamma})$
maximum queue length for $\gamma \in (0, 1/6]$, significantly improving over
existing results [1]. Moreover, barring the permissible range of $\gamma$, we
show that this trade-off between regret and average queue length is optimal up
to logarithmic factors under a class of policies, matching the optimal one as
in [2] which assumes the demand and supply curves to be known. Our proposed
policy has two noteworthy features: a dynamic component that optimizes the
tradeoff between low regret and small queue lengths; and a probabilistic
component that resolves the tension between obtaining useful samples for fast
learning and maintaining small queue lengths.

</details>


### [161] [Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey](https://arxiv.org/abs/2510.14114)
*Yazid Janati,Alain Durmus,Jimmy Olsson,Eric Moulines*

Main category: cs.LG

TL;DR: 综述探讨了如何利用预训练扩散模型和蒙特卡洛方法解决贝叶斯逆问题，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现出色，但尚未充分探索其在贝叶斯逆问题中的应用潜力。

Method: 通过扩散过程中的“扭曲”机制引导模拟朝向后验分布，并利用蒙特卡洛方法辅助采样。

Result: 展示了预训练扩散模型与蒙特卡洛方法结合的有效性。

Conclusion: 该方法为贝叶斯逆问题提供了一种无需额外训练的高效解决方案。

Abstract: Diffusion models enable the synthesis of highly accurate samples from complex
distributions and have become foundational in generative modeling. Recently,
they have demonstrated significant potential for solving Bayesian inverse
problems by serving as priors. This review offers a comprehensive overview of
current methods that leverage \emph{pre-trained} diffusion models alongside
Monte Carlo methods to address Bayesian inverse problems without requiring
additional training. We show that these methods primarily employ a
\emph{twisting} mechanism for the intermediate distributions within the
diffusion process, guiding the simulations toward the posterior distribution.
We describe how various Monte Carlo methods are then used to aid in sampling
from these twisted distributions.

</details>


### [162] [Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants](https://arxiv.org/abs/2510.14125)
*Waqar Muhammad Ashraf,Talha Ansar,Abdulelah S. Alshehri,Peipei Chen,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 提出了一种神经网络驱动的鲁棒优化框架，解决参数化神经网络模型与优化求解器交互时产生的领域不一致问题。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络模型与优化求解器交互时产生的领域不一致问题，以提高能源效率。

Method: 将数据驱动领域作为约束集成到非线性规划技术中。

Result: 在1180 MW联合循环燃气电厂中，实现了0.76%的平均能效提升，全球范围内估计每年可减少26 Mt CO₂排放。

Conclusion: 机器学习在提供近期的、可扩展的全球气候行动脱碳路径中发挥了协同作用。

Abstract: We introduce a neural network-driven robust optimisation framework that
integrates data-driven domain as a constraint into the nonlinear programming
technique, addressing the overlooked issue of domain-inconsistent solutions
arising from the interaction of parametrised neural network models with
optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power
plant, our framework delivers domain-consistent robust optimal solutions that
achieve a verified 0.76 percentage point mean improvement in energy efficiency.
For the first time, scaling this efficiency gain to the global fleet of gas
power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with
10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results
underscore the synergetic role of machine learning in delivering near-term,
scalable decarbonisation pathways for global climate action.

</details>


### [163] [Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL](https://arxiv.org/abs/2510.14129)
*Mahsa Bastankhah,Grace Liu,Dilip Arumugam,Thomas L. Griffiths,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: SGCRL是一种无监督强化学习算法，通过隐式奖励和低秩状态表示驱动探索，并能适应安全感知探索。


<details>
  <summary>Details</summary>
Motivation: 研究无监督强化学习中探索行为的机制，特别是SGCRL算法如何在没有外部奖励或课程的情况下解决长时程目标达成任务。

Method: 结合理论分析和控制实验，研究SGCRL的目标函数及其隐式奖励机制，以及低秩状态表示对探索的影响。

Result: SGCRL通过学习的表示自动调整奖励景观，促进探索和利用的平衡，且探索行为源于低秩状态表示而非神经网络近似。

Conclusion: 对SGCRL机制的深入理解使其能够适应安全感知探索，为无监督强化学习提供了新的研究方向。

Abstract: In this work, we take a first step toward elucidating the mechanisms behind
emergent exploration in unsupervised reinforcement learning. We study
Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised
algorithm capable of solving challenging long-horizon goal-reaching tasks
without external rewards or curricula. We combine theoretical analysis of the
algorithm's objective function with controlled experiments to understand what
drives its exploration. We show that SGCRL maximizes implicit rewards shaped by
its learned representations. These representations automatically modify the
reward landscape to promote exploration before reaching the goal and
exploitation thereafter. Our experiments also demonstrate that these
exploration dynamics arise from learning low-rank representations of the state
space rather than from neural network function approximation. Our improved
understanding enables us to adapt SGCRL to perform safety-aware exploration.

</details>


### [164] [Learning Wireless Interference Patterns: Decoupled GNN for Throughput Prediction in Heterogeneous Multi-Hop p-CSMA Networks](https://arxiv.org/abs/2510.14137)
*Faezeh Dehghan Tarzjani,Bhaskar Krishnamachari*

Main category: cs.LG

TL;DR: 论文提出了一种名为D-GCN的新架构，用于解决异构多跳无线网络中预测饱和吞吐量的难题，通过解耦节点自身传输概率与邻居干扰效应，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在异构多跳无线网络中预测饱和吞吐量时存在精度不足或计算复杂度高的问题，需要一种既能保持高精度又能高效计算的新方法。

Method: 提出D-GCN架构，通过解耦节点自身传输概率与邻居干扰效应，并采用可学习的注意力机制替代均值聚合，以捕捉复杂的多跳干扰模式。

Result: D-GCN在异构网络中实现了3.3%的NMAE，优于基线模型，并在计算复杂度上保持高效。

Conclusion: D-GCN不仅显著提升了吞吐量预测的精度，还为基于梯度的网络优化提供了可能，接近理论最优值。

Abstract: The p-persistent CSMA protocol is central to random-access MAC analysis, but
predicting saturation throughput in heterogeneous multi-hop wireless networks
remains a hard problem. Simplified models that assume a single, shared
interference domain can underestimate throughput by 48--62\% in sparse
topologies. Exact Markov-chain analyses are accurate but scale exponentially in
computation time, making them impractical for large networks. These
computational barriers motivate structural machine learning approaches like
GNNs for scalable throughput prediction in general network topologies. Yet
off-the-shelf GNNs struggle here: a standard GCN yields 63.94\% normalized mean
absolute error (NMAE) on heterogeneous networks because symmetric normalization
conflates a node's direct interference with higher-order, cascading effects
that pertain to how interference propagates over the network graph.
  Building on these insights, we propose the Decoupled Graph Convolutional
Network (D-GCN), a novel architecture that explicitly separates processing of a
node's own transmission probability from neighbor interference effects. D-GCN
replaces mean aggregation with learnable attention, yielding interpretable,
per-neighbor contribution weights while capturing complex multihop interference
patterns. D-GCN attains 3.3\% NMAE, outperforms strong baselines, remains
tractable even when exact analytical methods become computationally infeasible,
and enables gradient-based network optimization that achieves within 1\% of
theoretical optima.

</details>


### [165] [Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks](https://arxiv.org/abs/2510.14139)
*Islam Akef Ebeid,Haoteng Tang,Pengfei Gu*

Main category: cs.LG

TL;DR: 提出了一种新的蛋白质-蛋白质相互作用预测框架ProtGram-DirectGCN，结合了层次化n-gram图和定向图卷积网络。


<details>
  <summary>Details</summary>
Motivation: 现有方法计算成本高，需要探索更高效的替代方案。

Method: 两阶段框架：ProtGram建模蛋白质一级结构为n-gram图，DirectGCN处理定向图并生成嵌入。

Result: 在标准节点分类任务中表现优异，PPI预测中即使训练数据有限也能保持强预测能力。

Conclusion: ProtGram-DirectGCN为PPI预测提供了高效且强大的解决方案。

Abstract: Introduction Accurate prediction of protein-protein interactions (PPIs) is
crucial for understanding cellular functions and advancing drug development.
Existing in-silico methods use direct sequence embeddings from Protein Language
Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein
structures. This study explores less computationally intensive alternatives. We
introduce a novel framework for downstream PPI prediction through link
prediction. Methods We introduce a two-stage graph representation learning
framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach
models a protein's primary structure as a hierarchy of globally inferred n-gram
graphs. In these graphs, residue transition probabilities define edge weights.
Each edge connects a pair of residues in a directed graph. The probabilities
are aggregated from a large corpus of sequences. Second, we propose DirectGCN,
a custom directed graph convolutional neural network. This model features a
unique convolutional layer. It processes information through separate
path-specific transformations: incoming, outgoing, and undirected. A shared
transformation is also applied. These paths are combined via a learnable gating
mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level
embeddings. These embeddings are pooled via attention to generate protein-level
embeddings for prediction. Results We first established the efficacy of
DirectGCN on standard node classification benchmarks. Its performance matches
established methods on general datasets. The model excels at complex, directed
graphs with dense, heterophilic structures. When applied to PPI prediction, the
full ProtGram-DirectGCN framework delivers robust predictive power. This strong
performance holds even with limited training data.

</details>


### [166] [On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model](https://arxiv.org/abs/2510.14156)
*Jan Kwiatkowski,Jarosław A. Chudziak*

Main category: cs.LG

TL;DR: 论文系统评估了多种高级损失函数对Transformer模型在股票排名中的影响，为基于排名的投资策略提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 金融市场的复杂性和变化性使得标准损失函数难以直接指导模型学习正确的股票回报顺序，需要更有效的排名损失函数。

Method: 使用点对、成对和列表式损失函数，结合Transformer模型，在S&P 500数据上进行股票回报预测和排名评估。

Result: 研究提供了全面的基准，揭示了不同损失函数对模型学习横截面和时间模式的影响。

Conclusion: 论文为优化基于排名的交易策略提供了实用指导，填补了现有研究的空白。

Abstract: Quantitative trading strategies rely on accurately ranking stocks to identify
profitable investments. Effective portfolio management requires models that can
reliably order future stock returns. Transformer models are promising for
understanding financial time series, but how different training loss functions
affect their ability to rank stocks well is not yet fully understood. Financial
markets are challenging due to their changing nature and complex relationships
between stocks. Standard loss functions, which aim for simple prediction
accuracy, often aren't enough. They don't directly teach models to learn the
correct order of stock returns. While many advanced ranking losses exist from
fields such as information retrieval, there hasn't been a thorough comparison
to see how well they work for ranking financial returns, especially when used
with modern Transformer models for stock selection. This paper addresses this
gap by systematically evaluating a diverse set of advanced loss functions
including pointwise, pairwise, listwise for daily stock return forecasting to
facilitate rank-based portfolio selection on S&P 500 data. We focus on
assessing how each loss function influences the model's ability to discern
profitable relative orderings among assets. Our research contributes a
comprehensive benchmark revealing how different loss functions impact a model's
ability to learn cross-sectional and temporal patterns crucial for portfolio
selection, thereby offering practical guidance for optimizing ranking-based
trading strategies.

</details>


### [167] [Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods](https://arxiv.org/abs/2510.14161)
*Matthew D. Merris,Tim Andersen*

Main category: cs.LG

TL;DR: 论文探讨了传统数据集表征方法的局限性，并提出了基于张量的方法作为更优替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法在深度理解和可解释性方面不足，阻碍了数据驱动的创新。

Method: 通过调查现有技术和分析其局限性，提出并讨论了张量方法的应用。

Result: 张量方法能揭示更细致的数据特征，提升可解释性和实用性。

Conclusion: 建议采用张量方法，以推动复杂数据集的理解和可解释性数据发现。

Abstract: In the evolving domains of Machine Learning and Data Analytics, existing
dataset characterization methods such as statistical, structural, and
model-based analyses often fail to deliver the deep understanding and insights
essential for innovation and explainability. This work surveys the current
state-of-the-art conventional data analytic techniques and examines their
limitations, and discusses a variety of tensor-based methods and how these may
provide a more robust alternative to traditional statistical, structural, and
model-based dataset characterization techniques. Through examples, we
illustrate how tensor methods unveil nuanced data characteristics, offering
enhanced interpretability and actionable intelligence. We advocate for the
adoption of tensor-based characterization, promising a leap forward in
understanding complex datasets and paving the way for intelligent, explainable
data-driven discoveries.

</details>


### [168] [Towards Reversible Model Merging For Low-rank Weights](https://arxiv.org/abs/2510.14163)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.LG

TL;DR: 论文提出了一种名为可逆模型合并（RMM）的新方法，用于在低秩压缩模型合并时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统合并方法在低秩权重下性能下降严重，因此需要一种新方法来解决这一问题。

Method: 通过构建一个紧凑的基，使得原始任务特定模型可以通过线性组合恢复，而不是直接合并为一个模型。

Result: RMM在多种数据集和模型规模上显著优于现有合并方法，保持了低秩压缩模型的性能。

Conclusion: RMM提供了一种高效、无需数据且灵活的方法，解决了低秩模型合并的性能问题。

Abstract: Model merging aims to combine multiple fine-tuned models into a single set of
weights that performs well across all source tasks. While prior work has shown
that merging can approximate the performance of individual fine-tuned models
for each task, it largely overlooks scenarios where models are compressed into
low-rank representations, either through low-rank adaptation (LoRA) or
post-training singular value decomposition (SVD). We first demonstrate that
applying conventional merging methods to low-rank weights leads to severe
performance degradation in the merged model. Motivated by this phenomenon, we
propose a fundamentally different approach: instead of collapsing all adapters
into one set of weights, we construct a compact basis (e.g., an equivalent of
holding two or more models) from which original task-specific models can be
recovered via linear combination. This reframes merging as generating a
reconstruction-capable model space rather than producing a single merged model.
Crucially, this allows us to ``revert'' to each individual model when needed,
recognizing that no merged model can consistently outperform one specialized
for its task. Building on this insight, we introduce our method, Reversible
Model Merging (RMM), an efficient, data-free, and flexible method that provides
a closed-form solution for selecting the optimal basis of model weights and
task-specific coefficients for linear combination. Extensive experiments across
diverse datasets and model scales demonstrate that RMM consistently outperforms
existing merging approaches, preserving the performance of low-rank compressed
models by a significant margin.

</details>


### [169] [Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming](https://arxiv.org/abs/2510.14168)
*Guan-Horng Liu,Tianrong Chen,Evangelos A. Theodorou*

Main category: cs.LG

TL;DR: 论文提出了一种基于最优控制理论的新优化方法OCNOpt，通过将深度神经网络（DNNs）视为动态系统，利用动态规划与反向传播的相似性，开发了更高阶的训练方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的优化问题可以视为最优控制问题，反向传播算法与动态规划的最优条件有显著相似性，这为开发新的优化方法提供了理论基础。

Method: 通过将反向传播与动态规划结合，提出OCNOpt方法，利用贝尔曼方程的高阶展开，实现层间反馈策略和连续时间模型的高阶训练。

Result: 实验表明，OCNOpt在鲁棒性和效率上优于现有方法，同时保持可管理的计算复杂度。

Conclusion: OCNOpt为基于动态系统和最优控制理论的算法设计开辟了新途径，具有广泛的应用潜力。

Abstract: Optimization of deep neural networks (DNNs) has been a driving force in the
advancement of modern machine learning and artificial intelligence. With DNNs
characterized by a prolonged sequence of nonlinear propagation, determining
their optimal parameters given an objective naturally fits within the framework
of Optimal Control Programming. Such an interpretation of DNNs as dynamical
systems has proven crucial in offering a theoretical foundation for principled
analysis from numerical equations to physics. In parallel to these theoretical
pursuits, this paper focuses on an algorithmic perspective. Our motivated
observation is the striking algorithmic resemblance between the Backpropagation
algorithm for computing gradients in DNNs and the optimality conditions for
dynamical systems, expressed through another backward process known as dynamic
programming. Consolidating this connection, where Backpropagation admits a
variational structure, solving an approximate dynamic programming up to the
first-order expansion leads to a new class of optimization methods exploring
higher-order expansions of the Bellman equation. The resulting optimizer,
termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich
algorithmic opportunities, including layer-wise feedback policies,
game-theoretic applications, and higher-order training of continuous-time
models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt
improves upon existing methods in robustness and efficiency while maintaining
manageable computational complexity, paving new avenues for principled
algorithmic design grounded in dynamical systems and optimal control theory.

</details>


### [170] [MAFA: A Multi-Agent Framework for Enterprise-Scale Annotation with Configurable Task Adaptation](https://arxiv.org/abs/2510.14184)
*Mahmood Hegazy,Aaron Rodrigues,Azzam Naeem*

Main category: cs.LG

TL;DR: MAFA是一个多智能体标注框架，通过配置化多智能体协作解决金融领域标注积压问题，显著提升效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 解决金融服务中大规模客户话语标注积压问题，提升标注效率和准确性。

Method: 结合专用智能体、结构化推理和基于裁判的共识机制，支持动态任务适配。

Result: 在摩根大通部署后，消除100万条标注积压，平均86%与人工标注一致，每年节省5000小时。

Conclusion: MAFA填补了多智能体系统理论与实际企业部署的差距，为类似标注挑战提供解决方案。

Abstract: We present MAFA (Multi-Agent Framework for Annotation), a production-deployed
system that transforms enterprise-scale annotation workflows through
configurable multi-agent collaboration. Addressing the critical challenge of
annotation backlogs in financial services, where millions of customer
utterances require accurate categorization, MAFA combines specialized agents
with structured reasoning and a judge-based consensus mechanism. Our framework
uniquely supports dynamic task adaptation, allowing organizations to define
custom annotation types (FAQs, intents, entities, or domain-specific
categories) through configuration rather than code changes. Deployed at JP
Morgan Chase, MAFA has eliminated a 1 million utterance backlog while
achieving, on average, 86% agreement with human annotators, annually saving
over 5,000 hours of manual annotation work. The system processes utterances
with annotation confidence classifications, which are typically 85% high, 10%
medium, and 5% low across all datasets we tested. This enables human annotators
to focus exclusively on ambiguous and low-coverage cases. We demonstrate MAFA's
effectiveness across multiple datasets and languages, showing consistent
improvements over traditional and single-agent annotation baselines: 13.8%
higher Top-1 accuracy, 15.1% improvement in Top-5 accuracy, and 16.9% better F1
in our internal intent classification dataset and similar gains on public
benchmarks. This work bridges the gap between theoretical multi-agent systems
and practical enterprise deployment, providing a blueprint for organizations
facing similar annotation challenges.

</details>


### [171] [Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation](https://arxiv.org/abs/2510.14190)
*Ruchi Sandilya,Sumaira Perez,Charles Lynch,Lindsay Victoria,Benjamin Zebley,Derrick Matthew Buchanan,Mahendra T. Bhati,Nolan Williams,Timothy J. Spellman,Faith M. Gunning,Conor Liston,Logan Grosenick*

Main category: cs.LG

TL;DR: ConDA通过对比学习在扩散嵌入中对齐潜在几何与系统动态，提升扩散模型的可解释控制。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的潜在空间缺乏显式组织以实现可解释控制，ConDA旨在解决这一问题。

Method: 应用对比学习于扩散嵌入，对齐潜在几何与动态因素，支持非线性轨迹遍历。

Result: 在流体动力学、神经钙成像等领域，ConDA生成的可解释潜在表示优于基线方法。

Conclusion: 扩散潜在空间编码动态相关结构，但需通过潜在组织和遍历来利用。

Abstract: Diffusion models excel at generation, but their latent spaces are not
explicitly organized for interpretable control. We introduce ConDA (Contrastive
Diffusion Alignment), a framework that applies contrastive learning within
diffusion embeddings to align latent geometry with system dynamics. Motivated
by recent advances showing that contrastive objectives can recover more
disentangled and structured representations, ConDA organizes diffusion latents
such that traversal directions reflect underlying dynamical factors. Within
this contrastively structured space, ConDA enables nonlinear trajectory
traversal that supports faithful interpolation, extrapolation, and controllable
generation. Across benchmarks in fluid dynamics, neural calcium imaging,
therapeutic neurostimulation, and facial expression, ConDA produces
interpretable latent representations with improved controllability compared to
linear traversals and conditioning-based baselines. These results suggest that
diffusion latents encode dynamics-relevant structure, but exploiting this
structure requires latent organization and traversal along the latent manifold.

</details>


### [172] [Incentive-Based Federated Learning](https://arxiv.org/abs/2510.14208)
*Chanuka A. S. Hewa Kaluannakkage,Rajkumar Buyya*

Main category: cs.LG

TL;DR: 联邦学习通过保护数据隐私实现协作模型训练，但参与困境限制了其实际应用。本章探讨了激励机制的设计挑战，结合经济学和博弈论，提出了集中式和去中心化架构的分类，并展示了激励机制对联邦学习成功的关键作用。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的参与实体可能因缺乏激励而不愿贡献或搭便车，这限制了其实际适应性。

Method: 结合经济学、博弈论、区块链和深度强化学习，提出集中式和去中心化架构的分类。

Result: 展示了激励机制在联邦学习中的关键作用，并总结了现有解决方案和未解决的挑战。

Conclusion: 设计良好的激励机制是联邦学习实际成功的必要条件，但仍需解决可持续性、公平性和鲁棒性等挑战。

Abstract: Federated learning promises to revolutionize machine learning by enabling
collaborative model training without compromising data privacy. However,
practical adaptability can be limited by critical factors, such as the
participation dilemma. Participating entities are often unwilling to contribute
to a learning system unless they receive some benefits, or they may pretend to
participate and free-ride on others. This chapter identifies the fundamental
challenges in designing incentive mechanisms for federated learning systems. It
examines how foundational concepts from economics and game theory can be
applied to federated learning, alongside technology-driven solutions such as
blockchain and deep reinforcement learning. This work presents a comprehensive
taxonomy that thoroughly covers both centralized and decentralized
architectures based on the aforementioned theoretical concepts. Furthermore,
the concepts described are presented from an application perspective, covering
emerging industrial applications, including healthcare, smart infrastructure,
vehicular networks, and blockchain-based decentralized systems. Through this
exploration, this chapter demonstrates that well-designed incentive mechanisms
are not merely optional features but essential components for the practical
success of federated learning. This analysis reveals both the promising
solutions that have emerged and the significant challenges that remain in
building truly sustainable, fair, and robust federated learning ecosystems.

</details>


### [173] [Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization](https://arxiv.org/abs/2510.14217)
*Asma Jamali,Tin Sum Cheng,Rodrigo A. Vargas-Hernández*

Main category: cs.LG

TL;DR: 论文对分子核的光谱特性进行了首次全面分析，发现光谱丰富性并不总是提高预测准确性，甚至在某些情况下与性能呈负相关。


<details>
  <summary>Details</summary>
Motivation: 理解核的光谱特性可以为泛化和表示质量提供理论依据，但目前对分子核的系统光谱分析较少。

Method: 在QM9数据集上，使用核岭回归对多种分子表示（包括分子指纹、预训练Transformer、全局和局部3D表示）进行光谱分析，并采用四种光谱指标和截断核技术。

Result: 光谱丰富性并不一致提高准确性，某些情况下与性能负相关；截断核显示前2%的特征值即可恢复大部分性能。

Conclusion: 研究挑战了“更丰富的光谱带来更好泛化”的常见启发式，揭示了表示、核特征与预测性能之间的复杂关系。

Abstract: Understanding the spectral properties of kernels offers a principled
perspective on generalization and representation quality. While deep models
achieve state-of-the-art accuracy in molecular property prediction, kernel
methods remain widely used for their robustness in low-data regimes and
transparent theoretical grounding. Despite extensive studies of kernel spectra
in machine learning, systematic spectral analyses of molecular kernels are
scarce. In this work, we provide the first comprehensive spectral analysis of
kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained
transformer-based, global and local 3D representations across seven molecular
properties. Surprisingly, richer spectral features, measured by four different
spectral metrics, do not consistently improve accuracy. Pearson correlation
tests further reveal that for transformer-based and local 3D representations,
spectral richness can even have a negative correlation with performance. We
also implement truncated kernels to probe the relationship between spectrum and
predictive performance: in many kernels, retaining only the top 2% of
eigenvalues recovers nearly all performance, indicating that the leading
eigenvalues capture the most informative features. Our results challenge the
common heuristic that "richer spectra yield better generalization" and
highlight nuanced relationships between representation, kernel features, and
predictive performance. Beyond molecular property prediction, these findings
inform how kernel and self-supervised learning methods are evaluated in
data-limited scientific and real-world tasks.

</details>


### [174] [When Flatness Does (Not) Guarantee Adversarial Robustness](https://arxiv.org/abs/2510.14231)
*Nils Philipp Walter,Linara Adilova,Jilles Vreeken,Michael Kamp*

Main category: cs.LG

TL;DR: 论文探讨了平坦极小值与对抗鲁棒性的关系，发现平坦性仅提供局部而非全局鲁棒性，并揭示了对抗样本常位于模型自信但错误的平坦区域。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络对抗扰动的脆弱性，验证平坦极小值是否真正提升鲁棒性。

Method: 推导了倒数第二层相对平坦度的闭式表达式，并分析损失函数在输入空间的变化，从而形式化分析网络的对抗鲁棒性。

Result: 平坦性仅提供局部鲁棒性，全局鲁棒性需要损失函数在数据流形外急剧弯曲。对抗样本常位于模型自信但错误的平坦区域。

Conclusion: 平坦性与鲁棒性的关系比直觉更复杂，挑战了简化观点，提供了更细致的理解。

Abstract: Despite their empirical success, neural networks remain vulnerable to small,
adversarial perturbations. A longstanding hypothesis suggests that flat minima,
regions of low curvature in the loss landscape, offer increased robustness.
While intuitive, this connection has remained largely informal and incomplete.
By rigorously formalizing the relationship, we show this intuition is only
partially correct: flatness implies local but not global adversarial
robustness. To arrive at this result, we first derive a closed-form expression
for relative flatness in the penultimate layer, and then show we can use this
to constrain the variation of the loss in input space. This allows us to
formally analyze the adversarial robustness of the entire network. We then show
that to maintain robustness beyond a local neighborhood, the loss needs to
curve sharply away from the data manifold. We validate our theoretical
predictions empirically across architectures and datasets, uncovering the
geometric structure that governs adversarial vulnerability, and linking
flatness to model confidence: adversarial examples often lie in large, flat
regions where the model is confidently wrong. Our results challenge simplified
views of flatness and provide a nuanced understanding of its role in
robustness.

</details>


### [175] [Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models](https://arxiv.org/abs/2510.14232)
*Mehrzad Samadi,Aleksander Ficek,Sean Narenthiran,Siddhartha Jain,Wasi Uddin Ahmad,Somshubra Majumdar,Vahid Noroozi,Boris Ginsburg*

Main category: cs.LG

TL;DR: GenCluster框架通过大规模生成、行为聚类和排名策略，首次在开源模型上实现IOI金牌水平性能。


<details>
  <summary>Details</summary>
Motivation: 评估开源模型在竞争性编程中的表现，填补与闭源模型的性能差距。

Method: 结合大规模生成、行为聚类、排名和轮询提交策略，高效探索解决方案空间。

Result: GenCluster在IOI 2025中首次使用开源模型gpt-oss-120b获得金牌。

Conclusion: 为LLM的透明和可复现评估设定了新基准。

Abstract: Competitive programming has become a rigorous benchmark for evaluating the
reasoning and problem-solving capabilities of large language models (LLMs). The
International Olympiad in Informatics (IOI) stands out as one of the most
prestigious annual competitions in competitive programming and has become a key
benchmark for comparing human and AI-level programming ability. While several
proprietary models have been claimed to achieve gold medal-level performance at
the IOI, often with undisclosed methods, achieving comparable results with
open-weight models remains a significant challenge. In this paper, we present
\gencluster, a scalable and reproducible test-time compute framework that
attains IOI gold-level performance using open-weight models. It combines
large-scale generation, behavioral clustering, ranking, and a round-robin
submission strategy to efficiently explore diverse solution spaces under
limited validation budgets. Our experiments show that the performance of our
proposed approach scales consistently with available compute, narrowing the gap
between open and closed systems. Notably, we will show that GenCluster can
achieve a gold medal at IOI 2025 for the first time with an open-weight model
gpt-oss-120b, setting a new benchmark for transparent and reproducible
evaluation of reasoning in LLMs.

</details>


### [176] [Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation](https://arxiv.org/abs/2510.14246)
*Jingwen Gu,Yiting He,Zhishuai Liu,Pan Xu*

Main category: cs.LG

TL;DR: 论文提出了一种名为DR-RPO的在线策略优化算法，用于在分布偏移下学习鲁棒策略，具有次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究在强化学习中分布偏移下的决策问题，特别是在在线设置中样本效率和探索的重要性。

Method: 提出DR-RPO算法，结合参考策略正则化和线性函数逼近，实现鲁棒策略优化。

Result: 理论证明DR-RPO在鲁棒RL中具有多项式次优界和样本效率，实验验证其鲁棒性。

Conclusion: DR-RPO填补了鲁棒RL中策略优化的空白，理论与实验结果一致。

Abstract: Decision-making under distribution shift is a central challenge in
reinforcement learning (RL), where training and deployment environments differ.
We study this problem through the lens of robust Markov decision processes
(RMDPs), which optimize performance against adversarial transition dynamics.
Our focus is the online setting, where the agent has only limited interaction
with the environment, making sample efficiency and exploration especially
critical. Policy optimization, despite its success in standard RL, remains
theoretically and empirically underexplored in robust RL. To bridge this gap,
we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized
\textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online
policy optimization method that learns robust policies with sublinear regret.
To enable tractable optimization within the softmax policy class, DR-RPO
incorporates reference-policy regularization, yielding RMDP variants that are
doubly constrained in both transitions and policies. To scale to large
state-action spaces, we adopt the $d$-rectangular linear MDP formulation and
combine linear function approximation with an upper confidence bonus for
optimistic exploration. We provide theoretical guarantees showing that policy
optimization can achieve polynomial suboptimality bounds and sample efficiency
in robust RL, matching the performance of value-based approaches. Finally,
empirical results across diverse domains corroborate our theory and demonstrate
the robustness of DR-RPO.

</details>


### [177] [A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters](https://arxiv.org/abs/2510.14250)
*Lianzi Jiang,Jianxin Zhang,Xinyu Han,Huanhe Dong,Xiangrong Wang*

Main category: cs.LG

TL;DR: 提出了一种物理先验引导的双流注意力网络（PhysAttnNet），用于提高弹性布拉格防波堤运动响应预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在未见海况下泛化能力有限，且忽略了海洋系统的自然衰减和波-结构相互作用（WSI）的建模不足。

Method: 结合可学习时间衰减的双向自注意力模块（DBSA）和基于相位差的双向交叉注意力模块（PDG-BCA），并通过全局上下文融合模块（GCF）整合。

Result: PhysAttnNet在波槽数据集上显著优于主流模型，且在跨场景泛化测试中表现出鲁棒性和适应性。

Conclusion: PhysAttnNet为海洋工程中复杂系统的预测模型开发提供了潜在框架。

Abstract: Accurate motion response prediction for elastic Bragg breakwaters is critical
for their structural safety and operational integrity in marine environments.
However, conventional deep learning models often exhibit limited generalization
capabilities when presented with unseen sea states. These deficiencies stem
from the neglect of natural decay observed in marine systems and inadequate
modeling of wave-structure interaction (WSI). To overcome these challenges,
this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network
(PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module
incorporates a learnable temporal decay to assign higher weights to recent
states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase
differences guided bidirectional cross-attention (PDG-BCA) module explicitly
captures the bidirectional interaction and phase relationship between waves and
the structure using a cosine-based bias within a bidirectional
cross-computation paradigm. These streams are synergistically integrated
through a global context fusion (GCF) module. Finally, PhysAttnNet is trained
with a hybrid time-frequency loss that jointly minimizes time-domain prediction
errors and frequency-domain spectral discrepancies. Comprehensive experiments
on wave flume datasets demonstrate that PhysAttnNet significantly outperforms
mainstream models. Furthermore,cross-scenario generalization tests validate the
model's robustness and adaptability to unseen environments, highlighting its
potential as a framework to develop predictive models for complex systems in
ocean engineering.

</details>


### [178] [Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals](https://arxiv.org/abs/2510.14254)
*Saurabh Kataria,Yi Wu,Zhaoliang Chen,Hyunjung Gloria Kwak,Yuhao Xu,Lovely Yeswanth Panchumarthi,Ran Xiao,Jiaying Lu,Ayca Ermis,Anni Zhao,Runze Yan,Alex Federov,Zewen Liu,Xu Wu,Wei Jin,Carl Yang,Jocelyn Grunwell,Stephanie R. Brown,Amit Shah,Craig Jabaley,Tim Buchman,Sivasubramanium V Bhavani,Randall J. Lee,Xiao Hu*

Main category: cs.LG

TL;DR: 对比通用和专用时间序列基础模型在PPG信号任务上的性能，发现专用模型在全面调优场景下表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究通用和专用时间序列基础模型在生理信号（如PPG）任务中的性能差异，以指导模型选择和应用。

Method: 通过51个任务（包括心脏状态评估、实验室值估计和跨模态推理）对两种模型进行七维度的综合评估。

Result: 专用模型在全面调优场景下胜率高出27%。

Conclusion: 专用模型在特定任务中表现更优，但通用模型在适应性和可扩展性方面有潜力。

Abstract: Foundation models are large-scale machine learning models that are
pre-trained on massive amounts of data and can be adapted for various
downstream tasks. They have been extensively applied to tasks in Natural
Language Processing and Computer Vision with models such as GPT, BERT, and
CLIP. They are now also increasingly gaining attention in time-series analysis,
particularly for physiological sensing. However, most time series foundation
models are specialist models - with data in pre-training and testing of the
same type, such as Electrocardiogram, Electroencephalogram, and
Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time
series foundation model with data from multiple domains, such as weather,
traffic, and electricity. This paper aims to conduct a comprehensive
benchmarking study to compare the performance of generalist and specialist
models, with a focus on PPG signals. Through an extensive suite of total 51
tasks covering cardiac state assessment, laboratory value estimation, and
cross-modal inference, we comprehensively evaluate both models across seven
dimensions, including win score, average performance, feature quality, tuning
gain, performance variance, transferability, and scalability. These metrics
jointly capture not only the models' capability but also their adaptability,
robustness, and efficiency under different fine-tuning strategies, providing a
holistic understanding of their strengths and limitations for diverse
downstream scenarios. In a full-tuning scenario, we demonstrate that the
specialist model achieves a 27% higher win score. Finally, we provide further
analysis on generalization, fairness, attention visualizations, and the
importance of training data choice.

</details>


### [179] [CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions](https://arxiv.org/abs/2510.14262)
*Zihao Fu,Ming Liao,Chris Russell,Zhenguang G. Cai*

Main category: cs.LG

TL;DR: CAST是一种无需探针的框架，通过直接估计变换矩阵和光谱分析，为理解Transformer层功能提供新视角。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型内部机制不透明，现有解释方法各有局限，CAST旨在提供互补视角。

Method: 使用Moore-Penrose伪逆估计每层的变换矩阵，并通过六种可解释指标进行光谱分析。

Result: 发现编码器和解码器模型行为差异：解码器呈现压缩-扩展循环，编码器保持高秩处理；层间功能关系分为特征提取、压缩和专业化三阶段。

Conclusion: CAST为Transformer模型解释性提供了新工具，揭示了层间功能模式，补充了现有方法。

Abstract: Large language models have achieved remarkable success but remain largely
black boxes with poorly understood internal mechanisms. To address this
limitation, many researchers have proposed various interpretability methods
including mechanistic analysis, probing classifiers, and activation
visualization, each providing valuable insights from different perspectives.
Building upon this rich landscape of complementary approaches, we introduce
CAST (Compositional Analysis via Spectral Tracking), a probe-free framework
that contributes a novel perspective by analyzing transformer layer functions
through direct transformation matrix estimation and comprehensive spectral
analysis. CAST offers complementary insights to existing methods by estimating
the realized transformation matrices for each layer using Moore-Penrose
pseudoinverse and applying spectral analysis with six interpretable metrics
characterizing layer behavior. Our analysis reveals distinct behaviors between
encoder-only and decoder-only models, with decoder models exhibiting
compression-expansion cycles while encoder models maintain consistent high-rank
processing. Kernel analysis further demonstrates functional relationship
patterns between layers, with CKA similarity matrices clearly partitioning
layers into three phases: feature extraction, compression, and specialization.

</details>


### [180] [Nonparametric Data Attribution for Diffusion Models](https://arxiv.org/abs/2510.14269)
*Yutian Zhao,Chao Du,Xiaosen Zheng,Tianyu Pang,Min Lin*

Main category: cs.LG

TL;DR: 提出了一种基于数据非参数化的生成模型归因方法，通过图像块相似性衡量训练数据对生成结果的影响，无需模型梯度或重训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要模型梯度或重训练，限制了在专有或大规模场景中的应用。

Method: 基于数据的方法，通过图像块相似性计算影响，支持多尺度表示，并通过卷积加速计算。

Result: 方法在归因性能上接近基于梯度的方法，显著优于现有非参数基线。

Conclusion: 该方法高效且可解释，揭示了训练数据与输出之间的内在关系。

Abstract: Data attribution for generative models seeks to quantify the influence of
individual training examples on model outputs. Existing methods for diffusion
models typically require access to model gradients or retraining, limiting
their applicability in proprietary or large-scale settings. We propose a
nonparametric attribution method that operates entirely on data, measuring
influence via patch-level similarity between generated and training images. Our
approach is grounded in the analytical form of the optimal score function and
naturally extends to multiscale representations, while remaining
computationally efficient through convolution-based acceleration. In addition
to producing spatially interpretable attributions, our framework uncovers
patterns that reflect intrinsic relationships between training data and
outputs, independent of any specific model. Experiments demonstrate that our
method achieves strong attribution performance, closely matching gradient-based
approaches and substantially outperforming existing nonparametric baselines.
Code is available at https://github.com/sail-sg/NDA.

</details>


### [181] [Stable Prediction of Adverse Events in Medical Time-Series Data](https://arxiv.org/abs/2510.14286)
*Mayank Keoliya,Seewon Choi,Rajeev Alur,Mayur Naik,Eric Wong*

Main category: cs.LG

TL;DR: CAREBench是一个早期事件预测（EEP）基准测试，评估多模态输入（如电子健康记录、心电图波形和临床文本）的部署能力，并同时关注预测准确性和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前EEP系统在风险轨迹的稳定性和多模态输入评估方面存在不足，CAREBench旨在填补这一空白，提升临床决策的信任度。

Method: 引入CAREBench基准测试，提出稳定性指标量化短期风险变异性，并基于局部Lipschitz常数惩罚突变。

Result: 现有方法（尤其是零样本LLMs）难以同时优化准确性和稳定性，高精度操作点下召回率较差。

Conclusion: 需要开发证据对齐且稳定的模型，以在连续监测环境中赢得临床医生的信任。

Abstract: Early event prediction (EEP) systems continuously estimate a patient's
imminent risk to support clinical decision-making. For bedside trust, risk
trajectories must be accurate and temporally stable, shifting only with new,
relevant evidence. However, current benchmarks (a) ignore stability of risk
scores and (b) evaluate mainly on tabular inputs, leaving trajectory behavior
untested. To address this gap, we introduce CAREBench, an EEP benchmark that
evaluates deployability using multi-modal inputs-tabular EHR, ECG waveforms,
and clinical text-and assesses temporal stability alongside predictive
accuracy. We propose a stability metric that quantifies short-term variability
in per-patient risk and penalizes abrupt oscillations based on local-Lipschitz
constants. CAREBench spans six prediction tasks such as sepsis onset and
compares classical learners, deep sequence models, and zero-shot LLMs. Across
tasks, existing methods, especially LLMs, struggle to jointly optimize accuracy
and stability, with notably poor recall at high-precision operating points.
These results highlight the need for models that produce evidence-aligned,
stable trajectories to earn clinician trust in continuous monitoring settings.
(Code: https://github.com/SeewonChoi/CAREBench.)

</details>


### [182] [Enhancing Time-Series Anomaly Detection by Integrating Spectral-Residual Bottom-Up Attention with Reservoir Computing](https://arxiv.org/abs/2510.14287)
*Hayato Nihei,Sou Nobukawa,Yusuke Sakemi,Kazuyuki Aihara*

Main category: cs.LG

TL;DR: 提出了一种结合谱残差（SR）方法与储层计算（RC）的SR-RC模型，用于提升时间序列异常检测性能，同时保持学习效率。


<details>
  <summary>Details</summary>
Motivation: 储层计算（RC）在边缘AI应用中具有潜力，但单独使用时可能需要过大的储层，影响资源受限设备的性能。

Method: 将学习无关的谱残差（SR）方法与RC结合，形成SR-RC模型。

Result: SR-RC在基准任务和真实数据集上优于传统RC和基于SR的逻辑回归模型。

Conclusion: SR-RC为边缘AI中的时间序列异常检测提供了实用解决方案。

Abstract: Reservoir computing (RC) establishes the basis for the processing of
time-series data by exploiting the high-dimensional spatiotemporal response of
a recurrent neural network to an input signal. In particular, RC trains only
the output layer weights. This simplicity has drawn attention especially in
Edge Artificial Intelligence (AI) applications. Edge AI enables time-series
anomaly detection in real time, which is important because detection delays can
lead to serious incidents. However, achieving adequate anomaly-detection
performance with RC alone may require an unacceptably large reservoir on
resource-constrained edge devices. Without enlarging the reservoir, attention
mechanisms can improve accuracy, although they may require substantial
computation and undermine the learning efficiency of RC. In this study, to
improve the anomaly detection performance of RC without sacrificing learning
efficiency, we propose a spectral residual RC (SR-RC) that integrates the
spectral residual (SR) method - a learning-free, bottom-up attention mechanism
- with RC. We demonstrated that SR-RC outperformed conventional RC and
logistic-regression models based on values extracted by the SR method across
benchmark tasks and real-world time-series datasets. Moreover, because the SR
method, similarly to RC, is well suited for hardware implementation, SR-RC
suggests a practical direction for deploying RC as Edge AI for time-series
anomaly detection.

</details>


### [183] [TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening](https://arxiv.org/abs/2510.14299)
*Nam Le,Leo Yu Zhang,Kewen Liao,Shirui Pan,Wei Luo*

Main category: cs.LG

TL;DR: TED++ 是一种子流形感知框架，用于检测现有防御方法难以发现的隐蔽后门攻击。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络在关键应用中的普及，隐蔽的后门攻击（即看似良性的训练输入触发恶意行为）成为严重的安全威胁。现有防御方法在攻击者利用细微距离异常或干净样本稀缺时表现不佳。

Method: TED++ 通过构建每个类隐藏特征流形的管状邻域，估计其局部“厚度”，并应用局部自适应排名（LAR）检测偏离可接受范围的激活。通过聚合各层的 LAR 调整排名，TED++ 捕捉输入在演化子流形上的忠实性。

Result: 在基准数据集和任务上的实验表明，TED++ 在自适应攻击和有限数据场景下均达到最先进的检测性能，仅需每类五个样本即可实现近乎完美的检测。

Conclusion: TED++ 显著提升了后门攻击检测的鲁棒性和准确性，尤其在数据稀缺时表现突出。

Abstract: As deep neural networks power increasingly critical applications, stealthy
backdoor attacks, where poisoned training inputs trigger malicious model
behaviour while appearing benign, pose a severe security risk. Many existing
defences are vulnerable when attackers exploit subtle distance-based anomalies
or when clean examples are scarce. To meet this challenge, we introduce TED++,
a submanifold-aware framework that effectively detects subtle backdoors that
evade existing defences. TED++ begins by constructing a tubular neighbourhood
around each class's hidden-feature manifold, estimating its local ``thickness''
from a handful of clean activations. It then applies Locally Adaptive Ranking
(LAR) to detect any activation that drifts outside the admissible tube. By
aggregating these LAR-adjusted ranks across all layers, TED++ captures how
faithfully an input remains on the evolving class submanifolds. Based on such
characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose
LAR-based ranking sequences deviate significantly. Extensive experiments are
conducted on benchmark datasets and tasks, demonstrating that TED++ achieves
state-of-the-art detection performance under both adaptive-attack and
limited-data scenarios. Remarkably, even with only five held-out examples per
class, TED++ still delivers near-perfect detection, achieving gains of up to
14\% in AUROC over the next-best method. The code is publicly available at
https://github.com/namle-w/TEDpp.

</details>


### [184] [Active Measuring in Reinforcement Learning With Delayed Negative Effects](https://arxiv.org/abs/2510.14315)
*Daiqi Gao,Ziping Xu,Aseel Rawashdeh,Predrag Klasnja,Susan A. Murphy*

Main category: cs.LG

TL;DR: 论文提出了一种主动可观测马尔可夫决策过程（AOMDP），允许代理在控制动作之外决定是否测量潜在状态，以平衡测量成本和收益。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，测量状态可能成本高昂且对后续结果产生负面影响，因此需要一种方法在减少不确定性和测量成本之间取得平衡。

Method: 将AOMDP建模为周期性部分可观测MDP，提出基于信念状态的在线RL算法，并使用序列蒙特卡洛方法近似未知静态参数和潜在状态的后验分布。

Result: 研究表明，减少不确定性可以提高样本效率和最优策略的价值，尽管测量可能带来延迟成本。

Conclusion: AOMDP框架在数字健康应用中验证了其有效性，代理能够权衡干预和状态测量的时机。

Abstract: Measuring states in reinforcement learning (RL) can be costly in real-world
settings and may negatively influence future outcomes. We introduce the
Actively Observable Markov Decision Process (AOMDP), where an agent not only
selects control actions but also decides whether to measure the latent state.
The measurement action reveals the true latent state but may have a negative
delayed effect on the environment. We show that this reduced uncertainty may
provably improve sample efficiency and increase the value of the optimal policy
despite these costs. We formulate an AOMDP as a periodic partially observable
MDP and propose an online RL algorithm based on belief states. To approximate
the belief states, we further propose a sequential Monte Carlo method to
jointly approximate the posterior of unknown static environment parameters and
unobserved latent states. We evaluate the proposed algorithm in a digital
health application, where the agent decides when to deliver digital
interventions and when to assess users' health status through surveys.

</details>


### [185] [LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search](https://arxiv.org/abs/2510.14331)
*Shivam Singhal,Eran Malach,Tomaso Poggio,Tomer Galanti*

Main category: cs.LG

TL;DR: LLM-ERM框架通过LLM引导搜索和验证，显著提高了程序学习的样本效率和计算可行性。


<details>
  <summary>Details</summary>
Motivation: 解决传统程序学习中样本效率低和计算成本高的问题。

Method: 使用预训练的LLM生成候选程序，并通过验证选择最佳假设。

Result: LLM-ERM在少量样本下成功解决复杂任务，优于梯度训练方法。

Conclusion: LLM-ERM结合了统计效率和计算可行性，为程序学习提供了实用路径。

Abstract: We seek algorithms for program learning that are both sample-efficient and
computationally feasible. Classical results show that targets admitting short
program descriptions (e.g., with short ``python code'') can be learned with a
``small'' number of examples (scaling with the size of the code) via
length-first program enumeration, but the search is exponential in description
length. Consequently, Gradient-based training avoids this cost yet can require
exponentially many samples on certain short-program families.
  To address this gap, we introduce LLM-ERM, a propose-and-verify framework
that replaces exhaustive enumeration with an LLM-guided search over candidate
programs while retaining ERM-style selection on held-out data. Specifically, we
draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and
check each on the data, and return the best verified hypothesis, with no
feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise
online mini-batch SGD requires many samples to learn certain short programs.
{\em Empirically, LLM-ERM solves tasks such as parity variants, pattern
matching, and primality testing with as few as 200 samples, while SGD-trained
transformers overfit even with 100,000 samples}. These results indicate that
language-guided program synthesis recovers much of the statistical efficiency
of finite-class ERM while remaining computationally tractable, offering a
practical route to learning succinct hypotheses beyond the reach of
gradient-based training.

</details>


### [186] [DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis](https://arxiv.org/abs/2510.14336)
*Shruti Sarika Chakraborty,Peter Minary*

Main category: cs.LG

TL;DR: 论文提出了一种基于不对称性和DARTS的图Transformer改进方法（DARTS-GT），通过深度特异性组件选择和因果消融量化可解释性，实现了性能与可解释性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer设计僵化，缺乏可量化可解释性，且性能提升难以区分有意义模式与虚假相关性。

Method: 通过不对称性重新设计注意力机制，分离结构编码与特征表示，并利用DARTS选择每层最优GNN算子。

Result: 在八个基准测试中，DARTS-GT在四个数据集上达到SOTA，其余保持竞争力，且发现架构具有数据集特异性。

Conclusion: DARTS-GT证明图Transformer无需在性能与可解释性间妥协，且可视化注意力与因果重要性并不总相关。

Abstract: Graph Transformers (GTs) have emerged as powerful architectures for
graph-structured data, yet remain constrained by rigid designs and lack
quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN
types across all layers, missing potential benefits of depth-specific component
selection, while their complex architectures become opaque where performance
gains cannot be distinguished between meaningful patterns and spurious
correlations. We redesign GT attention through asymmetry, decoupling structural
encoding from feature representation: queries derive from node features while
keys and values come from GNN transformations. Within this framework, we use
Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at
each layer, enabling depth-wise heterogeneity inside transformer attention
itself (DARTS-GT). To understand discovered architectures, we develop the first
quantitative interpretability framework for GTs through causal ablation. Our
metrics (Head-deviation, Specialization, and Focus), identify which heads and
nodes drive predictions while enabling model comparison. Experiments across
eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while
remaining competitive on others, with discovered architectures revealing
dataset-specific patterns. Our interpretability analysis reveals that visual
attention salience and causal importance do not always correlate, indicating
widely used visualization approaches may miss components that actually matter.
Crucially, heterogeneous architectures found by DARTS-GT consistently produced
more interpretable models than baselines, establishing that Graph Transformers
need not choose between performance and interpretability.

</details>


### [187] [Stop-RAG: Value-Based Retrieval Control for Iterative RAG](https://arxiv.org/abs/2510.14337)
*Jaewan Park,Solbee Cho,Jay-Yoon Lee*

Main category: cs.LG

TL;DR: Stop-RAG是一种基于价值的控制器，通过自适应决定何时停止检索来优化迭代RAG系统的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 迭代RAG在多跳问题中表现良好，但每次检索会增加延迟、成本和干扰证据的风险，需要高效的停止策略。

Method: 将迭代RAG建模为有限马尔可夫决策过程，提出Stop-RAG控制器，利用Q(λ)目标训练自适应停止策略。

Result: 在多跳问答基准测试中，Stop-RAG优于固定迭代和基于提示的停止方法。

Conclusion: 自适应停止是当前代理系统的关键缺失组件，基于价值的控制能提升RAG系统的准确性。

Abstract: Iterative retrieval-augmented generation (RAG) enables large language models
to answer complex multi-hop questions, but each additional loop increases
latency, costs, and the risk of introducing distracting evidence, motivating
the need for an efficient stopping strategy. Existing methods either use a
predetermined number of iterations or rely on confidence proxies that poorly
reflect whether more retrieval will actually help. We cast iterative RAG as a
finite-horizon Markov decision process and introduce Stop-RAG, a value-based
controller that adaptively decides when to stop retrieving. Trained with
full-width forward-view Q($\lambda$) targets from complete trajectories,
Stop-RAG learns effective stopping policies while remaining compatible with
black-box APIs and existing pipelines. On multi-hop question-answering
benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines
and prompting-based stopping with LLMs. These results highlight adaptive
stopping as a key missing component in current agentic systems, and demonstrate
that value-based control can improve the accuracy of RAG systems.

</details>


### [188] [Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis](https://arxiv.org/abs/2510.14342)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 论文提出了一种基于射丛和Weil代数的自动微分几何框架，揭示了反向模式AD的余切拉回本质，并展示了高阶导数的代数精确性。


<details>
  <summary>Details</summary>
Motivation: 通过几何视角重新理解自动微分（AD），为深度学习和科学计算中的结构保持微分方法提供理论基础。

Method: 利用射丛和Weil代数构建几何框架，将反向模式AD解释为余切拉回，泰勒模式对应Weil代数中的求值。

Result: 证明了反向模式的函子恒等式、高阶导数的代数精确性，并提出了避免组合爆炸的张量化Weil代数方法。

Conclusion: 该框架为AD理论提供了微分几何视角，并为结构保持微分方法的发展奠定了基础。

Abstract: We present a geometric formulation of automatic differentiation (AD) using
jet bundles and Weil algebras. Reverse-mode AD emerges as cotangent-pullback,
while Taylor-mode corresponds to evaluation in a Weil algebra. From these
principles, we derive concise statements on correctness, stability, and
complexity: a functorial identity for reverse-mode, algebraic exactness of
higher-order derivatives, and explicit bounds on truncation error. We further
show that tensorized Weil algebras permit one-pass computation of all mixed
derivatives with cost linear in the algebra dimension, avoiding the
combinatorial blow-up of nested JVP/VJP schedules. This framework interprets AD
theory through the lens of differential geometry and offers a foundation for
developing structure-preserving differentiation methods in deep learning and
scientific computing. Code and examples are available at
https://git.nilu.no/geometric-ad/jet-weil-ad.

</details>


### [189] [Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers](https://arxiv.org/abs/2510.14381)
*Andrew Zhao,Reshmi Ghosh,Vitor Carvalho,Emily Lawton,Keegan Hines,Gao Huang,Jack W. Stokes*

Main category: cs.LG

TL;DR: 该论文首次系统分析了基于LLM的提示优化中的中毒风险，发现反馈攻击比查询注入更危险，并提出了一种简单的防御方法。


<details>
  <summary>Details</summary>
Motivation: 研究LLM提示优化阶段的安全性问题，填补该领域的研究空白。

Method: 使用HarmBench评估系统对操纵反馈的脆弱性，提出假奖励攻击和轻量级高亮防御。

Result: 反馈攻击显著提高攻击成功率（ΔASR=0.48），防御方法将假奖励攻击的ΔASR从0.23降至0.07。

Conclusion: 提示优化管道是一类新的攻击面，需加强对反馈通道和优化框架的安全防护。

Abstract: Large language model (LLM) systems now underpin everyday AI applications such
as chatbots, computer-use assistants, and autonomous robots, where performance
often depends on carefully designed prompts. LLM-based prompt optimizers reduce
that effort by iteratively refining prompts from scored feedback, yet the
security of this optimization stage remains underexamined. We present the first
systematic analysis of poisoning risks in LLM-based prompt optimization. Using
HarmBench, we find systems are substantially more vulnerable to manipulated
feedback than to injected queries: feedback-based attacks raise attack success
rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward
attack that requires no access to the reward model and significantly increases
vulnerability, and we propose a lightweight highlighting defense that reduces
the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These
results establish prompt optimization pipelines as a first-class attack surface
and motivate stronger safeguards for feedback channels and optimization
frameworks.

</details>


### [190] [SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences](https://arxiv.org/abs/2510.14386)
*Kartikay Agrawal,Abhijeet Vikram,Vedant Sharma,Vaishnavi N.,Ayon Borthakur*

Main category: cs.LG

TL;DR: SHaRe-SSM是一种新型的脉冲状态空间模型，用于长序列建模，具有高能效和性能优势。


<details>
  <summary>Details</summary>
Motivation: 结合脉冲神经网络（SNNs）和状态空间模型（SSMs）的优势，解决长序列建模中的计算效率和资源消耗问题。

Method: 设计了一种二阶脉冲SSM（SHaRe-SSM），利用并行扫描实现稳定高效的动态系统实现，并首次提出基于核的脉冲回归器。

Result: SHaRe-SSM在18k序列上比二阶ANN-SSM节能73倍，且在50k序列上表现优异。

Conclusion: SHaRe-SSM在长序列建模中表现出高效能和性能优势，适合资源受限的应用场景。

Abstract: In recent years, with the emergence of large models, there has been a
significant interest in spiking neural networks (SNNs) primarily due to their
energy efficiency, multiplication-free, and sparse event-based deep learning.
Similarly, state space models (SSMs) in varying designs have evolved as a
powerful alternative to transformers for target modeling in long sequences,
thereby overcoming the quadratic dependence on sequence length of a
transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking
Harmonic Resonate and Fire State Space Model), for target variable modeling
(including both classification and regression) for very-long-range sequences.
Our second-order spiking SSM, on average, performs better than transformers or
first-order SSMs while circumventing multiplication operations, making it ideal
for resource-constrained applications. The proposed block consumes $73 \times$
less energy than second-order ANN-based SSMs for an 18k sequence, while
retaining performance. To ensure learnability over the long-range sequences, we
propose exploiting the stable and efficient implementation of the dynamical
system using parallel scans. Moreover, for the first time, we propose a
kernel-based spiking regressor using resonate and fire neurons for very
long-range sequences. Our network shows superior performance on even a 50k
sequence while being significantly energy-efficient. In addition, we conducted
a systematic analysis of the impact of heterogeneity, dissipation, and
conservation in resonate-and-fire SSMs.

</details>


### [191] [Revisit Modality Imbalance at the Decision Layer](https://arxiv.org/abs/2510.14411)
*Xiaoyu Ma,Hao Chen*

Main category: cs.LG

TL;DR: 论文揭示了多模态学习中决策层的不平衡问题，并提出未来系统应关注自适应权重分配机制。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因模态不平衡导致主导模态压制弱势模态，本文发现这种不平衡在决策层尤为显著。

Method: 通过音频-视觉数据集（CREMAD和Kinetic-Sounds）实验，分析特征空间和决策权重分布的差异。

Result: 实验表明，即使经过预训练和平衡优化，模型仍对某些模态（如音频）存在系统性偏差。

Conclusion: 未来多模态系统应在决策层引入自适应权重分配机制，以平衡各模态的贡献。

Abstract: Multimodal learning integrates information from different modalities to
enhance model performance, yet it often suffers from modality imbalance, where
dominant modalities overshadow weaker ones during joint optimization. This
paper reveals that such an imbalance not only occurs during representation
learning but also manifests significantly at the decision layer. Experiments on
audio-visual datasets (CREMAD and Kinetic-Sounds) show that even after
extensive pretraining and balanced optimization, models still exhibit
systematic bias toward certain modalities, such as audio. Further analysis
demonstrates that this bias originates from intrinsic disparities in
feature-space and decision-weight distributions rather than from optimization
dynamics alone. We argue that aggregating uncalibrated modality outputs at the
fusion stage leads to biased decision-layer weighting, hindering weaker
modalities from contributing effectively. To address this, we propose that
future multimodal systems should focus more on incorporate adaptive weight
allocation mechanisms at the decision layer, enabling relative balanced
according to the capabilities of each modality.

</details>


### [192] [Interaction Concordance Index: Performance Evaluation for Interaction Prediction Methods](https://arxiv.org/abs/2510.14419)
*Tapio Pahikkala,Riikka Numminen,Parisa Movahedi,Napsu Karmitsa,Antti Airola*

Main category: cs.LG

TL;DR: 提出了一种新的评估指标IC-index，用于衡量药物-靶标亲和力（DTA）预测中相互作用方向的准确性，补充现有性能评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有DTA预测方法主要关注亲和力值，而忽略了相互作用方向的重要性。正确捕捉相互作用方向能优化决策，如药物分配。

Method: 引入IC-index评估相互作用方向的预测性能，分析预测器的不变性和学习算法的置换等变性。

Result: 实验表明IC-index能有效补充现有评估方法，揭示不同预测方法在相互作用方向上的表现差异。

Conclusion: IC-index为DTA预测提供了新的评估维度，强调了相互作用方向的重要性，并展示了如何通过侧信息改进预测性能。

Abstract: Consider two sets of entities and their members' mutual affinity values, say
drug-target affinities (DTA). Drugs and targets are said to interact in their
effects on DTAs if drug's effect on it depends on the target. Presence of
interaction implies that assigning a drug to a target and another drug to
another target does not provide the same aggregate DTA as the reversed
assignment would provide. Accordingly, correctly capturing interactions enables
better decision-making, for example, in allocation of limited numbers of drug
doses to their best matching targets. Learning to predict DTAs is popularly
done from either solely from known DTAs or together with side information on
the entities, such as chemical structures of drugs and targets. In this paper,
we introduce interaction directions' prediction performance estimator we call
interaction concordance index (IC-index), for both fixed predictors and machine
learning algorithms aimed for inferring them. IC-index complements the
popularly used DTA prediction performance estimators by evaluating the ratio of
correctly predicted directions of interaction effects in data. First, we show
the invariance of IC-index on predictors unable to capture interactions.
Secondly, we show that learning algorithm's permutation equivariance regarding
drug and target identities implies its inability to capture interactions when
either drug, target or both are unseen during training. In practical
applications, this equivariance is remedied via incorporation of appropriate
side information on drugs and targets. We make a comprehensive empirical
evaluation over several biomedical interaction data sets with various
state-of-the-art machine learning algorithms. The experiments demonstrate how
different types of affinity strength prediction methods perform in terms of
IC-index complementing existing prediction performance estimators.

</details>


### [193] [MergeMoE: Efficient Compression of MoE Models via Expert Output Merging](https://arxiv.org/abs/2510.14436)
*Ruijie Miao,Yilun Yao,Zihan Wang,Zhiming Wang,Bairen Yi,LingJun Liu,Yikai Zhao,Tong Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于专家合并的MoE模型压缩方法MergeMoE，通过数学优化构建压缩矩阵，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: MoE模型的内存开销大，压缩成为重要研究方向，专家合并是一种新兴技术。

Method: 从专家输出合并的角度分析专家合并，提出优化公式，并设计MergeMoE方法。

Result: 在多个MoE模型上验证，MergeMoE在相同压缩比下优于基线方法。

Conclusion: MergeMoE通过优化专家合并过程，有效压缩MoE模型，性能优于现有方法。

Abstract: The Mixture-of-Experts (MoE) technique has proven to be a promising solution
to efficiently scale the model size, which has been widely applied in recent
LLM advancements. However, the substantial memory overhead of MoE models has
made their compression an important research direction. In this work, we
provide a theoretical analysis of expert merging, a recently proposed technique
for compressing MoE models. Rather than interpreting expert merging from the
conventional perspective of parameter aggregation, we approach it from the
perspective of merging experts' outputs. Our key insight is that the merging
process can be interpreted as inserting additional matrices into the forward
computation, which naturally leads to an optimization formulation. Building on
this analysis, we introduce MergeMoE, a method that leverages mathematical
optimization to construct the compression matrices. We evaluate MergeMoE on
multiple MoE models and show that our algorithm consistently outperforms the
baselines with the same compression ratios.

</details>


### [194] [A Free Lunch in LLM Compression: Revisiting Retraining after Pruning](https://arxiv.org/abs/2510.14444)
*Moritz Wagner,Christophe Roux,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 研究发现，在大型语言模型（LLM）剪枝后，单独重构注意力与MLP组件比全量重训练更高效且性能更好，挑战了传统避免重训练的直觉。


<details>
  <summary>Details</summary>
Motivation: 探讨剪枝后权重重构或重训练的关键设计选择，以解决LLM剪枝后性能恢复的问题。

Method: 在GPT架构上进行广泛计算研究，比较不同重构与重训练策略的效果。

Result: 发现单独重构组件比全量重训练更高效且性能更优，且简单剪枝标准在正确执行重构时能超越复杂方法。

Conclusion: 研究挑战了避免重训练的直觉，为LLM剪枝后性能恢复提供了重要见解。

Abstract: While Neural Network pruning typically requires retraining the model to
recover pruning-induced performance degradation, state-of-the-art Large
Language Models (LLMs) pruning methods instead solve a layer-wise mask
selection and reconstruction problem on a small set of calibration data to
avoid full retraining, as it is considered computationally infeasible for LLMs.
Reconstructing single matrices in isolation has favorable properties, such as
convexity of the objective and significantly reduced memory requirements
compared to full retraining. In practice, however, reconstruction is often
implemented at coarser granularities, e.g., reconstructing a whole transformer
block against its dense activations instead of a single matrix. In this work,
we study the key design choices when reconstructing or retraining the remaining
weights after pruning. We conduct an extensive computational study on
state-of-the-art GPT architectures, and report several surprising findings that
challenge common intuitions about retraining after pruning. In particular, we
observe a free lunch scenario: reconstructing attention and MLP components
separately within each transformer block is nearly the most resource-efficient
yet achieves the best perplexity. Most importantly, this Pareto-optimal setup
achieves better performance than full retraining, despite requiring only a
fraction of the memory. Furthermore, we demonstrate that simple and efficient
pruning criteria such as Wanda can outperform much more complex approaches when
the reconstruction step is properly executed, highlighting its importance. Our
findings challenge the narrative that retraining should be avoided at all costs
and provide important insights into post-pruning performance recovery for LLMs.

</details>


### [195] [Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits](https://arxiv.org/abs/2510.14445)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: GAN用于生成3D河流沉积物图像，表现优于传统方法，能稳定训练并重现地质结构。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型难以准确模拟河流沉积物的连续性，研究探索GAN是否能更好地生成此类地质结构。

Method: 使用基于过程的模型生成训练数据，训练GAN生成3D河流沉积物图像，并通过消融实验验证其效果。

Result: GAN能稳定生成非平稳且细节丰富的沉积物图像，未出现模式崩溃或记忆训练数据的问题。

Conclusion: GAN在地质结构生成中表现稳健，未来需验证其在更大3D图像和多模态数据集中的表现。

Abstract: The distribution of resources in the subsurface is deeply linked to the
variations of its physical properties. Generative modeling has long been used
to predict those physical properties while quantifying the associated
uncertainty. But current approaches struggle to properly reproduce geological
structures, and fluvial deposits in particular, because of their continuity.
This study explores whether a generative adversarial network (GAN) - a type of
deep-learning algorithm for generative modeling - can be trained to reproduce
fluvial deposits simulated by a process-based model - a more expensive model
that mimics geological processes. An ablation study shows that developments
from the deep-learning community to generate large 2D images are directly
transferable to 3D images of fluvial deposits. Training remains stable, and the
generated samples reproduce the non-stationarity and details of the deposits
without mode collapse or pure memorization of the training data. Using a
process-based model to generate those training data allows us to include
valuable properties other than the usual physical properties. We show how the
deposition time let us monitor and validate the performance of a GAN by
checking that its samples honor the law of superposition. Our work joins a
series of previous studies suggesting that GANs are more robust that given
credit for, at least for training datasets targeting specific geological
structures. Whether this robustness transfers to larger 3D images and
multimodal datasets remains to be seen. Exploring how deep generative models
can leverage geological principles like the law of superposition shows a lot of
promise.

</details>


### [196] [Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints](https://arxiv.org/abs/2510.14449)
*Jahidul Arafat,Fariha Tasmin,Md Kaosar Uddin,Sanjaya Poudel,Eftakhar Ahmed Arnob*

Main category: cs.LG

TL;DR: 论文研究了多类葡萄酒分类的模型准确性、特征维度和可解释性之间的权衡，通过实验验证了手动梯度下降和scikit-learn优化器的性能差异，并探讨了L1正则化对特征稀疏性的影响。


<details>
  <summary>Details</summary>
Motivation: 解决多类葡萄酒分类中模型准确性、特征维度和可解释性之间的权衡问题，为实际生产部署提供指导。

Method: 使用One-vs-Rest逻辑回归对UCI Wine数据集进行实验，比较手动梯度下降和scikit-learn优化器的性能，并分析L1正则化的效果。

Result: 手动梯度下降达到92.59%的测试准确率，scikit-learn提供24倍训练加速和98.15%准确率。L1正则化实现54-69%特征减少，仅损失4.63%准确率。

Conclusion: 提出了一种最优5特征子集，显著降低复杂度并保持高准确率，适合资源受限环境中的实时质量控制。

Abstract: Multi-class wine classification presents fundamental trade-offs between model
accuracy, feature dimensionality, and interpretability - critical factors for
production deployment in analytical chemistry. This paper presents a
comprehensive empirical study of One-vs-Rest logistic regression on the UCI
Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing
from-scratch gradient descent implementation against scikit-learn's optimized
solvers and quantifying L1 regularization effects on feature sparsity. Manual
gradient descent achieves 92.59 percent mean test accuracy with smooth
convergence, validating theoretical foundations, though scikit-learn provides
24x training speedup and 98.15 percent accuracy. Class-specific analysis
reveals distinct chemical signatures with heterogeneous patterns where color
intensity varies dramatically (0.31 to 16.50) across cultivars. L1
regularization produces 54-69 percent feature reduction with only 4.63 percent
accuracy decrease, demonstrating favorable interpretability-performance
trade-offs. We propose an optimal 5-feature subset achieving 62 percent
complexity reduction with estimated 92-94 percent accuracy, enabling
cost-effective deployment with 80 dollars savings per sample and 56 percent
time reduction. Statistical validation confirms robust generalization with
sub-2ms prediction latency suitable for real-time quality control. Our findings
provide actionable guidelines for practitioners balancing comprehensive
chemical analysis against targeted feature measurement in resource-constrained
environments.

</details>


### [197] [Coder as Editor: Code-driven Interpretable Molecular Optimization](https://arxiv.org/abs/2510.14455)
*Wenyu Zhu,Chengzhu Li,Xiaohe Tian,Yifan Wang,Yinjun Jia,Jianhui Wang,Bowen Gao,Ya-Qin Zhang,Wei-Ying Ma,Yanyan Lan*

Main category: cs.LG

TL;DR: MECo框架通过将编辑意图转化为可执行代码，显著提高了分子优化的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在分子优化中难以忠实执行编辑意图的问题，特别是在非直观表示（如SMILES）上。

Method: 采用级联框架：首先生成人类可理解的编辑意图，再通过代码生成将其转化为可执行的结构编辑。

Result: 在重现化学反应的编辑中达到98%的准确率，下游优化任务中一致性提升38-86个百分点至90%以上。

Conclusion: MECo通过对齐意图与执行，实现了可控、可解释的分子设计，为药物发现中的人机协作奠定了基础。

Abstract: Molecular optimization is a central task in drug discovery that requires
precise structural reasoning and domain knowledge. While large language models
(LLMs) have shown promise in generating high-level editing intentions in
natural language, they often struggle to faithfully execute these
modifications-particularly when operating on non-intuitive representations like
SMILES. We introduce MECo, a framework that bridges reasoning and execution by
translating editing actions into executable code. MECo reformulates molecular
optimization for LLMs as a cascaded framework: generating human-interpretable
editing intentions from a molecule and property goal, followed by translating
those intentions into executable structural edits via code generation. Our
approach achieves over 98% accuracy in reproducing held-out realistic edits
derived from chemical reactions and target-specific compound pairs. On
downstream optimization benchmarks spanning physicochemical properties and
target activities, MECo substantially improves consistency by 38-86 percentage
points to 90%+ and achieves higher success rates over SMILES-based baselines
while preserving structural similarity. By aligning intention with execution,
MECo enables consistent, controllable and interpretable molecular design,
laying the foundation for high-fidelity feedback loops and collaborative
human-AI workflows in drug discovery.

</details>


### [198] [Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning](https://arxiv.org/abs/2510.14459)
*Ling Zhang,Xianliang Yang,Juwon Yu,Park Cheonyoung,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 提出了一种基于上下文近似（ICA）的高效数据选择和重加权框架，用于优化语言模型的微调过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或昂贵的重训练，缺乏系统且高效的数据选择方式。

Method: 通过ICA估计候选样本对模型的影响，无需参考模型或额外微调，动态调整梯度更新权重。

Result: 在多种任务和数据集上，ICA显著提升了模型对齐性能，且计算开销低。

Conclusion: ICA是一种高效的数据选择方法，但需进一步研究快速漂移的在线更新问题。

Abstract: Fine-tuning large pretrained language models is a common approach for
aligning them with human preferences, but noisy or off-target examples can
dilute supervision. While small, well-chosen datasets often match the
performance of much larger ones, systematic and efficient ways to identify
high-value training data remain underexplored. Many current methods rely on
heuristics or expensive retraining. We present a theoretically grounded,
resource-efficient framework for data selection and reweighting. At its core is
an In-Context Approximation (ICA) that estimates the holdout loss a model would
incur after training on a candidate example by conditioning on a small, curated
holdout set in context. ICA requires no reference model and no additional
finetuning. Under a local linearization, ICA is equivalent to a first-order
update toward the holdout optimum, motivating its use as a proxy for data
value. We derive per-example weights from ICA scores, dynamically reweighting
gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and
over diverse backbones and datasets, ICA-based reweighting consistently
improves model alignment with minimal overhead. We analyze sensitivity to score
update frequency and the choice of $k$ holdout examples for in-context
demonstrations, and note limitations for rapidly drifting on-policy updates,
highlighting directions for future work. Code and prompts will be released.

</details>


### [199] [From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?](https://arxiv.org/abs/2510.14488)
*Sujai Hiremath,Dominik Janzing,Philipp Faller,Patrick Blöbaum,Elke Kirschbaum,Shiva Prasad Kasiviswanathan,Kyra Gan*

Main category: cs.LG

TL;DR: G2G框架通过专家猜测引导统计测试序列，提升因果发现算法在小样本下的性能，同时保持统计一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖完美预测或不确定性估计，实际应用中不可靠，G2G旨在通过专家输入提升性能。

Method: 提出Guess2Graph框架，开发PC-Guess和gPC-Guess两种实现，后者通过增强学习更好利用高质量专家输入。

Result: 理论证明两种方法在专家错误时仍保持正确性，gPC-Guess在专家优于随机时表现更优；实验显示性能随专家准确性单调提升。

Conclusion: G2G框架有效结合专家知识，提升因果发现算法的实际性能，尤其gPC-Guess在高质量专家输入下表现突出。

Abstract: Causal discovery algorithms often perform poorly with limited samples. While
integrating expert knowledge (including from LLMs) as constraints promises to
improve performance, guarantees for existing methods require perfect
predictions or uncertainty estimates, making them unreliable for practical use.
We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide
the sequence of statistical tests rather than replacing them. This maintains
statistical consistency while enabling performance improvements. We develop two
instantiations of G2G: PC-Guess, which augments the PC algorithm, and
gPC-Guess, a learning-augmented variant designed to better leverage
high-quality expert input. Theoretically, both preserve correctness regardless
of expert error, with gPC-Guess provably outperforming its non-augmented
counterpart in finite samples when experts are "better than random."
Empirically, both show monotonic improvement with expert accuracy, with
gPC-Guess achieving significantly stronger gains.

</details>


### [200] [Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals](https://arxiv.org/abs/2510.14503)
*Andrejs Sorstkins,Omer Tariq,Muhammad Bilal*

Main category: cs.LG

TL;DR: 提出可逆学习框架，提升基于价值的强化学习代理的鲁棒性和效率，解决价值高估和部分不可逆环境中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 针对强化学习中价值高估和不可逆环境导致的性能下降和安全性问题，提出一种可逆学习框架。

Method: 框架包含两个核心机制：1) 状态动作可逆性度量Phi；2) 选择性状态回滚操作。Phi动态调整TD更新中的惩罚项，回滚机制在低回报时返回前一状态。

Result: 在CliffWalking和Taxi环境中，分别减少99.8%的灾难性跌落和99.9%的非法动作，平均回报提升55%和65.7%。

Conclusion: 回滚机制是安全和性能提升的关键，为安全可靠的序列决策提供了有效方法。

Abstract: This paper proposes a reversible learning framework to improve the robustness
and efficiency of value based Reinforcement Learning agents, addressing
vulnerability to value overestimation and instability in partially irreversible
environments. The framework has two complementary core mechanisms: an
empirically derived transition reversibility measure called Phi of s and a, and
a selective state rollback operation. We introduce an online per state action
estimator called Phi that quantifies the likelihood of returning to a prior
state within a fixed horizon K. This measure is used to adjust the penalty term
during temporal difference updates dynamically, integrating reversibility
awareness directly into the value function. The system also includes a
selective rollback operator. When an action yields an expected return markedly
lower than its instantaneous estimated value and violates a predefined
threshold, the agent is penalized and returns to the preceding state rather
than progressing. This interrupts sub optimal high risk trajectories and avoids
catastrophic steps. By combining reversibility aware evaluation with targeted
rollback, the method improves safety, performance, and stability. In the
CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8
percent and yielded a 55 percent increase in mean episode return. In the Taxi
v3 domain, it suppressed illegal actions by greater than or equal to 99.9
percent and achieved a 65.7 percent improvement in cumulative reward, while
also sharply reducing reward variance in both environments. Ablation studies
confirm that the rollback mechanism is the critical component underlying these
safety and performance gains, marking a robust step toward safe and reliable
sequential decision making.

</details>


### [201] [Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective](https://arxiv.org/abs/2510.14510)
*Xingjian Wu,Xiangfei Qiu,Hanyin Cheng,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 论文提出了一种选择性表示空间（SRS）模块，通过可学习的选择性分块和动态重组技术，灵活选择时间序列中最具信息量的分块，提升了基于分块模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统分块方法将时间序列划分为相邻分块，导致表示空间固定，信息表达不足。

Method: 提出SRS模块，结合选择性分块和动态重组技术，自适应选择和重组分块。

Result: SRSNet在多个领域的真实数据集上实现了最先进的性能，并能提升现有分块模型的性能。

Conclusion: SRS模块是一种即插即用的创新方法，显著提升了时间序列预测的表现。

Abstract: Time Series Forecasting has made significant progress with the help of
Patching technique, which partitions time series into multiple patches to
effectively retain contextual semantic information into a representation space
beneficial for modeling long-term dependencies. However, conventional patching
partitions a time series into adjacent patches, which causes a fixed
representation space, thus resulting in insufficiently expressful
representations. In this paper, we pioneer the exploration of constructing a
selective representation space to flexibly include the most informative patches
for forecasting. Specifically, we propose the Selective Representation Space
(SRS) module, which utilizes the learnable Selective Patching and Dynamic
Reassembly techniques to adaptively select and shuffle the patches from the
contextual time series, aiming at fully exploiting the information of
contextual time series to enhance the forecasting performance of patch-based
models. To demonstrate the effectiveness of SRS module, we propose a simple yet
effective SRSNet consisting of SRS and an MLP head, which achieves
state-of-the-art performance on real-world datasets from multiple domains.
Furthermore, as a novel plugin-and-play module, SRS can also enhance the
performance of existing patch-based models. The resources are available at
https://github.com/decisionintelligence/SRSNet.

</details>


### [202] [On the Identifiability of Tensor Ranks via Prior Predictive Matching](https://arxiv.org/abs/2510.14523)
*Eliezer da Silva,Arto Klami,Diego Mesquita,Iñigo Urteaga*

Main category: cs.LG

TL;DR: 论文提出了一种基于先验预测矩匹配的严格方法，用于确定概率张量模型中的秩可识别性，并推导了可识别模型的显式闭式秩估计器。


<details>
  <summary>Details</summary>
Motivation: 解决张量分解中潜在维度（秩）选择的挑战，避免依赖启发式方法。

Method: 将矩匹配条件转化为对数线性方程组，通过解方程组判断秩的可识别性。

Result: 证明了PARAFAC/CP、Tensor Train和Tensor Ring模型的秩可识别，而Tucker模型不可识别；并推导了显式秩估计器。

Conclusion: 该方法为秩选择提供了理论支持，并通过实验验证了其有效性和鲁棒性。

Abstract: Selecting the latent dimensions (ranks) in tensor factorization is a central
challenge that often relies on heuristic methods. This paper introduces a
rigorous approach to determine rank identifiability in probabilistic tensor
models, based on prior predictive moment matching. We transform a set of moment
matching conditions into a log-linear system of equations in terms of marginal
moments, prior hyperparameters, and ranks; establishing an equivalence between
rank identifiability and the solvability of such system. We apply this
framework to four foundational tensor-models, demonstrating that the linear
structure of the PARAFAC/CP model, the chain structure of the Tensor Train
model, and the closed-loop structure of the Tensor Ring model yield solvable
systems, making their ranks identifiable. In contrast, we prove that the
symmetric topology of the Tucker model leads to an underdetermined system,
rendering the ranks unidentifiable by this method. For the identifiable models,
we derive explicit closed-form rank estimators based on the moments of observed
data only. We empirically validate these estimators and evaluate the robustness
of the proposal.

</details>


### [203] [Agentic Entropy-Balanced Policy Optimization](https://arxiv.org/abs/2510.14545)
*Guanting Dong,Licheng Bao,Zhongyuan Wang,Kangzhi Zhao,Xiaoxi Li,Jiajie Jin,Jinghan Yang,Hangyu Mao,Fuzheng Zhang,Kun Gai,Guorui Zhou,Yutao Zhu,Ji-Rong Wen,Zhicheng Dou*

Main category: cs.LG

TL;DR: AEPO是一种新的强化学习算法，通过平衡熵信号解决了训练崩溃问题，显著提升了多轮长时程工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 主流强化学习算法过度依赖熵信号，导致训练崩溃，因此需要一种平衡熵的方法。

Method: AEPO包含动态熵平衡机制和熵平衡策略优化，通过预监控熵和梯度操作实现平衡。

Result: 在14个数据集上优于7种主流算法，Qwen3-14B结合AEPO在多个任务中表现优异。

Conclusion: AEPO提升了采样多样性并保持策略熵稳定，适用于可扩展的Web代理训练。

Abstract: Recently, Agentic Reinforcement Learning (Agentic RL) has made significant
progress in incentivizing the multi-turn, long-horizon tool-use capabilities of
web agents. While mainstream agentic RL algorithms autonomously explore
high-uncertainty tool-call steps under the guidance of entropy, excessive
reliance on entropy signals can impose further constraints, leading to the
training collapse. In this paper, we delve into the challenges caused by
entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an
agentic RL algorithm designed to balance entropy in both the rollout and policy
update phases. AEPO comprises two core components: (1) a dynamic
entropy-balanced rollout mechanism that adaptively allocate global and branch
sampling budget through entropy pre-monitoring, while imposing a branch penalty
on consecutive high-entropy tool-call steps to prevent over-branching issues;
and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient
operation into the high-entropy clipping term to preserve and properly rescale
gradients on high-entropy tokens, while incorporating entropy-aware advantage
estimation to prioritize learning on high-uncertainty tokens. Results across 14
challenging datasets show that AEPO consistently outperforms 7 mainstream RL
algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive
results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker
for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on
WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout
sampling diversity while maintaining stable policy entropy, facilitating
scalable web agent training.

</details>


### [204] [MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving](https://arxiv.org/abs/2510.14557)
*Jungi Lee,Junyong Park,Soohyun Cha,Jaehoon Cho,Jaewoong Sim*

Main category: cs.LG

TL;DR: MX+是一种低成本、非侵入性的扩展方法，用于改进低精度BFP格式，显著提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有超低比特BFP格式因块内异常值问题导致语言模型性能不佳，需一种更高效的解决方案。

Method: 提出MX+，通过重新利用异常值的指数字段作为扩展尾数，提高异常值元素的精度。

Result: MX+在4位MX格式（MXFP4）基础上显著提升模型性能，存储开销和速度损失可忽略。

Conclusion: MX+为高效LLM推理提供了优于MXFP4或MXFP6的替代方案。

Abstract: Reduced-precision data formats are crucial for cost-effective serving of
large language models (LLMs). While numerous reduced-precision formats have
been introduced thus far, they often require intrusive modifications to the
software frameworks or are rather unconventional for widespread adoption across
hardware vendors. In this paper, we instead focus on recent industry-driven
variants of block floating-point (BFP) formats and conduct a comprehensive
analysis to push their limits for efficient LLM serving. Our analysis shows
that existing ultra low-bit BFP variants struggle to provide reasonable
language model performance due to outlier values in blocks. To address the
outliers with BFPs, we propose MX+, a cost-effective and non-intrusive
extension designed for seamless integration into the microscaling (MX) formats.
MX+ builds on the key insight that the outlier does not need to use its
exponent field in the element data type, which allows us to repurpose the
exponent field as an extended mantissa to increase the precision of the outlier
element. Our evaluation shows that MX+ achieves significantly higher model
performance compared to the 4-bit MX format (MXFP4) with negligible storage
overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6
for efficient LLM inference.

</details>


### [205] [Redundancy-Aware Test-Time Graph Out-of-Distribution Detection](https://arxiv.org/abs/2510.14562)
*Yue Hou,He Zhu,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: RedOUT是一种无监督框架，通过结构熵减少冗余，提升图分类中的OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 训练与测试数据分布不一致导致OOD样本预测不准确，现有方法因结构冗余而性能受限。

Method: 提出ReGIB，分解目标为关键信息和冗余，通过最小化结构熵优化。

Result: 在真实数据集上表现优异，平均提升6.7%，在ClinTox/LIPO数据集上超越最佳竞争对手17.3%。

Conclusion: RedOUT通过减少冗余显著提升了OOD检测性能。

Abstract: Distributional discrepancy between training and test data can lead models to
make inaccurate predictions when encountering out-of-distribution (OOD) samples
in real-world applications. Although existing graph OOD detection methods
leverage data-centric techniques to extract effective representations, their
performance remains compromised by structural redundancy that induces semantic
shifts. To address this dilemma, we propose RedOUT, an unsupervised framework
that integrates structural entropy into test-time OOD detection for graph
classification. Concretely, we introduce the Redundancy-aware Graph Information
Bottleneck (ReGIB) and decompose the objective into essential information and
irrelevant redundancy. By minimizing structural entropy, the decoupled
redundancy is reduced, and theoretically grounded upper and lower bounds are
proposed for optimization. Extensive experiments on real-world datasets
demonstrate the superior performance of RedOUT on OOD detection. Specifically,
our method achieves an average improvement of 6.7%, significantly surpassing
the best competitor by 17.3% on the ClinTox/LIPO dataset pair.

</details>


### [206] [State-Space Models for Tabular Prior-Data Fitted Networks](https://arxiv.org/abs/2510.14573)
*Felix Koch,Marcel Wever,Fabian Raisch,Benjamin Tischler*

Main category: cs.LG

TL;DR: 探索Hydra（双向线性时间结构化状态空间模型）作为TabPFN中Transformer的替代方案，以减少计算复杂度并解决输入顺序敏感性问题。


<details>
  <summary>Details</summary>
Motivation: Transformer在表格数据中的二次复杂度问题促使寻找更高效的序列模型。

Method: 使用Hydra模型，研究其双向线性时间特性如何减少对输入顺序的依赖。

Result: 实验表明，Hydra在保持效率的同时，预测性能与原始TabPFN模型相当。

Conclusion: Hydra是一种有效的替代方案，能够减少顺序依赖性并保持高性能。

Abstract: Recent advancements in foundation models for tabular data, such as TabPFN,
demonstrated that pretrained Transformer architectures can approximate Bayesian
inference with high predictive performance. However, Transformers suffer from
quadratic complexity with respect to sequence length, motivating the
exploration of more efficient sequence models. In this work, we investigate the
potential of using Hydra, a bidirectional linear-time structured state space
model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies
in SSM's inherent sensitivity to the order of input tokens - an undesirable
property for tabular datasets where the row order is semantically meaningless.
We investigate to what extent a bidirectional approach can preserve efficiency
and enable symmetric context aggregation. Our experiments show that this
approach reduces the order-dependence, achieving predictive performance
competitive to the original TabPFN model.

</details>


### [207] [Selective Labeling with False Discovery Rate Control](https://arxiv.org/abs/2510.14581)
*Huipeng Huang,Wenbo Liao,Huajun Xi,Hao Zeng,Mengchen Zhao,Hongxin Wei*

Main category: cs.LG

TL;DR: 提出了一种名为Conformal Labeling的新方法，通过控制假发现率（FDR）来确保AI预测标签的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对AI标签质量的理论保证，导致AI标记子集中的标签错误率较高。

Method: 通过构建每个测试实例的conformal p值，选择p值低于数据相关阈值的实例，确保AI预测的可信度。

Result: 实验表明，该方法在多种任务中实现了严格的FDR控制和高效率。

Conclusion: Conformal Labeling能够有效控制FDR，确保AI标签的质量。

Abstract: Obtaining high-quality labels for large datasets is expensive, requiring
massive annotations from human experts. While AI models offer a cost-effective
alternative by predicting labels, their label quality is compromised by the
unavoidable labeling errors. Existing methods mitigate this issue through
selective labeling, where AI labels a subset and human labels the remainder.
However, these methods lack theoretical guarantees on the quality of
AI-assigned labels, often resulting in unacceptably high labeling error within
the AI-labeled subset. To address this, we introduce \textbf{Conformal
Labeling}, a novel method to identify instances where AI predictions can be
provably trusted. This is achieved by controlling the false discovery rate
(FDR), the proportion of incorrect labels within the selected subset. In
particular, we construct a conformal $p$-value for each test instance by
comparing AI models' predicted confidence to those of calibration instances
mislabeled by AI models. Then, we select test instances whose $p$-values are
below a data-dependent threshold, certifying AI models' predictions as
trustworthy. We provide theoretical guarantees that Conformal Labeling controls
the FDR below the nominal level, ensuring that a predefined fraction of
AI-assigned labels is correct on average. Extensive experiments demonstrate
that our method achieves tight FDR control with high power across various
tasks, including image and text labeling, and LLM QA.

</details>


### [208] [Matcha: Multi-Stage Riemannian Flow Matching for Accurate and Physically Valid Molecular Docking](https://arxiv.org/abs/2510.14586)
*Daria Frolova,Talgat Daulbaev,Egor Sevryugov,Sergei A. Nikolenko,Dmitry N. Ivankov,Ivan Oseledets,Marina A. Pak*

Main category: cs.LG

TL;DR: Matcha是一种新型分子对接流程，结合多阶段流匹配、学习评分和物理有效性过滤，显著提高了蛋白质-配体结合姿势预测的速度、准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在速度、准确性和物理合理性之间难以平衡，影响了结构药物设计的效率。

Method: Matcha采用三阶段流匹配模型，分别在$\mathbb{R}^3$、$\mathrm{SO}(3)$和$\mathrm{SO}(2)$几何空间上操作，结合学习评分和物理有效性过滤。

Result: 在Astex和PDBbind测试集上，Matcha表现出更高的对接成功率和物理合理性，速度比现代大规模共折叠模型快25倍。

Conclusion: Matcha为蛋白质-配体对接提供了一种高效、准确且物理合理的方法，推动了结构药物设计的发展。

Abstract: Accurate prediction of protein-ligand binding poses is crucial for
structure-based drug design, yet existing methods struggle to balance speed,
accuracy, and physical plausibility. We introduce Matcha, a novel molecular
docking pipeline that combines multi-stage flow matching with learned scoring
and physical validity filtering. Our approach consists of three sequential
stages applied consecutively to refine docking predictions, each implemented as
a flow matching model operating on appropriate geometric spaces
($\mathbb{R}^3$, $\mathrm{SO}(3)$, and $\mathrm{SO}(2)$). We enhance the
prediction quality through a dedicated scoring model and apply unsupervised
physical validity filters to eliminate unrealistic poses. Compared to various
approaches, Matcha demonstrates superior performance on Astex and PDBbind test
sets in terms of docking success rate and physical plausibility. Moreover, our
method works approximately 25 times faster than modern large-scale co-folding
models. The model weights and inference code to reproduce our results are
available at https://github.com/LigandPro/Matcha.

</details>


### [209] [Multimodal RAG for Unstructured Data:Leveraging Modality-Aware Knowledge Graphs with Hybrid Retrieval](https://arxiv.org/abs/2510.14592)
*Rashmi R,Vidyadhar Upadhya*

Main category: cs.LG

TL;DR: MAHA是一种多模态感知的混合检索架构，用于多模态问答和推理，通过结合密集向量检索和结构化图遍历，显著提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的RAG系统在处理多模态非结构化文档时效果有限，MAHA旨在解决这一问题。

Method: MAHA结合密集向量检索和结构化图遍历，利用多模态知识图编码跨模态语义和关系。

Result: 在多个基准数据集上，MAHA显著优于基线方法，ROUGE-L得分为0.486，实现了完整的模态覆盖。

Conclusion: MAHA为RAG系统提供了一种可扩展且可解释的检索框架，支持多模态感知推理。

Abstract: Current Retrieval-Augmented Generation (RAG) systems primarily operate on
unimodal textual data, limiting their effectiveness on unstructured multimodal
documents. Such documents often combine text, images, tables, equations, and
graphs, each contributing unique information. In this work, we present a
Modality-Aware Hybrid retrieval Architecture (MAHA), designed specifically for
multimodal question answering with reasoning through a modality-aware knowledge
graph. MAHA integrates dense vector retrieval with structured graph traversal,
where the knowledge graph encodes cross-modal semantics and relationships. This
design enables both semantically rich and context-aware retrieval across
diverse modalities. Evaluations on multiple benchmark datasets demonstrate that
MAHA substantially outperforms baseline methods, achieving a ROUGE-L score of
0.486, providing complete modality coverage. These results highlight MAHA's
ability to combine embeddings with explicit document structure, enabling
effective multimodal retrieval. Our work establishes a scalable and
interpretable retrieval framework that advances RAG systems by enabling
modality-aware reasoning over unstructured multimodal data.

</details>


### [210] [First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training](https://arxiv.org/abs/2510.14614)
*Gyudong Kim,Hyukju Na,Jin Hyeon Kim,Hyunsung Jang,Jaemin Park,Jaegi Hwang,Namkoo Ha,Seungryong Kim,Young Geun Kim*

Main category: cs.LG

TL;DR: FAL是一种高效的Transformer架构，通过绕过MHA-MLP连接减少通信开销，提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer设计在Tensor Parallelism中存在显著的通信开销，尤其是MHA-MLP连接需要all-reduce通信。研究发现这些连接可以绕过以提高效率。

Method: 提出FAL架构，将第一层MHA输出重定向到后续层的MLP输入，消除每块的MHA-MLP连接。FAL+进一步归一化第一层注意力输出以增强MLP输入。

Result: FAL减少多GPU训练时间达44%，单GPU吞吐量提升1.18倍，困惑度优于基线GPT。FAL+在不增加训练时间的情况下进一步降低困惑度。

Conclusion: FAL和FAL+通过优化通信和并行执行显著提升了Transformer的训练效率和模型质量。

Abstract: As training billion-scale transformers becomes increasingly common, employing
multiple distributed GPUs along with parallel training methods has become a
standard practice. However, existing transformer designs suffer from
significant communication overhead, especially in Tensor Parallelism (TP),
where each block's MHA-MLP connection requires an all-reduce communication.
Through our investigation, we show that the MHA-MLP connections can be bypassed
for efficiency, while the attention output of the first layer can serve as an
alternative signal for the bypassed connection. Motivated by the observations,
we propose FAL (First Attentions Last), an efficient transformer architecture
that redirects the first MHA output to the MLP inputs of the following layers,
eliminating the per-block MHA-MLP connections. This removes the all-reduce
communication and enables parallel execution of MHA and MLP on a single GPU. We
also introduce FAL+, which adds the normalized first attention output to the
MHA outputs of the following layers to augment the MLP input for the model
quality. Our evaluation shows that FAL reduces multi-GPU training time by up to
44%, improves single-GPU throughput by up to 1.18x, and achieves better
perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity
without increasing the training time than the baseline.

</details>


### [211] [LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching](https://arxiv.org/abs/2510.14623)
*Zhuo Cao,Xuan Zhao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: LeapFactual是一种基于条件流匹配的新型反事实解释算法，解决了现有方法的局限性，如梯度消失和不连续潜在空间。


<details>
  <summary>Details</summary>
Motivation: 高风险的领域（如医疗和科研）需要准确且可解释的模型，现有反事实解释方法存在局限性。

Method: LeapFactual采用模型无关的方法，基于条件流匹配生成可靠的反事实解释。

Result: 实验表明，LeapFactual生成的反事实解释准确且分布内，可用于增强模型训练。

Conclusion: LeapFactual广泛适用，提升了科学知识发现和非专家可解释性。

Abstract: The growing integration of machine learning (ML) and artificial intelligence
(AI) models into high-stakes domains such as healthcare and scientific research
calls for models that are not only accurate but also interpretable. Among the
existing explainable methods, counterfactual explanations offer
interpretability by identifying minimal changes to inputs that would alter a
model's prediction, thus providing deeper insights. However, current
counterfactual generation methods suffer from critical limitations, including
gradient vanishing, discontinuous latent spaces, and an overreliance on the
alignment between learned and true decision boundaries. To overcome these
limitations, we propose LeapFactual, a novel counterfactual explanation
algorithm based on conditional flow matching. LeapFactual generates reliable
and informative counterfactuals, even when true and learned decision boundaries
diverge. Following a model-agnostic approach, LeapFactual is not limited to
models with differentiable loss functions. It can even handle human-in-the-loop
systems, expanding the scope of counterfactual explanations to domains that
require the participation of human annotators, such as citizen science. We
provide extensive experiments on benchmark and real-world datasets showing that
LeapFactual generates accurate and in-distribution counterfactual explanations
that offer actionable insights. We observe, for instance, that our reliable
counterfactual samples with labels aligning to ground truth can be beneficially
used as new training data to enhance the model. The proposed method is broadly
applicable and enhances both scientific knowledge discovery and non-expert
interpretability.

</details>


### [212] [Galaxy Morphology Classification with Counterfactual Explanation](https://arxiv.org/abs/2510.14655)
*Zhuo Cao,Lena Krieger,Hanno Scharr,Ira Assent*

Main category: cs.LG

TL;DR: 论文提出了一种结合可逆流的编码器-解码器架构，用于星系形态分类，既提高了预测性能，又提供了反事实解释。


<details>
  <summary>Details</summary>
Motivation: 星系形态分类对研究星系演化至关重要，但传统机器学习方法缺乏可解释性。

Method: 扩展了经典的编码器-解码器架构，引入了可逆流，以生成反事实解释。

Result: 模型不仅预测性能良好，还能提供决策过程的额外信息。

Conclusion: 该方法在保持高性能的同时增强了模型的可解释性。

Abstract: Galaxy morphologies play an essential role in the study of the evolution of
galaxies. The determination of morphologies is laborious for a large amount of
data giving rise to machine learning-based approaches. Unfortunately, most of
these approaches offer no insight into how the model works and make the results
difficult to understand and explain. We here propose to extend a classical
encoder-decoder architecture with invertible flow, allowing us to not only
obtain a good predictive performance but also provide additional information
about the decision process with counterfactual explanations.

</details>


### [213] [Geometric Moment Alignment for Domain Adaptation via Siegel Embeddings](https://arxiv.org/abs/2510.14666)
*Shayan Gharib,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 提出了一种基于黎曼距离的无监督域适应方法，通过Siegel嵌入将一阶和二阶矩表示为SPD矩阵，实现更准确的跨域对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在无监督域适应中低阶统计矩对齐时使用临时相似性度量的问题。

Method: 利用黎曼距离对齐源和目标分布的SPD矩阵，保留均值和协方差结构。

Result: 在图像去噪和分类任务中验证了方法的有效性。

Conclusion: 提出的几何对齐方法在跨域比较中提供了更准确的度量。

Abstract: We address the problem of distribution shift in unsupervised domain
adaptation with a moment-matching approach. Existing methods typically align
low-order statistical moments of the source and target distributions in an
embedding space using ad-hoc similarity measures. We propose a principled
alternative that instead leverages the intrinsic geometry of these
distributions by adopting a Riemannian distance for this alignment. Our key
novelty lies in expressing the first- and second-order moments as a single
symmetric positive definite (SPD) matrix through Siegel embeddings. This
enables simultaneous adaptation of both moments using the natural geometric
distance on the shared manifold of SPD matrices, preserving the mean and
covariance structure of the source and target distributions and yielding a more
faithful metric for cross-domain comparison. We connect the Riemannian manifold
distance to the target-domain error bound, and validate the method on image
denoising and image classification benchmarks. Our code is publicly available
at https://github.com/shayangharib/GeoAdapt.

</details>


### [214] [Online Reliable Anomaly Detection via Neuromorphic Sensing and Communications](https://arxiv.org/abs/2510.14688)
*Junya Shiraishi,Jiechen Chen,Osvaldo Simeone,Petar Popovski*

Main category: cs.LG

TL;DR: 提出了一种基于神经形态无线传感器网络的低功耗在线异常检测框架，适用于脑机接口和远程环境监测等场景。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的无线传感器网络中实现高效、低功耗的异常检测问题，同时严格控制误报率。

Method: 采用事件驱动的神经形态传感器节点，结合脉冲无线电传输和在线假设检验方法，动态优化传感器查询策略。

Result: 实验表明，该方法在严格误报率要求下能可靠检测异常，同时高效调度传感器通信并实现低检测延迟。

Conclusion: 该框架为低功耗、高可靠性的异常检测提供了有效解决方案，适用于多种实际应用场景。

Abstract: This paper proposes a low-power online anomaly detection framework based on
neuromorphic wireless sensor networks, encompassing possible use cases such as
brain-machine interfaces and remote environmental monitoring. In the considered
system, a central reader node actively queries a subset of neuromorphic sensor
nodes (neuro-SNs) at each time frame. The neuromorphic sensors are
event-driven, producing spikes in correspondence to relevant changes in the
monitored system. The queried neuro-SNs respond to the reader with impulse
radio (IR) transmissions that directly encode the sensed local events. The
reader processes these event-driven signals to determine whether the monitored
environment is in a normal or anomalous state, while rigorously controlling the
false discovery rate (FDR) of detections below a predefined threshold. The
proposed approach employs an online hypothesis testing method with e-values to
maintain FDR control without requiring knowledge of the anomaly rate, and it
dynamically optimizes the sensor querying strategy by casting it as a best-arm
identification problem in a multi-armed bandit framework. Extensive performance
evaluation demonstrates that the proposed method can reliably detect anomalies
under stringent FDR requirements, while efficiently scheduling sensor
communications and achieving low detection latency.

</details>


### [215] [FedPPA: Progressive Parameter Alignment for Personalized Federated Learning](https://arxiv.org/abs/2510.14698)
*Maulidi Adi Prasetia,Muhamad Risqi U. Saputra,Guntur Dharma Putra*

Main category: cs.LG

TL;DR: FedPPA是一种新的个性化联邦学习方法，通过渐进式参数对齐和熵加权平均，解决了模型和数据异构性问题，提升了非IID数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，联邦学习的客户端通常具有异构计算资源和数据分布（非IID），现有PFL方法未能同时解决模型和数据异构性问题。

Method: 提出FedPPA方法，渐进对齐客户端与全局模型的公共层权重，并引入熵加权平均以优化全局模型性能。

Result: 在MNIST、FMNIST和CIFAR-10数据集上，FedPPA表现优于现有FL算法，个性化适应能力更强。

Conclusion: FedPPA有效解决了异构性问题，提升了联邦学习在非IID数据下的鲁棒性和性能。

Abstract: Federated Learning (FL) is designed as a decentralized, privacy-preserving
machine learning paradigm that enables multiple clients to collaboratively
train a model without sharing their data. In real-world scenarios, however,
clients often have heterogeneous computational resources and hold
non-independent and identically distributed data (non-IID), which poses
significant challenges during training. Personalized Federated Learning (PFL)
has emerged to address these issues by customizing models for each client based
on their unique data distribution. Despite its potential, existing PFL
approaches typically overlook the coexistence of model and data heterogeneity
arising from clients with diverse computational capabilities. To overcome this
limitation, we propose a novel method, called Progressive Parameter Alignment
(FedPPA), which progressively aligns the weights of common layers across
clients with the global model's weights. Our approach not only mitigates
inconsistencies between global and local models during client updates, but also
preserves client's local knowledge, thereby enhancing personalization
robustness in non-IID settings. To further enhance the global model performance
while retaining strong personalization, we also integrate entropy-based
weighted averaging into the FedPPA framework. Experiments on three image
classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate
that FedPPA consistently outperforms existing FL algorithms, achieving superior
performance in personalized adaptation.

</details>


### [216] [Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling](https://arxiv.org/abs/2510.14717)
*Alexandru Meterez,Depen Morwani,Jingfeng Wu,Costin-Andrei Oncescu,Cengiz Pehlevan,Sham Kakade*

Main category: cs.LG

TL;DR: Seesaw是一种批处理大小调度框架，通过调整学习率和批处理大小来加速训练，同时保持损失动态。


<details>
  <summary>Details</summary>
Motivation: 研究如何在大语言模型预训练中通过批处理大小调度（batch ramp）来加速训练，尤其是针对自适应优化器（如Adam）的优化策略。

Method: 提出Seesaw框架，在标准调度器降低学习率时，将学习率乘以$1/\sqrt{2}$并加倍批处理大小，以减少串行步骤。

Result: 在150M/300M/600M参数模型上，Seesaw在相同FLOPs下与余弦衰减性能相当，同时减少约36%的墙钟时间。

Conclusion: Seesaw提供了一种理论支持且高效的批处理大小调度方法，接近理论极限，适用于大规模语言模型训练。

Abstract: Increasing the batch size during training -- a ''batch ramp'' -- is a
promising strategy to accelerate large language model pretraining. While for
SGD, doubling the batch size can be equivalent to halving the learning rate,
the optimal strategy for adaptive optimizers like Adam is less clear. As a
result, any batch-ramp scheduling, if used at all, is typically tuned
heuristically. This work develops a principled framework for batch-size
scheduling and introduces Seesaw: whenever a standard scheduler would halve the
learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the
batch size, preserving loss dynamics while reducing serial steps.
Theoretically, we provide, to our knowledge, the first finite-sample proof of
equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy
linear regression, and we extend this equivalence to normalized SGD, a
tractable proxy for Adam, under a variance-dominated regime observed in
practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla
scale using a constant (critical) batch size, Seesaw matches cosine decay at
equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the
theoretical limit implied by our analysis.

</details>


### [217] [Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References](https://arxiv.org/abs/2510.14719)
*Hongzheng Chen,Bin Fan,Alexander Collins,Bastian Hagedorn,Evghenii Gaburov,Masahiro Masuda,Matthew Brookhart,Chris Sullivan,Jason Knight,Zhiru Zhang,Vinod Grover*

Main category: cs.LG

TL;DR: Tawa是一个自动化编译器，通过新颖的IR抽象（异步引用aref）生成高性能的warp专用代码，显著简化GPU编程并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代GPU的SIMT编程模型与任务并行硬件不匹配，导致编程复杂且性能受限，需要手动管理低层通信和软件管道。

Method: 提出Tawa编译器，利用异步引用（aref）抽象，自动划分程序为生产者-消费者角色并管理数据流管道。

Result: 在NVIDIA H100 GPU上，Tawa比优化的cuBLAS GEMM内核快1.1倍，在注意力任务中比Triton快1.2倍，且性能接近手工优化的CUTLASS FlashAttention-3内核。

Conclusion: Tawa通过自动化编译器显著降低了GPU编程复杂性，同时实现了高性能，为开发者提供了高效的工具。

Abstract: Modern GPUs feature specialized hardware units that enable high-performance,
asynchronous dataflow execution. However, the conventional SIMT programming
model is fundamentally misaligned with this task-parallel hardware, creating a
significant programmability gap. While hardware-level warp specialization is
the key to unlocking peak performance, it forces developers to manually
orchestrate complex, low-level communication and software pipelines--a process
that is labor-intensive, error-prone, and unsustainable. To address this
challenge, we present Tawa, an automated compiler that systematically generates
high-performance, warp-specialized code from a high-level, tile-based program.
Central to our approach is a novel IR abstraction, asynchronous references
(aref), which expresses warp-level communication without exposing low-level
hardware details. Using this abstraction, Tawa automatically partitions
programs into producer-consumer roles and manages the intricate dataflow
pipeline, relieving developers of invasive kernel rewriting. Evaluation on
NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers
high hardware utilization, achieving up to 1.1$\times$ speedup over highly
optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains
1.2$\times$ speedup over Triton and matches the performance of the
hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming
effort.

</details>


### [218] [The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents](https://arxiv.org/abs/2510.14727)
*Antony Bartlett,Cynthia Liem,Annibale Panichella*

Main category: cs.LG

TL;DR: INDAGO-Nexus是一种多目标搜索方法，用于发现深度强化学习代理的多样化故障场景，比现有工具更有效。


<details>
  <summary>Details</summary>
Motivation: 现有工具如INDAGO仅专注于最大化故障数量，无法确保发现多样化场景或揭示不同错误类型。

Method: 引入INDAGO-Nexus，采用多目标进化算法，联合优化故障可能性和测试场景多样性。

Result: 在三种DRL代理上评估，INDAGO-Nexus平均发现更多独特故障，并减少故障时间。

Conclusion: INDAGO-Nexus显著提高了测试效果和效率。

Abstract: Testing deep reinforcement learning (DRL) agents in safety-critical domains
requires discovering diverse failure scenarios. Existing tools such as INDAGO
rely on single-objective optimization focused solely on maximizing failure
counts, but this does not ensure discovered scenarios are diverse or reveal
distinct error types. We introduce INDAGO-Nexus, a multi-objective search
approach that jointly optimizes for failure likelihood and test scenario
diversity using multi-objective evolutionary algorithms with multiple diversity
metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on
three DRL agents: humanoid walker, self-driving car, and parking agent. On
average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test
effectiveness) than INDAGO in the SDC and Parking scenarios, respectively,
while reducing time-to-failure by up to 67% across all agents.

</details>


### [219] [Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries](https://arxiv.org/abs/2510.14751)
*Divyat Mahajan,Sachin Goyal,Badr Youbi Idrissi,Mohammad Pezeshki,Ioannis Mitliagkas,David Lopez-Paz,Kartik Ahuja*

Main category: cs.LG

TL;DR: 未来摘要预测（FSP）通过预测长期未来的紧凑表示，改进了传统NTP和MTP在长程推理和生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: NTP和MTP在长程推理、规划和创意写作方面表现不佳，FSP旨在解决这些问题。

Method: 提出FSP，训练辅助头预测长期未来的紧凑表示，包括手工摘要和基于反向语言模型的学习摘要。

Result: 在3B和8B参数模型上，FSP在数学、推理和编码任务中优于NTP和MTP。

Conclusion: FSP为长程生成任务提供了更有效的训练方法。

Abstract: Next-token prediction (NTP) has driven the success of large language models
(LLMs), but it struggles with long-horizon reasoning, planning, and creative
writing, with these limitations largely attributed to teacher-forced training.
Multi-token prediction (MTP) partially mitigates these issues by predicting
several future tokens at once, but it mostly captures short-range dependencies
and offers limited improvement. We propose future summary prediction (FSP),
which trains an auxiliary head to predict a compact representation of the
long-term future, preserving information relevant for long-form generations. We
explore two variants of FSP: handcrafted summaries, for example, a bag of words
summary of the future of the sequence, and learned summaries, which use
embeddings produced by a reverse language model trained from right to left.
Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate
that FSP provides improvements over both NTP and MTP across math, reasoning,
and coding benchmarks.

</details>


### [220] [Causal Discovery for Linear DAGs with Dependent Latent Variables via Higher-order Cumulants](https://arxiv.org/abs/2510.14780)
*Ming Cai,Penggang Gao,Hisayuki Hara*

Main category: cs.LG

TL;DR: 提出了一种新算法，用于在存在潜在混杂因素的线性非高斯无环模型中识别因果有向无环图（DAG）。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设潜在混杂因素相互独立或无法正确处理观测变量间因果关系的模型。

Method: 利用观测数据的高阶累积量识别因果结构，允许潜在变量间、观测变量间及两者间的因果结构。

Result: 通过大量模拟和真实数据实验验证了算法的有效性和实用性。

Conclusion: 新算法在复杂因果结构中表现出色，具有实际应用价值。

Abstract: This paper addresses the problem of estimating causal directed acyclic graphs
in linear non-Gaussian acyclic models with latent confounders (LvLiNGAM).
Existing methods assume mutually independent latent confounders or cannot
properly handle models with causal relationships among observed variables.
  We propose a novel algorithm that identifies causal DAGs in LvLiNGAM,
allowing causal structures among latent variables, among observed variables,
and between the two. The proposed method leverages higher-order cumulants of
observed data to identify the causal structure. Extensive simulations and
experiments with real-world data demonstrate the validity and practical utility
of the proposed algorithm.

</details>


### [221] [Active Jammer Localization via Acquisition-Aware Path Planning](https://arxiv.org/abs/2510.14790)
*Luis González-Gudiño,Mariona Jaramillo-Civill,Pau Closas,Tales Imbiriba*

Main category: cs.LG

TL;DR: 提出了一种结合贝叶斯优化和路径规划的主动干扰源定位框架，通过自适应引导移动代理收集高效用信号强度测量，优于被动众包方法。


<details>
  <summary>Details</summary>
Motivation: 传统被动众包方法在定位干扰源时效率较低，无法适应城市障碍和移动约束，需要更高效的主动方法。

Method: 改进A*算法为A-UCB*，将采集值纳入轨迹成本，规划高采集路径。

Result: 在真实城市场景模拟中，该方法以较少测量实现高精度定位，性能稳定。

Conclusion: 该方法显著提升了干扰源定位效率，适用于复杂城市环境。

Abstract: We propose an active jammer localization framework that combines Bayesian
optimization with acquisition-aware path planning. Unlike passive crowdsourced
methods, our approach adaptively guides a mobile agent to collect high-utility
Received Signal Strength measurements while accounting for urban obstacles and
mobility constraints. For this, we modified the A* algorithm, A-UCB*, by
incorporating acquisition values into trajectory costs, leading to
high-acquisition planned paths. Simulations on realistic urban scenarios show
that the proposed method achieves accurate localization with fewer measurements
compared to uninformed baselines, demonstrating consistent performance under
different environments.

</details>


### [222] [Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning](https://arxiv.org/abs/2510.14810)
*Shikuang Deng,Jiayuan Zhang,Yuhang Wu,Ting Chen,Shi Gu*

Main category: cs.LG

TL;DR: SPHeRe是一种新型无监督学习方法，通过结合正交性和结构信息保留，解决了传统Hebbian学习在机器学习中存在的问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Hebbian学习在机器学习中存在连接更新无约束和缺乏反馈调节的问题，限制了其在复杂任务中的扩展性。

Method: SPHeRe通过局部辅助非线性块整合正交性和结构信息保留，利用轻量级投影作为反馈调节，并通过正交约束限制更新幅度。

Result: 在CIFAR-10、CIFAR-100和Tiny-ImageNet等基准测试中，SPHeRe在无监督突触可塑性方法中达到SOTA性能，并在持续学习和迁移学习中表现优异。

Conclusion: SPHeRe展示了Hebbian无监督学习规则在现代深度学习框架中的竞争力，为高效且生物启发的学习算法提供了可能性。

Abstract: Hebbian learning is a biological principle that intuitively describes how
neurons adapt their connections through repeated stimuli. However, when applied
to machine learning, it suffers serious issues due to the unconstrained updates
of the connections and the lack of accounting for feedback mediation. Such
shortcomings limit its effective scaling to complex network architectures and
tasks. To this end, here we introduce the Structural Projection Hebbian
Representation (SPHeRe), a novel unsupervised learning method that integrates
orthogonality and structural information preservation through a local auxiliary
nonlinear block. The loss for structural information preservation
backpropagates to the input through an auxiliary lightweight projection that
conceptually serves as feedback mediation while the orthogonality constraints
account for the boundedness of updating magnitude. Extensive experimental
results show that SPHeRe achieves SOTA performance among unsupervised synaptic
plasticity approaches on standard image classification benchmarks, including
CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong
effectiveness in continual learning and transfer learning scenarios, and image
reconstruction tasks show the robustness and generalizability of the extracted
features. This work demonstrates the competitiveness and potential of Hebbian
unsupervised learning rules within modern deep learning frameworks,
demonstrating the possibility of efficient and biologically inspired learning
algorithms without the strong dependence on strict backpropagation. Our code is
available at https://github.com/brain-intelligence-lab/SPHeRe.

</details>


### [223] [Efficient Dynamic Structured Sparse Training with Learned Shuffles](https://arxiv.org/abs/2510.14812)
*Abhishek Tyagi,Arjun Iyer,Liam Young,William H Renninger,Christopher Kanan,Yuhao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种结合结构化稀疏和动态稀疏训练（DST）的方法，通过引入排列矩阵提升表达性，从而在保持高效性的同时达到与无结构DST相当的精度。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏在GPU上的训练和推理速度更快，但在精度上仍落后于无结构的动态稀疏训练（DST）。主要原因是结构化稀疏的表达性受限。

Method: 提出了一种排列增强的动态稀疏训练（PA-DST），为每层学习一个排列矩阵与结构化权重矩阵联合优化，应用于块、N:M和对角线三种结构。

Result: 在ImageNet-1K（ViT-B/16）和WikiText-103（GPT-2）上，PA-DST在90-95%稀疏度下与无结构基线（RigL、SET）精度相当，训练速度提升1.21倍，推理速度提升2.9倍。

Conclusion: 结合结构化和学习排列的方法在精度和效率之间找到了平衡点。

Abstract: Structured sparsity accelerates training and inference on modern GPUs, yet it
still trails unstructured dynamic sparse training (DST) in accuracy. The
shortfall stems from a loss of expressivity: whereas a dense layer can realize
every possible mask obtained by choosing any $w$ active weights out of $n$, a
fixed block or N:M layout explores only a subset of those possibilities. We
propose to close this gap by learning, for each layer, a single permutation
matrix jointly with the structured weight matrix. Applied to three canonical
structures -- block, N:M, and diagonals -- we show that permutation-augmented
DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\% sparsity on
ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$
and infers up to $2.9\times$ faster. The results position structure + learned
permutation as a sweet spot between accuracy and efficiency.

</details>


### [224] [Tackling Time-Series Forecasting Generalization via Mitigating Concept Drift](https://arxiv.org/abs/2510.14814)
*Zhiyuan Zhao,Haoxin Liu,B. Aditya Prakash*

Main category: cs.LG

TL;DR: 论文提出了一种名为ShifTS的框架，通过软注意力机制处理时间序列中的概念漂移和时序偏移，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注时序偏移，而概念漂移在时间序列预测中较少被关注，需要一种统一的方法解决这两种分布偏移。

Method: 提出软注意力机制从回看和预测时间序列中提取不变模式，并引入ShifTS框架，先处理时序偏移再解决概念漂移。

Result: 实验表明ShifTS能显著提升多种模型的预测准确性，优于现有基线方法。

Conclusion: ShifTS框架有效解决了时间序列中的分布偏移问题，为实际应用提供了可靠解决方案。

Abstract: Time-series forecasting finds broad applications in real-world scenarios. Due
to the dynamic nature of time series data, it is important for time-series
forecasting models to handle potential distribution shifts over time. In this
paper, we initially identify two types of distribution shifts in time series:
concept drift and temporal shift. We acknowledge that while existing studies
primarily focus on addressing temporal shift issues in time series forecasting,
designing proper concept drift methods for time series forecasting has received
comparatively less attention.
  Motivated by the need to address potential concept drift, while conventional
concept drift methods via invariant learning face certain challenges in
time-series forecasting, we propose a soft attention mechanism that finds
invariant patterns from both lookback and horizon time series. Additionally, we
emphasize the critical importance of mitigating temporal shifts as a
preliminary to addressing concept drift. In this context, we introduce ShifTS,
a method-agnostic framework designed to tackle temporal shift first and then
concept drift within a unified approach. Extensive experiments demonstrate the
efficacy of ShifTS in consistently enhancing the forecasting accuracy of
agnostic models across multiple datasets, and outperforming existing concept
drift, temporal shift, and combined baselines.

</details>


### [225] [Programmatic Representation Learning with Language Models](https://arxiv.org/abs/2510.14825)
*Gabriel Poesia,Georgia Gabriela Sampaio*

Main category: cs.LG

TL;DR: 论文提出了一种名为LeaPR的模型，结合代码生成的特征和决策树预测器，利用LLM生成特征函数，实现了高效且可解释的机器学习。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习模型（如决策树）依赖输入特征的选择，而神经网络虽能直接从原始数据学习表示，但牺牲了可解释性和硬件效率。LeaPR旨在解决这一矛盾。

Method: 提出两种算法：1) 改进FunSearch以学习特征函数；2) 改进ID3算法，在决策树分裂时动态生成特征。

Result: 在棋局评估、图像和文本分类任务中，LeaPR模型表现与神经网络相当，且无需神经网络硬件支持。

Conclusion: LeaPR为端到端学习可解释表示提供了灵活范式，特征和预测均可直观理解。

Abstract: Classical models for supervised machine learning, such as decision trees, are
efficient and interpretable predictors, but their quality is highly dependent
on the particular choice of input features. Although neural networks can learn
useful representations directly from raw data (e.g., images or text), this
comes at the expense of interpretability and the need for specialized hardware
to run them efficiently. In this paper, we explore a hypothesis class we call
Learned Programmatic Representations (LeaPR) models, which stack arbitrary
features represented as code (functions from data points to scalars) and
decision tree predictors. We synthesize feature functions using Large Language
Models (LLMs), which have rich prior knowledge in a wide range of domains and a
remarkable ability to write code using existing domain-specific libraries. We
propose two algorithms to learn LeaPR models from supervised data. First, we
design an adaptation of FunSearch to learn features rather than directly
generate predictors. Then, we develop a novel variant of the classical ID3
algorithm for decision tree learning, where new features are generated on
demand when splitting leaf nodes. In experiments from chess position evaluation
to image and text classification, our methods learn high-quality, neural
network-free predictors often competitive with neural networks. Our work
suggests a flexible paradigm for learning interpretable representations
end-to-end where features and predictions can be readily inspected and
understood.

</details>


### [226] [To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models](https://arxiv.org/abs/2510.14826)
*Eran Malach,Omid Saremi,Sinead Williamson,Arwen Bradley,Aryo Lotfi,Emmanuel Abbe,Josh Susskind,Etai Littwin*

Main category: cs.LG

TL;DR: SSMs在长序列建模中效率高，但理论证明其无法解决真正长形式生成问题，通过引入外部工具可克服此限制，并在多个任务中实现长度泛化。


<details>
  <summary>Details</summary>
Motivation: 探讨SSMs在长序列建模中的局限性，尤其是其在长形式生成问题上的不足，并提出通过工具增强来提升其性能。

Method: 通过理论分析证明SSMs的局限性，并提出工具增强的SSMs方法，结合问题相关的训练数据。

Result: 工具增强的SSMs在算术、推理和编码任务中表现出卓越的长度泛化能力。

Conclusion: SSMs在交互式工具和代理设置中可能成为Transformers的高效替代方案。

Abstract: State Space Models (SSMs) have become the leading alternative to Transformers
for sequence modeling. Their primary advantage is efficiency in long-context
and long-form generation, enabled by fixed-size memory and linear scaling of
computational complexity. We begin this work by showing a simple theoretical
result stating that SSMs cannot accurately solve any ``truly long-form''
generation problem (in a sense we formally define), undermining their main
competitive advantage. However, we show that this limitation can be mitigated
by allowing SSMs interactive access to external tools. In fact, we show that
given the right choice of tool access and problem-dependent training data, SSMs
can learn to solve any tractable problem and generalize to arbitrary problem
length/complexity (i.e., achieve length generalization). Following our
theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable
length generalization on a variety of arithmetic, reasoning, and coding tasks.
These findings highlight SSMs as a potential efficient alternative to
Transformers in interactive tool-based and agentic settings.

</details>


### [227] [Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction in 6G Multi-RAT Networks](https://arxiv.org/abs/2510.14832)
*Maria Lamprini A. Bartsioka,Anastasios Giannopoulos,Sotirios Spantideas*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的预测性条件切换（P-CHO）框架，用于6G多无线接入技术（multi-RAT）网络中的主动切换决策。


<details>
  <summary>Details</summary>
Motivation: 6G多RAT网络中，蜂窝和WiFi共存，传统切换决策依赖瞬时测量和阈值事件，难以应对快速信道动态和异构覆盖。

Method: 采用模型驱动和短时信号质量预测，基于LSTM网络训练RAT感知的预测模型，并设计标准化的P-CHO工作流程。

Result: 实验表明，P-CHO能减少切换失败和乒乓事件，适用于6G多RAT网络的低延迟主动切换。

Conclusion: P-CHO框架为6G多RAT网络提供了准确、低延迟的主动切换方案。

Abstract: The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT)
networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist,
requires mobility decisions that remain reliable under fast channel dynamics,
interference, and heterogeneous coverage. Handover in multi-RAT deployments is
still highly reactive and event-triggered, relying on instantaneous
measurements and threshold events. This work proposes a Machine Learning
(ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a
model-driven and short-horizon signal quality forecasts. We present a
generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller,
which standardizes data collection, parallel per-RAT predictions, decision
logic with hysteresis-based conditions, and CHO execution. Considering a
realistic multi-RAT environment, we train RAT-aware Long Short Term Memory
(LSTM) networks to forecast the signal quality indicators of mobile users along
randomized trajectories. The proposed P-CHO models are trained and evaluated
under different channel models for cellular and IEEE 802.11 WiFi integrated
coverage. We study the impact of hyperparameter tuning of LSTM models under
different system settings, and compare direct multi-step versus recursive P-CHO
variants. Comparisons against baseline predictors are also carried out.
Finally, the proposed P-CHO is tested under soft and hard handover settings,
showing that hysteresis-enabled P-CHO scheme is able to reduce handover
failures and ping-pong events. Overall, the proposed P-CHO framework can enable
accurate, low-latency, and proactive handovers suitable for ML-assisted
handover steering in 6G multi-RAT deployments.

</details>


### [228] [Reinforcement Learning with Stochastic Reward Machines](https://arxiv.org/abs/2510.14837)
*Jan Corazza,Ivan Gavran,Daniel Neider*

Main category: cs.LG

TL;DR: 提出了一种新型的随机奖励机及其学习算法，用于处理噪声奖励的强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有奖励机学习算法假设奖励无噪声，实际应用中存在局限性。

Method: 基于约束求解的算法，从强化学习智能体的探索中学习最小随机奖励机。

Result: 算法在案例研究中表现优于现有方法和朴素噪声处理方法。

Conclusion: 算法能与现有奖励机强化学习算法结合，保证在极限情况下收敛到最优策略。

Abstract: Reward machines are an established tool for dealing with reinforcement
learning problems in which rewards are sparse and depend on complex sequences
of actions. However, existing algorithms for learning reward machines assume an
overly idealized setting where rewards have to be free of noise. To overcome
this practical limitation, we introduce a novel type of reward machines, called
stochastic reward machines, and an algorithm for learning them. Our algorithm,
based on constraint solving, learns minimal stochastic reward machines from the
explorations of a reinforcement learning agent. This algorithm can easily be
paired with existing reinforcement learning algorithms for reward machines and
guarantees to converge to an optimal policy in the limit. We demonstrate the
effectiveness of our algorithm in two case studies and show that it outperforms
both existing methods and a naive approach for handling noisy reward functions.

</details>


### [229] [Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks](https://arxiv.org/abs/2510.14844)
*Odelia Melamed,Gilad Yehudai,Gal Vardi*

Main category: cs.LG

TL;DR: 论文提出了一种理论分析方法，评估梯度上升法在机器遗忘中的效果，并提出了新的成功标准。


<details>
  <summary>Details</summary>
Motivation: 解决隐私和伦理问题，研究如何高效移除特定数据点对模型的影响。

Method: 利用梯度上升法，结合KKT条件，量化遗忘模型的质量。

Result: 梯度上升法在特定条件下能有效近似重新训练的模型，并保持泛化能力。

Conclusion: 梯度上升法是一种有效的机器遗忘方法，适用于线性模型和两层神经网络。

Abstract: Machine Unlearning aims to remove specific data from trained models,
addressing growing privacy and ethical concerns. We provide a theoretical
analysis of a simple and widely used method - gradient ascent - used to reverse
the influence of a specific data point without retraining from scratch.
Leveraging the implicit bias of gradient descent towards solutions that satisfy
the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we
quantify the quality of the unlearned model by evaluating how well it satisfies
these conditions w.r.t. the retained data. To formalize this idea, we propose a
new success criterion, termed \textbf{$(\epsilon, \delta, \tau)$-successful}
unlearning, and show that, for both linear models and two-layer neural networks
with high dimensional data, a properly scaled gradient-ascent step satisfies
this criterion and yields a model that closely approximates the retrained
solution on the retained data. We also show that gradient ascent performs
successful unlearning while still preserving generalization in a synthetic
Gaussian-mixture setting.

</details>


### [230] [Backdoor Unlearning by Linear Task Decomposition](https://arxiv.org/abs/2510.14845)
*Amel Abdelraheem,Alessandro Favero,Gerome Bovet,Pascal Frossard*

Main category: cs.LG

TL;DR: 提出一种简单的方法，通过解耦模型权重空间中的后门任务与良性任务，实现高效后门移除，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有后门移除方法依赖昂贵微调且可能影响模型性能，研究后门在模型权重空间中的编码方式以解决这一问题。

Method: 利用后门任务与良性任务的解耦特性，设计一种简单的遗忘方法，通过隔离和擦除后门影响实现移除。

Result: 在CLIP模型上实验显示，该方法近乎完美移除后门，平均保留96%的干净准确率，且优于现有防御方法。

Conclusion: 该方法通过解耦后门与良性任务，实现了高效后门移除与性能平衡，为模型安全提供了新思路。

Abstract: Foundation models have revolutionized computer vision by enabling broad
generalization across diverse tasks. Yet, they remain highly susceptible to
adversarial perturbations and targeted backdoor attacks. Mitigating such
vulnerabilities remains an open challenge, especially given that the
large-scale nature of the models prohibits retraining to ensure safety.
Existing backdoor removal approaches rely on costly fine-tuning to override the
harmful behavior, and can often degrade performance on other unrelated tasks.
This raises the question of whether backdoors can be removed without
compromising the general capabilities of the models. In this work, we address
this question and study how backdoors are encoded in the model weight space,
finding that they are disentangled from other benign tasks. Specifically, this
separation enables the isolation and erasure of the backdoor's influence on the
model with minimal impact on clean performance. Building on this insight, we
introduce a simple unlearning method that leverages such disentanglement.
Through extensive experiments with CLIP-based models and common adversarial
triggers, we show that, given the knowledge of the attack, our method achieves
approximately perfect unlearning, while retaining, on average, 96% of clean
accuracy. Additionally, we demonstrate that even when the attack and its
presence are unknown, our method successfully unlearns backdoors by proper
estimation using reverse-engineered triggers. Overall, our method consistently
yields better unlearning and clean accuracy tradeoffs when compared to present
state-of-the-art defenses.

</details>


### [231] [Predicting kernel regression learning curves from only raw data statistics](https://arxiv.org/abs/2510.14878)
*Dhruva Karkada,Joseph Turnbull,Yuxi Liu,James B. Simon*

Main category: cs.LG

TL;DR: 论文提出了一种称为Hermite本征结构假设（HEA）的理论框架，用于预测核回归的学习曲线，并通过实验验证其在真实数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究核回归在真实数据集上的表现，并开发一种理论框架，能够仅通过少量测量预测学习曲线。

Method: 提出HEA假设，近似核的本征值和本征函数，并将其应用于预测学习曲线。

Result: HEA在真实图像数据上表现良好，且多层感知机（MLP）在特征学习阶段的行为与HEA预测一致。

Conclusion: HEA框架证明了从数据集结构到模型性能的端到端理论在非平凡学习算法上的可行性。

Abstract: We study kernel regression with common rotation-invariant kernels on real
datasets including CIFAR-5m, SVHN, and ImageNet. We give a theoretical
framework that predicts learning curves (test risk vs. sample size) from only
two measurements: the empirical data covariance matrix and an empirical
polynomial decomposition of the target function $f_*$. The key new idea is an
analytical approximation of a kernel's eigenvalues and eigenfunctions with
respect to an anisotropic data distribution. The eigenfunctions resemble
Hermite polynomials of the data, so we call this approximation the Hermite
eigenstructure ansatz (HEA). We prove the HEA for Gaussian data, but we find
that real image data is often "Gaussian enough" for the HEA to hold well in
practice, enabling us to predict learning curves by applying prior results
relating kernel eigenstructure to test risk. Extending beyond kernel
regression, we empirically find that MLPs in the feature-learning regime learn
Hermite polynomials in the order predicted by the HEA. Our HEA framework is a
proof of concept that an end-to-end theory of learning which maps dataset
structure all the way to model performance is possible for nontrivial learning
algorithms on real datasets.

</details>


### [232] [Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with Unbounded Rewards](https://arxiv.org/abs/2510.14884)
*Sarah Liaw,Benjamin Plaut*

Main category: cs.LG

TL;DR: 论文提出了一种在高风险AI应用中避免不可逆错误的谨慎探索算法。


<details>
  <summary>Details</summary>
Motivation: 在高风险AI应用中，传统决策理论假设错误可恢复，但实际中某些错误不可逆，且导师可能不可用。

Method: 提出了一种基于上下文的双动作bandit模型，包含放弃选项，算法在可信区域内谨慎探索。

Result: 在i.i.d.输入下，算法实现了次线性遗憾保证。

Conclusion: 谨慎探索在高风险环境中安全部署学习代理是有效的。

Abstract: In high-stakes AI applications, even a single action can cause irreparable
damage. However, nearly all of sequential decision-making theory assumes that
all errors are recoverable (e.g., by bounding rewards). Standard bandit
algorithms that explore aggressively may cause irreparable damage when this
assumption fails. Some prior work avoids irreparable errors by asking for help
from a mentor, but a mentor may not always be available. In this work, we
formalize a model of learning with unbounded rewards without a mentor as a
two-action contextual bandit with an abstain option: at each round the agent
observes an input and chooses either to abstain (always 0 reward) or to commit
(execute a preexisting task policy). Committing yields rewards that are
upper-bounded but can be arbitrarily negative, and the commit reward is assumed
Lipschitz in the input. We propose a caution-based algorithm that learns when
not to learn: it chooses a trusted region and commits only where the available
evidence does not already certify harm. Under these conditions and i.i.d.
inputs, we establish sublinear regret guarantees, theoretically demonstrating
the effectiveness of cautious exploration for deploying learning agents safely
in high-stakes environments.

</details>


### [233] [Reasoning with Sampling: Your Base Model is Smarter Than You Think](https://arxiv.org/abs/2510.14901)
*Aayush Karan,Yilun Du*

Main category: cs.LG

TL;DR: 提出一种无需额外训练的迭代采样算法，提升基础模型的推理能力，性能接近甚至超过RL后训练模型。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过纯采样在推理时激发基础模型的推理能力，而无需额外训练。

Method: 基于MCMC技术，提出一种利用基础模型自身似然的迭代采样算法。

Result: 在多种单次任务（如MATH500、HumanEval、GPQA）上表现优异，且避免了RL后训练的多样性下降问题。

Conclusion: 该方法无需训练、数据集或验证器，具有广泛适用性。

Abstract: Frontier reasoning models have exhibited incredible capabilities across a
wide array of disciplines, driven by posttraining large language models (LLMs)
with reinforcement learning (RL). However, despite the widespread success of
this paradigm, much of the literature has been devoted to disentangling truly
novel behaviors that emerge during RL but are not present in the base models.
In our work, we approach this question from a different angle, instead asking
whether comparable reasoning capabilites can be elicited from base models at
inference time by pure sampling, without any additional training. Inspired by
Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened
distributions, we propose a simple iterative sampling algorithm leveraging the
base models' own likelihoods. Over different base models, we show that our
algorithm offers substantial boosts in reasoning that nearly match and even
outperform those from RL on a wide variety of single-shot tasks, including
MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in
diversity over multiple samples that is characteristic of RL-posttraining.
Crucially, our method does not require training, curated datasets, or a
verifier, suggesting broad applicability beyond easily verifiable domains.

</details>


### [234] [Circuit Insights: Towards Interpretability Beyond Activations](https://arxiv.org/abs/2510.14936)
*Elena Golimblevskaia,Aakriti Jain,Bruno Puri,Ammar Ibrahim,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.LG

TL;DR: 论文提出了WeightLens和CircuitLens两种方法，通过直接分析权重和特征交互，提升神经网络的可解释性和电路分析的效率。


<details>
  <summary>Details</summary>
Motivation: 现有解释性AI方法依赖手动检查或外部LLM，难以捕捉特征交互且扩展性有限。

Method: WeightLens直接从权重解释特征，无需外部模型或数据集；CircuitLens分析特征间的交互动态。

Result: 新方法在独立特征上表现优于现有方法，并能识别激活方法无法捕捉的电路级动态。

Conclusion: WeightLens和CircuitLens增强了可解释性的鲁棒性，支持更高效的电路机制分析。

Abstract: The fields of explainable AI and mechanistic interpretability aim to uncover
the internal structure of neural networks, with circuit discovery as a central
tool for understanding model computations. Existing approaches, however, rely
on manual inspection and remain limited to toy tasks. Automated
interpretability offers scalability by analyzing isolated features and their
activations, but it often misses interactions between features and depends
strongly on external LLMs and dataset quality. Transcoders have recently made
it possible to separate feature attributions into input-dependent and
input-invariant components, providing a foundation for more systematic circuit
analysis. Building on this, we propose WeightLens and CircuitLens, two
complementary methods that go beyond activation-based analysis. WeightLens
interprets features directly from their learned weights, removing the need for
explainer models or datasets while matching or exceeding the performance of
existing methods on context-independent features. CircuitLens captures how
feature activations arise from interactions between components, revealing
circuit-level dynamics that activation-only approaches cannot identify.
Together, these methods increase interpretability robustness and enhance
scalable mechanistic analysis of circuits while maintaining efficiency and
quality.

</details>


### [235] [Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models](https://arxiv.org/abs/2510.14961)
*Jonas Geiping,Xinyu Yang,Guinan Su*

Main category: cs.LG

TL;DR: 提出了一种新的扩散强制采样器，用于加速循环深度模型的生成，理论证明其表达能力更强，并在现有模型上实现了5倍加速。


<details>
  <summary>Details</summary>
Motivation: 探索循环深度模型与扩散语言模型之间的关系，利用相似性提升生成效率。

Method: 开发扩散强制采样器，通过并行细化潜在状态加速生成。

Result: 采样器在相同时间预算下比自回归基线更具表达力，且无需调整即可应用于现有模型，实现5倍加速。

Conclusion: 循环深度模型可视为强连续扩散语言模型，提供高效并行推理机制。

Abstract: Language models with recurrent depth, also referred to as universal or looped
when considering transformers, are defined by the capacity to increase their
computation through the repetition of layers. Recent efforts in pretraining
have demonstrated that these architectures can scale to modern language
modeling tasks while exhibiting advantages in reasoning tasks. In this work, we
examine the relationship between recurrent-depth models and diffusion language
models. Building on their similarities, we develop a new diffusion forcing
sampler for these models to accelerate generation. The sampler advances by
decoding new tokens at every forward pass of the model, while the latent states
of these tokens can be further refined in parallel through recurrence.
Theoretically, generation with our sampler is strictly more expressive than the
baseline autoregressive generation using the same time budget on modern
hardware. Moreover, this sampler, based on principles from diffusion
literature, can be directly applied to existing 3.5B recurrent-depth
transformers without any tuning, leading to up to a 5x speedup. Consequently,
our findings not only provide an efficient mechanism for parallelizing the
extra computation in recurrent-depth models at inference, but also suggest that
such models can be naturally viewed as strong continuous, though causal,
diffusion language models.

</details>


### [236] [Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores](https://arxiv.org/abs/2510.14966)
*Zachary Robertson*

Main category: cs.LG

TL;DR: 论文提出了一种基于TVD-MI的线性模型，用于高效评估大语言模型，避免了非线性链接函数带来的问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过TVD-MI的几何特性，优化大语言模型的评估方法，避免非线性链接函数破坏数据的可加性。

Method: 使用TVD-MI的二进制决策，通过平均化生成概率分数，并采用线性模型（而非logit/probit）进行IRT分析。

Result: 实验表明，线性模型在保持数据可加性方面表现更好，且评估效率提高了3倍。

Conclusion: TVD-MI的几何特性适合线性映射，适用于高效评估大语言模型及其他有界响应领域。

Abstract: Pairwise comparisons of large language models using total variation distance
mutual information (TVD-MI) produce binary critic decisions per pair. We show
that averaging TVD-MI's binary trials yields centered-probability scores with
additive structure suitable for item-response theory (IRT) without nonlinear
link functions. Maximum-likelihood approaches to IRT use logistic links, but we
find empirically that these transformations introduce curvature that breaks
additivity: across three domains, the identity link yields median curl on raw
data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce
substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We
derive this clipped-linear model from Gini entropy maximization, yielding a
box-constrained least-squares formulation that handles boundary saturation. At
33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent
rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations
than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows
strong agreement in agent rankings ($\rho = 0.872$) and consistent
identity-link advantage. TVD-MI's geometry is best preserved by identity
mapping for efficient LLM evaluation, applicable to other bounded-response
domains.

</details>


### [237] [Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability](https://arxiv.org/abs/2510.14970)
*Katiana Kontolati,Rini Jasmine Gladstone,Ian Davis,Ethan Pickering*

Main category: cs.LG

TL;DR: BINNs通过整合多组学数据和先验生物知识，显著提高了基因组预测和选择的准确性，尤其在稀疏数据条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统G2P模型准确性有限，依赖大规模田间试验，成本高。BINNs旨在通过利用中间分子表型数据（如基因表达）和生物知识，提高预测精度。

Method: BINNs通过编码通路级归纳偏置，仅在训练时使用多组学数据，推理时仅需基因型数据。

Result: 在玉米基因表达和多环境田间试验数据中，BINNs将秩相关准确性提高了56%，并能非线性识别GWAS/TWAS未发现的基因。在合成代谢组学基准测试中，预测误差降低了75%。

Conclusion: BINNs通过学习生物学相关的表示，为基因组选择、候选基因筛选和基因编辑优先级提供了新框架。

Abstract: We extend biologically-informed neural networks (BINNs) for genomic
prediction (GP) and selection (GS) in crops by integrating thousands of
single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior
biological knowledge. Traditional genotype-to-phenotype (G2P) models depend
heavily on direct mappings that achieve only modest accuracy, forcing breeders
to conduct large, costly field trials to maintain or marginally improve genetic
gain. Models that incorporate intermediate molecular phenotypes such as gene
expression can achieve higher predictive fit, but they remain impractical for
GS since such data are unavailable at deployment or design time. BINNs overcome
this limitation by encoding pathway-level inductive biases and leveraging
multi-omics data only during training, while using genotype data alone during
inference. Applied to maize gene-expression and multi-environment field-trial
data, BINN improves rank-correlation accuracy by up to 56% within and across
subpopulations under sparse-data conditions and nonlinearly identifies genes
that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic
metabolomics benchmark, BINN reduces prediction error by 75% relative to
conventional neural nets and correctly identifies the most important nonlinear
pathway. Importantly, both cases show highly sensitive BINN latent variables
correlate with the experimental quantities they represent, despite not being
trained on them. This suggests BINNs learn biologically-relevant
representations, nonlinear or linear, from genotype to phenotype. Together,
BINNs establish a framework that leverages intermediate domain information to
improve genomic prediction accuracy and reveal nonlinear biological
relationships that can guide genomic selection, candidate gene selection,
pathway enrichment, and gene-editing prioritization.

</details>


### [238] [pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation](https://arxiv.org/abs/2510.14974)
*Hansheng Chen,Kai Zhang,Hao Tan,Leonidas Guibas,Gordon Wetzstein,Sai Bi*

Main category: cs.LG

TL;DR: $\pi$-Flow 是一种基于策略的流模型，通过模仿教师模型的行为，避免了复杂蒸馏过程中的质量-多样性权衡，实现了快速准确的ODE集成。


<details>
  <summary>Details</summary>
Motivation: 解决传统蒸馏方法中因格式不匹配导致的质量-多样性权衡问题。

Method: $\pi$-Flow 修改学生流模型的输出层，预测无网络的策略，并通过模仿蒸馏方法匹配教师模型的轨迹。

Result: 在ImageNet 256$^2$上达到1-NFE FID 2.85，优于MeanFlow；在FLUX.1-12B和Qwen-Image-20B上，4 NFEs下多样性显著优于现有方法。

Conclusion: $\pi$-Flow 通过简单模仿教师行为，实现了稳定、可扩展的训练，同时避免了质量-多样性权衡。

Abstract: Few-step diffusion or flow-based generative models typically distill a
velocity-predicting teacher into a student that predicts a shortcut towards
denoised data. This format mismatch has led to complex distillation procedures
that often suffer from a quality-diversity trade-off. To address this, we
propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output
layer of a student flow model to predict a network-free policy at one timestep.
The policy then produces dynamic flow velocities at future substeps with
negligible overhead, enabling fast and accurate ODE integration on these
substeps without extra network evaluations. To match the policy's ODE
trajectory to the teacher's, we introduce a novel imitation distillation
approach, which matches the policy's velocity to the teacher's along the
policy's trajectory using a standard $\ell_2$ flow matching loss. By simply
mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable
training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it
attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT
architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves
substantially better diversity than state-of-the-art few-step methods, while
maintaining teacher-level quality.

</details>
