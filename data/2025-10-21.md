<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.LG](#cs.LG) [Total: 224]
- [cs.RO](#cs.RO) [Total: 48]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: ESC框架通过SGClip模型实现多模态大语言模型（MLLMs）的细粒度视觉-文本对齐，显著提升感知性能。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs训练依赖高层视觉-声音-文本对，缺乏像素级视觉内容与文本语义的结构化对齐。

Method: 提出SGClip模型，通过神经符号学习管道在8.7万+开放域视频上训练，无需人工标注场景图。

Result: SGClip在场景图生成和动作定位任务中表现优异，ESCA框架显著提升MLLMs性能，超越专有基线。

Conclusion: ESC框架通过结构化时空理解提升MLLMs的感知能力，为通用智能体发展提供新方向。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 稀疏跨模态检测器优于BEV检测器，但现有方法忽视令牌表示质量。本文提出Sparse Selector（SS），通过Ray-Aware Supervision和Class-Balanced Supervision提升性能，并设计Ray PE解决LiDAR与图像差异。最终模型CrossRay3D在nuScenes基准测试中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器在令牌表示质量上不足，影响前景质量和性能。

Method: 提出SS模块，包含Ray-Aware Supervision（保留几何信息）和Class-Balanced Supervision（平衡类别语义），并设计Ray PE解决模态差异。

Result: CrossRay3D在nuScenes上达到72.4 mAP和74.7 NDS，速度提升1.84倍，且对数据缺失鲁棒。

Conclusion: SS模块和Ray PE有效提升稀疏检测器性能，CrossRay3D在多模态检测中表现优异。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 提出了一种利用CCTV视频流进行多缺陷检测和分割的管道，结合YOLO目标检测器和视觉语言模型（VLM）生成结构化维护计划。


<details>
  <summary>Details</summary>
Motivation: 智能城市基础设施的缺陷（如裂缝、坑洞和泄漏）威胁公共安全，现有自动系统无法提供结构化输出以指导维护工作。

Method: 使用YOLO目标检测器进行缺陷检测和分割，再通过VLM生成包含描述、工具建议和修复计划的JSON格式输出。

Result: 在公共数据集和CCTV片段上的实验表明，系统能准确识别多种缺陷并生成连贯的总结。

Conclusion: 讨论了系统扩展至城市范围部署的挑战和方向。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: IAD-GPT是一种基于多模态大语言模型（MLLMs）的新方法，用于工业异常检测（IAD），通过生成详细异常提示和增强视觉定位能力，显著提升了检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统IAD方法缺乏多轮对话和详细描述能力，而现有大模型在异常检测任务中未充分发挥潜力。

Method: 结合文本语义与图像信息，使用异常提示生成器（APG）激活预训练视觉语言模型（如CLIP），并通过文本引导增强器和多掩码融合模块提升模型性能。

Result: 在MVTec-AD和VisA数据集上实现了自监督和少样本异常检测与分割任务的最先进性能。

Conclusion: IAD-GPT通过多模态融合和专家知识集成，显著提升了工业异常检测的准确性和实用性。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [5] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 结构化报告（SR）和人工智能辅助结构化报告（AI-SR）提高了放射科医生的工作效率和诊断准确性，AI-SR效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨不同报告模式（自由文本、结构化报告、AI辅助结构化报告）对放射科医生图像分析行为、诊断准确性、效率和用户体验的影响。

Method: 前瞻性研究，使用眼动追踪系统和定制化查看器，评估8名读者（4名新手和4名非新手）分析35张床边胸片的表现。

Result: AI-SR诊断准确性最高（κ=0.71），报告时间最短（25±9秒），眼动指标和用户体验最佳。

Conclusion: 结构化报告通过引导视觉注意力提高效率，AI预填充的SR进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [6] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 论文提出了一种数据驱动的框架IFEF和BWA方法，用于分析和缓解图像分类中的交叉偏见，实验显示BWA显著提高了少数类别的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型在数据不平衡时产生的交叉偏见问题，如物体类别与环境条件的交互影响。

Method: 引入IFEF框架结合公平性指标和可解释性工具，提出BWA数据增强策略，基于子组统计调整变换强度。

Result: 在Open Images V7数据集上，BWA将少数类别的准确性提升24%，公平性差异减少35%，统计显著（p < 0.05）。

Conclusion: 该方法为图像分类系统中的交叉偏见问题提供了可复现的分析和解决途径。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [7] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 该论文提出了一种可微分的量化方法，支持多比特量化，并在ImageNet数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法多为非可微，且难以同时量化权重和激活函数，导致精度损失。本文旨在解决这些问题。

Method: 采用可微分的量化方法，支持多比特量化（如$2^n$形式），并提供了收敛性证明。

Result: 在ResNet18上仅量化权重时，精度损失小于1%；同时量化权重和激活函数时，精度与SOTA相当，仅需15个训练周期。

Conclusion: 该方法在保持高精度的同时，显著降低了计算和内存需求，适用于高效推理。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [8] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: StripRFNet是一种新型深度神经网络，用于高精度检测道路表面损伤，解决了形状多样、细长裂缝和小尺度损伤识别的挑战。


<details>
  <summary>Details</summary>
Motivation: 道路表面损伤威胁交通安全并阻碍可持续发展，但现有方法在形状多样性、细长裂缝和小尺度损伤检测方面存在不足。

Method: StripRFNet包含三个模块：形状感知模块（SPM）、条带感受野模块（SRFM）和小尺度增强模块（SSEM），分别用于增强形状识别、捕捉细长裂缝特征和提升小目标检测。

Result: 在RDD2022基准测试中，StripRFNet在F1-score、mAP50和mAP50:95上分别比基线提高了4.4、2.9和3.4个百分点，达到80.33%的最高F1-score。

Conclusion: StripRFNet在精度和实时性上均达到先进水平，为智能道路维护和可持续基础设施管理提供了有效工具。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [9] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms通过对象特定变换在训练和推理时量化并减少不确定性，提升视觉目标检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中可靠的感知对安全决策至关重要，但视觉目标检测器易受数据偏差和分布偏移等不确定性影响。

Method: 在训练时对对象进行颜色空间扰动和扩散模型生成多样化行人实例；推理时对检测对象应用扰动，利用检测分数方差量化不确定性。

Result: 在NuImages 10K数据集上实验显示，方法显著提升准确性并减少不确定性，同时有效区分假阳性和真阳性。

Conclusion: ObjectTransforms是一种轻量且有效的机制，可在训练和推理阶段分别减少和量化不确定性。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [10] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是一个多模态的开放数据集，通过Aria Gen 2眼镜记录日常活动，提供原始传感器数据和算法输出数据。


<details>
  <summary>Details</summary>
Motivation: 提供及时访问的多模态数据集，展示设备在不同用户和条件下的感知能力。

Method: 使用Aria Gen 2眼镜记录日常活动，涵盖五种主要场景，并提供原始数据和算法输出。

Result: 数据集展示了设备对佩戴者、环境和互动的感知能力，并保持稳健性能。

Conclusion: A2PD是一个公开可用的数据集，附带开源工具和使用示例。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [11] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出了一种基于预训练整流流模型的训练自由方法，通过周期性添加指导来改进3D资产的外观转移，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在输入和外观对象几何差异较大时效果不佳，直接应用3D生成模型效果不理想。

Method: 利用预训练的整流流模型，通过周期性添加可微损失函数（如部分感知损失和自相似性）指导采样过程。

Result: 成功将纹理和几何细节转移到3D资产，定性和定量上优于基线方法。

Conclusion: 方法通用性强，可扩展到不同类型的扩散模型和指导函数，并通过GPT评估和用户研究验证了效果。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [12] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 利用深度学习自动化血栓切除术的关键步骤，提高效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 血栓切除术是治疗缺血性中风的有效方法，但资源密集且依赖专业人员。

Method: 提出一种自监督框架，通过基于回归的预训练任务分类骨骼标志点。

Result: 模型在回归和分类任务中优于现有方法，位置预训练任务显著提升下游分类性能。

Conclusion: 未来工作将扩展框架以实现完全自主的C臂控制，优化手术轨迹。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [13] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch是一种新型的双分支半监督框架，通过异步优化和多种正则化技术提升医学图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注数据有限，半监督学习能够利用不完美的监督信息，但现有方法在联合优化时存在收敛和稳定性问题。

Method: 提出DuetMatch框架，采用异步优化分支（分别优化编码器和解码器），引入解耦Dropout扰动和Pair-wise CutMix交叉引导增强多样性，并通过一致性匹配减少噪声伪标签的影响。

Result: 在ISLES2022和BraTS等脑MRI分割数据集上，DuetMatch性能优于现有方法。

Conclusion: DuetMatch通过创新的双分支设计和正则化技术，有效提升了半监督医学图像分割的鲁棒性和性能。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [14] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出了一种自主导航C臂到预定义解剖标志的流程，结合X射线图像和不确定性校准，提高定位精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 手动调整C臂增加辐射暴露和手术延迟，需要自动化解决方案。

Method: 利用X射线图像预测3D位移向量，结合概率损失和骨骼姿态正则化，使用符合预测校准不确定性。

Result: 在合成X射线数据集上验证，定位准确且预测边界校准良好。

Conclusion: 该流程有望用于安全可靠的自主C臂系统。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [15] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 论文提出了一种自动预过滤方法，通过提高图像质量评估（IQA）的效率，显著降低了获取高质量图像的成本。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型生成的图像质量尚未达到传统摄影标准，手动IQA过程成本高昂且效率低。

Method: 引入自动预过滤阶段，提出一个公式估计成本节省，并在背景修复用例中应用AutoML解决方案。

Result: 在背景修复用例中，实现了51.61%的成本节省。

Conclusion: 自动预过滤方法能有效降低IQA成本，提升生成图像的工作流程效率。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [16] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 论文提出PRISM模型，通过将fMRI信号映射到结构化文本空间，优化视觉刺激重建，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索fMRI信号如何有效编码视觉信息，并确定最佳潜在空间类型以支持图像重建。

Method: 提出PRISM模型，包含对象中心扩散模块和属性关系搜索模块，将fMRI信号映射到结构化文本空间。

Result: 实验表明PRISM在真实数据集上表现优异，感知损失降低8%。

Conclusion: 结构化文本空间是连接fMRI信号与图像重建的关键中间表示。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [17] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 论文提出了一种数据为中心的人工智能（DCAI）方法，用于解决热带农业遥感中的挑战，如数据质量差、标注成本高和区域泛化问题。


<details>
  <summary>Details</summary>
Motivation: 热带农业遥感面临数据质量差、标注成本高、数据多变和区域泛化等挑战，传统模型中心方法效果有限。

Method: 采用数据为中心的方法，包括自信学习、核心集选择、数据增强和主动学习等技术，提出25种策略，并推荐9种成熟方法。

Result: 论文总结了25种适用于大规模热带农业制图的策略，并提出了一个实用的数据中心化流程。

Conclusion: 数据为中心的方法更适合热带农业的动态现实，提出的9种成熟方法可直接应用于实际项目。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [18] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 论文提出了一种灵活的训练方法StretchySnake，通过动态调整时空分辨率，提升了状态空间模型（SSMs）在视频理解任务中的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解任务的训练方法主要针对Transformer设计，未能充分利用SSMs的线性复杂性和隐藏状态递归特性，导致模型在未见过的时空分辨率下性能下降。

Method: 提出了一种灵活的训练方法，通过在训练中采样不同时空分辨率的视频，并动态插值模型权重，使SSM具备时空灵活性。

Result: StretchySnake在短动作和长动作基准测试中表现优于Transformer和SSM基线模型，最高提升28%，且对细粒度动作有强适应性。

Conclusion: 该方法为视频SSMs提供了一种简单高效的训练方案，使其在多样化动作识别场景中更鲁棒、分辨率无关且高效。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [19] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出了一种名为VM-BeautyNet的异构集成架构，结合了Vision Transformer和Mamba-based Vision模型的优势，在SCUT-FBP5500数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在面部美观预测中难以捕捉全局特征的问题，同时降低计算复杂度。

Method: 通过融合ViT和Mamba-based Vision模型的互补优势，ViT捕捉全局结构，Mamba高效建模长距离依赖。

Result: 在SCUT-FBP5500数据集上，PC为0.9212，MAE为0.2085，RMSE为0.2698，性能最优。

Conclusion: VM-BeautyNet不仅提升了性能，还通过可视化分析提供了模型决策的新见解，为计算美学提供了新范式。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [20] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 使用卷积神经网络（CNN）和图像处理硬件早期检测口腔鳞状细胞癌（OCSCC），通过提高图像分辨率提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: OCSCC早期症状隐蔽且发展缓慢，常被忽视，导致可预防的死亡。通过CNN和硬件系统提高检测效率。

Method: 训练CNN识别OCSCC，设计硬件系统捕获和处理图像，测试不同分辨率对预测准确性的影响。

Result: 图像分辨率越高，预测准确性呈对数增长，但高像素数带来的回报递减。

Conclusion: CNN结合硬件系统可有效提升OCSCC早期检测的准确性，但需平衡分辨率与效率。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [21] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Meta的Codec Avatars Lab发布了Embody 3D数据集，包含439名参与者的500小时3D运动数据，涵盖单人和多人行为。


<details>
  <summary>Details</summary>
Motivation: 提供多模态的3D运动数据，支持研究人类行为和交互。

Method: 在多摄像头环境中收集数据，包括身体和手部追踪、文本注释和独立音频。

Result: 数据集包含54百万帧3D运动数据，涵盖多种行为和情感状态。

Conclusion: Embody 3D为研究人类行为和交互提供了丰富的资源。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [22] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 提出了一种基于人-物交互的动态场景分解与重建方法，利用高斯泼溅技术实现高效渲染。


<details>
  <summary>Details</summary>
Motivation: 人类行为是场景动态的主要来源，通过观察人-物交互可以动态优化场景分解与重建过程，解决静态对象重建中的模糊性问题。

Method: 提出了一种在线方法，通过人-物交互迭代分解和重建环境，结合相机和物体姿态估计、实例分解和在线地图更新等任务。

Result: 在多个真实场景中验证了方法的有效性，实现了准确且一致的动态场景建模。

Conclusion: 该方法为动态场景重建提供了一种灵活且渐进式的替代方案，优于传统的对象级重建方法。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [23] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus是一种高效的两阶段级联系统，用于实时视频异常检测，结合轻量级过滤和细粒度视觉语言模型推理，显著提升速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视频异常检测中计算成本高且视觉定位不稳定，阻碍实时部署。

Method: Cerberus通过离线学习正常行为规则，在线推理时结合运动掩码提示和基于规则的偏差检测。

Result: 在四个数据集上平均达到57.68 fps（151.79倍加速）和97.2%准确率。

Conclusion: Cerberus是实时视频分析的实用解决方案，性能与最先进方法相当。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [24] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准测试，揭示了评估大型视觉语言模型（LVLMs）成员推理攻击（MIA）时的基本挑战。


<details>
  <summary>Details</summary>
Motivation: 先前的研究报告了高攻击成功率，但分析表明这些结果通常源于检测数据集构建中引入的分布偏差，而非真实的成员状态。

Method: 引入了一个包含6,000张图像的受控基准测试，其中成员和非成员样本的分布被仔细平衡，并提供了三个不同训练阶段的真实成员标签。

Result: 实验表明，在无偏条件下，最先进的MIA方法性能趋近于随机猜测。

Conclusion: OpenLVLM-MIA为LVLMs的MIA研究提供了透明且无偏的基准，为开发更强的隐私保护技术奠定了基础。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [25] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一种无需训练的新框架，通过跨图像笔画注意力机制实现参考风格的笔画属性精确转移。


<details>
  <summary>Details</summary>
Motivation: 生成具有参考风格的草图需要精确转移笔画属性（如线条粗细、变形和纹理稀疏性），同时保持语义结构和内容保真度。

Method: 提出Stroke2Sketch框架，嵌入跨图像笔画注意力机制，结合自适应对比增强和语义聚焦注意力。

Result: 生成的草图在风格忠实度和语义一致性上优于现有方法，接近手工绘制效果。

Conclusion: Stroke2Sketch在笔画控制和语义保持方面表现优异，代码已开源。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [26] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 论文研究了深度伪造检测任务的扩展规律，构建了最大数据集ScaleDF，并发现检测误差随数据规模呈幂律下降。


<details>
  <summary>Details</summary>
Motivation: 研究深度伪造检测任务的扩展规律，以应对不断演变的深度伪造技术。

Method: 构建ScaleDF数据集，分析模型性能与真实图像域、深度伪造方法和训练图像数量的关系。

Result: 检测误差随数据规模呈幂律下降，可通过增加数据规模预测性能提升。

Conclusion: 扩展数据规模是提升深度伪造检测性能的有效途径，但需注意其局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [27] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT是一种新的扩散框架，通过分层局部注意力和低分辨率全局引导，实现了高效、可扩展的超高分辨率图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型由于注意力机制的二次复杂性和缺乏原生4K训练数据，难以生成超高分辨率图像。

Method: Scale-DiT采用分层局部注意力、低分辨率全局语义注入、LoRA适应和Hilbert曲线排序等技术，优化了计算效率和内存使用。

Result: Scale-DiT在4K分辨率下比基线方法快2倍以上，内存占用更低，且在定量和定性评估中表现优异。

Conclusion: 分层局部注意力结合低分辨率引导是推进超高分辨率图像生成的有效方法。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [28] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX是一个云边协作框架，通过轻量级设备模型快速生成预览图像，再通过云端模型完成最终优化，显著降低生成时间和云资源负担。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了显著进展，但生成过程计算密集且需要多次迭代优化提示，增加了延迟和云资源负担。

Method: 提出DiffusionX框架，结合轻量级设备模型和云端高容量模型，并引入噪声水平预测器动态平衡计算负载。

Result: 实验表明，DiffusionX比Stable Diffusion v1.5平均生成时间减少15.8%，图像质量相当，且仅比Tiny-SD慢0.9%，但图像质量显著提升。

Conclusion: DiffusionX在保持高效和可扩展性的同时，显著降低了生成时间和云资源负担。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [29] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: TokenAR框架通过令牌级增强机制解决多参考图像生成中的身份混淆问题，显著提升了自回归模型在条件图像生成中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有自回归模型在多参考图像生成中难以解耦不同参考身份的问题。

Method: 提出TokenAR框架，包括令牌索引嵌入、指令令牌注入和身份令牌解耦策略。

Result: 在多个参考图像生成任务中超越了现有最先进模型，并发布了InstructAR数据集。

Conclusion: TokenAR框架有效提升了多参考图像生成的质量和多样性，代码和数据集已公开。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [30] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 研究发现，强化学习（RL）比监督微调（SFT）在多模态语言模型（MLLM）的视觉编码器中产生更强且更精确的视觉表示，并提出了一种高效优化方法PIVOT。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注MLLM中LLM主干的作用，而忽视了视觉编码器的重要性，尤其是在训练范式从SFT转向RL后，缺乏对视觉编码器如何被重塑的分析。

Method: 通过多种实验（如ImageNet分类、分割和梯度可视化）分析视觉编码器，并提出了PIVOT方法优化视觉编码器。

Result: RL训练的视觉编码器在视觉相关任务中表现优于SFT，且PIVOT方法在低计算成本下超越更大规模的视觉预训练模型。

Conclusion: PIVOT为MLLM的视觉主干提供了一种高效且有效的优化路径，显著提升了视觉编码器的性能。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [31] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出了一种基于梯度的框架GradNorm，用于在语言辅助图像聚类（LaIC）中筛选正名词，理论上有保障且实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖CLIP特征空间，缺乏理论基础，需要更严谨的筛选策略。

Method: 通过反向传播梯度的幅度衡量名词的积极性，提出理论误差界限。

Result: GradNorm在多个基准测试中达到最先进的聚类性能。

Conclusion: GradNorm不仅理论严谨，还优于现有方法，是LaIC领域的有效解决方案。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [32] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: MIRAD数据集是首个针对社交制造中异常检测的基准数据集，涵盖多样化产品、地理分散节点和成像异质性，评估显示现有方法在该领域表现显著下降。


<details>
  <summary>Details</summary>
Motivation: 社交制造中的质量控制和缺陷检测面临高度定制化、小批量订单和成像环境差异的挑战，缺乏真实数据集和专用算法。

Method: 提出MIRAD数据集，包含多样化产品、六地理分散节点的数据及成像异质性，并评估多种SOTA异常检测方法。

Result: 所有模型在MIRAD上的性能均显著下降，突显现实个性化生产中缺陷检测的复杂性。

Conclusion: MIRAD为工业5.0的稳健质量控制解决方案提供了现实基础，数据集已公开。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [33] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出了一个包含3000个白内障手术视频的数据集，具有多层次注释，用于训练深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 当前白内障手术数据集缺乏多样性和注释深度，限制了深度学习模型的泛化能力。

Method: 收集了来自两个手术中心的3000个视频，包含四种注释层：手术阶段、实例分割、器械-组织交互跟踪和技能评分。

Result: 通过基准实验验证了数据集在手术AI任务中的技术质量，并建立了领域适应基线。

Conclusion: 该数据集为计算机辅助手术系统的发展提供了重要资源。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [34] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个自动化平台，用于实时检测道路坑洼、GPS地理标记和动态道路健康可视化，支持智能治理和数据驱动的道路维护。


<details>
  <summary>Details</summary>
Motivation: 印度道路网络多样且维护不足，坑洼问题严重，亟需自动化解决方案以提高道路安全和维护效率。

Method: 使用自注释数据集和YOLO模型进行坑洼检测，结合OCR和GPS日志进行精确定位，并通过后端数据库管理元数据。

Result: 开发了一个成本效益高、可扩展的平台，支持实时监测、智能治理和公众参与。

Conclusion: iWatchRoadv2通过自动化监测生命周期，实现了数据驱动的智能城市管理和透明治理，提升了道路维护的可持续性。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [35] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter是一个数据驱动的参数化模型，用于建模植物的形态，包括拓扑、形状、关节和变形。


<details>
  <summary>Details</summary>
Motivation: 现有的参数化模型在人类和动物建模上表现强大，但在植物建模方面缺乏同等表达能力。

Method: Demeter通过编码植物的拓扑、形状、关节和变形到一个紧凑的学习表示中，处理不同物种的拓扑变化，并建模三种形状变化来源。

Result: 实验表明，Demeter能有效合成形状、重建结构和模拟生物物理过程。

Conclusion: Demeter为植物建模提供了一个强大的工具，适用于多种应用场景。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [36] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 提出了一种轻量级框架，通过稀疏卷积和SPLite解码器提升AR/VR设备上的深度学习模型效率，实现42%的端到端效率提升和3.1倍的解码加速。


<details>
  <summary>Details</summary>
Motivation: AR/VR设备的普及需要实时推理、低功耗和低延迟，但现有框架难以平衡效率与性能。

Method: 采用编码器-解码器架构，结合稀疏卷积和SPLite解码器，并应用量化感知训练。

Result: 在Raspberry Pi 5上实现2.98倍加速，解码帧率提升3.1倍，内存占用减少，精度损失极小（PA-MPJPE仅增加0.1 mm）。

Conclusion: 该方法在保持精度的同时显著提升了计算效率，适用于边缘设备。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [37] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM是一种创新的MLLM-agent框架，通过3D高斯溅射表示和全局到局部空间定位策略，实现了无需大量3D特定后训练的开世界推理分割。


<details>
  <summary>Details</summary>
Motivation: 解决复杂人类指令与精确3D对象定位之间的差距，现有方法在模糊推理指令和3D空间理解方面存在不足。

Method: 采用3D高斯溅射表示，提出全局到局部空间定位策略，先通过多全局视图粗定位，再通过局部视图细分割。

Result: 在LERF、3D-OVS和REALM3D基准测试中表现优异，支持多种3D交互任务。

Conclusion: REALM在3D对象分割和交互任务中具有实用性和多功能性。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [38] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL框架利用自监督学习任务作为RL微调的奖励信号，显著提升了视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视觉任务中依赖语言先验或文本捷径，缺乏可靠的奖励机制限制了RL的应用。

Method: 提出SSL4RL框架，将自监督学习目标转化为密集的自动奖励信号，无需人工偏好数据。

Result: 实验表明SSL4RL在视觉和视觉语言推理任务中表现优异，并验证了任务难度、模型规模等因素的影响。

Conclusion: SSL4RL为多模态模型对齐提供了通用且有效的自监督范式。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [39] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一种轻量级的点和线段匹配器，通过Attentional Line Message Passing（ALMP）技术，显著提升了匹配效率，并在多个基准测试中达到最新水平。


<details>
  <summary>Details</summary>
Motivation: 传统方法将点和线匹配视为独立任务，计算复杂度高且难以实时应用。GlueStick虽联合匹配但架构复杂，无法满足实时或边缘设备需求。

Method: 提出LightGlueStick，引入ALMP技术，显式地将线段连通性暴露给网络，实现节点间高效通信。

Result: 在多个基准测试中达到最新性能，代码已开源。

Conclusion: LightGlueStick通过轻量化设计和ALMP技术，解决了联合匹配的实时性问题，适用于边缘设备。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [40] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种可解释的深度伪造视频检测（EDVD）任务，并设计了EDVD-LLaMA多模态大语言模型推理框架，结合时空特征提取和细粒度多模态思维链机制，显著提升了检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法缺乏透明性和泛化能力，亟需能够提供可验证推理解释的检测器。

Method: 提出ST-SIT时空特征提取和Fg-MCoT细粒度多模态思维链机制，结合ER-FF++数据集进行双监督训练。

Result: EDVD-LLaMA在检测准确性、可解释性及跨伪造方法和跨数据集场景中表现出色。

Conclusion: 该方法为深度伪造视频检测提供了更可解释且性能优越的解决方案。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [41] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: RAVAR任务旨在通过自然语言描述识别特定人物的细粒度原子动作。RefAVA++数据集扩展至290万帧和75.1k标注人物。RefAtomNet++通过多层级语义对齐跨模态注意力机制和Mamba建模，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统动作识别任务在多人物复杂场景中难以精确理解语言引导的动作，RAVAR任务填补了这一空白。

Method: 提出RefAtomNet++框架，结合多层级语义对齐跨注意力机制和多轨迹Mamba建模，动态选择视觉空间标记。

Result: RefAtomNet++在RefAVA++数据集上达到新SOTA性能。

Conclusion: RefAtomNet++通过跨模态和多层级语义对齐显著提升了原子动作识别的精度。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [42] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 提出了一种改进的损失函数，通过高斯边界框表示和Bhattacharyya距离提升旋转物体检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统物体检测框架在旋转物体场景中表现不佳，无法有效捕捉方向变化。

Method: 采用各向异性高斯表示和旋转不变损失函数，结合深度学习检测器。

Result: 实验显示在平均精度指标上显著优于现有方法。

Conclusion: 该方法为旋转物体检测设立了新基准，适用于需要精确定位的多种应用。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [43] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种视觉提示初始化策略，通过优化提示与语义信息区域的对齐，并引入新的表示方向，显著提升了自监督模型的适应能力。


<details>
  <summary>Details</summary>
Motivation: 在大规模基础模型时代，完全微调预训练网络对每个下游任务资源消耗巨大。现有视觉提示调优方法在自监督骨干网络上表现不佳，尤其在挑战性任务和数据稀缺场景下。

Method: VIPAMIN通过（1）对齐提示与嵌入空间中的语义信息区域，（2）注入预训练子空间外的新表示方向，增强自监督模型的适应能力。

Result: VIPAMIN在多样任务和数据集规模上均表现优异，成为视觉提示调优的新标杆。

Conclusion: VIPAMIN以简单高效的方式提升了视觉提示调优的性能，尤其在资源受限场景下表现突出。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [44] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出了一种弱监督域适应（WDA）方法，利用稀疏点标签提升电子显微镜图像中线粒体分割的性能。


<details>
  <summary>Details</summary>
Motivation: 减少标注成本，解决无监督域适应（UDA）在实际应用中性能较低的问题。

Method: 多任务学习框架，结合分割与中心检测任务，采用交叉教学机制和类聚焦跨域对比学习。

Result: 在挑战性数据集上表现优于现有UDA和WDA方法，显著缩小与监督上限的性能差距。

Conclusion: 该方法在WDA和UDA设置下均显著提升性能，为生物和神经科学研究提供了高效解决方案。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [45] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出了一种基于Q学习的远见导航方法，结合未来信息优化决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖历史信息，忽视未来影响，需开发远见智能体。

Method: 利用Q学习训练Q模型生成Q特征，结合跨模态编码器预测未来行动得分。

Result: 在目标导向VLN数据集上验证了方法的有效性。

Conclusion: 结合未来信息的导航策略显著提升了性能。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [46] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: HGC-Avatar提出了一种分层高斯压缩框架，用于动态化身的传输和高质量渲染，显著提升了视觉质量和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于3DGS的压缩方法缺乏人体先验知识，导致比特率效率和重建质量不足，限制了其在可流式3D化身系统中的应用。

Method: 将高斯表示解耦为结构层（通过StyleUNet生成器映射姿势）和运动层（利用SMPL-X模型紧凑表示姿势变化），支持分层压缩和渐进解码。

Result: 实验表明，HGC-Avatar在视觉质量和压缩效率上均优于现有方法。

Conclusion: HGC-Avatar为快速3D化身渲染提供了可流式解决方案，并在低比特率下保持了面部细节。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [47] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench是首个基于真实科学论文中审稿人标记的不一致性的基准测试，用于评估大型多模态模型（LMMs）在多模态科学推理中的能力。


<details>
  <summary>Details</summary>
Motivation: 科学论文中的多模态不一致性（如文本、图表、公式间的矛盾）会影响清晰性、可重复性和信任，但现有基准测试未能有效捕捉这一问题。

Method: 通过审稿挖掘、LLM辅助过滤和人工验证的多阶段流程，收集了242篇论文中的262个不一致性，并设计了三个任务（识别、纠正和配对匹配）来评估LMMs。

Result: 测试了21个领先的LMMs，结果显示其性能较低（26.1-54.2%），凸显了多模态科学推理的挑战。

Conclusion: PRISMM-Bench揭示了LMMs在多模态科学推理中的局限性，为开发更可信的科学助手提供了动力。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [48] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: OOS-DSD是一种基于深度学习的零售货架缺货检测方法，通过辅助学习提升性能，超越现有方法1.8% mAP。


<details>
  <summary>Details</summary>
Motivation: 零售货架缺货检测（OOS）对库存管理至关重要，现有方法性能有待提升。

Method: 扩展YOLOv8架构，增加卷积分支同时检测OOS、分割产品和估计场景深度，深度分支使用伪标签训练。

Result: OOS-DSD在mAP上超越SOTA方法1.8%，辅助学习和深度归一化分别贡献3.7%和4.2%提升。

Conclusion: OOS-DSD通过多任务学习和深度归一化显著提升OOS检测性能。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [49] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出了一种基于图注意力网络（GAT）自编码器的图像分类与检索方法，通过构建图像和类别的代表模型实现分类与检索。


<details>
  <summary>Details</summary>
Motivation: 传统方法可能忽略图像间的上下文关系，本文旨在利用图结构和注意力机制捕捉关键特征和关系。

Method: 构建图像相似性图，使用GAT自编码器生成上下文感知的潜在表示，并基于代表模型进行分类和检索。

Result: 实验表明，该方法在分类和检索任务中优于传统基于特征的技术。

Conclusion: 代表模型结合GAT自编码器能有效提升图像分类与检索性能。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [50] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的小型无人机自主飞行系统，结合语义分割和单目深度估计，实现无GPS或LiDAR的避障、场景探索和安全着陆。


<details>
  <summary>Details</summary>
Motivation: 解决在受控室内环境中，无人机依赖昂贵传感器（如LiDAR）或GPS的问题，提供轻量级且高效的视觉导航方案。

Method: 采用自适应尺度因子算法将非度量深度预测转换为度量距离，结合知识蒸馏框架（SVM教师网络训练轻量级U-Net学生网络）实现实时语义分割。

Result: 在真实和数字孪生环境中测试，系统显著增加监视距离并减少任务时间，保持100%成功率；端到端学习后自主任务成功率达87.5%。

Conclusion: 该系统在结构化环境中推进了基于视觉的无人机导航，解决了度量深度估计和计算效率问题，适用于资源受限平台。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [51] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ方法通过添加重建和对齐目标，提升了视觉语言模型的组合推理能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在组合推理方面表现不佳，主要因为文本编码器过于关注单词而非其关系。

Method: READ方法在对比学习中引入两个辅助目标：token级重建和句子级对齐。

Result: READ-CLIP在五个组合推理基准测试中达到最优，比基线提升4.1%。

Conclusion: 重建和对齐目标互补，分别增强单词关系捕捉和语义一致性。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [52] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 提出了一种基于检索增强生成的方法，利用图数据库和Cypher查询语言接口，使大语言模型能够高效地处理3D场景图，显著提升自然语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理大规模或复杂3D场景图时无法扩展的问题，通过检索增强生成技术优化语言模型的场景图处理能力。

Method: 使用图数据库编码3D场景图，并引入Cypher查询语言接口作为工具，使语言模型能够检索相关数据以完成语言接地任务。

Result: 在指令跟随和场景问答任务中，该方法显著优于基线方法，且在处理大规模场景图时表现更优，同时大幅减少令牌数量。

Conclusion: 通过Cypher接口处理3D场景图的方法具有更好的扩展性和性能，适用于复杂的自然语言接地任务。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [53] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: GaitRDAE框架通过动态搜索运动区域和自适应时间尺度，提升步态识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态运动区域建模和适应特定模式方面存在不足。

Method: 提出Region-aware Dynamic Aggregation和Excitation模块，动态搜索最优时间感受野并强调稳定行为模式。

Result: GaitRDAE在多个基准数据集上达到最优性能。

Conclusion: GaitRDAE有效解决了动态运动区域建模问题，提升了步态识别准确性。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [54] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 该论文构建了一个高质量的开源荔枝数据集，用于目标检测和成熟度分类，包含多种天气和时间条件下的图像，并通过深度学习模型验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中标注一致的荔枝数据集，阻碍了基于视觉的采摘机器人开发。

Method: 通过采集不同品种、天气和时间条件下的荔枝图像，进行数据增强和深度图像采集，并由多人独立标注后验证。

Result: 数据集包含11,414张图像，标注了9,658对标签，并通过三种深度学习模型验证了其有效性。

Conclusion: 该数据集为荔枝检测和成熟度分类提供了高质量资源，推动了采摘机器人的发展。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [55] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 论文提出了首个基于真实世界政治深度伪造事件的系统基准测试，评估了现有检测模型的性能，发现其泛化能力不足，呼吁开发更具政治背景的检测框架。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的快速扩散增加了政治深度伪造的风险，现有检测模型在实验室数据上表现良好，但在真实场景中泛化能力有限。

Method: 使用政治深度伪造事件数据库作为基准，系统评估了学术界、政府和行业的最先进检测模型。

Result: 发现现有检测器在真实政治深度伪造上表现不佳，尤其是视频领域，且易受简单操作影响。

Conclusion: 需要开发更具政治背景的深度伪造检测框架，以更好地保护公众。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [56] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H是一种新型多任务学习框架，用于从单目图像进行语义分割、深度、边缘和表面法线估计，通过跨任务注意力模块提升预测一致性，并在轻量级ViT骨干上实现实时部署。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署实时空间感知需要高效的多任务模型，以利用互补任务信息并最小化计算开销。

Method: M2H引入基于窗口的跨任务注意力模块，支持结构化特征交换，同时保留任务特定细节，基于轻量级ViT骨干DINOv2。

Result: M2H在NYUDv2、Hypersim和Cityscapes数据集上优于现有多任务模型和单任务基线，同时保持计算效率。

Conclusion: M2H在真实数据中验证了实用性，适用于动态环境中的3D场景图构建。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [57] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: SHIELD框架通过重新加权视觉标记、引入噪声衍生标记和对抗攻击，有效减少LVLM中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在跨模态任务中表现出色，但物体幻觉问题（模型生成看似合理但不准确的物体描述）仍然是一个重要挑战。本文首次将LVLM幻觉问题追溯到视觉编码器，并识别出三个关键问题：统计偏差、固有偏差和脆弱性。

Method: 提出SHIELD框架，采用三种策略：重新加权视觉标记以减少统计偏差，引入噪声衍生标记以对抗固有偏差，应用对抗攻击和对比解码以解决脆弱性。

Result: 实验表明，SHIELD在多种基准测试和LVLM家族中有效减少了物体幻觉，同时在通用LVLM基准测试中表现优异。

Conclusion: SHIELD是一种无需训练的框架，能够广泛适用于解决LVLM中的物体幻觉问题。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [58] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector是一种轻量级插件框架，通过可学习的决策过程优化多模态大语言模型中的视觉令牌压缩，显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有令牌压缩技术因启发式规则和注意力偏差导致的关键信息丢失和性能下降问题。

Method: 提出VisionSelector，采用可微分Top-K机制和课程退火策略，实现高效自适应的令牌选择。

Result: 在30%保留预算下保持100%准确率，10%预算下性能提升12.14%，预填充速度翻倍。

Conclusion: VisionSelector在多种压缩率下表现优异，能自适应识别关键令牌，显著提升模型效率。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [59] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的实时医学图像分析框架，结合多种神经网络架构和优化策略，显著提升诊断准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像处理方法在精度、鲁棒性和速度上存在不足，无法满足实时临床需求。

Method: 整合U-Net、EfficientNet和Transformer等神经网络架构，采用模型剪枝、量化和GPU加速等实时优化策略。

Result: 在公开数据集上取得分类准确率92%以上、分割Dice分数91%以上，推理时间低于80毫秒。

Conclusion: 该框架能加速诊断流程，减轻临床负担，并支持可信AI在医疗环境中的集成。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [60] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse是一个新的多轮对话基准测试，包含647个对话，覆盖12个VLM评估基准的484个任务，用于评估VLMs在多轮交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能全面捕捉真实场景中的多轮对话复杂性，需要更全面的评估工具。

Method: 引入MultiVerse数据集，使用GPT-4o作为自动评估器，基于37个关键指标评估18个VLMs。

Result: 最强模型（如GPT-4o）在复杂多轮对话中成功率仅50%，完整对话上下文对小模型性能提升显著。

Conclusion: MultiVerse为评估VLMs的多轮交互能力提供了重要工具，凸显了上下文学习的重要性。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [61] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: UTAP是一种针对病理学基础模型的通用对抗扰动，能显著降低模型性能，具有普适性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型的关键漏洞，评估其鲁棒性，并为防御机制提供基准。

Method: 通过深度学习优化固定且微弱的噪声模式，添加到病理图像中，破坏模型特征表示能力。

Result: UTAP导致多种病理学基础模型性能显著下降，且对未见过的数据和模型有效。

Conclusion: UTAP为模型鲁棒性评估设定了高标准，强调了防御机制的重要性，并为对抗训练提供了工具。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [62] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 论文提出了一种名为HYDRA的新方法，通过知识蒸馏和光谱重建架构，显著提升了光谱重建的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有多尺度注意力方法在稀疏光谱下表现良好但无法适应现代高通道数HSI传感器的问题。

Method: 使用教师模型封装潜在高光谱数据，学生模型学习从自然图像到教师编码域的映射，并结合新颖的训练方法。

Result: 在所有指标上达到SOTA性能，包括18%的准确率提升，并在不同通道深度下实现更快的推理速度。

Conclusion: HYDRA方法有效解决了现有光谱重建模型的局限性，为高光谱图像应用提供了更高效的解决方案。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [63] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: MSSR框架通过构建最小充分信息集（MSS）解决VLMs在空间推理中的瓶颈，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决VLMs因2D预训练和冗余3D信息导致的空间推理能力不足问题。

Method: 提出双智能体框架（Perception Agent和Reasoning Agent），通过提取和迭代优化3D信息构建MSS。

Result: 在多个基准测试中实现最优性能，并提供可解释的推理路径。

Conclusion: MSSR通过追求信息的充分性和最小化，显著提升了空间推理能力，并为未来模型提供高质量训练数据。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [64] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为SDPA++的自监督去噪框架，仅需噪声OCT图像即可生成伪真实图像，并通过补丁聚合策略提升图像清晰度。


<details>
  <summary>Details</summary>
Motivation: 由于OCT图像中的固有斑点噪声和临床环境限制，获取干净的配对数据集困难，因此需要一种无需干净参考的自监督方法。

Method: 通过自融合和自监督去噪生成伪真实图像，并利用补丁聚合策略训练去噪模型。

Result: 在真实数据集上验证了性能提升，包括CNR、MSR、TP和EP等指标。

Conclusion: SDPA++在无干净参考的情况下提升了OCT图像质量，具有临床应用潜力。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [65] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 论文提出了一种新的域连接对比学习（DCCL）方法，通过增强跨域的概念连通性，解决了领域泛化（DG）中直接应用对比学习（CL）导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 训练和测试样本之间的分布偏移会阻碍模型泛化性能，领域泛化（DG）旨在仅使用源域数据预测未见目标域数据的标签。直接应用对比学习（CL）会降低性能，原因是DG设置中缺乏类内连通性。

Method: 提出DCCL方法，通过更激进的数据增强和跨域正样本提高类内连通性，同时采用模型锚定和生成变换损失来嵌入未见测试域。

Result: 在五个标准DG基准测试中，DCCL优于现有方法，即使没有域监督。

Conclusion: DCCL通过增强跨域连通性和类内表示，显著提升了领域泛化性能。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [66] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一种基于一致性模型的一步人体运动预测框架，通过单步生成高效预测运动状态，优于多步去噪的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型依赖多步去噪，效率较低，HumanCM旨在通过单步生成提高效率。

Method: 采用Transformer时空架构，学习噪声与干净运动状态的自一致映射，保留运动连贯性。

Result: 在Human3.6M和HumanEva-I数据集上，HumanCM性能媲美或优于扩散模型，推理步骤减少两个数量级。

Conclusion: HumanCM通过单步生成实现高效运动预测，为实时应用提供潜力。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [67] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SCENECOT的新框架，通过分步推理和视觉线索解决3D场景中的问答问题，并创建了首个大规模数据集SCENECOT-185K。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在问答任务中缺乏人类式的场景-对象推理机制，导致效果不佳。

Method: 引入基于多模态专家模块的SCENECOT方法，将复杂任务分解为简单问题，并构建视觉线索。

Result: 在多个3D场景推理基准测试中表现优异，问答一致性高。

Conclusion: 首次成功将CoT推理应用于3D场景理解，展示了扩展潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [68] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM是一种隐式残差世界模型，专注于建模当前状态和世界演变，通过预测变化而非完整重建未来场景，显著提升了自动驾驶系统的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶系统在视觉中心世界模型中存在冗余建模静态背景的问题，导致效率低下。

Method: IR-WM通过BEV表示当前状态，利用前一时刻的BEV特征作为时间先验，仅预测变化（残差），并通过对齐模块校准误差。

Result: 在nuScenes基准测试中，IR-WM在4D占用预测和轨迹规划方面表现最佳。

Conclusion: IR-WM通过隐式残差建模和误差校准，显著提升了自动驾驶系统的预测和规划性能。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [69] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer是一种新型语义分割模型，用于在噪声监督下实现高精度珊瑚礁地图绘制，性能优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 全球珊瑚礁地图产品（如Allen Coral Atlas）在空间精度和语义一致性上存在不足，尤其是在需要细粒度边界划分的区域。

Method: 基于UKAN架构，UKANFormer在解码器中引入了全局-局部Transformer（GL-Trans）块，以提取全局语义结构和局部边界细节。

Result: 在实验中，UKANFormer的珊瑚类IoU达到67.00%，像素准确率为83.98%，优于传统基线。

Conclusion: UKANFormer证明模型设计可以缓解标签噪声，支持在不可靠监督下进行可扩展的珊瑚礁地图绘制。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [70] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 该论文提出了一个关于世界模型在具身AI中的统一框架，包括问题设定、学习目标和三轴分类法，并总结了数据资源、评估指标及开放挑战。


<details>
  <summary>Details</summary>
Motivation: 研究世界模型在具身AI中的作用，以支持感知、预测和决策，并提供一个系统化的分类和评估框架。

Method: 提出三轴分类法（功能、时间建模、空间表示），系统化数据资源和评估指标，并进行定量比较。

Result: 总结了当前世界模型的性能，并指出了数据集统一性不足、评估指标需改进等开放挑战。

Conclusion: 世界模型在具身AI中至关重要，但仍需解决数据集、评估指标和计算效率等挑战。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [71] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 研究表明，离散的自回归模型在图像生成中通过搜索策略（如束搜索）能显著提升性能，优于更大规模的扩散模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何在图像生成中有效应用搜索策略，以提升性能。

Method: 采用离散的自回归模型，结合束搜索策略进行图像生成。

Result: 2B参数的自回归模型通过束搜索优于12B参数的扩散模型。

Conclusion: 模型架构（离散性）对推理时优化至关重要，而不仅仅是规模。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [72] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类感知的显著性评估方法，用于检测和评估图像超分辨率中的伪影，并发布了相关数据集和代码。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率模型在提升视觉质量的同时，容易产生伪影，且不同伪影对人类感知的影响差异较大，因此需要一种基于显著性的评估方法。

Method: 构建了一个包含1302个伪影示例的数据集，并通过众包评分训练了一个轻量级回归器，生成空间显著性热图。

Result: 提出的回归器在检测显著伪影方面优于现有方法。

Conclusion: 通过显著性评估方法可以更有效地检测和缓解超分辨率伪影，相关数据集和代码已开源。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [73] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: WaMaIR是一种新型图像修复框架，通过扩大感受野和增强通道特征建模，显著提升了纹理细节的恢复效果。


<details>
  <summary>Details</summary>
Motivation: 解决CNN方法在图像修复中因小感受野和缺乏通道特征建模而难以恢复精细纹理的问题。

Method: 提出GMWTConvs扩大感受野，MCAM模块捕获长程依赖，MTELoss指导纹理保留。

Result: 实验表明WaMaIR在图像修复和计算效率上优于现有方法。

Conclusion: WaMaIR通过多尺度特征和通道感知模块，有效提升了纹理细节的恢复能力。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [74] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出了一种基于文本的图像编辑框架Region in Context，通过多级语义对齐实现更一致和协调的编辑效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理图像区域时孤立依赖局部线索，导致编辑不一致或不自然，缺乏全局视觉和语义协调。

Method: 引入双级引导机制，结合全图像上下文和详细区域描述，同时匹配场景级描述，实现局部修改与全局结构的协调。

Result: 实验表明，该方法能生成更一致且符合指令的编辑结果。

Conclusion: Region in Context通过全局上下文理解，显著提升了文本条件图像编辑的精确性和协调性。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [75] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: EMRRG框架通过参数高效方法微调预训练的Mamba网络，结合Partial LoRA和混合解码器LLM，显著提升了X光报告生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MRG模型主要依赖LLM，忽视了预训练视觉基础模型和高级微调技术的潜力，且非Transformer架构如Mamba网络未被充分探索。

Method: 将X光图像分块并标记化，通过SSM视觉骨干提取特征，采用Partial LoRA优化性能，结合混合解码器LLM生成报告。

Result: 在三个基准数据集上验证了方法的有效性，性能显著提升。

Conclusion: EMRRG框架为X光报告生成提供了新思路，展示了非Transformer架构的潜力，代码将开源。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [76] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE提出了一种基于Bundle Adjustment原理的6D物体姿态估计新方法，通过Lie代数和3DGS技术优化姿态估计，并在纹理缺失和光照变化条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在纹理缺失和光照变化条件下表现不佳，GS2POSE旨在解决这些问题。

Method: 利用Lie代数和3DGS技术，构建姿态可微渲染管道，通过迭代优化输入图像与渲染图像的差异来估计姿态，并更新3DGS模型的颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上，GS2POSE分别提升了1.4%、2.8%和2.5%的准确率。

Conclusion: GS2POSE通过结合Bundle Adjustment和3DGS技术，显著提升了6D姿态估计的精度和鲁棒性。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [77] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出了一种无需训练的视频理解框架，结合预训练视觉语言模型和经典机器学习算法，实现零样本视频内容分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据，成本高且扩展性差，本文旨在解决这一问题。

Method: 将视频理解重构为自监督时空聚类问题，利用预训练VLM提取语义特征，再通过KTS和密度聚类分割视频内容。

Result: 框架能自动生成视频的多模态结构化摘要，实现零样本分析。

Conclusion: 该方法为视频内容分析提供了一种高效、可解释且模型无关的途径。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [78] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种新型的即插即用解决方案，通过在冻结的MLLM上附加轻量级可训练头部，实现像素级分割，同时保持模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法在MLLM中引入分割能力时需要微调模型，这会改变输出空间并损害其泛化能力，违背了构建统一模型的目标。

Method: LENS通过优化注意力图中的空间线索提取关键点，并将其描述为与掩码解码器兼容的点级特征。

Result: LENS在分割性能上优于或与基于重新训练的方法相当，同时完全保留了MLLM的泛化能力。

Conclusion: LENS的设计为扩展MLLM提供了一种高效且强大的范例，推动了真正多功能的统一模型的发展。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [79] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的二元道路分割方法，利用几何和时序信息，无需手动标注数据。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵手动标注数据的依赖，实现可扩展的无监督道路分割。

Method: 通过几何先验生成弱标签，利用时序一致性优化分割结果。

Result: 在Cityscapes数据集上达到0.82的IoU，表现高效且简单。

Conclusion: 几何约束与时序一致性结合在无监督道路分割中具有潜力。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [80] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: PIF是一种基于预训练扩散模型的个性化图像滤镜，用于学习和转移摄影风格。


<details>
  <summary>Details</summary>
Motivation: 摄影风格是著名摄影师的魅力所在，但学习和转移风格需要深入理解照片的编辑过程。现有方法无法有效学习摄影概念或保留内容图像。

Method: 基于文本到图像扩散模型，PIF通过文本反转技术学习参考图像的摄影风格，优化摄影概念的提示词。

Result: PIF在提取和转移多种摄影风格方面表现出色。

Conclusion: PIF通过生成先验和文本反转技术，有效解决了摄影风格学习和转移的问题。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [81] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大型公开珊瑚礁图像数据集，提供细粒度、全球范围的分类标签，旨在推动珊瑚礁监测和保护的自动化研究。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁因气候变化等人为压力迅速减少，亟需可扩展的自动化监测方法。

Method: ReefNet整合了76个CoralNet来源和红海Al Wajh的图像，包含约925000个专家验证的硬珊瑚注释，并提出了两种评估设置：源内和跨源基准。

Result: 监督学习在源内表现良好，但在跨域时性能显著下降；零样本模型表现普遍较差，尤其是对稀有和视觉相似的属。

Conclusion: ReefNet为领域泛化和细粒度珊瑚分类提供了挑战性基准，旨在推动全球珊瑚礁监测和保护的研究。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [82] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提出了一种名为AdaptMoist的领域自适应方法，结合五种纹理特征预测木屑水分含量，准确率达95%，跨领域预测精度提升23%。


<details>
  <summary>Details</summary>
Motivation: 现有直接方法耗时且破坏样本，间接方法因木屑来源多样导致准确性不足，需一种能应对来源变异的稳健方法。

Method: 分析五种纹理特征，提出AdaptMoist方法进行领域自适应，利用调整互信息作为模型保存标准。

Result: 组合特征准确率95%，AdaptMoist跨领域平均精度80%，比非自适应模型高23%。

Conclusion: AdaptMoist是跨领域木屑水分预测的有效解决方案，适用于依赖木屑的行业。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [83] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: M2HVideo框架通过动态姿态感知头部编码器和镜像损失，解决了人偶到真人视频生成中的身份漂移和细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 人偶展示服装成本低但缺乏真实感，需生成身份可控、逼真的真人视频。

Method: 提出M2HVideo框架，结合动态姿态感知头部编码器、镜像损失和分布感知适配器。

Result: 在多个数据集上表现优于现有方法，服装一致性、身份保留和视频保真度更优。

Conclusion: M2HVideo有效解决了人偶视频生成中的关键挑战，提升了真实感和一致性。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [84] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 2DGS-R通过分层训练方法提升渲染质量，同时保持几何精度，仅增加1%存储和少量训练时间。


<details>
  <summary>Details</summary>
Motivation: 3DGS和2DGS在渲染质量和几何精度之间存在矛盾，难以同时优化。

Method: 采用分层训练：先训练原始2D高斯并正则化，再选择渲染质量不足的高斯进行克隆增强，最后微调模型。

Result: 实验显示2DGS-R仅增加1%存储和少量训练时间，但显著提升渲染质量和几何精度。

Conclusion: 2DGS-R在效率和性能间取得平衡，提升了视觉保真度和几何重建精度。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [85] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer是一种轻量级基于Transformer的语义分割框架，结合CBAM和MixVisionTransformer，在边缘设备上实现高精度武器检测。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法缺乏细粒度分割能力，现有语义分割模型在精度和计算效率之间难以平衡。

Method: 结合CBAM增强的编码器与注意力集成的解码器，实现多类武器分割。

Result: 在80.64% mIoU和89.13% mFscore下达到实时推理（82.26 FPS），计算资源需求极低。

Conclusion: ArmFormer是分布式安全基础设施中部署的理想解决方案。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [86] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: BARL框架通过双边对齐标签和表示空间，显著提升半监督医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法忽视表示空间对齐，导致模型学习到的特征缺乏判别性和空间一致性。

Method: BARL框架结合双路径正则化和渐进认知偏差校正，实现标签和表示空间的双边对齐。

Result: 在四个公开基准和私有CBCT数据集上，BARL均超越现有方法。

Conclusion: BARL通过双边对齐策略有效解决了半监督医学图像分割中的关键问题。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [87] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种基于配准的旋转不变特征提取框架，用于3D点云异常检测，解决了现有方法在特征变换不一致和局部几何细节捕捉不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于记忆库的方法在特征变换和旋转不变性方面表现不佳，尤其是在配准失败时，检测结果不可靠。

Method: 通过将特征提取嵌入配准学习过程，联合优化配准和表示学习，获得旋转鲁棒且适用于异常检测的特征。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上，方法在效果和泛化性上优于现有方法。

Conclusion: 配准不仅用于几何结构对齐，还能引导特征提取，实现旋转不变性和局部判别性，提升异常检测性能。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [88] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 论文提出了一种神经元级别的分析框架，通过结合人工神经元分析和fMRI编码，研究了视觉语言模型（VLMs）的多模态信息处理机制，揭示了与人类大脑神经活动的相似性。


<details>
  <summary>Details</summary>
Motivation: 当前对人工神经网络（ANNs）与人类大脑处理之间相似性的理解有限，尤其是多模态研究和神经元级别分析的缺失。

Method: 结合细粒度人工神经元分析和基于fMRI的体素编码，研究了两种不同架构的VLMs（CLIP和METER）。

Result: 发现ANNs能预测生物神经元活动，功能冗余性、极性模式相似，且不同架构的VLMs驱动不同的生物神经元活动。

Conclusion: 研究证明了VLMs在神经元级别上具有类似大脑的分层处理能力。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [89] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出了一种分类引导的扩散模型Class-N-Diff，用于同时生成和分类皮肤镜图像，提升生成图像的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型难以准确生成特定医学类别的图像，限制了其在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成分类器，基于类别条件引导图像生成。

Result: 模型能更好地控制类别条件图像合成，生成更真实多样的图像，分类器性能也有所提升。

Conclusion: Class-N-Diff是一种增强扩散模型合成皮肤镜图像质量和实用性的强大工具。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [90] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1是一个基于策略优化的后训练框架，用于指令驱动的图像编辑，通过DiffusionNFT和MLLM奖励模型提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 监督微调模型容易过拟合，限制了其在训练分布外的探索和泛化能力。

Method: 采用DiffusionNFT策略优化方法，结合MLLM作为统一奖励模型，并设计低方差组过滤机制减少噪声。

Result: 在ImgEdit和GEdit-Bench基准测试中取得SOTA结果（4.49和7.83分），且框架适用于多种基础模型。

Conclusion: Edit-R1框架具有广泛适用性，显著提升了指令驱动图像编辑的性能。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [91] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 论文提出了一种基于地面摄像机的尾迹-航班归因方法，解决了卫星数据分辨率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 航空非CO2效应（如尾迹）对气候影响显著，但现有卫星数据难以准确追踪尾迹来源。

Method: 利用地面摄像机高分辨率数据，结合航班监控和气象数据，提出模块化归因框架。

Result: 框架支持多种几何表示和距离度量，提供灵活的概率分配策略。

Conclusion: 研究为未来尾迹-航班关联研究奠定了坚实基础。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [92] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 该论文探讨了基于Transformer的架构在热成像武器分割中的应用，展示了其在低光和遮挡环境下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光和视觉遮挡条件下具有重要应用价值，但传统CNN方法在捕捉长距离依赖和精细结构细节方面存在局限。

Method: 论文评估了四种基于Transformer的架构（SegFormer、DeepLabV3+、SegNeXt和Swin Transformer），使用自定义热成像数据集进行训练和比较。

Result: SegFormer-b5表现最佳（mIoU 94.15%），SegFormer-b0速度最快（98.32 FPS），其他架构也展示了良好的性能平衡。

Conclusion: Transformer架构在热成像武器分割中表现出色，适用于实时安全应用，提供了灵活的精度-速度权衡。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [93] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: Res-Bench是一个评估多模态大语言模型（MLLMs）分辨率鲁棒性的新基准，包含14,400个样本，覆盖12种分辨率和6种核心能力维度。


<details>
  <summary>Details</summary>
Motivation: 当前评估范式主要关注语义性能，忽略了分辨率鲁棒性（即性能在不同输入分辨率下的稳定性）。

Method: 设计了包含Spearman相关性和绝对/相对连续误差（ACE/RCE）的新评估框架，用于大规模评估MLLMs。

Result: 分析了模型和任务的鲁棒性、预处理策略（如填充和超分辨率）以及微调对稳定性的影响。

Conclusion: Res-Bench填补了分辨率鲁棒性评估的空白，为MLLMs的优化提供了新方向。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [94] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文综述了医学图像分析中基础模型（FMs）的研究进展，系统分类了视觉和视觉语言FMs，并讨论了挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析领域缺乏对FMs架构、训练范式及临床应用的统一综述，本文旨在填补这一空白。

Method: 通过系统分类和定量元分析，总结了FMs在医学图像中的应用及趋势。

Result: 揭示了数据集利用和应用领域的时间趋势，并提出了领域适应、高效微调等挑战的解决方案。

Conclusion: 未来研究应增强FMs的鲁棒性、可解释性和临床整合，以加速其实际应用。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [95] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: Di-Bregman提出了一种基于Bregman散度的密度比匹配框架，用于加速扩散模型的蒸馏过程，实验证明其在一步生成中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型生成质量高但计算成本大，现有蒸馏方法缺乏统一的理论基础。

Method: 提出Di-Bregman框架，将扩散蒸馏建模为Bregman散度密度比匹配问题。

Result: 在CIFAR-10和文本到图像生成任务中，Di-Bregman在一步FID上优于反向KL蒸馏，并保持高视觉保真度。

Conclusion: Bregman密度比匹配是一种实用且理论完备的方法，可用于高效的一步扩散生成。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [96] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: CARE框架通过序列-图像对比对齐（SICA）和分类联合优化，实现了对事件触发传感器流中日常活动（ADL）的高效识别，并在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在表示层面存在局限性，序列方法对噪声敏感且缺乏空间感知，图像方法则压缩了时间动态和扭曲了传感器布局。需要一种方法能结合两者的互补优势。

Method: 提出CARE框架，结合时间感知的序列编码和空间感知的图像表示，通过对比对齐和分类联合优化学习对齐且可区分的嵌入。

Result: 在三个CASAS数据集上，CARE分别达到89.8%、88.9%和73.3%的准确率，表现出对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过联合优化表示学习和分类任务，显著提升了ADL识别的性能，适用于智能家居中的可靠应用。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [97] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练、在线视频步骤定位（VSG）的方法，利用大型多模态模型（LMMs）的零样本能力，并通过贝叶斯过滤进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统VSG方法需要标注数据和离线处理，成本高且不适用于在线场景。本文探索如何在无训练和在线条件下实现VSG。

Method: 使用LMMs预测视频帧的步骤，并结合贝叶斯过滤（BaGLM）注入历史帧知识，通过依赖矩阵和步骤进度估计优化预测。

Result: BaGLM在三个数据集上优于基于训练的离线方法。

Conclusion: BaGLM展示了无训练和在线VSG的可行性，性能超越现有方法。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [98] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 论文研究了不同视频特征对时间视频定位任务的影响，发现特征选择对性能有显著差异，并揭示了潜在的特征互补性。


<details>
  <summary>Details</summary>
Motivation: 由于现有研究集中在少数视频表示上，可能导致架构过拟合，因此需要研究不同视频特征的影响。

Method: 使用基于CNN、时间推理和Transformer的视频编码器提取特征，并在三个基准数据集上进行实验。

Result: 结果显示不同视频编码器对模型性能有显著影响，并揭示了特定特征的使用模式和错误。

Conclusion: 研究表明视频特征的选择对时间视频定位任务至关重要，并暗示了特征互补性的潜力。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [99] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 研究挑战了专用基础模型在遥感应用中优于通用视觉基础模型的观点，发现小规模下无显著优势。


<details>
  <summary>Details</summary>
Motivation: 遥感图像具有独特特性和应用需求，但研究质疑专用模型是否比通用模型更有用。

Method: 设计基准测试评估模型在低分辨率图像上的泛化能力，并在MillionAID数据集上训练改进的iBOT模型。

Result: 专用预训练模型在ViT-B规模下未带来一致改进。

Conclusion: 小规模场景中，通用视觉基础模型与专用模型表现相当。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [100] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种利用多模态大语言模型进行细粒度视频时间定位的方法，通过两阶段处理显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 利用多模态大语言模型联合处理文本和视频，以更准确地定位视频中的自然语言查询。

Method: 采用两阶段方法：首先将查询转换为包含更多细节的句子，然后使用轻量级解码器进行定位，并通过多实例学习目标减少噪声影响。

Result: 在多个基准测试中达到最先进水平，显著优于其他基于LLM的方法，并在零样本评估中表现优异。

Conclusion: ED-VTG在多模态视频时间定位任务中表现出色，尤其在零样本场景下具有明显优势。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [101] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 提出W2R2框架，通过解耦表示学习和抑制2D语义偏差，提升3D定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D定位模型过度依赖2D图像特征，导致3D几何信息利用不足，性能受限。

Method: 设计双目标损失函数（对齐损失和伪标签损失），分别优化语义和空间特征表示。

Result: 在ScanRefer和ScanQA数据集上验证，定位精度和鲁棒性显著提升。

Conclusion: W2R2框架有效解决了2D语义偏差问题，无需修改推理架构即可实现精确3D定位。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [102] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 提出了一种利用StyleGAN2-ADA和StyleGAN3生成高分辨率合成指纹的方法，并通过CycleGANs生成逼真的假指纹，解决了生物特征数据收集中的隐私、成本和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 解决真实指纹数据集收集耗时、昂贵且隐私要求严格的问题，探索合成指纹数据的应用。

Method: 使用条件StyleGAN2-ADA和StyleGAN3生成合成指纹，CycleGANs生成假指纹，创建了两个合成数据集（DB2和DB3）。

Result: StyleGAN3的FID低至5，合成指纹在0.01% FAR下的TAR为99.47%，隐私保护性能强。

Conclusion: 合成指纹数据集在性能和隐私保护方面表现优异，适用于生物特征识别和防伪系统开发。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [103] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 该研究开发了一种结合临床医生参与的深度学习（DL）流程，用于提高肺癌CT图像分割的重复性、预后准确性和临床信任度。VNet模型表现最佳，半监督学习（SSL）优于监督学习（SL），临床医生更倾向于使用AI生成的初始掩模进行细化。


<details>
  <summary>Details</summary>
Motivation: 肺癌是癌症死亡的主要原因，CT成像是筛查、预后和治疗的核心。手动分割存在变异性且耗时，而DL虽能自动化但面临临床采用障碍。

Method: 研究使用多中心CT数据（999名患者，12个公共数据集），评估了五种DL模型（如3D Attention U-Net、VNet等），并通过放射组学特征和预后建模（SL和SSL）进行验证。六名医生对掩模进行了定性评估。

Result: VNet表现最佳（Dice = 0.83，IoU = 0.71），SSL在预测准确性上优于SL（准确率=0.88，F1=0.83）。临床医生更倾向于使用AI生成的掩模进行细化。

Conclusion: 结合VNet和SSL的方法能够提供准确、可重复且受临床信任的肺癌预后工具，为以医生为中心的AI转化提供了可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [104] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出一种广义选择方法，改进行人重识别性能，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有方法在类别代表选择上研究不足，导致重识别性能不佳。

Method: 提出不限于类别质心的广义选择方法，平衡准确性和平均精度。

Result: 在多种嵌入方法上应用，均显著提升性能。

Conclusion: 广义选择方法有效改进行人重识别，适应不同应用需求。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [105] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 提出V-Reason方法，通过熵信号优化LMM的推理行为，无需训练即可提升视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频推理方法依赖昂贵的RL和冗长的思维链，计算开销大且控制机制有限。

Method: 利用模型输出的熵信号，通过小型可训练控制器优化LMM的值缓存，调整推理时的微观探索与利用行为。

Result: 在多个视频推理数据集上显著超越基础模型，接近RL模型精度（差距0.6%），同时减少58.6%的输出token。

Conclusion: V-Reason通过熵驱动的推理优化，实现了高效且高性能的视频推理，无需额外训练。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [106] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 比较通用视觉模型Hiera与专用分割模型SAM2的特征适应性，发现专用化在空间任务中高效但牺牲了语义广度。


<details>
  <summary>Details</summary>
Motivation: 研究通用与专用视觉模型在特征编码设计中的权衡，以优化下游应用策略。

Method: 使用轻量可训练颈部模块探测冻结特征的适应性，量化专用化的信息成本。

Result: SAM2在空间任务（如深度估计）中表现优异，但在语义任务（如姿态估计）中不如Hiera。

Conclusion: 专用化带来任务效率提升，但需权衡语义信息损失，为特征编码设计提供量化依据。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [107] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: ProDAT是一种基于密度感知的渐进式点云编码方法，支持多比特率解码，显著提升编码效率。


<details>
  <summary>Details</summary>
Motivation: 3D点云在实时处理和低延迟应用中需求增长，但大数据量和带宽限制阻碍了高质量服务的部署。现有学习型编码方法不支持渐进解码。

Method: 提出ProDAT，利用密度信息指导潜在特征和坐标的自适应解码，实现单一模型支持多比特率渐进解码。

Result: 在基准数据集上，ProDAT不仅支持渐进编码，且编码效率优于现有技术，PSNR-D2在SemanticKITTI上提升28.6%，在ShapeNet上提升18.15%。

Conclusion: ProDAT通过密度感知机制有效解决了渐进式点云编码问题，显著提升了编码效率和适应性。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [108] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: FMCAF是一种预处理架构，通过频域过滤和跨注意力融合提升RGB与红外图像的多模态检测性能，具有通用性。


<details>
  <summary>Details</summary>
Motivation: 多模态目标检测在复杂条件下通过融合多传感器数据提升鲁棒性，但现有方法缺乏通用性。

Method: FMCAF结合频域过滤块（Freq-Filter）和跨注意力融合模块（MCAF），抑制冗余特征并增强模态间特征共享。

Result: 在LLVIP和VEDAI数据集上，FMCAF分别提升1.1%和13.9%的mAP@50，优于传统融合方法。

Conclusion: FMCAF作为一种通用且灵活的多模态融合方法，为未来检测流程提供了坚实基础。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [109] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane利用平面先验提升高斯泼溅的几何精度，生成结构化网格。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法在平面区域重建时平滑性和精度不足。

Method: 结合分割和法线预测模型提取平面先验，动态高斯重分类器优化训练。

Result: 显著提升几何精度，保持渲染质量，减少网格顶点和面数。

Conclusion: GSPlane通过结构化平面表示增强下游应用能力。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [110] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 论文提出了一种优化预训练扩散模型条件化的策略，通过潜在细化管道和动态交互提升低光场景下的内容保真度。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型在低光场景中因信息退化导致内容保真度不足，需解决潜在条件建模缺失和双向交互不足的问题。

Method: 引入潜在细化管道恢复VAE编码丢失的空间细节，并通过动态交互优化条件潜在与噪声潜在的关系。

Result: 实验表明，该方法显著提升了预训练扩散模型的内容保真度，同时保持感知真实性和美学效果。

Conclusion: 提出的方法为预训练扩散模型提供了更有效的控制，适用于现有网络，提升了低光场景的恢复性能。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [111] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 利用LED环境光生成视觉不可见水印的方法，适用于消费级相机。


<details>
  <summary>Details</summary>
Motivation: 通过优化LED光谱，实现对人眼不可见但对相机可检测的水印，以支持隐私保护和内容验证。

Method: 结合人眼视觉敏感度、相机传感器光谱敏感度和LED窄带光谱能力，采用光谱调制而非强度调制。

Result: 在10秒视频中嵌入128位信息，支持低帧率（30-60 fps）提取。

Conclusion: 该方法为隐私保护和内容验证提供了实用且高效的水印解决方案。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [112] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: GOOD是一个通过双级引导（图像级和特征级）直接指导扩散采样轨迹生成OOD样本的框架，显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖文本条件嵌入的扰动，导致语义不稳定和多样性不足，限制了OOD检测的泛化能力。

Method: GOOD框架利用现成的ID分类器，通过图像级和特征级双引导机制生成OOD样本。

Result: 实验表明，GOOD生成的样本显著提升了OOD检测性能。

Conclusion: GOOD通过双引导设计和统一OOD评分，实现了更可控和多样化的OOD样本生成，增强了检测鲁棒性。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [113] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D：一种基于扩散模型的统一框架，用于从单视图输入重建和生成类别级铰接物体形状，并估计其姿态。


<details>
  <summary>Details</summary>
Motivation: 铰接物体（如笔记本电脑和抽屉）的多部分几何结构和可变关节配置导致3D重建和姿态估计困难，需要处理结构多样性。

Method: 1. 使用Kinematic-Aware VAE (KA-VAE)将完整几何（SDF）、关节角度和部件分割编码到结构化潜在空间；2. 采用两个条件扩散模型分别回归全局姿态（SE(3)）和关节参数，以及从部分观测生成运动学感知潜在代码；3. 通过迭代优化模块双向优化重建精度和运动学参数。

Result: 在合成、半合成和真实数据集上验证了方法的有效性，能够准确重建铰接物体并估计其运动学属性。

Conclusion: KineDiff3D通过运动学感知的扩散模型和迭代优化，成功解决了铰接物体的重建和姿态估计问题。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [114] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD是一个两阶段后训练框架，通过结合几何先验和强化学习，从单张图像生成更准确且简洁的CAD模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在从2D图像推断3D几何时存在空间推理能力不足的问题，限制了工业概念设计的效率。

Method: GACO-CAD采用两阶段方法：1) 监督微调阶段，利用深度和表面法线图作为几何先验；2) 强化学习阶段，引入组长度奖励以生成更简洁的建模序列。

Result: 在DeepCAD和Fusion360数据集上，GACO-CAD在代码有效性、几何准确性和建模简洁性方面均优于现有方法。

Conclusion: GACO-CAD通过结合几何先验和强化学习，显著提升了从单张图像生成CAD模型的性能。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [115] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 研究探讨了人脸识别系统中预处理技术对对抗攻击迁移性的影响，并提出了一种预处理不变的方法以提高攻击迁移性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易受到对抗样本攻击，而预处理技术在此过程中常被忽视，尤其是在黑盒设置中。研究旨在填补这一空白。

Method: 通过实验评估不同预处理技术（如人脸检测模型和插值方法）对对抗攻击迁移性的影响，并提出一种预处理不变的方法。

Result: 人脸检测模型的选择可使攻击成功率降低78%，而插值方法影响较小。提出的预处理不变方法将攻击迁移性提高了27%。

Conclusion: 预处理在人脸识别系统中至关重要，应考虑其对对抗攻击迁移性的影响以提高系统的鲁棒性。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [116] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: GtR是一种训练免费的分层采样策略，通过两阶段生成（结构生成和细节重建）加速MAR模型，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决MAR模型在并行生成中因空间相关视觉标记建模复杂而受限的加速潜力问题。

Method: 提出GtR策略，分两阶段生成：全局语义结构生成和高效细节重建，并结合FTS方法优化计算资源分配。

Result: 在ImageNet和文本到图像生成任务中实现3.72倍加速，质量接近原始模型（FID:1.59，IS:304.4）。

Conclusion: GtR显著优于现有加速方法，适用于不同模型规模和生成任务。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [117] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 论文针对浮游生物识别模型在真实部署中因分布偏移（OoD）导致的挑战，设计了系列OoD基准测试，评估了22种方法，发现ViM方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 浮游生物的复杂形态、物种多样性和新物种的不断发现导致训练与测试数据分布偏移，引发预测错误，需系统评估OoD检测方法。

Method: 基于DYB-PlanktonNet数据集设计多种分布偏移场景的OoD基准测试，系统评估22种OoD检测方法。

Result: ViM方法在基准测试中显著优于其他方法，尤其在Far-OoD场景下关键指标提升明显。

Conclusion: 研究为浮游生物OoD检测提供了算法选择参考和未来研究基础，是首次大规模系统评估浮游生物OoD检测方法。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [118] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 提出了一种联合学习头部虚拟形象和手脸交互引起的非刚性变形的新框架，解决了现有方法忽略手脸交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了手脸交互（如手托下巴）传达的认知状态，因此需要一种能够同时建模头部虚拟形象和手脸交互变形的方法。

Method: 结合深度顺序损失和接触正则化进行姿态跟踪，学习手引起的面部变形的PCA基，并通过接触损失减少穿透伪影。

Result: 在iPhone拍摄的RGB(D)视频和合成数据集上验证，相比现有方法能捕捉更准确的面部变形几何和外观。

Conclusion: 该方法能够更准确地建模手脸交互引起的面部变形，提升了虚拟形象的物理合理性和真实感。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [119] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC是一种双曲表示学习框架，用于解决广义类别发现（GCD）中的领域泛化问题，无需依赖目标域数据或复杂的训练模拟。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法通常假设训练和测试数据来自同一领域，且需要同时访问标记和未标记数据，限制了在开放世界场景中的适用性。DG-GCD旨在解决这一问题，但现有方法（如DG2CD-Net）计算成本高且易出错。

Method: HIDISC通过GPT引导的扩散增强源域数据，避免过拟合；引入Tangent CutMix在切线空间合成伪新样本；结合惩罚Busemann对齐、双曲对比正则化和自适应离群排斥的统一损失函数，优化嵌入空间。

Result: HIDISC在PACS、Office-Home和DomainNet数据集上表现优异，超越现有欧几里得和双曲GCD基线方法。

Conclusion: HIDISC通过双曲表示学习和高效数据增强，实现了领域和类别的泛化，为GCD问题提供了更高效的解决方案。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [120] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出了一种零样本方法，通过引入提示感知视角，平衡任务相关性和信息多样性，显著降低视觉令牌冗余和推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在修剪视觉令牌时通常忽略文本提示的指导，导致无法优先考虑任务相关性。

Method: 采用分层方法，先选择任务相关的核心视觉令牌，再补充多样性令牌以保留更广泛的上下文。

Result: 在多个模型和基准测试中，即使修剪90%的令牌，性能仍匹配或超越现有技术，且显著降低GPU内存占用和推理延迟。

Conclusion: 该方法通过平衡任务相关性和信息多样性，有效解决了视觉令牌冗余问题，提升了效率和性能。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [121] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 论文提出了一种基于Segment Anything Model (SAM)的方法，用于高精度追踪孟加拉国河流侵蚀导致的土地和村庄消失问题。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国的河流每年侵蚀大量土地和村庄，传统人工追踪方法效率低下，亟需自动化解决方案。

Method: 通过颜色通道分析初步分割土地和水域，并微调SAM的掩码解码器以识别河岸侵蚀特征。

Result: 模型在IoU和Dice分数上分别达到86.30%和92.60%，显著优于传统方法和现成深度学习模型。

Conclusion: 该方法为政策制定者和灾害管理机构提供了监测河岸侵蚀和保护脆弱社区的新工具。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [122] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 利用VALORANT游戏的小地图信息构建回合结果预测模型，通过战术特征提升预测准确率至81%。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于比赛日志和统计数据，而FPS游戏如VALORANT需要复杂策略，因此探索通过小地图信息分析预测结果。

Method: 基于TimeSformer视频识别模型，提取小地图中的战术特征（如角色位置和游戏内事件）来改进预测。

Result: 初步结果显示，加入战术事件标签的模型预测准确率达81%，尤其在回合中后期表现显著优于仅使用小地图信息的模型。

Conclusion: 从比赛录像中提取战术特征对VALORANT回合结果预测非常有效。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [123] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: EndoCIL是一种针对内窥镜图像诊断的类增量学习框架，通过分布对齐、类平衡损失和梯度校准解决领域差异和类不平衡问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析需要模型持续适应新数据同时保持对旧数据的性能，现有方法因领域差异和类不平衡而表现不佳。

Method: EndoCIL包含三个关键组件：基于最大均值差异的重放（MDBR）、先验正则化类平衡损失（PRCBL）和全连接梯度校准（CFG）。

Result: 在四个公开数据集上的实验表明，EndoCIL在不同缓冲区大小和评估指标上优于现有方法。

Conclusion: EndoCIL有效平衡了稳定性和可塑性，具有临床应用的潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [124] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出了一种基于DINOv2的欺骗攻击检测方法，用于区分真实和伪造的人脸图像。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统可能被恶意攻击者利用伪造照片绕过认证，因此需要检测欺骗攻击。

Method: 使用DINOv2提取通用特征，并通过注意力机制抑制扰动，聚焦关键细微特征。

Result: 在ICCV2025和SiW数据集上的实验验证了方法的有效性。

Conclusion: 该方法能有效检测欺骗攻击，提升人脸识别系统的安全性。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [125] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: VisiPruner是一种无需训练的剪枝框架，通过分析MLLMs的三阶段跨模态交互过程，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉语言任务中表现优异，但计算开销大，现有方法缺乏对跨模态交互的理解。

Method: 通过系统分析发现三阶段跨模态交互过程，提出VisiPruner框架，减少视觉相关注意力计算。

Result: VisiPruner在LLaVA-v1.5 7B上减少99%视觉相关注意力计算和53.9% FLOPs，优于现有方法。

Conclusion: 研究不仅提供高效剪枝方法，还为训练高效MLLMs提供了指导。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [126] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 论文提出了一个多时刻检索（MMR）数据集QV-M$^2$和框架FlashMMR，解决了现有单时刻检索（SMR）方法在现实应用中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注单时刻检索，而现实应用中一个查询可能对应多个相关时刻，现有数据集和方法不足以支持视频时间定位。

Method: 提出了FlashMMR框架，包含多时刻后验证模块，通过约束时间调整和验证模块优化候选片段。

Result: 在QV-M$^2$数据集上，FlashMMR在多个指标上优于现有方法，如G-mAP提升3.00%。

Conclusion: QV-M$^2$和FlashMMR为视频时间定位研究提供了有效基准和基线，推动了更现实和挑战性场景的研究。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [127] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出了一种公平感知的深度伪造检测框架，结合时序特征学习和人口统计感知数据增强，以提高公平性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度且无法捕捉时序信息，导致对不同人口群体的决策偏差和不可靠结果。

Method: 采用序列聚类进行时序建模，结合概念提取提升检测可靠性；引入人口统计感知数据增强方法，平衡少数群体并保留伪造伪影。

Result: 在多个数据集上验证了方法在公平性和准确性之间的最佳权衡。

Conclusion: 提出的框架显著提升了深度伪造检测的公平性和可解释性，优于现有方法。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [128] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集和整理的视觉语言模型数据集，包含2400万个样本，通过半自动流程统一了200多个来源，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的发展受到数据集碎片化和污染的阻碍，需要高质量、统一的数据集来推动研究。

Method: 采用半自动化流程，结合人工审核，进行数据收集、去重、去污染，并统一了GUI任务的行动空间。

Result: 在广泛评估中，FineVision训练的模型性能优于现有开放数据集训练的模型。

Conclusion: FineVision通过规模、数据清洁和人工监督的平衡自动化，为数据中心的视觉语言模型研究提供了加速工具。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [129] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: PnF利用多模态大语言模型（MLLMs）增强现有运动预测模型，通过自然语言描述复杂场景，无需微调即可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在多样化场景中泛化能力不足，PnF旨在低成本高效解决这一问题。

Method: 设计提示词从MLLMs提取结构化场景理解，转化为可学习嵌入以增强行为预测模型。

Result: 在Waymo和nuScenes数据集上验证，性能显著提升。

Conclusion: PnF是一种即插即用的高效方法，适用于现有模型，提升预测性能。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [130] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出了一种基于显著性引导的跨层深度特征融合框架（SG-CLDFF），用于白细胞（WBC）的精确分割和分类，提高了鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 白细胞分割和分类在血液疾病诊断中至关重要，但存在染色变异、复杂背景和类别不平衡等挑战。

Method: 结合显著性预处理和多尺度深度特征融合，使用轻量级混合主干网络（EfficientSwin-style）和跨层融合模块（ResNeXt-CC-inspired），多任务训练（分割和分类）并采用加权损失和显著性对齐正则化。

Result: 在标准公开数据集（BCCD、LISC、ALL-IDB）上表现优于CNN和Transformer基线，IoU、F1和分类准确率均有提升。

Conclusion: SG-CLDFF为临床工作流程提供了可靠且可解释的自动化白细胞分析方案。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [131] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv11算法的自动手术照明系统，通过识别蓝色标记自动调整光源，减少外科医生疲劳并提高照明一致性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统依赖手动调整，导致外科医生疲劳、颈部劳损和照明不一致。

Method: 利用YOLOv11算法识别目标手术区域上的蓝色标记，并通过伺服电机调整高功率LED光源。

Result: YOLO模型在验证集上达到96.7% mAP@50，显著减少外科医生体力负担并提高照明效果。

Conclusion: 该机器视觉解决方案自动化照明过程，改善了手术效果并减轻了外科医生的身体负担。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [132] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 论文发现自监督学习（SSL）中长时间训练可能损害密集预测任务性能，提出DSE指标和策略以优化模型选择和正则化。


<details>
  <summary>Details</summary>
Motivation: 观察到自监督学习中长时间训练对密集预测任务的负面影响（SDD现象），需解决无标注时评估密集性能的挑战。

Method: 提出DSE指标（包含类相关性和有效维度度量），并基于此设计模型选择策略和正则化方法。

Result: 实验验证DSE与下游性能强相关，模型选择平均提升mIoU 3.0%，正则化有效缓解SDD。

Conclusion: DSE为SSL密集任务提供理论支持与实践工具，显著提升性能且计算成本低。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [133] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个专注于评估模型在长视频理解能力的基准测试，涵盖视觉、音频和文本模态，包含1000个信息密集的长视频和六种挑战性任务场景。


<details>
  <summary>Details</summary>
Motivation: 现有模型在多模态长视频理解方面存在不足，尤其是时间定位和长距离因果推理任务。LongInsightBench旨在填补这一空白。

Method: 通过精选长视频、设计多样化任务场景（包括事件内和事件间任务）和严格的数据质量保证流程，构建了一个全面的基准测试。

Result: 实验表明，全模态模型在时间定位和长距离因果推理任务上表现不佳，多模态融合中存在信息丢失和处理偏差。

Conclusion: LongInsightBench为长视频理解研究提供了重要工具，揭示了全模态模型的局限性，并推动了多模态融合技术的改进。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [134] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba是一个可扩展的框架，解决了fMRI因果推断中的基本限制，包括BOLD信号失真和计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 解决从血流动力学扭曲的BOLD信号推断神经因果性的不适定问题，以及现有方法（如DCM）的计算不可行性。

Method: 将问题分解为两个阶段：BOLD反卷积恢复潜在神经活动，然后使用Conditional Mamba架构进行因果图推断。

Result: 在模拟数据上比DCM准确率高37%；在真实任务fMRI数据中恢复已知神经通路的准确率达88%。

Conclusion: CausalMamba为神经科学家提供了大规模因果推断工具，捕捉了认知功能的基本电路模式和动态网络重构。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [135] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 论文研究了对抗性衣物对深度学习目标检测器的攻击，发现现有防御方法在对抗性衣物面前表现不佳。


<details>
  <summary>Details</summary>
Motivation: 对抗性补丁的防御方法在对抗性衣物攻击下失效，因此需要评估防御方法对大规模覆盖的对抗性衣物的有效性。

Method: 通过实验评估多种防御方法在数字和物理世界中对对抗性衣物的防御效果，并设计了一种能突破多种防御的对抗性衣物。

Result: 对抗性衣物在数字和物理世界中均能有效突破现有防御方法，单一衣物能对多种防御模型造成高攻击成功率。

Conclusion: 现有对抗性防御方法在对抗性衣物攻击下存在普遍脆弱性，需进一步研究更鲁棒的防御策略。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [136] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: CharDiff是一种基于扩散模型的车牌图像恢复框架，通过字符级引导显著提升恢复质量和识别准确率。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅用于车牌识别预处理，还能提高证据价值、增强视觉清晰度并促进进一步利用。

Method: CharDiff结合外部分割和OCR模块提取字符级先验，并引入CHARM模块实现精确的区域引导。

Result: 在Roboflow-LP数据集上，CharDiff比基线模型CER相对降低28%，恢复质量和识别准确率显著提升。

Conclusion: 字符级引导条件有效增强了扩散模型在实际车牌恢复和识别中的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [137] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: iDETEX是一个多模态大语言模型，用于详细可解释的图像质量评估，支持质量定位、感知和描述任务，并在ViDA-UGC基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决图像质量评估（IQA）中详细可解释性的挑战，提出统一的多任务模型。

Method: 设计任务特定的离线增强模块、数据混合策略和在线增强策略，利用多源监督进行训练。

Result: 在ViDA-UGC基准测试中达到最优性能，并在ICCV MIPI 2025挑战赛中排名第一。

Conclusion: iDETEX能提供准确且可解释的质量评估，具有高效性和鲁棒性。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [138] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出了一种后处理的开放集识别方法，通过比较特征空间和logit空间的一致性来分类已知类别并拒绝未知样本。


<details>
  <summary>Details</summary>
Motivation: 现有开放集识别方法需要重新训练预训练模型，本研究旨在提供一种无需重新训练的后处理方法。

Method: 基于输入到最近类均值的距离生成概率分布，并与softmax概率比较以衡量一致性。

Result: 在两个数据集上表现稳定，AUROC分别达到93.41和95.35。

Conclusion: 该方法在无需重新训练的情况下，实现了与现有方法相当的开放集识别性能。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [139] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: Semantic-E2VID通过跨模态特征对齐和语义感知特征融合，显著提升了事件到视频重建的质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机缺乏语义信息，现有E2V方法忽视语义信息的重要性。

Method: 引入跨模态特征对齐模块和语义感知特征融合块，利用SAM模型增强语义信息。

Result: 在多个基准测试中优于现有E2V方法。

Conclusion: Semantic-E2VID有效提升了事件到视频重建的语义细节和帧质量。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [140] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过视觉令牌选择、递归处理和基于字幕的问答，提升视频大语言模型在流媒体场景中的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 解决视频大语言模型在流媒体场景中处理长视频和实时响应的问题。

Method: 1) 基于LLM的视觉令牌选择；2) 递归处理历史令牌；3) 基于字幕的问答。

Result: 在流媒体视频基准测试中达到最优性能，丢弃95%不重要令牌且性能损失最小。

Conclusion: 该方法在效率和效果之间取得了平衡，适用于流媒体场景。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [141] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 该研究通过系统评估合成面部识别数据集，证明其可以替代真实数据集，同时解决隐私和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统需要大量未经同意的真实面部数据，引发隐私和伦理问题，而合成数据作为替代方案缺乏实证支持。

Method: 通过文献综述和实验验证，评估25个合成数据集在隐私保护、多样性、身份分离等七个关键要求上的表现。

Result: 最佳合成数据集（VariFace, VIGFace）识别准确率超过真实数据集（如CASIA-WebFace），且能有效控制偏见。

Conclusion: 合成面部数据是科学可行且伦理必要的面部识别研究替代方案。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [142] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出一种基于多表情特征融合和自适应类别平衡的帕金森病严重程度诊断方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一表情且忽略类别不平衡问题，导致误诊和性能下降。

Method: 通过注意力机制融合多表情特征，并采用自适应类别平衡策略。

Result: 实验证明该方法在帕金森病严重程度诊断中表现优异。

Conclusion: 注意力特征融合和自适应类别平衡有效提升了诊断性能。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [143] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans是一个闭环框架，通过双向知识转移提升弱监督可操作性定位的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅从外中心图像单向转移知识到自我中心图像，限制了复杂交互场景的应用。

Method: 提出LoopTrans框架，引入统一跨模态定位和去噪知识蒸馏机制。

Result: 在所有指标上均取得一致提升，甚至能处理完全遮挡的挑战性场景。

Conclusion: LoopTrans通过闭环双向知识转移显著提升了可操作性定位的效果。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [144] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 开发了一个基于视觉的自动化系统，用于监测马厩中马和人的行为，利用YOLOv11和BoT-SORT技术进行检测与跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统监测方法耗时耗力，需要自动化解决方案以提升马匹健康和福利的早期检测效率。

Method: 结合YOLOv11和BoT-SORT进行目标检测与跟踪，利用CLIP和GroundingDINO构建自定义数据集，推断事件状态。

Result: 系统能区分五种事件类型，对马相关事件表现可靠，但对人的检测因数据不足存在局限。

Conclusion: 该系统为实时行为监测提供了基础，对动物福利和马厩管理有重要意义。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [145] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一种基于深度学习的密集关键点检测器，通过融合多种传统检测器的输出生成真实掩码，训练轻量级模型ESPNet，显著提升了关键点密度、重复性和匹配准确性。


<details>
  <summary>Details</summary>
Motivation: 传统关键点检测器（如SIFT、SURF）和学习方法（如SuperPoint）存在对光照变化敏感、关键点密度低、适应性差等问题，DeepDetect旨在解决这些局限性。

Method: 通过融合7种关键点检测器和2种边缘检测器的输出生成真实掩码，训练轻量级模型ESPNet，实现语义感知和高密度关键点检测。

Result: 在Oxford Affine Covariant Regions数据集上，DeepDetect在关键点密度（0.5143）、重复性（0.9582）和正确匹配数（59,003）上均优于其他检测器。

Conclusion: DeepDetect通过深度学习统一传统检测器的优势，显著提升了关键点检测的性能，适用于多样化和视觉退化场景。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [146] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 利用AV1运动向量生成密集亚像素对应关系，通过余弦一致性过滤短轨迹，在短视频中性能接近SIFT但CPU占用更低，匹配更密集且几何性能竞争。


<details>
  <summary>Details</summary>
Motivation: 探索压缩域对应关系的实用性，以资源高效的方式提升密集匹配和几何重建的性能。

Method: 重新利用AV1运动向量生成密集亚像素对应关系，并通过余弦一致性过滤短轨迹。

Result: 在117帧视频中，匹配成功注册所有图像并重建46-62万点，重投影误差0.51-0.53像素。

Conclusion: 压缩域对应关系是一种实用且资源高效的前端方法，具有扩展到完整流程的潜力。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [147] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 提出了一种新的高质量夜间图像去雨基准数据集HQ-NightRain，并开发了CST-Net方法，通过颜色空间转换和隐式光照引导提升去雨效果。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨任务因场景复杂性和缺乏高质量数据集而面临挑战，现有数据集难以准确反映雨和光照的耦合效应。

Method: 提出可学习的颜色空间转换器（CSC）在Y通道中更有效去除雨纹，并引入隐式光照引导以提升模型在复杂场景中的鲁棒性。

Result: 实验证明HQ-NightRain数据集和CST-Net方法的有效性。

Conclusion: 新数据集和方法为夜间图像去雨任务提供了更高质量和更实用的解决方案。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [148] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 论文提出了一种改进稀疏视图3D高斯泼溅（3DGS）初始化的方法，通过频率感知SfM、3DGS自初始化和点云正则化，显著提升了稀疏视图下的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3DGS容易过拟合训练视图，导致新视图渲染时出现模糊等伪影。研究发现初始化是关键因素，而现有方法主要通过训练时约束改进，效果有限。

Method: 设计了频率感知SfM（提升低纹理区域覆盖）、3DGS自初始化（通过光度监督补充点云）和点云正则化（几何/可见性先验保证一致性）。

Result: 在LLFF和Mip-NeRF360数据集上，稀疏视图设置下表现一致优于基线，验证了方法的有效性。

Conclusion: 初始化是稀疏视图3DGS性能的决定因素，提出的方法通过改进初始化策略显著提升了渲染质量。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [149] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld提出了一种新型的4D占用世界模型，通过稀疏动态查询实现灵活、自适应和高效的感知与预测。


<details>
  <summary>Details</summary>
Motivation: 现有占用世界模型依赖静态嵌入或网格，限制了感知的灵活性，且分类方法可能与动态连续的真实场景不匹配。

Method: 提出Range-Adaptive Perception模块和State-Conditioned Forecasting模块，结合Temporal-Aware Self-Scheduling训练策略。

Result: 在感知、预测和规划任务中达到SOTA性能，验证了灵活性、适应性和效率优势。

Conclusion: SparseWorld通过动态查询和回归引导的预测，显著提升了4D环境建模能力。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [150] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: POTNet通过熵引导的双聚类头和最优传输生成高质量伪掩码，AutoSOD在无监督显著目标检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 显著目标检测（SOD）是计算机视觉的基础任务，但依赖像素级标签。研究旨在实现无需标签的高精度SOD。

Method: 提出POTNet，结合谱聚类和k-means生成原型，通过最优传输对齐，训练MaskFormer式编码器-解码器。

Result: AutoSOD在五个基准测试中优于无监督方法26%，弱监督方法36%，接近全监督模型。

Conclusion: POTNet和AutoSOD展示了无监督SOD的潜力，显著缩小了与全监督模型的差距。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [151] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于评分标准引导的伪标签提示框架，用于零样本视频摘要，平衡局部显著性和全局连贯性，性能接近监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法标注成本高且泛化能力有限，无监督方法难以捕捉高级语义，零样本方法对提示模板敏感。

Method: 利用少量真实标注生成高置信度伪标签，构建数据集自适应评分标准，结合上下文提示评估场景。

Result: 在SumMe和TVSum上F1分数分别为57.58和63.05，超越无监督和零样本基线。

Conclusion: 评分标准引导的伪标签稳定了LLM评分，为零样本视频摘要提供了通用且可解释的范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [152] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出了一种优化大规模视频生成模型训练的框架，涵盖数据处理、模型架构、训练策略和基础设施，显著提升了效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频生成模型训练中的跨模态对齐、长序列和复杂时空依赖等挑战。

Method: 通过优化数据预处理、视频压缩、参数缩放、课程预训练和对齐后训练四个支柱。

Result: 模型MUG-V 10B在整体性能上匹配最新技术，并在电商视频生成任务中超越开源基线。

Conclusion: 开源了完整的训练代码和模型权重，首次公开利用Megatron-Core实现高效训练。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [153] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: MambaX-Net是一种半监督、双扫描3D分割架构，用于纵向主动监测中的前列腺分割，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习模型在纵向主动监测中因单时间点和专家标注稀缺导致的性能不足问题。

Method: 提出MambaX-Net，包含Mamba增强的交叉注意力模块和形状提取模块，结合半监督自训练策略。

Result: 在纵向数据集上显著优于U-Net和基于Transformer的模型，即使数据有限且噪声多。

Conclusion: MambaX-Net为纵向前列腺分割提供了高效解决方案，减少对专家标注的依赖。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [154] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet是一种弱监督方法，仅需图像级标签即可实现像素级道路裂缝检测，通过分类器、重构器和检测器的协同学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵像素级标注的依赖，实现高效的道路裂缝检测。

Method: 结合分类器、重构器和检测器，通过对抗学习和伪标签训练，并引入PAAM和CECCM模块优化性能。

Result: 在三个数据集上表现优于现有弱监督方法，接近监督方法效果。

Conclusion: WP-CrackNet显著提升了道路裂缝检测的可扩展性和准确性。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [155] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D扩展了VGGT模型，通过动态感知聚合器解决动态场景中的多任务冲突，显著提升了动态场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D前馈模型在静态场景表现良好，但在动态场景（如移动物体）中表现不佳。

Method: 提出PAGE-4D模型，引入动态感知聚合器，通过预测动态掩码分离静态和动态信息。

Result: 在动态场景中，PAGE-4D在相机姿态估计、深度预测和点云重建等任务上优于VGGT。

Conclusion: PAGE-4D有效解决了动态场景中的多任务冲突，提升了模型在复杂环境中的适用性。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [156] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 论文提出了首个水下伪装实例分割数据集UCIS4K，并基于Segment Anything Model设计了UCIS-SAM网络，通过三个关键模块提升水下伪装物体的分割性能。


<details>
  <summary>Details</summary>
Motivation: 水下环境因颜色失真、低对比度和模糊等问题，使得伪装实例分割更具挑战性。传统方法在陆地数据集上训练，水下场景表现不佳。

Method: 提出UCIS-SAM网络，包含三个模块：CBOM优化通道特征，FDTIM减少伪装干扰，MFFAM增强多频段边界特征。

Result: 在UCIS4K和公开基准测试中，UCIS-SAM优于现有方法。

Conclusion: UCIS-SAM通过多模块协同，显著提升了水下伪装实例分割的精度。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [157] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft是一个多智能体框架，通过图形化程序形状表示（GPS）从自然语言生成结构化、可交互的3D资产，解决了现有方法生成非结构化网格和交互性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的3D资产通常是非结构化的且交互性差，难以满足艺术工作流的需求。ShapeCraft旨在通过结构化表示和智能体协作提升生成质量。

Method: ShapeCraft采用基于图的程序形状（GPS）表示，将复杂自然语言分解为子任务图，利用LLM智能体分层解析和迭代优化建模与纹理。

Result: 实验表明，ShapeCraft生成的3D资产在几何精度和语义丰富性上优于现有方法，并支持动画和用户自定义编辑。

Conclusion: ShapeCraft展示了在交互式3D生成中的潜力，为艺术创作和更广泛的应用提供了实用工具。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [158] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的框架，用于自动分割3D点云，结合无人机扫描和BIM合成数据，提高了基础设施模型分割的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的手动标记方法耗时且易出错，需要一种自动化解决方案来分割3D点云中的结构组件。

Method: 结合无人机扫描的真实点云和BIM生成的合成数据，利用机器学习框架进行自动分割。

Result: 在铁路轨道数据集上验证了高精度分割，同时通过小规模数据集和BIM数据减少了训练时间。

Conclusion: 该框架提升了3D基础设施模型分割的效率和精度，推动了无人机与BIM技术在结构健康监测中的整合。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [159] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是一个统一的图像异常检测框架，通过简单设计在多类、多模态和少样本任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有多类异常检测模型性能不足且方法碎片化，需要统一解决方案。

Method: 采用基于重建的框架，通过五个简单元素的组合实现高性能。

Result: 在12个基准测试中表现优异，如MVTec-AD和VisA上达到99.9%和99.3%的I-AUROC。

Conclusion: Dinomaly2以其简约设计、计算可扩展性和通用性成为异常检测的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [160] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT数据集用于研究汽车模型随时间变化的视觉识别和生成，支持监督和自监督学习，提出时间增量学习策略以提高时间鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统如何适应视觉环境中对象外观随时间变化的问题，特别是在汽车模型这类技术制品上。

Method: 引入CaMiT数据集，提出时间增量分类设置，评估两种时间增量学习策略（更新主干网络或仅更新分类器），并探索时间感知图像生成。

Result: 静态预训练在域内数据上表现良好但跨年测试准确率下降，时间增量学习策略提高了时间鲁棒性，时间感知图像生成更真实。

Conclusion: CaMiT为细粒度视觉识别和生成中的时间适应研究提供了丰富基准，时间增量学习策略有效提升模型的时间鲁棒性。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [161] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: DINO-CV是一种基于自监督学习的框架，用于自动映射低矮干石墙，解决了植被遮挡和标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 干石墙具有重要的遗产和环境价值，但传统手动测绘成本高且难以覆盖。

Method: 利用高分辨率LiDAR DEM数据，通过自监督跨视图预训练策略学习不变特征。

Result: 在Budj Bim测试中，mIoU达68.6%，仅用10%标注数据微调后仍保持63.8%。

Conclusion: DINO-CV展示了自监督学习在高分辨率DEM数据中自动测绘干石墙的潜力。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [162] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 比较了两种节俭的联邦学习方法用于暴力检测：零样本和联邦微调的视觉语言模型（VLMs）与个性化训练的紧凑3D卷积神经网络（CNN3D）。结果显示两者均超过90%准确率，CNN3D在能耗上更优，而VLMs在上下文推理和多模态推理上更佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索高效且环保的联邦学习方法，用于暴力检测，特别关注能耗和环境影响。

Method: 使用LLaVA-7B和65.8M参数的CNN3D，评估了在非独立同分布（non-IID）设置下的准确性、校准和能耗。

Result: 两种方法均表现优异，CNN3D在ROC AUC和log loss上略优于LoRA微调的VLMs，且能耗更低。VLMs在多模态推理上更具优势。

Conclusion: 提出混合模型：轻量级CNN用于常规分类，选择性激活VLM处理复杂场景。为视频监控提供了一种可重复、资源高效的AI框架。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings.
  Both approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank
Adaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy.
VLMs remain favorable for contextual reasoning and multimodal inference. We
quantify energy and CO$_2$ emissions across training and inference, and analyze
sustainability trade-offs for deployment.
  To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics.
  These findings support a hybrid model: lightweight CNNs for routine
classification, with selective VLM activation for complex or descriptive
scenarios. The resulting framework offers a reproducible baseline for
responsible, resource-aware AI in video surveillance, with extensions toward
real-time, multimodal, and lifecycle-aware systems.

</details>


### [163] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 4DSegStreamer是一个用于4D全景分割的双线程框架，适用于动态环境中的实时感知。


<details>
  <summary>Details</summary>
Motivation: 在高度动态环境中（如密集人群疏散和复杂自动驾驶场景），需要实时、细粒度的感知能力。

Method: 采用双线程系统（预测线程和推理线程），预测线程利用历史信息预测未来动态，推理线程确保及时处理新帧。

Result: 在HOI4D、SemanticKITTI和nuScenes数据集上表现优异，尤其在复杂场景中准确预测动态对象。

Conclusion: 4DSegStreamer是一个通用且鲁棒的框架，可提升现有3D/4D分割方法的实时性。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [164] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: PICABench评估图像编辑的物理真实性，提出PICAEval评估协议，并探索从视频学习物理的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型和基准主要关注指令完成，但忽略了物理效果（如阴影、反射等），导致生成结果缺乏真实感。

Method: 引入PICABench评估八个子维度的物理真实性，提出PICAEval评估协议（使用VLM作为评判），并构建PICA-100K数据集。

Result: 主流模型在物理真实性方面仍有较大提升空间。

Conclusion: PICABench和提出的解决方案为未来从简单内容编辑转向物理一致的真实感提供了基础。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [165] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: IC-MoE模型通过混合专家和对比学习增强医学图像分割的高层特征表示，同时保持预训练权重的结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在高层特征表示不足且破坏预训练权重结构完整性方面存在局限。

Method: 构建三类专家模型并采用像素概率自适应投票策略，结合语义引导的对比学习方法。

Result: 在三个公共数据集上超越其他SOTA模型，验证了优越的泛化能力。

Conclusion: IC-MoE有效补充了医学图像分割基础模型的高层特征和预训练结构完整性。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [166] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种多语言文本到图像人物检索（TIPR）任务，并开发了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决模态异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态对齐中忽略了细粒度差异或需要先验信息，且多为英语中心，限制了多语言应用。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块和多维全局对齐模块，支持多语言和跨模态对齐。

Result: 在多个多语言TIPR数据集上取得了最先进的结果。

Conclusion: Bi-IRRA通过隐式关系推理和全局对齐，有效解决了多语言TIPR任务中的模态异质性问题。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [167] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: OP3Det是一种无需手工文本提示的开放世界3D检测器，通过结合2D语义先验和3D几何先验，实现广义3D物体检测。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测器难以泛化到开放世界场景，而直接使用3D开放词汇模型又面临词汇扩展和语义重叠问题。

Method: 提出OP3Det，利用2D基础模型的强泛化和零样本能力，结合点云和RGB图像的跨模态信息，动态路由特征学习广义3D物体性。

Result: 实验显示OP3Det在AR指标上显著超越现有开放世界3D检测器16.0%，比封闭世界检测器提升13.5%。

Conclusion: OP3Det通过跨模态混合专家方法，成功实现了广义3D物体检测，为开放世界场景提供了高效解决方案。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [168] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 论文提出了一种广义对抗求解器（GAS），通过结合蒸馏损失和对抗训练，提高了扩散模型的采样效率和质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量高，但采样计算成本高。现有方法通过蒸馏ODE求解器减少计算量，但训练复杂且细节保留不足。

Method: 提出广义求解器，简化ODE采样器参数化，无需额外训练技巧，并结合对抗训练提升细节保真度。

Result: GAS在相同资源限制下优于现有求解器训练方法，代码已开源。

Conclusion: GAS通过简单设计和对抗训练，显著提升了扩散模型的采样效率和生成质量。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [169] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT是一种后预训练结构化剪枝方法，支持弹性推理，无需标记数据或重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型尺寸固定，无法灵活适应实际部署需求。

Method: 结合梯度信息和跨网络结构相关性，通过进化算法近似Hessian非对角结构，提出自监督重要性评分机制。

Result: 在多种稀疏度下优于现有方法，生成弹性模型仅需不到5分钟。

Conclusion: SnapViT为预训练视觉Transformer提供高效剪枝策略，支持任意计算预算的弹性部署。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [170] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出了一种新的两阶段方法，通过分块处理和集成方法提高帕金森病检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数据集不足和处理未见患者数据时表现不佳。

Method: 两阶段方法：先分类绘图类型，再分块提取特征并检测；使用集成方法合并决策。

Result: 在NewHandPD数据集上，对已知和未知患者的准确率分别为97.08%和94.91%，差距仅2.17%。

Conclusion: 新方法显著提升了检测性能，尤其在处理未见患者数据时表现优异。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [171] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv11的两步计算框架，用于自动分析流式细胞术中的循环血细胞簇（CCCs）图像，分类和表型识别准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏自动分析CCCs图像的工具，且细胞簇形状不规则、细胞类型异质性高，需要多通道染色。

Method: 使用YOLOv11模型分类图像为细胞簇和非簇组，再通过多通道荧光染色区域叠加识别细胞类型。

Result: 框架在簇分类和表型识别中均达到95%以上的准确率。

Conclusion: 该自动化框架可有效分析流式细胞术中的CCCs图像，未来可扩展至免疫和肿瘤细胞簇分析。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [172] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS是一个用于评估3D高斯溅射（3DGS）在雨滴干扰下性能的综合基准，包括数据准备、处理和评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常使用合成雨滴图像评估3DGS，忽略了真实场景中雨滴对相机姿态估计和点云初始化的干扰，以及合成与真实雨滴的域差距。

Method: RaindropGS包含真实雨滴重建数据集，分为雨滴聚焦、背景聚焦和无雨地面真实图像集，并通过实验分析不同组件对3DGS性能的影响。

Result: 实验揭示了现有3DGS方法在无约束雨滴图像上的性能限制，以及相机焦点位置、姿态和点云初始化对重建的干扰。

Conclusion: RaindropGS为开发更鲁棒的雨滴条件下3DGS方法提供了明确方向。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [173] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: MT-Video-Bench是一个用于评估多模态大语言模型在多轮对话中视频理解能力的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准仅限于单轮问答，忽略了多轮对话的复杂性。

Method: 构建包含987个多轮对话的MT-Video-Bench，评估六项核心能力。

Result: 发现现有模型在多轮视频对话中存在显著性能差异和局限性。

Conclusion: MT-Video-Bench将公开以促进未来研究。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [174] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 研究探讨了签名伪造检测的特征学习策略，重点关注跨数据集泛化能力，比较了基于原始图像和预处理方法的两种实验流程。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在离线签名验证中取得进展，但跨数据集泛化能力仍不足，需改进模型鲁棒性。

Method: 使用三个公共数据集（CEDAR、ICDAR、GPDS Synthetic），开发了基于原始图像和预处理（shell预处理）的两种实验流程。

Result: 原始图像模型在多个基准测试中表现更好，但预处理模型显示出未来优化的潜力。

Conclusion: 研究为跨域签名验证的鲁棒性提供了方向，预处理方法有待进一步优化。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [175] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 研究基于扩散变换器（DiT）的图像到视频（I2V）模型是否能生成拥挤公共场景中真实的行人运动模式。


<details>
  <summary>Details</summary>
Motivation: 探索高性能I2V模型是否具备生成真实行人运动的能力。

Method: 通过从行人轨迹基准中提取关键帧来条件化I2V模型，并评估其轨迹预测性能。

Result: 使用行人动力学的定量指标评估模型性能。

Conclusion: 研究验证了I2V模型在生成真实行人运动模式方面的潜力。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [176] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出一种无需训练、描述符无关的多参考视觉地点识别方法，通过矩阵分解实现残差匹配，并在多视角数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多参考视觉地点识别中训练成本高和现有融合方法效果有限的问题。

Method: 采用矩阵分解将多个参考描述符联合建模为基础表示，实现基于投影的残差匹配。

Result: 在多外观数据上，Recall@1提升约18%，在非结构化数据上也有约5%的提升。

Conclusion: 该方法在多参考视觉地点识别中表现优异，且计算轻量，具有强泛化能力。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [177] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出了一种基于双编码器和注意力的框架，结合分割病灶和临床数据，提升皮肤病变分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测至关重要，但现有深度学习模型多为“黑箱”，缺乏临床信任。

Method: 采用Deep-UNet分割病灶，双DenseNet201编码器融合特征，并通过多头交叉注意力和临床数据增强分类。

Result: 在HAM10000和ISIC数据集上取得最优分割性能，分类准确率和AUC显著提升。

Conclusion: 结合精确分割和临床数据的注意力融合方法，显著提高了皮肤癌分类的准确性和可解释性。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [178] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA是一种高效视觉语言模型推理的新范式，通过解耦视觉稀疏性，显著提升推理速度，同时保持多轮对话的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）的推理延迟主要由视觉令牌数量增加引起，限制了其扩展性。

Method: SparseVILA在预填充和解码阶段解耦视觉稀疏性，预填充阶段剪枝冗余视觉令牌，解码阶段仅检索与查询相关的令牌。

Result: SparseVILA在长上下文视频任务中实现了4.0倍预填充加速、2.5倍解码加速和2.6倍端到端加速，同时在文档理解和推理任务中提升了准确性。

Conclusion: SparseVILA为高效多模态推理提供了无需训练、架构无关的框架，显著加速大型VLMs而不牺牲能力。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [179] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA是一种基础模型，通过混合动作（GUI原语与高级程序化工具调用）提升计算机使用代理的性能。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理仅依赖原始动作（点击、输入、滚动），导致性能瓶颈和级联失败，而其他代理可利用丰富的程序化接口。

Method: 提出四个关键组件：自动化工具扩展管道、合成数据引擎、混合动作轨迹收集、两阶段训练管道。

Result: 实验显示，UltraCUA在OSWorld上相对改进22%，速度提升11%；在WindowsAgentArena上成功率21.7%，优于基线。

Conclusion: 混合动作机制显著减少错误传播，同时保持执行效率。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [180] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph框架通过将长文本渲染为图像，利用视觉语言模型处理，实现3-4倍的文本压缩，同时保持准确性，并显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文建模中计算和内存成本过高的问题。

Method: 将文本渲染为图像，使用视觉语言模型处理，并通过遗传搜索优化渲染配置。

Result: 在保持准确性的同时实现3-4倍压缩，计算效率提升4倍。

Conclusion: Glyph为长上下文任务提供了一种高效且实用的解决方案。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: ConsistEdit是一种针对MM-DiT的新型注意力控制方法，通过视觉注意力控制、掩码引导预注意力融合和差异化操作，实现了高效且一致的图像和视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在编辑强度和一致性之间存在矛盾，尤其在多轮和视频编辑中问题显著，且缺乏细粒度控制能力。MM-DiT架构的进步为解决这些问题提供了新机会。

Method: 基于MM-DiT的注意力机制分析，提出ConsistEdit方法，结合视觉注意力控制、掩码引导预注意力融合，以及对查询、键和值令牌的差异化操作。

Result: 实验表明，ConsistEdit在图像和视频编辑任务中表现优异，支持多轮和多区域编辑，并能渐进调整结构一致性。

Conclusion: ConsistEdit首次实现了无需手工干预的全推理步骤和注意力层编辑，显著提升了可靠性和一致性，为细粒度编辑提供了新工具。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [182] [Lean Finder: Semantic Search for Mathlib That Understands User Intents](https://arxiv.org/abs/2510.15940)
*Jialin Lu,Kye Emond,Kaiyu Yang,Swarat Chaudhuri,Weiran Sun,Wuyang Chen*

Main category: cs.LG

TL;DR: Lean Finder是一个为Lean和mathlib设计的语义搜索引擎，专注于理解数学家的意图，相比现有工具提升了30%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Lean搜索引擎主要依赖非正式化翻译，忽略了用户实际查询需求，阻碍了形式化定理证明的进展。

Method: 通过分析公开讨论的语义聚类，微调文本嵌入以模拟用户意图，并结合多样反馈信号优化搜索。

Result: 在真实查询、非正式化语句和证明状态上，Lean Finder相对提升了30%以上。

Conclusion: Lean Finder不仅性能优越，还能与LLM定理证明器兼容，推动检索与形式推理的结合。

Abstract: We present Lean Finder, a semantic search engine for Lean and mathlib that
understands and aligns with the intents of mathematicians. Progress in formal
theorem proving is often hindered by the difficulty of locating relevant
theorems and the steep learning curve of the Lean 4 language, making
advancement slow and labor-intensive. Existing Lean search engines, though
helpful, rely primarily on informalizations (natural language translation of
the formal statements), while largely overlooking the mismatch with real-world
user queries. In contrast, we propose a user-centered semantic search tailored
to the needs of mathematicians. Our approach begins by analyzing and clustering
the semantics of public Lean discussions, then fine-tuning text embeddings on
synthesized queries that emulate user intents. We further align Lean Finder
with mathematicians' preferences using diverse feedback signals, encoding it
with a rich awareness of their goals from multiple perspectives. Evaluations on
real-world queries, informalized statements, and proof states demonstrate that
our Lean Finder achieves over $30\%$ relative improvement compared to previous
search engines and GPT-4o. In addition, Lean Finder is compatible with
LLM-based theorem provers, bridging retrieval with formal reasoning. Lean
Finder is available at: https://leanfinder.github.io

</details>


### [183] [Lyapunov-Stable Adaptive Control for Multimodal Concept Drift](https://arxiv.org/abs/2510.15944)
*Tianyu Bell Pan,Mengdi Zhu,Alexa Jordyn Cole,Ronald Wilson,Damon L. Woodard*

Main category: cs.LG

TL;DR: LS-OGD是一种自适应控制框架，用于解决多模态学习中的概念漂移问题，通过动态调整学习率和模态融合权重来提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态学习系统在非平稳环境中因概念漂移和模态特定漂移而性能下降，缺乏持续稳定适应机制。

Method: 提出LS-OGD框架，利用在线控制器动态调整学习率和模态融合权重，以应对漂移和预测误差变化。

Result: 理论证明在有限漂移条件下，预测误差有界且漂移停止时收敛至零；自适应融合策略有效隔离和缓解模态特定漂移。

Conclusion: LS-OGD为开发可靠且持续自适应的多模态学习系统提供了理论基础。

Abstract: Multimodal learning systems often struggle in non-stationary environments due
to concept drift, where changing data distributions can degrade performance.
Modality-specific drifts and the lack of mechanisms for continuous, stable
adaptation compound this challenge. This paper introduces LS-OGD, a novel
adaptive control framework for robust multimodal learning in the presence of
concept drift. LS-OGD uses an online controller that dynamically adjusts the
model's learning rate and the fusion weights between different data modalities
in response to detected drift and evolving prediction errors. We prove that
under bounded drift conditions, the LS-OGD system's prediction error is
uniformly ultimately bounded and converges to zero if the drift ceases.
Additionally, we demonstrate that the adaptive fusion strategy effectively
isolates and mitigates the impact of severe modality-specific drift, thereby
ensuring system resilience and fault tolerance. These theoretical guarantees
establish a principled foundation for developing reliable and continuously
adapting multimodal learning systems.

</details>


### [184] [BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling](https://arxiv.org/abs/2510.15945)
*Guangya Wan,Zixin Stephen Xu,Sasa Zorc,Manel Baucells,Mengxuan Hu,Hao Wang,Sheng Li*

Main category: cs.LG

TL;DR: BEACON是一个基于贝叶斯学习的自适应采样框架，用于优化LLM生成样本的停止时机，平衡准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 多采样能提升LLM输出质量，但计算成本高，需找到停止采样的最优时机。

Method: 结合序列搜索和贝叶斯学习，实时更新奖励分布后验，动态决定停止采样。

Result: BEACON减少80%采样量，保持输出质量，适用于高效偏好数据生成。

Conclusion: BEACON提供理论最优性和实用性，为未来研究提供可行方案。

Abstract: Sampling multiple responses is a common way to improve LLM output quality,
but it comes at the cost of additional computation. The key challenge is
deciding when to stop generating new samples to balance accuracy gains against
efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive
Criterion for Optimal N-stopping), a principled adaptive sampling framework
grounded in Sequential Search with Bayesian Learning. BEACON sequentially
generates responses from the policy LLM, updates posterior belief over reward
distributions in real time without further training, and determines when to
stop by weighing expected gains against computational cost. Sampling terminates
once the marginal utility of further exploration no longer justifies the
expense. We establish both theoretical optimality guarantees and practical
tractability, and show empirically that BEACON reduces average sampling by up
to 80% while maintaining response quality. We further demonstrate BEACON's
utility for cost-efficient preference data generation and outline practical
extensions, offering actionable insights for future researchers.

</details>


### [185] [Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns](https://arxiv.org/abs/2510.15946)
*Wenshuo Wang,Ziyou Jiang,Junjie Wang,Mingyang Li,Jie Huang,Yuekai Huang,Zhiyuan Chang,Feiyan Duan,Qing Wang*

Main category: cs.LG

TL;DR: PatMD通过识别和主动缓解误判风险模式，提升有害网络模因的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以识别模因中的隐含有害表达，导致误判。

Method: 构建误判风险模式知识库，动态指导MLLM推理。

Result: 在5个任务中平均F1-score提升8.30%，准确率提升7.71%。

Conclusion: PatMD显著提升有害模因检测的泛化能力和准确性。

Abstract: Internet memes have emerged as a popular multimodal medium, yet they are
increasingly weaponized to convey harmful opinions through subtle rhetorical
devices like irony and metaphor. Existing detection approaches, including
MLLM-based techniques, struggle with these implicit expressions, leading to
frequent misjudgments. This paper introduces PatMD, a novel approach that
improves harmful meme detection by learning from and proactively mitigating
these potential misjudgment risks. Our core idea is to move beyond superficial
content-level matching and instead identify the underlying misjudgment risk
patterns, proactively guiding the MLLMs to avoid known misjudgment pitfalls. We
first construct a knowledge base where each meme is deconstructed into a
misjudgment risk pattern explaining why it might be misjudged, either
overlooking harmful undertones (false negative) or overinterpreting benign
content (false positive). For a given target meme, PatMD retrieves relevant
patterns and utilizes them to dynamically guide the MLLM's reasoning.
Experiments on a benchmark of 6,626 memes across 5 harmful detection tasks show
that PatMD outperforms state-of-the-art baselines, achieving an average of
8.30\% improvement in F1-score and 7.71\% improvement in accuracy,
demonstrating strong generalizability and improved detection capability of
harmful memes.

</details>


### [186] [WaveNet's Precision in EEG Classification](https://arxiv.org/abs/2510.15947)
*Casper van Laar,Khubaib Ahmed*

Main category: cs.LG

TL;DR: WaveNet模型用于EEG信号分类，优于传统CNN和LSTM方法，但生理与病理信号分类存在轻微混淆。


<details>
  <summary>Details</summary>
Motivation: 传统EEG信号分类依赖专家视觉检查，随着数据量和复杂度增加，自动化分类需求迫切。

Method: 使用WaveNet架构，基于公开数据集训练，采用扩张因果卷积和残差连接处理EEG信号。

Result: 分类准确率超过CNN和LSTM，噪声和伪迹分类精度高，生理与病理信号分类存在轻微混淆。

Conclusion: WaveNet适合EEG信号分类，能捕捉细粒度和长程时间依赖，但需改进生理与病理信号的区分。

Abstract: This study introduces a WaveNet-based deep learning model designed to
automate the classification of EEG signals into physiological, pathological,
artifact, and noise categories. Traditional methods for EEG signal
classification, which rely on expert visual review, are becoming increasingly
impractical due to the growing complexity and volume of EEG recordings.
Leveraging a publicly available annotated dataset from Mayo Clinic and St.
Anne's University Hospital, the WaveNet model was trained, validated, and
tested on 209,232 samples with a 70/20/10 percent split. The model achieved a
classification accuracy exceeding previous CNN and LSTM-based approaches, and
was benchmarked against a Temporal Convolutional Network (TCN) baseline.
Notably, the model distinguishes noise and artifacts with high precision,
although it reveals a modest but explainable degree of misclassification
between physiological and pathological signals, reflecting inherent clinical
overlap. WaveNet's architecture, originally developed for raw audio synthesis,
is well suited for EEG data due to its use of dilated causal convolutions and
residual connections, enabling it to capture both fine-grained and long-range
temporal dependencies. The research also details the preprocessing pipeline,
including dynamic dataset partitioning and normalization steps that support
model generalization.

</details>


### [187] [Cross-dataset Multivariate Time-series Model for Parkinson's Diagnosis via Keyboard Dynamics](https://arxiv.org/abs/2510.15950)
*Arianna Francesconi,Donato Cappetta,Fabio Rebecchi,Paolo Soda,Valerio Guarrasi,Rosa Sicilia*

Main category: cs.LG

TL;DR: 提出了一种基于击键动力学的新型管道，用于帕金森病的远程筛查和监测，通过深度学习方法取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，传统临床评估受限，需要非侵入性、可扩展的生物标志物。

Method: 包括数据预处理、预训练八种深度学习架构、微调及外部验证。

Result: 混合卷积-循环和基于Transformer的模型在外部验证中表现优异，AUC-ROC超过90%，F1-Score超过70%。

Conclusion: 击键动力学可作为可靠的数字生物标志物，为帕金森病的早期检测和持续监测提供新途径。

Abstract: Parkinson's disease (PD) presents a growing global challenge, affecting over
10 million individuals, with prevalence expected to double by 2040. Early
diagnosis remains difficult due to the late emergence of motor symptoms and
limitations of traditional clinical assessments. In this study, we propose a
novel pipeline that leverages keystroke dynamics as a non-invasive and scalable
biomarker for remote PD screening and telemonitoring. Our methodology involves
three main stages: (i) preprocessing of data from four distinct datasets,
extracting four temporal signals and addressing class imbalance through the
comparison of three methods; (ii) pre-training eight state-of-the-art
deep-learning architectures on the two largest datasets, optimizing temporal
windowing, stride, and other hyperparameters; (iii) fine-tuning on an
intermediate-sized dataset and performing external validation on a fourth,
independent cohort. Our results demonstrate that hybrid convolutional-recurrent
and transformer-based models achieve strong external validation performance,
with AUC-ROC scores exceeding 90% and F1-Score over 70%. Notably, a temporal
convolutional model attains an AUC-ROC of 91.14% in external validation,
outperforming existing methods that rely solely on internal validation. These
findings underscore the potential of keystroke dynamics as a reliable digital
biomarker for PD, offering a promising avenue for early detection and
continuous monitoring.

</details>


### [188] [Fire-EnSF: Wildfire Spread Data Assimilation using Ensemble Score Filter](https://arxiv.org/abs/2510.15954)
*Hongzheng Shi,Yuhang Wang,Xiao Liu*

Main category: cs.LG

TL;DR: 本文探讨了Ensemble Score Filter (EnSF)在实时野火蔓延预测中的应用，展示了其在准确性、稳定性和计算效率上的优势。


<details>
  <summary>Details</summary>
Motivation: 野火破坏性增强且控制成本高昂，需要实时准确的蔓延预测来优化管理。数据同化技术结合观测与模型预测，可提升预测精度。

Method: 采用基于扩散模型的Ensemble Score Filter (EnSF)算法，用于高维非线性滤波问题，适用于野火蔓延模型的数据同化。

Result: EnSF在准确性、稳定性和计算效率上表现优异，成为野火数据同化的实用方法。

Conclusion: EnSF是一种高效、稳健的野火数据同化方法，代码已公开。

Abstract: As wildfires become increasingly destructive and expensive to control,
effective management of active wildfires requires accurate, real-time fire
spread predictions. To enhance the forecasting accuracy of active fires, data
assimilation plays a vital role by integrating observations (such as
remote-sensing data) and fire predictions generated from numerical models. This
paper provides a comprehensive investigation on the application of a recently
proposed diffusion-model-based filtering algorithm -- the Ensemble Score Filter
(EnSF) -- to the data assimilation problem for real-time active wildfire spread
predictions. Leveraging a score-based generative diffusion model, EnSF has been
shown to have superior accuracy for high-dimensional nonlinear filtering
problems, making it an ideal candidate for the filtering problems of wildfire
spread models. Technical details are provided, and our numerical investigations
demonstrate that EnSF provides superior accuracy, stability, and computational
efficiency, establishing it as a robust and practical method for wildfire data
assimilation. Our code has been made publicly available.

</details>


### [189] [How Good Are LLMs at Processing Tool Outputs?](https://arxiv.org/abs/2510.15955)
*Kiran Kate,Yara Rizk,Poulami Ghosh,Ashu Gulati,Tathagata Chakraborti,Zidane Wright,Mayank Agarwal*

Main category: cs.LG

TL;DR: LLMs处理JSON工具响应的能力不足，不同策略性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs处理结构化（JSON）工具响应的能力，填补研究空白。

Method: 创建数据集，评估15种开放和闭源模型，采用多种提示策略。

Result: JSON处理对前沿模型仍具挑战性，性能差异达3%至50%。

Conclusion: 最优处理策略取决于工具输出的性质和大小，以及推理复杂度。

Abstract: Most realistic task automation problems require large language models (LLMs)
to call tools, which often return complex JSON responses. These responses must
be further processed to derive the information necessary for task completion.
The ability of LLMs to do so is under-studied. In this paper, we study the tool
response processing task and LLMs' abilities to process structured (JSON)
responses. We created a dataset for this task, and evaluated 15 open and closed
weight models using multiple prompting approaches. Our results show that JSON
processing remains a difficult task even for frontier models across multiple
prompting strategies. The optimal response processing strategy depends on both
the nature and size of the tool outputs, as well as the complexity of the
required reasoning. Variations in processing approaches can lead to performance
differences ranging from 3\% to 50\%.

</details>


### [190] [Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling](https://arxiv.org/abs/2510.15960)
*Sana Kordoghli,Abdelhakim Settar,Oumayma Belaati,Mohammad Alkhatib*

Main category: cs.LG

TL;DR: 研究通过热化学转化和AI优化，探索废弃咖啡渣和枣核等生物质资源在可持续氢生产中的潜力。


<details>
  <summary>Details</summary>
Motivation: 推动可持续能源和废物管理策略，利用AI提高热解过程的建模精度和优化效率。

Method: 采用热重分析、动力学建模和LSTM模型，评估纯物质及混合物的热解性能。

Result: Blend 3氢产率最高但活化能最大，Blend 1活化能最佳；LSTM模型预测热重曲线精度极高（R²: 0.9996-0.9998）。

Conclusion: AI整合显著提升热解过程理解与优化，为可持续氢生产提供有效途径。

Abstract: This work contributes to advancing sustainable energy and waste management
strategies by investigating the thermochemical conversion of food-based biomass
through pyrolysis, highlighting the role of artificial intelligence (AI) in
enhancing process modelling accuracy and optimization efficiency. The main
objective is to explore the potential of underutilized biomass resources, such
as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen
production. Specifically, it aims to optimize the pyrolysis process while
evaluating the performance of these resources both individually and as blends.
Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC
analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS
- 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential
but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1
exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic
modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS
as the most accurate. These approaches provide a detailed understanding of the
pyrolysis process, with particular emphasis on the integration of artificial
intelligence. An LSTM model trained with lignocellulosic data predicted TGA
curves with exceptional accuracy (R^2: 0.9996-0.9998).

</details>


### [191] [Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use](https://arxiv.org/abs/2510.15961)
*Yiyang Li,Zehong Wang,Zhengqing Yuan,Zheyuan Zhang,Keerthiram Murugesan,Chuxu Zhang,Yanfang Ye*

Main category: cs.LG

TL;DR: LAMI是一种新型的联合图-语言建模框架，用于检测青少年和年轻成年人（TYAs）的非法药物使用行为，并解释其行为风险因素。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立处理调查变量，忽略了变量间的潜在关联结构，导致对非法药物使用的检测和解释不足。

Method: LAMI通过将个体响应表示为关系图，学习潜在连接，并结合大型语言模型生成自然语言解释。

Result: 在YRBS和NSDUH数据集上，LAMI在预测准确性上优于基线方法，并能揭示有意义的行为子结构和心理社会路径。

Conclusion: LAMI不仅提高了预测准确性，还提供了对非法药物使用行为风险因素的深入解释。

Abstract: Illicit drug use among teenagers and young adults (TYAs) remains a pressing
public health concern, with rising prevalence and long-term impacts on health
and well-being. To detect illicit drug use among TYAs, researchers analyze
large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the
National Survey on Drug Use and Health (NSDUH), which preserve rich
demographic, psychological, and environmental factors related to substance use.
However, existing modeling methods treat survey variables independently,
overlooking latent and interconnected structures among them. To address this
limitation, we propose LAMI (LAtent relation Mining with bi-modal
Interpretability), a novel joint graph-language modeling framework for
detecting illicit drug use and interpreting behavioral risk factors among TYAs.
LAMI represents individual responses as relational graphs, learns latent
connections through a specialized graph structure learning layer, and
integrates a large language model to generate natural language explanations
grounded in both graph structures and survey semantics. Experiments on the YRBS
and NSDUH datasets show that LAMI outperforms competitive baselines in
predictive accuracy. Interpretability analyses further demonstrate that LAMI
reveals meaningful behavioral substructures and psychosocial pathways, such as
family dynamics, peer influence, and school-related distress, that align with
established risk factors for substance use.

</details>


### [192] [CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models](https://arxiv.org/abs/2510.15962)
*Zhuxuanzi Wang,Mingqiao Mo,Xi Xiao,Chen Liu,Chenrui Ma,Yunbei Zhang,Xiao Wang,Smita Krishnaswamy,Tianyang Wang*

Main category: cs.LG

TL;DR: CTR-LoRA是一种基于曲率信任区域的参数高效微调框架，通过动态调整秩分配和稳定性优化，显著提升了性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法在参数分配与训练动态之间缺乏耦合，导致效率与性能受限。

Method: 结合秩调度与稳定性感知优化，利用二阶代理分配参数，并通过Fisher/Hessian度量约束更新。

Result: 在7B-13B模型上，CTR-LoRA在分布内外任务中均优于基线，同时提升稳定性、降低内存需求。

Conclusion: CTR-LoRA为PEFT提供了一条更稳健、可部署的路径，平衡了性能与效率。

Abstract: Parameter-efficient fine-tuning (PEFT) has become the standard approach for
adapting large language models under limited compute and memory budgets.
Although previous methods improve efficiency through low-rank updates,
quantization, or heuristic budget reallocation, they often decouple the
allocation of capacity from the way updates evolve during training. In this
work, we introduce CTR-LoRA, a framework guided by curvature trust region that
integrates rank scheduling with stability-aware optimization. CTR-LoRA
allocates parameters based on marginal utility derived from lightweight
second-order proxies and constrains updates using a Fisher/Hessian-metric trust
region. Experiments on multiple open-source backbones (7B-13B), evaluated on
both in-distribution and out-of-distribution benchmarks, show consistent
improvements over strong PEFT baselines. In addition to increased accuracy,
CTR-LoRA enhances training stability, reduces memory requirements, and achieves
higher throughput, positioning it on the Pareto frontier of performance and
efficiency. These results highlight a principled path toward more robust and
deployable PEFT.

</details>


### [193] [Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity](https://arxiv.org/abs/2510.15964)
*Tuowei Wang,Kun Li,Zixu Hao,Donglin Bai,Ju Ren,Yaoxue Zhang,Ting Cao,Mao Yang*

Main category: cs.LG

TL;DR: 提出了一种名为Long Exposure的系统，通过Shadowy Sparsity加速参数高效微调（PEFT），显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT技术在时间和成本上效率不足，Shadowy Sparsity在微调中的稀疏性未被充分挖掘。

Method: Long Exposure系统包含三个组件：Shadowy-sparsity Exposer、Sequence-oriented Predictor和Dynamic-aware Operator，分别用于捕捉稀疏性、高效预测和处理动态稀疏操作。

Result: 实验表明，Long Exposure在端到端微调中最高可提速2.49倍。

Conclusion: Long Exposure为LLMs的PEFT提供了高效加速方案。

Abstract: The adaptation of pre-trained large language models (LLMs) to diverse
downstream tasks via fine-tuning is critical for numerous applications.
However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques
presents significant challenges in terms of time investments and operational
costs. In this paper, we first introduce a nuanced form of sparsity, termed
Shadowy Sparsity, which is distinctive in fine-tuning and has not been
adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long
Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure
comprises three key components: Shadowy-sparsity Exposer employs a prolonged
sensing range to capture more sparsity details under shadowy sparsity;
Sequence-oriented Predictor provides efficient yet accurate predictions to
handle large sequence inputs and constantly-evolving parameters; and
Dynamic-aware Operator facilitates more structured computational patterns and
coalesced memory accesses, addressing dynamic sparse operations. Extensive
evaluations show that Long Exposure outperforms state-of-the-arts with up to a
$2.49\times$ speedup in end-to-end fine-tuning, offering promising advancements
in accelerating PEFT for LLMs.

</details>


### [194] [One Token Embedding Is Enough to Deadlock Your Large Reasoning Model](https://arxiv.org/abs/2510.15965)
*Mohan Zhang,Yihua Zhang,Jinghan Jia,Zhangyang Wang,Sijia Liu,Tianlong Chen*

Main category: cs.LG

TL;DR: Deadlock Attack通过恶意嵌入诱导大型推理模型陷入无限推理循环，揭示了推理效率的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 揭示大型推理模型在链式推理中的新安全漏洞，即资源耗尽攻击。

Method: 提出Deadlock Attack，通过优化嵌入和植入后门策略，诱导模型陷入无限推理。

Result: 在四种先进LRM和三个数学推理基准上实现100%攻击成功率，模型被迫生成最大令牌限制。

Conclusion: Deadlock Attack暴露了LRM在推理效率上的关键安全漏洞，且难以缓解。

Abstract: Modern large reasoning models (LRMs) exhibit impressive multi-step
problem-solving via chain-of-thought (CoT) reasoning. However, this iterative
thinking mechanism introduces a new vulnerability surface. We present the
Deadlock Attack, a resource exhaustion method that hijacks an LRM's generative
control flow by training a malicious adversarial embedding to induce perpetual
reasoning loops. Specifically, the optimized embedding encourages transitional
tokens (e.g., "Wait", "But") after reasoning steps, preventing the model from
concluding its answer. A key challenge we identify is the
continuous-to-discrete projection gap: na\"ive projections of adversarial
embeddings to token sequences nullify the attack. To overcome this, we
introduce a backdoor implantation strategy, enabling reliable activation
through specific trigger tokens. Our method achieves a 100% attack success rate
across four advanced LRMs (Phi-RM, Nemotron-Nano, R1-Qwen, R1-Llama) and three
math reasoning benchmarks, forcing models to generate up to their maximum token
limits. The attack is also stealthy (in terms of causing negligible utility
loss on benign user inputs) and remains robust against existing strategies
trying to mitigate the overthinking issue. Our findings expose a critical and
underexplored security vulnerability in LRMs from the perspective of reasoning
(in)efficiency.

</details>


### [195] [Gains: Fine-grained Federated Domain Adaptation in Open Set](https://arxiv.org/abs/2510.15967)
*Zhengyi Zhong,Wenzheng Jiang,Weidong Bao,Ji Wang,Cheems Wang,Guanbo Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 提出了一种细粒度的联邦域适应方法（Gains），用于开放环境下的新知识发现与适应。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，新客户端不断加入联邦学习过程，带来新知识，现有方法在知识发现和适应效率上存在不足。

Method: 将模型分为编码器和分类器，利用细粒度知识发现和贡献驱动聚合技术，并设计抗遗忘机制。

Result: 在多域数据集和三种典型数据偏移场景下，Gains显著优于基线方法。

Conclusion: Gains有效平衡了源域和目标域的性能，提升了知识适应效率。

Abstract: Conventional federated learning (FL) assumes a closed world with a fixed
total number of clients. In contrast, new clients continuously join the FL
process in real-world scenarios, introducing new knowledge. This raises two
critical demands: detecting new knowledge, i.e., knowledge discovery, and
integrating it into the global model, i.e., knowledge adaptation. Existing
research focuses on coarse-grained knowledge discovery, and often sacrifices
source domain performance and adaptation efficiency. To this end, we propose a
fine-grained federated domain adaptation approach in open set (Gains). Gains
splits the model into an encoder and a classifier, empirically revealing
features extracted by the encoder are sensitive to domain shifts while
classifier parameters are sensitive to class increments. Based on this, we
develop fine-grained knowledge discovery and contribution-driven aggregation
techniques to identify and incorporate new knowledge. Additionally, an
anti-forgetting mechanism is designed to preserve source domain performance,
ensuring balanced adaptation. Experimental results on multi-domain datasets
across three typical data-shift scenarios demonstrate that Gains significantly
outperforms other baselines in performance for both source-domain and
target-domain clients. Code is available at:
https://github.com/Zhong-Zhengyi/Gains.

</details>


### [196] [Self-Attention to Operator Learning-based 3D-IC Thermal Simulation](https://arxiv.org/abs/2510.15968)
*Zhen Huang,Hong Wang,Wenkai Yang,Muxi Tang,Depeng Xie,Ting-Jung Lin,Yu Zhang,Wei W. Xing,Lei He*

Main category: cs.LG

TL;DR: SAU-FNO结合自注意力和U-Net的FNO框架，显著提升3D IC热管理效率，比传统FEM方法快842倍。


<details>
  <summary>Details</summary>
Motivation: 3D IC的热管理因高功率密度而日益复杂，传统方法速度慢，机器学习方法存在高频信息丢失和高保真数据依赖问题。

Method: 提出SAU-FNO框架，结合自注意力、U-Net和FNO，捕捉长程依赖和局部高频特征，并利用迁移学习优化低保真数据。

Result: SAU-FNO在热预测精度上达到最优，比传统FEM方法快842倍。

Conclusion: SAU-FNO是高效3D IC热模拟工具，减少对高保真数据的依赖并加速训练。

Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power
densities. Traditional PDE-solving-based methods, while accurate, are too slow
for iterative design. Machine learning approaches like FNO provide faster
alternatives but suffer from high-frequency information loss and high-fidelity
data dependency. We introduce Self-Attention U-Net Fourier Neural Operator
(SAU-FNO), a novel framework combining self-attention and U-Net with FNO to
capture long-range dependencies and model local high-frequency features
effectively. Transfer learning is employed to fine-tune low-fidelity data,
minimizing the need for extensive high-fidelity datasets and speeding up
training. Experiments demonstrate that SAU-FNO achieves state-of-the-art
thermal prediction accuracy and provides an 842x speedup over traditional FEM
methods, making it an efficient tool for advanced 3D IC thermal simulations.

</details>


### [197] [LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems](https://arxiv.org/abs/2510.15969)
*Paul-Niklas Ken Kandora,Simon Caspar Zeller,Aaron Jeremias Elsing,Elena Kuss,Steffen Rebennack*

Main category: cs.LG

TL;DR: LinearizeLLM利用基于代理的框架和大型语言模型（LLM）自动将非线性优化问题线性化。


<details>
  <summary>Details</summary>
Motivation: 非线性优化问题的线性化通常依赖专家手动完成，效率低且成本高。

Method: 通过为每种非线性模式分配专门的“线性化代理”，利用LLM生成精确的线性化模型，最终整合为可求解的线性问题。

Result: 在20个真实非线性优化问题上测试，结果表明该方法能有效自动化线性化任务。

Conclusion: LinearizeLLM为非线性优化问题的全自动建模提供了新途径。

Abstract: Reformulating nonlinear optimization problems is largely manual and
expertise-intensive, yet it remains essential for solving such problems with
linear optimization solvers or applying special-purpose algorithms. We
introduce \textit{LinearizeLLM}, an agent-based framework that solves this task
by leveraging Large Language Models (LLMs). The framework assigns each
nonlinear pattern to a \textit{reformulation agent} that is explicitly
instructed to derive an exact linear reformulation for its nonlinearity
pattern, for instance, absolute-value terms or bilinear products of decision
variables. The agents then coordinate to assemble a solver-ready linear model
equivalent to the original problem. To benchmark the approach, we create a
dataset of 20 real-world nonlinear optimization problems derived from the
established ComplexOR dataset of linear optimization problems. We evaluate our
approach with several LLMs. Our results indicate that specialized LLM agents
can automate linearization tasks, opening a path toward fully conversational
modeling pipelines for nonlinear optimization.

</details>


### [198] [Predict Training Data Quality via Its Geometry in Metric Space](https://arxiv.org/abs/2510.15970)
*Yang Ba,Mohammad Sadeq Abolhasani,Rong Pan*

Main category: cs.LG

TL;DR: 论文探讨了训练数据的几何结构对模型性能的影响，提出利用持久同调量化数据多样性。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据是AI的基础，但其几何结构对性能的影响尚未充分研究。

Method: 使用持久同调从度量空间中提取拓扑特征，量化数据多样性。

Result: 持久同调是分析并增强训练数据的有效工具。

Conclusion: 持久同调为提升AI系统训练数据提供了新方法。

Abstract: High-quality training data is the foundation of machine learning and
artificial intelligence, shaping how models learn and perform. Although much is
known about what types of data are effective for training, the impact of the
data's geometric structure on model performance remains largely underexplored.
We propose that both the richness of representation and the elimination of
redundancy within training data critically influence learning outcomes. To
investigate this, we employ persistent homology to extract topological features
from data within a metric space, thereby offering a principled way to quantify
diversity beyond entropy-based measures. Our findings highlight persistent
homology as a powerful tool for analyzing and enhancing the training data that
drives AI systems.

</details>


### [199] [Bolster Hallucination Detection via Prompt-Guided Data Augmentation](https://arxiv.org/abs/2510.15977)
*Wenyun Li,Zheng Zhang,Dongmei Jiang,Xiangyuan Lan*

Main category: cs.LG

TL;DR: PALE框架通过提示引导的数据增强和对比马氏距离评分（CM Score）提升大语言模型（LLM）幻觉检测性能，无需人工标注，性能提升6.55%。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成内容中幻觉检测的数据稀缺问题，提升检测可靠性。

Method: 提出PALE框架，利用提示引导生成真实和幻觉数据，并设计CM Score评估嵌入空间分布。

Result: PALE在幻觉检测任务中显著优于基线方法6.55%。

Conclusion: PALE是一种高效、通用的幻觉检测解决方案，适用于实际应用。

Abstract: Large language models (LLMs) have garnered significant interest in AI
community. Despite their impressive generation capabilities, they have been
found to produce misleading or fabricated information, a phenomenon known as
hallucinations. Consequently, hallucination detection has become critical to
ensure the reliability of LLM-generated content. One primary challenge in
hallucination detection is the scarcity of well-labeled datasets containing
both truthful and hallucinated outputs. To address this issue, we introduce
Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework
that leverages prompt-guided responses from LLMs as data augmentation for
hallucination detection. This strategy can generate both truthful and
hallucinated data under prompt guidance at a relatively low cost. To more
effectively evaluate the truthfulness of the sparse intermediate embeddings
produced by LLMs, we introduce an estimation metric called the Contrastive
Mahalanobis Score (CM Score). This score is based on modeling the distributions
of truthful and hallucinated data in the activation space. CM Score employs a
matrix decomposition approach to more accurately capture the underlying
structure of these distributions. Importantly, our framework does not require
additional human annotations, offering strong generalizability and practicality
for real-world applications. Extensive experiments demonstrate that PALE
achieves superior hallucination detection performance, outperforming the
competitive baseline by a significant margin of 6.55%.

</details>


### [200] [DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space](https://arxiv.org/abs/2510.15978)
*Junchao Gong,Jingyi Xu,Ben Fei,Fenghua Ling,Wenlong Zhang,Kun Chen,Wanghan Xu,Weidong Yang,Xiaokang Yang,Lei Bai*

Main category: cs.LG

TL;DR: DAWP框架通过AIDA模块初始化，解决了AIWP依赖再分析数据的问题，实现了在观测空间中的高效天气预报。


<details>
  <summary>Details</summary>
Motivation: 依赖再分析数据的AIWP存在数据同化偏差和时间差异问题，观测预报成为新范式，但需解决不规则高分辨率观测数据的时空动态学习挑战。

Method: 提出DAWP框架，包含AIDA模块（使用掩码多模态自编码器同化不规则卫星观测数据）和时空解耦Transformer（带跨区域边界条件），实现观测空间动态学习。

Result: 实验表明AIDA初始化显著提升AIWP的滚动预测效率和效果，DAWP在全局降水预报中潜力显著。

Conclusion: DAWP通过观测空间初始化，为AIWP摆脱再分析数据依赖提供了创新解决方案，展示了在天气预报中的广泛应用前景。

Abstract: Weather prediction is a critical task for human society, where impressive
progress has been made by training artificial intelligence weather prediction
(AIWP) methods with reanalysis data. However, reliance on reanalysis data
limits the AIWPs with shortcomings, including data assimilation biases and
temporal discrepancies. To liberate AIWPs from the reanalysis data, observation
forecasting emerges as a transformative paradigm for weather prediction. One of
the key challenges in observation forecasting is learning spatiotemporal
dynamics across disparate measurement systems with irregular high-resolution
observation data, which constrains the design and prediction of AIWPs. To this
end, we propose our DAWP as an innovative framework to enable AIWPs to operate
in a complete observation space by initialization with an artificial
intelligence data assimilation (AIDA) module. Specifically, our AIDA module
applies a mask multi-modality autoencoder(MMAE)for assimilating irregular
satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a
spatiotemporal decoupling transformer with cross-regional boundary conditioning
(CBC), learning the dynamics in observation space, to enable sub-image-based
global observation forecasting. Comprehensive experiments demonstrate that AIDA
initialization significantly improves the roll out and efficiency of AIWP.
Additionally, we show that DAWP holds promising potential to be applied in
global precipitation forecasting.

</details>


### [201] [Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2510.15979)
*Zexu Sun,Yongcheng Zeng,Erxue Min,Heyang Gao,Bokai Ji,Xu Chen*

Main category: cs.LG

TL;DR: Cog-Rethinker是一种新颖的分层元认知强化学习框架，旨在提高大型语言模型在推理任务中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定提示模板激活LLM的推理能力，导致样本利用率低，尤其是在弱LLM中产生大量无效输出。

Method: 提出Cog-Rethinker框架，通过分层元认知两阶段方法改进RL训练中的样本利用：1）分解零准确率问题为子问题；2）参考错误答案优化结果。

Result: 实验表明，Cog-Rethinker在数学推理基准测试中表现优异，样本效率显著提升，加速收敛。

Conclusion: Cog-Rethinker通过分层元认知策略有效解决了样本浪费问题，提升了LLM的推理效率和性能。

Abstract: Contemporary progress in large language models (LLMs) has revealed notable
inferential capacities via reinforcement learning (RL) employing verifiable
reward, facilitating the development of O1 and R1-like reasoning models.
Directly training from base models with RL is called zero-RL. However, previous
works rely upon activating LLMs' inherent capacities through fixed prompt
templates. This strategy introduces substantial sampling inefficiencies for
weak LLMs, as the majority of problems generate invalid outputs during
accuracy-driven filtration in reasoning tasks, which causes a waste of samples.
To solve this issue, we propose Cog-Rethinker, a novel hierarchical
metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses
on the rollout procedure in RL training. After the direct rollout, our
Cog-Rethinker improves sample utilization in a hierarchical metacognitive
two-stage framework. By leveraging human cognition during solving problems,
firstly, it prompts policy to decompose zero-accuracy problems into subproblems
to produce final reasoning results. Secondly, with zero-accuracy problems in
previous rollout stage, it further prompts policy to refine these answers by
referencing previous wrong solutions. Moreover, to enable cold-start of the two
new reasoning patterns and maintain train-test consistency across prompt
templates, our Cog-Rethinker applies supervised fine-tuning on the policy using
correct samples of the two stages with direct rollout template. Experimental
results demonstrate Cog-Rethinker's superior performance on various
mathematical reasoning benchmarks, we also analyzed its improved sample
efficiency that accelerates convergence compared to baseline methods.

</details>


### [202] [AMiD: Knowledge Distillation for LLMs with $α$-mixture Assistant Distribution](https://arxiv.org/abs/2510.15982)
*Donghyeok Shin,Yeongmin Kim,Suhyeon Jo,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMiD的统一知识蒸馏框架，通过引入α-混合辅助分布，解决了现有方法中辅助分布设计零散和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 大型自回归语言模型（LLMs）虽然性能优异，但计算和内存成本高。知识蒸馏（KD）通过将知识从大模型转移到小模型来缓解这一问题，但现有方法因高维输出和容量差距导致训练不稳定。

Method: 提出α-混合辅助分布，通过引入可调节参数α扩展辅助分布的设计空间，并基于最优性统一了辅助分布使用的散度族。

Result: 实验表明，AMiD在性能和训练稳定性上优于现有方法。

Conclusion: AMiD通过理论支持的辅助分布空间，为知识蒸馏提供了更优的解决方案。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable
improvement across many tasks but incur high computational and memory costs.
Knowledge distillation (KD) mitigates this issue by transferring knowledge from
a large teacher to a smaller student through distributional alignment. Previous
studies have proposed various discrepancy metrics, but the capacity gap and
training instability caused by near-zero probabilities, stemming from the
high-dimensional output of LLMs, remain fundamental limitations. To overcome
these challenges, several approaches implicitly or explicitly incorporating
assistant distribution have recently been proposed. However, the past proposals
of assistant distributions have been a fragmented approach without a systematic
investigation of the interpolation path and the divergence. This paper proposes
$\alpha$-mixture assistant distribution, a novel generalized family of
assistant distributions, and $\alpha$-mixture distillation, coined AMiD, a
unified framework for KD using the assistant distribution. The $\alpha$-mixture
assistant distribution provides a continuous extension of the assistant
distribution by introducing a new distribution design variable $\alpha$, which
has been fixed in all previous approaches. Furthermore, AMiD generalizes the
family of divergences used with the assistant distributions based on
optimality, which has also been restricted in previous works. Through extensive
experiments, we demonstrate that AMiD offers superior performance and training
stability by leveraging a broader and theoretically grounded assistant
distribution space.

</details>


### [203] [MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction Representation Learning for Early Sepsis Prediction](https://arxiv.org/abs/2510.15985)
*Zexi Tan,Tao Xie,Binbin Sun,Xiang Zhang,Yiqun Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: MEET-Sepsis框架通过MERE机制和CDTA模块，显著提升了早期脓毒症预测的准确性，仅需20%的ICU监测时间。


<details>
  <summary>Details</summary>
Motivation: 脓毒症在ICU中死亡率高，早期预测困难，现有AI方法难以捕捉早期微弱信号。

Method: 提出MERE机制构建丰富特征视图，结合CDTA模块进行多尺度时间序列学习。

Result: MEET-Sepsis在仅需20%监测时间下达到竞争性预测准确率。

Conclusion: 该框架显著推进了早期脓毒症预测，验证了其有效性。

Abstract: Sepsis is a life-threatening infectious syndrome associated with high
mortality in intensive care units (ICUs). Early and accurate sepsis prediction
(SP) is critical for timely intervention, yet remains challenging due to subtle
early manifestations and rapidly escalating mortality. While AI has improved SP
efficiency, existing methods struggle to capture weak early temporal signals.
This paper introduces a Multi-Endogenous-view Representation Enhancement (MERE)
mechanism to construct enriched feature views, coupled with a Cascaded
Dual-convolution Time-series Attention (CDTA) module for multi-scale temporal
representation learning. The proposed MEET-Sepsis framework achieves
competitive prediction accuracy using only 20% of the ICU monitoring time
required by SOTA methods, significantly advancing early SP. Extensive
validation confirms its efficacy. Code is available at:
https://github.com/yueliangy/MEET-Sepsis.

</details>


### [204] [User Profiles of Sleep Disorder Sufferers: Towards Explainable Clustering and Differential Variable Analysis](https://arxiv.org/abs/2510.15986)
*Sifeddine Sellami,Juba Agoun,Lamia Yessad,Louenas Bounia*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的可解释人工智能方法，用于分析睡眠障碍患者的关键因素。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍诊断复杂，技术结合医学数据分析为理解这些障碍提供了新视角。

Method: 采用基于聚类的可解释人工智能方法，对患者进行分组并识别关键影响因素。

Result: 在匿名真实数据上的实验验证了方法的有效性和相关性。

Conclusion: 该方法为睡眠障碍的诊断和理解提供了新的工具和视角。

Abstract: Sleep disorders have a major impact on patients' health and quality of life,
but their diagnosis remains complex due to the diversity of symptoms. Today,
technological advances, combined with medical data analysis, are opening new
perspectives for a better understanding of these disorders. In particular,
explainable artificial intelligence (XAI) aims to make AI model decisions
understandable and interpretable for users. In this study, we propose a
clustering-based method to group patients according to different sleep disorder
profiles. By integrating an explainable approach, we identify the key factors
influencing these pathologies. An experiment on anonymized real data
illustrates the effectiveness and relevance of our approach.

</details>


### [205] [Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models](https://arxiv.org/abs/2510.15987)
*Samuel Lippl,Thomas McGee,Kimberly Lopez,Ziwen Pan,Pierce Zhang,Salma Ziadi,Oliver Eberle,Ida Momennejad*

Main category: cs.LG

TL;DR: 论文提出了一种框架，通过追踪和操控算法原语来研究大语言模型（LLMs）的多步推理能力，揭示了推理背后的几何逻辑和跨任务、跨模型的通用性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型如何通过潜在和推理时间计算解决多步推理问题，并研究其背后的算法原语。

Method: 通过聚类神经激活并标记匹配的推理轨迹，利用函数向量方法提取可重用的算法原语向量，并通过几何操作（如加减和标量运算）验证其组合性。

Result: 实验表明，算法原语在跨任务和跨模型（如Phi-4、Llama-3-8B）中具有共享性和任务特异性，推理微调（如Phi-4-Reasoning）能增强算法的泛化能力。

Conclusion: LLMs的推理可能由算法原语的组合几何支持，这些原语具有跨任务和跨模型的通用性，推理微调能进一步提升其泛化能力。

Abstract: How do latent and inference time computations enable large language models
(LLMs) to solve multi-step reasoning? We introduce a framework for tracing and
steering algorithmic primitives that underlie model reasoning. Our approach
links reasoning traces to internal activation patterns and evaluates
algorithmic primitives by injecting them into residual streams and measuring
their effect on reasoning steps and task performance. We consider four
benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph
navigation. We operationalize primitives by clustering neural activations and
labeling their matched reasoning traces. We then apply function vector methods
to derive primitive vectors as reusable compositional building blocks of
reasoning. Primitive vectors can be combined through addition, subtraction, and
scalar operations, revealing a geometric logic in activation space. Cross-task
and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both
shared and task-specific primitives. Notably, comparing Phi-4 with its
reasoning-finetuned variant highlights compositional generalization after
finetuning: Phi-4-Reasoning exhibits more systematic use of verification and
path-generation primitives. Injecting the associated primitive vectors in
Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning.
Together, these findings demonstrate that reasoning in LLMs may be supported by
a compositional geometry of algorithmic primitives, that primitives transfer
cross-task and cross-model, and that reasoning finetuning strengthens
algorithmic generalization across domains.

</details>


### [206] [Can GRPO Help LLMs Transcend Their Pretraining Origin?](https://arxiv.org/abs/2510.15990)
*Kangqi Ni,Zhen Tan,Zijie Liu,Pingzhi Li,Tianlong Chen*

Main category: cs.LG

TL;DR: RLVR通过GRPO算法提升LLM的推理能力，但效果不一致。研究发现GRPO受限于基础模型的分布，无法发现全新解决方案，仅在目标任务与预训练偏好一致时有效。


<details>
  <summary>Details</summary>
Motivation: 探究GRPO在何种条件下能提升推理能力并泛化到分布外（OOD）任务。

Method: 从数据分布角度分析，理论证明GRPO是保守的权重调整方案，并通过实验验证其泛化能力。

Result: GRPO仅在目标任务与预训练偏好一致时有效，对分布内（ID）任务的提升随性能饱和而减弱。

Conclusion: GRPO并非通用推理增强工具，而是强化预训练偏好的工具，未来需开发能超越预训练限制的算法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by
the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach
for enhancing the reasoning abilities of Large Language Models (LLMs). Despite
its wide adoption, GRPO's gains are often inconsistent; for instance, a model
may show significant improvement in one reasoning domain, like mathematics, yet
remain stagnant in another, such as medicine. This inconsistency raises a
critical question: under what conditions does GRPO improve reasoning and
generalize out-of-distribution (OOD)? We investigate this from a data
distribution perspective. We first prove theoretically that GRPO is a
conservative reweighting scheme, bounded by the base model's distribution and
thus unable to discover completely novel solutions. We further validate this in
carefully designed controlled studies by training transformers from scratch,
evaluating generalization across reasoning depth, input length, token
representation, and compositionality. Our results provide a principled
explanation for GRPO's boundaries: OOD improvement emerges only when the target
task aligns with the model's pretrained biases, while gains on in-distribution
(ID) tasks diminish as performance saturates. This reframes GRPO not as a
universal reasoning enhancer but as a tool that sharpens pretraining biases.
Our findings motivate future development of algorithms that can expand a
model's capabilities beyond its pretraining origin.

</details>


### [207] [Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments](https://arxiv.org/abs/2510.15992)
*Ziming Dai,Tuo Zhang,Fei Gao,Xingyi Cai,Xiaofei Wang,Cheng Zhang,Wenyu Wang,Chengjie Zang*

Main category: cs.LG

TL;DR: Stratos是一个端到端的大语言模型蒸馏框架，自动选择服务器和模型，优化知识蒸馏和部署，满足用户定义的性能和预算约束。


<details>
  <summary>Details</summary>
Motivation: 工业对定制化、低成本大语言模型的需求增长，现有蒸馏框架需要手动干预且难以满足复杂需求。

Method: Stratos自动化选择Pareto最优服务器，动态匹配师生模型对，根据任务复杂度调整蒸馏策略。

Result: 在特定领域任务中，Stratos生成的学生模型精度是GPT-4o基线的四倍，同时降低延迟和成本。

Conclusion: Stratos展示了在垂直领域部署大语言模型的潜力。

Abstract: The growing industrial demand for customized and cost-efficient large
language models (LLMs) is fueled by the rise of vertical, domain-specific tasks
and the need to optimize performance under constraints such as latency and
budget. Knowledge distillation, as an efficient model compression and transfer
technique, offers a feasible solution. However, existing distillation
frameworks often require manual intervention and struggle to meet such complex
user-defined distillation requirements. To bridge this gap, we propose Stratos,
an end-to-end LLM distillation pipeline that automates server and model
selection, knowledge distillation, and deployment in distributed cloud
environments. Given user-defined constraints on model performance and system
budget, Stratos automatically selects Pareto-optimal servers, dynamically
matches teacher-student pairs, and adapts distillation strategies based on task
complexity to optimize cloud hosting. Experiments show that Stratos produces a
student model that achieves four times the accuracy of its GPT-4o teacher
baseline on a rare, domain-specific Mahjong reasoning task with reverse
synthetic data and knowledge injection. Moreover, it achieves reduced latency
and cost without compromising accuracy. These results highlight its promise for
vertical-domain LLM deployment.

</details>


### [208] [Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning](https://arxiv.org/abs/2510.15996)
*Ozan K. Tonguz,Federico Taschin*

Main category: cs.LG

TL;DR: 论文探讨了使用Kolmogorov-Smirnov (KS) 测试来监测和量化测试数据与训练数据分布偏差的方法，并展示了其对AI代理性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中测试数据分布与训练数据分布偏差导致的预测误差问题，特别是在智能交通等关键应用中。

Method: 提出使用KS测试和KS距离来实时监测和量化分布偏差，并分析其对AI代理性能的影响。

Result: 实验表明，即使KS距离为0.02，也可能导致强化学习代理在单个交叉路口的旅行时间增加50%。

Conclusion: KS测试和KS距离可作为实时监测AI代理性能退化的有效工具，帮助AI代理更智能地应对分布偏差。

Abstract: One of the major problems in Machine Learning (ML) and Artificial
Intelligence (AI) is the fact that the probability distribution of the test
data in the real world could deviate substantially from the probability
distribution of the training data set. When this happens, the predictions of an
ML system or an AI agent could involve large errors which is very troublesome
and undesirable. While this is a well-known hard problem plaguing the AI and ML
systems' accuracy and reliability, in certain applications such errors could be
critical for safety and reliability of AI and ML systems. One approach to deal
with this problem is to monitor and measure the deviation in the probability
distribution of the test data in real time and to compensate for this
deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov
(KS) Test for measuring the distribution shift and we show how the KS distance
can be used to quantify the distribution shift and its impact on an AI agent's
performance. Our results suggest that KS distance could be used as a valuable
statistical tool for monitoring and measuring the distribution shift. More
specifically, it is shown that even a distance of KS=0.02 could lead to about
50\% increase in the travel time at a single intersection using a Reinforcement
Learning agent which is quite significant. It is hoped that the use of KS Test
and KS distance in AI-based smart transportation could be an important step
forward for gauging the performance degradation of an AI agent in real time and
this, in turn, could help the AI agent to cope with the distribution shift in a
more informed manner.

</details>


### [209] [AMStraMGRAM: Adaptive Multi-cutoff Strategy Modification for ANaGRAM](https://arxiv.org/abs/2510.15998)
*Nilo Schwencke,Cyriaque Rousselot,Alena Shilova,Cyril Furtlehner*

Main category: cs.LG

TL;DR: 论文提出了一种多截止适应策略，改进了ANaGRAM方法，显著提升了PINNs的训练效果，并在实验中达到机器精度。


<details>
  <summary>Details</summary>
Motivation: 研究自然梯度方法在训练物理信息神经网络（PINNs）中的优势，并分析其训练动态。

Method: 采用基于奇异值分解和截止正则化的ANaGRAM方法，并提出多截止适应策略。

Result: 在基准PDE实验中验证了方法的有效性，部分实验达到机器精度。

Conclusion: 通过谱理论框架解释了正则化的必要性，并扩展了与格林函数理论的联系。

Abstract: Recent works have shown that natural gradient methods can significantly
outperform standard optimizers when training physics-informed neural networks
(PINNs). In this paper, we analyze the training dynamics of PINNs optimized
with ANaGRAM, a natural-gradient-inspired approach employing singular value
decomposition with cutoff regularization. Building on this analysis, we propose
a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance.
Experiments on benchmark PDEs validate the effectiveness of our method, which
allows to reach machine precision on some experiments. To provide theoretical
grounding, we develop a framework based on spectral theory that explains the
necessity of regularization and extend previous shown connections with Green's
functions theory.

</details>


### [210] [Layer-Aware Influence for Online Data Valuation Estimation](https://arxiv.org/abs/2510.16007)
*Ziao Yang,Longbo Huang,Hongfu Liu*

Main category: cs.LG

TL;DR: 提出了一种层感知在线估计器，用于高效动态评估训练样本的影响，避免了参数级和全网络梯度计算。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注静态影响评估，忽略了优化过程中样本影响的动态变化，尤其是在深度模型中。

Method: 开发了一种仅需损失到输出梯度的层感知在线估计器，减少了计算负担。

Result: 在LLM预训练、微调和图像分类任务中，方法显著提高了准确性，同时大幅降低了时间和内存成本。

Conclusion: 该方法使动态数据筛选在实践中更高效和可扩展。

Abstract: Data-centric learning emphasizes curating high-quality training samples to
boost performance rather than designing new architectures. A central problem is
to estimate the influence of training sample efficiently. Prior studies largely
focus on static influence measured on a converged model, overlooking how data
valuation dynamically changes during optimization. This omission neglects the
dynamic nature of sample influence during optimization, especially in deep
models. To address the computational burden of frequent influence estimation,
we develop a layer-aware online estimator that requires only loss-to-output
gradients. This design avoids parameter-level and full-network gradients while
preserving ranking fidelity. Extensive experiments across LLM pretraining,
fine-tuning, and image classification show our method improves accuracy with
substantially lower time and memory cost, making dynamic data curation
efficient and scalable in practice.

</details>


### [211] [STAR: Boosting Time Series Foundation Models for Anomaly Detection through State-aware Adapter](https://arxiv.org/abs/2510.16014)
*Hanyin Cheng,Ruitong Zhang,Yuning Lu,Peng Chen,Meng Wang,Yang Shu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: STAR是一个即插即用的模块，用于增强时间序列基础模型（TSFMs）在建模和利用状态变量方面的能力，从而提升多元时间序列异常检测（MTSAD）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有TSFMs通常忽视状态变量的分类特性及其作为条件的关键作用，导致检测性能下降。

Method: STAR包含三个核心组件：Identity-guided State Encoder、Conditional Bottleneck Adapter和Numeral-State Matching模块。

Result: 在真实数据集上的实验表明，STAR能显著提升TSFMs在MTSAD中的性能。

Conclusion: STAR通过有效建模状态变量，解决了现有方法的局限性，提升了异常检测效果。

Abstract: While Time Series Foundation Models (TSFMs) have demonstrated remarkable
success in Multivariate Time Series Anomaly Detection (MTSAD), however, in
real-world industrial scenarios, many time series comprise not only numerical
variables such as temperature and flow, but also numerous discrete state
variables that describe the system status, such as valve on/off or day of the
week. Existing TSFMs often overlook the distinct categorical nature of state
variables and their critical role as conditions, typically treating them
uniformly with numerical variables. This inappropriate modeling approach
prevents the model from fully leveraging state information and even leads to a
significant degradation in detection performance after state variables are
integrated. To address this critical limitation, this paper proposes a novel
STate-aware AdapteR (STAR). STAR is a plug-and-play module designed to enhance
the capability of TSFMs in modeling and leveraging state variables during the
fine-tuning stage. Specifically, STAR comprisesthree core components: (1) We
design an Identity-guided State Encoder, whicheffectively captures the complex
categorical semantics of state variables through a learnable State Memory. (2)
We propose a Conditional Bottleneck Adapter, which dynamically generates
low-rank adaptation parameters conditioned on the current state, thereby
flexibly injecting the influence of state variables into the backbone model.
(3) We also introduce a Numeral-State Matching module to more effectively
detect anomalies inherent to the state variables themselves. Extensive
experiments conducted on real-world datasets demonstrate that STAR can improve
the performance of existing TSFMs on MTSAD.

</details>


### [212] [Decision-focused Sensing and Forecasting for Adaptive and Rapid Flood Response: An Implicit Learning Approach](https://arxiv.org/abs/2510.16015)
*Qian Sun,Graham Hults,Susu Xu*

Main category: cs.LG

TL;DR: 提出了一种新型决策导向框架，通过优化传感器布局和洪水预测模型，以减少洪水应急响应决策的遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统洪水管理系统依赖固定策略部署传感器和训练预测模型，忽视了相同感知增益和预测误差可能导致不同决策的问题。

Method: 结合上下文评分网络、可微分传感器选择模块、时空洪水重建与预测模型以及任务特定的可微分决策层，利用I-MLE实现离散传感器配置的梯度学习。

Result: 通过端到端管道优化下游洪水响应决策，减少决策遗憾。

Conclusion: 该框架为洪水应急响应提供了更高效和任务导向的决策支持。

Abstract: Timely and reliable decision-making is vital for flood emergency response,
yet it remains severely hindered by limited and imprecise situational awareness
due to various budget and data accessibility constraints. Traditional flood
management systems often rely on in-situ sensors to calibrate remote
sensing-based large-scale flood depth forecasting models, and further take
flood depth estimates to optimize flood response decisions. However, these
approaches often take fixed, decision task-agnostic strategies to decide where
to put in-situ sensors (e.g., maximize overall information gain) and train
flood forecasting models (e.g., minimize average forecasting errors), but
overlook that systems with the same sensing gain and average forecasting errors
may lead to distinct decisions. To address this, we introduce a novel
decision-focused framework that strategically selects locations for in-situ
sensor placement and optimize spatio-temporal flood forecasting models to
optimize downstream flood response decision regrets. Our end-to-end pipeline
integrates four components: a contextual scoring network, a differentiable
sensor selection module under hard budget constraints, a spatio-temporal flood
reconstruction and forecasting model, and a differentiable decision layer
tailored to task-specific objectives. Central to our approach is the
incorporation of Implicit Maximum Likelihood Estimation (I-MLE) to enable
gradient-based learning over discrete sensor configurations, and probabilistic
decision heads to enable differentiable approximation to various constrained
disaster response tasks.

</details>


### [213] [Transfer learning strategies for accelerating reinforcement-learning-based flow control](https://arxiv.org/abs/2510.16016)
*Saeed Salehi*

Main category: cs.LG

TL;DR: 研究探讨了如何通过迁移学习加速深度强化学习（DRL）在混沌流体多保真控制中的应用，比较了渐进神经网络（PNNs）与传统微调策略的性能。


<details>
  <summary>Details</summary>
Motivation: 混沌流体流动的多保真控制需要高效的深度强化学习方法，但传统方法存在收敛慢和知识遗忘问题，因此需要探索更稳健的迁移学习框架。

Method: 采用渐进神经网络（PNNs）和传统微调策略，以Kuramoto-Sivashinsky（KS）系统为基准，评估知识从低保真环境到高保真环境的迁移效果。

Result: PNNs在知识迁移中表现稳定且高效，避免了灾难性遗忘，而微调策略对预训练时长敏感且易失效。

Conclusion: PNNs为复杂流动控制提供了稳健、可扩展且计算高效的迁移学习框架。

Abstract: This work investigates transfer learning strategies to accelerate deep
reinforcement learning (DRL) for multifidelity control of chaotic fluid flows.
Progressive neural networks (PNNs), a modular architecture designed to preserve
and reuse knowledge across tasks, are employed for the first time in the
context of DRL-based flow control. In addition, a comprehensive benchmarking of
conventional fine-tuning strategies is conducted, evaluating their performance,
convergence behavior, and ability to retain transferred knowledge. The
Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how
knowledge encoded in control policies, trained in low-fidelity environments,
can be effectively transferred to high-fidelity settings. Systematic
evaluations show that while fine-tuning can accelerate convergence, it is
highly sensitive to pretraining duration and prone to catastrophic forgetting.
In contrast, PNNs enable stable and efficient transfer by preserving prior
knowledge and providing consistent performance gains, and are notably robust to
overfitting during the pretraining phase. Layer-wise sensitivity analysis
further reveals how PNNs dynamically reuse intermediate representations from
the source policy while progressively adapting deeper layers to the target
task. Moreover, PNNs remain effective even when the source and target
environments differ substantially, such as in cases with mismatched physical
regimes or control objectives, where fine-tuning strategies often result in
suboptimal adaptation or complete failure of knowledge transfer. The results
highlight the potential of novel transfer learning frameworks for robust,
scalable, and computationally efficient flow control that can potentially be
applied to more complex flow configurations.

</details>


### [214] [Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality](https://arxiv.org/abs/2510.16020)
*Sangjoon Lee,Haris Moazam Sheikh*

Main category: cs.LG

TL;DR: AirDbM是一种专为翼型优化设计的形态设计方法，通过选择最优基线翼型减少设计空间维度，实现高效优化。


<details>
  <summary>Details</summary>
Motivation: 传统翼型优化方法需要大量设计变量，探索设计空间效率低，因此需要一种能减少变量并保持多样性的方法。

Method: 从UIUC翼型数据库中选择12个最优基线翼型，通过形态设计方法重构99%的数据库，误差低于0.005。

Result: 在多目标优化中，AirDbM快速收敛并发现新的帕累托最优解，提升升阻比；在强化学习中表现优于传统参数化方法。

Conclusion: AirDbM展示了形态设计在机器学习和优化中的潜力，为高效翼型设计提供了新思路。

Abstract: Effective airfoil geometry optimization requires exploring a diverse range of
designs using as few design variables as possible. This study introduces
AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil
optimization that systematically reduces design-space dimensionality. AirDbM
selects an optimal set of 12 baseline airfoils from the UIUC airfoil database,
which contains over 1,600 shapes, by sequentially adding the baseline that most
increases the design capacity. With these baselines, AirDbM reconstructs 99 \%
of the database with a mean absolute error below 0.005, which matches the
performance of a previous DbM approach that used more baselines. In
multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence
and achieves a Pareto front with a greater hypervolume than that of the
previous larger-baseline study, where new Pareto-optimal solutions are
discovered with enhanced lift-to-drag ratios at moderate stall tolerances.
Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement
learning (RL) agents in generating airfoil geometry when compared to
conventional airfoil parameterization methods, implying the broader potential
of DbM in machine learning-driven design.

</details>


### [215] [Feature-driven reinforcement learning for photovoltaic in continuous intraday trading](https://arxiv.org/abs/2510.16021)
*Arega Getaneh Abate,Xiufeng Liu,Ruyu Liu,Xiaobing Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于特征驱动的强化学习方法，用于光伏发电商的日内交易，通过实时调整头寸提高收益并减少不平衡成本。


<details>
  <summary>Details</summary>
Motivation: 光伏发电商面临发电量和短期电价的不确定性，需要一种实时调整交易策略的方法以优化收益。

Method: 将问题建模为马尔可夫决策过程，使用近端策略优化（PPO）训练线性可解释的策略，结合市场微观结构和历史特征。

Result: 在历史市场数据上训练并在样本外评估，策略表现优于基准方法，具有快速收敛、实时推理和透明决策规则的特点。

Conclusion: 特征驱动的强化学习为光伏发电商提供了一种实用、高效且可部署的日内交易解决方案。

Abstract: Photovoltaic (PV) operators face substantial uncertainty in generation and
short-term electricity prices. Continuous intraday markets enable producers to
adjust their positions in real time, potentially improving revenues and
reducing imbalance costs. We propose a feature-driven reinforcement learning
(RL) approach for PV intraday trading that integrates data-driven features into
the state and learns bidding policies in a sequential decision framework. The
problem is cast as a Markov Decision Process with a reward that balances
trading profit and imbalance penalties and is solved with Proximal Policy
Optimization (PPO) using a predominantly linear, interpretable policy. Trained
on historical market data and evaluated out-of-sample, the strategy
consistently outperforms benchmark baselines across diverse scenarios.
Extensive validation shows rapid convergence, real-time inference, and
transparent decision rules. Learned weights highlight the central role of
market microstructure and historical features. Taken together, these results
indicate that feature-driven RL offers a practical, data-efficient, and
operationally deployable pathway for active intraday participation by PV
producers.

</details>


### [216] [Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization](https://arxiv.org/abs/2510.16022)
*Changsheng Wang,Xin Chen,Sijia Liu,Ke Ding*

Main category: cs.LG

TL;DR: 论文提出了一种名为IB-FT的方法，通过信息瓶颈（IB）惩罚来缓解预训练大语言模型（LLMs）在代码生成任务中的记忆障碍问题，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现预训练LLMs在代码生成任务中存在记忆障碍问题，即模型过度记忆下游代码数据，阻碍其学习新的、可泛化的代码知识。

Method: 提出IB-FT方法，通过对代码数据的隐藏表示施加IB惩罚，压缩无关记忆特征，同时保留任务相关信息。

Result: 在两个代码基准测试（OriGen和Evol-CodeAlpaca-V1）中，IB-FT显著缓解了记忆障碍，提高了Pass@1性能，并在更严格的多样本指标Pass@k^(m)下表现更稳定。

Conclusion: IB-FT方法有效克服了记忆障碍，提升了代码生成任务的性能，为LLMs在代码领域的适应提供了新思路。

Abstract: Adapting pretrained large language models (LLMs) to code domains via
supervised fine-tuning (FT) has been commonly used for code generation.
However, we identify a previously underappreciated failure mode, the
memorization barrier, where strong memorization of downstream code data in the
base model could trap optimization and prevent the standard FT from effectively
acquiring new, generalizable code knowledge. To overcome this barrier, we
propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which
applies an IB penalty on hidden representations of the code data to compress
spurious, memorized features while preserving task-relevant information.
Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1)
show that IB-FT substantially alleviates the memorization barrier, improves
top-1 performance (Pass@$1$), and yields far more stable gains under the
stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if
at least $m$ of $k$ samples pass unit tests) compared with conventional FT.

</details>


### [217] [Unifying Polymer Modeling and Design via a Conformation-Centric Generative Foundation Model](https://arxiv.org/abs/2510.16023)
*Fanmeng Wang,Shan Mei,Wentao Guo,Hongshuai Wang,Qi Ou,Zhifeng Gao,Hongteng Xu*

Main category: cs.LG

TL;DR: PolyConFM是一种基于构象的聚合物基础模型，通过生成预训练统一聚合物建模与设计，解决了现有方法忽略全局结构信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有聚合物深度学习方法仅关注单体级描述符，忽略了聚合物构象的全局结构信息，且缺乏通用基础模型。

Method: PolyConFM通过条件生成范式预训练，利用掩码自回归建模重建局部构象，并生成方向变换以恢复聚合物构象。

Result: 实验表明，PolyConFM在多种下游任务中优于特定任务方法。

Conclusion: PolyConFM为聚合物科学提供了通用且强大的工具。

Abstract: Polymers, macromolecules formed from covalently bonded monomers, underpin
countless technologies and are indispensable to modern life. While deep
learning is advancing polymer science, existing methods typically represent the
whole polymer solely through monomer-level descriptors, overlooking the global
structural information inherent in polymer conformations, which ultimately
limits their practical performance. Moreover, this field still lacks a
universal foundation model that can effectively support diverse downstream
tasks, thereby severely constraining progress. To address these challenges, we
introduce PolyConFM, the first polymer foundation model that unifies polymer
modeling and design through conformation-centric generative pretraining.
Recognizing that each polymer conformation can be decomposed into a sequence of
local conformations (i.e., those of its repeating units), we pretrain PolyConFM
under the conditional generation paradigm, reconstructing these local
conformations via masked autoregressive (MAR) modeling and further generating
their orientation transformations to recover the corresponding polymer
conformation. Besides, we construct the first high-quality polymer conformation
dataset via molecular dynamics simulations to mitigate data sparsity, thereby
enabling conformation-centric pretraining. Experiments demonstrate that
PolyConFM consistently outperforms representative task-specific methods on
diverse downstream tasks, equipping polymer science with a universal and
powerful tool.

</details>


### [218] [A tutorial on discovering and quantifying the effect of latent causal sources of multimodal EHR data](https://arxiv.org/abs/2510.16026)
*Marco Barbero-Mota,Eric V. Strobl,John M. Still,William W. Stead,Thomas A. Lasko*

Main category: cs.LG

TL;DR: 论文提出了一种通用的因果机器学习流程，用于从大规模电子健康记录中发现潜在因果源，并量化其对临床结果的影响。


<details>
  <summary>Details</summary>
Motivation: 旨在解决多模态临床数据处理中的不完美性，通过分解为独立潜在源，训练任务特定的因果模型。

Method: 使用概率独立的潜在源分解数据，并训练因果模型以估计个体因果效应。

Result: 通过两个实际应用验证了方法的通用性和在医学发现中的实用性。

Conclusion: 该方法为大规模医学发现提供了一种有效的因果分析工具。

Abstract: We provide an accessible description of a peer-reviewed generalizable causal
machine learning pipeline to (i) discover latent causal sources of large-scale
electronic health records observations, and (ii) quantify the source causal
effects on clinical outcomes. We illustrate how imperfect multimodal clinical
data can be processed, decomposed into probabilistic independent latent
sources, and used to train taskspecific causal models from which individual
causal effects can be estimated. We summarize the findings of the two
real-world applications of the approach to date as a demonstration of its
versatility and utility for medical discovery at scale.

</details>


### [219] [RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction](https://arxiv.org/abs/2510.16035)
*Yingguang Yang,Xianghua Zeng,Qi Wu,Hao Peng,Yutong Xia,Hao Liu,Bin Chong,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体强化学习的对抗性框架RoBCtrl，用于攻击基于GNN的社交机器人检测器，通过扩散模型生成高保真机器人账户，并通过强化学习优化攻击策略。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的社交机器人检测方法存在局限性，如对社交代理的控制有限、检测器黑盒性及机器人异质性，导致其鲁棒性不足。

Method: 使用扩散模型生成高保真机器人账户，并通过多智能体强化学习（MARL）模拟对抗行为，同时设计基于结构熵的分层状态抽象以加速学习。

Result: 实验表明，RoBCtrl能有效削弱基于GNN的检测器性能。

Conclusion: 该框架为社交机器人检测的鲁棒性研究提供了新视角，并展示了扩散模型和强化学习在此领域的潜力。

Abstract: Social networks have become a crucial source of real-time information for
individuals. The influence of social bots within these platforms has garnered
considerable attention from researchers, leading to the development of numerous
detection technologies. However, the vulnerability and robustness of these
detection methods is still underexplored. Existing Graph Neural Network
(GNN)-based methods cannot be directly applied due to the issues of limited
control over social agents, the black-box nature of bot detectors, and the
heterogeneity of bots. To address these challenges, this paper proposes the
first adversarial multi-agent Reinforcement learning framework for social Bot
control attacks (RoBCtrl) targeting GNN-based social bot detectors.
Specifically, we use a diffusion model to generate high-fidelity bot accounts
by reconstructing existing account data with minor modifications, thereby
evading detection on social platforms. To the best of our knowledge, this is
the first application of diffusion models to mimic the behavior of evolving
social bots effectively. We then employ a Multi-Agent Reinforcement Learning
(MARL) method to simulate bots adversarial behavior. We categorize social
accounts based on their influence and budget. Different agents are then
employed to control bot accounts across various categories, optimizing the
attachment strategy through reinforcement learning. Additionally, a
hierarchical state abstraction based on structural entropy is designed to
accelerate the reinforcement learning. Extensive experiments on social bot
detection datasets demonstrate that our framework can effectively undermine the
performance of GNN-based detectors.

</details>


### [220] [Vector Quantization in the Brain: Grid-like Codes in World Models](https://arxiv.org/abs/2510.16039)
*Xiangyuan Peng,Xingsi Dong,Si Wu*

Main category: cs.LG

TL;DR: GCQ是一种受大脑启发的时空压缩方法，通过动态选择网格状模式来压缩观察-动作序列。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化方法处理静态输入，无法有效压缩时空序列，GCQ旨在解决这一问题。

Method: 使用动作条件化码本，基于连续吸引子神经网络动态选择码字，实现时空联合压缩。

Result: GCQ在多种任务中表现出高效的压缩能力和下游任务性能。

Conclusion: GCQ不仅提供了高效的序列建模工具，还为神经系统中网格状编码的形成提供了理论视角。

Abstract: We propose Grid-like Code Quantization (GCQ), a brain-inspired method for
compressing observation-action sequences into discrete representations using
grid-like patterns in attractor dynamics. Unlike conventional vector
quantization approaches that operate on static inputs, GCQ performs
spatiotemporal compression through an action-conditioned codebook, where
codewords are derived from continuous attractor neural networks and dynamically
selected based on actions. This enables GCQ to jointly compress space and time,
serving as a unified world model. The resulting representation supports
long-horizon prediction, goal-directed planning, and inverse modeling.
Experiments across diverse tasks demonstrate GCQ's effectiveness in compact
encoding and downstream performance. Our work offers both a computational tool
for efficient sequence modeling and a theoretical perspective on the formation
of grid-like codes in neural systems.

</details>


### [221] [AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization](https://arxiv.org/abs/2510.16045)
*Mengtao Lv,Ruiqi Zhu,Xinyu Wang,Yun Li*

Main category: cs.LG

TL;DR: AMS-Quant通过非整数位宽浮点量化技术，显著加速LLM推理，减少内存占用，且精度损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数庞大，导致存储和推理效率瓶颈，浮点量化可缓解此问题，但传统方法限于整数位宽。

Method: 提出Mantissa-bit Sharing和Adaptive Searching技术，实现非整数位宽量化，并通过CUDA内核优化内存访问。

Result: 在FP-5.33-e2m3和FP4.25-e2m2量化下，LLM解码速度分别提升2.8倍和3.2倍，精度损失极小。

Conclusion: AMS-Quant为非整数位宽量化提供了有效解决方案，显著提升LLM推理效率。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
various kinds of tasks, while the billion or even trillion parameters bring
storage and efficiency bottlenecks for inference. Quantization, particularly
floating-point quantization, is known to be capable of speeding up LLM
inference by reducing memory footprint and data movement during the inference
process. For the first time, we advance the floating-point quantization
exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant,
to further approach the quantization sweet spot. AMS-Quant incorporates two
novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing,
which groups k quantized weights and lets them share the least significant
mantissa bit, allowing us to further approach the minimum quantization
bit-width without accuracy loss. (2) It introduces Adaptive Searching, which
employs an offline optimization strategy to minimize the accuracy degradation
introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA
Linear kernels, which translates memory savings into wall-clock latency
reduction by reducing memory access. Extensive experiments on large-scale
datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3
and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16
inference (2.8x and 3.2x), with negligible accuracy loss.

</details>


### [222] [GUIrilla: A Scalable Framework for Automated Desktop UI Exploration](https://arxiv.org/abs/2510.16051)
*Sofiya Garkot,Maksym Shamrai,Ivan Synytsia,Mariya Hirna*

Main category: cs.LG

TL;DR: GUIrilla是一个自动化框架，用于通过原生API探索macOS应用程序，生成大规模数据集GUIrilla-Task，显著提升LLM在UI任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决桌面自动化中数据收集的挑战，尤其是macOS生态系统中数据不足的问题。

Method: 利用原生可访问性API系统探索应用程序，构建层次化GUI图，并开发交互处理器以实现全面覆盖。

Result: 发布了包含27,171个任务的GUIrilla-Task数据集，实验显示其显著提升LLM性能，优于合成基线。

Conclusion: GUIrilla框架和数据集为桌面自动化研究提供了开放资源，支持未来研究。

Abstract: Autonomous agents capable of operating complex graphical user interfaces
(GUIs) have the potential to transform desktop automation. While recent
advances in large language models (LLMs) have significantly improved UI
understanding, navigating full-window, multi-application desktop environments
remains a major challenge. Data availability is limited by costly manual
annotation, closed-source datasets and surface-level synthetic pipelines. We
introduce GUIrilla, an automated scalable framework that systematically
explores applications via native accessibility APIs to address the critical
data collection challenge in GUI automation. Our framework focuses on macOS -
an ecosystem with limited representation in current UI datasets - though many
of its components are designed for broader cross-platform applicability.
GUIrilla organizes discovered interface elements and crawler actions into
hierarchical GUI graphs and employs specialized interaction handlers to achieve
comprehensive application coverage. Using the application graphs from GUIrilla
crawler, we construct and release GUIrilla-Task, a large-scale dataset of
27,171 functionally grounded tasks across 1,108 macOS applications, each
annotated with full-desktop and window-level screenshots, accessibility
metadata, and semantic action traces. Empirical results show that tuning
LLM-based agents on GUIrilla-Task significantly improves performance on
downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro
benchmark while using 97% less data. We also release macapptree, an open-source
library for reproducible collection of structured accessibility metadata, along
with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold
benchmark, and the framework code to support open research in desktop autonomy.

</details>


### [223] [FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting](https://arxiv.org/abs/2510.16053)
*Chenyang Yu,Xinpeng Xie,Yan Huang,Chenxi Qiu*

Main category: cs.LG

TL;DR: 论文探讨了基于图神经网络（GNNs）的交通流量预测方法，强调了其在捕捉复杂空间依赖性和动态时间模式中的优势，并指出当前方法在事件信息处理上的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着城市化加剧，交通拥堵问题日益严重，需要更可靠的交通预测模型来优化资源分配和提升出行体验。

Method: 采用图神经网络（GNNs）及其变体（如STGCN、GraphWaveNet等）来建模交通数据的空间依赖性和时间动态性。

Result: 现有方法在标准数据集上表现优异，但在处理未知事件时依赖专家知识，泛化能力有限。

Conclusion: 未来研究需减少对专家知识的依赖，提升模型对复杂未知事件的适应能力。

Abstract: Accurate traffic forecasting is a core technology for building Intelligent
Transportation Systems (ITS), enabling better urban resource allocation and
improved travel experiences. With growing urbanization, traffic congestion has
intensified, highlighting the need for reliable and responsive forecasting
models. In recent years, deep learning, particularly Graph Neural Networks
(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can
effectively capture complex spatial dependencies in road network topology and
dynamic temporal evolution patterns in traffic flow data. Foundational models
such as STGCN and GraphWaveNet, along with more recent developments including
STWave and D2STGNN, have achieved impressive performance on standard traffic
datasets. These approaches incorporate sophisticated graph convolutional
structures and temporal modeling mechanisms, demonstrating particular
effectiveness in capturing and forecasting traffic patterns characterized by
periodic regularities. To address this challenge, researchers have explored
various ways to incorporate event information. Early attempts primarily relied
on manually engineered event features. For instance, some approaches introduced
manually defined incident effect scores or constructed specific subgraphs for
different event-induced traffic conditions. While these methods somewhat
enhance responsiveness to specific events, their core drawback lies in a heavy
reliance on domain experts' prior knowledge, making generalization to diverse
and complex unknown events difficult, and low-dimensional manual features often
lead to the loss of rich semantic details.

</details>


### [224] [Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?](https://arxiv.org/abs/2510.16060)
*Coen Adler,Yuxin Chang,Felix Draxler,Samar Abdi,Padhraic Smyth*

Main category: cs.LG

TL;DR: 时间序列基础模型的校准性能优于基线模型，且不会系统性过度自信或不足自信。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在预测性能上表现优异，但其校准特性尚未充分研究，而校准确实对许多实际应用至关重要。

Method: 研究了五种时间序列基础模型和两种基线模型的校准相关特性，通过系统评估校准情况、预测头变化的影响以及长期自回归预测下的校准。

Result: 时间序列基础模型比基线模型校准更好，且不会系统性过度自信或不足自信，与其他深度学习模型常见的过度自信形成对比。

Conclusion: 时间序列基础模型在校准方面表现优异，为实际应用提供了可靠的基础。

Abstract: The recent development of foundation models for time series data has
generated considerable interest in using such models across a variety of
applications. Although foundation models achieve state-of-the-art predictive
performance, their calibration properties remain relatively underexplored,
despite the fact that calibration can be critical for many practical
applications. In this paper, we investigate the calibration-related properties
of five recent time series foundation models and two competitive baselines. We
perform a series of systematic evaluations assessing model calibration (i.e.,
over- or under-confidence), effects of varying prediction heads, and
calibration under long-term autoregressive forecasting. We find that time
series foundation models are consistently better calibrated than baseline
models and tend not to be either systematically over- or under-confident, in
contrast to the overconfidence often seen in other deep learning models.

</details>


### [225] [Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks](https://arxiv.org/abs/2510.16063)
*Muhy Eddin Za'ter,Bri-Mathias Hodge*

Main category: cs.LG

TL;DR: 本文提出了一种基于分层图神经网络的变电站级电压估计方法，解决了传统方法在稀疏测量和大规模网络中的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源渗透和电压波动增加，需要更鲁棒的状态估计方法来确保电网安全高效运行。

Method: 利用电气拓扑和物理特征，构建分层图神经网络模型，并在低观测条件下保持鲁棒性。

Result: 在公开数据集上验证，模型RMSE比替代方法低2倍，且在1%测量覆盖率下仍保持高精度。

Conclusion: 图神经网络为配电系统提供了可扩展、可重复且数据驱动的电压监测方案。

Abstract: Accurate voltage estimation in distribution networks is critical for
real-time monitoring and increasing the reliability of the grid. As DER
penetration and distribution level voltage variability increase, robust
distribution system state estimation (DSSE) has become more essential to
maintain safe and efficient operations. Traditional DSSE techniques, however,
struggle with sparse measurements and the scale of modern feeders, limiting
their scalability to large networks. This paper presents a hierarchical graph
neural network for substation-level voltage estimation that exploits both
electrical topology and physical features, while remaining robust to the low
observability levels common to real-world distribution networks. Leveraging the
public SMART-DS datasets, the model is trained and evaluated on thousands of
buses across multiple substations and DER penetration scenarios. Comprehensive
experiments demonstrate that the proposed method achieves up to 2 times lower
RMSE than alternative data-driven models, and maintains high accuracy with as
little as 1\% measurement coverage. The results highlight the potential of GNNs
to enable scalable, reproducible, and data-driven voltage monitoring for
distribution systems.

</details>


### [226] [Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions](https://arxiv.org/abs/2510.16064)
*Muhy Eddin Za'ter,Bri-Mathias Hodge,Kyri Baker*

Main category: cs.LG

TL;DR: 提出了一种基于残差学习的图神经网络方法，用于快速求解非线性交流最优潮流问题，显著提升了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 非线性交流最优潮流（AC OPF）问题是实时电网运行中的主要计算瓶颈，需要一种更高效且准确的解决方案。

Method: 使用快速直流最优潮流（DC OPF）解作为基线，通过拓扑感知的图神经网络学习非线性修正，并结合物理约束的损失函数。

Result: 在57、118和2000总线系统上，MSE降低约25%，可行性误差减少3倍，运行速度提升13倍。

Conclusion: 残差学习是连接线性近似与交流可行最优潮流的实用且可扩展的方法，支持近实时决策。

Abstract: Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major
computational bottleneck for real-time grid operations. In this paper, we
propose a residual learning paradigm that uses fast DC optimal power flow (DC
OPF) solutions as a baseline, and learns only the nonlinear corrections
required to provide the full AC-OPF solution. The method utilizes a
topology-aware Graph Neural Network with local attention and two-level DC
feature integration, trained using a physics-informed loss that enforces AC
power-flow feasibility and operational limits. Evaluations on OPFData for 57-,
118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in
feasibility error, and up to 13X runtime speedup compared to conventional AC
OPF solvers. The model maintains accuracy under N-1 contingencies and scales
efficiently to large networks. These results demonstrate that residual learning
is a practical and scalable bridge between linear approximations and
AC-feasible OPF, enabling near real-time operational decision making.

</details>


### [227] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: FedPURIN是一种新型的个性化联邦学习框架，通过整数编程和稀疏聚合显著减少通信开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化联邦学习方法通信效率低的问题，以支持实际部署。

Method: 提出FedPURIN框架，利用整数编程识别关键参数并通过稀疏聚合减少通信。

Result: 在非独立同分布数据下，标准图像分类任务中表现优异，通信开销显著降低。

Conclusion: FedPURIN为通信高效的个性化联邦学习提供了新范式，特别适用于边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [228] [MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data](https://arxiv.org/abs/2510.16071)
*Qinxuan Wang,Chuang Wang,Mingyu Zhang,Jingwei Sun,Peipei Yang,Shuo Tang,Shiming Xiang*

Main category: cs.LG

TL;DR: 提出了一种多尺度神经算子（MNO），用于解决三维非结构化点云上的计算流体动力学问题，显著提升了精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在处理不规则域上的多尺度流体结构时，精度和可扩展性不足。

Method: MNO通过全局维度收缩注意力模块、局部图注意力模块和微观点级注意力模块，显式分解多尺度信息。

Result: 在四个基准测试中，MNO比现有方法减少了5%至40%的预测误差，并展示了更好的鲁棒性。

Conclusion: 多尺度设计对神经算子至关重要，MNO为学习复杂流体动力学提供了可扩展框架。

Abstract: Neural operators have emerged as a powerful data-driven paradigm for solving
Partial Differential Equations (PDEs), offering orders-of-magnitude
acceleration over traditional solvers. However, existing approaches still
suffer from limited accuracy and scalability, particularly on irregular domains
where fluid flows exhibit rich multiscale structures. In this work, we
introduce the Multiscale Neural Operator (MNO), a new architecture for
Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point
clouds. MNO explicitly decomposes information across three scales: a global
dimension-shrinkage attention module for long-range dependencies, a local graph
attention module for neighborhood-level interactions, and a micro point-wise
attention module for fine-grained details. This design preserves multiscale
inductive biases while remaining computationally efficient. We evaluate MNO on
four diverse benchmarks, covering both steady-state and unsteady flow scenarios
with up to 300K points. Across all tasks, MNO consistently outperforms
state-of-the-art baselines, reducing prediction errors by 5% to 40% and
demonstrating improved robustness in challenging 3D CFD problems. Our results
highlight the importance of explicit multiscale design for neural operators and
establish MNO as a scalable framework for learning complex fluid dynamics on
irregular domains.

</details>


### [229] [Early-stopping for Transformer model training](https://arxiv.org/abs/2510.16074)
*Jing He,Hua Jiang,Cheng Li,Siqian Xin,Shuzhen Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于随机矩阵理论（RMT）的新框架，用于分析Transformer训练动态，并提出了无验证的早期停止准则。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer训练动态的底层机制，以指导性能改进和早期停止决策。

Method: 利用RMT分析浅层自注意力矩阵V的谱密度，将其分为三个阶段，并提出两个无验证的停止准则。

Result: 观察到谱密度演化为重尾分布，并提出了定量指标和谱特征作为停止信号。

Conclusion: RMT框架能有效监控和诊断Transformer训练过程，准则与训练阶段高度一致。

Abstract: This work introduces a novel theoretical framework grounded in Random Matrix
Theory (RMT) for analyzing Transformer training dynamics. We focus on the
underlying mechanisms that drive performance improvements and derive principled
early-stopping criteria. Empirically, we observe that the spectral density of
the shallow self-attention matrix V consistently evolves into a heavy-tailed
distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we
demarcate training into three stages: structural exploration, heavy-tailed
structure stabilization, and convergence saturation. This staging provides
guidance for preliminary stopping decisions. Crucially, we propose two
consistent and validation-free criteria: a quantitative metric for heavy-tailed
dynamics and a novel spectral signature indicative of convergence. The strong
alignment between these criteria highlights the utility of RMT for monitoring
and diagnosing the progression of Transformer model training.

</details>


### [230] [Optimization of the quantization of dense neural networks from an exact QUBO formulation](https://arxiv.org/abs/2510.16075)
*Sergio Muñiz Subiñas,Manuel L. González,Jorge Ruiz Gómez,Alejandro Mata Ali,Jorge Martínez Martín,Miguel Franco Hernando,Ángel Miguel García-Vico*

Main category: cs.LG

TL;DR: 提出了一种基于ADAROUND的QUBO公式的后训练量化方法，用于密集神经网络。


<details>
  <summary>Details</summary>
Motivation: 通过量化方法减少神经网络的计算和存储开销，同时保持模型性能。

Method: 使用Frobenius距离作为目标函数，构建QUBO问题，并通过分解和启发式方法（如模拟退火）高效求解。

Result: 在多个数据集（MNIST、Fashion-MNIST等）和不同整数精度（int8到int1）上验证了方法的有效性。

Conclusion: 该方法优于传统的四舍五入量化方法，能够高效实现神经网络的量化。

Abstract: This work introduces a post-training quantization (PTQ) method for dense
neural networks via a novel ADAROUND-based QUBO formulation. Using the
Frobenius distance between the theoretical output and the dequantized output
(before the activation function) as the objective, an explicit QUBO whose
binary variables represent the rounding choice for each weight and bias is
obtained. Additionally, by exploiting the structure of the coefficient QUBO
matrix, the global problem can be exactly decomposed into $n$ independent
subproblems of size $f+1$, which can be efficiently solved using some
heuristics such as simulated annealing. The approach is evaluated on MNIST,
Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1
and compared with a round-to-nearest traditional quantization methodology.

</details>


### [231] [BPL: Bias-adaptive Preference Distillation Learning for Recommender System](https://arxiv.org/abs/2510.16076)
*SeongKu Kang,Jianxun Lian,Dongha Lee,Wonbin Kweon,Sanghwan Jang,Jaehyun Lee,Jindong Wang,Xing Xie,Hwanjo Yu*

Main category: cs.LG

TL;DR: BPL框架通过双重蒸馏策略在事实和反事实测试环境中均表现优异，解决了推荐系统中的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统存在偏差问题，导致用户偏好反馈不完整。现有方法多专注于反事实测试环境，忽视了事实测试环境的重要性。

Method: 提出BPL框架，结合教师-学生蒸馏和自蒸馏策略，逐步揭示用户偏好。

Result: 实验验证BPL在事实和反事实测试中均表现优异。

Conclusion: BPL是一种有效的学习框架，能够同时优化推荐系统在两种测试环境中的性能。

Abstract: Recommender systems suffer from biases that cause the collected feedback to
incompletely reveal user preference. While debiasing learning has been
extensively studied, they mostly focused on the specialized (called
counterfactual) test environment simulated by random exposure of items,
significantly degrading accuracy in the typical (called factual) test
environment based on actual user-item interactions. In fact, each test
environment highlights the benefit of a different aspect: the counterfactual
test emphasizes user satisfaction in the long-terms, while the factual test
focuses on predicting subsequent user behaviors on platforms. Therefore, it is
desirable to have a model that performs well on both tests rather than only
one. In this work, we introduce a new learning framework, called Bias-adaptive
Preference distillation Learning (BPL), to gradually uncover user preferences
with dual distillation strategies. These distillation strategies are designed
to drive high performance in both factual and counterfactual test environments.
Employing a specialized form of teacher-student distillation from a biased
model, BPL retains accurate preference knowledge aligned with the collected
feedback, leading to high performance in the factual test. Furthermore, through
self-distillation with reliability filtering, BPL iteratively refines its
knowledge throughout the training process. This enables the model to produce
more accurate predictions across a broader range of user-item combinations,
thereby improving performance in the counterfactual test. Comprehensive
experiments validate the effectiveness of BPL in both factual and
counterfactual tests. Our implementation is accessible via:
https://github.com/SeongKu-Kang/BPL.

</details>


### [232] [Continual Knowledge Consolidation LORA for Domain Incremental Learning](https://arxiv.org/abs/2510.16077)
*Naeem Paeedeh,Mahardhika Pratama,Weiping Ding,Jimmy Cao,Wolfgang Mayer,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: CONEC-LoRA提出了一种解决领域增量学习中灾难性遗忘问题的方法，通过整合任务共享和任务特定的LoRA模块，结合随机分类器和辅助网络，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决领域增量学习中现有方法忽视共享知识、分类器泛化能力不足的问题。

Method: 结合任务共享和任务特定的LoRA模块，引入随机分类器和辅助网络，利用不同深度网络结构。

Result: 在4个基准问题上性能优于现有方法，提升超过5%。

Conclusion: CONEC-LoRA通过知识整合和优化分类器设计，有效解决了领域增量学习中的关键问题。

Abstract: Domain Incremental Learning (DIL) is a continual learning sub-branch that
aims to address never-ending arrivals of new domains without catastrophic
forgetting problems. Despite the advent of parameter-efficient fine-tuning
(PEFT) approaches, existing works create task-specific LoRAs overlooking shared
knowledge across tasks. Inaccurate selection of task-specific LORAs during
inference results in significant drops in accuracy, while existing works rely
on linear or prototype-based classifiers, which have suboptimal generalization
powers. Our paper proposes continual knowledge consolidation low rank
adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed
from consolidations between task-shared LORA to extract common knowledge and
task-specific LORA to embrace domain-specific knowledge. Unlike existing
approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose
parameters are sampled from a distribution, thus enhancing the likelihood of
correct classifications. Last but not least, an auxiliary network is deployed
to optimally predict the task-specific LoRAs for inferences and implements the
concept of a different-depth network structure in which every layer is
connected with a local classifier to take advantage of intermediate
representations. This module integrates the ball-generator loss and
transformation module to address the synthetic sample bias problem. Our
rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in
4 popular benchmark problems with over 5% margins.

</details>


### [233] [PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites](https://arxiv.org/abs/2510.16083)
*Jaehan Kim,Minkyoo Song,Minjae Seo,Youngjin Jin,Seungwon Shin,Jinwoo Kim*

Main category: cs.LG

TL;DR: PassREfinder-FL是一个基于图神经网络和联邦学习的框架，用于预测跨网站的密码重用风险，保护用户隐私并提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在检测密码重用攻击时对用户体验的负面影响和隐私泄露问题。

Method: 通过构建网站图表示密码重用关系，利用图神经网络进行链接预测，并结合联邦学习保护用户隐私。

Result: 在真实数据集上达到0.9153的F1分数，性能优于其他先进模型。

Conclusion: PassREfinder-FL能有效量化密码重用风险，并提供可操作的风险评分。

Abstract: Credential stuffing attacks have caused significant harm to online users who
frequently reuse passwords across multiple websites. While prior research has
attempted to detect users with reused passwords or identify malicious login
attempts, existing methods often compromise usability by restricting password
creation or website access, and their reliance on complex account-sharing
mechanisms hinders real-world deployment. To address these limitations, we
propose PassREfinder-FL, a novel framework that predicts credential stuffing
risks across websites. We introduce the concept of password reuse relations --
defined as the likelihood of users reusing passwords between websites -- and
represent them as edges in a website graph. Using graph neural networks (GNNs),
we perform a link prediction task to assess credential reuse risk between
sites. Our approach scales to a large number of arbitrary websites by
incorporating public website information and linking newly observed websites as
nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a
federated learning (FL) approach that eliminates the need to share user
sensitive information across administrators. Evaluation on a real-world dataset
of 360 million breached accounts from 22,378 websites shows that
PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further
validate that our FL-based GNN achieves a 4-11% performance improvement over
other state-of-the-art GNN models through an ablation study. Finally, we
demonstrate that the predicted results can be used to quantify password reuse
likelihood as actionable risk scores.

</details>


### [234] [Near-Equilibrium Propagation training in nonlinear wave systems](https://arxiv.org/abs/2510.16084)
*Karol Sajnok,Michał Matuszewski*

Main category: cs.LG

TL;DR: 扩展平衡传播（EP）学习算法到离散和连续复值波系统，适用于弱耗散区域，并在多种物理系统中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播算法在物理神经网络中难以实现的问题，探索一种适用于广泛物理系统的替代学习方法。

Method: 将EP学习扩展到离散和连续复值波系统，提出一种适用于弱耗散区域的方案，并在激子-极化子凝聚体中进行数值验证。

Result: 在标准基准测试中（如逻辑任务和手写数字识别）表现出稳定收敛，证明其在物理系统中的实用性。

Conclusion: 为物理系统中仅能控制局部参数的情况提供了一种实用的原位学习方法。

Abstract: Backpropagation learning algorithm, the workhorse of modern artificial
intelligence, is notoriously difficult to implement in physical neural
networks. Equilibrium Propagation (EP) is an alternative with comparable
efficiency and strong potential for in-situ training. We extend EP learning to
both discrete and continuous complex-valued wave systems. In contrast to
previous EP implementations, our scheme is valid in the weakly dissipative
regime, and readily applicable to a wide range of physical settings, even
without well defined nodes, where trainable inter-node connections can be
replaced by trainable local potential. We test the method in driven-dissipative
exciton-polariton condensates governed by generalized Gross-Pitaevskii
dynamics. Numerical studies on standard benchmarks, including a simple logical
task and handwritten-digit recognition, demonstrate stable convergence,
establishing a practical route to in-situ learning in physical systems in which
system control is restricted to local parameters.

</details>


### [235] [An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning](https://arxiv.org/abs/2510.17564)
*Lindsay Spoor,Álvaro Serra-Gómez,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 论文分析了拉格朗日乘子在安全强化学习中的最优性和稳定性，发现自动更新乘子能恢复甚至超过最优性能，但存在振荡问题，需进一步研究稳定方法。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，拉格朗日方法常用于平衡性能与约束，但乘子选择对效果至关重要，缺乏实证研究。

Method: 通过分析不同任务中拉格朗日乘子的最优性和稳定性，提供λ-剖面图可视化性能与约束的权衡。

Result: 自动更新乘子能恢复或超过最优性能，但存在振荡；PID控制可缓解振荡，但需精细调参。

Conclusion: 拉格朗日方法在安全强化学习中需进一步研究稳定性，自动更新乘子虽有潜力但需改进。

Abstract: In safety-critical domains such as robotics, navigation and power systems,
constrained optimization problems arise where maximizing performance must be
carefully balanced with associated constraints. Safe reinforcement learning
provides a framework to address these challenges, with Lagrangian methods being
a popular choice. However, the effectiveness of Lagrangian methods crucially
depends on the choice of the Lagrange multiplier $\lambda$, which governs the
trade-off between return and constraint cost. A common approach is to update
the multiplier automatically during training. Although this is standard in
practice, there remains limited empirical evidence on the robustness of an
automated update and its influence on overall performance. Therefore, we
analyze (i) optimality and (ii) stability of Lagrange multipliers in safe
reinforcement learning across a range of tasks. We provide $\lambda$-profiles
that give a complete visualization of the trade-off between return and
constraint cost of the optimization problem. These profiles show the highly
sensitive nature of $\lambda$ and moreover confirm the lack of general
intuition for choosing the optimal value $\lambda^*$. Our findings additionally
show that automated multiplier updates are able to recover and sometimes even
exceed the optimal performance found at $\lambda^*$ due to the vast difference
in their learning trajectories. Furthermore, we show that automated multiplier
updates exhibit oscillatory behavior during training, which can be mitigated
through PID-controlled updates. However, this method requires careful tuning to
achieve consistently better performance across tasks. This highlights the need
for further research on stabilizing Lagrangian methods in safe reinforcement
learning. The code used to reproduce our results can be found at
https://github.com/lindsayspoor/Lagrangian_SafeRL.

</details>


### [236] [FSRF: Factorization-guided Semantic Recovery for Incomplete Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.16086)
*Ziyang Liu,Pengjunfei Chu,Shuming Dong,Chen Zhang,Mingcheng Li,Jin Wang*

Main category: cs.LG

TL;DR: 提出了一种名为FSRF的框架，用于解决多模态情感分析中的模态缺失问题，通过分解和自蒸馏模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据常因遮挡、隐私或设备故障而缺失，现有方法对此问题处理不足，导致泛化性低。

Method: 设计了去冗余的同质-异质分解模块和分布对齐的自蒸馏模块，分别用于模态表示学习和缺失语义恢复。

Result: 在两个数据集上的实验表明，FSRF在模态缺失情况下显著优于现有方法。

Conclusion: FSRF有效解决了模态缺失问题，提升了多模态情感分析的鲁棒性和性能。

Abstract: In recent years, Multimodal Sentiment Analysis (MSA) has become a research
hotspot that aims to utilize multimodal data for human sentiment understanding.
Previous MSA studies have mainly focused on performing interaction and fusion
on complete multimodal data, ignoring the problem of missing modalities in
real-world applications due to occlusion, personal privacy constraints, and
device malfunctions, resulting in low generalizability.
  To this end, we propose a Factorization-guided Semantic Recovery Framework
(FSRF) to mitigate the modality missing problem in the MSA task.
  Specifically, we propose a de-redundant homo-heterogeneous factorization
module that factorizes modality into modality-homogeneous,
modality-heterogeneous, and noisy representations and design elaborate
constraint paradigms for representation learning.
  Furthermore, we design a distribution-aligned self-distillation module that
fully recovers the missing semantics by utilizing bidirectional knowledge
transfer.
  Comprehensive experiments on two datasets indicate that FSRF has a
significant performance advantage over previous methods with uncertain missing
modalities.

</details>


### [237] [STABLE: Gated Continual Learning for Large Language Models](https://arxiv.org/abs/2510.16089)
*William Hoy,Nurcin Celik*

Main category: cs.LG

TL;DR: STABLE框架通过参数高效微调（LoRA）和门控机制，在连续更新中减少遗忘，同时保持模型适应性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLM）在连续更新中的灾难性遗忘问题，确保新知识整合时不损害已有知识。

Method: 使用LoRA进行参数高效微调，并通过三种指标（EM、bits增加、KL散度）评估编辑候选，门控机制决定是否接受或调整更新。

Result: 在Qwen-2.5-7B模型上，门控机制有效减少遗忘，EM门控在短序列中表现最佳。

Conclusion: STABLE提供了一种原则性的连续编辑方法，平衡新知识整合与模型可靠性。

Abstract: Large language models (LLMs) increasingly require mechanisms for continual
adaptation without full retraining. However, sequential updates can lead to
catastrophic forgetting, where new edits degrade previously acquired knowledge.
This work presents STABLE, a gated continual self editing framework that
constrains forgetting during sequential updates using parameter efficient fine
tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate
edit is evaluated against a stability budget using one of three metrics: (i)
Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase,
reflecting reduced model confidence; and (iii) KL divergence, quantifying
distributional drift between the base and adapted models. If a threshold is
exceeded, the LoRA update is rescaled through a clipping procedure or rejected.
Experiments on the Qwen-2.5-7B model show that gating effectively mitigates
forgetting while preserving adaptability. EM based gating achieved the highest
cumulative performance in short continual learning sequences. Our results show
that different gating strategies can achieve comparable distribution shift
(measured by KL divergence) while producing different accuracy outcomes,
highlighting the importance of gating design in continual adaptation. This
approach offers a principled method for continual model editing, enabling LLMs
to integrate new knowledge while maintaining reliability. Code:
https://github.com/Bhoy1/STABLE

</details>


### [238] [Compressing Many-Shots in In-Context Learning](https://arxiv.org/abs/2510.16092)
*Devvrit Khatri,Pranamya Kulkarni,Nilesh Gupta,Yerram Varun,Liqian Peng,Jay Yagnik,Praneeth Netrapalli,Cho-Jui Hsieh,Alec Go,Inderjit S Dhillon,Aditya Kusupati,Prateek Jain*

Main category: cs.LG

TL;DR: 论文提出了一种名为MemComp的层间压缩方法，用于提升多示例提示（many-shot prompts）的内存和计算效率，并在高压缩比下保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 多示例提示（ICL）虽然能提升任务性能，但会增加内存和计算成本，因此需要高效的压缩方法。

Method: 提出MemComp方法，通过层间压缩多示例表示，并使用更强的压缩模型。

Result: MemComp在高压缩比（3x-8x）下优于基线方法，性能下降小于10%，而基线方法下降20-30%。

Conclusion: MemComp是一种高效的多示例提示压缩方法，适用于不同模型和任务。

Abstract: Large Language Models (LLMs) have been shown to be able to learn different
tasks without explicit finetuning when given many input-output examples /
demonstrations through In-Context Learning (ICL). Increasing the number of
examples, called ``shots'', improves downstream task performance but incurs
higher memory and computational costs. In this work, we study an approach to
improve the memory and computational efficiency of ICL inference by compressing
the many-shot prompts. Given many shots comprising t tokens, our goal is to
generate a m soft-token summary, where m < t. We first show that existing
prompt compression methods are ineffective for many-shot compression, and
simply using fewer shots as a baseline is surprisingly strong. To achieve
effective compression, we find that: (a) a stronger compressor model with more
trainable parameters is necessary, and (b) compressing many-shot
representations at each transformer layer enables more fine-grained compression
by providing each layer with its own compressed representation. Based on these
insights, we propose MemCom, a layer-wise compression method. We systematically
evaluate various compressor models and training approaches across different
model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence
lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms
strong baselines across all compression ratios on multiple classification tasks
with large label sets. Notably, while baseline performance degrades sharply at
higher compression ratios, often by over 20-30%, MemCom maintains high accuracy
with minimal degradation, typically dropping by less than 10%.

</details>


### [239] [Narrowing Action Choices with AI Improves Human Sequential Decisions](https://arxiv.org/abs/2510.16097)
*Eleni Straitouri,Stratis Tsirtsis,Ander Artola Velasco,Manuel Gomez-Rodriguez*

Main category: cs.LG

TL;DR: 论文提出了一种决策支持系统，通过预训练的AI代理缩小人类行动选择范围，实现互补性，并在实验中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索在序列决策任务中，通过控制人类代理水平实现互补性的可能性。

Method: 开发决策支持系统，结合AI代理缩小人类行动选择范围，并引入带算法优化代理水平。

Result: 实验显示，使用系统的参与者表现优于单独行动或仅依赖AI代理。

Conclusion: 系统有效提升决策性能，数据与实现已开源。

Abstract: Recent work has shown that, in classification tasks, it is possible to design
decision support systems that do not require human experts to understand when
to cede agency to a classifier or when to exercise their own agency to achieve
complementarity$\unicode{x2014}$experts using these systems make more accurate
predictions than those made by the experts or the classifier alone. The key
principle underpinning these systems reduces to adaptively controlling the
level of human agency, by design. Can we use the same principle to achieve
complementarity in sequential decision making tasks? In this paper, we answer
this question affirmatively. We develop a decision support system that uses a
pre-trained AI agent to narrow down the set of actions a human can take to a
subset, and then asks the human to take an action from this action set. Along
the way, we also introduce a bandit algorithm that leverages the smoothness
properties of the action sets provided by our system to efficiently optimize
the level of human agency. To evaluate our decision support system, we conduct
a large-scale human subject study ($n = 1{,}600$) where participants play a
wildfire mitigation game. We find that participants who play the game supported
by our system outperform those who play on their own by $\sim$$30$% and the AI
agent used by our system by $>$$2$%, even though the AI agent largely
outperforms participants playing without support. We have made available the
data gathered in our human subject study as well as an open source
implementation of our system at
https://github.com/Networks-Learning/narrowing-action-choices .

</details>


### [240] [Zero-shot World Models via Search in Memory](https://arxiv.org/abs/2510.16123)
*Federico Malato,Ville Hautamäki*

Main category: cs.LG

TL;DR: 提出了一种基于相似性搜索和随机表示的无训练世界模型，与PlaNet对比，在潜在重建和长时预测上表现相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 探索无需训练的世界模型，提高样本效率，并与传统训练模型（如PlaNet）对比。

Method: 利用相似性搜索和随机表示近似世界模型，避免训练过程。

Result: 在潜在重建和长时预测任务中表现与PlaNet相当，长时预测表现更优。

Conclusion: 搜索式世界模型在无需训练的情况下，性能与传统训练模型相当，长时预测更优。

Abstract: World Models have vastly permeated the field of Reinforcement Learning. Their
ability to model the transition dynamics of an environment have greatly
improved sample efficiency in online RL. Among them, the most notorious example
is Dreamer, a model that learns to act in a diverse set of image-based
environments. In this paper, we leverage similarity search and stochastic
representations to approximate a world model without a training procedure. We
establish a comparison with PlaNet, a well-established world model of the
Dreamer family. We evaluate the models on the quality of latent reconstruction
and on the perceived similarity of the reconstructed image, on both next-step
and long horizon dynamics prediction. The results of our study demonstrate that
a search-based world model is comparable to a training based one in both cases.
Notably, our model show stronger performance in long-horizon prediction with
respect to the baseline on a range of visually different environments.

</details>


### [241] [A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies](https://arxiv.org/abs/2510.16132)
*Phalguni Nanda,Zaiwei Chen*

Main category: cs.LG

TL;DR: 首次对时变学习策略下的Q-learning算法进行了有限时间分析，证明了其收敛速率和样本复杂度，并揭示了其探索与利用的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究在时变学习策略（即同策略采样）下Q-learning算法的收敛性，仅需最小假设（如存在诱导不可约马尔可夫链的策略）。

Method: 利用泊松方程分解马尔可夫噪声，并通过敏感性分析控制残差项。

Result: 证明了Q-learning在同策略下的收敛速率和样本复杂度，数值实验验证了理论。

Conclusion: 时变策略下的Q-learning在探索上较弱但利用上有优势，方法可推广到其他强化学习算法。

Abstract: In this work, we present the first finite-time analysis of the Q-learning
algorithm under time-varying learning policies (i.e., on-policy sampling) with
minimal assumptions -- specifically, assuming only the existence of a policy
that induces an irreducible Markov chain over the state space. We establish a
last-iterate convergence rate for $\mathbb{E}[\|Q_k - Q^*\|_\infty^2]$,
implying a sample complexity of order $O(1/\epsilon^2)$ for achieving
$\mathbb{E}[\|Q_k - Q^*\|_\infty] \le \epsilon$, matching that of off-policy
Q-learning but with a worse dependence on exploration-related parameters. We
also derive an explicit rate for $\mathbb{E}[\|Q^{\pi_k} - Q^*\|_\infty^2]$,
where $\pi_k$ is the learning policy at iteration $k$. These results reveal
that on-policy Q-learning exhibits weaker exploration than its off-policy
counterpart but enjoys an exploitation advantage, as its policy converges to an
optimal one rather than remaining fixed. Numerical simulations corroborate our
theory.
  Technically, the combination of time-varying learning policies (which induce
rapidly time-inhomogeneous Markovian noise) and the minimal assumption on
exploration presents significant analytical challenges. To address these
challenges, we employ a refined approach that leverages the Poisson equation to
decompose the Markovian noise corresponding to the lazy transition matrix into
a martingale-difference term and residual terms. To control the residual terms
under time inhomogeneity, we perform a sensitivity analysis of the Poisson
equation solution with respect to both the Q-function estimate and the learning
policy. These tools may further facilitate the analysis of general
reinforcement learning algorithms with rapidly time-varying learning policies
-- such as single-timescale actor--critic methods and learning-in-games
algorithms -- and are of independent interest.

</details>


### [242] [Expert Merging in Sparse Mixture of Experts with Nash Bargaining](https://arxiv.org/abs/2510.16138)
*Dung V. Nguyen,Anh T. Nguyen,Minh H. Nguyen,Luc Q. Nguyen,Shiqi Jiang,Ethan Fetaya,Linh Duy Tran,Gal Chechik,Tan M. Nguyen*

Main category: cs.LG

TL;DR: NAMEx框架通过博弈论视角改进专家合并策略，引入纳什议价和复杂动量，提升效率和平衡性。


<details>
  <summary>Details</summary>
Motivation: 现有SMoE专家合并策略缺乏原则性权重机制，需更高效平衡的合作方法。

Method: 提出NAMEx框架，结合纳什议价和复杂动量，优化专家合并过程。

Result: 在语言建模、文本分类等任务中表现优异，适用于大规模系统。

Conclusion: NAMEx为专家合并提供了理论保障和实际效果，具有广泛适用性。

Abstract: Existing expert merging strategies for Sparse Mixture of Experts (SMoE)
typically rely on input-dependent or input-independent averaging of expert
parameters, but often lack a principled weighting mechanism. In this work, we
reinterpret expert merging through the lens of game theory, revealing
cooperative and competitive dynamics among experts. Based on this perspective,
we introduce Nash Merging of Experts (NAMEx), a novel framework that
incorporates Nash Bargaining into the merging process, enabling more balanced
and efficient collaboration among experts. Additionally, we incorporate complex
momentum into NAMEx to accelerate expert propagation with theoretical
guarantees for convergence. Extensive experiments across language modelling,
text classification, image classification, and zero-shot robustness under data
corruption show that NAMEx consistently outperforms competing methods while
integrating seamlessly with popular MoE architectures. Finally, we demonstrate
NAMEx's scalability by applying it to large-scale systems, including
Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both
zero-shot and fine-tuning settings.

</details>


### [243] [Zeroth-Order Sharpness-Aware Learning with Exponential Tilting](https://arxiv.org/abs/2510.16157)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 论文通过指数倾斜目标将零阶优化与SAM方法连接，提出了一种平滑过渡平均和最大损失的框架，并开发了新的零阶算法。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化方法关注扰动集的平均损失，而SAM方法关注邻域内的最大损失。论文旨在通过指数倾斜目标连接这两种方法，探索更有效的优化策略。

Method: 提出了一种基于指数倾斜目标的软SAM框架，开发了新的零阶算法，并分析了其尖锐性概念。

Result: 该方法在分类、多选QA和语言生成等任务中表现优于传统零阶基线，且可作为梯度无关、内存高效的SAM替代方案。

Conclusion: 论文通过连接零阶优化与SAM方法，提出了一种更通用的优化框架，并在多个任务中验证了其有效性。

Abstract: Classic zeroth-order optimization approaches typically optimize for a
smoothed version of the original function, i.e., the expected objective under
randomly perturbed model parameters. This can be interpreted as encouraging the
loss values in the perturbation set to be small on average. Popular
sharpness-aware minimization (SAM) objectives, however, typically focus on the
largest loss within the neighborhood to arrive at flat minima more effectively.
In this work, we connect zeroth-order optimization (and its corresponding
objectives) with SAM approaches explicitly, through an exponential tilting
objective that provides a smooth transition between the average- and the
max-loss formulations. We explore new zeroth-order algorithms to solve a soft
SAM objective parameterized by a tilting parameter $t$. We provide precise
characterizations of the sharpness notions of the tilted SAM framework.
Practically, our approach can be used as a gradient-free and memory-efficient
alternative to SAM variants, and it achieves better generalization compared to
vanilla zeroth-order baselines on a wide range of downstream tasks, including
classification, multiple choice QA, and language generation.

</details>


### [244] [Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction](https://arxiv.org/abs/2510.16161)
*Ankitkumar Joshi,Milos Hauskrecht*

Main category: cs.LG

TL;DR: GRUwE是一种基于GRU的改进模型，用于处理不规则采样的多变量时间序列，表现优于或与现有SOTA方法相当，且更简单高效。


<details>
  <summary>Details</summary>
Motivation: 现有复杂架构在不规则采样时间序列预测中的优势不明确，简单高效的RNN改进方法可能仍有竞争力。

Method: GRUwE通过两种重置机制（观测触发和时间触发的指数衰减）更新马尔可夫状态，支持连续时间预测。

Result: 在多个真实世界基准测试中，GRUwE在预测任务上表现优于或与SOTA方法相当。

Conclusion: GRUwE因其简单性、易实现性和低计算开销，在实际部署中具有显著优势。

Abstract: Modeling irregularly sampled multivariate time series is a persistent
challenge in domains like healthcare and sensor networks. While recent works
have explored a variety of complex learning architectures to solve the
prediction problems for irregularly sampled time series, it remains unclear
what are the true benefits of some of these architectures, and whether clever
modifications of simpler and more efficient RNN-based algorithms are still
competitive, i.e. they are on par with or even superior to these methods. In
this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential
basis functions, that builds upon RNN-based architectures for observations made
at irregular times. GRUwE supports both regression-based and event-based
predictions in continuous time. GRUwE works by maintaining a Markov state
representation of the time series that updates with the arrival of irregular
observations. The Markov state update relies on two reset mechanisms: (i)
observation-triggered reset, and (ii) time-triggered reset of the GRU state
using learnable exponential decays, to support the predictions in continuous
time. Our empirical evaluations across several real-world benchmarks on
next-observation and next-event prediction tasks demonstrate that GRUwE can
indeed achieve competitive to superior performance compared to the recent
state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers
compelling advantages: it is easy to implement, requires minimal
hyper-parameter tuning efforts, and significantly reduces the computational
overhead in the online deployment.

</details>


### [245] [AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures](https://arxiv.org/abs/2510.16165)
*Charles Rhys Campbell,Aldo H. Romero,Kamal Choudhary*

Main category: cs.LG

TL;DR: 论文对三种生成模型（AtomGPT、CDVAE、FlowMM）在材料数据集上的性能进行了系统比较，CDVAE表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在材料探索中应用广泛，但缺乏对其性能的严格比较评估。

Method: 使用KL散度和MAE评估模型在超导数据集上的表现。

Result: CDVAE表现最优，其次是AtomGPT和FlowMM。

Conclusion: 研究为生成模型在材料科学中的应用提供了基准，代码和配置将公开。

Abstract: Generative models have become significant assets in the exploration and
identification of new materials, enabling the rapid proposal of candidate
crystal structures that satisfy target properties. Despite the increasing
adoption of diverse architectures, a rigorous comparative evaluation of their
performance on materials datasets is lacking. In this work, we present a
systematic benchmark of three representative generative models- AtomGPT (a
transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE),
and FlowMM (a Riemannian flow matching model). These models were trained to
reconstruct crystal structures from subsets of two publicly available
superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria
database. Performance was assessed using the Kullback-Leibler (KL) divergence
between predicted and reference distributions of lattice parameters, as well as
the mean absolute error (MAE) of individual lattice constants. For the computed
KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and
then FlowMM. All benchmarking code and model configurations will be made
publicly available at https://github.com/atomgptlab/atombench_inverse.

</details>


### [246] [Alignment is Localized: A Causal Probe into Preference Layers](https://arxiv.org/abs/2510.16167)
*Archie Chaudhury*

Main category: cs.LG

TL;DR: 论文通过因果修补技术分析语言模型对齐的偏好优化，发现对齐过程集中在中间层，且是低秩、方向性的。


<details>
  <summary>Details</summary>
Motivation: 研究人类反馈强化学习（RLHF）如何实现语言模型对齐的内部机制，目前这一过程仍不透明。

Method: 在Llama-3.2-1B模型上应用层间因果修补技术，分析基模型与调优模型在人类偏好对中的差异。

Result: 对齐过程集中在中间层激活的特定子空间，且只有少数层与奖励增益相关。

Conclusion: 人类偏好调优的对齐过程是方向性、低秩的，而非扩散和参数化的。

Abstract: Reinforcement Learning frameworks, particularly those utilizing human
annotations, have become an increasingly popular method for preference
fine-tuning, where the outputs of a language model are tuned to match a certain
set of behavioral policies or guidelines. Reinforcement Learning through Human
Feedback (RLHF) is perhaps the most popular implementation of such a framework,
particularly for aligning LMs toward safety and human intent. However, the
internal workings of how such alignment is achieved remain largely opaque. In
this work, we systematically analyze preference optimization for language model
alignment by applying layer-wide causal patching between a base model and its
tuned counterpart across human preference pairs. We implement our methodology
on \textit{Llama-3.2-1B}, and find that alignment is spatially localized:
mid-layer activations encode a distinct subspace that causally determines
reward-consistent behavior, while early and late layers remain largely
unaffected. Utilizing LASSO regression, we also find that only a small number
of layers possess non-zero coefficients linking activation distances to reward
gains. Overall, we show that, at least for some language models, alignment from
human-based, preferential tuning is a directional, low rank process, rather
than diffuse and parameteric.

</details>


### [247] [Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness](https://arxiv.org/abs/2510.16171)
*Longwei Wang,Ifrat Ikhtear Uddin,KC Santosh,Chaowei Zhang,Xiao Qin,Yang Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种通过嵌入旋转和尺度等变卷积层的架构方法，提升对抗鲁棒性，无需对抗训练。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽为主流防御策略，但计算成本高且可能影响干净数据准确性。

Method: 提出两种对称感知架构：并行设计和级联设计，利用等变卷积层编码对称先验。

Result: 理论证明降低假设空间复杂性，实验显示在CIFAR数据集上提升对抗鲁棒性和泛化能力。

Conclusion: 对称强制架构是数据增强防御的高效替代方案。

Abstract: Adversarial examples reveal critical vulnerabilities in deep neural networks
by exploiting their sensitivity to imperceptible input perturbations. While
adversarial training remains the predominant defense strategy, it often incurs
significant computational cost and may compromise clean-data accuracy. In this
work, we investigate an architectural approach to adversarial robustness by
embedding group-equivariant convolutions-specifically, rotation- and
scale-equivariant layers-into standard convolutional neural networks (CNNs).
These layers encode symmetry priors that align model behavior with structured
transformations in the input space, promoting smoother decision boundaries and
greater resilience to adversarial attacks. We propose and evaluate two
symmetry-aware architectures: a parallel design that processes standard and
equivariant features independently before fusion, and a cascaded design that
applies equivariant operations sequentially. Theoretically, we demonstrate that
such models reduce hypothesis space complexity, regularize gradients, and yield
tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme
Value for nEtwork Robustness) framework. Empirically, our models consistently
improve adversarial robustness and generalization across CIFAR-10, CIFAR-100,
and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial
training. These findings underscore the potential of symmetry-enforcing
architectures as efficient and principled alternatives to data
augmentation-based defenses.

</details>


### [248] [The Formalism-Implementation Gap in Reinforcement Learning Research](https://arxiv.org/abs/2510.16175)
*Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 论文呼吁强化学习研究应更关注学习动态和理解，而非仅追求性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习研究过于注重性能表现，忽视了学习动态和理解，可能导致技术难以迁移到新问题。

Method: 以Arcade Learning Environment为例，探讨如何利用饱和的基准促进理解和实际应用。

Result: 提出应平衡性能研究与科学理解，并更精确地映射基准与数学形式。

Conclusion: 强化学习研究需转向更科学的方向，提升理解并促进实际应用。

Abstract: The last decade has seen an upswing in interest and adoption of reinforcement
learning (RL) techniques, in large part due to its demonstrated capabilities at
performing certain tasks at "super-human levels". This has incentivized the
community to prioritize research that demonstrates RL agent performance, often
at the expense of research aimed at understanding their learning dynamics.
Performance-focused research runs the risk of overfitting on academic
benchmarks -- thereby rendering them less useful -- which can make it difficult
to transfer proposed techniques to novel problems. Further, it implicitly
diminishes work that does not push the performance-frontier, but aims at
improving our understanding of these techniques. This paper argues two points:
(i) RL research should stop focusing solely on demonstrating agent
capabilities, and focus more on advancing the science and understanding of
reinforcement learning; and (ii) we need to be more precise on how our
benchmarks map to the underlying mathematical formalisms. We use the popular
Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a
benchmark that, despite being increasingly considered "saturated", can be
effectively used for developing this understanding, and facilitating the
deployment of RL techniques in impactful real-world problems.

</details>


### [249] [Expressive Reward Synthesis with the Runtime Monitoring Language](https://arxiv.org/abs/2510.16185)
*Daniel Donnelly,Angelo Ferrando,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 提出了一种基于Runtime Monitoring Language (RML)的新型语言奖励机，解决了传统奖励机无法处理非正则、非马尔可夫任务的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习中奖励函数的模糊定义可能导致有害行为，传统奖励机表达能力有限，无法处理复杂任务。

Method: 利用RML的内置内存，开发了语言奖励机，支持非正则、非马尔可夫任务的奖励函数。

Result: 实验证明了该方法的表达能力，并在事件处理和任务规范上优于现有方法。

Conclusion: 语言奖励机扩展了奖励函数的表达能力，为复杂任务提供了更灵活的解决方案。

Abstract: A key challenge in reinforcement learning (RL) is reward (mis)specification,
whereby imprecisely defined reward functions can result in unintended, possibly
harmful, behaviours. Indeed, reward functions in RL are typically treated as
black-box mappings from state-action pairs to scalar values. While effective in
many settings, this approach provides no information about why rewards are
given, which can hinder learning and interpretability. Reward Machines address
this issue by representing reward functions as finite state automata, enabling
the specification of structured, non-Markovian reward functions. However, their
expressivity is typically bounded by regular languages, leaving them unable to
capture more complex behaviours such as counting or parametrised conditions. In
this work, we build on the Runtime Monitoring Language (RML) to develop a novel
class of language-based Reward Machines. By leveraging the built-in memory of
RML, our approach can specify reward functions for non-regular, non-Markovian
tasks. We demonstrate the expressiveness of our approach through experiments,
highlighting additional advantages in flexible event-handling and task
specification over existing Reward Machine-based methods.

</details>


### [250] [Human-Allied Relational Reinforcement Learning](https://arxiv.org/abs/2510.16188)
*Fateme Golivand Darvishvand,Hikaru Shindo,Sahil Sidheekh,Kristian Kersting,Sriraam Natarajan*

Main category: cs.LG

TL;DR: 提出了一种结合关系强化学习与对象中心表示的新框架，支持结构化与非结构化数据，并通过主动查询专家提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在处理结构化问题时缺乏灵活性，关系强化学习虽能解决部分问题，但对问题结构有强假设。

Method: 结合关系强化学习与对象中心表示，并引入主动查询专家机制以建模策略不确定性。

Result: 实证评估表明该方法在效果和效率上均表现优异。

Conclusion: 新框架有效解决了结构化与非结构化数据的强化学习问题，并通过专家交互提升了学习性能。

Abstract: Reinforcement learning (RL) has experienced a second wind in the past decade.
While incredibly successful in images and videos, these systems still operate
within the realm of propositional tasks ignoring the inherent structure that
exists in the problem. Consequently, relational extensions (RRL) have been
developed for such structured problems that allow for effective generalization
to arbitrary number of objects. However, they inherently make strong
assumptions about the problem structure. We introduce a novel framework that
combines RRL with object-centric representation to handle both structured and
unstructured data. We enhance learning by allowing the system to actively query
the human expert for guidance by explicitly modeling the uncertainty over the
policy. Our empirical evaluation demonstrates the effectiveness and efficiency
of our proposed approach.

</details>


### [251] [Explore-then-Commit for Nonstationary Linear Bandits with Latent Dynamics](https://arxiv.org/abs/2510.16208)
*Sunmook Choi,Yahya Sattar,Yassir Jedra,Maryam Fazel,Sarah Dean*

Main category: cs.LG

TL;DR: 研究非稳态多臂老虎机问题，提出探索-提交算法，实现$\tilde{\mathcal{O}}(T^{2/3})$遗憾。


<details>
  <summary>Details</summary>
Motivation: 解决动作和潜在状态对奖励的影响，平衡短期与长期奖励的冲突。

Method: 探索阶段使用随机动作估计线性动态参数，提交阶段优化动作序列。

Result: 算法在系统识别和动作序列优化方面表现优异。

Conclusion: 通过双线性奖励和NP难问题的子优化保证，实现了理论上的遗憾上界。

Abstract: We study a nonstationary bandit problem where rewards depend on both actions
and latent states, the latter governed by unknown linear dynamics. Crucially,
the state dynamics also depend on the actions, resulting in tension between
short-term and long-term rewards. We propose an explore-then-commit algorithm
for a finite horizon $T$. During the exploration phase, random Rademacher
actions enable estimation of the Markov parameters of the linear dynamics,
which characterize the action-reward relationship. In the commit phase, the
algorithm uses the estimated parameters to design an optimized action sequence
for long-term reward. Our proposed algorithm achieves
$\tilde{\mathcal{O}}(T^{2/3})$ regret. Our analysis handles two key challenges:
learning from temporally correlated rewards, and designing action sequences
with optimal long-term reward. We address the first challenge by providing
near-optimal sample complexity and error bounds for system identification using
bilinear rewards. We address the second challenge by proving an equivalence
with indefinite quadratic optimization over a hypercube, a known NP-hard
problem. We provide a sub-optimality guarantee for this problem, enabling our
regret upper bound. Lastly, we propose a semidefinite relaxation with
Goemans-Williamson rounding as a practical approach.

</details>


### [252] [Benchmarking noisy label detection methods](https://arxiv.org/abs/2510.16211)
*Henrique Pickler,Jorge K. S. Kamassury,Danilo Silva*

Main category: cs.LG

TL;DR: 论文提出了一种系统评估标签噪声检测方法的新框架，通过分解为三个核心组件，并在多种数据集上验证了最佳组合。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中标签噪声普遍存在，影响模型训练和评估，但现有方法缺乏统一评估标准。

Method: 将检测方法分解为标签一致性函数、聚合方法和信息收集方式，提出统一基准任务和新评估指标。

Result: 在多种数据集和噪声条件下，基于样本内信息、平均概率聚合和logit margin的方法表现最佳。

Conclusion: 研究为设计新方法和选择现有技术提供了实用指导。

Abstract: Label noise is a common problem in real-world datasets, affecting both model
training and validation. Clean data are essential for achieving strong
performance and ensuring reliable evaluation. While various techniques have
been proposed to detect noisy labels, there is no clear consensus on optimal
approaches. We perform a comprehensive benchmark of detection methods by
decomposing them into three fundamental components: label agreement function,
aggregation method, and information gathering approach (in-sample vs
out-of-sample). This decomposition can be applied to many existing detection
methods, and enables systematic comparison across diverse approaches. To fairly
compare methods, we propose a unified benchmark task, detecting a fraction of
training samples equal to the dataset's noise rate. We also introduce a novel
metric: the false negative rate at this fixed operating point. Our evaluation
spans vision and tabular datasets under both synthetic and real-world noise
conditions. We identify that in-sample information gathering using average
probability aggregation combined with the logit margin as the label agreement
function achieves the best results across most scenarios. Our findings provide
practical guidance for designing new detection methods and selecting techniques
for specific applications.

</details>


### [253] [Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal](https://arxiv.org/abs/2510.16233)
*Patricia West,Michelle WL Wan,Alexander Hepburn,Edwin Simpson,Raul Santos-Rodriguez,Jeffrey N Clark*

Main category: cs.LG

TL;DR: 研究利用机器学习分析欧洲绿色协议中的气候政策进展，ClimateBERT在文本特征上表现最佳，BERT结合元数据特征效果更好。


<details>
  <summary>Details</summary>
Motivation: 气候变化需要有效的立法行动，研究旨在通过机器学习理解气候政策从宣布到采纳的进展。

Method: 使用165项政策的文本和元数据，比较TF-IDF、BERT和ClimateBERT等文本表示方法，结合元数据评估预测性能。

Result: ClimateBERT在纯文本特征上表现最佳（RMSE=0.17，R²=0.29），BERT结合元数据特征效果更好（RMSE=0.16，R²=0.38）。

Conclusion: 机器学习工具在支持气候政策分析和决策中具有潜力，可解释AI方法揭示了政策措辞和元数据的影响。

Abstract: Climate change demands effective legislative action to mitigate its impacts.
This study explores the application of machine learning (ML) to understand the
progression of climate policy from announcement to adoption, focusing on
policies within the European Green Deal. We present a dataset of 165 policies,
incorporating text and metadata. We aim to predict a policy's progression
status, and compare text representation methods, including TF-IDF, BERT, and
ClimateBERT. Metadata features are included to evaluate the impact on
predictive performance. On text features alone, ClimateBERT outperforms other
approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance
with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods
from explainable AI highlights the influence of factors such as policy wording
and metadata including political party and country representation. These
findings underscore the potential of ML tools in supporting climate policy
analysis and decision-making.

</details>


### [254] [One-Bit Quantization for Random Features Models](https://arxiv.org/abs/2510.16250)
*Danil Akhtiamov,Reza Ghane,Babak Hassibi*

Main category: cs.LG

TL;DR: 论文分析了神经网络中一比特权重压缩的理论基础，证明在随机特征模型中，除最后一层外量化权重不会损失泛化误差，并展示了实际加速效果。


<details>
  <summary>Details</summary>
Motivation: 解决一比特权重压缩在神经网络中理论基础不足的问题，探索其在资源受限设备上的应用潜力。

Method: 在随机特征模型中分析一比特量化，理论证明其泛化误差无损失，并通过实验验证实际加速效果。

Result: 理论证明除最后一层外量化权重无泛化误差损失，实验显示一比特量化显著提升推理速度。

Conclusion: 研究为一比特权重压缩提供了理论支持，并展示了其实际应用价值，为神经网络压缩领域贡献了新见解。

Abstract: Recent advances in neural networks have led to significant computational and
memory demands, spurring interest in one-bit weight compression to enable
efficient inference on resource-constrained devices. However, the theoretical
underpinnings of such compression remain poorly understood. We address this gap
by analyzing one-bit quantization in the Random Features model, a simplified
framework that corresponds to neural networks with random representations. We
prove that, asymptotically, quantizing weights of all layers except the last
incurs no loss in generalization error, compared to the full precision random
features model. Our findings offer theoretical insights into neural network
compression. We also demonstrate empirically that one-bit quantization leads to
significant inference speed ups for the Random Features models even on a laptop
GPU, confirming the practical benefits of our work. Additionally, we provide an
asymptotically precise characterization of the generalization error for Random
Features with an arbitrary number of layers. To the best of our knowledge, our
analysis yields more general results than all previous works in the related
literature.

</details>


### [255] [WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale](https://arxiv.org/abs/2510.16252)
*Yuxuan Lu,Jing Huang,Hui Liu,Jiri Gesi,Yan Han,Shihan Fu,Tianqi Zheng,Dakuo Wang*

Main category: cs.LG

TL;DR: WEBSERV是一个可扩展的RL环境，解决了现有浏览器交互环境中的噪声、非确定性和扩展性问题，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL环境在浏览器交互中存在噪声、非确定性或扩展性不足的问题，WEBSERV旨在提供一个更高效、可控的解决方案。

Method: WEBSERV包含紧凑的浏览器环境和可扩展的服务器端支持，通过高效启动和重置实现并行RL训练。

Result: 在WebArena任务中，WEBSERV实现了最佳成功率，同时显著降低了延迟和存储需求，支持200+并发容器。

Conclusion: WEBSERV为RL训练提供了一个高效、可扩展的环境，解决了现有工具的局限性。

Abstract: Training and evaluation of Reinforcement Learning (RL) web agents have gained
increasing attention, yet a scalable and efficient environment that couples
realistic and robust browser-side interaction with controllable server-side
state at scale is still missing. Existing environments tend to have one or more
of the following issues: they overwhelm policy models with excessive and noisy
context; they perform actions non-deterministically without waiting for the UI
or network to stabilize; or they cannot scale isolated client-server containers
effectively for parallel RL rollouts. We propose WEBSERV, an environment that
includes 1) a compact, site-agnostic browser environment that balances context
and action complexity, and 2) a scalable RL environment via efficient launching
and resetting web-servers to enable scalable RL training and evaluation. We
evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving
state-of-the-art single-prompt success rates while cutting launch latency by
~5x and storage need by ~240x, with a comparable memory footprint, enabling
200+ concurrent containers on a single host.

</details>


### [256] [Protein Folding with Neural Ordinary Differential Equations](https://arxiv.org/abs/2510.16253)
*Arielle Sanford,Shuo Sun,Christian B. Mendl*

Main category: cs.LG

TL;DR: 提出了一种基于神经ODE的连续深度Evoformer，替代传统48层离散块，降低计算成本并保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统Evoformer的深度和离散层导致高计算成本和刚性结构，希望通过连续深度模型解决这些问题。

Method: 用神经ODE参数化替代48个离散块，利用伴随方法实现恒定内存成本，并通过自适应ODE求解器平衡运行时间和精度。

Result: 模型在蛋白质结构预测中生成合理预测，捕捉二级结构（如α螺旋），但精度略低于原模型，资源消耗显著降低。

Conclusion: 连续深度模型为生物分子建模提供了轻量化和可解释的替代方案，为高效蛋白质结构预测开辟了新方向。

Abstract: Recent advances in protein structure prediction, such as AlphaFold, have
demonstrated the power of deep neural architectures like the Evoformer for
capturing complex spatial and evolutionary constraints on protein conformation.
However, the depth of the Evoformer, comprising 48 stacked blocks, introduces
high computational costs and rigid layerwise discretization. Inspired by Neural
Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth
formulation of the Evoformer, replacing its 48 discrete blocks with a Neural
ODE parameterization that preserves its core attention-based operations. This
continuous-time Evoformer achieves constant memory cost (in depth) via the
adjoint method, while allowing a principled trade-off between runtime and
accuracy through adaptive ODE solvers. Benchmarking on protein structure
prediction tasks, we find that the Neural ODE-based Evoformer produces
structurally plausible predictions and reliably captures certain secondary
structure elements, such as alpha-helices, though it does not fully replicate
the accuracy of the original architecture. However, our model achieves this
performance using dramatically fewer resources, just 17.5 hours of training on
a single GPU, highlighting the promise of continuous-depth models as a
lightweight and interpretable alternative for biomolecular modeling. This work
opens new directions for efficient and adaptive protein structure prediction
frameworks.

</details>


### [257] [Disentangling Hyperedges through the Lens of Category Theory](https://arxiv.org/abs/2510.16289)
*Yoonho Lee,Junseok Lee,Sangwoo Seo,Sungwon Kim,Yeongmin Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: 论文提出了一种基于范畴论视角的超边解缠新准则，并在基因通路中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索超图结构数据中的解缠表示学习，以利用隐藏的超边语义（如节点间未标注的关系）。

Method: 从范畴论的自然性条件出发，提出新的解缠准则，并构建概念验证模型。

Result: 模型成功捕捉了基因通路中基因（节点）的功能关系。

Conclusion: 提出的解缠准则在超图神经网络中具有潜力，能够挖掘隐藏的语义信息。

Abstract: Despite the promising results of disentangled representation learning in
discovering latent patterns in graph-structured data, few studies have explored
disentanglement for hypergraph-structured data. Integrating hyperedge
disentanglement into hypergraph neural networks enables models to leverage
hidden hyperedge semantics, such as unannotated relations between nodes, that
are associated with labels. This paper presents an analysis of hyperedge
disentanglement from a category-theoretical perspective and proposes a novel
criterion for disentanglement derived from the naturality condition. Our
proof-of-concept model experimentally showed the potential of the proposed
criterion by successfully capturing functional relations of genes (nodes) in
genetic pathways (hyperedges).

</details>


### [258] [QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models](https://arxiv.org/abs/2510.16292)
*Yutong Wang,Haiyu Wang,Sai Qian Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合SVD和量化的方法，显著降低VLMs的计算开销和内存占用，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: VLMs的高计算成本限制了其扩展性和实时应用，需要更高效的解决方案。

Method: 利用SVD减少KV缓存大小，动态调整SVD秩，并结合量化技术优化权重和激活。

Result: 在降低硬件成本的同时，精度提升超过10%，优于单独使用量化或SVD的方法。

Conclusion: 该方法适合资源受限设备的实时部署，代码已开源。

Abstract: Vision-Language Models (VLMs) are integral to tasks such as image captioning
and visual question answering, but their high computational cost, driven by
large memory footprints and processing time, limits their scalability and
real-time applicability. In this work, we propose leveraging Singular-Value
Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight
matrices to reduce KV cache size and computational overhead. We in addition
introduce an efficient rank allocation strategy that dynamically adjusts the
SVD rank based on its impact on VLM accuracy, achieving a significant reduction
in both memory usage and computational cost. Finally, we extend this approach
by applying quantization to both VLM weights and activations, resulting in a
highly efficient VLM. Our method outperforms previous approaches that rely
solely on quantization or SVD by achieving more than $10\%$ accuracy
improvement while consuming less hardware cost, making it better for real-time
deployment on resource-constrained devices. We open source our code at
\href{https://github.com/SAI-Lab-NYU/QSVD}{\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.

</details>


### [259] [Scaffold-Aware Generative Augmentation and Reranking for Enhanced Virtual Screening](https://arxiv.org/abs/2510.16306)
*Xin Wang,Yu Wang,Yunchao Liu,Jens Meiler,Tyler Derr*

Main category: cs.LG

TL;DR: ScaffAug是一个基于配体的虚拟筛选框架，通过生成合成数据、自训练和重排序模块解决类别和结构不平衡问题，提升药物发现效率。


<details>
  <summary>Details</summary>
Motivation: 虚拟筛选面临类别不平衡、结构不平衡和需要识别结构多样的活性化合物三大挑战。

Method: ScaffAug包含三个模块：生成合成数据的增强模块、模型无关的自训练模块和提升支架多样性的重排序模块。

Result: 在五个靶标类别上的实验表明，ScaffAug在性能指标和支架多样性上优于基线方法。

Conclusion: ScaffAug通过生成增强和支架感知，为虚拟筛选提供了新的有效视角。

Abstract: Ligand-based virtual screening (VS) is an essential step in drug discovery
that evaluates large chemical libraries to identify compounds that potentially
bind to a therapeutic target. However, VS faces three major challenges: class
imbalance due to the low active rate, structural imbalance among active
molecules where certain scaffolds dominate, and the need to identify
structurally diverse active compounds for novel drug development. We introduce
ScaffAug, a scaffold-aware VS framework that addresses these challenges through
three modules. The augmentation module first generates synthetic data
conditioned on scaffolds of actual hits using generative AI, specifically a
graph diffusion model. This helps mitigate the class imbalance and furthermore
the structural imbalance, due to our proposed scaffold-aware sampling
algorithm, designed to produce more samples for active molecules with
underrepresented scaffolds. A model-agnostic self-training module is then used
to safely integrate the generated synthetic data from our augmentation module
with the original labeled data. Lastly, we introduce a reranking module that
improves VS by enhancing scaffold diversity in the top recommended set of
molecules, while still maintaining and even enhancing the overall general
performance of identifying novel, active compounds. We conduct comprehensive
computational experiments across five target classes, comparing ScaffAug
against existing baseline methods by reporting the performance of multiple
evaluation metrics and performing ablation studies on ScaffAug. Overall, this
work introduces novel perspectives on effectively enhancing VS by leveraging
generative augmentations, reranking, and general scaffold-awareness.

</details>


### [260] [Toward General Digraph Contrastive Learning: A Dual Spatial Perspective](https://arxiv.org/abs/2510.16311)
*Daohan Su,Yang Zhang,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: S2-DiGCL是一种新颖的有向图对比学习框架，通过复杂域和实域视角提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法主要关注无向图，忽略了有向图中关键的方向信息。

Method: S2-DiGCL结合磁拉普拉斯矩阵的个性化扰动和路径子图增强策略，构建高质量样本。

Result: 在7个真实数据集上，节点分类和链接预测性能分别提升4.41%和4.34%。

Conclusion: S2-DiGCL通过多视角空间建模，显著提升了有向图对比学习的效果。

Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful tool for
extracting consistent representations from graphs, independent of labeled
information. However, existing methods predominantly focus on undirected
graphs, disregarding the pivotal directional information that is fundamental
and indispensable in real-world networks (e.g., social networks and
recommendations).In this paper, we introduce S2-DiGCL, a novel framework that
emphasizes spatial insights from complex and real domain perspectives for
directed graph (digraph) contrastive learning. From the complex-domain
perspective, S2-DiGCL introduces personalized perturbations into the magnetic
Laplacian to adaptively modulate edge phases and directional semantics. From
the real-domain perspective, it employs a path-based subgraph augmentation
strategy to capture fine-grained local asymmetries and topological
dependencies. By jointly leveraging these two complementary spatial views,
S2-DiGCL constructs high-quality positive and negative samples, leading to more
general and robust digraph contrastive learning. Extensive experiments on 7
real-world digraph datasets demonstrate the superiority of our approach,
achieving SOTA performance with 4.41% improvement in node classification and
4.34% in link prediction under both supervised and unsupervised settings.

</details>


### [261] [Memorizing Long-tail Data Can Help Generalization Through Composition](https://arxiv.org/abs/2510.16322)
*Mo Zhou,Haoyang Ma,Rong Ge*

Main category: cs.LG

TL;DR: 论文探讨了深度学习中记忆化与简单组合的协同作用，证明记忆化与组合能力有助于模型对罕见测试样本的正确预测。


<details>
  <summary>Details</summary>
Motivation: 研究记忆化与组合能力在深度学习中的关系，特别是在处理长尾数据时的作用。

Method: 通过理论分析线性模型，并在神经网络架构上进行实验验证。

Result: 记忆化与组合能力能帮助模型从未见过的长尾特征组合中做出正确预测，且组合能力受模型架构影响。

Conclusion: 记忆化与组合能力的结合在深度学习中具有潜力，尤其是在处理罕见数据时。

Abstract: Deep learning has led researchers to rethink the relationship between
memorization and generalization. In many settings, memorization does not hurt
generalization due to implicit regularization and may help by memorizing
long-tailed examples. In this paper, we consider the synergy between
memorization and simple composition -- the ability to make correct prediction
on a combination of long-tailed features. Theoretically, we show that for a
linear setting, memorization together with composition can help the model make
correct predictions on rare test examples that require a combination of
long-tailed features, even if such combinations were never observed in the
training data. Experiments on neural network architecture on simple data show
that the theoretical insight extends beyond the linear setting, and we further
observe that the composition capability of the model depends on its
architecture.

</details>


### [262] [MGTS-Net: Exploring Graph-Enhanced Multimodal Fusion for Augmented Time Series Forecasting](https://arxiv.org/abs/2510.16350)
*Shule Hao,Junpeng Bao,Wenli Li*

Main category: cs.LG

TL;DR: MGTS-Net是一种多模态图增强网络，用于时间序列预测，解决了细粒度时间模式提取不足、多模态信息融合不佳和动态多尺度特征适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间序列预测中面临细粒度时间模式提取不足、多模态信息融合不佳和动态多尺度特征适应性差的挑战。

Method: MGTS-Net包含三个核心组件：多模态特征提取层（MFE）、多模态特征融合层（MFF）和多尺度预测层（MSP）。

Result: 实验表明，MGTS-Net在轻量高效的同时表现出色，优于其他先进基线模型。

Conclusion: MGTS-Net通过优化特征提取、融合和预测层，显著提升了时间序列预测的准确性。

Abstract: Recent research in time series forecasting has explored integrating
multimodal features into models to improve accuracy. However, the accuracy of
such methods is constrained by three key challenges: inadequate extraction of
fine-grained temporal patterns, suboptimal integration of multimodal
information, and limited adaptability to dynamic multi-scale features. To
address these problems, we propose MGTS-Net, a Multimodal Graph-enhanced
Network for Time Series forecasting. The model consists of three core
components: (1) a Multimodal Feature Extraction layer (MFE), which optimizes
feature encoders according to the characteristics of temporal, visual, and
textual modalities to extract temporal features of fine-grained patterns; (2) a
Multimodal Feature Fusion layer (MFF), which constructs a heterogeneous graph
to model intra-modal temporal dependencies and cross-modal alignment
relationships and dynamically aggregates multimodal knowledge; (3) a
Multi-Scale Prediction layer (MSP), which adapts to multi-scale features by
dynamically weighting and fusing the outputs of short-term, medium-term, and
long-term predictors. Extensive experiments demonstrate that MGTS-Net exhibits
excellent performance with light weight and high efficiency. Compared with
other state-of-the-art baseline models, our method achieves superior
performance, validating the superiority of the proposed methodology.

</details>


### [263] [Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior](https://arxiv.org/abs/2510.16356)
*Fuqun Han,Stanley Osher,Wuchen Li*

Main category: cs.LG

TL;DR: 提出了一种稀疏Transformer架构，通过引入数据分布的先验信息，优化了模型的凸性和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 通过最优传输问题（正则化Wasserstein近端算子）的启发，设计了一种更高效的Transformer结构。

Method: 结合最优传输问题的闭式解，构建稀疏Transformer，提升优化问题的凸性和样本稀疏性。

Result: 在生成建模和贝叶斯逆问题中，稀疏Transformer比传统神经ODE方法更准确且收敛更快。

Conclusion: 稀疏Transformer在理论和实验中均表现出优越性，适用于复杂数据分布建模。

Abstract: In this work, we propose a sparse transformer architecture that incorporates
prior information about the underlying data distribution directly into the
transformer structure of the neural network. The design of the model is
motivated by a special optimal transport problem, namely the regularized
Wasserstein proximal operator, which admits a closed-form solution and turns
out to be a special representation of transformer architectures. Compared with
classical flow-based models, the proposed approach improves the convexity
properties of the optimization problem and promotes sparsity in the generated
samples. Through both theoretical analysis and numerical experiments, including
applications in generative modeling and Bayesian inverse problems, we
demonstrate that the sparse transformer achieves higher accuracy and faster
convergence to the target distribution than classical neural ODE-based methods.

</details>


### [264] [Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures](https://arxiv.org/abs/2510.16411)
*Minh-Khoi Nguyen-Nhat,Rachel S. Y. Teo,Laziz Abdullaev,Maurice Mok,Viet-Hoang Tran,Tan Minh Nguyen*

Main category: cs.LG

TL;DR: SymphonySMoE通过引入社交图增强稀疏专家混合模型的鲁棒性，适用于大规模系统。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏专家混合模型（SMoE）在数据分布变化时表现不佳，缺乏鲁棒性。

Method: 提出SymphonySMoE，通过社交图建模专家间交互，改进令牌路由过程。

Result: 实验证明SymphonySMoE在语言建模和视觉指令调优中优于基线模型。

Conclusion: SymphonySMoE轻量、模块化，适用于大规模模型（如42亿和74亿参数）。

Abstract: Sparse Mixture of Experts (SMoE) has emerged as a promising solution to
achieving unparalleled scalability in deep learning by decoupling model
parameter count from computational cost. By activating only a small subset of
parameters per sample, SMoE enables significant growth in model capacity while
maintaining efficiency. However, SMoE struggles to adapt to distributional
shifts, leading to reduced robustness under data contamination. In this work,
we introduce SymphonySMoE, a novel family of SMoE that introduces a social
graph to model interactions among experts. This graph-based structure enhances
the token routing process, addressing the robustness challenges that are
inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular,
and integrates seamlessly with existing SMoE-based models such as the XMoE and
the Generalist Language Model. We provide both theoretical analysis and
empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE.
Extensive experiments on language modeling and visual instruction tuning
validate our method's effectiveness. We further highlight the scalability of
SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its
applicability in fine-tuning tasks for large-scale systems.

</details>


### [265] [Colliding with Adversaries at ECML-PKDD 2025 Adversarial Attack Competition 1st Prize Solution](https://arxiv.org/abs/2510.16440)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文介绍了在ECML-PKDD 2025竞赛中获胜的对抗攻击解决方案，通过多轮梯度策略和随机初始化技术，实现了最小的扰动和最高的误分类率。


<details>
  <summary>Details</summary>
Motivation: 竞赛任务要求设计一种对抗攻击，能够在最小化扰动的同时最大化误分类率，以测试模型的鲁棒性。

Method: 采用多轮梯度策略，结合模型的微分结构，并引入随机初始化和样本混合技术以提高攻击效果。

Result: 该攻击在扰动大小和欺骗成功率上表现最佳，赢得了竞赛第一名。

Conclusion: 该方法展示了在对抗攻击设计中的高效性，为高能物理发现中的鲁棒学习提供了新思路。

Abstract: This report presents the winning solution for Task 1 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The task required designing an adversarial attack against a
provided classification model that maximizes misclassification while minimizing
perturbations. Our approach employs a multi-round gradient-based strategy that
leverages the differentiable structure of the model, augmented with random
initialization and sample-mixing techniques to enhance effectiveness. The
resulting attack achieved the best results in perturbation size and fooling
success rate, securing first place in the competition.

</details>


### [266] [Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution](https://arxiv.org/abs/2510.16443)
*Dimitris Stefanopoulos,Andreas Voskou*

Main category: cs.LG

TL;DR: 本文介绍了ECML-PKDD 2025挑战赛中Task 2的获胜解决方案，通过数据生成和鲁棒模型训练两阶段方法，在对抗性数据上实现了80%的混合准确率。


<details>
  <summary>Details</summary>
Motivation: 设计一个在高能物理发现中对抗性数据下仍能保持高准确率的鲁棒ANN模型。

Method: 1. 数据生成阶段：使用RDSA衍生方法生成1500万训练样本；2. 模型训练阶段：采用特征嵌入块和密集融合尾部的鲁棒架构。

Result: 混合准确率达到80%，超过第二名2个百分点。

Conclusion: 提出的两阶段方法有效提升了模型在对抗性数据上的鲁棒性，证明了架构设计的优越性。

Abstract: This report presents the winning solution for Task 2 of Colliding with
Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at
ECML-PKDD 2025. The goal of the challenge was to design and train a robust
ANN-based model capable of achieving high accuracy in a binary classification
task on both clean and adversarial data generated with the Random Distribution
Shuffle Attack (RDSA). Our solution consists of two components: a data
generation phase and a robust model training phase. In the first phase, we
produced 15 million artificial training samples using a custom methodology
derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we
introduced a robust architecture comprising (i)a Feature Embedding Block with
shared weights among features of the same type and (ii)a Dense Fusion Tail
responsible for the final prediction. Training this architecture on our
adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the
second-place solution by two percentage points.

</details>


### [267] [Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts](https://arxiv.org/abs/2510.16448)
*Yongxiang Hua,Haoyu Cao,Zhou Tao,Bocheng Li,Zihao Wu,Chaohu Liu,Linli Xu*

Main category: cs.LG

TL;DR: 提出了一种新的路由框架Input Domain Aware MoE，通过概率混合模型更好地划分输入空间，提升了专家利用平衡和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性评分的路由机制难以有效捕捉输入结构，导致专家专业化和计算平衡之间的权衡问题。

Method: 采用概率混合模型建模路由概率，使专家形成清晰的 specialization 边界，同时实现平衡利用。

Result: 在视觉语言任务中表现优于现有 sMoE 方法，任务性能和专家利用平衡均有提升。

Conclusion: 新路由框架通过独立于任务目标的训练，实现了稳定优化和明确的专家分配。

Abstract: Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling
large vision-language models, offering substantial capacity while maintaining
computational efficiency through dynamic, sparse activation of experts.
However, existing routing mechanisms, typically based on similarity scoring,
struggle to effectively capture the underlying input structure. This limitation
leads to a trade-off between expert specialization and balanced computation,
hindering both scalability and performance. We propose Input Domain Aware MoE,
a novel routing framework that leverages a probabilistic mixture model to
better partition the input space. By modeling routing probabilities as a
mixture of distributions, our method enables experts to develop clear
specialization boundaries while achieving balanced utilization. Unlike
conventional approaches, our routing mechanism is trained independently of
task-specific objectives, allowing for stable optimization and decisive expert
assignments. Empirical results on vision-language tasks demonstrate that our
method consistently outperforms existing sMoE approaches, achieving higher task
performance and improved expert utilization balance.

</details>


### [268] [Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making](https://arxiv.org/abs/2510.16462)
*Emmanuelle Claeys,Elena Kerjean,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出了一种用于模仿学习的序列强化学习框架，专注于蜜蜂的认知策略建模，解决了现有方法在策略变化和可解释性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习方法在蜜蜂行为建模中无法处理策略变化和非最优行为，且缺乏可解释性，限制了生物学洞察。

Method: 结合轨迹相似性建模不同策略，最小化预测损失并确定有效记忆窗口，提供数学框架和数据集。

Result: 新方法能更准确地捕捉蜜蜂行为模式，提供可解释性，并发布了包含80只蜜蜂的追踪数据集。

Conclusion: 该框架为传粉者认知研究提供了新工具，支持生态治理中的昆虫行为模拟。

Abstract: We introduce a sequential reinforcement learning framework for imitation
learning designed to model heterogeneous cognitive strategies in pollinators.
Focusing on honeybees, our approach leverages trajectory similarity to capture
and forecast behavior across individuals that rely on distinct strategies: some
exploiting numerical cues, others drawing on memory, or being influenced by
environmental factors such as weather. Through empirical evaluation, we show
that state-of-the-art imitation learning methods often fail in this setting:
when expert policies shift across memory windows or deviate from optimality,
these models overlook both fast and slow learning behaviors and cannot
faithfully reproduce key decision patterns. Moreover, they offer limited
interpretability, hindering biological insight. Our contribution addresses
these challenges by (i) introducing a model that minimizes predictive loss
while identifying the effective memory horizon most consistent with behavioral
data, and (ii) ensuring full interpretability to enable biologists to analyze
underlying decision-making strategies and finally (iii) providing a
mathematical framework linking bee policy search with bandit formulations under
varying exploration-exploitation dynamics, and releasing a novel dataset of 80
tracked bees observed under diverse weather conditions. This benchmark
facilitates research on pollinator cognition and supports ecological governance
by improving simulations of insect behavior in agroecosystems. Our findings
shed new light on the learning strategies and memory interplay shaping
pollinator decision-making.

</details>


### [269] [SCALAR: Self-Calibrating Adaptive Latent Attention Representation Learning](https://arxiv.org/abs/2510.16474)
*Farwa Abbas,Hussain Ahmad,Claudia Szabo*

Main category: cs.LG

TL;DR: 提出了一种基于自适应核注意力机制的新方法，用于处理高维异构数据中的复杂非线性关系。


<details>
  <summary>Details</summary>
Motivation: 传统方法如PLS难以处理高维数据中的非线性关系和跨尺度交互，且静态特征权重无法适应上下文变化。

Method: 采用自适应核注意力机制，分别处理不同特征组后再整合，以捕捉局部模式并保持全局关系。

Result: 实验结果表明，该方法在多种数据集上显著优于现有技术。

Conclusion: 新方法通过架构创新有效提升了高维异构数据中的预测性能。

Abstract: High-dimensional, heterogeneous data with complex feature interactions pose
significant challenges for traditional predictive modeling approaches. While
Projection to Latent Structures (PLS) remains a popular technique, it struggles
to model complex non-linear relationships, especially in multivariate systems
with high-dimensional correlation structures. This challenge is further
compounded by simultaneous interactions across multiple scales, where local
processing fails to capture crossgroup dependencies. Additionally, static
feature weighting limits adaptability to contextual variations, as it ignores
sample-specific relevance. To address these limitations, we propose a novel
method that enhances predictive performance through novel architectural
innovations. Our architecture introduces an adaptive kernel-based attention
mechanism that processes distinct feature groups separately before integration,
enabling capture of local patterns while preserving global relationships.
Experimental results show substantial improvements in performance metrics,
compared to the state-of-the-art methods across diverse datasets.

</details>


### [270] [Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection](https://arxiv.org/abs/2510.16511)
*Dongchan Cho,Jiho Han,Keumyeong Kang,Minsang Kim,Honggyu Ryu,Namsoon Jung*

Main category: cs.LG

TL;DR: OracleAD是一个简单且可解释的无监督框架，用于多元时间序列异常检测，通过因果嵌入和稳定潜在结构（SLS）实现高精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多元时间序列异常稀少且通常无标签，现有方法复杂且性能被高估，需要一种简单且可解释的解决方案。

Method: OracleAD通过因果嵌入预测当前时间点并重构输入窗口，利用自注意力机制捕捉空间关系，并通过SLS识别异常。

Result: OracleAD在多个真实数据集上达到最先进性能，并能精确定位异常根源。

Conclusion: OracleAD提供了一种高效且可解释的多元时间序列异常检测方法，优于现有复杂方法。

Abstract: Real-world multivariate time series anomalies are rare and often unlabeled.
Additionally, prevailing methods rely on increasingly complex architectures
tuned to benchmarks, detecting only fragments of anomalous segments and
overstating performance. In this paper, we introduce OracleAD, a simple and
interpretable unsupervised framework for multivariate time series anomaly
detection. OracleAD encodes each variable's past sequence into a single causal
embedding to jointly predict the present time point and reconstruct the input
window, effectively modeling temporal dynamics. These embeddings then undergo a
self-attention mechanism to project them into a shared latent space and capture
spatial relationships. These relationships are not static, since they are
modeled by a property that emerges from each variable's temporal dynamics. The
projected embeddings are aligned to a Stable Latent Structure (SLS)
representing normal-state relationships. Anomalies are identified using a dual
scoring mechanism based on prediction error and deviation from the SLS,
enabling fine-grained anomaly diagnosis at each time point and across
individual variables. Since any noticeable SLS deviation originates from
embeddings that violate the learned temporal causality of normal data, OracleAD
directly pinpoints the root-cause variables at the embedding level. OracleAD
achieves state-of-the-art results across multiple real-world datasets and
evaluation protocols, while remaining interpretable through SLS.

</details>


### [271] [eDCF: Estimating Intrinsic Dimension using Local Connectivity](https://arxiv.org/abs/2510.16513)
*Dhruv Gupta,Aditya Nagarsekar,Vraj Shah,Sujith Thomas*

Main category: cs.LG

TL;DR: 提出了一种基于连通性因子（CF）的新方法eDCF，用于在不同尺度下稳健估计高维数据的内在维度（ID），并在噪声和大数据集下表现优异。


<details>
  <summary>Details</summary>
Motivation: 高维数据中复杂依赖关系使得内在维度估计困难，现有方法在不同尺度下表现不稳定。

Method: 基于连通性因子（CF）的eDCF方法，具有可扩展性和并行化能力。

Result: 在合成基准测试中，eDCF与主流估计器表现相当，且在噪声和大数据集下ID匹配率更高（25.0% vs. 16.7%和12.5%）。

Conclusion: eDCF能有效估计内在维度，并在复杂数据（如分形几何）中表现出实用性。

Abstract: Modern datasets often contain high-dimensional features exhibiting complex
dependencies. To effectively analyze such data, dimensionality reduction
methods rely on estimating the dataset's intrinsic dimension (id) as a measure
of its underlying complexity. However, estimating id is challenging due to its
dependence on scale: at very fine scales, noise inflates id estimates, while at
coarser scales, estimates stabilize to lower, scale-invariant values. This
paper introduces a novel, scalable, and parallelizable method called eDCF,
which is based on Connectivity Factor (CF), a local connectivity-based metric,
to robustly estimate intrinsic dimension across varying scales. Our method
consistently matches leading estimators, achieving comparable values of mean
absolute error (MAE) on synthetic benchmarks with noisy samples. Moreover, our
approach also attains higher exact intrinsic dimension match rates, reaching up
to 25.0% compared to 16.7% for MLE and 12.5% for TWO-NN, particularly excelling
under medium to high noise levels and large datasets. Further, we showcase our
method's ability to accurately detect fractal geometries in decision
boundaries, confirming its utility for analyzing realistic, structured data.

</details>


### [272] [Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks](https://arxiv.org/abs/2510.16530)
*Ashutosh Srivastava,Lokesh Nagalapatti,Gautam Jajoo,Aniket Vashishtha,Parameswari Krishnamurthy,Amit Sharma*

Main category: cs.LG

TL;DR: 论文指出当前大语言模型（LLM）在因果发现中的评估存在缺陷，提出需开发防泄漏的评估协议和结合LLM与统计方法的混合方法。


<details>
  <summary>Details</summary>
Motivation: 挑战LLM在因果发现中的真实能力，避免因预训练数据泄漏导致的虚假表现。

Method: 提出基于新科学研究的评估协议，并设计结合LLM预测与经典PC算法的混合方法。

Result: LLM在真实科学数据上表现较差，但作为PC算法的先验能显著提升准确性。

Conclusion: 呼吁采用科学基础的评估基准和混合方法，以支持真实世界的因果发现。

Abstract: Recent claims of strong performance by Large Language Models (LLMs) on causal
discovery are undermined by a key flaw: many evaluations rely on benchmarks
likely included in pretraining corpora. Thus, apparent success suggests that
LLM-only methods, which ignore observational data, outperform classical
statistical approaches. We challenge this narrative by asking: Do LLMs truly
reason about causal structure, and how can we measure it without memorization
concerns? Can they be trusted for real-world scientific discovery? We argue
that realizing LLMs' potential for causal analysis requires two shifts: (P.1)
developing robust evaluation protocols based on recent scientific studies to
guard against dataset leakage, and (P.2) designing hybrid methods that combine
LLM-derived knowledge with data-driven statistics. To address P.1, we encourage
evaluating discovery methods on novel, real-world scientific studies. We
outline a practical recipe for extracting causal graphs from recent
publications released after an LLM's training cutoff, ensuring relevance and
preventing memorization while capturing both established and novel relations.
Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy,
they perform far worse on our curated graphs, underscoring the need for
statistical grounding. Supporting P.2, we show that using LLM predictions as
priors for the classical PC algorithm significantly improves accuracy over both
LLM-only and purely statistical methods. We call on the community to adopt
science-grounded, leakage-resistant benchmarks and invest in hybrid causal
discovery methods suited to real-world inquiry.

</details>


### [273] [Predicting life satisfaction using machine learning and explainable AI](https://arxiv.org/abs/2510.16547)
*Alif Elham Khan,Mohammad Junayed Hasan,Humayra Anjum,Nabeel Mohammed,Sifat Momen*

Main category: cs.LG

TL;DR: 机器学习和大语言模型（LLMs）用于预测生活满意度，准确率达93.80%，并探讨了数据重采样和特征选择的影响。


<details>
  <summary>Details</summary>
Motivation: 传统测量生活满意度的方法复杂且易错，研究旨在通过AI技术提高预测准确性和可解释性。

Method: 使用特征学习提取27个关键问题，并探索临床和生物医学LLMs将表格数据转换为自然语言句子。

Result: 模型准确率达93.80%，健康条件是所有年龄段最重要的决定因素。

Conclusion: 机器学习、LLMs和XAI共同增强了对人类行为的理解，为量化主观幸福感提供了新方法。

Abstract: Life satisfaction is a crucial facet of human well-being. Hence, research on
life satisfaction is incumbent for understanding how individuals experience
their lives and influencing interventions targeted at enhancing mental health
and well-being. Life satisfaction has traditionally been measured using analog,
complicated, and frequently error-prone methods. These methods raise questions
concerning validation and propagation. However, this study demonstrates the
potential for machine learning algorithms to predict life satisfaction with a
high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a
government survey of 19000 people aged 16-64 years in Denmark. Using feature
learning techniques, 27 significant questions for assessing contentment were
extracted, making the study highly reproducible, simple, and easily
interpretable. Furthermore, clinical and biomedical large language models
(LLMs) were explored for predicting life satisfaction by converting tabular
data into natural language sentences through mapping and adding meaningful
counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It
was found that life satisfaction prediction is more closely related to the
biomedical domain than the clinical domain. Ablation studies were also
conducted to understand the impact of data resampling and feature selection
techniques on model performance. Moreover, the correlation between primary
determinants with different age brackets was analyzed, and it was found that
health condition is the most important determinant across all ages. This study
demonstrates how machine learning, large language models and XAI can jointly
contribute to building trust and understanding in using AI to investigate human
behavior, with significant ramifications for academics and professionals
working to quantify and comprehend subjective well-being.

</details>


### [274] [NeurIPT: Foundation Model for Neural Interfaces](https://arxiv.org/abs/2510.16548)
*Zitao Fang,Chenxuan Li,Hongting Zhou,Shuyang Yu,Guodong Du,Ashwaq Qasem,Yang Lu,Jing Li,Junsong Zhang,Sim Kuan Goh*

Main category: cs.LG

TL;DR: NeurIPT是一种基于预训练Transformer的EEG基础模型，通过捕捉时空特征提升神经解码性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG数据中因受试者、任务和条件差异以及电极配置多样性导致的泛化挑战。

Method: 提出AAMP（基于幅度的掩码预训练）和PMoE（渐进专家混合架构）处理时间特征；利用电极3D坐标和IILP（脑区内-间池化）优化空间特征。

Result: 在八个BCI数据集中通过微调实现SOTA性能。

Conclusion: NeurIPT推动了EEG基础模型的发展，为可扩展和泛化的神经信息处理系统提供了新思路。

Abstract: Electroencephalography (EEG) has wide-ranging applications, from clinical
diagnosis to brain-computer interfaces (BCIs). With the increasing volume and
variety of EEG data, there has been growing interest in establishing foundation
models (FMs) to scale up and generalize neural decoding. Despite showing early
potential, applying FMs to EEG remains challenging due to substantial
inter-subject, inter-task, and inter-condition variability, as well as diverse
electrode configurations across recording setups. To tackle these open
challenges, we propose NeurIPT, a foundation model developed for diverse
EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both
homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG
signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP),
masking based on signal amplitude rather than random intervals, to learn robust
representations across varying signal intensities beyond local interpolation.
Moreover, this temporal representation is enhanced by a Progressive
Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks
are progressively introduced at deeper layers, adapting effectively to the
diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages
the 3D physical coordinates of electrodes, enabling effective transfer of
embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling
(IILP) during fine-tuning to efficiently exploit regional brain features.
Empirical evaluations across eight downstream BCI datasets, via fine-tuning,
demonstrated NeurIPT consistently achieved state-of-the-art performance,
highlighting its broad applicability and robust generalization. Our work pushes
forward the state of FMs in EEG and offers insights into scalable and
generalizable neural information processing systems.

</details>


### [275] [LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs](https://arxiv.org/abs/2510.16552)
*Ang Li,Yifei Wang,Zhihang Yuan,Stefanie Jegelka,Yisen Wang*

Main category: cs.LG

TL;DR: LANPO框架通过分离语言反馈和数值奖励的作用，提升大型语言模型在强化学习中的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标量奖励，忽略了文本反馈的价值，导致样本效率低和探索困难。

Method: 提出LANPO框架，动态构建经验池，引入奖励无关反思和相关抽象原则。

Result: 在数学推理基准测试中，7B和14B模型显著优于GRPO基线。

Conclusion: LANPO为LLM强化学习提供了一种高效利用历史经验的方法，提升了学习效果和数据效率。

Abstract: Reinforcement learning in large language models (LLMs) often relies on scalar
rewards, a practice that discards valuable textual rationale buried in the
rollouts, forcing the model to explore \textit{de novo} with each attempt and
hindering sample efficiency. While LLMs can uniquely learn from language
feedback provided in-context, naively integrating on-line experiences into RL
training presents a paradox: feedback from the same problem risks information
leakage and memorization, while feedback from different problems often leads to
behavior collapse due to irrelevant context. To resolve this tension, we
propose \textbf{Language-And-Numerical Policy Optimization (LANPO)}, a
framework that cleanly separates the roles of feedback: language guides
exploration, while numerical rewards drive optimization. LANPO builds a dynamic
experience pool from past trials and introduces two principles to ensure
feedback is effective: \emph{Reward-Agnostic Reflection} for safe intra-sample
self-correction and \emph{Relevant Abstraction} to distill generalizable
lessons from inter-sample experiences. Across mathematical reasoning
benchmarks, LANPO enables 7B and 14B models to significantly outperform strong
baselines trained with GRPO in test accuracy. Our work provides a robust method
for integrating historical experiences into the LLM RL loop, creating more
effective and data-efficient learning agents.

</details>


### [276] [Copy-Augmented Representation for Structure Invariant Template-Free Retrosynthesis](https://arxiv.org/abs/2510.16588)
*Jiaxi Zhuang,Yu Zhang,Aimin Zhou,Ying Qian*

Main category: cs.LG

TL;DR: 提出了一种名为C-SMILES的新分子表示方法，通过分解SMILES为元素-标记对，结合复制增强机制和SMILES对齐指导，显著提高了逆合成预测的准确性和分子生成的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有无模板方法难以捕捉化学反应中的结构不变性，导致搜索空间过大和预测准确性下降。

Method: 引入C-SMILES表示法，结合复制增强机制和SMILES对齐指导，动态决定生成新标记或保留未变分子片段。

Result: 在USPTO-50K和USPTO-FULL数据集上分别达到67.2%和50.8%的top-1准确率，生成分子有效性达99.9%。

Conclusion: C-SMILES为结构感知分子生成提供了新范式，可直接应用于计算药物发现。

Abstract: Retrosynthesis prediction is fundamental to drug discovery and chemical
synthesis, requiring the identification of reactants that can produce a target
molecule. Current template-free methods struggle to capture the structural
invariance inherent in chemical reactions, where substantial molecular
scaffolds remain unchanged, leading to unnecessarily large search spaces and
reduced prediction accuracy. We introduce C-SMILES, a novel molecular
representation that decomposes traditional SMILES into element-token pairs with
five special tokens, effectively minimizing editing distance between reactants
and products. Building upon this representation, we incorporate a
copy-augmented mechanism that dynamically determines whether to generate new
tokens or preserve unchanged molecular fragments from the product. Our approach
integrates SMILES alignment guidance to enhance attention consistency with
ground-truth atom mappings, enabling more chemically coherent predictions.
Comprehensive evaluation on USPTO-50K and large-scale USPTO-FULL datasets
demonstrates significant improvements: 67.2% top-1 accuracy on USPTO-50K and
50.8% on USPTO-FULL, with 99.9% validity in generated molecules. This work
establishes a new paradigm for structure-aware molecular generation with direct
applications in computational drug discovery.

</details>


### [277] [Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration](https://arxiv.org/abs/2510.16590)
*Alan Kai Hassen,Andrius Bernatavicius,Antonius P. A. Janssen,Mike Preuss,Gerard J. P. van Westen,Djork-Arné Clevert*

Main category: cs.LG

TL;DR: 利用大型语言模型（LLM）进行分子推理的框架，无需标记数据，成功应用于单步逆合成任务。


<details>
  <summary>Details</summary>
Motivation: 化学中标记数据稀缺且昂贵，限制了传统监督学习方法的应用。

Method: 通过原子标识符锚定思维链推理，结合单样本和少样本任务预测化学转化。

Result: 在识别反应位点（≥90%）、命名反应类别（≥40%）和最终反应物（≥74%）方面取得高成功率。

Conclusion: 该方法不仅解决了复杂化学任务，还提供了生成理论支持的合成数据集的方法。

Abstract: Applications of machine learning in chemistry are often limited by the
scarcity and expense of labeled data, restricting traditional supervised
methods. In this work, we introduce a framework for molecular reasoning using
general-purpose Large Language Models (LLMs) that operates without requiring
labeled training data. Our method anchors chain-of-thought reasoning to the
molecular structure by using unique atomic identifiers. First, the LLM performs
a one-shot task to identify relevant fragments and their associated chemical
labels or transformation classes. In an optional second step, this
position-aware information is used in a few-shot task with provided class
examples to predict the chemical transformation. We apply our framework to
single-step retrosynthesis, a task where LLMs have previously underperformed.
Across academic benchmarks and expert-validated drug discovery molecules, our
work enables LLMs to achieve high success rates in identifying chemically
plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and
final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work
also provides a method to generate theoretically grounded synthetic datasets by
mapping chemical knowledge onto the molecular structure and thereby addressing
data scarcity.

</details>


### [278] [Symmetry and Generalisation in Neural Approximations of Renormalisation Transformations](https://arxiv.org/abs/2510.16591)
*Cassidy Ashworth,Pietro Liò,Francesco Caso*

Main category: cs.LG

TL;DR: 研究探讨了参数对称性和网络表达能力在神经网络学习重整化群变换时的作用，揭示了对称约束与表达能力之间的竞争关系。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型通过多层表示学习结构化数据的特征，但如何将物理对称性编码到模型中以提高性能仍是一个挑战。

Method: 使用多层感知机（MLP）和图神经网络（GNN），通过改变权重对称性和激活函数来评估模型的表现。

Result: 研究发现，过于复杂或约束过强的模型泛化能力较差，并通过理论和实验验证了这一现象。

Conclusion: 研究为对称网络的学习动态及其在建模物理变换中的局限性提供了新见解。

Abstract: Deep learning models have proven enormously successful at using multiple
layers of representation to learn relevant features of structured data.
Encoding physical symmetries into these models can improve performance on
difficult tasks, and recent work has motivated the principle of parameter
symmetry breaking and restoration as a unifying mechanism underlying their
hierarchical learning dynamics. We evaluate the role of parameter symmetry and
network expressivity in the generalisation behaviour of neural networks when
learning a real-space renormalisation group (RG) transformation, using the
central limit theorem (CLT) as a test case map. We consider simple multilayer
perceptrons (MLPs) and graph neural networks (GNNs), and vary weight symmetries
and activation functions across architectures. Our results reveal a competition
between symmetry constraints and expressivity, with overly complex or
overconstrained models generalising poorly. We analytically demonstrate this
poor generalisation behaviour for certain constrained MLP architectures by
recasting the CLT as a cumulant recursion relation and making use of an
established framework to propagate cumulants through MLPs. We also empirically
validate an extension of this framework from MLPs to GNNs, elucidating the
internal information processing performed by these more complex models. These
findings offer new insight into the learning dynamics of symmetric networks and
their limitations in modelling structured physical transformations.

</details>


### [279] [Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules](https://arxiv.org/abs/2510.16607)
*Tianwei Wang,Xinhui Ma,Wei Pang*

Main category: cs.LG

TL;DR: 提出了一种基于四元数的监督学习Hopfield结构神经网络（QSHNN），通过周期性投影策略保持四元数一致性，实验显示高精度和快速收敛。


<details>
  <summary>Details</summary>
Motivation: 利用四元数在表示旋转和姿态中的几何优势，扩展Hopfield神经网络到四元数域。

Method: 从连续时间动态模型出发，引入周期性投影策略修改梯度下降，保持权重矩阵的四元数结构。

Result: 实验表明模型具有高精度、快速收敛和强可靠性，轨迹平滑适用于机器人控制。

Conclusion: QSHNN为超复数或非交换代数结构下的神经网络设计提供了实用框架和数学方法。

Abstract: Motivated by the geometric advantages of quaternions in representing
rotations and postures, we propose a quaternion-valued supervised learning
Hopfield-structured neural network (QSHNN) with a fully connected structure
inspired by the classic Hopfield neural network (HNN). Starting from a
continuous-time dynamical model of HNNs, we extend the formulation to the
quaternionic domain and establish the existence and uniqueness of fixed points
with asymptotic stability. For the learning rules, we introduce a periodic
projection strategy that modifies standard gradient descent by periodically
projecting each 4*4 block of the weight matrix onto the closest quaternionic
structure in the least-squares sense. This approach preserves both convergence
and quaternionic consistency throughout training. Benefiting from this rigorous
mathematical foundation, the experimental model implementation achieves high
accuracy, fast convergence, and strong reliability across randomly generated
target sets. Moreover, the evolution trajectories of the QSHNN exhibit
well-bounded curvature, i.e., sufficient smoothness, which is crucial for
applications such as control systems or path planning modules in robotic arms,
where joint postures are parameterized by quaternion neurons. Beyond these
application scenarios, the proposed model offers a practical implementation
framework and a general mathematical methodology for designing neural networks
under hypercomplex or non-commutative algebraic structures.

</details>


### [280] [Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods](https://arxiv.org/abs/2510.16609)
*Avrim Blum,Daniel Hsu,Cyrus Rashtchian,Donya Saless*

Main category: cs.LG

TL;DR: 论文研究了测试时增强（如RAG或工具使用）中模型参数知识与外部检索信息的关系，提出了一个理论框架来分析所需增强步骤的数量。


<details>
  <summary>Details</summary>
Motivation: 理解模型预训练知识与外部增强信息之间的理论关系，特别是在少量增强步骤下回答查询的能力。

Method: 将多步推理建模为知识图上的s-t连通性问题，将预训练知识表示为部分子图，并通过查询真实边来增强知识。

Result: 发现了一个相变现象：若知识图被分割为小组件，则路径查找效率低；若知识密度超过阈值，则路径查找效率高。

Conclusion: 预训练知识图的密度对增强步骤的效率有决定性影响，高密度知识图可显著减少所需查询次数。

Abstract: Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool
use, critically depends on an interplay between a model's parametric knowledge
and externally retrieved information. However, the theoretical underpinnings of
this relationship remain poorly understood. Specifically, it is not clear how
much pre-training knowledge is required to answer queries with a small number
of augmentation steps, which is a desirable property in practice. To address
this question, we formulate multi-step reasoning as an $s$-$t$ connectivity
problem on a knowledge graph. We represent a model's pre-training parametric
knowledge as a partial, potentially noisy subgraph. We view augmentation as
querying an oracle for true edges that augment the model's knowledge. Then, we
characterize the necessary and sufficient number of augmentation steps for the
model to generate an accurate answer given partial prior knowledge. One key
result shows a phase transition: if the prior knowledge graph over $n$ vertices
is disconnected into small components, then finding a path via augmentation is
inefficient and requires $\Omega(\sqrt{n})$ queries. On the other hand, once
the density of correct knowledge surpasses a threshold, forming a giant
component, we can find paths with an expected constant number of queries.

</details>


### [281] [On the Impossibility of Retrain Equivalence in Machine Unlearning](https://arxiv.org/abs/2510.16629)
*Jiatong Yu,Yinghui He,Anirudh Goyal,Sanjeev Arora*

Main category: cs.LG

TL;DR: 论文探讨了多阶段训练对机器遗忘的影响，指出局部遗忘方法无法普遍实现重训练等效性。


<details>
  <summary>Details</summary>
Motivation: 研究多阶段训练（如LLM微调）中机器遗忘的挑战，揭示路径依赖性对遗忘效果的影响。

Method: 通过理论和实验（使用Llama和Qwen模型）分析局部遗忘算法的表现，比较不同训练路径下的遗忘效果。

Result: 实验显示，遗忘效果受训练路径顺序影响，GSM8K准确率差异超过20%，且某些路径导致模型遗忘缓慢。

Conclusion: 重训练等效性在多阶段训练中不适用，需重新定义机器遗忘的目标和方法。

Abstract: Machine unlearning seeks to selectively remove the "influence" of specific
training data on a model's outputs. The ideal goal is Retrain
Equivalence--behavior identical to a model trained from scratch on only the
retained data. This goal was formulated for models trained on i.i.d. data
batches, but modern pipelines often involve multi-stage training, with each
stage having a distinct data distribution and objective. Examples include LLM
fine-tuning for alignment, reasoning ability, etc. Our study shows via theory
and experiments that this shift to multi-stage training introduces a
fundamental barrier for machine unlearning. The theory indicates that the
outcome of local unlearning--methods that only use gradients computed on the
forget set--is path-dependent. That is, a model's behavior during unlearning is
influenced by the order of its training stages during learning, making it
impossible for path-oblivious algorithms to universally achieve Retrain
Equivalence. We empirically demonstrate the same phenomenon in LLM
post-training across Llama and Qwen models (1B to 14B) with gradient ascent,
NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different
orderings of identical training stages diverge in behavior during unlearning,
with the degradation in GSM8K accuracy after unlearning varying by over 20%
across paths. We also observe that some learning paths consistently produce
models that unlearn slowly. During unlearning, whether the probability mass
gets squeezed into paraphrasing or alternative concepts is also path-dependent.
These results consistently show that Retrain Equivalence is an ill-posed target
for local unlearning algorithms, so long as the target models are trained in
stages. In situations where access to models' training histories is hard, the
current work calls for rethinking the definition and desiderata of machine
unlearning.

</details>


### [282] [Simulation-free Structure Learning for Stochastic Dynamics](https://arxiv.org/abs/2510.16656)
*Noah El Rimawi-Fine,Adam Stecklov,Lucas Nelson,Mathieu Blanchette,Alexander Tong,Stephen Y. Zhang,Lazar Atanackovic*

Main category: cs.LG

TL;DR: StructureFlow是一种新颖的方法，用于同时学习物理系统的结构和随机群体动态。


<details>
  <summary>Details</summary>
Motivation: 高维随机系统通常只能获得部分、噪声的状态测量，现有方法难以同时解决结构学习和动态建模问题。

Method: 提出StructureFlow，一种无模拟的方法，联合学习系统的结构和随机群体动态。

Result: 在合成系统、生物模拟系统和实验单细胞数据集上验证了方法的有效性。

Conclusion: StructureFlow能够同时学习系统结构和建模条件群体动态，有助于理解系统行为的机制。

Abstract: Modeling dynamical systems and unraveling their underlying causal
relationships is central to many domains in the natural sciences. Various
physical systems, such as those arising in cell biology, are inherently
high-dimensional and stochastic in nature, and admit only partial, noisy state
measurements. This poses a significant challenge for addressing the problems of
modeling the underlying dynamics and inferring the network structure of these
systems. Existing methods are typically tailored either for structure learning
or modeling dynamics at the population level, but are limited in their ability
to address both problems together. In this work, we address both problems
simultaneously: we present StructureFlow, a novel and principled
simulation-free approach for jointly learning the structure and stochastic
population dynamics of physical systems. We showcase the utility of
StructureFlow for the tasks of structure learning from interventions and
dynamical (trajectory) inference of conditional population dynamics. We
empirically evaluate our approach on high-dimensional synthetic systems, a set
of biologically plausible simulated systems, and an experimental single-cell
dataset. We show that StructureFlow can learn the structure of underlying
systems while simultaneously modeling their conditional population dynamics --
a key step toward the mechanistic understanding of systems behavior.

</details>


### [283] [Evaluating protein binding interfaces with PUMBA](https://arxiv.org/abs/2510.16674)
*Azam Shirali,Giri Narasimhan*

Main category: cs.LG

TL;DR: PUMBA是一种基于Vision Mamba架构的蛋白质-蛋白质对接评分函数，优于其前身PIsToN。


<details>
  <summary>Details</summary>
Motivation: 提高蛋白质-蛋白质对接工具的准确性，以支持药物和疫苗开发。

Method: 用Vision Mamba替换PIsToN中的Vision Transformer，提升长序列建模能力。

Result: PUMBA在多个数据集上表现优于PIsToN。

Conclusion: Vision Mamba在蛋白质对接任务中具有潜力。

Abstract: Protein-protein docking tools help in studying interactions between proteins,
and are essential for drug, vaccine, and therapeutic development. However, the
accuracy of a docking tool depends on a robust scoring function that can
reliably differentiate between native and non-native complexes. PIsToN is a
state-of-the-art deep learning-based scoring function that uses Vision
Transformers in its architecture. Recently, the Mamba architecture has
demonstrated exceptional performance in both natural language processing and
computer vision, often outperforming Transformer-based models in their domains.
In this study, we introduce PUMBA (Protein-protein interface evaluation with
Vision Mamba), which improves PIsToN by replacing its Vision Transformer
backbone with Vision Mamba. This change allows us to leverage Mamba's efficient
long-range sequence modeling for sequences of image patches. As a result, the
model's ability to capture both global and local patterns in protein-protein
interface features is significantly improved. Evaluation on several
widely-used, large-scale public datasets demonstrates that PUMBA consistently
outperforms its original Transformer-based predecessor, PIsToN.

</details>


### [284] [Active Target Discovery under Uninformative Prior: The Power of Permanent and Transient Memory](https://arxiv.org/abs/2510.16676)
*Anindya Sarkar,Binglin Ji,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: 提出一种新方法，用于在数据有限或采样成本高的领域中进行有效的主动目标发现，克服了传统生成模型在无信息先验情况下的局限性。


<details>
  <summary>Details</summary>
Motivation: 在数据获取成本高的领域（如医学影像、环境监测等），传统生成模型因无法学习强先验而难以推广。本文旨在解决这一问题。

Method: 结合神经科学原理，设计了一种理论上有依据、可解释的方法，确保每次观测都能单调改进先验估计。

Result: 在物种分布建模和遥感等多个领域的实验中，该方法显著优于基线方法。

Conclusion: 该方法在无信息先验情况下仍能实现鲁棒的探索和适应性，适用于复杂现实场景。

Abstract: In many scientific and engineering fields, where acquiring high-quality data
is expensive--such as medical imaging, environmental monitoring, and remote
sensing--strategic sampling of unobserved regions based on prior observations
is crucial for maximizing discovery rates within a constrained budget. The rise
of powerful generative models, such as diffusion models, has enabled active
target discovery in partially observable environments by leveraging learned
priors--probabilistic representations that capture underlying structure from
data. With guidance from sequentially gathered task-specific observations,
these models can progressively refine exploration and efficiently direct
queries toward promising regions. However, in domains where learning a strong
prior is infeasible due to extremely limited data or high sampling cost (such
as rare species discovery, diagnostics for emerging diseases, etc.), these
methods struggle to generalize. To overcome this limitation, we propose a novel
approach that enables effective active target discovery even in settings with
uninformative priors, ensuring robust exploration and adaptability in complex
real-world scenarios. Our framework is theoretically principled and draws
inspiration from neuroscience to guide its design. Unlike black-box policies,
our approach is inherently interpretable, providing clear insights into
decision-making. Furthermore, it guarantees a strong, monotonic improvement in
prior estimates with each new observation, leading to increasingly accurate
sampling and reinforcing both reliability and adaptability in dynamic settings.
Through comprehensive experiments and ablation studies across various domains,
including species distribution modeling and remote sensing, we demonstrate that
our method substantially outperforms baseline approaches.

</details>


### [285] [Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers](https://arxiv.org/abs/2510.16677)
*Ran Tong,Jiaqi Liu,Su Liu,Xin Hu,Lanruo Wang*

Main category: cs.LG

TL;DR: 论文提出了一个紧凑、严格因果的基准，用于在MIT-BIH心律失常数据库上使用每秒心率进行流式临床时间序列分析。比较了GRU-D和Transformer在两种任务下的表现。


<details>
  <summary>Details</summary>
Motivation: 研究在流式临床时间序列中，不同模型（GRU-D和Transformer）在短期心动过速风险预测和心率预测任务中的表现差异。

Method: 使用记录级、非重叠分割的数据集，比较GRU-D和Transformer在匹配训练预算下的表现，并采用校准感知的分类评估和适当的预测评估方法。

Result: 在MIT-BIH数据上，GRU-D在心动过速风险预测中略优于Transformer，而Transformer在心率预测中明显优于GRU-D和持续性基线。

Conclusion: 模型选择依赖于任务：紧凑的RNN在短期风险评分中仍有竞争力，而紧凑的Transformer在点预测中表现更优。

Abstract: We present a compact, strictly causal benchmark for streaming clinical time
series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two
tasks are studied under record-level, non-overlapping splits: near-term
tachycardia risk (next ten seconds) and one-step heart rate forecasting. We
compare a GRU-D (RNN) and a Transformer under matched training budgets against
strong non-learned baselines. Evaluation is calibration-aware for
classification and proper for forecasting, with temperature scaling and grouped
bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the
Transformer for tachycardia risk, while the Transformer clearly lowers
forecasting error relative to GRU-D and persistence. Our results show that, in
longitudinal monitoring, model choice is task-dependent: compact RNNs remain
competitive for short-horizon risk scoring, whereas compact Transformers
deliver clearer gains for point forecasting.

</details>


### [286] [High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares](https://arxiv.org/abs/2510.16687)
*Shurong Lin,Eric D. Kolaczyk,Adam Smith,Elliot Paquette*

Main category: cs.LG

TL;DR: 论文通过扩散方法精确分析噪声SGD，提供高维统计风险和隐私损失的连续时间视角，并研究无需梯度敏感性的变体。


<details>
  <summary>Details</summary>
Motivation: 优化与隐私的交互是隐私保护机器学习的核心问题，但噪声SGD在高维下的精确行为尚不明确。

Method: 采用扩散方法分析噪声SGD，研究无需梯度敏感性的变体，聚焦于带ℓ2正则的最小二乘问题。

Result: 提供了高维统计风险和隐私损失的动态演化视角。

Conclusion: 扩散方法为噪声SGD提供了更精确的分析框架，扩展了无需梯度敏感性的算法设计。

Abstract: The interplay between optimization and privacy has become a central theme in
privacy-preserving machine learning. Noisy stochastic gradient descent (SGD)
has emerged as a cornerstone algorithm, particularly in large-scale settings.
These variants of gradient methods inject carefully calibrated noise into each
update to achieve differential privacy, the gold standard notion of rigorous
privacy guarantees. Prior work primarily provides various bounds on statistical
risk and privacy loss for noisy SGD, yet the \textit{exact} behavior of the
process remains unclear, particularly in high-dimensional settings. This work
leverages a diffusion approach to analyze noisy SGD precisely, providing a
continuous-time perspective that captures both statistical risk evolution and
privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy
SGD that does not require explicit knowledge of gradient sensitivity, unlike
existing work that assumes or enforces sensitivity through gradient clipping.
Specifically, we focus on the least squares problem with $\ell_2$
regularization.

</details>


### [287] [CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning](https://arxiv.org/abs/2510.16694)
*Anthony DiMaggio,Raghav Sharma,Gururaj Saileshwar*

Main category: cs.LG

TL;DR: CLIP是一种客户端不变神经元剪枝技术，结合网络感知剪枝，解决了安全联邦学习中因慢速客户端导致的性能瓶颈，加速训练并保持较高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决安全联邦学习中因异构设备导致的性能瓶颈问题，特别是慢速客户端（stragglers）对整体训练速度的影响。

Method: 提出CLIP技术，结合客户端不变神经元剪枝和网络感知剪枝，减少计算和网络瓶颈。

Result: 在多个数据集上加速训练13%至34%，准确性影响在1.3%提升至2.6%下降之间。

Conclusion: CLIP有效解决了安全联邦学习中的性能瓶颈问题，显著提升训练速度且对准确性影响较小。

Abstract: Secure federated learning (FL) preserves data privacy during distributed
model training. However, deploying such frameworks across heterogeneous devices
results in performance bottlenecks, due to straggler clients with limited
computational or network capabilities, slowing training for all participating
clients. This paper introduces the first straggler mitigation technique for
secure aggregation with deep neural networks. We propose CLIP, a client-side
invariant neuron pruning technique coupled with network-aware pruning, that
addresses compute and network bottlenecks due to stragglers during training
with minimal accuracy loss. Our technique accelerates secure FL training by 13%
to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an
accuracy impact of between 1.3% improvement to 2.6% reduction.

</details>


### [288] [Resolution-Aware Retrieval Augmented Zero-Shot Forecasting](https://arxiv.org/abs/2510.16695)
*Iman Deznabi,Peeyush Kumar,Madalina Fiterau*

Main category: cs.LG

TL;DR: 提出了一种分辨率感知的检索增强预测模型，用于零样本预测，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统预测方法在零样本预测中的局限性，尤其是在缺乏历史数据的情况下。

Method: 通过将信号分解为不同频率成分，采用分辨率感知检索策略，动态检索相关数据。

Result: 在微气候预测中，模型表现优于传统方法和现代时间序列模型，MSE显著降低。

Conclusion: 检索增强和分辨率感知策略有效，为零样本预测提供了可扩展且数据高效的解决方案。

Abstract: Zero-shot forecasting aims to predict outcomes for previously unseen
conditions without direct historical data, posing a significant challenge for
traditional forecasting methods. We introduce a Resolution-Aware
Retrieval-Augmented Forecasting model that enhances predictive accuracy by
leveraging spatial correlations and temporal frequency characteristics. By
decomposing signals into different frequency components, our model employs
resolution-aware retrieval, where lower-frequency components rely on broader
spatial context, while higher-frequency components focus on local influences.
This allows the model to dynamically retrieve relevant data and adapt to new
locations with minimal historical context.
  Applied to microclimate forecasting, our model significantly outperforms
traditional forecasting methods, numerical weather prediction models, and
modern foundation time series models, achieving 71% lower MSE than HRRR and 34%
lower MSE than Chronos on the ERA5 dataset.
  Our results highlight the effectiveness of retrieval-augmented and
resolution-aware strategies, offering a scalable and data-efficient solution
for zero-shot forecasting in microclimate modeling and beyond.

</details>


### [289] [On the Granularity of Causal Effect Identifiability](https://arxiv.org/abs/2510.16703)
*Yizuo Chen,Adnan Darwiche*

Main category: cs.LG

TL;DR: 本文探讨了基于状态的因果效应可识别性，指出在某些情况下，即使基于变量的因果效应不可识别，基于状态的因果效应仍可能识别。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于扩展传统的因果效应识别框架，以涵盖更细粒度的状态干预和结果状态，从而在变量层面不可识别的情况下仍能识别因果效应。

Method: 通过引入额外的知识（如上下文特定独立性和条件函数依赖），分析了状态干预对结果状态的影响，并探讨了变量状态约束知识的作用。

Result: 研究发现，基于状态的因果效应在某些情况下是可识别的，而基于变量的因果效应则不可识别。此外，变量状态约束知识需与其他知识结合才能提升可识别性。

Conclusion: 结论强调了在现有变量框架下可能被忽视的因果效应识别机会，为从观测数据中估计因果效应提供了新视角。

Abstract: The classical notion of causal effect identifiability is defined in terms of
treatment and outcome variables. In this note, we consider the identifiability
of state-based causal effects: how an intervention on a particular state of
treatment variables affects a particular state of outcome variables. We
demonstrate that state-based causal effects may be identifiable even when
variable-based causal effects may not. Moreover, we show that this separation
occurs only when additional knowledge -- such as context-specific
independencies and conditional functional dependencies -- is available. We
further examine knowledge that constrains the states of variables, and show
that such knowledge does not improve identifiability on its own but can improve
both variable-based and state-based identifiability when combined with other
knowledge such as context-specific independencies. Our findings highlight
situations where causal effects of interest may be estimable from observational
data and this identifiability may be missed by existing variable-based
frameworks.

</details>


### [290] [LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus](https://arxiv.org/abs/2510.16719)
*Zak Ressler,Marcus Grijalva,Angelica Marie Ignacio,Melanie Torres,Abelardo Cuadra Rojas,Rohollah Moghadam,Mohammad Rasoul narimani*

Main category: cs.LG

TL;DR: 提出了一种基于LSTM的EV充电负荷预测框架，通过数据预处理和特征提取实现多时间尺度预测。


<details>
  <summary>Details</summary>
Motivation: 解决EV充电负荷预测问题，为基础设施规划和能源管理提供支持。

Method: 使用LSTM处理多地点原始数据，包括数据插补、归一化和特征提取。

Result: 模型能准确预测日、周、月尺度的充电需求，适应不同场景。

Conclusion: 框架模块化设计适用于多样化部署，为电网集成提供实用工具。

Abstract: This paper presents a framework for processing EV charging load data in order
to forecast future load predictions using a Recurrent Neural Network,
specifically an LSTM. The framework processes a large set of raw data from
multiple locations and transforms it with normalization and feature extraction
to train the LSTM. The pre-processing stage corrects for missing or incomplete
values by interpolating and normalizing the measurements. This information is
then fed into a Long Short-Term Memory Model designed to capture the short-term
fluctuations while also interpreting the long-term trends in the charging data.
Experimental results demonstrate the model's ability to accurately predict
charging demand across multiple time scales (daily, weekly, and monthly),
providing valuable insights for infrastructure planning, energy management, and
grid integration of EV charging facilities. The system's modular design allows
for adaptation to different charging locations with varying usage patterns,
making it applicable across diverse deployment scenarios.

</details>


### [291] [Zero-Shot Performance Prediction for Probabilistic Scaling Laws](https://arxiv.org/abs/2510.16743)
*Viktoria Schram,Markus Hiller,Daniel Beck,Trevor Cohn*

Main category: cs.LG

TL;DR: 本文提出了一种多任务学习方法，通过潜在变量多输出高斯过程预测NLP模型的学习曲线，降低计算成本并支持零样本预测。


<details>
  <summary>Details</summary>
Motivation: 预测NLP模型的学习曲线可以优化性能目标，减少计算开销和数据获取成本。

Method: 采用多任务学习框架，利用潜在变量多输出高斯过程建模任务间的共享信息和依赖关系。

Result: 实验表明，该方法能以较低成本生成概率性扩展规律，并通过主动学习策略降低预测不确定性。

Conclusion: 该方法在三个小型NLP数据集上验证有效，适用于不同规模和任务的模型。

Abstract: The prediction of learning curves for Natural Language Processing (NLP)
models enables informed decision-making to meet specific performance
objectives, while reducing computational overhead and lowering the costs
associated with dataset acquisition and curation. In this work, we formulate
the prediction task as a multitask learning problem, where each task's data is
modelled as being organized within a two-layer hierarchy. To model the shared
information and dependencies across tasks and hierarchical levels, we employ
latent variable multi-output Gaussian Processes, enabling to account for task
correlations and supporting zero-shot prediction of learning curves (LCs). We
demonstrate that this approach facilitates the development of probabilistic
scaling laws at lower costs. Applying an active learning strategy, LCs can be
queried to reduce predictive uncertainty and provide predictions close to
ground truth scaling laws. We validate our framework on three small-scale NLP
datasets with up to $30$ LCs. These are obtained from nanoGPT models, from
bilingual translation using mBART and Transformer models, and from multilingual
translation using M2M100 models of varying sizes.

</details>


### [292] [An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications](https://arxiv.org/abs/2510.16747)
*Danish Nazir,Gowtham Sai Inti,Timo Bartels,Jan Piewek,Thorsten Bagdonat,Tim Fingscheidt*

Main category: cs.LG

TL;DR: 提出了一种基于SegDeformer的联合特征和任务解码方法，显著降低了车载和分布式应用的计算复杂度，同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用SegDeformer在车载和分布式应用中降低计算复杂度，同时保持或提升语义分割的性能。

Method: 采用SegDeformer进行联合特征和任务解码，优化计算复杂度。

Result: 在车载应用中，帧率显著提升（Cityscapes上11.7倍，ADE20K上3.5倍）；在分布式应用中，以极少的云DNN参数实现了SOTA性能。

Conclusion: 该方法在降低计算复杂度的同时，显著提升了性能，适用于现代汽车系统的语义分割任务。

Abstract: Modern automotive systems leverage deep neural networks (DNNs) for semantic
segmentation and operate in two key application areas: (1) In-car, where the
DNN solely operates in the vehicle without strict constraints on the data rate.
(2) Distributed, where one DNN part operates in the vehicle and the other part
typically on a large-scale cloud platform with a particular constraint on
transmission bitrate efficiency. Typically, both applications share an image
and source encoder, while each uses distinct (joint) source and task decoders.
Prior work utilized convolutional neural networks for joint source and task
decoding but did not investigate transformer-based alternatives such as
SegDeformer, which offer superior performance at the cost of higher
computational complexity. In this work, we propose joint feature and task
decoding for SegDeformer, thereby enabling lower computational complexity in
both in-car and distributed applications, despite SegDeformer's computational
demands. This improves scalability in the cloud while reducing in-car
computational complexity. For the in-car application, we increased the frames
per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on
Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on
ADE20K, while being on-par w.r.t.\ the mean intersection over union (mIoU) of
the transformer-based baseline that doesn't compress by a source codec. For the
distributed application, we achieve state-of-the-art (SOTA) over a wide range
of bitrates on the mIoU metric, while using only $0.14$\% ($0.04$\%) of cloud
DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).

</details>


### [293] [SAMOSA: Sharpness Aware Minimization for Open Set Active learning](https://arxiv.org/abs/2510.16757)
*Young In Kim,Andrea Agiollo,Rajiv Khanna*

Main category: cs.LG

TL;DR: SAMOSA是一种基于典型性的开放集主动学习方法，通过选择信息丰富的样本提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 减少数据标注成本，解决开放集中无关或未知类别的样本选择问题。

Method: 利用典型性和锐度感知最小化（SAM）理论，主动查询接近决策边界的非典型样本。

Result: 在多个数据集上实现最高3%的准确率提升，且无额外计算开销。

Conclusion: SAMOSA在开放集主动学习中表现出色，有效平衡了样本选择的信息性和区分性。

Abstract: Modern machine learning solutions require extensive data collection where
labeling remains costly. To reduce this burden, open set active learning
approaches aim to select informative samples from a large pool of unlabeled
data that includes irrelevant or unknown classes. In this context, we propose
Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an
effective querying algorithm. Building on theoretical findings concerning the
impact of data typicality on the generalization properties of traditional
stochastic gradient descent (SGD) and sharpness-aware minimization (SAM),
SAMOSA actively queries samples based on their typicality. SAMOSA effectively
identifies atypical samples that belong to regions of the embedding manifold
close to the model decision boundaries. Therefore, SAMOSA prioritizes the
samples that are (i) highly informative for the targeted classes, and (ii)
useful for distinguishing between targeted and unwanted classes. Extensive
experiments show that SAMOSA achieves up to 3% accuracy improvement over the
state of the art across several datasets, while not introducing computational
overhead. The source code of our experiments is available at:
https://anonymous.4open.science/r/samosa-DAF4

</details>


### [294] [Learning to play: A Multimodal Agent for 3D Game-Play](https://arxiv.org/abs/2510.16774)
*Yuguang Yue,Irakli Salia,Samuel Hunt,Christopher Green,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.LG

TL;DR: 论文探讨了3D第一人称视频游戏作为实时多模态推理的挑战性环境，提出了一个更大、更多样化的数据集，并训练了一个基于文本指令的游戏代理。


<details>
  <summary>Details</summary>
Motivation: 3D第一人称游戏环境复杂，需要实时多模态推理能力，但目前公开的数据集规模有限且多样性不足。

Method: 收集了大规模多样化的游戏数据集，训练了逆动力学模型和行为克隆代理，支持实时推理。

Result: 模型能够玩多种3D游戏并响应文本指令，但仍面临长时任务和跨游戏定量评估的挑战。

Conclusion: 研究展示了在复杂游戏环境中实时多模态推理的潜力，但需进一步解决长时任务和评估问题。

Abstract: We argue that 3-D first-person video games are a challenging environment for
real-time multi-modal reasoning. We first describe our dataset of human
game-play, collected across a large variety of 3-D first-person games, which is
both substantially larger and more diverse compared to prior publicly disclosed
datasets, and contains text instructions. We demonstrate that we can learn an
inverse dynamics model from this dataset, which allows us to impute actions on
a much larger dataset of publicly available videos of human game play that lack
recorded actions. We then train a text-conditioned agent for game playing using
behavior cloning, with a custom architecture capable of realtime inference on a
consumer GPU. We show the resulting model is capable of playing a variety of
3-D games and responding to text input. Finally, we outline some of the
remaining challenges such as long-horizon tasks and quantitative evaluation
across a large set of games.

</details>


### [295] [3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding](https://arxiv.org/abs/2510.16780)
*Chang Wu,Zhiyuan Liu,Wen Shu,Liang Wang,Yanchen Luo,Wenqiang Lei,Yatao Bian,Junfeng Fang,Xiang Wang*

Main category: cs.LG

TL;DR: 3D-GSRD提出了一种选择性重掩码解码方法，用于3D分子图表示学习，避免了2D结构泄漏，同时在MD17基准测试中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D分子图表示学习中2D结构泄漏与提供足够2D上下文之间的矛盾。

Method: 提出选择性重掩码解码（SRD）与3D关系Transformer编码器结合结构无关解码器。

Result: 在MD17基准测试中，8个目标中有7个达到最新最优性能。

Conclusion: 3D-GSRD通过SRD和结构无关解码器提升了分子表示学习效果。

Abstract: Masked graph modeling (MGM) is a promising approach for molecular
representation learning (MRL).However, extending the success of re-mask
decoding from 2D to 3D MGM is non-trivial, primarily due to two conflicting
challenges: avoiding 2D structure leakage to the decoder, while still providing
sufficient 2D context for reconstructing re-masked atoms.To address these
challenges, we propose 3D-GSRD: a 3D Molecular Graph Auto-Encoder with
Selective Re-mask Decoding. The core innovation of 3D-GSRD lies in its
Selective Re-mask Decoding(SRD), which re-masks only 3D-relevant information
from encoder representations while preserving the 2D graph structures.This SRD
is synergistically integrated with a 3D Relational-Transformer(3D-ReTrans)
encoder alongside a structure-independent decoder. We analyze that SRD,
combined with the structure-independent decoder, enhances the encoder's role in
MRL. Extensive experiments show that 3D-GSRD achieves strong downstream
performance, setting a new state-of-the-art on 7 out of 8 targets in the widely
used MD17 molecular property prediction benchmark. The code is released at
https://github.com/WuChang0124/3D-GSRD.

</details>


### [296] [Mixed-Precision Quantization for Language Models: Techniques and Prospects](https://arxiv.org/abs/2510.16805)
*Mariam Rakka,Marios Fournarakis,Olga Krestinskaya,Jinane Bazzi,Khaled N. Salama,Fadi Kurdahi,Ahmed M. Eltawil,Mohammed E. Fouda*

Main category: cs.LG

TL;DR: 本文综述了混合精度量化（MXPLM）在大型语言模型中的应用，探讨了其效率与精度的平衡，并总结了当前研究进展与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模的扩大，计算和内存需求激增，量化技术成为减少模型大小和加速推理的关键方法。混合精度量化通过选择性分配精度，解决了低比特量化可能导致的精度下降问题。

Method: 文章回顾了量化基础，包括均匀和非均匀量化器、量化粒度及后训练量化方法，并对MXPLM框架按比特分配策略和精度配置进行分类比较。

Result: 比较分析显示MXPLM在困惑度、零样本任务性能和部署权衡方面的差异，并指出其在LM环境中的适用性与挑战。

Conclusion: 本文总结了混合精度量化的研究现状，提出了硬件感知设计、激活量化和大规模优化等未来方向，为相关研究提供了参考。

Abstract: The rapid scaling of language models (LMs) has resulted in unprecedented
computational, memory, and energy requirements, making their training and
deployment increasingly unsustainable. Quantization has emerged as an essential
compression technique to reduce model size, alleviate memory bottlenecks, and
accelerate inference. However, while uniform low-bit quantization (e.g., INT8,
INT4) provides significant efficiency gains, it can degrade accuracy in
sensitive components of transformer-based LMs. Mixed-precision quantization
offers a promising alternative by selectively allocating precision across
layers or within tensors to balance efficiency and accuracy. This survey
provides a comprehensive overview of Mixed-Precision quantization frameworks
for LMs (MXPLMs). We first review quantization fundamentals, including uniform
and non-uniform quantizers, quantization granularity, and methods widely used
in post-training quantization. We then categorize and compare recent MXPLM
frameworks according to their bit allocation strategies and precision
configurations across weights, activations, and key-value caches. A comparative
analysis highlights differences in perplexity, zero-shot task performance, and
deployment trade-offs. Furthermore, we contrast MXPLMs with earlier
mixed-precision quantization methods for deep neural networks, identifying
strategies that transfer and those that face challenges in the LM setting.
Finally, we summarize open issues and future directions, including
hardware-aware design, activation quantization, and scalable optimization
methods for billion-parameter models. By consolidating recent advances, this
work serves as a reference for understanding the current landscape and research
prospects of mixed-precision quantization for large-scale language models.

</details>


### [297] [Computational Budget Should Be Considered in Data Selection](https://arxiv.org/abs/2510.16806)
*Weilin Wan,Weizhong Zhang,Cheng Jin*

Main category: cs.LG

TL;DR: 提出了一种计算预算感知的数据选择方法（CADS），通过双层优化框架显著提升效率，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法忽略计算预算，导致在不同预算下无法稳定优于随机选择。

Method: 采用双层优化框架，内层在预算约束下训练模型，外层优化数据选择，并解决了Hessian矩阵估计和内层优化效率问题。

Result: 在视觉和语言基准测试中，性能提升高达14.42%。

Conclusion: 计算预算应作为数据选择的核心因素，CADS方法有效解决了预算约束下的数据选择问题。

Abstract: Data selection improves computational efficiency by choosing informative
subsets of training samples. However, existing methods ignore the compute
budget, treating data selection and importance evaluation independently of
compute budget constraints. Yet empirical studies show no algorithm can
consistently outperform others (or even random selection) across varying
budgets. We therefore argue that compute budget must be integral to
data-selection strategies, since different budgets impose distinct requirements
on data quantity, quality, and distribution for effective training. To this
end, we propose a novel Computational budget-Aware Data Selection (CADS) method
and naturally formulate it into a bilevel optimization framework, where the
inner loop trains the model within the constraints of the computational budget
on some selected subset of training data, while the outer loop optimizes data
selection based on model evaluation. Our technical contributions lie in
addressing two main challenges in solving this bilevel optimization problem:
the expensive Hessian matrix estimation for outer-loop gradients and the
computational burden of achieving inner-loop optimality during iterations. To
solve the first issue, we propose a probabilistic reparameterization strategy
and compute the gradient using a Hessian-free policy gradient estimator. To
address the second challenge, we transform the inner optimization problem into
a penalty term in the outer objective, further discovering that we only need to
estimate the minimum of a one-dimensional loss to calculate the gradient,
significantly improving efficiency. Extensive experiments show that our method
achieves performance gains of up to 14.42% over baselines in vision and
language benchmarks.

</details>


### [298] [Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads](https://arxiv.org/abs/2510.16807)
*Zhoutong Wu,Yuan Zhang,Yiming Dong,Chenheng Zhang,Cong Fang,Kun Yuan,Zhouchen Lin*

Main category: cs.LG

TL;DR: SkipV1Former是一种Transformer变体，通过重用第一层的Value头来增强表示并减少KV缓存，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在语言任务中表现出色，但扩展时资源消耗大。SkipV1Former旨在通过重用第一层的Value头来优化表示并减少KV缓存。

Method: 从第二层开始，每层重用第一层的一半Value头，另一半正常计算，减少Value投影和V缓存近50%。

Result: SkipV1Former在不同模型规模下减少约25%的KV缓存，同时提升困惑度表现。

Conclusion: SkipV1Former通过重用第一层Value头，有效减少资源消耗并提升性能，且可与现有方法结合进一步优化。

Abstract: Transformer models have driven breakthroughs across various language tasks by
their strong capability to learn rich contextual representations. Scaling them
to improve representation, however, often demands substantial memory and
compute costs, such as the Key-Value (KV) cache used during auto-regressive
decoding. Skip connections offer a promising way to improve representation
without bloating resource usage, yet most prior works either improve
expressivity while leaving KV costs unchanged, or reduce memory at the cost of
weaker representation. In this work, we propose SkipV1Former, a Transformer
variant that uses skip connections from the first layer's Value heads to
strengthen model representation and reduce KV cache. Specifically, from the
second block onward, each layer reuses half of its Value heads from the very
first layer, while computing the other half as usual-cutting Value projections
and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed
first-layer Values into deeper layers restores information lost to compression
and accelerates the model's implicit mesa-optimization-a key pattern of
Transformer in auto-regressive tasks. Empirically, across different model
scales, SkipV1Former delivers consistent reductions of approximately 25 \% in
KV cache while improving perplexity relative to standard Multi-Head Attention
(MHA) Transformers and some advanced variants. Moreover, we propose a recipe
for uptraining existing MHA Transformer checkpoints to SkipV1Former with only
10-15\% additional compute. Finally, SkipV1Former can seamlessly combine
advanced methods like Group-Query Attention and Multi-Latent Attention to
achieve further KV cache savings and performance improvement. When combined
with YOCO, it cuts KV cache size by nearly 50 \% while still improving
performance.

</details>


### [299] [Graph Learning is Suboptimal in Causal Bandits](https://arxiv.org/abs/2510.16811)
*Mohammad Shahverdikondori,Jalal Etesami,Negar Kiyavash*

Main category: cs.LG

TL;DR: 研究发现，在因果强盗问题中，学习父节点集并非最优策略，甚至可能与最小化遗憾冲突。


<details>
  <summary>Details</summary>
Motivation: 探讨在未知因果结构下，是否必须识别父节点集才能最小化遗憾。

Method: 通过理论证明和实验验证，提出绕过父节点恢复的算法。

Result: 证明父节点识别对遗憾最小化不必要，并提出接近最优的算法。

Conclusion: 父节点识别并非遗憾最小化的必要条件，新算法性能显著优于现有方法。

Abstract: We study regret minimization in causal bandits under causal sufficiency where
the underlying causal structure is not known to the agent. Previous work has
focused on identifying the reward's parents and then applying classic bandit
methods to them, or jointly learning the parents while minimizing regret. We
investigate whether such strategies are optimal. Somewhat counterintuitively,
our results show that learning the parent set is suboptimal. We do so by
proving that there exist instances where regret minimization and parent
identification are fundamentally conflicting objectives. We further analyze
both the known and unknown parent set size regimes, establish novel regret
lower bounds that capture the combinatorial structure of the action space.
Building on these insights, we propose nearly optimal algorithms that bypass
graph and parent recovery, demonstrating that parent identification is indeed
unnecessary for regret minimization. Experiments confirm that there exists a
large performance gap between our method and existing baselines in various
environments.

</details>


### [300] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 论文提出了一种半监督学习方法，用于解决考古预测建模中的标签稀缺问题，通过动态伪标签和CRF提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 考古预测建模中，已知遗址稀少且大部分区域未标记，导致标签稀缺问题。

Method: 采用半监督学习和正未标记（PU）策略，结合动态伪标签和CRF优化模型。

Result: 在DEM和卫星影像数据集上表现优异，性能与LAMAP相当，且Dice分数更高。

Conclusion: 半监督学习是解决稀疏标记景观中遗址预测的有效方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [301] [Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator](https://arxiv.org/abs/2510.16816)
*Ming Zhong,Zhenya Yan*

Main category: cs.LG

TL;DR: LANO是一种新型神经算子，通过代理机制实现线性复杂度和高精度，解决了传统注意力机制在可扩展性和准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的神经算子在处理大规模数据时面临计算复杂度高和准确性下降的问题，需要一种既能保持高精度又能线性扩展的方法。

Method: 提出线性注意力神经算子（LANO），通过引入少量代理令牌（M≪N）来全局协调N个令牌的交互，实现线性复杂度O(MNd)。

Result: LANO在理论上证明了其通用逼近性，并在实验中超越了现有最佳神经PDE求解器，平均精度提升19.5%。

Conclusion: LANO为科学机器学习提供了一种可扩展且高精度的基础，填补了线性复杂度和软注意力性能之间的空白。

Abstract: Neural operators offer a powerful data-driven framework for learning mappings
between function spaces, in which the transformer-based neural operator
architecture faces a fundamental scalability-accuracy trade-off: softmax
attention provides excellent fidelity but incurs quadratic complexity
$\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$,
while linear attention variants reduce cost to $\mathcal{O}(N d^2)$ but often
suffer significant accuracy degradation. To address the aforementioned
challenge, in this paper, we present a novel type of neural operators, Linear
Attention Neural Operator (LANO), which achieves both scalability and high
accuracy by reformulating attention through an agent-based mechanism. LANO
resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \ll
N)$ that mediate global interactions among $N$ tokens. This agent attention
mechanism yields an operator layer with linear complexity $\mathcal{O}(MN d)$
while preserving the expressive power of softmax attention. Theoretically, we
demonstrate the universal approximation property, thereby demonstrating
improved conditioning and stability properties. Empirically, LANO surpasses
current state-of-the-art neural PDE solvers, including Transolver with
slice-based softmax attention, achieving average $19.5\%$ accuracy improvement
across standard benchmarks. By bridging the gap between linear complexity and
softmax-level performance, LANO establishes a scalable, high-accuracy
foundation for scientific machine learning applications.

</details>


### [302] [Trace Regularity PINNs: Enforcing $\mathrm{H}^{\frac{1}{2}}(\partial Ω)$ for Boundary Data](https://arxiv.org/abs/2510.16817)
*Doyoon Kim,Junbin Song*

Main category: cs.LG

TL;DR: TRPINN是一种改进的物理信息神经网络，通过使用Sobolev-Slobodeckij范数$H^{1/2}(\partial \Omega)$来增强边界损失的计算，提高了收敛速度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 标准PINN在处理高振荡Dirichlet边界条件时可能失败，TRPINN旨在通过精确的$H^{1/2}(\partial \Omega)$范数提升性能。

Method: TRPINN通过计算半范数的理论必要部分减少计算成本，并避免离散化中的分母评估以增强稳定性。

Result: TRPINN在Laplace方程实验中表现出比标准PINN更好的性能，提升1到3个小数位的精度。

Conclusion: TRPINN在收敛速度和稳定性上优于标准PINN，尤其适用于复杂边界条件问题。

Abstract: We propose an enhanced physics-informed neural network (PINN), the Trace
Regularity Physics-Informed Neural Network (TRPINN), which enforces the
boundary loss in the Sobolev-Slobodeckij norm $H^{1/2}(\partial \Omega)$, the
correct trace space associated with $H^1(\Omega)$. We reduce computational cost
by computing only the theoretically essential portion of the semi-norm and
enhance convergence stability by avoiding denominator evaluations in the
discretization. By incorporating the exact $H^{1/2}(\partial \Omega)$ norm, we
show that the approximation converges to the true solution in the
$H^{1}(\Omega)$ sense, and, through Neural Tangent Kernel (NTK) analysis, we
demonstrate that TRPINN can converge faster than standard PINNs. Numerical
experiments on the Laplace equation with highly oscillatory Dirichlet boundary
conditions exhibit cases where TRPINN succeeds even when standard PINNs fail,
and show performance improvements of one to three decimal digits.

</details>


### [303] [Finding Manifolds With Bilinear Autoencoders](https://arxiv.org/abs/2510.16820)
*Thomas Dooms,Ward Gauderis*

Main category: cs.LG

TL;DR: 论文提出使用双线性自编码器分解表示成二次多项式，改进了解释性、聚类和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器依赖输入，解释性受限。多项式作为代数基元，可独立分析，适用于复杂结构。

Method: 采用双线性自编码器，将表示分解为二次多项式，并引入重要性排序、聚类和稀疏性改进。

Result: 实现了非线性但可分析的潜在表示，通过代数性质提升了解释性。

Conclusion: 这是通过代数性质实现非线性且可分析潜在表示的第一步。

Abstract: Sparse autoencoders are a standard tool for uncovering interpretable latent
representations in neural networks. Yet, their interpretation depends on the
inputs, making their isolated study incomplete. Polynomials offer a solution;
they serve as algebraic primitives that can be analysed without reference to
input and can describe structures ranging from linear concepts to complicated
manifolds. This work uses bilinear autoencoders to efficiently decompose
representations into quadratic polynomials. We discuss improvements that induce
importance ordering, clustering, and activation sparsity. This is an initial
step toward nonlinear yet analysable latents through their algebraic
properties.

</details>


### [304] [ProtoMol: Enhancing Molecular Property Prediction via Prototype-Guided Multimodal Learning](https://arxiv.org/abs/2510.16824)
*Yingxu Wang,Kunyu Zhang,Jiaxin Huang,Nan Yin,Siwei Liu,Eran Segal*

Main category: cs.LG

TL;DR: ProtoMol是一种原型引导的多模态框架，通过分层编码和跨模态注意力机制提升分子图与文本描述的整合与对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态交互和统一原型空间方面存在不足，限制了分子属性预测的准确性和可解释性。

Method: ProtoMol采用双分支分层编码器（GNN和Transformer）和层间双向跨模态注意力机制，构建共享原型空间。

Result: 在多个基准数据集上，ProtoMol优于现有方法，显著提升了分子属性预测性能。

Conclusion: ProtoMol通过细粒度整合和语义对齐，为多模态分子表示学习提供了更可靠和可解释的解决方案。

Abstract: Multimodal molecular representation learning, which jointly models molecular
graphs and their textual descriptions, enhances predictive accuracy and
interpretability by enabling more robust and reliable predictions of drug
toxicity, bioactivity, and physicochemical properties through the integration
of structural and semantic information. However, existing multimodal methods
suffer from two key limitations: (1) they typically perform cross-modal
interaction only at the final encoder layer, thus overlooking hierarchical
semantic dependencies; (2) they lack a unified prototype space for robust
alignment between modalities. To address these limitations, we propose
ProtoMol, a prototype-guided multimodal framework that enables fine-grained
integration and consistent semantic alignment between molecular graphs and
textual descriptions. ProtoMol incorporates dual-branch hierarchical encoders,
utilizing Graph Neural Networks to process structured molecular graphs and
Transformers to encode unstructured texts, resulting in comprehensive
layer-wise representations. Then, ProtoMol introduces a layer-wise
bidirectional cross-modal attention mechanism that progressively aligns
semantic features across layers. Furthermore, a shared prototype space with
learnable, class-specific anchors is constructed to guide both modalities
toward coherent and discriminative representations. Extensive experiments on
multiple benchmark datasets demonstrate that ProtoMol consistently outperforms
state-of-the-art baselines across a variety of molecular property prediction
tasks.

</details>


### [305] [DrivAerStar: An Industrial-Grade CFD Dataset for Vehicle Aerodynamic Optimization](https://arxiv.org/abs/2510.16857)
*Jiyan Qiu,Lyulin Kuang,Guan Wang,Yichen Xu,Leiyao Cui,Shaotong Fu,Yixin Zhu,Ruihua Zhang*

Main category: cs.LG

TL;DR: DrivAerStar是一个包含12,000个工业级CFD模拟的数据集，用于车辆空气动力学优化，显著提高了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法在车辆空气动力学优化中面临计算成本高或精度不足的问题，现有数据集存在分辨率低、组件缺失和验证误差大等缺陷。

Method: 通过STAR-CCM+软件生成高精度CFD模拟，使用FFD算法探索20个CAD参数，包括完整的发动机舱和冷却系统。

Result: 数据集验证误差低于1.04%，比现有数据集提高五倍，训练模型可在几分钟内完成原本需要数周的计算。

Conclusion: DrivAerStar首次将学术机器学习与工业CFD实践结合，为数据驱动的空气动力学优化设定了新标准，并展示了高保真物理模拟与AI结合的潜力。

Abstract: Vehicle aerodynamics optimization has become critical for automotive
electrification, where drag reduction directly determines electric vehicle
range and energy efficiency. Traditional approaches face an intractable
trade-off: computationally expensive Computational Fluid Dynamics (CFD)
simulations requiring weeks per design iteration, or simplified models that
sacrifice production-grade accuracy. While machine learning offers
transformative potential, existing datasets exhibit fundamental limitations --
inadequate mesh resolution, missing vehicle components, and validation errors
exceeding 5% -- preventing deployment in industrial workflows. We present
DrivAerStar, comprising 12,000 industrial-grade automotive CFD simulations
generated using $\text{STAR-CCM+}^\unicode{xAE}$ software. The dataset
systematically explores three vehicle configurations through 20 Computer Aided
Design (CAD) parameters via Free Form Deformation (FFD) algorithms, including
complete engine compartments and cooling systems with realistic internal
airflow. DrivAerStar achieves wind tunnel validation accuracy below 1.04% -- a
five-fold improvement over existing datasets -- through refined mesh strategies
with strict wall $y^+$ control. Benchmarks demonstrate that models trained on
this data achieve production-ready accuracy while reducing computational costs
from weeks to minutes. This represents the first dataset bridging academic
machine learning research and industrial CFD practice, establishing a new
standard for data-driven aerodynamic optimization in automotive development.
Beyond automotive applications, DrivAerStar demonstrates a paradigm for
integrating high-fidelity physics simulations with Artificial Intelligence (AI)
across engineering disciplines where computational constraints currently limit
innovation.

</details>


### [306] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一种受生物启发的框架，通过解决多共线性问题，显著减少训练时间，同时性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 直接利用预训练特征进行下游任务时，相似性匹配阶段常出现多共线性问题，且高级方法计算成本高。

Method: 提出Fly-CL框架，受果蝇嗅觉回路启发，兼容多种预训练模型，通过渐进解决多共线性实现高效相似性匹配。

Result: Fly-CL在多种网络架构和数据场景下表现优异，训练时间大幅减少，性能达到或超越现有方法。

Conclusion: Fly-CL通过生物启发设计有效解决了多共线性和计算效率问题，适用于实时低延迟应用。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [307] [Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning](https://arxiv.org/abs/2510.16882)
*Heming Zou,Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: UDS框架通过动态评分和过滤样本优化监督微调，兼顾数据效用和多样性，无需外部资源，显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在完整数据集上计算成本高且易过拟合或放大偏差，需高效数据筛选方法。现有方法忽视多样性、依赖外部资源且耗时。

Method: 提出UDS框架，利用核范数捕捉数据效用和样本内多样性，通过低维嵌入比较估计样本间多样性，无需外部资源。

Result: UDS在多个基准测试中优于现有方法，显著减少训练时间。

Conclusion: UDS为高效在线批量选择提供有效解决方案，兼顾效用与多样性，提升监督微调效率。

Abstract: Supervised fine-tuning (SFT) is a commonly used technique to adapt large
language models (LLMs) to downstream tasks. In practice, SFT on a full dataset
is computationally expensive and sometimes suffers from overfitting or bias
amplification. This facilitates the rise of data curation in SFT, which
prioritizes the most valuable data to optimze. This work studies the online
batch selection family that dynamically scores and filters samples during the
training process. However, existing popular methods often (i) rely merely on
the utility of data to select a subset while neglecting other crucial factors
like diversity, (ii) rely on external resources such as reference models or
validation sets, and (iii) incur extra training time over full-dataset
training. To address these limitations, this work develops \textbf{UDS
(Utility-Diversity Sampling)}, a framework for efficient online batch selection
in SFT. UDS leverages the nuclear norm of the logits matrix to capture both
data utility and intra-sample diversity, while estimating inter-sample
diversity through efficient low-dimensional embedding comparisons with a
lightweight memory buffer of historical samples. Such a design eliminates the
need for external resources and unnecessary backpropagation, securing
computational efficiency. Experiments on multiple benchmarks demonstrate that
UDS consistently outperforms state-of-the-art online batch selection methods
under varying data budgets, and significantly reduces training time compared to
full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.

</details>


### [308] [UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains](https://arxiv.org/abs/2510.16885)
*Duo Wang,Yuan Zuo,Guangyue Lu,Junjie Wu*

Main category: cs.LG

TL;DR: UniGTE是一个结合图结构和语义推理的框架，通过指令调优实现跨任务和跨领域的零样本图推理。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络固定标签空间和大型语言模型难以捕捉图结构的问题。

Method: 使用可学习的对齐标记和结构感知的图-文本注意力机制，结合冻结的LLM解码器进行预测和重构。

Result: 在节点分类、链接预测、图分类和图回归任务中取得零样本最优结果。

Conclusion: 紧密整合图结构与LLM语义可实现鲁棒且可迁移的图推理。

Abstract: Generalizing to unseen graph tasks without task-specific supervision is
challenging: conventional graph neural networks are typically tied to a fixed
label space, while large language models (LLMs) struggle to capture graph
structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework
that unifies structural and semantic reasoning. The encoder augments a
pretrained autoregressive LLM with learnable alignment tokens and a
structure-aware graph-text attention mechanism, enabling it to attend jointly
to a tokenized graph and a natural-language task prompt while remaining
permutation-invariant to node order. This yields compact, task-aware graph
representations. Conditioned solely on these representations, a frozen LLM
decoder predicts and reconstructs: it outputs the task answer and
simultaneously paraphrases the input graph in natural language. The
reconstruction objective regularizes the encoder to preserve structural cues.
UniGTE is instruction-tuned on five datasets spanning node-level, edge-level,
and graph-level tasks across diverse domains, yet requires no fine-tuning at
inference. It achieves new state-of-the-art zero-shot results on node
classification, link prediction, graph classification, and graph regression
under cross-task and cross-domain settings, demonstrating that tight
integration of graph structure with LLM semantics enables robust, transferable
graph reasoning.

</details>


### [309] [DeepChem Equivariant: SE(3)-Equivariant Support in an Open-Source Molecular Machine Learning Library](https://arxiv.org/abs/2510.16897)
*Jose Siguenza,Bharath Ramsundar*

Main category: cs.LG

TL;DR: 论文介绍了在DEEPCHEM中扩展支持SE(3)-等变神经网络，使科学家能够轻松构建、训练和评估模型。


<details>
  <summary>Details</summary>
Motivation: SE(3)-等变神经网络在分子应用中越来越重要，但现有工具需要较多深度学习或数学背景，且缺乏完整的训练流程。

Method: 扩展DEEPCHEM，提供即用型等变模型、完整训练流程和工具包，支持SE(3)-Transformer和Tensor Field Networks等模型。

Result: 实现了包含等变模型、训练流程和工具包的完整系统，支持应用和进一步开发SE(3)-等变模型。

Conclusion: 该扩展降低了使用SE(3)-等变模型的门槛，促进了其在科学领域的应用和发展。

Abstract: Neural networks that incorporate geometric relationships respecting SE(3)
group transformations (e.g. rotations and translations) are increasingly
important in molecular applications, such as molecular property prediction,
protein structure modeling, and materials design. These models, known as
SE(3)-equivariant neural networks, ensure outputs transform predictably with
input coordinate changes by explicitly encoding spatial atomic positions.
Although libraries such as E3NN [4] and SE(3)-TRANSFORMER [3 ] offer powerful
implementations, they often require substantial deep learning or mathematical
prior knowledge and lack complete training pipelines. We extend DEEPCHEM [ 13]
with support for ready-to-use equivariant models, enabling scientists with
minimal deep learning background to build, train, and evaluate models, such as
SE(3)-Transformer and Tensor Field Networks. Our implementation includes
equivariant models, complete training pipelines, and a toolkit of equivariant
utilities, supported with comprehensive tests and documentation, to facilitate
both application and further development of SE(3)-equivariant models.

</details>


### [310] [Adaptive Online Learning with LSTM Networks for Energy Price Prediction](https://arxiv.org/abs/2510.16898)
*Salih Salihoglu,Ibrahim Ahmed,Afshin Asadi*

Main category: cs.LG

TL;DR: 利用LSTM网络和自定义损失函数预测加州电力市场的日前电价，结合在线学习和多特征集成，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 电力市场参与者需要准确的电价预测以支持决策，现有方法在峰值时段和实时数据适应性上存在不足。

Method: 采用LSTM网络，结合历史电价、天气条件和能源结构等特征，引入包含MAE、JSD和平滑惩罚的自定义损失函数，并实施在线学习。

Result: 自定义损失函数和在线学习显著提升预测准确性，尤其在峰值时段，且能源结构特征的加入进一步增强了模型性能。

Conclusion: 研究为动态电力市场提供了强大的电价预测框架，支持更优决策。

Abstract: Accurate prediction of electricity prices is crucial for stakeholders in the
energy market, particularly for grid operators, energy producers, and
consumers. This study focuses on developing a predictive model leveraging Long
Short-Term Memory (LSTM) networks to forecast day-ahead electricity prices in
the California energy market. The model incorporates a variety of features,
including historical price data, weather conditions, and the energy generation
mix. A novel custom loss function that integrates Mean Absolute Error (MAE),
Jensen-Shannon Divergence (JSD), and a smoothness penalty is introduced to
enhance the prediction accuracy and interpretability. Additionally, an online
learning approach is implemented to allow the model to adapt to new data
incrementally, ensuring continuous relevance and accuracy. The results
demonstrate that the custom loss function can improve the model's performance,
aligning predicted prices more closely with actual values, particularly during
peak intervals. Also, the online learning model outperforms other models by
effectively incorporating real-time data, resulting in lower prediction error
and variability. The inclusion of the energy generation mix further enhances
the model's predictive capabilities, highlighting the importance of
comprehensive feature integration. This research provides a robust framework
for electricity price forecasting, offering valuable insights and tools for
better decision-making in dynamic electricity markets.

</details>


### [311] [SNOMED CT-powered Knowledge Graphs for Structured Clinical Data and Diagnostic Reasoning](https://arxiv.org/abs/2510.16899)
*Dun Liu,Qin Pang,Guangai Liu,Hongyu Mou,Jipeng Fan,Yiming Miao,Pin-Han Ho,Limei Peng*

Main category: cs.LG

TL;DR: 提出了一种基于知识驱动的框架，利用SNOMED CT和Neo4j构建结构化医学知识图谱，提升AI在医疗中的逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化临床文档导致的训练数据噪声和不一致问题。

Method: 整合SNOMED CT标准化术语与Neo4j图数据库，构建医学知识图谱，并生成结构化数据集用于微调大语言模型。

Result: 实验表明该方法显著提高了AI生成诊断推理的有效性和可解释性。

Conclusion: 该方法为构建可靠的AI辅助临床系统提供了可扩展的解决方案。

Abstract: The effectiveness of artificial intelligence (AI) in healthcare is
significantly hindered by unstructured clinical documentation, which results in
noisy, inconsistent, and logically fragmented training data. To address this
challenge, we present a knowledge-driven framework that integrates the
standardized clinical terminology SNOMED CT with the Neo4j graph database to
construct a structured medical knowledge graph. In this graph, clinical
entities such as diseases, symptoms, and medications are represented as nodes,
and semantic relationships such as ``caused by,'' ``treats,'' and ``belongs
to'' are modeled as edges in Neo4j, with types mapped from formal SNOMED CT
relationship concepts (e.g., \texttt{Causative agent}, \texttt{Indicated for}).
This design enables multi-hop reasoning and ensures terminological consistency.
By extracting and standardizing entity-relationship pairs from clinical texts,
we generate structured, JSON-formatted datasets that embed explicit diagnostic
pathways. These datasets are used to fine-tune large language models (LLMs),
significantly improving the clinical logic consistency of their outputs.
Experimental results demonstrate that our knowledge-guided approach enhances
the validity and interpretability of AI-generated diagnostic reasoning,
providing a scalable solution for building reliable AI-assisted clinical
systems.

</details>


### [312] [A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch](https://arxiv.org/abs/2510.16911)
*Sarah Al-Shareeda,Gulcihan Ozdemir,Heung Seok Jeon,Khaleel Ahmad*

Main category: cs.LG

TL;DR: 提出了一种轻量级深度学习流程，结合降采样、双模式填补和标准化，用于噪声和不完整数据下的短期能耗预测，取得了较高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传感器数据噪声大、不完整且缺乏上下文信息时，如何准确预测短期能耗的问题。

Method: 采用GRU-LSTM序列到一模型，结合小时降采样、双模式填补（均值和多项式回归）和标准化预处理。

Result: 模型平均RMSE为601.9W，MAE为468.9W，准确率84.36%，并能捕捉非线性需求模式。

Conclusion: 针对性预处理与紧凑循环架构结合，可在实际条件下实现快速、准确且可部署的能耗预测。

Abstract: How can short-term energy consumption be accurately forecasted when sensor
data is noisy, incomplete, and lacks contextual richness? This question guided
our participation in the \textit{2025 Competition on Electric Energy
Consumption Forecast Adopting Multi-criteria Performance Metrics}, which
challenged teams to predict next-day power demand using real-world
high-frequency data. We proposed a robust yet lightweight Deep Learning (DL)
pipeline combining hourly downsizing, dual-mode imputation (mean and polynomial
regression), and comprehensive normalization, ultimately selecting Standard
Scaling for optimal balance. The lightweight GRU-LSTM sequence-to-one model
achieves an average RMSE of 601.9~W, MAE of 468.9~W, and 84.36\% accuracy.
Despite asymmetric inputs and imputed gaps, it generalized well, captured
nonlinear demand patterns, and maintained low inference latency. Notably,
spatiotemporal heatmap analysis reveals a strong alignment between temperature
trends and predicted consumption, further reinforcing the model's reliability.
These results demonstrate that targeted preprocessing paired with compact
recurrent architectures can still enable fast, accurate, and deployment-ready
energy forecasting in real-world conditions.

</details>


### [313] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为域泛化持续学习（DGCL）的新设置，并提出了自适应域变换（DoT）方法来解决其挑战。


<details>
  <summary>Details</summary>
Motivation: 智能系统需要在动态环境中持续学习新技能并泛化到未见场景，现有持续学习方法在域泛化上表现不佳。

Method: 提出DoT方法，基于预训练模型，分离语义和域相关信息，并通过自适应变换实现输出对齐。

Result: DoT显著提升了现有持续学习方法的性能，并在资源效率上表现优异。

Conclusion: DoT是一种轻量级且高效的解决方案，适用于域泛化持续学习任务。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [314] [SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search](https://arxiv.org/abs/2510.16916)
*Dong Li,Xujiang Zhao,Linlin Yu,Yanchi Liu,Wei Cheng,Zhengzhang Chen,Zhong Chen,Feng Chen,Chen Zhao,Haifeng Chen*

Main category: cs.LG

TL;DR: SolverLLM是一个无需训练、基于测试时扩展的框架，通过生成数学公式并转化为求解器代码来解决多样化优化问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖提示工程或监督训练，泛化能力差且成本高，SolverLLM旨在解决这些问题。

Method: 采用蒙特卡洛树搜索（MCTS）策略，结合动态扩展、提示反向传播和不确定性反向传播来优化搜索过程。

Result: 在六个标准基准数据集上，SolverLLM表现优于基于提示和学习的基线方法。

Conclusion: SolverLLM无需额外训练即可实现强泛化能力，为优化问题提供了高效解决方案。

Abstract: Large Language Models (LLMs) offer promising capabilities for tackling
complex reasoning tasks, including optimization problems. However, existing
methods either rely on prompt engineering, which leads to poor generalization
across problem types, or require costly supervised training. We introduce
SolverLLM, a training-free framework that leverages test-time scaling to solve
diverse optimization problems. Rather than solving directly, SolverLLM
generates mathematical formulations and translates them into solver-ready code,
guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the
search process, we modify classical MCTS with (1) dynamic expansion for
adaptive formulation generation, (2) prompt backpropagation to guide
exploration via outcome-driven feedback, and (3) uncertainty backpropagation to
incorporate reward reliability into decision-making. Experiments on six
standard benchmark datasets demonstrate that SolverLLM outperforms both
prompt-based and learning-based baselines, achieving strong generalization
without additional training.

</details>


### [315] [Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws](https://arxiv.org/abs/2510.16927)
*Egor Petrov,Nikita Kiselev,Vladislav Meshkov,Andrey Grabovoy*

Main category: cs.LG

TL;DR: 论文填补了Transformer优化景观中Layer Normalization和前馈Hessians的理论空白，推导了二阶表达式，分析了子层在曲率传播中的作用，并提出了基于泰勒展开的框架。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer优化景观中Layer Normalization和前馈Hessians缺乏理论分析的问题。

Method: 推导二阶表达式，分析子层曲率传播，提出泰勒展开框架量化收敛轨迹。

Result: 完成了Transformer块的Hessian表征，揭示了子层在曲率传播中的作用，为大规模深度学习优化提供了理论基础。

Conclusion: 为大规模深度学习的理论和实证研究建立了新的基础。

Abstract: The lack of theoretical results for Layer Normalization and feedforward
Hessians has left a gap in the study of Transformer optimization landscapes. We
address this by deriving explicit second-order expressions for these
components, thereby completing the Hessian characterization of full Transformer
blocks. Our results generalize prior self-attention analyses and yield
estimations for the role of each sublayer in curvature propagation. We
demonstrate how these Hessian structures inform both convergence dynamics and
the empirical scaling laws governing large-model performance. Further, we
propose a Taylor-expansion-based framework for analyzing loss differences to
quantify convergence trajectories. By extending Hessian theory to the full
Transformer architecture, this work establishes a new foundation for
theoretical and empirical investigations of optimization in large-scale deep
learning.

</details>


### [316] [A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2510.16940)
*Cristian J. Vaca-Rubio,Roberto Pereira,Luis Blanco,Engin Zeydan,Màrius Caus*

Main category: cs.LG

TL;DR: P-KAN是一种新型概率时间序列预测模型，通过参数化预测分布和基于样条的连接，优于传统MLP，适用于卫星通信等领域。


<details>
  <summary>Details</summary>
Motivation: 传统KAN模型缺乏概率表达能力，无法直接建模预测不确定性，而P-KAN通过概率扩展解决了这一问题。

Method: 用样条函数替代标量权重，直接参数化高斯和Student-t分布，构建参数高效的非线性模型。

Result: 在卫星流量预测中，P-KAN在准确性和校准性上均优于MLP，且参数更少，能平衡效率与风险。

Conclusion: P-KAN为概率预测提供了强大框架，尤其适用于资源受限领域，如卫星通信。

Abstract: This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel
probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series
forecasting. By replacing scalar weights with spline-based functional
connections and directly parameterizing predictive distributions, P-KANs offer
expressive yet parameter-efficient models capable of capturing nonlinear and
heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting,
where uncertainty-aware predictions enable dynamic thresholding for resource
allocation. Results show that P-KANs consistently outperform Multi Layer
Perceptron (MLP) baselines in both accuracy and calibration, achieving superior
efficiency-risk trade-offs while using significantly fewer parameters. We build
up P-KANs on two distributions, namely Gaussian and Student-t distributions.
The Gaussian variant provides robust, conservative forecasts suitable for
safety-critical scenarios, whereas the Student-t variant yields sharper
distributions that improve efficiency under stable demand. These findings
establish P-KANs as a powerful framework for probabilistic forecasting with
direct applicability to satellite communications and other resource-constrained
domains.

</details>


### [317] [Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation](https://arxiv.org/abs/2510.16943)
*Dania Refai,Moataz Ahmed*

Main category: cs.LG

TL;DR: 论文提出了一个组件级评估框架，用于评估LLM生成的数学优化公式，并分析了不同模型和提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法过于粗糙，无法捕捉结构或数值错误，需要更精细的评估框架。

Method: 引入多种指标（如变量和约束的精确率、召回率、RMSE等），评估GPT-5、LLaMA 3.1 Instruct和DeepSeek Math在不同提示策略下的表现。

Result: GPT-5表现最佳，约束召回率和低约束RMSE是关键，简洁输出提升计算效率。

Conclusion: 提出了NLP到优化建模的三原则，框架为LLM在优化建模中的细粒度评估奠定了基础。

Abstract: Large language models (LLMs) are increasingly used to convert natural
language descriptions into mathematical optimization formulations. Current
evaluations often treat formulations as a whole, relying on coarse metrics like
solution accuracy or runtime, which obscure structural or numerical errors. In
this study, we present a comprehensive, component-level evaluation framework
for LLM-generated formulations. Beyond the conventional optimality gap, our
framework introduces metrics such as precision and recall for decision
variables and constraints, constraint and objective root mean squared error
(RMSE), and efficiency indicators based on token usage and latency. We evaluate
GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of
varying complexity under six prompting strategies. Results show that GPT-5
consistently outperforms other models, with chain-of-thought, self-consistency,
and modular prompting proving most effective. Analysis indicates that solver
performance depends primarily on high constraint recall and low constraint
RMSE, which together ensure structural correctness and solution reliability.
Constraint precision and decision variable metrics play secondary roles, while
concise outputs enhance computational efficiency. These findings highlight
three principles for NLP-to-optimization modeling: (i) Complete constraint
coverage prevents violations, (ii) minimizing constraint RMSE ensures
solver-level accuracy, and (iii) concise outputs improve computational
efficiency. The proposed framework establishes a foundation for fine-grained,
diagnostic evaluation of LLMs in optimization modeling.

</details>


### [318] [Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction](https://arxiv.org/abs/2510.16958)
*Ganglin Tian,Anastase Alexandre Charantonis,Camille Le Coz,Alexis Tantet,Riwal Plougonven*

Main category: cs.LG

TL;DR: 研究通过概率深度学习方法改进次季节预报中风速降尺度的不确定性空间表示。


<details>
  <summary>Details</summary>
Motivation: 次季节预报依赖大尺度大气预测因子（如Z500），但现有随机扰动方法无法充分表示空间相关性和物理一致性。

Method: 评估三种概率方法：分位数回归神经网络、变分自编码器和扩散模型，用于风速降尺度。

Result: 概率降尺度方法比简单随机方法提供更真实的空间不确定性表示，各模型在集合离散度、确定性技能和物理一致性方面表现不同。

Conclusion: 概率降尺度是改进次季节风速预报的有效方法，适用于可再生能源规划和风险评估。

Abstract: This study aims to improve the spatial representation of uncertainties when
regressing surface wind speeds from large-scale atmospheric predictors for
sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale
atmospheric predictors such as 500 hPa geopotential height (Z500), which
exhibit higher predictability than surface variables and can be downscaled to
obtain more localised information. Previous work by Tian et al. (2024)
demonstrated that stochastic perturbations based on model residuals can improve
ensemble dispersion representation in statistical downscaling frameworks, but
this method fails to represent spatial correlations and physical consistency
adequately. More sophisticated approaches are needed to capture the complex
relationships between large-scale predictors and local-scale predictands while
maintaining physical consistency. Probabilistic deep learning models offer
promising solutions for capturing complex spatial dependencies. This study
evaluates three probabilistic methods with distinct uncertainty quantification
mechanisms: Quantile Regression Neural Network that directly models
distribution quantiles, Variational Autoencoders that leverage latent space
sampling, and Diffusion Models that utilise iterative denoising. These models
are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts
to regress probabilistic wind speed ensembles. Our results show that
probabilistic downscaling approaches provide more realistic spatial uncertainty
representations compared to simpler stochastic methods, with each probabilistic
model offering different strengths in terms of ensemble dispersion,
deterministic skill, and physical consistency. These findings establish
probabilistic downscaling as an effective enhancement to operational
sub-seasonal wind forecasts for renewable energy planning and risk assessment.

</details>


### [319] [Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures](https://arxiv.org/abs/2510.16968)
*Pingzhi Li,Morris Yu-Chao Huang,Zhen Tan,Qingquan Song,Jie Peng,Kai Zou,Yu Cheng,Kaidi Xu,Tianlong Chen*

Main category: cs.LG

TL;DR: 提出了一种基于MoE结构习惯的知识蒸馏检测框架，适用于白盒和黑盒场景，检测准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏检测方法易被提示工程规避，需更鲁棒的检测手段。

Method: 利用MoE内部路由模式的结构习惯，提出白盒和黑盒（Shadow-MoE）检测方法。

Result: 检测准确率高（>94%），对提示工程鲁棒，优于现有基线。

Conclusion: MoE结构习惯是知识蒸馏检测的有效信号，框架可扩展性强。

Abstract: Knowledge Distillation (KD) accelerates training of large language models
(LLMs) but poses intellectual property protection and LLM diversity risks.
Existing KD detection methods based on self-identity or output similarity can
be easily evaded through prompt engineering. We present a KD detection
framework effective in both white-box and black-box settings by exploiting an
overlooked signal: the transfer of MoE "structural habits", especially internal
routing patterns. Our approach analyzes how different experts specialize and
collaborate across various inputs, creating distinctive fingerprints that
persist through the distillation process. To extend beyond the white-box setup
and MoE architectures, we further propose Shadow-MoE, a black-box method that
constructs proxy MoE representations via auxiliary distillation to compare
these patterns between arbitrary model pairs. We establish a comprehensive,
reproducible benchmark that offers diverse distilled checkpoints and an
extensible framework to facilitate future research. Extensive experiments
demonstrate >94% detection accuracy across various scenarios and strong
robustness to prompt-based evasion, outperforming existing baselines while
highlighting the structural habits transfer in LLMs.

</details>


### [320] [Differentially Private Linear Regression and Synthetic Data Generation with Statistical Guarantees](https://arxiv.org/abs/2510.16974)
*Shurong Lin,Aleksandra Slavković,Deekshith Reddy Bhoomireddy*

Main category: cs.LG

TL;DR: 提出了一种在高斯差分隐私（DP）下进行线性回归（LR）的方法，支持有效推断和合成数据生成（SDG），适用于社会科学中的中小规模数据集。


<details>
  <summary>Details</summary>
Motivation: 社会科学中常见中小规模数据集，现有DP LR方法多关注点估计，缺乏不确定性量化和SDG支持，主流SDG方法对连续回归数据适应性不足。

Method: 提出DP偏置校正估计器，支持渐近置信区间（CIs），并设计通用SDG流程，确保合成数据回归与DP回归一致。采用分箱-聚合策略适应中小维度场景。

Result: 实验表明，该方法（1）准确性优于现有方法，（2）提供有效CIs，（3）生成更可靠的合成数据，适用于下游ML任务。

Conclusion: 该方法填补了DP LR在不确定性量化和SDG支持上的空白，适用于社会科学中的中小规模连续数据，提升了推断和合成数据的可靠性。

Abstract: In social sciences, small- to medium-scale datasets are common and linear
regression (LR) is canonical. In privacy-aware settings, much work has focused
on differentially private (DP) LR, but mostly on point estimation with limited
attention to uncertainty quantification. Meanwhile, synthetic data generation
(SDG) is increasingly important for reproducibility studies, yet current DP LR
methods do not readily support it. Mainstream SDG approaches are either
tailored to discretized data, making them less suitable for continuous
regression, or rely on deep models that require large datasets, limiting their
use for the smaller, continuous data typical in social science. We propose a
method for LR with valid inference under Gaussian DP: a DP bias-corrected
estimator with asymptotic confidence intervals (CIs) and a general SDG
procedure in which regression on the synthetic data matches our DP regression.
Our binning-aggregation strategy is effective in small- to moderate-dimensional
settings. Experiments show our method (1) improves accuracy over existing
methods, (2) provides valid CIs, and (3) produces more reliable synthetic data
for downstream ML tasks than current DP SDGs.

</details>


### [321] [Towards Interpretable and Trustworthy Time Series Reasoning: A BlueSky Vision](https://arxiv.org/abs/2510.16980)
*Kanghui Ning,Zijie Pan,Yushan Jiang,Anderson Schneider,Yuriy Nevmyvaka,Dongjin Song*

Main category: cs.LG

TL;DR: 论文提出了一个时间序列推理的框架，结合基础理论和系统级方法，旨在实现可解释和可信赖的时序智能。


<details>
  <summary>Details</summary>
Motivation: 时间序列推理是时序分析的新前沿，目标是超越模式识别，实现明确、可解释和可信赖的推理。

Method: 通过两个互补方向：一是构建时间序列推理的稳健基础，包括全面时序理解、结构化多步推理和可信评估框架；二是推进系统级推理，结合多智能体协作、多模态上下文和检索增强方法。

Result: 提出了一个灵活可扩展的框架，用于推进时间序列推理。

Conclusion: 该框架旨在为多样领域提供可解释和可信赖的时序智能。

Abstract: Time series reasoning is emerging as the next frontier in temporal analysis,
aiming to move beyond pattern recognition towards explicit, interpretable, and
trustworthy inference. This paper presents a BlueSky vision built on two
complementary directions. One builds robust foundations for time series
reasoning, centered on comprehensive temporal understanding, structured
multi-step reasoning, and faithful evaluation frameworks. The other advances
system-level reasoning, moving beyond language-only explanations by
incorporating multi-agent collaboration, multi-modal context, and
retrieval-augmented approaches. Together, these directions outline a flexible
and extensible framework for advancing time series reasoning, aiming to deliver
interpretable and trustworthy temporal intelligence across diverse domains.

</details>


### [322] [MuonBP: Faster Muon via Block-Periodic Orthogonalization](https://arxiv.org/abs/2510.16981)
*Ahmed Khaled,Kaan Ozkara,Tao Yu,Mingyi Hong,Youngsuk Park*

Main category: cs.LG

TL;DR: MuonBP通过块周期性正交化减少梯度正交化的通信开销，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 梯度正交化在模型并行中引入额外通信开销，影响训练效率。

Method: 提出MuonBP，独立对设备上的矩阵分片进行正交化，并周期性执行全正交化。

Result: 在8B模型训练中，MuonBP比Muon提升8%的吞吐量，性能无下降。

Conclusion: MuonBP在保持训练稳定性的同时，显著提升了训练效率。

Abstract: Gradient orthogonalization is a simple strategy that shows great utility in
speeding up gradient descent. The Muon optimizer (Jordan, Jin, et al., 2024)
combines gradient orthogonalization with first-order momentum and achieves
significant improvement in data efficiency over Adam/AdamW (Loshchilov and
Hutter, 2019) for language model training. However, when using model
parallelism, gradient orthogonalization introduces additional overhead compared
to coordinate-wise optimizers (such as AdamW) due to additional gather and
scatter operations on gradient matrix shards from different devices. This
additional communication can amount to a throughput hit of 5%-10% compared to
Adam/AdamW. To remedy this, we propose Muon with Block-Periodic
Orthogonalization (MuonBP), which applies orthogonalization independently to
matrix shards on each device and periodically performs full orthogonalization
to maintain training stability at scale. We show how to adjust the learning
rate from the baseline to MuonBP and give convergence guarantees for this
algorithm. Crucially, our theory dictates that we use two stepsizes: one for
the blockwise orthogonalization steps, and one for the full orthogonalization
steps. Our method is simple, requires minimal hyperparameter adjustments, and
achieves competitive iteration complexity compared with baseline Muon while
providing per-iteration throughput comparable to coordinate-wise methods such
as AdamW. When training an 8B model with eight-way tensor parallelism and ZeRO
optimizer state sharding, MuonBP achieves 8% throughput increase compared to
Muon with no degradation in performance.

</details>


### [323] [Graph4MM: Weaving Multimodal Learning with Structural Information](https://arxiv.org/abs/2510.16990)
*Xuying Ning,Dongqi Fu,Tianxin Wei,Wujiang Xu,Jingrui He*

Main category: cs.LG

TL;DR: Graph4MM框架通过Hop-Diffused Attention和MM-QFormer解决多模态学习中多跳邻居和模态融合问题，实验显示性能提升6.93%。


<details>
  <summary>Details</summary>
Motivation: 现实多模态数据具有复杂结构关系，传统方法无法区分多跳邻居且将图视为独立模态，限制了理解。

Method: 提出Graph4MM框架，包含Hop-Diffused Attention（整合多跳结构信息）和MM-QFormer（跨模态融合）。

Result: 在生成和判别任务中，Graph4MM优于现有方法，平均提升6.93%。

Conclusion: 利用图结构整合模态内外交互可提升多模态理解，Graph4MM为有效解决方案。

Abstract: Real-world multimodal data usually exhibit complex structural relationships
beyond traditional one-to-one mappings like image-caption pairs. Entities
across modalities interact in intricate ways, with images and text forming
diverse interconnections through contextual dependencies and co-references.
Graphs provide powerful structural information for modeling intra-modal and
inter-modal relationships. However, previous works fail to distinguish
multi-hop neighbors and treat the graph as a standalone modality, which
fragments the overall understanding. This limitation presents two key
challenges in multimodal learning: (1) integrating structural information from
multi-hop neighbors into foundational models, and (2) fusing modality-specific
information in a principled manner. To address these challenges, we revisit the
role of graphs in multimodal learning within the era of foundation models and
propose Graph4MM, a graph-based multimodal learning framework. To be specific,
we introduce Hop-Diffused Attention, which integrates multi-hop structural
information into self-attention through causal masking and hop diffusion.
Furthermore, we design MM-QFormer, a multi-mapping querying transformer for
cross-modal fusion. Through theoretical and empirical analysis, we show that
leveraging structures to integrate both intra- and inter-modal interactions
improves multimodal understanding beyond treating them as a standalone
modality. Experiments on both generative and discriminative tasks show that
Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines,
achieving a 6.93% average improvement.

</details>


### [324] [EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](https://arxiv.org/abs/2510.17002)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEschematic是一个基于多模态大语言模型（MLLM）的AI代理，用于将SPICE网表转换为可编辑的电路图，提升视觉可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的电路设计方法依赖文本表示（如SPICE网表），缺乏视觉可解释性，限制了设计师的理解和验证。

Method: EEschematic整合文本、视觉和符号模态，采用少量示例和视觉思维链（VCoT）策略迭代优化布局和布线。

Result: 实验表明，EEschematic生成的电路图在视觉质量和结构正确性上表现优异。

Conclusion: EEschematic通过多模态方法有效解决了电路图的视觉可解释性问题，为电路设计提供了更直观的工具。

Abstract: Circuit schematics play a crucial role in analog integrated circuit design,
serving as the primary medium for human understanding and verification of
circuit functionality. While recent large language model (LLM)-based approaches
have shown promise in circuit topology generation and device sizing, most rely
solely on textual representations such as SPICE netlists, which lack visual
interpretability for circuit designers. To address this limitation, we propose
EEschematic, an AI agent for automatic analog schematic generation based on a
Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual,
and symbolic modalities to translate SPICE netlists into schematic diagrams
represented in a human-editable format. The framework uses six analog
substructure examples for few-shot placement and a Visual Chain-of-Thought
(VCoT) strategy to iteratively refine placement and wiring, enhancing schematic
clarity and symmetry. Experimental results on representative analog circuits,
including a CMOS inverter, a five-transistor operational transconductance
amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that
EEschematic produces schematics with high visual quality and structural
correctness.

</details>


### [325] [Justitia: Fair and Efficient Scheduling for LLM Applications](https://arxiv.org/abs/2510.17015)
*Mingyan Yang,Guanjie Wang,Manqi Luo,Yifei Liu,Chen Chen,Han Zhao,Yu Feng,Quan Chen,Minyi Guo*

Main category: cs.LG

TL;DR: Justitia是一个新型调度器，旨在公平高效地服务LLM应用，通过内存中心化建模、轻量级需求预测和虚拟时间公平队列算法提升调度效率。


<details>
  <summary>Details</summary>
Motivation: 主流LLM调度器因资源分配问题无法高效服务LLM应用，需设计兼顾公平与效率的解决方案。

Method: Justitia采用内存中心化建模、轻量级神经网络需求预测和虚拟时间公平队列算法。

Result: 实验表明，Justitia显著提升调度效率并保持公平性。

Conclusion: Justitia为LLM应用提供了一种高效且公平的调度方案。

Abstract: In the era of Large Language Models (LLMs), it has been popular to launch a
series of LLM inferences -- we call an LLM application -- to better solve
real-world problems. When serving those applications in shared GPU servers, the
schedulers are expected to attain fast application completions with guaranteed
worst-case performance. However, mainstream LLM schedulers fail to behave well
for LLM applications -- due to head-of-line blocking or over-constrained
resource allocation. In this paper, we propose to serve LLM applications in a
fair and also efficient manner. To this end, we design Justitia, a novel
scheduler with three key techniques. First, given that memory is prevalently a
bottleneck for mainstream inference frameworks like vLLM, Justitia models the
service cost of LLM applications in a memory-centric manner. Meanwhile, it uses
a simple neural network model to conduct light-weight and also accurate demand
prediction. Moreover, Justitia adopts a virtual-time based fair queuing
algorithm to reduce the overall performance with guaranteed worst-case delay.
We have implemented Justitia atop vLLM, and experimental results involving
diverse LLM applications show that it can substantially enhance the scheduling
efficiency with fairness preserved.

</details>


### [326] [Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning](https://arxiv.org/abs/2510.17021)
*Bingqi Shang,Yiwei Chen,Yihua Zhang,Bingquan Shen,Sijia Liu*

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLM）的“反学习”过程可能被植入后门攻击的问题，揭示了注意力机制中的“注意力汇”现象如何增强后门效果。


<details>
  <summary>Details</summary>
Motivation: 随着开源权重的LLM增多，研究反学习过程是否会被后门攻击是一个重要问题，旨在揭示潜在的安全风险。

Method: 通过分析注意力机制中的“注意力汇”现象，设计后门攻击策略，并在实验中验证其有效性。

Result: 实验表明，基于“注意力汇”的后门攻击能有效恢复被遗忘的知识，同时在无触发条件下表现正常。

Conclusion: 论文揭示了反学习过程中的后门风险，强调了注意力机制在安全研究中的重要性。

Abstract: Large language model (LLM) unlearning has become a critical mechanism for
removing undesired data, knowledge, or behaviors from pre-trained models while
retaining their general utility. Yet, with the rise of open-weight LLMs, we
ask: can the unlearning process itself be backdoored, appearing successful
under normal conditions yet reverting to pre-unlearned behavior when a hidden
trigger is activated? Drawing inspiration from classical backdoor attacks that
embed triggers into training data to enforce specific behaviors, we investigate
backdoor unlearning, where models forget as intended in the clean setting but
recover forgotten knowledge when the trigger appears. We show that designing
such attacks presents unique challenges, hinging on where triggers are placed
and how backdoor training is reinforced. We uncover a strong link between
backdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens
consistently attract disproportionate attention in LLMs. Our analysis reveals
that these attention sinks serve as gateways for backdoor unlearning: placing
triggers at sink positions and aligning their attention values markedly
enhances backdoor persistence. Extensive experiments validate these findings,
showing that attention-sink-guided backdoor unlearning reliably restores
forgotten knowledge in the presence of backdoor triggers, while behaving
indistinguishably from a normally unlearned model when triggers are absent.
Code is available at https://github.com/OPTML-Group/Unlearn-Backdoor.

</details>


### [327] [Curiosity-driven RL for symbolic equation solving](https://arxiv.org/abs/2510.17022)
*Kevin P. O Keeffe*

Main category: cs.LG

TL;DR: RL（强化学习）结合好奇心探索和图基动作可解决非线性方程。


<details>
  <summary>Details</summary>
Motivation: 探索RL在符号数学中的潜在应用，尤其是解决非线性方程的能力。

Method: 使用无模型PPO算法，结合好奇心探索和图基动作。

Result: 成功解决了涉及根号、指数和三角函数的非线性方程。

Conclusion: 好奇心探索可能对一般符号推理任务有益。

Abstract: We explore if RL can be useful for symbolic mathematics. Previous work showed
contrastive learning can solve linear equations in one variable. We show
model-free PPO \cite{schulman2017proximal} augmented with curiosity-based
exploration and graph-based actions can solve nonlinear equations such as those
involving radicals, exponentials, and trig functions. Our work suggests
curiosity-based exploration may be useful for general symbolic reasoning tasks.

</details>


### [328] [Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation](https://arxiv.org/abs/2510.17036)
*Nguyen Do,Bach Ngo,Youval Kashuv,Canh V. Pham,Hanghang Tong,My T. Thai*

Main category: cs.LG

TL;DR: 提出了一种名为PIMMA的自增强生成框架，用于解决非线性边权重函数下的服务质量退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法直接处理非线性边权重函数下的QoSD问题，PIMMA填补了这一空白。

Method: PIMMA包含三个阶段：Forge（预测路径压力算法）、Morph（基于能量的条件VAE混合模型训练）和Refine（强化学习优化）。

Result: 在合成和真实网络上的实验表明，PIMMA在非线性成本函数场景下优于传统和机器学习基线。

Conclusion: PIMMA为非线性QoSD问题提供了高效且可扩展的解决方案。

Abstract: We study the Quality of Service Degradation (QoSD) problem, in which an
adversary perturbs edge weights to degrade network performance. This setting
arises in both network infrastructures and distributed ML systems, where
communication quality, not just connectivity, determines functionality. While
classical methods rely on combinatorial optimization, and recent ML approaches
address only restricted linear variants with small-size networks, no prior
model directly tackles the QoSD problem under nonlinear edge-weight functions.
This work proposes \PIMMA, a self-reinforcing generative framework that
synthesizes feasible solutions in latent space, to fill this gap. Our method
includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm
that uses graph learning and approximation to produce feasible solutions with
performance guarantee, (2) Morph: a new theoretically grounded training
paradigm for Mixture of Conditional VAEs guided by an energy-based model to
capture solution feature distributions, and (3) Refine: a reinforcement
learning agent that explores this space to generate progressively near-optimal
solutions using our designed differentiable reward function. Experiments on
both synthetic and real-world networks show that our approach consistently
outperforms classical and ML baselines, particularly in scenarios with
nonlinear cost functions where traditional methods fail to generalize.

</details>


### [329] [Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability](https://arxiv.org/abs/2510.17040)
*Hoang-Son Nguyen,Xiao Fu*

Main category: cs.LG

TL;DR: DICA框架通过最大化雅可比矩阵体积（J-VolMax）实现非线性混合中潜在组件的识别，无需辅助信息或独立性假设。


<details>
  <summary>Details</summary>
Motivation: 解决非线性混合中潜在组件识别的挑战，扩展可识别性分析的范围。

Method: 提出DICA框架和J-VolMax准则，利用混合函数雅可比矩阵的凸几何特性。

Result: 在合理条件下实现潜在组件的可识别性，无需依赖辅助信息或雅可比稀疏性假设。

Conclusion: DICA为现有方法提供了互补视角，扩展了非线性独立成分分析的应用范围。

Abstract: Latent component identification from unknown nonlinear mixtures is a
foundational challenge in machine learning, with applications in tasks such as
disentangled representation learning and causal inference. Prior work in
nonlinear independent component analysis (nICA) has shown that auxiliary
signals -- such as weak supervision -- can support identifiability of
conditionally independent latent components. More recent approaches explore
structural assumptions, e.g., sparsity in the Jacobian of the mixing function,
to relax such requirements. In this work, we introduce Diverse Influence
Component Analysis (DICA), a framework that exploits the convex geometry of the
mixing function's Jacobian. We propose a Jacobian Volume Maximization
(J-VolMax) criterion, which enables latent component identification by
encouraging diversity in their influence on the observed variables. Under
reasonable conditions, this approach achieves identifiability without relying
on auxiliary information, latent component independence, or Jacobian sparsity
assumptions. These results extend the scope of identifiability analysis and
offer a complementary perspective to existing methods.

</details>


### [330] [The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs](https://arxiv.org/abs/2510.17057)
*Nikolaus Howe,Micah Carroll*

Main category: cs.LG

TL;DR: 研究探讨了强化学习与思维链推理结合时，模型在指令冲突下产生的系统性动机推理行为，并指出前沿模型能检测到这种行为，但小型模型可能无法识别，甚至被说服。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在指令冲突时的推理行为，特别是动机推理现象，以评估其对模型监督的影响。

Method: 在简单设置中研究模型行为，分析动机推理的生成及其检测能力。

Result: 发现动机推理可被前沿模型检测，但小型模型可能失效，甚至被说服，凸显监督挑战。

Conclusion: 强调在依赖思维链进行模型评估和监督时，需考虑动机推理的影响。

Abstract: The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning
has emerged as a promising approach for developing more capable language
models. In turn, this has led to investigation of CoT monitoring as a
compelling method for detecting harmful behaviors such as reward hacking, under
the assumption that models' reasoning processes reflect their internal
decision-making. In practice, LLM training often produces unintended behaviors
due to imperfect reward signals, leading models to develop misaligned
tendencies. A common corrective approach is to apply post-hoc instructions to
avoid problematic behaviors like sycophancy, but what happens to the model's
reasoning process when these instructions conflict with learned behaviors? We
investigate this question in simple settings and find that models engage in
systematic motivated reasoning -- generating plausible-sounding justifications
for violating their instructions while downplaying potential harms. Beyond
being an interesting property of training, we find that while motivated
reasoning can be detected by most frontier reasoning models, smaller LLM judges
can fail to identify a portion of it, and in rare cases can themselves be
persuaded that the reasoning is correct, despite it contradicting clear
instructions. This capability gap raises concerns that as models become more
sophisticated, their motivated reasoning may become increasingly difficult for
monitors to detect. Our results underscore the need to account for motivated
reasoning when relying on chain-of-thought processes for model evaluation and
oversight. All code for this paper will be made available. WARNING: some
examples in this paper may be upsetting.

</details>


### [331] [Bitwidth-Specific Logarithmic Arithmetic for Future Hardware-Accelerated Training](https://arxiv.org/abs/2510.17058)
*Hassan Hamad,Yuou Qiu,Peter A. Beerel,Keith M. Chugg*

Main category: cs.LG

TL;DR: 论文提出了一种低精度对数定点训练方法，通过硬件友好的分段线性近似和模拟退火优化，显著减少了训练的计算成本和硬件资源消耗。


<details>
  <summary>Details</summary>
Motivation: 深度学习训练仍依赖复杂的浮点运算，低精度定点训练是一种有潜力的替代方案，但需要更高效的硬件加速设计。

Method: 提出了一种基于位宽设计的对数加法近似方法，使用分段线性近似和模拟退火优化，并在C++中进行位真模拟验证。

Result: 在VGG-11和VGG-16模型上，使用12位整数运算训练，精度接近32位浮点训练，硬件资源消耗显著降低。

Conclusion: 低精度对数定点训练在保持精度的同时，显著减少了硬件面积和能耗，适合未来硬件加速器设计。

Abstract: While advancements in quantization have significantly reduced the
computational costs of inference in deep learning, training still predominantly
relies on complex floating-point arithmetic. Low-precision fixed-point training
presents a compelling alternative. This work introduces a novel enhancement in
low-precision logarithmic fixed-point training, geared towards future hardware
accelerator designs. We propose incorporating bitwidth in the design of
approximations to arithmetic operations. To this end, we introduce a new
hardware-friendly, piece-wise linear approximation for logarithmic addition.
Using simulated annealing, we optimize this approximation at different
precision levels. A C++ bit-true simulation demonstrates training of VGG-11 and
VGG-16 models on CIFAR-100 and TinyImageNet, respectively, using 12-bit integer
arithmetic with minimal accuracy degradation compared to 32-bit floating-point
training. Our hardware study reveals up to 32.5% reduction in area and 53.5%
reduction in energy consumption for the proposed LNS multiply-accumulate units
compared to that of linear fixed-point equivalents.

</details>


### [332] [Consistent Zero-Shot Imitation with Contrastive Goal Inference](https://arxiv.org/abs/2510.17059)
*Kathryn Wantlin,Chongyi Zheng,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 提出一种自监督预训练交互式代理的方法，使其能快速模仿人类演示。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型（如VLMs、LLMs）缺乏明确的动作概念，无法快速适应新任务，需探索自监督训练方法。

Method: 将目标（观察）作为原子构造，自动提出目标并练习达成，评估时通过逆强化学习解释演示为最优目标达成行为。

Result: 在标准基准测试中，该方法在零样本模仿任务上优于现有方法。

Conclusion: 自监督预训练交互式代理的方法有效，能快速适应新任务并模仿人类行为。

Abstract: In the same way that generative models today conduct most of their training
in a self-supervised fashion, how can agentic models conduct their training in
a self-supervised fashion, interactively exploring, learning, and preparing to
quickly adapt to new tasks? A prerequisite for embodied agents deployed in real
world interactions ought to be training with interaction, yet today's most
successful AI models (e.g., VLMs, LLMs) are trained without an explicit notion
of action. The problem of pure exploration (which assumes no data as input) is
well studied in the reinforcement learning literature and provides agents with
a wide array of experiences, yet it fails to prepare them for rapid adaptation
to new tasks. Today's language and vision models are trained on data provided
by humans, which provides a strong inductive bias for the sorts of tasks that
the model will have to solve (e.g., modeling chords in a song, phrases in a
sonnet, sentences in a medical record). However, when they are prompted to
solve a new task, there is a faulty tacit assumption that humans spend most of
their time in the most rewarding states. The key contribution of our paper is a
method for pre-training interactive agents in a self-supervised fashion, so
that they can instantly mimic human demonstrations. Our method treats goals
(i.e., observations) as the atomic construct. During training, our method
automatically proposes goals and practices reaching them, building off prior
work in reinforcement learning exploration. During evaluation, our method
solves an (amortized) inverse reinforcement learning problem to explain
demonstrations as optimal goal-reaching behavior. Experiments on standard
benchmarks (not designed for goal-reaching) show that our approach outperforms
prior methods for zero-shot imitation.

</details>


### [333] [Data Reliability Scoring](https://arxiv.org/abs/2510.17085)
*Yiling Chen,Shi Feng,Paul Kattuman,Fang-Yi Yu*

Main category: cs.LG

TL;DR: 提出Gram行列式评分方法，用于评估无真实数据时的数据集可靠性，具有实验无关性。


<details>
  <summary>Details</summary>
Motivation: 解决在无法获取真实数据时，如何评估来自潜在策略性数据源的数据集可靠性问题。

Method: 定义基于真实数据的排序，提出Gram行列式评分，衡量观测数据与实验结果的向量体积。

Result: Gram行列式评分能保持多种真实数据排序，且在实验中具有唯一性。

Conclusion: Gram行列式评分能有效捕捉不同观测过程中的数据质量。

Abstract: How can we assess the reliability of a dataset without access to ground
truth? We introduce the problem of reliability scoring for datasets collected
from potentially strategic sources. The true data are unobserved, but we see
outcomes of an unknown statistical experiment that depends on them. To
benchmark reliability, we define ground-truth-based orderings that capture how
much reported data deviate from the truth. We then propose the Gram determinant
score, which measures the volume spanned by vectors describing the empirical
distribution of the observed data and experiment outcomes. We show that this
score preserves several ground-truth based reliability orderings and, uniquely
up to scaling, yields the same reliability ranking of datasets regardless of
the experiment -- a property we term experiment agnosticism. Experiments on
synthetic noise models, CIFAR-10 embeddings, and real employment data
demonstrate that the Gram determinant score effectively captures data quality
across diverse observation processes.

</details>


### [334] [Explainable Heterogeneous Anomaly Detection in Financial Networks via Adaptive Expert Routing](https://arxiv.org/abs/2510.17088)
*Zan Li,Rui Fan*

Main category: cs.LG

TL;DR: 提出了一种自适应图学习框架，通过专家网络实现金融异常的多机制检测与解释。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法无法区分不同机制，导致监管响应不精准。

Method: 结合BiLSTM、自注意力、跨模态注意力等技术，动态学习市场图结构并路由异常到特定专家网络。

Result: 在100只美股上检测到92.3%的重大事件，领先基线30.8个百分点。

Conclusion: 框架不仅提升检测性能，还能自动识别异常机制，无需监督标签。

Abstract: Financial anomalies exhibit heterogeneous mechanisms (price shocks, liquidity
freezes, contagion cascades, regime shifts), but existing detectors treat all
anomalies uniformly, producing scalar scores without revealing which mechanism
is failing, where risks concentrate, or how to intervene. This opacity prevents
targeted regulatory responses. Three unsolved challenges persist: (1) static
graph structures cannot adapt when market correlations shift during regime
changes; (2) uniform detection mechanisms miss type-specific signatures across
multiple temporal scales while failing to integrate individual behaviors with
network contagion; (3) black-box outputs provide no actionable guidance on
anomaly mechanisms or their temporal evolution.
  We address these via adaptive graph learning with specialized expert networks
that provide built-in interpretability. Our framework captures multi-scale
temporal dependencies through BiLSTM with self-attention, fuses temporal and
spatial information via cross-modal attention, learns dynamic graphs through
neural multi-source interpolation, adaptively balances learned dynamics with
structural priors via stress-modulated fusion, routes anomalies to four
mechanism-specific experts, and produces dual-level interpretable attributions.
Critically, interpretability is embedded architecturally rather than applied
post-hoc.
  On 100 US equities (2017-2024), we achieve 92.3% detection of 13 major events
with 3.8-day lead time, outperforming best baseline by 30.8pp. Silicon Valley
Bank case study demonstrates anomaly evolution tracking: Price-Shock expert
weight rose to 0.39 (33% above baseline 0.29) during closure, peaking at 0.48
(66% above baseline) one week later, revealing automatic temporal mechanism
identification without labeled supervision.

</details>


### [335] [On the Universal Near Optimality of Hedge in Combinatorial Settings](https://arxiv.org/abs/2510.17099)
*Zhiyuan Fan,Arnab Maiti,Kevin Jamieson,Lillian J. Ratliff,Gabriele Farina*

Main category: cs.LG

TL;DR: 论文研究了Hedge算法在组合设置中的最优性，发现其在某些情况下是接近最优的，但在其他情况下存在次优性。


<details>
  <summary>Details</summary>
Motivation: 探讨Hedge算法在所有组合设置中是否最优，并确定其适用范围和局限性。

Method: 通过建立下界和具体案例分析，验证Hedge算法的性能。

Result: Hedge在某些组合设置中是接近最优的，但在其他情况下存在次优性；同时证明了其在在线多任务学习中的最优性。

Conclusion: Hedge算法在组合设置中具有广泛适用性，但需根据具体场景调整，且其性能可通过正则化方法进一步优化。

Abstract: In this paper, we study the classical Hedge algorithm in combinatorial
settings. In each round, the learner selects a vector $\boldsymbol{x}_t$ from a
set $X \subseteq \{0,1\}^d$, observes a full loss vector $\boldsymbol{y}_t \in
\mathbb{R}^d$, and incurs a loss $\langle \boldsymbol{x}_t, \boldsymbol{y}_t
\rangle \in [-1,1]$. This setting captures several important problems,
including extensive-form games, resource allocation, $m$-sets, online multitask
learning, and shortest-path problems on directed acyclic graphs (DAGs). It is
well known that Hedge achieves a regret of $O\big(\sqrt{T \log |X|}\big)$ after
$T$ rounds of interaction. In this paper, we ask whether Hedge is optimal
across all combinatorial settings. To that end, we show that for any $X
\subseteq \{0,1\}^d$, Hedge is near-optimal--specifically, up to a $\sqrt{\log
d}$ factor--by establishing a lower bound of $\Omega\big(\sqrt{T \log(|X|)/\log
d}\big)$ that holds for any algorithm. We then identify a natural class of
combinatorial sets--namely, $m$-sets with $\log d \leq m \leq \sqrt{d}$--for
which this lower bound is tight, and for which Hedge is provably suboptimal by
a factor of exactly $\sqrt{\log d}$. At the same time, we show that Hedge is
optimal for online multitask learning, a generalization of the classical
$K$-experts problem. Finally, we leverage the near-optimality of Hedge to
establish the existence of a near-optimal regularizer for online shortest-path
problems in DAGs--a setting that subsumes a broad range of combinatorial
domains. Specifically, we show that the classical Online Mirror Descent (OMD)
algorithm, when instantiated with the dilated entropy regularizer, is
iterate-equivalent to Hedge, and therefore inherits its near-optimal regret
guarantees for DAGs.

</details>


### [336] [Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback](https://arxiv.org/abs/2510.17103)
*Shinji Ito,Kevin Jamieson,Haipeng Luo,Arnab Maiti,Taira Tsuchiya*

Main category: cs.LG

TL;DR: 该论文研究了在有限时间段的马尔可夫决策过程（MDP）中，基于累积损失反馈的在线学习问题，提出了首个在随机和对抗环境中均表现最优的算法。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注最坏情况分析，而本文旨在设计一种在随机和对抗环境中均能实现低遗憾的算法。

Method: 结合FTRL（Follow-the-Regularized-Leader）占用度量、自边界技术和新型损失估计器，提出了适用于已知和未知转移概率的算法。

Result: 在已知转移概率的情况下，算法在随机环境中实现O(log T)遗憾，在对抗环境中实现O(√T)遗憾，并证明了其最优性。

Conclusion: 该研究填补了累积损失反馈下MDP在线学习的空白，为相关领域提供了理论基础和实用算法。

Abstract: We study online learning in finite-horizon episodic Markov decision processes
(MDPs) under the challenging aggregate bandit feedback model, where the learner
observes only the cumulative loss incurred in each episode, rather than
individual losses at each state-action pair. While prior work in this setting
has focused exclusively on worst-case analysis, we initiate the study of
best-of-both-worlds (BOBW) algorithms that achieve low regret in both
stochastic and adversarial environments. We propose the first BOBW algorithms
for episodic tabular MDPs with aggregate bandit feedback. In the case of known
transitions, our algorithms achieve $O(\log T)$ regret in stochastic settings
and ${O}(\sqrt{T})$ regret in adversarial ones. Importantly, we also establish
matching lower bounds, showing the optimality of our algorithms in this
setting. We further extend our approach to unknown-transition settings by
incorporating confidence-based techniques. Our results rely on a combination of
FTRL over occupancy measures, self-bounding techniques, and new loss estimators
inspired by recent advances in online shortest path problems. Along the way, we
also provide the first individual-gap-dependent lower bounds and demonstrate
near-optimal BOBW algorithms for shortest path problems with bandit feedback.

</details>


### [337] [Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling](https://arxiv.org/abs/2510.17106)
*Chen Zhang,Weixin Bu,Wendong Xu,Runsheng Yu,Yik-Chung Wu,Ngai Wong*

Main category: cs.LG

TL;DR: 论文揭示了Transformer编码器与图卷积网络（GCN）的基本等价性，并提出了一种名为Fighter的简化架构，结合了多跳图聚合，提升了可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在时间序列建模中的内部机制，揭示其与GCN的等价性，以提升模型的可解释性和效率。

Method: 通过理论分析证明Transformer的注意力分布矩阵等同于动态邻接矩阵，并提出Fighter架构，去除冗余线性投影并引入多跳图聚合。

Result: 实验表明Fighter在标准预测基准上具有竞争力，同时提供了更清晰的预测机制解释。

Conclusion: Transformer与GCN的等价性为时间序列建模提供了新的理论视角，Fighter架构在性能和可解释性上均表现出色。

Abstract: Transformers have achieved remarkable success in time series modeling, yet
their internal mechanisms remain opaque. This work demystifies the Transformer
encoder by establishing its fundamental equivalence to a Graph Convolutional
Network (GCN). We show that in the forward pass, the attention distribution
matrix serves as a dynamic adjacency matrix, and its composition with
subsequent transformations performs computations analogous to graph
convolution. Moreover, we demonstrate that in the backward pass, the update
dynamics of value and feed-forward projections mirror those of GCN parameters.
Building on this unified theoretical reinterpretation, we propose
\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined
architecture that removes redundant linear projections and incorporates
multi-hop graph aggregation. This perspective yields an explicit and
interpretable representation of temporal dependencies across different scales,
naturally expressed as graph edges. Experiments on standard forecasting
benchmarks confirm that Fighter achieves competitive performance while
providing clearer mechanistic interpretability of its predictions.

</details>


### [338] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出了一种基于矩阵自由能量的新型自编码器正则化方案，通过优化代码矩阵的奇异值分布实现高斯化编码。


<details>
  <summary>Details</summary>
Motivation: 旨在通过正则化方法提升自编码器的泛化能力，并解决欠定逆问题。

Method: 定义了一个基于代码矩阵奇异值的可微损失函数，通过随机矩阵理论优化其分布。

Result: 实验表明，该方法能生成高斯化编码，并在训练和测试集上表现良好。

Conclusion: 该方法不仅提升了自编码器的性能，还能有效应用于欠定逆问题。

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [339] [Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control](https://arxiv.org/abs/2510.17122)
*Chengxiu Hua,Jiawen Gu,Yushun Tang*

Main category: cs.LG

TL;DR: 提出了一种连续时间强化学习方法CQSM，通过鞅条件和动态规划原理将扩散策略分数与连续Q函数的动作梯度联系起来，解决了连续时间RL中长期存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 大多数现有强化学习方法基于离散时间，而连续时间控制问题需要新的方法。

Method: 通过鞅条件表征连续时间Q函数，并将扩散策略分数与连续Q函数的动作梯度联系起来，提出CQSM算法。

Result: 在模拟环境中验证了方法的有效性，并提供了线性二次控制问题的理论闭式解。

Conclusion: CQSM方法在连续时间RL中有效保留了Q函数的动作评估能力，无需时间离散化。

Abstract: Reinforcement learning (RL) has achieved significant success across a wide
range of domains, however, most existing methods are formulated in discrete
time. In this work, we introduce a novel RL method for continuous-time control,
where stochastic differential equations govern state-action dynamics. Departing
from traditional value function-based approaches, our key contribution is the
characterization of continuous-time Q-functions via a martingale condition and
the linking of diffusion policy scores to the action gradient of a learned
continuous Q-function by the dynamic programming principle. This insight
motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement
algorithm. Notably, our method addresses a long-standing challenge in
continuous-time RL: preserving the action-evaluation capability of Q-functions
without relying on time discretization. We further provide theoretical
closed-form solutions for linear-quadratic (LQ) control problems within our
framework. Numerical results in simulated environments demonstrate the
effectiveness of our proposed method and compare it to popular baselines.

</details>


### [340] [Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction](https://arxiv.org/abs/2510.17132)
*Ioannis Tsaknakis,Bingqing Song,Shuyu Gan,Dongyeop Kang,Alfredo Garcia,Gaowen Liu,Charles Fleming,Mingyi Hong*

Main category: cs.LG

TL;DR: LLMs在对话中发现和利用用户潜在信息的能力通过多轮交互评估，结果显示其效果因任务复杂度而异。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能通过对话揭示和利用用户的潜在偏好，以提升个性化交互效果。

Method: 引入统一基准，涵盖三种任务（20 Questions、个性化问答、个性化文本摘要），采用三智能体框架（用户、助手、裁判）评估。

Result: LLMs能通过对话揭示潜在信息，但成功率（32%至98%）受任务复杂度、主题和隐藏属性数量影响。

Conclusion: 该基准为研究潜在信息发现提供了系统框架，表明有效偏好推断仍是自适应AI系统的开放问题。

Abstract: Large Language Models (LLMs) excel at producing broadly relevant text, but
this generality becomes a limitation when user-specific preferences are
required, such as recommending restaurants or planning travel. In these
scenarios, users rarely articulate every preference explicitly; instead, much
of what they care about remains latent, waiting to be inferred. This raises a
fundamental question: Can LLMs uncover and reason about such latent information
through conversation?
  We address this problem by introducing a unified benchmark for evaluating
latent information discovery - the ability of LLMs to reveal and utilize hidden
user attributes through multi-turn interaction. The benchmark spans three
progressively realistic settings: the classic 20 Questions game, Personalized
Question Answering, and Personalized Text Summarization. All tasks share a
tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of
elicitation and adaptation. Our results reveal that while LLMs can indeed
surface latent information through dialogue, their success varies dramatically
with context: from 32% to 98%, depending on task complexity, topic, and number
of hidden attributes. This benchmark provides the first systematic framework
for studying latent information discovery in personalized interaction,
highlighting that effective preference inference remains an open frontier for
building truly adaptive AI systems.

</details>


### [341] [In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models](https://arxiv.org/abs/2510.17136)
*Enhao Gu,Haolin Hou*

Main category: cs.LG

TL;DR: 提出了一种名为In-situ Autoguidance的方法，无需辅助模型即可实现高质量的图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在提高图像质量和提示对齐时牺牲多样性的问题，同时避免使用额外模型的开销。

Method: 通过动态生成劣质预测，将引导视为推理时的自我校正。

Result: 证明该方法不仅可行，还能作为成本高效引导的新基准。

Conclusion: 无需外部模型即可实现自我引导的优势。

Abstract: The generation of high-quality, diverse, and prompt-aligned images is a
central goal in image-generating diffusion models. The popular classifier-free
guidance (CFG) approach improves quality and alignment at the cost of reduced
variation, creating an inherent entanglement of these effects. Recent work has
successfully disentangled these properties by guiding a model with a separately
trained, inferior counterpart; however, this solution introduces the
considerable overhead of requiring an auxiliary model. We challenge this
prerequisite by introducing In-situ Autoguidance, a method that elicits
guidance from the model itself without any auxiliary components. Our approach
dynamically generates an inferior prediction on the fly using a stochastic
forward pass, reframing guidance as a form of inference-time self-correction.
We demonstrate that this zero-cost approach is not only viable but also
establishes a powerful new baseline for cost-efficient guidance, proving that
the benefits of self-guidance can be achieved without external models.

</details>


### [342] [Learning After Model Deployment](https://arxiv.org/abs/2510.17160)
*Derda Kaymak,Gyuhak Kim,Tomoya Kaichi,Tatsuya Konishi,Bing Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为ALMD的学习范式，旨在解决动态开放环境中模型部署后无法处理新类别样本的问题，并提出了PLDA方法来实现动态OOD检测和增量学习。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习模型部署后无法更新，不适合动态开放环境。ALMD旨在让模型能够检测新类别样本并学习它们。

Method: 提出了PLDA方法，动态检测OOD样本并增量学习新类别。

Result: PLDA在实验中表现出色。

Conclusion: PLDA有效解决了ALMD中的动态OOD检测和增量学习问题。

Abstract: In classic supervised learning, once a model is deployed in an application,
it is fixed. No updates will be made to it during the application. This is
inappropriate for many dynamic and open environments, where unexpected samples
from unseen classes may appear. In such an environment, the model should be
able to detect these novel samples from unseen classes and learn them after
they are labeled. We call this paradigm Autonomous Learning after Model
Deployment (ALMD). The learning here is continuous and involves no human
engineers. Labeling in this scenario is performed by human co-workers or other
knowledgeable agents, which is similar to what humans do when they encounter an
unfamiliar object and ask another person for its name. In ALMD, the detection
of novel samples is dynamic and differs from traditional out-of-distribution
(OOD) detection in that the set of in-distribution (ID) classes expands as new
classes are learned during application, whereas ID classes is fixed in
traditional OOD detection. Learning is also different from classic supervised
learning because in ALMD, we learn the encountered new classes immediately and
incrementally. It is difficult to retrain the model from scratch using all the
past data from the ID classes and the novel samples from newly discovered
classes, as this would be resource- and time-consuming. Apart from these two
challenges, ALMD faces the data scarcity issue because instances of new classes
often appear sporadically in real-life applications. To address these issues,
we propose a novel method, PLDA, which performs dynamic OOD detection and
incremental learning of new classes on the fly. Empirical evaluations will
demonstrate the effectiveness of PLDA.

</details>


### [343] [ALPINE: A Lightweight and Adaptive Privacy-Decision Agent Framework for Dynamic Edge Crowdsensing](https://arxiv.org/abs/2510.17162)
*Guanjie Cheng,Siyang Liu,Junqin Huang,Xinkui Zhao,Yin Wang,Mengying Zhu,Linghe Kong,Shuiguang Deng*

Main category: cs.LG

TL;DR: ALPINE是一个轻量级自适应框架，用于在移动边缘众感系统中实时调整差分隐私级别，平衡隐私、数据效用和能源成本。


<details>
  <summary>Details</summary>
Motivation: 移动边缘众感系统在动态、资源受限的环境中运行，静态差分隐私机制无法适应不断变化的风险，导致噪声过多或保护不足。

Method: ALPINE采用闭环控制系统，包括动态风险感知、基于TD3的隐私决策、本地隐私执行和边缘节点性能验证。

Result: 理论分析和实际模拟表明，ALPINE能有效减轻推理攻击，同时保持数据效用和成本效益。

Conclusion: ALPINE适用于大规模边缘应用，实现了隐私、效用和成本的动态平衡。

Abstract: Mobile edge crowdsensing (MECS) systems continuously generate and transmit
user data in dynamic, resource-constrained environments, exposing users to
significant privacy threats. In practice, many privacy-preserving mechanisms
build on differential privacy (DP). However, static DP mechanisms often fail to
adapt to evolving risks, for example, shifts in adversarial capabilities,
resource constraints and task requirements, resulting in either excessive noise
or inadequate protection. To address this challenge, we propose ALPINE, a
lightweight, adaptive framework that empowers terminal devices to autonomously
adjust differential privacy levels in real time. ALPINE operates as a
closed-loop control system consisting of four modules: dynamic risk perception,
privacy decision via twin delayed deep deterministic policy gradient (TD3),
local privacy execution and performance verification from edge nodes. Based on
environmental risk assessments, we design a reward function that balances
privacy gains, data utility and energy cost, guiding the TD3 agent to
adaptively tune noise magnitude across diverse risk scenarios and achieve a
dynamic equilibrium among privacy, utility and cost. Both the collaborative
risk model and pretrained TD3-based agent are designed for low-overhead
deployment. Extensive theoretical analysis and real-world simulations
demonstrate that ALPINE effectively mitigates inference attacks while
preserving utility and cost, making it practical for large-scale edge
applications.

</details>


### [344] [Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses](https://arxiv.org/abs/2510.17185)
*Runlin Lei,Lu Yi,Mingguo He,Pengyu Qiu,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.LG

TL;DR: 论文提出一个统一框架评估文本属性图（TAG）学习中GNNs和LLMs的鲁棒性，揭示了模型在文本和结构上的鲁棒性权衡，并提出新框架SFT-auto以提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前对GNNs和LLMs在TAG学习中的鲁棒性评估分散且不系统，缺乏对文本和结构扰动的全面分析。

Method: 引入统一框架，评估经典GNNs、鲁棒GNNs（RGNNs）和GraphLLMs在十种数据集上的表现，涵盖文本、结构和混合扰动。

Result: 发现模型在文本和结构上存在鲁棒性权衡，GNNs和RGNNs性能依赖文本编码器和攻击类型，GraphLLMs易受训练数据污染。

Conclusion: 提出SFT-auto框架以平衡文本和结构攻击的鲁棒性，为TAG安全研究奠定基础并提供实用解决方案。

Abstract: While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are
powerful approaches for learning on Text-Attributed Graphs (TAGs), a
comprehensive understanding of their robustness remains elusive. Current
evaluations are fragmented, failing to systematically investigate the distinct
effects of textual and structural perturbations across diverse models and
attack scenarios. To address these limitations, we introduce a unified and
comprehensive framework to evaluate robustness in TAG learning. Our framework
evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten
datasets from four domains, under diverse text-based, structure-based, and
hybrid perturbations in both poisoning and evasion scenarios. Our extensive
analysis reveals multiple findings, among which three are particularly
noteworthy: 1) models have inherent robustness trade-offs between text and
structure, 2) the performance of GNNs and RGNNs depends heavily on the text
encoder and attack type, and 3) GraphLLMs are particularly vulnerable to
training data corruption. To overcome the identified trade-offs, we introduce
SFT-auto, a novel framework that delivers superior and balanced robustness
against both textual and structural attacks within a single model. Our work
establishes a foundation for future research on TAG security and offers
practical solutions for robust TAG learning in adversarial environments. Our
code is available at: https://github.com/Leirunlin/TGRB.

</details>


### [345] [A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling](https://arxiv.org/abs/2510.17187)
*Alexander Aghili,Andy Bruce,Daniel Sabo,Sanya Murdeshwar,Kevin Bachelor,Ionut Mistreanu,Ashwin Lokapally,Razvan Marinescu*

Main category: cs.LG

TL;DR: 提出了一种模块化基准测试框架，用于系统评估蛋白质分子动力学方法，支持多种模拟引擎和评估指标。


<details>
  <summary>Details</summary>
Motivation: 分子动力学方法快速发展，但缺乏标准化验证工具，导致方法比较困难。

Method: 使用加权集合采样和TICA分析，结合灵活模拟引擎接口，提供19种以上评估指标。

Result: 贡献了9种蛋白质数据集，验证了框架在经典MD和机器学习模型中的实用性。

Conclusion: 开源平台为分子模拟社区提供了标准化、可重复的基准测试基础。

Abstract: The rapid evolution of molecular dynamics (MD) methods, including
machine-learned dynamics, has outpaced the development of standardized tools
for method validation. Objective comparison between simulation approaches is
often hindered by inconsistent evaluation metrics, insufficient sampling of
rare conformational states, and the absence of reproducible benchmarks. To
address these challenges, we introduce a modular benchmarking framework that
systematically evaluates protein MD methods using enhanced sampling analysis.
Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble
Simulation Toolkit with Parallelization and Analysis (WESTPA), based on
progress coordinates derived from Time-lagged Independent Component Analysis
(TICA), enabling fast and efficient exploration of protein conformational
space. The framework includes a flexible, lightweight propagator interface that
supports arbitrary simulation engines, allowing both classical force fields and
machine learning-based models. Additionally, the framework offers a
comprehensive evaluation suite capable of computing more than 19 different
metrics and visualizations across a variety of domains. We further contribute a
dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a
variety of folding complexities and topologies. Each protein has been
extensively simulated at 300K for one million MD steps per starting point (4
ns). To demonstrate the utility of our framework, we perform validation tests
using classic MD simulations with implicit solvent and compare protein
conformational sampling using a fully trained versus under-trained CGSchNet
model. By standardizing evaluation protocols and enabling direct, reproducible
comparisons across MD approaches, our open-source platform lays the groundwork
for consistent, rigorous benchmarking across the molecular simulation
community.

</details>


### [346] [SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference](https://arxiv.org/abs/2510.17189)
*Wenxun Wang,Shuchang Zhou,Wenyu Sun,Peiqin Sun,Yongpan Liu*

Main category: cs.LG

TL;DR: SOLE是一种软硬件协同设计方法，通过E2Softmax和AILayerNorm优化Softmax和LayerNorm的计算效率，显著提升推理速度和能效。


<details>
  <summary>Details</summary>
Motivation: Transformer在NLP和CV任务中表现出色，但Softmax和LayerNorm的计算效率限制了其实时推理能力。现有方法因忽略内存开销和依赖重训练而效率不足。

Method: SOLE结合E2Softmax（基于log2量化和对数除法近似Softmax）和AILayerNorm（低精度统计计算），实现低精度计算和低比特存储。

Result: 实验表明，SOLE在不重训练的情况下保持推理精度，相比GPU和现有硬件设计，速度和能效显著提升（Softmax和LayerNorm分别提升3.04x、3.86x能效和2.82x、3.32x面积效率）。

Conclusion: SOLE通过软硬件协同设计，有效解决了Softmax和LayerNorm的效率问题，为Transformer的实时推理提供了高效解决方案。

Abstract: Transformers have shown remarkable performance in both natural language
processing (NLP) and computer vision (CV) tasks. However, their real-time
inference speed and efficiency are limited due to the inefficiency in Softmax
and Layer Normalization (LayerNorm). Previous works based on function
approximation suffer from inefficient implementation as they place emphasis on
computation while disregarding memory overhead concerns. Moreover, such methods
rely on retraining to compensate for approximation error which can be costly
and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and
LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes
log2 quantization of exponent function and log-based division to approximate
Softmax while AILayerNorm adopts low-precision statistic calculation. Compared
with state-of-the-art designs, we achieve both low-precision calculation and
low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE
maintains inference accuracy without retraining while offering orders of
magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x
energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements
over prior state-of-the-art custom hardware for Softmax and LayerNorm,
respectively.

</details>


### [347] [Soft-Masked Diffusion Language Models](https://arxiv.org/abs/2510.17206)
*Michael Hersche,Samuel Moor-Smith,Thomas Hofmann,Abbas Rahimi*

Main category: cs.LG

TL;DR: 论文提出了一种名为软掩码（SM）的新方法，用于改进基于掩码扩散的语言模型，通过动态混合掩码标记和预测标记的嵌入，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统掩码扩散方法在解码时仅保留或替换掩码，丢弃了有价值的预测信息，限制了模型性能。

Method: 引入软掩码（SM），动态混合掩码标记和预测标记的嵌入，提供更丰富的先验信息，并提出了相应的训练方法。

Result: 在169M参数模型上，SM改善了困惑度和MAUVE分数；在Dream-7B和Dream-Coder-7B模型上，SM在编码任务中表现更优。

Conclusion: 软掩码方法有效提升了扩散语言模型的性能，尤其在高速生成场景中表现突出。

Abstract: Diffusion models have demonstrated strong potential in language modeling,
offering various advantages over traditional autoregressive approaches. Their
ability to generate and revise entire responses in parallel enables faster
generation and built-in self-correction mechanisms. Most modern diffusion-based
language models employ masked diffusion, where decoding involves iteratively
processing masked tokens based on a binary decision: either retaining the mask
or replacing it with the predicted token. However, this binary choice discards
valuable predictive information when the mask is retained. To address this
limitation, we introduce soft-masking (SM), a novel method that dynamically
blends the embedding of the mask token with the embeddings of the top-$k$
predicted tokens from the previous decoding step, for each retained mask. This
provides the model with a more informative prior, preserving context from
earlier computations and allowing partial information about masked tokens to
propagate beyond a single step. We propose a training methodology that adapts a
pretrained masked diffusion language model to incorporate SM. We demonstrate
that continuing pretraining a 169M parameter model with SM leads to improved
perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art
diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently
improves performance across multiple coding benchmarks, particularly in
high-throughput settings.

</details>


### [348] [D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks](https://arxiv.org/abs/2510.17212)
*Jundong Zhang,Yuhui Situ,Fanji Zhang,Rongji Deng,Tianqi Wei*

Main category: cs.LG

TL;DR: 提出了一种针对高风险高回报任务的强化学习框架，通过离散化动作空间和双评论家架构提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法假设单峰高斯策略和标量评论家，无法有效处理高风险高回报任务的多模态动作分布和随机回报。

Method: 离散化连续动作空间以近似多模态分布，采用熵正则化探索，并引入双评论家架构进行离散值分布估计。

Result: 在运动和操作任务中表现优于基线方法，验证了显式建模多模态和风险的重要性。

Conclusion: 该框架为高风险高回报任务提供了有效的解决方案，强调了多模态和风险建模在强化学习中的关键作用。

Abstract: Tasks involving high-risk-high-return (HRHR) actions, such as obstacle
crossing, often exhibit multimodal action distributions and stochastic returns.
Most reinforcement learning (RL) methods assume unimodal Gaussian policies and
rely on scalar-valued critics, which limits their effectiveness in HRHR
settings. We formally define HRHR tasks and theoretically show that Gaussian
policies cannot guarantee convergence to the optimal solution. To address this,
we propose a reinforcement learning framework that (i) discretizes continuous
action spaces to approximate multimodal distributions, (ii) employs
entropy-regularized exploration to improve coverage of risky but rewarding
actions, and (iii) introduces a dual-critic architecture for more accurate
discrete value distribution estimation. The framework scales to
high-dimensional action spaces, supporting complex control domains. Experiments
on locomotion and manipulation benchmarks with high risks of failure
demonstrate that our method outperforms baselines, underscoring the importance
of explicitly modeling multimodality and risk in RL.

</details>


### [349] [Diagnosis of Fuel Cell Health Status with Deep Sparse Auto-Encoder Neural Network](https://arxiv.org/abs/2510.17214)
*Chenyan Fei,Dalin Zhang,Chen Melinda Dang*

Main category: cs.LG

TL;DR: 使用深度稀疏自编码网络预测和分类燃料电池高频阻抗，准确率超92%，并在FPGA上部署，硬件识别率近90%。


<details>
  <summary>Details</summary>
Motivation: 高频阻抗是评估燃料电池状态和健康状况的关键指标，但其在线测试复杂且成本高。

Method: 采用深度稀疏自编码网络进行预测和分类。

Result: 准确率超过92%，硬件识别率近90%。

Conclusion: 该方法有效且高效，适用于燃料电池健康状态诊断。

Abstract: Effective and accurate diagnosis of fuel cell health status is crucial for
ensuring the stable operation of fuel cell stacks. Among various parameters,
high-frequency impedance serves as a critical indicator for assessing fuel cell
state and health conditions. However, its online testing is prohibitively
complex and costly. This paper employs a deep sparse auto-encoding network for
the prediction and classification of high-frequency impedance in fuel cells,
achieving metric of accuracy rate above 92\%. The network is further deployed
on an FPGA, attaining a hardware-based recognition rate almost 90\%.

</details>


### [350] [A Prototypical Network with an Attention-based Encoder for Drivers Identification Application](https://arxiv.org/abs/2510.17250)
*Wei-Hsun Lee,Che-Yu Chang,Kuang-Yu Li*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的编码器（AttEnc）和原型网络结合的架构（P-AttEnc），用于驾驶员识别，解决了数据短缺和未知驾驶员分类问题。


<details>
  <summary>Details</summary>
Motivation: 传统生物识别技术存在隐私问题，且现有方法对数据短缺和未知驾驶员处理不灵活。

Method: 结合注意力机制和原型网络，采用少样本学习技术。

Result: AttEnc在三个数据集上识别准确率分别为99.3%、99.0%和99.9%，模型参数减少87.6%，预测时间快44%-79%。P-AttEnc在少样本场景下识别准确率为69.8%，对未知驾驶员分类准确率为65.7%。

Conclusion: P-AttEnc在驾驶员识别中表现出色，尤其在数据短缺和未知驾驶员分类方面具有优势。

Abstract: Driver identification has become an area of increasing interest in recent
years, especially for data- driven applications, because biometric-based
technologies may incur privacy issues. This study proposes a deep learning
neural network architecture, an attention-based encoder (AttEnc), which uses an
attention mechanism for driver identification and uses fewer model parameters
than current methods. Most studies do not address the issue of data shortages
for driver identification, and most of them are inflexible when encountering
unknown drivers. In this study, an architecture that combines a prototypical
network and an attention-based encoder (P-AttEnc) is proposed. It applies
few-shot learning to overcome the data shortage issues and to enhance model
generalizations. The experiments showed that the attention-based encoder can
identify drivers with accuracies of 99.3%, 99.0% and 99.9% in three different
datasets and has a prediction time that is 44% to 79% faster because it
significantly reduces, on average, 87.6% of the model parameters. P-AttEnc
identifies drivers based on few shot data, extracts driver fingerprints to
address the issue of data shortages, and is able to classify unknown drivers.
The first experiment showed that P-AttEnc can identify drivers with an accuracy
of 69.8% in the one-shot scenario. The second experiment showed that P-AttEnc,
in the 1-shot scenario, can classify unknown drivers with an average accuracy
of 65.7%.

</details>


### [351] [Adaptive Discretization for Consistency Models](https://arxiv.org/abs/2510.17266)
*Jiayu Bai,Zhanbo Feng,Zhijie Deng,Tianqi Hou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

TL;DR: 提出了一种自动和自适应离散化的统一框架ADCMs，显著提高了CMs的训练效率和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有CMs依赖手动设计的离散化方案，对不同噪声计划和数据集需要重复调整，缺乏灵活性。

Method: 将离散化步骤作为优化问题，以局部一致性为优化目标，全局一致性为约束，通过拉格朗日乘子平衡两者，并采用高斯-牛顿方法实现自适应离散化。

Result: 在CIFAR-10和ImageNet上，ADCMs显著提高了训练效率，生成性能优于现有方法，且适应性强。

Conclusion: ADCMs为CMs提供了一种高效且自适应的离散化方案，具有广泛的应用潜力。

Abstract: Consistency Models (CMs) have shown promise for efficient one-step
generation. However, most existing CMs rely on manually designed discretization
schemes, which can cause repeated adjustments for different noise schedules and
datasets. To address this, we propose a unified framework for the automatic and
adaptive discretization of CMs, formulating it as an optimization problem with
respect to the discretization step. Concretely, during the consistency training
process, we propose using local consistency as the optimization objective to
ensure trainability by avoiding excessive discretization, and taking global
consistency as a constraint to ensure stability by controlling the denoising
error in the training target. We establish the trade-off between local and
global consistency with a Lagrange multiplier. Building on this framework, we
achieve adaptive discretization for CMs using the Gauss-Newton method. We refer
to our approach as ADCMs. Experiments demonstrate that ADCMs significantly
improve the training efficiency of CMs, achieving superior generative
performance with minimal training overhead on both CIFAR-10 and ImageNet.
Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code
is available at https://github.com/rainstonee/ADCM.

</details>


### [352] [Uncertainty-aware data assimilation through variational inference](https://arxiv.org/abs/2510.17268)
*Anthony Frion,David S Greenberg*

Main category: cs.LG

TL;DR: 提出了一种基于变分推断的数据同化方法，用于处理不确定性，并在Lorenz-96动力学中验证了其预测校准效果。


<details>
  <summary>Details</summary>
Motivation: 数据同化中通常存在不确定性，现有确定性机器学习方法无法充分处理这一问题。

Method: 扩展了现有的确定性机器学习方法，引入变分推断，使预测状态服从多元高斯分布。

Result: 在Lorenz-96动力学测试中，新模型实现了近乎完美的预测校准，并能整合到更广泛的变分数据同化流程中。

Conclusion: 该方法显著提升了数据同化窗口长度增加时的性能，代码已开源。

Abstract: Data assimilation, consisting in the combination of a dynamical model with a
set of noisy and incomplete observations in order to infer the state of a
system over time, involves uncertainty in most settings. Building upon an
existing deterministic machine learning approach, we propose a variational
inference-based extension in which the predicted state follows a multivariate
Gaussian distribution. Using the chaotic Lorenz-96 dynamics as a testing
ground, we show that our new model enables to obtain nearly perfectly
calibrated predictions, and can be integrated in a wider variational data
assimilation pipeline in order to achieve greater benefit from increasing
lengths of data assimilation windows. Our code is available at
https://github.com/anthony-frion/Stochastic_CODA.

</details>


### [353] [Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems](https://arxiv.org/abs/2510.17276)
*Rishi Jha,Harold Triedman,Justin Wagle,Vitaly Shmatikov*

Main category: cs.LG

TL;DR: ControlValve防御机制通过生成允许的控制流图并强制执行，解决了多智能体系统中控制流劫持攻击的问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制（如LlamaFirewall）依赖对齐检查，但仍易受攻击，且安全性与功能性目标存在冲突。

Method: 提出ControlValve，生成允许的控制流图并强制执行，结合零样本生成的上下文规则。

Result: ControlValve有效防御控制流劫持攻击。

Conclusion: ControlValve解决了现有防御机制的局限性，提升了多智能体系统的安全性。

Abstract: Control-flow hijacking attacks manipulate orchestration mechanisms in
multi-agent systems into performing unsafe actions that compromise the system
and exfiltrate sensitive information. Recently proposed defenses, such as
LlamaFirewall, rely on alignment checks of inter-agent communications to ensure
that all agent invocations are "related to" and "likely to further" the
original objective.
  We start by demonstrating control-flow hijacking attacks that evade these
defenses even if alignment checks are performed by advanced LLMs. We argue that
the safety and functionality objectives of multi-agent systems fundamentally
conflict with each other. This conflict is exacerbated by the brittle
definitions of "alignment" and the checkers' incomplete visibility into the
execution context.
  We then propose, implement, and evaluate ControlValve, a new defense inspired
by the principles of control-flow integrity and least privilege. ControlValve
(1) generates permitted control-flow graphs for multi-agent systems, and (2)
enforces that all executions comply with these graphs, along with contextual
rules (generated in a zero-shot manner) for each agent invocation.

</details>


### [354] [MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems](https://arxiv.org/abs/2510.17281)
*Qingyao Ai,Yichen Tang,Changyue Wang,Jianming Long,Weihang Su,Yiqun Liu*

Main category: cs.LG

TL;DR: 论文提出了一种用户反馈模拟框架和综合基准测试，用于评估LLM系统的持续学习能力，发现现有方法的有效性不足。


<details>
  <summary>Details</summary>
Motivation: 由于高质量数据逐渐耗尽和计算资源边际收益递减，传统方法（如扩大数据和参数规模）的改进空间有限，因此研究LLM系统的记忆和持续学习能力成为重要方向。

Method: 提出用户反馈模拟框架和多领域、多语言、多任务类型的综合基准测试，用于评估LLM系统的持续学习能力。

Result: 实验表明，现有方法的有效性和效率远未达到理想水平。

Conclusion: 该基准测试为未来LLM记忆和优化算法的研究提供了方向。

Abstract: Scaling up data, parameters, and test-time computation has been the
mainstream methods to improve LLM systems (LLMsys), but their upper bounds are
almost reached due to the gradual depletion of high-quality data and marginal
gains obtained from larger computational resource consumption. Inspired by the
abilities of human and traditional AI systems in learning from practice,
constructing memory and continual learning frameworks for LLMsys has become an
important and popular research direction in recent literature. Yet, existing
benchmarks for LLM memory often focus on evaluating the system on homogeneous
reading comprehension tasks with long-form inputs rather than testing their
abilities to learn from accumulated user feedback in service time. Therefore,
we propose a user feedback simulation framework and a comprehensive benchmark
covering multiple domains, languages, and types of tasks to evaluate the
continual learning abilities of LLMsys. Experiments show that the effectiveness
and efficiency of state-of-the-art baselines are far from satisfying, and we
hope this benchmark could pave the way for future studies on LLM memory and
optimization algorithms.

</details>


### [355] [Symmetries in PAC-Bayesian Learning](https://arxiv.org/abs/2510.17303)
*Armin Beck,Peter Ochs*

Main category: cs.LG

TL;DR: 论文扩展了对称性在机器学习中的理论保证，涵盖非紧致对称性和非不变数据分布，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注紧致群对称性且假设数据分布不变，但现实中这一假设很少成立。本文旨在填补这一理论空白。

Method: 基于PAC-Bayes框架，改进并收紧现有边界，适用于多种PAC-Bayes边界。

Result: 在非均匀旋转的MNIST数据集上验证了理论，证明了对称模型的优越性。

Conclusion: 对称模型在更广泛的对称性和数据分布下仍优于非对称模型，为对称性研究提供了新方向。

Abstract: Symmetries are known to improve the empirical performance of machine learning
models, yet theoretical guarantees explaining these gains remain limited. Prior
work has focused mainly on compact group symmetries and often assumes that the
data distribution itself is invariant, an assumption rarely satisfied in
real-world applications. In this work, we extend generalization guarantees to
the broader setting of non-compact symmetries, such as translations and to
non-invariant data distributions. Building on the PAC-Bayes framework, we adapt
and tighten existing bounds, demonstrating the approach on McAllester's
PAC-Bayes bound while showing that it applies to a wide range of PAC-Bayes
bounds. We validate our theory with experiments on a rotated MNIST dataset with
a non-uniform rotation group, where the derived guarantees not only hold but
also improve upon prior results. These findings provide theoretical evidence
that, for symmetric data, symmetric models are preferable beyond the narrow
setting of compact groups and invariant distributions, opening the way to a
more general understanding of symmetries in machine learning.

</details>


### [356] [Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations](https://arxiv.org/abs/2510.17313)
*Tal Barami,Nimrod Berman,Ilan Naiman,Amos H. Hason,Rotem Ezra,Omri Azencot*

Main category: cs.LG

TL;DR: 论文提出了首个多因素序列解缠的标准基准，并引入了一种Koopman启发的模型和Vision-Language Models自动化评估方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据涉及多个相互作用的语义因素，但现有研究多集中于简单的两因素静态和动态场景，忽略了多因素特性。

Method: 提出了标准化基准，包括数据集集成工具、模型开发工具和评估指标，并引入了后验潜在探索阶段和Koopman启发模型。

Result: Koopman模型取得了最先进的结果，Vision-Language Models可自动化数据集标注和零样本解缠评估。

Conclusion: 这些贡献为多因素序列解缠提供了稳健且可扩展的基础。

Abstract: Learning disentangled representations in sequential data is a key goal in
deep learning, with broad applications in vision, audio, and time series. While
real-world data involves multiple interacting semantic factors over time, prior
work has mostly focused on simpler two-factor static and dynamic settings,
primarily because such settings make data collection easier, thereby
overlooking the inherently multi-factor nature of real-world data. We introduce
the first standardized benchmark for evaluating multi-factor sequential
disentanglement across six diverse datasets spanning video, audio, and time
series. Our benchmark includes modular tools for dataset integration, model
development, and evaluation metrics tailored to multi-factor analysis. We
additionally propose a post-hoc Latent Exploration Stage to automatically align
latent dimensions with semantic factors, and introduce a Koopman-inspired model
that achieves state-of-the-art results. Moreover, we show that Vision-Language
Models can automate dataset annotation and serve as zero-shot disentanglement
evaluators, removing the need for manual labels and human intervention.
Together, these contributions provide a robust and scalable foundation for
advancing multi-factor sequential disentanglement.

</details>


### [357] [Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling](https://arxiv.org/abs/2510.17314)
*Lipeng Xie,Sen Huang,Zhuo Zhang,Anni Zou,Yunpeng Zhai,Dingchao Ren,Kezun Zhang,Haoyuan Hu,Boyin Liu,Haoran Chen,Zhaoyang Liu,Bolin Ding*

Main category: cs.LG

TL;DR: 提出了一种无需训练的新框架，通过两阶段方法高效生成可解释的奖励模型，显著减少数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖昂贵的数据集且缺乏可解释性，需要在可扩展性和可靠性之间权衡。

Method: 采用两阶段方法：1) 通过验证引导的Propose-Evaluate-Revise流程生成高质量、查询特定的评分标准；2) 通过信息论编码率最大化将其泛化为紧凑的核心集。

Result: 仅需70个偏好对（1.5%源数据），即可使小型模型（如Qwen3-8B）超越完全训练的专用模型。

Conclusion: 该框架为奖励模型提供了一条可扩展、可解释且数据高效的路径。

Abstract: Reward models are essential for aligning Large Language Models (LLMs) with
human values, yet their development is hampered by costly preference datasets
and poor interpretability. While recent rubric-based approaches offer
transparency, they often lack systematic quality control and optimization,
creating a trade-off between scalability and reliability. We address these
limitations with a novel, training-free framework built on a key assumption:
\textit{evaluation rubrics underlying human preferences exhibit significant
generalization ability across diverse queries}, a property that enables
remarkable data efficiency. Our two-stage approach first infers high-quality,
query-specific rubrics using a validation-guided
\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these
granular rubrics into a compact, non-redundant core set by maximizing an
\textbf{information-theoretic coding rate}. The final output is an
interpretable, hierarchical "Theme-Tips" rubric set. Extensive experiments
demonstrate the framework's exceptional data efficiency and performance.
Critically, using just 70 preference pairs (1.5\% of the source data), our
method also empowers smaller models like Qwen3-8B to outperform specialized,
fully-trained counterparts. This work pioneers a scalable, interpretable, and
data-efficient path for reward modeling.

</details>


### [358] [Localist LLMs with Recruitment Learning](https://arxiv.org/abs/2510.17358)
*Joachim Diederich*

Main category: cs.LG

TL;DR: 提出了一种新型框架，用于训练具有可调内部表示的大型语言模型，支持从局部到分布式编码的动态切换。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在透明性和性能之间的权衡问题，提供动态调整的能力。

Method: 引入局部性调节参数、信息理论招募机制和分层招募框架，结合稀疏惩罚和动态规则注入。

Result: 提供了严格的数学结果，证明注意力集中在语义相关块的条件，并确保模型复杂性与数据编码效率的平衡。

Conclusion: 该框架支持在透明性和高性能之间动态切换，适用于需要透明性和能力的监管领域。

Abstract: We present a novel framework for training large language models with
continuously adjustable internal representations that span the full spectrum
from localist (interpretable, rule-based) to distributed (generalizable,
efficient) encodings. The key innovations are (1) a locality dial, a tunable
parameter that dynamically controls the degree of localization during both
training and inference without requiring model retraining, (2) an
information-theoretic recruitment mechanism that adaptively allocates semantic
blocks as needed, eliminating the requirement for complete domain knowledge at
initialization, and (3) a hierarchical recruitment framework that extends
capacity allocation to entire specialized LLMs, enabling multi-granularity
architectural adaptation. This is achieved through group sparsity penalties on
attention mechanisms, information-theoretic anchor design, dynamic rule
injection, and principled recruitment criteria based on penalized likelihood
with explicit units. We provide rigorous mathematical results establishing
explicit threshold conditions under which attention provably concentrates on
semantically relevant blocks at stationary points, with exact bounds on
attention entropy and pointer fidelity. The hierarchical recruitment mechanism
provides convergence guarantees at both the block level (fine-grained,
within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the
system discovers semantic partitions that balance model complexity against data
encoding efficiency. This framework enables practitioners to continuously
interpolate between interpretable and high-performance modes while adapting
architectural capacity at multiple granularities, supporting applications in
regulated domains requiring both transparency and capability.

</details>


### [359] [Model Metamers Reveal Invariances in Graph Neural Networks](https://arxiv.org/abs/2510.17378)
*Wei Xu,Xiaoyi Jiang,Lixiang Xu,Dechao Tang*

Main category: cs.LG

TL;DR: 论文研究了图神经网络（GNNs）中的不变性问题，通过生成模型“metamers”揭示了GNNs在表示空间中极端的不变性，并探讨了其与人类不变性之间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在模拟人类大脑的不变性机制方面取得进展，但人工神经网络与人类在不变性属性上仍存在显著差距。本文旨在探究GNNs中的不变性行为。

Method: 提出了一种生成模型“metamers”的技术，通过优化输入图使其内部节点激活与参考图匹配，从而在表示空间中生成等效但结构和节点特征不同的图。

Result: 研究发现多个经典GNN架构中存在极端的表示不变性，且模型架构和训练策略的调整仅能部分缓解这一问题，无法根本缩小与人类不变性的差距。

Conclusion: 通过量化metamer图与原图的偏差，揭示了当前GNNs的独特失败模式，并提供了模型评估的补充基准。

Abstract: In recent years, deep neural networks have been extensively employed in
perceptual systems to learn representations endowed with invariances, aiming to
emulate the invariance mechanisms observed in the human brain. However, studies
in the visual and auditory domains have confirmed that significant gaps remain
between the invariance properties of artificial neural networks and those of
humans. To investigate the invariance behavior within graph neural networks
(GNNs), we introduce a model ``metamers'' generation technique. By optimizing
input graphs such that their internal node activations match those of a
reference graph, we obtain graphs that are equivalent in the model's
representation space, yet differ significantly in both structure and node
features. Our theoretical analysis focuses on two aspects: the local metamer
dimension for a single node and the activation-induced volume change of the
metamer manifold. Utilizing this approach, we uncover extreme levels of
representational invariance across several classic GNN architectures. Although
targeted modifications to model architecture and training strategies can
partially mitigate this excessive invariance, they fail to fundamentally bridge
the gap to human-like invariance. Finally, we quantify the deviation between
metamer graphs and their original counterparts, revealing unique failure modes
of current GNNs and providing a complementary benchmark for model evaluation.

</details>


### [360] [Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks](https://arxiv.org/abs/2510.17380)
*Julen Cestero,Carmine Delle Femine,Kenji S. Muro,Marco Quartulli,Marcello Restelli*

Main category: cs.LG

TL;DR: 用物理信息神经网络（PINNs）替代高成本智能电网模拟器，优化强化学习策略训练，提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 智能电网中能量管理的复杂性及强化学习（RL）样本效率低的问题。

Method: 使用PINNs构建替代模型，减少对高成本模拟器的依赖，加速RL策略收敛。

Result: 在更短时间内达到收敛结果，显著提高训练效率。

Conclusion: PINNs是优化RL在智能电网中应用的有效方法。

Abstract: Optimizing the energy management within a smart grids scenario presents
significant challenges, primarily due to the complexity of real-world systems
and the intricate interactions among various components. Reinforcement Learning
(RL) is gaining prominence as a solution for addressing the challenges of
Optimal Power Flow in smart grids. However, RL needs to iterate compulsively
throughout a given environment to obtain the optimal policy. This means
obtaining samples from a, most likely, costly simulator, which can lead to a
sample efficiency problem. In this work, we address this problem by
substituting costly smart grid simulators with surrogate models built using
Phisics-informed Neural Networks (PINNs), optimizing the RL policy training
process by arriving to convergent results in a fraction of the time employed by
the original environment.

</details>


### [361] [Beyond Binary Out-of-Distribution Detection: Characterizing Distributional Shifts with Multi-Statistic Diffusion Trajectories](https://arxiv.org/abs/2510.17381)
*Achref Jaziri,Martin Rogmann,Martin Mundt,Visvanathan Ramesh*

Main category: cs.LG

TL;DR: DISC方法通过扩散模型提取多维特征，不仅提升OOD检测性能，还能分类OOD类型。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法仅输出标量异常分数，无法区分OOD数据类型，限制了其实际应用。

Method: 提出DISC方法，利用扩散模型的迭代去噪过程提取多维特征，捕捉不同噪声水平的统计差异。

Result: 实验表明DISC在OOD检测上达到或超越现有方法，并能分类OOD类型。

Conclusion: DISC实现了从简单二元检测到更细粒度OOD检测的转变。

Abstract: Detecting out-of-distribution (OOD) data is critical for machine learning, be
it for safety reasons or to enable open-ended learning. However, beyond mere
detection, choosing an appropriate course of action typically hinges on the
type of OOD data encountered. Unfortunately, the latter is generally not
distinguished in practice, as modern OOD detection methods collapse
distributional shifts into single scalar outlier scores. This work argues that
scalar-based methods are thus insufficient for OOD data to be properly
contextualized and prospectively exploited, a limitation we overcome with the
introduction of DISC: Diffusion-based Statistical Characterization. DISC
leverages the iterative denoising process of diffusion models to extract a
rich, multi-dimensional feature vector that captures statistical discrepancies
across multiple noise levels. Extensive experiments on image and tabular
benchmarks show that DISC matches or surpasses state-of-the-art detectors for
OOD detection and, crucially, also classifies OOD type, a capability largely
absent from prior work. As such, our work enables a shift from simple binary
OOD detection to a more granular detection.

</details>


### [362] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 论文探讨了生成视觉模型中内部表征的演变，比较了GANs、VAEs与扩散模型，提出了严格与广义合成的区分，并论证了扩散模型如何挑战统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解生成模型中内部表征的演变，特别是扩散模型如何通过分布式表征挑战传统假设。

Method: 通过模型架构分析和实验干预层间表征，展示了扩散模型如何分散表征负担。

Result: 扩散模型通过分布式表征挑战了统一内部空间的假设，表明生成AI是专业化过程的涌现配置。

Conclusion: 生成AI应被视为专业化过程的涌现配置，而非内容的直接合成。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [363] [TabR1: Taming GRPO for tabular reasoning LLMs](https://arxiv.org/abs/2510.17385)
*Pengxiang Cai,Zihao Gao,Jintai Chen*

Main category: cs.LG

TL;DR: TabR1是首个用于表格预测的多步推理LLM，通过PRPO方法提升泛化能力和解释性，在少样本和零样本任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统表格预测方法（如梯度提升决策树）解释性差且跨任务迁移能力弱，LLMs虽具潜力但未充分开发。

Method: 提出PRPO方法，通过列排列不变性结构先验和多标签保留排列，将稀疏奖励转化为密集学习信号。

Result: TabR1在全监督微调下与强基线相当，零样本下接近32-shot基线性能，8B模型显著优于更大LLMs。

Conclusion: TabR1结合PRPO有效激活LLMs的推理能力，为表格预测提供高效、可解释且适应性强的解决方案。

Abstract: Tabular prediction has traditionally relied on gradient-boosted decision
trees and specialized deep learning models, which excel within tasks but
provide limited interpretability and weak transfer across tables. Reasoning
large language models (LLMs) promise cross-task adaptability with trans- parent
reasoning traces, yet their potential has not been fully realized for tabular
data. This paper presents TabR1, the first reasoning LLM for tabular prediction
with multi-step reasoning. At its core is Permutation Relative Policy
Optimization (PRPO), a simple yet efficient reinforcement learning method that
encodes column-permutation invariance as a structural prior. By construct- ing
multiple label-preserving permutations per sample and estimating advantages
both within and across permutations, PRPO transforms sparse rewards into dense
learning signals and improves generalization. With limited supervision, PRPO
activates the reasoning ability of LLMs for tabular prediction, enhancing
few-shot and zero-shot performance as well as interpretability. Comprehensive
experiments demonstrate that TabR1 achieves performance comparable to strong
baselines under full-supervision fine-tuning. In the zero-shot setting, TabR1
approaches the performance of strong baselines under the 32-shot setting.
Moreover, TabR1 (8B) substantially outperforms much larger LLMs across various
tasks, achieving up to 53.17% improvement over DeepSeek-R1 (685B).

</details>


### [364] [Exploration via Feature Perturbation in Contextual Bandits](https://arxiv.org/abs/2510.17390)
*Seouh-won Yi,Min-hwan Oh*

Main category: cs.LG

TL;DR: 提出了一种名为特征扰动的简单但强大的技术，通过直接对特征输入注入随机性，避免了传统随机化方法的计算复杂性和理论限制。


<details>
  <summary>Details</summary>
Motivation: 现有随机化算法在广义线性赌博机问题中通常具有较高的遗憾界（$\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$），且计算效率较低。本文旨在提出一种更高效且理论性能更优的方法。

Method: 通过直接对特征输入注入随机性（特征扰动），避免了参数采样或奖励噪声的添加。

Result: 算法实现了$\tilde{\mathcal{O}}(d\sqrt{T})$的最坏情况遗憾界，优于现有方法，并在计算效率和模型扩展性上表现优异。

Conclusion: 特征扰动不仅超越了现有方法，还统一了强实践性能和最佳理论保证。

Abstract: We propose feature perturbation, a simple yet powerful technique that injects
randomness directly into feature inputs, instead of randomizing unknown
parameters or adding noise to rewards. Remarkably, this algorithm achieves
$\tilde{\mathcal{O}}(d\sqrt{T})$ worst-case regret bound for generalized linear
bandits, while avoiding the $\tilde{\mathcal{O}}(d^{3/2}\sqrt{T})$ regret
typical of existing randomized bandit algorithms. Because our algorithm eschews
parameter sampling, it is both computationally efficient and naturally extends
to non-parametric or neural network models. We verify these advantages through
empirical evaluations, demonstrating that feature perturbation not only
surpasses existing methods but also unifies strong practical performance with
best-known theoretical guarantees.

</details>


### [365] [Finite-Time Bounds for Average-Reward Fitted Q-Iteration](https://arxiv.org/abs/2510.17391)
*Jongmin Lee,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 该论文首次在弱通信MDP假设下，提出了平均奖励离线强化学习的样本复杂度结果，并引入了锚定拟合Q迭代方法。


<details>
  <summary>Details</summary>
Motivation: 现有关于平均奖励离线强化学习的研究较少，且依赖严格假设（如遍历性或线性MDP），本文旨在放宽这些限制。

Method: 提出锚定拟合Q迭代（Anchored Fitted Q-Iteration），结合标准拟合Q迭代与锚定机制，后者类似权重衰减。

Result: 锚定机制是实现有限时间分析的关键，且方法适用于单轨迹生成的数据集。

Conclusion: 锚定机制在平均奖励设置中至关重要，扩展了离线强化学习的适用范围。

Abstract: Although there is an extensive body of work characterizing the sample
complexity of discounted-return offline RL with function approximations, prior
work on the average-reward setting has received significantly less attention,
and existing approaches rely on restrictive assumptions, such as ergodicity or
linearity of the MDP. In this work, we establish the first sample complexity
results for average-reward offline RL with function approximation for weakly
communicating MDPs, a much milder assumption. To this end, we introduce
Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration
with an anchor mechanism. We show that the anchor, which can be interpreted as
a form of weight decay, is crucial for enabling finite-time analysis in the
average-reward setting. We also extend our finite-time analysis to the setup
where the dataset is generated from a single-trajectory rather than IID
transitions, again leveraging the anchor mechanism.

</details>


### [366] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: MILES是一种动态调整学习率的调度器，用于平衡多模态学习，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态网络常因模态过拟合导致性能不佳，MILES旨在解决这一问题。

Method: 通过动态调整学习率，平衡各模态的学习速度。

Result: MILES在四个任务中优于七种基线方法，平衡了模态使用。

Conclusion: 平衡多模态学习显著提升模型性能，MILES是有效解决方案。

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [367] [RINS-T: Robust Implicit Neural Solvers for Time Series Linear Inverse Problems](https://arxiv.org/abs/2510.17396)
*Keivan Faghih Niresi,Zepeng Zhang,Olga Fink*

Main category: cs.LG

TL;DR: RINS-T是一种无需预训练数据的深度先验框架，用于解决时间序列线性逆问题，具有高恢复性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常受缺失值、噪声和异常值影响，传统深度学习方法需要大量预训练且难以应对分布变化。

Method: RINS-T利用神经网络作为隐式先验，结合鲁棒优化技术，并引入引导输入初始化、输入扰动和凸输出组合技术。

Result: RINS-T在无需预训练数据的情况下实现了高恢复性能，且对异常值具有鲁棒性。

Conclusion: RINS-T是一种灵活有效的解决方案，适用于复杂现实世界中的时间序列问题。

Abstract: Time series data are often affected by various forms of corruption, such as
missing values, noise, and outliers, which pose significant challenges for
tasks such as forecasting and anomaly detection. To address these issues,
inverse problems focus on reconstructing the original signal from corrupted
data by leveraging prior knowledge about its underlying structure. While deep
learning methods have demonstrated potential in this domain, they often require
extensive pretraining and struggle to generalize under distribution shifts. In
this work, we propose RINS-T (Robust Implicit Neural Solvers for Time Series
Linear Inverse Problems), a novel deep prior framework that achieves high
recovery performance without requiring pretraining data. RINS-T leverages
neural networks as implicit priors and integrates robust optimization
techniques, making it resilient to outliers while relaxing the reliance on
Gaussian noise assumptions. To further improve optimization stability and
robustness, we introduce three key innovations: guided input initialization,
input perturbation, and convex output combination techniques. Each of these
contributions strengthens the framework's optimization stability and
robustness. These advancements make RINS-T a flexible and effective solution
for addressing complex real-world time series challenges. Our code is available
at https://github.com/EPFL-IMOS/RINS-T.

</details>


### [368] [S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction](https://arxiv.org/abs/2510.17406)
*Tiezhi Wang,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.LG

TL;DR: S4ECG是一种新型深度学习架构，利用结构化状态空间模型进行多时段心律失常分类，显著优于单时段方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以同时捕捉心电信号的全局趋势和局部波形特征，S4ECG旨在填补这一空白。

Method: 采用结构化状态空间模型，结合多时段预测，优化时间依赖窗口（10-20分钟）。

Result: 多时段预测在宏观AUROC上提升1.0-11.6%，房颤特异性从0.718-0.979提升至0.967-0.998。

Conclusion: S4ECG推动了时间感知心律失常检测算法的发展，为复杂心律失常（如房颤和房扑）的ECG解读提供了新可能。

Abstract: The electrocardiogram (ECG) exemplifies biosignal-based time series with
continuous, temporally ordered structure reflecting cardiac physiological and
pathophysiological dynamics. Detailed analysis of these dynamics has proven
challenging, as conventional methods capture either global trends or local
waveform features but rarely their simultaneous interplay at high temporal
resolution. To bridge global and local signal analysis, we introduce S4ECG, a
novel deep learning architecture leveraging structured state space models for
multi-epoch arrhythmia classification. Our joint multi-epoch predictions
significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC,
with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998,
demonstrating superior performance in-distribution and enhanced
out-of-distribution robustness. Systematic investigation reveals optimal
temporal dependency windows spanning 10-20 minutes for peak performance. This
work contributes to a paradigm shift toward temporally-aware arrhythmia
detection algorithms, opening new possibilities for ECG interpretation, in
particular for complex arrhythmias like atrial fibrillation and atrial flutter.

</details>


### [369] [A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation](https://arxiv.org/abs/2510.17414)
*Hequn Li,Zhongwei Deng,Chunlin Jiang,Yvxin He andZhansheng Ning*

Main category: cs.LG

TL;DR: 提出了一种名为CDUA的新方法，结合特征工程和深度学习，用于锂离子电池容量的准确预测和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池容量的准确预测及其不确定性量化对可靠的电池管理至关重要，但由于老化的随机性，这一任务具有挑战性。

Method: CDUA方法结合了扩散生成模型和注意力机制，通过特征选择和U-Net结构捕捉时间依赖性，并使用去噪网络重构容量值。

Result: 在真实车辆数据上的实验显示，CDUA模型的相对MAE为0.94%，相对RMSE为1.14%，95%置信区间宽度为3.74%。

Conclusion: CDUA在电池容量估计和不确定性量化方面表现出色，优于现有主流方法。

Abstract: Accurate prediction of lithium-ion battery capacity and its associated
uncertainty is essential for reliable battery management but remains
challenging due to the stochastic nature of aging. This paper presents a novel
method, termed the Condition Diffusion U-Net with Attention (CDUA), which
integrates feature engineering and deep learning to address this challenge. The
proposed approach employs a diffusion-based generative model for time-series
forecasting and incorporates attention mechanisms to enhance predictive
performance. Battery capacity is first derived from real-world vehicle
operation data. The most relevant features are then identified using the
Pearson correlation coefficient and the XGBoost algorithm. These features are
used to train the CDUA model, which comprises two core components: (1) a
contextual U-Net with self-attention to capture complex temporal dependencies,
and (2) a denoising network to reconstruct accurate capacity values from noisy
observations. Experimental validation on the real-world vehicle data
demonstrates that the proposed CDUA model achieves a relative Mean Absolute
Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%,
with a narrow 95% confidence interval of 3.74% in relative width. These results
confirm that CDUA provides both accurate capacity estimation and reliable
uncertainty quantification. Comparative experiments further verify its
robustness and superior performance over existing mainstream approaches.

</details>


### [370] [Diffusion Models as Dataset Distillation Priors](https://arxiv.org/abs/2510.17421)
*Duo Su,Huyu Wu,Huanran Chen,Yiming Shi,Yuzhu Wang,Xi Ye,Jun Zhu*

Main category: cs.LG

TL;DR: 提出Diffusion As Priors (DAP)方法，通过量化特征空间中合成数据与真实数据的相似性，提升数据集蒸馏的代表性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有生成式数据集蒸馏方法忽视扩散模型的内在代表性先验，需依赖外部约束提升数据质量。

Method: 利用Mercer核量化特征空间相似性，将代表性先验作为反向扩散过程的指导。

Result: 在ImageNet-1K等大规模数据集上，DAP在生成高保真数据集和跨架构泛化方面优于现有方法。

Conclusion: DAP建立了扩散先验与数据集蒸馏目标的理论联系，并提供了无需训练的实用框架。

Abstract: Dataset distillation aims to synthesize compact yet informative datasets from
large ones. A significant challenge in this field is achieving a trifecta of
diversity, generalization, and representativeness in a single distilled
dataset. Although recent generative dataset distillation methods adopt powerful
diffusion models as their foundation models, the inherent representativeness
prior in diffusion models is overlooked. Consequently, these approaches often
necessitate the integration of external constraints to enhance data quality. To
address this, we propose Diffusion As Priors (DAP), which formalizes
representativeness by quantifying the similarity between synthetic and real
data in feature space using a Mercer kernel. We then introduce this prior as
guidance to steer the reverse diffusion process, enhancing the
representativeness of distilled samples without any retraining. Extensive
experiments on large-scale datasets, such as ImageNet-1K and its subsets,
demonstrate that DAP outperforms state-of-the-art methods in generating
high-fidelity datasets while achieving superior cross-architecture
generalization. Our work not only establishes a theoretical connection between
diffusion priors and the objectives of dataset distillation but also provides a
practical, training-free framework for improving the quality of the distilled
dataset.

</details>


### [371] [Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models](https://arxiv.org/abs/2510.17457)
*Li Sun,Zhenhao Huang,Ming Zhang,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于局部结构的自适应消息传递方法（GBN），通过连接局部黎曼几何与MPNNs，解决了图神经网络中的过度平滑和过度挤压问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要采用全局方法，可能导致某些区域受益而其他区域受损，从而限制了模型的表达能力。本文通过理论分析发现，增加谱间隙会导致梯度消失，从而影响消息传递的效果。

Method: 提出了一种局部方法，通过非齐次边界条件（Robin条件）设计GBN网络，结合局部瓶颈调整，并提供了理论保证。

Result: 在亲同性和异质性图上的实验表明，GBN具有强大的表达能力，且在网络深度超过256层时仍无性能下降。

Conclusion: GBN通过局部调整有效解决了过度平滑和过度挤压问题，展示了其在深层网络中的鲁棒性和表达能力。

Abstract: Message Passing Neural Networks (MPNNs) is the building block of graph
foundation models, but fundamentally suffer from oversmoothing and
oversquashing. There has recently been a surge of interest in fixing both
issues. Existing efforts primarily adopt global approaches, which may be
beneficial in some regions but detrimental in others, ultimately leading to the
suboptimal expressiveness. In this paper, we begin by revisiting oversquashing
through a global measure -- spectral gap $\lambda$ -- and prove that the
increase of $\lambda$ leads to gradient vanishing with respect to the input
features, thereby undermining the effectiveness of message passing. Motivated
by such theoretical insights, we propose a \textbf{local} approach that
adaptively adjusts message passing based on local structures. To achieve this,
we connect local Riemannian geometry with MPNNs, and establish a novel
nonhomogeneous boundary condition to address both oversquashing and
oversmoothing. Building on the Robin condition, we design a GBN network with
local bottleneck adjustment, coupled with theoretical guarantees. Extensive
experiments on homophilic and heterophilic graphs show the expressiveness of
GBN. Furthermore, GBN does not exhibit performance degradation even when the
network depth exceeds $256$ layers.

</details>


### [372] [Explainable AI for microseismic event detection](https://arxiv.org/abs/2510.17458)
*Ayrat Abdullin,Denis Anikiev,Umair bin Waheed*

Main category: cs.LG

TL;DR: 论文通过可解释AI技术（如Grad-CAM和SHAP）解释PhaseNet模型的决策，并引入SHAP门控推理方案提升其可靠性，最终在测试集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管PhaseNet等深度神经网络在检测微地震事件中表现出高精度，但其黑盒特性在关键应用中存在隐患，因此需要提升其可解释性和可靠性。

Method: 应用Grad-CAM和SHAP技术解释PhaseNet的决策，并设计SHAP门控推理方案结合模型输出与解释性指标以减少错误。

Result: 在9,000个波形测试集上，SHAP门控模型的F1分数为0.98（精度0.99，召回率0.97），优于基线PhaseNet（F1分数0.97），且对噪声更具鲁棒性。

Conclusion: 可解释AI不仅能解释深度学习模型，还能直接提升其性能，为构建可信的自动化地震检测器提供了模板。

Abstract: Deep neural networks like PhaseNet show high accuracy in detecting
microseismic events, but their black-box nature is a concern in critical
applications. We apply explainable AI (XAI) techniques, such as
Gradient-weighted Class Activation Mapping (Grad-CAM) and Shapley Additive
Explanations (SHAP), to interpret the PhaseNet model's decisions and improve
its reliability. Grad-CAM highlights that the network's attention aligns with
P- and S-wave arrivals. SHAP values quantify feature contributions, confirming
that vertical-component amplitudes drive P-phase picks while horizontal
components dominate S-phase picks, consistent with geophysical principles.
Leveraging these insights, we introduce a SHAP-gated inference scheme that
combines the model's output with an explanation-based metric to reduce errors.
On a test set of 9,000 waveforms, the SHAP-gated model achieved an F1-score of
0.98 (precision 0.99, recall 0.97), outperforming the baseline PhaseNet
(F1-score 0.97) and demonstrating enhanced robustness to noise. These results
show that XAI can not only interpret deep learning models but also directly
enhance their performance, providing a template for building trust in automated
seismic detectors.

</details>


### [373] [CrossStateECG: Multi-Scale Deep Convolutional Network with Attention for Rest-Exercise ECG Biometrics](https://arxiv.org/abs/2510.17467)
*Dan Zheng,Jing Feng,Juan Liu*

Main category: cs.LG

TL;DR: CrossStateECG是一种针对休息-运动跨状态条件下的ECG生物识别模型，通过多尺度深度卷积特征提取和注意力机制，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 当前ECG生物识别研究主要关注静态条件，而休息-运动场景下的性能下降问题尚未解决。

Method: 结合多尺度深度卷积特征提取和注意力机制，构建跨状态认证模型。

Result: 在Rest-to-Exercise和Exercise-to-Rest场景下分别达到92.50%和94.72%的准确率，且在静态和混合场景下表现优异。

Conclusion: CrossStateECG在动态现实场景中具有潜力，验证了其泛化能力。

Abstract: Current research in Electrocardiogram (ECG) biometrics mainly emphasizes
resting-state conditions, leaving the performance decline in rest-exercise
scenarios largely unresolved. This paper introduces CrossStateECG, a robust
ECG-based authentication model explicitly tailored for cross-state
(rest-exercise) conditions. The proposed model creatively combines multi-scale
deep convolutional feature extraction with attention mechanisms to ensure
strong identification across different physiological states. Experimental
results on the exercise-ECGID dataset validate the effectiveness of
CrossStateECG, achieving an identification accuracy of 92.50% in the
Rest-to-Exercise scenario (training on resting ECG and testing on post-exercise
ECG) and 94.72% in the Exercise-to-Rest scenario (training on post-exercise ECG
and testing on resting ECG). Furthermore, CrossStateECG demonstrates
exceptional performance across both state combinations, reaching an accuracy of
99.94% in Rest-to-Rest scenarios and 97.85% in Mixed-to-Mixed scenarios.
Additional validations on the ECG-ID and MIT-BIH datasets further confirmed the
generalization abilities of CrossStateECG, underscoring its potential as a
practical solution for post-exercise ECG-based authentication in dynamic
real-world settings.

</details>


### [374] [Layer Specialization Underlying Compositional Reasoning in Transformers](https://arxiv.org/abs/2510.17469)
*Jing Liu*

Main category: cs.LG

TL;DR: Transformers通过训练发展出模块化、可解释的机制，支持组合推理，其内部算法结构与行为表现相关。


<details>
  <summary>Details</summary>
Motivation: 研究Transformers在未训练序列上的组合推理能力，探索其内部机制与行为表现的关系。

Method: 使用随机层次模型（RHM）生成序列，训练模型并评估四种泛化条件。

Result: 模型在任务复杂性和上下文示例数量增加时表现提升，且不同层出现专业化分工。

Conclusion: Transformers通过分层专业化机制实现组合推理，内部结构与行为表现密切相关。

Abstract: Transformers exhibit compositional reasoning on sequences not observed during
training, a capability often attributed to in-context learning (ICL) and skill
composition. We investigate this phenomenon using the Random Hierarchy Model
(RHM), a probabilistic context-free grammar that generates sequences through
recursive rule application. Models are trained on subsets of sequences and
evaluated across four generalization conditions: memorization, in-distribution
generalization, out-of-distribution generalization with the same rules, and
cross-layer transfer. Behaviorally, performance improves systematically with
task complexity and the number of in-context examples, with out-of-distribution
tasks requiring substantially more examples than in-distribution scenarios.
Mechanistically, we identify a progressive emergence of layer specialization
during training that correlates with generalization performance. Principal
component analysis and attention pattern clustering reveal that transformers
develop structured, hierarchically organized representations in specialized
layers. These results demonstrate that transformers develop modular,
interpretable mechanisms supporting compositional reasoning, linking internal
algorithmic structure to observed behavioral capabilities.

</details>


### [375] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种轻量级视觉Transformer，用于区分心源性肺水肿与非心源性肺部疾病，在LUS视频分类中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决心源性肺水肿与非心源性肺部疾病在LUS视频中视觉变异性高、分类困难的问题。

Method: 提出ZACH-ViT模型，移除位置嵌入和[CLS]标记，并采用ShuffleStrides数据增强技术。

Result: 在380个LUS视频上验证，ZACH-ViT的ROC-AUC达0.80/0.79，优于其他模型。

Conclusion: 通过架构设计与数据结构对齐，小数据医学影像中性能可超越规模优势。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


### [376] [DAMSDAN: Distribution-Aware Multi-Source Domain Adaptation Network for Cross-Domain EEG-based Emotion Recognition](https://arxiv.org/abs/2510.17475)
*Fo Hu,Can Wang,Qinxu Zheng,Xusheng Yang,Bin Zhou,Gang Li,Yu Sun,Wen-an Zhang*

Main category: cs.LG

TL;DR: DAMSDAN模型通过动态多源域适应和原型引导对齐，提升了跨域EEG情绪识别的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决跨域EEG情绪识别中个体差异大、负迁移和语义一致性不足的问题。

Method: 结合原型约束与对抗学习，动态加权源域贡献，并通过双伪标签交互增强类别级对齐。

Result: 在SEED、SEED-IV和FACED数据集上取得高准确率（最高95.12%）。

Conclusion: DAMSDAN有效提升了跨域EEG情绪识别的性能，并通过实验验证了其鲁棒性。

Abstract: Significant inter-individual variability limits the generalization of
EEG-based emotion recognition under cross-domain settings. We address two core
challenges in multi-source adaptation: (1) dynamically modeling distributional
heterogeneity across sources and quantifying their relevance to a target to
reduce negative transfer; and (2) achieving fine-grained semantic consistency
to strengthen class discrimination. We propose a distribution-aware
multi-source domain adaptation network (DAMSDAN). DAMSDAN integrates
prototype-based constraints with adversarial learning to drive the encoder
toward discriminative, domain-invariant emotion representations. A domain-aware
source weighting strategy based on maximum mean discrepancy (MMD) dynamically
estimates inter-domain shifts and reweights source contributions. In addition,
a prototype-guided conditional alignment module with dual pseudo-label
interaction enhances pseudo-label reliability and enables category-level,
fine-grained alignment, mitigating noise propagation and semantic drift.
Experiments on SEED and SEED-IV show average accuracies of 94.86\% and 79.78\%
for cross-subject, and 95.12\% and 83.15\% for cross-session protocols. On the
large-scale FACED dataset, DAMSDAN achieves 82.88\% (cross-subject). Extensive
ablations and interpretability analyses corroborate the effectiveness of the
proposed framework for cross-domain EEG-based emotion recognition.

</details>


### [377] [Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement](https://arxiv.org/abs/2510.17478)
*Guillaume Rongier,Luk Peeters*

Main category: cs.LG

TL;DR: 研究探讨了生成对抗网络（GAN）在预测河流沉积物中的应用，发现其潜在空间纠缠导致数据匹配困难，但通过微调可改善结果。


<details>
  <summary>Details</summary>
Motivation: 地下决策成本高且不确定性大，直接嵌入地质知识到预测模型是一种有价值的替代方案。

Method: 使用GAN训练生成模型，并尝试四种反演方法匹配井和地震数据。

Result: GAN在匹配数据时遇到困难，但通过微调潜在空间可减少不匹配。

Conclusion: GAN已具备整合到地质建模工作流的能力，但需进一步评估其稳健性和在地质解释中的应用。

Abstract: High costs and uncertainties make subsurface decision-making challenging, as
acquiring new data is rarely scalable. Embedding geological knowledge directly
into predictive models offers a valuable alternative. A joint approach enables
just that: process-based models that mimic geological processes can help train
generative models that make predictions more efficiently. This study explores
whether a generative adversarial network (GAN) - a type of deep-learning
algorithm for generative modeling - trained to produce fluvial deposits can be
inverted to match well and seismic data. Four inversion approaches applied to
three test samples with 4, 8, and 20 wells struggled to match these well data,
especially as the well number increased or as the test sample diverged from the
training data. The key bottleneck lies in the GAN's latent representation: it
is entangled, so samples with similar sedimentological features are not
necessarily close in the latent space. Label conditioning or latent
overparameterization can partially disentangle the latent space during
training, although not yet sufficiently for a successful inversion. Fine-tuning
the GAN to restructure the latent space locally reduces mismatches to
acceptable levels for all test cases, with and without seismic data. But this
approach depends on an initial, partially successful inversion step, which
influences the quality and diversity of the final samples. Overall, GANs can
already handle the tasks required for their integration into geomodeling
workflows. We still need to further assess their robustness, and how to best
leverage them in support of geological interpretation.

</details>


### [378] [Unified Privacy Guarantees for Decentralized Learning via Matrix Factorization](https://arxiv.org/abs/2510.17480)
*Aurélien Bellet,Edwige Cyffers,Davide Frey,Romaric Gaudel,Dimitri Lerévérend,François Taïani*

Main category: cs.LG

TL;DR: 论文提出了一种基于矩阵分解的隐私计算方法，用于去中心化学习中的差分隐私分析，并开发了新的算法MAFALDA-SGD。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习（DL）在保护数据隐私的同时实现协作训练，但现有差分隐私（DP）方法在DL中的隐私-效用权衡表现不佳，可能是由于DP计算方法受限。

Method: 通过矩阵分解（MF）方法分析时间噪声相关性，将其推广到DL中，提出统一框架，并开发了MAFALDA-SGD算法。

Result: MAFALDA-SGD在合成和真实图数据上优于现有方法。

Conclusion: 论文证明了MF方法在DL中的适用性，为DP-DL算法提供了更严格的隐私计算框架。

Abstract: Decentralized Learning (DL) enables users to collaboratively train models
without sharing raw data by iteratively averaging local updates with neighbors
in a network graph. This setting is increasingly popular for its scalability
and its ability to keep data local under user control. Strong privacy
guarantees in DL are typically achieved through Differential Privacy (DP), with
results showing that DL can even amplify privacy by disseminating noise across
peer-to-peer communications. Yet in practice, the observed privacy-utility
trade-off often appears worse than in centralized training, which may be due to
limitations in current DP accounting methods for DL. In this paper, we show
that recent advances in centralized DP accounting based on Matrix Factorization
(MF) for analyzing temporal noise correlations can also be leveraged in DL. By
generalizing existing MF results, we show how to cast both standard DL
algorithms and common trust models into a unified formulation. This yields
tighter privacy accounting for existing DP-DL algorithms and provides a
principled way to develop new ones. To demonstrate the approach, we introduce
MAFALDA-SGD, a gossip-based DL algorithm with user-level correlated noise that
outperforms existing methods on synthetic and real-world graphs.

</details>


### [379] [Local properties of neural networks through the lens of layer-wise Hessians](https://arxiv.org/abs/2510.17486)
*Maxim Bolshim,Alexander Kugaevskikh*

Main category: cs.LG

TL;DR: 论文提出了一种通过层间Hessian矩阵分析神经网络的方法，揭示了参数空间局部几何特性与网络性能的关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析局部Hessian矩阵的谱特性，揭示神经网络在过拟合、欠参数化和表达能力等方面的定量模式。

Method: 定义了每层参数对标量函数的二阶导数矩阵（局部Hessian），并通过111个实验在37个数据集上进行了实证研究。

Result: 实验结果表明，局部Hessian在训练过程中表现出结构规律性，其谱特性与泛化性能相关。

Conclusion: 该框架为利用局部几何分析指导神经网络诊断和设计提供了基础，连接了优化几何与功能行为。

Abstract: We introduce a methodology for analyzing neural networks through the lens of
layer-wise Hessian matrices. The local Hessian of each functional block (layer)
is defined as the matrix of second derivatives of a scalar function with
respect to the parameters of that layer. This concept provides a formal tool
for characterizing the local geometry of the parameter space. We show that the
spectral properties of local Hessians, such as the distribution of eigenvalues,
reveal quantitative patterns associated with overfitting,
underparameterization, and expressivity in neural network architectures. We
conduct an extensive empirical study involving 111 experiments across 37
datasets. The results demonstrate consistent structural regularities in the
evolution of local Hessians during training and highlight correlations between
their spectra and generalization performance. These findings establish a
foundation for using local geometric analysis to guide the diagnosis and design
of deep neural networks. The proposed framework connects optimization geometry
with functional behavior and offers practical insight for improving network
architectures and training stability.

</details>


### [380] [I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models](https://arxiv.org/abs/2510.17496)
*Giacomo Camposampiero,Michael Hersche,Roger Wattenhofer,Abu Sebastian,Abbas Rahimi*

Main category: cs.LG

TL;DR: I-RAVEN-X是一个用于评估LLMs和LRMs在类比和数学推理中泛化性和鲁棒性的符号基准，扩展了I-RAVEN的复杂性。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型和大型推理模型在复杂推理任务中的表现，尤其是在不确定性和多概率结果下的能力。

Method: 通过增加操作数复杂性、属性范围和引入感知不确定性来扩展I-RAVEN基准。

Result: LRMs在长推理关系和宽属性范围上表现优于LLMs，但在不确定性和多概率结果推理中仍有挑战。

Conclusion: LRMs在复杂推理任务中有进步，但在处理不确定性和多概率结果方面仍需改进。

Abstract: We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate
generalization and robustness in analogical and mathematical reasoning for
Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X
extends I-RAVEN by increasing operand complexity, attribute range, and
introducing perceptual uncertainty. Compared to LLMs, empirical results show
that LRMs achieve improved productivity and systematicity on longer reasoning
relations and wider attribute ranges, respectively. However, LRMs are still
significantly challenged by reasoning under uncertainty and cannot effectively
explore multiple probabilistic outcomes.

</details>


### [381] [Stochastic Difference-of-Convex Optimization with Momentum](https://arxiv.org/abs/2510.17503)
*El Mahdi Chayti,Martin Jaggi*

Main category: cs.LG

TL;DR: 动量方法在小批量随机DC优化中实现收敛，弥补了传统方法在大批量或强噪声假设下的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要大批量或强噪声假设，限制了实际应用，因此研究小批量下的收敛性至关重要。

Method: 提出基于动量的算法，证明其在标准光滑性和有界方差假设下对任何批量大小均收敛。

Result: 动量方法在小批量下实现收敛，且实证表现优异。

Conclusion: 动量是确保小批量随机DC优化收敛的关键因素。

Abstract: Stochastic difference-of-convex (DC) optimization is prevalent in numerous
machine learning applications, yet its convergence properties under small batch
sizes remain poorly understood. Existing methods typically require large
batches or strong noise assumptions, which limit their practical use. In this
work, we show that momentum enables convergence under standard smoothness and
bounded variance assumptions (of the concave part) for any batch size. We prove
that without momentum, convergence may fail regardless of stepsize,
highlighting its necessity. Our momentum-based algorithm achieves provable
convergence and demonstrates strong empirical performance.

</details>


### [382] [Convergence Rates for Gradient Descent on the Edge of Stability in Overparametrised Least Squares](https://arxiv.org/abs/2510.17506)
*Lachlan Ewen MacDonald,Hancheng Min,Leandro Palma,Salma Tarmoun,Ziqing Xu,René Vidal*

Main category: cs.LG

TL;DR: 论文研究了梯度下降在过参数化最小二乘问题中的收敛性，特别是在大学习率下的三种收敛机制。


<details>
  <summary>Details</summary>
Motivation: 传统优化理论仅适用于小步长梯度下降，而神经网络训练常在大步长下进行，表现出非单调下降和偏好平坦最小值的现象。本文旨在量化这一现象。

Method: 通过将全局最小值集视为黎曼流形，将梯度下降动态分解为平行和正交分量，分别对应黎曼梯度下降和分叉动力系统。

Result: 在大学习率下，梯度下降表现出三种收敛机制：亚临界、临界和超临界，分别对应不同的收敛速度和稳定性。

Conclusion: 过参数化和大学习率导致梯度下降动态复杂化，但可通过流形分解理解其收敛行为。

Abstract: Classical optimisation theory guarantees monotonic objective decrease for
gradient descent (GD) when employed in a small step size, or ``stable", regime.
In contrast, gradient descent on neural networks is frequently performed in a
large step size regime called the ``edge of stability", in which the objective
decreases non-monotonically with an observed implicit bias towards flat minima.
In this paper, we take a step toward quantifying this phenomenon by providing
convergence rates for gradient descent with large learning rates in an
overparametrised least squares setting. The key insight behind our analysis is
that, as a consequence of overparametrisation, the set of global minimisers
forms a Riemannian manifold $M$, which enables the decomposition of the GD
dynamics into components parallel and orthogonal to $M$. The parallel component
corresponds to Riemannian gradient descent on the objective sharpness, while
the orthogonal component is a bifurcating dynamical system. This insight allows
us to derive convergence rates in three regimes characterised by the learning
rate size: (a) the subcritical regime, in which transient instability is
overcome in finite time before linear convergence to a suboptimally flat global
minimum; (b) the critical regime, in which instability persists for all time
with a power-law convergence toward the optimally flat global minimum; and (c)
the supercritical regime, in which instability persists for all time with
linear convergence to an orbit of period two centred on the optimally flat
global minimum.

</details>


### [383] [The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis](https://arxiv.org/abs/2510.17515)
*Hoang Pham,The-Anh Ta,Tom Jacobs,Rebekka Burkholz,Long Tran-Thanh*

Main category: cs.LG

TL;DR: 提出了一种基于图极限理论的新框架，用于分析稀疏神经网络的训练动态，揭示了不同剪枝方法的结构偏差及其对训练收敛性的影响。


<details>
  <summary>Details</summary>
Motivation: 稀疏神经网络的高效性备受关注，但如何有效训练它们仍是一个基本挑战。现有方法对稀疏结构的可训练性差异缺乏系统理解。

Method: 利用图极限理论（特别是图论中的图子）描述稀疏网络的连接模式，提出Graphon Limit Hypothesis，并推导出Graphon NTK来分析无限宽度下的训练动态。

Result: 通过Graphon NTK的谱分析，解释了不同剪枝方法的收敛行为差异，验证了图子表示的有效性。

Conclusion: 该框架为稀疏网络的可训练性提供了理论依据，揭示了连接模式对训练动态的关键影响。

Abstract: Sparse neural networks promise efficiency, yet training them effectively
remains a fundamental challenge. Despite advances in pruning methods that
create sparse architectures, understanding why some sparse structures are
better trainable than others with the same level of sparsity remains poorly
understood. Aiming to develop a systematic approach to this fundamental
problem, we propose a novel theoretical framework based on the theory of graph
limits, particularly graphons, that characterizes sparse neural networks in the
infinite-width regime. Our key insight is that connectivity patterns of sparse
neural networks induced by pruning methods converge to specific graphons as
networks' width tends to infinity, which encodes implicit structural biases of
different pruning methods. We postulate the Graphon Limit Hypothesis and
provide empirical evidence to support it. Leveraging this graphon
representation, we derive a Graphon Neural Tangent Kernel (Graphon NTK) to
study the training dynamics of sparse networks in the infinite width limit.
Graphon NTK provides a general framework for the theoretical analysis of sparse
networks. We empirically show that the spectral analysis of Graphon NTK
correlates with observed training dynamics of sparse networks, explaining the
varying convergence behaviours of different pruning methods. Our framework
provides theoretical insights into the impact of connectivity patterns on the
trainability of various sparse network architectures.

</details>


### [384] [SAFE-D: A Spatiotemporal Detection Framework for Abnormal Driving Among Parkinson's Disease-like Drivers](https://arxiv.org/abs/2510.17517)
*Hangcheng Cao,Baixiang Huang,Longzhi Yuan,Haonan An,Zihan Fang,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: 论文提出SAFE-D框架，用于检测帕金森病患者的驾驶行为异常，以提高驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注临时性驾驶异常（如疲劳、分心），但对慢性病（如帕金森病）引发的行为异常研究不足。

Method: 通过分析帕金森病症状与驾驶行为的关系，整合多源数据构建行为档案，并设计基于注意力的网络进行异常检测。

Result: 在模拟实验中，SAFE-D的平均准确率达到96.8%。

Conclusion: SAFE-D能有效识别帕金森病相关的驾驶行为异常，为慢性病患者的驾驶安全提供了新解决方案。

Abstract: A driver's health state serves as a determinant factor in driving behavioral
regulation. Subtle deviations from normalcy can lead to operational anomalies,
posing risks to public transportation safety. While prior efforts have
developed detection mechanisms for functionally-driven temporary anomalies such
as drowsiness and distraction, limited research has addressed
pathologically-triggered deviations, especially those stemming from chronic
medical conditions. To bridge this gap, we investigate the driving behavior of
Parkinson's disease patients and propose SAFE-D, a novel framework for
detecting Parkinson-related behavioral anomalies to enhance driving safety. Our
methodology starts by performing analysis of Parkinson's disease
symptomatology, focusing on primary motor impairments, and establishes causal
links to degraded driving performance. To represent the subclinical behavioral
variations of early-stage Parkinson's disease, our framework integrates data
from multiple vehicle control components to build a behavioral profile. We then
design an attention-based network that adaptively prioritizes spatiotemporal
features, enabling robust anomaly detection under physiological variability.
Finally, we validate SAFE-D on the Logitech G29 platform and CARLA simulator,
using data from three road maps to emulate real-world driving. Our results show
SAFE-D achieves 96.8% average accuracy in distinguishing normal and
Parkinson-affected driving patterns.

</details>


### [385] [Curiosity Meets Cooperation: A Game-Theoretic Approach to Long-Tail Multi-Label Learning](https://arxiv.org/abs/2510.17520)
*Canran Xiao,Chuangxin Zhao,Zong Ke,Fei Shen*

Main category: cs.LG

TL;DR: CD-GTMLL框架通过博弈论方法解决多标签学习中的长尾不平衡问题，提升罕见标签的梯度信号。


<details>
  <summary>Details</summary>
Motivation: 多标签学习中，罕见标签常被忽视，而头标签主导梯度信号。

Method: 将标签空间分配给多个合作玩家，引入好奇心奖励机制，激励罕见标签的学习。

Result: 在多个数据集上实现SOTA，罕见标签F1提升4.3%，P@3提升1.6%。

Conclusion: CD-GTMLL为多标签预测中的长尾问题提供了可扩展的解决方案。

Abstract: Long-tail imbalance is endemic to multi-label learning: a few head labels
dominate the gradient signal, while the many rare labels that matter in
practice are silently ignored. We tackle this problem by casting the task as a
cooperative potential game. In our Curiosity-Driven Game-Theoretic Multi-Label
Learning (CD-GTMLL) framework, the label space is split among several
cooperating players that share a global accuracy payoff yet earn additional
curiosity rewards that rise with label rarity and inter-player disagreement.
These curiosity bonuses inject gradient on under-represented tags without
hand-tuned class weights. We prove that gradient best-response updates ascend a
differentiable potential and converge to tail-aware stationary points that
tighten a lower bound on the expected Rare-F1. Extensive experiments on
conventional benchmarks and three extreme-scale datasets show consistent
state-of-the-art gains, delivering up to +4.3% Rare-F1 and +1.6% P@3 over the
strongest baselines, while ablations reveal emergent division of labour and
faster consensus on rare classes. CD-GTMLL thus offers a principled, scalable
route to long-tail robustness in multi-label prediction.

</details>


### [386] [Mitigating Clever Hans Strategies in Image Classifiers through Generating Counterexamples](https://arxiv.org/abs/2510.17524)
*Sidney Bender,Ole Delzer,Jan Herrmann,Heike Antje Marxfeld,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: CFKD通过生成多样反事实样本，无需组标签即可提升模型鲁棒性，适用于多混淆因素场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式组标签且样本不足，难以处理多虚假相关性。

Method: 提出反事实知识蒸馏（CFKD），通过生成反事实样本丰富数据并修正决策边界。

Result: 在五个数据集上验证有效性，尤其在低数据和高虚假相关性场景表现突出。

Conclusion: CFKD无需混淆标签，能有效处理多混淆因素，提升模型泛化能力。

Abstract: Deep learning models remain vulnerable to spurious correlations, leading to
so-called Clever Hans predictors that undermine robustness even in large-scale
foundation and self-supervised models. Group distributional robustness methods,
such as Deep Feature Reweighting (DFR) rely on explicit group labels to
upweight underrepresented subgroups, but face key limitations: (1) group labels
are often unavailable, (2) low within-group sample sizes hinder coverage of the
subgroup distribution, and (3) performance degrades sharply when multiple
spurious correlations fragment the data into even smaller groups. We propose
Counterfactual Knowledge Distillation (CFKD), a framework that sidesteps these
issues by generating diverse counterfactuals, enabling a human annotator to
efficiently explore and correct the model's decision boundaries through a
knowledge distillation step. Unlike DFR, our method not only reweights the
undersampled groups, but it also enriches them with new data points. Our method
does not require any confounder labels, achieves effective scaling to multiple
confounders, and yields balanced generalization across groups. We demonstrate
CFKD's efficacy across five datasets, spanning synthetic tasks to an industrial
application, with particularly strong gains in low-data regimes with pronounced
spurious correlations. Additionally, we provide an ablation study on the effect
of the chosen counterfactual explainer and teacher model, highlighting their
impact on robustness.

</details>


### [387] [How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?](https://arxiv.org/abs/2510.17526)
*Wei Huang,Andi Han,Yujin Song,Yilan Chen,Denny Wu,Difan Zou,Taiji Suzuki*

Main category: cs.LG

TL;DR: 研究探讨了在低信噪比（SNR）情况下，通过引入标签噪声的梯度下降（GD）算法，能否抑制噪声记忆并提升神经网络的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 深度模型容易在训练中记忆噪声，尤其在低SNR下泛化性能差。标签噪声可能提供隐式正则化，改善泛化。

Method: 采用两层神经网络，在理想化的信号-噪声数据设置中，使用标签噪声GD算法进行训练。

Result: 标签噪声GD能抑制噪声记忆，控制过拟合，实现良好的泛化；而标准GD在低SNR下会过拟合噪声，测试误差有非零下界。

Conclusion: 在梯度训练中引入标签噪声有助于提升低SNR下的泛化性能。

Abstract: The capacity of deep learning models is often large enough to both learn the
underlying statistical signal and overfit to noise in the training set. This
noise memorization can be harmful especially for data with a low
signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior
observations that label noise provides implicit regularization that improves
generalization, in this work, we investigate whether introducing label noise to
the gradient updates can enhance the test performance of neural network (NN) in
the low SNR regime. Specifically, we consider training a two-layer NN with a
simple label noise gradient descent (GD) algorithm, in an idealized
signal-noise data setting. We prove that adding label noise during training
suppresses noise memorization, preventing it from dominating the learning
process; consequently, label noise GD enjoys rapid signal growth while the
overfitting remains controlled, thereby achieving good generalization despite
the low SNR. In contrast, we also show that NN trained with standard GD tends
to overfit to noise in the same low SNR setting and establish a non-vanishing
lower bound on its test error, thus demonstrating the benefit of introducing
label noise in gradient-based training.

</details>


### [388] [Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment](https://arxiv.org/abs/2510.17543)
*Jiayi Huang,Sangwoo Park,Nicola Paoletti,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 论文提出了一种基于共形对齐的边缘-云级联机制（CAb），确保边缘预测集在用户指定概率下包含真实标签，同时减少向云端的卸载。


<details>
  <summary>Details</summary>
Motivation: 边缘智能需要低延迟推理，但保证可靠性仍具挑战性。研究旨在确保边缘预测集的条件覆盖性，即与云端模型预测分布一致。

Method: 通过共形对齐（CA）和多假设检验（MHT）问题，设计CAb级联机制，选择可安全在边缘处理的输入。

Result: 在CIFAR-100和TeleQnA基准测试中，CAb级联保持了目标条件覆盖性，显著减少云端卸载，预测集大小略有增加。

Conclusion: CAb方法在保证统计覆盖性的同时，优化了边缘-云级联的效率，适用于多种预测集类型。

Abstract: Edge intelligence enables low-latency inference via compact on-device models,
but assuring reliability remains challenging. We study edge-cloud cascades that
must preserve conditional coverage: whenever the edge returns a prediction set,
it should contain the true label with a user-specified probability, as if
produced by the cloud model. We formalize conditional coverage with respect to
the cloud predictive distribution, and introduce a conformal alignment-based
(CAb) cascading mechanism that certifies this property with user control over
the risk level. Our method casts escalation from edge to cloud models as a
multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA)
to select which inputs can be safely handled at the edge. The proposed CAb
model cascading method yields statistical guarantees on the average fraction of
edge decisions that satisfy cloud-level conditional coverage. The procedure
applies to arbitrary edge prediction sets, including variants of conformal
prediction (CP), and exposes a tunable trade-off among coverage, deferral rate,
and set size. Experiments on CIFAR-100 image classification and the TeleQnA
question-answering (QA) benchmark show that the proposed CAb cascade maintains
the target conditional coverage for edge predictions while substantially
reducing offloading to the cloud and incurring modest increases in
prediction-set size.

</details>


### [389] [TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model](https://arxiv.org/abs/2510.17545)
*Yichen Liu,Yan Lin,Shengnan Guo,Zeyu Zhou,Youfang Lin,Huaiyu Wan*

Main category: cs.LG

TL;DR: TrajMamba是一种高效且语义丰富的车辆轨迹学习方法，通过联合建模GPS和道路视角，结合旅行目的预训练和知识蒸馏，解决了轨迹数据中的冗余和计算负担问题。


<details>
  <summary>Details</summary>
Motivation: 车辆GPS轨迹包含宝贵的旅行语义，但现有方法面临计算负担高和轨迹冗余的问题。

Method: 提出TrajMamba，包含Traj-Mamba编码器、旅行目的预训练和知识蒸馏预训练方案。

Result: 在两个真实数据集和三个下游任务中，TrajMamba在效率和准确性上均优于现有方法。

Conclusion: TrajMamba为车辆轨迹学习提供了高效且语义丰富的解决方案。

Abstract: Vehicle GPS trajectories record how vehicles move over time, storing valuable
travel semantics, including movement patterns and travel purposes. Learning
travel semantics effectively and efficiently is crucial for real-world
applications of trajectory data, which is hindered by two major challenges.
First, travel purposes are tied to the functions of the roads and
points-of-interest (POIs) involved in a trip. Such information is encoded in
textual addresses and descriptions and introduces heavy computational burden to
modeling. Second, real-world trajectories often contain redundant points, which
harm both computational efficiency and trajectory embedding quality. To address
these challenges, we propose TrajMamba, a novel approach for efficient and
semantically rich vehicle trajectory learning. TrajMamba introduces a
Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS
and road perspectives of trajectories, enabling robust representations of
continuous travel behaviors. It also incorporates a Travel Purpose-aware
Pre-training procedure to integrate travel purposes into the learned embeddings
without introducing extra overhead to embedding calculation. To reduce
redundancy in trajectories, TrajMamba features a Knowledge Distillation
Pre-training scheme to identify key trajectory points through a learnable mask
generator and obtain effective compressed trajectory embeddings. Extensive
experiments on two real-world datasets and three downstream tasks show that
TrajMamba outperforms state-of-the-art baselines in both efficiency and
accuracy.

</details>


### [390] [The Free Transformer](https://arxiv.org/abs/2510.17558)
*François Fleuret*

Main category: cs.LG

TL;DR: 提出了一种基于随机潜变量的解码器Transformer扩展方法，通过无监督变分学习提升生成任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过引入随机潜变量来增强Transformer的生成能力，以提升下游任务表现。

Method: 扩展解码器Transformer，引入随机潜变量，并通过变分方法进行无监督学习。

Result: 实验表明，该方法在下游任务中显著提升了性能。

Conclusion: 通过潜变量条件化，Transformer的生成能力得到有效增强，适用于多种任务。

Abstract: We propose an extension of the decoder Transformer that conditions its
generative process on random latent variables which are learned without
supervision thanks to a variational procedure. Experimental evaluations show
that allowing such a conditioning translates into substantial improvements on
downstream tasks.

</details>


### [391] [Formally Exploring Time-Series Anomaly Detection Evaluation Metrics](https://arxiv.org/abs/2510.17562)
*Dennis Wagner,Arjun Nair,Billy Joe Franks,Justus Arweiler,Aparna Muraleedharan,Indra Jungjohann,Fabian Hartung,Mayank C. Ahuja,Andriy Balinskyy,Saurabh Varshneya,Nabeel Hussain Syed,Mayank Nagda,Phillip Liznerski,Steffen Reithermann,Maja Rudolph,Sebastian Vollmer,Ralf Schulz,Torsten Katz,Stephan Mandt,Michael Bortz,Heike Leitte,Daniel Neider,Jakob Burger,Fabian Jirasek,Hans Hasse,Sophie Fellenz,Marius Kloft*

Main category: cs.LG

TL;DR: 论文提出了一种新的时间序列异常检测评估框架，解决了现有指标不一致的问题，并提出了满足所有理论属性的新指标LARM和ALARM。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列异常检测方法的评估指标存在局限性，导致结果不一致且不可靠，亟需一种理论支持的评估框架。

Method: 通过形式化时间序列异常检测的基本需求，提出了一套可验证的理论属性，并基于此分析了37种常用指标。随后提出了满足所有属性的新指标LARM及其扩展ALARM。

Result: 分析表明，现有指标大多仅满足少数属性，且无一满足全部属性。LARM和ALARM则能全面满足理论需求。

Conclusion: 论文为时间序列异常检测提供了理论支持的评估框架和新指标，解决了现有方法的局限性，提升了评估的可靠性。

Abstract: Undetected anomalies in time series can trigger catastrophic failures in
safety-critical systems, such as chemical plant explosions or power grid
outages. Although many detection methods have been proposed, their performance
remains unclear because current metrics capture only narrow aspects of the task
and often yield misleading results. We address this issue by introducing
verifiable properties that formalize essential requirements for evaluating
time-series anomaly detection. These properties enable a theoretical framework
that supports principled evaluations and reliable comparisons. Analyzing 37
widely used metrics, we show that most satisfy only a few properties, and none
satisfy all, explaining persistent inconsistencies in prior results. To close
this gap, we propose LARM, a flexible metric that provably satisfies all
properties, and extend it to ALARM, an advanced variant meeting stricter
requirements.

</details>


### [392] [Semi-supervised Latent Bayesian Optimization for Designing Antimicrobial Peptides](https://arxiv.org/abs/2510.17569)
*Jyler Menard,R. A. Mansbach*

Main category: cs.LG

TL;DR: 研究探讨了通过降维和物理化学属性组织潜在空间以优化抗菌肽设计的方法。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽设计因序列空间庞大而困难，现有生成模型缺乏可解释性和潜在空间质量的量化。

Method: 研究了降维、潜在空间可解释性及物理化学属性组织对优化效率的影响。

Result: 降维结合物理化学属性组织可提升潜在空间的可解释性和优化效率。

Conclusion: 降维和属性组织是优化抗菌肽设计的有效策略。

Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat
bacterial infections. Discovering and designing such peptides is difficult
because of the vast number of possible sequences of amino acids. Deep
generative models, such as variational autoencoders, have shown value in
peptide design due to their ability to model sequence space with a
continuous-valued latent space. Although such models have already been used to
great effect in biomolecular design, they still suffer from a lack of
interpretability and rigorous quantification of latent space quality as a
search space. We investigate (1) whether further compression of the design
space via dimensionality reduction may facilitate optimization, (2) the
interpretability of the spaces, and (3) how organizing latent spaces with
physicochemical properties may improve the efficiency of optimizing
antimicrobial activity. We find that further reduction of the latent space via
dimensionality reduction can be advantageous when organizing the space with
more relevant information at data availability, that using the dimensionality
reduction search space can be more interpretable, and that we can organize the
latent space with different physicochemical properties even at different
percentages of available labels.

</details>


### [393] [CEPerFed: Communication-Efficient Personalized Federated Learning for Multi-Pulse MRI Classification](https://arxiv.org/abs/2510.17584)
*Ludi Li,Junbin Mao,Hanhe Lin,Xu Tian,Fang-Xiang Wu,Jin Liu*

Main category: cs.LG

TL;DR: CEPerFed是一种高效的个性化联邦学习方法，通过历史梯度协调本地与全局优化，并采用分层SVD策略减少通信开销，解决了多脉冲MRI分类中的数据异构性和高通信成本问题。


<details>
  <summary>Details</summary>
Motivation: 多脉冲MRI分类需要大量多样化数据，但隐私保护限制了数据共享。联邦学习虽可行，但面临数据异构性和高通信开销的挑战。

Method: CEPerFed结合客户端历史风险梯度和历史平均梯度协调优化，采用分层SVD策略减少通信开销。

Result: 在五个分类任务中验证了CEPerFed的有效性。

Conclusion: CEPerFed通过优化协调和通信策略，显著提升了多脉冲MRI分类的性能和效率。

Abstract: Multi-pulse magnetic resonance imaging (MRI) is widely utilized for clinical
practice such as Alzheimer's disease diagnosis. To train a robust model for
multi-pulse MRI classification, it requires large and diverse data from various
medical institutions while protecting privacy by preventing raw data sharing
across institutions. Although federated learning (FL) is a feasible solution to
address this issue, it poses challenges of model convergence due to the effect
of data heterogeneity and substantial communication overhead due to large
numbers of parameters transmitted within the model. To address these
challenges, we propose CEPerFed, a communication-efficient personalized FL
method. It mitigates the effect of data heterogeneity by incorporating
client-side historical risk gradients and historical mean gradients to
coordinate local and global optimization. The former is used to weight the
contributions from other clients, enhancing the reliability of local updates,
while the latter enforces consistency between local updates and the global
optimization direction to ensure stable convergence across heterogeneous data
distributions. To address the high communication overhead, we propose a
hierarchical SVD (HSVD) strategy that transmits only the most critical
information required for model updates. Experiments on five classification
tasks demonstrate the effectiveness of the CEPerFed method. The code will be
released upon acceptance at https://github.com/LD0416/CEPerFed.

</details>


### [394] [Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction](https://arxiv.org/abs/2510.17661)
*Vaishnavi Visweswaraiah,Tanvi Banerjee,William Romine*

Main category: cs.LG

TL;DR: 论文利用机器学习和GAN生成合成数据解决自杀预测中的样本不平衡问题，测试结果显示不同模型在精度、召回率和F1分数上表现各异。


<details>
  <summary>Details</summary>
Motivation: 自杀预测数据中阳性样本稀少，导致极端类别不平衡，需要数据增强以支持建模。

Method: 结合机器学习和深度学习（如GAN）生成合成数据，使用多种模型（如LR、RF、SVM）进行预测。

Result: LR、RF和SVM在测试数据上表现良好，GAN在数据增强中发挥了关键作用。

Conclusion: 模型在自杀预测中有效，GAN生成的数据支持了建模工作，但不同模型在敏感性和特异性上各有优劣。

Abstract: Suicide prediction is the key for prevention, but real data with sufficient
positive samples is rare and causes extreme class imbalance. We utilized
machine learning (ML) to build the model and deep learning (DL) techniques,
like Generative Adversarial Networks (GAN), to generate synthetic data samples
to enhance the dataset. The initial dataset contained 656 samples, with only
four positive cases, prompting the need for data augmentation. A variety of
machine learning models, ranging from interpretable data models to black box
algorithmic models, were used. On real test data, Logistic Regression (LR)
achieved a weighted precision of 0.99, a weighted recall of 0.85, and a
weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99,
respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86.
LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and
misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 &
0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0)
with 0 false positives (specificity: 1.0). These results highlight the models'
effectiveness, with GAN playing a key role in generating synthetic data to
support suicide prevention modeling efforts.

</details>


### [395] [On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration](https://arxiv.org/abs/2510.17670)
*Yehonathan Refael,Amit Aides,Aviad Barzilai,George Leifman,Genady Beryozkin,Vered Silverman,Bolous Jaber,Tomer Shekel*

Main category: cs.LG

TL;DR: 提出了一种结合预训练OVD模型和轻量级少样本分类器的级联方法，用于提升遥感领域开放词汇目标检测的精度。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测模型在遥感等专业领域因自然语言模糊性导致性能下降，影响关键应用。

Method: 采用级联方法，先用零-shot模型生成高召回率提案，再用少量标注样本训练的轻量分类器提升精度，结合FLAME主动学习策略选择样本。

Result: 方法在遥感基准测试中超越现有技术，实现快速适应（一分钟内）且资源高效。

Conclusion: 该框架为适应特定用户需求提供了一种实用且资源高效的解决方案。

Abstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by
detecting objects from arbitrary text queries. However, their zero-shot
performance in specialized domains like Remote Sensing (RS) is often
compromised by the inherent ambiguity of natural language, limiting critical
downstream applications. For instance, an OVD model may struggle to distinguish
between fine-grained classes such as "fishing boat" and "yacht" since their
embeddings are similar and often inseparable. This can hamper specific user
goals, such as monitoring illegal fishing, by producing irrelevant detections.
To address this, we propose a cascaded approach that couples the broad
generalization of a large pre-trained OVD model with a lightweight few-shot
classifier. Our method first employs the zero-shot model to generate
high-recall object proposals. These proposals are then refined for high
precision by a compact classifier trained in real-time on only a handful of
user-annotated examples - drastically reducing the high costs of RS imagery
annotation.The core of our framework is FLAME, a one-step active learning
strategy that selects the most informative samples for training. FLAME
identifies, on the fly, uncertain marginal candidates near the decision
boundary using density estimation, followed by clustering to ensure sample
diversity. This efficient sampling technique achieves high accuracy without
costly full-model fine-tuning and enables instant adaptation, within less then
a minute, which is significantly faster than state-of-the-art alternatives.Our
method consistently surpasses state-of-the-art performance on RS benchmarks,
establishing a practical and resource-efficient framework for adapting
foundation models to specific user needs.

</details>


### [396] [LILO: Bayesian Optimization with Interactive Natural Language Feedback](https://arxiv.org/abs/2510.17671)
*Katarzyna Kobalczyk,Zhiyuan Jerry Lin,Benjamin Letham,Zhuokai Zhao,Maximilian Balandat,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出了一种结合大语言模型（LLM）和贝叶斯优化（BO）的语言循环框架，将自然语言反馈转化为标量效用，优化数值搜索空间。


<details>
  <summary>Details</summary>
Motivation: 解决传统贝叶斯优化（BO）反馈格式受限、需定制模型的问题，利用LLM处理多样化文本反馈，提升灵活性和效率。

Method: 使用LLM将非结构化自然语言反馈转化为标量效用信号，结合BO进行高效优化，无需手动设计核函数。

Result: 该方法在反馈有限的情况下优于传统BO和纯LLM优化器，提供更自然的决策界面。

Conclusion: 语言循环框架结合了LLM的灵活性和BO的高效性，适用于复杂目标的量化优化。

Abstract: For many real-world applications, feedback is essential in translating
complex, nuanced, or subjective goals into quantifiable optimization
objectives. We propose a language-in-the-loop framework that uses a large
language model (LLM) to convert unstructured feedback in the form of natural
language into scalar utilities to conduct BO over a numeric search space.
Unlike preferential BO, which only accepts restricted feedback formats and
requires customized models for each domain-specific problem, our approach
leverages LLMs to turn varied types of textual feedback into consistent utility
signals and to easily include flexible user priors without manual kernel
design. At the same time, our method maintains the sample efficiency and
principled uncertainty quantification of BO. We show that this hybrid method
not only provides a more natural interface to the decision maker but also
outperforms conventional BO baselines and LLM-only optimizers, particularly in
feedback-limited regimes.

</details>


### [397] [Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning](https://arxiv.org/abs/2510.17690)
*Xihong Su*

Main category: cs.LG

TL;DR: 论文提出了三种主要贡献：1）CADP算法连接策略梯度和动态规划；2）为ERM-TRC和EVaR-TRC建立最优策略条件；3）提出无模型Q学习算法用于风险规避目标。


<details>
  <summary>Details</summary>
Motivation: 研究如何在不确定模型中优化马尔可夫策略，并解决风险规避目标下的策略计算问题。

Method: 1）提出CADP算法；2）分析ERM Bellman算子的收缩性；3）设计Q学习算法。

Result: 证明了CADP的单调改进性，ERM-TRC和EVaR-TRC的最优策略存在性，以及Q学习算法的收敛性。

Conclusion: 论文通过理论分析和算法设计，解决了风险规避目标下的策略优化问题。

Abstract: This dissertation makes three main contributions. First, We identify a new
connection between policy gradient and dynamic programming in MMDPs and propose
the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov
policy that maximizes the discounted return averaged over the uncertain models.
CADP adjusts model weights iteratively to guarantee monotone policy
improvements to a local maximum. Second, We establish sufficient and necessary
conditions for the exponential ERM Bellman operator to be a contraction and
prove the existence of stationary deterministic optimal policies for ERM-TRC
and EVaR-TRC. We also propose exponential value iteration, policy iteration,
and linear programming algorithms for computing optimal stationary policies for
ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for
computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The
challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we
use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous
proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the
optimal risk-averse value functions. The proposed Q-learning algorithms compute
the optimal stationary policy for ERM-TRC and EVaR-TRC.

</details>


### [398] [Closing the Sim2Real Performance Gap in RL](https://arxiv.org/abs/2510.17709)
*Akhil S Anand,Shambhuraj Sawant,Jasper Hoffmann,Dirk Reinhardt,Sebastien Gros*

Main category: cs.LG

TL;DR: 提出了一种新的双层强化学习框架，通过直接基于真实世界性能调整模拟器参数，以缩小Sim2Real性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有Sim2Real方法通过优化模拟器精度和变异性作为代理指标，但这些指标与真实世界性能不一定相关。

Method: 采用双层强化学习框架：内层训练模拟策略，外层调整模拟模型和奖励参数以最大化真实性能。

Result: 在简单示例中验证了数学工具的有效性。

Conclusion: 该方法有望缩小Sim2Real性能差距。

Abstract: Sim2Real aims at training policies in high-fidelity simulation environments
and effectively transferring them to the real world. Despite the developments
of accurate simulators and Sim2Real RL approaches, the policies trained purely
in simulation often suffer significant performance drops when deployed in real
environments. This drop is referred to as the Sim2Real performance gap. Current
Sim2Real RL methods optimize the simulator accuracy and variability as proxies
for real-world performance. However, these metrics do not necessarily correlate
with the real-world performance of the policy as established theoretically and
empirically in the literature. We propose a novel framework to address this
issue by directly adapting the simulator parameters based on real-world
performance. We frame this problem as a bi-level RL framework: the inner-level
RL trains a policy purely in simulation, and the outer-level RL adapts the
simulation model and in-sim reward parameters to maximize real-world
performance of the in-sim policy. We derive and validate in simple examples the
mathematical tools needed to develop bi-level RL algorithms that close the
Sim2Real performance gap.

</details>


### [399] [Enabling Fine-Grained Operating Points for Black-Box LLMs](https://arxiv.org/abs/2510.17727)
*Ege Beyazit,KL Navaneet,Prashant Mathur,Roi Blanco,Vidit Bansal,Karim Bouyarmane*

Main category: cs.LG

TL;DR: 黑盒大语言模型（LLMs）在决策问题中提供实用且易用的替代方案，但其低数值输出基数限制了操作点的精细调整。本文提出高效方法，显著增加操作点数量和多样性，提升性能。


<details>
  <summary>Details</summary>
Motivation: 黑盒LLMs在需要特定指标约束（如精度≥95%）的应用中表现不佳，因其低数值输出基数限制了操作点的精细控制。

Method: 研究LLMs低基数输出的原因，尝试标准提示工程、不确定性估计和置信度引出技术，最终提出高效方法增加操作点。

Result: 在11个数据集和3个LLMs上，提出的方法提供更细粒度的操作点，性能优于基准方法。

Conclusion: 本文方法显著提升黑盒LLMs的操作粒度和性能，适用于需要精细决策调整的应用。

Abstract: Black-box Large Language Models (LLMs) provide practical and accessible
alternatives to other machine learning methods, as they require minimal labeled
data and machine learning expertise to develop solutions for various decision
making problems. However, for applications that need operating with constraints
on specific metrics (e.g., precision $\geq$ 95%), decision making with
black-box LLMs remains unfavorable, due to their low numerical output
cardinalities. This results in limited control over their operating points,
preventing fine-grained adjustment of their decision making behavior. In this
paper, we study using black-box LLMs as classifiers, focusing on efficiently
improving their operational granularity without performance loss. Specifically,
we first investigate the reasons behind their low-cardinality numerical outputs
and show that they are biased towards generating rounded but informative
verbalized probabilities. Then, we experiment with standard prompt engineering,
uncertainty estimation and confidence elicitation techniques, and observe that
they do not effectively improve operational granularity without sacrificing
performance or increasing inference cost. Finally, we propose efficient
approaches to significantly increase the number and diversity of available
operating points. Our proposed approaches provide finer-grained operating
points and achieve comparable to or better performance than the benchmark
methods across 11 datasets and 3 LLMs.

</details>


### [400] [Prediction of Sea Ice Velocity and Concentration in the Arctic Ocean using Physics-informed Neural Network](https://arxiv.org/abs/2510.17756)
*Younghyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: 论文提出了一种物理信息神经网络（PINN）策略，用于改进北极海冰速度和浓度的预测，解决了纯数据驱动模型的泛化性和物理一致性问题。


<details>
  <summary>Details</summary>
Motivation: 随着北极海冰进入新阶段（冰层变薄、融化加速），纯数据驱动的机器学习模型因过度依赖历史数据的数量和质量，难以准确预测未来动态变化的海冰条件。

Method: 基于Hierarchical Information-sharing U-net（HIS-Unet）架构，结合物理损失函数和激活函数，开发了PINN模型，以生成物理上合理的海冰速度和浓度输出。

Result: PINN模型在少量样本训练下，仍优于纯数据驱动模型，特别是在融化和早期冻结季节以及快速移动冰区的海冰浓度预测中表现更优。

Conclusion: PINN方法通过整合物理知识，显著提升了海冰预测的准确性和物理一致性，尤其在数据稀缺或动态变化剧烈的区域表现突出。

Abstract: As an increasing amount of remote sensing data becomes available in the
Arctic Ocean, data-driven machine learning (ML) techniques are becoming widely
used to predict sea ice velocity (SIV) and sea ice concentration (SIC).
However, fully data-driven ML models have limitations in generalizability and
physical consistency due to their excessive reliance on the quantity and
quality of training data. In particular, as Arctic sea ice entered a new phase
with thinner ice and accelerated melting, there is a possibility that an ML
model trained with historical sea ice data cannot fully represent the
dynamically changing sea ice conditions in the future. In this study, we
develop physics-informed neural network (PINN) strategies to integrate physical
knowledge of sea ice into the ML model. Based on the Hierarchical
Information-sharing U-net (HIS-Unet) architecture, we incorporate the physics
loss function and the activation function to produce physically plausible SIV
and SIC outputs. Our PINN model outperforms the fully data-driven model in the
daily predictions of SIV and SIC, even when trained with a small number of
samples. The PINN approach particularly improves SIC predictions in melting and
early freezing seasons and near fast-moving ice regions.

</details>


### [401] [Atlas-based Manifold Representations for Interpretable Riemannian Machine Learning](https://arxiv.org/abs/2510.17772)
*Ryan A. Robinett,Sophia A. Madejski,Kyle Ruark,Samantha J. Riesenfeld,Lorenzo Orecchia*

Main category: cs.LG

TL;DR: 论文提出了一种基于可微图册的流形学习方法，支持直接在潜在数据流形上进行机器学习，展示了其高效性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前流形学习方法主要关注降维，忽略了潜在流形的关键特征，而直接学习可微图册的方法尚未充分探索。

Method: 实现了一种通用的数据结构来维护可微图册，支持流形上的黎曼优化，并结合无监督启发式方法从点云数据中学习图册。

Result: 实验表明该方法在效率和准确性上有优势，并在监督分类任务和RNA速度分析中展示了更好的可解释性和鲁棒性。

Conclusion: 基于可微图册的方法在流形学习中具有潜力，尤其在直接利用流形特征的任务中表现优异。

Abstract: Despite the popularity of the manifold hypothesis, current manifold-learning
methods do not support machine learning directly on the latent $d$-dimensional
data manifold, as they primarily aim to perform dimensionality reduction into
$\mathbb{R}^D$, losing key manifold features when the embedding dimension $D$
approaches $d$.
  On the other hand, methods that directly learn the latent manifold as a
differentiable atlas have been relatively underexplored.
  In this paper, we aim to give a proof of concept of the effectiveness and
potential of atlas-based methods. To this end, we implement a generic data
structure to maintain a differentiable atlas that enables Riemannian
optimization over the manifold. We complement this with an unsupervised
heuristic that learns a differentiable atlas from point cloud data. We
experimentally demonstrate that this approach has advantages in terms of
efficiency and accuracy in selected settings. Moreover, in a supervised
classification task over the Klein bottle and in RNA velocity analysis of
hematopoietic data, we showcase the improved interpretability and robustness of
our approach.

</details>


### [402] [Mapping Post-Training Forgetting in Language Models at Scale](https://arxiv.org/abs/2510.17776)
*Jackson Harmon,Andreas Hochlehnert,Matthias Bethge,Ameya Prabhu*

Main category: cs.LG

TL;DR: 提出了一种样本级度量方法，量化后训练对预训练知识的遗忘和反向迁移，发现不同后训练阶段对知识的影响各异。


<details>
  <summary>Details</summary>
Motivation: 理解后训练如何影响预训练语言模型的知识，避免知识遗忘对模型能力的不利影响。

Method: 通过样本级1->0和0->1转换量化遗忘和反向迁移，并针对多选题基准调整随机猜测的影响。

Result: 不同后训练阶段（如领域持续预训练、RL/SFT后训练）对知识的遗忘和反向迁移效果各异，模型合并无法可靠缓解遗忘。

Conclusion: 该框架为大规模后训练对知识的影响提供了实用度量标准，有助于推动通用AI系统的发展。

Abstract: Scaled post-training now drives many of the largest capability gains in
language models (LMs), yet its effect on pretrained knowledge remains poorly
understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.
president or an API call) does not "average out" by recalling another. Hence,
we propose a sample-wise paradigm to measure what is forgotten and when
backward transfer occurs. Our metric counts 1->0 transitions (correct before
post-training, incorrect after) to quantify forgetting and 0->1 transitions to
quantify backward transfer. Traditional task averages conflate these effects
and obscure large changes. For multiple-choice benchmarks, we add
chance-adjusted variants that subtract the expected contribution of random
guessing from pre- and post-training accuracies. We apply this framework across
post-training stages, model sizes, and data scales. Our large-scale analysis
shows that: (1) Domain-continual pretraining induces moderate forgetting with
low-to-moderate backward transfer; (2) RL/SFT post-training applied to base
models and Instruction tuning yields moderate-to-large backward transfer on
math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to
instruction-tuned models is sensitive on data scale: at small scales, both
forgetting and backward transfer are small; at larger scales, effects are mixed
and warrant further study with better controls; (4) Model merging does not
reliably mitigate forgetting. Overall, our framework offers a practical
yardstick for mapping how post-training alters pretrained knowledge at scale --
enabling progress towards generally capable AI systems.

</details>


### [403] [Inference-Time Compute Scaling For Flow Matching](https://arxiv.org/abs/2510.17786)
*Adam Stecklov,Noah El Rimawi-Fine,Mathieu Blanchette*

Main category: cs.LG

TL;DR: 论文提出了一种新的推理时间扩展方法，用于Flow Matching（FM），保留了线性插值，并在图像生成和无条件蛋白质生成中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Flow Matching在语言、视觉和科学领域受到关注，但其推理时间扩展方法尚未充分探索。现有方法牺牲了FM的高效采样特性。

Method: 引入新的推理时间扩展程序，保留线性插值，应用于图像和蛋白质生成任务。

Result: 样本质量随计算量增加而提升，且FM推理时间扩展可应用于科学领域。

Conclusion: 新方法在保持FM高效采样的同时，扩展了其应用范围，尤其在科学领域表现出潜力。

Abstract: Allocating extra computation at inference time has recently improved sample
quality in large language models and diffusion-based image generation. In
parallel, Flow Matching (FM) has gained traction in language, vision, and
scientific domains, but inference-time scaling methods for it remain
under-explored. Concurrently, Kim et al., 2025 approach this problem but
replace the linear interpolant with a non-linear variance-preserving (VP)
interpolant at inference, sacrificing FM's efficient and straight sampling.
Additionally, inference-time compute scaling for flow matching has only been
applied to visual tasks, like image generation. We introduce novel
inference-time scaling procedures for FM that preserve the linear interpolant
during sampling. Evaluations of our method on image generation, and for the
first time (to the best of our knowledge), unconditional protein generation,
show that I) sample quality consistently improves as inference compute
increases, and II) flow matching inference-time scaling can be applied to
scientific domains.

</details>


### [404] [Functional Distribution Networks (FDN)](https://arxiv.org/abs/2510.17794)
*Omer Haq*

Main category: cs.LG

TL;DR: FDN是一种输入条件化的权重分布网络，通过beta-ELBO和蒙特卡洛采样训练，旨在解决概率回归器在分布偏移下的过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 现代概率回归器在分布偏移下往往过于自信，需要一种能够自适应输入分布的预测模型。

Method: 提出Functional Distribution Networks (FDN)，通过输入条件化的权重分布生成预测混合，并使用beta-ELBO和蒙特卡洛采样进行训练。

Result: 在标准回归任务中，FDN在参数和更新预算匹配的情况下，优于贝叶斯、集成、dropout和超网络基线，表现出更好的准确性、校准性和偏移感知能力。

Conclusion: FDN框架和评估协议旨在使OOD感知且校准良好的神经回归变得实用和模块化。

Abstract: Modern probabilistic regressors often remain overconfident under distribution
shift. We present Functional Distribution Networks (FDN), an input-conditioned
distribution over network weights that induces predictive mixtures whose
dispersion adapts to the input. FDN is trained with a beta-ELBO and Monte Carlo
sampling. We further propose an evaluation protocol that cleanly separates
interpolation from extrapolation and stresses OOD sanity checks (e.g., that
predictive likelihood degrades under shift while in-distribution accuracy and
calibration are maintained). On standard regression tasks, we benchmark against
strong Bayesian, ensemble, dropout, and hypernetwork baselines under matched
parameter and update budgets, and assess accuracy, calibration, and
shift-awareness with standard diagnostics. Together, the framework and protocol
aim to make OOD-aware, well-calibrated neural regression practical and modular.

</details>


### [405] [Unbiased Gradient Low-Rank Projection](https://arxiv.org/abs/2510.17802)
*Rui Pan,Yang Luo,Yuxing Liu,Yang You,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为GUM的无偏低秩优化方法，通过层间采样技术解决了现有低秩投影方法的偏差问题，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有低秩投影方法（如GaLore）缺乏收敛保证，且因偏差导致性能与全参数训练存在差距。本文旨在解决这一问题。

Method: 结合GaLore机制和Muon算法，提出GUM方法，利用层间采样技术实现无偏低秩优化。

Result: 理论证明GUM与Muon算法收敛性一致，实验显示其在LLM微调和预训练中优于GaLore，甚至超越全参数训练。

Conclusion: GUM通过更均匀的层内知识分布，提升了参数空间利用效率和记忆能力，是一种高效且无偏的低秩优化方法。

Abstract: Memory-efficient optimization is critical for training increasingly large
language models (LLMs). A popular strategy involves gradient low-rank
projection, storing only the projected optimizer states, with GaLore being a
representative example. However, a significant drawback of many such methods is
their lack of convergence guarantees, as various low-rank projection approaches
introduce inherent biases relative to the original optimization algorithms,
which contribute to performance gaps compared to full-parameter training.
Aiming to tackle this problem, this paper investigates the layerwise sampling
technique for debiasing low-rank projection mechanisms. In particular, an
instantiation of the paradigm gives rise to a novel and unbiased low-rank
optimization method built upon GaLore's mechanism and the Muon algorithm, named
GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the
convergence guarantees of the base Muon algorithm while preserving the memory
efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and
pretraining also demonstrate non-trivial improvements over GaLore and even
better performance than full-parameter training. Further investigation shows
that the improvement of this technique comes from a more uniform distribution
of knowledge inside layers, leading to more efficient utilization of the model
parameter space and better memorization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [406] [VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments](https://arxiv.org/abs/2510.16205)
*João Carlos Virgolino Soares,Gabriel Fischer Abati,Claudio Semini*

Main category: cs.RO

TL;DR: VAR-SLAM是一种基于ORB-SLAM3的系统，结合轻量级语义关键点过滤器和自适应鲁棒损失，提升动态环境下的SLAM性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中现有SLAM方法因依赖固定鲁棒核或仅处理已知移动物体而导致的精度下降问题。

Method: 结合语义关键点过滤器处理已知移动物体，使用Barron自适应鲁棒损失处理未知移动物体，并通过在线估计鲁棒核形状参数调整高斯和重尾行为。

Result: 在TUM RGB-D、Bonn RGB-D Dynamic和OpenLORIS数据集上表现优于现有方法，轨迹精度提升25%，平均运行速度为27 FPS。

Conclusion: VAR-SLAM在动态环境中表现出更高的鲁棒性和准确性，适用于处理已知和未知移动物体。

Abstract: Visual SLAM in dynamic environments remains challenging, as several existing
methods rely on semantic filtering that only handles known object classes, or
use fixed robust kernels that cannot adapt to unknown moving objects, leading
to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual
Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a
lightweight semantic keypoint filter to deal with known moving objects, with
Barron's adaptive robust loss to handle unknown ones. The shape parameter of
the robust kernel is estimated online from residuals, allowing the system to
automatically adjust between Gaussian and heavy-tailed behavior. We evaluate
VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which
include both known and unknown moving objects. Results show improved trajectory
accuracy and robustness over state-of-the-art baselines, achieving up to 25%
lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining
performance at 27 FPS on average.

</details>


### [407] [DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly](https://arxiv.org/abs/2510.16231)
*Bihao Zhang,Davood Soleymanzadeh,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: DeGrip是一种专为拆卸废旧电脑桌面设计的定制夹具，具有三自由度，能在狭小空间操作，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 废旧产品的智能机器人拆卸一直是机器人领域的挑战，现有技术因缺乏专用硬件而难以实际应用。

Method: 开发了DeGrip夹具，采用电缆驱动机制减小体积，并设计手腕以解耦关节驱动。在Isaac Sim中搭建仿真环境进行测试。

Result: 评估结果表明DeGrip能够在狭小空间和任意配置下有效拆卸废旧电脑桌面组件。

Conclusion: DeGrip证明了其在废旧电脑桌面拆卸任务中的实用性和有效性。

Abstract: Intelligent robotic disassembly of end-of-life (EOL) products has been a
long-standing challenge in robotics. While machine learning techniques have
shown promise, the lack of specialized hardware limits their application in
real-world scenarios. We introduce DeGrip, a customized gripper designed for
the disassembly of EOL computer desktops. DeGrip provides three degrees of
freedom (DOF), enabling arbitrary configurations within the disassembly
environment when mounted on a robotic manipulator. It employs a cable-driven
transmission mechanism that reduces its overall size and enables operation in
confined spaces. The wrist is designed to decouple the actuation of wrist and
jaw joints. We also developed an EOL desktop disassembly environment in Isaac
Sim to evaluate the effectiveness of DeGrip. The tasks were designed to
demonstrate its ability to operate in confined spaces and disassemble
components in arbitrary configurations. The evaluation results confirm the
capability of DeGrip for EOL desktop disassembly.

</details>


### [408] [Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning](https://arxiv.org/abs/2510.16240)
*Lukas Zbinden,Nigel Nelson,Juo-Tung Chen,Xinhao Chen,Ji Woong,Kim,Mahdi Azizian,Axel Krieger,Sean Huver*

Main category: cs.RO

TL;DR: Cosmos-Surg-dVRK是一种基于世界基础模型（WFM）的手术模拟平台，用于自动化评估和基准测试手术策略，解决了物理机器人平台的高成本和执行变异性问题。


<details>
  <summary>Details</summary>
Motivation: 物理手术机器人平台（如dVRK）的高成本、时间消耗和变异性限制了手术策略的直接评估，需要一种高保真模拟方法。

Method: 开发了Cosmos-Surg-dVRK，结合视频分类器，实现手术策略的自动化在线评估和基准测试。

Result: 在缝合任务中，模拟与真实平台结果强相关；在胆囊切除术任务中，模拟与真实评估初步一致。

Conclusion: Cosmos-Surg-dVRK展示了在复杂手术任务中替代物理平台的潜力。

Abstract: The rise of surgical robots and vision-language-action models has accelerated
the development of autonomous surgical policies and efficient assessment
strategies. However, evaluating these policies directly on physical robotic
platforms such as the da Vinci Research Kit (dVRK) remains hindered by high
costs, time demands, reproducibility challenges, and variability in execution.
World foundation models (WFM) for physical AI offer a transformative approach
to simulate complex real-world surgical tasks, such as soft tissue deformation,
with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune
of the Cosmos WFM, which, together with a trained video classifier, enables
fully automated online evaluation and benchmarking of surgical policies. We
evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop
suture pad tasks, the automated pipeline achieves strong correlation between
online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si
platform, as well as good agreement between human labelers and the V-JEPA
2-derived video classifier. Additionally, preliminary experiments with ex-vivo
porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising
alignment with real-world evaluations, highlighting the platform's potential
for more complex surgical procedures.

</details>


### [409] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA是一个统一的生态系统，用于单臂操作任务，提供细粒度评估和系统性压力测试，以解决现有VLA代理评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA代理的评估仅依赖粗粒度的任务成功指标，无法精确诊断技能或衡量对现实扰动的鲁棒性，且数据碎片化阻碍了可重复研究和通用模型的发展。

Method: NEBULA引入双轴评估协议，结合细粒度能力测试和系统性压力测试，并提供标准化API和大规模聚合数据集。

Result: 研究发现，表现最佳的VLA代理在空间推理和动态适应等关键能力上表现不佳，这些缺陷被传统任务成功指标掩盖。

Conclusion: NEBULA为构建鲁棒、通用的具身代理提供了实用基础，通过同时衡量代理的能力和可靠性。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [410] [Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification](https://arxiv.org/abs/2510.16281)
*Yilin Wu,Anqi Li,Tucker Hermans,Fabio Ramos,Andrea Bajcsy,Claudia P'erez-D'Arpino*

Main category: cs.RO

TL;DR: 提出了一种无需训练的运行时策略调整方法，通过模拟和VLM选择最佳动作序列，提升VLA模型在OOD场景下的鲁棒性和行为组合能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型生成的文本计划虽正确，但动作执行可能偏离预期，尤其在OOD场景中。

Method: 通过采样候选动作序列、模拟预测结果，并用VLM选择与文本计划最匹配的序列执行。

Result: 在OOD场景下性能提升15%，且支持行为组合无需重新训练。

Conclusion: 该方法将动作多样性转化为优势，显著提升模型鲁棒性和适应性。

Abstract: Reasoning Vision Language Action (VLA) models improve robotic
instruction-following by generating step-by-step textual plans before low-level
actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language
models. Yet even with a correct textual plan, the generated actions can still
miss the intended outcomes in the plan, especially in out-of-distribution (OOD)
scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness,
and introduce a training-free, runtime policy steering method for
reasoning-action alignment. Given a reasoning VLA's intermediate textual plan,
our framework samples multiple candidate action sequences from the same model,
predicts their outcomes via simulation, and uses a pre-trained Vision-Language
Model (VLM) to select the sequence whose outcome best aligns with the VLA's own
textual plan. Only executing action sequences that align with the textual
reasoning turns our base VLA's natural action diversity from a source of error
into a strength, boosting robustness to semantic and visual OOD perturbations
and enabling novel behavior composition without costly re-training. We also
contribute a reasoning-annotated extension of LIBERO-100, environment
variations tailored for OOD evaluation, and demonstrate up to 15% performance
gain over prior work on behavior composition tasks and scales with compute and
data diversity. Project Website at:
https://yilin-wu98.github.io/steering-reasoning-vla/

</details>


### [411] [SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling](https://arxiv.org/abs/2510.16308)
*Chi Zhang,Xian Huang,Wei Dong*

Main category: cs.RO

TL;DR: SPOT框架通过障碍物威胁建模，将感知目标融入运动规划，显著提升无人机在动态环境中的避障能力。


<details>
  <summary>Details</summary>
Motivation: 无人机单深度相机视野有限，现有方法将运动规划与感知分离，导致避障效果不佳。

Method: 基于高斯过程的障碍物置信地图，结合碰撞感知推理机制，生成时变观测紧迫性地图，实现实时规划。

Result: 仿真和实验显示，SPOT比基线方法提前2.8秒检测动态障碍物，可见性提升500%。

Conclusion: SPOT框架有效解决了动态环境中的避障问题，适用于复杂场景。

Abstract: UAVs equipped with a single depth camera encounter significant challenges in
dynamic obstacle avoidance due to limited field of view and inevitable blind
spots. While active vision strategies that steer onboard cameras have been
proposed to expand sensing coverage, most existing methods separate motion
planning from sensing considerations, resulting in less effective and delayed
obstacle response. To address this limitation, we introduce SPOT
(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning
framework for observation-aware trajectory planning that explicitly
incorporates sensing objectives into motion optimization. At the core of our
method is a Gaussian Process-based obstacle belief map, which establishes a
unified probabilistic representation of both recognized (previously observed)
and potential obstacles. This belief is further processed through a
collision-aware inference mechanism that transforms spatial uncertainty and
trajectory proximity into a time-varying observation urgency map. By
integrating urgency values within the current field of view, we define
differentiable objectives that enable real-time, observation-aware trajectory
planning with computation times under 10 ms. Simulation and real-world
experiments in dynamic, cluttered, and occluded environments show that our
method detects potential dynamic obstacles 2.8 seconds earlier than baseline
approaches, increasing dynamic obstacle visibility by over 500\%, and enabling
safe navigation through cluttered, occluded environments.

</details>


### [412] [Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models](https://arxiv.org/abs/2510.16344)
*Chenrui Tie,Shengxiang Sun,Yudi Lin,Yanbo Wang,Zhongrui Li,Zhouhan Zhong,Jinxuan Zhu,Yiman Pang,Haonan Chen,Junting Chen,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: Manual2Skill++ 是一个视觉语言框架，从装配手册中自动提取结构化连接信息，将装配任务编码为层次图，以连接关系为核心。


<details>
  <summary>Details</summary>
Motivation: 现有机器人装配方法通常将连接器视为次要问题，而连接是装配成功的关键。

Method: 利用大规模视觉语言模型解析手册中的符号图和注释，构建层次图表示连接关系。

Result: 在包含20多个装配任务的数据集上验证了方法，并在四个复杂装配场景中评估了从理解到执行的完整流程。

Conclusion: 将连接作为装配表示的核心元素，能够更可靠地完成装配任务。

Abstract: Assembly hinges on reliably forming connections between parts; yet most
robotic approaches plan assembly sequences and part poses while treating
connectors as an afterthought. Connections represent the critical "last mile"
of assembly execution, while task planning may sequence operations and motion
plan may position parts, the precise establishment of physical connections
ultimately determines assembly success or failure. In this paper, we consider
connections as first-class primitives in assembly representation, including
connector types, specifications, quantities, and placement locations. Drawing
inspiration from how humans learn assembly tasks through step-by-step
instruction manuals, we present Manual2Skill++, a vision-language framework
that automatically extracts structured connection information from assembly
manuals. We encode assembly tasks as hierarchical graphs where nodes represent
parts and sub-assemblies, and edges explicitly model connection relationships
between components. A large-scale vision-language model parses symbolic
diagrams and annotations in manuals to instantiate these graphs, leveraging the
rich connection knowledge embedded in human-designed instructions. We curate a
dataset containing over 20 assembly tasks with diverse connector types to
validate our representation extraction approach, and evaluate the complete task
understanding-to-execution pipeline across four complex assembly scenarios in
simulation, spanning furniture, toys, and manufacturing components with
real-world correspondence.

</details>


### [413] [Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach](https://arxiv.org/abs/2510.16424)
*Dan Guo,Xibin Jin,Shuai Wang,Zhigang Wen,Miaowen Wen,Chengzhong Xu*

Main category: cs.RO

TL;DR: 提出了一种集成感知、运动和通信（IPMC）的边缘机器人系统，通过动态调整通信策略减少通信开销，并利用学习优化（LTO）降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了机器人功能与通信条件的相互依赖，导致通信开销过大。

Method: 通过IPMC动态调整通信策略，并设计模仿学习神经网络（LTO）优化计算。

Result: 实验表明IPMC优于现有方法，LTO的计算复杂度降低10倍以上，具备实时执行能力。

Conclusion: IPMC和LTO的结合显著提升了边缘机器人系统的效率和实时性。

Abstract: Edge robotics involves frequent exchanges of large-volume multi-modal data.
Existing methods ignore the interdependency between robotic functionalities and
communication conditions, leading to excessive communication overhead. This
paper revolutionizes edge robotics systems through integrated perception,
motion, and communication (IPMC). As such, robots can dynamically adapt their
communication strategies (i.e., compression ratio, transmission frequency,
transmit power) by leveraging the knowledge of robotic perception and motion
dynamics, thus reducing the need for excessive sensor data uploads.
Furthermore, by leveraging the learning to optimize (LTO) paradigm, an
imitation learning neural network is designed and implemented, which reduces
the computational complexity by over 10x compared to state-of-the art
optimization solvers. Experiments demonstrate the superiority of the proposed
IPMC and the real-time execution capability of LTO.

</details>


### [414] [What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics](https://arxiv.org/abs/2510.16435)
*Lennart Wachowiak,Andrew Coles,Gerard Canal,Oya Celiktutan*

Main category: cs.RO

TL;DR: 论文介绍了一个包含1,893个用户问题的数据集，用于家庭机器人，涵盖12个类别和70个子类别，旨在帮助机器人更好地回答用户问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型和对话界面在人机交互中的应用增加，机器人回答用户问题的能力变得尤为重要。

Method: 通过15个视频和7个文本刺激材料，收集了100名参与者在不同情境下对家庭机器人提出的问题。

Result: 数据集显示，用户最常问的是任务执行细节（22.5%）、机器人能力（12.7%）和性能评估（11.3%），但用户认为最重要的问题是机器人如何处理复杂场景。

Conclusion: 该数据集为机器人日志记录、问答模块基准测试和解释策略设计提供了重要基础。

Abstract: With the growing use of large language models and conversational interfaces
in human-robot interaction, robots' ability to answer user questions is more
important than ever. We therefore introduce a dataset of 1,893 user questions
for household robots, collected from 100 participants and organized into 12
categories and 70 subcategories. Most work in explainable robotics focuses on
why-questions. In contrast, our dataset provides a wide variety of questions,
from questions about simple execution details to questions about how the robot
would act in hypothetical scenarios -- thus giving roboticists valuable
insights into what questions their robot needs to be able to answer. To collect
the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots
performing varied household tasks. We then asked participants on Prolific what
questions they would want to ask the robot in each portrayed situation. In the
final dataset, the most frequent categories are questions about task execution
details (22.5%), the robot's capabilities (12.7%), and performance assessments
(11.3%). Although questions about how robots would handle potentially difficult
scenarios and ensure correct behavior are less frequent, users rank them as the
most important for robots to be able to answer. Moreover, we find that users
who identify as novices in robotics ask different questions than more
experienced users. Novices are more likely to inquire about simple facts, such
as what the robot did or the current state of the environment. As robots enter
environments shared with humans and language becomes central to giving
instructions and interaction, this dataset provides a valuable foundation for
(i) identifying the information robots need to log and expose to conversational
interfaces, (ii) benchmarking question-answering modules, and (iii) designing
explanation strategies that align with user expectations.

</details>


### [415] [Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks](https://arxiv.org/abs/2510.16500)
*Chen Min,Jilin Mei,Heng Zhai,Shuai Wang,Tong Sun,Fanjie Kong,Haoyang Li,Fangyuan Mao,Fuyang Liu,Shuo Wang,Yiming Nie,Qi Zhu,Liang Xiao,Dawei Zhao,Yu Hu*

Main category: cs.RO

TL;DR: ORAD-3D是最大的越野自动驾驶数据集，涵盖多种地形和环境变化，并提供了五项基准任务。


<details>
  <summary>Details</summary>
Motivation: 解决越野自动驾驶研究中大规模高质量数据集和基准的稀缺问题。

Method: 构建ORAD-3D数据集，覆盖多种地形和天气条件，并设计五项基准任务。

Result: 提供了统一的资源，支持越野场景下的感知和规划研究。

Conclusion: ORAD-3D数据集和基准为越野自动驾驶研究提供了重要支持。

Abstract: A major bottleneck in off-road autonomous driving research lies in the
scarcity of large-scale, high-quality datasets and benchmarks. To bridge this
gap, we present ORAD-3D, which, to the best of our knowledge, is the largest
dataset specifically curated for off-road autonomous driving. ORAD-3D covers a
wide spectrum of terrains, including woodlands, farmlands, grasslands,
riversides, gravel roads, cement roads, and rural areas, while capturing
diverse environmental variations across weather conditions (sunny, rainy,
foggy, and snowy) and illumination levels (bright daylight, daytime, twilight,
and nighttime). Building upon this dataset, we establish a comprehensive suite
of benchmark evaluations spanning five fundamental tasks: 2D free-space
detection, 3D occupancy prediction, rough GPS-guided path planning,
vision-language model-driven autonomous driving, and world model for off-road
environments. Together, the dataset and benchmarks provide a unified and robust
resource for advancing perception and planning in challenging off-road
scenarios. The dataset and code will be made publicly available at
https://github.com/chaytonmin/ORAD-3D.

</details>


### [416] [A Novel Gripper with Semi-Peaucellier Linkage and Idle-Stroke Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16517)
*Haokai Ding,Wenzeng Zhang*

Main category: cs.RO

TL;DR: 介绍了一种新型SPD机械夹爪，具有线性平行夹持功能，解决了传统夹爪需调整高度的问题，并具备自适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统工业夹爪的弧形运动轨迹需要调整机械臂高度，SPD夹爪通过线性运动轨迹解决了这一问题，同时适应不同形状和大小的物体。

Method: 提出了SPD夹爪的设计理念、组成原理和优化分析理论，并开发了原型进行测试。

Result: 实验证明SPD夹爪成功实现了线性平行夹持功能，并表现出良好的适应性。

Conclusion: SPD夹爪为机器人有效抓取提供了新方案，有助于提升深度学习训练的数据收集能力。

Abstract: This paper introduces a novel robotic gripper, named as the SPD gripper. It
features a palm and two mechanically identical and symmetrically arranged
fingers, which can be driven independently or by a single motor. The fingertips
of the fingers follow a linear motion trajectory, facilitating the grasping of
objects of various sizes on a tabletop without the need to adjust the overall
height of the gripper. Traditional industrial grippers with parallel gripping
capabilities often exhibit an arcuate motion at the fingertips, requiring the
entire robotic arm to adjust its height to avoid collisions with the tabletop.
The SPD gripper, with its linear parallel gripping mechanism, effectively
addresses this issue. Furthermore, the SPD gripper possesses adaptive
capabilities, accommodating objects of different shapes and sizes. This paper
presents the design philosophy, fundamental composition principles, and
optimization analysis theory of the SPD gripper. Based on the design theory, a
robotic gripper prototype was developed and tested. The experimental results
demonstrate that the robotic gripper successfully achieves linear parallel
gripping functionality and exhibits good adaptability. In the context of the
ongoing development of embodied intelligence technologies, this robotic gripper
can assist various robots in achieving effective grasping, laying a solid
foundation for collecting data to enhance deep learning training.

</details>


### [417] [DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation](https://arxiv.org/abs/2510.16518)
*Jesús Ortega-Peimbert,Finn Lukas Busch,Timon Homberger,Quantao Yang,Olov Andersson*

Main category: cs.RO

TL;DR: DIV-Nav是一个实时导航系统，通过分解复杂空间约束的自然语言指令为简单对象查询，计算语义信念图的交集，并通过LVLM验证对象，有效解决了复杂自由文本查询的导航问题。


<details>
  <summary>Details</summary>
Motivation: 现有零-shot对象导航通常仅支持简单查询（如“电视”或“蓝色地毯”），而无法处理带有空间关系的复杂自由文本查询（如“在桌子上找到遥控器”）。

Method: 1. 分解复杂空间约束的自然语言指令为简单对象查询；2. 计算个体语义信念图的交集以确定对象共存的区域；3. 通过LVLM验证对象是否符合原始空间约束。

Result: 在MultiON基准测试和波士顿动力Spot机器人上的实际部署中验证了系统的有效性。

Conclusion: DIV-Nav通过一系列松弛方法，成功实现了对复杂自由文本查询的实时导航，并提升了搜索效率。

Abstract: Advances in open-vocabulary semantic mapping and object navigation have
enabled robots to perform an informed search of their environment for an
arbitrary object. However, such zero-shot object navigation is typically
designed for simple queries with an object name like "television" or "blue
rug". Here, we consider more complex free-text queries with spatial
relationships, such as "find the remote on the table" while still leveraging
robustness of a semantic map. We present DIV-Nav, a real-time navigation system
that efficiently addresses this problem through a series of relaxations: i)
Decomposing natural language instructions with complex spatial constraints into
simpler object-level queries on a semantic map, ii) computing the Intersection
of individual semantic belief maps to identify regions where all objects
co-exist, and iii) Validating the discovered objects against the original,
complex spatial constrains via a LVLM. We further investigate how to adapt the
frontier exploration objectives of online semantic mapping to such spatial
search queries to more effectively guide the search process. We validate our
system through extensive experiments on the MultiON benchmark and real-world
deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More
details and videos are available at https://anonsub42.github.io/reponame/

</details>


### [418] [Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping](https://arxiv.org/abs/2510.16524)
*Haokai Ding,Zhaohan Chen,Tao Yang,Wenzeng Zhang*

Main category: cs.RO

TL;DR: SP-Diff平行夹爪系统通过创新的差动连杆机制和模块化对称双指设计，提升工业自动化中夹爪的适应性。


<details>
  <summary>Details</summary>
Motivation: 解决传统末端执行器在智能工业自动化中适应性不足的问题。

Method: 采用差动连杆机制和行星齿轮传动，实现线性平行抓取和独立手指姿态调整。

Result: 系统结构刚性增强，Z轴重新校准需求减少30%，适用于多样化工件和可变形物体。

Conclusion: SP-Diff通过自适应架构提升机器人末端执行器的智能性，适用于协作机器人、物流自动化等场景。

Abstract: This paper presents the SP-Diff parallel gripper system, addressing the
limited adaptability of conventional end-effectors in intelligent industrial
automation. The proposed design employs an innovative differential linkage
mechanism with a modular symmetric dual-finger configuration to achieve
linear-parallel grasping. By integrating a planetary gear transmission, the
system enables synchronized linear motion and independent finger pose
adjustment while maintaining structural rigidity, reducing Z-axis recalibration
requirements by 30% compared to arc-trajectory grippers. The compact palm
architecture incorporates a kinematically optimized parallelogram linkage and
Differential mechanism, demonstrating adaptive grasping capabilities for
diverse industrial workpieces and deformable objects such as citrus fruits.
Future-ready interfaces are embedded for potential force/vision sensor
integration to facilitate multimodal data acquisition (e.g., trajectory
planning and object deformation) in digital twin frameworks. Designed as a
flexible manufacturing solution, SP-Diff advances robotic end-effector
intelligence through its adaptive architecture, showing promising applications
in collaborative robotics, logistics automation, and specialized operational
scenarios.

</details>


### [419] [MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation](https://arxiv.org/abs/2510.16617)
*Ruihan Zhao,Tyler Ingebrand,Sandeep Chinchali,Ufuk Topcu*

Main category: cs.RO

TL;DR: MoS-VLA框架通过线性组合学习到的基函数表示机器人策略，仅需单次演示即可适应新任务。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在新环境或任务中表现不佳，需要更高效的适应方法。

Method: 联合学习基函数，通过轻量级凸优化推断新任务的技能表示。

Result: 在五个未见数据集上动作预测误差更低，仿真和真实任务中表现优于现有VLA模型。

Conclusion: MoS-VLA通过梯度优化和技能空间结构，显著提升了模型的适应性和性能。

Abstract: Vision-Language-Action (VLA) models trained on large robot datasets promise
general-purpose, robust control across diverse domains and embodiments.
However, existing approaches often fail out-of-the-box when deployed in novel
environments, embodiments, or tasks. We introduce Mixture of Skills VLA
(MoS-VLA), a framework that represents robot manipulation policies as linear
combinations of a finite set of learned basis functions. During pretraining,
MoS-VLA jointly learns these basis functions across datasets from the Open
X-Embodiment project, producing a structured skill space. At test time,
adapting to a new task requires only a single expert demonstration. The
corresponding skill representation is then inferred via a lightweight convex
optimization problem that minimizes the L1 action error, without requiring
gradient updates. This gradient-free adaptation incurs minimal overhead while
enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower
action-prediction error on five out of five unseen datasets and succeeds in
both simulation and real-robot tasks where a pretrained VLA model fails
outright. Project page: mos-vla.github.io/

</details>


### [420] [First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response](https://arxiv.org/abs/2510.16692)
*Tianshu Ruan,Zoe Betta,Georgios Tzoumas,Rustam Stolkin,Manolis Chiou*

Main category: cs.RO

TL;DR: 研究调查了急救人员对在紧急行动中使用语义信息和情境感知（SA）的机器人系统的态度，发现大多数急救人员对机器人持积极态度，并认为语义信息对构建SA有用。


<details>
  <summary>Details</summary>
Motivation: 了解急救人员对语义信息和SA在机器人系统中的态度，以开发更符合用户需求的情境感知机器人系统。

Method: 对来自八个国家的22名急救人员进行结构化问卷调查，收集其人口统计信息、对机器人的态度及语义增强SA的体验。

Result: 急救人员对机器人持积极态度，语义信息对构建SA的平均评分为3.6/5，预测突发事件的评分为3.9。信任语义输出需74.6%的准确率，实用性需67.8%。

Conclusion: 研究揭示了急救人员最重视的语义信息类型（如物体身份、空间关系、风险背景），并指出实验室机器人能力与现场部署之间的差距，强调需加强急救人员与机器人研究者的合作。

Abstract: This study investigates First Responders' (FRs) attitudes toward the use of
semantic information and Situational Awareness (SA) in robotic systems during
emergency operations. A structured questionnaire was administered to 22 FRs
across eight countries, capturing their demographic profiles, general attitudes
toward robots, and experiences with semantics-enhanced SA. Results show that
most FRs expressed positive attitudes toward robots, and rated the usefulness
of semantic information for building SA at an average of 3.6 out of 5. Semantic
information was also valued for its role in predicting unforeseen emergencies
(mean 3.9). Participants reported requiring an average of 74.6\% accuracy to
trust semantic outputs and 67.8\% for them to be considered useful, revealing a
willingness to use imperfect but informative AI support tools.
  To the best of our knowledge, this study offers novel insights by being one
of the first to directly survey FRs on semantic-based SA in a cross-national
context. It reveals the types of semantic information most valued in the field,
such as object identity, spatial relationships, and risk context-and connects
these preferences to the respondents' roles, experience, and education levels.
The findings also expose a critical gap between lab-based robotics capabilities
and the realities of field deployment, highlighting the need for more
meaningful collaboration between FRs and robotics researchers. These insights
contribute to the development of more user-aligned and situationally aware
robotic systems for emergency response.

</details>


### [421] [Towards Active Excitation-Based Dynamic Inertia Identification in Satellites](https://arxiv.org/abs/2510.16738)
*Matteo El-Hariry,Vittorio Franzese,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 分析激励设计对纳米和微型卫星惯性特性识别的影响，比较两种估计器在不同条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 研究激励设计如何影响卫星惯性特性的识别，为在轨自适应惯性识别提供实用指导。

Method: 模拟非线性姿态动力学，使用八种不同频谱特性的扭矩激励系统，比较最小二乘法和扩展卡尔曼滤波器的性能。

Result: 激励频率内容和估计器假设共同决定估计的准确性和鲁棒性，明确了每种方法的最佳适用条件。

Conclusion: 研究结果为在轨自适应惯性识别提供了实用指导，并开源了相关代码。

Abstract: This paper presents a comprehensive analysis of how excitation design
influences the identification of the inertia properties of rigid nano- and
micro-satellites. We simulate nonlinear attitude dynamics with reaction-wheel
coupling, actuator limits, and external disturbances, and excite the system
using eight torque profiles of varying spectral richness. Two estimators are
compared, a batch Least Squares method and an Extended Kalman Filter, across
three satellite configurations and time-varying inertia scenarios. Results show
that excitation frequency content and estimator assumptions jointly determine
estimation accuracy and robustness, offering practical guidance for in-orbit
adaptive inertia identification by outlining the conditions under which each
method performs best. The code is provided as open-source .

</details>


### [422] [Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2510.16755)
*Kyung-Hwan Kim,DongHyun Ahn,Dong-hyun Lee,JuYoung Yoon,Dong Jin Hyun*

Main category: cs.RO

TL;DR: 提出了一种自适应不变扩展卡尔曼滤波器，用于提高腿式机器人的本体感知状态估计。


<details>
  <summary>Details</summary>
Motivation: 腿式机器人的状态估计对控制性能和运动稳定性至关重要，传统方法在接触条件变化和小滑动情况下表现不佳。

Method: 通过在线协方差估计自适应调整接触脚模型的噪声水平，并采用接触检测算法替代接触传感器。

Result: 在四足机器人LeoQuad上的实验验证了该方法在动态运动场景中提升了状态估计性能。

Conclusion: 该方法有效改善了腿式机器人在不同接触条件下的状态估计，减少了对额外硬件的依赖。

Abstract: State estimation is crucial for legged robots as it directly affects control
performance and locomotion stability. In this paper, we propose an Adaptive
Invariant Extended Kalman Filter to improve proprioceptive state estimation for
legged robots. The proposed method adaptively adjusts the noise level of the
contact foot model based on online covariance estimation, leading to improved
state estimation under varying contact conditions. It effectively handles small
slips that traditional slip rejection fails to address, as overly sensitive
slip rejection settings risk causing filter divergence. Our approach employs a
contact detection algorithm instead of contact sensors, reducing the reliance
on additional hardware. The proposed method is validated through real-world
experiments on the quadruped robot LeoQuad, demonstrating enhanced state
estimation performance in dynamic locomotion scenarios.

</details>


### [423] [T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic](https://arxiv.org/abs/2510.16767)
*Jia Li,Guoxiang Zhao*

Main category: cs.RO

TL;DR: T3 Planner是一个基于LLM的机器人运动规划框架，通过形式化方法自我纠正输出，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域专业知识且难以处理时空耦合问题，LLM虽擅长语义推理但可能生成不可行运动计划。

Method: T3 Planner通过三个级联模块分解时空任务约束，利用STL验证器筛选可行轨迹。

Result: 实验表明T3 Planner在不同场景下表现优异，且可蒸馏为轻量级模型Qwen3-4B。

Conclusion: T3 Planner有效解决了LLM在运动规划中的幻觉问题，提升了任务执行的可行性。

Abstract: Translating natural language instructions into executable motion plans is a
fundamental challenge in robotics. Traditional approaches are typically
constrained by their reliance on domain-specific expertise to customize
planners, and often struggle with spatio-temporal couplings that usually lead
to infeasible motions or discrepancies between task planning and motion
execution. Despite the proficiency of Large Language Models (LLMs) in
high-level semantic reasoning, hallucination could result in infeasible motion
plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic
motion planning framework that self-corrects it output with formal methods. The
framework decomposes spatio-temporal task constraints via three cascaded
modules, each of which stimulates an LLM to generate candidate trajectory
sequences and examines their feasibility via a Signal Temporal Logic (STL)
verifier until one that satisfies complex spatial, temporal, and logical
constraints is found.Experiments across different scenarios show that T3
Planner significantly outperforms the baselines. The required reasoning can be
distilled into a lightweight Qwen3-4B model that enables efficient deployment.
All supplementary materials are accessible at
https://github.com/leeejia/T3_Planner.

</details>


### [424] [A Preliminary Exploration of the Differences and Conjunction of Traditional PNT and Brain-inspired PNT](https://arxiv.org/abs/2510.16771)
*Xu He,Xiaolin Meng,Wenxuan Yin,Youdong Zhang,Lingfei Mo,Xiangdong An,Fangwen Yu,Shuguo Pan,Yufeng Liu,Jingnan Liu,Yujia Zhang,Wang Gao*

Main category: cs.RO

TL;DR: 论文探讨如何通过脑启发空间认知导航提升无人系统的PNT能力，提出从“工具导向”转向“认知驱动”的新视角和路线图。


<details>
  <summary>Details</summary>
Motivation: 当前复杂环境需要更弹性、节能且认知能力强的PNT系统，结合机器PNT的高精度与脑启发认知以推动通用PNT发展。

Method: 提出四层（观测-能力-决策-硬件）融合框架，结合数值精度与脑启发智能，并对比传统PNT、生物脑PNT与脑启发PNT的差异。

Result: 提供了脑启发PNT未来发展的前瞻性建议，为通用PNT系统设计提供了新思路。

Conclusion: 通过脑启发认知与机器PNT的结合，有望实现更先进的通用PNT系统，推动从工具导向到认知驱动的转变。

Abstract: Developing universal Positioning, Navigation, and Timing (PNT) is our
enduring goal. Today's complex environments demand PNT that is more resilient,
energy-efficient and cognitively capable. This paper asks how we can endow
unmanned systems with brain-inspired spatial cognition navigation while
exploiting the high precision of machine PNT to advance universal PNT. We
provide a new perspective and roadmap for shifting PNT from "tool-oriented" to
"cognition-driven". Contributions: (1) multi-level dissection of differences
among traditional PNT, biological brain PNT and brain-inspired PNT; (2) a
four-layer (observation-capability-decision-hardware) fusion framework that
unites numerical precision and brain-inspired intelligence; (3) forward-looking
recommendations for future development of brain-inspired PNT.

</details>


### [425] [C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control](https://arxiv.org/abs/2510.16905)
*Yukang Cao,Rahul Moorthy,O. Goktug Poyrazoglu,Volkan Isler*

Main category: cs.RO

TL;DR: 论文提出了一种名为C-Free-Uniform的轨迹采样方法，通过均匀采样自由配置空间并考虑环境信息，显著提升了导航任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹采样方法生成的分布独立于环境，无法充分利用局部地图信息，限制了导航性能。

Method: 提出C-Free-Uniform方法，生成控制输入分布以均匀采样自由配置空间，并集成到MPPI控制器中（CFU-MPPI）。

Result: CFU-MPPI在复杂多边形环境中的导航任务中，以更小的采样预算实现了更高的成功率。

Conclusion: C-Free-Uniform通过结合环境信息优化采样分布，显著提升了导航任务的性能。

Abstract: Trajectory sampling is a key component of sampling-based control mechanisms.
Trajectory samplers rely on control input samplers, which generate control
inputs u from a distribution p(u | x) where x is the current state. We
introduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for
short) which has two key features: (i) it generates a control input
distribution so as to uniformly sample the free configuration space, and (ii)
in contrast to previously introduced trajectory sampling mechanisms where the
distribution p(u | x) is independent of the environment, C-Free-Uniform is
explicitly conditioned on the current local map. Next, we integrate this
sampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.
Experiments show that CFU-MPPI outperforms existing methods in terms of success
rate in challenging navigation tasks in cluttered polygonal environments while
requiring a much smaller sampling budget.

</details>


### [426] [Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems](https://arxiv.org/abs/2510.16931)
*Zhaoliang Wan,Zida Zhou,Zetong Bi,Zehui Yang,Hao Ding,Hui Cheng*

Main category: cs.RO

TL;DR: RAPID Hand是一种低成本、20自由度的灵巧手，用于灵巧遥操作，通过新型驱动和传动设计提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决灵巧遥操作中低成本、高自由度灵巧手稀缺的问题，以支持大规模真实机器人数据收集。

Method: 采用3D打印部件和定制齿轮，结合新型驱动和传动设计，包括非拇指手指的通用指骨传动和拇指的全向驱动机制。

Result: 在多项灵巧任务测试中表现优异，展示了其在灵巧遥操作中的潜力。

Conclusion: RAPID Hand的低成本和高性能设计为灵巧遥操作提供了有前景的解决方案。

Abstract: This paper addresses the scarcity of affordable, fully-actuated five-fingered
hands for dexterous teleoperation, which is crucial for collecting large-scale
real-robot data within the "Learning from Demonstrations" paradigm. We
introduce the prototype version of the RAPID Hand, the first low-cost,
20-degree-of-actuation (DoA) dexterous hand that integrates a novel
anthropomorphic actuation and transmission scheme with an optimized motor
layout and structural design to enhance dexterity. Specifically, the RAPID Hand
features a universal phalangeal transmission scheme for the non-thumb fingers
and an omnidirectional thumb actuation mechanism. Prioritizing affordability,
the hand employs 3D-printed parts combined with custom gears for easier
replacement and repair. We assess the RAPID Hand's performance through
quantitative metrics and qualitative testing in a dexterous teleoperation
system, which is evaluated on three challenging tasks: multi-finger retrieval,
ladle handling, and human-like piano playing. The results indicate that the
RAPID Hand's fully actuated 20-DoF design holds significant promise for
dexterous teleoperation.

</details>


### [427] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA是一种多模态目标条件行为克隆框架，用于实现导管导航的自主性，减少医生依赖。


<details>
  <summary>Details</summary>
Motivation: 现有导管导航系统依赖手动操作，导致医生疲劳、辐射暴露增加和结果不一致，需要智能自主解决方案。

Method: 提出DINO-CVA框架，融合视觉和运动学数据到联合嵌入空间，通过专家演示自回归预测动作，目标条件引导导航。

Result: DINO-CVA在动作预测上表现高精度，与仅运动学基线相当，同时结合解剖环境信息。

Conclusion: 多模态目标条件架构在导管导航中可行，是减少医生依赖、提高导管治疗可靠性的重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [428] [Learning to Design Soft Hands using Reward Models](https://arxiv.org/abs/2510.17086)
*Xueqian Bai,Nicklas Hansen,Adabhav Singh,Michael T. Tolley,Yan Duan,Pieter Abbeel,Xiaolong Wang,Sha Yi*

Main category: cs.RO

TL;DR: 提出了一种基于交叉熵方法和奖励模型的框架（CEM-RM），用于高效优化肌腱驱动软体机器人手的设计，显著减少评估次数，并提升抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 设计既灵活又功能多样的软体机器人手具有挑战性，硬件与控制的协同设计虽有效但计算成本高。

Method: 采用CEM-RM框架，结合预收集的遥操作数据，优化软体机器人手的设计空间，并通过并行化仿真训练实现高效优化。

Result: 优化后的设计在仿真和硬件实验中均显著提升了抓取成功率，且评估次数减少一半以上。

Conclusion: CEM-RM框架为软体机器人手的设计优化提供了高效且实用的解决方案。

Abstract: Soft robotic hands promise to provide compliant and safe interaction with
objects and environments. However, designing soft hands to be both compliant
and functional across diverse use cases remains challenging. Although co-design
of hardware and control better couples morphology to behavior, the resulting
search space is high-dimensional, and even simulation-based evaluation is
computationally expensive. In this paper, we propose a Cross-Entropy Method
with Reward Model (CEM-RM) framework that efficiently optimizes tendon-driven
soft robotic hands based on teleoperation control policy, reducing design
evaluations by more than half compared to pure optimization while learning a
distribution of optimized hand designs from pre-collected teleoperation data.
We derive a design space for a soft robotic hand composed of flexural soft
fingers and implement parallelized training in simulation. The optimized hands
are then 3D-printed and deployed in the real world using both teleoperation
data and real-time teleoperation. Experiments in both simulation and hardware
demonstrate that our optimized design significantly outperforms baseline hands
in grasping success rates across a diverse set of challenging objects.

</details>


### [429] [Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey](https://arxiv.org/abs/2510.17111)
*Weifan Guan,Qinghao Hu,Aosheng Li,Jian Cheng*

Main category: cs.RO

TL;DR: 本文综述了提升视觉-语言-动作（VLA）模型效率的方法，重点关注降低延迟、内存占用及训练/推理成本。


<details>
  <summary>Details</summary>
Motivation: VLA模型在实时边缘平台（如移动机器人）上的应用面临计算和内存需求高的挑战，需更高效解决方案。

Method: 将现有方法分为四个维度：模型架构、感知特征、动作生成和训练/推理策略，并总结代表性技术。

Result: 系统梳理了提升VLA效率的技术，为未来研究提供参考。

Conclusion: 未来需进一步研究高效具身智能的方向和挑战。

Abstract: Vision-Language-Action (VLA) models extend vision-language models to embodied
control by mapping natural-language instructions and visual observations to
robot actions. Despite their capabilities, VLA systems face significant
challenges due to their massive computational and memory demands, which
conflict with the constraints of edge platforms such as on-board mobile
manipulators that require real-time performance. Addressing this tension has
become a central focus of recent research. In light of the growing efforts
toward more efficient and scalable VLA systems, this survey provides a
systematic review of approaches for improving VLA efficiency, with an emphasis
on reducing latency, memory footprint, and training and inference costs. We
categorize existing solutions into four dimensions: model architecture,
perception feature, action generation, and training/inference strategies,
summarizing representative techniques within each category. Finally, we discuss
future trends and open challenges, highlighting directions for advancing
efficient embodied intelligence.

</details>


### [430] [Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning](https://arxiv.org/abs/2510.17143)
*Shantnav Agarwal,Javier Alonso-Mora,Sihao Sun*

Main category: cs.RO

TL;DR: 提出了一种基于机器学习的分散式动力学规划方法，用于多无人机协同运输悬挂负载，无需通信且适应部分可观测性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖集中控制或可靠通信，限制了灵活性和鲁棒性。

Method: 通过模仿学习训练分散式学生策略，利用物理信息神经网络生成平滑轨迹。

Result: 在仿真和实际环境中验证，性能接近集中式方法，且训练高效。

Conclusion: 该方法为分散式负载运输提供了高效且可靠的解决方案。

Abstract: Existing approaches for transporting and manipulating cable-suspended loads
using multiple UAVs along reference trajectories typically rely on either
centralized control architectures or reliable inter-agent communication. In
this work, we propose a novel machine learning based method for decentralized
kinodynamic planning that operates effectively under partial observability and
without inter-agent communication. Our method leverages imitation learning to
train a decentralized student policy for each UAV by imitating a centralized
kinodynamic motion planner with access to privileged global observations. The
student policy generates smooth trajectories using physics-informed neural
networks that respect the derivative relationships in motion. During training,
the student policies utilize the full trajectory generated by the teacher
policy, leading to improved sample efficiency. Moreover, each student policy
can be trained in under two hours on a standard laptop. We validate our method
in both simulation and real-world environments to follow an agile reference
trajectory, demonstrating performance comparable to that of centralized
approaches.

</details>


### [431] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一个增强的自动驾驶框架，通过度量引导对齐结合认知推理和端到端规划。


<details>
  <summary>Details</summary>
Motivation: 传统端到端驾驶模型缺乏世界知识，难以处理长尾场景；而视觉-语言-动作模型3D推理能力有限，可能导致物理不可行动作。

Method: 1. 构建VLA模块生成语义驱动的轨迹；2. 设计端到端模块确保物理可行性；3. 引入度量引导的轨迹评分器对齐两个模块。

Result: 在ICCV 2025挑战赛中，DiffVLA++的EPDMS达到49.12。

Conclusion: DiffVLA++成功结合了认知推理和物理可行性，提升了自动驾驶性能。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [432] [OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation](https://arxiv.org/abs/2510.17150)
*Heng Zhang,Wei-Hsing Huang,Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: OmniVIC是一种增强型通用可变阻抗控制器，结合视觉语言模型，提升安全性和适应性，适用于接触密集的机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 传统可变阻抗控制器在复杂、未知任务中缺乏泛化能力，OmniVIC旨在通过视觉语言模型实现任务上下文理解和自适应阻抗参数生成。

Method: OmniVIC采用自改进的检索增强生成（RAG）和上下文学习（ICL），结合实时力/扭矩反馈，生成自适应阻抗参数。

Result: 在仿真和实际机器人任务中，OmniVIC显著提高了任务成功率（从27%提升至61.4%）并减少了力违规。

Conclusion: OmniVIC通过结合高级语义推理和低级合规控制，实现了更安全和更通用的机器人操作。

Abstract: We present OmniVIC, a universal variable impedance controller (VIC) enhanced
by a vision language model (VLM), which improves safety and adaptation in any
contact-rich robotic manipulation task to enhance safe physical interaction.
Traditional VIC have shown advantages when the robot physically interacts with
the environment, but lack generalization in unseen, complex, and unstructured
safe interactions in universal task scenarios involving contact or uncertainty.
To this end, the proposed OmniVIC interprets task context derived reasoning
from images and natural language and generates adaptive impedance parameters
for a VIC controller. Specifically, the core of OmniVIC is a self-improving
Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG
retrieves relevant prior experiences from a structured memory bank to inform
the controller about similar past tasks, and ICL leverages these retrieved
examples and the prompt of current task to query the VLM for generating
context-aware and adaptive impedance parameters for the current manipulation
scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in
universal task scenarios. The impedance parameter regulation is further
informed by real-time force/torque feedback to ensure interaction forces remain
within safe thresholds. We demonstrate that our method outperforms baselines on
a suite of complex contact-rich tasks, both in simulation and on real-world
robotic tasks, with improved success rates and reduced force violations.
OmniVIC takes a step towards bridging high-level semantic reasoning and
low-level compliant control, enabling safer and more generalizable
manipulation. Overall, the average success rate increases from 27% (baseline)
to 61.4% (OmniVIC).

</details>


### [433] [SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving](https://arxiv.org/abs/2510.17191)
*Peiru Zheng,Yun Zhao,Zhan Gong,Hong Zhu,Shaohua Wu*

Main category: cs.RO

TL;DR: SimpleVSF框架通过结合视觉语言模型和轨迹融合技术，提升端到端自动驾驶规划的性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端方法在复杂场景中决策不佳，需改进。

Method: 结合传统评分器和VLM增强评分器，使用权重融合器和VLM融合器进行决策。

Result: 在ICCV 2025 NAVSIM v2挑战中表现最佳，平衡安全、舒适和效率。

Conclusion: SimpleVSF通过VLM和融合技术显著提升自动驾驶性能。

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
achieving robust and intelligent driving policies. However, existing end-to-end
methods still face significant challenges, such as suboptimal decision-making
in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring
Fusion), a novel framework that enhances end-to-end planning by leveraging the
cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory
fusion techniques. We utilize the conventional scorers and the novel
VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative
aggregation and a powerful VLM-based fusioner for qualitative, context-aware
decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End
Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art
performance, achieving a superior balance between safety, comfort, and
efficiency.

</details>


### [434] [Performance Evaluation of an Integrated System for Visible Light Communication and Positioning Using an Event Camera](https://arxiv.org/abs/2510.17203)
*Ryota Soga,Masataka Kobayashi,Tsukasa Shimizu,Shintaro Shiba,Quan Kong,Shan Lu,Takaya Yamazato*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机的新型自定位系统，结合VLC和VLP技术，用于GPS失效环境下的车辆定位。


<details>
  <summary>Details</summary>
Motivation: 解决GPS在隧道等环境中失效的问题，利用事件相机的高动态范围和高时间分辨率特性。

Method: 使用Walsh-Hadamard码区分LED光源，通过相位相关估计距离，实现VLC和VLP功能。

Result: 在车速30 km/h下，距离估计误差小于0.75米，误码率低于0.01。

Conclusion: 首次实现单事件相机同时支持VLC和VLP，展示了在真实环境中的高精度和鲁棒性。

Abstract: Event cameras, featuring high temporal resolution and high dynamic range,
offer visual sensing capabilities comparable to conventional image sensors
while capturing fast-moving objects and handling scenes with extreme lighting
contrasts such as tunnel exits. Leveraging these properties, this study
proposes a novel self-localization system that integrates visible light
communication (VLC) and visible light positioning (VLP) within a single event
camera. The system enables a vehicle to estimate its position even in
GPS-denied environments, such as tunnels, by using VLC to obtain coordinate
information from LED transmitters and VLP to estimate the distance to each
transmitter.
  Multiple LEDs are installed on the transmitter side, each assigned a unique
pilot sequence based on Walsh-Hadamard codes. The event camera identifies
individual LEDs within its field of view by correlating the received signal
with these codes, allowing clear separation and recognition of each light
source. This mechanism enables simultaneous high-capacity MISO (multi-input
single-output) communication through VLC and precise distance estimation via
phase-only correlation (POC) between multiple LED pairs.
  To the best of our knowledge, this is the first vehicle-mounted system to
achieve simultaneous VLC and VLP functionalities using a single event camera.
Field experiments were conducted by mounting the system on a vehicle traveling
at 30 km/h (8.3 m/s). The results demonstrated robust real-world performance,
with a root mean square error (RMSE) of distance estimation within 0.75 m for
ranges up to 100 m and a bit error rate (BER) below 0.01 across the same range.

</details>


### [435] [Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance](https://arxiv.org/abs/2510.17237)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 提出了一种名为Pole-Image的混合方法，利用杆状地标作为锚点生成周围3D结构的签名，通过对比学习实现鲁棒的自定位和地图维护。


<details>
  <summary>Details</summary>
Motivation: 传统地标方法在高可检测性和高独特性之间存在权衡，需要一种既能稳定检测又具有高区分度的地标表示方法。

Method: 使用杆状地标作为锚点，生成周围点云的2D极坐标图像表示，并应用对比学习训练视角不变的高区分度描述符。

Result: 提出的描述符能够克服感知混淆，实现鲁棒自定位，同时高精度编码支持高灵敏度变化检测，有助于地图维护。

Conclusion: Pole-Image方法通过结合杆状地标的稳定性和周围点云的独特性，显著提升了长期自主移动机器人的定位和地图维护能力。

Abstract: Long-term autonomy for mobile robots requires both robust self-localization
and reliable map maintenance. Conventional landmark-based methods face a
fundamental trade-off between landmarks with high detectability but low
distinctiveness (e.g., poles) and those with high distinctiveness but difficult
stable detection (e.g., local point cloud structures). This work addresses the
challenge of descriptively identifying a unique "signature" (local point cloud)
by leveraging a detectable, high-precision "anchor" (like a pole). To solve
this, we propose a novel canonical representation, "Pole-Image," as a hybrid
method that uses poles as anchors to generate signatures from the surrounding
3D structure. Pole-Image represents a pole-like landmark and its surrounding
environment, detected from a LiDAR point cloud, as a 2D polar coordinate image
with the pole itself as the origin. This representation leverages the pole's
nature as a high-precision reference point, explicitly encoding the "relative
geometry" between the stable pole and the variable surrounding point cloud. The
key advantage of pole landmarks is that "detection" is extremely easy. This
ease of detection allows the robot to easily track the same pole, enabling the
automatic and large-scale collection of diverse observational data (positive
pairs). This data acquisition feasibility makes "Contrastive Learning (CL)"
applicable. By applying CL, the model learns a viewpoint-invariant and highly
discriminative descriptor. The contributions are twofold: 1) The descriptor
overcomes perceptual aliasing, enabling robust self-localization. 2) The
high-precision encoding enables high-sensitivity change detection, contributing
to map maintenance.

</details>


### [436] [An adaptive hierarchical control framework for quadrupedal robots in planetary exploration](https://arxiv.org/abs/2510.17249)
*Franek Stark,Rohit Kumar,Shubham Vyas,Hannah Isermann,Jonas Haack,Mihaela Popescu,Jakob Middelberg,Dennis Mronga,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一种模块化控制框架，结合模型动态控制和在线模型适应，用于四足机器人在未知环境中的导航。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在未知地形和机器人参数不确定时的控制问题。

Method: 结合模型动态控制、在线模型适应和自适应步态规划，支持状态估计和运行时重新配置。

Result: 在两种四足机器人平台和火山实地测试中验证了性能，机器人行走超过700米。

Conclusion: 该框架有效解决了四足机器人在未知环境中的控制挑战，并具备开源和可扩展性。

Abstract: Planetary exploration missions require robots capable of navigating extreme
and unknown environments. While wheeled rovers have dominated past missions,
their mobility is limited to traversable surfaces. Legged robots, especially
quadrupeds, can overcome these limitations by handling uneven, obstacle-rich,
and deformable terrains. However, deploying such robots in unknown conditions
is challenging due to the need for environment-specific control, which is
infeasible when terrain and robot parameters are uncertain. This work presents
a modular control framework that combines model-based dynamic control with
online model adaptation and adaptive footstep planning to address uncertainties
in both robot and terrain properties. The framework includes state estimation
for quadrupeds with and without contact sensing, supports runtime
reconfiguration, and is integrated into ROS 2 with open-source availability.
Its performance was validated on two quadruped platforms, multiple hardware
architectures, and in a volcano field test, where the robot walked over 700 m.

</details>


### [437] [High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection](https://arxiv.org/abs/2510.17261)
*Fernando Salanova,Jesús Roche,Cristian Mahuela,Eduardo Montijano*

Main category: cs.RO

TL;DR: 提出了一种基于Nets-within-Nets和Transformer的异常检测方法，用于识别多机器人系统中的异常行为，实验显示高准确率。


<details>
  <summary>Details</summary>
Motivation: 多机器人系统中异构代理的高层任务执行需要鲁棒的异常行为检测方法。

Method: 结合Nets-within-Nets框架协调机器人动作与LTL任务规范，并采用Transformer进行异常检测。

Result: 实验显示方法在识别执行低效（91.3%）、核心任务违规（88.3%）和约束异常（66.8%）方面表现优异。

Conclusion: 提出的方法在多机器人系统中有效检测异常行为，优于简单表示方法。

Abstract: The reliable execution of high-level missions in multi-robot systems with
heterogeneous agents, requires robust methods for detecting spurious behaviors.
In this paper, we address the challenge of identifying spurious executions of
plans specified as a Linear Temporal Logic (LTL) formula, as incorrect task
sequences, violations of spatial constraints, timing inconsis- tencies, or
deviations from intended mission semantics. To tackle this, we introduce a
structured data generation framework based on the Nets-within-Nets (NWN)
paradigm, which coordinates robot actions with LTL-derived global mission
specifications. We further propose a Transformer-based anomaly detection
pipeline that classifies robot trajectories as normal or anomalous. Experi-
mental evaluations show that our method achieves high accuracy (91.3%) in
identifying execution inefficiencies, and demonstrates robust detection
capabilities for core mission violations (88.3%) and constraint-based adaptive
anomalies (66.8%). An ablation experiment of the embedding and architecture was
carried out, obtaining successful results where our novel proposition performs
better than simpler representations.

</details>


### [438] [Floating-Base Deep Lagrangian Networks](https://arxiv.org/abs/2510.17270)
*Lucas Schulze,Juliano Decico Negri,Victor Barasuol,Vivian Suzano Medeiros,Marcelo Becker,Jan Peters,Oleg Arenz*

Main category: cs.RO

TL;DR: 提出了一种结合物理约束的深度学习模型FeLaN，用于浮动基座系统的惯性矩阵参数化，提高了物理一致性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有灰盒模型忽略了浮动基座系统（如人形机器人和四足机器人）的特定物理约束，导致物理不一致性。

Method: 基于Deep Lagrangian Networks (DeLaN)，训练神经网络预测物理上合理的惯性矩阵，最小化拉格朗日力学下的逆动力学误差。

Result: 在模拟和真实机器人实验中，FeLaN表现出色，同时提供了更高的物理可解释性。

Conclusion: FeLaN不仅提升了性能，还增强了模型的物理一致性，适用于浮动基座系统的动力学建模。

Abstract: Grey-box methods for system identification combine deep learning with
physics-informed constraints, capturing complex dependencies while improving
out-of-distribution generalization. Yet, despite the growing importance of
floating-base systems such as humanoids and quadrupeds, current grey-box models
ignore their specific physical constraints. For instance, the inertia matrix is
not only positive definite but also exhibits branch-induced sparsity and input
independence. Moreover, the 6x6 composite spatial inertia of the floating base
inherits properties of single-rigid-body inertia matrices. As we show, this
includes the triangle inequality on the eigenvalues of the composite rotational
inertia. To address the lack of physical consistency in deep learning models of
floating-base systems, we introduce a parameterization of inertia matrices that
satisfies all these constraints. Inspired by Deep Lagrangian Networks (DeLaN),
we train neural networks to predict physically plausible inertia matrices that
minimize inverse dynamics error under Lagrangian mechanics. For evaluation, we
collected and released a dataset on multiple quadrupeds and humanoids. In these
experiments, our Floating-Base Deep Lagrangian Networks (FeLaN) achieve highly
competitive performance on both simulated and real robots, while providing
greater physical interpretability.

</details>


### [439] [Implicit State Estimation via Video Replanning](https://arxiv.org/abs/2510.17315)
*Po-Chen Ko,Jiayuan Mao,Yu-Hsiang Fu,Hsien-Jeng Yeh,Chu-Rong Chen,Wei-Chiu Ma,Yilun Du,Shao-Hua Sun*

Main category: cs.RO

TL;DR: 提出了一种新框架，通过在线更新模型参数和过滤失败计划，提升视频规划在部分观测环境中的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有视频规划框架难以适应交互时的失败，因无法处理部分观测环境中的不确定性。

Method: 集成交互时数据到规划过程，在线更新参数并过滤失败计划，实现隐式状态估计。

Result: 在新模拟操作基准测试中，展示了改进的重新规划性能。

Conclusion: 该框架提升了视频决策的适应性，推动了领域发展。

Abstract: Video-based representations have gained prominence in planning and
decision-making due to their ability to encode rich spatiotemporal dynamics and
geometric relationships. These representations enable flexible and
generalizable solutions for complex tasks such as object manipulation and
navigation. However, existing video planning frameworks often struggle to adapt
to failures at interaction time due to their inability to reason about
uncertainties in partially observed environments. To overcome these
limitations, we introduce a novel framework that integrates interaction-time
data into the planning process. Our approach updates model parameters online
and filters out previously failed plans during generation. This enables
implicit state estimation, allowing the system to adapt dynamically without
explicitly modeling unknown state variables. We evaluate our framework through
extensive experiments on a new simulated manipulation benchmark, demonstrating
its ability to improve replanning performance and advance the field of
video-based decision-making.

</details>


### [440] [DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials](https://arxiv.org/abs/2510.17335)
*Xintong Yang,Minglun Wei,Ze Ji,Yu-Kun Lai*

Main category: cs.RO

TL;DR: 论文提出了一种名为DDBot的新框架，用于高效、高精度地操控未知物理特性的颗粒材料（如沙土）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在颗粒材料操控任务中效率与准确性不足，研究填补了这一空白。

Method: DDBot结合了可微分物理模拟器、GPU加速并行计算和自动微分技术，支持高效系统识别和技能优化。

Result: 实验表明，DDBot能在5-20分钟内收敛，并在零样本现实部署中实现高精度结果。

Conclusion: DDBot在颗粒材料挖掘任务中表现出高效性和鲁棒性，优于现有基线方法。

Abstract: Automating the manipulation of granular materials poses significant
challenges due to complex contact dynamics, unpredictable material properties,
and intricate system states. Existing approaches often fail to achieve
efficiency and accuracy in such tasks. To fill the research gap, this paper
studies the small-scale and high-precision granular material digging task with
unknown physical properties. A new framework, named differentiable digging
robot (DDBot), is proposed to manipulate granular materials, including sand and
soil.
  Specifically, we equip DDBot with a differentiable physics-based simulator,
tailored for granular material manipulation, powered by GPU-accelerated
parallel computing and automatic differentiation. DDBot can perform efficient
differentiable system identification and high-precision digging skill
optimisation for unknown granular materials, which is enabled by a
differentiable skill-to-action mapping, a task-oriented demonstration method,
gradient clipping and line search-based gradient descent.
  Experimental results show that DDBot can efficiently (converge within 5 to 20
minutes) identify unknown granular material dynamics and optimise digging
skills, with high-precision results in zero-shot real-world deployments,
highlighting its practicality. Benchmark results against state-of-the-art
baselines also confirm the robustness and efficiency of DDBot in such digging
tasks.

</details>


### [441] [Interactive Force-Impedance Control](https://arxiv.org/abs/2510.17341)
*Fan Shao,Satoshi Endo,Sandra Hirche,Fanny Ficuciello*

Main category: cs.RO

TL;DR: 提出了一种统一的交互力-阻抗控制（IFIC）框架，确保在接触丰富的环境中实现安全、轻松的交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法在接触稀疏环境中有效，但在接触丰富环境中机器人可能失去被动性，影响安全性。

Method: 基于端口哈密尔顿框架，设计了包含交互和任务控制端口的控制架构，确保系统被动性。

Result: IFIC框架能够适应交互功率流，实现安全、轻松的交互。

Conclusion: IFIC框架解决了接触丰富环境中的被动性和安全性问题。

Abstract: Human collaboration with robots requires flexible role adaptation, enabling
robot to switch between active leader and passive follower. Effective role
switching depends on accurately estimating human intention, which is typically
achieved through external force analysis, nominal robot dynamics, or
data-driven approaches. However, these methods are primarily effective in
contact-sparse environments. When robots under hybrid or unified
force-impedance control physically interact with active humans or non-passive
environments, the robotic system may lose passivity and thus compromise safety.
To address this challenge, this paper proposes the unified Interactive
Force-Impedance Control (IFIC) framework that adapts to the interaction power
flow, ensuring effortless and safe interaction in contact-rich environments.
The proposed control architecture is formulated within a port-Hamiltonian
framework, incorporating both interaction and task control ports, through which
system passivity is guaranteed.

</details>


### [442] [Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots](https://arxiv.org/abs/2510.17369)
*Haochen Su,Cristian Meo,Francesco Stella,Andrea Peirone,Kai Junge,Josie Hughes*

Main category: cs.RO

TL;DR: VLA模型在软体连续机械臂上的部署，通过微调实现安全人机交互。


<details>
  <summary>Details</summary>
Motivation: 解决刚性机械臂在非结构化环境中安全性和适应性不足的问题。

Method: 在软体连续机械臂上部署VLA模型，并进行结构化微调。

Result: 微调后软体机械臂性能与刚性机械臂相当。

Conclusion: VLA模型与软体机器人结合可实现安全灵活的人机交互。

Abstract: Robotic systems are increasingly expected to operate in human-centered,
unstructured environments where safety, adaptability, and generalization are
essential. Vision-Language-Action (VLA) models have been proposed as a language
guided generalized control framework for real robots. However, their deployment
has been limited to conventional serial link manipulators. Coupled by their
rigidity and unpredictability of learning based control, the ability to safely
interact with the environment is missing yet critical. In this work, we present
the deployment of a VLA model on a soft continuum manipulator to demonstrate
autonomous safe human-robot interaction. We present a structured finetuning and
deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and
$\pi_0$) across representative manipulation tasks, and show while
out-of-the-box policies fail due to embodiment mismatch, through targeted
finetuning the soft robot performs equally to the rigid counterpart. Our
findings highlight the necessity of finetuning for bridging embodiment gaps,
and demonstrate that coupling VLA models with soft robots enables safe and
flexible embodied AI in human-shared environments.

</details>


### [443] [Integrating Trustworthy Artificial Intelligence with Energy-Efficient Robotic Arms for Waste Sorting](https://arxiv.org/abs/2510.17408)
*Halima I. Kure,Jishna Retnakumari,Augustine O. Nwajana,Umar M. Ismail,Bilyaminu A. Romo,Ehigiator Egho-Promise*

Main category: cs.RO

TL;DR: 论文提出了一种结合可信AI与节能机械臂的智能垃圾分类方法，通过改进的CNN模型实现高精度分类，并在虚拟环境中验证了节能效果。


<details>
  <summary>Details</summary>
Motivation: 解决城市垃圾管理中分类效率低和能源消耗高的问题，同时确保AI系统的可信性。

Method: 使用基于MobileNetV2的CNN进行迁移学习，结合机械臂模拟器优化动作能耗。

Result: 训练准确率99.8%，验证准确率80.5%，机械臂动作能耗通过欧氏距离优化。

Conclusion: 该方法为智能垃圾管理提供了高效、可信的解决方案，具备可扩展性。

Abstract: This paper presents a novel methodology that integrates trustworthy
artificial intelligence (AI) with an energy-efficient robotic arm for
intelligent waste classification and sorting. By utilizing a convolutional
neural network (CNN) enhanced through transfer learning with MobileNetV2, the
system accurately classifies waste into six categories: plastic, glass, metal,
paper, cardboard, and trash. The model achieved a high training accuracy of
99.8% and a validation accuracy of 80.5%, demonstrating strong learning and
generalization. A robotic arm simulator is implemented to perform virtual
sorting, calculating the energy cost for each action using Euclidean distance
to ensure optimal and efficient movement. The framework incorporates key
elements of trustworthy AI, such as transparency, robustness, fairness, and
safety, making it a reliable and scalable solution for smart waste management
systems in urban settings.

</details>


### [444] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON是一种新型的VLA模型，通过注入3D空间标记来增强空间推理能力，无需依赖专用传感器或牺牲视觉-语言对齐。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型基于2D编码器，存在空间推理缺陷，限制了泛化能力和适应性。

Method: FALCON利用空间基础模型从RGB图像中提取几何先验，并通过空间增强动作头处理空间标记，保持语言推理能力。

Result: 在三个仿真基准和十一个现实任务中，FALCON表现最优，超越基线模型，并在复杂场景中保持鲁棒性。

Conclusion: FALCON通过创新的3D空间标记注入方法，显著提升了VLA模型的空间表示能力和跨模态适应性。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [445] [A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions](https://arxiv.org/abs/2510.17448)
*Mirko Mizzoni,Pieter van Goor,Barbara Bazzana,Antonio Franchi*

Main category: cs.RO

TL;DR: 提出了一种通过反馈线性化控制非线性系统的输出切换框架，证明了在适当条件下切换时系统状态的均匀有界性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性系统在输出切换时的稳定性和无缝跟踪问题。

Method: 引入“meld”概念定义可反馈线性化的输出子集，并给出切换条件和稳定性证明。

Result: 证明了切换时系统状态有界，误差动态指数稳定，且共同输出无缝跟踪。

Conclusion: 该框架适用于任何可反馈线性化的非线性系统，如机器人、车辆等，并通过仿真验证。

Abstract: This letter presents a systematic framework for switching between different
sets of outputs for the control of nonlinear systems via feedback
linearization. We introduce the concept of a meld to formally define a valid,
feedback-linearizable subset of outputs that can be selected from a larger deck
of possible outputs. The main contribution is a formal proof establishing that
under suitable dwell-time and compatibility conditions, it is possible to
switch between different melds while guaranteeing the uniform boundedness of
the system state. We further show that the error dynamics of the active outputs
remain exponentially stable within each switching interval and that outputs
common to consecutive melds are tracked seamlessly through transitions. The
proposed theory is valid for any feedback linearizable nonlinear system, such
as, e.g., robots, aerial and terrestrial vehicles, etc.. We demonstrate it on a
simple numerical simulation of a robotic manipulator.

</details>


### [446] [HumanMPC - Safe and Efficient MAV Navigation among Humans](https://arxiv.org/abs/2510.17525)
*Simon Schaefer,Helen Oleynikova,Sandra Hirche,Stefan Leutenegger*

Main category: cs.RO

TL;DR: HumanMPC是一个结合理论安全保证和数据驱动模型的3D无人机导航框架，用于在人群中安全高效地导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注简化的2D人群导航，未能全面考虑人体动态复杂性，因此需要更安全高效的3D导航方案。

Method: 采用模型预测控制（MPC）框架，结合基于可达性的安全约束和数据驱动的人类运动预测模型。

Result: 在仿真和实际实验中验证了其有效性，任务包括目标导航和视觉伺服跟踪，表现优于基线方法。

Conclusion: HumanMPC在保证安全的同时避免了过度保守，适用于多种平台，展示了高效和可靠性。

Abstract: Safe and efficient robotic navigation among humans is essential for
integrating robots into everyday environments. Most existing approaches focus
on simplified 2D crowd navigation and fail to account for the full complexity
of human body dynamics beyond root motion. We present HumanMPC, a Model
Predictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation
among humans that combines theoretical safety guarantees with data-driven
models for realistic human motion forecasting. Our approach introduces a novel
twist to reachability-based safety formulation that constrains only the initial
control input for safety while modeling its effects over the entire planning
horizon, enabling safe yet efficient navigation. We validate HumanMPC in both
simulated experiments using real human trajectories and in the real-world,
demonstrating its effectiveness across tasks ranging from goal-directed
navigation to visual servoing for human tracking. While we apply our method to
MAVs in this work, it is generic and can be adapted by other platforms. Our
results show that the method ensures safety without excessive conservatism and
outperforms baseline approaches in both efficiency and reliability.

</details>


### [447] [Distributed Spatial-Temporal Trajectory Optimization for Unmanned-Aerial-Vehicle Swarm](https://arxiv.org/abs/2510.17541)
*Xiaobo Zheng,Pan Tang,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 提出了一种基于ADMM和DDP的空间-时间轨迹优化框架D-PDDP，用于大规模无人机群的分布式轨迹优化，并通过自适应调参减少迭代次数。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预先设定无人机最终时间且迭代耗时，难以应用于大规模无人机群。

Method: 采用双层架构，结合PDDP进行局部轨迹优化，ADMM实现时空参数共识，并提出自适应调参准则。

Result: 仿真验证了算法的有效性。

Conclusion: D-PDDP框架解决了大规模无人机群的轨迹优化问题，具有高效性和分布式特性。

Abstract: Swarm trajectory optimization problems are a well-recognized class of
multi-agent optimal control problems with strong nonlinearity. However, the
heuristic nature of needing to set the final time for agents beforehand and the
time-consuming limitation of the significant number of iterations prohibit the
application of existing methods to large-scale swarm of Unmanned Aerial
Vehicles (UAVs) in practice. In this paper, we propose a spatial-temporal
trajectory optimization framework that accomplishes multi-UAV consensus based
on the Alternating Direction Multiplier Method (ADMM) and uses Differential
Dynamic Programming (DDP) for fast local planning of individual UAVs. The
introduced framework is a two-level architecture that employs Parameterized DDP
(PDDP) as the trajectory optimizer for each UAV, and ADMM to satisfy the local
constraints and accomplish the spatial-temporal parameter consensus among all
UAVs. This results in a fully distributed algorithm called Distributed
Parameterized DDP (D-PDDP). In addition, an adaptive tuning criterion based on
the spectral gradient method for the penalty parameter is proposed to reduce
the number of algorithmic iterations. Several simulation examples are presented
to verify the effectiveness of the proposed algorithm.

</details>


### [448] [Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries](https://arxiv.org/abs/2510.17576)
*Cansu Erdogan,Cesar Alan Contreras,Alireza Rastegarpanah,Manolis Chiou,Rustam Stolkin*

Main category: cs.RO

TL;DR: 论文提出了一种意图驱动的规划管道，用于多机器人协作完成复杂操作任务，结合感知、大语言模型和验证机制，实现了高效且安全的任务执行。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在非结构化场景中协作完成复杂操作任务时，如何根据人类简单指令生成可靠且安全的动作序列的问题。

Method: 提出了一种意图驱动的规划管道，包括感知到文本的场景编码、LLM生成候选动作序列、LLM验证器确保格式和约束、确定性一致性过滤。

Result: 在200个真实场景和600个操作提示中，验证了方法的有效性，结果表明该方法能可靠地将操作意图映射为安全且可执行的多机器人计划。

Conclusion: 该方法在保持低用户努力的同时，能够可靠地生成安全且可执行的多机器人协作计划。

Abstract: This paper addresses the problem of planning complex manipulation tasks, in
which multiple robots with different end-effectors and capabilities, informed
by computer vision, must plan and execute concatenated sequences of actions on
a variety of objects that can appear in arbitrary positions and configurations
in unstructured scenes. We propose an intent-driven planning pipeline which can
robustly construct such action sequences with varying degrees of supervisory
input from a human using simple language instructions. The pipeline integrates:
(i) perception-to-text scene encoding, (ii) an ensemble of large language
models (LLMs) that generate candidate removal sequences based on the operator's
intent, (iii) an LLM-based verifier that enforces formatting and precedence
constraints, and (iv) a deterministic consistency filter that rejects
hallucinated objects. The pipeline is evaluated on an example task in which two
robot arms work collaboratively to dismantle an Electric Vehicle battery for
recycling applications. A variety of components must be grasped and removed in
specific sequences, determined by human instructions and/or by task-order
feasibility decisions made by the autonomous system. On 200 real scenes with
600 operator prompts across five component classes, we used metrics of
full-sequence correctness and next-task correctness to evaluate and compare
five LLM-based planners (including ablation analyses of pipeline components).
We also evaluated the LLM-based human interface in terms of time to execution
and NASA TLX with human participant experiments. Results indicate that our
ensemble-with-verification approach reliably maps operator intent to safe,
executable multi-robot plans while maintaining low user effort.

</details>


### [449] [Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm](https://arxiv.org/abs/2510.17604)
*Hao Qiao,Yan Wang,Shuo Yang,Xiaoyao Yu,Jian kuang,Xiaoji Niu*

Main category: cs.RO

TL;DR: 论文提出了一种改进的混合专家模型（MoE），用于自行车定位，降低了计算成本，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: 随着共享单车的快速发展和骑行应用的多样化，精确的自行车定位变得至关重要。传统方法存在多路径效应或依赖精确建模，而现有方法计算成本高。

Method: 扩展了TLIO方法，结合原始IMU数据和神经网络预测位移，并引入改进的MoE模型以减少训练和推理成本。

Result: 相比LLIO框架，新方法在精度相当的情况下，参数减少64.7%，计算成本降低81.8%。

Conclusion: 改进的MoE模型在自行车定位中实现了高效且精确的性能，适合移动设备部署。

Abstract: With the rapid growth of bike sharing and the increasing diversity of cycling
applications, accurate bicycle localization has become essential. traditional
GNSS-based methods suffer from multipath effects, while existing inertial
navigation approaches rely on precise modeling and show limited robustness.
Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining
raw IMU data with predicted displacements by neural networks, but its high
computational cost restricts deployment on mobile devices. To overcome this, we
extend TLIO to bicycle localization and introduce an improved Mixture-of
Experts (MoE) model that reduces both training and inference costs. Experiments
show that, compared to the state-of-the-art LLIO framework, our method achieves
comparable accuracy while reducing parameters by 64.7% and computational cost
by 81.8%.

</details>


### [450] [RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation](https://arxiv.org/abs/2510.17640)
*Yuquan Xue,Guanxing Lu,Zhenyu Wu,Chuanrui Zhang,Bofang Jia,Zhengyi Gu,Yansong Tang,Ziwei Wang*

Main category: cs.RO

TL;DR: RESample框架通过探索性采样自动增强OOD数据，提升VLA模型在分布偏移下的鲁棒性和恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习数据集仅包含成功轨迹，缺乏失败或恢复数据，导致VLA模型在OOD状态下表现不佳。

Method: 利用离线强化学习获取动作价值网络，通过rollout采样潜在OOD状态，设计探索性采样机制增强训练数据。

Result: 在LIBERO基准和真实机器人任务中，RESample显著提升了VLA模型的稳定性和泛化能力。

Conclusion: RESample通过数据增强有效解决了VLA模型在OOD状态下的性能问题。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
on complex robotic manipulation tasks through imitation learning. However,
existing imitation learning datasets contain only successful trajectories and
lack failure or recovery data, especially for out-of-distribution (OOD) states
where the robot deviates from the main policy due to minor perturbations or
errors, leading VLA models to struggle with states deviating from the training
distribution. To this end, we propose an automated OOD data augmentation
framework named RESample through exploratory sampling. Specifically, we first
leverage offline reinforcement learning to obtain an action-value network that
accurately identifies sub-optimal actions under the current manipulation
policy. We further sample potential OOD states from trajectories via rollout,
and design an exploratory sampling mechanism that adaptively incorporates these
action proxies into the training dataset to ensure efficiency. Subsequently,
our framework explicitly encourages the VLAs to recover from OOD states and
enhances their robustness against distributional shifts. We conduct extensive
experiments on the LIBERO benchmark as well as real-world robotic manipulation
tasks, demonstrating that RESample consistently improves the stability and
generalization ability of VLA models.

</details>


### [451] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot系统利用立体相机、数字转盘和机器人臂构建植物的详细数字孪生，并通过算法处理叶片遮挡问题，实现高精度图像采集。


<details>
  <summary>Details</summary>
Motivation: 解决固定相机因叶片遮挡无法捕捉植物细节的问题。

Method: 使用立体相机、数字转盘、机器人臂和3D分割高斯模型，结合叶片操控算法。

Result: 叶片分割准确率90.8%，检测准确率86.2%，叶片操控准确率77.9%，图像采集准确率77.3%。

Conclusion: Botany-Bot能有效构建植物数字孪生并处理遮挡问题，代码和数据已开源。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


### [452] [SoftMimic: Learning Compliant Whole-body Control from Examples](https://arxiv.org/abs/2510.17792)
*Gabriel B. Margolis,Michelle Wang,Nolan Fey,Pulkit Agrawal*

Main category: cs.RO

TL;DR: SoftMimic是一个框架，通过模仿人类动作学习人形机器人的柔顺全身控制策略，使其能够柔顺响应外力并保持平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法倾向于僵硬控制，导致机器人在意外接触时行为脆弱且不安全。SoftMimic旨在解决这一问题。

Method: 利用逆运动学求解器生成柔顺动作数据集，训练强化学习策略以匹配柔顺响应而非刚性跟踪参考动作。

Result: 实验验证了SoftMimic能够吸收干扰并从单一动作片段泛化到多种任务。

Conclusion: SoftMimic实现了安全有效的环境交互，适用于人形机器人的柔顺控制。

Abstract: We introduce SoftMimic, a framework for learning compliant whole-body control
policies for humanoid robots from example motions. Imitating human motions with
reinforcement learning allows humanoids to quickly learn new skills, but
existing methods incentivize stiff control that aggressively corrects
deviations from a reference motion, leading to brittle and unsafe behavior when
the robot encounters unexpected contacts. In contrast, SoftMimic enables robots
to respond compliantly to external forces while maintaining balance and
posture. Our approach leverages an inverse kinematics solver to generate an
augmented dataset of feasible compliant motions, which we use to train a
reinforcement learning policy. By rewarding the policy for matching compliant
responses rather than rigidly tracking the reference motion, SoftMimic learns
to absorb disturbances and generalize to varied tasks from a single motion
clip. We validate our method through simulations and real-world experiments,
demonstrating safe and effective interaction with the environment.

</details>


### [453] [Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain](https://arxiv.org/abs/2510.17801)
*Yulin Luo,Chun-Kai Fan,Menghang Dong,Jiayu Shi,Mengdi Zhao,Bo-Wen Zhang,Cheng Chi,Jiaming Liu,Gaole Dai,Rongyu Zhang,Ruichuan An,Kun Wu,Zhengping Che,Shaoxuan Xie,Guocai Yao,Zhongxia Zhao,Pengwei Wang,Guang Liu,Zhongyuan Wang,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboBench是一个系统评估多模态大语言模型（MLLMs）作为机器人认知核心的基准测试，涵盖五个维度和14种能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估机器人高层面推理能力时存在不足，无法全面反映认知能力，因此需要更全面的评估工具。

Method: 通过定义五个维度（指令理解、感知推理、泛化规划、功能预测和失败分析），构建包含14种能力、25个任务和6092个QA对的基准测试，并利用真实机器人数据进行验证。

Result: 实验发现当前MLLMs在隐式指令理解、时空推理、跨场景规划等方面存在显著局限性。

Conclusion: RoboBench为量化高层面认知能力提供了全面框架，并指导下一代MLLMs的开发。

Abstract: Building robots that can perceive, reason, and act in dynamic, unstructured
environments remains a core challenge. Recent embodied systems often adopt a
dual-system paradigm, where System 2 handles high-level reasoning while System
1 executes low-level control. In this work, we refer to System 2 as the
embodied brain, emphasizing its role as the cognitive core for reasoning and
decision-making in manipulation tasks. Given this role, systematic evaluation
of the embodied brain is essential. Yet existing benchmarks emphasize execution
success, or when targeting high-level reasoning, suffer from incomplete
dimensions and limited task realism, offering only a partial picture of
cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark
that systematically evaluates multimodal large language models (MLLMs) as
embodied brains. Motivated by the critical roles across the full manipulation
pipeline, RoboBench defines five dimensions-instruction comprehension,
perception reasoning, generalized planning, affordance prediction, and failure
analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure
realism, we curate datasets across diverse embodiments, attribute-rich objects,
and multi-view scenes, drawing from large-scale real robotic data. For
planning, RoboBench introduces an evaluation framework,
MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether
predicted plans can achieve critical object-state changes. Experiments on 14
MLLMs reveal fundamental limitations: difficulties with implicit instruction
comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained
affordance understanding, and execution failure diagnosis. RoboBench provides a
comprehensive scaffold to quantify high-level cognition, and guide the
development of next-generation embodied MLLMs. The project page is in
https://robo-bench.github.io.

</details>
