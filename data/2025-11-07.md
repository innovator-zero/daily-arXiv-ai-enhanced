<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 63]
- [cs.LG](#cs.LG) [Total: 78]
- [cs.RO](#cs.RO) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: LoRA-Edge是一种参数高效的微调方法，通过低秩适应和张量训练辅助，显著减少训练参数数量，同时保持接近全微调的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备上全微调CNN时内存、计算和能源预算不足的问题。

Method: 使用TT-SVD分解预训练卷积层，选择性更新输出侧核心，并通过零初始化保持辅助路径初始不活跃。

Result: 在HAR数据集和CNN骨干上，LoRA-Edge仅更新最多1.49%的参数，准确率接近全微调（差距4.7%以内），收敛速度更快。

Conclusion: LoRA-Edge为边缘平台提供了一种结构对齐、参数高效的CNN微调方法。

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [2] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: SILVI是一个开源标注软件，用于在视频数据中标注行为和互动，填补了现有工具在定位个体和捕捉互动方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有工具无法同时支持行为标注和互动定位，限制了计算机视觉在动物行为分析中的应用。

Method: 开发了SILVI软件，整合行为标注和互动定位功能，生成结构化输出。

Result: SILVI能够直接标注视频中的行为和互动，适用于训练和验证计算机视觉模型。

Conclusion: SILVI填补了行为生态学与计算机视觉之间的工具缺口，适用于动物和人类互动的标注。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [3] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 论文研究了通过噪声注入技术提升深度学习模型在COVID-19胸部X光检测中的泛化能力，显著缩小了分布内和分布外数据的性能差距。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在图像识别中容易依赖源数据特有的伪影而非合理生物标志物，导致泛化能力差，尤其是在COVID-19检测中。

Method: 在训练过程中应用高斯、斑点、泊松和椒盐噪声注入技术。

Result: 噪声注入技术将分布内和分布外数据的性能差距从0.10-0.20显著降低到0.01-0.06。

Conclusion: 噪声注入技术能有效提升模型对分布偏移的鲁棒性，适用于COVID-19检测任务。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [4] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 论文探讨了模仿学习在X射线引导的脊柱手术中的应用，开发了高真实度的模拟环境，并训练了基于视觉信息的策略，取得了一定成功，但也指出了局限性。


<details>
  <summary>Details</summary>
Motivation: 研究模仿学习在复杂X射线引导手术（如脊柱器械植入）中的适用性，解决多视角X射线解析的难题。

Method: 开发了高真实度的模拟环境，收集正确轨迹和双平面X射线序列数据，训练模仿学习策略进行规划和开环控制。

Result: 策略在68.5%的案例中首次尝试成功，能适应复杂解剖结构，并在真实X射线上生成合理轨迹。

Conclusion: 初步结果有前景，但需改进入口点精度和闭环控制反馈频率，未来可能为无CT的机器人脊柱导航提供基础。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [5] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 提出了一种基于轻量化YOLOv12和自对抗训练的实时垃圾检测框架，适用于沙漠环境，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统垃圾收集方法在偏远或恶劣环境中效率低且危险，现有研究多集中于城市和可回收垃圾，忽视了有机、危险垃圾及沙漠等未充分探索的地形。

Method: 采用轻量化YOLOv12结合自对抗训练和专用数据增强策略，使用DroneTrashNet数据集进行训练。

Result: 在精度、召回率和mAP上显著提升，同时实现低延迟和小模型体积，适合资源受限的无人机部署。

Conclusion: 结合数据与模型优化的方法在沙漠环境中实现了高效、鲁棒的实时垃圾检测。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [6] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 论文提出了一种基于类别的图像合成方法（Class-Based Image Composition），通过将同一类别的多张图像融合为复合图像（CoImg），提升训练样本的信息密度和模型对细微疾病模式的区分能力。


<details>
  <summary>Details</summary>
Motivation: 解决小样本、不平衡数据集和低质量输入图像导致深度学习模型预测错误率高的问题。

Method: 通过融合同一类别的多张图像生成复合图像（CoImg），构建平衡数据集Co-OCTDL，并使用VGG16模型进行对比实验。

Result: 在OCTDL数据集上，复合图像方法显著提升了模型性能，准确率达99.6%，F1-score为0.995，AUC为0.9996，错误预测率显著降低。

Conclusion: 该方法能有效提升模型在小样本或不平衡数据集上的预测质量。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [7] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出了一种无监督、无需专家标注的异常检测框架，通过逐步扩展正常样本集来提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 医疗影像中异常检测的挑战在于标注稀缺和专家监督成本高，需要一种无需标注的方法。

Method: 使用冻结的预训练视觉骨干网络，结合轻量级卷积适配器和不确定性门控样本准入机制。

Result: 在多个数据集上显著提升性能，如COVID-CXR的ROC-AUC从0.9489提升至0.9982。

Conclusion: 该框架在标签稀缺的医疗影像应用中高效且有效。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [8] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出两种互补方法（BDR和ATR）优化时序动作定位的边界检测和计算分配，显著提升精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前方法在边界检测时采用统一计算，忽略了不同边界难度的差异，导致效率低下。

Method: 1. BDR通过符号距离回归优化边界定位；2. ATR通过连续深度选择动态分配计算资源。

Result: BDR提升边界峰值43%，ATR在THUMOS14上以更低计算量实现更高精度（56.5% vs 53.6%）。

Conclusion: 方法在多个基准测试中验证有效，尤其对短动作效果显著，且通过知识蒸馏降低训练成本。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [9] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了一种统一优化几何和外观的方法，通过高斯引导的网格可微分渲染，同时优化网格几何和顶点颜色，提升3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将几何精度和真实感渲染分开优化，限制了后续编辑任务的效果。本文旨在统一几何和外观优化，提升重建质量。

Method: 采用高斯引导的网格可微分渲染框架，同时优化网格几何（顶点位置和面）和顶点颜色，利用输入图像的光度一致性和法线/深度图的几何正则化。

Result: 实现了高质量的3D重建，适用于后续编辑任务（如重新光照和形状变形）。

Conclusion: 统一优化几何和外观的方法显著提升了3D重建的质量和编辑灵活性。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [10] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 提出了一种基于线性分数变换（LFT）参数α的方法，用于解耦光场相机中的主镜头和微透镜阵列（MLA），并通过最小二乘法和非线性优化实现校准。


<details>
  <summary>Details</summary>
Motivation: 光场相机内部参数的精确校准是3D重建的关键但具有挑战性。

Method: 使用LFT参数α解耦主镜头和MLA，结合最小二乘法解析解和非线性优化，并引入特征检测方法。

Result: 在物理和模拟数据上的实验验证了方法的有效性，且模型加速了光场图像的模拟。

Conclusion: 该方法为数据驱动的深度学习方法提供了高效支持，代码已公开。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [11] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 论文提出了一种合成数据集Room Envelopes，用于改进场景结构元素（如墙壁、地板）的预测方法。


<details>
  <summary>Details</summary>
Motivation: 现有场景重建方法无法完整预测被遮挡的结构元素，而这些元素通常简单且重复，适合低成本方法。

Method: 通过合成数据集提供RGB图像和两种点图（可见表面和结构布局），用于监督单目几何估计器。

Result: 数据集支持直接监督，能够预测可见表面和结构布局，从而理解场景范围和物体形状。

Conclusion: 该方法有助于更全面地理解场景结构，填补了现有方法的不足。

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [12] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: 人类依赖3D视觉空间姿态信息进行社交互动判断，而大多数AI视觉模型缺乏此类信息。通过提取3D关节位置和社交姿态特征，研究发现这些特征显著提升了AI模型的性能。


<details>
  <summary>Details</summary>
Motivation: 理解人类如何从视觉输入中快速提取社交互动信息，以及为何当前AI视觉系统在此任务上表现不佳。

Method: 结合先进的姿态和深度估计算法，提取视频中人物的3D关节位置，并与AI视觉模型进行比较。

Result: 3D关节位置和社交姿态特征显著优于当前AI模型，且这些特征能提升AI模型的性能。

Conclusion: 人类社交场景理解依赖于3D姿态的显式表示，可通过简单的视觉空间原语支持。

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [13] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: R3DGS通过CaRF框架解决多视角一致性问题，引入GFCE和ITPVS方法，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D伪监督和视角特定特征学习，导致跨视角一致性不足。

Method: 提出CaRF框架，结合GFCE编码相机几何和ITPVS训练策略。

Result: 在三个基准测试中平均提升16.8%、4.3%和2.0%的mIoU。

Conclusion: CaRF提升了3D场景理解的可靠性和视角一致性，适用于AR/VR和自主感知。

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [14] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

TL;DR: PhysCorr是一个统一框架，用于建模、评估和优化视频生成中的物理一致性，通过PhysicsRM和PhyDPO提升物理真实感。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在物理合理性上存在不足，阻碍了其在机器人等领域的应用。

Method: 提出PhysicsRM（双维度奖励模型）和PhyDPO（直接偏好优化管道），结合对比反馈和物理感知重加权。

Result: PhysCorr在多个基准测试中显著提升了物理真实感，同时保持视觉保真度和语义对齐。

Conclusion: 该研究为物理基础和可信的视频生成迈出了关键一步。

Abstract: Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [15] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

TL;DR: GNN-MoE提出了一种基于图神经网络的混合专家框架，用于高效微调Vision Transformer，以提升领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 领域泛化（DG）需要ViT在未见过的领域上表现鲁棒，但标准微调方法成本高且可能损害泛化能力。

Method: 采用混合专家（MoE）框架和高效的Kronecker适配器，通过GNN路由器动态分配图像块给专家，利用块间关系提升适应性。

Result: GNN-MoE在DG基准测试中达到或接近最优性能，同时保持高参数效率。

Conclusion: 基于图的上下文路由方法为轻量且鲁棒的领域泛化提供了有效解决方案。

Abstract: Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [16] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

TL;DR: MedDChest是一个专为胸部影像优化的ViT模型，通过大规模领域内预训练和内容感知数据增强，显著优于ImageNet预训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决自然图像预训练模型在医学影像中的领域差距问题。

Method: 提出MedDChest模型，使用1.2百万张多模态胸部影像预训练，并开发了Guided Random Resized Crops数据增强策略。

Result: 在多种下游诊断任务中表现优于ImageNet预训练模型。

Conclusion: MedDChest为胸部诊断任务提供了更强大的特征提取器，模型权重将公开以促进研究。

Abstract: The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


### [17] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出了一种名为Faithful Contouring的稀疏体素化表示方法，支持2048+分辨率，无需将网格转换为场函数或提取等值面，实现了近乎无损的几何保真度。


<details>
  <summary>Details</summary>
Motivation: 现有基于等值面的表示方法依赖水密化或渲染优化，会损害几何保真度。

Method: 通过保留锐度和内部结构，提出稀疏体素化表示方法，并设计了双模式自编码器以实现可扩展的形状重建。

Result: 在表示和重建任务中均优于现有方法，距离误差达到10^-5级别，重建任务的Chamfer Distance减少93%，F-score提高35%。

Conclusion: Faithful Contouring作为一种3D学习任务的表示方法，具有卓越的保真度和灵活性。

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [18] [A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals](https://arxiv.org/abs/2511.04037)
*Arfina Rahman,Mahesh Banavar*

Main category: cs.CV

TL;DR: 提出了一种基于PPG信号的轻量级生物认证框架，结合CVT-ConvMixer-LSTM混合深度学习模型，在46名受试者上达到98%的认证准确率。


<details>
  <summary>Details</summary>
Motivation: PPG信号因其非侵入性、活体检测特性和低成本可穿戴设备的适用性，成为生物认证的研究热点，但其信号质量易受运动伪影和个体差异影响。

Method: 通过预处理（PCA去噪、带通滤波等）将PPG信号转换为时频标量图，并设计CVT-ConvMixer-LSTM模型融合时空特征。

Result: 在14Hz采样的CFIHSR数据集上实现98%的认证准确率，模型对噪声和个体差异具有鲁棒性。

Conclusion: 该系统高效、可扩展且具备活体检测能力，适用于移动和嵌入式生物安全应用。

Abstract: Photoplethysmography (PPG) signals, which measure changes in blood volume in
the skin using light, have recently gained attention in biometric
authentication because of their non-invasive acquisition, inherent liveness
detection, and suitability for low-cost wearable devices. However, PPG signal
quality is challenged by motion artifacts, illumination changes, and
inter-subject physiological variability, making robust feature extraction and
classification crucial. This study proposes a lightweight and cost-effective
biometric authentication framework based on PPG signals extracted from
low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings
from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The
raw PPG signals undergo a standard preprocessing pipeline involving baseline
drift removal, motion artifact suppression using Principal Component Analysis
(PCA), bandpass filtering, Fourier-based resampling, and amplitude
normalization. To generate robust representations, each one-dimensional PPG
segment is converted into a two-dimensional time-frequency scalogram via the
Continuous Wavelet Transform (CWT), effectively capturing transient
cardiovascular dynamics. We developed a hybrid deep learning model, termed
CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision
Transformer (CVT) and ConvMixer branches with temporal features from a Long
Short-Term Memory network (LSTM). The experimental results on 46 subjects
demonstrate an authentication accuracy of 98%, validating the robustness of the
model to noise and variability between subjects. Due to its efficiency,
scalability, and inherent liveness detection capability, the proposed system is
well-suited for real-world mobile and embedded biometric security applications.

</details>


### [19] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

TL;DR: Bratrix是一个端到端的多模态框架，通过语言锚定的视觉-大脑对齐，提升神经信号（如EEG、MEG、fMRI）与视觉语义的对齐精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接将神经活动与视觉嵌入对齐，但视觉表示难以捕捉潜在语义维度，限制了可解释性和鲁棒性。

Method: Bratrix将视觉刺激解耦为层次化的视觉和语言语义组件，并将其投影到共享潜在空间，结合不确定性感知模块和两阶段训练策略。

Result: 在EEG、MEG和fMRI基准测试中，Bratrix在检索、重建和描述任务上优于现有方法，EEG检索任务提升14.3%。

Conclusion: Bratrix通过多模态对齐和不确定性处理，显著提升了神经信号与视觉语义的关联性能。

Abstract: Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [20] [Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score](https://arxiv.org/abs/2511.04083)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 论文研究了无配对和自监督条件下的CT图像去噪，比较了CycleGAN和Noise2Score两种方法，发现CycleGAN在最终图像质量上表现最佳，而Noise2Score在无配对数据时表现稳健。


<details>
  <summary>Details</summary>
Motivation: 探索在无配对和自监督条件下，如何高效地去除CT图像中的噪声。

Method: 通过配置扫描选择最佳CycleGAN设置（lambda_cycle=30, lambda_iden=2, ngf=ndf=64），并延长训练时间；同时评估Noise2Score的去噪效果。

Result: CycleGAN将输入图像从34.66 dB/0.9234 SSIM提升至38.913 dB/0.971 SSIM，Noise2Score在噪声较大时表现优异。

Conclusion: CycleGAN在图像质量上表现最佳，Noise2Score是无配对数据时的强有力替代方案。

Abstract: We study CT image denoising in the unpaired and self-supervised regimes by
evaluating two strong, training-data-efficient paradigms: a CycleGAN-based
residual translator and a Noise2Score (N2S) score-matching denoiser. Under a
common evaluation protocol, a configuration sweep identifies a simple standard
U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =
64) as the most reliable setting; we then train it to convergence with a longer
schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234
SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an
unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly
behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,
highlighting its utility when clean pairs are unavailable. Overall, CycleGAN
offers the strongest final image quality, whereas Noise2Score provides a robust
pair-free alternative with competitive performance. Source code is available at
https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.

</details>


### [21] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

TL;DR: UKAST是一种结合Kolmogorov-Arnold Networks（KANs）和Swin Transformer的U-Net架构，用于高效且数据友好的医学图像分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割因复杂解剖结构和有限标注数据而具挑战性，CNN和Transformer各有局限。

Method: 集成基于有理函数的KANs到Swin Transformer编码器，提升表达能力和数据效率。

Result: 在四个2D和3D医学图像分割基准上实现SOTA，数据稀缺时表现尤佳。

Conclusion: KAN增强的Transformer有望推动数据高效的医学图像分割发展。

Abstract: Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [22] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

TL;DR: SpatialLock框架通过结合感知信号和定位信息，实现了对生成图像中物体空间布局的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用位置信息，导致对物体空间布局的理解不足。

Method: 提出SpatialLock框架，包含Position-Engaged Injection (PoI)和Position-Guided Learning (PoG)两个组件，分别通过注意力层和感知监督优化空间定位。

Result: 在多个数据集上实现了IOU分数超过0.9的精确物体定位，达到新SOTA。

Conclusion: SpatialLock显著提升了生成图像中物体的空间布局精度和视觉质量。

Abstract: Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [23] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

TL;DR: THG是一种无需训练的策略，通过多速率ODE系统加速扩散采样，同时保持高保真生成。


<details>
  <summary>Details</summary>
Motivation: 传统扩散采样方法在计算上效率低下，THG旨在通过多速率ODE系统优化计算冗余。

Method: 将CFG ODE重构为多速率ODE系统，分别用细粒度（tortoise）和粗粒度（hare）网格计算噪声估计和额外指导项。

Result: THG减少30%的函数评估次数，生成质量几乎无损（ΔImageReward ≤ 0.032），优于现有方法。

Conclusion: THG展示了多速率ODE在扩散求解器中的潜力，为实时高质量图像合成提供了新思路。

Abstract: In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [24] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的训练免费框架，通过文本提示和参考风格草图实现精确的风格控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对草图风格的精确控制机制。

Method: 采用线性平滑和风格-内容引导机制，减少内容泄漏并提升合成质量。

Result: 实验表明，该方法实现了高质量草图生成，风格对齐准确且灵活性高。

Conclusion: 提出的框架在风格控制方面表现出色，支持多风格生成。

Abstract: Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [25] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: 提出了一种自动化网球比赛分析的全流程框架，结合多种深度学习模型实时检测和跟踪球员与网球，并提供详细比赛分析。


<details>
  <summary>Details</summary>
Motivation: 为教练、转播方和球员提供实时、详细的比赛数据分析，以优化训练和比赛策略。

Method: 使用YOLOv8检测球员，定制YOLOv5跟踪网球，ResNet50检测球场关键点，整合为完整分析系统。

Result: 实验表明系统在不同球场条件和比赛场景下表现稳健，输出带注释视频和详细性能指标。

Conclusion: 该框架为网球比赛提供了高效、自动化的分析工具，具有实际应用价值。

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [26] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

TL;DR: DMSORT是一种高效的双分支海上多目标跟踪方法，通过并行跟踪器和动态相机运动估计提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 复杂的海上环境导致相机运动和视觉退化，对多目标跟踪提出挑战。

Method: 结合目标检测与ReID分支，以及动态相机运动估计分支，使用RCDN和Li-TAE提取特征，并通过聚类优化特征融合。

Result: 在新加坡海事数据集上表现最优，运行速度最快且身份一致性高。

Conclusion: DMSORT在复杂环境下实现了高效且鲁棒的多目标跟踪。

Abstract: Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [27] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

TL;DR: 提出了一种轻量级单目深度估计模型BoRe-Depth，在嵌入式系统上高效运行，显著提升深度图质量和边界清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计方法在嵌入式系统上性能不佳，边界模糊，需改进。

Method: 设计了增强特征自适应融合模块（EFAF）并引入语义知识，提升边界细节和物体识别能力。

Result: 模型仅8.7M参数，在NVIDIA Jetson Orin上以50.7 FPS运行，性能优于现有轻量级模型。

Conclusion: BoRe-Depth在嵌入式系统上高效且性能优越，为3D感知提供了实用解决方案。

Abstract: Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low-cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [28] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: 提出一种框架，使计算机代理能从在线视频中学习，通过检索、过滤和动态选择视频轨迹作为上下文指导，显著提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 计算机代理在执行需要领域特定知识的任务时表现不如人类，人类通过观看视频教程学习，而现有代理缺乏这种能力。

Method: 框架包括视频检索与过滤、转换为结构化轨迹、动态选择轨迹作为上下文指导，使用VLM推断UI动作并分段。

Result: 在两个基准测试中表现优于仅使用文本教程的代理，验证了轨迹分割、选择和视觉信息的重要性。

Conclusion: 在线视频可系统性地转化为可操作指导，提升计算机代理性能。

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [29] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 论文提出了OCR-Rotation-Bench（ORB）基准测试，用于评估OCR对图像旋转的鲁棒性，并开发了一种基于Phi-3.5-Vision模型的轻量级旋转分类方法，显著提升了OCR性能。


<details>
  <summary>Details</summary>
Motivation: 解决文档扫描或拍摄时的方向错误问题，提升OCR任务性能。

Method: 使用Phi-3.5-Vision模型的视觉编码器，结合动态图像裁剪，微调用于4类旋转分类任务。

Result: 在ORB-En和ORB-Indic数据集上分别达到96%和92%的准确率，显著提升OCR性能。

Conclusion: 提出的方法在旋转分类和OCR性能提升方面表现出色，适用于多语言场景。

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [30] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 研究了不同颜色转换技术对H&E染色图像与非线性多模态图像配准的影响，发现CycleGAN方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中，多模态图像配准对信息整合至关重要，但颜色差异会影响配准精度。

Method: 使用20对组织样本，应用多种颜色转换方法（CycleGAN、Macenko等），并通过VALIS配准方法评估性能。

Result: CycleGAN在两种场景下均表现最佳，其他方法误差较高。

Conclusion: 颜色转换可提升多模态图像配准精度，支持更可靠的病理分析。

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [31] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 研究了协方差描述符在医学图像分类中的有效性，结合预训练视觉编码器（GVEs）和SPDNet，结果显示其优于传统手工特征和现有方法。


<details>
  <summary>Details</summary>
Motivation: 协方差描述符在计算机视觉中表现优异，但在医学影像中尚未充分探索，本文旨在填补这一空白。

Method: 使用预训练的GVEs（DINOv2和MedSAM）提取特征构建协方差描述符，并与手工描述符对比，在MedMNSIT基准的11个数据集上评估。

Result: GVEs衍生的协方差描述符优于手工特征，尤其是结合DINOv2和SPDNet时性能最佳。

Conclusion: 协方差描述符与预训练视觉编码器的结合在医学图像分析中具有潜力。

Abstract: Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [32] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 论文提出了一种基于高阶统计量（偏度和峰度）的运动风格迁移方法AStF，通过SDM和HOS-Attn模块提升效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于均值和方差的风格迁移方法在运动数据上表现不足，需更全面的统计量捕捉动态模式。

Method: 提出AStF框架，包含SDM和HOS-Attn模块，结合MCR判别器训练。

Result: 实验表明AStF在运动风格迁移上优于现有方法。

Conclusion: 高阶统计量能更全面建模运动风格，AStF表现出色。

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [33] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出MedSapiens模型，通过调整人类中心基础模型Sapiens用于医学影像中的解剖标志点检测，在多数据集上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 探索人类中心基础模型在医学影像中的潜力，填补其在解剖标志点检测领域的空白。

Method: 通过多数据集预训练调整Sapiens模型，提出MedSapiens。

Result: MedSapiens在多个数据集上表现优异，平均检测成功率提升5.26%（通用模型）和21.81%（专用模型）。

Conclusion: 人类中心基础模型在医学影像中具有潜力，MedSapiens为解剖标志点检测提供了新思路。

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [34] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: Proto-LeakNet是一种基于信号泄漏的、可解释的图像来源追踪框架，通过潜在空间分析和原型学习实现高精度分类和开放集评估。


<details>
  <summary>Details</summary>
Motivation: 随着合成图像和深度伪造技术的进步，来源追踪和真实性验证成为计算机视觉系统的关键挑战。扩散模型在输出中无意中留下了统计痕迹（信号泄漏），这为来源追踪提供了可能。

Method: Proto-LeakNet在扩散模型的潜在空间中工作，通过部分前向扩散重新模拟来暴露生成器特定的线索。它结合了时间注意力编码器和特征加权原型头，实现透明分类。

Result: 在封闭数据上训练，Proto-LeakNet的Macro AUC达到98.13%，在潜在空间中保持鲁棒性，并能有效区分已知和未知生成器。

Conclusion: 研究表明，在潜在空间中建模信号泄漏偏差能够实现可靠且可解释的AI图像和深度伪造取证。

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [35] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

TL;DR: 提出DinoGRL框架，结合DINOv2的视觉先验和步态特征，提升跨模态行人重识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视步态特征，而步态具有模态不变性和时序动态性，对跨模态视频匹配至关重要。

Method: 提出SASGL模型和PBMGE模块，结合语义先验和多粒度交互，优化步态与外观特征。

Result: 在HITSZ-VCM和BUPT数据集上显著优于现有方法。

Conclusion: DinoGRL通过融合步态与外观特征，实现了更鲁棒的跨模态行人重识别。

Abstract: Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [36] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

TL;DR: FastGS提出了一种基于多视角一致性的高斯分布优化框架，显著提升了3D高斯溅射的训练速度，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射加速方法未能有效调控高斯分布数量，导致计算冗余。

Method: 设计了基于多视角一致性的高斯分布密集化和剪枝策略，无需预算机制。

Result: 在多个数据集上实现了3.32×至15.45×的训练加速，渲染质量与现有方法相当。

Conclusion: FastGS具有广泛适用性，适用于动态场景重建等多种任务。

Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [37] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

TL;DR: 本文提出了一种针对除草剂试验的领域专用视觉基础模型，显著提升了物种识别和损伤分类的准确性，尤其在未见条件下表现更优。


<details>
  <summary>Details</summary>
Motivation: 除草剂试验需要精确识别植物物种和评估损伤，但通用视觉模型在农业领域的细粒度任务中表现有限。

Method: 采用自监督学习方法，在大规模农业数据集上训练领域专用模型。

Result: 模型在物种识别（F1从0.91提升至0.94）和损伤分类（从0.26提升至0.33）上显著优于通用模型，且在未见条件下表现更佳。

Conclusion: 领域专用基础模型具有强大的泛化能力，可显著减少人工标注需求，为除草剂试验分析提供可扩展的自动化解决方案。

Abstract: Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [38] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: 研究通过结合合成与真实卫星图像训练YOLOv10模型，提升海洋基础设施检测性能，并验证其地理可迁移性。


<details>
  <summary>Details</summary>
Motivation: 海洋基础设施扩张需要有效监测系统，但现有模型在样本稀缺时表现不佳，尤其是对少数类。

Method: 使用合成与真实Sentinel-1卫星图像训练YOLOv10模型，并在未训练区域测试地理迁移能力。

Result: 模型在未训练区域检测到3,529个平台，F1分数从0.85提升至0.90。

Conclusion: 合成数据能平衡数据集并提升模型性能，为全球海洋基础设施监测提供潜力。

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [39] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

TL;DR: RISE-T2V通过集成提示重述和语义特征提取，提升文本到视频生成模型的能力，使其能更好地理解用户意图并生成高质量视频。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频（T2V）扩散模型依赖预训练文本编码器，但对简洁提示的语义理解不足，且无法在线重述提示以匹配用户意图，限制了模型的扩展性和可用性。

Method: 提出RISE-T2V框架，集成提示重述和语义特征提取为一个步骤，并引入Rephrasing Adapter模块，利用LLM的隐藏状态作为视频生成条件。

Result: 实验表明，RISE-T2V适用于多种视频扩散模型架构，显著提升了生成视频的质量和用户意图对齐能力。

Conclusion: RISE-T2V是一个通用框架，通过结合LLM的强大能力，显著提升了T2V任务的性能。

Abstract: Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [40] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 提出了一种基于体素稀疏化和子流形稀疏卷积网络的两阶段方法，用于高分辨率3D医学图像分割，显著减少计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统卷积神经网络在3D医学图像分割中因计算资源需求高而需降采样或分块处理的问题。

Method: 采用两阶段方法：体素稀疏化和子流形稀疏卷积网络，实现高分辨率输入和原生3D模型架构。

Result: 在KiTS23挑战中，Dice相似系数达95.8%（肾脏+肿块）、85.7%（肿瘤+囊肿）、80.3%（肿瘤），计算资源需求显著降低。

Conclusion: 该方法在保持高精度的同时，显著降低了计算资源需求，为临床3D医学图像分割提供了实用解决方案。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [41] [Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset](https://arxiv.org/abs/2511.04344)
*Muhammad Annas Shaikh,Hamza Zaman,Arbaz Asif*

Main category: cs.CV

TL;DR: 论文评估了九种卷积神经网络架构在VOC 2008数据集上对马和摩托车的二分类任务中的表现，重点解决了类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较不同现代架构在二分类任务中的性能，并探讨数据增强对类别不平衡问题的缓解效果。

Method: 通过实施少数类增强技术，评估了包括ResNet-50、ConvNeXt-Tiny等架构的性能。

Result: ConvNeXt-Tiny表现最佳，马检测AP达95.53%，摩托车检测AP达89.12%。数据增强显著提升了少数类检测性能。

Conclusion: 研究为不平衡二分类任务的架构选择提供了参考，并量化了数据增强策略在缓解类别不平衡中的作用。

Abstract: This paper presents a comprehensive evaluation of nine convolutional neural
network architectures for binary classification of horses and motorcycles in
the VOC 2008 dataset. We address the significant class imbalance problem by
implementing minority-class augmentation techniques. Our experiments compare
modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and
Vision Transformer across multiple performance metrics. Results demonstrate
substantial performance variations, with ConvNeXt-Tiny achieving the highest
Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle
detection. We observe that data augmentation significantly improves minority
class detection, particularly benefiting deeper architectures. This study
provides insights into architecture selection for imbalanced binary
classification tasks and quantifies the impact of data augmentation strategies
in mitigating class imbalance issues in object detection.

</details>


### [42] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

TL;DR: 研究探讨了传感器遮挡对BEVFusion架构在3D物体检测中的影响，发现相机和LiDAR在不同遮挡条件下的性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究传感器遮挡对3D物体检测的影响，以提升自动驾驶车辆在复杂环境中的安全性。

Method: 使用BEVFusion架构和nuScenes数据集，分析相机和LiDAR在遮挡条件下的性能变化。

Result: 相机遮挡导致mAP下降41.3%，LiDAR在重度遮挡下下降47.3%；融合模式下，LiDAR遮挡影响更大。

Conclusion: 未来需研究遮挡感知评估方法和改进传感器融合技术，以应对部分传感器失效或环境干扰。

Abstract: Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [43] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

TL;DR: 教程提供逐步指南，利用开源深度学习模型从成像数据中提取空间信息，并与其他数据源整合。


<details>
  <summary>Details</summary>
Motivation: 传统化学计量方法在提取和分析空间信息方面存在挑战，深度学习虽强大但缺乏实施指导。

Method: 使用现有开源模型提取深度特征，提供MATLAB代码演示。

Result: 教程展示了多种成像模态数据的处理方法，读者可自行运行代码。

Conclusion: 该教程填补了深度学习在分析化学中应用的指导空白，促进技术落地。

Abstract: Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [44] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: 提出一个多任务框架，用于医学视觉问答（VQA）、解释生成和视觉定位，通过LoRA调优的Florence-2模型实现。


<details>
  <summary>Details</summary>
Motivation: 解决医学VQA任务中单一任务模型的局限性，提升答案准确性和可解释性。

Method: 整合三个数据集（Kvasir-VQA-x1、合成解释数据集、文本-区域对），通过多任务学习联合训练模型。

Result: 在多任务学习中显著优于单任务基线，提高了答案准确性和视觉定位能力。

Conclusion: 基于多任务学习的医学VQA框架有效提升了模型的性能和可解释性。

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [45] [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394)
*Ke Du,Yimin Peng,Chao Gao,Fan Zhou,Siqiao Xue*

Main category: cs.CV

TL;DR: DORAEMON是一个开源的PyTorch库，统一了不同尺度的视觉对象建模和表示学习，提供分类、检索和度量学习的YAML驱动工作流，支持1000多个预训练模型，并实现高效的研究到部署转换。


<details>
  <summary>Details</summary>
Motivation: 为视觉识别和表示学习提供一个统一的平台，整合数据集、模型和训练技术，加速研究向实际应用的转化。

Method: 通过YAML驱动的单一工作流覆盖多种任务，提供模块化损失、增强和分布式训练工具，支持ONNX和HuggingFace导出。

Result: 在ImageNet-1K、MS-Celeb-1M和Stanford在线产品等数据集上达到或超过参考结果。

Conclusion: DORAEMON为视觉研究提供了一个可扩展的基础平台，支持快速实验和高效部署。

Abstract: DORAEMON is an open-source PyTorch library that unifies visual object
modeling and representation learning across diverse scales. A single
YAML-driven workflow covers classification, retrieval and metric learning; more
than 1000 pretrained backbones are exposed through a timm-compatible interface,
together with modular losses, augmentations and distributed-training utilities.
Reproducible recipes match or exceed reference results on ImageNet-1K,
MS-Celeb-1M and Stanford online products, while one-command export to ONNX or
HuggingFace bridges research and deployment. By consolidating datasets, models,
and training techniques into one platform, DORAEMON offers a scalable
foundation for rapid experimentation in visual recognition and representation
learning, enabling efficient transfer of research advances to real-world
applications. The repository is available at https://github.com/wuji3/DORAEMON.

</details>


### [46] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

TL;DR: HideAndSeg是一种基于AI的工具，用于在缺乏大规模标注数据的情况下分割章鱼视频，通过结合SAM2和YOLOv11实现自动化，并引入无监督指标评估分割质量。


<details>
  <summary>Details</summary>
Motivation: 由于章鱼的伪装能力、快速变化的皮肤纹理和颜色、非刚性身体变形以及频繁遮挡，分析其自然栖息地具有挑战性。现有方法缺乏大规模标注数据集。

Method: HideAndSeg结合SAM2和YOLOv11，用户提供初始点坐标生成分割掩码，随后训练YOLO模型实现全自动化。引入无监督指标$DICE_t$和$NC_t$评估质量。

Result: HideAndSeg在减少分割噪声和重新识别完全遮挡的章鱼方面表现优于手动提示模型。

Conclusion: 该工具为野生头足类动物行为研究提供了高效方法，减少了对人工分析的依赖。

Abstract: Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [47] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 本文提出了一种针对凸多边形拼图的贪婪求解器，扩展了传统方形拼图求解器的应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有拼图求解器主要针对方形拼图，限制了实际应用。本文旨在解决凸多边形拼图这一更广泛的拼图类型。

Method: 结合几何和图像兼容性，提出了一种贪婪求解器，并建立了首个凸多边形拼图基准数据集。

Result: 展示了求解器的性能指标，并提供了凸多边形拼图的基准数据集。

Conclusion: 本文扩展了拼图求解器的应用范围，为凸多边形拼图提供了有效的解决方案和基准数据。

Abstract: Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [48] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

TL;DR: V-Thinker是一个通用的多模态推理助手，通过端到端强化学习实现交互式视觉中心思维，显著提升了大型多模态模型的图像交互和长程推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在图像交互和长程推理能力上存在局限，视觉工具空间有限且任务特定设计限制了进展。

Method: V-Thinker包含数据进化飞轮和视觉渐进训练课程，通过强化学习框架实现感知对齐和交互推理。

Result: V-Thinker在通用和交互式推理场景中均优于基线模型，验证了其有效性。

Conclusion: V-Thinker为图像交互推理应用提供了有价值的见解，推动了该领域的进展。

Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [49] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

TL;DR: 提出了一种三轴分析框架（传感器、标签、域）来改进地理空间基础模型（GeoFMs）在滑坡测绘中的表现，优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在不同传感器、区域或训练数据有限的情况下表现不佳，需要更稳健的解决方案。

Method: 基于全局预训练、自监督和可适应微调的GeoFM框架，实验对比了多种模型。

Result: GeoFM在光谱变化、标签稀缺和跨数据集泛化方面表现优异，优于任务特定CNN和ViT模型。

Conclusion: GeoFMs为滑坡风险减少和环境监测提供了更稳健和可扩展的方法，但仍面临计算成本和数据可用性挑战。

Abstract: Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [50] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 论文提出了一种新的评估框架，包含8个指标，用于评估视频生成的质量、自然性和同步性，旨在解决当前评估指标不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术发展迅速，但评估指标滞后，尤其是对头部动作生成（talking head generation）的评估有限，主要依赖通用视频质量、唇同步和用户研究。

Method: 提出一个包含8个指标的评估框架，重点关注效率和对人类偏好的对齐，分析头部、嘴巴、眉毛的细粒度动态以及面部质量。

Result: 实验表明，许多算法在唇同步方面表现优异，但在生成表情和无伪影细节方面存在挑战。

Conclusion: 提出的基准框架旨在评估生成方法的改进，并将公开代码、数据集和排行榜，以反映领域进展。

Abstract: Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [51] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: STC-Net是一种基于单时间戳的框架，用于通过Parkland分级量表（PGS）评估腹腔镜胆囊切除术（LC）的复杂性，适用于完整视频分析。


<details>
  <summary>Details</summary>
Motivation: 准确评估手术复杂性对LC至关重要，但现有方法依赖静态图像或手动剪辑视频，无法满足实际需求。

Method: STC-Net通过定位、窗口提议和分级模块，结合硬软定位目标和背景感知分级监督，实现全视频分析。

Result: 在1,859个LC视频的私有数据集上，STC-Net准确率为62.11%，F1分数为61.42%，优于非定位基线10%以上。

Conclusion: STC-Net为全视频自动PGS评估提供了一种可扩展且有效的方法，适用于术后分析和手术培训。

Abstract: Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [52] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 论文提出“Thinking with Video”新范式，利用视频生成模型（如Sora-2）统一视觉与文本推理，并通过VideoThinkBench验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有“Thinking with Text”和“Thinking with Images”范式存在局限性：图像无法捕捉动态过程，且文本与视觉分离阻碍多模态统一理解。

Method: 引入“Thinking with Video”范式，结合视频生成模型Sora-2，开发VideoThinkBench评估其在视觉中心任务和文本中心任务的表现。

Result: Sora-2在视觉任务中与SOTA VLMs相当，部分任务（如Eyeballing Games）表现更优；在文本任务中，MATH准确率92%，MMMU达75.53%。自一致性和上下文学习可进一步提升性能。

Conclusion: 视频生成模型有望成为统一多模态理解与生成的模型，“Thinking with Video”是统一多模态推理的新范式。

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


### [53] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: UniSplat是一种用于自动驾驶的动态场景重建框架，通过统一的潜在时空融合实现高效重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏、非重叠相机视图和复杂场景动态性方面表现不佳，UniSplat旨在解决这些问题。

Method: 构建3D潜在支架，利用预训练基础模型捕捉几何和语义信息，并通过双分支解码器生成动态感知的高斯分布。

Result: 在真实数据集上，UniSplat在新视角合成和渲染质量上达到最优性能。

Conclusion: UniSplat提供了一种高效且鲁棒的动态场景重建方法，适用于自动驾驶等应用。

Abstract: Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [54] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: PixCLIP通过结合视觉提示和长文本处理，提升了CLIP在细粒度图像-文本对齐上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过增加视觉信息处理的粒度来提升CLIP的性能，但CLIP的文本编码器限制了其对长文本的处理能力。

Method: 提出PixCLIP框架，结合视觉提示和长文本处理，并构建LongGRIT数据集。

Result: PixCLIP在像素级交互和长文本处理上取得突破，性能达到最优。

Conclusion: PixCLIP通过协同提升视觉和文本处理粒度，显著提升了细粒度对齐能力。

Abstract: While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [55] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

TL;DR: 提出了一种自动化评估虚拟IHC染色图像质量的框架，通过颜色解卷积生成IHC阳性像素掩膜，计算染色准确性指标，发现传统图像保真度指标与染色准确性相关性差。


<details>
  <summary>Details</summary>
Motivation: 当前评估虚拟IHC染色图像质量的方法主要基于图像保真度，而非染色准确性，缺乏可靠评估手段。

Method: 使用颜色解卷积生成IHC阳性像素掩膜，计算Dice、IoU和Hausdorff距离等染色准确性指标。

Result: 传统图像保真度指标（如FID、PSNR、SSIM）与染色准确性相关性差；配对模型（如PyramidPix2Pix、AdaptiveNCE）表现最佳。

Conclusion: 该框架为虚拟IHC模型质量评估提供了可重复的方法，有助于加速其在病理学中的常规应用。

Abstract: Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [56] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

TL;DR: 提出了一种无需参考视频和人工标注的流式视频质量评估模型，通过时间感知卷积架构预测质量指标，优于传统图像基线。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估方法依赖参考视频或人工标注，且多数忽略时间上下文，难以适应实际需求。

Method: 利用DAVIS数据集合成退化视频，训练时间感知卷积网络直接预测质量指标（LPIPS、PSNR、SSIM）。

Result: 模型在多样化退化条件下优于图像基线，与全参考指标相关性高于BRISQUE。

Conclusion: 时间建模对视频质量评估至关重要，该模型为实际视觉系统提供了可扩展的解决方案。

Abstract: Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [57] [Polarization-resolved imaging improves eye tracking](https://arxiv.org/abs/2511.04652)
*Mantas Žurauskas,Tom Bu,Sanaz Alali,Beyza Kalkanli,Derek Shi,Fernando Alamos,Gauresh Pandit,Christopher Mei,Ali Behrooz,Ramin Mirjalili,Dave Stronks,Alexander Fix,Dmitri Model*

Main category: cs.CV

TL;DR: 偏振分辨近红外成像通过测量眼部组织反射光的偏振状态，为眼动追踪提供了额外的光学对比机制，显著提升了追踪精度。


<details>
  <summary>Details</summary>
Motivation: 传统眼动追踪仅依赖光强信息，限制了追踪精度和鲁棒性。偏振成像提供了新的对比机制，有望解决这一问题。

Method: 使用偏振滤光片阵列相机和线性偏振近红外光源构建偏振眼动追踪（PET）系统，结合卷积神经网络训练模型。

Result: 在346名参与者中，PET系统将95百分位绝对注视误差中位数降低了10-16%，且在多种干扰条件下表现稳健。

Conclusion: 偏振效应为眼动追踪提供了实用增益，PET系统有望成为未来可穿戴设备的简单、鲁棒传感方案。

Abstract: Polarization-resolved near-infrared imaging adds a useful optical contrast
mechanism to eye tracking by measuring the polarization state of light
reflected by ocular tissues in addition to its intensity. In this paper we
demonstrate how this contrast can be used to enable eye tracking. Specifically,
we demonstrate that a polarization-enabled eye tracking (PET) system composed
of a polarization--filter--array camera paired with a linearly polarized
near-infrared illuminator can reveal trackable features across the sclera and
gaze-informative patterns on the cornea, largely absent in intensity-only
images. Across a cohort of 346 participants, convolutional neural network based
machine learning models trained on data from PET reduced the median
95th-percentile absolute gaze error by 10--16\% relative to capacity-matched
intensity baselines under nominal conditions and in the presence of eyelid
occlusions, eye-relief changes, and pupil-size variation. These results link
light--tissue polarization effects to practical gains in human--computer
interaction and position PET as a simple, robust sensing modality for future
wearable devices.

</details>


### [58] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 论文提出了一种诊断和去偏方法，通过测试集压力测试和迭代偏置剪枝来识别和减少多模态基准中的非视觉偏差。


<details>
  <summary>Details</summary>
Motivation: 现有基准容易被模型利用非视觉偏差（如语言先验）通过，导致无法准确评估模型的视觉理解能力。

Method: 采用测试集压力测试（TsT）和迭代偏置剪枝（IBP）方法，通过微调LLM和随机森林诊断基准偏差，并过滤高偏差样本。

Result: 在四个基准测试中发现普遍的非视觉偏差，并通过去偏后的VSI-Bench-Debiased验证了方法的有效性。

Conclusion: 基准设计需主动诊断和去除非视觉偏差，以确保评估的准确性。

Abstract: Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>


### [59] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown,Arijit Ray,Ranjay Krishna,Ross Girshick,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: SIMS-V框架利用3D模拟器生成空间丰富的视频训练数据，通过系统消融实验发现三种问题类型最有效，7B参数模型在少量数据上表现优于更大基线。


<details>
  <summary>Details</summary>
Motivation: 解决多模态语言模型在时空推理上的不足，缓解真实视频数据标注的瓶颈。

Method: 提出SIMS-V框架，利用3D模拟器生成数据，并通过消融实验分析问题类型、混合和规模的影响。

Result: 7B模型在25K模拟数据上表现优于72B基线，在真实空间推理任务中表现优异。

Conclusion: 模拟数据可高效提升空间推理能力，同时保持通用视频理解性能。

Abstract: Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.

</details>


### [60] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 论文提出了一种超越传统多模态智能的‘超感知’范式，强调空间认知和预测建模的重要性，并设计了新基准VSI-SUPER来推动这一领域的发展。


<details>
  <summary>Details</summary>
Motivation: 当前多模态系统主要关注反应式任务和长上下文处理，缺乏对空间认知和世界建模的深入探索，限制了智能系统的潜力。

Method: 提出了VSI-SUPER基准（包括VSR和VSC任务），并通过数据扩展和模型训练（Cambrian-S）进行验证，同时引入基于预测误差的自监督方法。

Result: Cambrian-S在VSI-Bench上提升了30%，但VSI-SUPER表现仍有限；预测感知方法显著优于现有基线。

Conclusion: 空间超感知需要模型具备预测和组织经验的能力，仅靠数据扩展不足以实现这一目标。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


### [61] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu,Jian Han,Bin Yan,Hui Wu,Fengda Zhu,Xing Wang,Yi Jiang,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityStar是一个统一的时空自回归框架，用于高分辨率图像和动态视频合成，性能优越且高效。


<details>
  <summary>Details</summary>
Motivation: 基于自回归模型在视觉和语言领域的成功，旨在通过单一架构同时捕捉空间和时间依赖性，支持多种生成任务。

Method: 采用纯离散方法，通过时空自回归联合建模，支持文本到图像、文本到视频、图像到视频等任务。

Result: 在VBench上得分83.74，优于所有自回归模型，甚至超过部分扩散模型，生成720p视频速度快10倍。

Conclusion: InfinityStar是首个能生成工业级720p视频的离散自回归视频生成器，代码和模型已开源。

Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.

</details>


### [62] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun,Xinyu Yang,Jennifer J. Sun,Bharath Hariharan*

Main category: cs.CV

TL;DR: 提出了一种名为TubeletGraph的零样本系统，用于跟踪物体在状态变化中的轨迹，并生成状态图描述变化。


<details>
  <summary>Details</summary>
Motivation: 现实世界中物体经常发生状态变化，现有方法在物体外观显著变化时容易丢失跟踪目标。

Method: TubeletGraph通过识别潜在被忽略的轨迹，基于语义和邻近性先验决定是否整合，并生成状态图描述变化。

Result: TubeletGraph在状态变化下实现了最先进的跟踪性能，并展示了对物体变化的深入理解。

Conclusion: TubeletGraph在复杂物体变化的时间定位和语义推理方面表现出潜力。

Abstract: Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.

</details>


### [63] [Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping](https://arxiv.org/abs/2511.04680)
*Rafe Loya,Andrew Hamara,Benjamin Estell,Benjamin Kilpatrick,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 本文提出了一种自动图像裁剪方法，用于生成多个具有美感的裁剪区域，并引入了一个包含277张图像的数据集进行评估。


<details>
  <summary>Details</summary>
Motivation: 现代社交媒体应用需要多个美观的裁剪图像，但现有技术主要关注单一裁剪，缺乏多裁剪解决方案。

Method: 通过图像分割算法作为预处理步骤，评估了多个单裁剪模型的效果。

Result: 引入了一个公开数据集，并展示了多裁剪方法的可行性。

Conclusion: 本文为多裁剪问题提供了数据集和初步解决方案，为未来研究奠定了基础。

Abstract: Automatic image cropping is a method for maximizing the human-perceived
quality of cropped regions in photographs. Although several works have proposed
techniques for producing singular crops, little work has addressed the problem
of producing multiple, distinct crops with aesthetic appeal. In this paper, we
motivate the problem with a discussion on modern social media applications,
introduce a dataset of 277 relevant images and human labels, and evaluate the
efficacy of several single-crop models with an image partitioning algorithm as
a pre-processing step. The dataset is available at
https://github.com/RafeLoya/carousel.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [Applying Time Series Deep Learning Models to Forecast the Growth of Perennial Ryegrass in Ireland](https://arxiv.org/abs/2511.03749)
*Oluwadurotimi Onibonoje,Vuong M. Ngo,Andrew McCarre,Elodie Ruelle,Bernadette O-Briend,Mark Roantree*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的模型，用于预测爱尔兰牧场多年生黑麦草的生长，提高了可持续乳制品生产的可靠性。


<details>
  <summary>Details</summary>
Motivation: 爱尔兰乳业面临盈利和可持续性挑战，现有草生长预测模型不实用，需更高效方法。

Method: 设计了适用于单变量数据集的时间卷积网络，利用历史草高数据进行预测。

Result: 模型在34年1,757周的数据上表现优异，RMSE为2.74，MAE为3.46。

Conclusion: 研究提升了草生长预测的可靠性，有助于可持续乳业发展。

Abstract: Grasslands, constituting the world's second-largest terrestrial carbon sink,
play a crucial role in biodiversity and the regulation of the carbon cycle.
Currently, the Irish dairy sector, a significant economic contributor, grapples
with challenges related to profitability and sustainability. Presently, grass
growth forecasting relies on impractical mechanistic models. In response, we
propose deep learning models tailored for univariate datasets, presenting
cost-effective alternatives. Notably, a temporal convolutional network designed
for forecasting Perennial Ryegrass growth in Cork exhibits high performance,
leveraging historical grass height data with RMSE of 2.74 and MAE of 3.46.
Validation across a comprehensive dataset spanning 1,757 weeks over 34 years
provides insights into optimal model configurations. This study enhances our
understanding of model behavior, thereby improving reliability in grass growth
forecasting and contributing to the advancement of sustainable dairy farming
practices.

</details>


### [65] [Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices](https://arxiv.org/abs/2511.03753)
*Youssef Elmir,Yassine Himeur,Abbes Amira*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的隐私保护心电图分类框架，通过将1D信号转换为2D图像，结合CNN实现高效特征提取，并在多设备上验证了性能。


<details>
  <summary>Details</summary>
Motivation: 解决物联网医疗环境中隐私保护和高效心电图分类的需求。

Method: 将1D ECG信号转换为2D GAF图像，使用CNN进行特征提取，并在联邦学习框架下部署。

Result: 在多设备实验中，分类准确率达95.18%，优于单设备基线，且资源利用高效。

Conclusion: 该框架展示了轻量级隐私保护AI在物联网医疗中的潜力，支持可扩展的边缘部署。

Abstract: This study presents a federated learning (FL) framework for
privacy-preserving electrocardiogram (ECG) classification in Internet of Things
(IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian
Angular Field (GAF) images, the proposed approach enables efficient feature
extraction through Convolutional Neural Networks (CNNs) while ensuring that
sensitive medical data remain local to each device. This work is among the
first to experimentally validate GAF-based federated ECG classification across
heterogeneous IoT devices, quantifying both performance and communication
efficiency. To evaluate feasibility in realistic IoT settings, we deployed the
framework across a server, a laptop, and a resource-constrained Raspberry Pi 4,
reflecting edge-cloud integration in IoT ecosystems. Experimental results
demonstrate that the FL-GAF model achieves a high classification accuracy of
95.18% in a multi-client setup, significantly outperforming a single-client
baseline in both accuracy and training time. Despite the added computational
complexity of GAF transformations, the framework maintains efficient resource
utilization and communication overhead. These findings highlight the potential
of lightweight, privacy-preserving AI for IoT-based healthcare monitoring,
supporting scalable and secure edge deployments in smart health systems.

</details>


### [66] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: LOLGORITHM是一个模块化多代理系统，用于生成可控的短视频评论，支持六种风格，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 短视频评论在促进社区参与和内容再创作中起关键作用，但生成合规且多样化的评论仍具挑战性。

Method: 系统整合视频分割、上下文和情感分析，以及风格感知提示构建，利用多模态大语言模型处理视频输入。

Result: 在抖音和YouTube上分别获得超过90%和87.55%的偏好率，显著优于基线模型。

Conclusion: LOLGORITHM提供了一个可扩展且文化适应的框架，有望增强用户参与度和创意互动。

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [67] [What's in Common? Multimodal Models Hallucinate When Reasoning Across Scenes](https://arxiv.org/abs/2511.03768)
*Candace Ross,Florian Bordes,Adina Williams,Polina Kirichenko,Mark Ibrahim*

Main category: cs.LG

TL;DR: 论文提出了一个名为Common-O的新基准测试，用于评估多模态语言模型在真实场景中的推理能力，发现即使最佳模型在复杂场景中的表现也较差。


<details>
  <summary>Details</summary>
Motivation: 现有模型在感知基准测试中表现良好，但在真实世界场景中仍存在幻觉问题，需要新的评估方法。

Method: 构建了包含10.5k个新图像的Common-O基准测试，通过提问“共同点是什么”来评估模型的跨场景推理能力。

Result: 最佳模型在Common-O上仅达到35%准确率，在更复杂的Common-O Complex上仅为1%。多图像训练模型表现更好。

Conclusion: Common-O揭示了模型在跨场景推理中的挑战，多图像训练可能有助于改进，基准测试已公开以促进研究。

Abstract: Multimodal language models possess a remarkable ability to handle an
open-vocabulary's worth of objects. Yet the best models still suffer from
hallucinations when reasoning about scenes in the real world, revealing a gap
between their seemingly strong performance on existing perception benchmarks
that are saturating and their reasoning in the real world. To address this gap,
we build a novel benchmark of in-the-wild scenes that we call Common-O. With
more than 10.5k examples using exclusively new images not found in web training
data to avoid contamination, Common-O goes beyond just perception, inspired by
cognitive tests for humans, to probe reasoning across scenes by asking "what's
in common?". We evaluate leading multimodal language models, including models
specifically trained to perform chain-of-thought reasoning. We find that
perceiving objects in single images is tractable for most models, yet reasoning
across scenes is very challenging even for the best models, including reasoning
models. Despite saturating many leaderboards focusing on perception, the best
performing model only achieves 35% on Common-O -- and on Common-O Complex,
consisting of more complex scenes, the best model achieves only 1%. Curiously,
we find models are more prone to hallucinate when similar objects are present
in the scene, suggesting models may be relying on object co-occurrence seen
during training. Among the models we evaluated, we found scale can provide
modest improvements while models explicitly trained with multi-image inputs
show bigger improvements, suggesting scaled multi-image training may offer
promise. We make our benchmark publicly available to spur research into the
challenge of hallucination when reasoning across scenes.

</details>


### [68] [Contamination Detection for VLMs using Multi-Modal Semantic Perturbation](https://arxiv.org/abs/2511.03774)
*Jaden Park,Mu Cai,Feng Yao,Jingbo Shang,Soochahn Lee,Yong Jae Lee*

Main category: cs.LG

TL;DR: 提出了一种检测视觉语言模型（VLM）中测试集泄漏的新方法，通过多模态语义扰动验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测VLM测试集泄漏时表现不佳，需要一种更可靠的检测手段。

Method: 故意污染开源VLM，提出基于多模态语义扰动的检测方法。

Result: 污染模型在受控扰动下泛化能力下降，验证了方法的有效性。

Conclusion: 新方法在多种污染策略下表现稳健，代码和扰动数据集将公开。

Abstract: Recent advances in Vision-Language Models (VLMs) have achieved
state-of-the-art performance on numerous benchmark tasks. However, the use of
internet-scale, often proprietary, pretraining corpora raises a critical
concern for both practitioners and users: inflated performance due to test-set
leakage. While prior works have proposed mitigation strategies such as
decontamination of pretraining data and benchmark redesign for LLMs, the
complementary direction of developing detection methods for contaminated VLMs
remains underexplored. To address this gap, we deliberately contaminate
open-source VLMs on popular benchmarks and show that existing detection
approaches either fail outright or exhibit inconsistent behavior. We then
propose a novel simple yet effective detection method based on multi-modal
semantic perturbation, demonstrating that contaminated models fail to
generalize under controlled perturbations. Finally, we validate our approach
across multiple realistic contamination strategies, confirming its robustness
and effectiveness. The code and perturbed dataset will be released publicly.

</details>


### [69] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: FusionDP提出了一种两阶段框架，通过基础模型插补敏感特征并改进DP-SGD算法，在保护特征级隐私的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统DP-SGD对所有特征施加隐私保护，导致噪声过多和性能下降。FusionDP针对部分敏感特征提供隐私保护，优化隐私-效用权衡。

Method: 1. 使用基础模型插补敏感特征；2. 改进DP-SGD算法，结合原始和插补特征训练模型。

Result: 在PhysioNet和MIMIC-III数据集上，FusionDP显著优于基线方法，同时保持严格的隐私保护。

Conclusion: FusionDP通过基础模型插补和改进DP-SGD，有效提升了特征级隐私保护的模型性能。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [70] [Fair and Explainable Credit-Scoring under Concept Drift: Adaptive Explanation Frameworks for Evolving Populations](https://arxiv.org/abs/2511.03807)
*Shivogo John*

Main category: cs.LG

TL;DR: 该研究提出了一种自适应解释框架，用于动态调整信用评分模型的解释性和公平性，以应对数据分布变化带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统解释方法（如SHAP）假设数据静态分布，导致在概念漂移时解释不稳定且不公平。

Method: 结合XGBoost预测模型和三种自适应SHAP变体：特征分布调整、滑动窗口背景样本和增量岭回归校准。

Result: 自适应方法显著提高了时间稳定性并减少了不同人口群体的不公平影响，同时保持预测准确性。

Conclusion: 自适应解释框架为动态信用系统提供了透明、可靠和公平的解决方案，适用于其他类似领域。

Abstract: Evolving borrower behaviors, shifting economic conditions, and changing
regulatory landscapes continuously reshape the data distributions underlying
modern credit-scoring systems. Conventional explainability techniques, such as
SHAP, assume static data and fixed background distributions, making their
explanations unstable and potentially unfair when concept drift occurs. This
study addresses that challenge by developing adaptive explanation frameworks
that recalibrate interpretability and fairness in dynamically evolving credit
models. Using a multi-year credit dataset, we integrate predictive modeling via
XGBoost with three adaptive SHAP variants: (A) per-slice explanation
reweighting that adjusts for feature distribution shifts, (B) drift-aware SHAP
rebaselining with sliding-window background samples, and (C) online surrogate
calibration using incremental Ridge regression. Each method is benchmarked
against static SHAP explanations using metrics of predictive performance (AUC,
F1), directional and rank stability (cosine, Kendall tau), and fairness
(demographic parity and recalibration). Results show that adaptive methods,
particularly rebaselined and surrogate-based explanations, substantially
improve temporal stability and reduce disparate impact across demographic
groups without degrading predictive accuracy. Robustness tests, including
counterfactual perturbations, background sensitivity analysis, and
proxy-variable detection, confirm the resilience of adaptive explanations under
real-world drift conditions. These findings establish adaptive explainability
as a practical mechanism for sustaining transparency, accountability, and
ethical reliability in data-driven credit systems, and more broadly, in any
domain where decision models evolve with population change.

</details>


### [71] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出了一种路由方法，通过将问题分配给最小且能解决的模型，减少计算成本而不牺牲准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能优秀，但部署成本高，因此需要一种更高效的方法来降低计算开销。

Method: 利用中间表示训练轻量级预测器，预测问题难度或模型正确性，以指导模型池中的路由选择。

Result: 在多样化的数学基准测试中，路由方法比随机分配更高效，且性能与大型模型相当，同时显著减少计算量。

Conclusion: 难度感知路由是高效部署推理模型的有效方法。

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [72] [One Size Does Not Fit All: Architecture-Aware Adaptive Batch Scheduling with DEBA](https://arxiv.org/abs/2511.03809)
*François Belias,Naser Ezzati-Jivan,Foutse Khomh*

Main category: cs.LG

TL;DR: DEBA是一种动态高效批量适应方法，通过监测梯度方差、梯度范数变化和损失变化来调整批量大小，实验表明不同架构的适应效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有自适应批量方法假设所有架构适用相同策略，但实际效果因架构而异，因此需要一种更智能的适应方法。

Method: DEBA通过监测梯度方差、梯度范数变化和损失变化动态调整批量大小，并引入基线特征框架预测适应效果。

Result: 轻量级和中深度架构（如MobileNet-V3）获得45-62%的训练加速和1-7%的精度提升，而深度残差网络（如ResNet-50）表现不稳定。

Conclusion: 自适应批量方法需考虑架构特性，DEBA通过架构感知设计显著提升训练效率和精度。

Abstract: Adaptive batch size methods aim to accelerate neural network training, but
existing approaches apply identical adaptation strategies across all
architectures, assuming a one-size-fits-all solution. We introduce DEBA
(Dynamic Efficient Batch Adaptation), an adaptive batch scheduler that monitors
gradient variance, gradient norm variation and loss variation to guide batch
size adaptations. Through systematic evaluation across six architectures
(ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V3, ViT-B16) on
CIFAR-10 and CIFAR-100, with five random seeds per configuration, we
demonstrate that the architecture fundamentally determines adaptation efficacy.
Our findings reveal that: (1) lightweight and medium-depth architectures
(MobileNet-V3, DenseNet-121, EfficientNet-B0) achieve a 45-62% training speedup
with simultaneous accuracy improvements of 1-7%; (2) shallow residual networks
(ResNet-18) show consistent gains of +2.4 - 4.0% in accuracy, 36 - 43% in
speedup, while deep residual networks (ResNet-50) exhibit high variance and
occasional degradation; (3) already-stable architectures (ViT-B16) show minimal
speedup (6%) despite maintaining accuracy, indicating that adaptation benefits
vary with baseline optimization characteristics. We introduce a baseline
characterization framework using gradient stability metrics (stability score,
gradient norm variation) that predicts which architectures will benefit from
adaptive scheduling. Our ablation studies reveal critical design choices often
overlooked in prior work: sliding window statistics (vs. full history) and
sufficient cooldown periods (5+ epochs) between adaptations are essential for
success. This work challenges the prevailing assumption that adaptive methods
generalize across architectures and provides the first systematic evidence that
batch size adaptation requires an architecture-aware design.

</details>


### [73] [Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks](https://arxiv.org/abs/2511.03824)
*Ryien Hosseini,Filippo Simini,Venkatram Vishwanath,Rebecca Willett,Henry Hoffmann*

Main category: cs.LG

TL;DR: 该论文提出了一种通过注入随机全局嵌入（称为“Sketched Random Features”）来增强标准GNN的方法，以解决GNN中的长距离信息压缩、节点表示过度平滑和表达能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: GNN在聚合局部邻居信息时面临长距离信息压缩、节点表示过度平滑和表达能力有限的挑战，需要一种方法来高效捕获长距离依赖关系。

Method: 通过注入独特的、距离敏感且与拓扑无关的随机全局嵌入（Sketched Random Features）到标准GNN中。

Result: 实验证明该方法在真实图学习任务中显著提升了基线GNN的性能，并可作为现有技术的补充增强。

Conclusion: Sketched Random Features是一种有效的方法，能够缓解GNN的局限性，同时提升其性能。

Abstract: Graph Neural Networks learn on graph-structured data by iteratively
aggregating local neighborhood information. While this local message passing
paradigm imparts a powerful inductive bias and exploits graph sparsity, it also
yields three key challenges: (i) oversquashing of long-range information, (ii)
oversmoothing of node representations, and (iii) limited expressive power. In
this work we inject randomized global embeddings of node features, which we
term \textit{Sketched Random Features}, into standard GNNs, enabling them to
efficiently capture long-range dependencies. The embeddings are unique,
distance-sensitive, and topology-agnostic -- properties which we analytically
and empirically show alleviate the aforementioned limitations when injected
into GNNs. Experimental results on real-world graph learning tasks confirm that
this strategy consistently improves performance over baseline GNNs, offering
both a standalone solution and a complementary enhancement to existing
techniques such as graph positional encodings. Our source code is available at
\href{https://github.com/ryienh/sketched-random-features}{https://github.com/ryienh/sketched-random-features}.

</details>


### [74] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: StratDiff方法通过扩散模型和能量函数优化离线到在线强化学习的过渡，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决离线到在线强化学习中因策略分布变化导致的挑战。

Method: 使用扩散模型学习离线数据，通过能量函数分层样本，分别采用离线和在线学习策略。

Result: 在D4RL基准测试中表现优于现有方法。

Conclusion: StratDiff有效提升离线到在线强化学习的适应性和稳定性。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [75] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种扩展因果加性模型（CAM）的方法，引入高阶交互作用，通过有向无环超图（hyper DAG）表示，并提供了理论工具和可识别性结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多过程具有高阶交互作用，但因果发现中对交互作用的显式处理研究较少。

Method: 扩展CAM模型以包含高阶交互作用，提出有向无环超图表示，并开发了贪婪CAM算法的扩展版本。

Result: 理论结果表明，更复杂的超图结构可能带来更好的实证结果，且有限样本复杂度更低。

Conclusion: 论文展示了高阶交互作用在因果结构学习中的重要性，并通过实验验证了方法的有效性。

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [76] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: SADQ通过建模环境动态和整合后继状态分布，改进了DQN的稳定性与学习效率。


<details>
  <summary>Details</summary>
Motivation: DQN的目标更新依赖于过去策略生成的状态，可能导致学习信号不明确和高方差。

Method: 提出SADQ，利用随机转移模型建模环境动态，整合后继状态分布到Q值估计中。

Result: 理论证明SADQ保持无偏估计并降低方差，实验显示其在稳定性和效率上优于DQN变体。

Conclusion: SADQ通过更稳定的策略对齐更新，显著提升了强化学习性能。

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [77] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: 论文提出了Lead-Lag Forecasting（LLF）问题，并提供了两个基准数据集（arXiv和GitHub）用于研究早期行为与延迟高影响行为之间的关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于社会协作平台中早期行为（如浏览、点赞）与延迟高影响行为（如引用、销售）之间存在时间偏移的相关性，但缺乏统一的研究框架和标准化数据集。

Method: 方法包括构建arXiv和GitHub两个高容量数据集，并通过统计和分类测试验证了Lead-Lag动态的存在，同时比较了参数化和非参数化回归基线。

Result: 研究结果表明，LLF是一个新的预测范式，数据集为系统探索社会和使用数据中的Lead-Lag动态提供了实证基础。

Conclusion: 结论是LLF作为一种新的预测范式，为未来研究提供了标准化数据集和方法论基础，并展示了其在多个领域的潜在应用。

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [78] [DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets](https://arxiv.org/abs/2511.03911)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: Decomposition方法用于压缩超维计算（HDC）网络，提出DecoHD，通过分解参数化学习，实现高压缩比且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统HDC压缩方法会损害性能和鲁棒性，DecoHD旨在解决这一问题。

Method: DecoHD采用分层共享通道和乘法绑定，结合轻量级bundling head实现端到端训练。

Result: DecoHD在极端内存节省下仅轻微损失精度，硬件性能显著提升。

Conclusion: DecoHD是一种高效且鲁棒的HDC压缩方法，适用于内存和能效敏感场景。

Abstract: Decomposition is a proven way to shrink deep networks without changing I/O.
We bring this idea to hyperdimensional computing (HDC), where footprint cuts
usually shrink the feature axis and erode concentration and robustness. Prior
HDC decompositions decode via fixed atomic hypervectors, which are ill-suited
for compressing learned class prototypes. We introduce DecoHD, which learns
directly in a decomposed HDC parameterization: a small, shared set of per-layer
channels with multiplicative binding across layers and bundling at the end,
yielding a large representational space from compact factors. DecoHD compresses
along the class axis via a lightweight bundling head while preserving native
bind-bundle-score; training is end-to-end, and inference remains pure HDC,
aligning with in/near-memory accelerators. In evaluation, DecoHD attains
extreme memory savings with only minor accuracy degradation under tight
deployment budgets. On average it stays within about 0.1-0.15% of a strong
non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip
noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,
and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU
(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x
over a baseline HDC ASIC.

</details>


### [79] [On Predicting Sociodemographics from Mobility Signals](https://arxiv.org/abs/2511.03924)
*Ekin Uğurel,Cynthia Chen,Brian H. Y. Lee,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 论文提出了一种基于高阶移动描述符的多任务学习框架，用于从移动数据中预测社会人口属性，提高了预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于移动模式与社会人口特征之间的关系较弱且不一致，以及跨上下文泛化能力有限，从移动数据推断社会人口属性具有挑战性。

Method: 1. 引入基于有向移动图的行为高阶移动描述符；2. 提出度量标准和可视化诊断工具以量化不确定性；3. 开发多任务学习框架，联合预测多个社会人口属性。

Result: 新方法显著提高了年龄、性别、收入和家庭结构等属性的预测准确性，尤其在数据有限或测试集分布与训练集不同时表现更优。

Conclusion: 通过行为驱动的特征和多任务学习，该方法为交通规划者提供了更准确且可解释的社会人口属性预测工具。

Abstract: Inferring sociodemographic attributes from mobility data could help
transportation planners better leverage passively collected datasets, but this
task remains difficult due to weak and inconsistent relationships between
mobility patterns and sociodemographic traits, as well as limited
generalization across contexts. We address these challenges from three angles.
First, to improve predictive accuracy while retaining interpretability, we
introduce a behaviorally grounded set of higher-order mobility descriptors
based on directed mobility graphs. These features capture structured patterns
in trip sequences, travel modes, and social co-travel, and significantly
improve prediction of age, gender, income, and household structure over
baselines features. Second, we introduce metrics and visual diagnostic tools
that encourage evenness between model confidence and accuracy, enabling
planners to quantify uncertainty. Third, to improve generalization and sample
efficiency, we develop a multitask learning framework that jointly predicts
multiple sociodemographic attributes from a shared representation. This
approach outperforms single-task models, particularly when training data are
limited or when applying models across different time periods (i.e., when the
test set distribution differs from the training set).

</details>


### [80] [SynQuE: Estimating Synthetic Dataset Quality Without Annotations](https://arxiv.org/abs/2511.03928)
*Arthur Chen,Victor Zhong*

Main category: cs.LG

TL;DR: 论文提出了SynQuE框架，用于在真实数据稀缺的情况下评估和选择合成数据集的质量，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决在数据稀缺（如收集成本高或隐私限制）情况下，如何选择高质量合成数据集以提升任务性能的问题。

Method: 提出了SynQuE问题，并引入基于分布和多样性的代理指标，以及基于大语言模型推理的LENS方法。

Result: 实验表明，SynQuE代理指标与真实任务性能相关，LENS在复杂任务中表现最佳，例如在Text2SQL任务中提升8.1%的准确率。

Conclusion: SynQuE为合成数据选择提供了实用框架，并激励未来研究基于基础模型的数据表征和细粒度数据选择。

Abstract: We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE)
problem: ranking synthetic datasets by their expected real-world task
performance using only limited unannotated real data. This addresses a critical
and open challenge where data is scarce due to collection costs or privacy
constraints. We establish the first comprehensive benchmarks for this problem
by introducing and evaluating proxy metrics that choose synthetic data for
training to maximize task performance on real data. We introduce the first
proxy metrics for SynQuE by adapting distribution and diversity-based distance
measures to our context via embedding models. To address the shortcomings of
these metrics on complex planning tasks, we propose LENS, a novel proxy that
leverages large language model reasoning. Our results show that SynQuE proxies
correlate with real task performance across diverse tasks, including sentiment
analysis, Text2SQL, web navigation, and image classification, with LENS
consistently outperforming others on complex tasks by capturing nuanced
characteristics. For instance, on text-to-SQL parsing, training on the top-3
synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to
38.4 (+8.1)% on average compared to selecting data indiscriminately. This work
establishes SynQuE as a practical framework for synthetic data selection under
real-data scarcity and motivates future research on foundation model-based data
characterization and fine-grained data selection.

</details>


### [81] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron Nano V2 VL 是 Nemotron 系列的最新模型，专注于文档理解、长视频理解和推理任务，相比前代模型 Llama-3.1-Nemotron-Nano-VL-8B 有显著改进。


<details>
  <summary>Details</summary>
Motivation: 提升在真实世界文档理解、长视频理解和推理任务中的性能。

Method: 基于混合 Mamba-Transformer 架构和创新的 token 缩减技术，优化模型架构、数据集和训练方法。

Result: 在所有视觉和文本领域均有显著提升，并支持高效的长文档和视频推理。

Conclusion: 模型检查点和部分数据集、训练代码已公开，为相关研究提供支持。

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [82] [LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction](https://arxiv.org/abs/2511.03938)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: LogHD通过类轴对数压缩减少内存需求，同时保持高维度和鲁棒性，显著提升能效和速度。


<details>
  <summary>Details</summary>
Motivation: 解决传统HDC内存需求高的问题，同时保持模型的鲁棒性和准确性。

Method: 采用对数类轴压缩，使用容量感知的码本和基于配置文件的解码，结合特征轴稀疏化。

Result: 在相同内存下，LogHD比特征轴压缩方法更鲁棒，能效和速度显著提升。

Conclusion: LogHD是一种高效且鲁棒的HDC方法，适用于资源受限系统。

Abstract: Hyperdimensional computing (HDC) suits memory, energy, and
reliability-constrained systems, yet the standard "one prototype per class"
design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior
compaction reduces $D$ (feature axis), improving storage/compute but weakening
robustness. We introduce LogHD, a logarithmic class-axis reduction that
replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$
bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional
activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD
uses a capacity-aware codebook and profile-based decoding, and composes with
feature-axis sparsification. Across datasets and injected bit flips, LogHD
attains competitive accuracy with smaller models and higher resilience at
matched memory. Under equal memory, it sustains target accuracy at roughly
$2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC
instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup
over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX
4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a
feature-axis HDC ASIC baseline.

</details>


### [83] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 综述了RLHF在多模态对齐、文化公平和低延迟优化方面的最新进展，为研究者提供了重要路线图。


<details>
  <summary>Details</summary>
Motivation: 解决多模态对齐、文化公平和低延迟优化等关键问题，推动更鲁棒、高效和公平的AI系统发展。

Method: 回顾PPO、DPO和GRPO等基础算法，并分析最新创新技术。

Result: 提供了这些技术的比较综合，并概述了开放挑战。

Conclusion: 该研究为构建更鲁棒、高效和公平的AI系统提供了重要指导。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [84] [Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels](https://arxiv.org/abs/2511.03953)
*Wuxia Chen,Taposh Banerjee,Vahid Tarokh*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件分数的高维马尔可夫过程快速变化检测方法，避免了显式似然计算。


<details>
  <summary>Details</summary>
Motivation: 解决马尔可夫过程中未知转移核的快速变化检测问题，避免高维数据下的显式似然计算困难。

Method: 通过样本对直接学习条件分数，提出基于分数的CUSUM检测方法，并使用截断统计量确保增量有界。

Result: 证明了误报时间的指数下界和检测延迟的渐近上界。

Conclusion: 方法在高维马尔可夫模型中具有理论保证和实际可行性。

Abstract: We address the problem of quickest change detection in Markov processes with
unknown transition kernels. The key idea is to learn the conditional score
$\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs
$( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are
high-dimensional data generated by the same transition kernel. In this way, we
avoid explicit likelihood evaluation and provide a practical way to learn the
transition dynamics. Based on this estimation, we develop a score-based CUSUM
procedure that uses conditional Hyvarinen score differences to detect changes
in the kernel. To ensure bounded increments, we propose a truncated version of
the statistic. With Hoeffding's inequality for uniformly ergodic Markov
processes, we prove exponential lower bounds on the mean time to false alarm.
We also prove asymptotic upper bounds on detection delay. These results give
both theoretical guarantees and practical feasibility for score-based detection
in high-dimensional Markov models.

</details>


### [85] [PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis](https://arxiv.org/abs/2511.03966)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.LG

TL;DR: 论文提出了一种名为HIF的新算法，用于解决认知诊断模型中的数据遗忘问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 用户对“被遗忘权”的需求日益增长，但现有认知诊断模型缺乏有效的数据遗忘机制，直接应用通用遗忘算法效果不佳。

Method: 提出分层重要性引导遗忘（HIF）算法，利用参数重要性的分层特性，通过平滑机制区分待遗忘数据相关参数。

Result: 在三个真实数据集上的实验表明，HIF在关键指标上显著优于基线方法。

Conclusion: HIF为认知诊断模型提供了首个有效的数据遗忘解决方案，支持高性能且隐私保护的AI系统部署。

Abstract: The need to remove specific student data from cognitive diagnosis (CD) models
has become a pressing requirement, driven by users' growing assertion of their
"right to be forgotten". However, existing CD models are largely designed
without privacy considerations and lack effective data unlearning mechanisms.
Directly applying general purpose unlearning algorithms is suboptimal, as they
struggle to balance unlearning completeness, model utility, and efficiency when
confronted with the unique heterogeneous structure of CD models. To address
this, our paper presents the first systematic study of the data unlearning
problem for CD models, proposing a novel and efficient algorithm: hierarchical
importanceguided forgetting (HIF). Our key insight is that parameter importance
in CD models exhibits distinct layer wise characteristics. HIF leverages this
via an innovative smoothing mechanism that combines individual and layer, level
importance, enabling a more precise distinction of parameters associated with
the data to be unlearned. Experiments on three real world datasets show that
HIF significantly outperforms baselines on key metrics, offering the first
effective solution for CD models to respond to user data removal requests and
for deploying high-performance, privacy preserving AI systems

</details>


### [86] [Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models](https://arxiv.org/abs/2511.03972)
*Semih Cayci*

Main category: cs.LG

TL;DR: 论文研究了高阶优化方法（如随机高斯-牛顿法）对深度学习泛化性能的影响，并分析了其在过参数化深度神经网络中的收敛性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 探讨高阶优化方法（如随机高斯-牛顿法）在深度学习中的泛化性能，特别是在过参数化网络中的表现。

Method: 使用带有Levenberg-Marquardt阻尼和mini-batch采样的随机高斯-牛顿法（SGN）训练过参数化深度神经网络，并通过变量度量分析和均匀稳定性理论推导收敛和泛化界限。

Result: 理论结果表明，SGN在优化路径上高斯-牛顿矩阵的最小特征值较大时，具有更紧的稳定性界限和更好的泛化性能。

Conclusion: SGN在特定条件下（如较大的高斯-牛顿矩阵最小特征值）能够实现更好的泛化性能，为高阶优化方法在深度学习中的应用提供了理论支持。

Abstract: An important question in deep learning is how higher-order optimization
methods affect generalization. In this work, we analyze a stochastic
Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch
sampling for training overparameterized deep neural networks with smooth
activations in a regression setting. Our theoretical contributions are twofold.
First, we establish finite-time convergence bounds via a variable-metric
analysis in parameter space, with explicit dependencies on the batch size,
network width and depth. Second, we derive non-asymptotic generalization bounds
for SGN using uniform stability in the overparameterized regime, characterizing
the impact of curvature, batch size, and overparameterization on generalization
performance. Our theoretical results identify a favorable generalization regime
for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along
the optimization path yields tighter stability bounds.

</details>


### [87] [PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction](https://arxiv.org/abs/2511.03976)
*Xu Zou*

Main category: cs.LG

TL;DR: PETRA是一种基于进化轨迹的Transformer方法，用于预测SARS-CoV-2的突变，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: SARS-CoV-2的快速进化对公共卫生和疫苗开发构成挑战，现有方法对噪声基因组序列效果有限。

Method: PETRA利用系统发育树提取进化轨迹，而非原始RNA序列，以减少噪声并捕捉病毒进化的层次结构。

Result: PETRA在核苷酸和刺突蛋白突变预测中表现优异，召回率显著高于基线。

Conclusion: PETRA为实时预测病毒突变提供了有效工具，代码已开源。

Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable
evolutionary trajectory, characterized by the continual emergence of
immune-evasive variants. This poses persistent challenges to public health and
vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have
revolutionized the modeling of sequential data, their direct applications to
noisy viral genomic sequences are limited. In this paper, we introduce
PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based
on evolutionary trajectories derived from phylogenetic trees rather than raw
RNA sequences. This method effectively mitigates sequencing noise and captures
the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and
temporal imbalances in global sequence data, PETRA excels in predicting future
SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide
mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and
6.64% respectively for the best baseline. PETRA also demonstrates its ability
to aid in the real-time mutation prediction of major clades like 24F(XEC) and
25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra

</details>


### [88] [Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models](https://arxiv.org/abs/2511.03981)
*Yuxiao Wang,Di Wu,Feng Liu,Zhimin Qiu,Chenrui Hu*

Main category: cs.LG

TL;DR: 提出了一种结合图结构先验和模块化适配器的可组合微调方法，用于解决大规模预训练模型在多任务适应中的高计算成本和结构不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型在多任务适应中面临高计算成本和结构不稳定的挑战，需要一种高效且稳定的方法。

Method: 通过关系矩阵建模任务依赖关系，将节点和路径相关性编码为图结构先验，嵌入模块化适配器以实现高效跨任务组合和重用。

Result: 实验表明，该方法显著提高了任务预测精度、适配器权重分配精度和计算效率，同时保持模型轻量化。

Conclusion: 图结构先验和模块化机制在可组合微调中具有协同优势，能够有效提升多任务适应性能。

Abstract: This paper proposes a composable fine-tuning method that integrates graph
structural priors with modular adapters to address the high computational cost
and structural instability faced by large-scale pre-trained models in
multi-task adaptation. The method introduces a relation matrix to model
dependencies among tasks, explicitly encoding correlations between nodes and
paths into graph structural priors, which provide unified structural
constraints for adapter weight allocation and path selection. Modular adapters
are embedded into different layers through low-rank mapping and a pluggable
mechanism, enabling efficient cross-task composition and reuse under prior
guidance. This mechanism not only improves parameter efficiency and training
stability but also alleviates path conflicts and redundant computation in
multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity,
environmental sensitivity, and data sensitivity are conducted to systematically
analyze key factors such as routing temperature, gating thresholds, and
relation matrix regularization strength, verifying the consistency and superior
performance of the method under structural constraints. The results demonstrate
that the proposed framework significantly enhances task prediction accuracy,
adapter weight allocation precision, and overall computational efficiency while
maintaining model lightweight design, highlighting the synergistic advantages
of graph priors and modular mechanisms in composable fine-tuning.

</details>


### [89] [TwIST: Rigging the Lottery in Transformers with Independent Subnetwork Training](https://arxiv.org/abs/2511.03983)
*Michael Menezes,Barbara Su,Xinze Feng,Yehya Farhat,Hamza Shili,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: TwIST是一种分布式训练框架，用于高效的大语言模型稀疏化，通过并行训练多个子网络并定期聚合参数，实现零成本剪枝。


<details>
  <summary>Details</summary>
Motivation: 解决传统稀疏化方法需要后训练校准或Hessian恢复的问题，提供更高效的训练时稀疏化方案。

Method: 并行训练多个子网络，定期聚合参数并重新采样，识别高质量子网络（“黄金票”）。

Result: 在50%+的高稀疏度下，TwIST显著优于基线方法（如PPL 23.14 vs. 31.64），且生成结构化密集矩阵，提升推理速度。

Conclusion: TwIST为可部署的稀疏大语言模型提供了高效的训练时路径，无需额外微调或恢复开销。

Abstract: We introduce TwIST, a distributed training framework for efficient large
language model (LLM) sparsification. TwIST trains multiple subnetworks in
parallel, periodically aggregates their parameters, and resamples new
subnetworks during training. This process identifies high-quality subnetworks
("golden tickets") without requiring post-training procedures such as
calibration or Hessian-based recovery. As a result, TwIST enables zero-cost
pruning at deployment time while achieving perplexity competitive with
state-of-the-art post-training sparsification methods. The benefits are most
pronounced under aggressive sparsity (e.g., 50%+), where TwIST significantly
outperforms baseline methods; for example, reaching 23.14 PPL compared to 31.64
for the closest prior approach. Unlike unstructured pruning, TwIST produces
structured, dense matrices that offer practical inference speedups and memory
reductions on commodity hardware (e.g., CPUs) that do not support efficient
sparse computation. TwIST provides an efficient training-time path to
deployable sparse LLMs without additional fine-tuning or recovery overhead.

</details>


### [90] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 利用连续血糖监测和可穿戴技术，结合机器学习模型，实现动态代谢表型分析，为糖尿病精准预防提供新方法。


<details>
  <summary>Details</summary>
Motivation: 传统静态血糖阈值分类方法无法反映糖尿病和前驱糖尿病的病理生理异质性，需要更精准的动态代谢表型分析。

Method: 通过连续血糖监测（CGM）和可穿戴技术收集高分辨率血糖数据，结合机器学习模型预测胰岛素抵抗和β细胞功能。

Result: 研究表明，个体对标准化餐食的餐后血糖反应（PPGR）可作为代谢亚型的生物标志物，且饮食、睡眠和运动模式与特定代谢功能障碍相关。

Conclusion: CGM技术能够分解早期血糖异常的复杂性，为精准糖尿病预防提供个性化营养、行为和药物干预策略。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [91] [Multiscale Astrocyte Network Calcium Dynamics for Biologically Plausible Intelligence in Anomaly Detection](https://arxiv.org/abs/2511.03993)
*Berk Iskar,Michael Taynnan Barros*

Main category: cs.LG

TL;DR: 提出了一种基于钙离子信号调节的学习框架，用于网络异常检测，解决了传统离线训练检测器在概念漂移和新威胁下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统网络异常检测系统在离线训练后易受概念漂移和新威胁（如零日攻击）影响，需要一种能够快速适应动态数据的方法。

Method: 结合多细胞星形胶质细胞动态模拟器和深度神经网络，模拟钙离子信号机制（IP3介导的释放、SERCA泵摄取和间隙连接扩散）。

Result: 在CTU-13数据集上测试，钙离子门控模型优于基线DNN，准确率达98%，假阳性和假阴性减少，运行时开销可忽略。

Conclusion: 该框架不仅适用于网络安全，还为需要快速适应动态数据的流式检测任务提供了通用解决方案。

Abstract: Network anomaly detection systems encounter several challenges with
traditional detectors trained offline. They become susceptible to concept drift
and new threats such as zero-day or polymorphic attacks. To address this
limitation, we propose a Ca$^{2+}$-modulated learning framework that draws
inspiration from astrocytic Ca$^{2+}$ signaling in the brain, where rapid,
context-sensitive adaptation enables robust information processing. Our
approach couples a multicellular astrocyte dynamics simulator with a deep
neural network (DNN). The simulator models astrocytic Ca$^{2+}$ dynamics
through three key mechanisms: IP$_3$-mediated Ca$^{2+}$ release, SERCA pump
uptake, and conductance-aware diffusion through gap junctions between cells.
Evaluation of our proposed network on CTU-13 (Neris) network traffic data
demonstrates the effectiveness of our biologically plausible approach. The
Ca$^{2+}$-gated model outperforms a matched baseline DNN, achieving up to
$\sim$98\% accuracy with reduced false positives and negatives across multiple
train/test splits. Importantly, this improved performance comes with negligible
runtime overhead once Ca$^{2+}$ trajectories are precomputed. While
demonstrated here for cybersecurity applications, this Ca$^{2+}$-modulated
learning framework offers a generic solution for streaming detection tasks that
require rapid, biologically grounded adaptation to evolving data patterns.

</details>


### [92] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 提出一种高效、可扩展的方法，通过生成合成预训练数据实现决策树的元学习，性能接近真实数据预训练。


<details>
  <summary>Details</summary>
Motivation: 决策树在高风险领域（如金融和医疗）中因其可解释性被广泛使用，但传统方法计算成本高且数据生成受限。

Method: 通过采样接近最优的合成决策树生成大规模、真实的数据集，结合MetaTree transformer架构进行元学习。

Result: 该方法性能与真实数据预训练或计算昂贵的优化决策树相当，同时显著降低计算成本。

Conclusion: 该方法为可扩展、高效的元学习决策树模型提供了新途径，增强了数据生成的灵活性。

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [93] [Accelerating scientific discovery with the common task framework](https://arxiv.org/abs/2511.04001)
*J. Nathan Kutz,Peter Battaglia,Michael Brenner,Kevin Carlberg,Aric Hagberg,Shirley Ho,Stephan Hoyer,Henning Lange,Hod Lipson,Michael W. Mahoney,Frank Noe,Max Welling,Laure Zanna,Francis Zhu,Steven L. Brunton*

Main category: cs.LG

TL;DR: 论文提出了一种用于科学和工程的通用任务框架（CTF），以评估和比较不同机器学习算法在动态系统建模中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习算法在工程、物理和生物科学中的应用日益广泛，需要一种统一的评估标准来比较不同算法的性能，尤其是在数据有限和噪声测量的情况下。

Method: 引入了一个通用的任务框架（CTF），包含多样化的挑战数据集，用于评估算法的预测、状态重建、泛化和控制等能力。

Result: CTF为科学和工程领域提供了一种标准化的评估工具，有助于加速机器学习算法的发展和应用。

Conclusion: CTF是评估和比较机器学习算法的重要工具，能够推动算法在科学和工程中的实际应用。

Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are
transforming and empowering the characterization and control of dynamic systems
in the engineering, physical, and biological sciences. These emerging modeling
paradigms require comparative metrics to evaluate a diverse set of scientific
objectives, including forecasting, state reconstruction, generalization, and
control, while also considering limited data scenarios and noisy measurements.
We introduce a common task framework (CTF) for science and engineering, which
features a growing collection of challenge data sets with a diverse set of
practical and common objectives. The CTF is a critically enabling technology
that has contributed to the rapid advance of ML/AI algorithms in traditional
applications such as speech recognition, language processing, and computer
vision. There is a critical need for the objective metrics of a CTF to compare
the diverse algorithms being rapidly developed and deployed in practice today
across science and engineering.

</details>


### [94] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 提出了一种针对边缘设备部署大语言模型的自回归感知分割计算框架，通过混合精度量化和中间压缩技术，显著减少通信开销并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在资源受限的物联网设备上部署困难，现有分割计算方法未能解决自回归推理的独特挑战。

Method: 开发了混合精度量化方案（OPSC）、两阶段中间压缩管道（TS和TAB-Q）以及联合优化框架。

Result: 在多种大语言模型和硬件平台上表现优异，推理速度提升1.49倍，通信开销显著减少，同时保持或提高模型精度。

Conclusion: 该框架为边缘设备高效部署大语言模型提供了可行解决方案。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [95] [Enhancing Multimodal Protein Function Prediction Through Dual-Branch Dynamic Selection with Reconstructive Pre-Training](https://arxiv.org/abs/2511.04040)
*Xiaoling Luo,Peng Chen,Chengliang Liu,Xiaopeng Jin,Jie Wen,Yumeng Liu,Junsong Wang*

Main category: cs.LG

TL;DR: 提出了一种多模态蛋白质功能预测方法DSRPGO，通过动态选择和重建预训练机制，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态蛋白质特征包含复杂信息，难以解析其关联性，因此需要一种有效的方法来整合这些特征并提升预测准确性。

Method: 采用重建预训练挖掘细粒度信息，提出双向交互模块（BInM）促进多模态特征交互学习，设计动态选择模块（DSM）优化特征表示。

Result: 在人类数据集上，DSRPGO在BPO、MFO和CCO方面显著优于其他基准模型。

Conclusion: DSRPGO通过动态选择和重建预训练机制，有效提升了多模态蛋白质功能预测的准确性。

Abstract: Multimodal protein features play a crucial role in protein function
prediction. However, these features encompass a wide range of information,
ranging from structural data and sequence features to protein attributes and
interaction networks, making it challenging to decipher their complex
interconnections. In this work, we propose a multimodal protein function
prediction method (DSRPGO) by utilizing dynamic selection and reconstructive
pre-training mechanisms. To acquire complex protein information, we introduce
reconstructive pre-training to mine more fine-grained information with low
semantic levels. Moreover, we put forward the Bidirectional Interaction Module
(BInM) to facilitate interactive learning among multimodal features.
Additionally, to address the difficulty of hierarchical multi-label
classification in this task, a Dynamic Selection Module (DSM) is designed to
select the feature representation that is most conducive to current protein
function prediction. Our proposed DSRPGO model improves significantly in BPO,
MFO, and CCO on human datasets, thereby outperforming other benchmark models.

</details>


### [96] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: DartQuant是一种高效的分布感知旋转校准方法，通过约束旋转后激活的分布来降低旋转优化的复杂性，减少对任务特定损失的依赖，避免过拟合。


<details>
  <summary>Details</summary>
Motivation: 解决旋转优化算法在端到端微调中计算成本高且容易过拟合的问题。

Method: 提出DartQuant方法，结合QR-Orth优化方案，替代昂贵的交替优化。

Result: 在70B模型上实现47倍加速和10倍内存节省，首次在单3090 GPU上完成70B模型的旋转校准。

Conclusion: DartQuant为资源受限环境下的大语言模型量化提供了可行方案。

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [97] [Pediatric Appendicitis Detection from Ultrasound Images](https://arxiv.org/abs/2511.04069)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: 开发并评估了一种基于预训练ResNet架构的深度学习模型，用于从超声图像中自动检测小儿阑尾炎。


<details>
  <summary>Details</summary>
Motivation: 小儿阑尾炎是儿童急性腹痛的常见原因，诊断困难，症状重叠且影像质量不一。

Method: 使用Regensburg Pediatric Appendicitis Dataset，对ResNet进行微调，预处理包括归一化、调整大小和数据增强。

Result: 模型准确率为93.44%，精确率为91.53%，召回率为89.8%，表现优异。

Conclusion: 模型能有效识别阑尾炎，克服了低对比度、噪声和解剖变异等挑战。

Abstract: Pediatric appendicitis remains one of the most common causes of acute
abdominal pain in children, and its diagnosis continues to challenge clinicians
due to overlapping symptoms and variable imaging quality. This study aims to
develop and evaluate a deep learning model based on a pretrained ResNet
architecture for automated detection of appendicitis from ultrasound images. We
used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound
scans, laboratory data, and clinical scores from pediatric patients admitted
with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each
subject had 1 to 15 ultrasound views covering the right lower quadrant,
appendix, lymph nodes, and related structures. For the image based
classification task, ResNet was fine tuned to distinguish appendicitis from
non-appendicitis cases. Images were preprocessed by normalization, resizing,
and augmentation to enhance generalization. The proposed ResNet model achieved
an overall accuracy of 93.44, precision of 91.53, and recall of 89.8,
demonstrating strong performance in identifying appendicitis across
heterogeneous ultrasound views. The model effectively learned discriminative
spatial features, overcoming challenges posed by low contrast, speckle noise,
and anatomical variability in pediatric imaging.

</details>


### [98] [Left Atrial Segmentation with nnU-Net Using MRI](https://arxiv.org/abs/2511.04071)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: 使用nnU-Net框架自动分割左心房MRI图像，性能优于传统方法，Dice得分为93.5。


<details>
  <summary>Details</summary>
Motivation: 手动分割左心房耗时且依赖观察者，深度学习可提高效率和准确性。

Method: 应用nnU-Net框架，自动适应MRI数据特性，进行预处理、网络配置和训练。

Result: 模型Dice得分为93.5，泛化能力强，能准确分割心房体和肺静脉。

Conclusion: nnU-Net在左心房分割任务中表现优异，适用于临床工作流。

Abstract: Accurate segmentation of the left atrium (LA) from cardiac MRI is critical
for guiding atrial fibrillation (AF) ablation and constructing biophysical
cardiac models. Manual delineation is time-consuming, observer-dependent, and
impractical for large-scale or time-sensitive clinical workflows. Deep learning
methods, particularly convolutional architectures, have recently demonstrated
superior performance in medical image segmentation tasks. In this study, we
applied the nnU-Net framework, an automated, self-configuring deep learning
segmentation architecture, to the Left Atrial Segmentation Challenge 2013
dataset. The dataset consists of thirty MRI scans with corresponding
expert-annotated masks. The nnU-Net model automatically adapted its
preprocessing, network configuration, and training pipeline to the
characteristics of the MRI data. Model performance was quantitatively evaluated
using the Dice similarity coefficient (DSC), and qualitative results were
compared against expert segmentations. The proposed nnUNet model achieved a
mean Dice score of 93.5, demonstrating high overlap with expert annotations and
outperforming several traditional segmentation approaches reported in previous
studies. The network exhibited robust generalization across variations in left
atrial shape, contrast, and image quality, accurately delineating both the
atrial body and proximal pulmonary veins.

</details>


### [99] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 提出了一种基于数据学习的方法，优化过滤近似最近邻搜索中的向量距离和过滤匹配的权衡，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用固定的数据无关惩罚，难以适应不同标签和向量分布的数据集。

Method: 将问题建模为约束线性优化问题，学习反映过滤分布的最优权重。

Result: 实验表明，该方法比固定惩罚方法准确率提高了5-10%。

Conclusion: 提供了一种更灵活、可泛化的过滤近似最近邻搜索框架。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [100] [DeNoise: Learning Robust Graph Representations for Unsupervised Graph-Level Anomaly Detection](https://arxiv.org/abs/2511.04086)
*Qingfeng Chen,Haojin Zeng,Jingyi Jie,Shichao Zhang,Debo Cheng*

Main category: cs.LG

TL;DR: DeNoise是一种针对污染训练数据的鲁棒无监督图级异常检测框架，通过联合优化编码器和解码器，结合对抗目标和对比学习，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法假设训练集干净，但实际中常被异常图污染，导致性能下降。

Method: 提出DeNoise框架，联合优化图级编码器、属性解码器和结构解码器，引入锚对齐去噪机制和对比学习。

Result: 在八个真实数据集上，DeNoise在不同噪声强度下表现稳定，显著优于现有方法。

Conclusion: DeNoise通过噪声抵抗嵌入和对比学习，有效解决了污染训练数据下的异常检测问题。

Abstract: With the rapid growth of graph-structured data in critical domains,
unsupervised graph-level anomaly detection (UGAD) has become a pivotal task.
UGAD seeks to identify entire graphs that deviate from normal behavioral
patterns. However, most Graph Neural Network (GNN) approaches implicitly assume
that the training set is clean, containing only normal graphs, which is rarely
true in practice. Even modest contamination by anomalous graphs can distort
learned representations and sharply degrade performance. To address this
challenge, we propose DeNoise, a robust UGAD framework explicitly designed for
contaminated training data. It jointly optimizes a graph-level encoder, an
attribute decoder, and a structure decoder via an adversarial objective to
learn noise-resistant embeddings. Further, DeNoise introduces an encoder
anchor-alignment denoising mechanism that fuses high-information node
embeddings from normal graphs into all graph embeddings, improving
representation quality while suppressing anomaly interference. A contrastive
learning component then compacts normal graph embeddings and repels anomalous
ones in the latent space. Extensive experiments on eight real-world datasets
demonstrate that DeNoise consistently learns reliable graph-level
representations under varying noise intensities and significantly outperforms
state-of-the-art UGAD baselines.

</details>


### [101] [KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea](https://arxiv.org/abs/2511.04094)
*Hyungjong Na,Wonho Song,Seungyong Han,Donghyeon Jo,Sejin Myung,Hyungjoon Kim*

Main category: cs.LG

TL;DR: KoTaP是一个长期面板数据集，用于研究韩国非金融企业的避税行为及其与其他财务和治理指标的关系。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提供一个标准化的数据集，用于分析企业避税行为对多个财务和治理领域的影响，并支持国际比较和政策评估。

Method: 通过排除特定企业并标准化变量，构建了包含12,653个企业年观测值的数据集，使用多种避税指标（如CETR、GETR等）进行分析。

Result: KoTaP数据集具有平衡面板结构和国际可比性，同时反映了韩国企业的独特制度特征。

Conclusion: KoTaP是一个重要的开放资源，适用于会计、金融及跨学科研究，支持模型验证和政策分析。

Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.

</details>


### [102] [Decomposable Neuro Symbolic Regression](https://arxiv.org/abs/2511.04124)
*Giorgio Morales,John W. Sheppard*

Main category: cs.LG

TL;DR: 提出了一种可分解的符号回归方法，结合Transformer、遗传算法和遗传编程，生成可解释的多变量表达式。


<details>
  <summary>Details</summary>
Motivation: 传统符号回归方法过于关注预测误差，导致生成的表达式复杂或不准确，缺乏对底层数学结构的捕捉。

Method: 使用Multi-Set Transformer生成单变量符号骨架，通过遗传算法筛选高质量候选，再通过遗传编程逐步合并骨架，最后优化系数。

Result: 在噪声控制实验中，该方法在插值和外推误差上表现优于或与现有方法相当，且能准确匹配原始数学结构。

Conclusion: 该方法能生成更准确且可解释的数学表达式，优于现有符号回归方法。

Abstract: Symbolic regression (SR) models complex systems by discovering mathematical
expressions that capture underlying relationships in observed data. However,
most SR methods prioritize minimizing prediction error over identifying the
governing equations, often producing overly complex or inaccurate expressions.
To address this, we present a decomposable SR method that generates
interpretable multivariate expressions leveraging transformer models, genetic
algorithms (GAs), and genetic programming (GP). In particular, our explainable
SR method distills a trained ``opaque'' regression model into mathematical
expressions that serve as explanations of its computed function. Our method
employs a Multi-Set Transformer to generate multiple univariate symbolic
skeletons that characterize how each variable influences the opaque model's
response. We then evaluate the generated skeletons' performance using a
GA-based approach to select a subset of high-quality candidates before
incrementally merging them via a GP-based cascade procedure that preserves
their original skeleton structure. The final multivariate skeletons undergo
coefficient optimization via a GA. We evaluated our method on problems with
controlled and varying degrees of noise, demonstrating lower or comparable
interpolation and extrapolation errors compared to two GP-based methods, three
neural SR methods, and a hybrid approach. Unlike them, our approach
consistently learned expressions that matched the original mathematical
structure.

</details>


### [103] [Exploring the Feasibility of End-to-End Large Language Model as a Compiler](https://arxiv.org/abs/2511.04132)
*Hongbin Zhang,Shihao Gao,Yang Liu,Mingjie Xing,Yanjun Wu,Chen Zhao*

Main category: cs.LG

TL;DR: 论文探讨了将大型语言模型（LLM）作为端到端编译器（LaaC）的可行性，设计了CompilerEval数据集和框架评估LLM在代码理解和汇编生成中的表现，发现LLM具备基本能力但成功率低，通过优化提示和模型规模可提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为编译器的潜力，填补其在端到端编译任务中的研究空白。

Method: 设计CompilerEval数据集和框架，评估LLM在代码理解和汇编生成中的能力，分析错误并尝试优化提示、模型规模和推理方法。

Result: LLM具备基本编译能力但成功率低，优化后汇编代码质量显著提升。

Conclusion: LaaC前景乐观，需针对性训练和知识丰富的提示，未来可能推动编译领域范式转变。

Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown
substantial advantages across various domains. As critical system software and
infrastructure, compilers are responsible for transforming source code into
target code. While LLMs have been leveraged to assist in compiler development
and maintenance, their potential as an end-to-end compiler remains largely
unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and
its future directions. We designed the CompilerEval dataset and framework
specifically to evaluate the capabilities of mainstream LLMs in source code
comprehension and assembly code generation. In the evaluation, we analyzed
various errors, explored multiple methods to improve LLM-generated code, and
evaluated cross-platform compilation capabilities. Experimental results
demonstrate that LLMs exhibit basic capabilities as compilers but currently
achieve low compilation success rates. By optimizing prompts, scaling up the
model, and incorporating reasoning methods, the quality of assembly code
generated by LLMs can be significantly enhanced. Based on these findings, we
maintain an optimistic outlook for LaaC and propose practical architectural
designs and future research directions. We believe that with targeted training,
knowledge-rich prompts, and specialized infrastructure, LaaC has the potential
to generate high-quality assembly code and drive a paradigm shift in the field
of compilation.

</details>


### [104] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: EPO是一种解决半无限安全强化学习问题的算法框架，通过迭代优化和约束调整实现安全性和性能的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决半无限约束条件下的安全强化学习问题，确保在连续参数空间中满足安全要求。

Method: 提出EPO框架，通过迭代解决有限约束子问题，动态调整约束集（扩展和删除）。

Result: EPO在理论分析中表现出与全局最优解相当的性能，且约束违反严格受控。

Conclusion: EPO是一种高效且安全的强化学习算法，适用于复杂约束场景。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [105] [Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories](https://arxiv.org/abs/2511.04155)
*Olav Finne Praesteng Larsen,Massimiliano Ruocco,Michail Spitieris,Abdulmajid Murad,Martina Ragosta*

Main category: cs.LG

TL;DR: 论文研究了如何通过迁移学习将数据丰富机场训练的生成模型应用于数据稀缺机场，结果显示扩散模型在仅5%目标数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决次要和地区机场因数据稀缺导致机器学习方法应用受限的问题。

Method: 采用扩散模型和流匹配架构，先在苏黎世机场预训练，再在都柏林机场微调。

Result: 扩散模型仅需5%都柏林数据即可达到竞争性表现，20%时达到基线水平。

Conclusion: 迁移学习能显著减少ATM中轨迹生成的数据需求，适用于历史记录有限的环境。

Abstract: Access to trajectory data is a key requirement for developing and validating
Air Traffic Management (ATM) solutions, yet many secondary and regional
airports face severe data scarcity. This limits the applicability of machine
learning methods and the ability to perform large-scale simulations or
"what-if" analyses. In this paper, we investigate whether generative models
trained on data-rich airports can be efficiently adapted to data-scarce
airports using transfer learning. We adapt state-of-the-art diffusion- and
flow-matching-based architectures to the aviation domain and evaluate their
transferability between Zurich (source) and Dublin (target) landing trajectory
datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying
amounts of local data, ranging from 0% to 100%. Results show that
diffusion-based models achieve competitive performance with as little as 5% of
the Dublin data and reach baseline-level performance around 20%, consistently
outperforming models trained from scratch across metrics and visual
inspections. Latent flow matching and latent diffusion models also benefit from
pretraining, though with more variable gains, while flow matching models show
weaker generalization. Despite challenges in capturing rare trajectory
patterns, these findings demonstrate the potential of transfer learning to
substantially reduce data requirements for trajectory generation in ATM,
enabling realistic synthetic data generation even in environments with limited
historical records.

</details>


### [106] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的纵向建模方法，用于处理异构电子健康记录（EHR）数据中的临床风险分类问题，解决了时间模式不规则、模态差异大和语义结构复杂等挑战。


<details>
  <summary>Details</summary>
Motivation: 临床风险分类中，异构EHR数据的不规则时间模式、大模态差异和复杂语义结构增加了建模难度，需要一种高效的方法来统一处理这些挑战。

Method: 采用特征嵌入层统一表示结构和非结构化数据，引入可学习的时间编码机制捕捉动态演化，使用多头自注意力结构建模全局依赖关系，并通过语义加权池化模块增强语义表示。

Result: 实验表明，该方法在准确性、召回率、精确度和F1分数上优于传统机器学习和时序深度学习模型，实现了多源异构EHR环境中的稳定精准风险识别。

Conclusion: 该方法为临床智能决策提供了一个高效可靠的框架，能够有效处理异构EHR数据并提升风险分类性能。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [107] [On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/abs/2511.04160)
*Laurits Fredsgaard,Mikkel N. Schmidt*

Main category: cs.LG

TL;DR: 研究了联合调优权重衰减、温度缩放和早停对深度集成模型的预测性能和不确定性量化的影响，并提出了一种部分重叠的保留策略。


<details>
  <summary>Details</summary>
Motivation: 探索联合调优是否能提升深度集成模型的性能，并解决数据利用与评估之间的权衡问题。

Method: 通过联合调优权重衰减、温度缩放和早停，并采用部分重叠的保留策略进行实验。

Result: 联合调优通常能匹配或提升性能，效果因任务和指标而异；部分重叠保留策略是实用解决方案。

Conclusion: 研究为优化深度集成模型提供了有价值的见解，部分重叠保留策略是实用且有效的。

Abstract: Deep ensembles are a powerful tool in machine learning, improving both model
performance and uncertainty calibration. While ensembles are typically formed
by training and tuning models individually, evidence suggests that jointly
tuning the ensemble can lead to better performance. This paper investigates the
impact of jointly tuning weight decay, temperature scaling, and early stopping
on both predictive performance and uncertainty quantification. Additionally, we
propose a partially overlapping holdout strategy as a practical compromise
between enabling joint evaluation and maximizing the use of data for training.
Our results demonstrate that jointly tuning the ensemble generally matches or
improves performance, with significant variation in effect size across
different tasks and metrics. We highlight the trade-offs between individual and
joint optimization in deep ensemble training, with the overlapping holdout
strategy offering an attractive practical solution. We believe our findings
provide valuable insights and guidance for practitioners looking to optimize
deep ensemble models. Code is available at:
https://github.com/lauritsf/ensemble-optimality-gap

</details>


### [108] [ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads](https://arxiv.org/abs/2511.04162)
*Xiaokai Wang,Shaoyuan Huang,Yuting Li,Xiaofei Wang*

Main category: cs.LG

TL;DR: ScaleDL是一种结合非线性分层建模和图神经网络的运行时预测框架，显著提高了DNN运行时预测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着DNN模型规模和复杂度的增加，传统运行时预测方法在准确性和泛化性上存在局限，且数据收集成本高。

Method: ScaleDL结合非线性分层建模和GNN的跨层交互机制，并使用D-optimal方法降低数据收集成本。

Result: 实验表明，ScaleDL在五种流行DNN模型上，MRE和RMSE分别降低了6倍和5倍。

Conclusion: ScaleDL在准确性、泛化性和数据收集成本之间取得了平衡，为DNN运行时预测提供了高效解决方案。

Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services,
supporting a wide range of applications, including autonomous driving,
chatbots, and recommendation systems. As models increase in size and
complexity, DNN workloads like training and inference tasks impose
unprecedented demands on distributed computing resources, making the accurate
prediction of runtime essential for optimizing development and resource
allocation. Traditional methods rely on additive computational unit models,
limiting their accuracy and generalizability. In contrast, graph-enhanced
modeling improves performance but significantly increases data collection
costs. Therefore, there is a critical need for a method that strikes a balance
between accuracy, generalizability, and the costs of data collection. To
address these challenges, we propose ScaleDL, a novel runtime prediction
framework that combines nonlinear layer-wise modeling with graph neural network
(GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime
prediction and hierarchical generalizability across different network
architectures. Additionally, we employ the D-optimal method to reduce data
collection costs. Experiments on the workloads of five popular DNN models prove
that ScaleDL enhances runtime prediction accuracy and generalizability,
achieving 6$\times$ lower MRE and 5$\times$ lower RMSE compared to baseline
models.

</details>


### [109] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 论文研究了MXFP4格式下的后训练量化（PTQ）方法，发现现有旋转方法不兼容MXFP4，并提出了一种改进策略。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）规模迅速增长导致内存、计算和能源成本高昂，MXFP4作为一种新的FP4格式出现，但现有PTQ方法是否适用尚不明确。

Method: 通过系统评估建立MXFP4格式下的PTQ方法基准，分析旋转方法与MXFP4的不兼容性，并提出块旋转策略。

Result: GPTQ方法表现良好，而旋转方法与MXFP4不兼容；提出的块旋转策略显著提高了准确性。

Conclusion: 研究为MXFP4格式下的PTQ提供了实践指导，并为低精度格式的研究奠定了基础。

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [110] [The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms](https://arxiv.org/abs/2511.04217)
*Hikari Otsuka,Daiki Chijiwa,Yasuyuki Okoshi,Daichi Fujiki,Susumu Takeuchi,Masato Motomura*

Main category: cs.LG

TL;DR: 论文研究了随机初始化的Transformer架构中是否存在高性能子网络（强彩票），并证明了多注意力头机制中强彩票的存在性。


<details>
  <summary>Details</summary>
Motivation: 当前强彩票假设理论尚未涵盖Transformer架构的核心组件——多头注意力机制，因此需要填补这一理论空白。

Method: 通过理论分析，证明了随机初始化的多头注意力机制在一定条件下包含强彩票，并扩展到无归一化层的Transformer。

Result: 理论证明和实验验证表明，源模型的隐藏维度增加时，强彩票与目标模型的近似误差呈指数下降。

Conclusion: 研究填补了强彩票假设在Transformer架构中的理论空白，并验证了其有效性。

Abstract: The strong lottery ticket hypothesis (SLTH) conjectures that high-performing
subnetworks, called strong lottery tickets (SLTs), are hidden in randomly
initialized neural networks. Although recent theoretical studies have
established the SLTH across various neural architectures, the SLTH for
transformer architectures still lacks theoretical understanding. In particular,
the current theory of the SLTH does not yet account for the multi-head
attention (MHA) mechanism, a core component of transformers. To address this
gap, we introduce a theoretical analysis of the existence of SLTs within MHAs.
We prove that, if a randomly initialized MHA of $H$ heads and input dimension
$d$ has the hidden dimension $O(d\log(Hd^{3/2}))$ for the key and value, it
contains an SLT that approximates an arbitrary MHA with the same input
dimension with high probability. Furthermore, by leveraging this theory for
MHAs, we extend the SLTH to transformers without normalization layers. We
empirically validate our theoretical findings, demonstrating that the
approximation error between the SLT within a source model (MHA and transformer)
and an approximate target counterpart decreases exponentially by increasing the
hidden dimension of the source model.

</details>


### [111] [seqme: a Python library for evaluating biological sequence design](https://arxiv.org/abs/2511.04239)
*Rasmus Møller-Larsen,Adam Izdebski,Jan Olszewski,Pankhil Gawade,Michal Kmicikiewicz,Wojciech Zarzecki,Ewa Szczurek*

Main category: cs.LG

TL;DR: seqme是一个用于评估生物序列设计方法的开源Python库，提供序列、嵌入和属性三类指标。


<details>
  <summary>Details</summary>
Motivation: 缺乏统一的软件库来评估生物序列设计方法的性能。

Method: 开发了seqme库，包含模型无关的指标，适用于多种生物序列。

Result: seqme支持多种序列类型，提供嵌入和属性模型，以及诊断和可视化功能。

Conclusion: seqme是一个模块化、可扩展的工具，适用于评估一次性或迭代设计方法。

Abstract: Recent advances in computational methods for designing biological sequences
have sparked the development of metrics to evaluate these methods performance
in terms of the fidelity of the designed sequences to a target distribution and
their attainment of desired properties. However, a single software library
implementing these metrics was lacking. In this work we introduce seqme, a
modular and highly extendable open-source Python library, containing
model-agnostic metrics for evaluating computational methods for biological
sequence design. seqme considers three groups of metrics: sequence-based,
embedding-based, and property-based, and is applicable to a wide range of
biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.
The library offers a number of embedding and property models for biological
sequences, as well as diagnostics and visualization functions to inspect the
results. seqme can be used to evaluate both one-shot and iterative
computational design methods.

</details>


### [112] [Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2511.04244)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: STELLE是一种神经符号框架，通过将时间序列嵌入到时间逻辑概念空间，实现分类与解释的统一。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分类中深度学习方法的黑盒问题，提供可理解的解释。

Method: 引入STL启发的核函数，将原始时间序列映射到预定义的STL公式对齐空间，联合优化准确性和可解释性。

Result: 在多种真实世界基准测试中，STELLE达到竞争性准确性并提供逻辑忠实的解释。

Conclusion: STELLE在保持高准确性的同时，提供了可解释的局部和全局解释。

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a novel approach, STELLE (Signal Temporal logic Embedding for
Logically-grounded Learning and Explanation), a neuro-symbolic framework that
unifies classification and explanation through direct embedding of trajectories
into a space of temporal logic concepts. By introducing a novel STL-inspired
kernel that maps raw time series to their alignment with predefined STL
formulae, our model jointly optimises accuracy and interpretability, as each
prediction is accompanied by the most relevant logical concepts that
characterise it. This yields (i) local explanations as human-readable STL
conditions justifying individual predictions, and (ii) global explanations as
class-characterising formulae. Experiments demonstrate that STELLE achieves
competitive accuracy while providing logically faithful explanations, validated
on diverse real-world benchmarks.

</details>


### [113] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: 提出了一种结合RLHF和PBO优势的混合框架，以提高偏好数据收集的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据收集成本高且耗时，需要更高效的学习方法。

Method: 在RLHF流程中集成主动查询模块，结合RLHF的可扩展性和PBO的样本效率。

Result: 在两个代表性领域（高维偏好优化和LLM微调）中，样本效率和整体性能均得到提升。

Conclusion: 混合框架有效结合了两种方法的优势，为偏好学习提供了更高效的解决方案。

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [114] [Differentially Private In-Context Learning with Nearest Neighbor Search](https://arxiv.org/abs/2511.04332)
*Antti Koskela,Tejas Kulkarni,Laith Zumot*

Main category: cs.LG

TL;DR: 提出了一种结合差分隐私的上下文学习方法，通过隐私感知的最近邻搜索提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了现代大语言模型流程中的相似性搜索，存在隐私风险。

Method: 采用最近邻检索结合隐私过滤器，跟踪累积隐私成本以确保差分隐私预算。

Result: 在文本分类和文档问答任务中显著优于现有基线。

Conclusion: 该方法在隐私-效用权衡上表现更优，适用于实际应用。

Abstract: Differentially private in-context learning (DP-ICL) has recently become an
active research topic due to the inherent privacy risks of in-context learning.
However, existing approaches overlook a critical component of modern large
language model (LLM) pipelines: the similarity search used to retrieve relevant
context data. In this work, we introduce a DP framework for in-context learning
that integrates nearest neighbor search of relevant examples in a privacy-aware
manner. Our method outperforms existing baselines by a substantial margin
across all evaluated benchmarks, achieving more favorable privacy-utility
trade-offs. To achieve this, we employ nearest neighbor retrieval from a
database of context data, combined with a privacy filter that tracks the
cumulative privacy cost of selected samples to ensure adherence to a central
differential privacy budget. Experimental results on text classification and
document question answering show a clear advantage of the proposed method over
existing baselines.

</details>


### [115] [LUME-DBN: Full Bayesian Learning of DBNs from Incomplete data in Intensive Care](https://arxiv.org/abs/2511.04333)
*Federico Pirola,Fabio Stella,Marco Grzegorczyk*

Main category: cs.LG

TL;DR: 提出了一种基于Gibbs采样的动态贝叶斯网络方法，用于处理临床数据中的缺失值，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分考虑临床数据的时序性，限制了不确定性量化，影响模型可信度。

Method: 采用Gibbs采样，将缺失值视为高斯分布的未知参数，通过条件分布采样进行填补和不确定性估计。

Result: 在模拟和真实重症监护数据上表现优于MICE等传统方法，重建精度更高。

Conclusion: 全贝叶斯推断能提升时序模型的可靠性，支持更安全的临床决策。

Abstract: Dynamic Bayesian networks (DBNs) are increasingly used in healthcare due to
their ability to model complex temporal relationships in patient data while
maintaining interpretability, an essential feature for clinical
decision-making. However, existing approaches to handling missing data in
longitudinal clinical datasets are largely derived from static Bayesian
networks literature, failing to properly account for the temporal nature of the
data. This gap limits the ability to quantify uncertainty over time, which is
particularly critical in settings such as intensive care, where understanding
the temporal dynamics is fundamental for model trustworthiness and
applicability across diverse patient groups. Despite the potential of DBNs, a
full Bayesian framework that integrates missing data handling remains
underdeveloped. In this work, we propose a novel Gibbs sampling-based method
for learning DBNs from incomplete data. Our method treats each missing value as
an unknown parameter following a Gaussian distribution. At each iteration, the
unobserved values are sampled from their full conditional distributions,
allowing for principled imputation and uncertainty estimation. We evaluate our
method on both simulated datasets and real-world intensive care data from
critically ill patients. Compared to standard model-agnostic techniques such as
MICE, our Bayesian approach demonstrates superior reconstruction accuracy and
convergence properties. These results highlight the clinical relevance of
incorporating full Bayesian inference in temporal models, providing more
reliable imputations and offering deeper insight into model behavior. Our
approach supports safer and more informed clinical decision-making,
particularly in settings where missing data are frequent and potentially
impactful.

</details>


### [116] [Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness](https://arxiv.org/abs/2511.04401)
*Subeen Park,Joowang Kim,Hakyung Lee,Sunjae Yoo,Kyungwoo Song*

Main category: cs.LG

TL;DR: SCER是一种通过正则化特征表示来抑制虚假相关性的方法，旨在提高模型在最差群体上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在分布偏移下表现不佳，尤其是在子群体偏移场景中，现有方法缺乏理论框架连接嵌入空间表示与最差群体误差。

Method: SCER通过理论约束嵌入空间，减少对虚假模式的依赖，鼓励模型关注核心特征。

Result: SCER在多个视觉和语言任务上优于现有方法，提高了最差群体准确率。

Conclusion: SCER通过理论驱动的嵌入正则化，有效提升了模型在分布偏移下的鲁棒性。

Abstract: Deep learning models achieve strong performance across various domains but
often rely on spurious correlations, making them vulnerable to distribution
shifts. This issue is particularly severe in subpopulation shift scenarios,
where models struggle in underrepresented groups. While existing methods have
made progress in mitigating this issue, their performance gains are still
constrained. They lack a rigorous theoretical framework connecting the
embedding space representations with worst-group error. To address this
limitation, we propose Spurious Correlation-Aware Embedding Regularization for
Worst-Group Robustness (SCER), a novel approach that directly regularizes
feature representations to suppress spurious cues. We show theoretically that
worst-group error is influenced by how strongly the classifier relies on
spurious versus core directions, identified from differences in group-wise mean
embeddings across domains and classes. By imposing theoretical constraints at
the embedding level, SCER encourages models to focus on core features while
reducing sensitivity to spurious patterns. Through systematic evaluation on
multiple vision and language, we show that SCER outperforms prior
state-of-the-art studies in worst-group accuracy. Our code is available at
\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.

</details>


### [117] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 研究发现当前LLM的不确定性量化方法在无歧义数据上表现良好，但在歧义数据上接近随机性能，揭示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在歧义语言环境下的不确定性量化问题，揭示现有方法的不足。

Method: 引入MAQA*和AmbigQA*数据集，评估不同不确定性估计方法在歧义数据上的表现。

Result: 现有方法在歧义数据上表现显著下降，理论分析揭示了其根本局限性。

Conclusion: 需重新思考当前LLM不确定性量化方法的建模范式。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [118] [On the Equivalence of Regression and Classification](https://arxiv.org/abs/2511.04422)
*Jayadeva,Naman Dwivedi,Hari Krishnan,N. M. Anoop Krishnan*

Main category: cs.LG

TL;DR: 论文通过建立回归与分类的等价性，提出了一种新的回归方法，并展示了如何利用这种等价性评估数据集的回归难度。


<details>
  <summary>Details</summary>
Motivation: 回归与分类之间的联系一直不明确，论文旨在通过等价性证明和新的回归方法填补这一空白。

Method: 通过将回归问题转化为线性可分类问题，利用边缘最大化技术提出新的回归公式。

Result: 提出了一个“可回归性”度量，用于评估数据集的回归难度，并展示了如何训练神经网络学习线性化映射。

Conclusion: 论文通过等价性证明了回归与分类的联系，并提出了一种新的回归方法和评估工具。

Abstract: A formal link between regression and classification has been tenuous. Even
though the margin maximization term $\|w\|$ is used in support vector
regression, it has at best been justified as a regularizer. We show that a
regression problem with $M$ samples lying on a hyperplane has a one-to-one
equivalence with a linearly separable classification task with $2M$ samples. We
show that margin maximization on the equivalent classification task leads to a
different regression formulation than traditionally used. Using the
equivalence, we demonstrate a ``regressability'' measure, that can be used to
estimate the difficulty of regressing a dataset, without needing to first learn
a model for it. We use the equivalence to train neural networks to learn a
linearizing map, that transforms input variables into a space where a linear
regressor is adequate.

</details>


### [119] [ForecastGAN: A Decomposition-Based Adversarial Framework for Multi-Horizon Time Series Forecasting](https://arxiv.org/abs/2511.04445)
*Syeda Sitara Wishal Fatima,Afshin Rahimi*

Main category: cs.LG

TL;DR: ForecastGAN是一种基于分解的对抗框架，用于多时间范围预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在短期预测中表现不佳且忽略分类特征。

Method: 通过分解模块、模型选择模块和对抗训练模块集成数值和分类特征。

Result: 在11个基准数据集上优于现有模型，尤其在短期预测中表现突出。

Conclusion: ForecastGAN提供了一种更通用的时间序列预测方法，适应性强且无需大量调参。

Abstract: Time series forecasting is essential across domains from finance to supply
chain management. This paper introduces ForecastGAN, a novel decomposition
based adversarial framework addressing limitations in existing approaches for
multi-horizon predictions. Although transformer models excel in long-term
forecasting, they often underperform in short-term scenarios and typically
ignore categorical features. ForecastGAN operates through three integrated
modules: a Decomposition Module that extracts seasonality and trend components;
a Model Selection Module that identifies optimal neural network configurations
based on forecasting horizon; and an Adversarial Training Module that enhances
prediction robustness through Conditional Generative Adversarial Network
training. Unlike conventional approaches, ForecastGAN effectively integrates
both numerical and categorical features. We validate our framework on eleven
benchmark multivariate time series datasets that span various forecasting
horizons. The results show that ForecastGAN consistently outperforms
state-of-the-art transformer models for short-term forecasting while remaining
competitive for long-term horizons. This research establishes a more
generalizable approach to time series forecasting that adapts to specific
contexts while maintaining strong performance across diverse data
characteristics without extensive hyperparameter tuning.

</details>


### [120] [Federated Stochastic Minimax Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2511.04456)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文研究了在联邦学习中非凸-PL极小极大优化问题下的重尾梯度噪声，提出了两种新算法Fed-NSGDA-M和FedMuon-DA，并证明了其收敛速率。


<details>
  <summary>Details</summary>
Motivation: 重尾噪声在非凸随机优化中越来越受关注，因其比标准有界方差假设更符合实际。

Method: 提出了两种算法：Fed-NSGDA-M（结合归一化梯度）和FedMuon-DA（利用Muon优化器进行局部更新）。

Result: 理论证明两种算法在较温和条件下达到$O({1}/{(TNp)^{rac{s-1}{2s}}})$的收敛速率。

Conclusion: 这是首个在重尾噪声下具有严格理论保证的联邦极小极大优化算法，实验验证了其有效性。

Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic
optimization, as numerous empirical studies suggest it offers a more realistic
assumption than standard bounded variance assumption. In this work, we
investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise
in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which
integrates normalized gradients, and FedMuon-DA, which leverages the Muon
optimizer for local updates. Both algorithms are designed to effectively
address heavy-tailed noise in federated minimax optimization, under a milder
condition. We theoretically establish that both algorithms achieve a
convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our
knowledge, these are the first federated minimax optimization algorithms with
rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments
further validate their effectiveness.

</details>


### [121] [Towards Causal Market Simulators](https://arxiv.org/abs/2511.04469)
*Dennis Thumm,Luis Ontaneda Mijares*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器和结构因果模型的时间序列神经因果模型（TNCM-VAE），用于生成具有因果关系的金融时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 现有市场生成器缺乏因果推理能力，无法支持反事实分析和风险评估。

Method: 通过变分自编码器和结构因果模型结合，在解码器架构中引入有向无环图约束，并使用因果Wasserstein距离进行训练。

Result: 在合成自回归模型上验证，反事实概率估计的L1距离低至0.03-0.10，优于基准。

Conclusion: 该模型支持金融压力测试、情景分析和增强的回测，生成的反事实市场轨迹符合因果机制。

Abstract: Market generators using deep generative models have shown promise for
synthetic financial data generation, but existing approaches lack causal
reasoning capabilities essential for counterfactual analysis and risk
assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that
combines variational autoencoders with structural causal models to generate
counterfactual financial time series while preserving both temporal
dependencies and causal relationships. Our approach enforces causal constraints
through directed acyclic graphs in the decoder architecture and employs the
causal Wasserstein distance for training. We validate our method on synthetic
autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating
superior performance in counterfactual probability estimation with L1 distances
as low as 0.03-0.10 compared to ground truth. The model enables financial
stress testing, scenario analysis, and enhanced backtesting by generating
plausible counterfactual market trajectories that respect underlying causal
mechanisms.

</details>


### [122] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: SynthKGQA是一个生成高质量合成知识图谱问答数据集的框架，用于评估和改进知识图谱检索方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以比较，因为缺乏具有真实目标的知识图谱检索挑战性QA数据集。

Method: 提出SynthKGQA框架，从任意知识图谱生成合成数据集，并提供完整的事实集用于推理。

Result: 生成GTSQA数据集，测试零样本泛化能力，并评估流行的KG增强LLM方法。

Conclusion: SynthKGQA不仅支持更有效的方法比较，还能训练更好的模型。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [123] [Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training](https://arxiv.org/abs/2511.04485)
*Ipsita Ghosh,Ethan Nguyen,Christian Kümmerle*

Main category: cs.LG

TL;DR: 提出了一种名为Q3R的低秩优化方法，用于在预训练和微调任务中保持低秩结构，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩优化方法在预训练任务中难以保持低秩结构和目标性能，需要一种更有效的方法。

Method: 基于二次加权秩正则化器（Q3R），通过平滑对数行列式作为秩替代目标，实现低秩训练。

Result: 在ViT-Tiny模型上，Q3R能够减少60%和80%的参数，仅导致1.3%和4%的准确率下降。

Conclusion: Q3R在图像和语言任务中均表现出色，适用于低秩预训练和微调。

Abstract: Parameter-efficient training, based on low-rank optimization, has become a
highly successful tool for fine-tuning large deep-learning models. However,
these methods fail at low-rank pre-training tasks where maintaining the
low-rank structure and the objective remains a challenging task. We propose the
Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel
low-rank inducing training strategy inspired by the iteratively reweighted
least squares (IRLS) framework. Q3R is based on a quadratic regularizer term
which majorizes a smoothed log determinant serving as rank surrogate objective.
Unlike other low-rank training techniques, Q3R is able to train weight matrices
with prescribed, low target ranks of models that achieve comparable predictive
performance as dense models, with small computational overhead, while remaining
fully compatible with existing architectures. For example, we demonstrated one
experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of
a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance
respectively. The efficacy of Q3R is confirmed on Transformers across both
image and language tasks, including for low-rank fine-tuning.

</details>


### [124] [Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks](https://arxiv.org/abs/2511.04494)
*Alper Kalle,Theo Rudkiewicz,Mohamed-Oumar Ouerfelli,Mohamed Tamaazousti*

Main category: cs.LG

TL;DR: 该论文提出了一种基于数据信息的神经网络压缩方法，通过优化函数空间误差而非权重空间误差，减少了压缩后的微调需求。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络压缩方法通常基于权重空间的各向同性范数（如Frobenius范数），而忽略了数据分布的影响。本文旨在通过数据信息优化压缩过程，减少对微调的依赖。

Method: 提出了一种新的交替最小二乘算法，用于Tucker-2和CPD张量分解，直接优化基于输入协方差矩阵的函数空间误差范数。

Result: 实验表明，该方法在多个CNN架构（如ResNet-18/50和GoogLeNet）和数据集（如ImageNet、FGVC-Aircraft等）上表现优异，无需微调即可达到竞争性精度。

Conclusion: 该方法不仅减少了压缩后的微调需求，还能在原始训练数据集不可用时，通过协方差范数迁移实现有效压缩。

Abstract: Neural networks are widely used for image-related tasks but typically demand
considerable computing power. Once a network has been trained, however, its
memory- and compute-footprint can be reduced by compression. In this work, we
focus on compression through tensorization and low-rank representations.
Whereas classical approaches search for a low-rank approximation by minimizing
an isotropic norm such as the Frobenius norm in weight-space, we use
data-informed norms that measure the error in function space. Concretely, we
minimize the change in the layer's output distribution, which can be expressed
as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is
the square root of the covariance matrix of the layer's input and $W$,
$\widetilde{W}$ are the original and compressed weights. We propose new
alternating least square algorithms for the two most common tensor
decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike
conventional compression pipelines, which almost always require
post-compression fine-tuning, our data-informed approach often achieves
competitive accuracy without any fine-tuning. We further show that the same
covariance-based norm can be transferred from one dataset to another with only
a minor accuracy drop, enabling compression even when the original training
dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50,
and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100)
confirm the advantages of the proposed method.

</details>


### [125] [Alternative Fairness and Accuracy Optimization in Criminal Justice](https://arxiv.org/abs/2511.04505)
*Shaolong Wu,James Blume,Geshi Yeung*

Main category: cs.LG

TL;DR: 论文提出了一种改进的群体公平性方法，通过加权误差损失最小化并在小范围内容忍假阴性率差异，以平衡公平性与预测准确性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决算法公平性在刑事司法中的关键概念冲突，并提出更实用的解决方案。

Method: 方法包括修改标准群体公平性，采用加权误差损失最小化，并限制假阴性率差异。

Result: 结果表明该方法能提高预测准确性，并明确错误成本的伦理选择。

Conclusion: 结论是提出了一个基于需求决策、透明度和定制化解决方案的实用框架，以提升技术设计的合法性和可操作性。

Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts
remain unsettled, especially in criminal justice. We review group, individual,
and process fairness and map the conditions under which they conflict. We then
develop a simple modification to standard group fairness. Rather than exact
parity across protected groups, we minimize a weighted error loss while keeping
differences in false negative rates within a small tolerance. This makes
solutions easier to find, can raise predictive accuracy, and surfaces the
ethical choice of error costs. We situate this proposal within three classes of
critique: biased and incomplete data, latent affirmative action, and the
explosion of subgroup constraints. Finally, we offer a practical framework for
deployment in public decision systems built on three pillars: need-based
decisions, Transparency and accountability, and narrowly tailored definitions
and solutions. Together, these elements link technical design to legitimacy and
provide actionable guidance for agencies that use risk assessment and related
tools.

</details>


### [126] [Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers](https://arxiv.org/abs/2511.04514)
*C. Hepburn,T. Zielke,A. P. Raulf*

Main category: cs.LG

TL;DR: 研究了线性模式连接性（LMC）在数据偏移下的表现，发现小学习率和大批量可减轻其影响，并探讨了LMC在训练效率和模型多样性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 探索LMC在数据偏移下的行为及其对训练稳定性、局部最小值平滑性和泛化能力的影响。

Method: 通过实验研究LMC在数据偏移下的表现，分析学习率和批量大小对模型收敛的影响。

Result: 小学习率和大批量可减轻数据偏移的影响，LMC模型倾向于产生相似错误，但能平衡训练效率和多样性。

Conclusion: LMC在数据偏移下仍能保持训练效率与模型多样性的平衡，为深度学习优化提供了新视角。

Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of
deep learning, including training stability under noisy stochastic gradients,
the smoothness and generalization of local minima (basins), the similarity and
functional diversity of sampled models, and architectural effects on data
processing. In this work, we experimentally study LMC under data shifts and
identify conditions that mitigate their impact. We interpret data shifts as an
additional source of stochastic gradient noise, which can be reduced through
small learning rates and large batch sizes. These parameters influence whether
models converge to the same local minimum or to regions of the loss landscape
with varying smoothness and generalization. Although models sampled via LMC
tend to make similar errors more frequently than those converging to different
basins, the benefit of LMC lies in balancing training efficiency against the
gains achieved from larger, more diverse ensembles. Code and supplementary
materials will be made publicly available at https://github.com/DLR-KI/LMC in
due course.

</details>


### [127] [Comparing EPGP Surrogates and Finite Elements Under Degree-of-Freedom Parity](https://arxiv.org/abs/2511.04518)
*Obed Amo,Samit Ghosh,Markus Lange-Hegermann,Bogdan Raiţă,Michael Pokojovy*

Main category: cs.LG

TL;DR: 比较B-EPGP和CN-FEM在二维波动方程求解中的性能，B-EPGP在误差上表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究不同方法在求解波动方程时的性能差异，验证B-EPGP的优越性。

Method: B-EPGP使用指数多项式基和惩罚最小二乘法，CN-FEM结合有限元法和Crank-Nicolson时间步进。

Result: 在匹配自由度下，B-EPGP的空间-时间L2误差和最大时间L2误差均优于CN-FEM，精度提高约两个数量级。

Conclusion: B-EPGP在求解波动方程时比传统CN-FEM更精确。

Abstract: We present a new benchmarking study comparing a boundary-constrained
Ehrenpreis--Palamodov Gaussian Process (B-EPGP) surrogate with a classical
finite element method combined with Crank--Nicolson time stepping (CN-FEM) for
solving the two-dimensional wave equation with homogeneous Dirichlet boundary
conditions. The B-EPGP construction leverages exponential-polynomial bases
derived from the characteristic variety to enforce the PDE and boundary
conditions exactly and employs penalized least squares to estimate the
coefficients. To ensure fairness across paradigms, we introduce a
degrees-of-freedom (DoF) matching protocol. Under matched DoF, B-EPGP
consistently attains lower space-time $L^2$-error and maximum-in-time
$L^{2}$-error in space than CN-FEM, improving accuracy by roughly two orders of
magnitude.

</details>


### [128] [End-to-End Reinforcement Learning of Koopman Models for eNMPC of an Air Separation Unit](https://arxiv.org/abs/2511.04522)
*Daniel Mayfrank,Kayra Dernek,Laura Lang,Alexander Mitsos,Manuel Dahmen*

Main category: cs.LG

TL;DR: 基于强化学习的Koopman代理模型在大型空气分离单元中表现优异，避免约束违规。


<details>
  <summary>Details</summary>
Motivation: 验证强化学习方法在更具挑战性的大规模需求响应案例中的扩展性。

Method: 使用强化学习训练Koopman代理模型，应用于非线性模型预测控制。

Result: 相比传统方法，新方法在保持经济性能的同时避免了约束违规。

Conclusion: 该方法在大规模工业应用中具有潜力。

Abstract: With our recently proposed method based on reinforcement learning (Mayfrank
et al. (2024), Comput. Chem. Eng. 190), Koopman surrogate models can be trained
for optimal performance in specific (economic) nonlinear model predictive
control ((e)NMPC) applications. So far, our method has exclusively been
demonstrated on a small-scale case study. Herein, we show that our method
scales well to a more challenging demand response case study built on a
large-scale model of a single-product (nitrogen) air separation unit. Across
all numerical experiments, we assume observability of only a few realistically
measurable plant variables. Compared to a purely system identification-based
Koopman eNMPC, which generates small economic savings but frequently violates
constraints, our method delivers similar economic performance while avoiding
constraint violations.

</details>


### [129] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: 提出了一种模型无关的后验不确定性量化框架，用于潜在空间降阶模型，无需修改架构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于特定架构或训练方式，限制了灵活性和泛化能力。

Method: 使用保形预测方法，对潜在动力学、重构和端到端预测等多个组件进行统计预测区间估计。

Result: 在云微物理学的潜在空间动力学模型中，该方法准确预测了液滴大小分布的演化，并量化了整个ROM流程中的不确定性。

Conclusion: 该方法为降阶模型提供了一种灵活且通用的不确定性量化解决方案。

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


### [130] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 提出了一种新的图模型RGP，结合时空依赖和多任务学习，在关系数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注空间结构，忽视时间信息，且多为单任务设计，无法满足复杂关系数据的建模需求。

Method: 引入时间子图采样器捕获全局上下文，并提出RGP图变换器架构，利用交叉注意力瓶颈整合时空信息。

Result: 在RelBench、SALT和CTU数据集上，RGP实现了最先进的性能。

Conclusion: RGP为关系深度学习提供了通用且可扩展的解决方案，支持多样化预测任务。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [131] [ARETE: an R package for Automated REtrieval from TExt with large language models](https://arxiv.org/abs/2511.04573)
*Vasco V. Branco,Jandó Benedek,Lidia Pivovarova,Luís Correia,Pedro Cardoso*

Main category: cs.LG

TL;DR: ARETE R包利用大语言模型自动提取物种出现数据，显著提升数据获取效率，扩展已知物种分布范围。


<details>
  <summary>Details</summary>
Motivation: 缺乏关键物种数据（尤其是出现数据）阻碍了保护计划的实施，且现有文献数据非机器可读，需大量人工处理。

Method: 开发ARETE R包，整合OCR、异常检测等步骤，利用chatGPT API自动化数据提取与验证。

Result: 通过100种蜘蛛的测试，新数据使已知分布范围平均扩大三个数量级，揭示新的历史分布区。

Conclusion: ARETE能快速获取未开发数据，优化资源分配，为保护规划和风险评估提供新工具。

Abstract: 1. A hard stop for the implementation of rigorous conservation initiatives is
our lack of key species data, especially occurrence data. Furthermore,
researchers have to contend with an accelerated speed at which new information
must be collected and processed due to anthropogenic activity. Publications
ranging from scientific papers to gray literature contain this crucial
information but their data are often not machine-readable, requiring extensive
human work to be retrieved. 2. We present the ARETE R package, an open-source
software aiming to automate data extraction of species occurrences powered by
large language models, namely using the chatGPT Application Programming
Interface. This R package integrates all steps of the data extraction and
validation process, from Optical Character Recognition to detection of outliers
and output in tabular format. Furthermore, we validate ARETE through systematic
comparison between what is modelled and the work of human annotators. 3. We
demonstrate the usefulness of the approach by comparing range maps produced
using GBIF data and with those automatically extracted for 100 species of
spiders. Newly extracted data allowed to expand the known Extent of Occurrence
by a mean three orders of magnitude, revealing new areas where the species were
found in the past, which mayhave important implications for spatial
conservation planning and extinction risk assessments. 4. ARETE allows faster
access to hitherto untapped occurrence data, a potential game changer in
projects requiring such data. Researchers will be able to better prioritize
resources, manually verifying selected species while maintaining automated
extraction for the majority. This workflow also allows predicting available
bibliographic data during project planning.

</details>


### [132] [Complexity as Advantage: A Regret-Based Perspective on Emergent Structure](https://arxiv.org/abs/2511.04590)
*Oshri Naparstek*

Main category: cs.LG

TL;DR: CAA框架通过观察者的预测遗憾来定义系统复杂性，认为复杂性源于不同观察者的信息优势差异。


<details>
  <summary>Details</summary>
Motivation: 传统复杂性度量常忽略观察者视角，CAA旨在通过观察者依赖的预测遗憾来统一多种涌现行为概念。

Method: 提出CAA框架，通过观察者对系统的预测遗憾差异来量化复杂性，并结合动态模型进行验证。

Result: CAA统一了多尺度熵、预测信息等涌现行为概念，表明“有趣”系统能产生观察者间的差异化遗憾。

Conclusion: CAA为复杂性功能价值提供量化基础，对学习、进化和人工代理等领域有潜在启示。

Abstract: We introduce Complexity as Advantage (CAA), a framework that defines the
complexity of a system relative to a family of observers. Instead of measuring
complexity as an intrinsic property, we evaluate how much predictive regret a
system induces for different observers attempting to model it. A system is
complex when it is easy for some observers and hard for others, creating an
information advantage. We show that this formulation unifies several notions of
emergent behavior, including multiscale entropy, predictive information, and
observer-dependent structure. The framework suggests that "interesting" systems
are those positioned to create differentiated regret across observers,
providing a quantitative grounding for why complexity can be functionally
valuable. We demonstrate the idea through simple dynamical models and discuss
implications for learning, evolution, and artificial agents.

</details>


### [133] [Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems](https://arxiv.org/abs/2511.04594)
*Utkarsh U. Chavan,Prashant Trivedi,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: 论文研究了多智能体随机最短路径问题（Dec-MASSPs）的学习复杂性，提出了首个基于线性函数近似的遗憾下界。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）在去中心化控制中具有广泛应用，但多智能体随机最短路径问题的学习问题尚未充分探索。

Method: 通过线性函数近似和对称性分析，识别最优策略结构，并构建难以学习的实例。

Result: 提出了遗憾下界Ω(√K)，揭示了Dec-MASSPs中的学习困难。

Conclusion: 研究结果阐明了去中心化控制的学习复杂性，为多智能体系统的高效学习算法设计提供了指导。

Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics
and traffic routing, where agents must coordinate in a decentralized manner to
achieve a common objective. Stochastic Shortest Path (SSP) problems provide a
natural framework for modeling decentralized control in such settings. While
the problem of learning in SSP has been extensively studied in single-agent
settings, the decentralized multi-agent variant remains largely unexplored. In
this work, we take a step towards addressing that gap. We study decentralized
multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the
transition dynamics and costs are represented using linear models. Applying
novel symmetry-based arguments, we identify the structure of optimal policies.
Our main contribution is the first regret lower bound for this setting based on
the construction of hard-to-learn instances for any number of agents, $n$. Our
regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the
inherent learning difficulty in Dec-MASSPs. These insights clarify the learning
complexity of decentralized control and can further guide the design of
efficient learning algorithms in multi-agent systems.

</details>


### [134] [Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning](https://arxiv.org/abs/2511.04598)
*Hampus Åström,Elin Anna Topp,Jacek Malec*

Main category: cs.LG

TL;DR: 将常规强化学习环境转化为目标条件环境，使代理能够自主且无需奖励地学习任务。


<details>
  <summary>Details</summary>
Motivation: 研究如何让代理在无外部奖励的情况下自主选择目标并学习任务。

Method: 将环境转化为目标条件环境，代理自主选择目标，方法独立于底层离策略学习算法。

Result: 平均目标成功率提高并稳定，代理能学习环境中任何观察到的目标。

Conclusion: 该方法支持代理在特定用例前进行通用训练，但单个目标性能可能不稳定。

Abstract: In this paper we study how transforming regular reinforcement learning
environments into goal-conditioned environments can let agents learn to solve
tasks autonomously and reward-free. We show that an agent can learn to solve
tasks by selecting its own goals in an environment-agnostic way, at training
times comparable to externally guided reinforcement learning. Our method is
independent of the underlying off-policy learning algorithm. Since our method
is environment-agnostic, the agent does not value any goals higher than others,
leading to instability in performance for individual goals. However, in our
experiments, we show that the average goal success rate improves and
stabilizes. An agent trained with this method can be instructed to seek any
observations made in the environment, enabling generic training of agents prior
to specific use cases.

</details>


### [135] [Addressing divergent representations from causal interventions on neural networks](https://arxiv.org/abs/2511.04638)
*Satchel Grant,Simon Jerome Han,Alexa Tartaglini,Christopher Potts*

Main category: cs.LG

TL;DR: 论文探讨了因果干预技术在模型解释性中可能产生的分布偏移问题，并提出了理论分析和改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证因果干预是否会导致模型内部表示偏离自然分布，从而影响解释的准确性。

Method: 通过实证展示常见干预技术的分布偏移，理论分析无害和有害的偏移，并改进CL损失以减少有害偏移。

Result: 发现干预常导致分布偏移，改进的CL损失能有效减少有害偏移，保持解释能力。

Conclusion: 研究为开发更可靠的模型解释方法提供了理论和实践基础。

Abstract: A common approach to mechanistic interpretability is to causally manipulate
model representations via targeted interventions in order to understand what
those representations encode. Here we ask whether such interventions create
out-of-distribution (divergent) representations, and whether this raises
concerns about how faithful their resulting explanations are to the target
model in its natural state. First, we demonstrate empirically that common
causal intervention techniques often do shift internal representations away
from the natural distribution of the target model. Then, we provide a
theoretical analysis of two classes of such divergences: `harmless' divergences
that occur in the null-space of the weights and from covariance within
behavioral decision boundaries, and `pernicious' divergences that activate
hidden network pathways and cause dormant behavioral changes. Finally, in an
effort to mitigate the pernicious cases, we modify the Counterfactual Latent
(CL) loss from Grant (2025) that regularizes interventions to remain closer to
the natural distributions, reducing the likelihood of harmful divergences while
preserving the interpretive power of interventions. Together, these results
highlight a path towards more reliable interpretability methods.

</details>


### [136] [Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems](https://arxiv.org/abs/2511.04641)
*Hans Harder,Abhijeet Vishwasrao,Luca Guastoni,Ricardo Vinuesa,Sebastian Peitz*

Main category: cs.LG

TL;DR: 论文研究了用于预测偏微分方程描述的动力系统的概率技术，比较了多种减少采样步骤的流匹配范式扩展方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何高效预测复杂动力系统，特别是减少采样步骤以提高效率。

Method: 比较了直接蒸馏、渐进蒸馏、对抗扩散蒸馏、Wasserstein GANs和修正流等方法，并在挑战性系统上进行了实验。

Result: 实验结果表明这些方法在减少采样步骤方面有效，特别是在预测大规模3D模拟的2D切片方面。

Conclusion: 研究为高效生成流入解算器的数据提供了新途径，展示了流匹配范式扩展的潜力。

Abstract: This paper is concerned with probabilistic techniques for forecasting
dynamical systems described by partial differential equations (such as, for
example, the Navier-Stokes equations). In particular, it is investigating and
comparing various extensions to the flow matching paradigm that reduce the
number of sampling steps. In this regard, it compares direct distillation,
progressive distillation, adversarial diffusion distillation, Wasserstein GANs
and rectified flows. Moreover, experiments are conducted on a set of
challenging systems. In particular, we also address the challenge of directly
predicting 2D slices of large-scale 3D simulations, paving the way for
efficient inflow generation for solvers.

</details>


### [137] [Optimal Inference Schedules for Masked Diffusion Models](https://arxiv.org/abs/2511.04647)
*Sitan Chen,Kevin Cong,Jerry Li*

Main category: cs.LG

TL;DR: 论文研究了扩散语言模型（如MDM）的并行采样能力，提出了新的理论框架和上下界，展示了在某些自然场景下可实现高效并行采样。


<details>
  <summary>Details</summary>
Motivation: 标准自回归语言模型的推理过程是顺序的，导致推理时间长且成本高。扩散语言模型（如MDM）支持并行采样，但其性能与并行采样能力的关系尚不明确。

Method: 通过建立与单变量函数逼近理论的联系，提出新的理论框架，精确刻画采样分布与真实分布的差异，并推导出新的上下界。

Result: 证明了在某些自然分布下，可在O(log n)步内实现高效并行采样，且性能无明显损失。

Conclusion: 论文为扩散语言模型的并行采样提供了理论支持，并展示了其在实际应用中的潜力。

Abstract: A major bottleneck of standard auto-regressive large language models is that
their inference process is inherently sequential, resulting in very long and
costly inference times. To circumvent this, practitioners proposed a class of
language models called diffusion language models, of which the masked diffusion
model (MDM) is the most successful. The MDM is able to sample tokens
out-of-order and, ostensibly, many tokens at once and in parallel. However,
there is very limited rigorous understanding of how much parallel sampling
these models can perform without noticeable degradation in their sampling
performance. Prior work of Li and Cai obtained some preliminary bounds, but
these are not tight for many natural classes of distributions. In this work, we
give a new, exact characterization of the expected divergence between the true
distribution and the sampled distribution, for any distribution and any
unmasking schedule for the sampler, showing an elegant connection to the theory
of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and
upper bounds for this problem. While the connection to function approximation
in principle gives the optimal unmasking schedule for any distribution, we show
that it is in general impossible to compete with it without strong a priori
knowledge of the distribution, even in seemingly benign settings. However, we
also demonstrate new upper bounds and new sampling schedules in terms of
well-studied information-theoretic properties of the base distribution, namely,
its total correlation and dual total correlation, which show that in some
natural settings, one can sample in $O(log n)$ steps without any visible loss
in performance, where $n$ is the total sequence length.

</details>


### [138] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 论文提出了一种自适应模型剪枝方法，用于无线时间触发联邦学习（TT-Fed），通过联合优化剪枝率和带宽分配，减少通信成本40%且保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在数据隐私方面有优势，但无线带宽有限导致通信开销和延迟问题。时间触发联邦学习（TT-Fed）虽灵活，但仍需解决这些问题。

Method: 提出自适应模型剪枝方法，基于收敛分析联合优化剪枝率和带宽分配，利用KKT条件求解闭式解。

Result: 仿真显示，模型剪枝可减少40%通信成本，同时保持模型性能不变。

Conclusion: 自适应模型剪枝有效解决了TT-Fed中的通信开销问题，为联邦学习的实际应用提供了可行方案。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [139] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种结合物理约束和数据驱动的三维降水临近预报框架，显著提升了预报精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有降水临近预报方法在时空分辨率和预报时间上存在局限，无法准确捕捉快速演变的对流现象。

Method: 采用灰盒方法，结合三维雷达反射率数据，利用物理约束的神经算子和数据驱动学习，学习垂直变化的3D平流场和空间变化的扩散参数。

Result: 在盲评中，该框架在57%的情况下排名第一，预报精度显著提升，最长预报时间可达三小时。

Conclusion: 通过恢复三维动力学并保持物理一致性，该框架为极端降水临近预报提供了可扩展且可靠的解决方案。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


### [140] [Forgetting is Everywhere](https://arxiv.org/abs/2511.04666)
*Ben Sanati,Thomas L. Lee,Trevor McInroe,Aidan Scannell,Nikolay Malkin,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 论文提出了一种与算法和任务无关的理论，将遗忘定义为学习者在未来经验预测分布中缺乏自一致性，并提出了一种通用的遗忘度量方法。


<details>
  <summary>Details</summary>
Motivation: 解决学习算法在适应新数据时遗忘过去知识的根本挑战，缺乏统一的遗忘定义阻碍了对学习动态的理解。

Method: 提出了一种算法和任务无关的理论，将遗忘与预测信息损失联系起来，并设计了跨分类、回归、生成建模和强化学习的实验验证。

Result: 实验证明遗忘在所有学习设置中都存在，并对学习效率有显著影响。

Conclusion: 研究为遗忘提供了理论基础，为分析和改进通用学习算法的信息保留能力奠定了基础。

Abstract: A fundamental challenge in developing general learning algorithms is their
tendency to forget past knowledge when adapting to new data. Addressing this
problem requires a principled understanding of forgetting; yet, despite decades
of study, no unified definition has emerged that provides insights into the
underlying dynamics of learning. We propose an algorithm- and task-agnostic
theory that characterises forgetting as a lack of self-consistency in a
learner's predictive distribution over future experiences, manifesting as a
loss of predictive information. Our theory naturally yields a general measure
of an algorithm's propensity to forget. To validate the theory, we design a
comprehensive set of experiments that span classification, regression,
generative modelling, and reinforcement learning. We empirically demonstrate
how forgetting is present across all learning settings and plays a significant
role in determining learning efficiency. Together, these results establish a
principled understanding of forgetting and lay the foundation for analysing and
improving the information retention capabilities of general learning
algorithms.

</details>


### [141] [Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches](https://arxiv.org/abs/2511.04667)
*Julian D. Allagan,Dasia A. Singleton,Shanae N. Perry,Gabrielle C. Morgan,Essence A. Morgan*

Main category: cs.LG

TL;DR: 多方法框架评估数学分班考试，结合经典测试理论、机器学习和无监督聚类，发现部分题目区分度不足，机器学习表现优异，聚类分析揭示潜在分类问题，提出优化建议。


<details>
  <summary>Details</summary>
Motivation: 评估数学分班考试的有效性，通过多方法整合提供更全面的分析，优化分班决策。

Method: 结合经典测试理论、机器学习（随机森林和梯度提升）和无监督聚类（K-means）分析198名学生的40题考试数据。

Result: 55%题目区分度优秀，30%需替换；机器学习准确率达97.5%；聚类发现自然二分结构，与机构阈值不符。

Conclusion: 多方法整合为数学分班优化提供实证基础，建议替换低区分度题目、采用两阶段评估并整合机器学习预测。

Abstract: This study evaluates a 40-item mathematics placement examination administered
to 198 students using a multi-method framework combining Classical Test Theory,
machine learning, and unsupervised clustering. Classical Test Theory analysis
reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)
while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement.
Question 6 (Graph Interpretation) emerges as the examination's most powerful
discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA
F-statistic ($F = 4609.1$), and maximum Random Forest feature importance
(0.206), accounting for 20.6\% of predictive power. Machine learning algorithms
demonstrate exceptional performance, with Random Forest and Gradient Boosting
achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering
identifies a natural binary competency structure with a boundary at 42.5\%,
diverging from the institutional threshold of 55\% and suggesting potential
overclassification into remedial categories. The two-cluster solution exhibits
exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster
purity. Convergent evidence across methods supports specific refinements:
replace poorly discriminating items, implement a two-stage assessment, and
integrate Random Forest predictions with transparency mechanisms. These
findings demonstrate that multi-method integration provides a robust empirical
foundation for evidence-based mathematics placement optimization.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [142] [Dynamic Shape Control of Soft Robots Enabled by Data-Driven Model Reduction](https://arxiv.org/abs/2511.03931)
*Iman Adibnazari,Harsh Sharma,Myungsun Park,Jacobo Cervera-Torralba,Boris Kramer,Michael T. Tolley*

Main category: cs.RO

TL;DR: 比较了三种数据驱动的模型降维技术（ERA、DMDc、LOpInf）在软体机器人动态形状控制中的效果，LOpInf表现最佳。


<details>
  <summary>Details</summary>
Motivation: 软体机器人动态形状控制需要高维动力学模型，但缺乏通用建模工具。

Method: 使用ERA、DMDc和LOpInf三种方法生成线性模型，并在模拟实验中评估其性能。

Result: 在所有实验中，基于LOpInf的策略跟踪误差最小。

Conclusion: LOpInf方法在软体机器人动态形状控制中表现最优。

Abstract: Soft robots have shown immense promise in settings where they can leverage
dynamic control of their entire bodies. However, effective dynamic shape
control requires a controller that accounts for the robot's high-dimensional
dynamics--a challenge exacerbated by a lack of general-purpose tools for
modeling soft robots amenably for control. In this work, we conduct a
comparative study of data-driven model reduction techniques for generating
linear models amendable to dynamic shape control. We focus on three
methods--the eigensystem realization algorithm, dynamic mode decomposition with
control, and the Lagrangian operator inference (LOpInf) method. Using each
class of model, we explored their efficacy in model predictive control policies
for the dynamic shape control of a simulated eel-inspired soft robot in three
experiments: 1) tracking simulated reference trajectories guaranteed to be
feasible, 2) tracking reference trajectories generated from a biological model
of eel kinematics, and 3) tracking reference trajectories generated by a
reduced-scale physical analog. In all experiments, the LOpInf-based policies
generated lower tracking errors than policies based on other models.

</details>


### [143] [Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots](https://arxiv.org/abs/2511.03996)
*Yushi Wang,Changsheng Luo,Penghui Chen,Jianran Liu,Weijian Sun,Tong Guo,Kechang Yang,Biao Hu,Yangang Zhang,Mingguo Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于强化学习的统一控制器，通过视觉感知与运动控制的直接集成，使人形机器人能够获得反应式足球技能。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常依赖解耦模块，导致动态环境中响应延迟和行为不连贯，而现实世界的感知限制进一步加剧了这些问题。

Method: 扩展了Adversarial Motion Priors到现实动态环境的感知设置，结合编码器-解码器架构和虚拟感知系统，建模现实视觉特征。

Result: 控制器表现出强反应性，在各种场景（包括真实RoboCup比赛）中执行连贯且稳健的足球行为。

Conclusion: 该方法成功实现了感知与动作的主动协调，解决了动态环境中的响应和行为一致性问题。

Abstract: Humanoid soccer poses a representative challenge for embodied intelligence,
requiring robots to operate within a tightly coupled perception-action loop.
However, existing systems typically rely on decoupled modules, resulting in
delayed responses and incoherent behaviors in dynamic environments, while
real-world perceptual limitations further exacerbate these issues. In this
work, we present a unified reinforcement learning-based controller that enables
humanoid robots to acquire reactive soccer skills through the direct
integration of visual perception and motion control. Our approach extends
Adversarial Motion Priors to perceptual settings in real-world dynamic
environments, bridging motion imitation and visually grounded dynamic control.
We introduce an encoder-decoder architecture combined with a virtual perception
system that models real-world visual characteristics, allowing the policy to
recover privileged states from imperfect observations and establish active
coordination between perception and action. The resulting controller
demonstrates strong reactivity, consistently executing coherent and robust
soccer behaviors across various scenarios, including real RoboCup matches.

</details>


### [144] [Integrating Ergonomics and Manipulability for Upper Limb Postural Optimization in Bimanual Human-Robot Collaboration](https://arxiv.org/abs/2511.04009)
*Chenzui Li,Yiming Chen,Xi Wu,Giacinto Barresi,Fei Chen*

Main category: cs.RO

TL;DR: 提出了一种上肢姿势优化方法，用于提升双人-机器人协作任务中的物理人机工程和力操纵性。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常关注人类安全或操纵效率，而本文方法独特地整合了这两方面，以增强不同条件下的协作能力。

Method: 通过最小化成本函数优化简化人体骨骼模型的关节角度，优先考虑安全和操纵能力，并通过变换模块生成机器人末端执行器参考位姿。

Result: 实验结果表明，优化后目标肌肉的激活情况显著改善。

Conclusion: 该方法在人类-人类协作和人类-机器人协作中均表现出显著效果。

Abstract: This paper introduces an upper limb postural optimization method for
enhancing physical ergonomics and force manipulability during bimanual
human-robot co-carrying tasks. Existing research typically emphasizes human
safety or manipulative efficiency, whereas our proposed method uniquely
integrates both aspects to strengthen collaboration across diverse conditions
(e.g., different grasping postures of humans, and different shapes of objects).
Specifically, the joint angles of a simplified human skeleton model are
optimized by minimizing the cost function to prioritize safety and manipulative
capability. To guide humans towards the optimized posture, the reference
end-effector poses of the robot are generated through a transformation module.
A bimanual model predictive impedance controller (MPIC) is proposed for our
human-like robot, CURI, to recalibrate the end effector poses through planned
trajectories. The proposed method has been validated through various subjects
and objects during human-human collaboration (HHC) and human-robot
collaboration (HRC). The experimental results demonstrate significant
improvement in muscle conditions by comparing the activation of target muscles
before and after optimization.

</details>


### [145] [An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue](https://arxiv.org/abs/2511.04042)
*Kailun Ji,Xiaoyu Hu,Xinyu Zhang,Jun Chen*

Main category: cs.RO

TL;DR: 论文提出了一种基于LLM-CRF的系统，通过自然交互和LLM认知引擎，显著提升无人机群在搜救任务中的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决无人机群在复杂搜救任务中的人机协作瓶颈，即‘意图到行动’的转换问题。

Method: 利用LLM作为认知引擎，通过多模态交互捕获操作员意图，并进行任务分解和规划。

Result: 实验显示，任务完成时间减少64.2%，成功率提高7%，认知负荷降低42.9%。

Conclusion: LLM在高压场景下能显著提升人机协作的直观性和有效性。

Abstract: Large-scale disaster Search And Rescue (SAR) operations are persistently
challenged by complex terrain and disrupted communications. While Unmanned
Aerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area
search and supply delivery, yet their effective coordination places a
significant cognitive burden on human operators. The core human-machine
collaboration bottleneck lies in the ``intention-to-action gap'', which is an
error-prone process of translating a high-level rescue objective into a
low-level swarm command under high intensity and pressure. To bridge this gap,
this study proposes a novel LLM-CRF system that leverages Large Language Models
(LLMs) to model and augment human-swarm teaming cognition. The proposed
framework initially captures the operator's intention through natural and
multi-modal interactions with the device via voice or graphical annotations. It
then employs the LLM as a cognitive engine to perform intention comprehension,
hierarchical task decomposition, and mission planning for the UAV swarm. This
closed-loop framework enables the swarm to act as a proactive partner,
providing active feedback in real-time while reducing the need for manual
monitoring and control, which considerably advances the efficacy of the SAR
task. We evaluate the proposed framework in a simulated SAR scenario.
Experimental results demonstrate that, compared to traditional order and
command-based interfaces, the proposed LLM-driven approach reduced task
completion time by approximately $64.2\%$ and improved task success rate by
$7\%$. It also leads to a considerable reduction in subjective cognitive
workload, with NASA-TLX scores dropping by $42.9\%$. This work establishes the
potential of LLMs to create more intuitive and effective human-swarm
collaborations in high-stakes scenarios.

</details>


### [146] [Enhancing Fault-Tolerant Space Computing: Guidance Navigation and Control (GNC) and Landing Vision System (LVS) Implementations on Next-Gen Multi-Core Processors](https://arxiv.org/abs/2511.04052)
*Kyongsik Yun,David Bayard,Gerik Kubiak,Austin Owens,Andrew Johnson,Ryan Johnson,Dan Scharf,Thomas Lu*

Main category: cs.RO

TL;DR: 论文评估了在多核处理器上部署GNC和LVS算法的性能，展示了显著的加速效果，并提出了ARBITER机制以确保计算可靠性。


<details>
  <summary>Details</summary>
Motivation: 未来行星探索任务需要高性能、容错的计算能力，以支持自主的GNC和LVS操作。

Method: 在多核处理器（HPSC、Snapdragon VOXL2、AMD Xilinx Versal）上部署算法，并开发ARBITER机制进行实时故障检测和纠正。

Result: LVS图像处理速度提升15倍，GFOLD轨迹优化速度提升250倍以上。ARBITER在静态和动态任务中均验证有效。

Conclusion: 该研究为未来任务提供了可扩展且高效的架构，支持低延迟和容错能力。

Abstract: Future planetary exploration missions demand high-performance, fault-tolerant
computing to enable autonomous Guidance, Navigation, and Control (GNC) and
Lander Vision System (LVS) operations during Entry, Descent, and Landing (EDL).
This paper evaluates the deployment of GNC and LVS algorithms on
next-generation multi-core processors--HPSC, Snapdragon VOXL2, and AMD Xilinx
Versal--demonstrating up to 15x speedup for LVS image processing and over 250x
speedup for Guidance for Fuel-Optimal Large Divert (GFOLD) trajectory
optimization compared to legacy spaceflight hardware. To ensure computational
reliability, we present ARBITER (Asynchronous Redundant Behavior Inspection for
Trusted Execution and Recovery), a Multi-Core Voting (MV) mechanism that
performs real-time fault detection and correction across redundant cores.
ARBITER is validated in both static optimization tasks (GFOLD) and dynamic
closed-loop control (Attitude Control System). A fault injection study further
identifies the gradient computation stage in GFOLD as the most sensitive to
bit-level errors, motivating selective protection strategies and vector-based
output arbitration. This work establishes a scalable and energy-efficient
architecture for future missions, including Mars Sample Return, Enceladus
Orbilander, and Ceres Sample Return, where onboard autonomy, low latency, and
fault resilience are critical.

</details>


### [147] [CBMC-V3: A CNS-inspired Control Framework Towards Manipulation Agility with SNN](https://arxiv.org/abs/2511.04109)
*Yanbo Pang,Qingkai Li,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于脉冲神经网络（SNN）的仿生控制框架，用于实现机器臂在复杂环境中的敏捷控制。


<details>
  <summary>Details</summary>
Motivation: 现有控制算法在动态轨迹、不可预测交互和多样化对象的复杂环境中难以实现敏捷操作。

Method: 框架包含五个控制模块（大脑皮层、小脑、丘脑、脑干、脊髓）和三个控制层级，全部基于SNN实现。

Result: 实验表明，该方法在操作敏捷性上优于工业级位置控制。

Conclusion: 仿生SNN框架为机器臂在复杂环境中的控制提供了有效解决方案。

Abstract: As robotic arm applications extend beyond industrial settings into
healthcare, service, and daily life, existing control algorithms struggle to
achieve the agile manipulation required for complex environments with dynamic
trajectories, unpredictable interactions, and diverse objects. This paper
presents a biomimetic control framework based on Spiking Neural Networks (SNN),
inspired by the human Central Nervous System (CNS), to achieve agile control in
such environments. The proposed framework features five control modules
(cerebral cortex, cerebellum, thalamus, brainstem, spinal cord), three
hierarchical control levels (first-order, second-order, third-order), and two
information pathways (ascending, descending). Each module is fully implemented
using SNN. The spinal cord module uses spike encoding and Leaky
Integrate-and-Fire (LIF) neurons for feedback control. The brainstem module
employs a network of LIF and non-spiking LIF neurons to dynamically adjust
spinal cord parameters via reinforcement learning. The thalamus module
similarly adjusts the cerebellum's torque outputs. The cerebellum module uses a
recurrent SNN to learn the robotic arm's dynamics through regression, providing
feedforward gravity compensation torques. The framework is validated both in
simulation and on real-world robotic arm platform under various loads and
trajectories. Results demonstrate that our method outperforms the
industrial-grade position control in manipulation agility.

</details>


### [148] [BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning](https://arxiv.org/abs/2511.04131)
*Yitang Li,Zhengyi Luo,Tonghe Zhang,Cunxi Dai,Anssi Kanervisto,Andrea Tirinzoni,Haoyang Weng,Kris Kitani,Mateusz Guzek,Ahmed Touati,Alessandro Lazaric,Matteo Pirotta,Guanya Shi*

Main category: cs.RO

TL;DR: BFM-Zero是一个用于人形机器人的行为基础模型框架，通过共享潜在表示实现多任务控制，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么局限于模拟环境，要么仅适用于特定任务，BFM-Zero旨在统一多任务控制并实现真实世界部署。

Method: 利用无监督强化学习和FB模型，结合奖励塑造、领域随机化和历史依赖学习，构建共享潜在表示。

Result: BFM-Zero在真实人形机器人上实现了零样本运动跟踪、目标到达和奖励优化等多样化任务。

Conclusion: BFM-Zero为可扩展、可提示的人形机器人行为基础模型奠定了基础。

Abstract: Building Behavioral Foundation Models (BFMs) for humanoid robots has the
potential to unify diverse control tasks under a single, promptable generalist
policy. However, existing approaches are either exclusively deployed on
simulated humanoid characters, or specialized to specific tasks such as
tracking. We propose BFM-Zero, a framework that learns an effective shared
latent representation that embeds motions, goals, and rewards into a common
space, enabling a single policy to be prompted for multiple downstream tasks
without retraining. This well-structured latent space in BFM-Zero enables
versatile and robust whole-body skills on a Unitree G1 humanoid in the real
world, via diverse inference methods, including zero-shot motion tracking, goal
reaching, and reward optimization, and few-shot optimization-based adaptation.
Unlike prior on-policy reinforcement learning (RL) frameworks, BFM-Zero builds
upon recent advancements in unsupervised RL and Forward-Backward (FB) models,
which offer an objective-centric, explainable, and smooth latent representation
of whole-body motions. We further extend BFM-Zero with critical reward shaping,
domain randomization, and history-dependent asymmetric learning to bridge the
sim-to-real gap. Those key design choices are quantitatively ablated in
simulation. A first-of-its-kind model, BFM-Zero establishes a step toward
scalable, promptable behavioral foundation models for whole-body humanoid
control.

</details>


### [149] [PUL-SLAM: Path-Uncertainty Co-Optimization with Lightweight Stagnation Detection for Efficient Robotic Exploration](https://arxiv.org/abs/2511.04180)
*Yizhen Yin,Dapeng Feng,Hongbo Chen,Yuhua Qi*

Main category: cs.RO

TL;DR: 提出了一种结合路径-不确定性协同优化深度强化学习和轻量级停滞检测的混合框架，显著提升了探索效率和路径优化。


<details>
  <summary>Details</summary>
Motivation: 现有Active SLAM方法存在探索速度慢和路径次优的问题，需要改进。

Method: 采用双目标奖励函数优化路径和地图不确定性，并引入轻量级停滞检测机制减少冗余探索。

Result: 实验显示，探索时间缩短65%，路径距离减少42%，同时保持地图完整性。

Conclusion: 该方法在复杂环境中高效且实用，仿真到现实的迁移性验证成功。

Abstract: Existing Active SLAM methodologies face issues such as slow exploration speed
and suboptimal paths. To address these limitations, we propose a hybrid
framework combining a Path-Uncertainty Co-Optimization Deep Reinforcement
Learning framework and a Lightweight Stagnation Detection mechanism. The
Path-Uncertainty Co-Optimization framework jointly optimizes travel distance
and map uncertainty through a dual-objective reward function, balancing
exploration and exploitation. The Lightweight Stagnation Detection reduces
redundant exploration through Lidar Static Anomaly Detection and Map Update
Stagnation Detection, terminating episodes on low expansion rates. Experimental
results show that compared with the frontier-based method and RRT method, our
approach shortens exploration time by up to 65% and reduces path distance by up
to 42%, significantly improving exploration efficiency in complex environments
while maintaining reliable map completeness. Ablation studies confirm that the
collaborative mechanism accelerates training convergence. Empirical validation
on a physical robotic platform demonstrates the algorithm's practical
applicability and its successful transferability from simulation to real-world
environments.

</details>


### [150] [GraspView: Active Perception Scoring and Best-View Optimization for Robotic Grasping in Cluttered Environments](https://arxiv.org/abs/2511.04199)
*Shenglin Wang,Mingtong Dai,Jingxuan Su,Lingbo Liu,Chunjie Chen,Xinyu Wu,Liang Lin*

Main category: cs.RO

TL;DR: GraspView是一种仅使用RGB的机器人抓取框架，在杂乱环境中无需深度传感器即可实现精确操作。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖RGB-D相机，但在透明或反光物体及近距离时表现不佳。

Method: 结合全局感知场景重建、动态选择最佳视角的主动感知策略和在线度量对齐模块。

Result: 在多样桌面物体实验中，GraspView显著优于RGB-D和单视角RGB基线。

Conclusion: GraspView是RGB-D管道的实用替代方案，适用于非结构化现实环境。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation, yet
remains highly challenging in cluttered environments where occlusion, poor
perception quality, and inconsistent 3D reconstructions often lead to unstable
or failed grasps. Conventional pipelines have widely relied on RGB-D cameras to
provide geometric information, which fail on transparent or glossy objects and
degrade at close range. We present GraspView, an RGB-only robotic grasping
pipeline that achieves accurate manipulation in cluttered environments without
depth sensors. Our framework integrates three key components: (i) global
perception scene reconstruction, which provides locally consistent, up-to-scale
geometry from a single RGB view and fuses multi-view projections into a
coherent global 3D scene; (ii) a render-and-score active perception strategy,
which dynamically selects next-best-views to reveal occluded regions; and (iii)
an online metric alignment module that calibrates VGGT predictions against
robot kinematics to ensure physical scale consistency. Building on these
tailor-designed modules, GraspView performs best-view global grasping, fusing
multi-view reconstructions and leveraging GraspNet for robust execution.
Experiments on diverse tabletop objects demonstrate that GraspView
significantly outperforms both RGB-D and single-view RGB baselines, especially
under heavy occlusion, near-field sensing, and with transparent objects. These
results highlight GraspView as a practical and versatile alternative to RGB-D
pipelines, enabling reliable grasping in unstructured real-world environments.

</details>


### [151] [Can Context Bridge the Reality Gap? Sim-to-Real Transfer of Context-Aware Policies](https://arxiv.org/abs/2511.04249)
*Marco Iannotta,Yuxuan Yang,Johannes A. Stork,Erik Schaffernicht,Todor Stoyanov*

Main category: cs.RO

TL;DR: 论文研究了通过动态参数估计（上下文）改进强化学习中的模拟到现实迁移问题，提出了一种结合上下文估计模块的方法，并在实验中验证了其优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 模拟到现实迁移是机器人强化学习中的主要挑战，传统方法（如域随机化）虽然能缓解问题，但会导致性能下降。论文探索了通过动态参数估计（上下文）来改进迁移效果。

Method: 在基于域随机化的强化学习框架中集成了上下文估计模块，并系统比较了多种监督策略。

Result: 实验表明，上下文感知策略在所有设置中均优于传统方法，但最佳监督策略因任务而异。

Conclusion: 上下文感知策略能有效改进模拟到现实迁移，但需根据任务选择合适的监督策略。

Abstract: Sim-to-real transfer remains a major challenge in reinforcement learning (RL)
for robotics, as policies trained in simulation often fail to generalize to the
real world due to discrepancies in environment dynamics. Domain Randomization
(DR) mitigates this issue by exposing the policy to a wide range of randomized
dynamics during training, yet leading to a reduction in performance. While
standard approaches typically train policies agnostic to these variations, we
investigate whether sim-to-real transfer can be improved by conditioning the
policy on an estimate of the dynamics parameters -- referred to as context. To
this end, we integrate a context estimation module into a DR-based RL framework
and systematically compare SOTA supervision strategies. We evaluate the
resulting context-aware policies in both a canonical control benchmark and a
real-world pushing task using a Franka Emika Panda robot. Results show that
context-aware policies outperform the context-agnostic baseline across all
settings, although the best supervision strategy depends on the task.

</details>


### [152] [Design and Control of a Coaxial Dual-rotor Reconfigurable Tailsitter UAV Based on Swashplateless Mechanism](https://arxiv.org/abs/2511.04251)
*Jinfeng Liang,Haocheng Guo,Ximin Lyu*

Main category: cs.RO

TL;DR: 该论文提出了一种可重构机翼设计的尾座式VTOL无人机，通过优化结构和控制机制，解决了多旋翼模式下的风扰问题，并验证了其稳定飞行性能。


<details>
  <summary>Details</summary>
Motivation: 尾座式VTOL无人机在多旋翼模式下易受风扰，因其暴露的机身面积较大。

Method: 采用可重构机翼设计、同轴异构双旋翼配置和无斜盘机制，优化结构以减少振动和功耗。

Result: 显著降低了总功耗，减少了结构重量和复杂性，并实现了稳定飞行。

Conclusion: 通过优化设计和测试验证，该无人机在多旋翼和固定翼模式下均表现出稳定的飞行性能。

Abstract: The tailsitter vertical takeoff and landing (VTOL) UAV is widely used due to
its lower dead weight, which eliminates the actuators and mechanisms for
tilting. However, the tailsitter UAV is susceptible to wind disturbances in
multi-rotor mode, as it exposes a large frontal fuselage area. To address this
issue, our tailsitter UAV features a reconfigurable wing design, allowing wings
to retract in multi-rotor mode and extend in fixed- wing mode. Considering
power efficiency, we design a coaxial heterogeneous dual-rotor configuration,
which significantly re- duces the total power consumption. To reduce structural
weight and simplify structural complexity, we employ a swashplateless mechanism
with an improved design to control pitch and roll in multi-rotor mode. We
optimize the structure of the swashplateless mechanism by adding flapping
hinges, which reduces vibration during cyclic acceleration and deceleration.
Finally, we perform comprehensive transition flight tests to validate stable
flight performance across the entire flight envelope of the tailsitter UAV.

</details>


### [153] [MacroNav: Multi-Task Context Representation Learning Enables Efficient Navigation in Unknown Environments](https://arxiv.org/abs/2511.04320)
*Kuankuan Sima,Longbin Tang,Haozhe Ma,Lin Zhao*

Main category: cs.RO

TL;DR: MacroNav是一个基于学习的导航框架，通过轻量级上下文编码器和强化学习策略，在未知环境中实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法在丰富上下文表示与导航效率之间难以平衡，MacroNav旨在解决这一问题。

Method: 结合多任务自监督学习的上下文编码器和基于图的强化学习策略。

Result: 在成功率和路径长度加权成功率上显著优于现有方法，且计算成本低。

Conclusion: MacroNav在未知环境中实现了高效且鲁棒的导航。

Abstract: Autonomous navigation in unknown environments requires compact yet expressive
spatial understanding under partial observability to support high-level
decision making. Existing approaches struggle to balance rich contextual
representation with navigation efficiency. We present MacroNav, a
learning-based navigation framework featuring two key components: (1) a
lightweight context encoder trained via multi-task self-supervised learning to
capture multi-scale, navigation-centric spatial representations; and (2) a
reinforcement learning policy that seamlessly integrates these representations
with graph-based reasoning for efficient action selection. Extensive
experiments demonstrate the context encoder's efficient and robust
environmental understanding. Real-world deployments further validate MacroNav's
effectiveness, yielding significant gains over state-of-the-art navigation
methods in both Success Rate (SR) and Success weighted by Path Length (SPL),
while maintaining low computational cost. Code will be released upon
acceptance.

</details>


### [154] [GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies](https://arxiv.org/abs/2511.04357)
*Maëlic Neau,Zoe Falomir,Paulo E. Santos,Anne-Gwenn Bosser,Cédric Buche*

Main category: cs.RO

TL;DR: GraSP-VLA是一种新型神经符号方法，结合连续场景图表示和VLA策略，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏高级符号规划能力，而AML方法缺乏泛化和扩展性。

Method: 使用连续场景图表示生成符号表示，并协调低层VLA策略。

Result: 在自动规划领域生成任务中表现有效，并在长时任务中展示了潜力。

Conclusion: GraSP-VLA为机器人学习新技能提供了一种可扩展的解决方案。

Abstract: Deploying autonomous robots that can learn new skills from demonstrations is
an important challenge of modern robotics. Existing solutions often apply
end-to-end imitation learning with Vision-Language Action (VLA) models or
symbolic approaches with Action Model Learning (AML). On the one hand, current
VLA models are limited by the lack of high-level symbolic planning, which
hinders their abilities in long-horizon tasks. On the other hand, symbolic
approaches in AML lack generalization and scalability perspectives. In this
paper we present a new neuro-symbolic approach, GraSP-VLA, a framework that
uses a Continuous Scene Graph representation to generate a symbolic
representation of human demonstrations. This representation is used to generate
new planning domains during inference and serves as an orchestrator for
low-level VLA policies, scaling up the number of actions that can be reproduced
in a row. Our results show that GraSP-VLA is effective for modeling symbolic
representations on the task of automatic planning domain generation from
observations. In addition, results on real-world experiments show the potential
of our Continuous Scene Graph representation to orchestrate low-level VLA
policies in long-horizon tasks.

</details>


### [155] [Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories](https://arxiv.org/abs/2511.04375)
*Anna Mészáros,Javier Alonso-Mora,Jens Kober*

Main category: cs.RO

TL;DR: 研究探讨了如何最好地表示场景中多个智能体的联合分布，发现明确定义的交互方式比数据驱动的隐式学习更有效。


<details>
  <summary>Details</summary>
Motivation: 为了提升自动驾驶决策的准确性，需要更好地捕捉场景中智能体的联合分布，但目前对交互表示方式尚无共识。

Method: 在同一网络结构中比较不同交互描述方式，包括隐式学习和基于时空关系的显式建模。

Result: 研究发现，明确定义的交互方式（如明确谁先通过路口）通常能显著提升性能，而数据驱动的隐式学习反而可能降低效果。

Conclusion: 显式建模交互关系比隐式学习更有效，尤其是在需要明确决策的场景中。

Abstract: Effectively capturing the joint distribution of all agents in a scene is
relevant for predicting the true evolution of the scene and in turn providing
more accurate information to the decision processes of autonomous vehicles.
While new models have been developed for this purpose in recent years, it
remains unclear how to best represent the joint distributions particularly from
the perspective of the interactions between agents. Thus far there is no clear
consensus on how best to represent interactions between agents; whether they
should be learned implicitly from data by neural networks, or explicitly
modeled using the spatial and temporal relations that are more grounded in
human decision-making. This paper aims to study various means of describing
interactions within the same network structure and their effect on the final
learned joint distributions. Our findings show that more often than not, simply
allowing a network to establish interactive connections between agents based on
data has a detrimental effect on performance. Instead, having well defined
interactions (such as which agent of an agent pair passes first at an
intersection) can often bring about a clear boost in performance.

</details>


### [156] [ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation](https://arxiv.org/abs/2511.04381)
*Dexin wang,Faliang Chang,Chunsheng Liu*

Main category: cs.RO

TL;DR: ForeRobo是一种生成式机器人代理，通过生成模拟自主获取操作技能，结合生成范式与经典控制，显著提升效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 利用模拟高效获取高级操作技能具有挑战性且意义重大，传统端到端策略学习方法缺乏可解释性和效率。

Method: 提出ForeRobo代理，采用"提议-生成-学习-执行"循环，通过ForeGen生成目标状态数据，训练ForeFormer模型预测3D目标位置，结合经典控制执行。

Result: ForeFormer在多种任务中平均提升56.32%，实现在20多个任务中零样本迁移，平均成功率79.28%。

Conclusion: ForeRobo结合生成模拟与经典控制，显著提升操作技能的获取效率和泛化能力。

Abstract: Efficiently leveraging simulation to acquire advanced manipulation skills is
both challenging and highly significant. We introduce \textit{ForeRobo}, a
generative robotic agent that utilizes generative simulations to autonomously
acquire manipulation skills driven by envisioned goal states. Instead of
directly learning low-level policies, we advocate integrating generative
paradigms with classical control. Our approach equips a robotic agent with a
self-guided \textit{propose-generate-learn-actuate} cycle. The agent first
proposes the skills to be acquired and constructs the corresponding simulation
environments; it then configures objects into appropriate arrangements to
generate skill-consistent goal states (\textit{ForeGen}). Subsequently, the
virtually infinite data produced by ForeGen are used to train the proposed
state generation model (\textit{ForeFormer}), which establishes point-wise
correspondences by predicting the 3D goal position of every point in the
current state, based on the scene state and task instructions. Finally,
classical control algorithms are employed to drive the robot in real-world
environments to execute actions based on the envisioned goal states. Compared
with end-to-end policy learning methods, ForeFormer offers superior
interpretability and execution efficiency. We train and benchmark ForeFormer
across a variety of rigid-body and articulated-object manipulation tasks, and
observe an average improvement of 56.32\% over the state-of-the-art state
generation models, demonstrating strong generality across different
manipulation patterns. Moreover, in real-world evaluations involving more than
20 robotic tasks, ForeRobo achieves zero-shot sim-to-real transfer and exhibits
remarkable generalization capabilities, attaining an average success rate of
79.28\%.

</details>


### [157] [Temporal Action Selection for Action Chunking](https://arxiv.org/abs/2511.04421)
*Yueyang Weng,Xiaopeng Zhang,Yongjin Mu,Yingcong Zhu,Yanjie Li,Qi Liu*

Main category: cs.RO

TL;DR: 提出了一种新算法Temporal Action Selector (TAS)，通过缓存多时间步的动作块并动态选择最优动作，解决了动作分块方法在反应性和决策一致性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 动作分块方法在LfD中广泛应用，但降低了决策频率，导致对近期观察的利用不足，影响反应性，尤其在传感器噪声和动态环境变化中表现不佳。

Method: 提出TAS算法，通过缓存预测的动作块并使用轻量级选择器网络动态选择最优动作，平衡反应性、决策一致性和运动连贯性。

Result: 实验表明，TAS显著提高了任务成功率（绝对增益高达73.3%），并与残差强化学习结合提升了训练效率和性能上限。

Conclusion: TAS在仿真和物理机器人实验中均表现出高效性，解决了动作分块方法的局限性。

Abstract: Action chunking is a widely adopted approach in Learning from Demonstration
(LfD). By modeling multi-step action chunks rather than single-step actions,
action chunking significantly enhances modeling capabilities for human expert
policies. However, the reduced decision frequency restricts the utilization of
recent observations, degrading reactivity - particularly evident in the
inadequate adaptation to sensor noise and dynamic environmental changes.
Existing efforts to address this issue have primarily resorted to trading off
reactivity against decision consistency, without achieving both. To address
this limitation, we propose a novel algorithm, Temporal Action Selector (TAS),
which caches predicted action chunks from multiple timesteps and dynamically
selects the optimal action through a lightweight selector network. TAS achieves
balanced optimization across three critical dimensions: reactivity, decision
consistency, and motion coherence. Experiments across multiple tasks with
diverse base policies show that TAS significantly improves success rates -
yielding an absolute gain of up to 73.3%. Furthermore, integrating TAS as a
base policy with residual reinforcement learning (RL) substantially enhances
training efficiency and elevates the performance plateau. Experiments in both
simulation and physical robots confirm the method's efficacy.

</details>


### [158] [Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment](https://arxiv.org/abs/2511.04555)
*Tao Lin,Yilei Zhong,Yuxin Du,Jingjing Zhang,Jiting Liu,Yinxinyu Chen,Encheng Gu,Ziyan Liu,Hongyi Cai,Yanwen Zou,Lixing Zou,Zhaoye Zhou,Gen Li,Bo Zhao*

Main category: cs.RO

TL;DR: Evo-1是一种轻量级的视觉-语言-动作（VLA）模型，通过创新的架构和两阶段训练方法，显著降低了计算成本并提升了部署效率，同时在多个基准测试中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型参数庞大且依赖大规模机器人数据预训练，导致计算成本高、部署效率低，且容易过拟合和泛化能力差。

Method: Evo-1基于原生多模态视觉-语言模型（VLM），引入跨调制扩散变换器和优化集成模块，并采用两阶段训练方法逐步对齐动作与感知。

Result: Evo-1在Meta-World和RoboTwin上分别超越之前最佳模型12.4%和6.9%，在LIBERO上达到94.8%的竞争性结果，实际部署中成功率为78%。

Conclusion: Evo-1展示了轻量级VLA模型的潜力，为未来高效多模态机器人研究提供了重要参考。

Abstract: Vision-Language-Action (VLA) models have emerged as a powerful framework that
unifies perception, language, and control, enabling robots to perform diverse
tasks through multimodal understanding. However, current VLA models typically
contain massive parameters and rely heavily on large-scale robot data
pretraining, leading to high computational costs during training, as well as
limited deployability for real-time inference. Moreover, most training
paradigms often degrade the perceptual representations of the vision-language
backbone, resulting in overfitting and poor generalization to downstream tasks.
In this work, we present Evo-1, a lightweight VLA model that reduces
computation and improves deployment efficiency, while maintaining strong
performance without pretraining on robot data. Evo-1 builds on a native
multimodal Vision-Language model (VLM), incorporating a novel cross-modulated
diffusion transformer along with an optimized integration module, together
forming an effective architecture. We further introduce a two-stage training
paradigm that progressively aligns action with perception, preserving the
representations of the VLM. Notably, with only 0.77 billion parameters, Evo-1
achieves state-of-the-art results on the Meta-World and RoboTwin suite,
surpassing the previous best models by 12.4% and 6.9%, respectively, and also
attains a competitive result of 94.8% on LIBERO. In real-world evaluations,
Evo-1 attains a 78% success rate with high inference frequency and low memory
overhead, outperforming all baseline methods. We release code, data, and model
weights to facilitate future research on lightweight and efficient VLA models.

</details>


### [159] [SAFe-Copilot: Unified Shared Autonomy Framework](https://arxiv.org/abs/2511.04664)
*Phat Nguyen,Erfan Aasi,Shiva Sreeram,Guy Rosman,Andrew Silva,Sertac Karaman,Daniela Rus*

Main category: cs.RO

TL;DR: 提出了一种基于语义和语言表示的共享自动驾驶框架，通过VLMs推断驾驶员意图，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶在罕见、模糊和分布外场景中的脆弱性，通过共享自治结合人类输入。

Method: 利用VLMs从多模态线索推断驾驶员意图，并在语义层面协调人类与自动驾驶控制。

Result: 在模拟和人类实验中表现优异，碰撞率显著降低，92%的参与者认可仲裁结果。

Conclusion: 语义层面的共享自治是设计原则，能提升系统推理能力并与人类意图保持一致。

Abstract: Autonomous driving systems remain brittle in rare, ambiguous, and
out-of-distribution scenarios, where human driver succeed through contextual
reasoning. Shared autonomy has emerged as a promising approach to mitigate such
failures by incorporating human input when autonomy is uncertain. However, most
existing methods restrict arbitration to low-level trajectories, which
represent only geometric paths and therefore fail to preserve the underlying
driving intent. We propose a unified shared autonomy framework that integrates
human input and autonomous planners at a higher level of abstraction. Our
method leverages Vision Language Models (VLMs) to infer driver intent from
multi-modal cues -- such as driver actions and environmental context -- and to
synthesize coherent strategies that mediate between human and autonomous
control. We first study the framework in a mock-human setting, where it
achieves perfect recall alongside high accuracy and precision. A human-subject
survey further shows strong alignment, with participants agreeing with
arbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive
benchmark demonstrates a substantial reduction in collision rate and
improvement in overall performance compared to pure autonomy. Arbitration at
the level of semantic, language-based representations emerges as a design
principle for shared autonomy, enabling systems to exercise common-sense
reasoning and maintain continuity with human intent.

</details>


### [160] [Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions](https://arxiv.org/abs/2511.04665)
*Kaifeng Zhang,Shuo Sha,Hanxiao Jiang,Matthew Loper,Hyunjong Song,Guangyan Cai,Zhuo Xu,Xiaochen Hu,Changxi Zheng,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出了一种基于真实视频构建软体数字孪生体的仿真框架，用于高效评估机器人操作策略。


<details>
  <summary>Details</summary>
Motivation: 真实世界中评估机器人操作策略成本高、耗时长且难以复现，尤其是涉及可变形物体的任务。仿真虽为替代方案，但现有模拟器难以捕捉软体交互的视觉与物理复杂性。

Method: 利用3D高斯泼溅技术构建软体数字孪生体，并结合物理信息重建与高质量渲染技术。

Result: 在代表性任务（如毛绒玩具打包、绳索布线等）中验证，仿真结果与真实执行性能高度相关，并揭示了策略的关键行为模式。

Conclusion: 结合物理重建与高质量渲染可实现机器人操作策略的可复现、可扩展且精确的评估。

Abstract: Robotic manipulation policies are advancing rapidly, but their direct
evaluation in the real world remains costly, time-consuming, and difficult to
reproduce, particularly for tasks involving deformable objects. Simulation
provides a scalable and systematic alternative, yet existing simulators often
fail to capture the coupled visual and physical complexity of soft-body
interactions. We present a real-to-sim policy evaluation framework that
constructs soft-body digital twins from real-world videos and renders robots,
objects, and environments with photorealistic fidelity using 3D Gaussian
Splatting. We validate our approach on representative deformable manipulation
tasks, including plush toy packing, rope routing, and T-block pushing,
demonstrating that simulated rollouts correlate strongly with real-world
execution performance and reveal key behavioral patterns of learned policies.
Our results suggest that combining physics-informed reconstruction with
high-quality rendering enables reproducible, scalable, and accurate evaluation
of robotic manipulation policies. Website: https://real2sim-eval.github.io/

</details>


### [161] [X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations](https://arxiv.org/abs/2511.04671)
*Maximus A. Pace,Prithwish Dan,Chuanruo Ning,Atiksh Bhardwaj,Audrey Du,Edward W. Duan,Wei-Chiu Ma,Kushal Kedia*

Main category: cs.RO

TL;DR: X-Diffusion框架通过噪声扩散过程有效利用人类视频数据训练机器人策略，避免学习不可行动作。


<details>
  <summary>Details</summary>
Motivation: 人类视频数据丰富但动作执行与机器人不匹配，直接使用会导致不可行动作。

Method: 利用前向扩散过程，通过噪声区分人类与机器人动作，训练扩散策略。

Result: X-Diffusion在五个任务中平均成功率比基线高16%。

Conclusion: X-Diffusion能有效利用人类数据提升机器人策略性能。

Abstract: Human videos can be recorded quickly and at scale, making them an appealing
source of training data for robot learning. However, humans and robots differ
fundamentally in embodiment, resulting in mismatched action execution. Direct
kinematic retargeting of human hand motion can therefore produce actions that
are physically infeasible for robots. Despite these low-level differences,
human demonstrations provide valuable motion cues about how to manipulate and
interact with objects. Our key idea is to exploit the forward diffusion
process: as noise is added to actions, low-level execution differences fade
while high-level task guidance is preserved. We present X-Diffusion, a
principled framework for training diffusion policies that maximally leverages
human data without learning dynamically infeasible motions. X-Diffusion first
trains a classifier to predict whether a noisy action is executed by a human or
robot. Then, a human action is incorporated into policy training only after
adding sufficient noise such that the classifier cannot discern its embodiment.
Actions consistent with robot execution supervise fine-grained denoising at low
noise levels, while mismatched human actions provide only coarse guidance at
higher noise levels. Our experiments show that naive co-training under
execution mismatches degrades policy performance, while X-Diffusion
consistently improves it. Across five manipulation tasks, X-Diffusion achieves
a 16% higher average success rate than the best baseline. The project website
is available at https://portal-cornell.github.io/X-Diffusion/.

</details>


### [162] [GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction](https://arxiv.org/abs/2511.04679)
*Qingzhou Lu,Yao Feng,Baiyu Shi,Michael Piseno,Zhenan Bao,C. Karen Liu*

Main category: cs.RO

TL;DR: GentleHumanoid框架通过整合阻抗控制和全身运动跟踪策略，实现了上半身的柔顺性，减少了接触力峰值，使交互更自然。


<details>
  <summary>Details</summary>
Motivation: 人形机器人需要在以人为中心的环境中安全自然地交互，但现有强化学习策略过于强调刚性跟踪，缺乏柔顺性。

Method: 采用基于弹簧的统一模型，模拟阻力和引导接触，确保运动一致性，并通过可调力阈值保障安全。

Result: 在仿真和Unitree G1人形机器人上测试，任务成功率高且接触力峰值显著降低。

Conclusion: GentleHumanoid框架为人形机器人与人类安全协作提供了有效解决方案。

Abstract: Humanoid robots are expected to operate in human-centered environments where
safe and natural physical interaction is essential. However, most recent
reinforcement learning (RL) policies emphasize rigid tracking and suppress
external forces. Existing impedance-augmented approaches are typically
restricted to base or end-effector control and focus on resisting extreme
forces rather than enabling compliance. We introduce GentleHumanoid, a
framework that integrates impedance control into a whole-body motion tracking
policy to achieve upper-body compliance. At its core is a unified spring-based
formulation that models both resistive contacts (restoring forces when pressing
against surfaces) and guiding contacts (pushes or pulls sampled from human
motion data). This formulation ensures kinematically consistent forces across
the shoulder, elbow, and wrist, while exposing the policy to diverse
interaction scenarios. Safety is further supported through task-adjustable
force thresholds. We evaluate our approach in both simulation and on the
Unitree G1 humanoid across tasks requiring different levels of compliance,
including gentle hugging, sit-to-stand assistance, and safe object
manipulation. Compared to baselines, our policy consistently reduces peak
contact forces while maintaining task success, resulting in smoother and more
natural interactions. These results highlight a step toward humanoid robots
that can safely and effectively collaborate with humans and handle objects in
real-world environments.

</details>
