{"id": "2512.05969", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05969", "abs": "https://arxiv.org/abs/2512.05969", "authors": ["Hokin Deng"], "title": "Video Models Start to Solve Chess, Maze, Sudoku, Mental Rotation, and Raven' Matrices", "comment": "See $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and $\\href{https://github.com/hokindeng/VMEvalKit}{code}$", "summary": "We show that video generation models could reason now. Testing on tasks such as chess, maze, Sudoku, mental rotation, and Raven's Matrices, leading models such as Sora-2 achieve sixty percent success rates. We establish a robust experimental paradigm centered on the \"Task Pair\" design. We build a code framework, with 39 models available already, that supports this paradigm and allows for easy scaling - users can add models and tasks efficiently. We show our automated evaluation strongly correlates with human judgment, and therefore this paradigm is highly scalable. We see an opportunity, given the availability of our paradigm, to do reinforcement learning for improving reasoning in video models. You could checkout all of our raw $\\href{https://grow-ai-like-a-child.com/video-reason/}{results}$ and our $\\href{https://github.com/hokindeng/VMEvalKit}{VMEvalKit}$ codebase.", "AI": {"tldr": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u5df2\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u5728\u68cb\u7c7b\u3001\u8ff7\u5bab\u3001\u6570\u72ec\u7b49\u4efb\u52a1\u4e0a\u8fbe\u523060%\u6210\u529f\u7387\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\"\u4efb\u52a1\u5bf9\"\u8bbe\u8ba1\u7684\u5b9e\u9a8c\u8303\u5f0f\uff0c\u5e76\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u63a2\u7d22\u89c6\u9891\u751f\u6210\u6a21\u578b\u662f\u5426\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u5efa\u7acb\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u4f53\u7cfb\u6765\u91cf\u5316\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u3002", "method": "\u91c7\u7528\"\u4efb\u52a1\u5bf9\"\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u6784\u5efa\u5305\u542b39\u4e2a\u6a21\u578b\u7684\u4ee3\u7801\u6846\u67b6VMEvalKit\uff0c\u652f\u6301\u81ea\u52a8\u5316\u8bc4\u4f30\u5e76\u4e0e\u4eba\u7c7b\u5224\u65ad\u5f3a\u76f8\u5173\u3002", "result": "Sora-2\u7b49\u9886\u5148\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523060%\u6210\u529f\u7387\uff0c\u9a8c\u8bc1\u4e86\u89c6\u9891\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u89c6\u9891\u751f\u6210\u6a21\u578b\u5df2\u5c55\u73b0\u51fa\u521d\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u8be5\u8bc4\u4f30\u8303\u5f0f\u5177\u6709\u9ad8\u5ea6\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.05987", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05987", "abs": "https://arxiv.org/abs/2512.05987", "authors": ["Chenyue Yu", "Jianyu Yu"], "title": "Adaptive Dataset Quantization: A New Direction for Dataset Pruning", "comment": "Accepted by ICCPR 2025", "summary": "This paper addresses the challenges of storage and communication costs for large-scale datasets in resource-constrained edge devices by proposing a novel dataset quantization approach to reduce intra-sample redundancy. Unlike traditional dataset pruning and distillation methods that focus on inter-sample redundancy, the proposed method compresses each image by reducing redundant or less informative content within samples while preserving essential features. It first applies linear symmetric quantization to obtain an initial quantization range and scale for each sample. Then, an adaptive quantization allocation algorithm is introduced to distribute different quantization ratios for samples with varying precision requirements, maintaining a constant total compression ratio. The main contributions include: (1) being the first to use limited bits to represent datasets for storage reduction; (2) introducing a dataset-level quantization algorithm with adaptive ratio allocation; and (3) validating the method's effectiveness through extensive experiments on CIFAR-10, CIFAR-100, and ImageNet-1K. Results show that the method maintains model training performance while achieving significant dataset compression, outperforming traditional quantization and dataset pruning baselines under the same compression ratios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u96c6\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u6837\u672c\u5185\u5197\u4f59\u6765\u964d\u4f4e\u8fb9\u7f18\u8bbe\u5907\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5b58\u50a8\u548c\u901a\u4fe1\u6210\u672c\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8bad\u7ec3\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u7684\u6570\u636e\u96c6\u538b\u7f29\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e2d\u5927\u89c4\u6a21\u6570\u636e\u96c6\u7684\u5b58\u50a8\u548c\u901a\u4fe1\u6210\u672c\u6311\u6218\uff0c\u4f20\u7edf\u7684\u6570\u636e\u96c6\u526a\u679d\u548c\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6837\u672c\u95f4\u5197\u4f59\uff0c\u800c\u672c\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u51cf\u5c11\u6837\u672c\u5185\u5197\u4f59\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u5bf9\u79f0\u91cf\u5316\u83b7\u53d6\u6bcf\u4e2a\u6837\u672c\u7684\u521d\u59cb\u91cf\u5316\u8303\u56f4\u548c\u5c3a\u5ea6\uff0c\u7136\u540e\u5f15\u5165\u81ea\u9002\u5e94\u91cf\u5316\u5206\u914d\u7b97\u6cd5\uff0c\u4e3a\u4e0d\u540c\u7cbe\u5ea6\u9700\u6c42\u7684\u6837\u672c\u5206\u914d\u4e0d\u540c\u7684\u91cf\u5316\u6bd4\u4f8b\uff0c\u540c\u65f6\u4fdd\u6301\u6052\u5b9a\u7684\u603b\u538b\u7f29\u6bd4\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cImageNet-1K\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u76f8\u540c\u538b\u7f29\u6bd4\u4e0b\u4f18\u4e8e\u4f20\u7edf\u91cf\u5316\u548c\u6570\u636e\u96c6\u526a\u679d\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u4f7f\u7528\u6709\u9650\u4f4d\u6570\u8868\u793a\u6570\u636e\u96c6\u4ee5\u5b9e\u73b0\u5b58\u50a8\u7f29\u51cf\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u7ea7\u91cf\u5316\u7b97\u6cd5\u5177\u6709\u81ea\u9002\u5e94\u6bd4\u4f8b\u5206\u914d\u80fd\u529b\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u96c6\u538b\u7f29\u3002"}}
{"id": "2512.05988", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.05988", "abs": "https://arxiv.org/abs/2512.05988", "authors": ["Junho Kim", "Seongwon Lee"], "title": "VG3T: Visual Geometry Grounded Gaussian Transformer", "comment": null, "summary": "Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.", "AI": {"tldr": "VG3T\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u89c6\u89d2\u524d\u9988\u7f51\u7edc\uff0c\u901a\u8fc73D\u9ad8\u65af\u8868\u793a\u9884\u6d4b3D\u8bed\u4e49\u5360\u636e\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u89d2\u878d\u5408\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e861.7%\u7684mIoU\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c11\u4e8646%\u7684\u57fa\u5143\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u89c6\u89d2\u878d\u5408\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f43D\u8868\u793a\u788e\u7247\u5316\u548c\u6027\u80fd\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u8868\u793a\u51e0\u4f55\u548c\u8bed\u4e49\u7684\u591a\u89c6\u89d2\u5904\u7406\u65b9\u6cd5\u3002", "method": "VG3T\u91c7\u7528\u591a\u89c6\u89d2\u524d\u9988\u7f51\u7edc\u76f4\u63a5\u9884\u6d4b\u4e00\u7ec4\u5177\u6709\u8bed\u4e49\u5c5e\u6027\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u7f51\u683c\u7684\u91c7\u6837\u548c\u4f4d\u7f6e\u7ec6\u5316\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u6765\u7f13\u89e3\u8ddd\u79bb\u76f8\u5173\u7684\u5bc6\u5ea6\u504f\u5dee\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVG3T\u76f8\u6bd4\u4e4b\u524d\u7684\u6700\u4f18\u65b9\u6cd5\u5b9e\u73b0\u4e861.7%\u7684mIoU\u63d0\u5347\uff0c\u540c\u65f6\u4f7f\u7528\u7684\u57fa\u5143\u6570\u91cf\u51cf\u5c11\u4e8646%\u3002", "conclusion": "VG3T\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u89c6\u89d23D\u573a\u666f\u8868\u793a\u8303\u5f0f\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a3D\u8bed\u4e49\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.05991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.05991", "abs": "https://arxiv.org/abs/2512.05991", "authors": ["Chang Liu", "Tianjiao Jing", "Chengcheng Ma", "Xuanqi Zhou", "Zhengxuan Lian", "Qin Jin", "Hongliang Yuan", "Shi-Sheng Huang"], "title": "EmoDiffTalk:Emotion-aware Diffusion for Editable 3D Gaussian Talking Head", "comment": null, "summary": "Recent photo-realistic 3D talking head via 3D Gaussian Splatting still has significant shortcoming in emotional expression manipulation, especially for fine-grained and expansive dynamics emotional editing using multi-modal control. This paper introduces a new editable 3D Gaussian talking head, i.e. EmoDiffTalk. Our key idea is a novel Emotion-aware Gaussian Diffusion, which includes an action unit (AU) prompt Gaussian diffusion process for fine-grained facial animator, and moreover an accurate text-to-AU emotion controller to provide accurate and expansive dynamic emotional editing using text input. Experiments on public EmoTalk3D and RenderMe-360 datasets demonstrate superior emotional subtlety, lip-sync fidelity, and controllability of our EmoDiffTalk over previous works, establishing a principled pathway toward high-quality, diffusion-driven, multimodal editable 3D talking-head synthesis. To our best knowledge, our EmoDiffTalk is one of the first few 3D Gaussian Splatting talking-head generation framework, especially supporting continuous, multimodal emotional editing within the AU-based expression space.", "AI": {"tldr": "EmoDiffTalk\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u53ef\u7f16\u8f913D\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u611f\u77e5\u9ad8\u65af\u6269\u6563\u548c\u6587\u672c\u5230\u52a8\u4f5c\u5355\u5143\uff08AU\uff09\u7684\u60c5\u611f\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u548c\u591a\u6a21\u6001\u7684\u60c5\u611f\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u903c\u771f3D\u8bf4\u8bdd\u5934\u90e8\u5728\u60c5\u611f\u8868\u8fbe\u64cd\u7eb5\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u7ec6\u7c92\u5ea6\u548c\u591a\u6a21\u6001\u63a7\u5236\u7684\u52a8\u6001\u60c5\u611f\u7f16\u8f91\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u60c5\u611f\u611f\u77e5\u9ad8\u65af\u6269\u6563\u65b9\u6cd5\uff0c\u5305\u62ec\u52a8\u4f5c\u5355\u5143\uff08AU\uff09\u63d0\u793a\u7684\u9ad8\u65af\u6269\u6563\u8fc7\u7a0b\u7528\u4e8e\u7ec6\u7c92\u5ea6\u9762\u90e8\u52a8\u753b\uff0c\u4ee5\u53ca\u51c6\u786e\u7684\u6587\u672c\u5230AU\u60c5\u611f\u63a7\u5236\u5668\uff0c\u652f\u6301\u6587\u672c\u8f93\u5165\u7684\u7cbe\u786e\u548c\u52a8\u6001\u60c5\u611f\u7f16\u8f91\u3002", "result": "\u5728EmoTalk3D\u548cRenderMe-360\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEmoDiffTalk\u5728\u60c5\u611f\u7ec6\u817b\u5ea6\u3001\u5507\u90e8\u540c\u6b65\u4fdd\u771f\u5ea6\u548c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u3002", "conclusion": "EmoDiffTalk\u662f\u9996\u6279\u652f\u6301\u57fa\u4e8eAU\u8868\u8fbe\u7a7a\u95f4\u7684\u8fde\u7eed\u591a\u6a21\u6001\u60c5\u611f\u7f16\u8f91\u76843D\u9ad8\u65af\u6cfc\u6e85\u8bf4\u8bdd\u5934\u90e8\u751f\u6210\u6846\u67b6\u4e4b\u4e00\uff0c\u4e3a\u9ad8\u8d28\u91cf\u3001\u6269\u6563\u9a71\u52a8\u7684\u591a\u6a21\u6001\u53ef\u7f16\u8f913D\u8bf4\u8bdd\u5934\u90e8\u5408\u6210\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\u3002"}}
{"id": "2512.06002", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06002", "abs": "https://arxiv.org/abs/2512.06002", "authors": ["Evan Conway", "David Porfirio", "David Chan", "Mark Roberts", "Laura M. Hiatt"], "title": "POrTAL: Plan-Orchestrated Tree Assembly for Lookahead", "comment": "Submitted to ICRA 26", "summary": "Assigning tasks to robots often involves supplying the robot with an overarching goal, such as through natural language, and then relying on the robot to uncover and execute a plan to achieve that goal. In many settings common to human-robot interaction, however, the world is only partially observable to the robot, requiring that it create plans under uncertainty. Although many probabilistic planning algorithms exist for this purpose, these algorithms can be inefficient if executed with the robot's limited computational resources, or may require more steps than expected to achieve the goal. We thereby created a new, lightweight, probabilistic planning algorithm, Plan-Orchestrated Tree Assembly for Lookahead (POrTAL), that combines the strengths of two baseline planning algorithms, FF-Replan and POMCP. In a series of case studies, we demonstrate POrTAL's ability to quickly arrive at solutions that outperform these baselines in terms of number of steps. We additionally demonstrate how POrTAL performs under varying temporal constraints.", "AI": {"tldr": "POrTAL\u662f\u4e00\u79cd\u65b0\u7684\u8f7b\u91cf\u7ea7\u6982\u7387\u89c4\u5212\u7b97\u6cd5\uff0c\u7ed3\u5408FF-Replan\u548cPOMCP\u7684\u4f18\u52bf\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u4e3a\u673a\u5668\u4eba\u9ad8\u6548\u89c4\u5212\u4efb\u52a1\u3002", "motivation": "\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u573a\u666f\u4e2d\uff0c\u673a\u5668\u4eba\u901a\u5e38\u9762\u4e34\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\uff0c\u9700\u8981\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u4e0b\u8fdb\u884c\u89c4\u5212\u3002\u73b0\u6709\u6982\u7387\u89c4\u5212\u7b97\u6cd5\u5728\u673a\u5668\u4eba\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u6548\u7387\u4e0d\u9ad8\uff0c\u6216\u9700\u8981\u8fc7\u591a\u6b65\u9aa4\u8fbe\u6210\u76ee\u6807\u3002", "method": "\u5f00\u53d1\u4e86POrTAL\u7b97\u6cd5\uff0c\u878d\u5408\u4e86FF-Replan\u548cPOMCP\u4e24\u79cd\u57fa\u7ebf\u89c4\u5212\u7b97\u6cd5\u7684\u4f18\u70b9\uff0c\u901a\u8fc7\u524d\u77bb\u6027\u6811\u7ec4\u88c5\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6982\u7387\u89c4\u5212\u3002", "result": "\u5728\u4e00\u7cfb\u5217\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cPOrTAL\u80fd\u591f\u5feb\u901f\u627e\u5230\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6b65\u9aa4\u6570\u91cf\u4e0a\u4f18\u4e8eFF-Replan\u548cPOMCP\u57fa\u7ebf\u7b97\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u65f6\u95f4\u7ea6\u675f\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002", "conclusion": "POrTAL\u7b97\u6cd5\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u89c4\u5212\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2512.05989", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.05989", "abs": "https://arxiv.org/abs/2512.05989", "authors": ["Selma Dahms", "Luca Torresi", "Shahbaz Tareq Bandesha", "Jan Hansmann", "Holger R\u00f6hm", "Alexander Colsmann", "Marco Schott", "Pascal Friederich"], "title": "A self-driving lab for solution-processed electrochromic thin films", "comment": null, "summary": "Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5229\u7528\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u5ba4\u52a0\u901f\u7535\u81f4\u53d8\u8272\u6d82\u5c42\u5f00\u53d1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u81ea\u52a8\u5316\u4e0e\u673a\u5668\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u9ad8\u6548\u5904\u7406\u53c2\u6570\u63a2\u7d22\u3002", "motivation": "\u65cb\u6d82\u7535\u81f4\u53d8\u8272\u8584\u819c\u5c42\u7684\u4f18\u5316\u590d\u6742\u6027\u7ed9\u5feb\u901f\u5f00\u53d1\u5e26\u6765\u6311\u6218\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u6750\u6599\u53d1\u73b0\u548c\u5de5\u827a\u4f18\u5316\u3002", "method": "\u7ed3\u5408\u81ea\u52a8\u5316\u6570\u636e\u91c7\u96c6\u3001\u56fe\u50cf\u5904\u7406\u3001\u5149\u8c31\u5206\u6790\u548c\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u6784\u5efa\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u5ba4\u7cfb\u7edf\u6765\u63a2\u7d22\u5904\u7406\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u901a\u91cf\uff0c\u8fd8\u5b9e\u73b0\u4e86\u5bf9\u6700\u4f18\u5904\u7406\u53c2\u6570\u7684\u9488\u5bf9\u6027\u641c\u7d22\uff0c\u53ef\u5e94\u7528\u4e8e\u5404\u79cd\u6eb6\u6db2\u5904\u7406\u6750\u6599\u3002", "conclusion": "\u81ea\u52a8\u9a7e\u9a76\u5b9e\u9a8c\u5ba4\u5728\u589e\u5f3a\u6750\u6599\u53d1\u73b0\u548c\u5de5\u827a\u4f18\u5316\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u7535\u81f4\u53d8\u8272\u6750\u6599\u5f00\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.05993", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.05993", "abs": "https://arxiv.org/abs/2512.05993", "authors": ["Ruchika Verma", "Shrishtee Kandoi", "Robina Afzal", "Shengjia Chen", "Jannes Jegminat", "Michael W. Karlovich", "Melissa Umphlett", "Timothy E. Richardson", "Kevin Clare", "Quazi Hossain", "Jorge Samanamud", "Phyllis L. Faust", "Elan D. Louis", "Ann C. McKee", "Thor D. Stein", "Jonathan D. Cherry", "Jesse Mez", "Anya C. McGoldrick", "Dalilah D. Quintana Mora", "Melissa J. Nirenberg", "Ruth H. Walker", "Yolfrankcis Mendez", "Susan Morgello", "Dennis W. Dickson", "Melissa E. Murray", "Carlos Cordon-Cardo", "Nadejda M. Tsankova", "Jamie M. Walker", "Diana K. Dangoor", "Stephanie McQuillan", "Emma L. Thorn", "Claudia De Sanctis", "Shuying Li", "Thomas J. Fuchs", "Kurt Farrell", "John F. Crary", "Gabriele Campanella"], "title": "Domain-Specific Foundation Model Improves AI-Based Analysis of Neuropathology", "comment": null, "summary": "Foundation models have transformed computational pathology by providing generalizable representations from large-scale histology datasets. However, existing models are predominantly trained on surgical pathology data, which is enriched for non-nervous tissue and overrepresents neoplastic, inflammatory, metabolic, and other non-neurological diseases. Neuropathology represents a markedly different domain of histopathology, characterized by unique cell types (neurons, glia, etc.), distinct cytoarchitecture, and disease-specific pathological features including neurofibrillary tangles, amyloid plaques, Lewy bodies, and pattern-specific neurodegeneration. This domain mismatch may limit the ability of general-purpose foundation models to capture the morphological patterns critical for interpreting neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, and cerebellar ataxias. To address this gap, we developed NeuroFM, a foundation model trained specifically on whole-slide images of brain tissue spanning diverse neurodegenerative pathologies. NeuroFM demonstrates superior performance compared to general-purpose models across multiple neuropathology-specific downstream tasks, including mixed dementia disease classification, hippocampal region segmentation, and neurodegenerative ataxia identification encompassing cerebellar essential tremor and spinocerebellar ataxia subtypes. This work establishes that domain-specialized foundation models trained on brain tissue can better capture neuropathology-specific features than models trained on general surgical pathology datasets. By tailoring foundation models to the unique morphological landscape of neurodegenerative diseases, NeuroFM enables more accurate and reliable AI-based analysis for brain disease diagnosis and research, setting a precedent for domain-specific model development in specialized areas of digital pathology.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86NeuroFM\uff0c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u795e\u7ecf\u75c5\u7406\u5b66\u9886\u57df\u7684\u75c5\u7406\u57fa\u7840\u6a21\u578b\uff0c\u76f8\u6bd4\u901a\u7528\u6a21\u578b\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u5206\u6790\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u75c5\u7406\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u5916\u79d1\u75c5\u7406\u6570\u636e\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u795e\u7ecf\u75c5\u7406\u5b66\u72ec\u7279\u7279\u5f81\uff08\u5982\u795e\u7ecf\u5143\u3001\u80f6\u8d28\u7ec6\u80de\u7b49\uff09\u7684\u6355\u6349\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5728\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1NeuroFM\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u95e8\u4f7f\u7528\u8111\u7ec4\u7ec7\u5168\u73bb\u7247\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u6db5\u76d6\u591a\u79cd\u795e\u7ecf\u9000\u884c\u6027\u75c5\u7406\u3002", "result": "NeuroFM\u5728\u6df7\u5408\u6027\u75f4\u5446\u5206\u7c7b\u3001\u6d77\u9a6c\u533a\u57df\u5206\u5272\u3001\u795e\u7ecf\u9000\u884c\u6027\u5171\u6d4e\u5931\u8c03\u8bc6\u522b\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "\u9886\u57df\u4e13\u4e1a\u5316\u7684\u57fa\u7840\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u795e\u7ecf\u75c5\u7406\u5b66\u7279\u5f02\u6027\u7279\u5f81\uff0c\u4e3a\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u7684\u4e13\u4e1a\u9886\u57df\u6a21\u578b\u5f00\u53d1\u6811\u7acb\u4e86\u5148\u4f8b\u3002"}}
{"id": "2512.06017", "categories": ["cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.06017", "abs": "https://arxiv.org/abs/2512.06017", "authors": ["Laurence Liang"], "title": "Training-Free Robot Pose Estimation using Off-the-Shelf Foundational Models", "comment": "Accepted at CVIS 2025", "summary": "Pose estimation of a robot arm from visual inputs is a challenging task. However, with the increasing adoption of robot arms for both industrial and residential use cases, reliable joint angle estimation can offer improved safety and performance guarantees, and also be used as a verifier to further train robot policies. This paper introduces using frontier vision-language models (VLMs) as an ``off-the-shelf\" tool to estimate a robot arm's joint angles from a single target image. By evaluating frontier VLMs on both synthetic and real-world image-data pairs, this paper establishes a performance baseline attained by current FLMs. In addition, this paper presents empirical results suggesting that test time scaling or parameter scaling alone does not lead to improved joint angle predictions.", "AI": {"tldr": "\u4f7f\u7528\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u73b0\u6210\u5de5\u5177\uff0c\u4ece\u5355\u5f20\u76ee\u6807\u56fe\u50cf\u4f30\u8ba1\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u4e0a\u7684\u6027\u80fd\u57fa\u51c6\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u624b\u81c2\u5728\u5de5\u4e1a\u548c\u4f4f\u5b85\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u53ef\u9760\u7684\u5173\u8282\u89d2\u5ea6\u4f30\u8ba1\u53ef\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6027\u80fd\u4fdd\u8bc1\uff0c\u5e76\u53ef\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u8fdb\u4e00\u6b65\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002", "method": "\u5c06\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u73b0\u6210\u5de5\u5177\uff0c\u4ece\u5355\u5f20\u76ee\u6807\u56fe\u50cf\u4f30\u8ba1\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u65b9\u9762\u7684\u6027\u80fd\u57fa\u51c6\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u4ec5\u9760\u6d4b\u8bd5\u65f6\u7f29\u653e\u6216\u53c2\u6570\u7f29\u653e\u5e76\u4e0d\u80fd\u6539\u5584\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "\u524d\u6cbf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3a\u73b0\u6210\u5de5\u5177\u7528\u4e8e\u673a\u5668\u4eba\u624b\u81c2\u5173\u8282\u89d2\u5ea6\u4f30\u8ba1\uff0c\u4f46\u9700\u8981\u66f4\u6709\u6548\u7684\u7f29\u653e\u7b56\u7565\u6765\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.05990", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.05990", "abs": "https://arxiv.org/abs/2512.05990", "authors": ["Xin Li"], "title": "Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure", "comment": null, "summary": "Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \\textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \\textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \\textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \\textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \\textbf{Search $\\to$ Closure $\\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \\textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \\textit{Dynamic Programming} in P) via the mechanism of \\textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Memory-Amortized Inference (MAI)\u6846\u67b6\uff0c\u57fa\u4e8e\u4ee3\u6570\u62d3\u6251\u7edf\u4e00\u5b66\u4e60\u548c\u8bb0\u5fc6\uff0c\u901a\u8fc7\u540c\u8c03\u5947\u5076\u6027\u539f\u7406\u533a\u5206\u5185\u5bb9\u4e0e\u4e0a\u4e0b\u6587\uff0c\u5c06\u9ad8\u590d\u6742\u5ea6\u641c\u7d22\u8f6c\u5316\u4e3a\u4f4e\u590d\u6742\u5ea6\u67e5\u627e\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u673a\u5668\u5b66\u4e60\u4e2d\u53c2\u6570\u9759\u6001\u7ed3\u6784\u4e0e\u63a8\u7406\u52a8\u6001\u6d41\u7a0b\u5206\u79bb\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u70ed\u529b\u5b66\u6548\u7387\uff0c\u6a21\u4eff\u751f\u7269\u8ba4\u77e5\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u4ee3\u6570\u62d3\u6251\u7684MAI\u6846\u67b6\uff0c\u5305\u542b\u540c\u8c03\u5947\u5076\u6027\u539f\u7406\u548c\u62d3\u6251\u4e09\u4f4d\u4e00\u4f53\u53d8\u6362\uff08\u641c\u7d22\u2192\u95ed\u5408\u2192\u7ed3\u6784\uff09\uff0c\u901a\u8fc7\u62d3\u6251\u5faa\u73af\u95ed\u5408\u673a\u5236\u5b9e\u73b0\u590d\u6742\u5ea6\u964d\u4f4e\u3002", "result": "\u5efa\u7acb\u4e86\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u5feb\u901f\u601d\u7ef4\u4ece\u6162\u901f\u601d\u7ef4\u4e2d\u6d8c\u73b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u4e3a\u540e\u56fe\u7075\u67b6\u6784\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002", "conclusion": "MAI\u6846\u67b6\u4e3a\u7edf\u4e00\u5b66\u4e60\u548c\u8bb0\u5fc6\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6570\u5b66\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u66f4\u9ad8\u6548\u7684\u8ba4\u77e5\u8ba1\u7b97\u67b6\u6784\u53d1\u5c55\u3002"}}
{"id": "2512.05996", "categories": ["cs.CV", "cs.CY", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.05996", "abs": "https://arxiv.org/abs/2512.05996", "authors": ["Yi Liu", "Jingyu Song", "Vedanth Kallakuri", "Katherine A. Skinner"], "title": "FishDetector-R1: Unified MLLM-Based Framework with Reinforcement Fine-Tuning for Weakly Supervised Fish Detection, Segmentation, and Counting", "comment": "18 pages, under review", "summary": "Analyzing underwater fish imagery is critical for ecological monitoring but remains difficult due to visual degradation and costly annotations. We introduce FishDetector-R1, a unified MLLM-based framework for fish detection, segmentation, and counting under weak supervision. On the DeepFish dataset, our framework achieves substantial gains over baselines, improving AP by 20% and mIoU by 10%, while reducing MAE by 30% and GAME by 35%. These improvements stem from two key components: a novel detect-to-count prompt that enforces spatially consistent detections and counts, and Reinforcement Learning from Verifiable Reward (RLVR) with a complementary scalable paradigm leveraging sparse point labels. Ablation studies further validate the effectiveness of this reward design. Moreover, the improvement generalizes well to other underwater datasets, confirming strong cross-domain robustness. Overall, FishDetector-R1 provides a reliable and scalable solution for accurate marine visual understanding via weak supervision. The project page for FishDetector-R1 is https://umfieldrobotics.github.io/FishDetector-R1.", "AI": {"tldr": "FishDetector-R1\u662f\u4e00\u4e2a\u57fa\u4e8eMLLM\u7684\u5f31\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u3001\u5206\u5272\u548c\u8ba1\u6570\uff0c\u5728DeepFish\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u6307\u6807", "motivation": "\u6c34\u4e0b\u9c7c\u7c7b\u56fe\u50cf\u5206\u6790\u5bf9\u751f\u6001\u76d1\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u89c6\u89c9\u8d28\u91cf\u5dee\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898", "method": "\u7ed3\u5408\u65b0\u9896\u7684detect-to-count\u63d0\u793a\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u9a8c\u8bc1\u5956\u52b1\u673a\u5236(RLVR)\uff0c\u5229\u7528\u7a00\u758f\u70b9\u6807\u7b7e\u8fdb\u884c\u5f31\u76d1\u7763\u5b66\u4e60", "result": "\u5728DeepFish\u6570\u636e\u96c6\u4e0aAP\u63d0\u534720%\uff0cmIoU\u63d0\u534710%\uff0cMAE\u964d\u4f4e30%\uff0cGAME\u964d\u4f4e35%\uff0c\u5e76\u5728\u5176\u4ed6\u6c34\u4e0b\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "FishDetector-R1\u4e3a\u5f31\u76d1\u7763\u4e0b\u7684\u51c6\u786e\u6d77\u6d0b\u89c6\u89c9\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06038", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06038", "abs": "https://arxiv.org/abs/2512.06038", "authors": ["Kelsey Fontenot", "Anjali Gorti", "Iva Goel", "Tonio Buonassisi", "Alexander E. Siemenn"], "title": "Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction", "comment": "15 pages, 8 figures", "summary": "Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86ASHE\u7cfb\u7edf\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u6280\u672f\u3001\u53cc\u6267\u884c\u5668\u5206\u914d\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u57fa\u677f\u5904\u7406\u4e0e\u4ea4\u6362\uff0c\u63d0\u9ad8\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "motivation": "\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u5728\u5316\u5b66\u548c\u6750\u6599\u5b9e\u9a8c\u81ea\u52a8\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u57fa\u677f\u5904\u7406\u548c\u91cd\u65b0\u52a0\u8f7d\u73af\u8282\u5e38\u88ab\u5ffd\u89c6\uff0c\u9650\u5236\u4e86\u81ea\u52a8\u5316\u7ba1\u9053\u7684\u5b8c\u6574\u6027\u3002", "method": "\u91c7\u7528\u673a\u5668\u4eba\u6280\u672f\u3001\u53cc\u6267\u884c\u5668\u5206\u914d\u5668\u548c\u6df1\u5ea6\u5b66\u4e60\u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\uff0c\u5f00\u53d1\u4e86\u81ea\u52a8\u57fa\u677f\u5904\u7406\u4e0e\u4ea4\u6362\uff08ASHE\uff09\u65b9\u6cd5\uff0c\u80fd\u591f\u68c0\u6d4b\u5e76\u7ea0\u6b63\u8106\u5f31\u900f\u660e\u57fa\u677f\u64cd\u4f5c\u4e2d\u7684\u9519\u8bef\u3002", "result": "\u5728130\u6b21\u72ec\u7acb\u8bd5\u9a8c\u4e2d\uff0cASHE\u7cfb\u7edf\u5b9e\u73b0\u4e8698.5%\u7684\u9996\u6b21\u653e\u7f6e\u51c6\u786e\u7387\uff0c\u4ec5\u53d1\u751f\u4e24\u6b21\u57fa\u677f\u9519\u4f4d\u4e14\u5747\u88ab\u6210\u529f\u68c0\u6d4b\u5e76\u81ea\u52a8\u7ea0\u6b63\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u66f4\u51c6\u786e\u53ef\u9760\u7684\u57fa\u677f\u5904\u7406\u65b9\u6cd5\uff0cASHE\u7cfb\u7edf\u63d0\u5347\u4e86\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u52a0\u901f\u65b0\u578b\u5316\u5b66\u548c\u6750\u6599\u53d1\u73b0\u3002"}}
{"id": "2512.06059", "categories": ["cs.LG", "physics.app-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2512.06059", "abs": "https://arxiv.org/abs/2512.06059", "authors": ["Andrea Della Valle", "Annalisa D'Arco", "Tiziana Mancini", "Rosanna Mosetti", "Maria Chiara Paolozzi", "Stefano Lupi", "Sebastiano Pilati", "Andrea Perali"], "title": "Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra", "comment": null, "summary": "Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b9e\u9a8c\u6570\u636e\u548c\u5408\u6210\u5149\u8c31\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u589e\u5f3aVOC\u7ea2\u5916\u5149\u8c31\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51fa\u80fd\u591f\u51c6\u786e\u8bc6\u522b9\u79cdVOC\u5e76\u9884\u6d4b\u5176\u6d53\u5ea6\u7684\u5224\u522b\u795e\u7ecf\u7f51\u7edc\u3002", "motivation": "VOC\u5bf9\u4eba\u4f53\u5065\u5eb7\u6709\u91cd\u5927\u98ce\u9669\uff0c\u9700\u8981\u51c6\u786e\u68c0\u6d4b\u3002\u7ea2\u5916\u5149\u8c31\u6280\u672f\u867d\u7136\u80fd\u5b9e\u73b0\u8d85\u7075\u654f\u68c0\u6d4b\uff0c\u4f46\u590d\u6742\u7684\u5149\u8c31\u9650\u5236\u4e86\u5b9e\u65f6\u8bc6\u522b\u548c\u5b9a\u91cf\u5206\u6790\u7684\u53ef\u80fd\u6027\u3002\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u5927\u91cf\u6570\u636e\u8bad\u7ec3\uff0c\u800c\u5b9e\u9a8c\u6570\u636e\u83b7\u53d6\u56f0\u96be\u3002", "method": "\u521b\u5efa\u4e869\u79cdVOC\u5728\u4e0d\u540c\u6d53\u5ea6\u4e0b\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u6761\u4ef6\u751f\u6210\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u5408\u6210\u5149\u8c31\u6765\u589e\u5f3a\u6570\u636e\u96c6\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0c\u7136\u540e\u8bad\u7ec3\u5224\u522b\u795e\u7ecf\u7f51\u7edc\u8fdb\u884cVOC\u8bc6\u522b\u548c\u6d53\u5ea6\u9884\u6d4b\u3002", "result": "\u8bad\u7ec3\u51fa\u7684\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u53ef\u9760\u5730\u8bc6\u522b9\u79cdVOC\uff0c\u5e76\u7cbe\u786e\u9884\u6d4b\u5176\u6d53\u5ea6\uff0c\u9002\u5408\u96c6\u6210\u5230VOC\u4f20\u611f\u8bbe\u5907\u4e2d\u8fdb\u884c\u5b9e\u65f6\u8bc6\u522b\u548c\u5206\u6790\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86VOC\u7ea2\u5916\u5149\u8c31\u8bc6\u522b\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6VOC\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2512.06003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06003", "abs": "https://arxiv.org/abs/2512.06003", "authors": ["Ramin Sharifi", "Pouya Shiri", "Amirali Baniasadi"], "title": "PrunedCaps: A Case For Primary Capsules Discrimination", "comment": null, "summary": "Capsule Networks (CapsNets) are a generation of image classifiers with proven advantages over Convolutional Neural Networks (CNNs). Better robustness to affine transformation and overlapping image detection are some of the benefits associated with CapsNets. However, CapsNets cannot be classified as resource-efficient deep learning architecture due to the high number of Primary Capsules (PCs). In addition, CapsNets' training and testing are slow and resource hungry. This paper investigates the possibility of Primary Capsules pruning in CapsNets on MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and SVHN datasets. We show that a pruned version of CapsNet performs up to 9.90 times faster than the conventional architecture by removing 95 percent of Capsules without a loss of accuracy. Also, our pruned architecture saves on more than 95.36 percent of floating-point operations in the dynamic routing stage of the architecture. Moreover, we provide insight into why some datasets benefit significantly from pruning while others fall behind.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86Capsule Networks\u4e2dPrimary Capsules\u7684\u526a\u679d\u53ef\u80fd\u6027\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u526a\u679d\u540e\u7684CapsNet\u6027\u80fd\uff0c\u53d1\u73b0\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u901f\u5ea6\u5e76\u51cf\u5c11\u8ba1\u7b97\u91cf\u800c\u4e0d\u635f\u5931\u7cbe\u5ea6\u3002", "motivation": "CapsNets\u867d\u7136\u6bd4CNNs\u5177\u6709\u66f4\u597d\u7684\u4eff\u5c04\u53d8\u6362\u9c81\u68d2\u6027\u548c\u91cd\u53e0\u56fe\u50cf\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u7531\u4e8ePrimary Capsules\u6570\u91cf\u8fc7\u591a\uff0c\u5bfc\u81f4\u8bad\u7ec3\u548c\u6d4b\u8bd5\u901f\u5ea6\u6162\u3001\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9700\u8981\u63d0\u9ad8\u5176\u8d44\u6e90\u6548\u7387\u3002", "method": "\u5728MNIST\u3001Fashion-MNIST\u3001CIFAR-10\u548cSVHN\u6570\u636e\u96c6\u4e0a\u5bf9CapsNets\u8fdb\u884cPrimary Capsules\u526a\u679d\u5b9e\u9a8c\uff0c\u79fb\u966495%\u7684Capsules\u3002", "result": "\u526a\u679d\u540e\u7684CapsNet\u6bd4\u4f20\u7edf\u67b6\u6784\u5feb9.90\u500d\uff0c\u52a8\u6001\u8def\u7531\u9636\u6bb5\u6d6e\u70b9\u8fd0\u7b97\u51cf\u5c1195.36%\u4ee5\u4e0a\uff0c\u4e14\u7cbe\u5ea6\u6ca1\u6709\u635f\u5931\u3002\u540c\u65f6\u5206\u6790\u4e86\u4e0d\u540c\u6570\u636e\u96c6\u5bf9\u526a\u679d\u6548\u679c\u7684\u5dee\u5f02\u539f\u56e0\u3002", "conclusion": "Primary Capsules\u526a\u679d\u662f\u53ef\u884c\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u5347CapsNets\u7684\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06112", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06112", "abs": "https://arxiv.org/abs/2512.06112", "authors": ["Yifang Xu", "Jiahao Cui", "Feipeng Cai", "Zhihao Zhu", "Hanlin Shang", "Shan Luan", "Mingwang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving", "comment": "18 pages, 11 figures", "summary": "We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.", "AI": {"tldr": "WAM-Flow\u662f\u4e00\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5c06\u8f68\u8ff9\u89c4\u5212\u4f5c\u4e3a\u7ed3\u6784\u5316\u6807\u8bb0\u7a7a\u95f4\u4e2d\u7684\u79bb\u6563\u6d41\u5339\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u53cc\u5411\u53bb\u566a\u5b9e\u73b0\u7c97\u5230\u7ec6\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u89e3\u7801\u5668\u5728\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u4e2d\u6548\u7387\u8f83\u4f4e\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u751f\u6210\u65b9\u6cd5\uff0c\u540c\u65f6\u8981\u5e73\u8861\u8ba1\u7b97\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3001\u8212\u9002\u5ea6\u7b49\u591a\u76ee\u6807\u4f18\u5316\u3002", "method": "\u7ed3\u5408\u5ea6\u91cf\u5bf9\u9f50\u7684\u6570\u503c\u6807\u8bb0\u5668\uff08\u901a\u8fc7\u4e09\u5143\u7ec4\u8fb9\u754c\u5b66\u4e60\u4fdd\u6301\u51e0\u4f55\u4fe1\u606f\uff09\u3001\u51e0\u4f55\u611f\u77e5\u6d41\u76ee\u6807\u51fd\u6570\u548c\u6a21\u62df\u5668\u5f15\u5bfc\u7684GRPO\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u4e3b\u5e72\u7f51\u7edc\u8f6c\u6362\u4e3a\u975e\u56e0\u679c\u6d41\u6a21\u578b\u3002", "result": "\u5728NAVSIM v1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c1\u6b65\u63a8\u7406\u8fbe\u523089.1 PDMS\uff0c5\u6b65\u63a8\u7406\u8fbe\u523090.3 PDMS\uff0c\u8d85\u8d8a\u4e86\u81ea\u56de\u5f52\u548c\u57fa\u4e8e\u6269\u6563\u7684VLA\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u79bb\u6563\u6d41\u5339\u914d\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u5e76\u884c\u751f\u6210\u548c\u53ef\u8c03\u8ba1\u7b97\u7cbe\u5ea6\u6743\u8861\u7684\u4f18\u52bf\u3002"}}
{"id": "2512.06062", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06062", "abs": "https://arxiv.org/abs/2512.06062", "authors": ["S. M. Mustaqim", "Anantaa Kotal", "Paul H. Yi"], "title": "When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models", "comment": null, "summary": "Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u548c\u8d28\u5fc3\u5206\u6790\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e2d\u4ecd\u4f1a\u6cc4\u9732\u8bad\u7ec3\u6837\u672c\u9690\u79c1\u4fe1\u606f\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u88ab\u5e7f\u6cdb\u7528\u4e8e\u751f\u6210\u9690\u79c1\u4fdd\u62a4\u7684\u5408\u6210\u6570\u636e\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u4f4e\u4f30\u4e86\u6570\u636e\u6d41\u5f62\u7ed3\u6784\u91cd\u53e0\u5e26\u6765\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002\u4f5c\u8005\u53d1\u73b0\u5373\u4f7f\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u7b49\u6280\u672f\uff0c\u5408\u6210\u6570\u636e\u4ecd\u53ef\u80fd\u901a\u8fc7\u5206\u5e03\u90bb\u57df\u63a8\u65ad\u6cc4\u9732\u8bad\u7ec3\u6837\u672c\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u9ed1\u76d2\u6210\u5458\u63a8\u65ad\u653b\u51fb\uff1a1\uff09\u91cd\u590d\u67e5\u8be2\u751f\u6210\u6a21\u578b\u83b7\u53d6\u5927\u91cf\u5408\u6210\u6837\u672c\uff1b2\uff09\u8fdb\u884c\u65e0\u76d1\u7763\u805a\u7c7b\u8bc6\u522b\u5408\u6210\u5206\u5e03\u7684\u5bc6\u96c6\u533a\u57df\uff1b3\uff09\u5206\u6790\u805a\u7c7b\u8d28\u5fc3\u548c\u90bb\u57df\uff0c\u8fd9\u4e9b\u533a\u57df\u5bf9\u5e94\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9ad8\u5bc6\u5ea6\u533a\u57df\uff0c\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u6837\u672c\u7684\u4ee3\u7406\u6765\u63a8\u65ad\u6210\u5458\u8eab\u4efd\u6216\u91cd\u5efa\u8fd1\u4f3c\u8bb0\u5f55\u3002", "result": "\u5728\u533b\u7597\u3001\u91d1\u878d\u7b49\u654f\u611f\u9886\u57df\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u771f\u5b9e\u6570\u636e\u4e0e\u5408\u6210\u6570\u636e\u4e4b\u95f4\u7684\u805a\u7c7b\u91cd\u53e0\u4f1a\u5bfc\u81f4\u53ef\u6d4b\u91cf\u7684\u6210\u5458\u4fe1\u606f\u6cc4\u9732\uff0c\u5373\u4f7f\u751f\u6210\u5668\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u6216\u5176\u4ed6\u566a\u58f0\u673a\u5236\u8fdb\u884c\u8bad\u7ec3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053\u4e2d\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u653b\u51fb\u9762\uff0c\u547c\u5401\u9700\u8981\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u8bc1\uff0c\u4e0d\u4ec5\u8981\u8003\u8651\u6837\u672c\u7ea7\u8bb0\u5fc6\uff0c\u8fd8\u8981\u8003\u8651\u5206\u5e03\u90bb\u57df\u63a8\u65ad\uff0c\u8fd9\u5bf9\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u53d1\u5e03\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.06006", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06006", "abs": "https://arxiv.org/abs/2512.06006", "authors": ["Xuefei", "Wang", "Kai A. Horstmann", "Ethan Lin", "Jonathan Chen", "Alexander R. Farhang", "Sophia Stiles", "Atharva Sehgal", "Jonathan Light", "David Van Valen", "Yisong Yue", "Jennifer J. Sun"], "title": "Simple Agents Outperform Experts in Biomedical Imaging Workflow Optimization", "comment": null, "summary": "Adapting production-level computer vision tools to bespoke scientific datasets is a critical \"last mile\" bottleneck. Current solutions are impractical: fine-tuning requires large annotated datasets scientists often lack, while manual code adaptation costs scientists weeks to months of effort. We consider using AI agents to automate this manual coding, and focus on the open question of optimal agent design for this targeted task. We introduce a systematic evaluation framework for agentic code optimization and use it to study three production-level biomedical imaging pipelines. We demonstrate that a simple agent framework consistently generates adaptation code that outperforms human-expert solutions. Our analysis reveals that common, complex agent architectures are not universally beneficial, leading to a practical roadmap for agent design. We open source our framework and validate our approach by deploying agent-generated functions into a production pipeline, demonstrating a clear pathway for real-world impact.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2aAI\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u79d1\u5b66\u6570\u636e\u96c6\u4e2d\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5de5\u5177\u9002\u914d\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u6216\u624b\u52a8\u7f16\u7801\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc1\u660e\u7b80\u5355\u4ee3\u7406\u8bbe\u8ba1\u80fd\u751f\u6210\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u7684\u9002\u914d\u4ee3\u7801\u3002", "motivation": "\u89e3\u51b3\u751f\u4ea7\u7ea7\u8ba1\u7b97\u673a\u89c6\u89c9\u5de5\u5177\u9002\u914d\u79d1\u5b66\u6570\u636e\u96c6\u7684\"\u6700\u540e\u4e00\u516c\u91cc\"\u74f6\u9888\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u5982\u5fae\u8c03\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff08\u79d1\u5b66\u5bb6\u5f80\u5f80\u7f3a\u4e4f\uff09\uff0c\u800c\u624b\u52a8\u7f16\u7801\u5219\u9700\u8981\u6570\u5468\u81f3\u6570\u6708\u7684\u52aa\u529b\u3002", "method": "\u5f15\u5165\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u7814\u7a76AI\u4ee3\u7406\u5728\u4ee3\u7801\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u751f\u4ea7\u7ea7\u751f\u7269\u533b\u5b66\u6210\u50cf\u6d41\u7a0b\u3002\u6bd4\u8f83\u4e86\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4ee3\u7406\u67b6\u6784\uff0c\u53d1\u73b0\u7b80\u5355\u4ee3\u7406\u6846\u67b6\u6548\u679c\u6700\u4f73\u3002", "result": "\u7b80\u5355\u4ee3\u7406\u6846\u67b6\u751f\u6210\u7684\u9002\u914d\u4ee3\u7801\u6301\u7eed\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u89e3\u51b3\u65b9\u6848\u3002\u590d\u6742\u4ee3\u7406\u67b6\u6784\u5e76\u975e\u666e\u904d\u6709\u76ca\uff0c\u4e3a\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u7ebf\u56fe\u3002", "conclusion": "\u5f00\u6e90\u4e86\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5c06\u4ee3\u7406\u751f\u6210\u7684\u51fd\u6570\u90e8\u7f72\u5230\u751f\u4ea7\u6d41\u7a0b\u4e2d\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684AI\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2512.06130", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06130", "abs": "https://arxiv.org/abs/2512.06130", "authors": ["Grant Stagg", "Isaac E. Weintraub", "Cameron K. Peterson"], "title": "Probabilistic Weapon Engagement Zones for a Turn Constrained Pursuer", "comment": "Accepted for presentation at AIAA SciTech 2026. 17 pages, 7 figures", "summary": "Curve-straight probabilistic engagement zones (CSPEZ) quantify the spatial regions an evader should avoid to reduce capture risk from a turn-rate-limited pursuer following a curve-straight path with uncertain parameters including position, heading, velocity, range, and maximum turn rate. This paper presents methods for generating evader trajectories that minimize capture risk under such uncertainty. We first derive an analytic solution for the deterministic curve-straight basic engagement zone (CSBEZ), then extend this formulation to a probabilistic framework using four uncertainty-propagation approaches: Monte Carlo sampling, linearization, quadratic approximation, and neural-network regression. We evaluate the accuracy and computational cost of each approximation method and demonstrate how CSPEZ constraints can be integrated into a trajectory-optimization algorithm to produce safe paths that explicitly account for pursuer uncertainty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u66f2\u7ebf-\u76f4\u7ebf\u6982\u7387\u4ea4\u6218\u533a\u57df(CSPEZ)\u6982\u5ff5\uff0c\u7528\u4e8e\u91cf\u5316\u89c4\u907f\u8005\u5e94\u907f\u514d\u7684\u7a7a\u95f4\u533a\u57df\u4ee5\u964d\u4f4e\u88ab\u8f6c\u5f2f\u7387\u53d7\u9650\u7684\u8ffd\u8e2a\u8005\u6355\u83b7\u7684\u98ce\u9669\u3002\u5f00\u53d1\u4e86\u5728\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u4e0b\u6700\u5c0f\u5316\u6355\u83b7\u98ce\u9669\u7684\u8f68\u8ff9\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4ea4\u6218\u533a\u57df\u5206\u6790\u901a\u5e38\u5047\u8bbe\u8ffd\u8e2a\u8005\u53c2\u6570\u5b8c\u5168\u5df2\u77e5\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u4f4d\u7f6e\u3001\u822a\u5411\u3001\u901f\u5ea6\u3001\u8ddd\u79bb\u548c\u6700\u5927\u8f6c\u5f2f\u7387\u7b49\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u7684\u6982\u7387\u6846\u67b6\u6765\u751f\u6210\u66f4\u5b89\u5168\u7684\u89c4\u907f\u8f68\u8ff9\u3002", "method": "\u9996\u5148\u63a8\u5bfc\u4e86\u786e\u5b9a\u6027\u66f2\u7ebf-\u76f4\u7ebf\u57fa\u672c\u4ea4\u6218\u533a\u57df(CSBEZ)\u7684\u89e3\u6790\u89e3\uff0c\u7136\u540e\u4f7f\u7528\u56db\u79cd\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u65b9\u6cd5\u6269\u5c55\u4e3a\u6982\u7387\u6846\u67b6\uff1a\u8499\u7279\u5361\u6d1b\u91c7\u6837\u3001\u7ebf\u6027\u5316\u3001\u4e8c\u6b21\u8fd1\u4f3c\u548c\u795e\u7ecf\u7f51\u7edc\u56de\u5f52\u3002\u6700\u540e\u5c06CSPEZ\u7ea6\u675f\u96c6\u6210\u5230\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u4e2d\u3002", "result": "\u8bc4\u4f30\u4e86\u5404\u8fd1\u4f3c\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u8bc1\u660e\u4e86CSPEZ\u7ea6\u675f\u53ef\u4ee5\u6709\u6548\u5730\u96c6\u6210\u5230\u8f68\u8ff9\u4f18\u5316\u7b97\u6cd5\u4e2d\uff0c\u751f\u6210\u80fd\u591f\u660e\u786e\u8003\u8651\u8ffd\u8e2a\u8005\u4e0d\u786e\u5b9a\u6027\u7684\u5b89\u5168\u8def\u5f84\u3002", "conclusion": "\u63d0\u51fa\u7684CSPEZ\u6846\u67b6\u4e3a\u5728\u8ffd\u8e2a\u8005\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u89c4\u907f\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6982\u7387\u5de5\u5177\uff0c\u56db\u79cd\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u65b9\u6cd5\u5404\u6709\u4f18\u52a3\uff0c\u53ef\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.06102", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06102", "abs": "https://arxiv.org/abs/2512.06102", "authors": ["Ufuk \u00c7ak\u0131r", "Victor-Alexandru Darvariu", "Bruno Lacerda", "Nick Hawes"], "title": "JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning", "comment": "To be presented at the NeurIPS 2025 Workshop on Machine Learning and the Physical Sciences (ML4PS)", "summary": "Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.", "AI": {"tldr": "JaxWildfire\u662f\u4e00\u4e2a\u57fa\u4e8eJAX\u5b9e\u73b0\u7684\u9ad8\u6027\u80fd\u91ce\u706b\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u8ba1\u7b97\u5728GPU\u4e0a\u5b9e\u73b06-35\u500d\u52a0\u901f\uff0c\u652f\u6301\u57fa\u4e8e\u68af\u5ea6\u7684\u53c2\u6570\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u91ce\u706b\u6a21\u62df\u5668\u901f\u5ea6\u6162\uff0c\u9650\u5236\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u7136\u707e\u5bb3\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u6a21\u62df\u5668\u6765\u652f\u6301RL\u7b97\u6cd5\u7684\u8bad\u7ec3\u3002", "method": "\u57fa\u4e8e\u6982\u7387\u7ec6\u80de\u81ea\u52a8\u673a\u7684\u91ce\u706b\u4f20\u64ad\u6a21\u578b\uff0c\u4f7f\u7528JAX\u6846\u67b6\u5b9e\u73b0\u5411\u91cf\u5316\u6a21\u62df\uff0c\u5229\u7528vmap\u5b9e\u73b0GPU\u4e0a\u7684\u9ad8\u541e\u5410\u91cf\u8ba1\u7b97\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u8f6f\u4ef6\u5b9e\u73b06-35\u500d\u52a0\u901f\uff0c\u80fd\u591f\u8fdb\u884c\u57fa\u4e8e\u68af\u5ea6\u7684\u53c2\u6570\u4f18\u5316\uff0c\u5e76\u6210\u529f\u8bad\u7ec3RL\u4ee3\u7406\u5b66\u4e60\u91ce\u706b\u6291\u5236\u7b56\u7565\u3002", "conclusion": "JaxWildfire\u4e3a\u63a8\u8fdb\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u7136\u707e\u5bb3\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u662f\u5411\u4e3b\u52a8\u5f0f\u707e\u5bb3\u7ba1\u7406\u7b56\u7565\u8fc8\u8fdb\u7684\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2512.06010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06010", "abs": "https://arxiv.org/abs/2512.06010", "authors": ["Thomas Massena", "Corentin Friedrich", "Franck Mamalet", "Mathieu Serrurier"], "title": "Fast and Flexible Robustness Certificates for Semantic Segmentation", "comment": null, "summary": "Deep Neural Networks are vulnerable to small perturbations that can drastically alter their predictions for perceptually unchanged inputs. The literature on adversarially robust Deep Learning attempts to either enhance the robustness of neural networks (e.g, via adversarial training) or to certify their decisions up to a given robustness level (e.g, by using randomized smoothing, formal methods or Lipschitz bounds). These studies mostly focus on classification tasks and few efficient certification procedures currently exist for semantic segmentation. In this work, we introduce a new class of certifiably robust Semantic Segmentation networks with built-in Lipschitz constraints that are efficiently trainable and achieve competitive pixel accuracy on challenging datasets such as Cityscapes. Additionally, we provide a novel framework that generalizes robustness certificates for semantic segmentation tasks, where we showcase the flexibility and computational efficiency of using Lipschitz networks. Our approach unlocks real-time compatible certifiably robust semantic segmentation for the first time. Moreover, it allows the computation of worst-case performance under $\\ell_2$ attacks of radius $\u03b5$ across a wide range of performance measures. Crucially, we benchmark the runtime of our certification process and find our approach to be around 600 times faster than randomized smoothing methods at inference with comparable certificates on an NVIDIA A100 GPU. Finally, we evaluate the tightness of our worstcase certificates against state-of-the-art adversarial attacks to further validate the performance of our method.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53ef\u8ba4\u8bc1\u9c81\u68d2\u7684\u8bed\u4e49\u5206\u5272\u7f51\u7edc\uff0c\u901a\u8fc7\u5185\u7f6eLipschitz\u7ea6\u675f\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u5728Cityscapes\u7b49\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u50cf\u7d20\u7cbe\u5ea6\uff0c\u8ba4\u8bc1\u901f\u5ea6\u6bd4\u968f\u673a\u5e73\u6ed1\u65b9\u6cd5\u5feb\u7ea6600\u500d\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u5fae\u5c0f\u6270\u52a8\u654f\u611f\uff0c\u73b0\u6709\u9c81\u68d2\u6027\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7c7b\u4efb\u52a1\uff0c\u8bed\u4e49\u5206\u5272\u7684\u9ad8\u6548\u8ba4\u8bc1\u65b9\u6cd5\u8f83\u5c11\u3002", "method": "\u91c7\u7528\u5185\u7f6eLipschitz\u7ea6\u675f\u7684\u8bed\u4e49\u5206\u5272\u7f51\u7edc\uff0c\u63d0\u4f9b\u53ef\u6cdb\u5316\u7684\u9c81\u68d2\u6027\u8ba4\u8bc1\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u6027\u80fd\u6307\u6807\u5728\u21132\u653b\u51fb\u4e0b\u7684\u6700\u574f\u60c5\u51b5\u6027\u80fd\u8ba1\u7b97\u3002", "result": "\u5728NVIDIA A100 GPU\u4e0a\u8ba4\u8bc1\u901f\u5ea6\u6bd4\u968f\u673a\u5e73\u6ed1\u65b9\u6cd5\u5feb600\u500d\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u7ade\u4e89\u6027\u50cf\u7d20\u7cbe\u5ea6\uff0c\u6700\u574f\u60c5\u51b5\u8bc1\u4e66\u7d27\u5bc6\u5ea6\u901a\u8fc7\u5bf9\u6297\u653b\u51fb\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u5b9e\u65f6\u517c\u5bb9\u7684\u53ef\u8ba4\u8bc1\u9c81\u68d2\u8bed\u4e49\u5206\u5272\uff0c\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u9c81\u68d2\u6027\u8ba4\u8bc1\u65b9\u6848\u3002"}}
{"id": "2512.06147", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.06147", "abs": "https://arxiv.org/abs/2512.06147", "authors": ["Hochul Hwang", "Soowan Yang", "Jahir Sadik Monon", "Nicholas A Giudice", "Sunghoon Ivan Lee", "Joydeep Biswas", "Donghyun Kim"], "title": "GuideNav: User-Informed Development of a Vision-Only Robotic Navigation Assistant For Blind Travelers", "comment": null, "summary": "While commendable progress has been made in user-centric research on mobile assistive systems for blind and low-vision (BLV) individuals, references that directly inform robot navigation design remain rare. To bridge this gap, we conducted a comprehensive human study involving interviews with 26 guide dog handlers, four white cane users, nine guide dog trainers, and one O\\&M trainer, along with 15+ hours of observing guide dog-assisted walking. After de-identification, we open-sourced the dataset to promote human-centered development and informed decision-making for assistive systems for BLV people. Building on insights from this formative study, we developed GuideNav, a vision-only, teach-and-repeat navigation system. Inspired by how guide dogs are trained and assist their handlers, GuideNav autonomously repeats a path demonstrated by a sighted person using a robot. Specifically, the system constructs a topological representation of the taught route, integrates visual place recognition with temporal filtering, and employs a relative pose estimator to compute navigation actions - all without relying on costly, heavy, power-hungry sensors such as LiDAR. In field tests, GuideNav consistently achieved kilometer-scale route following across five outdoor environments, maintaining reliability despite noticeable scene variations between teach and repeat runs. A user study with 3 guide dog handlers and 1 guide dog trainer further confirmed the system's feasibility, marking (to our knowledge) the first demonstration of a quadruped mobile system retrieving a path in a manner comparable to guide dogs.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86GuideNav\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528\u89c6\u89c9\u7684\u6559\u5bfc-\u91cd\u590d\u5bfc\u822a\u7cfb\u7edf\uff0c\u7075\u611f\u6765\u81ea\u5bfc\u76f2\u72ac\u7684\u8bad\u7ec3\u548c\u5de5\u4f5c\u65b9\u5f0f\uff0c\u80fd\u591f\u5728\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u516c\u91cc\u7ea7\u7684\u53ef\u9760\u8def\u5f84\u8ddf\u968f\u3002", "motivation": "\u76ee\u524d\u9488\u5bf9\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u79fb\u52a8\u8f85\u52a9\u7cfb\u7edf\u7814\u7a76\u4e2d\uff0c\u76f4\u63a5\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u8bbe\u8ba1\u63d0\u4f9b\u53c2\u8003\u7684\u8d44\u6599\u975e\u5e38\u7a00\u7f3a\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u5168\u9762\u7684\u4eba\u56e0\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u8bbf\u8c0826\u540d\u5bfc\u76f2\u72ac\u4f7f\u7528\u8005\u30014\u540d\u767d\u624b\u6756\u4f7f\u7528\u8005\u30019\u540d\u5bfc\u76f2\u72ac\u8bad\u7ec3\u5e08\u548c1\u540d\u5b9a\u5411\u884c\u8d70\u8bad\u7ec3\u5e08\uff0c\u4ee5\u53ca15+\u5c0f\u65f6\u7684\u5bfc\u76f2\u72ac\u8f85\u52a9\u884c\u8d70\u89c2\u5bdf\uff0c\u5f00\u53d1\u4e86GuideNav\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u6784\u5efa\u62d3\u6251\u8def\u5f84\u8868\u793a\uff0c\u96c6\u6210\u89c6\u89c9\u4f4d\u7f6e\u8bc6\u522b\u4e0e\u65f6\u95f4\u6ee4\u6ce2\uff0c\u4f7f\u7528\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u5668\u8ba1\u7b97\u5bfc\u822a\u52a8\u4f5c\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684LiDAR\u7b49\u4f20\u611f\u5668\u3002", "result": "\u5728\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0cGuideNav\u5728\u4e94\u79cd\u6237\u5916\u73af\u5883\u4e2d\u6301\u7eed\u5b9e\u73b0\u4e86\u516c\u91cc\u7ea7\u7684\u8def\u5f84\u8ddf\u968f\uff0c\u5373\u4f7f\u5728\u6559\u5bfc\u548c\u91cd\u590d\u8fd0\u884c\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u573a\u666f\u53d8\u5316\u7684\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u53ef\u9760\u6027\u3002\u7528\u6237\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c55\u793a\u56db\u8db3\u79fb\u52a8\u7cfb\u7edf\u4ee5\u7c7b\u4f3c\u5bfc\u76f2\u72ac\u65b9\u5f0f\u68c0\u7d22\u8def\u5f84\u7684\u7814\u7a76\uff0c\u4e3a\u76f2\u4eba\u548c\u4f4e\u89c6\u529b\u4eba\u7fa4\u7684\u8f85\u52a9\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2512.06104", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06104", "abs": "https://arxiv.org/abs/2512.06104", "authors": ["Isaac Liao", "Albert Gu"], "title": "ARC-AGI Without Pretraining", "comment": null, "summary": "Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI \"training set\". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.", "AI": {"tldr": "CompressARC\u662f\u4e00\u4e2a\u4ec576K\u53c2\u6570\u7684\u6a21\u578b\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5373\u53ef\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u6700\u5c0f\u5316\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u89e3\u51b320%\u7684ARC-AGI\u89c6\u89c9\u8c1c\u9898\uff0c\u5c55\u793a\u4e86\u6781\u7aef\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u70b9\uff0c\u5373\u89e3\u51b3ARC-AGI\u89c6\u89c9\u8c1c\u9898\u9700\u8981\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u63a2\u7d22\u5728\u6781\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u5b9e\u73b0\u667a\u80fd\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u539f\u5219\uff0c\u5728\u63a8\u7406\u65f6\u4ec5\u57fa\u4e8e\u5355\u4e2a\u76ee\u6807\u8c1c\u9898\uff08\u4e0d\u542b\u6700\u7ec8\u7b54\u6848\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6216\u6807\u51c6\u8bad\u7ec3\u96c6\u3002", "result": "\u5728\u6781\u7aef\u6570\u636e\u9650\u5236\u4e0b\u6210\u529f\u89e3\u51b320%\u7684\u8bc4\u4f30\u8c1c\u9898\uff0c\u8868\u73b0\u51fa\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7f55\u89c1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MDL\u662f\u9664\u4f20\u7edf\u9884\u8bad\u7ec3\u5916\u5b9e\u73b0\u667a\u80fd\u7684\u53ef\u884c\u66ff\u4ee3\u9014\u5f84\uff0c\u4e3a\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u7684AGI\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.06012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06012", "abs": "https://arxiv.org/abs/2512.06012", "authors": ["Emmanuel Akeweje", "Conall Kirk", "Chi-Wai Chan", "Denis Dowling", "Mimi Zhang"], "title": "High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing", "comment": null, "summary": "Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u6790\u91d1\u5c5e\u7c89\u672b\u5f62\u6001\uff0c\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\u548c\u805a\u7c7b\u7b97\u6cd5\u8bc4\u4f30\u7c89\u672b\u8d28\u91cf\uff0c\u4e3aSLM\u5de5\u827a\u63d0\u4f9b\u5b9e\u65f6\u76d1\u6d4b\u652f\u6301\u3002", "motivation": "\u4f20\u7edf\u7c89\u672b\u8868\u5f81\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u5b9a\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5de5\u4e1a\u89c4\u6a21\u6279\u6b21\u7684\u5f02\u8d28\u6027\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u901a\u91cf\u3001\u81ea\u52a8\u5316\u7684\u5f62\u6001\u5206\u6790\u5de5\u5177\u6765\u6539\u5584SLM\u5de5\u827a\u7684\u7c89\u672b\u8d28\u91cf\u63a7\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u805a\u7c7b\u6d41\u7a0b\uff1a\u81ea\u7f16\u7801\u5668\u6d41\u7a0b\u3001\u5f62\u72b6\u63cf\u8ff0\u7b26\u6d41\u7a0b\u548c\u529f\u80fd\u6570\u636e\u6d41\u7a0b\uff0c\u57fa\u4e8e\u7ea612.6\u4e07\u5f20\u7c89\u672b\u56fe\u50cf\uff08\u76f4\u5f840.5-102\u5fae\u7c73\uff09\u8fdb\u884c\u5f62\u6001\u5206\u6790\uff0c\u4f7f\u7528\u5085\u91cc\u53f6\u63cf\u8ff0\u7b26+k-means\u7b97\u6cd5\u4f5c\u4e3a\u6700\u4f18\u65b9\u6848\u3002", "result": "\u5085\u91cc\u53f6\u63cf\u8ff0\u7b26+k-means\u6d41\u7a0b\u5728\u5185\u90e8\u6709\u6548\u6027\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230\u6700\u4f4e\u7684Davies-Bouldin\u6307\u6570\u548c\u6700\u9ad8\u7684Calinski-Harabasz\u5206\u6570\uff0c\u540c\u65f6\u5728\u6807\u51c6\u684c\u9762\u5de5\u4f5c\u7ad9\u4e0a\u4fdd\u6301\u6bcf\u9897\u7c92\u4e9a\u6beb\u79d2\u7ea7\u7684\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u5feb\u901f\u81ea\u52a8\u8bc4\u4f30\u7c89\u672b\u5f62\u6001\uff0c\u652f\u6301\u8ddf\u8e2a\u5f62\u72b6\u6f14\u53d8\uff0c\u4e3aSLM\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5b9e\u65f6\u539f\u6599\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2512.06151", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06151", "abs": "https://arxiv.org/abs/2512.06151", "authors": ["Ratnangshu Das", "Siddhartha Upadhyay", "Pushpak Jagtap"], "title": "Real-Time Spatiotemporal Tubes for Dynamic Unsafe Sets", "comment": null, "summary": "This paper presents a real-time control framework for nonlinear pure-feedback systems with unknown dynamics to satisfy reach-avoid-stay tasks within a prescribed time in dynamic environments. To achieve this, we introduce a real-time spatiotemporal tube (STT) framework. An STT is defined as a time-varying ball in the state space whose center and radius adapt online using only real-time sensory input. A closed-form, approximation-free control law is then derived to constrain the system output within the STT, ensuring safety and task satisfaction. We provide formal guarantees for obstacle avoidance and on-time task completion. The effectiveness and scalability of the framework are demonstrated through simulations and hardware experiments on a mobile robot and an aerial vehicle, navigating in cluttered dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u975e\u7ebf\u6027\u7eaf\u53cd\u9988\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\uff0c\u53ef\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5728\u89c4\u5b9a\u65f6\u95f4\u5185\u5b8c\u6210\u5230\u8fbe-\u907f\u969c-\u505c\u7559\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u65f6\u65f6\u7a7a\u7ba1\uff08STT\uff09\u5b9e\u73b0\u5b89\u5168\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u975e\u7ebf\u6027\u7eaf\u53cd\u9988\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff08\u5230\u8fbe\u76ee\u6807\u3001\u907f\u969c\u3001\u505c\u7559\uff09\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u7cfb\u7edf\u52a8\u529b\u5b66\u672a\u77e5\u4e14\u9700\u8981\u4e25\u683c\u65f6\u95f4\u7ea6\u675f\u65f6\u3002", "method": "\u5f15\u5165\u5b9e\u65f6\u65f6\u7a7a\u7ba1\uff08STT\uff09\u6846\u67b6\uff0c\u5b9a\u4e49\u4e3a\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u65f6\u53d8\u7403\u4f53\uff0c\u5176\u4e2d\u5fc3\u548c\u534a\u5f84\u5728\u7ebf\u81ea\u9002\u5e94\u8c03\u6574\uff1b\u63a8\u5bfc\u51fa\u65e0\u8fd1\u4f3c\u95ed\u5f0f\u63a7\u5236\u5f8b\uff0c\u5c06\u7cfb\u7edf\u8f93\u51fa\u7ea6\u675f\u5728STT\u5185\u3002", "result": "\u63d0\u4f9b\u4e86\u907f\u969c\u548c\u6309\u65f6\u5b8c\u6210\u4efb\u52a1\u7684\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u901a\u8fc7\u79fb\u52a8\u673a\u5668\u4eba\u548c\u98de\u884c\u5668\u7684\u4eff\u771f\u53ca\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684STT\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u975e\u7ebf\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b89\u5168\u53ef\u9760\u7684\u4efb\u52a1\u5b8c\u6210\u65b9\u6848\u3002"}}
{"id": "2512.06111", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06111", "abs": "https://arxiv.org/abs/2512.06111", "authors": ["Arthur Mukwaya", "Nancy Kasamala", "Nana Kankam Gyimah", "Judith Mwakalonge", "Gurcan Comert", "Saidi Siuhi", "Denis Ruganuza", "Mark Ngotonie"], "title": "A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts", "comment": null, "summary": "The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u8fdb\u884c\u77ed\u671f\u4ea4\u901a\u8ba1\u6570\u7684\u6700\u4f73\u4ee3\u8868\u65e5\uff0c\u4ee5\u63d0\u9ad8\u5e74\u5ea6\u5e73\u5747\u65e5\u4ea4\u901a\u91cf\u9884\u6d4b\u7cbe\u5ea6\u3002\u4e0e\u5f53\u524d\u5b9e\u8df5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5fb7\u514b\u8428\u65af\u5dde\u6570\u636e\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u7f8e\u56fd\u5404\u5dde\u4ea4\u901a\u90e8\u95e8\u9700\u8981\u6536\u96c6\u53ef\u9760\u7684\u5e74\u5ea6\u5e73\u5747\u65e5\u4ea4\u901a\u91cf\u6570\u636e\uff0c\u4f46\u8fde\u7eed\u8ba1\u6570\u7ad9\u90e8\u7f72\u6210\u672c\u9ad8\u6602\uff0c\u5bfc\u81f4\u8bb8\u591a\u673a\u6784\u4f9d\u8d56\u77ed\u671f\u8ba1\u6570\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u672a\u76d1\u6d4b\u9053\u8def\u4e0a\u7684AADT\u9884\u6d4b\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u65e5\u671f\u8fdb\u884c\u77ed\u671f\u8ba1\u6570\u3002\u4f7f\u7528\u5fb7\u514b\u8428\u65af\u5dde2022-2023\u5e74\u4ea4\u901a\u6570\u636e\uff0c\u6bd4\u8f83'\u6700\u4f18\u65e5'\u65b9\u6cd5\u548c'\u65e0\u6700\u4f18\u65e5'\u57fa\u7ebf\u65b9\u6cd5\u3002\u5229\u7528\u8fde\u7eed\u8ba1\u6570\u6570\u636e\u6a21\u62df24\u5c0f\u65f6\u77ed\u671f\u8ba1\u6570\uff0c\u5e76\u901a\u8fc7\u7559\u4e00\u6cd5\u6280\u672f\u751f\u6210\u65e0\u504f\u7684\u4ee3\u8868\u6027\u65e5\u4ea4\u901a\u7279\u5f81\u3002", "result": "\u6700\u4f18\u65e5\u65b9\u6cd5\u5728\u524d5\u4e2a\u6700\u4f73\u65e5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6700\u4f73\u65e5\uff08\u7b2c186\u5929\uff09\u7684\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff08RMSE: 7,871.15 vs 11,185.00\uff0cMAE: 3,645.09 vs 5,118.57\uff0cMAPE: 11.95% vs 14.42%\uff09\uff0cR\u00b2\u66f4\u9ad8\uff080.9756 vs 0.9499\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ea4\u901a\u90e8\u95e8\u63d0\u4f9b\u4e86\u66ff\u4ee3\u4f20\u7edf\u77ed\u671f\u8ba1\u6570\u5b9e\u8df5\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8AADT\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u652f\u6301\u9ad8\u901f\u516c\u8def\u6027\u80fd\u76d1\u6d4b\u7cfb\u7edf\u5408\u89c4\u6027\uff0c\u5e76\u964d\u4f4e\u5168\u5dde\u4ea4\u901a\u6570\u636e\u6536\u96c6\u7684\u8fd0\u8425\u6210\u672c\u3002"}}
{"id": "2512.06013", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06013", "abs": "https://arxiv.org/abs/2512.06013", "authors": ["Wenhao Li", "Chengwei Ma", "Weixin Mao"], "title": "VAT: Vision Action Transformer by Unlocking Full Representation of ViT", "comment": null, "summary": "In robot learning, Vision Transformers (ViTs) are standard for visual perception, yet most methods discard valuable information by using only the final layer's features. We argue this provides an insufficient representation and propose the Vision Action Transformer (VAT), a novel architecture that is extended from ViT and unlocks the full feature hierarchy of ViT. VAT processes specialized action tokens with visual features across all transformer layers, enabling a deep and progressive fusion of perception and action generation. On a suite of simulated manipulation tasks, VAT achieves a 98.15\\% average success rate across four LIBERO benchmarks, establishing a new state-of-the-art by outperforming prior methods like OpenVLA-OFT. Our work presents not only a powerful model for imitation learning but also demonstrates the critical importance of leveraging the complete ''representation trajectory'' of vision models to advance robotic policy. The GitHub URL for the project code is https://github.com/sellerbubble/VAT.", "AI": {"tldr": "\u63d0\u51fa\u4e86Vision Action Transformer (VAT)\u67b6\u6784\uff0c\u901a\u8fc7\u5229\u7528ViT\u6240\u6709\u5c42\u7684\u7279\u5f81\u800c\u4e0d\u4ec5\u4ec5\u662f\u6700\u540e\u4e00\u5c42\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u8fbe\u523098.15%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f7f\u7528ViT\u6700\u540e\u4e00\u5c42\u7684\u7279\u5f81\uff0c\u4e22\u5f03\u4e86\u6709\u4ef7\u503c\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u8868\u793a\u4e0d\u5145\u5206\u3002\u9700\u8981\u5145\u5206\u5229\u7528ViT\u7684\u5b8c\u6574\u7279\u5f81\u5c42\u6b21\u7ed3\u6784\u3002", "method": "\u57fa\u4e8eViT\u6269\u5c55\u7684VAT\u67b6\u6784\uff0c\u5728\u6240\u6709transformer\u5c42\u4e2d\u5904\u7406\u4e13\u95e8\u7684\u52a8\u4f5c\u4ee4\u724c\u4e0e\u89c6\u89c9\u7279\u5f81\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u52a8\u4f5c\u751f\u6210\u7684\u6e10\u8fdb\u5f0f\u6df1\u5ea6\u878d\u5408\u3002", "result": "\u5728\u56db\u4e2aLIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523098.15%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u8d85\u8d8aOpenVLA-OFT\u7b49\u5148\u524d\u65b9\u6cd5\uff0c\u5efa\u7acb\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "VAT\u4e0d\u4ec5\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\uff0c\u66f4\u8bc1\u660e\u4e86\u5229\u7528\u89c6\u89c9\u6a21\u578b\u5b8c\u6574\"\u8868\u793a\u8f68\u8ff9\"\u5bf9\u63a8\u8fdb\u673a\u5668\u4eba\u7b56\u7565\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.06182", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06182", "abs": "https://arxiv.org/abs/2512.06182", "authors": ["Shuhao Qi", "Qiling Aori", "Luyao Zhang", "Mircea Lazar", "Sofie Haesaert"], "title": "Situation-Aware Interactive MPC Switching for Autonomous Driving", "comment": null, "summary": "To enable autonomous driving in interactive traffic scenarios, various model predictive control (MPC) formulations have been proposed, each employing different interaction models. While higher-fidelity models enable more intelligent behavior, they incur increased computational cost. Since strong interactions are relatively infrequent in traffic, a practical strategy for balancing performance and computational overhead is to invoke an appropriate controller based on situational demands. To achieve this approach, we first conduct a comparative study to assess and hierarchize the interactive capabilities of different MPC formulations. Furthermore, we develop a neural network-based classifier to enable situation-aware switching among controllers with different levels of interactive capability. We demonstrate that this situation-aware switching can both substantially improve overall performance by activating the most advanced interactive MPC in rare but critical situations, and significantly reduce computational load by using a basic MPC in the majority of scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u5883\u611f\u77e5\u7684\u63a7\u5236\u5668\u5207\u6362\u7b56\u7565\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\u5728\u4e0d\u540c\u4ea4\u4e92\u9700\u6c42\u7684\u60c5\u5883\u4e0b\u9009\u62e9\u5408\u9002\u7684MPC\u63a7\u5236\u5668\uff0c\u4ee5\u5e73\u8861\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u5728\u4ea4\u4e92\u5f0f\u4ea4\u901a\u573a\u666f\u4e2d\uff0c\u4e0d\u540c\u7684MPC\u63a7\u5236\u5668\u5177\u6709\u4e0d\u540c\u7684\u4ea4\u4e92\u80fd\u529b\u548c\u8ba1\u7b97\u6210\u672c\u3002\u7531\u4e8e\u5f3a\u4ea4\u4e92\u5728\u4ea4\u901a\u4e2d\u76f8\u5bf9\u8f83\u5c11\u53d1\u751f\uff0c\u9700\u8981\u4e00\u79cd\u667a\u80fd\u7b56\u7565\u6765\u6839\u636e\u60c5\u5883\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u63a7\u5236\u5668\uff0c\u4ee5\u5b9e\u73b0\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\u3002", "method": "\u9996\u5148\u5bf9\u4e0d\u540cMPC\u63a7\u5236\u5668\u7684\u4ea4\u4e92\u80fd\u529b\u8fdb\u884c\u6bd4\u8f83\u7814\u7a76\u5e76\u5efa\u7acb\u5c42\u6b21\u7ed3\u6784\uff0c\u7136\u540e\u5f00\u53d1\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u6839\u636e\u60c5\u5883\u9700\u6c42\u5728\u4e0d\u540c\u4ea4\u4e92\u80fd\u529b\u7684\u63a7\u5236\u5668\u4e4b\u95f4\u8fdb\u884c\u5207\u6362\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u60c5\u5883\u611f\u77e5\u7684\u63a7\u5236\u5668\u5207\u6362\u7b56\u7565\u80fd\u591f\u5728\u5173\u952e\u4f46\u7f55\u89c1\u7684\u5f3a\u4ea4\u4e92\u60c5\u5883\u4e0b\u6fc0\u6d3b\u6700\u5148\u8fdb\u7684\u4ea4\u4e92MPC\u4ee5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u573a\u666f\u4e2d\u4f7f\u7528\u57fa\u7840MPC\u6765\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u60c5\u5883\u611f\u77e5\u5207\u6362\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u4ea4\u4e92\u80fd\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06134", "categories": ["cs.LG", "cs.AI", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.06134", "abs": "https://arxiv.org/abs/2512.06134", "authors": ["Georgi Hrusanov", "Duy-Thanh Vu", "Duy-Cat Can", "Sophie Tascedda", "Margaret Ryan", "Julien Bodelet", "Katarzyna Koscielska", "Carsten Magnus", "Oliver Y. Ch\u00e9n"], "title": "Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting", "comment": null, "summary": "Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($\u03b1$) and biological ($\u03b2$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.", "AI": {"tldr": "NKM\u662f\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u540c\u65f6\u9884\u6d4b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u591a\u79cd\u8ba4\u77e5\u8bc4\u5206\uff0c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u8fdb\u884c\u7eb5\u5411\u4e2a\u6027\u5316\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u8ba4\u77e5\u8870\u9000\u7684\u65e9\u671f\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "NKM\u7ed3\u5408\u5206\u6790\u6027\uff08\u03b1\uff09\u548c\u751f\u7269\u5b66\uff08\u03b2\uff09\u77e5\u8bc6\u6307\u5bfc\u7279\u5f81\u5206\u7ec4\uff0c\u901a\u8fc7\u878d\u5408\u7ec4\u611f\u77e5\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u5728Koopman\u7b97\u5b50\u6846\u67b6\u5185\u5c06\u590d\u6742\u975e\u7ebf\u6027\u8f68\u8ff9\u8f6c\u5316\u4e3a\u53ef\u89e3\u91ca\u7684\u7ebf\u6027\u8868\u793a\u3002", "result": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNKM\u5728\u9884\u6d4b\u8ba4\u77e5\u8870\u9000\u8f68\u8ff9\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u9884\u6d4b\u591a\u79cd\u8ba4\u77e5\u8bc4\u5206\u53d8\u5316\u3001\u91cf\u5316\u751f\u7269\u6807\u5fd7\u7269\u8d21\u732e\u5e76\u8bc6\u522b\u6700\u76f8\u5173\u7684\u8111\u533a\u3002", "conclusion": "NKM\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u663e\u5f0f\u7cfb\u7edf\u63a8\u8fdb\u4e86\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u4e2a\u6027\u5316\u9884\u6d4b\uff0c\u63ed\u793a\u4e86\u75be\u75c5\u8fdb\u5c55\u7684\u591a\u6a21\u6001\u751f\u7269\u5b66\u57fa\u7840\u3002"}}
{"id": "2512.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06014", "abs": "https://arxiv.org/abs/2512.06014", "authors": ["Jiho Shin", "Dominic Marshall", "Matthieu Komorowski"], "title": "Benchmarking CXR Foundation Models With Publicly Available MIMIC-CXR and NIH-CXR14 Datasets", "comment": null, "summary": "Recent foundation models have demonstrated strong performance in medical image representation learning, yet their comparative behaviour across datasets remains underexplored. This work benchmarks two large-scale chest X-ray (CXR) embedding models (CXR-Foundation (ELIXR v2.0) and MedImagelnsight) on public MIMIC-CR and NIH ChestX-ray14 datasets. Each model was evaluated using a unified preprocessing pipeline and fixed downstream classifiers to ensure reproducible comparison. We extracted embeddings directly from pre-trained encoders, trained lightweight LightGBM classifiers on multiple disease labels, and reported mean AUROC, and F1-score with 95% confidence intervals. MedImageInsight achieved slightly higher performance across most tasks, while CXR-Foundation exhibited strong cross-dataset stability. Unsupervised clustering of MedImageIn-sight embeddings further revealed a coherent disease-specific structure consistent with quantitative results. The results highlight the need for standardised evaluation of medical foundation models and establish reproducible baselines for future multimodal and clinical integration studies.", "AI": {"tldr": "\u672c\u6587\u5bf9\u4e24\u79cd\u5927\u89c4\u6a21\u80f8\u7247\u5d4c\u5165\u6a21\u578b\uff08CXR-Foundation\u548cMedImagelnsight\uff09\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0MedImagelnsight\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u8868\u73b0\u7565\u4f18\uff0c\u800cCXR-Foundation\u5177\u6709\u66f4\u597d\u7684\u8de8\u6570\u636e\u96c6\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u8868\u793a\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6bd4\u8f83\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u7684\u9884\u5904\u7406\u6d41\u7a0b\u548c\u56fa\u5b9a\u4e0b\u6e38\u5206\u7c7b\u5668\uff0c\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u63d0\u53d6\u5d4c\u5165\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7LightGBM\u5206\u7c7b\u5668\uff0c\u62a5\u544a\u5e73\u5747AUROC\u548cF1\u5206\u6570\u53ca95%\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "MedImagelnsight\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e2d\u6027\u80fd\u7565\u9ad8\uff0cCXR-Foundation\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8de8\u6570\u636e\u96c6\u7a33\u5b9a\u6027\uff1b\u65e0\u76d1\u7763\u805a\u7c7b\u663e\u793aMedImagelnsight\u5d4c\u5165\u5177\u6709\u4e0e\u5b9a\u91cf\u7ed3\u679c\u4e00\u81f4\u7684\u75be\u75c5\u7279\u5f02\u6027\u7ed3\u6784\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u533b\u5b66\u57fa\u7840\u6a21\u578b\u6807\u51c6\u5316\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u672a\u6765\u591a\u6a21\u6001\u548c\u4e34\u5e8a\u6574\u5408\u7814\u7a76\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u3002"}}
{"id": "2512.06192", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06192", "abs": "https://arxiv.org/abs/2512.06192", "authors": ["Takahiro Hattori", "Kento Kawaharazuka", "Temma Suzuki", "Keita Yoneda", "Kei Okada"], "title": "REWW-ARM -- Remote Wire-Driven Mobile Robot: Design, Control, and Experimental Validation", "comment": "Accepted on Advanced Intelligent Systems", "summary": "Electronic devices are essential for robots but limit their usable environments. To overcome this, methods excluding electronics from the operating environment while retaining advanced electronic control and actuation have been explored. These include the remote hydraulic drive of electronics-free mobile robots, which offer high reachability, and long wire-driven robot arms with motors consolidated at the base, which offer high environmental resistance. To combine the advantages of both, this study proposes a new system, \"Remote Wire Drive.\" As a proof-of-concept, we designed and developed the Remote Wire-Driven robot \"REWW-ARM\", which consists of the following components: 1) a novel power transmission mechanism, the \"Remote Wire Transmission Mechanism\" (RWTM), the key technology of the Remote Wire Drive; 2) an electronics-free distal mobile robot driven by it; and 3) a motor-unit that generates power and provides electronic closed-loop control based on state estimation via the RWTM. In this study, we evaluated the mechanical and control performance of REWW-ARM through several experiments, demonstrating its capability for locomotion, posture control, and object manipulation both on land and underwater. This suggests the potential for applying the Remote Wire-Driven system to various types of robots, thereby expanding their operational range.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u8fdc\u7a0b\u7ebf\u7f06\u9a71\u52a8\"\u7684\u65b0\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u7535\u5b50\u8bbe\u5907\u4e0e\u64cd\u4f5c\u73af\u5883\u5206\u79bb\uff0c\u540c\u65f6\u4fdd\u7559\u7535\u5b50\u63a7\u5236\u548c\u9a71\u52a8\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7535\u5b50\u8bbe\u5907\u9650\u5236\u5de5\u4f5c\u73af\u5883\u7684\u95ee\u9898\u3002", "motivation": "\u7535\u5b50\u8bbe\u5907\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u53ef\u7528\u73af\u5883\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u6392\u9664\u7535\u5b50\u8bbe\u5907\u7684\u73af\u5883\u4e2d\u8fd0\u884c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7ea7\u7535\u5b50\u63a7\u5236\u548c\u9a71\u52a8\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86REWW-ARM\u673a\u5668\u4eba\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u8fdc\u7a0b\u7ebf\u7f06\u4f20\u8f93\u673a\u5236\uff08RWTM\uff09\uff1b2\uff09\u65e0\u7535\u5b50\u8bbe\u5907\u7684\u8fdc\u7aef\u79fb\u52a8\u673a\u5668\u4eba\uff1b3\uff09\u57fa\u4e8eRWTM\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u5e76\u63d0\u4f9b\u7535\u5b50\u95ed\u73af\u63a7\u5236\u7684\u7535\u673a\u5355\u5143\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86REWW-ARM\u5728\u9646\u5730\u548c\u6c34\u4e2d\u90fd\u80fd\u5b9e\u73b0\u8fd0\u52a8\u3001\u59ff\u6001\u63a7\u5236\u548c\u7269\u4f53\u64cd\u4f5c\u7684\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u7cfb\u7edf\u7684\u673a\u68b0\u548c\u63a7\u5236\u6027\u80fd\u3002", "conclusion": "\u8fdc\u7a0b\u7ebf\u7f06\u9a71\u52a8\u7cfb\u7edf\u6709\u6f5c\u529b\u5e94\u7528\u4e8e\u5404\u79cd\u7c7b\u578b\u7684\u673a\u5668\u4eba\uff0c\u4ece\u800c\u6269\u5c55\u5176\u64cd\u4f5c\u8303\u56f4\u3002"}}
{"id": "2512.06143", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.06143", "abs": "https://arxiv.org/abs/2512.06143", "authors": ["Marcus M. Noack", "Mark D. Risser", "Hengrui Luo", "Vardaan Tekriwal", "Ronald J. Pandolfi"], "title": "gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points", "comment": "None", "summary": "Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \\emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.", "AI": {"tldr": "\u63d0\u51fagp2Scale\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u6838\u51fd\u6570\u7684\u81ea\u7136\u7a00\u758f\u7ed3\u6784\uff0c\u5c06\u7cbe\u786e\u9ad8\u65af\u8fc7\u7a0b\u6269\u5c55\u5230\u8d85\u8fc71000\u4e07\u4e2a\u6570\u636e\u70b9\uff0c\u65e0\u9700\u4f9d\u8d56\u8bf1\u5bfc\u70b9\u6216\u8fd1\u4f3c\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9ad8\u65af\u8fc7\u7a0b\u6269\u5c55\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u5404\u79cd\u8fd1\u4f3c\uff0c\u964d\u4f4e\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u5e76\u9650\u5236\u4e86\u6838\u51fd\u6570\u548c\u566a\u58f0\u6a21\u578b\u8bbe\u8ba1\u7684\u7075\u6d3b\u6027\uff0c\u8fd9\u5728\u975e\u5e73\u7a33\u6838\u51fd\u6570\u65e5\u76ca\u91cd\u8981\u7684\u80cc\u666f\u4e0b\u662f\u4e0d\u53ef\u63a5\u53d7\u7684\u3002", "method": "\u5229\u7528\u9ad8\u5ea6\u7075\u6d3b\u3001\u7d27\u652f\u6491\u548c\u975e\u5e73\u7a33\u7684\u6838\u51fd\u6570\u6765\u8bc6\u522b\u534f\u65b9\u5dee\u77e9\u9635\u4e2d\u81ea\u7136\u51fa\u73b0\u7684\u7a00\u758f\u7ed3\u6784\uff0c\u7136\u540e\u5229\u7528\u8fd9\u79cd\u7a00\u758f\u6027\u8fdb\u884c\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u548c\u5bf9\u6570\u884c\u5217\u5f0f\u8ba1\u7b97\u4ee5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u8fd1\u4f3c\u7b97\u6cd5\u76f8\u6bd4\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u8fd1\u4f3c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7684\u5173\u952e\u4f18\u52bf\u5728\u4e8e\u5bf9\u4efb\u610f\u9ad8\u65af\u8fc7\u7a0b\u5b9a\u5236\uff08\u6838\u5fc3\u6838\u8bbe\u8ba1\u3001\u566a\u58f0\u548c\u5747\u503c\u51fd\u6570\uff09\u4ee5\u53ca\u8f93\u5165\u7a7a\u95f4\u7c7b\u578b\u7684\u4e0d\u53ef\u77e5\u6027\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u73b0\u4ee3\u9ad8\u65af\u8fc7\u7a0b\u5e94\u7528\u3002"}}
{"id": "2512.06020", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06020", "abs": "https://arxiv.org/abs/2512.06020", "authors": ["Wenyi Mo", "Tianyu Zhang", "Yalong Bai", "Ligong Han", "Ying Ba", "Dimitris N. Metaxas"], "title": "PrefGen: Multimodal Preference Learning for Preference-Conditioned Image Generation", "comment": "Project Page: \\href{https://prefgen.github.io/}{\\texttt{https://prefgen.github.io}}", "summary": "Preference-conditioned image generation seeks to adapt generative models to individual users, producing outputs that reflect personal aesthetic choices beyond the given textual prompt. Despite recent progress, existing approaches either fail to capture nuanced user preferences or lack effective mechanisms to encode personalized visual signals. In this work, we propose a multimodal framework that leverages multimodal large language models (MLLMs) to extract rich user representations and inject them into diffusion-based image generation. We train the MLLM with a preference-oriented visual question answering task to capture fine-grained semantic cues. To isolate preference-relevant features, we introduce two complementary probing tasks: inter-user discrimination to distinguish between different users, and intra-user discrimination to separate liked from disliked content. To ensure compatibility with diffusion text encoders, we design a maximum mean discrepancy-based alignment loss that bridges the modality gap while preserving multimodal structure. The resulting embeddings are used to condition the generator, enabling faithful adherence to both prompts and user preferences. Extensive experiments demonstrate that our method substantially outperforms strong baselines in both image quality and preference alignment, highlighting the effectiveness of representation extraction and alignment for personalized generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u7528\u6237\u504f\u597d\u8868\u5f81\u5e76\u6ce8\u5165\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u540c\u65f6\u6ee1\u8db3\u6587\u672c\u63d0\u793a\u548c\u7528\u6237\u5ba1\u7f8e\u504f\u597d\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u7528\u6237\u7ec6\u5fae\u5ba1\u7f8e\u504f\u597d\u548c\u7f16\u7801\u4e2a\u6027\u5316\u89c6\u89c9\u4fe1\u53f7\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u53cd\u6620\u7528\u6237\u4e2a\u4eba\u5316\u7684\u7f8e\u5b66\u9009\u62e9\u3002", "method": "1. \u4f7f\u7528\u504f\u597d\u5bfc\u5411\u7684\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u8bad\u7ec3MLLM\u4ee5\u6355\u83b7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7ebf\u7d22\uff1b2. \u5f15\u5165\u7528\u6237\u95f4\u533a\u5206\u548c\u7528\u6237\u5185\u533a\u5206\u4e24\u4e2a\u4e92\u8865\u7684\u63a2\u6d4b\u4efb\u52a1\u6765\u5206\u79bb\u504f\u597d\u76f8\u5173\u7279\u5f81\uff1b3. \u8bbe\u8ba1\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\u7684\u5bf9\u9f50\u635f\u5931\u6765\u5f25\u5408\u6a21\u6001\u5dee\u5f02\uff1b4. \u5c06\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7528\u4e8e\u6761\u4ef6\u5316\u6269\u6563\u751f\u6210\u5668\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u504f\u597d\u5bf9\u9f50\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8868\u5f81\u63d0\u53d6\u548c\u5bf9\u9f50\u7b56\u7565\u5728\u4e2a\u6027\u5316\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u6846\u67b6\u901a\u8fc7\u6709\u6548\u7684\u7528\u6237\u504f\u597d\u8868\u5f81\u5b66\u4e60\u548c\u6a21\u6001\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6587\u672c\u63d0\u793a\u548c\u7528\u6237\u504f\u597d\u7684\u5fe0\u5b9e\u9075\u5faa\uff0c\u4e3a\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06198", "abs": "https://arxiv.org/abs/2512.06198", "authors": ["Oussama Sifour", "Soulaimane Berkane", "Abdelhamid Tayebi"], "title": "Cascaded Tightly-Coupled Observer Design for Single-Range-Aided Inertial Navigation", "comment": "8 pages", "summary": "This work introduces a single-range-aided navigation observer that reconstructs the full state of a rigid body using only an Inertial Measurement Unit (IMU), a body-frame vector measurement (e.g., magnetometer), and a distance measurement from a fixed anchor point. The design first formulates an extended linear time-varying (LTV) system to estimate body-frame position, body-frame velocity, and the gravity direction. The recovered gravity direction, combined with the body-frame vector measurement, is then used to reconstruct the full orientation on $\\mathrm{SO}(3)$, resulting in a cascaded observer architecture. Almost Global Asymptotic Stability (AGAS) of the cascaded design is established under a uniform observability condition, ensuring robustness to sensor noise and trajectory variations. Simulation studies on three-dimensional trajectories demonstrate accurate estimation of position, velocity, and orientation, highlighting single-range aiding as a lightweight and effective modality for autonomous navigation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f7f\u7528IMU\u3001\u4f53\u5750\u6807\u7cfb\u5411\u91cf\u6d4b\u91cf\u548c\u56fa\u5b9a\u951a\u70b9\u8ddd\u79bb\u6d4b\u91cf\u7684\u5355\u8ddd\u79bb\u8f85\u52a9\u5bfc\u822a\u89c2\u6d4b\u5668\uff0c\u7528\u4e8e\u91cd\u6784\u521a\u4f53\u7684\u5b8c\u6574\u72b6\u6001\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u6709\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u590d\u6742\u4f20\u611f\u5668\u7cfb\u7edf\u7684\u4f9d\u8d56\uff0c\u4ec5\u5229\u7528\u57fa\u672c\u4f20\u611f\u5668\u5b9e\u73b0\u5b8c\u6574\u7684\u59ff\u6001\u548c\u4f4d\u7f6e\u4f30\u8ba1\u3002", "method": "\u9996\u5148\u6784\u5efa\u6269\u5c55\u7ebf\u6027\u65f6\u53d8\u7cfb\u7edf\u4f30\u8ba1\u4f53\u5750\u6807\u7cfb\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u91cd\u529b\u65b9\u5411\uff0c\u7136\u540e\u7ed3\u5408\u4f53\u5750\u6807\u7cfb\u5411\u91cf\u6d4b\u91cf\u5728SO(3)\u4e0a\u91cd\u6784\u5b8c\u6574\u59ff\u6001\uff0c\u91c7\u7528\u7ea7\u8054\u89c2\u6d4b\u5668\u67b6\u6784\u3002", "result": "\u5728\u4e09\u7ef4\u8f68\u8ff9\u4e0a\u7684\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u4f4d\u7f6e\u3001\u901f\u5ea6\u548c\u59ff\u6001\uff0c\u8bc1\u660e\u4e86\u5355\u8ddd\u79bb\u8f85\u52a9\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5bfc\u822a\u65b9\u5f0f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7ea7\u8054\u8bbe\u8ba1\u5728\u4e00\u81f4\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5177\u6709\u51e0\u4e4e\u5168\u5c40\u6e10\u8fd1\u7a33\u5b9a\u6027\uff0c\u5bf9\u4f20\u611f\u5668\u566a\u58f0\u548c\u8f68\u8ff9\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5355\u8ddd\u79bb\u8f85\u52a9\u662f\u4e00\u79cd\u6709\u6548\u7684\u8f7b\u91cf\u7ea7\u81ea\u4e3b\u5bfc\u822a\u65b9\u5f0f\u3002"}}
{"id": "2512.06154", "categories": ["cs.LG", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06154", "abs": "https://arxiv.org/abs/2512.06154", "authors": ["Barproda Halder", "Pasan Dissanayake", "Sanghamitra Dutta"], "title": "Learning Invariant Graph Representations Through Redundant Information", "comment": null, "summary": "Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u4fe1\u606f\u5206\u89e3(PID)\u7684\u5197\u4f59\u5f15\u5bfc\u4e0d\u53d8\u56fe\u5b66\u4e60(RIG)\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56fe\u8868\u793a\u5b66\u4e60\u4e2d\u5206\u5e03\u5916(OOD)\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u79bb\u5197\u4f59\u4fe1\u606f\u6765\u63d0\u9ad8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7ecf\u5178\u4fe1\u606f\u8bba\u7684\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5206\u79bb\u865a\u5047\u6210\u5206\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u8868\u793a\u4ecd\u4fdd\u7559\u865a\u5047\u4fe1\u606f\uff0c\u5f71\u54cdOOD\u6cdb\u5316\u6027\u80fd\u3002\u9700\u8981\u66f4\u7cbe\u786e\u5730\u8bc6\u522b\u548c\u5904\u7406\u865a\u5047\u5b50\u56fe\u4e0e\u4e0d\u53d8\u5b50\u56fe\u4e4b\u95f4\u7684\u5197\u4f59\u4fe1\u606f\u3002", "method": "\u63d0\u51faRIG\u591a\u7ea7\u4f18\u5316\u6846\u67b6\uff1a1)\u4f7f\u7528PID\u8bc6\u522b\u865a\u5047\u5b50\u56feGs\u548c\u4e0d\u53d8\u5b50\u56feGc\u4e4b\u95f4\u7684\u5197\u4f59\u4fe1\u606f\uff1b2)\u4ea4\u66ff\u8fdb\u884c\u5197\u4f59\u4fe1\u606f\u4e0b\u754c\u4f30\u8ba1\u548c\u6700\u5927\u5316\uff1b3)\u7ed3\u5408\u989d\u5916\u76ee\u6807\u51fd\u6570\u5206\u79bb\u865a\u5047\u548c\u56e0\u679c\u5b50\u56fe\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRIG\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u591a\u79cd\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PID\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u8d85\u8d8a\u7ecf\u5178\u4fe1\u606f\u8bba\u7684\u65b0\u5de5\u5177\uff0cRIG\u6846\u67b6\u901a\u8fc7\u5197\u4f59\u4fe1\u606f\u5f15\u5bfc\u6210\u529f\u5b9e\u73b0\u4e86\u66f4\u597d\u7684OOD\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.06024", "categories": ["cs.CV", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2512.06024", "abs": "https://arxiv.org/abs/2512.06024", "authors": ["Jiabin Liu", "Zihao Zhou", "Jialei Yan", "Anxin Guo", "Alvise Benetazzo", "Hui Li"], "title": "Neural reconstruction of 3D ocean wave hydrodynamics from camera sensing", "comment": null, "summary": "Precise three-dimensional (3D) reconstruction of wave free surfaces and associated velocity fields is essential for developing a comprehensive understanding of ocean physics. To address the high computational cost of dense visual reconstruction in long-term ocean wave observation tasks and the challenges introduced by persistent visual occlusions, we propose an wave free surface visual reconstruction neural network, which is designed as an attention-augmented pyramid architecture tailored to the multi-scale and temporally continuous characteristics of wave motions. Using physics-based constraints, we perform time-resolved reconstruction of nonlinear 3D velocity fields from the evolving free-surface boundary. Experiments under real-sea conditions demonstrate millimetre-level wave elevation prediction in the central region, dominant-frequency errors below 0.01 Hz, precise estimation of high-frequency spectral power laws, and high-fidelity 3D reconstruction of nonlinear velocity fields, while enabling dense reconstruction of two million points in only 1.35 s. Built on a stereo-vision dataset, the model outperforms conventional visual reconstruction approaches and maintains strong generalization in occluded conditions, owing to its global multi-scale attention and its learned encoding of wave propagation dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u589e\u5f3a\u91d1\u5b57\u5854\u67b6\u6784\u7684\u6ce2\u6d6a\u81ea\u7531\u8868\u9762\u89c6\u89c9\u91cd\u5efa\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u6d77\u6d0b\u6ce2\u6d6a\u89c2\u6d4b\u4e2d\u5bc6\u96c6\u89c6\u89c9\u91cd\u5efa\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u89c6\u89c9\u906e\u6321\u95ee\u9898\u3002", "motivation": "\u7cbe\u786e\u7684\u4e09\u7ef4\u6ce2\u6d6a\u81ea\u7531\u8868\u9762\u91cd\u5efa\u548c\u76f8\u5173\u901f\u5ea6\u573a\u5bf9\u4e8e\u7406\u89e3\u6d77\u6d0b\u7269\u7406\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u671f\u89c2\u6d4b\u4e2d\u9762\u4e34\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6301\u7eed\u89c6\u89c9\u906e\u6321\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u91d1\u5b57\u5854\u67b6\u6784\u795e\u7ecf\u7f51\u7edc\uff0c\u9488\u5bf9\u6ce2\u6d6a\u8fd0\u52a8\u7684\u591a\u5c3a\u5ea6\u548c\u65f6\u95f4\u8fde\u7eed\u6027\u7279\u5f81\uff0c\u5229\u7528\u7269\u7406\u7ea6\u675f\u4ece\u6f14\u5316\u7684\u81ea\u7531\u8868\u9762\u8fb9\u754c\u8fdb\u884c\u65f6\u95f4\u5206\u8fa8\u7684\u4e09\u7ef4\u901f\u5ea6\u573a\u91cd\u5efa\u3002", "result": "\u771f\u5b9e\u6d77\u51b5\u5b9e\u9a8c\u663e\u793a\uff1a\u4e2d\u5fc3\u533a\u57df\u6beb\u7c73\u7ea7\u6ce2\u6d6a\u9ad8\u7a0b\u9884\u6d4b\u3001\u4e3b\u9891\u8bef\u5dee\u4f4e\u4e8e0.01Hz\u3001\u9ad8\u9891\u8c31\u529f\u7387\u5f8b\u7cbe\u786e\u4f30\u8ba1\u3001\u975e\u7ebf\u6027\u901f\u5ea6\u573a\u9ad8\u4fdd\u771f\u4e09\u7ef4\u91cd\u5efa\uff0c200\u4e07\u70b9\u5bc6\u96c6\u91cd\u5efa\u4ec5\u97001.35\u79d2\u3002", "conclusion": "\u8be5\u6a21\u578b\u57fa\u4e8e\u7acb\u4f53\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728\u906e\u6321\u6761\u4ef6\u4e0b\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5f97\u76ca\u4e8e\u5176\u5168\u5c40\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\u548c\u5b66\u4e60\u5230\u7684\u6ce2\u6d6a\u4f20\u64ad\u52a8\u529b\u5b66\u7f16\u7801\u3002"}}
{"id": "2512.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06207", "abs": "https://arxiv.org/abs/2512.06207", "authors": ["Harshil Suthar", "Dipankar Maity"], "title": "Where to Fly, What to Send: Communication-Aware Aerial Support for Ground Robots", "comment": "Submitted to conference", "summary": "In this work we consider a multi-robot team operating in an unknown environment where one aerial agent is tasked to map the environment and transmit (a portion of) the mapped environment to a group of ground agents that are trying to reach their goals. The entire operation takes place over a bandwidth-limited communication channel, which motivates the problem of determining what and how much information the assisting agent should transmit and when while simultaneously performing exploration/mapping. The proposed framework enables the assisting aerial agent to decide what information to transmit based on the Value-of-Information (VoI), how much to transmit using a Mixed-Integer Linear Programming (MILP), and how to acquire additional information through an utility score-based environment exploration strategy. We perform a communication-motion trade-off analysis between the total amount of map data communicated by the aerial agent and the navigation cost incurred by the ground agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u591a\u673a\u5668\u4eba\u534f\u4f5c\u6846\u67b6\uff0c\u5176\u4e2d\u7a7a\u4e2d\u673a\u5668\u4eba\u901a\u8fc7\u5e26\u5bbd\u53d7\u9650\u7684\u901a\u4fe1\u901a\u9053\u5411\u5730\u9762\u673a\u5668\u4eba\u4f20\u8f93\u5730\u56fe\u4fe1\u606f\uff0c\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c(VoI)\u51b3\u5b9a\u4f20\u8f93\u5185\u5bb9\uff0c\u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u786e\u5b9a\u4f20\u8f93\u91cf\uff0c\u5e76\u901a\u8fc7\u6548\u7528\u8bc4\u5206\u7b56\u7565\u8fdb\u884c\u73af\u5883\u63a2\u7d22\u3002", "motivation": "\u89e3\u51b3\u5728\u5e26\u5bbd\u53d7\u9650\u901a\u4fe1\u73af\u5883\u4e0b\uff0c\u7a7a\u4e2d\u8f85\u52a9\u673a\u5668\u4eba\u5982\u4f55\u667a\u80fd\u51b3\u7b56\u4f20\u8f93\u54ea\u4e9b\u5730\u56fe\u4fe1\u606f\u3001\u4f20\u8f93\u591a\u5c11\u4fe1\u606f\u4ee5\u53ca\u4f55\u65f6\u4f20\u8f93\uff0c\u540c\u65f6\u8fdb\u884c\u73af\u5883\u63a2\u7d22\u548c\u5efa\u56fe\u7684\u95ee\u9898\u3002", "method": "1. \u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c(VoI)\u51b3\u5b9a\u4f20\u8f93\u5185\u5bb9\uff1b2. \u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u786e\u5b9a\u4f20\u8f93\u91cf\uff1b3. \u91c7\u7528\u6548\u7528\u8bc4\u5206\u7b56\u7565\u8fdb\u884c\u73af\u5883\u63a2\u7d22\uff1b4. \u5206\u6790\u901a\u4fe1\u4e0e\u8fd0\u52a8\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5b9e\u73b0\u4e86\u901a\u4fe1-\u8fd0\u52a8\u6743\u8861\u5206\u6790\uff0c\u91cf\u5316\u4e86\u7a7a\u4e2d\u673a\u5668\u4eba\u4f20\u8f93\u7684\u5730\u56fe\u6570\u636e\u603b\u91cf\u4e0e\u5730\u9762\u673a\u5668\u4eba\u5bfc\u822a\u6210\u672c\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e73\u8861\u901a\u4fe1\u8d44\u6e90\u4f7f\u7528\u548c\u5730\u9762\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u5e26\u5bbd\u53d7\u9650\u73af\u5883\u4e0b\u7684\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06183", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06183", "abs": "https://arxiv.org/abs/2512.06183", "authors": ["Lindong Liu", "Zhixiong Jin", "Seongjin Choi"], "title": "PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations", "comment": null, "summary": "High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.", "AI": {"tldr": "PMA-Diffusion\u662f\u4e00\u4e2a\u7269\u7406\u5f15\u5bfc\u7684\u63a9\u7801\u611f\u77e5\u6269\u6563\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u7a00\u758f\u89c2\u6d4b\u6570\u636e\u4e2d\u91cd\u5efa\u9ad8\u901f\u516c\u8def\u901f\u5ea6\u573a\uff0c\u5728\u4e25\u91cd\u7a00\u758f\u6027\uff08\u4ec55%\u53ef\u89c1\u5ea6\uff09\u4e0b\u4ecd\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u9ad8\u5206\u8fa8\u7387\u4ea4\u901a\u72b6\u6001\u4fe1\u606f\u5bf9\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u4ea4\u901a\u6570\u636e\uff08\u5982\u73af\u5f62\u63a2\u6d4b\u5668\u548c\u63a2\u6d4b\u8f66\u8f86\uff09\u901a\u5e38\u8fc7\u4e8e\u7a00\u758f\u548c\u5608\u6742\uff0c\u65e0\u6cd5\u6355\u6349\u4ea4\u901a\u6d41\u7684\u8be6\u7ec6\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u63a9\u7801\u611f\u77e5\u8bad\u7ec3\u7b56\u7565\uff08\u5355\u63a9\u7801\u548c\u53cc\u63a9\u7801\uff09\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u91c7\u7528\u7269\u7406\u5f15\u5bfc\u7684\u540e\u9a8c\u91c7\u6837\u5668\uff0c\u4ea4\u66ff\u8fdb\u884c\u53cd\u5411\u6269\u6563\u66f4\u65b0\u3001\u89c2\u6d4b\u6295\u5f71\u548c\u57fa\u4e8e\u81ea\u9002\u5e94\u5404\u5411\u5f02\u6027\u5e73\u6ed1\u7684\u7269\u7406\u5f15\u5bfc\u6295\u5f71\u3002", "result": "\u5728I-24 MOTION\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5373\u4f7f\u5728\u4ec55%\u53ef\u89c1\u5ea6\u7684\u4e25\u91cd\u7a00\u758f\u6761\u4ef6\u4e0b\uff0cPMA-Diffusion\u5728\u4e09\u4e2a\u91cd\u5efa\u8bef\u5dee\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u8bad\u7ec3\u4e8e\u7a00\u758f\u89c2\u6d4b\u7684\u6a21\u578b\u6027\u80fd\u63a5\u8fd1\u57fa\u4e8e\u5b8c\u6574\u89c2\u6d4b\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5c06\u63a9\u7801\u611f\u77e5\u6269\u6563\u5148\u9a8c\u4e0e\u7269\u7406\u5f15\u5bfc\u540e\u9a8c\u91c7\u6837\u5668\u76f8\u7ed3\u5408\uff0c\u4e3a\u73b0\u5b9e\u611f\u77e5\u7a00\u758f\u6027\u4e0b\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06032", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06032", "abs": "https://arxiv.org/abs/2512.06032", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee"], "title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation", "comment": null, "summary": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86SAM2\u548cSAM3\u4e4b\u95f4\u7684\u6839\u672c\u6027\u5dee\u5f02\uff0c\u6307\u51faSAM2\u57fa\u4e8e\u7a7a\u95f4\u63d0\u793a\u7684\u51e0\u4f55\u5206\u5272\u4e0eSAM3\u57fa\u4e8e\u591a\u6a21\u6001\u6982\u5ff5\u9a71\u52a8\u7684\u5206\u5272\u8303\u5f0f\u5b58\u5728\u672c\u8d28\u533a\u522b\uff0cSAM3\u4ee3\u8868\u4e86\u5206\u5272\u57fa\u7840\u6a21\u578b\u7684\u65b0\u7c7b\u522b\u3002", "motivation": "\u7814\u7a76SAM2\u5230SAM3\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48SAM2\u7684\u63d0\u793a\u5206\u5272\u4e13\u4e1a\u77e5\u8bc6\u65e0\u6cd5\u8fc1\u79fb\u5230SAM3\u7684\u591a\u6a21\u6001\u6982\u5ff5\u9a71\u52a8\u67b6\u6784\uff0c\u9610\u660e\u4e24\u8005\u4e4b\u95f4\u7684\u6839\u672c\u6027\u4e0d\u8fde\u7eed\u6027\u3002", "method": "\u901a\u8fc7\u4e94\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\uff1a\u6982\u5ff5\u5dee\u5f02\u3001\u67b6\u6784\u5dee\u5f02\u3001\u6570\u636e\u96c6\u5dee\u5f02\u3001\u8bad\u7ec3\u5dee\u5f02\u548c\u8bc4\u4f30\u5dee\u5f02\uff0c\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e24\u79cd\u6a21\u578b\u7684\u8bbe\u8ba1\u7406\u5ff5\u548c\u6280\u672f\u5b9e\u73b0\u3002", "result": "\u786e\u7acb\u4e86SAM3\u4f5c\u4e3a\u65b0\u4e00\u4ee3\u5206\u5272\u57fa\u7840\u6a21\u578b\u7684\u5730\u4f4d\uff0c\u5c55\u793a\u4e86\u4ece\u51e0\u4f55\u5206\u5272\u5230\u8bed\u4e49\u6982\u5ff5\u5206\u5272\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u6982\u5ff5\u9a71\u52a8\u5206\u5272\u65f6\u4ee3\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "SAM3\u4ee3\u8868\u4e86\u5206\u5272\u6280\u672f\u7684\u91cd\u8981\u7a81\u7834\uff0c\u5f15\u5165\u4e86\u7edf\u4e00\u7684\u591a\u6a21\u6001\u67b6\u6784\uff0c\u5f00\u542f\u4e86\u6982\u5ff5\u9a71\u52a8\u5206\u5272\u7684\u65b0\u65f6\u4ee3\uff0c\u5176\u4e0eSAM2\u7684\u6839\u672c\u5dee\u5f02\u8981\u6c42\u91cd\u65b0\u601d\u8003\u5206\u5272\u6a21\u578b\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2512.06261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06261", "abs": "https://arxiv.org/abs/2512.06261", "authors": ["Taekyung Kim", "Keyvan Majd", "Hideki Okamoto", "Bardh Hoxha", "Dimitra Panagou", "Georgios Fainekos"], "title": "Safe Model Predictive Diffusion with Shielding", "comment": "Project page: https://www.taekyung.me/safe-mpd", "summary": "Generating safe, kinodynamically feasible, and optimal trajectories for complex robotic systems is a central challenge in robotics. This paper presents Safe Model Predictive Diffusion (Safe MPD), a training-free diffusion planner that unifies a model-based diffusion framework with a safety shield to generate trajectories that are both kinodynamically feasible and safe by construction. By enforcing feasibility and safety on all samples during the denoising process, our method avoids the common pitfalls of post-processing corrections, such as computational intractability and loss of feasibility. We validate our approach on challenging non-convex planning problems, including kinematic and acceleration-controlled tractor-trailer systems. The results show that it substantially outperforms existing safety strategies in success rate and safety, while achieving sub-second computation times.", "AI": {"tldr": "Safe MPD\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u514d\u8bad\u7ec3\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u6846\u67b6\u548c\u5b89\u5168\u9632\u62a4\u673a\u5236\uff0c\u80fd\u591f\u751f\u6210\u65e2\u6ee1\u8db3\u52a8\u529b\u5b66\u7ea6\u675f\u53c8\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u8f68\u8ff9\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u751f\u6210\u5b89\u5168\u3001\u52a8\u529b\u5b66\u53ef\u884c\u4e14\u6700\u4f18\u8f68\u8ff9\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u907f\u514d\u4f20\u7edf\u540e\u5904\u7406\u6821\u6b63\u65b9\u6cd5\u5e26\u6765\u7684\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\u548c\u53ef\u884c\u6027\u635f\u5931\u95ee\u9898\u3002", "method": "\u5c06\u57fa\u4e8e\u6a21\u578b\u7684\u6269\u6563\u6846\u67b6\u4e0e\u5b89\u5168\u9632\u62a4\u673a\u5236\u76f8\u7ed3\u5408\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u5bf9\u6240\u6709\u6837\u672c\u5f3a\u5236\u6267\u884c\u53ef\u884c\u6027\u548c\u5b89\u5168\u6027\u7ea6\u675f\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u975e\u51f8\u89c4\u5212\u95ee\u9898\uff08\u5305\u62ec\u8fd0\u52a8\u5b66\u548c\u52a0\u901f\u5ea6\u63a7\u5236\u7684\u62d6\u62c9\u673a-\u62d6\u8f66\u7cfb\u7edf\uff09\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u5b89\u5168\u7b56\u7565\u5728\u6210\u529f\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u5b9e\u73b0\u4e9a\u79d2\u7ea7\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "Safe MPD\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u751f\u6210\u5b89\u5168\u4e14\u52a8\u529b\u5b66\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u907f\u514d\u4e86\u540e\u5904\u7406\u6821\u6b63\u7684\u5e38\u89c1\u7f3a\u9677\uff0c\u5728\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.06200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06200", "abs": "https://arxiv.org/abs/2512.06200", "authors": ["Tomohiro Yamashita", "Daichi Amagata", "Yusuke Matsui"], "title": "How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?", "comment": "4 pages, 4 figures. Accepted at NeurIPS 2025 Workshop on Machine Learning for Systems", "summary": "Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u548c\u6307\u6807\u6765\u8bc4\u4f30\u52a8\u6001\u6570\u636e\u4e2d\u8fd1\u4f3c\u6700\u8fd1\u90bb\u641c\u7d22\uff08ANNS\uff09\u7d22\u5f15\u7684\u6570\u636e\u5220\u9664\u6548\u7387\uff0c\u5c06\u56fe\u57faANNS\u7684\u6570\u636e\u5220\u9664\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\u5e76\u6570\u5b66\u5f62\u5f0f\u5316\uff0c\u6700\u540e\u5e94\u7528\u4e8eHNSW\u65b9\u6cd5\u5e76\u63d0\u51fa\u52a8\u6001\u9009\u62e9\u5220\u9664\u65b9\u6cd5\u7684Deletion Control\u7b56\u7565\u3002", "motivation": "\u7531\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7b49\u5e94\u7528\u9700\u8981\u652f\u6301\u52a8\u6001\u6570\u636e\u7684ANNS\u7b97\u6cd5\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u6570\u636e\u5220\u9664\u7684\u5168\u9762\u8bc4\u4f30\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5c06\u56fe\u57faANNS\u7684\u6570\u636e\u5220\u9664\u65b9\u6cd5\u5206\u4e3a\u4e09\u7c7b\u5e76\u6570\u5b66\u5f62\u5f0f\u5316\uff0c\u63d0\u51fa\u5305\u542b\u51c6\u786e\u6027\u3001\u67e5\u8be2\u901f\u5ea6\u7b49\u6307\u6807\u7684\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u5e94\u7528\u4e8eHNSW\u65b9\u6cd5\u5e76\u5f00\u53d1Deletion Control\u7b56\u7565\u3002", "result": "\u5efa\u7acb\u4e86\u6570\u636e\u5220\u9664\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u5220\u9664\u65b9\u6cd5\u5bf9ANNS\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u7684Deletion Control\u65b9\u6cd5\u80fd\u6839\u636e\u6240\u9700\u641c\u7d22\u7cbe\u5ea6\u52a8\u6001\u9009\u62e9\u6700\u4f18\u5220\u9664\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86ANNS\u6570\u636e\u5220\u9664\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u4e3a\u52a8\u6001ANNS\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0cDeletion Control\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u5220\u9664\u6548\u7387\u548c\u641c\u7d22\u6027\u80fd\u3002"}}
{"id": "2512.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06058", "abs": "https://arxiv.org/abs/2512.06058", "authors": ["Siming Yan"], "title": "Representation Learning for Point Cloud Understanding", "comment": "181 pages", "summary": "With the rapid advancement of technology, 3D data acquisition and utilization have become increasingly prevalent across various fields, including computer vision, robotics, and geospatial analysis. 3D data, captured through methods such as 3D scanners, LiDARs, and RGB-D cameras, provides rich geometric, shape, and scale information. When combined with 2D images, 3D data offers machines a comprehensive understanding of their environment, benefiting applications like autonomous driving, robotics, remote sensing, and medical treatment. This dissertation focuses on three main areas: supervised representation learning for point cloud primitive segmentation, self-supervised learning methods, and transfer learning from 2D to 3D. Our approach, which integrates pre-trained 2D models to support 3D network training, significantly improves 3D understanding without merely transforming 2D data. Extensive experiments validate the effectiveness of our methods, showcasing their potential to advance point cloud representation learning by effectively integrating 2D knowledge.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u76842D\u6a21\u578b\u6765\u589e\u5f3a3D\u70b9\u4e91\u8868\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u6db5\u76d6\u76d1\u7763\u5b66\u4e60\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u4ece2D\u52303D\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u77403D\u6570\u636e\u91c7\u96c6\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c3D\u6570\u636e\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u673a\u5668\u4eba\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\u3002\u5c063D\u6570\u636e\u4e0e2D\u56fe\u50cf\u7ed3\u5408\u53ef\u4ee5\u4e3a\u673a\u5668\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u57283D\u8868\u793a\u5b66\u4e60\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6574\u5408\u9884\u8bad\u7ec32D\u6a21\u578b\u6765\u652f\u63013D\u7f51\u7edc\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff08\u70b9\u4e91\u57fa\u5143\u5206\u5272\uff09\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4ee5\u53ca\u4ece2D\u52303D\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u907f\u514d\u7b80\u5355\u5730\u5c062D\u6570\u636e\u8f6c\u6362\u4e3a3D\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u901a\u8fc7\u6709\u6548\u6574\u54082D\u77e5\u8bc6\u53ef\u4ee5\u663e\u8457\u63d0\u53473D\u7406\u89e3\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u70b9\u4e91\u8868\u793a\u5b66\u4e60\u7684\u8fdb\u6b65\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5de7\u5999\u6574\u54082D\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u6210\u529f\u63d0\u5347\u4e863D\u70b9\u4e91\u8868\u793a\u5b66\u4e60\u7684\u6548\u679c\uff0c\u4e3a3D\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.06423", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06423", "abs": "https://arxiv.org/abs/2512.06423", "authors": ["Leonardo F. Dos Santos", "Elisa G. Vergamini", "C\u00edcero Zanette", "Lucca Maitan", "Thiago Boaventura"], "title": "Leveraging Port-Hamiltonian Theory for Impedance Control Benchmarking", "comment": "This is the author's version of the paper accepted for publication in the 2025 International Conference on Advanced Robotics (ICAR). The final version will be available at IEEE Xplore", "summary": "This work proposes PH-based metrics for benchmarking impedance control. A causality-consistent PH model is introduced for mass-spring-damper impedance in Cartesian space. Based on this model, a differentiable, force-torque sensing-independent, n-DoF passivity condition is derived, valid for time-varying references. An impedance fidelity metric is also defined from step-response power in free motion, capturing dynamic decoupling. The proposed metrics are validated in Gazebo simulations with a six-DoF manipulator and a quadruped leg. Results demonstrate the suitability of the PH framework for standardized impedance control benchmarking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u7aef\u53e3\u54c8\u5bc6\u987f(PH)\u7cfb\u7edf\u7684\u963b\u6297\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u6307\u6807\uff0c\u5305\u62ec\u56e0\u679c\u4e00\u81f4\u7684PH\u6a21\u578b\u3001n\u81ea\u7531\u5ea6\u65e0\u6e90\u6027\u6761\u4ef6\u548c\u963b\u6297\u4fdd\u771f\u5ea6\u5ea6\u91cf\u3002", "motivation": "\u5f53\u524d\u963b\u6297\u63a7\u5236\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc4\u4f30\u65f6\u95f4\u53d8\u5316\u53c2\u8003\u4e0b\u591a\u81ea\u7531\u5ea6\u7cfb\u7edf\u6027\u80fd\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u5f15\u5165\u56e0\u679c\u4e00\u81f4\u7684\u7aef\u53e3\u54c8\u5bc6\u987f\u6a21\u578b\u6765\u63cf\u8ff0\u7b1b\u5361\u5c14\u7a7a\u95f4\u4e2d\u7684\u8d28\u91cf-\u5f39\u7c27-\u963b\u5c3c\u963b\u6297\uff0c\u63a8\u5bfc\u51fa\u53ef\u5fae\u5206\u3001\u4e0d\u4f9d\u8d56\u529b-\u529b\u77e9\u4f20\u611f\u7684n\u81ea\u7531\u5ea6\u65e0\u6e90\u6027\u6761\u4ef6\uff0c\u5e76\u57fa\u4e8e\u81ea\u7531\u8fd0\u52a8\u4e2d\u7684\u9636\u8dc3\u54cd\u5e94\u529f\u7387\u5b9a\u4e49\u963b\u6297\u4fdd\u771f\u5ea6\u5ea6\u91cf\u3002", "result": "\u5728Gazebo\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u6240\u63d0\u6307\u6807\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u548c\u56db\u8db3\u673a\u5668\u4eba\u817f\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660ePH\u6846\u67b6\u9002\u7528\u4e8e\u6807\u51c6\u5316\u963b\u6297\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u7aef\u53e3\u54c8\u5bc6\u987f\u6846\u67b6\u4e3a\u963b\u6297\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6240\u63d0\u51fa\u7684\u6307\u6807\u80fd\u591f\u51c6\u786e\u8bc4\u4f30\u591a\u81ea\u7531\u5ea6\u7cfb\u7edf\u7684\u52a8\u6001\u6027\u80fd\u3002"}}
{"id": "2512.06201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06201", "abs": "https://arxiv.org/abs/2512.06201", "authors": ["K2 Team", "Zhengzhong Liu", "Liping Tang", "Linghao Jin", "Haonan Li", "Nikhil Ranjan", "Desai Fan", "Shaurya Rohatgi", "Richard Fan", "Omkar Pangarkar", "Huijuan Wang", "Zhoujun Cheng", "Suqi Sun", "Seungwook Han", "Bowen Tan", "Gurpreet Gosal", "Xudong Han", "Varad Pimpalkhute", "Shibo Hao", "Ming Shan Hee", "Joel Hestness", "Haolong Jia", "Liqun Ma", "Aaryamonvikram Singh", "Daria Soboleva", "Natalia Vassilieva", "Renxi Wang", "Yingquan Wu", "Yuekai Sun", "Taylor Killian", "Alexander Moreno", "John Maggs", "Hector Ren", "Guowei He", "Hongyi Wang", "Xuezhe Ma", "Yuqi Wang", "Mikhail Yurochkin", "Eric P. Xing"], "title": "K2-V2: A 360-Open, Reasoning-Enhanced LLM", "comment": null, "summary": "We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.", "AI": {"tldr": "K2-V2\u662f\u4e00\u4e2a\u4ece\u5934\u6784\u5efa\u7684360\u5ea6\u5f00\u653eLLM\uff0c\u4e13\u4e3a\u63a8\u7406\u9002\u5e94\u800c\u8bbe\u8ba1\uff0c\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8d85\u8d8a\u540c\u5c3a\u5bf8\u5f00\u6e90\u6a21\u578b\uff0c\u63a5\u8fd1\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u6784\u5efa\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4f18\u5316\u7684\u5f00\u6e90\u57fa\u7840\u6a21\u578b\uff0c\u586b\u8865\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u5f3a\u5927\u7684\u63a8\u7406\u4e2d\u5fc3\u57fa\u7840\u3002", "method": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u3001\u63a8\u7406\u80fd\u529b\u3001\u957f\u4e0a\u4e0b\u6587\u548c\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u76d1\u7763\u5fae\u8c03\u5efa\u7acb\u5f3a\u57fa\u7ebf\u3002", "result": "K2-V2\u6210\u4e3a\u6700\u5f3a\u7684\u5b8c\u5168\u5f00\u6e90\u6a21\u578b\uff0c\u5728\u540c\u7b49\u5c3a\u5bf8\u4e0b\u8d85\u8d8aQwen2.5-72B\uff0c\u63a5\u8fd1Qwen3-235B\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03\u5b8c\u6574\u7684\u8bad\u7ec3\u5386\u53f2\u548c\u6570\u636e\u7ec4\u5408\uff0c\u6700\u5927\u5316\u6301\u7eed\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u4e00\u4e2a\u80fd\u529b\u5f3a\u5927\u3001\u4ee5\u63a8\u7406\u4e3a\u4e2d\u5fc3\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2512.06065", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06065", "abs": "https://arxiv.org/abs/2512.06065", "authors": ["Runjia Li", "Moayed Haji-Ali", "Ashkan Mirzaei", "Chaoyang Wang", "Arpit Sahni", "Ivan Skorokhodov", "Aliaksandr Siarohin", "Tomas Jakab", "Junlin Han", "Sergey Tulyakov", "Philip Torr", "Willi Menapace"], "title": "EgoEdit: Dataset, Real-Time Streaming Model, and Benchmark for Egocentric Video Editing", "comment": "Project page: https://snap-research.github.io/EgoEdit", "summary": "We study instruction-guided editing of egocentric videos for interactive AR applications. While recent AI video editors perform well on third-person footage, egocentric views present unique challenges - including rapid egomotion and frequent hand-object interactions - that create a significant domain gap. Moreover, existing offline editing pipelines suffer from high latency, limiting real-time interaction. To address these issues, we present a complete ecosystem for egocentric video editing. First, we construct EgoEditData, a carefully designed and manually curated dataset specifically designed for egocentric editing scenarios, featuring rich hand-object interactions, while explicitly preserving hands. Second, we develop EgoEdit, an instruction-following egocentric video editor that supports real-time streaming inference on a single GPU. Finally, we introduce EgoEditBench, an evaluation suite targeting instruction faithfulness, hand and interaction preservation, and temporal stability under egomotion. Across both egocentric and general editing tasks, EgoEdit produces temporally stable, instruction-faithful results with interactive latency. It achieves clear gains on egocentric editing benchmarks-where existing methods struggle-while maintaining performance comparable to the strongest baselines on general editing tasks. EgoEditData and EgoEditBench will be made public for the research community. See our website at https://snap-research.github.io/EgoEdit", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u7f16\u8f91\u7684\u5b8c\u6574\u751f\u6001\u7cfb\u7edf\uff0c\u5305\u62ec\u4e13\u95e8\u6784\u5efa\u7684\u6570\u636e\u96c6EgoEditData\u3001\u5b9e\u65f6\u89c6\u9891\u7f16\u8f91\u5668EgoEdit\u548c\u8bc4\u4f30\u5957\u4ef6EgoEditBench\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709AI\u89c6\u9891\u7f16\u8f91\u5668\u5728\u7b2c\u4e09\u4eba\u79f0\u89c6\u9891\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u4e0a\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u5982\u5feb\u901f\u7684\u81ea\u4f53\u8fd0\u52a8\u548c\u9891\u7e41\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u9886\u57df\u5dee\u8ddd\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u79bb\u7ebf\u7f16\u8f91\u6d41\u7a0b\u5b58\u5728\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u4ea4\u4e92\u3002", "method": "1) \u6784\u5efaEgoEditData\u6570\u636e\u96c6\uff0c\u4e13\u95e8\u9488\u5bf9\u7b2c\u4e00\u4eba\u79f0\u7f16\u8f91\u573a\u666f\u8bbe\u8ba1\uff0c\u5305\u542b\u4e30\u5bcc\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\u5e76\u660e\u786e\u4fdd\u7559\u624b\u90e8\u4fe1\u606f\uff1b2) \u5f00\u53d1EgoEdit\u89c6\u9891\u7f16\u8f91\u5668\uff0c\u652f\u6301\u5728\u5355\u4e2aGPU\u4e0a\u8fdb\u884c\u5b9e\u65f6\u6d41\u5f0f\u63a8\u7406\uff1b3) \u5f15\u5165EgoEditBench\u8bc4\u4f30\u5957\u4ef6\uff0c\u8bc4\u4f30\u6307\u4ee4\u5fe0\u5b9e\u5ea6\u3001\u624b\u90e8\u548c\u4ea4\u4e92\u4fdd\u7559\u4ee5\u53ca\u81ea\u4f53\u8fd0\u52a8\u4e0b\u7684\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "result": "EgoEdit\u5728\u7b2c\u4e00\u4eba\u79f0\u548c\u901a\u7528\u7f16\u8f91\u4efb\u52a1\u4e0a\u90fd\u4ea7\u751f\u4e86\u65f6\u95f4\u7a33\u5b9a\u3001\u6307\u4ee4\u5fe0\u5b9e\u7684\u7ed3\u679c\uff0c\u5177\u6709\u4ea4\u4e92\u5f0f\u5ef6\u8fdf\u3002\u5728\u7b2c\u4e00\u4eba\u79f0\u7f16\u8f91\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u660e\u663e\u4f18\u52bf\uff0c\u540c\u65f6\u5728\u901a\u7528\u7f16\u8f91\u4efb\u52a1\u4e0a\u4fdd\u6301\u4e86\u4e0e\u6700\u5f3a\u57fa\u7ebf\u7684\u53ef\u6bd4\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u751f\u6001\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7f16\u8f91\u7684\u72ec\u7279\u6311\u6218\uff0cEgoEditData\u548cEgoEditBench\u5c06\u4e3a\u7814\u7a76\u793e\u533a\u516c\u5f00\uff0c\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.06444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06444", "abs": "https://arxiv.org/abs/2512.06444", "authors": ["Xuehui Ma", "Shiliang Zhang", "Zhiyong Sun"], "title": "Fault Tolerant Control of Mecanum Wheeled Mobile Robots", "comment": null, "summary": "Mecanum wheeled mobile robots (MWMRs) are highly susceptible to actuator faults that degrade performance and risk mission failure. Current fault tolerant control (FTC) schemes for MWMRs target complete actuator failures like motor stall, ignoring partial faults e.g., in torque degradation. We propose an FTC strategy handling both fault types, where we adopt posterior probability to learn real-time fault parameters. We derive the FTC law by aggregating probability-weighed control laws corresponding to predefined faults. This ensures the robustness and safety of MWMR control despite varying levels of fault occurrence. Simulation results demonstrate the effectiveness of our FTC under diverse scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ea6\u514b\u7eb3\u59c6\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\uff08MWMRs\uff09\u7684\u5bb9\u9519\u63a7\u5236\u7b56\u7565\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u5b8c\u5168\u548c\u90e8\u5206\u6267\u884c\u5668\u6545\u969c\uff0c\u901a\u8fc7\u540e\u9a8c\u6982\u7387\u5b9e\u65f6\u5b66\u4e60\u6545\u969c\u53c2\u6570\uff0c\u63d0\u9ad8\u63a7\u5236\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7684MWMR\u5bb9\u9519\u63a7\u5236\u65b9\u6848\u4e3b\u8981\u9488\u5bf9\u5b8c\u5168\u6267\u884c\u5668\u6545\u969c\uff08\u5982\u7535\u673a\u5931\u901f\uff09\uff0c\u5ffd\u7565\u4e86\u90e8\u5206\u6545\u969c\uff08\u5982\u626d\u77e9\u9000\u5316\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u7cfb\u7edf\u5728\u590d\u6742\u6545\u969c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u540e\u9a8c\u6982\u7387\u5b9e\u65f6\u5b66\u4e60\u6545\u969c\u53c2\u6570\uff0c\u901a\u8fc7\u805a\u5408\u9884\u5b9a\u4e49\u6545\u969c\u5bf9\u5e94\u7684\u6982\u7387\u52a0\u6743\u63a7\u5236\u5f8b\u6765\u63a8\u5bfc\u5bb9\u9519\u63a7\u5236\u5f8b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u5bb9\u9519\u63a7\u5236\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5747\u80fd\u6709\u6548\u5e94\u5bf9\u591a\u79cd\u7c7b\u578b\u7684\u6267\u884c\u5668\u6545\u969c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5bb9\u9519\u63a7\u5236\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u5347MWMR\u5728\u590d\u6742\u6545\u969c\u60c5\u51b5\u4e0b\u7684\u63a7\u5236\u9c81\u68d2\u6027\u548c\u7cfb\u7edf\u5b89\u5168\u6027\u3002"}}
{"id": "2512.06204", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06204", "abs": "https://arxiv.org/abs/2512.06204", "authors": ["Rodney Lafuente-Mercado", "Daniela Rus", "T. Konstantin Rusch"], "title": "Quantifying Memory Use in Reinforcement Learning with Temporal Range", "comment": null, "summary": "How much does a trained RL policy actually use its past observations? We propose \\emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\\partial y_s/\\partial x_t\\in\\mathbb{R}^{c\\times d}$ averaged over final timesteps $s\\in\\{t+1,\\dots,T\\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Temporal Range\u6307\u6807\uff0c\u7528\u4e8e\u91cf\u5316\u8bad\u7ec3\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5bf9\u8fc7\u53bb\u89c2\u5bdf\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u901a\u8fc7\u5206\u6790\u8f93\u51fa\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u65f6\u95f4\u654f\u611f\u6027\u6765\u8bc4\u4f30\u8bb0\u5fc6\u4f9d\u8d56\u3002", "motivation": "\u7814\u7a76\u8bad\u7ec3\u597d\u7684RL\u7b56\u7565\u5b9e\u9645\u4f7f\u7528\u8fc7\u53bb\u89c2\u5bdf\u7684\u7a0b\u5ea6\uff0c\u4e3a\u6bd4\u8f83\u4e0d\u540c\u667a\u80fd\u4f53\u548c\u73af\u5883\u63d0\u4f9b\u91cf\u5316\u6307\u6807\uff0c\u5e2e\u52a9\u9009\u62e9\u6700\u77ed\u6709\u6548\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002", "method": "\u63d0\u51faTemporal Range\u6307\u6807\uff0c\u901a\u8fc7\u53cd\u5411\u81ea\u52a8\u5fae\u5206\u8ba1\u7b97Jacobian\u5757\uff0c\u5206\u6790\u591a\u5411\u91cf\u8f93\u51fa\u5bf9\u8f93\u5165\u5e8f\u5217\u7684\u4e00\u9636\u654f\u611f\u6027\uff0c\u5f62\u6210\u65f6\u95f4\u5f71\u54cd\u5206\u5e03\uff0c\u5e76\u7528\u5e45\u5ea6\u52a0\u6743\u5e73\u5747\u6ede\u540e\u8fdb\u884c\u603b\u7ed3\u3002", "result": "\u5728\u8bca\u65ad\u548c\u63a7\u5236\u4efb\u52a1\u4e2d\uff0cTemporal Range\u5728\u5b8c\u5168\u89c2\u5bdf\u63a7\u5236\u4e2d\u4fdd\u6301\u8f83\u5c0f\uff0c\u5728Copy-k\u4efb\u52a1\u4e2d\u4e0e\u771f\u5b9e\u6ede\u540e\u5c3a\u5ea6\u4e00\u81f4\uff0c\u4e0e\u7a97\u53e3\u6d88\u878d\u5b9e\u9a8c\u786e\u8ba4\u7684\u6700\u5c0f\u5386\u53f2\u7a97\u53e3\u9700\u6c42\u76f8\u5339\u914d\u3002", "conclusion": "Temporal Range\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5e8f\u5217\u7ea7\u8bb0\u5fc6\u4f9d\u8d56\u5ea6\u91cf\uff0c\u53ef\u7528\u4e8e\u6bd4\u8f83\u667a\u80fd\u4f53\u548c\u73af\u5883\uff0c\u4ee5\u53ca\u9009\u62e9\u6700\u77ed\u8db3\u591f\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2512.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06080", "abs": "https://arxiv.org/abs/2512.06080", "authors": ["Tzofi Klinghoffer", "Siddharth Somasundaram", "Xiaoyu Xiang", "Yuchen Fan", "Christian Richardt", "Akshat Dave", "Ramesh Raskar", "Rakesh Ranjan"], "title": "Shoot-Bounce-3D: Single-Shot Occlusion-Aware 3D from Lidar by Decomposing Two-Bounce Light", "comment": "SIGGRAPH Asia 2025. Project page: https://shoot-bounce-3d.github.io", "summary": "3D scene reconstruction from a single measurement is challenging, especially in the presence of occluded regions and specular materials, such as mirrors. We address these challenges by leveraging single-photon lidars. These lidars estimate depth from light that is emitted into the scene and reflected directly back to the sensor. However, they can also measure light that bounces multiple times in the scene before reaching the sensor. This multi-bounce light contains additional information that can be used to recover dense depth, occluded geometry, and material properties. Prior work with single-photon lidar, however, has only demonstrated these use cases when a laser sequentially illuminates one scene point at a time. We instead focus on the more practical - and challenging - scenario of illuminating multiple scene points simultaneously. The complexity of light transport due to the combined effects of multiplexed illumination, two-bounce light, shadows, and specular reflections is challenging to invert analytically. Instead, we propose a data-driven method to invert light transport in single-photon lidar. To enable this approach, we create the first large-scale simulated dataset of ~100k lidar transients for indoor scenes. We use this dataset to learn a prior on complex light transport, enabling measured two-bounce light to be decomposed into the constituent contributions from each laser spot. Finally, we experimentally demonstrate how this decomposed light can be used to infer 3D geometry in scenes with occlusions and mirrors from a single measurement. Our code and dataset are released at https://shoot-bounce-3d.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u591a\u53cd\u5c04\u5149\u6765\u91cd\u5efa\u5305\u542b\u906e\u6321\u548c\u955c\u9762\u53cd\u5c04\u76843D\u573a\u666f\u3002", "motivation": "\u5355\u6b21\u6d4b\u91cf\u91cd\u5efa3D\u573a\u666f\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u906e\u6321\u533a\u57df\u548c\u955c\u9762\u6750\u6599\uff08\u5982\u955c\u5b50\uff09\u7684\u60c5\u51b5\u4e0b\u3002\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u53ef\u4ee5\u6d4b\u91cf\u591a\u6b21\u53cd\u5c04\u7684\u5149\uff0c\u8fd9\u4e9b\u5149\u5305\u542b\u6062\u590d\u5bc6\u96c6\u6df1\u5ea6\u3001\u88ab\u906e\u6321\u51e0\u4f55\u5f62\u72b6\u548c\u6750\u6599\u5c5e\u6027\u7684\u989d\u5916\u4fe1\u606f\u3002", "method": "\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u5ba4\u5185\u573a\u666f\u6fc0\u5149\u96f7\u8fbe\u77ac\u6001\u6a21\u62df\u6570\u636e\u96c6\uff08\u7ea610\u4e07\u4e2a\u6837\u672c\uff09\uff0c\u5b66\u4e60\u590d\u6742\u5149\u4f20\u8f93\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5c06\u6d4b\u91cf\u7684\u53cc\u53cd\u5c04\u5149\u5206\u89e3\u4e3a\u6bcf\u4e2a\u6fc0\u5149\u70b9\u7684\u8d21\u732e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4ece\u5355\u6b21\u6d4b\u91cf\u4e2d\u63a8\u65ad\u51fa\u5305\u542b\u906e\u6321\u548c\u955c\u5b50\u7684\u573a\u666f\u76843D\u51e0\u4f55\u5f62\u72b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u591a\u573a\u666f\u70b9\u540c\u65f6\u7167\u660e\u6761\u4ef6\u4e0b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u5149\u4f20\u8f93\u7684\u9006\u63a8\uff0c\u4e3a\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u57283D\u573a\u666f\u91cd\u5efa\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.06486", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06486", "abs": "https://arxiv.org/abs/2512.06486", "authors": ["Wanru Gong", "Xinyi Zheng", "Xiaopeng Yang", "Xiaoqing Zhu"], "title": "Entropy-Controlled Intrinsic Motivation Reinforcement Learning for Quadruped Robot Locomotion in Complex Terrains", "comment": null, "summary": "Learning is the basis of both biological and artificial systems when it comes to mimicking intelligent behaviors. From the classical PPO (Proximal Policy Optimization), there is a series of deep reinforcement learning algorithms which are widely used in training locomotion policies for quadrupedal robots because of their stability and sample efficiency. However, among all these variants, experiments and simulations often converge prematurely, leading to suboptimal locomotion and reduced task performance. Therefore, in this paper, we introduce Entropy-Controlled Intrinsic Motivation (ECIM), an entropy-based reinforcement learning algorithm in contrast with the PPO series, that can reduce premature convergence by combining intrinsic motivation with adaptive exploration.\n  For experiments, in order to parallel with other baselines, we chose to apply it in Isaac Gym across six terrain categories: upward slopes, downward slopes, uneven rough terrain, ascending stairs, descending stairs, and flat ground as widely used. For comparison, our experiments consistently achieve better performance: task rewards increase by 4--12%, peak body pitch oscillation is reduced by 23--29%, joint acceleration decreases by 20--32%, and joint torque consumption declines by 11--20%. Overall, our model ECIM, by combining entropy control and intrinsic motivation control, achieves better results in stability across different terrains for quadrupedal locomotion, and at the same time reduces energetic cost and makes it a practical choice for complex robotic control tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ECIM\uff08\u71b5\u63a7\u5185\u5728\u52a8\u673a\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5185\u5728\u52a8\u673a\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u6765\u51cf\u5c11PPO\u7cfb\u5217\u7b97\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u8bad\u7ec3\u4e2d\u7684\u8fc7\u65e9\u6536\u655b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfPPO\u7cfb\u5217\u7b97\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u8bad\u7ec3\u4e2d\u5bb9\u6613\u8fc7\u65e9\u6536\u655b\uff0c\u5bfc\u81f4\u8fd0\u52a8\u6027\u80fd\u6b21\u4f18\u548c\u4efb\u52a1\u8868\u73b0\u4e0b\u964d\u3002", "method": "ECIM\u7b97\u6cd5\u7ed3\u5408\u71b5\u63a7\u5236\u548c\u5185\u5728\u52a8\u673a\u63a7\u5236\uff0c\u5728Isaac Gym\u73af\u5883\u4e2d\u5bf9\u516d\u79cd\u5730\u5f62\uff08\u4e0a\u5761\u3001\u4e0b\u5761\u3001\u4e0d\u5e73\u5766\u7c97\u7cd9\u5730\u5f62\u3001\u4e0a\u697c\u68af\u3001\u4e0b\u697c\u68af\u3001\u5e73\u5730\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4efb\u52a1\u5956\u52b1\u63d0\u53474-12%\uff0c\u5cf0\u503c\u8eab\u4f53\u4fef\u4ef0\u632f\u8361\u51cf\u5c1123-29%\uff0c\u5173\u8282\u52a0\u901f\u5ea6\u964d\u4f4e20-32%\uff0c\u5173\u8282\u626d\u77e9\u6d88\u8017\u4e0b\u964d11-20%\u3002", "conclusion": "ECIM\u7b97\u6cd5\u5728\u4e0d\u540c\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u56db\u8db3\u8fd0\u52a8\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u662f\u590d\u6742\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u5b9e\u7528\u9009\u62e9\u3002"}}
{"id": "2512.06218", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.06218", "abs": "https://arxiv.org/abs/2512.06218", "authors": ["Huizhen Yu", "Yi Wan", "Richard S. Sutton"], "title": "Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration", "comment": "24 pages. This paper presents the reinforcement-learning material previously contained in version 2 of arXiv:2409.03915, which is now being split into two stand-alone papers. Minor corrections and improvements to the main results have also been made in the course of this reformatting", "summary": "This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.", "AI": {"tldr": "\u672c\u6587\u5e94\u7528\u5f02\u6b65\u968f\u673a\u903c\u8fd1\u7406\u8bba\u5230\u5e73\u5747\u5956\u52b1\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5efa\u7acb\u4e86RVI Q-learning\u7b97\u6cd5\u7684\u6536\u655b\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5355\u8c03\u6027\u6761\u4ef6\u6765\u4f30\u8ba1\u6700\u4f18\u5956\u52b1\u7387\u3002", "motivation": "\u5c06Borkar-Meyn\u6846\u67b6\u4e0b\u7684\u5f02\u6b65\u968f\u673a\u903c\u8fd1\u7406\u8bba\u5e94\u7528\u4e8e\u534a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u6269\u5c55Schweitzer\u7ecf\u5178\u76f8\u5bf9\u503c\u8fed\u4ee3\u7b97\u6cd5\u5230\u5f02\u6b65\u8bbe\u7f6e\u3002", "method": "\u4f7f\u7528\u5f02\u6b65\u968f\u673a\u903c\u8fd1\u65b9\u6cd5\uff0c\u5efa\u7acbRVI Q-learning\u7b97\u6cd5\u7684\u6536\u655b\u6027\u5206\u6790\uff0c\u5f15\u5165\u65b0\u7684\u5355\u8c03\u6027\u6761\u4ef6\u6765\u4f30\u8ba1\u6700\u4f18\u5956\u52b1\u7387\u3002", "result": "\u8bc1\u660e\u7b97\u6cd5\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u5230\u5e73\u5747\u5956\u52b1\u6700\u4f18\u6027\u65b9\u7a0b\u89e3\u96c6\u7684\u7d27\u81f4\u8fde\u901a\u5b50\u96c6\uff0c\u5728\u989d\u5916\u6b65\u957f\u548c\u5f02\u6b65\u6761\u4ef6\u4e0b\u6536\u655b\u5230\u552f\u4e00\u89e3\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u5355\u8c03\u6027\u6761\u4ef6\u663e\u8457\u6269\u5c55\u4e86\u73b0\u6709\u7b97\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u7a33\u5b9a\u6027\u5206\u6790\u548c\u6536\u655b\u5206\u6790\u7684\u65b0\u8bba\u8bc1\u65b9\u6cd5\u89e3\u51b3\u4e86\u76f8\u5173\u95ee\u9898\u3002"}}
{"id": "2512.06096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06096", "abs": "https://arxiv.org/abs/2512.06096", "authors": ["Karthik Mohan", "Sonam Singh", "Amit Arvind Kale"], "title": "BeLLA: End-to-End Birds Eye View Large Language Assistant for Autonomous Driving", "comment": null, "summary": "The rapid development of Vision-Language models (VLMs) and Multimodal Language Models (MLLMs) in autonomous driving research has significantly reshaped the landscape by enabling richer scene understanding, context-aware reasoning, and more interpretable decision-making. However, a lot of existing work often relies on either single-view encoders that fail to exploit the spatial structure of multi-camera systems or operate on aggregated multi-view features, which lack a unified spatial representation, making it more challenging to reason about ego-centric directions, object relations, and the wider context. We thus present BeLLA, an end-to-end architecture that connects unified 360\u00b0 BEV representations with a large language model for question answering in autonomous driving. We primarily evaluate our work using two benchmarks - NuScenes-QA and DriveLM, where BeLLA consistently outperforms existing approaches on questions that require greater spatial reasoning, such as those involving relative object positioning and behavioral understanding of nearby objects, achieving up to +9.3% absolute improvement in certain tasks. In other categories, BeLLA performs competitively, demonstrating the capability of handling a diverse range of questions.", "AI": {"tldr": "BeLLA\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u67b6\u6784\uff0c\u5c06360\u00b0BEV\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u95ee\u7b54\uff0c\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8981\u4e48\u4f7f\u7528\u5355\u89c6\u89d2\u7f16\u7801\u5668\u65e0\u6cd5\u5229\u7528\u591a\u6444\u50cf\u5934\u7a7a\u95f4\u7ed3\u6784\uff0c\u8981\u4e48\u4f7f\u7528\u805a\u5408\u591a\u89c6\u89d2\u7279\u5f81\u7f3a\u4e4f\u7edf\u4e00\u7a7a\u95f4\u8868\u793a\uff0c\u96be\u4ee5\u8fdb\u884c\u7a7a\u95f4\u63a8\u7406", "method": "\u63d0\u51faBeLLA\u67b6\u6784\uff0c\u8fde\u63a5\u7edf\u4e00\u7684360\u00b0BEV\uff08\u9e1f\u77b0\u56fe\uff09\u8868\u793a\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u95ee\u7b54", "result": "\u5728NuScenes-QA\u548cDriveLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBeLLA\u5728\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u5bf9\u7269\u4f53\u5b9a\u4f4d\u548c\u884c\u4e3a\u7406\u89e3\u4efb\u52a1\u83b7\u5f97\u6700\u9ad8+9.3%\u7684\u7edd\u5bf9\u63d0\u5347", "conclusion": "BeLLA\u80fd\u591f\u5904\u7406\u591a\u6837\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00BEV\u8868\u793a\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u6709\u6548\u6027"}}
{"id": "2512.06517", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06517", "abs": "https://arxiv.org/abs/2512.06517", "authors": ["Shifa Sulaiman", "Akash Bachhar", "Ming Shen", "Simon B\u00f8gh"], "title": "Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments", "comment": null, "summary": "Recent advancements in prosthetic technology have increasingly focused on enhancing dexterity and autonomy through intelligent control systems. Vision-based approaches offer promising results for enabling prosthetic hands to interact more naturally with diverse objects in dynamic environments. Building on this foundation, the paper presents a vision-guided grasping algorithm for a prosthetic hand, integrating perception, planning, and control for dexterous manipulation. A camera mounted on the set up captures the scene, and a Bounding Volume Hierarchy (BVH)-based vision algorithm is employed to segment an object for grasping and define its bounding box. Grasp contact points are then computed by generating candidate trajectories using Rapidly-exploring Random Tree Star algorithm, and selecting fingertip end poses based on the minimum Euclidean distance between these trajectories and the objects point cloud. Each finger grasp pose is determined independently, enabling adaptive, object-specific configurations. Damped Least Square (DLS) based Inverse kinematics solver is used to compute the corresponding joint angles, which are subsequently transmitted to the finger actuators for execution. This modular pipeline enables per-finger grasp planning and supports real-time adaptability in unstructured environments. The proposed method is validated in simulation, and experimental integration on a Linker Hand O7 platform.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u5f15\u5bfc\u7684\u5047\u624b\u6293\u53d6\u7b97\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u5b9e\u73b0\u7075\u5de7\u64cd\u4f5c\uff0c\u91c7\u7528BVH\u89c6\u89c9\u7b97\u6cd5\u5206\u5272\u7269\u4f53\u5e76\u8ba1\u7b97\u6293\u53d6\u63a5\u89e6\u70b9\uff0c\u4f7f\u7528RRT*\u7b97\u6cd5\u751f\u6210\u8f68\u8ff9\uff0c\u901a\u8fc7DLS\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5173\u8282\u89d2\u5ea6\uff0c\u5728\u4eff\u771f\u548cLinker Hand O7\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u5047\u80a2\u6280\u672f\u9700\u8981\u63d0\u5347\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0e\u591a\u6837\u5316\u7269\u4f53\u81ea\u7136\u4ea4\u4e92\u7684\u7075\u5de7\u6027\u548c\u81ea\u4e3b\u6027\uff0c\u89c6\u89c9\u65b9\u6cd5\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6444\u50cf\u5934\u91c7\u96c6\u573a\u666f\uff0cBVH\u89c6\u89c9\u7b97\u6cd5\u5206\u5272\u7269\u4f53\u5e76\u5b9a\u4e49\u8fb9\u754c\u6846\uff1bRRT*\u7b97\u6cd5\u751f\u6210\u5019\u9009\u8f68\u8ff9\u5e76\u57fa\u4e8e\u6700\u5c0f\u6b27\u6c0f\u8ddd\u79bb\u9009\u62e9\u6307\u5c16\u672b\u7aef\u4f4d\u59ff\uff1b\u72ec\u7acb\u786e\u5b9a\u6bcf\u4e2a\u624b\u6307\u7684\u6293\u53d6\u4f4d\u59ff\uff1bDLS\u9006\u8fd0\u52a8\u5b66\u6c42\u89e3\u5173\u8282\u89d2\u5ea6\u5e76\u4f20\u8f93\u7ed9\u6267\u884c\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548cLinker Hand O7\u5b9e\u9a8c\u5e73\u53f0\u4e0a\u6210\u529f\u9a8c\u8bc1\uff0c\u80fd\u591f\u5b9e\u73b0\u6a21\u5757\u5316\u7684\u5355\u624b\u6307\u6293\u53d6\u89c4\u5212\uff0c\u5e76\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u652f\u6301\u5b9e\u65f6\u9002\u5e94\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u89c6\u89c9\u5f15\u5bfc\u6293\u53d6\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5047\u624b\u7684\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\uff0c\u6a21\u5757\u5316\u7ba1\u9053\u8bbe\u8ba1\u652f\u6301\u5b9e\u65f6\u81ea\u9002\u5e94\uff0c\u4e3a\u667a\u80fd\u5047\u80a2\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.06236", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06236", "abs": "https://arxiv.org/abs/2512.06236", "authors": ["Haiyang Yu", "Meng-Chieh Lee", "Xiang song", "Qi Zhu", "Christos Faloutsos"], "title": "Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph", "comment": null, "summary": "We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faGraphDeT\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u76ee\u6807\u56fe\u4e0a\u6dfb\u52a0\u8fb9\u53bb\u566a\u7684\u8f85\u52a9\u635f\u5931\u51fd\u6570\u6765\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u8282\u70b9\u5206\u7c7b\u6027\u80fd\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u8f85\u52a9\u4efb\u52a1\u80fd\u591f\u6536\u7d27\u56fe\u6cdb\u5316\u8fb9\u754c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u65f6\u95f4\u548c\u533a\u57df\u9886\u57df\u56fe\u504f\u79fb\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u56fe\u7ed3\u6784\u9886\u57df\u504f\u79fb\uff08\u5982\u4e0d\u540c\u65f6\u95f4\u6216\u533a\u57df\u91c7\u96c6\u7684\u56fe\u6570\u636e\uff09\u5bfc\u81f4\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u76ee\u6807\u56fe\u4e0a\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u56fe\u9886\u57df\u81ea\u9002\u5e94\u95ee\u9898\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faGraphDeT\u6846\u67b6\uff0c\u5728\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u96c6\u6210\u8fb9\u53bb\u566a\u7684\u8f85\u52a9\u4efb\u52a1\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u5c06\u8f85\u52a9\u4efb\u52a1\u4e0e\u56fe\u6cdb\u5316\u8fb9\u754c\u8054\u7cfb\u8d77\u6765\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u6536\u7d27\u6cdb\u5316\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGraphDeT\u5728\u5904\u7406\u65f6\u95f4\u548c\u533a\u57df\u9886\u57df\u56fe\u504f\u79fb\u65f6\uff0c\u8282\u70b9\u5206\u7c7b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7b80\u5355\u7684\u8fb9\u53bb\u566a\u8f85\u52a9\u4efb\u52a1\u80fd\u6709\u6548\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0cGraphDeT\u6846\u67b6\u4e3a\u89e3\u51b3\u56fe\u7ed3\u6784\u9886\u57df\u504f\u79fb\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.06103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06103", "abs": "https://arxiv.org/abs/2512.06103", "authors": ["Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "SpectraIrisPAD: Leveraging Vision Foundation Models for Spectrally Conditioned Multispectral Iris Presentation Attack Detection", "comment": "Accepted in IEEE T-BIOM", "summary": "Iris recognition is widely recognized as one of the most accurate biometric modalities. However, its growing deployment in real-world applications raises significant concerns regarding its vulnerability to Presentation Attacks (PAs). Effective Presentation Attack Detection (PAD) is therefore critical to ensure the integrity and security of iris-based biometric systems. While conventional iris recognition systems predominantly operate in the near-infrared (NIR) spectrum, multispectral imaging across multiple NIR bands provides complementary reflectance information that can enhance the generalizability of PAD methods. In this work, we propose \\textbf{SpectraIrisPAD}, a novel deep learning-based framework for robust multispectral iris PAD. The SpectraIrisPAD leverages a DINOv2 Vision Transformer (ViT) backbone equipped with learnable spectral positional encoding, token fusion, and contrastive learning to extract discriminative, band-specific features that effectively distinguish bona fide samples from various spoofing artifacts. Furthermore, we introduce a new comprehensive dataset Multispectral Iris PAD (\\textbf{MSIrPAD}) with diverse PAIs, captured using a custom-designed multispectral iris sensor operating at five distinct NIR wavelengths (800\\,nm, 830\\,nm, 850\\,nm, 870\\,nm, and 980\\,nm). The dataset includes 18,848 iris images encompassing eight diverse PAI categories, including five textured contact lenses, print attacks, and display-based attacks. We conduct comprehensive experiments under unseen attack evaluation protocols to assess the generalization capability of the proposed method. SpectraIrisPAD consistently outperforms several state-of-the-art baselines across all performance metrics, demonstrating superior robustness and generalizability in detecting a wide range of presentation attacks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86SpectraIrisPAD\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u5149\u8c31\u8679\u819c\u5448\u73b0\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528DINOv2 Vision Transformer\u548c\u521b\u65b0\u7684\u5149\u8c31\u4f4d\u7f6e\u7f16\u7801\u6280\u672f\uff0c\u5728\u65b0\u5efa\u7684MSIrPAD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8679\u819c\u8bc6\u522b\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5176\u9762\u4e34\u5448\u73b0\u653b\u51fb\u7684\u8106\u5f31\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u4f20\u7edf\u8679\u819c\u8bc6\u522b\u4e3b\u8981\u5728\u8fd1\u7ea2\u5916\u5149\u8c31\u4e0b\u5de5\u4f5c\uff0c\u800c\u591a\u5149\u8c31\u6210\u50cf\u80fd\u63d0\u4f9b\u4e92\u8865\u7684\u53cd\u5c04\u4fe1\u606f\uff0c\u589e\u5f3aPAD\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSpectraIrisPAD\u6846\u67b6\uff0c\u91c7\u7528DINOv2 Vision Transformer\u4e3b\u5e72\u7f51\u7edc\uff0c\u914d\u5907\u53ef\u5b66\u4e60\u7684\u5149\u8c31\u4f4d\u7f6e\u7f16\u7801\u3001\u4ee4\u724c\u878d\u5408\u548c\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\uff0c\u63d0\u53d6\u533a\u5206\u6027\u5f3a\u7684\u6ce2\u6bb5\u7279\u5f02\u6027\u7279\u5f81\u6765\u533a\u5206\u771f\u5b9e\u6837\u672c\u548c\u4f2a\u9020\u653b\u51fb\u3002", "result": "\u5728\u65b0\u5efa\u7684MSIrPAD\u6570\u636e\u96c6\uff08\u5305\u542b18,848\u5f20\u8679\u819c\u56fe\u50cf\uff0c\u6db5\u76d68\u79cd\u653b\u51fb\u7c7b\u578b\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0cSpectraIrisPAD\u5728\u6240\u6709\u6027\u80fd\u6307\u6807\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SpectraIrisPAD\u6846\u67b6\u5728\u68c0\u6d4b\u5404\u79cd\u5448\u73b0\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u591a\u5149\u8c31\u6210\u50cf\u548c\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u5728\u8679\u819cPAD\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.06524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06524", "abs": "https://arxiv.org/abs/2512.06524", "authors": ["Saekwang Nam", "Bowen Deng", "Loong Yi Lee", "Jonathan M. Rossiter", "Nathan F. Lepora"], "title": "TacFinRay: Soft Tactile Fin-Ray Finger with Indirect Tactile Sensing for Robust Grasping", "comment": "Accepted in IEEE Robotics Automation Letters. S. Nam, B. Deng co-first authors", "summary": "We present a tactile-sensorized Fin-Ray finger that enables simultaneous detection of contact location and indentation depth through an indirect sensing approach. A hinge mechanism is integrated between the soft Fin-Ray structure and a rigid sensing module, allowing deformation and translation information to be transferred to a bottom crossbeam upon which are an array of marker-tipped pins based on the biomimetic structure of the TacTip vision-based tactile sensor. Deformation patterns captured by an internal camera are processed using a convolutional neural network to infer contact conditions without directly sensing the finger surface. The finger design was optimized by varying pin configurations and hinge orientations, achieving 0.1\\,mm depth and 2mm location-sensing accuracies. The perception demonstrated robust generalization to various indenter shapes and sizes, which was applied to a pick-and-place task under uncertain picking positions, where the tactile feedback significantly improved placement accuracy. Overall, this work provides a lightweight, flexible, and scalable tactile sensing solution suitable for soft robotic structures where the sensing needs situating away from the contact interface.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e6\u89c9\u4f20\u611f\u5668\u5316\u7684Fin-Ray\u624b\u6307\uff0c\u901a\u8fc7\u95f4\u63a5\u4f20\u611f\u65b9\u6cd5\u540c\u65f6\u68c0\u6d4b\u63a5\u89e6\u4f4d\u7f6e\u548c\u538b\u5165\u6df1\u5ea6\uff0c\u91c7\u7528\u94f0\u94fe\u673a\u5236\u548c\u57fa\u4e8eTacTip\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u7684\u6807\u8bb0\u9488\u9635\u5217\uff0c\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u53d8\u5f62\u6a21\u5f0f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u89e6\u89c9\u611f\u77e5\u3002", "motivation": "\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7ed3\u6784\u63d0\u4f9b\u4e00\u79cd\u8f7b\u91cf\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u89e6\u89c9\u4f20\u611f\u89e3\u51b3\u65b9\u6848\uff0c\u4f7f\u4f20\u611f\u6a21\u5757\u65e0\u9700\u76f4\u63a5\u4f4d\u4e8e\u63a5\u89e6\u754c\u9762\uff0c\u540c\u65f6\u5b9e\u73b0\u63a5\u89e6\u4f4d\u7f6e\u548c\u6df1\u5ea6\u7684\u7cbe\u786e\u68c0\u6d4b\u3002", "method": "\u5728\u8f6fFin-Ray\u7ed3\u6784\u548c\u521a\u6027\u4f20\u611f\u6a21\u5757\u4e4b\u95f4\u96c6\u6210\u94f0\u94fe\u673a\u5236\uff0c\u901a\u8fc7\u5e95\u90e8\u6a2a\u6881\u4e0a\u7684\u6807\u8bb0\u9488\u9635\u5217\u4f20\u9012\u53d8\u5f62\u548c\u4f4d\u79fb\u4fe1\u606f\uff0c\u5185\u90e8\u6444\u50cf\u5934\u6355\u83b7\u53d8\u5f62\u6a21\u5f0f\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u63a8\u65ad\u63a5\u89e6\u6761\u4ef6\u3002", "result": "\u4f18\u5316\u624b\u6307\u8bbe\u8ba1\u540e\uff0c\u8fbe\u52300.1mm\u6df1\u5ea6\u548c2mm\u4f4d\u7f6e\u4f20\u611f\u7cbe\u5ea6\uff0c\u611f\u77e5\u5bf9\u4e0d\u540c\u5f62\u72b6\u548c\u5c3a\u5bf8\u7684\u538b\u5934\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u786e\u5b9a\u6293\u53d6\u4f4d\u7f6e\u7684\u62fe\u53d6\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u653e\u7f6e\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7ed3\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e6\u89c9\u4f20\u611f\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5c06\u4f20\u611f\u6a21\u5757\u8fdc\u79bb\u63a5\u89e6\u754c\u9762\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.06243", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06243", "abs": "https://arxiv.org/abs/2512.06243", "authors": ["Rohan Pandey", "Eric Ye"], "title": "Quantization Blindspots: How Model Compression Breaks Backdoor Defenses", "comment": "10 pages", "summary": "Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u540e\u95e8\u9632\u5fa1\u5728\u6a21\u578b\u91cf\u5316\u540e\u7684\u8868\u73b0\uff0c\u53d1\u73b0INT8\u91cf\u5316\u4f1a\u4f7f\u6240\u6709\u9632\u5fa1\u68c0\u6d4b\u7387\u964d\u81f30%\uff0c\u800cINT4\u91cf\u5316\u7684\u6548\u679c\u56e0\u6570\u636e\u96c6\u800c\u5f02\uff0c\u66b4\u9732\u4e86\u5f53\u524d\u9632\u5fa1\u8bc4\u4f30\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u6a21\u578b\u90e8\u7f72\u901a\u5e38\u91c7\u7528\u91cf\u5316\u6280\u672f\u6765\u51cf\u5c11\u5185\u5b58\u548c\u5ef6\u8fdf\uff0c\u4f46\u73b0\u6709\u540e\u95e8\u9632\u5fa1\u7814\u7a76\u4e3b\u8981\u5728FP32\u7cbe\u5ea6\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf9\u91cf\u5316\u540e\u9632\u5fa1\u6548\u679c\u7684\u4e86\u89e3\u3002", "method": "\u5728\u4e09\u4e2a\u7cbe\u5ea6\u8bbe\u7f6e\uff08FP32\u3001INT8\u52a8\u6001\u3001INT4\u6a21\u62df\uff09\u548c\u4e24\u4e2a\u6807\u51c6\u89c6\u89c9\u57fa\u51c6\u4e0a\uff0c\u5bf9\u4e94\u79cd\u4ee3\u8868\u6027\u540e\u95e8\u9632\u5fa1\u8fdb\u884c\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\uff0c\u4f7f\u7528\u7ecf\u5178\u7684BadNet\u653b\u51fb\u3002", "result": "INT8\u91cf\u5316\u4f7f\u6240\u6709\u9632\u5fa1\u68c0\u6d4b\u7387\u964d\u81f30%\uff0c\u800c\u653b\u51fb\u6210\u529f\u7387\u4ecd\u9ad8\u4e8e99%\uff1bINT4\u91cf\u5316\u6548\u679c\u56e0\u6570\u636e\u96c6\u800c\u5f02\uff08\u5728GTSRB\u4e0a\u6709\u6548\uff0c\u5728CIFAR-10\u4e0a\u5931\u6548\uff09\uff0c\u4f46\u540e\u95e8\u653b\u51fb\u6210\u529f\u7387\u4ecd\u9ad8\u4e8e90%\u3002", "conclusion": "\u91cf\u5316\u9c81\u68d2\u6027\u5e94\u6210\u4e3a\u672a\u6765\u540e\u95e8\u9632\u5fa1\u8bc4\u4f30\u548c\u8bbe\u8ba1\u7684\u5fc5\u8981\u7ef4\u5ea6\uff0c\u9700\u8981\u89e3\u51b3\u9632\u5fa1\u8bc4\u4f30\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2512.06105", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06105", "abs": "https://arxiv.org/abs/2512.06105", "authors": ["Junwen Zheng", "Xinran Xu", "Li Rong Wang", "Chang Cai", "Lucinda Siyun Tan", "Dingyuan Wang", "Hong Liang Tey", "Xiuyi Fan"], "title": "Explainable Melanoma Diagnosis with Contrastive Learning and LLM-based Report Generation", "comment": "AAAI-26-AIA", "summary": "Deep learning has demonstrated expert-level performance in melanoma classification, positioning it as a powerful tool in clinical dermatology. However, model opacity and the lack of interpretability remain critical barriers to clinical adoption, as clinicians often struggle to trust the decision-making processes of black-box models. To address this gap, we present a Cross-modal Explainable Framework for Melanoma (CEFM) that leverages contrastive learning as the core mechanism for achieving interpretability. Specifically, CEFM maps clinical criteria for melanoma diagnosis-namely Asymmetry, Border, and Color (ABC)-into the Vision Transformer embedding space using dual projection heads, thereby aligning clinical semantics with visual features. The aligned representations are subsequently translated into structured textual explanations via natural language generation, creating a transparent link between raw image data and clinical interpretation. Experiments on public datasets demonstrate 92.79% accuracy and an AUC of 0.961, along with significant improvements across multiple interpretability metrics. Qualitative analyses further show that the spatial arrangement of the learned embeddings aligns with clinicians' application of the ABC rule, effectively bridging the gap between high-performance classification and clinical trust.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CEFM\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5c06\u4e34\u5e8a\u8bca\u65ad\u6807\u51c6\uff08ABC\u89c4\u5219\uff09\u6620\u5c04\u5230\u89c6\u89c9Transformer\u5d4c\u5165\u7a7a\u95f4\uff0c\u751f\u6210\u7ed3\u6784\u5316\u6587\u672c\u89e3\u91ca\uff0c\u89e3\u51b3\u9ed1\u8272\u7d20\u7624\u5206\u7c7b\u4e2d\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u9ed1\u8272\u7d20\u7624\u5206\u7c7b\u4e2d\u5df2\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\uff0c\u4f46\u6a21\u578b\u4e0d\u900f\u660e\u548c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u91c7\u7528\uff0c\u533b\u751f\u96be\u4ee5\u4fe1\u4efb\u9ed1\u76d2\u6a21\u578b\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u4f5c\u4e3a\u6838\u5fc3\u673a\u5236\uff0c\u901a\u8fc7\u53cc\u6295\u5f71\u5934\u5c06\u4e34\u5e8a\u8bca\u65ad\u6807\u51c6\uff08\u4e0d\u5bf9\u79f0\u6027\u3001\u8fb9\u754c\u3001\u989c\u8272\uff09\u6620\u5c04\u5230ViT\u5d4c\u5165\u7a7a\u95f4\uff0c\u5c06\u4e34\u5e8a\u8bed\u4e49\u4e0e\u89c6\u89c9\u7279\u5f81\u5bf9\u9f50\uff0c\u7136\u540e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u89e3\u91ca\u3002", "result": "\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.79%\u7684\u51c6\u786e\u7387\u548c0.961\u7684AUC\uff0c\u5728\u591a\u4e2a\u53ef\u89e3\u91ca\u6027\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5b66\u4e60\u5230\u7684\u5d4c\u5165\u7a7a\u95f4\u5e03\u5c40\u4e0e\u533b\u751f\u5e94\u7528\u7684ABC\u89c4\u5219\u4e00\u81f4\u3002", "conclusion": "CEFM\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86\u9ad8\u6027\u80fd\u5206\u7c7b\u4e0e\u4e34\u5e8a\u4fe1\u4efb\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u4e34\u5e8a\u76ae\u80a4\u75c5\u5b66\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u9760\u7684AI\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2512.06558", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06558", "abs": "https://arxiv.org/abs/2512.06558", "authors": ["Md Mofijul Islam", "Alexi Gladstone", "Sujan Sarker", "Ganesh Nanduru", "Md Fahim", "Keyan Du", "Aman Chadha", "Tariq Iqbal"], "title": "Embodied Referring Expression Comprehension in Human-Robot Interaction", "comment": "14 pages, 7 figures, accepted at the ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026", "summary": "As robots enter human workspaces, there is a crucial need for them to comprehend embodied human instructions, enabling intuitive and fluent human-robot interaction (HRI). However, accurate comprehension is challenging due to a lack of large-scale datasets that capture natural embodied interactions in diverse HRI settings. Existing datasets suffer from perspective bias, single-view collection, inadequate coverage of nonverbal gestures, and a predominant focus on indoor environments. To address these issues, we present the Refer360 dataset, a large-scale dataset of embodied verbal and nonverbal interactions collected across diverse viewpoints in both indoor and outdoor settings. Additionally, we introduce MuRes, a multimodal guided residual module designed to improve embodied referring expression comprehension. MuRes acts as an information bottleneck, extracting salient modality-specific signals and reinforcing them into pre-trained representations to form complementary features for downstream tasks. We conduct extensive experiments on four HRI datasets, including the Refer360 dataset, and demonstrate that current multimodal models fail to capture embodied interactions comprehensively; however, augmenting them with MuRes consistently improves performance. These findings establish Refer360 as a valuable benchmark and exhibit the potential of guided residual learning to advance embodied referring expression comprehension in robots operating within human environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Refer360\u6570\u636e\u96c6\u548cMuRes\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u7406\u89e3\u5177\u8eab\u4eba\u7c7b\u6307\u4ee4\u7684\u6311\u6218\u3002Refer360\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u89c6\u89d2\u3001\u6db5\u76d6\u5ba4\u5185\u5916\u73af\u5883\u7684\u5177\u8eab\u4ea4\u4e92\u6570\u636e\u96c6\uff0cMuRes\u5219\u901a\u8fc7\u5f15\u5bfc\u6b8b\u5dee\u5b66\u4e60\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u5177\u8eab\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5b58\u5728\u89c6\u89d2\u504f\u5dee\u3001\u5355\u89c6\u56fe\u6536\u96c6\u3001\u975e\u8bed\u8a00\u624b\u52bf\u8986\u76d6\u4e0d\u8db3\u4ee5\u53ca\u4e3b\u8981\u5173\u6ce8\u5ba4\u5185\u73af\u5883\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u771f\u5b9e\u4eba\u7c7b\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7406\u89e3\u5177\u8eab\u6307\u4ee4\u7684\u80fd\u529b\u3002", "method": "1\uff09\u6784\u5efaRefer360\u6570\u636e\u96c6\uff1a\u5927\u89c4\u6a21\u5177\u8eab\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u4ea4\u4e92\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5ba4\u5185\u5916\u73af\u5883\u548c\u591a\u89c6\u89d2\uff1b2\uff09\u63d0\u51faMuRes\u6a21\u5757\uff1a\u591a\u6a21\u6001\u5f15\u5bfc\u6b8b\u5dee\u6a21\u5757\uff0c\u4f5c\u4e3a\u4fe1\u606f\u74f6\u9888\u63d0\u53d6\u6a21\u6001\u7279\u5b9a\u4fe1\u53f7\u5e76\u589e\u5f3a\u9884\u8bad\u7ec3\u8868\u793a\u3002", "result": "\u5728\u56db\u4e2aHRI\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u96be\u4ee5\u5168\u9762\u6355\u6349\u5177\u8eab\u4ea4\u4e92\uff0c\u4f46\u4f7f\u7528MuRes\u589e\u5f3a\u540e\u6027\u80fd\u5f97\u5230\u4e00\u81f4\u63d0\u5347\u3002", "conclusion": "Refer360\u4e3a\u5177\u8eab\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\uff0cMuRes\u5c55\u793a\u4e86\u5f15\u5bfc\u6b8b\u5dee\u5b66\u4e60\u5728\u63d0\u5347\u673a\u5668\u4eba\u5177\u8eab\u4ea4\u4e92\u7406\u89e3\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.06244", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.06244", "abs": "https://arxiv.org/abs/2512.06244", "authors": ["Caleb Ju", "Guanghui Lan"], "title": "Auto-exploration for online reinforcement learning", "comment": "35 pages (9 appendix), 1 figure. Comments are welcome", "summary": "The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(\u03b5^{-2})$ sample complexity to solve to $\u03b5$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u81ea\u7531\u7684\u65b9\u5f0f\u89e3\u51b3\u63a2\u7d22-\u5229\u7528\u56f0\u5883\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u95ee\u9898\u76f8\u5173\u53c2\u6570\u7684\u4f9d\u8d56\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6837\u672c\u590d\u6742\u5ea6\u548c\u53ef\u5b9e\u65bd\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u9700\u8981\u5047\u8bbe\u5145\u5206\u63a2\u7d22\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8fd9\u5bfc\u81f4\u7b97\u6cd5\u4e0d\u53ef\u5b9e\u65bd\u4e14\u6027\u80fd\u6b21\u4f18\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u63a2\u7d22\u7684\u53c2\u6570\u81ea\u7531\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u53d8\u4f53\uff1a\u8868\u683c\u8bbe\u7f6e\u548c\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u3002\u91c7\u7528\u52a8\u6001\u6df7\u5408\u65f6\u95f4\u3001\u6298\u6263\u72b6\u6001\u5206\u5e03\u91c7\u6837\u3001\u9c81\u68d2\u68af\u5ea6\u4f30\u8ba1\u5668\u548c\u4f18\u52bf\u5dee\u8ddd\u51fd\u6570\u7b49\u65b0\u7b97\u6cd5\u521b\u65b0\u3002", "result": "\u5728\u5b58\u5728\u63a2\u7d22\u6700\u4f18\u7b56\u7565\u7684\u5047\u8bbe\u4e0b\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u8fbe\u5230\u4e86O(\u03b5\u207b\u00b2)\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4e14\u590d\u6742\u5ea6\u4e0d\u5305\u542b\u7b97\u6cd5\u76f8\u5173\u53c2\u6570\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5b9e\u73b0\u4e86\u53c2\u6570\u81ea\u7531\u3001\u6613\u4e8e\u5b9e\u65bd\u7684\u81ea\u52a8\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRL\u7b97\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.06158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06158", "abs": "https://arxiv.org/abs/2512.06158", "authors": ["Su Sun", "Cheng Zhao", "Himangi Mittal", "Gaurav Mittal", "Rohith Kukkala", "Yingjie Victor Chen", "Mei Chen"], "title": "Tracking-Guided 4D Generation: Foundation-Tracker Motion Priors for 3D Model Animation", "comment": "15 pages, 11 figures", "summary": "Generating dynamic 4D objects from sparse inputs is difficult because it demands joint preservation of appearance and motion coherence across views and time while suppressing artifacts and temporal drift. We hypothesize that the view discrepancy arises from supervision limited to pixel- or latent-space video-diffusion losses, which lack explicitly temporally aware, feature-level tracking guidance. We present \\emph{Track4DGen}, a two-stage framework that couples a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor. The central idea is to explicitly inject tracker-derived motion priors into intermediate feature representations for both multi-view video generation and 4D-GS. In Stage One, we enforce dense, feature-level point correspondences inside the diffusion generator, producing temporally consistent features that curb appearance drift and enhance cross-view coherence. In Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion encoding that concatenates co-located diffusion features (carrying Stage-One tracking priors) with Hex-plane features, and augment them with 4D Spherical Harmonics for higher-fidelity dynamics modeling. \\emph{Track4DGen} surpasses baselines on both multi-view video generation and 4D generation benchmarks, yielding temporally stable, text-editable 4D assets. Lastly, we curate \\emph{Sketchfab28}, a high-quality dataset for benchmarking object-centric 4D generation and fostering future research.", "AI": {"tldr": "Track4DGen\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u89c6\u89d2\u89c6\u9891\u6269\u6563\u6a21\u578b\u3001\u57fa\u7840\u70b9\u8ddf\u8e2a\u5668\u548c\u6df7\u54084D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u5668\uff0c\u89e3\u51b3\u4e86\u4ece\u7a00\u758f\u8f93\u5165\u751f\u6210\u52a8\u60014D\u5bf9\u8c61\u7684\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u6ce8\u5165\u8ddf\u8e2a\u5668\u5bfc\u51fa\u7684\u8fd0\u52a8\u5148\u9a8c\uff0c\u5728\u6269\u6563\u751f\u6210\u548c4D\u91cd\u5efa\u4e2d\u589e\u5f3a\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u8de8\u89c6\u89d2\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u52a8\u60014D\u5bf9\u8c61\u65f6\u5b58\u5728\u5916\u89c2\u6f02\u79fb\u548c\u8fd0\u52a8\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u4ec5\u4f9d\u8d56\u4e8e\u50cf\u7d20\u6216\u6f5c\u5728\u7a7a\u95f4\u7684\u89c6\u9891\u6269\u6563\u635f\u5931\u76d1\u7763\uff0c\u7f3a\u4e4f\u663e\u5f0f\u7684\u65f6\u95f4\u611f\u77e5\u7279\u5f81\u7ea7\u8ddf\u8e2a\u5f15\u5bfc\u3002", "method": "Track4DGen\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5728\u591a\u89c6\u89d2\u89c6\u9891\u6269\u6563\u751f\u6210\u5668\u4e2d\u5f3a\u5236\u5bc6\u96c6\u7279\u5f81\u7ea7\u70b9\u5bf9\u5e94\u5173\u7cfb\uff0c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u7684\u7279\u5f81\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6df7\u5408\u8fd0\u52a8\u7f16\u7801\u91cd\u5efa4D\u9ad8\u65af\u6cfc\u6e85\uff0c\u7ed3\u5408\u6269\u6563\u7279\u5f81\u548cHex-plane\u7279\u5f81\uff0c\u5e76\u589e\u5f3a4D\u7403\u8c10\u51fd\u6570\u4ee5\u8fdb\u884c\u9ad8\u4fdd\u771f\u52a8\u6001\u5efa\u6a21\u3002", "result": "Track4DGen\u5728\u591a\u89c6\u89d2\u89c6\u9891\u751f\u6210\u548c4D\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751f\u6210\u7684\u65f6\u95f4\u7a33\u5b9a\u3001\u53ef\u6587\u672c\u7f16\u8f91\u76844D\u8d44\u6e90\u8d28\u91cf\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u76844D\u5bf9\u8c61\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8fd0\u52a8\u5148\u9a8c\u6ce8\u5165\u89e3\u51b3\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u8d21\u732e\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6Sketchfab28\u4ee5\u63a8\u52a8\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2512.06571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06571", "abs": "https://arxiv.org/abs/2512.06571", "authors": ["Zifan Xu", "Myoungkyu Seo", "Dongmyeong Lee", "Hao Fu", "Jiaheng Hu", "Jiaxun Cui", "Yuqian Jiang", "Zhihan Wang", "Anastasiia Brund", "Joydeep Biswas", "Peter Stone"], "title": "Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input", "comment": null, "summary": "Learning fast and robust ball-kicking skills is a critical capability for humanoid soccer robots, yet it remains a challenging problem due to the need for rapid leg swings, postural stability on a single support foot, and robustness under noisy sensory input and external perturbations (e.g., opponents). This paper presents a reinforcement learning (RL)-based system that enables humanoid robots to execute robust continual ball-kicking with adaptability to different ball-goal configurations. The system extends a typical teacher-student training framework -- in which a \"teacher\" policy is trained with ground truth state information and the \"student\" learns to mimic it with noisy, imperfect sensing -- by including four training stages: (1) long-distance ball chasing (teacher); (2) directional kicking (teacher); (3) teacher policy distillation (student); and (4) student adaptation and refinement (student). Key design elements -- including tailored reward functions, realistic noise modeling, and online constrained RL for adaptation and refinement -- are critical for closing the sim-to-real gap and sustaining performance under perceptual uncertainty. Extensive evaluations in both simulation and on a real robot demonstrate strong kicking accuracy and goal-scoring success across diverse ball-goal configurations. Ablation studies further highlight the necessity of the constrained RL, noise modeling, and the adaptation stage. This work presents a system for learning robust continual humanoid ball-kicking under imperfect perception, establishing a benchmark task for visuomotor skill learning in humanoid whole-body control.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u4f7f\u4eff\u4eba\u673a\u5668\u4eba\u80fd\u591f\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u7a33\u5065\u7684\u8fde\u7eed\u8e22\u7403\u52a8\u4f5c\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u56db\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff08\u5305\u62ec\u6559\u5e08\u7b56\u7565\u8bad\u7ec3\u548c\u5b66\u751f\u7b56\u7565\u84b8\u998f\uff09\u7ed3\u5408\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u548c\u566a\u58f0\u5efa\u6a21\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "motivation": "\u4eff\u4eba\u673a\u5668\u4eba\u8e22\u7403\u9700\u8981\u5feb\u901f\u817f\u90e8\u6446\u52a8\u3001\u5355\u811a\u652f\u6491\u7a33\u5b9a\u6027\u4ee5\u53ca\u5728\u566a\u58f0\u611f\u77e5\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u6559\u5e08-\u5b66\u751f\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u957f\u8ddd\u79bb\u8ffd\u7403\uff08\u6559\u5e08\uff09\uff1b2\uff09\u5b9a\u5411\u8e22\u7403\uff08\u6559\u5e08\uff09\uff1b3\uff09\u6559\u5e08\u7b56\u7565\u84b8\u998f\uff08\u5b66\u751f\uff09\uff1b4\uff09\u5b66\u751f\u9002\u5e94\u4e0e\u7cbe\u70bc\u3002\u5173\u952e\u8bbe\u8ba1\u5305\u62ec\u5b9a\u5236\u5956\u52b1\u51fd\u6570\u3001\u771f\u5b9e\u566a\u58f0\u5efa\u6a21\u548c\u5728\u7ebf\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7cfb\u7edf\u5728\u4e0d\u540c\u7403\u95e8\u914d\u7f6e\u4e0b\u5177\u6709\u5f3a\u5065\u7684\u8e22\u7403\u51c6\u786e\u6027\u548c\u8fdb\u7403\u6210\u529f\u7387\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u3001\u566a\u58f0\u5efa\u6a21\u548c\u9002\u5e94\u9636\u6bb5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u611f\u77e5\u4e0d\u5b8c\u5584\u6761\u4ef6\u4e0b\u5b66\u4e60\u7a33\u5065\u7684\u8fde\u7eed\u4eff\u4eba\u8e22\u7403\u6280\u80fd\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u65b9\u6848\uff0c\u4e3a\u4eff\u4eba\u5168\u8eab\u63a7\u5236\u7684\u89c6\u89c9\u8fd0\u52a8\u6280\u80fd\u5b66\u4e60\u5efa\u7acb\u4e86\u57fa\u51c6\u4efb\u52a1\u3002"}}
{"id": "2512.06250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06250", "abs": "https://arxiv.org/abs/2512.06250", "authors": ["Chris Tava"], "title": "Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning", "comment": "7 pages", "summary": "Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\\times$16 to 128$\\times$128 $\\times$ 10 unique mazes $\\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\\% threshold baselines. Results show 23-55\\% improvements in completion time, 83\\% reduction in runtime variance, and 71\\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\\% improvement for 16$\\times$16 mazes, 34\\% for 32$\\times$32, and 55\\% for 64$\\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u7b56\u7565\u5207\u6362\u65b9\u6cd5\uff0c\u8ba9\u667a\u80fd\u4f53\u5728\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u4e2d\u52a8\u6001\u5207\u6362\u7cfb\u7edf\u63a2\u7d22\u548c\u76ee\u6807\u5bfc\u5411\u4e24\u79cd\u7b56\u7565\uff0c\u76f8\u6bd4\u56fa\u5b9a\u9608\u503c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u81ea\u4e3b\u667a\u80fd\u4f53\u5728\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u65f6\u9700\u8981\u591a\u79cd\u7b56\u7565\uff0c\u4f46\u786e\u5b9a\u4f55\u65f6\u5207\u6362\u7b56\u7565\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u56fa\u5b9a\u9608\u503c\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u73af\u5883\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5b66\u4e60\u5207\u6362\u65f6\u673a\u7684\u6280\u672f\u3002", "method": "\u4f7f\u7528Q-learning\u7b97\u6cd5\u5b66\u4e60\u7b56\u7565\u5207\u6362\u9608\u503c\uff0c\u5c06\u72b6\u6001\u7a7a\u95f4\u79bb\u6563\u5316\u4e3a\u8986\u76d6\u7387\u548c\u8ddd\u79bb\u76ee\u6807\u8ddd\u79bb\u7684\u6876\u3002\u667a\u80fd\u4f53\u4e0d\u9700\u8981\u9884\u5148\u77e5\u9053\u5899\u58c1\u4f4d\u7f6e\u6216\u6700\u4f18\u9608\u503c\uff0c\u4ec5\u9700\u8ff7\u5bab\u5c3a\u5bf8\u548c\u76ee\u6807\u4f4d\u7f6e\u7b49\u6700\u5c0f\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u5728240\u4e2a\u6d4b\u8bd5\u914d\u7f6e\u4e2d\uff0c\u81ea\u9002\u5e94\u9608\u503c\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\u5355\u7b56\u7565\u667a\u80fd\u4f53\u548c\u56fa\u5b9a40%\u9608\u503c\u57fa\u7ebf\uff0c\u5b8c\u6210\u65f6\u95f4\u63d0\u534723-55%\uff0c\u8fd0\u884c\u65f6\u65b9\u5dee\u964d\u4f4e83%\uff0c\u6700\u574f\u60c5\u51b5\u6027\u80fd\u63d0\u534771%\u3002\u6027\u80fd\u589e\u76ca\u968f\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "\u81ea\u9002\u5e94\u7b56\u7565\u5207\u6362\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u968f\u7740\u95ee\u9898\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\u7684\u4ef7\u503c\u66f4\u52a0\u663e\u8457\u3002"}}
{"id": "2512.06171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06171", "abs": "https://arxiv.org/abs/2512.06171", "authors": ["Jessica Plassmann", "Nicolas Schuler", "Michael Schuth", "Georg von Freymann"], "title": "Automated Annotation of Shearographic Measurements Enabling Weakly Supervised Defect Detection", "comment": "11 pages, 4 figures", "summary": "Shearography is an interferometric technique sensitive to surface displacement gradients, providing high sensitivity for detecting subsurface defects in safety-critical components. A key limitation to industrial adoption is the lack of high-quality annotated datasets, since manual labeling remains labor-intensive, subjective, and difficult to standardize. We introduce an automated workflow that generates defect annotations from shearography measurements using deep learning, producing high-resolution segmentation and bounding-box labels. Evaluation against expert-labeled data demonstrates sufficient accuracy to enable weakly supervised training, reducing manual effort and supporting scalable dataset creation for robust defect detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u526a\u5207\u6563\u6591\u5e72\u6d89\u6d4b\u91cf\u6570\u636e\u4e2d\u751f\u6210\u7f3a\u9677\u6807\u6ce8\uff0c\u89e3\u51b3\u4e86\u5de5\u4e1a\u5e94\u7528\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "motivation": "\u526a\u5207\u6563\u6591\u5e72\u6d89\u6d4b\u91cf\u6280\u672f\u867d\u7136\u5bf9\u8868\u9762\u4f4d\u79fb\u68af\u5ea6\u654f\u611f\uff0c\u80fd\u591f\u9ad8\u7075\u654f\u5ea6\u68c0\u6d4b\u5b89\u5168\u5173\u952e\u90e8\u4ef6\u7684\u4e9a\u8868\u9762\u7f3a\u9677\uff0c\u4f46\u5de5\u4e1a\u5e94\u7528\u9762\u4e34\u7684\u4e3b\u8981\u9650\u5236\u662f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u56e0\u4e3a\u624b\u52a8\u6807\u6ce8\u52b3\u52a8\u5bc6\u96c6\u3001\u4e3b\u89c2\u6027\u5f3a\u4e14\u96be\u4ee5\u6807\u51c6\u5316\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u4ece\u526a\u5207\u6563\u6591\u6d4b\u91cf\u6570\u636e\u751f\u6210\u7f3a\u9677\u6807\u6ce8\uff0c\u4ea7\u751f\u9ad8\u5206\u8fa8\u7387\u7684\u5206\u5272\u548c\u8fb9\u754c\u6846\u6807\u7b7e\u3002", "result": "\u4e0e\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u793a\u51fa\u8db3\u591f\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u652f\u6301\u5f31\u76d1\u7763\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u624b\u52a8\u5de5\u4f5c\u91cf\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u6570\u636e\u96c6\u521b\u5efa\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u7f3a\u9677\u68c0\u6d4b\u3002"}}
{"id": "2512.06578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06578", "abs": "https://arxiv.org/abs/2512.06578", "authors": ["Waleed Razzaq"], "title": "Error-Centric PID Untrained Neural-Net (EC-PIDUNN) For Nonlinear Robotics Control", "comment": "Under review at SoftComputing", "summary": "Classical Proportional-Integral-Derivative (PID) control has been widely successful across various industrial systems such as chemical processes, robotics, and power systems. However, as these systems evolved, the increase in the nonlinear dynamics and the complexity of interconnected variables have posed challenges that classical PID cannot effectively handle, often leading to instability, overshooting, or prolonged settling times. Researchers have proposed PIDNN models that combine the function approximation capabilities of neural networks with PID control to tackle these nonlinear challenges. However, these models require extensive, highly refined training data and have significant computational costs, making them less favorable for real-world applications. In this paper, We propose a novel EC-PIDUNN architecture, which integrates an untrained neural network with an improved PID controller, incorporating a stabilizing factor (\\(\u03c4\\)) to generate the control signal. Like classical PID, our architecture uses the steady-state error \\(e_t\\) as input bypassing the need for explicit knowledge of the systems dynamics. By forming an input vector from \\(e_t\\) within the neural network, we increase the dimensionality of input allowing for richer data representation. Additionally, we introduce a vector of parameters \\( \u03c1_t \\) to shape the output trajectory and a \\textit{dynamic compute} function to adjust the PID coefficients from predefined values. We validate the effectiveness of EC-PIDUNN on multiple nonlinear robotics applications: (1) nonlinear unmanned ground vehicle systems that represent the Ackermann steering mechanism and kinematics control, (2) Pan-Tilt movement system. In both tests, it outperforms classical PID in convergence and stability achieving a nearly critically damped response.", "AI": {"tldr": "\u63d0\u51faEC-PIDUNN\u67b6\u6784\uff0c\u5c06\u672a\u7ecf\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u6539\u8fdb\u7684PID\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u901a\u8fc7\u7a33\u5b9a\u56e0\u5b50\u03c4\u751f\u6210\u63a7\u5236\u4fe1\u53f7\uff0c\u5728\u975e\u7ebf\u6027\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u4f18\u4e8e\u7ecf\u5178PID\u63a7\u5236\u3002", "motivation": "\u7ecf\u5178PID\u63a7\u5236\u96be\u4ee5\u5904\u7406\u975e\u7ebf\u6027\u52a8\u6001\u548c\u590d\u6742\u4e92\u8054\u53d8\u91cf\u7684\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684PIDNN\u6a21\u578b\u9700\u8981\u5927\u91cf\u7cbe\u70bc\u8bad\u7ec3\u6570\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002", "method": "EC-PIDUNN\u67b6\u6784\u4f7f\u7528\u7a33\u6001\u8bef\u5deee_t\u4f5c\u4e3a\u8f93\u5165\uff0c\u65e0\u9700\u7cfb\u7edf\u52a8\u6001\u77e5\u8bc6\uff1b\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u589e\u52a0\u8f93\u5165\u7ef4\u5ea6\uff0c\u5f15\u5165\u53c2\u6570\u5411\u91cf\u03c1_t\u548c\u52a8\u6001\u8ba1\u7b97\u51fd\u6570\u8c03\u6574PID\u7cfb\u6570\u3002", "result": "\u5728\u975e\u7ebf\u6027\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7cfb\u7edf\uff08\u963f\u514b\u66fc\u8f6c\u5411\u673a\u5236\u548c\u8fd0\u52a8\u5b66\u63a7\u5236\uff09\u548c\u4e91\u53f0\u8fd0\u52a8\u7cfb\u7edf\u7684\u6d4b\u8bd5\u4e2d\uff0cEC-PIDUNN\u5728\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178PID\uff0c\u5b9e\u73b0\u63a5\u8fd1\u4e34\u754c\u963b\u5c3c\u54cd\u5e94\u3002", "conclusion": "EC-PIDUNN\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u975e\u7ebf\u6027\u7cfb\u7edf\u7684\u63a7\u5236\u6311\u6218\uff0c\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.06252", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06252", "abs": "https://arxiv.org/abs/2512.06252", "authors": ["Homayoon Farrahi", "A. Rupam Mahmood"], "title": "Learning Without Time-Based Embodiment Resets in Soft-Actor Critic", "comment": "In Proceedings of the 4th Conference on Lifelong Learning Agents (CoLLAs)", "summary": "When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $\u03b3$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4e0d\u4f7f\u7528episode\u7ec8\u6b62\u548c\u673a\u5668\u4eba\u73af\u5883\u91cd\u7f6e\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86continuing SAC\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u901a\u8fc7\u8c03\u6574\u5956\u52b1\u51fd\u6570\u548c\u589e\u52a0\u7b56\u7565\u71b5\u53ef\u4ee5\u6062\u590d\u56e0\u4e0d\u4f7f\u7528\u73af\u5883\u91cd\u7f6e\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u635f\u5931\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\u5e38\u7528\u7684episode\u7ec8\u6b62\u548c\u73af\u5883\u91cd\u7f6e\u7b49\u8f85\u52a9\u7ec4\u4ef6\u867d\u7136\u80fd\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4f46\u4f1a\u5bfc\u81f4\u4e0d\u81ea\u7136\u7684\u4efb\u52a1\u8bbe\u7f6e\u5e76\u963b\u788d\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u957f\u671f\u6027\u80fd\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5728\u6ca1\u6709\u8fd9\u4e9b\u8f85\u52a9\u7ec4\u4ef6\u60c5\u51b5\u4e0b\u7684\u5b66\u4e60\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86continuing\u7248\u672c\u7684Soft Actor-Critic (SAC)\u7b97\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u4fee\u6539\u73b0\u6709\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5728\u6027\u80fd\u4e0b\u964d\u6216\u505c\u6ede\u65f6\u589e\u52a0\u7b56\u7565\u7684\u71b5\u6765\u6539\u5584\u5b66\u4e60\u6548\u679c\u3002", "result": "continuing SAC\u5728\u4fee\u6539\u540e\u7684Gym Reacher\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0eepisodic SAC\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u5bf9\u6298\u6263\u7387\u03b3\u503c\u7684\u654f\u611f\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u73af\u5883\u91cd\u7f6e\u6709\u52a9\u4e8e\u72b6\u6001\u7a7a\u95f4\u63a2\u7d22\uff0c\u79fb\u9664\u91cd\u7f6e\u4f1a\u5bfc\u81f4\u5b66\u4e60\u5931\u8d25\u6216\u663e\u8457\u53d8\u6162\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u8c03\u6574\u5956\u52b1\u51fd\u6570\u548c\u589e\u52a0\u7b56\u7565\u71b5\uff0c\u53ef\u4ee5\u6709\u6548\u8865\u507f\u56e0\u4e0d\u4f7f\u7528\u73af\u5883\u91cd\u7f6e\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u635f\u5931\uff0c\u4e3a\u5728\u66f4\u81ea\u7136\u73af\u5883\u4e2d\u8fdb\u884c\u957f\u671f\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.06174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06174", "abs": "https://arxiv.org/abs/2512.06174", "authors": ["Shilin Hu", "Jingyi Xu", "Akshat Dave", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Shadow Generation from Monocular 3D Geometry Priors and Approximate Light Direction", "comment": null, "summary": "Shadow generation aims to produce photorealistic shadows that are visually consistent with object geometry and scene illumination. In the physics of shadow formation, the occluder blocks some light rays casting from the light source that would otherwise arrive at the surface, creating a shadow that follows the silhouette of the occluder. However, such explicit physical modeling has rarely been used in deep-learning-based shadow generation. In this paper, we propose a novel framework that embeds explicit physical modeling - geometry and illumination - into deep-learning-based shadow generation. First, given a monocular RGB image, we obtain approximate 3D geometry in the form of dense point maps and predict a single dominant light direction. These signals allow us to recover fairly accurate shadow location and shape based on the physics of shadow formation. We then integrate this physics-based initial estimate into a diffusion framework that refines the shadow into a realistic, high-fidelity appearance while ensuring consistency with scene geometry and illumination. Trained on DESOBAV2, our model produces shadows that are both visually realistic and physically coherent, outperforming existing approaches, especially in scenes with complex geometry or ambiguous lighting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u663e\u5f0f\u7269\u7406\u5efa\u6a21\uff08\u51e0\u4f55\u548c\u5149\u7167\uff09\u5d4c\u5165\u5230\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9634\u5f71\u751f\u6210\u4e2d\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u9634\u5f71\u5f62\u6210\u539f\u7406\u548c\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u65e2\u771f\u5b9e\u53c8\u7269\u7406\u4e00\u81f4\u7684\u9634\u5f71\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9634\u5f71\u751f\u6210\u65b9\u6cd5\u5f88\u5c11\u4f7f\u7528\u663e\u5f0f\u7684\u7269\u7406\u5efa\u6a21\uff0c\u800c\u9634\u5f71\u5f62\u6210\u7684\u7269\u7406\u539f\u7406\u8868\u660e\u9634\u5f71\u5e94\u9075\u5faa\u906e\u6321\u7269\u7684\u8f6e\u5ed3\u3002\u672c\u6587\u65e8\u5728\u5c06\u7269\u7406\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u9634\u5f71\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\u3002", "method": "\u9996\u5148\u4ece\u5355\u76eeRGB\u56fe\u50cf\u4e2d\u83b7\u53d6\u5bc6\u96c6\u70b9\u4e91\u5f62\u5f0f\u7684\u8fd1\u4f3c3D\u51e0\u4f55\u4fe1\u606f\uff0c\u5e76\u9884\u6d4b\u4e3b\u5bfc\u5149\u7167\u65b9\u5411\uff0c\u57fa\u4e8e\u9634\u5f71\u5f62\u6210\u7684\u7269\u7406\u539f\u7406\u6062\u590d\u9634\u5f71\u7684\u4f4d\u7f6e\u548c\u5f62\u72b6\u3002\u7136\u540e\u5c06\u8fd9\u4e00\u57fa\u4e8e\u7269\u7406\u7684\u521d\u59cb\u4f30\u8ba1\u6574\u5408\u5230\u6269\u6563\u6846\u67b6\u4e2d\uff0c\u7ec6\u5316\u9634\u5f71\u7684\u5916\u89c2\uff0c\u786e\u4fdd\u4e0e\u573a\u666f\u51e0\u4f55\u548c\u5149\u7167\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728DESOBAV2\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u540e\uff0c\u8be5\u6a21\u578b\u751f\u6210\u7684\u9634\u5f71\u65e2\u89c6\u89c9\u771f\u5b9e\u53c8\u7269\u7406\u4e00\u81f4\uff0c\u5c24\u5176\u5728\u590d\u6742\u51e0\u4f55\u6216\u6a21\u7cca\u5149\u7167\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5c06\u663e\u5f0f\u7269\u7406\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u751f\u6210\u66f4\u51c6\u786e\u548c\u771f\u5b9e\u7684\u9634\u5f71\uff0c\u7279\u522b\u662f\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.06608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06608", "abs": "https://arxiv.org/abs/2512.06608", "authors": ["Xinyu Zhou", "Songhao Piao", "Chao Gao", "Liguo Chen"], "title": "A New Trajectory-Oriented Approach to Enhancing Comprehensive Crowd Navigation Performance", "comment": "8 pages, 6 figures", "summary": "Crowd navigation has garnered considerable research interest in recent years, especially with the proliferating application of deep reinforcement learning (DRL) techniques. Many studies, however, do not sufficiently analyze the relative priorities among evaluation metrics, which compromises the fair assessment of methods with divergent objectives. Furthermore, trajectory-continuity metrics, specifically those requiring $C^2$ smoothness, are rarely incorporated. Current DRL approaches generally prioritize efficiency and proximal comfort, often neglecting trajectory optimization or addressing it only through simplistic, unvalidated smoothness reward. Nevertheless, effective trajectory optimization is essential to ensure naturalness, enhance comfort, and maximize the energy efficiency of any navigation system. To address these gaps, this paper proposes a unified framework that enables the fair and transparent assessment of navigation methods by examining the prioritization and joint evaluation of multiple optimization objectives. We further propose a novel reward-shaping strategy that explicitly emphasizes trajectory-curvature optimization. The resulting trajectory quality and adaptability are significantly enhanced across multi-scale scenarios. Through extensive 2D and 3D experiments, we demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u516c\u5e73\u8bc4\u4f30\u4eba\u7fa4\u5bfc\u822a\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u5956\u52b1\u5851\u9020\u7b56\u7565\u4f18\u5316\u8f68\u8ff9\u66f2\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u8d28\u91cf\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u4eba\u7fa4\u5bfc\u822a\u7814\u7a76\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u7f3a\u4e4f\u5bf9\u8bc4\u4f30\u6307\u6807\u76f8\u5bf9\u4f18\u5148\u7ea7\u7684\u5145\u5206\u5206\u6790\uff0c\u5bfc\u81f4\u5bf9\u4e0d\u540c\u76ee\u6807\u65b9\u6cd5\u7684\u8bc4\u4f30\u4e0d\u516c\u5e73\uff1b2\uff09\u5f88\u5c11\u8003\u8651\u8f68\u8ff9\u8fde\u7eed\u6027\u6307\u6807\uff08\u7279\u522b\u662f\u9700\u8981C\u00b2\u5e73\u6ed1\u5ea6\u7684\u6307\u6807\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u6548\u7387\u548c\u8fd1\u7aef\u8212\u9002\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u516c\u5e73\u900f\u660e\u5730\u8bc4\u4f30\u5bfc\u822a\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5956\u52b1\u5851\u9020\u7b56\u7565\uff0c\u660e\u786e\u5f3a\u8c03\u8f68\u8ff9\u66f2\u7387\u4f18\u5316\u3002\u901a\u8fc72D\u548c3D\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u5c3a\u5ea6\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u8d28\u91cf\u548c\u9002\u5e94\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u4eba\u7fa4\u5bfc\u822a\u4e2d\u8bc4\u4f30\u4e0d\u516c\u5e73\u548c\u8f68\u8ff9\u4f18\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u66f2\u7387\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u3001\u8212\u9002\u548c\u8282\u80fd\u7684\u5bfc\u822a\u7cfb\u7edf\u3002"}}
{"id": "2512.06274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06274", "abs": "https://arxiv.org/abs/2512.06274", "authors": ["Hanmo Zhang", "Zenghui Sun", "Kai Wang"], "title": "Networked Restless Multi-Arm Bandits with Reinforcement Learning", "comment": null, "summary": "Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7f51\u7edc\u5316RMAB\u6846\u67b6\uff0c\u5c06RMAB\u6a21\u578b\u4e0e\u72ec\u7acb\u7ea7\u8054\u6a21\u578b\u7ed3\u5408\uff0c\u4ee5\u6355\u6349\u7f51\u7edc\u73af\u5883\u4e2d\u4e2a\u4f53\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRMAB\u5047\u8bbe\u72ec\u7acb\u6027\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfRMAB\u5047\u8bbe\u5404\u81c2\u72ec\u7acb\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e2a\u4f53\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u516c\u5171\u536b\u751f\u7b49\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u5b9a\u4e49\u4e86\u7f51\u7edc\u5316RMAB\u7684\u8d1d\u5c14\u66fc\u65b9\u7a0b\uff0c\u901a\u8fc7\u8bc1\u660e\u8d1d\u5c14\u66fc\u65b9\u7a0b\u7684\u5b50\u6a21\u6027\u5e76\u5e94\u7528\u722c\u5c71\u7b97\u6cd5\uff0c\u5728\u8d1d\u5c14\u66fc\u66f4\u65b0\u4e2d\u5b9e\u73b01-1/e\u7684\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u5e76\u5f00\u53d1\u4e86\u9488\u5bf9\u7f51\u7edc\u73af\u5883\u7684\u9ad8\u6548Q\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u771f\u5b9e\u56fe\u6570\u636e\u4e0a\uff0c\u6240\u63d0\u51fa\u7684Q\u5b66\u4e60\u65b9\u6cd5\u4f18\u4e8ek\u6b65\u524d\u77bb\u548c\u7f51\u7edc\u76f2\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6355\u6349\u7f51\u7edc\u6548\u5e94\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7f51\u7edc\u5316RMAB\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u4e2a\u4f53\u95f4\u4ea4\u4e92\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.06179", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06179", "abs": "https://arxiv.org/abs/2512.06179", "authors": ["Shilin Hu", "Jingyi Xu", "Sagnik Das", "Dimitris Samaras", "Hieu Le"], "title": "Physics-Grounded Attached Shadow Detection Using Approximate 3D Geometry and Light Direction", "comment": null, "summary": "Attached shadows occur on the surface of the occluder where light cannot reach because of self-occlusion. They are crucial for defining the three-dimensional structure of objects and enhancing scene understanding. Yet existing shadow detection methods mainly target cast shadows, and there are no dedicated datasets or models for detecting attached shadows. To address this gap, we introduce a framework that jointly detects cast and attached shadows by reasoning about their mutual relationship with scene illumination and geometry. Our system consists of a shadow detection module that predicts both shadow types separately, and a light estimation module that infers the light direction from the detected shadows. The estimated light direction, combined with surface normals, allows us to derive a geometry-consistent partial map that identifies regions likely to be self-occluded. This partial map is then fed back to refine shadow predictions, forming a closed-loop reasoning process that iteratively improves both shadow segmentation and light estimation. In order to train our method, we have constructed a dataset of 1,458 images with separate annotations for cast and attached shadows, enabling training and quantitative evaluation of both. Experimental results demonstrate that this iterative geometry-illumination reasoning substantially improves the detection of attached shadows, with at least 33% BER reduction, while maintaining strong full and cast shadow performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8054\u5408\u68c0\u6d4b\u6295\u5c04\u9634\u5f71\u548c\u9644\u7740\u9634\u5f71\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u9634\u5f71\u4e0e\u573a\u666f\u5149\u7167\u548c\u51e0\u4f55\u7684\u76f8\u4e92\u5173\u7cfb\u6765\u6539\u8fdb\u9634\u5f71\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u9634\u5f71\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6295\u5c04\u9634\u5f71\uff0c\u7f3a\u4e4f\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u6765\u68c0\u6d4b\u9644\u7740\u9634\u5f71\uff0c\u800c\u9644\u7740\u9634\u5f71\u5bf9\u4e8e\u5b9a\u4e49\u7269\u4f53\u4e09\u7ef4\u7ed3\u6784\u548c\u589e\u5f3a\u573a\u666f\u7406\u89e3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u9634\u5f71\u68c0\u6d4b\u6a21\u5757\u548c\u5149\u7167\u4f30\u8ba1\u6a21\u5757\uff0c\u901a\u8fc7\u4f30\u8ba1\u5149\u7167\u65b9\u5411\u7ed3\u5408\u8868\u9762\u6cd5\u7ebf\u63a8\u5bfc\u51e0\u4f55\u4e00\u81f4\u7684\u90e8\u5206\u5730\u56fe\uff0c\u5f62\u6210\u95ed\u73af\u63a8\u7406\u8fc7\u7a0b\u8fed\u4ee3\u6539\u8fdb\u9634\u5f71\u5206\u5272\u548c\u5149\u7167\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u9644\u7740\u9634\u5f71\u7684\u68c0\u6d4b\uff0cBER\u51cf\u5c11\u81f3\u5c1133%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5168\u9634\u5f71\u548c\u6295\u5c04\u9634\u5f71\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u9644\u7740\u9634\u5f71\u68c0\u6d4b\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u7684\u51e0\u4f55-\u5149\u7167\u63a8\u7406\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u9634\u5f71\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2512.06610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06610", "abs": "https://arxiv.org/abs/2512.06610", "authors": ["Marvin Harms", "Jaeyoung Lim", "David Rohr", "Friedrich Rockenbauer", "Nicholas Lawrance", "Roland Siegwart"], "title": "Robust Optimization-based Autonomous Dynamic Soaring with a Fixed-Wing UAV", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Dynamic soaring is a flying technique to exploit the energy available in wind shear layers, enabling potentially unlimited flight without the need for internal energy sources. We propose a framework for autonomous dynamic soaring with a fixed-wing unmanned aerial vehicle (UAV). The framework makes use of an explicit representation of the wind field and a classical approach for guidance and control of the UAV. Robustness to wind field estimation error is achieved by constructing point-wise robust reference paths for dynamic soaring and the development of a robust path following controller for the fixed-wing UAV. The framework is evaluated in dynamic soaring scenarios in simulation and real flight tests. In simulation, we demonstrate robust dynamic soaring flight subject to varied wind conditions, estimation errors and disturbances. Critical components of the framework, including energy predictions and path-following robustness, are further validated in real flights to assure small sim-to-real gap. Together, our results strongly indicate the ability of the proposed framework to achieve autonomous dynamic soaring flight in wind shear.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u7684\u6846\u67b6\uff0c\u5229\u7528\u98ce\u573a\u663e\u5f0f\u8868\u793a\u548c\u7ecf\u5178\u5236\u5bfc\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u9c81\u68d2\u53c2\u8003\u8def\u5f84\u548c\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\u5668\u5b9e\u73b0\u98ce\u573a\u4f30\u8ba1\u8bef\u5dee\u4e0b\u7684\u7a33\u5065\u98de\u884c\u3002", "motivation": "\u52a8\u6001\u7ff1\u7fd4\u662f\u4e00\u79cd\u5229\u7528\u98ce\u5207\u53d8\u5c42\u80fd\u91cf\u7684\u98de\u884c\u6280\u672f\uff0c\u53ef\u4ee5\u5b9e\u73b0\u65e0\u5185\u90e8\u80fd\u6e90\u7684\u65e0\u9650\u98de\u884c\u3002\u7814\u7a76\u76ee\u6807\u662f\u5f00\u53d1\u80fd\u591f\u5b9e\u73b0\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u98ce\u573a\u4f30\u8ba1\u8bef\u5dee\u548c\u5b9e\u9645\u98de\u884c\u4e2d\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u98ce\u573a\u663e\u5f0f\u8868\u793a\u548c\u7ecf\u5178\u5236\u5bfc\u63a7\u5236\u65b9\u6cd5\uff0c\u6784\u5efa\u70b9\u5bf9\u70b9\u9c81\u68d2\u53c2\u8003\u8def\u5f84\uff0c\u5f00\u53d1\u9c81\u68d2\u8def\u5f84\u8ddf\u8e2a\u63a7\u5236\u5668\uff0c\u5e76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u98de\u884c\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6846\u67b6\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u5728\u591a\u79cd\u98ce\u51b5\u3001\u4f30\u8ba1\u8bef\u5dee\u548c\u5e72\u6270\u4e0b\u90fd\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u52a8\u6001\u7ff1\u7fd4\u98de\u884c\u3002\u771f\u5b9e\u98de\u884c\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u80fd\u91cf\u9884\u6d4b\u548c\u8def\u5f84\u8ddf\u8e2a\u9c81\u68d2\u6027\u7b49\u5173\u952e\u7ec4\u4ef6\uff0c\u8868\u660e\u4eff\u771f\u4e0e\u5b9e\u9645\u98de\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u5f88\u5c0f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u6709\u529b\u5730\u8868\u660e\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u5728\u98ce\u5207\u53d8\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u52a8\u6001\u7ff1\u7fd4\u98de\u884c\uff0c\u4e3a\u65e0\u80fd\u6e90\u9650\u5236\u7684\u65e0\u4eba\u673a\u98de\u884c\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.06288", "categories": ["cs.LG", "math.ST"], "pdf": "https://arxiv.org/pdf/2512.06288", "abs": "https://arxiv.org/abs/2512.06288", "authors": ["Houssam El Cheairi", "David Gamarnik", "Rahul Mazumder"], "title": "Theoretical Compression Bounds for Wide Multilayer Perceptrons", "comment": null, "summary": "Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u8d2a\u5a6a\u538b\u7f29\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u540e\u526a\u679d\u548c\u91cf\u5316\uff0c\u5e76\u4e25\u683c\u8bc1\u660e\u4e86\u5728\u591a\u5c42\u611f\u77e5\u5668\uff08MLPs\uff09\u4e2d\u5b58\u5728\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\u7684\u526a\u679d/\u91cf\u5316\u5b50\u7f51\u7edc\u3002\u7814\u7a76\u8fd8\u6269\u5c55\u5230\u7ed3\u6784\u5316\u526a\u679d\uff0c\u5c55\u793a\u4e86\u538b\u7f29\u6027\u4e0e\u7f51\u7edc\u5bbd\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "motivation": "\u5c3d\u7ba1\u526a\u679d\u548c\u91cf\u5316\u6280\u672f\u5728\u51cf\u5c11\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u6570\u91cf\u65b9\u9762\u53d6\u5f97\u4e86\u5e7f\u6cdb\u6210\u529f\uff0c\u4f46\u5bf9\u5176\u7ecf\u9a8c\u6210\u529f\u7684\u7406\u8bba\u89e3\u91ca\u4ecd\u7136\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\uff0c\u4e3a\u538b\u7f29\u6280\u672f\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u91c7\u7528\u968f\u673a\u8d2a\u5a6a\u538b\u7f29\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4e0eOptimal Brain Damage\uff08OBD\uff09\u6709\u76f8\u4f3c\u4e4b\u5904\uff0c\u53ef\u89c6\u4e3a\u5176\u8bad\u7ec3\u540e\u968f\u673a\u7248\u672c\u3002\u7b97\u6cd5\u5e94\u7528\u4e8e\u591a\u5c42\u611f\u77e5\u5668\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u526a\u679d\u548c\u91cf\u5316\u3002", "result": "\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u5bbd\u7f51\u7edc\u4e2d\u5b58\u5728\u6027\u80fd\u5177\u6709\u7ade\u4e89\u529b\u7684\u526a\u679d/\u91cf\u5316\u5b50\u7f51\u7edc\uff0c\u4e14\u7ed3\u679c\u4e0d\u4f9d\u8d56\u4e8e\u6570\u636e\u5047\u8bbe\u3002\u53d1\u73b0\u4e86\u538b\u7f29\u6027\u4e0e\u7f51\u7edc\u5bbd\u5ea6\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u7684\u7406\u8bba\u7ed3\u679c\u5f25\u5408\u4e86\u526a\u679d/\u91cf\u5316\u7406\u8bba\u4e0e\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5bbd\u591a\u5c42\u611f\u77e5\u5668\u4e2d\u538b\u7f29\u6280\u672f\u7684\u7ecf\u9a8c\u6210\u529f\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2512.06185", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06185", "abs": "https://arxiv.org/abs/2512.06185", "authors": ["Ankit Gupta", "Christoph Adami", "Emily Dolson"], "title": "SPOOF: Simple Pixel Operations for Out-of-Distribution Fooling", "comment": "10 pages with 8 figures, plus 13 pages and 16 figures of supplementary material", "summary": "Deep neural networks (DNNs) excel across image recognition tasks, yet continue to exhibit overconfidence on inputs that bear no resemblance to natural images. Revisiting the \"fooling images\" work introduced by Nguyen et al. (2015), we re-implement both CPPN-based and direct-encoding-based evolutionary fooling attacks on modern architectures, including convolutional and transformer classifiers. Our re-implementation confirm that high-confidence fooling persists even in state-of-the-art networks, with transformer-based ViT-B/16 emerging as the most susceptible--achieving near-certain misclassifications with substantially fewer queries than convolution-based models. We then introduce SPOOF, a minimalist, consistent, and more efficient black-box attack generating high-confidence fooling images. Despite its simplicity, SPOOF generates unrecognizable fooling images with minimal pixel modifications and drastically reduced compute. Furthermore, retraining with fooling images as an additional class provides only partial resistance, as SPOOF continues to fool consistently with slightly higher query budgets--highlighting persistent fragility of modern deep classifiers.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u9a8c\u8bc1\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bf9\u975e\u81ea\u7136\u56fe\u50cf\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff0c\u53d1\u73b0\u5373\u4f7f\u5728\u73b0\u4ee3\u67b6\u6784\u4e2d\uff0c\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u6b3a\u9a97\u56fe\u50cf\u4f9d\u7136\u5b58\u5728\uff0c\u5176\u4e2dViT-B/16\u53d8\u6362\u5668\u6a21\u578b\u6700\u4e3a\u8106\u5f31\u3002\u4f5c\u8005\u63d0\u51fa\u4e86SPOOF\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u6210\u672c\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u6b3a\u9a97\u56fe\u50cf\uff0c\u4e14\u91cd\u65b0\u8bad\u7ec3\u4ec5\u80fd\u63d0\u4f9b\u90e8\u5206\u62b5\u6297\u529b\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u56fe\u50cf\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u975e\u81ea\u7136\u56fe\u50cf\u4ecd\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u91cd\u65b0\u9a8c\u8bc1\u8fd9\u4e00\u73b0\u8c61\u5728\u73b0\u4ee3\u67b6\u6784\u4e2d\u7684\u6301\u7eed\u6027\uff0c\u5e76\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u653b\u51fb\u65b9\u6cd5\u6765\u63ed\u793a\u6df1\u5ea6\u5206\u7c7b\u5668\u7684\u8106\u5f31\u6027\u3002", "method": "\u91cd\u65b0\u5b9e\u73b0\u4e86\u57fa\u4e8eCPPN\u548c\u76f4\u63a5\u7f16\u7801\u7684\u8fdb\u5316\u6b3a\u9a97\u653b\u51fb\uff0c\u5e76\u63d0\u51fa\u4e86SPOOF\u2014\u2014\u4e00\u79cd\u7b80\u7ea6\u3001\u4e00\u81f4\u4e14\u66f4\u9ad8\u6548\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u50cf\u7d20\u4fee\u6539\u751f\u6210\u9ad8\u7f6e\u4fe1\u5ea6\u6b3a\u9a97\u56fe\u50cf\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\u9ad8\u7f6e\u4fe1\u5ea6\u6b3a\u9a97\u5728\u73b0\u4ee3\u7f51\u7edc\u4e2d\u6301\u7eed\u5b58\u5728\uff0cViT-B/16\u53d8\u6362\u5668\u6a21\u578b\u6700\u4e3a\u8106\u5f31\u3002SPOOF\u80fd\u4ee5\u6781\u5c11\u7684\u67e5\u8be2\u6b21\u6570\u751f\u6210\u65e0\u6cd5\u8bc6\u522b\u7684\u6b3a\u9a97\u56fe\u50cf\uff0c\u4e14\u91cd\u65b0\u8bad\u7ec3\u4ec5\u80fd\u90e8\u5206\u62b5\u6297\u653b\u51fb\u3002", "conclusion": "\u73b0\u4ee3\u6df1\u5ea6\u5206\u7c7b\u5668\u5b58\u5728\u6301\u7eed\u6027\u8106\u5f31\uff0c\u5373\u4f7f\u91cd\u65b0\u8bad\u7ec3\u4e5f\u96be\u4ee5\u5b8c\u5168\u62b5\u6297\u9ad8\u6548\u7684\u9ed1\u76d2\u653b\u51fb\uff0c\u8868\u660e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5b89\u5168\u6027\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2512.06628", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06628", "abs": "https://arxiv.org/abs/2512.06628", "authors": ["Ruicheng Zhang", "Mingyang Zhang", "Jun Zhou", "Zhangrui Guo", "Xiaofan Liu", "Zunnan Xu", "Zhizhou Zhong", "Puxin Yan", "Haocheng Luo", "Xiu Li"], "title": "MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment", "comment": null, "summary": "Embodied imitation learning is constrained by the scarcity of diverse, long-horizon robotic manipulation data. Existing video generation models for this domain are limited to synthesizing short clips of simple actions and often rely on manually defined trajectories. To this end, we introduce MIND-V, a hierarchical framework designed to synthesize physically plausible and logically coherent videos of long-horizon robotic manipulation. Inspired by cognitive science, MIND-V bridges high-level reasoning with pixel-level synthesis through three core components: a Semantic Reasoning Hub (SRH) that leverages a pre-trained vision-language model for task planning; a Behavioral Semantic Bridge (BSB) that translates abstract instructions into domain-invariant representations; and a Motor Video Generator (MVG) for conditional video rendering. MIND-V employs Staged Visual Future Rollouts, a test-time optimization strategy to enhance long-horizon robustness. To align the generated videos with physical laws, we introduce a GRPO reinforcement learning post-training phase guided by a novel Physical Foresight Coherence (PFC) reward. PFC leverages the V-JEPA world model to enforce physical plausibility by aligning the predicted and actual dynamic evolutions in the feature space. MIND-V demonstrates state-of-the-art performance in long-horizon robotic manipulation video generation, establishing a scalable and controllable paradigm for embodied data synthesis.", "AI": {"tldr": "MIND-V\u662f\u4e00\u4e2a\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u3001\u884c\u4e3a\u8bed\u4e49\u6865\u548c\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u5668\u5b9e\u73b0\u7269\u7406\u5408\u7406\u4e14\u903b\u8f91\u8fde\u8d2f\u7684\u89c6\u9891\u5408\u6210\uff0c\u5e76\u91c7\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u548c\u7269\u7406\u9884\u89c1\u4e00\u81f4\u6027\u5956\u52b1\u6765\u589e\u5f3a\u7269\u7406\u5408\u7406\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u53ea\u80fd\u5408\u6210\u7b80\u5355\u52a8\u4f5c\u7684\u77ed\u89c6\u9891\uff0c\u4e14\u4f9d\u8d56\u624b\u52a8\u5b9a\u4e49\u8f68\u8ff9\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u957f\u65f6\u7a0b\u6570\u636e\u3002", "method": "MIND-V\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8bed\u4e49\u63a8\u7406\u67a2\u7ebd\uff08SRH\uff09\u7528\u4e8e\u4efb\u52a1\u89c4\u5212\uff0c\u884c\u4e3a\u8bed\u4e49\u6865\uff08BSB\uff09\u5c06\u62bd\u8c61\u6307\u4ee4\u8f6c\u6362\u4e3a\u9886\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u8fd0\u52a8\u89c6\u9891\u751f\u6210\u5668\uff08MVG\uff09\u7528\u4e8e\u6761\u4ef6\u89c6\u9891\u6e32\u67d3\u3002\u91c7\u7528\u5206\u9636\u6bb5\u89c6\u89c9\u672a\u6765\u6eda\u52a8\u7b56\u7565\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u901a\u8fc7\u7269\u7406\u9884\u89c1\u4e00\u81f4\u6027\uff08PFC\uff09\u5956\u52b1\u786e\u4fdd\u7269\u7406\u5408\u7406\u6027\u3002", "result": "MIND-V\u5728\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u548c\u53ef\u63a7\u7684\u5177\u8eab\u6570\u636e\u5408\u6210\u8303\u5f0f\u3002", "conclusion": "MIND-V\u901a\u8fc7\u5206\u5c42\u6846\u67b6\u548c\u7269\u7406\u5408\u7406\u6027\u589e\u5f3a\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\u751f\u6210\u7684\u6311\u6218\uff0c\u4e3a\u5177\u8eab\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06293", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06293", "abs": "https://arxiv.org/abs/2512.06293", "authors": ["Fatima Ashraf", "Muhammad Ayub Sabir", "Jiaxin Deng", "Junbiao Pang", "Haitao Yu"], "title": "Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media", "comment": null, "summary": "Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \\emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5efa\u6a21\u8bed\u8a00\u4ea4\u4e92\u548c\u7528\u6237\u5f71\u54cd\u529b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u793e\u4ea4\u5a92\u4f53\u4e2d\u68c0\u6d4b\u7a00\u758f\u7684\u57ce\u5e02\u4ea4\u901a\u670d\u52a1\u98ce\u9669\u4fe1\u53f7\u3002", "motivation": "\u57ce\u5e02\u4ea4\u901a\u673a\u6784\u4f7f\u7528\u793e\u4ea4\u5a92\u4f53\u76d1\u63a7\u670d\u52a1\u98ce\u9669\uff08\u5982\u62e5\u6324\u3001\u5ef6\u8bef\u3001\u5b89\u5168\u4e8b\u4ef6\uff09\uff0c\u4f46\u76f8\u5173\u4fe1\u53f7\u7a00\u758f\u3001\u7b80\u77ed\u4e14\u6613\u88ab\u5e38\u89c4\u4fe1\u606f\u6df9\u6ca1\u3002", "method": "\u6784\u5efa\u5f71\u54cd\u529b\u52a0\u6743\u7684\u5173\u952e\u8bcd\u5171\u73b0\u56fe\uff0c\u91c7\u7528\u6cca\u677e\u53cd\u5377\u79ef\u5206\u89e3\uff08PDF\uff09\u5c06\u56fe\u5206\u89e3\u4e3a\u4f4e\u79e9\u4e3b\u9898\u7ed3\u6784\u548c\u4e3b\u9898\u5c40\u90e8\u6b8b\u5dee\u4ea4\u4e92\uff0c\u7ed3\u5408\u53bb\u76f8\u5173\u6b63\u5219\u5316\u548c\u8f7b\u91cf\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u5728\u5927\u89c4\u6a21\u793e\u4ea4\u6570\u636e\u6d41\u4e0a\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u4e3b\u9898\u8fde\u8d2f\u6027\u548c\u5f3a\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u53d6\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u7a00\u758f\u98ce\u9669\u4fe1\u53f7\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u3002"}}
{"id": "2512.06190", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.06190", "abs": "https://arxiv.org/abs/2512.06190", "authors": ["Shichen Li", "Ahmadreza Eslaminia", "Chenhui Shao"], "title": "Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying", "comment": null, "summary": "Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u989c\u8272\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5c06\u9ad8\u7ef4\u65f6\u95f4\u989c\u8272\u4fe1\u606f\u4e0e\u5e72\u71e5\u8fc7\u7a0b\u53c2\u6570\u7ed3\u5408\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u6570\u636e\u9ad8\u6548\u7684\u989c\u8272\u8f68\u8ff9\u9884\u6d4b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4f4e\u7ef4\u989c\u8272\u7279\u5f81\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u98df\u54c1\u6837\u672c\u590d\u6742\u7684\u52a8\u6001\u989c\u8272\u8f68\u8ff9\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u672a\u89c1\u8fc7\u7a0b\u6761\u4ef6\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u5f00\u53d1\u591a\u6a21\u6001\u989c\u8272\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u6574\u5408\u9ad8\u7ef4\u65f6\u95f4\u989c\u8272\u4fe1\u606f\u548c\u5e72\u71e5\u8fc7\u7a0b\u53c2\u6570", "result": "\u5728\u672a\u89c1\u5e72\u71e5\u6761\u4ef6\u4e0b\uff0c\u6a21\u578b\u5728\u997c\u5e72\u5e72\u71e5\u4e2d\u8fbe\u5230RMSE 2.12\uff0c\u82f9\u679c\u5e72\u71e5\u4e2d\u8fbe\u5230RMSE 1.29\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u8bef\u5dee\u51cf\u5c11\u8d85\u8fc790%", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5177\u6709\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027"}}
{"id": "2512.06664", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06664", "abs": "https://arxiv.org/abs/2512.06664", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Jingreng Lei", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "Statistic-Augmented, Decoupled MoE Routing and Aggregating in Autonomous Driving", "comment": "9 pages", "summary": "Autonomous driving (AD) scenarios are inherently complex and diverse, posing significant challenges for a single deep learning model to effectively cover all possible conditions, such as varying weather, traffic densities, and road types. Large Model (LM)-Driven Mixture of Experts (MoE) paradigm offers a promising solution, where LM serves as the backbone to extract latent features while MoE serves as the downstream head to dynamically select and aggregate specialized experts to adapt to different scenarios. However, routing and aggregating in MoE face intrinsic challenges, including imprecise expert selection due to flawed routing strategy and inefficient expert aggregation leading to suboptimal prediction. To address these issues, we propose a statistic-augmented, decoupled MoE }outing and Aggregating Mechanism (MoE-RAM) driven by LM. Specifically, on the one hand, MoE-RAM enhances expert routing by incorporating statistical retrieval mechanism to match LM-extracted latent features with cached prototypical features of the most relevant experts; on the other hand, MoE-RAM adaptively reweights experts' outputs in fusion by measuring statistical distances of experts' instant features against LM-extracted latent features. Benefiting from the synergy of the statistic-augmented MoE's routing and aggregating, MoE-RAM ultimately improves the prediction performance. We take the AD semantic segmentation task as an example to assess the proposed MoE-RAM. Extensive experiments on AD datasets demonstrate the superiority of MoE-RAM compared to other MoE baselines and conventional single-model approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u7edf\u8ba1\u589e\u5f3a\u89e3\u8026MoE\u8def\u7531\u4e0e\u805a\u5408\u673a\u5236\uff08MoE-RAM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u5355\u4e00\u6a21\u578b\u96be\u4ee5\u8986\u76d6\u6240\u6709\u590d\u6742\u6761\u4ef6\u7684\u95ee\u9898\u3002\u901a\u8fc7\u7edf\u8ba1\u68c0\u7d22\u589e\u5f3a\u4e13\u5bb6\u8def\u7531\uff0c\u5e76\u57fa\u4e8e\u7edf\u8ba1\u8ddd\u79bb\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u4e13\u5bb6\u805a\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u590d\u6742\u591a\u6837\uff0c\u5355\u4e00\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u6709\u6548\u8986\u76d6\u6240\u6709\u53ef\u80fd\u6761\u4ef6\uff08\u5982\u4e0d\u540c\u5929\u6c14\u3001\u4ea4\u901a\u5bc6\u5ea6\u548c\u9053\u8def\u7c7b\u578b\uff09\u3002MoE\u8303\u5f0f\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u6765\u9002\u5e94\u4e0d\u540c\u573a\u666f\uff0c\u4f46\u5b58\u5728\u8def\u7531\u7b56\u7565\u4e0d\u7cbe\u786e\u548c\u4e13\u5bb6\u805a\u5408\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faMoE-RAM\u673a\u5236\uff1a1\uff09\u4f7f\u7528\u7edf\u8ba1\u68c0\u7d22\u673a\u5236\u589e\u5f3a\u4e13\u5bb6\u8def\u7531\uff0c\u5c06\u5927\u6a21\u578b\u63d0\u53d6\u7684\u6f5c\u5728\u7279\u5f81\u4e0e\u7f13\u5b58\u7684\u4e13\u5bb6\u539f\u578b\u7279\u5f81\u8fdb\u884c\u5339\u914d\uff1b2\uff09\u901a\u8fc7\u6d4b\u91cf\u4e13\u5bb6\u5373\u65f6\u7279\u5f81\u4e0e\u5927\u6a21\u578b\u63d0\u53d6\u6f5c\u5728\u7279\u5f81\u4e4b\u95f4\u7684\u7edf\u8ba1\u8ddd\u79bb\uff0c\u81ea\u9002\u5e94\u91cd\u52a0\u6743\u4e13\u5bb6\u8f93\u51fa\u878d\u5408\u3002", "result": "\u5728\u81ea\u52a8\u9a7e\u9a76\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoE-RAM\u76f8\u6bd4\u5176\u4ed6MoE\u57fa\u7ebf\u548c\u4f20\u7edf\u5355\u6a21\u578b\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "MoE-RAM\u901a\u8fc7\u7edf\u8ba1\u589e\u5f3a\u7684\u8def\u7531\u548c\u805a\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86MoE\u4e2d\u7684\u4e13\u5bb6\u9009\u62e9\u548c\u805a\u5408\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06297", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06297", "abs": "https://arxiv.org/abs/2512.06297", "authors": ["Luca Di Carlo", "Chase Goddard", "David J. Schwab"], "title": "Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks", "comment": "Under Review", "summary": "Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u635f\u5931\u666f\u89c2\u4e2d\u4e00\u4e2a\u6096\u8bba\uff1a\u867d\u7136\u5438\u5f15\u76c6\u4e4b\u95f4\u5b58\u5728\u4f4e\u635f\u5931\u8def\u5f84\uff0c\u4f46\u4f18\u5316\u52a8\u6001\u901a\u5e38\u5c40\u9650\u5728\u5355\u4e2a\u51f8\u76c6\u5185\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u66f2\u7387\u53d8\u5316\u4e0e\u4f18\u5316\u566a\u58f0\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u7684\u71b5\u52bf\u5792\u5bfc\u81f4\u7684\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4e2d\u5b58\u5728\u7684\u77db\u76fe\u73b0\u8c61\uff1a\u5c3d\u7ba1\u635f\u5931\u666f\u89c2\u4e2d\u5b58\u5728\u8fde\u63a5\u4e0d\u540c\u5438\u5f15\u76c6\u7684\u4f4e\u635f\u5931\u8def\u5f84\uff0c\u4f46\u4f18\u5316\u7b97\u6cd5\u901a\u5e38\u65e0\u6cd5\u63a2\u7d22\u8fd9\u4e9b\u8def\u5f84\uff0c\u800c\u662f\u88ab\u56f0\u5728\u5355\u4e2a\u51f8\u76c6\u5185\u3002", "method": "\u901a\u8fc7\u5206\u6790\u66f2\u7387\u6cbf\u4f4e\u635f\u5931\u8def\u5f84\u7684\u53d8\u5316\u89c4\u5f8b\uff0c\u7814\u7a76\u66f2\u7387\u53d8\u5316\u4e0e\u4f18\u5316\u52a8\u6001\u566a\u58f0\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u8bc6\u522b\u71b5\u52bf\u5792\u7684\u5f62\u6210\u673a\u5236\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u66f2\u7387\u5728\u8fdc\u79bb\u6700\u5c0f\u503c\u65f6\u7cfb\u7edf\u6027\u4e0a\u5347\uff0c\u4ea7\u751f\u5c06\u566a\u58f0\u52a8\u6001\u504f\u5411\u7aef\u70b9\u7684\u6709\u6548\u529b\uff0c\u5373\u4f7f\u635f\u5931\u4fdd\u6301\u8fd1\u4e4e\u5e73\u5766\u3002\u8fd9\u4e9b\u71b5\u52bf\u5792\u6bd4\u80fd\u91cf\u52bf\u5792\u6301\u7eed\u66f4\u4e45\uff0c\u5851\u9020\u4e86\u53c2\u6570\u7a7a\u95f4\u4e2d\u89e3\u7684\u540e\u671f\u5b9a\u4f4d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u66f2\u7387\u8bf1\u5bfc\u7684\u71b5\u529b\u5728\u6df1\u5ea6\u5b66\u4e60\u666f\u89c2\u4e2d\u540c\u65f6\u63a7\u5236\u7740\u8fde\u901a\u6027\u548c\u7ea6\u675f\u6027\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u4f18\u5316\u7b97\u6cd5\u96be\u4ee5\u63a2\u7d22\u8fde\u63a5\u4e0d\u540c\u5438\u5f15\u76c6\u7684\u8def\u5f84\u3002"}}
{"id": "2512.06206", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06206", "abs": "https://arxiv.org/abs/2512.06206", "authors": ["Akis Linardos", "Sarthak Pati", "Ujjwal Baid", "Brandon Edwards", "Patrick Foley", "Kevin Ta", "Verena Chung", "Micah Sheller", "Muhammad Irfan Khan", "Mojtaba Jafaritadi", "Elina Kontio", "Suleiman Khan", "Leon M\u00e4chler", "Ivan Ezhov", "Suprosanna Shit", "Johannes C. Paetzold", "Gustav Grimberg", "Manuel A. Nickel", "David Naccache", "Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni", "Daewoon Kim", "Leonard L. Klausmann", "Prashant Shah", "Bjoern Menze", "Dimitrios Makris", "Spyridon Bakas"], "title": "The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning", "comment": "Published at the Journal of Machine Learning for Biomedical Imaging (MELBA) https://melba-journal.org/2025:033", "summary": "We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.", "AI": {"tldr": "MICCAI FeTS 2024\u6311\u6218\u8d5b\u805a\u7126\u4e8e\u8054\u90a6\u5b66\u4e60\u5728\u80f6\u8d28\u7624\u4e9a\u533a\u5206\u5272\u4e2d\u7684\u5e94\u7528\uff0c\u8bc4\u4f30\u4e86\u65b0\u7684\u6743\u91cd\u805a\u5408\u65b9\u6cd5\uff0cPID\u63a7\u5236\u5668\u65b9\u6cd5\u5728\u5206\u5272\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a8\u52a8\u8054\u90a6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u9488\u5bf9\u591a\u4e2d\u5fc3\u80f6\u8d28\u7624\u5206\u5272\u4efb\u52a1\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\uff0c\u89e3\u51b3\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u534f\u4f5c\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u548c\u591a\u673a\u6784\u6570\u636e\u96c6\uff08BraTS\u57fa\u51c6\uff09\uff0c\u5305\u62ec1251\u4e2a\u8bad\u7ec3\u6848\u4f8b\u3001219\u4e2a\u9a8c\u8bc1\u6848\u4f8b\u548c570\u4e2a\u9690\u85cf\u6d4b\u8bd5\u6848\u4f8b\u3002\u91c7\u7528\u7d2f\u79ef\u8bc4\u5206\u7cfb\u7edf\u8bc4\u4f30\u5206\u5272\u6027\u80fd\uff08DSC\u548cHD95\uff09\u548c\u901a\u4fe1\u6548\u7387\uff08\u6536\u655b\u5206\u6570\uff09\u3002", "result": "PID\u63a7\u5236\u5668\u65b9\u6cd5\u83b7\u5f97\u6700\u9ad8\u6392\u540d\uff0cET\u3001TC\u3001WT\u7684DSC\u5747\u503c\u5206\u522b\u4e3a0.733\u30010.761\u30010.751\uff0cHD95\u503c\u5206\u522b\u4e3a33.922mm\u300133.623mm\u300132.309mm\uff0c\u6536\u655b\u5206\u6570\u4e3a0.764\uff0c\u8d85\u8d8a\u4e86\u4ee5\u5f80\u6311\u6218\u8d5b\u7684\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "PID\u63a7\u5236\u5668\u4f5c\u4e3a\u6709\u6548\u7684\u6743\u91cd\u805a\u5408\u673a\u5236\uff0c\u80fd\u591f\u7a33\u5b9a\u548c\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u8fc7\u7a0b\uff0c\u63a8\u52a8\u4e86\u533b\u5b66\u5f71\u50cf\u8054\u90a6\u5b66\u4e60\u7684\u53d1\u5c55\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.06676", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06676", "abs": "https://arxiv.org/abs/2512.06676", "authors": ["Wei-Bin Kou", "Guangxu Zhu", "Bingyang Cheng", "Chen Zhang", "Yik-Chung Wu", "Jianping Wang"], "title": "FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving", "comment": "9 pages", "summary": "Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.", "AI": {"tldr": "FedDSR\u662f\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8054\u90a6\u5b66\u4e60\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u4e2d\u95f4\u5c42\u76d1\u7763\u548c\u6b63\u5219\u5316\u89e3\u51b3\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u6cdb\u5316\u5dee\u548c\u6536\u655b\u6162\u95ee\u9898\uff0c\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8bad\u7ec3\u8f6e\u6b21\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9762\u4e34\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u6536\u655b\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5206\u5e03\u5f0f\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51faFedDSR\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u57fa\u4e8e\u67b6\u6784\u65e0\u5173\u6807\u51c6\u9009\u62e9\u591a\u4e2a\u4e2d\u95f4\u5c42\uff1b2\uff09\u5728\u9009\u5b9a\u5c42\u8ba1\u7b97\u4e92\u4fe1\u606f\u548c\u8d1f\u71b5\u4f5c\u4e3a\u4e2d\u95f4\u635f\u5931\u548c\u6b63\u5219\u9879\uff1b3\uff09\u4e0e\u8f93\u51fa\u5c42\u635f\u5931\u7ed3\u5408\u5f62\u6210\u7edf\u4e00\u4f18\u5316\u76ee\u6807\uff1b4\uff09\u5728\u4e2d\u592e\u670d\u52a1\u5668\u805a\u5408\u5404\u8f66\u8f86\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFedDSR\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u76f8\u6bd4\u5176\u4ed6\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0cmIoU\u63d0\u5347\u6700\u9ad8\u8fbe8.93%\uff0c\u8bad\u7ec3\u8f6e\u6b21\u51cf\u5c1128.57%\u3002", "conclusion": "FedDSR\u901a\u8fc7\u4e2d\u95f4\u5c42\u76d1\u7763\u548c\u6b63\u5219\u5316\u6709\u6548\u63d0\u5347\u4e86\u8054\u90a6\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6536\u655b\u901f\u5ea6\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u5e94\u7528\u3002"}}
{"id": "2512.06301", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06301", "abs": "https://arxiv.org/abs/2512.06301", "authors": ["Jihun Ahn", "Gabriella Pasya Irianti", "Vikram Thapar", "Su-Mi Hur"], "title": "Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics", "comment": null, "summary": "Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.", "AI": {"tldr": "CI-LLM\u6846\u67b6\u901a\u8fc7HAPPY\u5206\u5b50\u8868\u793a\u548c\u6570\u503c\u63cf\u8ff0\u7b26\uff0c\u5728\u805a\u5408\u7269\u79d1\u5b66\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5c5e\u6027\u9884\u6d4b\u548c\u9006\u5411\u8bbe\u8ba1\uff0c\u514b\u670d\u4e86\u6570\u636e\u7a00\u7f3a\u7684\u9650\u5236\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u65e0\u673a\u5316\u5408\u7269\u548c\u5c0f\u5206\u5b50\u6750\u6599\u53d1\u73b0\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u805a\u5408\u7269\u9886\u57df\u7531\u4e8e\u6570\u636e\u7a00\u7f3a\u7b49\u95ee\u9898\u8fdb\u5c55\u7f13\u6162\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u521b\u65b0\u7684\u5206\u5b50\u8868\u793a\u65b9\u6cd5\u7a81\u7834\u8fd9\u4e00\u74f6\u9888\u3002", "method": "\u63d0\u51faCI-LLM\u6846\u67b6\uff0c\u7ed3\u5408HAPPY\uff08\u5206\u5c42\u62bd\u8c61\u805a\u5408\u7269\u91cd\u590d\u5355\u5143\uff09\u5c06\u5316\u5b66\u4e9a\u7ed3\u6784\u7f16\u7801\u4e3atoken\uff0c\u5e76\u5728Transformer\u67b6\u6784\u4e2d\u6574\u5408\u6570\u503c\u63cf\u8ff0\u7b26\u3002\u4f7f\u7528De\u00b3BERTa\u8fdb\u884c\u5c5e\u6027\u9884\u6d4b\uff0cGPT\u6a21\u578b\u8fdb\u884c\u9006\u5411\u8bbe\u8ba1\u3002", "result": "De\u00b3BERTa\u6bd4\u57fa\u4e8eSMILES\u7684\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u5feb3.5\u500d\uff0c\u5728\u56db\u4e2a\u5c5e\u6027\u4e0a\u7684R\u00b2\u5f97\u5206\u63d0\u53470.9-4.1%\u3002GPT\u751f\u6210\u5668\u5b9e\u73b0100%\u652f\u67b6\u4fdd\u7559\uff0c\u6210\u529f\u4f18\u5316\u8d1f\u76f8\u5173\u76ee\u6807\u7684\u591a\u5c5e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u6218\u7565\u5206\u5b50\u8868\u793a\u5982\u4f55\u63a8\u52a8\u805a\u5408\u7269\u79d1\u5b66\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\uff0c\u5177\u5907\u6b63\u5411\u9884\u6d4b\u548c\u9006\u5411\u8bbe\u8ba1\u7684\u7efc\u5408\u80fd\u529b\u3002"}}
{"id": "2512.06221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06221", "abs": "https://arxiv.org/abs/2512.06221", "authors": ["Alena Makarova"], "title": "Revisiting SVD and Wavelet Difference Reduction for Lossy Image Compression: A Reproducibility Study", "comment": "15 pages, 13 figures. Reproducibility study", "summary": "This work presents an independent reproducibility study of a lossy image compression technique that integrates singular value decomposition (SVD) and wavelet difference reduction (WDR). The original paper claims that combining SVD and WDR yields better visual quality and higher compression ratios than JPEG2000 and standalone WDR. I re-implemented the proposed method, carefully examined missing implementation details, and replicated the original experiments as closely as possible. I then conducted additional experiments on new images and evaluated performance using PSNR and SSIM. In contrast to the original claims, my results indicate that the SVD+WDR technique generally does not surpass JPEG2000 or WDR in terms of PSNR, and only partially improves SSIM relative to JPEG2000. The study highlights ambiguities in the original description (e.g., quantization and threshold initialization) and illustrates how such gaps can significantly impact reproducibility and reported performance.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u5bf9\u7ed3\u5408SVD\u548cWDR\u7684\u635f\u5931\u56fe\u50cf\u538b\u7f29\u6280\u672f\u8fdb\u884c\u4e86\u72ec\u7acb\u53ef\u590d\u73b0\u6027\u7814\u7a76\uff0c\u53d1\u73b0\u8be5\u65b9\u6cd5\u5728PSNR\u6307\u6807\u4e0a\u5e76\u672a\u8d85\u8d8aJPEG2000\u6216WDR\uff0c\u4ec5\u5728SSIM\u6307\u6807\u4e0a\u90e8\u5206\u4f18\u4e8eJPEG2000\uff0c\u4e0e\u539f\u6587\u58f0\u79f0\u7684\u7ed3\u679c\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u9a8c\u8bc1\u539f\u6587\u58f0\u79f0\u7684SVD+WDR\u7ec4\u5408\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u538b\u7f29\u6bd4\u65b9\u9762\u4f18\u4e8eJPEG2000\u548c\u5355\u72ecWDR\u7684\u7ed3\u8bba\u662f\u5426\u53ef\u590d\u73b0\u3002", "method": "\u91cd\u65b0\u5b9e\u73b0\u539f\u6587\u63d0\u51fa\u7684SVD+WDR\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u91cf\u5316\u3001\u9608\u503c\u521d\u59cb\u5316\u7b49\u7f3a\u5931\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u4f7f\u7528PSNR\u548cSSIM\u6307\u6807\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u4e0e\u539f\u6587\u7ed3\u8bba\u76f8\u53cd\uff1aSVD+WDR\u5728PSNR\u6307\u6807\u4e0a\u666e\u904d\u4e0d\u5982JPEG2000\u6216WDR\uff0c\u4ec5\u5728SSIM\u6307\u6807\u4e0a\u90e8\u5206\u4f18\u4e8eJPEG2000\u3002", "conclusion": "\u539f\u6587\u63cf\u8ff0\u5b58\u5728\u6a21\u7cca\u4e4b\u5904\uff0c\u8fd9\u4e9b\u5b9e\u73b0\u7ec6\u8282\u7684\u7f3a\u5931\u4f1a\u663e\u8457\u5f71\u54cd\u53ef\u590d\u73b0\u6027\u548c\u62a5\u544a\u7684\u6027\u80fd\u7ed3\u679c\u3002"}}
{"id": "2512.06754", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06754", "abs": "https://arxiv.org/abs/2512.06754", "authors": ["Shrreya Rajneesh", "Nikita Pavle", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Model-Less Feedback Control of Space-based Continuum Manipulators using Backbone Tension Optimization", "comment": null, "summary": "Continuum manipulators offer intrinsic dexterity and safe geometric compliance for navigation within confined and obstacle-rich environments. However, their infinite-dimensional backbone deformation, unmodeled internal friction, and configuration-dependent stiffness fundamentally limit the reliability of model-based kinematic formulations, resulting in inaccurate Jacobian predictions, artificial singularities, and unstable actuation behavior. Motivated by these limitations, this work presents a complete model-less control framework that bypasses kinematic modeling by using an empirically initialized Jacobian refined online through differential convex updates. Tip motion is generated via a real-time quadratic program that computes actuator increments while enforcing tendon slack avoidance and geometric limits. A backbone tension optimization term is introduced in this paper to regulate axial loading and suppress co-activation compression. The framework is validated across circular, pentagonal, and square trajectories, demonstrating smooth convergence, stable tension evolution, and sub-millimeter steady-state accuracy without any model calibration or parameter identification. These results establish the proposed controller as a scalable alternative to model-dependent continuum manipulation in a constrained environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u6a21\u578b\u7684\u8fde\u7eed\u4f53\u673a\u68b0\u81c2\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u521d\u59cb\u5316\u96c5\u53ef\u6bd4\u77e9\u9635\u5e76\u5728\u7ebf\u4f18\u5316\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6a21\u578b\u4f9d\u8d56\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e9a\u6beb\u7c73\u7ea7\u7684\u7a33\u6001\u7cbe\u5ea6\u3002", "motivation": "\u8fde\u7eed\u4f53\u673a\u68b0\u81c2\u7684\u65e0\u9650\u7ef4\u53d8\u5f62\u3001\u672a\u5efa\u6a21\u5185\u90e8\u6469\u64e6\u548c\u914d\u7f6e\u76f8\u5173\u521a\u5ea6\u7b49\u7279\u6027\u9650\u5236\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u65b9\u6cd5\u7684\u53ef\u9760\u6027\uff0c\u5bfc\u81f4\u96c5\u53ef\u6bd4\u9884\u6d4b\u4e0d\u51c6\u786e\u3001\u4eba\u4e3a\u5947\u70b9\u548c\u4e0d\u7a33\u5b9a\u9a71\u52a8\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u7ecf\u9a8c\u521d\u59cb\u5316\u7684\u96c5\u53ef\u6bd4\u77e9\u9635\uff0c\u901a\u8fc7\u5dee\u5206\u51f8\u66f4\u65b0\u5728\u7ebf\u4f18\u5316\uff1b\u91c7\u7528\u5b9e\u65f6\u4e8c\u6b21\u89c4\u5212\u8ba1\u7b97\u9a71\u52a8\u5668\u589e\u91cf\uff0c\u540c\u65f6\u786e\u4fdd\u808c\u8171\u677e\u5f1b\u907f\u514d\u548c\u51e0\u4f55\u9650\u5236\uff1b\u5f15\u5165\u80cc\u9aa8\u5f20\u529b\u4f18\u5316\u9879\u6765\u8c03\u8282\u8f74\u5411\u8f7d\u8377\u5e76\u6291\u5236\u5171\u6fc0\u6d3b\u538b\u7f29\u3002", "result": "\u5728\u5706\u5f62\u3001\u4e94\u8fb9\u5f62\u548c\u65b9\u5f62\u8f68\u8ff9\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5e73\u6ed1\u6536\u655b\u3001\u7a33\u5b9a\u7684\u5f20\u529b\u6f14\u53d8\u548c\u4e9a\u6beb\u7c73\u7ea7\u7a33\u6001\u7cbe\u5ea6\uff0c\u65e0\u9700\u4efb\u4f55\u6a21\u578b\u6821\u51c6\u6216\u53c2\u6570\u8bc6\u522b\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u4e3a\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8fde\u7eed\u4f53\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6a21\u578b\u4f9d\u8d56\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.06303", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06303", "abs": "https://arxiv.org/abs/2512.06303", "authors": ["Preksha Girish", "Rachana Mysore", "Kiran K. N.", "Hiranmayee R.", "Shipra Prashanth", "Shrey Kumar"], "title": "Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization", "comment": "5 pages, 2 figures. IEEE conference-style format", "summary": "Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u6574\u5408\u7ed3\u6784MRI\u3001\u6269\u6563\u5f20\u91cf\u6210\u50cf\u548c\u529f\u80fdMRI\uff0c\u7528\u4e8e\u6a21\u62df\u5927\u8111\u7f51\u7edc\u7684\u65f6\u7a7a\u91cd\u7ec4\u52a8\u6001\uff0c\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u751f\u7269\u6807\u5fd7\u7269\u6765\u91cf\u5316\u4e2a\u4f53\u8ba4\u77e5\u8870\u9000\u98ce\u9669\u3002", "motivation": "\u7406\u89e3\u5927\u8111\u7f51\u7edc\u7684\u52a8\u6001\u91cd\u7ec4\u5bf9\u4e8e\u9884\u6d4b\u8ba4\u77e5\u8870\u9000\u3001\u795e\u7ecf\u8fdb\u5c55\u548c\u4e34\u5e8a\u7ed3\u679c\u7684\u4e2a\u4f53\u5dee\u5f02\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u65b0\u7684\u6570\u5b66\u6846\u67b6\u6765\u6355\u6349\u957f\u671f\u4f9d\u8d56\u6027\u548c\u968f\u673a\u6ce2\u52a8\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5c06\u5927\u8111\u533a\u57df\u8868\u793a\u4e3a\u8282\u70b9\uff0c\u7ed3\u6784\u548c\u529f\u80fd\u8fde\u63a5\u4f5c\u4e3a\u8fb9\uff0c\u5f62\u6210\u7eb5\u5411\u5927\u8111\u56fe\u3002\u901a\u8fc7\u5206\u6570\u968f\u673a\u5fae\u5206\u7b97\u5b50\u548c\u57fa\u4e8e\u56fe\u7684\u5faa\u73af\u7f51\u7edc\u6355\u6349\u65f6\u95f4\u6f14\u5316\uff0c\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "result": "\u5728\u7eb5\u5411\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u751f\u6210\u7684\u751f\u7269\u6807\u5fd7\u7269\uff08\u7f51\u7edc\u80fd\u91cf\u71b5\u3001\u56fe\u66f2\u7387\u3001\u5206\u6570\u8bb0\u5fc6\u6307\u6570\u7b49\uff09\u80fd\u591f\u6709\u6548\u91cf\u5316\u4e2a\u4f53\u7f51\u7edc\u4e0d\u7a33\u5b9a\u6216\u8ba4\u77e5\u8870\u9000\u98ce\u9669\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6570\u5b66\u4e25\u8c28\u7684\u591a\u6a21\u6001\u56fe\u7f51\u7edc\u65b9\u6cd5\u5728\u4ece\u73b0\u6709\u5f71\u50cf\u6570\u636e\u4e2d\u63a8\u5bfc\u4e34\u5e8a\u6709\u610f\u4e49\u751f\u7269\u6807\u5fd7\u7269\u7684\u6f5c\u529b\uff0c\u65e0\u9700\u65b0\u6570\u636e\u6536\u96c6\u3002"}}
{"id": "2512.06230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06230", "abs": "https://arxiv.org/abs/2512.06230", "authors": ["Pranav Balakrishnan", "Sidisha Barik", "Sean M. O'Rourke", "Benjamin M. Marlin"], "title": "GPU-GLMB: Assessing the Scalability of GPU-Accelerated Multi-Hypothesis Tracking", "comment": null, "summary": "Much recent research on multi-target tracking has focused on multi-hypothesis approaches leveraging random finite sets. Of particular interest are labeled random finite set methods that maintain temporally coherent labels for each object. While these methods enjoy important theoretical properties as closed-form solutions to the multi-target Bayes filter, the maintenance of multiple hypotheses under the standard measurement model is highly computationally expensive, even when hypothesis pruning approximations are applied. In this work, we focus on the Generalized Labeled Multi-Bernoulli (GLMB) filter as an example of this class of methods. We investigate a variant of the filter that allows multiple detections per object from the same sensor, a critical capability when deploying tracking in the context of distributed networks of machine learning-based virtual sensors. We show that this breaks the inter-detection dependencies in the filter updates of the standard GLMB filter, allowing updates with significantly improved parallel scalability and enabling efficient deployment on GPU hardware. We report the results of a preliminary analysis of a GPU-accelerated implementation of our proposed GLMB tracker, with a focus on run time scalability with respect to the number of objects and the maximum number of retained hypotheses.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684GLMB\u6ee4\u6ce2\u5668\uff0c\u5141\u8bb8\u540c\u4e00\u4f20\u611f\u5668\u5bf9\u540c\u4e00\u76ee\u6807\u4ea7\u751f\u591a\u4e2a\u68c0\u6d4b\uff0c\u4ece\u800c\u6253\u7834\u6ee4\u6ce2\u5668\u66f4\u65b0\u4e2d\u7684\u68c0\u6d4b\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u9ad8\u5e76\u884c\u53ef\u6269\u5c55\u6027\u5e76\u5b9e\u73b0GPU\u786c\u4ef6\u4e0a\u7684\u9ad8\u6548\u90e8\u7f72\u3002", "motivation": "\u6807\u51c6GLMB\u6ee4\u6ce2\u5668\u5728\u591a\u76ee\u6807\u8ddf\u8e2a\u4e2d\u867d\u7136\u5177\u6709\u7406\u8bba\u4f18\u52bf\uff0c\u4f46\u5728\u6807\u51c6\u6d4b\u91cf\u6a21\u578b\u4e0b\u7ef4\u62a4\u591a\u4e2a\u5047\u8bbe\u7684\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\u3002\u7279\u522b\u662f\u5728\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u865a\u62df\u4f20\u611f\u5668\u7f51\u7edc\u4e2d\uff0c\u9700\u8981\u80fd\u591f\u5904\u7406\u540c\u4e00\u76ee\u6807\u591a\u6b21\u68c0\u6d4b\u7684\u80fd\u529b\u3002", "method": "\u7814\u7a76GLMB\u6ee4\u6ce2\u5668\u7684\u53d8\u4f53\uff0c\u5141\u8bb8\u540c\u4e00\u4f20\u611f\u5668\u5bf9\u540c\u4e00\u76ee\u6807\u4ea7\u751f\u591a\u4e2a\u68c0\u6d4b\u3002\u8fd9\u79cd\u65b9\u6cd5\u6253\u7834\u4e86\u6ee4\u6ce2\u5668\u66f4\u65b0\u4e2d\u7684\u68c0\u6d4b\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f7f\u5f97\u66f4\u65b0\u8fc7\u7a0b\u5177\u6709\u66f4\u597d\u7684\u5e76\u884c\u53ef\u6269\u5c55\u6027\u3002", "result": "\u521d\u6b65\u5206\u6790\u663e\u793a\uff0cGPU\u52a0\u901f\u5b9e\u73b0\u7684GLMB\u8ddf\u8e2a\u5668\u5728\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u6570\u91cf\u548c\u4fdd\u7559\u5047\u8bbe\u6570\u91cf\u65b9\u9762\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684GLMB\u6ee4\u6ce2\u5668\u53d8\u4f53\u901a\u8fc7\u5141\u8bb8\u591a\u6b21\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5728GPU\u786c\u4ef6\u4e0a\u90e8\u7f72\u591a\u76ee\u6807\u8ddf\u8e2a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06796", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06796", "abs": "https://arxiv.org/abs/2512.06796", "authors": ["Akmaral Moldagalieva", "Keisuke Okumura", "Amanda Prorok", "Wolfgang H\u00f6nig"], "title": "db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF", "comment": null, "summary": "State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time.\n  In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations.\n  To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions.\n  The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics.\n  Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality.\n  The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator.\n  We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.", "AI": {"tldr": "db-LaCAM\u7ed3\u5408MAPF\u7b97\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u52a8\u529b\u5b66\u89c4\u5212\u5668\u7684\u52a8\u6001\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u8fd0\u52a8\u57fa\u5143\u548c\u4f7f\u7528\u7528\u6237\u5b9a\u4e49\u7684\u4e0d\u8fde\u7eed\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf950\u4e2a\u673a\u5668\u4eba\u7684\u9ad8\u6548\u8fd0\u52a8\u89c4\u5212\uff0c\u8fd0\u884c\u901f\u5ea6\u63d0\u534710\u500d\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u591a\u673a\u5668\u4eba\u52a8\u529b\u5b66\u8fd0\u52a8\u89c4\u5212\u5668\u7531\u4e8e\u8ba1\u7b97\u8d1f\u62c5\u9ad8\uff0c\u96be\u4ee5\u5904\u7406\u8d85\u8fc7\u51e0\u4e2a\u673a\u5668\u4eba\u7684\u60c5\u51b5\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u5e76\u5bfc\u81f4\u89c4\u5212\u65f6\u95f4\u7f13\u6162\u3002", "method": "\u63d0\u51fadiscontinuity-Bounded LaCAM (db-LaCAM)\uff0c\u5229\u7528\u9884\u8ba1\u7b97\u7684\u5c0a\u91cd\u673a\u5668\u4eba\u52a8\u529b\u5b66\u7684\u8fd0\u52a8\u57fa\u5143\u751f\u6210\u89c6\u754c\u957f\u5ea6\u8fd0\u52a8\u5e8f\u5217\uff0c\u540c\u65f6\u5141\u8bb8\u8fde\u7eed\u8fd0\u52a8\u4e4b\u95f4\u5b58\u5728\u7528\u6237\u5b9a\u4e49\u7684\u4e0d\u8fde\u7eed\u6027\u3002", "result": "db-LaCAM\u53ef\u9ad8\u6548\u6269\u5c55\u523050\u4e2a\u673a\u5668\u4eba\u573a\u666f\uff0c\u8fd0\u884c\u65f6\u95f4\u6bd4\u6700\u5148\u8fdb\u89c4\u5212\u5668\u5feb10\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002\u57282D\u548c3D\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5355\u8f6e\u8f66\u548c3D\u53cc\u79ef\u5206\u5668\u7b49\u52a8\u529b\u5b66\u6a21\u578b\u3002", "conclusion": "db-LaCAM\u5728\u7269\u7406\u5b9e\u9a8c\u4e2d\u6210\u529f\u9a8c\u8bc1\u4e86\u98de\u884c\u673a\u5668\u4eba\u548c\u5e26\u62d6\u8f66\u6c7d\u8f66\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u5b89\u5168\u6267\u884c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2512.06341", "categories": ["cs.LG", "cs.IR", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.06341", "abs": "https://arxiv.org/abs/2512.06341", "authors": ["Ronald Katende"], "title": "Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness", "comment": null, "summary": "Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u89e3\u91ca\u6548\u7387\u201d\u7684\u65b0\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u89e3\u91ca\u6027\u8868\u793a\u5982\u4f55\u6709\u6548\u4f20\u8f93\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u4e94\u4e2a\u516c\u7406\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u7406\u8bba\u5408\u7406\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u5b66\u4e60\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u7f3a\u4e4f\u80fd\u591f\u91cf\u5316\u6570\u636e\u5982\u4f55\u6709\u6548\u652f\u6301\u89e3\u91ca\u6027\u8868\u793a\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u9700\u8981\u4e00\u79cd\u7406\u8bba\u4e25\u8c28\u4e14\u5b9e\u7528\u7684\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u4e86\u89e3\u91ca\u6548\u7387\u8fd9\u4e00\u6807\u51c6\u5316\u3001\u4efb\u52a1\u611f\u77e5\u7684\u51fd\u6570\uff0c\u57fa\u4e8e\u4e94\u4e2a\u516c\u7406\uff08\u6709\u754c\u6027\u3001Blackwell\u5f0f\u5355\u8c03\u6027\u3001\u6570\u636e\u5904\u7406\u7a33\u5b9a\u6027\u3001\u5bb9\u8bb8\u4e0d\u53d8\u6027\u548c\u6e10\u8fd1\u4e00\u81f4\u6027\uff09\uff0c\u5c06\u5176\u4e0e\u4e92\u4fe1\u606f\u5173\u8054\u5e76\u63a8\u5bfc\u5c40\u90e8Fisher\u51e0\u4f55\u5c55\u5f00\uff0c\u4f7f\u7528\u6807\u51c6\u7ecf\u9a8c\u8fc7\u7a0b\u5de5\u5177\u5efa\u7acb\u4f30\u8ba1\u4fdd\u8bc1\u3002", "result": "\u5728\u53d7\u63a7\u56fe\u50cf\u548c\u4fe1\u53f7\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u5ea6\u91cf\u80fd\u591f\u6062\u590d\u7406\u8bba\u6392\u5e8f\u3001\u63ed\u793a\u88ab\u51c6\u786e\u6027\u63a9\u76d6\u7684\u8868\u793a\u5197\u4f59\uff0c\u5e76\u4e0e\u9c81\u68d2\u6027\u76f8\u5173\u3002", "conclusion": "\u89e3\u91ca\u6548\u7387\u662f\u4e00\u4e2a\u5b9e\u7528\u4e14\u7406\u8bba\u652f\u6301\u7684\u8868\u5f81\u8bbe\u8ba1\u8bca\u65ad\u5de5\u5177\uff0c\u4e3a\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u91cf\u5316\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2512.06232", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06232", "abs": "https://arxiv.org/abs/2512.06232", "authors": ["Ellen Su", "Solim Legris", "Todd M. Gureckis", "Mengye Ren"], "title": "Opinion: Learning Intuitive Physics May Require More than Visual Data", "comment": null, "summary": "Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u53d1\u5c55\u73b0\u5b9e\u4e3b\u4e49\u7684\u513f\u7ae5\u89c6\u89d2\u89c6\u9891\u6570\u636e\u96c6\uff08SAYCam\uff09\u8bad\u7ec3V-JEPA\u6a21\u578b\uff0c\u53d1\u73b0\u4ec5\u9760\u6570\u636e\u5206\u5e03\u7684\u53d8\u5316\uff08\u5373\u4f7f\u6570\u636e\u91cf\u4ec5\u4e3aSOTA\u6a21\u578b\u76840.01%\uff09\u65e0\u6cd5\u663e\u8457\u63d0\u5347\u76f4\u89c9\u7269\u7406\u63a8\u7406\u80fd\u529b\uff0c\u8868\u660e\u5f53\u524d\u67b6\u6784\u9700\u8981\u66f4\u6839\u672c\u7684\u6539\u8fdb\u3002", "motivation": "\u63a2\u7d22\u6570\u636e\u5206\u5e03\uff08\u800c\u975e\u6570\u636e\u91cf\uff09\u662f\u5426\u662f\u5b66\u4e60\u76f4\u89c9\u7269\u7406\u539f\u7406\u7684\u5173\u952e\uff0c\u901a\u8fc7\u4f7f\u7528\u53d1\u5c55\u73b0\u5b9e\u4e3b\u4e49\u7684\u513f\u7ae5\u89c6\u89d2\u89c6\u9891\u6765\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u3002", "method": "\u4f7f\u7528SAYCam\u6570\u636e\u96c6\uff08\u4e09\u4e2a\u513f\u7ae5\u7684\u65e5\u5e38\u89c6\u89c9\u4f53\u9a8c\uff09\u9884\u8bad\u7ec3Video Joint Embedding Predictive Architecture (V-JEPA)\u6a21\u578b\uff0c\u5e76\u5728IntPhys2\u57fa\u51c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u8bad\u7ec3\u7ed3\u679c\u672a\u5728IntPhys2\u57fa\u51c6\u4e0a\u663e\u793a\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u4ec5\u4f7f\u7528\u53d1\u5c55\u73b0\u5b9e\u4e3b\u4e49\u6570\u636e\u96c6\u4e0d\u8db3\u4ee5\u8ba9\u5f53\u524d\u67b6\u6784\u5b66\u4e60\u652f\u6301\u76f4\u89c9\u7269\u7406\u7684\u8868\u5f81\u3002", "conclusion": "\u4ec5\u6539\u53d8\u89c6\u89c9\u6570\u636e\u7684\u91cf\u548c\u5206\u5e03\u53ef\u80fd\u4e0d\u8db3\u4ee5\u6784\u5efa\u5177\u6709\u4eba\u5de5\u76f4\u89c9\u7269\u7406\u80fd\u529b\u7684\u7cfb\u7edf\uff0c\u9700\u8981\u66f4\u6839\u672c\u7684\u67b6\u6784\u6539\u8fdb\u3002"}}
{"id": "2512.06829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06829", "abs": "https://arxiv.org/abs/2512.06829", "authors": ["Oluwatimilehin Tijani", "Zhuo Chen", "Jiankang Deng", "Shan Luo"], "title": "MagicSkin: Balancing Marker and Markerless Modes in Vision-Based Tactile Sensors with a Translucent Skin", "comment": "Submitted to ICRA2026", "summary": "Vision-based tactile sensors (VBTS) face a fundamental trade-off in marker and markerless design on the tactile skin: opaque ink markers enable measurement of force and tangential displacement but completely occlude geometric features necessary for object and texture classification, while markerless skin preserves surface details but struggles in measuring tangential displacements effectively. Current practice to solve the above problem via UV lighting or virtual transfer using learning-based models introduces hardware complexity or computing burdens. This paper introduces MagicSkin, a novel tactile skin with translucent, tinted markers balancing the modes of marker and markerless for VBTS. It enables simultaneous tangential displacement tracking, force prediction, and surface detail preservation. This skin is easy to plug into GelSight-family sensors without requiring additional hardware or software tools. We comprehensively evaluate MagicSkin in downstream tasks. The translucent markers impressively enhance rather than degrade sensing performance compared with traditional markerless and inked marker design: it achieves best performance in object classification (99.17\\%), texture classification (93.51\\%), tangential displacement tracking (97\\% point retention) and force prediction (66\\% improvement in total force error). These experimental results demonstrate that translucent skin eliminates the traditional performance trade-off in marker or markerless modes, paving the way for multimodal tactile sensing essential in tactile robotics. See videos at this \\href{https://zhuochenn.github.io/MagicSkin_project/}{link}.", "AI": {"tldr": "MagicSkin\u662f\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u76ae\u80a4\uff0c\u91c7\u7528\u534a\u900f\u660e\u7740\u8272\u6807\u8bb0\uff0c\u5728\u6807\u8bb0\u548c\u65e0\u6807\u8bb0\u8bbe\u8ba1\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u5207\u5411\u4f4d\u79fb\u8ddf\u8e2a\u3001\u529b\u9884\u6d4b\u548c\u8868\u9762\u7ec6\u8282\u4fdd\u7559\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u8f6f\u4ef6\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u4e2d\u6807\u8bb0\u548c\u65e0\u6807\u8bb0\u8bbe\u8ba1\u7684\u6839\u672c\u6743\u8861\u95ee\u9898\uff1a\u4e0d\u900f\u660e\u58a8\u6c34\u6807\u8bb0\u80fd\u6d4b\u91cf\u529b\u548c\u5207\u5411\u4f4d\u79fb\u4f46\u5b8c\u5168\u906e\u6321\u51e0\u4f55\u7279\u5f81\uff0c\u800c\u65e0\u6807\u8bb0\u76ae\u80a4\u4fdd\u7559\u8868\u9762\u7ec6\u8282\u4f46\u96be\u4ee5\u6709\u6548\u6d4b\u91cf\u5207\u5411\u4f4d\u79fb\u3002", "method": "\u5f00\u53d1MagicSkin\u89e6\u89c9\u76ae\u80a4\uff0c\u4f7f\u7528\u534a\u900f\u660e\u7740\u8272\u6807\u8bb0\uff0c\u53ef\u76f4\u63a5\u63d2\u5165GelSight\u7cfb\u5217\u4f20\u611f\u5668\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u6216\u8f6f\u4ef6\u3002", "result": "\u5728\u7269\u4f53\u5206\u7c7b\uff0899.17%\uff09\u3001\u7eb9\u7406\u5206\u7c7b\uff0893.51%\uff09\u3001\u5207\u5411\u4f4d\u79fb\u8ddf\u8e2a\uff0897%\u70b9\u4fdd\u7559\uff09\u548c\u529b\u9884\u6d4b\uff08\u603b\u529b\u8bef\u5dee\u6539\u558466%\uff09\u65b9\u9762\u5747\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "conclusion": "\u534a\u900f\u660e\u76ae\u80a4\u6d88\u9664\u4e86\u6807\u8bb0\u6216\u65e0\u6807\u8bb0\u6a21\u5f0f\u7684\u4f20\u7edf\u6027\u80fd\u6743\u8861\uff0c\u4e3a\u89e6\u89c9\u673a\u5668\u4eba\u6240\u9700\u7684\u591a\u6a21\u6001\u89e6\u89c9\u611f\u77e5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.06343", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06343", "abs": "https://arxiv.org/abs/2512.06343", "authors": ["Tong Xie", "Andrew Bai", "Yuanhao Ban", "Yunqi Hong", "Haoyu Li", "Cho-jui Hsieh"], "title": "When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models", "comment": null, "summary": "Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86Bradley-Terry\u635f\u5931\u51fd\u6570\u5728\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5176\u68af\u5ea6\u8303\u6570\u53d7\u8868\u793a\u8ddd\u79bb\u5f71\u54cd\uff0c\u5bfc\u81f4\u5c0f\u8ddd\u79bb\u5bf9\u7684\u66f4\u65b0\u8fc7\u5f31\u800c\u5927\u8ddd\u79bb\u5bf9\u7684\u66f4\u65b0\u8fc7\u5f3a\u3002\u4f5c\u8005\u63d0\u51faNormBT\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u914d\u5bf9\u5f52\u4e00\u5316\u6765\u5e73\u8861\u8868\u793a\u9a71\u52a8\u6548\u5e94\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684Bradley-Terry\u635f\u5931\u51fd\u6570\u5728\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u65f6\u5b58\u5728\u68af\u5ea6\u4e0d\u5e73\u8861\u95ee\u9898\uff1a\u8868\u793a\u8ddd\u79bb\u5c0f\u7684\u914d\u5bf9\u5373\u4f7f\u8bef\u6392\u4e5f\u83b7\u5f97\u5fae\u5f31\u66f4\u65b0\uff0c\u800c\u8868\u793a\u8ddd\u79bb\u5927\u7684\u914d\u5bf9\u83b7\u5f97\u8fc7\u5f3a\u66f4\u65b0\uff0c\u8fd9\u5f71\u54cd\u4e86\u6a21\u578b\u5bf9\u7ec6\u5fae\u5dee\u522b\u7684\u5b66\u4e60\u80fd\u529b\u3002", "method": "\u63d0\u51faNormBT\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u81ea\u9002\u5e94\u914d\u5bf9\u5f52\u4e00\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u5e73\u8861\u8868\u793a\u9a71\u52a8\u6548\u5e94\uff0c\u5c06\u5b66\u4e60\u4fe1\u53f7\u96c6\u4e2d\u5728\u9884\u6d4b\u8bef\u5dee\u4e0a\u3002\u8be5\u65b9\u6cd5\u53ef\u76f4\u63a5\u66ff\u6362BT\u635f\u5931\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "result": "\u5728\u591a\u79cdLLM\u9aa8\u5e72\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\uff0cNormBT\u4e00\u81f4\u63d0\u5347\u4e86\u5956\u52b1\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728RewardBench\u7684\u63a8\u7406\u7c7b\u522b\u4e2d\u53d6\u5f97\u4e86\u8d85\u8fc75%\u7684\u663e\u8457\u589e\u76ca\uff0c\u8be5\u7c7b\u522b\u5305\u542b\u5927\u91cf\u5c0f\u8ddd\u79bb\u914d\u5bf9\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684BT\u76ee\u6807\u51fd\u6570\u7684\u5173\u952e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u4fee\u6b63\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u6539\u5584\u5956\u52b1\u6a21\u578b\u7684\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2512.06251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06251", "abs": "https://arxiv.org/abs/2512.06251", "authors": ["Fangzhou Lin", "Yuping Wang", "Yuliang Guo", "Zixun Huang", "Xinyu Huang", "Haichong Zhang", "Kazunori Yamada", "Zhengzhong Tu", "Liu Ren", "Ziming Zhang"], "title": "NexusFlow: Unifying Disparate Tasks under Partial Supervision via Invertible Flow Networks", "comment": "12 pages, 7 figures", "summary": "Partially Supervised Multi-Task Learning (PS-MTL) aims to leverage knowledge across tasks when annotations are incomplete. Existing approaches, however, have largely focused on the simpler setting of homogeneous, dense prediction tasks, leaving the more realistic challenge of learning from structurally diverse tasks unexplored. To this end, we introduce NexusFlow, a novel, lightweight, and plug-and-play framework effective in both settings. NexusFlow introduces a set of surrogate networks with invertible coupling layers to align the latent feature distributions of tasks, creating a unified representation that enables effective knowledge transfer. The coupling layers are bijective, preserving information while mapping features into a shared canonical space. This invertibility avoids representational collapse and enables alignment across structurally different tasks without reducing expressive capacity. We first evaluate NexusFlow on the core challenge of domain-partitioned autonomous driving, where dense map reconstruction and sparse multi-object tracking are supervised in different geographic regions, creating both structural disparity and a strong domain gap. NexusFlow sets a new state-of-the-art result on nuScenes, outperforming strong partially supervised baselines. To demonstrate generality, we further test NexusFlow on NYUv2 using three homogeneous dense prediction tasks, segmentation, depth, and surface normals, as a representative N-task PS-MTL scenario. NexusFlow yields consistent gains across all tasks, confirming its broad applicability.", "AI": {"tldr": "NexusFlow\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u4efb\u52a1\u4e0b\u7684\u90e8\u5206\u76d1\u7763\u591a\u4efb\u52a1\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u9006\u8026\u5408\u5c42\u5bf9\u9f50\u4efb\u52a1\u7279\u5f81\u5206\u5e03\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5ba4\u5185\u573a\u666f\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u540c\u6784\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u800c\u5ffd\u7565\u4e86\u7ed3\u6784\u591a\u6837\u5316\u4efb\u52a1\u7684\u5b66\u4e60\u6311\u6218\u3002\u73b0\u5b9e\u573a\u666f\u4e2d\u4efb\u52a1\u5f80\u5f80\u5177\u6709\u7ed3\u6784\u5dee\u5f02\u548c\u6807\u6ce8\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5e26\u6709\u53ef\u9006\u8026\u5408\u5c42\u7684\u4ee3\u7406\u7f51\u7edc\u6765\u5bf9\u9f50\u4efb\u52a1\u6f5c\u5728\u7279\u5f81\u5206\u5e03\uff0c\u521b\u5efa\u7edf\u4e00\u8868\u793a\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8de8\u4efb\u52a1\u77e5\u8bc6\u8fc1\u79fb\u3002\u53ef\u9006\u6027\u907f\u514d\u4e86\u8868\u793a\u5d29\u6e83\uff0c\u4fdd\u6301\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728nuScenes\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728NYUv2\u5ba4\u5185\u573a\u666f\u6570\u636e\u96c6\u4e0a\u6240\u6709\u4efb\u52a1\u5747\u53d6\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "NexusFlow\u80fd\u591f\u6709\u6548\u5904\u7406\u7ed3\u6784\u591a\u6837\u5316\u4efb\u52a1\u7684\u90e8\u5206\u76d1\u7763\u5b66\u4e60\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u591a\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06868", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06868", "abs": "https://arxiv.org/abs/2512.06868", "authors": ["Xingguang Zhong", "Liren Jin", "Marija Popovi\u0107", "Jens Behley", "Cyrill Stachniss"], "title": "Dynamic Visual SLAM using a General 3D Prior", "comment": "8 pages", "summary": "Reliable incremental estimation of camera poses and 3D reconstruction is key to enable various applications including robotics, interactive visualization, and augmented reality. However, this task is particularly challenging in dynamic natural environments, where scene dynamics can severely deteriorate camera pose estimation accuracy. In this work, we propose a novel monocular visual SLAM system that can robustly estimate camera poses in dynamic scenes. To this end, we leverage the complementary strengths of geometric patch-based online bundle adjustment and recent feed-forward reconstruction models. Specifically, we propose a feed-forward reconstruction model to precisely filter out dynamic regions, while also utilizing its depth prediction to enhance the robustness of the patch-based visual SLAM. By aligning depth prediction with estimated patches from bundle adjustment, we robustly handle the inherent scale ambiguities of the batch-wise application of the feed-forward reconstruction model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5355\u76ee\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u52a8\u6001\u573a\u666f\u4e2d\u9c81\u68d2\u5730\u4f30\u8ba1\u76f8\u673a\u4f4d\u59ff\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u8865\u4e01\u7684\u5728\u7ebf\u675f\u8c03\u6574\u548c\u524d\u9988\u91cd\u5efa\u6a21\u578b\u6765\u8fc7\u6ee4\u52a8\u6001\u533a\u57df\u5e76\u589e\u5f3aSLAM\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u81ea\u7136\u73af\u5883\u4e2d\uff0c\u573a\u666f\u52a8\u6001\u4f1a\u4e25\u91cd\u5f71\u54cd\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u800c\u53ef\u9760\u7684\u589e\u91cf\u5f0f\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c3D\u91cd\u5efa\u5bf9\u4e8e\u673a\u5668\u4eba\u6280\u672f\u3001\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u548c\u589e\u5f3a\u73b0\u5b9e\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u51e0\u4f55\u8865\u4e01\u7684\u5728\u7ebf\u675f\u8c03\u6574\u548c\u524d\u9988\u91cd\u5efa\u6a21\u578b\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u63d0\u51fa\u524d\u9988\u91cd\u5efa\u6a21\u578b\u7cbe\u786e\u8fc7\u6ee4\u52a8\u6001\u533a\u57df\uff0c\u540c\u65f6\u5229\u7528\u5176\u6df1\u5ea6\u9884\u6d4b\u589e\u5f3a\u57fa\u4e8e\u8865\u4e01\u7684\u89c6\u89c9SLAM\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5c06\u6df1\u5ea6\u9884\u6d4b\u4e0e\u675f\u8c03\u6574\u4f30\u8ba1\u7684\u8865\u4e01\u5bf9\u9f50\uff0c\u9c81\u68d2\u5904\u7406\u524d\u9988\u91cd\u5efa\u6a21\u578b\u6279\u91cf\u5e94\u7528\u7684\u56fa\u6709\u5c3a\u5ea6\u6a21\u7cca\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u6709\u6548\u5904\u7406\u573a\u666f\u52a8\u6001\u5e26\u6765\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u89c6\u89c9SLAM\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2512.06347", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06347", "abs": "https://arxiv.org/abs/2512.06347", "authors": ["Naoki Yoshida", "Isao Ishikawa", "Masaaki Imaizumi"], "title": "Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry", "comment": null, "summary": "We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.", "AI": {"tldr": "\u5728\u5e08\u751f\u6846\u67b6\u4e0b\uff0c\u7406\u8bba\u8bc1\u660e\u5f53\u8bad\u7ec3\u6837\u672c\u6570\u8d85\u8fc7\u67d0\u4e2a\u9608\u503c\u65f6\uff0c\u63d2\u503c\u5668\u7684\u6cdb\u5316\u8bef\u5dee\u53d8\u4e3a0\uff0c\u8fd9\u4e3b\u8981\u6e90\u4e8e\u63d2\u503c\u5668\u96c6\u5408\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\u7279\u6027\u3002", "motivation": "\u7406\u89e3\u5927\u89c4\u6a21\u6a21\u578b\uff08\u5982\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u9ad8\u6cdb\u5316\u80fd\u529b\u662f\u673a\u5668\u5b66\u4e60\u7406\u8bba\u7684\u6838\u5fc3\u5f00\u653e\u95ee\u9898\u3002\u73b0\u6709\u7814\u7a76\u5c06\u8fd9\u79cd\u73b0\u8c61\u5f52\u56e0\u4e8eSGD\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u4f46\u5b9e\u8bc1\u8868\u660e\u6a21\u578b\u672c\u8eab\u7279\u6027\uff08\u5982\u968f\u673a\u91c7\u6837\u7684\u63d2\u503c\u5668\u4e5f\u80fd\u6709\u6548\u6cdb\u5316\uff09\u662f\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u91c7\u7528\u5e08\u751f\u6846\u67b6\uff0c\u5229\u7528\u4ee3\u6570\u51e0\u4f55\u5de5\u5177\u6570\u5b66\u523b\u753b\u63d2\u503c\u5668\u96c6\u5408\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u8bc1\u660e\u6cdb\u5316\u8bef\u5dee\u5728\u8bad\u7ec3\u6837\u672c\u6570\u8d85\u8fc7\u9608\u503c\u65f6\u53d8\u4e3a0\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u968f\u673a\u91c7\u6837\u63d2\u503c\u5668\u7684\u6cdb\u5316\u8bef\u5dee\u5728\u8bad\u7ec3\u6837\u672c\u6570\u8d85\u8fc7\u7531\u53c2\u6570\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u51b3\u5b9a\u7684\u9608\u503c\u65f6\u7cbe\u786e\u53d8\u4e3a0\u3002", "conclusion": "\u6a21\u578b\u63d2\u503c\u5668\u7684\u6cdb\u5316\u80fd\u529b\u4e3b\u8981\u6e90\u4e8e\u5176\u53c2\u6570\u7a7a\u95f4\u7684\u51e0\u4f55\u7279\u6027\uff0c\u800c\u975e\u4f18\u5316\u7b97\u6cd5\u7684\u9690\u5f0f\u504f\u7f6e\uff0c\u8fd9\u4e3a\u7406\u89e3\u5927\u89c4\u6a21\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2512.06255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06255", "abs": "https://arxiv.org/abs/2512.06255", "authors": ["Shijie Wang", "Xin Yu", "Yadan Luo", "Zijian Wang", "Pengfei Zhang", "Zi Huang"], "title": "Language-driven Fine-grained Retrieval", "comment": null, "summary": "Existing fine-grained image retrieval (FGIR) methods learn discriminative embeddings by adopting semantically sparse one-hot labels derived from category names as supervision. While effective on seen classes, such supervision overlooks the rich semantics encoded in category names, hindering the modeling of comparability among cross-category details and, in turn, limiting generalization to unseen categories. To tackle this, we introduce LaFG, a Language-driven framework for Fine-Grained Retrieval that converts class names into attribute-level supervision using large language models (LLMs) and vision-language models (VLMs). Treating each name as a semantic anchor, LaFG prompts an LLM to generate detailed, attribute-oriented descriptions. To mitigate attribute omission in these descriptions, it leverages a frozen VLM to project them into a vision-aligned space, clustering them into a dataset-wide attribute vocabulary while harvesting complementary attributes from related categories. Leveraging this vocabulary, a global prompt template selects category-relevant attributes, which are aggregated into category-specific linguistic prototypes. These prototypes supervise the retrieval model to steer", "AI": {"tldr": "LaFG\u662f\u4e00\u4e2a\u8bed\u8a00\u9a71\u52a8\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7LLM\u548cVLM\u5c06\u7c7b\u522b\u540d\u79f0\u8f6c\u6362\u4e3a\u5c5e\u6027\u7ea7\u76d1\u7763\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u4f7f\u7528\u7a00\u758f\u7684one-hot\u6807\u7b7e\u4f5c\u4e3a\u76d1\u7763\uff0c\u5ffd\u7565\u4e86\u7c7b\u522b\u540d\u79f0\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u8de8\u7c7b\u522b\u7ec6\u8282\u53ef\u6bd4\u6027\u5efa\u6a21\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u4f7f\u7528LLM\u5c06\u7c7b\u522b\u540d\u79f0\u751f\u6210\u8be6\u7ec6\u7684\u5c5e\u6027\u5bfc\u5411\u63cf\u8ff0\uff1b2. \u5229\u7528\u51bb\u7ed3\u7684VLM\u5c06\u63cf\u8ff0\u6295\u5f71\u5230\u89c6\u89c9\u5bf9\u9f50\u7a7a\u95f4\uff0c\u805a\u7c7b\u5f62\u6210\u6570\u636e\u96c6\u7ea7\u5c5e\u6027\u8bcd\u6c47\u8868\uff1b3. \u901a\u8fc7\u5168\u5c40\u63d0\u793a\u6a21\u677f\u9009\u62e9\u7c7b\u522b\u76f8\u5173\u5c5e\u6027\uff0c\u805a\u5408\u6210\u7c7b\u522b\u7279\u5b9a\u7684\u8bed\u8a00\u539f\u578b\uff1b4. \u7528\u8fd9\u4e9b\u539f\u578b\u76d1\u7763\u68c0\u7d22\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21\u8de8\u7c7b\u522b\u7ec6\u8282\u7684\u53ef\u6bd4\u6027\uff0c\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "LaFG\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u7684\u5c5e\u6027\u7ea7\u76d1\u7763\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u7ec6\u7c92\u5ea6\u56fe\u50cf\u68c0\u7d22\u65b9\u6cd5\u5728\u8bed\u4e49\u5efa\u6a21\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u68c0\u7d22\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06892", "abs": "https://arxiv.org/abs/2512.06892", "authors": ["Hassan Jardali", "Durgakant Pushp", "Youwei Yu", "Mahmoud Ali", "Ihab S. Mohamed", "Alejandro Murillo-Gonzalez", "Paul D. Coen", "Md. Al-Masrur Khan", "Reddy Charan Pulivendula", "Saeoul Park", "Lingchuan Zhou", "Lantao Liu"], "title": "From Zero to High-Speed Racing: An Autonomous Racing Stack", "comment": null, "summary": "High-speed, head-to-head autonomous racing presents substantial technical and logistical challenges, including precise localization, rapid perception, dynamic planning, and real-time control-compounded by limited track access and costly hardware. This paper introduces the Autonomous Race Stack (ARS), developed by the IU Luddy Autonomous Racing team for the Indy Autonomous Challenge (IAC). We present three iterations of our ARS, each validated on different tracks and achieving speeds up to 260 km/h. Our contributions include: (i) the modular architecture and evolution of the ARS across ARS1, ARS2, and ARS3; (ii) a detailed performance evaluation that contrasts control, perception, and estimation across oval and road-course environments; and (iii) the release of a high-speed, multi-sensor dataset collected from oval and road-course tracks. Our findings highlight the unique challenges and insights from real-world high-speed full-scale autonomous racing.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u81ea\u4e3b\u8d5b\u8f66\u5806\u6808\uff08ARS\uff09\u7684\u5f00\u53d1\uff0c\u8be5\u5806\u6808\u5728\u692d\u5706\u8d5b\u9053\u548c\u516c\u8def\u8d5b\u9053\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe260\u516c\u91cc/\u5c0f\u65f6\u7684\u901f\u5ea6\uff0c\u5e76\u53d1\u5e03\u4e86\u9ad8\u901f\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\u3002", "motivation": "\u9ad8\u901f\u3001\u5934\u5bf9\u5934\u81ea\u4e3b\u8d5b\u8f66\u9762\u4e34\u7cbe\u786e\u5b9a\u4f4d\u3001\u5feb\u901f\u611f\u77e5\u3001\u52a8\u6001\u89c4\u5212\u548c\u5b9e\u65f6\u63a7\u5236\u7b49\u6280\u672f\u6311\u6218\uff0c\u4e14\u53d7\u9650\u4e8e\u8d5b\u9053\u8bbf\u95ee\u548c\u6602\u8d35\u786c\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u4e2a\u7248\u672c\u7684\u81ea\u4e3b\u8d5b\u8f66\u5806\u6808\uff08ARS1\u3001ARS2\u3001ARS3\uff09\uff0c\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5e76\u5728\u4e0d\u540c\u8d5b\u9053\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "ARS\u5728\u692d\u5706\u8d5b\u9053\u548c\u516c\u8def\u8d5b\u9053\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe260\u516c\u91cc/\u5c0f\u65f6\u7684\u901f\u5ea6\uff0c\u5e76\u8fdb\u884c\u4e86\u63a7\u5236\u3001\u611f\u77e5\u548c\u4f30\u8ba1\u7684\u6027\u80fd\u8bc4\u4f30\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u771f\u5b9e\u4e16\u754c\u9ad8\u901f\u5168\u5c3a\u5bf8\u81ea\u4e3b\u8d5b\u8f66\u7684\u72ec\u7279\u6311\u6218\u548c\u89c1\u89e3\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u96c6\u3002"}}
{"id": "2512.06351", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06351", "abs": "https://arxiv.org/abs/2512.06351", "authors": ["Zhiying Yang", "Fang Liu", "Wei Zhang", "Xin Lou", "Malcolm Yoke Hean Low", "Boon Ping Gan"], "title": "LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing", "comment": null, "summary": "This paper presents \\textsc{Luca}, a \\underline{l}arge language model (LLM)-\\underline{u}pgraded graph reinforcement learning framework for \\underline{c}arbon-\\underline{a}ware flexible job shop scheduling. \\textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \\textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \\textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\\% and up to 12.2\\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \\textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLuca\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u78b3\u611f\u77e5\u7684\u67d4\u6027\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\uff0c\u5728\u4fdd\u8bc1\u6392\u653e\u6c34\u5e73\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5b8c\u5de5\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u5236\u9020\u7cfb\u7edf\u4e2d\u52a8\u6001\u548c\u53ef\u6301\u7eed\u8c03\u5ea6\u7684\u6311\u6218\uff0c\u9700\u8981\u5728\u4f18\u5316\u5b8c\u5de5\u65f6\u95f4\u7684\u540c\u65f6\u8003\u8651\u78b3\u6392\u653e\u76ee\u6807\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u548cLLM\u751f\u6210\u878d\u5408\u5d4c\u5165\uff0c\u6355\u6349\u8c03\u5ea6\u72b6\u6001\u7684\u7ed3\u6784\u7279\u5f81\u548c\u4e0a\u4e0b\u6587\u8bed\u4e49\uff0c\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7f51\u7edc\u751f\u6210\u5b9e\u65f6\u8c03\u5ea6\u51b3\u7b56\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u964d\u4f4e\u5b8c\u5de5\u65f6\u95f44.1%\uff0c\u6700\u9ad8\u8fbe12.2%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u6392\u653e\u6c34\u5e73\uff1b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5bf9\u5b8c\u5de5\u65f6\u95f4\u548c\u6392\u653e\u5747\u6709\u989d\u5916\u6539\u5584\u3002", "conclusion": "Luca\u6846\u67b6\u5728\u667a\u80fd\u5236\u9020\u78b3\u611f\u77e5\u8c03\u5ea6\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06258", "abs": "https://arxiv.org/abs/2512.06258", "authors": ["Chaoyang Wang", "Yangfan He", "Yiyang Zhou", "Yixuan Wang", "Jiaqi Liu", "Peng Xia", "Zhengzhong Tu", "Mohit Bansal", "Huaxiu Yao"], "title": "Knowing the Answer Isn't Enough: Fixing Reasoning Path Failures in LVLMs", "comment": null, "summary": "We reveal a critical yet underexplored flaw in Large Vision-Language Models (LVLMs): even when these models know the correct answer, they frequently arrive there through incorrect reasoning paths. The core issue is not a lack of knowledge, but a path selection bias within the vast reasoning search space. Although LVLMs are often capable of sampling correct solution trajectories, they disproportionately favor unstable or logically inconsistent ones, leading to erratic and unreliable outcomes. The substantial disparity between Pass@K (with large K) and Pass@1 across numerous models provides compelling evidence that such failures primarily stem from misreasoning rather than ignorance. To systematically investigate and address this issue, we propose PSO (Path-Select Optimization), a two-stage post-training framework designed to enhance both the reasoning performance and stability of existing LVLMs. In the first stage, we employ Group Relative Policy Optimization (GRPO) with template and answer-based rewards to cultivate structured, step-by-step reasoning. In the second stage, we conduct online preference optimization, where the model samples reasoning paths from GRPO-generated data, self-evaluates them, and aligns itself toward the preferred trajectories. Incorrect or suboptimal paths are concurrently stored in a Negative Replay Memory (NRM) as hard negatives, which are periodically revisited to prevent the model from repeating prior mistakes and to facilitate continual reasoning refinement. Extensive experiments show that PSO effectively prunes invalid reasoning paths, substantially enhances reasoning accuracy (with 7.4% improvements on average), and yields more stable and consistent chains of thought. Our code will be available at https://github.com/aiming-lab/PSO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5b58\u5728\u8def\u5f84\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u5373\u6a21\u578b\u5373\u4f7f\u77e5\u9053\u6b63\u786e\u7b54\u6848\uff0c\u4e5f\u503e\u5411\u4e8e\u9009\u62e9\u4e0d\u7a33\u5b9a\u7684\u63a8\u7406\u8def\u5f84\u3002\u4f5c\u8005\u63d0\u51fa\u4e86PSO\uff08\u8def\u5f84\u9009\u62e9\u4f18\u5316\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6765\u63d0\u5347\u63a8\u7406\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709LVLMs\u5b58\u5728\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u7f3a\u9677\uff1a\u6a21\u578b\u5373\u4f7f\u62e5\u6709\u6b63\u786e\u7b54\u6848\uff0c\u4e5f\u7ecf\u5e38\u901a\u8fc7\u9519\u8bef\u7684\u63a8\u7406\u8def\u5f84\u5f97\u51fa\u7ed3\u679c\u3002\u6838\u5fc3\u95ee\u9898\u4e0d\u662f\u77e5\u8bc6\u7f3a\u4e4f\uff0c\u800c\u662f\u63a8\u7406\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u8def\u5f84\u9009\u62e9\u504f\u5dee\u3002", "method": "\u63d0\u51faPSO\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528GRPO\uff08\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u7ed3\u5408\u6a21\u677f\u548c\u7b54\u6848\u5956\u52b1\u6765\u57f9\u517b\u7ed3\u6784\u5316\u9010\u6b65\u63a8\u7406\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u5728\u7ebf\u504f\u597d\u4f18\u5316\uff0c\u6a21\u578b\u4eceGRPO\u751f\u6210\u7684\u6570\u636e\u4e2d\u91c7\u6837\u63a8\u7406\u8def\u5f84\uff0c\u81ea\u6211\u8bc4\u4f30\u5e76\u9009\u62e9\u504f\u597d\u8f68\u8ff9\uff0c\u540c\u65f6\u5c06\u9519\u8bef\u8def\u5f84\u5b58\u50a8\u5728\u8d1f\u5411\u56de\u653e\u5185\u5b58\u4e2d\u9632\u6b62\u91cd\u590d\u9519\u8bef\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPSO\u6709\u6548\u4fee\u526a\u65e0\u6548\u63a8\u7406\u8def\u5f84\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\uff08\u5e73\u5747\u63d0\u53477.4%\uff09\uff0c\u5e76\u4ea7\u751f\u66f4\u7a33\u5b9a\u4e00\u81f4\u7684\u601d\u7ef4\u94fe\u3002", "conclusion": "PSO\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LVLMs\u7684\u8def\u5f84\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u63a8\u7406\u8def\u5f84\u9009\u62e9\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.06896", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06896", "abs": "https://arxiv.org/abs/2512.06896", "authors": ["Chrysostomos Karakasis", "Camryn Scully", "Robert Salati", "Panagiotis Artemiadis"], "title": "Control of Powered Ankle-Foot Prostheses on Compliant Terrain: A Quantitative Approach to Stability Enhancement", "comment": null, "summary": "Walking on compliant terrain presents a substantial challenge for individuals with lower-limb amputation, further elevating their already high risk of falling. While powered ankle-foot prostheses have demonstrated adaptability across speeds and rigid terrains, control strategies optimized for soft or compliant surfaces remain underexplored. This work experimentally validates an admittance-based control strategy that dynamically adjusts the quasi-stiffness of powered prostheses to enhance gait stability on compliant ground. Human subject experiments were conducted with three healthy individuals walking on two bilaterally compliant surfaces with ground stiffness values of 63 and 25 kN/m, representative of real-world soft environments. Controller performance was quantified using phase portraits and two walking stability metrics, offering a direct assessment of fall risk. Compared to a standard phase-variable controller developed for rigid terrain, the proposed admittance controller consistently improved gait stability across all compliant conditions. These results demonstrate the potential of adaptive, stability-aware prosthesis control to reduce fall risk in real-world environments and advance the robustness of human-prosthesis interaction in rehabilitation robotics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bfc\u7eb3\u7684\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u52a8\u529b\u5047\u80a2\u7684\u51c6\u521a\u5ea6\u6765\u589e\u5f3a\u5728\u67d4\u987a\u5730\u9762\u4e0a\u7684\u6b65\u6001\u7a33\u5b9a\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u521a\u6027\u5730\u9762\u63a7\u5236\u5668\u80fd\u663e\u8457\u6539\u5584\u6b65\u6001\u7a33\u5b9a\u6027\u5e76\u964d\u4f4e\u8dcc\u5012\u98ce\u9669\u3002", "motivation": "\u4e0b\u80a2\u622a\u80a2\u8005\u5728\u67d4\u987a\u5730\u5f62\u4e0a\u884c\u8d70\u65f6\u8dcc\u5012\u98ce\u9669\u6781\u9ad8\uff0c\u800c\u73b0\u6709\u52a8\u529b\u8e1d\u8db3\u5047\u80a2\u7684\u63a7\u5236\u7b56\u7565\u4e3b\u8981\u9488\u5bf9\u521a\u6027\u5730\u9762\u4f18\u5316\uff0c\u5bf9\u8f6f\u8d28\u6216\u67d4\u987a\u8868\u9762\u7684\u9002\u5e94\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5bfc\u7eb3\u7684\u63a7\u5236\u7b56\u7565\u52a8\u6001\u8c03\u6574\u52a8\u529b\u5047\u80a2\u7684\u51c6\u521a\u5ea6\uff0c\u901a\u8fc7\u4eba\u4f53\u5b9e\u9a8c\u5728\u4e24\u79cd\u521a\u5ea6\u503c\uff0863\u548c25 kN/m\uff09\u7684\u53cc\u4fa7\u67d4\u987a\u8868\u9762\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4f7f\u7528\u76f8\u56fe\u548c\u4e24\u79cd\u6b65\u6001\u7a33\u5b9a\u6027\u6307\u6807\u91cf\u5316\u63a7\u5236\u5668\u6027\u80fd\u3002", "result": "\u4e0e\u9488\u5bf9\u521a\u6027\u5730\u9762\u8bbe\u8ba1\u7684\u6807\u51c6\u76f8\u4f4d\u53d8\u91cf\u63a7\u5236\u5668\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u5bfc\u7eb3\u63a7\u5236\u5668\u5728\u6240\u6709\u67d4\u987a\u6761\u4ef6\u4e0b\u90fd\u80fd\u4e00\u81f4\u5730\u6539\u5584\u6b65\u6001\u7a33\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u9002\u5e94\u3001\u5173\u6ce8\u7a33\u5b9a\u6027\u7684\u5047\u80a2\u63a7\u5236\u5728\u964d\u4f4e\u73b0\u5b9e\u73af\u5883\u4e2d\u8dcc\u5012\u98ce\u9669\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5eb7\u590d\u673a\u5668\u4eba\u4e2d\u4eba-\u5047\u80a2\u4ea4\u4e92\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.06356", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.06356", "abs": "https://arxiv.org/abs/2512.06356", "authors": ["Yifan Song", "Fenglin Yu", "Yihong Luo", "Xingjian Tao", "Siya Qiu", "Kai Han", "Jing Tang"], "title": "DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction", "comment": null, "summary": "Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.", "AI": {"tldr": "DDFI\u662f\u4e00\u79cd\u9488\u5bf9\u56fe\u795e\u7ecf\u7f51\u7edc\u4e2d\u8282\u70b9\u7279\u5f81\u7f3a\u5931\u95ee\u9898\u7684\u591a\u6837\u5316\u3001\u5206\u5e03\u611f\u77e5\u7684\u7279\u5f81\u8865\u5168\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7279\u5f81\u4f20\u64ad\u548c\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u975e\u5168\u8fde\u63a5\u56fe\u3001\u8fc7\u5e73\u6ed1\u95ee\u9898\u548c\u5f52\u7eb3\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u56fe\u6570\u636e\u5e38\u5b58\u5728\u8282\u70b9\u7279\u5f81\u7f3a\u5931\u95ee\u9898\uff0c\u4f20\u7edf\u7279\u5f81\u4f20\u64ad\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u96be\u4ee5\u5904\u7406\u975e\u5168\u8fde\u63a5\u56fe\uff1b2\uff09\u8865\u5168\u7279\u5f81\u9762\u4e34\u8fc7\u5e73\u6ed1\u95ee\u9898\uff1b3\uff09\u4ec5\u9002\u7528\u4e8e\u76f4\u63a8\u5f0f\u4efb\u52a1\uff0c\u5ffd\u7565\u5f52\u7eb3\u4efb\u52a1\u4e2d\u7684\u7279\u5f81\u5206\u5e03\u504f\u79fb\u3002", "method": "DDFI\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u6b65\u8868\u793a\u751f\u6210\u8fc7\u7a0b\uff1a\u9996\u5148\u8bbe\u8ba1\u540c\u6807\u7b7e\u94fe\u63a5\u7b97\u6cd5\u589e\u5f3a\u975e\u5168\u8fde\u63a5\u56fe\u7684\u8fde\u901a\u6027\uff0c\u7136\u540e\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u5b8c\u6574\u7684\u63a9\u7801\u81ea\u7f16\u7801\u5668\u91cd\u6784\u7279\u5f81\uff0c\u51cf\u5c11\u5206\u5e03\u504f\u79fb\u5e76\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u65b0\u6536\u96c6\u7684Sailing\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDDFI\u5728\u76f4\u63a8\u5f0f\u548c\u5f52\u7eb3\u5f0f\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "DDFI\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u8865\u5168\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u6570\u636e\u4e2d\u7279\u5f81\u7f3a\u5931\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u975e\u5168\u8fde\u63a5\u56fe\u548c\u5f52\u7eb3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u56fe\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06269", "abs": "https://arxiv.org/abs/2512.06269", "authors": ["Quan Tran", "Tuan Dang"], "title": "TriaGS: Differentiable Triangulation-Guided Geometric Consistency for 3D Gaussian Splatting", "comment": "10 pages", "summary": "3D Gaussian Splatting is crucial for real-time novel view synthesis due to its efficiency and ability to render photorealistic images. However, building a 3D Gaussian is guided solely by photometric loss, which can result in inconsistencies in reconstruction. This under-constrained process often results in \"floater\" artifacts and unstructured geometry, preventing the extraction of high-fidelity surfaces. To address this issue, our paper introduces a novel method that improves reconstruction by enforcing global geometry consistency through constrained multi-view triangulation. Our approach aims to achieve a consensus on 3D representation in the physical world by utilizing various estimated views. We optimize this process by penalizing the deviation of a rendered 3D point from a robust consensus point, which is re-triangulated from a bundle of neighboring views in a self-supervised fashion. We demonstrate the effectiveness of our method across multiple datasets, achieving state-of-the-art results. On the DTU dataset, our method attains a mean Chamfer Distance of 0.50 mm, outperforming comparable explicit methods. We will make our code open-source to facilitate community validation and ensure reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u7ea6\u675f\u591a\u89c6\u56fe\u4e09\u89d2\u6d4b\u91cf\u6765\u589e\u5f3a3D\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4ec5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u5bfc\u81f4\u7684\u6d6e\u70b9\u4f2a\u5f71\u548c\u65e0\u7ed3\u6784\u51e0\u4f55\u95ee\u9898\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u4ec5\u4f9d\u8d56\u5149\u5ea6\u635f\u5931\u8fdb\u884c\u91cd\u5efa\uff0c\u5bfc\u81f4\u91cd\u5efa\u4e0d\u4e00\u81f4\u3001\u6d6e\u70b9\u4f2a\u5f71\u548c\u51e0\u4f55\u7ed3\u6784\u4e0d\u6e05\u6670\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9ad8\u4fdd\u771f\u8868\u9762\u7684\u63d0\u53d6\u3002", "method": "\u901a\u8fc7\u81ea\u76d1\u7763\u65b9\u5f0f\u5229\u7528\u76f8\u90bb\u89c6\u56fe\u91cd\u65b0\u4e09\u89d2\u6d4b\u91cf\u751f\u6210\u9c81\u68d2\u5171\u8bc6\u70b9\uff0c\u5e76\u60e9\u7f5a\u6e32\u67d33D\u70b9\u4e0e\u5171\u8bc6\u70b9\u7684\u504f\u5dee\uff0c\u5f3a\u5236\u5b9e\u73b0\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5728DTU\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.50mm\u7684\u5e73\u5747\u5012\u89d2\u8ddd\u79bb\uff0c\u4f18\u4e8e\u540c\u7c7b\u663e\u5f0f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u91cd\u5efa\u8d28\u91cf\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u793e\u533a\u9a8c\u8bc1\u548c\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2512.06897", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06897", "abs": "https://arxiv.org/abs/2512.06897", "authors": ["Bradley Hobbs", "Panagiotis Artemiadis"], "title": "Ground Compliance Improves Retention of Visual Feedback-Based Propulsion Training for Gait Rehabilitation", "comment": null, "summary": "This study investigates whether adding ground compliance to visual feedback (VF) gait training is more effective at increasing push-off force (POF) compared to using VF alone, with implications for gait rehabilitation. Ten healthy participants walked on a custom split-belt treadmill. All participants received real-time visual feedback of their ground reaction forces. One group also experienced changes in ground compliance, while a control group received only visual feedback. Intentional increases in propulsive ground reaction forces (POF) were successfully achieved and sustained post-intervention, especially in the group that experienced ground compliance. This group also demonstrated lasting after-effects in muscle activity and joint kinematics, indicating a more robust learning of natural strategies to increase propulsion. This work demonstrates how visual and proprioceptive systems coordinate during gait adaptation. It uniquely shows that combining ground compliance with visual feedback enhances the learning of propulsive forces, supporting the potential use of compliant terrain in long-term rehabilitation targeting propulsion deficits, such as those following a stroke.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u89c6\u89c9\u53cd\u9988\u6b65\u6001\u8bad\u7ec3\u4e2d\u52a0\u5165\u5730\u9762\u987a\u5e94\u6027\u6bd4\u5355\u72ec\u4f7f\u7528\u89c6\u89c9\u53cd\u9988\u66f4\u80fd\u6709\u6548\u589e\u52a0\u63a8\u79bb\u529b\uff0c\u5bf9\u6b65\u6001\u5eb7\u590d\u6709\u79ef\u6781\u5f71\u54cd\u3002", "motivation": "\u63a2\u7a76\u89c6\u89c9\u53cd\u9988\u4e0e\u5730\u9762\u987a\u5e94\u6027\u7ed3\u5408\u662f\u5426\u80fd\u66f4\u6709\u6548\u5730\u589e\u5f3a\u63a8\u79bb\u529b\uff0c\u4e3a\u6b65\u6001\u5eb7\u590d\uff08\u5982\u4e2d\u98ce\u540e\u60a3\u8005\uff09\u63d0\u4f9b\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "10\u540d\u5065\u5eb7\u53c2\u4e0e\u8005\u5728\u5b9a\u5236\u5206\u5e26\u8dd1\u6b65\u673a\u4e0a\u884c\u8d70\uff0c\u6240\u6709\u53c2\u4e0e\u8005\u63a5\u53d7\u5b9e\u65f6\u5730\u9762\u53cd\u4f5c\u7528\u529b\u89c6\u89c9\u53cd\u9988\uff0c\u5176\u4e2d\u4e00\u7ec4\u8fd8\u4f53\u9a8c\u5730\u9762\u987a\u5e94\u6027\u53d8\u5316\uff0c\u5bf9\u7167\u7ec4\u4ec5\u63a5\u53d7\u89c6\u89c9\u53cd\u9988\u3002", "result": "\u4f53\u9a8c\u5730\u9762\u987a\u5e94\u6027\u7684\u7ec4\u522b\u6210\u529f\u5b9e\u73b0\u5e76\u7ef4\u6301\u4e86\u63a8\u79bb\u529b\u7684\u589e\u52a0\uff0c\u4e14\u5728\u808c\u8089\u6d3b\u52a8\u548c\u5173\u8282\u8fd0\u52a8\u5b66\u4e0a\u8868\u73b0\u51fa\u6301\u4e45\u7684\u540e\u6548\uff0c\u8868\u660e\u66f4\u7a33\u5065\u7684\u81ea\u7136\u63a8\u8fdb\u7b56\u7565\u5b66\u4e60\u3002", "conclusion": "\u89c6\u89c9\u4e0e\u672c\u4f53\u611f\u89c9\u7cfb\u7edf\u5728\u6b65\u6001\u9002\u5e94\u4e2d\u534f\u540c\u5de5\u4f5c\uff0c\u7ed3\u5408\u5730\u9762\u987a\u5e94\u6027\u4e0e\u89c6\u89c9\u53cd\u9988\u53ef\u589e\u5f3a\u63a8\u8fdb\u529b\u5b66\u4e60\uff0c\u652f\u6301\u5728\u957f\u671f\u5eb7\u590d\u4e2d\u5e94\u7528\u987a\u5e94\u6027\u5730\u9762\u8bad\u7ec3\u63a8\u8fdb\u529b\u7f3a\u9677\u3002"}}
{"id": "2512.06357", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06357", "abs": "https://arxiv.org/abs/2512.06357", "authors": ["Tony Sallooma", "Okyay Kaynak", "Xinbo Yub", "Wei He"], "title": "Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction", "comment": "Engineering Applications of Artificial Intelligence 2022", "summary": "Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePID\u63a7\u5236\u65b9\u6cd5\u7684\u6280\u672f\uff0c\u7528\u4e8e\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u6b65\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u590d\u6742\u5ea6\u4e0d\u53d8\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7PID\u63a7\u5236\u5668\u8c03\u6574\u9884\u6d4b\u503c\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u771f\u5b9e\u503c\uff0c\u5e76\u5728\u6c34\u9700\u6c42\u9884\u6d4b\u548c\u80fd\u6e90\u6d88\u8017\u9884\u6d4b\u4e24\u4e2a\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u591a\u6b65\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u5de5\u4e1a\u51b3\u7b56\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u7684\u590d\u6742\u6027\u5f71\u54cd\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7PID\u63a7\u5236\u65b9\u6cd5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u7cfb\u7edf\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6bd4\u4f8b-\u79ef\u5206-\u5fae\u5206\uff08PID\uff09\u63a7\u5236\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u6bcf\u4e2a\u65f6\u95f4\u6b65\u7684\u9884\u6d4b\u503c\uff0c\u4ee5\u4fee\u6b63\u8bef\u5dee\u3002\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff1a\u6c34\u9700\u6c42\u9884\u6d4b\u548c\u5c0f\u65f6\u80fd\u6e90\u6d88\u8017\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528PID\u589e\u5f3a\u6280\u672f\u540e\uff0c\u9884\u6d4b\u7cbe\u5ea6\u663e\u8457\u63d0\u9ad8\uff0c\u4e14\u7cfb\u7edf\u590d\u6742\u5ea6\u51e0\u4e4e\u4e0d\u53d8\u3002\u5728\u4e24\u4e2a\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5747\u4f18\u4e8e\u539f\u59cb\u9884\u6d4b\u6a21\u578b\u3002", "conclusion": "PID-based booster\u662f\u4e00\u79cd\u6709\u6548\u7684\u6280\u672f\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u5468\u671f\u6027\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u4e1a\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.06275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06275", "abs": "https://arxiv.org/abs/2512.06275", "authors": ["Kegang Wang", "Jiankai Tang", "Yuntao Wang", "Xin Liu", "Yuxuan Fan", "Jiatong Ji", "Yuanchun Shi", "Daniel McDuff"], "title": "FacePhys: State of the Heart Learning", "comment": null, "summary": "Vital sign measurement using cameras presents opportunities for comfortable, ubiquitous health monitoring. Remote photoplethysmography (rPPG), a foundational technology, enables cardiac measurement through minute changes in light reflected from the skin. However, practical deployment is limited by the computational constraints of performing analysis on front-end devices and the accuracy degradation of transmitting data through compressive channels that reduce signal quality. We propose a memory efficient rPPG algorithm - \\emph{FacePhys} - built on temporal-spatial state space duality, which resolves the trilemma of model scalability, cross-dataset generalization, and real-time operation. Leveraging a transferable heart state, FacePhys captures subtle periodic variations across video frames while maintaining a minimal computational overhead, enabling training on extended video sequences and supporting low-latency inference. FacePhys establishes a new state-of-the-art, with a substantial 49\\% reduction in error. Our solution enables real-time inference with a memory footprint of 3.6 MB and per-frame latency of 9.46 ms -- surpassing existing methods by 83\\% to 99\\%. These results translate into reliable real-time performance in practical deployments, and a live demo is available at https://www.facephys.com/.", "AI": {"tldr": "FacePhys\u662f\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u6027\u7684\u9ad8\u6548rPPG\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u53ef\u6269\u5c55\u6027\u3001\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u548c\u5b9e\u65f6\u64cd\u4f5c\u7684\u4e09\u96be\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6700\u5c0f\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u4e8649%\u7684\u9519\u8bef\u7387\u964d\u4f4e\u3002", "motivation": "\u5229\u7528\u6444\u50cf\u5934\u8fdb\u884c\u751f\u547d\u4f53\u5f81\u6d4b\u91cf\u4e3a\u8212\u9002\u3001\u666e\u9002\u7684\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u53d7\u5230\u524d\u7aef\u8bbe\u5907\u8ba1\u7b97\u9650\u5236\u548c\u6570\u636e\u538b\u7f29\u4f20\u8f93\u5bfc\u81f4\u4fe1\u53f7\u8d28\u91cf\u4e0b\u964d\u7684\u5236\u7ea6\u3002", "method": "\u57fa\u4e8e\u65f6\u7a7a\u72b6\u6001\u7a7a\u95f4\u5bf9\u5076\u6027\u6784\u5efa\u5185\u5b58\u9ad8\u6548\u7684rPPG\u7b97\u6cd5\uff0c\u5229\u7528\u53ef\u8f6c\u79fb\u7684\u5fc3\u8df3\u72b6\u6001\u6355\u6349\u89c6\u9891\u5e27\u95f4\u7684\u7ec6\u5fae\u5468\u671f\u6027\u53d8\u5316\uff0c\u652f\u6301\u6269\u5c55\u89c6\u9891\u5e8f\u5217\u8bad\u7ec3\u548c\u4f4e\u5ef6\u8fdf\u63a8\u7406\u3002", "result": "FacePhys\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9519\u8bef\u7387\u964d\u4f4e49%\uff0c\u5b9e\u65f6\u63a8\u7406\u5185\u5b58\u5360\u7528\u4ec53.6MB\uff0c\u6bcf\u5e27\u5ef6\u8fdf9.46ms\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534783%\u523099%\u3002", "conclusion": "\u8be5\u89e3\u51b3\u65b9\u6848\u5728\u5b9e\u8df5\u90e8\u7f72\u4e2d\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u5b9e\u65f6\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2512.06912", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06912", "abs": "https://arxiv.org/abs/2512.06912", "authors": ["Rushiraj Gadhvi", "Sandeep Manjanna"], "title": "Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields", "comment": "Under Review for International Conference on Robotics and Automation (ICRA 2026)", "summary": "For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u80fd\u91cf\u9ad8\u6548\u6c34\u9762\u8f66\u8f86\u5bfc\u822a\u65b9\u6cd5\uff0c\u5728\u6da1\u6d41\u573a\u4e2d\u4ec5\u4f7f\u7528\u5c40\u90e8\u901f\u5ea6\u6d4b\u91cf\u5b9e\u73b0\u6d41\u611f\u77e5\u5bfc\u822a\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u8282\u80fd30-50%", "motivation": "\u6a21\u4effkhalasi\u5229\u7528\u6d0b\u6d41\u5bfc\u822a\u7684\u76f4\u89c9\uff0c\u89e3\u51b3\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86\u5728\u4e25\u683c\u80fd\u91cf\u9884\u7b97\u4e0b\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u80fd\u91cf\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u524a\u5f31\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7684\u6da1\u6d41\u573a\u4e2d", "method": "\u57fa\u4e8eSoft Actor Critic\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u5c40\u90e8\u901f\u5ea6\u6d4b\u91cf\u5b66\u4e60\u6d41\u611f\u77e5\u5bfc\u822a\u7b56\u7565", "result": "\u5728\u591a\u6837\u5316\u52a8\u6001\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u80fd\u91cf\u8282\u7ea6\uff0c\u5e76\u80fd\u7a33\u5065\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6d41\u6761\u4ef6\uff0c\u5bfc\u822a\u8def\u5f84\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u8282\u80fd30-50%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u957f\u671f\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84"}}
{"id": "2512.06370", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06370", "abs": "https://arxiv.org/abs/2512.06370", "authors": ["Jaerin Lee", "Kyoung Mu Lee"], "title": "Optimizing Optimizers for Fast Gradient-Based Learning", "comment": "49 pages, 5 figures", "summary": "We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u57fa\u4e8e\u68af\u5ea6\u7684\u5b66\u4e60\u4e2d\u7684\u4f18\u5316\u5668\u8bbe\u8ba1\u81ea\u52a8\u5316\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u8d2a\u5a6a\u539f\u5219\u5c06\u4f18\u5316\u5668\u8bbe\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u6700\u5927\u5316\u635f\u5931\u77ac\u65f6\u51cf\u5c11\u7684\u51f8\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u52a8\u673a\u662f\u5efa\u7acb\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u6765\u81ea\u52a8\u8bbe\u8ba1\u4f18\u5316\u5668\u53ca\u5176\u8d85\u53c2\u6570\uff0c\u51cf\u5c11\u624b\u52a8\u8c03\u53c2\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u4f18\u5316\u5668\u89c6\u4e3a\u5c06\u635f\u5931\u68af\u5ea6\u4fe1\u53f7\u8f6c\u6362\u4e3a\u53c2\u6570\u8fd0\u52a8\u7684\u51fd\u6570\uff0c\u901a\u8fc7\u89e3\u51b3\u4e00\u7cfb\u5217\u51f8\u4f18\u5316\u95ee\u9898\u6765\u8bbe\u8ba1\u4f18\u5316\u5668\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u8d85\u53c2\u6570\u3002", "result": "\u7ed3\u679c\u4e0d\u4ec5\u6062\u590d\u4e86\u591a\u79cd\u6d41\u884c\u4f18\u5316\u5668\u7684\u95ed\u5f0f\u89e3\uff0c\u8fd8\u80fd\u6839\u636e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u68af\u5ea6\u7edf\u8ba1\u81ea\u52a8\u751f\u6210\u6700\u4f18\u8d85\u53c2\u6570\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u4e3a\u4f18\u5316\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5b9e\u73b0\u4e86\u4f18\u5316\u8fc7\u7a0b\u7684\u52a8\u6001\u81ea\u52a8\u5316\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2512.06276", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06276", "abs": "https://arxiv.org/abs/2512.06276", "authors": ["Tianyi Gao", "Hao Li", "Han Fang", "Xin Wei", "Xiaodong Dong", "Hongbo Sun", "Ye Yuan", "Zhongjiang He", "Jinglin Xu", "Jingmin Xin", "Hao Sun"], "title": "RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension", "comment": null, "summary": "Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RefBench-PRO\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u611f\u77e5\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86Ref-R1\u5b66\u4e60\u65b9\u6848\u6765\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709REC\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u611f\u77e5\u80fd\u529b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u673a\u5236\uff0c\u65e0\u6cd5\u63ed\u793aMLLM\u5728\u4e0d\u540c\u8ba4\u77e5\u80fd\u529b\u4e0a\u7684\u57fa\u7840\u80fd\u529b\u3002", "method": "\u63d0\u51faRefBench-PRO\u57fa\u51c6\uff0c\u5c06\u6307\u4ee3\u8868\u8fbe\u5206\u89e3\u4e3a\u611f\u77e5\u548c\u63a8\u7406\u4e24\u4e2a\u7ef4\u5ea6\uff0c\u7ec6\u5206\u4e3a\u516d\u4e2a\u6e10\u8fdb\u5f0f\u6311\u6218\u4efb\u52a1\uff1b\u5f00\u53d1\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\uff1b\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Ref-R1\u5b66\u4e60\u65b9\u6848\uff0c\u91c7\u7528\u52a8\u6001IoU\u7684GRPO\u65b9\u6cd5\u3002", "result": "RefBench-PRO\u80fd\u591f\u5bf9MLLM\u5728\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u4e0a\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\uff0c\u5728\u611f\u77e5\u548c\u63a8\u7406\u65b9\u9762\u90fd\u63d0\u51fa\u4e86\u66f4\u5927\u6311\u6218\u3002", "conclusion": "RefBench-PRO\u662f\u4e00\u4e2a\u5168\u9762\u7684REC\u57fa\u51c6\uff0c\u901a\u8fc7\u5206\u89e3\u8ba4\u77e5\u7ef4\u5ea6\u5b9e\u73b0\u4e86\u5bf9MLLM\u80fd\u529b\u7684\u66f4\u6df1\u5165\u8bc4\u4f30\uff0c\u4e3aREC\u4efb\u52a1\u5efa\u7acb\u4e86\u66f4\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2512.06935", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06935", "abs": "https://arxiv.org/abs/2512.06935", "authors": ["Nicol\u00f2 Botteghi", "Owen Brook", "Urban Fasel", "Federico Califano"], "title": "Interconnection and Damping Assignment Passivity-Based Control using Sparse Neural ODEs", "comment": null, "summary": "Interconnection and Damping Assignment Passivity-Based Control (IDA-PBC) is a nonlinear control technique that assigns a port-Hamiltonian (pH) structure to a controlled system using a state-feedback law. While IDA-PBC has been extensively studied and applied to many systems, its practical implementation often remains confined to academic examples and, almost exclusively, to stabilization tasks. The main limitation of IDA-PBC stems from the complexity of analytically solving a set of partial differential equations (PDEs), referred to as the matching conditions, which enforce the pH structure of the closed-loop system. However, this is extremely challenging, especially for complex physical systems and tasks.\n  In this work, we propose a novel numerical approach for designing IDA-PBC controllers without solving the matching PDEs exactly. We cast the IDA-PBC problem as the learning of a neural ordinary differential equation. In particular, we rely on sparse dictionary learning to parametrize the desired closed-loop system as a sparse linear combination of nonlinear state-dependent functions. Optimization of the controller parameters is achieved by solving a multi-objective optimization problem whose cost function is composed of a generic task-dependent cost and a matching condition-dependent cost. Our numerical results show that the proposed method enables (i) IDA-PBC to be applicable to complex tasks beyond stabilization, such as the discovery of periodic oscillatory behaviors, (ii) the derivation of closed-form expressions of the controlled system, including residual terms", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u7684IDA-PBC\u63a7\u5236\u5668\u6570\u503c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u65e0\u9700\u7cbe\u786e\u6c42\u89e3\u5339\u914dPDEs\uff0c\u4f7fIDA-PBC\u80fd\u591f\u5e94\u7528\u4e8e\u8d85\u8d8a\u7a33\u5b9a\u7684\u590d\u6742\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edfIDA-PBC\u65b9\u6cd5\u56e0\u9700\u8981\u89e3\u6790\u6c42\u89e3\u590d\u6742\u7684\u504f\u5fae\u5206\u65b9\u7a0b\uff08\u5339\u914d\u6761\u4ef6\uff09\u800c\u96be\u4ee5\u5e94\u7528\u4e8e\u590d\u6742\u7269\u7406\u7cfb\u7edf\u548c\u4efb\u52a1\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u3002", "method": "\u5c06IDA-PBC\u95ee\u9898\u8f6c\u5316\u4e3a\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u5b66\u4e60\u95ee\u9898\uff0c\u4f7f\u7528\u7a00\u758f\u5b57\u5178\u5b66\u4e60\u53c2\u6570\u5316\u95ed\u73af\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6c42\u89e3\u63a7\u5236\u5668\u53c2\u6570\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u8d85\u8d8a\u7a33\u5b9a\u7684\u590d\u6742\u4efb\u52a1\uff08\u5982\u5468\u671f\u6027\u632f\u8361\u884c\u4e3a\u53d1\u73b0\uff09\uff0c\u5e76\u80fd\u5bfc\u51fa\u5305\u542b\u6b8b\u5dee\u9879\u7684\u95ed\u73af\u7cfb\u7edf\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6570\u503c\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edfIDA-PBC\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u5176\u80fd\u591f\u6709\u6548\u5e94\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u590d\u6742\u63a7\u5236\u4efb\u52a1\u3002"}}
{"id": "2512.06392", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06392", "abs": "https://arxiv.org/abs/2512.06392", "authors": ["Runlong Zhou", "Lefan Zhang", "Shang-Chen Wu", "Kelvin Zou", "Hanzhi Zhou", "Ke Ye", "Yihao Feng", "Dong Yin", "Alex Guillen Garcia", "Dmytro Babych", "Rohit Chatterjee", "Matthew Hopkins", "Xiang Kong", "Chang Lan", "Lezhi Li", "Yiping Ma", "Daniele Molinari", "Senyu Tong", "Yanchao Sun", "Thomas Voice", "Jianyu Wang", "Chong Wang", "Simon Wang", "Floris Weers", "Yechen Xu", "Guolin Yin", "Muyang Yu", "Yi Zhang", "Zheng Zhou", "Danyang Zhuo", "Ruoming Pang", "Cheng Leong"], "title": "RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs", "comment": "14 pages, 6 figures", "summary": "Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.", "AI": {"tldr": "RLAX\u662f\u4e00\u4e2a\u57fa\u4e8eTPU\u7684\u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u53c2\u6570\u670d\u52a1\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u7cfb\u7edf\u4f18\u5316\u548c\u6570\u636e\u96c6\u6280\u672f\uff0c\u572812\u5c0f\u65f648\u5206\u949f\u5185\u5c06QwQ-32B\u6a21\u578b\u7684pass@8\u51c6\u786e\u7387\u63d0\u534712.8%", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5df2\u6210\u4e3a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4f46\u9700\u8981\u53ef\u6269\u5c55\u4e14\u80fd\u5bb9\u5fcd\u4e2d\u65ad\u7684\u8bad\u7ec3\u6846\u67b6", "method": "\u91c7\u7528\u53c2\u6570\u670d\u52a1\u5668\u67b6\u6784\uff0c\u4e3b\u8bad\u7ec3\u5668\u5b9a\u671f\u63a8\u9001\u6a21\u578b\u6743\u91cd\uff0c\u63a8\u7406\u5de5\u4f5c\u5668\u62c9\u53d6\u6700\u65b0\u6743\u91cd\u751f\u6210\u65b0\u6570\u636e\uff1b\u5f15\u5165\u7cfb\u7edf\u4f18\u5316\u6280\u672f\u652f\u6301\u591a\u79cdRL\u7b97\u6cd5\uff1b\u5f00\u53d1\u65b0\u7684\u6570\u636e\u96c6\u7b5b\u9009\u548c\u5bf9\u9f50\u6280\u672f", "result": "\u57281024\u4e2av5p TPU\u4e0a\uff0c\u4ec5\u752812\u5c0f\u65f648\u5206\u949f\u5c31\u5c06QwQ-32B\u7684pass@8\u51c6\u786e\u7387\u63d0\u534712.8%\uff0c\u4e14\u8bad\u7ec3\u8fc7\u7a0b\u5bf9\u4e2d\u65ad\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "RLAX\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21RL\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u6027\u548c\u5bb9\u9519\u6027\u95ee\u9898\uff0c\u663e\u8457\u52a0\u901f\u4e86LLM\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347"}}
{"id": "2512.06281", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06281", "abs": "https://arxiv.org/abs/2512.06281", "authors": ["Hengzhuang Li", "Xinsong Zhang", "Qiming Peng", "Bin Luo", "Han Hu", "Dengyang Jiang", "Han-Jia Ye", "Teng Zhang", "Hai Jin"], "title": "Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in multimodal tasks. Despite their impressive performance, MLLMs suffer from the modality imbalance issue, where visual information is often underutilized compared to textual representations in deeper layers, leading to degraded visual performance or hallucinations. This issue stems from the predominant reliance on next-text-token-prediction during training, which fails to provide direct visual supervisory signals, resulting in progressive homogenization of visual representations throughout the layers. To this end, we propose Latent Visual Reconstruction (LaVer), a novel training framework that facilitates MLLMs in learning more discriminative visual representations via masked image modeling in the joint latent semantic space of LLM. Our method offers direct visual activation to MLLMs, which exhibit increased visual attention allocation, indicating enhanced utilization of visual information. Extensive experiments across diverse benchmarks prove the superiority of our approach in various scenarios, especially those requiring dense visual capabilities. Code of LaVer is available at https://github.com/Fir-lat/LaVer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLaVer\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u89e3\u51b3MLLMs\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u89c6\u89c9\u4fe1\u606f\u5229\u7528\u7387\u3002", "motivation": "MLLMs\u5b58\u5728\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6df1\u5c42\u7f51\u7edc\u4e2d\u89c6\u89c9\u4fe1\u606f\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u5bfc\u81f4\u89c6\u89c9\u6027\u80fd\u4e0b\u964d\u6216\u5e7b\u89c9\u73b0\u8c61\uff0c\u8fd9\u6e90\u4e8e\u8bad\u7ec3\u65f6\u4e3b\u8981\u4f9d\u8d56\u6587\u672ctoken\u9884\u6d4b\u800c\u7f3a\u4e4f\u76f4\u63a5\u89c6\u89c9\u76d1\u7763\u4fe1\u53f7\u3002", "method": "\u63d0\u51faLaVer\u8bad\u7ec3\u6846\u67b6\uff0c\u5728LLM\u7684\u8054\u5408\u6f5c\u5728\u8bed\u4e49\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\uff0c\u4e3aMLLMs\u63d0\u4f9b\u76f4\u63a5\u89c6\u89c9\u6fc0\u6d3b\uff0c\u4fc3\u8fdb\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u7684\u89c6\u89c9\u8868\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u589e\u5f3aMLLMs\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u914d\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5bc6\u96c6\u89c6\u89c9\u80fd\u529b\u7684\u573a\u666f\u4e0b\u3002", "conclusion": "LaVer\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MLLMs\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4fe1\u606f\u7684\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2512.06951", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06951", "abs": "https://arxiv.org/abs/2512.06951", "authors": ["Ilia Larchenko", "Gleb Zarin", "Akash Karnatak"], "title": "Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge", "comment": "2025 NeurIPS Behavior Challenge 1st place solution", "summary": "We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.\n  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.\n  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u89c6\u89c9\u52a8\u4f5c\u7b56\u7565\uff0c\u57282025\u5e74BEHAVIOR\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\uff0c\u8be5\u7b56\u7565\u901a\u8fc7\u76f8\u5173\u566a\u58f0\u6d41\u5339\u914d\u3001\u53ef\u5b66\u4e60\u6df7\u5408\u5c42\u6ce8\u610f\u529b\u7b49\u521b\u65b0\u6280\u672f\uff0c\u572850\u4e2a\u591a\u6837\u5316\u957f\u89c6\u91ce\u5bb6\u5ead\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8626%\u7684q-score\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5bb6\u5ead\u73af\u5883\u4e2d\u957f\u89c6\u91ce\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u53cc\u624b\u64cd\u4f5c\u3001\u5bfc\u822a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728BEHAVIOR\u6311\u6218\u8d5b\u8fd9\u6837\u7684\u771f\u5b9e\u4eff\u771f\u73af\u5883\u4e2d\u3002", "method": "\u57fa\u4e8ePi0.5\u67b6\u6784\uff0c\u5f15\u5165\u76f8\u5173\u566a\u58f0\u6d41\u5339\u914d\u6280\u672f\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u91c7\u7528\u53ef\u5b66\u4e60\u6df7\u5408\u5c42\u6ce8\u610f\u529b\u548cSystem 2\u9636\u6bb5\u8ddf\u8e2a\u6765\u89e3\u51b3\u6b67\u4e49\u95ee\u9898\uff0c\u4f7f\u7528\u591a\u6837\u672c\u6d41\u5339\u914d\u51cf\u5c11\u65b9\u5dee\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u5e94\u7528\u52a8\u4f5c\u538b\u7f29\u548c\u6311\u6218\u7279\u5b9a\u6821\u6b63\u89c4\u5219\u3002", "result": "\u5728BEHAVIOR\u6311\u6218\u8d5b\u768450\u4e2a\u4efb\u52a1\u4e2d\uff0c\u5728\u516c\u5f00\u548c\u79c1\u6709\u6392\u884c\u699c\u4e0a\u90fd\u8fbe\u5230\u4e8626%\u7684q-score\uff0c\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002", "conclusion": "\u63d0\u51fa\u7684\u76f8\u5173\u566a\u58f0\u6d41\u5339\u914d\u548c\u76f8\u5173\u611f\u77e5\u4fee\u590d\u6280\u672f\u6709\u6548\u63d0\u5347\u4e86\u52a8\u4f5c\u5e8f\u5217\u7684\u5e73\u6ed1\u6027\uff0c\u6574\u4f53\u65b9\u6cd5\u5728\u590d\u6742\u5bb6\u5ead\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u957f\u89c6\u91ce\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06417", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.06417", "abs": "https://arxiv.org/abs/2512.06417", "authors": ["Yifan Sun", "Lei Cheng", "Jianlong Li", "Peter Gerstoft"], "title": "Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator", "comment": null, "summary": "Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.", "AI": {"tldr": "Hankel-FNO\u662f\u4e00\u79cd\u57fa\u4e8e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u7684\u9ad8\u6548\u6c34\u4e0b\u58f0\u5b66\u5236\u56fe\u6a21\u578b\uff0c\u7ed3\u5408\u58f0\u4f20\u64ad\u77e5\u8bc6\u548c\u5730\u5f62\u6570\u636e\uff0c\u5728\u4fdd\u6301\u9ad8\u8ba1\u7b97\u901f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u6c34\u4e0b\u58f0\u5b66\u5236\u56fe\u65b9\u6cd5\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7684\u6570\u503c\u6c42\u89e3\u5668\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u6216\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\uff1b\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u66ff\u4ee3\u6a21\u578b\u5b58\u5728\u56fa\u5b9a\u5206\u8fa8\u7387\u9650\u5236\u548c\u4f9d\u8d56\u663e\u5f0f\u504f\u5fae\u5206\u65b9\u7a0b\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faHankel-FNO\u6a21\u578b\uff0c\u57fa\u4e8e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u67b6\u6784\uff0c\u878d\u5165\u58f0\u4f20\u64ad\u77e5\u8bc6\u548c\u6d77\u5e95\u5730\u5f62\u4fe1\u606f\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u6c34\u4e0b\u58f0\u5b66\u573a\u9884\u6d4b\u3002", "result": "Hankel-FNO\u5728\u901f\u5ea6\u4e0a\u4f18\u4e8e\u4f20\u7edf\u6c42\u89e3\u5668\uff0c\u5728\u7cbe\u5ea6\u4e0a\u8d85\u8d8a\u6570\u636e\u9a71\u52a8\u66ff\u4ee3\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u957f\u8ddd\u79bb\u9884\u6d4b\u4e2d\u8868\u73b0\u7a81\u51fa\uff1b\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5bf9\u591a\u6837\u5316\u73af\u5883\u548c\u58f0\u6e90\u8bbe\u7f6e\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Hankel-FNO\u4e3a\u6c34\u4e0b\u58f0\u5b66\u5236\u56fe\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5feb\u901f\u54cd\u5e94\u548c\u9002\u5e94\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2512.06282", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06282", "abs": "https://arxiv.org/abs/2512.06282", "authors": ["Lyn Chao-ling Chen", "Kuan-Wen Chen", "Yi-Ping Hung"], "title": "A Sleep Monitoring System Based on Audio, Video and Depth Information", "comment": "Accepted in the Computer Vision, Graphics and Image Processing (CVGIP 2013)", "summary": "For quantitative evaluation of sleep disturbances, a noninvasive monitoring system is developed by introducing an event-based method. We observe sleeping in home context and classify the sleep disturbances into three types of events: motion events, light-on/off events and noise events. A device with an infrared depth sensor, a RGB camera, and a four-microphone array is used in sleep monitoring in an environment with barely light sources. One background model is established in depth signals for measuring magnitude of movements. Because depth signals cannot observe lighting changes, another background model is established in color images for measuring magnitude of lighting effects. An event detection algorithm is used to detect occurrences of events from the processed data of the three types of sensors. The system was tested in sleep condition and the experiment result validates the system reliability.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u7684\u65e0\u521b\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ea2\u5916\u6df1\u5ea6\u4f20\u611f\u5668\u3001RGB\u6444\u50cf\u5934\u548c\u56db\u9ea6\u514b\u98ce\u9635\u5217\u6765\u68c0\u6d4b\u8fd0\u52a8\u3001\u706f\u5149\u548c\u566a\u97f3\u4e09\u79cd\u7c7b\u578b\u7684\u7761\u7720\u5e72\u6270\u4e8b\u4ef6\u3002", "motivation": "\u5f00\u53d1\u975e\u4fb5\u5165\u5f0f\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9a\u91cf\u8bc4\u4f30\u7761\u7720\u5e72\u6270\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u5728\u4f4e\u5149\u7167\u73af\u5883\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u7761\u7720\u8d28\u91cf\u76d1\u6d4b\u3002", "method": "\u4f7f\u7528\u7ea2\u5916\u6df1\u5ea6\u4f20\u611f\u5668\u68c0\u6d4b\u8fd0\u52a8\u4e8b\u4ef6\uff0cRGB\u6444\u50cf\u5934\u68c0\u6d4b\u706f\u5149\u5f00\u5173\u4e8b\u4ef6\uff0c\u56db\u9ea6\u514b\u98ce\u9635\u5217\u68c0\u6d4b\u566a\u97f3\u4e8b\u4ef6\u3002\u5efa\u7acb\u6df1\u5ea6\u4fe1\u53f7\u548c\u989c\u8272\u56fe\u50cf\u7684\u80cc\u666f\u6a21\u578b\uff0c\u91c7\u7528\u4e8b\u4ef6\u68c0\u6d4b\u7b97\u6cd5\u4ece\u4e09\u7c7b\u4f20\u611f\u5668\u5904\u7406\u6570\u636e\u4e2d\u8bc6\u522b\u4e8b\u4ef6\u53d1\u751f\u3002", "result": "\u7cfb\u7edf\u5728\u7761\u7720\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u4e8b\u4ef6\u9a71\u52a8\u7684\u591a\u4f20\u611f\u5668\u7761\u7720\u76d1\u6d4b\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u91cf\u5316\u7761\u7720\u5e72\u6270\u4e8b\u4ef6\uff0c\u4e3a\u5bb6\u5ead\u73af\u5883\u4e0b\u7684\u7761\u7720\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2512.06963", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06963", "abs": "https://arxiv.org/abs/2512.06963", "authors": ["Yichao Shen", "Fangyun Wei", "Zhiying Du", "Yaobo Liang", "Yan Lu", "Jiaolong Yang", "Nanning Zheng", "Baining Guo"], "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators", "comment": "Project page: https://videovla-nips2025.github.io", "summary": "Generalization in robot manipulation is essential for deploying robots in open-world environments and advancing toward artificial general intelligence. While recent Vision-Language-Action (VLA) models leverage large pre-trained understanding models for perception and instruction following, their ability to generalize to novel tasks, objects, and settings remains limited. In this work, we present VideoVLA, a simple approach that explores the potential of transforming large video generation models into robotic VLA manipulators. Given a language instruction and an image, VideoVLA predicts an action sequence as well as the future visual outcomes. Built on a multi-modal Diffusion Transformer, VideoVLA jointly models video, language, and action modalities, using pre-trained video generative models for joint visual and action forecasting. Our experiments show that high-quality imagined futures correlate with reliable action predictions and task success, highlighting the importance of visual imagination in manipulation. VideoVLA demonstrates strong generalization, including imitating other embodiments' skills and handling novel objects. This dual-prediction strategy - forecasting both actions and their visual consequences - explores a paradigm shift in robot learning and unlocks generalization capabilities in manipulation systems.", "AI": {"tldr": "VideoVLA\u662f\u4e00\u4e2a\u5c06\u5927\u578b\u89c6\u9891\u751f\u6210\u6a21\u578b\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u5668\u7684\u7b80\u5355\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u89c6\u9891\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u6a21\u6001\uff0c\u5b9e\u73b0\u52a8\u4f5c\u5e8f\u5217\u548c\u672a\u6765\u89c6\u89c9\u7ed3\u679c\u7684\u9884\u6d4b", "motivation": "\u89e3\u51b3\u5f53\u524dVLA\u6a21\u578b\u5728\u65b0\u4efb\u52a1\u3001\u65b0\u7269\u4f53\u548c\u65b0\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u8303\u5f0f\u8f6c\u53d8", "method": "\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563Transformer\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u6a21\u578b\u8fdb\u884c\u8054\u5408\u89c6\u89c9\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u91c7\u7528\u53cc\u9884\u6d4b\u7b56\u7565\u540c\u65f6\u9884\u6d4b\u52a8\u4f5c\u5e8f\u5217\u548c\u672a\u6765\u89c6\u89c9\u7ed3\u679c", "result": "\u5b9e\u9a8c\u8868\u660e\u9ad8\u8d28\u91cf\u7684\u672a\u6765\u89c6\u89c9\u60f3\u8c61\u4e0e\u53ef\u9760\u7684\u52a8\u4f5c\u9884\u6d4b\u548c\u4efb\u52a1\u6210\u529f\u76f8\u5173\uff0cVideoVLA\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u6a21\u4eff\u5176\u4ed6\u5b9e\u65bd\u4f8b\u7684\u6280\u80fd\u548c\u5904\u7406\u65b0\u7269\u4f53", "conclusion": "\u901a\u8fc7\u9884\u6d4b\u52a8\u4f5c\u53ca\u5176\u89c6\u89c9\u540e\u679c\u7684\u53cc\u91cd\u9884\u6d4b\u7b56\u7565\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u8303\u5f0f\uff0c\u89e3\u9501\u4e86\u64cd\u4f5c\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2512.06427", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06427", "abs": "https://arxiv.org/abs/2512.06427", "authors": ["Andrea Combette", "Antoine Venaille", "Nelly Pustelnik"], "title": "A new initialisation to Control Gradients in Sinusoidal Neural network", "comment": null, "summary": "Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \\texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \\texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \\texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6b63\u5f26\u6fc0\u6d3b\u51fd\u6570\u7f51\u7edc\uff08\u5982SIREN\uff09\u7684\u65b0\u521d\u59cb\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u63a7\u5236\u68af\u5ea6\u548c\u9884\u6fc0\u6d3b\u5206\u5e03\u6765\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u79cd\u91cd\u5efa\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u521d\u59cb\u5316\u7b56\u7565\u5bf9\u6b63\u5f26\u6fc0\u6d3b\u51fd\u6570\u7f51\u7edc\u7f3a\u4e4f\u7cbe\u786e\u7684\u7406\u8bba\u7406\u89e3\uff0c\u65e0\u6cd5\u6709\u6548\u63a7\u5236\u68af\u5ea6\u7206\u70b8\u6216\u6d88\u5931\u95ee\u9898\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u548c\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u9884\u6fc0\u6d3b\u5206\u5e03\u6536\u655b\u6027\u548c\u96c5\u53ef\u6bd4\u77e9\u9635\u5e8f\u5217\u65b9\u5dee\uff0c\u63a8\u5bfc\u51fa\u53c2\u6570\u521d\u59cb\u5316\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u63a7\u5236\u68af\u5ea6\u5e76\u9488\u5bf9\u9884\u6fc0\u6d3b\u6d88\u5931\u95ee\u9898\uff0c\u9632\u6b62\u4e0d\u9002\u5f53\u9891\u7387\u51fa\u73b0\u3002", "result": "\u65b0\u521d\u59cb\u5316\u7b56\u7565\u5728\u51fd\u6570\u62df\u5408\u548c\u56fe\u50cf\u91cd\u5efa\u7b49\u4efb\u52a1\u4e2d\u4e00\u81f4\u4f18\u4e8e\u539f\u59cbSIREN\u65b9\u6848\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u63d0\u51fa\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u68af\u5ea6\u548c\u9884\u6fc0\u6d3b\u5206\u5e03\uff0c\u663e\u8457\u6539\u5584\u4e86\u6b63\u5f26\u6fc0\u6d3b\u51fd\u6570\u7f51\u7edc\u7684\u8bad\u7ec3\u52a8\u6001\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u76f8\u5173\u67b6\u6784\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u521d\u59cb\u5316\u65b9\u6848\u3002"}}
{"id": "2512.06290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06290", "abs": "https://arxiv.org/abs/2512.06290", "authors": ["Yiheng Huang", "Shuang She", "Zewei Wei", "Jianmin Lin", "Ming Yang", "Wenyin Liu"], "title": "StrokeNet: Unveiling How to Learn Fine-Grained Interactions in Online Handwritten Stroke Classification", "comment": "17 pages, 5 figures", "summary": "Stroke classification remains challenging due to variations in writing style, ambiguous content, and dynamic writing positions. The core challenge in stroke classification is modeling the semantic relationships between strokes. Our observations indicate that stroke interactions are typically localized, making it difficult for existing deep learning methods to capture such fine-grained relationships. Although viewing strokes from a point-level perspective can address this issue, it introduces redundancy. However, by selecting reference points and using their sequential order to represent strokes in a fine-grained manner, this problem can be effectively solved. This insight inspired StrokeNet, a novel network architecture encoding strokes as reference pair representations (points + feature vectors), where reference points enable spatial queries and features mediate interaction modeling. Specifically, we dynamically select reference points for each stroke and sequence them, employing an Inline Sequence Attention (ISA) module to construct contextual features. To capture spatial feature interactions, we devised a Cross-Ellipse Query (CEQ) mechanism that clusters reference points and extracts features across varying spatial scales. Finally, a joint optimization framework simultaneously predicts stroke categories via reference points regression and adjacent stroke semantic transition modeling through an Auxiliary Branch (Aux-Branch). Experimental results show that our method achieves state-of-the-art performance on multiple public online handwritten datasets. Notably, on the CASIA-onDo dataset, the accuracy improves from 93.81$\\%$ to 95.54$\\%$, demonstrating the effectiveness and robustness of our approach.", "AI": {"tldr": "StrokeNet\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7b14\u753b\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u70b9\u8868\u793a\u548c\u7a7a\u95f4\u67e5\u8be2\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7b14\u753b\u95f4\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7b14\u753b\u95f4\u7684\u5c40\u90e8\u5316\u4ea4\u4e92\u5173\u7cfb\uff0c\u800c\u70b9\u7ea7\u89c6\u89d2\u867d\u7136\u80fd\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u4f46\u4f1a\u5f15\u5165\u5197\u4f59\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5efa\u6a21\u7b14\u753b\u95f4\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "method": "1. \u52a8\u6001\u9009\u62e9\u53c2\u8003\u70b9\u5e76\u5e8f\u5217\u5316\u8868\u793a\u7b14\u753b\n2. \u4f7f\u7528Inline Sequence Attention\u6a21\u5757\u6784\u5efa\u4e0a\u4e0b\u6587\u7279\u5f81\n3. \u8bbe\u8ba1Cross-Ellipse Query\u673a\u5236\u8fdb\u884c\u7a7a\u95f4\u7279\u5f81\u4ea4\u4e92\n4. \u901a\u8fc7\u8054\u5408\u4f18\u5316\u6846\u67b6\u540c\u65f6\u9884\u6d4b\u7b14\u753b\u7c7b\u522b\u548c\u76f8\u90bb\u7b14\u753b\u8bed\u4e49\u8f6c\u79fb", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u5728\u7ebf\u624b\u5199\u6570\u636e\u96c6\u4e0a\u8fbe\u5230state-of-the-art\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728CASIA-onDo\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u4ece93.81%\u63d0\u5347\u523095.54%\u3002", "conclusion": "StrokeNet\u901a\u8fc7\u53c2\u8003\u70b9\u8868\u793a\u548c\u7a7a\u95f4\u67e5\u8be2\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u7b14\u753b\u5206\u7c7b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5173\u7cfb\u5efa\u6a21\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.06995", "categories": ["cs.RO", "physics.class-ph"], "pdf": "https://arxiv.org/pdf/2512.06995", "abs": "https://arxiv.org/abs/2512.06995", "authors": ["Maryam Seraj", "Mohammad Hossein Kamrava", "Carlo Tiseo"], "title": "Parametric Design of a Cable-Driven Coaxial Spherical Parallel Mechanism for Ultrasound Scans", "comment": null, "summary": "Haptic interfaces play a critical role in medical teleoperation by enabling surgeons to interact with remote environments through realistic force and motion feedback. Achieving high fidelity in such systems requires balancing performance trade-off among workspace, dexterity, stiffness, inertia, and bandwidth, particularly in applications demanding pure rotational motion. This paper presents the design methodology and kinematic analysis of a Cable-Driven Coaxial Spherical Parallel Mechanism (CDC-SPM) developed to address these challenges. The proposed cable-driven interface design allows for reducing the mass placed at the robot arm end-effector, thereby minimizing inertial loads, enhancing stiffness, and improving dynamic responsiveness. Through parallel and coaxial actuation, the mechanism achieves decoupled rotational degrees of freedom with isotropic force and torque transmission. Simulation and analysis demonstrate that the CDC-SPM provides accurate, responsive, and safe motion characteristics suitable for high-precision haptic applications. These results highlight the mechanism's potential for medical teleoperation tasks such as ultrasound imaging, where precise and intuitive manipulation is essential.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7535\u7f06\u9a71\u52a8\u540c\u8f74\u7403\u9762\u5e76\u8054\u673a\u6784(CDC-SPM)\u7684\u8bbe\u8ba1\u65b9\u6cd5\u548c\u8fd0\u52a8\u5b66\u5206\u6790\uff0c\u65e8\u5728\u89e3\u51b3\u533b\u7597\u9065\u64cd\u4f5c\u4e2d\u9ad8\u4fdd\u771f\u89e6\u89c9\u63a5\u53e3\u7684\u6027\u80fd\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u533b\u7597\u9065\u64cd\u4f5c\u4e2d\u7684\u89e6\u89c9\u63a5\u53e3\u9700\u8981\u5728\u5de5\u4f5c\u7a7a\u95f4\u3001\u7075\u6d3b\u6027\u3001\u521a\u5ea6\u3001\u60ef\u6027\u548c\u5e26\u5bbd\u7b49\u6027\u80fd\u6307\u6807\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7eaf\u65cb\u8f6c\u8fd0\u52a8\u7684\u5e94\u7528\u4e2d\u3002\u4f20\u7edf\u8bbe\u8ba1\u5b58\u5728\u60ef\u6027\u8d1f\u8f7d\u5927\u3001\u52a8\u6001\u54cd\u5e94\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7535\u7f06\u9a71\u52a8\u540c\u8f74\u7403\u9762\u5e76\u8054\u673a\u6784\u8bbe\u8ba1\uff0c\u901a\u8fc7\u5e76\u8054\u548c\u540c\u8f74\u9a71\u52a8\u5b9e\u73b0\u89e3\u8026\u7684\u65cb\u8f6c\u81ea\u7531\u5ea6\uff0c\u5177\u6709\u5404\u5411\u540c\u6027\u7684\u529b\u548c\u626d\u77e9\u4f20\u9012\u7279\u6027\u3002", "result": "\u4eff\u771f\u5206\u6790\u8868\u660eCDC-SPM\u80fd\u591f\u63d0\u4f9b\u7cbe\u786e\u3001\u54cd\u5e94\u8fc5\u901f\u4e14\u5b89\u5168\u7684\u8fd0\u52a8\u7279\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u89e6\u89c9\u5e94\u7528\u3002", "conclusion": "\u8be5\u673a\u6784\u5728\u533b\u7597\u9065\u64cd\u4f5c\u4efb\u52a1\uff08\u5982\u8d85\u58f0\u6210\u50cf\uff09\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u76f4\u89c2\u7684\u64cd\u4f5c\u3002"}}
{"id": "2512.06440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06440", "abs": "https://arxiv.org/abs/2512.06440", "authors": ["Angelos-Christos Maroudis", "Sotirios Xydis"], "title": "Neural expressiveness for beyond importance model compression", "comment": null, "summary": "Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named \"Expressiveness\". Unlike existing pruning methods that rely on the inherent \"Importance\" of neurons' and filters' weights, ``Expressiveness\" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the \"When to Prune\" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a \"hybrid\" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u8868\u8fbe\u529b\"\u7684\u65b0\u526a\u679d\u6807\u51c6\uff0c\u5f3a\u8c03\u795e\u7ecf\u5143\u6709\u6548\u91cd\u65b0\u5206\u914d\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4e0e\u6743\u91cd\u91cd\u8981\u6027\u65e0\u5173\uff0c\u5b9e\u73b0\u4e86\u6570\u636e\u65e0\u5173\u548c\u72b6\u6001\u65e0\u5173\u7684\u526a\u679d\uff0c\u5728\u53c2\u6570\u538b\u7f29\u6bd4\u4e0a\u76f8\u6bd4\u6743\u91cd\u65b9\u6cd5\u63d0\u534710\u500d\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d1%\u3002", "motivation": "\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u795e\u7ecf\u5143\u6743\u91cd\u7684\"\u91cd\u8981\u6027\"\uff0c\u4f46\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u6709\u6548\u91cd\u65b0\u5206\u914d\u4fe1\u606f\u7684\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u4e0e\u5b66\u4e60\u72b6\u6001\u65e0\u5173\u7684\u526a\u679d\u6807\u51c6\uff0c\u89e3\u51b3\"\u4f55\u65f6\u526a\u679d\"\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\"\u8868\u8fbe\u529b\"\u526a\u679d\u6807\u51c6\uff0c\u57fa\u4e8e\u6fc0\u6d3b\u91cd\u53e0\u6765\u8bc4\u4f30\u795e\u7ecf\u5143\u91cd\u65b0\u5206\u914d\u4fe1\u606f\u7684\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u4e0e\u7f51\u7edc\u521d\u59cb\u5316\u72b6\u6001\u76f8\u5173\uff0c\u53ef\u8fd1\u4f3c\u4f7f\u7528\u4efb\u610f\u6570\u636e\uff0c\u652f\u6301\u6570\u636e\u65e0\u5173\u7b56\u7565\uff0c\u5e76\u80fd\u4e0e\u91cd\u8981\u6027\u526a\u679d\u65b9\u6cd5\u7ed3\u5408\u3002", "result": "\u5728YOLOv8\u4e0a\u5b9e\u73b055.4%\u53c2\u6570\u526a\u679d\u548c46.1%MACs\u51cf\u5c11\uff0cCOCO\u6570\u636e\u96c6\u4e0amAP50-95\u63d0\u53473%\uff1b\u4e0e\u6743\u91cd\u65b9\u6cd5\u76f8\u6bd4\u53c2\u6570\u538b\u7f29\u6bd4\u63d0\u534710\u500d\uff0c\u5e73\u5747\u6027\u80fd\u4e0b\u964d\u4ec51%\u3002", "conclusion": "\u8868\u8fbe\u529b\u526a\u679d\u6807\u51c6\u4e3a\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\"\u4f55\u65f6\u526a\u679d\"\u7684\u95ee\u9898\uff0c\u652f\u6301\u6570\u636e\u65e0\u5173\u7b56\u7565\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u4e92\u8865\uff0c\u663e\u8457\u63d0\u5347\u538b\u7f29\u6548\u7387\u3002"}}
{"id": "2512.06306", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06306", "abs": "https://arxiv.org/abs/2512.06306", "authors": ["Haoxian Zhou", "Chuanzhi Xu", "Langyi Chen", "Haodong Chen", "Yuk Ying Chung", "Qiang Qu", "Xaoming Chen", "Weidong Cai"], "title": "Exploiting Spatiotemporal Properties for Efficient Event-Driven Human Pose Estimation", "comment": null, "summary": "Human pose estimation focuses on predicting body keypoints to analyze human motion. Event cameras provide high temporal resolution and low latency, enabling robust estimation under challenging conditions. However, most existing methods convert event streams into dense event frames, which adds extra computation and sacrifices the high temporal resolution of the event signal. In this work, we aim to exploit the spatiotemporal properties of event streams based on point cloud-based framework, designed to enhance human pose estimation performance. We design Event Temporal Slicing Convolution module to capture short-term dependencies across event slices, and combine it with Event Slice Sequencing module for structured temporal modeling. We also apply edge enhancement in point cloud-based event representation to enhance spatial edge information under sparse event conditions to further improve performance. Experiments on the DHP19 dataset show our proposed method consistently improves performance across three representative point cloud backbones: PointNet, DGCNN, and Point Transformer.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70b9\u4e91\u6846\u67b6\u7684\u4e8b\u4ef6\u6d41\u65f6\u7a7a\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8b\u4ef6\u65f6\u5e8f\u5207\u7247\u5377\u79ef\u548c\u4e8b\u4ef6\u5207\u7247\u5e8f\u5217\u5316\u6a21\u5757\uff0c\u63d0\u5347\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\uff0c\u5728DHP19\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u4e8b\u4ef6\u6d41\u8f6c\u6362\u4e3a\u5bc6\u96c6\u4e8b\u4ef6\u5e27\u4f1a\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u5e76\u727a\u7272\u4e8b\u4ef6\u4fe1\u53f7\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u56e0\u6b64\u9700\u8981\u5229\u7528\u4e8b\u4ef6\u6d41\u7684\u65f6\u7a7a\u7279\u6027\u6765\u63d0\u5347\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e8b\u4ef6\u65f6\u5e8f\u5207\u7247\u5377\u79ef\u6a21\u5757\u6355\u83b7\u4e8b\u4ef6\u5207\u7247\u95f4\u7684\u77ed\u671f\u4f9d\u8d56\uff0c\u7ed3\u5408\u4e8b\u4ef6\u5207\u7247\u5e8f\u5217\u5316\u6a21\u5757\u8fdb\u884c\u7ed3\u6784\u5316\u65f6\u5e8f\u5efa\u6a21\uff0c\u5e76\u5728\u70b9\u4e91\u8868\u793a\u4e2d\u5e94\u7528\u8fb9\u7f18\u589e\u5f3a\u6765\u6539\u5584\u7a00\u758f\u4e8b\u4ef6\u6761\u4ef6\u4e0b\u7684\u7a7a\u95f4\u8fb9\u7f18\u4fe1\u606f\u3002", "result": "\u5728DHP19\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728PointNet\u3001DGCNN\u548cPoint Transformer\u4e09\u79cd\u4ee3\u8868\u6027\u70b9\u4e91\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5747\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u70b9\u4e91\u6846\u67b6\u7684\u4e8b\u4ef6\u6d41\u65f6\u7a7a\u5efa\u6a21\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u4e8b\u4ef6\u76f8\u673a\u7684\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u7279\u6027\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2512.07032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07032", "abs": "https://arxiv.org/abs/2512.07032", "authors": ["Runcong Wang", "Fengyi Wang", "Gordon Cheng"], "title": "A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator", "comment": null, "summary": "This paper presents a hetero-associative sequential memory system for mobile manipulators that learns compact, neuromorphic bindings between robot joint states and tactile observations to produce step-wise action decisions with low compute and memory cost. The method encodes joint angles via population place coding and converts skin-measured forces into spike-rate features using an Izhikevich neuron model; both signals are transformed into bipolar binary vectors and bound element-wise to create associations stored in a large-capacity sequential memory. To improve separability in binary space and inject geometry from touch, we introduce 3D rotary positional embeddings that rotate subspaces as a function of sensed force direction, enabling fuzzy retrieval through a softmax weighted recall over temporally shifted action patterns. On a Toyota Human Support Robot covered by robot skin, the hetero-associative sequential memory system realizes a pseudocompliance controller that moves the link under touch in the direction and with speed correlating to the amplitude of applied force, and it retrieves multi-joint grasp sequences by continuing tactile input. The system sets up quickly, trains from synchronized streams of states and observations, and exhibits a degree of generalization while remaining economical. Results demonstrate single-joint and full-arm behaviors executed via associative recall, and suggest extensions to imitation learning, motion planning, and multi-modal integration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u5f02\u8d28\u5173\u8054\u987a\u5e8f\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u7ed1\u5b9a\u5b66\u4e60\u5173\u8282\u72b6\u6001\u4e0e\u89e6\u89c9\u89c2\u6d4b\u4e4b\u95f4\u7684\u7d27\u51d1\u8868\u793a\uff0c\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u7684\u5206\u6b65\u52a8\u4f5c\u51b3\u7b56\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\u5feb\u901f\u5efa\u7acb\u3001\u4ece\u72b6\u6001\u548c\u89c2\u6d4b\u540c\u6b65\u6d41\u4e2d\u8bad\u7ec3\uff0c\u5e76\u4fdd\u6301\u7ecf\u6d4e\u6027\u7684\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5b9e\u73b0\u901a\u8fc7\u89e6\u89c9\u8f93\u5165\u63a7\u5236\u673a\u5668\u4eba\u8fd0\u52a8\u548c\u884c\u4e3a\u5e8f\u5217\u3002", "method": "\u4f7f\u7528\u7fa4\u4f53\u4f4d\u7f6e\u7f16\u7801\u5bf9\u5173\u8282\u89d2\u5ea6\u8fdb\u884c\u7f16\u7801\uff0c\u901a\u8fc7Izhikevich\u795e\u7ecf\u5143\u6a21\u578b\u5c06\u76ae\u80a4\u6d4b\u91cf\u529b\u8f6c\u6362\u4e3a\u8109\u51b2\u7387\u7279\u5f81\uff0c\u5c06\u4e24\u79cd\u4fe1\u53f7\u8f6c\u6362\u4e3a\u53cc\u6781\u4e8c\u8fdb\u5236\u5411\u91cf\u5e76\u8fdb\u884c\u5143\u7d20\u7ea7\u7ed1\u5b9a\uff0c\u5b58\u50a8\u5728\u5927\u578b\u987a\u5e8f\u8bb0\u5fc6\u4e2d\u3002\u5f15\u51653D\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u6765\u6539\u5584\u4e8c\u8fdb\u5236\u7a7a\u95f4\u7684\u53ef\u5206\u79bb\u6027\u3002", "result": "\u5728\u4e30\u7530\u4eba\u652f\u6301\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u4f2a\u67d4\u987a\u63a7\u5236\u5668\uff0c\u4f7f\u94fe\u63a5\u5728\u89e6\u6478\u4e0b\u6cbf\u65bd\u52a0\u529b\u7684\u65b9\u5411\u548c\u901f\u5ea6\u79fb\u52a8\uff0c\u5e76\u901a\u8fc7\u6301\u7eed\u89e6\u89c9\u8f93\u5165\u68c0\u7d22\u591a\u5173\u8282\u6293\u53d6\u5e8f\u5217\u3002\u7cfb\u7edf\u5c55\u793a\u4e86\u901a\u8fc7\u5173\u8054\u53ec\u56de\u6267\u884c\u7684\u5355\u5173\u8282\u548c\u5168\u81c2\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u79fb\u52a8\u673a\u68b0\u81c2\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u6709\u6548\u7684\u8bb0\u5fc6\u548c\u5b66\u4e60\u7cfb\u7edf\uff0c\u53ef\u6269\u5c55\u5230\u6a21\u4eff\u5b66\u4e60\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u591a\u6a21\u6001\u96c6\u6210\u7b49\u5e94\u7528\u3002"}}
{"id": "2512.06457", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06457", "abs": "https://arxiv.org/abs/2512.06457", "authors": ["Huizheng Wang", "Hongbin Wang", "Shaojun Wei", "Yang Hu", "Shouyi Yin"], "title": "BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination", "comment": null, "summary": "Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.", "AI": {"tldr": "BitStopper\u662f\u4e00\u79cd\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f4d\u7ea7\u7a00\u758f\u63a8\u6d4b\u548c\u5f02\u6b65\u5904\u7406\u6280\u672f\uff0c\u65e0\u9700\u7a00\u758f\u9884\u6d4b\u5668\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u76f8\u6bd4\u73b0\u6709Transformer\u52a0\u901f\u5668\u5728\u6027\u80fd\u548c\u80fd\u6548\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u867d\u7136\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5176\u786c\u4ef6\u6548\u7387\u53d7\u9650\u4e8e\u989d\u5916\u7684\u9884\u6d4b\u9636\u6bb5\u548c\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\u3002", "method": "\u63d0\u51faBitStopper\u6846\u67b6\uff1a1\uff09\u4f4d\u4e32\u884c\u4f7f\u80fd\u9636\u6bb5\u878d\u5408\u673a\u5236\uff0c\u91cd\u7528\u5185\u5b58\u8bbf\u95ee\u5e76\u5408\u5e76\u9884\u6d4b\u4e0e\u6267\u884c\u9636\u6bb5\uff1b2\uff09\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94token\u9009\u62e9\u7b56\u7565\uff1b3\uff09\u4f4d\u7ea7\u5f02\u6b65\u5904\u7406\u7b56\u7565\uff1b4\uff09\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u786c\u4ef6\u67b6\u6784\u3002", "result": "\u76f8\u6bd4SOTA Transformer\u52a0\u901f\u5668\uff0cBitStopper\u5728Sanger\u548cSOFA\u4e0a\u5206\u522b\u5b9e\u73b02.03\u500d\u548c1.89\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u80fd\u6548\u5206\u522b\u63d0\u53472.4\u500d\u548c2.1\u500d\u3002", "conclusion": "BitStopper\u901a\u8fc7\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u7684\u786c\u4ef6\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.06328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06328", "abs": "https://arxiv.org/abs/2512.06328", "authors": ["Jiahao Li", "Yusheng Luo", "Yunzhong Lou", "Xiangdong Zhou"], "title": "ReCAD: Reinforcement Learning Enhanced Parametric CAD Model Generation with Vision-Language Models", "comment": "Accepted as an Oral presentation at AAAI 2026", "summary": "We present ReCAD, a reinforcement learning (RL) framework that bootstraps pretrained large models (PLMs) to generate precise parametric computer-aided design (CAD) models from multimodal inputs by leveraging their inherent generative capabilities. With just access to simple functional interfaces (e.g., point coordinates), our approach enables the emergence of complex CAD operations (e.g., pattern replication and mirror). This stands in contrast to previous methods, which typically rely on knowledge injected through supervised fine-tuning (SFT), offer limited support for editability, and fail to exploit the strong generative priors of PLMs. Specifically, the ReCAD framework begins by fine-tuning vision-language models (VLMs) to equip them with basic CAD model generation capabilities, where we rewrite CAD scripts into parameterized code that is leveraged to generate accurate textual descriptions for supervision. Then, we propose a novel RL strategy that incorporates parameterized code as guidance to enhance the model's reasoning on challenging questions. Furthermore, we employ a hierarchical primitive learning process to progressively teach structured and compositional skills under a unified reward function that ensures both geometric accuracy and semantic fidelity. ReCAD sets a new state-of-the-art in both text-to-CAD and image-to-CAD tasks, significantly improving geometric accuracy across in-distribution and out-of-distribution settings. In the image-to-CAD task, for instance, it reduces the mean Chamfer Distance from 73.47 to 29.61 (in-distribution) and from 272.06 to 80.23 (out-of-distribution), outperforming existing baselines by a substantial margin.", "AI": {"tldr": "ReCAD\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u4ece\u591a\u6a21\u6001\u8f93\u5165\u751f\u6210\u7cbe\u786e\u7684\u53c2\u6570\u5316CAD\u6a21\u578b\uff0c\u65e0\u9700\u76d1\u7763\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u590d\u6742CAD\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\u6ce8\u5165\u77e5\u8bc6\uff0c\u7f16\u8f91\u6027\u652f\u6301\u6709\u9650\uff0c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u5f3a\u751f\u6210\u5148\u9a8c\u3002ReCAD\u65e8\u5728\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1\uff09\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u57fa\u672cCAD\u751f\u6210\u80fd\u529b\uff1b2\uff09\u63d0\u51fa\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u53c2\u6570\u5316\u4ee3\u7801\u4e3a\u6307\u5bfc\u589e\u5f3a\u6a21\u578b\u63a8\u7406\uff1b3\uff09\u91c7\u7528\u5206\u5c42\u539f\u8bed\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5728\u7edf\u4e00\u5956\u52b1\u51fd\u6570\u4e0b\u9010\u6b65\u6559\u6388\u7ed3\u6784\u5316\u7ec4\u5408\u6280\u80fd\u3002", "result": "ReCAD\u5728\u6587\u672c\u5230CAD\u548c\u56fe\u50cf\u5230CAD\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51e0\u4f55\u7cbe\u5ea6\u3002\u5728\u56fe\u50cf\u5230CAD\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747Chamfer\u8ddd\u79bb\u4ece73.47\u964d\u81f329.61\uff08\u5206\u5e03\u5185\uff09\u548c\u4ece272.06\u964d\u81f380.23\uff08\u5206\u5e03\u5916\uff09\u3002", "conclusion": "ReCAD\u6846\u67b6\u6210\u529f\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u53c2\u6570\u5316CAD\u6a21\u578b\u751f\u6210\uff0c\u5728\u51e0\u4f55\u7cbe\u5ea6\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2512.07041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07041", "abs": "https://arxiv.org/abs/2512.07041", "authors": ["Hiroki Sawada", "Alexandre Pitti", "Mathias Quoy"], "title": "CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation", "comment": null, "summary": "Robots interacting with humans must not only generate learned movements in real-time, but also infer the intent behind observed behaviors and estimate the confidence of their own inferences. This paper proposes a unified model that achieves all three capabilities within a single hierarchical predictive-coding recurrent neural network (PC-RNN) equipped with a class embedding vector, CERNet, which leverages a dynamically updated class embedding vector to unify motor generation and recognition. The model operates in two modes: generation and inference. In the generation mode, the class embedding constrains the hidden state dynamics to a class-specific subspace; in the inference mode, it is optimized online to minimize prediction error, enabling real-time recognition. Validated on a humanoid robot across 26 kinesthetically taught alphabets, our hierarchical model achieves 76% lower trajectory reproduction error than a parameter-matched single-layer baseline, maintains motion fidelity under external perturbations, and infers the demonstrated trajectory class online with 68% Top-1 and 81% Top-2 accuracy. Furthermore, internal prediction errors naturally reflect the model's confidence in its recognition. This integration of robust generation, real-time recognition, and intrinsic uncertainty estimation within a compact PC-RNN framework offers a compact and extensible approach to motor memory in physical robots, with potential applications in intent-sensitive human-robot collaboration.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86CERNet\uff0c\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u9884\u6d4b\u7f16\u7801\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08PC-RNN\uff09\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7684\u7c7b\u522b\u5d4c\u5165\u5411\u91cf\u5b9e\u73b0\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u3001\u5b9e\u65f6\u610f\u56fe\u8bc6\u522b\u548c\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u673a\u5668\u4eba\u9700\u8981\u540c\u65f6\u751f\u6210\u5b66\u4e60\u5230\u7684\u8fd0\u52a8\u3001\u63a8\u65ad\u89c2\u5bdf\u5230\u7684\u884c\u4e3a\u610f\u56fe\uff0c\u5e76\u4f30\u8ba1\u81ea\u8eab\u63a8\u65ad\u7684\u7f6e\u4fe1\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u8fd9\u4e9b\u80fd\u529b\u5206\u5f00\u5904\u7406\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u5206\u5c42PC-RNN\u67b6\u6784\uff0c\u914d\u5907\u52a8\u6001\u66f4\u65b0\u7684\u7c7b\u522b\u5d4c\u5165\u5411\u91cf\u3002\u6a21\u578b\u6709\u4e24\u79cd\u5de5\u4f5c\u6a21\u5f0f\uff1a\u751f\u6210\u6a21\u5f0f\u4e2d\u5d4c\u5165\u5411\u91cf\u7ea6\u675f\u9690\u85cf\u72b6\u6001\u5230\u7c7b\u522b\u7279\u5b9a\u5b50\u7a7a\u95f4\uff1b\u63a8\u7406\u6a21\u5f0f\u4e2d\u5728\u7ebf\u4f18\u5316\u5d4c\u5165\u5411\u91cf\u4ee5\u6700\u5c0f\u5316\u9884\u6d4b\u8bef\u5dee\u3002", "result": "\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5355\u5c42\u57fa\u7ebf\u8f68\u8ff9\u518d\u73b0\u8bef\u5dee\u964d\u4f4e76%\uff0c\u5728\u5916\u90e8\u6270\u52a8\u4e0b\u4fdd\u6301\u8fd0\u52a8\u4fdd\u771f\u5ea6\uff0c\u5728\u7ebf\u63a8\u65ad\u6f14\u793a\u8f68\u8ff9\u7c7b\u522b\u7684Top-1\u51c6\u786e\u738768%\u3001Top-2\u51c6\u786e\u738781%\u3002\u5185\u90e8\u9884\u6d4b\u8bef\u5dee\u81ea\u7136\u53cd\u6620\u8bc6\u522b\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7d27\u51d1\u7684PC-RNN\u6846\u67b6\u5185\u96c6\u6210\u4e86\u9c81\u68d2\u751f\u6210\u3001\u5b9e\u65f6\u8bc6\u522b\u548c\u5185\u5728\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4e3a\u7269\u7406\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u7d27\u51d1\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5728\u610f\u56fe\u654f\u611f\u7684\u4eba\u673a\u534f\u4f5c\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.06471", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06471", "abs": "https://arxiv.org/abs/2512.06471", "authors": ["Nathan P. Lawrence", "Ali Mesbah"], "title": "Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control", "comment": "IFAC preprint", "summary": "Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u6700\u4f18\u63a7\u5236\u7684\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff0c\u63a8\u5bfc\u4e86\u7ecf\u5178\u4e8c\u6b21\u76ee\u6807\u4e0e\u76ee\u6807\u6761\u4ef6\u5956\u52b1\u4e4b\u95f4\u7684\u6700\u4f18\u6027\u5dee\u8ddd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u4f18\u52bf\u3002", "motivation": "\u7814\u7a76\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u7684\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u7ecf\u5178\u5bc6\u96c6\u5956\u52b1\u53ef\u80fd\u5931\u8d25\uff0c\u4ee5\u53ca\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002", "method": "\u57fa\u4e8e\u6700\u4f18\u63a7\u5236\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u76ee\u6807\u6761\u4ef6\u5956\u52b1\u4e0e\u7ecf\u5178\u4e8c\u6b21\u5956\u52b1\u7684\u6700\u4f18\u6027\u5dee\u8ddd\uff0c\u5e76\u5c06\u72b6\u6001\u4f30\u8ba1\u4e0e\u6982\u7387\u5956\u52b1\u8054\u7cfb\u8d77\u6765\uff0c\u5e94\u7528\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u95ee\u9898\u3002", "result": "\u5728\u975e\u7ebf\u6027\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u9884\u6d4b\u63a7\u5236\u6280\u672f\u9a8c\u8bc1\u4e86\u76ee\u6807\u6761\u4ef6\u7b56\u7565\u7684\u4f18\u52bf\u3002", "conclusion": "\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u5728\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u63a7\u5236\u95ee\u9898\u3002"}}
{"id": "2512.06330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06330", "abs": "https://arxiv.org/abs/2512.06330", "authors": ["Haoyu Zhang", "Junhan Luo", "Yugang Cao", "Siran Peng", "Jie Huang", "Liangjian-Deng"], "title": "S2WMamba: A Spectral-Spatial Wavelet Mamba for Pansharpening", "comment": null, "summary": "Pansharpening fuses a high-resolution PAN image with a low-resolution multispectral (LRMS) image to produce an HRMS image. A key difficulty is that jointly processing PAN and MS often entangles spatial detail with spectral fidelity. We propose S2WMamba, which explicitly disentangles frequency information and then performs lightweight cross-modal interaction. Concretely, a 2D Haar DWT is applied to PAN to localize spatial edges and textures, while a channel-wise 1D Haar DWT treats each pixel's spectrum as a 1D signal to separate low/high-frequency components and limit spectral distortion. The resulting Spectral branch injects wavelet-extracted spatial details into MS features, and the Spatial branch refines PAN features using spectra from the 1D pyramid; the two branches exchange information through Mamba-based cross-modulation that models long-range dependencies with linear complexity. A multi-scale dynamic gate (multiplicative + additive) then adaptively fuses branch outputs.On WV3, GF2, and QB, S2WMamba matches or surpasses recent strong baselines (FusionMamba, CANNet, U2Net, ARConv), improving PSNR by up to 0.23 dB and reaching HQNR 0.956 on full-resolution WV3. Ablations justify the choice of 2D/1D DWT placement, parallel dual branches, and the fusion gate. Our code is available at https://github.com/KagUYa66/S2WMamba.", "AI": {"tldr": "S2WMamba\u662f\u4e00\u4e2a\u7528\u4e8e\u5168\u8272\u9510\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc72D/1D\u5c0f\u6ce2\u53d8\u6362\u663e\u5f0f\u89e3\u8026\u9891\u7387\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528Mamba-based\u8de8\u6a21\u6001\u4ea4\u4e92\u6765\u878d\u5408\u9ad8\u5206\u8fa8\u7387\u5168\u8272\u56fe\u50cf\u548c\u4f4e\u5206\u8fa8\u7387\u591a\u5149\u8c31\u56fe\u50cf\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u5168\u8272\u9510\u5316\u4e2d\u540c\u65f6\u5904\u7406\u5168\u8272\u548c\u591a\u5149\u8c31\u56fe\u50cf\u5f80\u5f80\u4f1a\u4f7f\u7a7a\u95f4\u7ec6\u8282\u4e0e\u5149\u8c31\u4fdd\u771f\u5ea6\u7ea0\u7f20\u5728\u4e00\u8d77\uff0c\u5bfc\u81f4\u5149\u8c31\u5931\u771f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u663e\u5f0f\u89e3\u8026\u9891\u7387\u4fe1\u606f\u7684\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u75282D Haar DWT\u5904\u7406\u5168\u8272\u56fe\u50cf\u5b9a\u4f4d\u7a7a\u95f4\u8fb9\u7f18\u548c\u7eb9\u7406\uff0c\u4f7f\u7528\u901a\u9053\u7ea71D Haar DWT\u5904\u7406\u6bcf\u4e2a\u50cf\u7d20\u7684\u5149\u8c31\u5206\u79bb\u4f4e\u9891/\u9ad8\u9891\u5206\u91cf\u3002\u901a\u8fc7\u5149\u8c31\u5206\u652f\u548c\u7a7a\u95f4\u5206\u652f\u7684\u53cc\u5206\u652f\u7ed3\u6784\uff0c\u7ed3\u5408\u57fa\u4e8eMamba\u7684\u8de8\u6a21\u6001\u8c03\u5236\u548c\u52a8\u6001\u95e8\u878d\u5408\u673a\u5236\u3002", "result": "\u5728WV3\u3001GF2\u548cQB\u6570\u636e\u96c6\u4e0a\uff0cS2WMamba\u5339\u914d\u6216\u8d85\u8d8a\u4e86FusionMamba\u3001CANNet\u7b49\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0cPSNR\u6700\u9ad8\u63d0\u53470.23 dB\uff0c\u5728\u5b8c\u6574\u5206\u8fa8\u7387WV3\u4e0a\u8fbe\u5230HQNR 0.956\u3002", "conclusion": "\u63d0\u51fa\u7684S2WMamba\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u9891\u7387\u89e3\u8026\u548c\u8f7b\u91cf\u7ea7\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5168\u8272\u9510\u5316\u4e2d\u7684\u7a7a\u95f4-\u5149\u8c31\u7ea0\u7f20\u95ee\u9898\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.07091", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07091", "abs": "https://arxiv.org/abs/2512.07091", "authors": ["Tomoya Takahashi", "Yusaku Nakajima", "Cristian Camilo Beltran-Hernandez", "Yuki Kuroda", "Kazutoshi Tanaka", "Masashi Hamaya", "Kanta Ono", "Yoshitaka Ushiku"], "title": "A Flexible Funnel-Shaped Robotic Hand with an Integrated Single-Sheet Valve for Milligram-Scale Powder Handling", "comment": "9 pages, 8 figures", "summary": "Laboratory Automation (LA) has the potential to accelerate solid-state materials discovery by enabling continuous robotic operation without human intervention. While robotic systems have been developed for tasks such as powder grinding and X-ray diffraction (XRD) analysis, fully automating powder handling at the milligram scale remains a significant challenge due to the complex flow dynamics of powders and the diversity of laboratory tasks. To address this challenge, this study proposes a novel, funnel-shaped, flexible robotic hand that preserves the softness and conical sheet designs in prior work while incorporating a controllable valve at the cone apex to enable precise, incremental dispensing of milligram-scale powder quantities. The hand is integrated with an external balance through a feedback control system based on a model of powder flow and online parameter identification. Experimental evaluations with glass beads, monosodium glutamate, and titanium dioxide demonstrated that 80% of the trials achieved an error within 2 mg, and the maximum error observed was approximately 20 mg across a target range of 20 mg to 3 g. In addition, by incorporating flow prediction models commonly used for hoppers and performing online parameter identification, the system is able to adapt to variations in powder dynamics. Compared to direct PID control, the proposed model-based control significantly improved both accuracy and convergence speed. These results highlight the potential of the proposed system to enable efficient and flexible powder weighing, with scalability toward larger quantities and applicability to a broad range of laboratory automation tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6f0f\u6597\u5f62\u67d4\u6027\u673a\u5668\u4eba\u624b\uff0c\u7528\u4e8e\u89e3\u51b3\u6beb\u514b\u7ea7\u7c89\u672b\u5904\u7406\u7684\u81ea\u52a8\u5316\u6311\u6218\uff0c\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u548c\u5728\u7ebf\u53c2\u6570\u8bc6\u522b\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7c89\u672b\u5206\u914d\u3002", "motivation": "\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u5728\u56fa\u6001\u6750\u6599\u53d1\u73b0\u4e2d\u5177\u6709\u52a0\u901f\u6f5c\u529b\uff0c\u4f46\u6beb\u514b\u7ea7\u7c89\u672b\u5904\u7406\u56e0\u7c89\u672b\u6d41\u52a8\u590d\u6742\u6027\u548c\u5b9e\u9a8c\u4efb\u52a1\u591a\u6837\u6027\u800c\u96be\u4ee5\u5b8c\u5168\u81ea\u52a8\u5316\u3002", "method": "\u5f00\u53d1\u4e86\u6f0f\u6597\u5f62\u67d4\u6027\u673a\u5668\u4eba\u624b\uff0c\u7ed3\u5408\u53ef\u63a7\u9600\u95e8\u548c\u57fa\u4e8e\u7c89\u672b\u6d41\u52a8\u6a21\u578b\u7684\u53cd\u9988\u63a7\u5236\u7cfb\u7edf\uff0c\u96c6\u6210\u5916\u90e8\u5929\u5e73\u8fdb\u884c\u5728\u7ebf\u53c2\u6570\u8bc6\u522b\u3002", "result": "\u5b9e\u9a8c\u663e\u793a80%\u7684\u8bd5\u9a8c\u8bef\u5dee\u57282mg\u4ee5\u5185\uff0c\u6700\u5927\u8bef\u5dee\u7ea620mg\uff08\u76ee\u6807\u8303\u56f420mg-3g\uff09\uff0c\u76f8\u6bd4\u76f4\u63a5PID\u63a7\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u9ad8\u6548\u7075\u6d3b\u7c89\u672b\u79f0\u91cd\u7684\u6f5c\u529b\uff0c\u5177\u6709\u5411\u66f4\u5927\u89c4\u6a21\u6269\u5c55\u7684\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4efb\u52a1\u3002"}}
{"id": "2512.06490", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06490", "abs": "https://arxiv.org/abs/2512.06490", "authors": ["Agatsya Yadav", "Renta Chintala Bhargavi"], "title": "Optimizing LLMs Using Quantization for Mobile Execution", "comment": "11 pages, 1 equation, 2 tables. Author Accepted Manuscript (AAM) of a paper published in Springer LNNS, ICT4SD 2025. DOI: 10.1007/978-3-032-06697-8_33", "summary": "Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u75284\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316(PTQ)\u6280\u672f\u538b\u7f29LLaMA 3.2 3B\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5728Android\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1168.66%\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u7531\u4e8e\u5176\u5e9e\u5927\u7684\u89c4\u6a21\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u3002", "method": "\u4f7f\u7528BitsAndBytes\u5e93\u548cHugging Face Transformers\u6846\u67b6\u5bf9Meta\u7684LLaMA 3.2 3B\u6a21\u578b\u8fdb\u884c4\u4f4dPTQ\uff0c\u7136\u540e\u901a\u8fc7llama.cpp\u5de5\u5177\u8f6c\u6362\u4e3aGGUF\u683c\u5f0f\u4ee5\u4f18\u5316\u79fb\u52a8\u7aef\u63a8\u7406\u3002", "result": "\u91cf\u5316\u540e\u6a21\u578b\u5927\u5c0f\u51cf\u5c1168.66%\uff0c\u80fd\u591f\u5728Android\u8bbe\u5907\u4e0a\u6210\u529f\u6267\u884c\u63a8\u7406\u4efb\u52a1\uff0c\u901a\u8fc7Termux\u73af\u5883\u548cOllama\u6846\u67b6\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "4\u4f4dPTQ\u7ed3\u5408GGUF\u7b49\u79fb\u52a8\u4f18\u5316\u683c\u5f0f\uff0c\u4e3a\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u80fd\u529b\u5f3a\u5927\u7684LLM\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5728\u6a21\u578b\u5927\u5c0f\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2512.06332", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06332", "abs": "https://arxiv.org/abs/2512.06332", "authors": ["Jeffrey Gu", "Minkyu Jeon", "Ambri Ma", "Serena Yeung-Levy", "Ellen D. Zhong"], "title": "CryoHype: Reconstructing a thousand cryo-EM structures with transformer-based hypernetworks", "comment": null, "summary": "Cryo-electron microscopy (cryo-EM) is an indispensable technique for determining the 3D structures of dynamic biomolecular complexes. While typically applied to image a single molecular species, cryo-EM has the potential for structure determination of many targets simultaneously in a high-throughput fashion. However, existing methods typically focus on modeling conformational heterogeneity within a single or a few structures and are not designed to resolve compositional heterogeneity arising from mixtures of many distinct molecular species. To address this challenge, we propose CryoHype, a transformer-based hypernetwork for cryo-EM reconstruction that dynamically adjusts the weights of an implicit neural representation. Using CryoHype, we achieve state-of-the-art results on a challenging benchmark dataset containing 100 structures. We further demonstrate that CryoHype scales to the reconstruction of 1,000 distinct structures from unlabeled cryo-EM images in the fixed-pose setting.", "AI": {"tldr": "CryoHype\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u8d85\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u51b7\u51bb\u7535\u955c\u56fe\u50cf\u4e2d\u591a\u5206\u5b50\u7269\u79cd\u6df7\u5408\u7684\u91cd\u5efa\u6311\u6218\uff0c\u80fd\u591f\u540c\u65f6\u91cd\u5efa\u591a\u8fbe1000\u4e2a\u4e0d\u540c\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u51b7\u51bb\u7535\u955c\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6216\u5c11\u6570\u7ed3\u6784\u7684\u6784\u8c61\u5f02\u8d28\u6027\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u7531\u8bb8\u591a\u4e0d\u540c\u5206\u5b50\u7269\u79cd\u6df7\u5408\u4ea7\u751f\u7684\u7ec4\u6210\u5f02\u8d28\u6027\uff0c\u9650\u5236\u4e86\u9ad8\u901a\u91cf\u7ed3\u6784\u6d4b\u5b9a\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faCryoHype\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u8d85\u7f51\u7edc\u52a8\u6001\u8c03\u6574\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u7684\u6743\u91cd\uff0c\u5b9e\u73b0\u5bf9\u591a\u4e2a\u5206\u5b50\u7269\u79cd\u7684\u540c\u65f6\u91cd\u5efa\u3002", "result": "\u5728\u5305\u542b100\u4e2a\u7ed3\u6784\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5e76\u5728\u56fa\u5b9a\u59ff\u6001\u8bbe\u7f6e\u4e0b\u6210\u529f\u4ece\u65e0\u6807\u7b7e\u51b7\u51bb\u7535\u955c\u56fe\u50cf\u91cd\u5efa1000\u4e2a\u4e0d\u540c\u7ed3\u6784\u3002", "conclusion": "CryoHype\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u4e3a\u9ad8\u901a\u91cf\u51b7\u51bb\u7535\u955c\u7ed3\u6784\u6d4b\u5b9a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.07114", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07114", "abs": "https://arxiv.org/abs/2512.07114", "authors": ["Jue Wang", "Mingsong Jiang", "Luis A. Ramirez", "Bilige Yang", "Mujun Zhang", "Esteban Figueroa", "Wenzhong Yan", "Rebecca Kramer-Bottiglio"], "title": "Surrogate compliance modeling enables reinforcement learned locomotion gaits for soft robots", "comment": null, "summary": "Adaptive morphogenetic robots adapt their morphology and control policies to meet changing tasks and environmental conditions. Many such systems leverage soft components, which enable shape morphing but also introduce simulation and control challenges. Soft-body simulators remain limited in accuracy and computational tractability, while rigid-body simulators cannot capture soft-material dynamics. Here, we present a surrogate compliance modeling approach: rather than explicitly modeling soft-body physics, we introduce indirect variables representing soft-material deformation within a rigid-body simulator. We validate this approach using our amphibious robotic turtle, a quadruped with soft morphing limbs designed for multi-environment locomotion. By capturing deformation effects as changes in effective limb length and limb center of mass, and by applying reinforcement learning with extensive randomization of these indirect variables, we achieve reliable policy learning entirely in a rigid-body simulation. The resulting gaits transfer directly to hardware, demonstrating high-fidelity sim-to-real performance on hard, flat substrates and robust, though lower-fidelity, transfer on rheologically complex terrains. The learned closed-loop gaits exhibit unprecedented terrestrial maneuverability and achieve an order-of-magnitude reduction in cost of transport compared to open-loop baselines. Field experiments with the robot further demonstrate stable, multi-gait locomotion across diverse natural terrains, including gravel, grass, and mud.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66ff\u4ee3\u67d4\u987a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u521a\u4f53\u6a21\u62df\u5668\u4e2d\u5f15\u5165\u4ee3\u8868\u8f6f\u6750\u6599\u53d8\u5f62\u7684\u95f4\u63a5\u53d8\u91cf\uff0c\u89e3\u51b3\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u4eff\u771f\u548c\u63a7\u5236\u7684\u6311\u6218\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u80fd\u591f\u9002\u5e94\u5f62\u6001\u53d8\u5316\uff0c\u4f46\u8f6f\u4f53\u6a21\u62df\u5668\u5728\u7cbe\u5ea6\u548c\u8ba1\u7b97\u53ef\u884c\u6027\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u800c\u521a\u4f53\u6a21\u62df\u5668\u65e0\u6cd5\u6355\u6349\u8f6f\u6750\u6599\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u66ff\u4ee3\u67d4\u987a\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u5c06\u53d8\u5f62\u6548\u5e94\u8868\u793a\u4e3a\u6709\u6548\u80a2\u4f53\u957f\u5ea6\u548c\u80a2\u4f53\u8d28\u5fc3\u7684\u53d8\u5316\uff0c\u5728\u521a\u4f53\u6a21\u62df\u5668\u4e2d\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u5e76\u968f\u673a\u5316\u8fd9\u4e9b\u95f4\u63a5\u53d8\u91cf\u3002", "result": "\u5728\u786c\u8d28\u5e73\u5766\u57fa\u5e95\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u5ea6\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u6027\u80fd\uff0c\u5728\u6d41\u53d8\u590d\u6742\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u4f46\u4fdd\u771f\u5ea6\u8f83\u4f4e\u7684\u8f6c\u79fb\u3002\u5b66\u4e60\u5230\u7684\u95ed\u73af\u6b65\u6001\u5c55\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u9646\u5730\u673a\u52a8\u6027\uff0c\u8fd0\u8f93\u6210\u672c\u6bd4\u5f00\u73af\u57fa\u7ebf\u964d\u4f4e\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u521a\u4f53\u6a21\u62df\u5668\u4e2d\u8fdb\u884c\u53ef\u9760\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u76f4\u63a5\u8f6c\u79fb\u5230\u786c\u4ef6\u4e0a\uff0c\u5728\u591a\u79cd\u81ea\u7136\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u591a\u6b65\u6001\u8fd0\u52a8\u3002"}}
{"id": "2512.06511", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.06511", "abs": "https://arxiv.org/abs/2512.06511", "authors": ["Mengqi Xu", "Subha Maity", "Joel Dubin"], "title": "Diagnosis-based mortality prediction for intensive care unit patients via transfer learning", "comment": null, "summary": "In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5728ICU\u8bca\u65ad\u7279\u5f02\u6027\u6b7b\u4ea1\u7387\u9884\u6d4b\u4e2d\u5e94\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u53d1\u73b0\u8fc1\u79fb\u5b66\u4e60\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4ec5\u4f7f\u7528\u8bca\u65ad\u7279\u5f02\u6027\u6570\u636e\u6216APACHE IVa\u8bc4\u5206\u7684\u6a21\u578b\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u6821\u51c6\u6027\u3002", "motivation": "ICU\u4e2d\u5371\u91cd\u75c5\u7684\u6839\u672c\u539f\u56e0\u5728\u4e0d\u540c\u8bca\u65ad\u95f4\u5dee\u5f02\u5f88\u5927\uff0c\u4f46\u8003\u8651\u8bca\u65ad\u5f02\u8d28\u6027\u7684\u9884\u6d4b\u6a21\u578b\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u5728eICU\u534f\u4f5c\u7814\u7a76\u6570\u636e\u5e93\u4e2d\u5e94\u7528\u57fa\u4e8eGLM\u548cXGBoost\u7684\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bca\u65ad\u7279\u5f02\u6027\u6b7b\u4ea1\u7387\u9884\u6d4b\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u6301\u7eed\u4f18\u4e8e\u4ec5\u4f7f\u7528\u8bca\u65ad\u7279\u5f02\u6027\u6570\u636e\u7684\u6a21\u578b\u548c\u5355\u72ec\u4f7f\u7528APACHE IVa\u8bc4\u5206\u7684\u6a21\u578b\uff0c\u540c\u65f6\u6bd4\u57fa\u4e8e\u6c47\u603b\u6570\u636e\u7684\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u6821\u51c6\u6027\u3002Youden\u622a\u65ad\u503c\u6bd4\u4f20\u7edf\u76840.5\u622a\u65ad\u503c\u66f4\u9002\u5408\u4e8c\u5143\u7ed3\u5c40\u9884\u6d4b\u3002", "conclusion": "\u8fc1\u79fb\u5b66\u4e60\u5728ICU\u8bca\u65ad\u7279\u5f02\u6027\u6b7b\u4ea1\u7387\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0cYouden\u622a\u65ad\u503c\u662f\u66f4\u5408\u9002\u7684\u51b3\u7b56\u9608\u503c\uff0c\u8fc1\u79fb\u5b66\u4e60\u5728\u4e0d\u540c\u622a\u65ad\u6807\u51c6\u4e0b\u5747\u80fd\u4fdd\u6301\u9ad8\u9884\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.06344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06344", "abs": "https://arxiv.org/abs/2512.06344", "authors": ["Kaile Wang", "Lijun He", "Haisheng Fu", "Haixia Bi", "Fan Li"], "title": "Beyond Hallucinations: A Multimodal-Guided Task-Aware Generative Image Compression for Ultra-Low Bitrate", "comment": null, "summary": "Generative image compression has recently shown impressive perceptual quality, but often suffers from semantic deviations caused by generative hallucinations at ultra-low bitrate (bpp < 0.05), limiting its reliable deployment in bandwidth-constrained 6G semantic communication scenarios. In this work, we reassess the positioning and role of of multimodal guidance, and propose a Multimodal-Guided Task-Aware Generative Image Compression (MTGC) framework. Specifically, MTGC integrates three guidance modalities to enhance semantic consistency: a concise but robust text caption for global semantics, a highly compressed image (HCI) retaining low-level visual information, and Semantic Pseudo-Words (SPWs) for fine-grained task-relevant semantics. The SPWs are generated by our designed Task-Aware Semantic Compression Module (TASCM), which operates in a task-oriented manner to drive the multi-head self-attention mechanism to focus on and extract semantics relevant to the generation task while filtering out redundancy. Subsequently, to facilitate the synergistic guidance of these modalities, we design a Multimodal-Guided Diffusion Decoder (MGDD) employing a dual-path cooperative guidance mechanism that synergizes cross-attention and ControlNet additive residuals to precisely inject these three guidance into the diffusion process, and leverages the diffusion model's powerful generative priors to reconstruct the image. Extensive experiments demonstrate that MTGC consistently improves semantic consistency (e.g., DISTS drops by 10.59% on the DIV2K dataset) while also achieving remarkable gains in perceptual quality and pixel-level fidelity at ultra-low bitrate.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u5f15\u5bfc\u7684\u4efb\u52a1\u611f\u77e5\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u6846\u67b6(MTGC)\uff0c\u901a\u8fc7\u6574\u5408\u6587\u672c\u63cf\u8ff0\u3001\u9ad8\u538b\u7f29\u56fe\u50cf\u548c\u8bed\u4e49\u4f2a\u8bcd\u4e09\u79cd\u5f15\u5bfc\u6a21\u6001\uff0c\u5728\u8d85\u4f4e\u7801\u7387\u4e0b\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u751f\u6210\u5f0f\u56fe\u50cf\u538b\u7f29\u5728\u8d85\u4f4e\u7801\u7387\u4e0b\u5b58\u5728\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u57286G\u8bed\u4e49\u901a\u4fe1\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u3002\u9700\u8981\u89e3\u51b3\u751f\u6210\u5e7b\u89c9\u5bfc\u81f4\u7684\u8bed\u4e49\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "method": "MTGC\u6846\u67b6\u5305\u542b\u4efb\u52a1\u611f\u77e5\u8bed\u4e49\u538b\u7f29\u6a21\u5757(TASCM)\u751f\u6210\u8bed\u4e49\u4f2a\u8bcd\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u5f15\u5bfc\u6269\u6563\u89e3\u7801\u5668(MGDD)\u91c7\u7528\u53cc\u8def\u5f84\u534f\u4f5c\u5f15\u5bfc\u673a\u5236\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548cControlNet\u6b8b\u5dee\u6ce8\u5165\u4e09\u79cd\u5f15\u5bfc\u6a21\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMTGC\u663e\u8457\u63d0\u5347\u8bed\u4e49\u4e00\u81f4\u6027(DISTS\u5728DIV2K\u6570\u636e\u96c6\u4e0a\u964d\u4f4e10.59%)\uff0c\u540c\u65f6\u5728\u8d85\u4f4e\u7801\u7387\u4e0b\u83b7\u5f97\u4f18\u5f02\u7684\u611f\u77e5\u8d28\u91cf\u548c\u50cf\u7d20\u7ea7\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u5f15\u5bfc\u6709\u6548\u89e3\u51b3\u4e86\u8d85\u4f4e\u7801\u7387\u4e0b\u7684\u8bed\u4e49\u504f\u5dee\u95ee\u9898\uff0c\u4e3a6G\u8bed\u4e49\u901a\u4fe1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u56fe\u50cf\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07130", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07130", "abs": "https://arxiv.org/abs/2512.07130", "authors": ["Zebin Xing", "Yupeng Zheng", "Qichao Zhang", "Zhixing Ding", "Pengxuan Yang", "Songen Gu", "Zhongpu Xia", "Dongbin Zhao"], "title": "Mimir: Hierarchical Goal-Driven Diffusion with Uncertainty Propagation for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving has emerged as a pivotal direction in the field of autonomous systems. Recent works have demonstrated impressive performance by incorporating high-level guidance signals to steer low-level trajectory planners. However, their potential is often constrained by inaccurate high-level guidance and the computational overhead of complex guidance modules. To address these limitations, we propose Mimir, a novel hierarchical dual-system framework capable of generating robust trajectories relying on goal points with uncertainty estimation: (1) Unlike previous approaches that deterministically model, we estimate goal point uncertainty with a Laplace distribution to enhance robustness; (2) To overcome the slow inference speed of the guidance system, we introduce a multi-rate guidance mechanism that predicts extended goal points in advance. Validated on challenging Navhard and Navtest benchmarks, Mimir surpasses previous state-of-the-art methods with a 20% improvement in the driving score EPDMS, while achieving 1.6 times improvement in high-level module inference speed without compromising accuracy. The code and models will be released soon to promote reproducibility and further development. The code is available at https://github.com/ZebinX/Mimir-Uncertainty-Driving", "AI": {"tldr": "Mimir\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5206\u5c42\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u751f\u6210\u9c81\u68d2\u8f68\u8ff9\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4e2d\u9ad8\u7ea7\u5f15\u5bfc\u4fe1\u53f7\u4e0d\u51c6\u786e\u548c\u590d\u6742\u5f15\u5bfc\u6a21\u5757\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u62c9\u666e\u62c9\u65af\u5206\u5e03\u4f30\u8ba1\u76ee\u6807\u70b9\u4e0d\u786e\u5b9a\u6027\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u5f15\u5165\u591a\u901f\u7387\u5f15\u5bfc\u673a\u5236\u63d0\u524d\u9884\u6d4b\u6269\u5c55\u76ee\u6807\u70b9\u63d0\u5347\u63a8\u7406\u901f\u5ea6", "result": "\u5728Navhard\u548cNavtest\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9a7e\u9a76\u5206\u6570EPDMS\u63d0\u534720%\uff0c\u9ad8\u7ea7\u6a21\u5757\u63a8\u7406\u901f\u5ea6\u63d0\u53471.6\u500d\u4e14\u4e0d\u635f\u5931\u7cbe\u5ea6", "conclusion": "Mimir\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.06520", "categories": ["cs.LG", "cond-mat.stat-mech", "physics.comp-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.06520", "abs": "https://arxiv.org/abs/2512.06520", "authors": ["Zihan Pengmei", "Spencer C. Guo", "Chatipat Lorpaiboon", "Aaron R. Dinner"], "title": "Hierarchical geometric deep learning enables scalable analysis of molecular dynamics", "comment": "17 pages, 12 figures", "summary": "Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u4fe1\u606f\u805a\u5408\u6765\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u4f7f\u5f97\u80fd\u591f\u5728\u5927\u89c4\u6a21\u751f\u7269\u5206\u5b50\u7cfb\u7edf\uff08\u5982\u6570\u5343\u4e2a\u6b8b\u57fa\u7684\u86cb\u767d\u8d28-\u6838\u9178\u590d\u5408\u7269\uff09\u4e0a\u9ad8\u6548\u5206\u6790\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8f68\u8ff9\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u53ef\u4ee5\u751f\u6210\u590d\u6742\u7cfb\u7edf\u7684\u539f\u5b50\u7ea7\u8f68\u8ff9\uff0c\u4f46\u5f53\u7cfb\u7edf\u7f3a\u4e4f\u660e\u786e\u7684\u5b9a\u91cf\u63cf\u8ff0\u7b26\u65f6\uff0c\u5206\u6790\u8fd9\u4e9b\u52a8\u6001\u53d8\u5f97\u56f0\u96be\u3002\u4f20\u7edf\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u5927\u89c4\u6a21\u751f\u7269\u5206\u5b50\u7cfb\u7edf\u65f6\u9762\u4e34\u5185\u5b58\u548c\u8fd0\u884c\u65f6\u9650\u5236\uff0c\u4ee5\u53ca\u96be\u4ee5\u6355\u6349\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5c40\u90e8\u4fe1\u606f\u805a\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u6d88\u606f\u4f20\u9012\u8fc7\u7a0b\u4e2d\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u5b50\u7ea7\u7ec6\u8282\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u5355GPU\u4e0a\u5feb\u901f\u5206\u6790\u5305\u542b\u6570\u5343\u4e2a\u6b8b\u57fa\u7684\u86cb\u767d\u8d28-\u6838\u9178\u590d\u5408\u7269\u6a21\u62df\u3002", "result": "\u8be5\u65b9\u6cd5\u4f7f\u5f97\u5728\u5355GPU\u4e0a\u51e0\u5206\u949f\u5185\u5206\u6790\u6570\u5343\u4e2a\u6b8b\u57fa\u7684\u86cb\u767d\u8d28-\u6838\u9178\u590d\u5408\u7269\u6a21\u62df\u6210\u4e3a\u53ef\u80fd\u3002\u5bf9\u4e8e\u6570\u767e\u4e2a\u6b8b\u57fa\u7684\u7cfb\u7edf\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u5c40\u90e8\u4fe1\u606f\u805a\u5408\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u751f\u7269\u5206\u5b50\u7cfb\u7edf\u5206\u6790\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3a\u590d\u6742\u751f\u7269\u5206\u5b50\u52a8\u6001\u7684\u6df1\u5165\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u884c\u5de5\u5177\u3002"}}
{"id": "2512.06345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06345", "abs": "https://arxiv.org/abs/2512.06345", "authors": ["Xiangshuai Song", "Jun-Jie Huang", "Tianrui Liu", "Ke Liang", "Chang Tang"], "title": "CLUENet: Cluster Attention Makes Neural Networks Have Eyes", "comment": "10 pages, 6 figures, 2026 Association for the Advancement of Artificial Intelligence", "summary": "Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.", "AI": {"tldr": "CLUENet\u662f\u4e00\u79cd\u900f\u660e\u7684\u6df1\u5ea6\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u5c40\u8f6f\u805a\u5408\u4e0e\u786c\u5206\u914d\u3001\u6e29\u5ea6\u7f29\u653e\u4f59\u5f26\u6ce8\u610f\u529b\u3001\u95e8\u63a7\u6b8b\u5dee\u8fde\u63a5\u3001\u786c\u5171\u4eab\u7279\u5f81\u5206\u53d1\u548c\u6539\u8fdb\u7684\u805a\u7c7b\u6c60\u5316\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5377\u79ef\u548c\u6ce8\u610f\u529b\u6a21\u578b\u5728\u5efa\u6a21\u4e0d\u89c4\u5219\u7a7a\u95f4\u6a21\u5f0f\u53ca\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u548c\u6ce8\u610f\u529b\u6a21\u578b\u5177\u6709\u521a\u6027\u611f\u53d7\u91ce\u548c\u590d\u6742\u67b6\u6784\uff0c\u9650\u5236\u4e86\u5176\u5bf9\u4e0d\u89c4\u5219\u7a7a\u95f4\u6a21\u5f0f\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u963b\u788d\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u7684\u4efb\u52a1\u4e2d\u3002\u805a\u7c7b\u8303\u5f0f\u867d\u7136\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7075\u6d3b\u7684\u8bed\u4e49\u5efa\u6a21\uff0c\u4f46\u5b58\u5728\u7cbe\u5ea6\u6709\u9650\u3001\u6548\u7387\u4f4e\u548c\u8bad\u7ec3\u4e2d\u68af\u5ea6\u6d88\u5931\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CLUENet\uff0c\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a(i) \u5168\u5c40\u8f6f\u805a\u5408\u4e0e\u786c\u5206\u914d\uff0c\u7ed3\u5408\u6e29\u5ea6\u7f29\u653e\u4f59\u5f26\u6ce8\u610f\u529b\u548c\u95e8\u63a7\u6b8b\u5dee\u8fde\u63a5\u4ee5\u589e\u5f3a\u5c40\u90e8\u5efa\u6a21\uff1b(ii) \u5757\u95f4\u786c\u5171\u4eab\u7279\u5f81\u5206\u53d1\uff1b(iii) \u6539\u8fdb\u7684\u805a\u7c7b\u6c60\u5316\u7b56\u7565\u3002", "result": "\u5728CIFAR-100\u548cMini-ImageNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCLUENet\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u805a\u7c7b\u65b9\u6cd5\u548c\u4e3b\u6d41\u89c6\u89c9\u6a21\u578b\uff0c\u540c\u65f6\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u900f\u660e\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "conclusion": "CLUENet\u901a\u8fc7\u521b\u65b0\u7684\u805a\u7c7b\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u9700\u8981\u9ad8\u900f\u660e\u5ea6\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07137", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.07137", "abs": "https://arxiv.org/abs/2512.07137", "authors": ["Kang Yijie", "Hao Yuqing", "Wang Qingyun", "Chen Guanrong"], "title": "Time-Varying Formation Tracking Control of Wheeled Mobile Robots With Region Constraint: A Generalized Udwadia-Kalaba Framework", "comment": "10 pages,9 figures", "summary": "In this paper, the time-varying formation tracking control of wheeled mobile robots with region constraint is investigated from a generalized Udwadia-Kalaba framework. The communication topology is directed, weighted and has a spanning tree with the leader being the root. By reformulating the time-varying formation tracking control objective as a constrained equation and transforming the region constraint by a diffeomorphism, the time-varying formation tracking controller with the region constraint is designed under the generalized Udwadia-Kalaba framework. Compared with the existing works on time-varying formation tracking control, the region constraint is takeninto account in this paper, which ensures the safety of the robots.Finally, some numerical simulations are presented to illustrate the effectiveness of the proposed control strategy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5177\u6709\u533a\u57df\u7ea6\u675f\u7684\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u95ee\u9898\uff0c\u57fa\u4e8e\u5e7f\u4e49Udwadia-Kalaba\u6846\u67b6\u8bbe\u8ba1\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5728\u6307\u5b9a\u533a\u57df\u5185\u5b89\u5168\u8fd0\u884c\u3002", "motivation": "\u73b0\u6709\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u7814\u7a76\u672a\u8003\u8651\u533a\u57df\u7ea6\u675f\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5e7f\u4e49Udwadia-Kalaba\u6846\u67b6\uff0c\u5c06\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u63a7\u5236\u76ee\u6807\u91cd\u6784\u4e3a\u7ea6\u675f\u65b9\u7a0b\uff0c\u901a\u8fc7\u5fae\u5206\u540c\u80da\u53d8\u6362\u5904\u7406\u533a\u57df\u7ea6\u675f\uff0c\u8bbe\u8ba1\u5177\u6709\u533a\u57df\u7ea6\u675f\u7684\u63a7\u5236\u5668\u3002", "result": "\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u63a7\u5236\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u63a7\u5236\u5668\u80fd\u591f\u5b9e\u73b0\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u540c\u65f6\u786e\u4fdd\u673a\u5668\u4eba\u59cb\u7ec8\u5728\u5b89\u5168\u533a\u57df\u5185\u8fd0\u884c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5e7f\u4e49Udwadia-Kalaba\u6846\u67b6\u7684\u63a7\u5236\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u53d8\u7f16\u961f\u8ddf\u8e2a\u4e2d\u7684\u533a\u57df\u7ea6\u675f\u95ee\u9898\uff0c\u4e3a\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06533", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06533", "abs": "https://arxiv.org/abs/2512.06533", "authors": ["Ming Chen", "Sheng Tang", "Rong-Xi Tan", "Ziniu Li", "Jiacheng Chen", "Ke Xue", "Chao Qian"], "title": "Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning", "comment": null, "summary": "Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6765\u6539\u8fdb\u57fa\u4e8e\u89e3\u7801\u7684\u56de\u5f52\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u5956\u52b1\u6765\u589e\u5f3a\u5168\u5c40\u6570\u503c\u4e00\u81f4\u6027\uff0c\u5728\u8868\u683c\u56de\u5f52\u548c\u4ee3\u7801\u5ea6\u91cf\u56de\u5f52\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u57fa\u4e8e\u89e3\u7801\u7684\u56de\u5f52\u65b9\u6cd5\u5b58\u5728\u79bb\u6563token\u7ea7\u76ee\u6807\u4e0e\u8fde\u7eed\u6570\u503c\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u76ee\u6807\u503c\u7684\u5168\u5c40\u5e45\u5ea6\uff0c\u9650\u5236\u4e86\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u751f\u6210\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u5e8f\u5217\u7ea7\u5956\u52b1\u6765\u5f3a\u5236\u5168\u5c40\u6570\u503c\u4e00\u81f4\u6027\uff0c\u5177\u4f53\u91c7\u7528ReMax\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u8868\u683c\u56de\u5f52\u548c\u4ee3\u7801\u5ea6\u91cf\u56de\u5f52\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684token\u7ea7\u57fa\u7ebf\u548c\u4f20\u7edf\u56de\u5f52\u5934\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\u548c\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u7684\u5f15\u5165\u4f7f\u57fa\u4e8e\u89e3\u7801\u7684\u56de\u5f52\u6210\u4e3a\u901a\u7528\u6570\u503c\u9884\u6d4b\u7684\u7a33\u5065\u4e14\u51c6\u786e\u7684\u8303\u5f0f\u3002"}}
{"id": "2512.06353", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06353", "abs": "https://arxiv.org/abs/2512.06353", "authors": ["Kaicheng Yang", "Kaisen Yang", "Baiting Wu", "Xun Zhang", "Qianrui Yang", "Haotong Qin", "He Zhang", "Yulun Zhang"], "title": "TreeQ: Pushing the Quantization Boundary of Diffusion Transformer via Tree-Structured Mixed-Precision Search", "comment": "Code and Supplementary Material could be found at https://github.com/racoonykc/TreeQ", "summary": "Diffusion Transformers (DiTs) have emerged as a highly scalable and effective backbone for image generation, outperforming U-Net architectures in both scalability and performance. However, their real-world deployment remains challenging due to high computational and memory demands. Mixed-Precision Quantization (MPQ), designed to push the limits of quantization, has demonstrated remarkable success in advancing U-Net quantization to sub-4bit settings while significantly reducing computational and memory overhead. Nevertheless, its application to DiT architectures remains limited and underexplored. In this work, we propose TreeQ, a unified framework addressing key challenges in DiT quantization. First, to tackle inefficient search and proxy misalignment, we introduce Tree Structured Search (TSS). This DiT-specific approach leverages the architecture's linear properties to traverse the solution space in O(n) time while improving objective accuracy through comparison-based pruning. Second, to unify optimization objectives, we propose Environmental Noise Guidance (ENG), which aligns Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) configurations using a single hyperparameter. Third, to mitigate information bottlenecks in ultra-low-bit regimes, we design the General Monarch Branch (GMB). This structured sparse branch prevents irreversible information loss, enabling finer detail generation. Through extensive experiments, our TreeQ framework demonstrates state-of-the-art performance on DiT-XL/2 under W3A3 and W4A4 PTQ/PEFT settings. Notably, our work is the first to achieve near-lossless 4-bit PTQ performance on DiT models. The code and models will be available at https://github.com/racoonykc/TreeQ", "AI": {"tldr": "TreeQ\u662f\u4e00\u4e2a\u7edf\u4e00\u7684DiT\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u7ed3\u6784\u641c\u7d22\u3001\u73af\u5883\u566a\u58f0\u5f15\u5bfc\u548c\u901a\u7528Monarch\u5206\u652f\u6280\u672f\uff0c\u9996\u6b21\u5728DiT\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u76844\u4f4dPTQ\u6027\u80fd\u3002", "motivation": "DiT\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4f46\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u9ad8\uff0c\u73b0\u6709\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u5728DiT\u67b6\u6784\u4e0a\u5e94\u7528\u6709\u9650\u4e14\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9DiT\u7279\u6027\u7684\u91cf\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faTreeQ\u6846\u67b6\uff1a1) \u6811\u7ed3\u6784\u641c\u7d22(TSS)\u5229\u7528DiT\u7ebf\u6027\u7279\u6027\u5728O(n)\u65f6\u95f4\u5185\u641c\u7d22\u89e3\u7a7a\u95f4\uff1b2) \u73af\u5883\u566a\u58f0\u5f15\u5bfc(ENG)\u7edf\u4e00PTQ\u548cQAT\u4f18\u5316\u76ee\u6807\uff1b3) \u901a\u7528Monarch\u5206\u652f(GMB)\u9632\u6b62\u8d85\u4f4e\u4f4d\u91cf\u5316\u4e2d\u7684\u4fe1\u606f\u74f6\u9888\u3002", "result": "\u5728DiT-XL/2\u6a21\u578b\u4e0a\uff0cW3A3\u548cW4A4 PTQ/PEFT\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9996\u6b21\u5b9e\u73b0DiT\u6a21\u578b\u8fd1\u4e4e\u65e0\u635f\u76844\u4f4dPTQ\u6027\u80fd\u3002", "conclusion": "TreeQ\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86DiT\u91cf\u5316\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3aDiT\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5f00\u6e90\u3002"}}
{"id": "2512.07177", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07177", "abs": "https://arxiv.org/abs/2512.07177", "authors": ["Fanjun Bu", "Melina Tsai", "Audrey Tjokro", "Tapomayukh Bhattacharjee", "Jorge Ortiz", "Wendy Ju"], "title": "Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction", "comment": null, "summary": "Robots operating in everyday environments must often decide when and whether to engage with people, yet such decisions often hinge on subtle nonverbal cues that unfold over time and are difficult to model explicitly. Drawing on a five-day Wizard-of-Oz deployment of a mobile service robot in a university cafe, we analyze how people signal interaction readiness through nonverbal behaviors and how expert wizards use these cues to guide engagement. Motivated by these observations, we propose a two-stage pipeline in which lightweight perceptual detectors (gaze shifts and proxemics) are used to selectively trigger heavier video-based vision-language model (VLM) queries at socially meaningful moments. We evaluate this pipeline on replayed field interactions and compare two prompting strategies. Our findings suggest that selectively using VLMs as proxies for social reasoning enables socially responsive robot behavior, allowing robots to act appropriately by attending to the cues people naturally provide in real-world interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u7ba1\u9053\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u611f\u77e5\u68c0\u6d4b\u5668\u89e6\u53d1\u57fa\u4e8e\u89c6\u9891\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u67e5\u8be2\uff0c\u4ee5\u5b9e\u73b0\u793e\u4ea4\u54cd\u5e94\u5f0f\u673a\u5668\u4eba\u884c\u4e3a", "motivation": "\u673a\u5668\u4eba\u5728\u65e5\u5e38\u73af\u5883\u4e2d\u9700\u8981\u6839\u636e\u5fae\u5999\u7684\u975e\u8bed\u8a00\u7ebf\u7d22\u51b3\u5b9a\u662f\u5426\u4e0e\u4eba\u4e92\u52a8\uff0c\u8fd9\u4e9b\u7ebf\u7d22\u96be\u4ee5\u663e\u5f0f\u5efa\u6a21", "method": "\u57fa\u4e8e5\u5929Wizard-of-Oz\u5b9e\u5730\u90e8\u7f72\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u5148\u4f7f\u7528\u8f7b\u91cf\u7ea7\u611f\u77e5\u68c0\u6d4b\u5668\uff08\u6ce8\u89c6\u8f6c\u79fb\u548c\u7a7a\u95f4\u8ddd\u79bb\uff09\u68c0\u6d4b\u5173\u952e\u65f6\u523b\uff0c\u518d\u89e6\u53d1\u89c6\u9891VLM\u67e5\u8be2", "result": "\u5728\u91cd\u653e\u7684\u5b9e\u5730\u4e92\u52a8\u4e2d\u8bc4\u4f30\u8be5\u7ba1\u9053\uff0c\u6bd4\u8f83\u4e24\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u8bc1\u660e\u9009\u62e9\u6027\u4f7f\u7528VLM\u4f5c\u4e3a\u793e\u4ea4\u63a8\u7406\u4ee3\u7406\u53ef\u5b9e\u73b0\u793e\u4ea4\u54cd\u5e94\u884c\u4e3a", "conclusion": "\u9009\u62e9\u6027\u4f7f\u7528VLM\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5173\u6ce8\u4eba\u4eec\u5728\u771f\u5b9e\u4e92\u52a8\u4e2d\u81ea\u7136\u63d0\u4f9b\u7684\u7ebf\u7d22\uff0c\u4ece\u800c\u91c7\u53d6\u9002\u5f53\u884c\u4e3a"}}
{"id": "2512.06547", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.06547", "abs": "https://arxiv.org/abs/2512.06547", "authors": ["Xiaocan Li", "Shiliang Wu", "Zheng Shen"], "title": "A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation", "comment": null, "summary": "Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md", "AI": {"tldr": "A-3PO\u662f\u4e00\u79cd\u8fd1\u4f3c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u63d2\u503c\u8fd1\u4f3c\u8fd1\u7aef\u7b56\u7565\uff0c\u6d88\u9664\u5f02\u6b65RL\u4e2d\u989d\u5916\u524d\u5411\u4f20\u64ad\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u51cf\u5c1118%\u8bad\u7ec3\u65f6\u95f4\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u8026\u635f\u5931\u5728\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u4e2d\u5904\u7406\u6570\u636e\u9648\u65e7\u6027\u95ee\u9898\u5f88\u6709\u6548\uff0c\u4f46\u8fd1\u7aef\u7b56\u7565\u9700\u8981\u5728\u6bcf\u4e2a\u8bad\u7ec3\u6b65\u9aa4\u8fdb\u884c\u989d\u5916\u524d\u5411\u4f20\u64ad\uff0c\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9020\u6210\u8ba1\u7b97\u74f6\u9888\u3002", "method": "\u63d0\u51faA-3PO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5355\u63d2\u503c\u8fd1\u4f3c\u8fd1\u7aef\u7b56\u7565\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u8fd1\u7aef\u7b56\u7565\uff0c\u4ece\u800c\u6d88\u9664\u8ba1\u7b97\u5f00\u9500\u3002", "result": "A-3PO\u51cf\u5c1118%\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "A-3PO\u901a\u8fc7\u8fd1\u4f3c\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6b65RL\u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2512.06358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06358", "abs": "https://arxiv.org/abs/2512.06358", "authors": ["Mingjia Li", "Jin Hu", "Hainuo Wang", "Qiming Hu", "Jiarui Wang", "Xiaojie Guo"], "title": "Rectifying Latent Space for Generative Single-Image Reflection Removal", "comment": null, "summary": "Single-image reflection removal is a highly ill-posed problem, where existing methods struggle to reason about the composition of corrupted regions, causing them to fail at recovery and generalization in the wild. This work reframes an editing-purpose latent diffusion model to effectively perceive and process highly ambiguous, layered image inputs, yielding high-quality outputs. We argue that the challenge of this conversion stems from a critical yet overlooked issue, i.e., the latent space of semantic encoders lacks the inherent structure to interpret a composite image as a linear superposition of its constituent layers. Our approach is built on three synergistic components, including a reflection-equivariant VAE that aligns the latent space with the linear physics of reflection formation, a learnable task-specific text embedding for precise guidance that bypasses ambiguous language, and a depth-guided early-branching sampling strategy to harness generative stochasticity for promising results. Extensive experiments reveal that our model achieves new SOTA performance on multiple benchmarks and generalizes well to challenging real-world cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u6765\u89e3\u51b3\u53cd\u5c04\u56fe\u50cf\u5206\u89e3\u7684\u6a21\u7cca\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u53cd\u5c04\u56fe\u50cf\u4e2d\u6a21\u7cca\u533a\u57df\u7684\u7ec4\u6210\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6062\u590d\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u5173\u952e\u5728\u4e8e\u8bed\u4e49\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\u7f3a\u4e4f\u5c06\u590d\u5408\u56fe\u50cf\u89e3\u91ca\u4e3a\u7ebf\u6027\u53e0\u52a0\u5c42\u7684\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a\u53cd\u5c04\u7b49\u53d8VAE\uff08\u4f7f\u6f5c\u5728\u7a7a\u95f4\u4e0e\u53cd\u5c04\u5f62\u6210\u7684\u7ebf\u6027\u7269\u7406\u5bf9\u9f50\uff09\u3001\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u7279\u5b9a\u6587\u672c\u5d4c\u5165\uff08\u63d0\u4f9b\u7cbe\u786e\u6307\u5bfc\uff09\u3001\u6df1\u5ea6\u5f15\u5bfc\u7684\u65e9\u671f\u5206\u652f\u91c7\u6837\u7b56\u7565\uff08\u5229\u7528\u751f\u6210\u968f\u673a\u6027\u83b7\u5f97\u66f4\u597d\u7ed3\u679c\uff09\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\uff0c\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u53cd\u5c04\u5206\u79bb\u6548\u679c\u3002"}}
{"id": "2512.07221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07221", "abs": "https://arxiv.org/abs/2512.07221", "authors": ["Zichao Shu", "Shitao Bei", "Lijun Li", "Zetao Chen"], "title": "Spatiotemporal Calibration and Ground Truth Estimation for High-Precision SLAM Benchmarking in Extended Reality", "comment": null, "summary": "Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fde\u7eed\u65f6\u95f4\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u6574\u5408IMU\u6570\u636e\u8865\u507f\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u7684\u6296\u52a8\uff0c\u5e76\u91c7\u7528\u53ef\u53d8\u65f6\u95f4\u540c\u6b65\u65b9\u6cd5\u548c\u57fa\u4e8e\u87ba\u65cb\u7ea6\u675f\u7684\u4f4d\u59ff\u6b8b\u5dee\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684SLAM\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u7531\u4e8e\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u5728\u65f6\u7a7a\u6807\u5b9a\u548c\u56fa\u6709\u6296\u52a8\u65b9\u9762\u7684\u9650\u5236\uff0c\u73b0\u6709\u7684SLAM\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3XR\u5e94\u7528\u5bf9\u65cb\u8f6c\u8bef\u5dee\u548c\u5e27\u95f4\u6296\u52a8\u7b49\u5173\u952e\u6307\u6807\u7684\u7cbe\u786e\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8fde\u7eed\u65f6\u95f4\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u6574\u5408IMU\u6570\u636e\u8865\u507fMoCap\u6296\u52a8\uff1b\u91c7\u7528\u53ef\u53d8\u65f6\u95f4\u540c\u6b65\u65b9\u6cd5\u548c\u57fa\u4e8e\u87ba\u65cb\u7ea6\u675f\u7684\u4f4d\u59ff\u6b8b\u5dee\u8fdb\u884c\u591a\u4f20\u611f\u5668\u65f6\u7a7a\u6807\u5b9a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3aXR\u5e94\u7528\u4e2d\u6700\u5148\u8fdb\u7684SLAM\u7b97\u6cd5\u63d0\u4f9b\u5fc5\u8981\u7684\u57fa\u51c6\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u5e76\u5bf9\u591a\u6b3e\u9886\u5148XR\u8bbe\u5907\u548c\u5f00\u6e90SLAM\u7b97\u6cd5\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MoCap\u7cfb\u7edf\u5728SLAM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u7cbe\u5ea6\u9650\u5236\u95ee\u9898\uff0c\u4e3aXR\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684SLAM\u6027\u80fd\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.06563", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06563", "abs": "https://arxiv.org/abs/2512.06563", "authors": ["Max Y. Ma", "Gen-Hua Shi"], "title": "Deep Manifold Part 2: Neural Network Mathematics", "comment": null, "summary": "This work develops the global equations of neural networks through stacked piecewise manifolds, fixed--point theory, and boundary--conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high--order nonlinearity, and boundary conditions. Real--world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed--point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual--driven iteration. This perspective clarifies the limits of monolithic models under geometric and data--induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world--modeling framework grounded in geometry, algebra, fixed points, and real--data complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6d41\u5f62\u51e0\u4f55\u3001\u4e0d\u52a8\u70b9\u7406\u8bba\u548c\u8fb9\u754c\u6761\u4ef6\u8fed\u4ee3\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u4e3a\u53ef\u5b66\u4e60\u7684\u6570\u503c\u8ba1\u7b97\u7cfb\u7edf\uff0c\u63ed\u793a\u4e86\u5b66\u4e60\u80fd\u529b\u4ec5\u5f53\u4e0d\u52a8\u70b9\u533a\u57df\u7a33\u5b9a\u65f6\u624d\u4f1a\u51fa\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u5e03\u5f0f\u5f39\u6027\u6a21\u578b\u67b6\u6784\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u56fa\u5b9a\u5750\u6807\u548c\u7b97\u5b50\u6846\u67b6\u4e0b\u96be\u4ee5\u89e3\u91ca\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u590d\u6742\u6027\u3001\u89c4\u6a21\u6548\u5e94\u548c\u8bad\u7ec3\u52a8\u6001\uff0c\u9700\u8981\u4ece\u51e0\u4f55\u548c\u4ee3\u6570\u89d2\u5ea6\u91cd\u65b0\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u673a\u5236\u3002", "method": "\u91c7\u7528\u5806\u53e0\u5206\u6bb5\u6d41\u5f62\u3001\u4e0d\u52a8\u70b9\u7406\u8bba\u548c\u8fb9\u754c\u6761\u4ef6\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u7531\u6d41\u5f62\u590d\u6742\u5ea6\u3001\u9ad8\u9636\u975e\u7ebf\u6027\u548c\u8fb9\u754c\u6761\u4ef6\u5851\u9020\u7684\u53ef\u5b66\u4e60\u6570\u503c\u8ba1\u7b97\u7cfb\u7edf\u3002", "result": "\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u80fd\u529b\u53d6\u51b3\u4e8e\u4e0d\u52a8\u70b9\u533a\u57df\u7684\u7a33\u5b9a\u6027\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u901a\u8fc7\u6b8b\u5dee\u9a71\u52a8\u8fed\u4ee3\u6784\u5efa\u4e0d\u52a8\u70b9\uff0c\u800c\u975e\u521d\u59cb\u5c31\u5b58\u5728\u56fa\u5b9a\u70b9\u3002", "conclusion": "\u5355\u4e00\u6a21\u578b\u5728\u51e0\u4f55\u548c\u6570\u636e\u8bf1\u5bfc\u7684\u53ef\u5851\u6027\u4e0b\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u8bbe\u8ba1\u5206\u5e03\u5f0f\u67b6\u6784\u5c06\u6d41\u5f62\u590d\u6742\u5ea6\u5206\u914d\u5230\u591a\u4e2a\u5f39\u6027\u6a21\u578b\u4e2d\uff0c\u5f62\u6210\u57fa\u4e8e\u51e0\u4f55\u3001\u4ee3\u6570\u3001\u4e0d\u52a8\u70b9\u548c\u771f\u5b9e\u6570\u636e\u590d\u6742\u5ea6\u7684\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u6846\u67b6\u3002"}}
{"id": "2512.06363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06363", "abs": "https://arxiv.org/abs/2512.06363", "authors": ["Jiabao Guo", "Yadian Wang", "Hui Ma", "Yuhao Fu", "Ju Jia", "Hui Liu", "Shengeng Tang", "Lechao Cheng", "Yunfeng Diao", "Ajian Liu"], "title": "Spoofing-aware Prompt Learning for Unified Physical-Digital Facial Attack Detection", "comment": null, "summary": "Real-world face recognition systems are vulnerable to both physical presentation attacks (PAs) and digital forgery attacks (DFs). We aim to achieve comprehensive protection of biometric data by implementing a unified physical-digital defense framework with advanced detection. Existing approaches primarily employ CLIP with regularization constraints to enhance model generalization across both tasks. However, these methods suffer from conflicting optimization directions between physical and digital attack detection under same category prompt spaces. To overcome this limitation, we propose a Spoofing-aware Prompt Learning for Unified Attack Detection (SPL-UAD) framework, which decouples optimization branches for physical and digital attacks in the prompt space. Specifically, we construct a learnable parallel prompt branch enhanced with adaptive Spoofing Context Prompt Generation, enabling independent control of optimization for each attack type. Furthermore, we design a Cues-awareness Augmentation that leverages the dual-prompt mechanism to generate challenging sample mining tasks on data, significantly enhancing the model's robustness against unseen attack types. Extensive experiments on the large-scale UniAttackDataPlus dataset demonstrate that the proposed method achieves significant performance improvements in unified attack detection tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSPL-UAD\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u7684\u63d0\u793a\u7a7a\u95f4\u4f18\u5316\u5206\u652f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7edf\u4e00\u653b\u51fb\u68c0\u6d4b\u4e2d\u7684\u4f18\u5316\u51b2\u7a81\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u540c\u65f6\u9762\u4e34\u7269\u7406\u5448\u73b0\u653b\u51fb\u548c\u6570\u5b57\u4f2a\u9020\u653b\u51fb\u7684\u5a01\u80c1\uff0c\u73b0\u6709\u57fa\u4e8eCLIP\u7684\u65b9\u6cd5\u5728\u7edf\u4e00\u68c0\u6d4b\u4e2d\u5b58\u5728\u4f18\u5316\u65b9\u5411\u51b2\u7a81\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u53ef\u5b66\u4e60\u7684\u5e76\u884c\u63d0\u793a\u5206\u652f\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u6b3a\u9a97\u4e0a\u4e0b\u6587\u63d0\u793a\u751f\u6210\u6280\u672f\uff0c\u5b9e\u73b0\u7269\u7406\u548c\u6570\u5b57\u653b\u51fb\u7684\u72ec\u7acb\u4f18\u5316\u63a7\u5236\uff1b\u8bbe\u8ba1\u7ebf\u7d22\u611f\u77e5\u589e\u5f3a\u673a\u5236\uff0c\u5229\u7528\u53cc\u63d0\u793a\u673a\u5236\u751f\u6210\u6311\u6218\u6027\u6837\u672c\u6316\u6398\u4efb\u52a1\u3002", "result": "\u5728\u5927\u578bUniAttackDataPlus\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7edf\u4e00\u653b\u51fb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "SPL-UAD\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u4f18\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u7269\u7406-\u6570\u5b57\u653b\u51fb\u68c0\u6d4b\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u4e3a\u751f\u7269\u7279\u5f81\u6570\u636e\u7684\u5168\u9762\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07266", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07266", "abs": "https://arxiv.org/abs/2512.07266", "authors": ["Florian Tretter", "Daniel Fl\u00f6gel", "Alexandru Vasilache", "Max Grobbel", "J\u00fcrgen Becker", "S\u00f6ren Hohmann"], "title": "SINRL: Socially Integrated Navigation with Reinforcement Learning using Spiking Neural Networks", "comment": "8 pages, 6 figures", "summary": "Integrating autonomous mobile robots into human environments requires human-like decision-making and energy-efficient, event-based computation. Despite progress, neuromorphic methods are rarely applied to Deep Reinforcement Learning (DRL) navigation approaches due to unstable training. We address this gap with a hybrid socially integrated DRL actor-critic approach that combines Spiking Neural Networks (SNNs) in the actor with Artificial Neural Networks (ANNs) in the critic and a neuromorphic feature extractor to capture temporal crowd dynamics and human-robot interactions. Our approach enhances social navigation performance and reduces estimated energy consumption by approximately 1.69 orders of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u793e\u4f1a\u96c6\u6210DRL\u65b9\u6cd5\uff0c\u7ed3\u5408SNN\u548cANN\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u793e\u4f1a\u5bfc\u822a\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u80fd\u8017", "motivation": "\u5c06\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u96c6\u6210\u5230\u4eba\u7c7b\u73af\u5883\u9700\u8981\u7c7b\u4eba\u51b3\u7b56\u548c\u8282\u80fd\u7684\u4e8b\u4ef6\u9a71\u52a8\u8ba1\u7b97\uff0c\u4f46\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728DRL\u5bfc\u822a\u4e2d\u5e94\u7528\u8f83\u5c11\uff0c\u4e3b\u8981\u56e0\u4e3a\u8bad\u7ec3\u4e0d\u7a33\u5b9a", "method": "\u91c7\u7528\u6df7\u5408\u793e\u4f1a\u96c6\u6210DRL\u6f14\u5458-\u8bc4\u8bba\u5bb6\u65b9\u6cd5\uff0c\u6f14\u5458\u4f7f\u7528SNN\uff0c\u8bc4\u8bba\u5bb6\u4f7f\u7528ANN\uff0c\u5e76\u914d\u5907\u795e\u7ecf\u5f62\u6001\u7279\u5f81\u63d0\u53d6\u5668\u6355\u6349\u65f6\u95f4\u4eba\u7fa4\u52a8\u6001\u548c\u4eba\u673a\u4ea4\u4e92", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u793e\u4f1a\u5bfc\u822a\u6027\u80fd\uff0c\u5e76\u5c06\u4f30\u8ba1\u80fd\u8017\u964d\u4f4e\u4e86\u7ea61.69\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u795e\u7ecf\u5f62\u6001\u65b9\u6cd5\u5728DRL\u5bfc\u822a\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u8282\u80fd\u7684\u793e\u4f1a\u5bfc\u822a"}}
{"id": "2512.06582", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.06582", "abs": "https://arxiv.org/abs/2512.06582", "authors": ["Isaac Kofi Nti"], "title": "QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling", "comment": null, "summary": "Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.", "AI": {"tldr": "QL-LSTM\u662f\u4e00\u79cd\u65b0\u578b\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u7edf\u4e00\u95e8\u63a7\u673a\u5236\u548c\u5206\u5c42\u95e8\u63a7\u9012\u5f52\u7ed3\u5408\u52a0\u6cd5\u8df3\u8dc3\u8fde\u63a5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLSTM\u548cGRU\u7684\u53c2\u6570\u5197\u4f59\u548c\u957f\u8ddd\u79bb\u4fe1\u606f\u4fdd\u7559\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3LSTM\u548cGRU\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b58\u5728\u7684\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u95e8\u63a7\u7279\u5b9a\u53c2\u6570\u5197\u4f59\u4ee5\u53ca\u957f\u8ddd\u79bb\u4fe1\u606f\u4fdd\u7559\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1. \u53c2\u6570\u5171\u4eab\u7edf\u4e00\u95e8\u63a7\u673a\u5236\uff1a\u4f7f\u7528\u5355\u4e00\u5171\u4eab\u6743\u91cd\u77e9\u9635\u66ff\u4ee3\u6240\u6709\u95e8\u63a7\u7279\u5b9a\u53d8\u6362\uff0c\u51cf\u5c11\u7ea648%\u53c2\u6570\uff1b2. \u5206\u5c42\u95e8\u63a7\u9012\u5f52\u7ed3\u5408\u52a0\u6cd5\u8df3\u8dc3\u8fde\u63a5\uff1a\u589e\u52a0\u65e0\u4e58\u6cd5\u901a\u8def\uff0c\u6539\u5584\u957f\u8ddd\u79bb\u4fe1\u606f\u6d41\u3002", "result": "\u5728IMDB\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cQL-LSTM\u5728\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf\uff0c\u4f46\u5c1a\u672a\u5b9e\u73b0\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "QL-LSTM\u5728\u53c2\u6570\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7531\u4e8e\u5faa\u73af\u6a21\u578b\u7684\u56fa\u6709\u987a\u5e8f\u7279\u6027\uff0c\u5f53\u524d\u539f\u578b\u5c1a\u672a\u5b9e\u73b0\u8ba1\u7b97\u901f\u5ea6\u6539\u8fdb\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5185\u6838\u7ea7\u4f18\u5316\u3002"}}
{"id": "2512.06368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06368", "abs": "https://arxiv.org/abs/2512.06368", "authors": ["Weitao Xiong", "Zhiyuan Yuan", "Jiahao Lu", "Chengfeng Zhao", "Peng Li", "Yuan Liu"], "title": "Human3R: Incorporating Human Priors for Better 3D Dynamic Reconstruction from Monocular Videos", "comment": null, "summary": "Monocular dynamic video reconstruction faces significant challenges in dynamic human scenes due to geometric inconsistencies and resolution degradation issues. Existing methods lack 3D human structural understanding, producing geometrically inconsistent results with distorted limb proportions and unnatural human-object fusion, while memory-constrained downsampling causes human boundary drift toward background geometry. To address these limitations, we propose to incorporate hybrid geometric priors that combine SMPL human body models with monocular depth estimation. Our approach leverages structured human priors to maintain surface consistency while capturing fine-grained geometric details in human regions. We introduce Human3R, featuring a hierarchical pipeline with refinement components that processes full-resolution images for overall scene geometry, then applies strategic cropping and cross-attention fusion for human-specific detail enhancement. The method integrates SMPL priors through a Feature Fusion Module to ensure geometrically plausible reconstruction while preserving fine-grained human boundaries. Extensive experiments on TUM Dynamics and GTA-IM datasets demonstrate superior performance in dynamic human reconstruction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faHuman3R\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408SMPL\u4eba\u4f53\u6a21\u578b\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u6df7\u5408\u51e0\u4f55\u5148\u9a8c\uff0c\u89e3\u51b3\u5355\u76ee\u52a8\u6001\u89c6\u9891\u91cd\u5efa\u4e2d\u4eba\u4f53\u51e0\u4f55\u4e0d\u4e00\u81f4\u548c\u5206\u8fa8\u7387\u9000\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f3D\u4eba\u4f53\u7ed3\u6784\u7406\u89e3\uff0c\u5bfc\u81f4\u51e0\u4f55\u4e0d\u4e00\u81f4\u7684\u7ed3\u679c\uff0c\u5982\u80a2\u4f53\u6bd4\u4f8b\u626d\u66f2\u3001\u4eba-\u7269\u878d\u5408\u4e0d\u81ea\u7136\uff0c\u4e14\u5185\u5b58\u9650\u5236\u7684\u4e0b\u91c7\u6837\u5bfc\u81f4\u4eba\u4f53\u8fb9\u754c\u5411\u80cc\u666f\u51e0\u4f55\u6f02\u79fb\u3002", "method": "\u63d0\u51faHuman3R\u5206\u5c42\u6d41\u7a0b\uff0c\u5148\u5904\u7406\u5168\u5206\u8fa8\u7387\u56fe\u50cf\u83b7\u53d6\u6574\u4f53\u573a\u666f\u51e0\u4f55\uff0c\u518d\u901a\u8fc7\u7b56\u7565\u6027\u88c1\u526a\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u589e\u5f3a\u4eba\u4f53\u7ec6\u8282\uff0c\u901a\u8fc7\u7279\u5f81\u878d\u5408\u6a21\u5757\u96c6\u6210SMPL\u5148\u9a8c\u786e\u4fdd\u51e0\u4f55\u5408\u7406\u91cd\u5efa\u3002", "result": "\u5728TUM Dynamics\u548cGTA-IM\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u4eba\u4f53\u91cd\u5efa\u65b9\u9762\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "Human3R\u901a\u8fc7\u6df7\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u5206\u5c42\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u76ee\u52a8\u6001\u4eba\u4f53\u573a\u666f\u91cd\u5efa\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u95ee\u9898\u3002"}}
{"id": "2512.07303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07303", "abs": "https://arxiv.org/abs/2512.07303", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Efficient Computation of a Continuous Topological Model of the Configuration Space of Tethered Mobile Robots", "comment": "7 pages, 3 figures, submitted to IFAC World Congress 2026", "summary": "Despite the attention that the problem of path planning for tethered robots has garnered in the past few decades, the approaches proposed to solve it typically rely on a discrete representation of the configuration space and do not exploit a model that can simultaneously capture the topological information of the tether and the continuous location of the robot. In this work, we explicitly build a topological model of the configuration space of a tethered robot starting from a polygonal representation of the workspace where the robot moves. To do so, we first establish a link between the configuration space of the tethered robot and the universal covering space of the workspace, and then we exploit this link to develop an algorithm to compute a simplicial complex model of the configuration space. We show how this approach improves the performances of existing algorithms that build other types of representations of the configuration space. The proposed model can be computed in a fraction of the time required to build traditional homotopy-augmented graphs, and is continuous, allowing to solve the path planning task for tethered robots using a broad set of path planning algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u6a21\u578b\u6765\u8868\u793a\u7cfb\u7559\u673a\u5668\u4eba\u7684\u914d\u7f6e\u7a7a\u95f4\uff0c\u901a\u8fc7\u5c06\u914d\u7f6e\u7a7a\u95f4\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u7684\u6cdb\u8986\u76d6\u7a7a\u95f4\u8054\u7cfb\u8d77\u6765\uff0c\u6784\u5efa\u5355\u7eaf\u590d\u5f62\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7cfb\u7559\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u79bb\u6563\u7684\u914d\u7f6e\u7a7a\u95f4\u8868\u793a\uff0c\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u7ef3\u7d22\u7684\u62d3\u6251\u4fe1\u606f\u548c\u673a\u5668\u4eba\u7684\u8fde\u7eed\u4f4d\u7f6e\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u8fde\u7eed\u6a21\u578b\u3002", "method": "\u9996\u5148\u5efa\u7acb\u7cfb\u7559\u673a\u5668\u4eba\u914d\u7f6e\u7a7a\u95f4\u4e0e\u5de5\u4f5c\u7a7a\u95f4\u6cdb\u8986\u76d6\u7a7a\u95f4\u7684\u8054\u7cfb\uff0c\u7136\u540e\u5f00\u53d1\u7b97\u6cd5\u8ba1\u7b97\u914d\u7f6e\u7a7a\u95f4\u7684\u5355\u7eaf\u590d\u5f62\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u6784\u5efa\u6a21\u578b\u6240\u9700\u65f6\u95f4\u4ec5\u4e3a\u4f20\u7edf\u540c\u4f26\u589e\u5f3a\u56fe\u7684\u51e0\u5206\u4e4b\u4e00\uff0c\u4e14\u6a21\u578b\u662f\u8fde\u7eed\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u79cd\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u89e3\u51b3\u7cfb\u7559\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u62d3\u6251\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8fde\u7eed\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e3a\u7cfb\u7559\u673a\u5668\u4eba\u7684\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06592", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2512.06592", "abs": "https://arxiv.org/abs/2512.06592", "authors": ["James King", "Lewis Cornwall", "Andrei Cristian Nica", "James Day", "Aaron Sim", "Neil Dalchau", "Lilly Wollman", "Joshua Meyers"], "title": "On fine-tuning Boltz-2 for protein-protein affinity prediction", "comment": "MLSB 2025", "summary": "Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5c06\u86cb\u767d\u8d28-\u914d\u4f53\u4eb2\u548c\u529b\u9884\u6d4b\u5668Boltz-2\u9002\u914d\u7528\u4e8e\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u7ed3\u6784\u6a21\u578b\u5728\u4eb2\u548c\u529b\u9884\u6d4b\u4e0a\u8868\u73b0\u4e0d\u5982\u5e8f\u5217\u6a21\u578b\uff0c\u4f46\u7ed3\u5408\u4e24\u8005\u53ef\u4e92\u8865\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u7ed3\u5408\u4eb2\u548c\u529b\u5bf9\u4e8e\u7406\u89e3\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u548c\u836f\u7269\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u7ed3\u6784\u7684\u86cb\u767d\u8d28-\u914d\u4f53\u4eb2\u548c\u529b\u9884\u6d4b\u5668Boltz-2\u9002\u914d\u7528\u4e8e\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u4eb2\u548c\u529b\u56de\u5f52\uff0c\u5e76\u5728TCR3d\u548cPPB-affinity\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5c3d\u7ba1\u5177\u6709\u9ad8\u7ed3\u6784\u51c6\u786e\u6027\uff0cBoltz-2-PPI\u5728\u5927\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u90fd\u6bd4\u57fa\u4e8e\u5e8f\u5217\u7684\u66ff\u4ee3\u65b9\u6cd5\u8868\u73b0\u5dee\u3002\u5c06Boltz-2-PPI\u7684\u5d4c\u5165\u4e0e\u57fa\u4e8e\u5e8f\u5217\u7684\u5d4c\u5165\u7ed3\u5408\u53ef\u4ee5\u4ea7\u751f\u4e92\u8865\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u8f83\u5f31\u7684\u5e8f\u5217\u6a21\u578b\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u7ed3\u6784\u7684\u8868\u793a\u65b9\u6cd5\u4e0d\u9002\u5408\u8fdb\u884c\u9ad8\u6027\u80fd\u7684\u4eb2\u548c\u529b\u9884\u6d4b\uff0c\u7ed3\u679c\u53cd\u6620\u4e86\u4e0e\u7ed3\u6784\u6570\u636e\u8bad\u7ec3\u76f8\u5173\u7684\u5df2\u77e5\u504f\u5dee\u3002"}}
{"id": "2512.06373", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06373", "abs": "https://arxiv.org/abs/2512.06373", "authors": ["Yuji Wang", "Wenlong Liu", "Jingxuan Niu", "Haoji Zhang", "Yansong Tang"], "title": "VG-Refiner: Towards Tool-Refined Referring Grounded Reasoning via Agentic Reinforcement Learning", "comment": "The project page is [this url](https://github.com/VoyageWang/VG-Refiner)", "summary": "Tool-integrated visual reasoning (TiVR) has demonstrated great potential in enhancing multimodal problem-solving. However, existing TiVR paradigms mainly focus on integrating various visual tools through reinforcement learning, while neglecting to design effective response mechanisms for handling unreliable or erroneous tool outputs. This limitation is particularly pronounced in referring and grounding tasks, where inaccurate detection tool predictions often mislead TiVR models into generating hallucinated reasoning. To address this issue, we propose the VG-Refiner, the first framework aiming at the tool-refined referring grounded reasoning. Technically, we introduce a two-stage think-rethink mechanism that enables the model to explicitly analyze and respond to tool feedback, along with a refinement reward that encourages effective correction in response to poor tool results. In addition, we propose two new metrics and establish fair evaluation protocols to systematically measure the refinement ability of current models. We adopt a small amount of task-specific data to enhance the refinement capability of VG-Refiner, achieving a significant improvement in accuracy and correction ability on referring and reasoning grounding benchmarks while preserving the general capabilities of the pretrained model.", "AI": {"tldr": "VG-Refiner\u662f\u9996\u4e2a\u9488\u5bf9\u5de5\u5177\u7cbe\u70bc\u7684\u6307\u4ee3\u63a5\u5730\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u601d\u8003-\u91cd\u65b0\u601d\u8003\u673a\u5236\u548c\u7cbe\u70bc\u5956\u52b1\u673a\u5236\uff0c\u89e3\u51b3\u73b0\u6709TiVR\u6a21\u578b\u5728\u5904\u7406\u4e0d\u53ef\u9760\u5de5\u5177\u8f93\u51fa\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709TiVR\u8303\u5f0f\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6574\u5408\u5404\u79cd\u89c6\u89c9\u5de5\u5177\uff0c\u4f46\u5ffd\u89c6\u4e86\u8bbe\u8ba1\u6709\u6548\u54cd\u5e94\u673a\u5236\u6765\u5904\u7406\u4e0d\u53ef\u9760\u6216\u9519\u8bef\u5de5\u5177\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6307\u4ee3\u548c\u63a5\u5730\u4efb\u52a1\u4e2d\uff0c\u4e0d\u51c6\u786e\u7684\u68c0\u6d4b\u5de5\u5177\u9884\u6d4b\u7ecf\u5e38\u8bef\u5bfcTiVR\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u601d\u8003-\u91cd\u65b0\u601d\u8003\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u660e\u786e\u5206\u6790\u548c\u54cd\u5e94\u5de5\u5177\u53cd\u9988\uff1b\u5f15\u5165\u7cbe\u70bc\u5956\u52b1\u673a\u5236\uff0c\u9f13\u52b1\u5bf9\u4e0d\u826f\u5de5\u5177\u7ed3\u679c\u8fdb\u884c\u6709\u6548\u4fee\u6b63\uff1b\u4f7f\u7528\u5c11\u91cf\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u589e\u5f3a\u6a21\u578b\u7cbe\u70bc\u80fd\u529b\u3002", "result": "\u5728\u6307\u4ee3\u548c\u63a8\u7406\u63a5\u5730\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVG-Refiner\u5728\u51c6\u786e\u6027\u548c\u4fee\u6b63\u80fd\u529b\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\u3002", "conclusion": "VG-Refiner\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86TiVR\u6a21\u578b\u4e2d\u5de5\u5177\u8f93\u51fa\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u70bc\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee3\u63a5\u5730\u63a8\u7406\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u548c\u534f\u8bae\u6765\u7cfb\u7edf\u8861\u91cf\u6a21\u578b\u7684\u7cbe\u70bc\u80fd\u529b\u3002"}}
{"id": "2512.07316", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.07316", "abs": "https://arxiv.org/abs/2512.07316", "authors": ["Gianpietro Battocletti", "Dimitris Boskos", "Bart De Schutter"], "title": "Model Predictive Control for Cooperative Docking Between Autonomous Surface Vehicles with Disturbance Rejection", "comment": "7 pages, 4 figures, submitted to IFAC World Congress 2026", "summary": "Uncrewed Surface Vehicles (USVs) are a popular and efficient type of marine craft that find application in a large number of water-based tasks. When multiple USVs operate in the same area, they may be required to dock to each other to perform a shared task. Existing approaches for the docking between autonomous USVs generally consider one USV as a stationary target, while the second one is tasked to reach the required docking pose. In this work, we propose a cooperative approach for USV-USV docking, where two USVs work together to dock at an agreed location. We use a centralized Model Predictive Control (MPC) approach to solve the control problem, obtaining feasible trajectories that also guarantee constraint satisfaction. Owing to its model-based nature, this approach allows the rejection of disturbances, inclusive of exogenous inputs, by anticipating their effect on the USVs through the MPC prediction model. This is particularly effective in case of almost-stationary disturbances such as water currents. In simulations, we demonstrate how the proposed approach allows for a faster and more efficient docking with respect to existing approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u4e2d\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684USV-USV\u534f\u540c\u5bf9\u63a5\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u8fb9\u5bf9\u63a5\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u66f4\u9ad8\u6548\u7684\u5bf9\u63a5\u6548\u679c\u3002", "motivation": "\u73b0\u6709USV\u5bf9\u63a5\u65b9\u6cd5\u901a\u5e38\u5c06\u4e00\u8258USV\u89c6\u4e3a\u9759\u6b62\u76ee\u6807\uff0c\u53e6\u4e00\u8258\u8d1f\u8d23\u5bf9\u63a5\uff0c\u8fd9\u79cd\u5355\u8fb9\u65b9\u6cd5\u6548\u7387\u8f83\u4f4e\u3002\u672c\u6587\u63d0\u51fa\u534f\u540c\u5bf9\u63a5\u65b9\u6cd5\uff0c\u8ba9\u4e24\u8258USV\u5171\u540c\u534f\u4f5c\u5b8c\u6210\u5bf9\u63a5\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u96c6\u4e2d\u5f0f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\u89e3\u51b3\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u9884\u6d4b\u6a21\u578b\u8003\u8651\u5916\u90e8\u5e72\u6270\uff08\u5982\u6c34\u6d41\uff09\u7684\u5f71\u54cd\uff0c\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\u548c\u8f68\u8ff9\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u534f\u540c\u5bf9\u63a5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u3001\u66f4\u9ad8\u6548\u7684\u5bf9\u63a5\u6548\u679c\u3002", "conclusion": "\u57fa\u4e8eMPC\u7684\u534f\u540c\u5bf9\u63a5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5916\u90e8\u5e72\u6270\uff0c\u63d0\u9ad8USV\u5bf9\u63a5\u7684\u6548\u7387\u548c\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b58\u5728\u6c34\u6d41\u7b49\u51c6\u9759\u6001\u5e72\u6270\u7684\u73af\u5883\u3002"}}
{"id": "2512.06607", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06607", "abs": "https://arxiv.org/abs/2512.06607", "authors": ["Humzah Merchant", "Bradford Levy"], "title": "A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs", "comment": null, "summary": "Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8c03\u6574\u5927\u578b\u57fa\u7840\u6a21\u578blogits\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e00\u5bf9\u5c0f\u578b\u4e13\u4e1a\u5316\u6a21\u578b\u6765\u6d88\u9664\u524d\u77bb\u6027\u504f\u5dee\uff0c\u89e3\u51b3\u91d1\u878d\u9884\u6d4b\u4e2dLLMs\u7684look-ahead bias\u95ee\u9898\u3002", "motivation": "\u7531\u4e8eLLMs\u5728\u957f\u65f6\u5e8f\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u5bfc\u81f4look-ahead bias\uff0c\u4f7f\u5f97\u5728\u91d1\u878d\u9884\u6d4b\u4efb\u52a1\u4e2d\u65e0\u6cd5\u8fdb\u884c\u4f20\u7edf\u7684\u56de\u6d4b\uff0c\u800c\u4ece\u5934\u91cd\u65b0\u8bad\u7ec3\u524d\u6cbf\u6a21\u578b\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u4e00\u5bf9\u5c0f\u578b\u4e13\u4e1a\u5316\u6a21\u578b\u8c03\u6574\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684logits\uff1a\u4e00\u4e2a\u5728\u8981\u9057\u5fd8\u7684\u4fe1\u606f\u4e0a\u5fae\u8c03\uff0c\u53e6\u4e00\u4e2a\u5728\u8981\u4fdd\u7559\u7684\u4fe1\u606f\u4e0a\u5fae\u8c03\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u6d88\u9664\u4e86\u5b57\u9762\u548c\u8bed\u4e49\u77e5\u8bc6\uff0c\u7ea0\u6b63\u4e86\u504f\u5dee\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u6709\u6548\u4e14\u4f4e\u6210\u672c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86LLMs\u5728\u91d1\u878d\u9884\u6d4b\u4e2d\u7684look-ahead bias\u95ee\u9898\u3002"}}
{"id": "2512.06376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06376", "abs": "https://arxiv.org/abs/2512.06376", "authors": ["Xinhao Xiang", "Abhijeet Rastogi", "Jiawei Zhang"], "title": "Are AI-Generated Driving Videos Ready for Autonomous Driving? A Diagnostic Evaluation Framework", "comment": null, "summary": "Recent text-to-video models have enabled the generation of high-resolution driving scenes from natural language prompts. These AI-generated driving videos (AIGVs) offer a low-cost, scalable alternative to real or simulator data for autonomous driving (AD). But a key question remains: can such videos reliably support training and evaluation of AD models? We present a diagnostic framework that systematically studies this question. First, we introduce a taxonomy of frequent AIGV failure modes, including visual artifacts, physically implausible motion, and violations of traffic semantics, and demonstrate their negative impact on object detection, tracking, and instance segmentation. To support this analysis, we build ADGV-Bench, a driving-focused benchmark with human quality annotations and dense labels for multiple perception tasks. We then propose ADGVE, a driving-aware evaluator that combines static semantics, temporal cues, lane obedience signals, and Vision-Language Model(VLM)-guided reasoning into a single quality score for each clip. Experiments show that blindly adding raw AIGVs can degrade perception performance, while filtering them with ADGVE consistently improves both general video quality assessment metrics and downstream AD models, and turns AIGVs into a beneficial complement to real-world data. Our study highlights both the risks and the promise of AIGVs, and provides practical tools for safely leveraging large-scale video generation in future AD pipelines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bca\u65ad\u6846\u67b6\u6765\u8bc4\u4f30AI\u751f\u6210\u9a7e\u9a76\u89c6\u9891\uff08AIGV\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u4e86AIGV\u7684\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\u53ca\u5176\u5bf9\u611f\u77e5\u4efb\u52a1\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86ADGV-Bench\u57fa\u51c6\u548cADGVE\u8bc4\u4f30\u5668\u6765\u63d0\u5347AIGV\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u80fd\u591f\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u9a7e\u9a76\u573a\u666f\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9bAI\u751f\u6210\u7684\u9a7e\u9a76\u89c6\u9891\u662f\u5426\u80fd\u53ef\u9760\u5730\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "method": "1. \u5efa\u7acbAIGV\u5931\u8d25\u6a21\u5f0f\u5206\u7c7b\u6cd5\uff1b2. \u6784\u5efaADGV-Bench\u57fa\u51c6\u6570\u636e\u96c6\uff1b3. \u63d0\u51faADGVE\u8bc4\u4f30\u5668\uff0c\u7ed3\u5408\u9759\u6001\u8bed\u4e49\u3001\u65f6\u5e8f\u7ebf\u7d22\u3001\u8f66\u9053\u9075\u5b88\u4fe1\u53f7\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff1b4. \u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u8fc7\u6ee4\u540e\u7684AIGV\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u76ca\u5904\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f2\u76ee\u6dfb\u52a0\u539f\u59cbAIGV\u4f1a\u964d\u4f4e\u611f\u77e5\u6027\u80fd\uff0c\u4f46\u4f7f\u7528ADGVE\u8fc7\u6ee4\u540e\u80fd\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u548c\u4e0b\u6e38\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u6027\u80fd\uff0c\u4f7fAIGV\u6210\u4e3a\u771f\u5b9e\u6570\u636e\u7684\u6709\u6548\u8865\u5145\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86AIGV\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u98ce\u9669\u548c\u6f5c\u529b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b89\u5168\u5229\u7528\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u7684\u5b9e\u9645\u5de5\u5177\u3002"}}
{"id": "2512.07359", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07359", "abs": "https://arxiv.org/abs/2512.07359", "authors": ["Bin Zhao", "Yiwen Lu", "Haohua Zhu", "Xiao Li", "Sheng Yi"], "title": "Multi-Rigid-Body Approximation of Human Hands with Application to Digital Twin", "comment": "10 pages, 4 figures. Accepted at ICBSR'25 (International Conference on Biomechanical Systems and Robotics)", "summary": "Human hand simulation plays a critical role in digital twin applications, requiring models that balance anatomical fidelity with computational efficiency. We present a complete pipeline for constructing multi-rigid-body approximations of human hands that preserve realistic appearance while enabling real-time physics simulation. Starting from optical motion capture of a specific human hand, we construct a personalized MANO (Multi-Abstracted hand model with Neural Operations) model and convert it to a URDF (Unified Robot Description Format) representation with anatomically consistent joint axes. The key technical challenge is projecting MANO's unconstrained SO(3) joint rotations onto the kinematically constrained joints of the rigid-body model. We derive closed-form solutions for single degree-of-freedom joints and introduce a Baker-Campbell-Hausdorff (BCH)-corrected iterative method for two degree-of-freedom joints that properly handles the non-commutativity of rotations. We validate our approach through digital twin experiments where reinforcement learning policies control the multi-rigid-body hand to replay captured human demonstrations. Quantitative evaluation shows sub-centimeter reconstruction error and successful grasp execution across diverse manipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6784\u5efa\u4e2a\u6027\u5316\u591a\u521a\u4f53\u624b\u90e8\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06MANO\u6a21\u578b\u7684SO(3)\u5173\u8282\u65cb\u8f6c\u6295\u5f71\u5230\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u521a\u4f53\u5173\u8282\u4e0a\uff0c\u5b9e\u73b0\u5b9e\u65f6\u7269\u7406\u6a21\u62df\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u5e94\u7528\u4e2d\u9700\u8981\u5e73\u8861\u89e3\u5256\u4fdd\u771f\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u4eba\u624b\u6a21\u62df\u6a21\u578b\u3002", "method": "\u4ece\u5149\u5b66\u8fd0\u52a8\u6355\u6349\u6784\u5efa\u4e2a\u6027\u5316MANO\u6a21\u578b\u5e76\u8f6c\u6362\u4e3aURDF\u8868\u793a\uff0c\u9488\u5bf9\u5355\u81ea\u7531\u5ea6\u5173\u8282\u63a8\u5bfc\u95ed\u5f0f\u89e3\uff0c\u9488\u5bf9\u53cc\u81ea\u7531\u5ea6\u5173\u8282\u63d0\u51faBCH\u6821\u6b63\u8fed\u4ee3\u65b9\u6cd5\u5904\u7406\u65cb\u8f6c\u7684\u975e\u4ea4\u6362\u6027\u3002", "result": "\u6570\u5b57\u5b6a\u751f\u5b9e\u9a8c\u663e\u793a\u4e9a\u5398\u7c73\u7ea7\u91cd\u5efa\u8bef\u5dee\uff0c\u5728\u591a\u6837\u5316\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6210\u529f\u6267\u884c\u6293\u53d6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6784\u5efa\u4fdd\u7559\u771f\u5b9e\u5916\u89c2\u4e14\u652f\u6301\u5b9e\u65f6\u7269\u7406\u6a21\u62df\u7684\u591a\u521a\u4f53\u624b\u90e8\u8fd1\u4f3c\u6a21\u578b\u3002"}}
{"id": "2512.06609", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06609", "abs": "https://arxiv.org/abs/2512.06609", "authors": ["Tongda Xu", "Wendi Zheng", "Jiajun He", "Jose Miguel Hernandez-Lobato", "Yan Wang", "Ya-Qin Zhang", "Jie Tang"], "title": "Vector Quantization using Gaussian Variational Autoencoder", "comment": null, "summary": "Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGaussian Quant\uff08GQ\uff09\u7684\u6280\u672f\uff0c\u53ef\u5c06\u9ad8\u65afVAE\u8f6c\u6362\u4e3aVQ-VAE\u800c\u65e0\u9700\u8bad\u7ec3\uff0c\u901a\u8fc7\u751f\u6210\u968f\u673a\u9ad8\u65af\u566a\u58f0\u4f5c\u4e3a\u7801\u672c\u5e76\u627e\u5230\u4e0e\u540e\u9a8c\u5747\u503c\u6700\u63a5\u8fd1\u7684\u566a\u58f0\u6765\u5b9e\u73b0\u79bb\u6563\u5316\u3002", "motivation": "VQ-VAE\u7531\u4e8e\u79bb\u6563\u5316\u96be\u4ee5\u8bad\u7ec3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7b80\u5355\u6709\u6548\u7684\u79bb\u6563\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGQ\u6280\u672f\uff0c\u4f7f\u7528\u968f\u673a\u9ad8\u65af\u566a\u58f0\u4f5c\u4e3a\u7801\u672c\uff0c\u5e76\u5f15\u5165\u76ee\u6807\u53d1\u6563\u7ea6\u675f\uff08TDC\uff09\u6765\u8bad\u7ec3\u9ad8\u65afVAE\u4ee5\u63d0\u9ad8GQ\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eGQ\u5728UNet\u548cViT\u67b6\u6784\u4e0a\u4f18\u4e8eVQGAN\u3001FSQ\u3001LFQ\u548cBSQ\u7b49\u73b0\u6709VQ-VAE\u65b9\u6cd5\uff0cTDC\u4e5f\u6539\u8fdb\u4e86TokenBridge\u7b49\u9ad8\u65afVAE\u79bb\u6563\u5316\u65b9\u6cd5\u3002", "conclusion": "GQ\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684VQ-VAE\u66ff\u4ee3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.06377", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06377", "abs": "https://arxiv.org/abs/2512.06377", "authors": ["Yi Huo", "Yun Ge"], "title": "VAD-Net: Multidimensional Facial Expression Recognition in Intelligent Education System", "comment": null, "summary": "Current FER (Facial Expression Recognition) dataset is mostly labeled by emotion categories, such as happy, angry, sad, fear, disgust, surprise, and neutral which are limited in expressiveness. However, future affective computing requires more comprehensive and precise emotion metrics which could be measured by VAD(Valence-Arousal-Dominance) multidimension parameters. To address this, AffectNet has tried to add VA (Valence and Arousal) information, but still lacks D(Dominance). Thus, the research introduces VAD annotation on FER2013 dataset, takes the initiative to label D(Dominance) dimension. Then, to further improve network capacity, it enforces orthogonalized convolution on it, which extracts more diverse and expressive features and will finally increase the prediction accuracy. Experiment results show that D dimension could be measured but is difficult to obtain compared with V and A dimension no matter in manual annotation or regression network prediction. Secondly, the ablation test by introducing orthogonal convolution verifies that better VAD prediction could be obtained in the configuration of orthogonal convolution. Therefore, the research provides an initiative labelling for D dimension on FER dataset, and proposes a better prediction network for VAD prediction through orthogonal convolution. The newly built VAD annotated FER2013 dataset could act as a benchmark to measure VAD multidimensional emotions, while the orthogonalized regression network based on ResNet could act as the facial expression recognition baseline for VAD emotion prediction. The newly labeled dataset and implementation code is publicly available on https://github.com/YeeHoran/VAD-Net .", "AI": {"tldr": "\u8be5\u7814\u7a76\u5728FER2013\u6570\u636e\u96c6\u4e0a\u9996\u6b21\u6807\u6ce8\u4e86VAD\uff08Valence-Arousal-Dominance\uff09\u4e09\u7ef4\u60c5\u611f\u53c2\u6570\u4e2d\u7684Dominance\u7ef4\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6b63\u4ea4\u5377\u79ef\u7684\u6539\u8fdb\u7f51\u7edc\u6765\u63d0\u9ad8VAD\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524dFER\u6570\u636e\u96c6\u4e3b\u8981\u57fa\u4e8e\u79bb\u6563\u60c5\u611f\u7c7b\u522b\u6807\u6ce8\uff0c\u4f46\u672a\u6765\u60c5\u611f\u8ba1\u7b97\u9700\u8981\u66f4\u5168\u9762\u7cbe\u786e\u7684VAD\u591a\u7ef4\u53c2\u6570\u3002\u867d\u7136AffectNet\u5df2\u6807\u6ce8VA\u7ef4\u5ea6\uff0c\u4f46\u7f3a\u4e4fD\u7ef4\u5ea6\u3002", "method": "\u5728FER2013\u6570\u636e\u96c6\u4e0a\u6807\u6ce8D\u7ef4\u5ea6\uff0c\u5e76\u5f15\u5165\u6b63\u4ea4\u5377\u79ef\u6765\u589e\u5f3a\u7f51\u7edc\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u63d0\u9ad8VAD\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eD\u7ef4\u5ea6\u53ef\u6d4b\u91cf\u4f46\u6bd4VA\u7ef4\u5ea6\u66f4\u96be\u83b7\u53d6\uff0c\u6b63\u4ea4\u5377\u79ef\u914d\u7f6e\u80fd\u83b7\u5f97\u66f4\u597d\u7684VAD\u9884\u6d4b\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aFER\u6570\u636e\u96c6\u63d0\u4f9b\u4e86D\u7ef4\u5ea6\u6807\u6ce8\uff0c\u5e76\u901a\u8fc7\u6b63\u4ea4\u5377\u79ef\u7f51\u7edc\u5efa\u7acb\u4e86VAD\u60c5\u611f\u9884\u6d4b\u57fa\u51c6\uff0c\u76f8\u5173\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.07371", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07371", "abs": "https://arxiv.org/abs/2512.07371", "authors": ["Byungju Kim", "Jinu Pahk", "Chungwoo Lee", "Jaejoon Kim", "Jangha Lee", "Theo Taeyeong Kim", "Kyuhwan Shim", "Jun Ki Lee", "Byoung-Tak Zhang"], "title": "ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning", "comment": "project page: https://project-espada.github.io/espada/", "summary": "Behavior-cloning based visuomotor policies enable precise manipulation but often inherit the slow, cautious tempo of human demonstrations, limiting practical deployment. However, prior studies on acceleration methods mainly rely on statistical or heuristic cues that ignore task semantics and can fail across diverse manipulation settings. We present ESPADA, a semantic and spatially aware framework that segments demonstrations using a VLM-LLM pipeline with 3D gripper-object relations, enabling aggressive downsampling only in non-critical segments while preserving precision-critical phases, without requiring extra data or architectural modifications, or any form of retraining. To scale from a single annotated episode to the full dataset, ESPADA propagates segment labels via Dynamic Time Warping (DTW) on dynamics-only features. Across both simulation and real-world experiments with ACT and DP baselines, ESPADA achieves approximately a 2x speed-up while maintaining success rates, narrowing the gap between human demonstrations and efficient robot control.", "AI": {"tldr": "ESPADA\u662f\u4e00\u4e2a\u57fa\u4e8e\u8bed\u4e49\u548c\u7a7a\u95f4\u611f\u77e5\u7684\u6846\u67b6\uff0c\u901a\u8fc7VLM-LLM\u7ba1\u9053\u548c3D\u5939\u722a-\u7269\u4f53\u5173\u7cfb\u5206\u5272\u6f14\u793a\uff0c\u5728\u975e\u5173\u952e\u9636\u6bb5\u8fdb\u884c\u6fc0\u8fdb\u4e0b\u91c7\u6837\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u5173\u952e\u9636\u6bb5\uff0c\u5b9e\u73b0\u7ea62\u500d\u52a0\u901f\u800c\u4e0d\u964d\u4f4e\u6210\u529f\u7387\u3002", "motivation": "\u57fa\u4e8e\u884c\u4e3a\u514b\u9686\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7ee7\u627f\u4e86\u4eba\u7c7b\u6f14\u793a\u7684\u7f13\u6162\u8282\u594f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4f9d\u8d56\u5ffd\u7565\u4efb\u52a1\u8bed\u4e49\u7684\u7edf\u8ba1\u6216\u542f\u53d1\u5f0f\u7ebf\u7d22\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u7684\u64cd\u4f5c\u573a\u666f\u3002", "method": "\u4f7f\u7528VLM-LLM\u7ba1\u9053\u7ed3\u54083D\u5939\u722a-\u7269\u4f53\u5173\u7cfb\u5206\u5272\u6f14\u793a\uff0c\u901a\u8fc7\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u5728\u4ec5\u52a8\u6001\u7279\u5f81\u4e0a\u4f20\u64ad\u6bb5\u6807\u7b7e\uff0c\u5b9e\u73b0\u4ece\u5355\u4e2a\u6807\u6ce8\u7247\u6bb5\u5230\u5b8c\u6574\u6570\u636e\u96c6\u7684\u6269\u5c55\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u4e0eACT\u548cDP\u57fa\u7ebf\u76f8\u6bd4\uff0cESPADA\u5b9e\u73b0\u4e86\u7ea62\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6210\u529f\u7387\uff0c\u7f29\u5c0f\u4e86\u4eba\u7c7b\u6f14\u793a\u4e0e\u9ad8\u6548\u673a\u5668\u4eba\u63a7\u5236\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "ESPADA\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u6570\u636e\u3001\u67b6\u6784\u4fee\u6539\u6216\u91cd\u65b0\u8bad\u7ec3\u7684\u8bed\u4e49\u611f\u77e5\u52a0\u901f\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u57fa\u4e8e\u884c\u4e3a\u514b\u9686\u7684\u7b56\u7565\u7684\u5b9e\u9645\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2512.06630", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.06630", "abs": "https://arxiv.org/abs/2512.06630", "authors": ["Chi-Sheng Chen", "Xinyu Zhang", "Rong Fu", "Qiuzhe Xie", "Fan Zhang"], "title": "Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study", "comment": null, "summary": "Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u65f6\u5e8f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QTCNN\uff09\uff0c\u7ed3\u5408\u7ecf\u5178\u65f6\u5e8f\u7f16\u7801\u5668\u548c\u53c2\u6570\u9ad8\u6548\u7684\u91cf\u5b50\u5377\u79ef\u7535\u8def\uff0c\u7528\u4e8e\u80a1\u7968\u6536\u76ca\u9884\u6d4b\uff0c\u5728JPX\u4e1c\u4eac\u8bc1\u5238\u4ea4\u6613\u6240\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u9884\u6d4b\u6a21\u578b\u5728\u5904\u7406\u566a\u58f0\u8f93\u5165\u3001\u5236\u5ea6\u8f6c\u6362\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e3a\u590d\u6742\u52a8\u6001\u91d1\u878d\u73af\u5883\u4e0b\u7684\u80a1\u7968\u5e02\u573a\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "QTCNN\u5305\u542b\u7ecf\u5178\u65f6\u5e8f\u7f16\u7801\u5668\u63d0\u53d6\u591a\u5c3a\u5ea6\u6280\u672f\u6307\u6807\u6a21\u5f0f\uff0c\u4ee5\u53ca\u91cf\u5b50\u5377\u79ef\u7535\u8def\u5229\u7528\u91cf\u5b50\u53e0\u52a0\u548c\u7ea0\u7f20\u7279\u6027\u589e\u5f3a\u7279\u5f81\u8868\u793a\u5e76\u6291\u5236\u8fc7\u62df\u5408\u3002", "result": "\u5728JPX\u4e1c\u4eac\u8bc1\u5238\u4ea4\u6613\u6240\u6570\u636e\u96c6\u4e0a\uff0cQTCNN\u5b9e\u73b0\u4e860.538\u7684\u590f\u666e\u6bd4\u7387\uff0c\u6bd4\u6700\u4f73\u7ecf\u5178\u57fa\u51c6\u65b9\u6cd5\u9ad8\u51fa\u7ea672%\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u91cf\u5b50\u589e\u5f3a\u7684\u9884\u6d4b\u6a21\u578bQTCNN\u5728\u91cf\u5316\u91d1\u878d\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u652f\u6301\u7a33\u5065\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2512.06379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06379", "abs": "https://arxiv.org/abs/2512.06379", "authors": ["Yi Huo", "Lei Zhang"], "title": "OCFER-Net: Recognizing Facial Expression in Online Learning System", "comment": null, "summary": "Recently, online learning is very popular, especially under the global epidemic of COVID-19. Besides knowledge distribution, emotion interaction is also very important. It can be obtained by employing Facial Expression Recognition (FER). Since the FER accuracy is substantial in assisting teachers to acquire the emotional situation, the project explores a series of FER methods and finds that few works engage in exploiting the orthogonality of convolutional matrix. Therefore, it enforces orthogonality on kernels by a regularizer, which extracts features with more diversity and expressiveness, and delivers OCFER-Net. Experiments are carried out on FER-2013, which is a challenging dataset. Results show superior performance over baselines by 1.087. The code of the research project is publicly available on https://github.com/YeeHoran/OCFERNet.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faOCFER-Net\uff0c\u901a\u8fc7\u6b63\u4ea4\u6027\u6b63\u5219\u5316\u5377\u79ef\u6838\u6765\u63d0\u5347\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u51c6\u786e\u7387\uff0c\u5728FER-2013\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd51.087%\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u4e2d\u60c5\u611f\u4ea4\u4e92\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u65b9\u6cd5\u5f88\u5c11\u5229\u7528\u5377\u79ef\u77e9\u9635\u7684\u6b63\u4ea4\u6027\u3002\u6b63\u4ea4\u6027\u53ef\u4ee5\u63d0\u53d6\u66f4\u591a\u6837\u5316\u548c\u6709\u8868\u73b0\u529b\u7684\u7279\u5f81\u3002", "method": "\u63d0\u51faOCFER-Net\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u5668\u5f3a\u5236\u5377\u79ef\u6838\u7684\u6b63\u4ea4\u6027\uff0c\u4ece\u800c\u63d0\u53d6\u66f4\u5177\u591a\u6837\u6027\u548c\u8868\u8fbe\u529b\u7684\u7279\u5f81\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684FER-2013\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd51.087%\u3002", "conclusion": "\u6b63\u4ea4\u6027\u7ea6\u675f\u80fd\u6709\u6548\u63d0\u5347\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6027\u80fd\uff0cOCFER-Net\u5728\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.07464", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07464", "abs": "https://arxiv.org/abs/2512.07464", "authors": ["Haolin Song", "Hongbo Zhu", "Tao Yu", "Yan Liu", "Mingqi Yuan", "Wengang Zhou", "Hua Chen", "Houqiang Li"], "title": "Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction", "comment": null, "summary": "For full-size humanoid robots, even with recent advances in reinforcement learning-based control, achieving reliable locomotion on complex terrains, such as long staircases, remains challenging. In such settings, limited perception, ambiguous terrain cues, and insufficient adaptation of gait timing can cause even a single misplaced or mistimed step to result in rapid loss of balance. We introduce a perceptive locomotion framework that merges terrain sensing, gait regulation, and whole-body control into a single reinforcement learning policy. A downward-facing depth camera mounted under the base observes the support region around the feet, and a compact U-Net reconstructs a dense egocentric height map from each frame in real time, operating at the same frequency as the control loop. The perceptual height map, together with proprioceptive observations, is processed by a unified policy that produces joint commands and a global stepping-phase signal, allowing gait timing and whole-body posture to be adapted jointly to the commanded motion and local terrain geometry. We further adopt a single-stage successive teacher-student training scheme for efficient policy learning and knowledge transfer. Experiments conducted on a 31-DoF, 1.65 m humanoid robot demonstrate robust locomotion in both simulation and real-world settings, including forward and backward stair ascent and descent, as well as crossing a 46 cm gap. Project Page:https://ga-phl.github.io/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u5730\u5f62\u611f\u77e5\u3001\u6b65\u6001\u8c03\u8282\u548c\u5168\u8eab\u63a7\u5236\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5168\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\uff08\u5982\u957f\u697c\u68af\uff09\u4e0a\u7684\u53ef\u9760\u8fd0\u52a8\u95ee\u9898\u3002", "motivation": "\u5168\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\uff08\u5982\u957f\u697c\u68af\uff09\u4e0a\u5b9e\u73b0\u53ef\u9760\u8fd0\u52a8\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u611f\u77e5\u53d7\u9650\u3001\u5730\u5f62\u7ebf\u7d22\u6a21\u7cca\u4ee5\u53ca\u6b65\u6001\u65f6\u5e8f\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u8fd9\u4e9b\u56e0\u7d20\u5bb9\u6613\u5bfc\u81f4\u5355\u6b65\u5931\u8bef\u5f15\u53d1\u5931\u8861\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86\u5411\u4e0b\u6df1\u5ea6\u6444\u50cf\u5934\u5b9e\u65f6\u751f\u6210\u5bc6\u96c6\u7684\u81ea\u6211\u4e2d\u5fc3\u9ad8\u5ea6\u56fe\uff0c\u901a\u8fc7\u7d27\u51d1\u7684U-Net\u7f51\u7edc\u5904\u7406\uff0c\u5c06\u611f\u77e5\u9ad8\u5ea6\u56fe\u4e0e\u672c\u4f53\u611f\u89c9\u4fe1\u606f\u8f93\u5165\u7edf\u4e00\u7b56\u7565\uff0c\u751f\u6210\u5173\u8282\u547d\u4ee4\u548c\u5168\u5c40\u6b65\u8fdb\u76f8\u4f4d\u4fe1\u53f7\uff0c\u5b9e\u73b0\u6b65\u6001\u65f6\u5e8f\u548c\u5168\u8eab\u59ff\u6001\u7684\u8054\u5408\u9002\u5e94\u3002\u91c7\u7528\u5355\u9636\u6bb5\u8fde\u7eed\u5e08\u751f\u8bad\u7ec3\u65b9\u6848\u8fdb\u884c\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u548c\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u572831\u81ea\u7531\u5ea6\u30011.65\u7c73\u9ad8\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u7a33\u5065\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u5305\u62ec\u524d\u540e\u4e0a\u4e0b\u697c\u68af\u4ee5\u53ca\u8de8\u8d8a46\u5398\u7c73\u95f4\u9699\u3002", "conclusion": "\u8be5\u611f\u77e5\u8fd0\u52a8\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e0a\u7684\u53ef\u9760\u8fd0\u52a8\uff0c\u901a\u8fc7\u878d\u5408\u611f\u77e5\u4e0e\u63a7\u5236\u7684\u7edf\u4e00\u7b56\u7565\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u6b65\u6001\u9002\u5e94\u6027\u95ee\u9898\u3002"}}
{"id": "2512.06638", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06638", "abs": "https://arxiv.org/abs/2512.06638", "authors": ["Isha Karn", "David Jensen"], "title": "The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News", "comment": "Preprint. Approximately 15 pages, 5 figures, 3 tables", "summary": "Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51faGossipCop\u548cPolitiFact\u8fd9\u4e24\u4e2a\u5e38\u7528\u5047\u65b0\u95fb\u68c0\u6d4b\u57fa\u51c6\u6570\u636e\u96c6\u5b58\u5728\u56fe\u62d3\u6251\u7ed3\u6784\u6d45\u5c42\u5316\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u5728\u7ed3\u6784\u5efa\u6a21\u65b9\u9762\u7684\u4f18\u52bf\u65e0\u6cd5\u4f53\u73b0\uff0c\u591a\u5c42\u611f\u77e5\u673a(MLP)\u8868\u73b0\u4e0eGNNs\u76f8\u5f53\u3002", "motivation": "\u5f53\u524d\u5047\u65b0\u95fb\u68c0\u6d4b\u7814\u7a76\u4e2d\u5e7f\u6cdb\u4f7f\u7528GNNs\u6765\u5efa\u6a21\u65b0\u95fb\u4f20\u64ad\u7ed3\u6784\uff0c\u4f46\u5e38\u7528\u57fa\u51c6\u6570\u636e\u96c6\u662f\u5426\u771f\u6b63\u80fd\u591f\u8bc4\u4f30\u7ed3\u6784\u5efa\u6a21\u65b9\u6cd5\u7684\u6709\u6548\u6027\u5b58\u5728\u7591\u95ee\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f835\u79cdGNN\u67b6\u6784\u4e0e\u4f7f\u7528\u76f8\u540c\u8282\u70b9\u7279\u5f81\u7684MLP\uff0c\u901a\u8fc7\u7279\u5f81\u6d17\u724c\u548c\u8fb9\u7ed3\u6784\u968f\u673a\u5316\u5b9e\u9a8c\u6765\u5206\u79bb\u7ed3\u6784\u548c\u7279\u5f81\u7684\u8d21\u732e\uff0c\u5e76\u5bf9\u56fe\u62d3\u6251\u8fdb\u884c\u7ed3\u6784\u5206\u6790\u3002", "result": "MLP\u4e0eGNNs\u6027\u80fd\u5dee\u8ddd\u57281-2%\u5185\uff0c\u7f6e\u4fe1\u533a\u95f4\u91cd\u53e0\uff1b\u7279\u5f81\u6d17\u724c\u5bfc\u81f4\u6027\u80fd\u5d29\u6e83\u800c\u8fb9\u968f\u673a\u5316\u6027\u80fd\u7a33\u5b9a\uff1b75%\u4ee5\u4e0a\u8282\u70b9\u8ddd\u79bb\u6839\u8282\u70b9\u4ec5\u4e00\u8df3\uff0c\u7ed3\u6784\u591a\u6837\u6027\u4e0d\u8db3\uff1b\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0aGNNs\u663e\u8457\u4f18\u4e8eMLPs\u3002", "conclusion": "\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u65e0\u6cd5\u6709\u6548\u6d4b\u8bd5\u7ed3\u6784\u5efa\u6a21\u65b9\u6cd5\u7684\u6548\u7528\uff0c\u9700\u8981\u5f00\u53d1\u5177\u6709\u66f4\u4e30\u5bcc\u3001\u66f4\u591a\u6837\u5316\u56fe\u62d3\u6251\u7ed3\u6784\u7684\u6570\u636e\u96c6\u3002"}}
{"id": "2512.06400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06400", "abs": "https://arxiv.org/abs/2512.06400", "authors": ["Jing Tao", "Yonghong Zong", "Banglei Guana", "Pengju Sun", "Taihang Lei", "Yang Shanga", "Qifeng Yu"], "title": "Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement", "comment": "The paper has been accepted and officially published by IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "summary": "In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u533a\u57df\u611f\u77e5\u7684\u7ea2\u5916-\u53ef\u89c1\u5149\u5149\u8c31\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u66dd\u5149\u548c\u591a\u6a21\u6001\u6210\u50cf\u7ed3\u5408\uff0c\u5728\u6781\u7aef\u73af\u5883\u4e0b\u4fdd\u6301\u53ef\u89c1\u7279\u5f81\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u70ed\u8f90\u5c04\u4fe1\u606f", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u878d\u5408\u7ea2\u5916\u548c\u53ef\u89c1\u5149\u8c31\u65f6\u5f80\u5f80\u727a\u7272\u53ef\u89c1\u56fe\u50cf\u8d28\u91cf\uff0c\u5f71\u54cd\u6d4b\u91cf\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u6781\u7aef\u6761\u4ef6\u4e0b", "method": "\u4f7f\u7528\u7a7a\u95f4\u53ef\u53d8\u66dd\u5149\u76f8\u673a\u8fdb\u884c\u591a\u66dd\u5149\u548c\u591a\u6a21\u6001\u6210\u50cf\uff0c\u91c7\u7528\u533a\u57df\u611f\u77e5\u7279\u5f81\u878d\u5408\u786e\u4fdd\u7cbe\u786e\u914d\u51c6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u878d\u5408\u548c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\uff0c\u901a\u8fc7\u7ed3\u6784\u76f8\u4f3c\u6027\u8865\u507f\u673a\u5236\u4f18\u5316\u5149\u8c31\u878d\u5408", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u6e05\u6670\u5ea6\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6781\u7aef\u73af\u5883\u4e0b\u7ea2\u5916-\u53ef\u89c1\u5149\u8c31\u878d\u5408\u7684\u6311\u6218\uff0c\u4e3a\u591a\u6a21\u6001\u6210\u50cf\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.07472", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07472", "abs": "https://arxiv.org/abs/2512.07472", "authors": ["Siyu Xu", "Zijian Wang", "Yunke Wang", "Chenghao Xia", "Tao Huang", "Chang Xu"], "title": "Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the \"Memory Trap\". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($\u03c0_{0}$ and $\u03c0_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.06648", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06648", "abs": "https://arxiv.org/abs/2512.06648", "authors": ["Xiao Li"], "title": "Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network", "comment": "in Chinese language", "summary": "Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.\n  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.\n  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u4e2d\u56fdA\u80a1\u4e0a\u5e02\u516c\u53f8\u8d22\u52a1\u821e\u5f0a\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9762\u677f\u6570\u636e\u8f6c\u6362\u4e3a\u56fe\u50cf\u5f62\u5f0f\u5b9e\u73b0\u63d0\u524d\u9884\u6d4b\uff0c\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9884\u8b66\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8bc6\u522b\u5173\u952e\u821e\u5f0a\u6307\u6807\u3002", "motivation": "\u8d22\u52a1\u821e\u5f0a\u96be\u4ee5\u68c0\u6d4b\uff0c\u4f20\u7edf\u7edf\u8ba1\u6a21\u578b\u96be\u4ee5\u5904\u7406\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u53ea\u80fd\u57fa\u4e8e\u5f53\u5e74\u6570\u636e\u5224\u65ad\u5f53\u5e74\u821e\u5f0a\uff0c\u7f3a\u4e4f\u65f6\u6548\u6027\u3002", "method": "\u8bbe\u8ba1\u7279\u5f81\u5de5\u7a0b\u65b9\u6848\u5c06\u516c\u53f8\u5e74\u5ea6\u9762\u677f\u6570\u636e\u8f6c\u6362\u4e3a\u7c7b\u56fe\u50cf\u8868\u793a\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u6a2a\u622a\u9762\u548c\u65f6\u95f4\u6a21\u5f0f\uff0c\u5b9e\u73b0\u63d0\u524d\u9884\u6d4b\uff1b\u91c7\u7528\u5c40\u90e8\u89e3\u91ca\u6280\u672f\u4ece\u5b9e\u4f53\u3001\u7279\u5f81\u548c\u65f6\u95f4\u7ef4\u5ea6\u5206\u6790\u6a21\u578b\u3002", "result": "CNN\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9884\u8b66\u6027\u80fd\u4e0a\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u548cLightGBM\uff1b\u53d1\u73b0\u507f\u503a\u80fd\u529b\u3001\u6bd4\u7387\u7ed3\u6784\u3001\u6cbb\u7406\u7ed3\u6784\u548c\u5185\u90e8\u63a7\u5236\u662f\u821e\u5f0a\u7684\u901a\u7528\u9884\u6d4b\u56e0\u5b50\uff1b\u975e\u821e\u5f0a\u516c\u53f8\u7279\u5f81\u6a21\u5f0f\u7a33\u5b9a\uff0c\u821e\u5f0a\u516c\u53f8\u7279\u5f81\u6a21\u5f0f\u96c6\u4e2d\u5728\u77ed\u671f\u7a97\u53e3\u3002", "conclusion": "CNN\u6846\u67b6\u80fd\u6709\u6548\u63d0\u524d\u68c0\u6d4b\u8d22\u52a1\u821e\u5f0a\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8bc6\u522b\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6a21\u578b\u9884\u6d4b\u4e0e\u5b9e\u9645\u821e\u5f0a\u884c\u4e3a\u7684\u4e00\u81f4\u6027\uff0c\u4e3a\u9ad8\u98ce\u9669\u73af\u5883\u4e0b\u7684\u821e\u5f0a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2512.06421", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06421", "abs": "https://arxiv.org/abs/2512.06421", "authors": ["Gengze Zhou", "Chongjian Ge", "Hao Tan", "Feng Liu", "Yicong Hong"], "title": "Rethinking Training Dynamics in Scale-wise Autoregressive Generation", "comment": null, "summary": "Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u56de\u5f52\u7cbe\u70bc\uff08SAR\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ea4\u9519\u5c3a\u5ea6\u5c55\u5f00\u673a\u5236\u548c\u5bf9\u6bd4\u6027\u5b66\u751f\u5f3a\u5236\u635f\u5931\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u4e2d\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5c3a\u5ea6\u7ea7\u81ea\u56de\u5f52\u6a21\u578b\u5b58\u5728\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u4e3b\u8981\u8868\u73b0\u4e3a\u8bad\u7ec3-\u6d4b\u8bd5\u4e0d\u5339\u914d\u548c\u5c3a\u5ea6\u95f4\u5b66\u4e60\u96be\u5ea6\u4e0d\u5e73\u8861\uff0c\u8fd9\u5f71\u54cd\u4e86\u751f\u6210\u8d28\u91cf\u3002", "method": "\u63d0\u51faSAR\u65b9\u6cd5\uff0c\u5305\u542b\u4ea4\u9519\u5c3a\u5ea6\u5c55\u5f00\uff08SSR\uff09\u673a\u5236\u548c\u5bf9\u6bd4\u6027\u5b66\u751f\u5f3a\u5236\u635f\u5931\uff08CSFL\uff09\u3002SSR\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u56de\u5f52\u5c55\u5f00\u8ba9\u6a21\u578b\u63a5\u89e6\u81ea\u8eab\u4e2d\u95f4\u9884\u6d4b\uff0c\u5bf9\u9f50\u8bad\u7ec3\u6d4b\u8bd5\u6a21\u5f0f\uff1bCSFL\u4e3a\u81ea\u751f\u6210\u4e0a\u4e0b\u6587\u63d0\u4f9b\u5145\u5206\u76d1\u7763\u4ee5\u786e\u4fdd\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728\u9884\u8bad\u7ec3AR\u6a21\u578b\u4e0a\u5e94\u7528SAR\u53ef\u4e00\u81f4\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002\u4f8b\u5982\u5728ImageNet 256\u6570\u636e\u96c6\u4e0a\uff0cSAR\u4f7fFlexVAR-d16\u7684FID\u964d\u4f4e\u4e865.2%\uff0810\u4e2aepoch\uff0c32xA100 GPU\u4e0a5\u5c0f\u65f6\uff09\u3002", "conclusion": "SAR\u56e0\u5176\u9ad8\u6548\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\uff0c\u6709\u671b\u6210\u4e3a\u89c6\u89c9\u81ea\u56de\u5f52\u751f\u6210\u7684\u53ef\u9760\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2512.07482", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07482", "abs": "https://arxiv.org/abs/2512.07482", "authors": ["Florian L\u00fcttner", "Nicole Neis", "Daniel Stadler", "Robin Moss", "Mirjam Fehling-Kaschek", "Matthias Pfriem", "Alexander Stolz", "Jens Ziehn"], "title": "From Real-World Traffic Data to Relevant Critical Scenarios", "comment": "8 pages, 8 figures", "summary": "The reliable operation of autonomous vehicles, automated driving functions, and advanced driver assistance systems across a wide range of relevant scenarios is critical for their development and deployment. Identifying a near-complete set of relevant driving scenarios for such functionalities is challenging due to numerous degrees of freedom involved, each affecting the outcomes of the driving scenario differently. Moreover, with increasing technical complexity of new functionalities, the number of potentially relevant, particularly \"unknown unsafe\" scenarios is increasing. To enhance validation efficiency, it is essential to identify relevant scenarios in advance, starting with simpler domains like highways before moving to more complex environments such as urban traffic. To address this, this paper focuses on analyzing lane change scenarios in highway traffic, which involve multiple degrees of freedom and present numerous safetyrelevant scenarios. We describe the process of data acquisition and processing of real-world data from public highway traffic, followed by the application of criticality measures on trajectory data to evaluate scenarios, as conducted within the AVEAS project (www.aveas.org). By linking the calculated measures to specific lane change driving scenarios and the conditions under which the data was collected, we facilitate the identification of safetyrelevant driving scenarios for various applications. Further, to tackle the extensive range of \"unknown unsafe\" scenarios, we propose a way to generate relevant scenarios by creating synthetic scenarios based on recorded ones. Consequently, we demonstrate and evaluate a processing chain that enables the identification of safety-relevant scenarios, the development of data-driven methods for extracting these scenarios, and the generation of synthetic critical scenarios via sampling on highways.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u9ad8\u901f\u516c\u8def\u6362\u9053\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u771f\u5b9e\u4e16\u754c\u6570\u636e\u548c\u5e94\u7528\u5173\u952e\u6027\u5ea6\u91cf\u6765\u8bc6\u522b\u5b89\u5168\u76f8\u5173\u573a\u666f\uff0c\u5e76\u751f\u6210\u5408\u6210\u5173\u952e\u573a\u666f\u4ee5\u63d0\u9ad8\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u590d\u6742\u5ea6\u7684\u589e\u52a0\uff0c\u8bc6\u522b\u6f5c\u5728\u7684\u5b89\u5168\u76f8\u5173\u573a\u666f\uff08\u5c24\u5176\u662f\u201c\u672a\u77e5\u4e0d\u5b89\u5168\u201d\u573a\u666f\uff09\u53d8\u5f97\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u4ece\u7b80\u5355\u573a\u666f\uff08\u5982\u9ad8\u901f\u516c\u8def\uff09\u5f00\u59cb\u9010\u6b65\u6269\u5c55\u5230\u590d\u6742\u73af\u5883\u3002", "method": "\u91c7\u96c6\u548c\u5904\u7406\u771f\u5b9e\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u6570\u636e\uff0c\u5e94\u7528\u5173\u952e\u6027\u5ea6\u91cf\u8bc4\u4f30\u8f68\u8ff9\u6570\u636e\uff0c\u5c06\u8ba1\u7b97\u51fa\u7684\u5ea6\u91cf\u4e0e\u7279\u5b9a\u6362\u9053\u9a7e\u9a76\u573a\u666f\u53ca\u6570\u636e\u91c7\u96c6\u6761\u4ef6\u5173\u8054\uff0c\u5e76\u57fa\u4e8e\u8bb0\u5f55\u7684\u573a\u666f\u751f\u6210\u5408\u6210\u573a\u666f\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u6761\u5904\u7406\u94fe\uff0c\u80fd\u591f\u8bc6\u522b\u5b89\u5168\u76f8\u5173\u573a\u666f\u3001\u63d0\u53d6\u8fd9\u4e9b\u573a\u666f\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u751f\u6210\u5408\u6210\u5173\u952e\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u652f\u6301\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u9ad8\u901f\u516c\u8def\u6362\u9053\u573a\u666f\u4e2d\u7684\u5b89\u5168\u9a8c\u8bc1\uff0c\u4e3a\u66f4\u590d\u6742\u73af\u5883\u7684\u573a\u666f\u5206\u6790\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.06649", "categories": ["cs.LG", "cs.CV", "cs.CY", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.06649", "abs": "https://arxiv.org/abs/2512.06649", "authors": ["Camellia Zakaria", "Aryan Sadeghi", "Weaam Jaafar", "Junshi Xu", "Alex Mariakakis", "Marianne Hatzopoulou"], "title": "Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning", "comment": "12 pages, 16 figures, 4 tables, 4 pages Appendix, in submission and under review for ACM MobiSys 2026 as of December 6th, 2025", "summary": "Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u4ea4\u901a\u76d1\u63a7\u89c6\u9891\u548c\u5929\u6c14\u6570\u636e\u6765\u4f30\u7b97\u8857\u9053\u7ea7\u522b\u7684\u9ed1\u78b3\u6d53\u5ea6\uff0c\u4ee5\u89e3\u51b3\u57ce\u5e02\u4ea4\u901a\u6392\u653e\u76d1\u6d4b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u9ed1\u78b3\u6392\u653e\u4e3b\u8981\u6765\u81ea\u4ea4\u901a\uff0c\u4f46\u73b0\u6709\u76d1\u6d4b\u8bbe\u5907\u6210\u672c\u9ad8\u4e14\u6570\u636e\u7a00\u7f3a\uff0c\u800c\u4ea4\u901a\u76d1\u63a7\u7cfb\u7edf\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5b58\u5728\u76d1\u6d4b\u6570\u636e\u4e0e\u4ea4\u901a\u73af\u5883\u540e\u679c\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\u3002", "method": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4ece\u4ea4\u901a\u89c6\u9891\u4e2d\u63d0\u53d6\u8f66\u8f86\u884c\u4e3a\u548c\u72b6\u6001\u7279\u5f81\uff0c\u7ed3\u5408\u5929\u6c14\u6570\u636e\u6784\u5efa\u6a21\u578b\u6765\u4f30\u7b97\u8857\u9053\u7ea7\u522b\u7684\u9ed1\u78b3\u6d53\u5ea6\u3002", "result": "\u6a21\u578b\u8fbe\u5230\u4e86R\u5e73\u65b9\u503c0.72\u548cRMSE 129.42 ng/m\u00b3\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5229\u7528\u73b0\u6709\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u8d44\u6e90\uff0c\u4e3a\u5730\u65b9\u5e02\u653f\u5c42\u9762\u7684\u6c61\u67d3\u51cf\u6392\u3001\u57ce\u5e02\u89c4\u5212\u3001\u516c\u5171\u5065\u5eb7\u548c\u73af\u5883\u6b63\u4e49\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2512.06422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06422", "abs": "https://arxiv.org/abs/2512.06422", "authors": ["Chunwei Tian", "Jingyuan Xie", "Lingjun Li", "Wangmeng Zuo", "Yanning Zhang", "David Zhang"], "title": "A Perception CNN for Facial Expression Recognition", "comment": "in IEEE Transactions on Image Processing (2025)", "summary": "Convolutional neural networks (CNNs) can automatically learn data patterns to express face images for facial expression recognition (FER). However, they may ignore effect of facial segmentation of FER. In this paper, we propose a perception CNN for FER as well as PCNN. Firstly, PCNN can use five parallel networks to simultaneously learn local facial features based on eyes, cheeks and mouth to realize the sensitive capture of the subtle changes in FER. Secondly, we utilize a multi-domain interaction mechanism to register and fuse between local sense organ features and global facial structural features to better express face images for FER. Finally, we design a two-phase loss function to restrict accuracy of obtained sense information and reconstructed face images to guarantee performance of obtained PCNN in FER. Experimental results show that our PCNN achieves superior results on several lab and real-world FER benchmarks: CK+, JAFFE, FER2013, FERPlus, RAF-DB and Occlusion and Pose Variant Dataset. Its code is available at https://github.com/hellloxiaotian/PCNN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u611f\u77e5CNN\uff08PCNN\uff09\uff0c\u901a\u8fc7\u5e76\u884c\u7f51\u7edc\u5b66\u4e60\u5c40\u90e8\u9762\u90e8\u7279\u5f81\uff0c\u5e76\u5229\u7528\u591a\u57df\u4ea4\u4e92\u673a\u5236\u878d\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4e2d\u53ef\u80fd\u5ffd\u7565\u9762\u90e8\u5206\u5272\u7684\u5f71\u54cd\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u9762\u90e8\u8868\u60c5\u7684\u7ec6\u5fae\u53d8\u5316\u3002", "method": "1. \u4f7f\u7528\u4e94\u4e2a\u5e76\u884c\u7f51\u7edc\u540c\u65f6\u5b66\u4e60\u57fa\u4e8e\u773c\u775b\u3001\u8138\u988a\u548c\u5634\u5df4\u7684\u5c40\u90e8\u9762\u90e8\u7279\u5f81\uff1b2. \u91c7\u7528\u591a\u57df\u4ea4\u4e92\u673a\u5236\u6ce8\u518c\u548c\u878d\u5408\u5c40\u90e8\u5668\u5b98\u7279\u5f81\u4e0e\u5168\u5c40\u9762\u90e8\u7ed3\u6784\u7279\u5f81\uff1b3. \u8bbe\u8ba1\u4e24\u9636\u6bb5\u635f\u5931\u51fd\u6570\u6765\u7ea6\u675f\u611f\u77e5\u4fe1\u606f\u7684\u51c6\u786e\u6027\u548c\u91cd\u5efa\u56fe\u50cf\u7684\u8d28\u91cf\u3002", "result": "PCNN\u5728\u591a\u4e2a\u5b9e\u9a8c\u5ba4\u548c\u771f\u5b9e\u4e16\u754c\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u57fa\u51c6\u6570\u636e\u96c6\uff08CK+\u3001JAFFE\u3001FER2013\u3001FERPlus\u3001RAF-DB\u548c\u906e\u6321\u4e0e\u59ff\u6001\u53d8\u5316\u6570\u636e\u96c6\uff09\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u7ed3\u679c\u3002", "conclusion": "PCNN\u901a\u8fc7\u5c40\u90e8\u7279\u5f81\u5b66\u4e60\u548c\u591a\u57df\u7279\u5f81\u878d\u5408\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u9762\u90e8\u8868\u60c5\u7684\u7ec6\u5fae\u53d8\u5316\uff0c\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.07507", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.07507", "abs": "https://arxiv.org/abs/2512.07507", "authors": ["Yiming Cui", "Shiyu Fang", "Jiarui Zhang", "Yan Huang", "Chengkai Xu", "Bing Zhu", "Hao Zhang", "Peng Hang", "Jian Sun"], "title": "VP-AutoTest: A Virtual-Physical Fusion Autonomous Driving Testing Platform", "comment": null, "summary": "The rapid development of autonomous vehicles has led to a surge in testing demand. Traditional testing methods, such as virtual simulation, closed-course, and public road testing, face several challenges, including unrealistic vehicle states, limited testing capabilities, and high costs. These issues have prompted increasing interest in virtual-physical fusion testing. However, despite its potential, virtual-physical fusion testing still faces challenges, such as limited element types, narrow testing scope, and fixed evaluation metrics. To address these challenges, we propose the Virtual-Physical Testing Platform for Autonomous Vehicles (VP-AutoTest), which integrates over ten types of virtual and physical elements, including vehicles, pedestrians, and roadside infrastructure, to replicate the diversity of real-world traffic participants. The platform also supports both single-vehicle interaction and multi-vehicle cooperation testing, employing adversarial testing and parallel deduction to accelerate fault detection and explore algorithmic limits, while OBU and Redis communication enable seamless vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) cooperation across all levels of cooperative automation. Furthermore, VP-AutoTest incorporates a multidimensional evaluation framework and AI-driven expert systems to conduct comprehensive performance assessment and defect diagnosis. Finally, by comparing virtual-physical fusion test results with real-world experiments, the platform performs credibility self-evaluation to ensure both the fidelity and efficiency of autonomous driving testing. Please refer to the website for the full testing functionalities on the autonomous driving public service platform OnSite:https://www.onsite.com.cn.", "AI": {"tldr": "VP-AutoTest\u5e73\u53f0\u901a\u8fc7\u96c6\u6210\u865a\u62df\u4e0e\u7269\u7406\u5143\u7d20\uff0c\u652f\u6301\u591a\u8f66\u534f\u540c\u6d4b\u8bd5\u548cAI\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u4e2d\u5143\u7d20\u7c7b\u578b\u6709\u9650\u3001\u6d4b\u8bd5\u8303\u56f4\u7a84\u548c\u8bc4\u4f30\u6307\u6807\u56fa\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u65b9\u6cd5\uff08\u5982\u865a\u62df\u4eff\u771f\u3001\u5c01\u95ed\u573a\u5730\u548c\u516c\u5f00\u9053\u8def\u6d4b\u8bd5\uff09\u5b58\u5728\u8f66\u8f86\u72b6\u6001\u4e0d\u771f\u5b9e\u3001\u6d4b\u8bd5\u80fd\u529b\u6709\u9650\u548c\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u865a\u62df\u7269\u7406\u878d\u5408\u6d4b\u8bd5\u867d\u5177\u6f5c\u529b\u4f46\u9762\u4e34\u5143\u7d20\u7c7b\u578b\u6709\u9650\u3001\u6d4b\u8bd5\u8303\u56f4\u7a84\u548c\u8bc4\u4f30\u6307\u6807\u56fa\u5b9a\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faVP-AutoTest\u5e73\u53f0\uff0c\u96c6\u6210\u5341\u4f59\u79cd\u865a\u62df\u4e0e\u7269\u7406\u5143\u7d20\uff08\u5982\u8f66\u8f86\u3001\u884c\u4eba\u3001\u8def\u4fa7\u8bbe\u65bd\uff09\uff0c\u652f\u6301\u5355\u8f66\u4ea4\u4e92\u4e0e\u591a\u8f66\u534f\u540c\u6d4b\u8bd5\uff0c\u91c7\u7528\u5bf9\u6297\u6d4b\u8bd5\u548c\u5e73\u884c\u63a8\u6f14\u52a0\u901f\u6545\u969c\u53d1\u73b0\uff0c\u901a\u8fc7OBU\u548cRedis\u5b9e\u73b0V2V/V2I\u901a\u4fe1\uff0c\u7ed3\u5408\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\u548cAI\u4e13\u5bb6\u7cfb\u7edf\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u4e0e\u7f3a\u9677\u8bca\u65ad\u3002", "result": "\u5e73\u53f0\u901a\u8fc7\u865a\u62df\u7269\u7406\u878d\u5408\u6d4b\u8bd5\u4e0e\u771f\u5b9e\u5b9e\u9a8c\u5bf9\u6bd4\uff0c\u8fdb\u884c\u53ef\u4fe1\u5ea6\u81ea\u8bc4\u4f30\uff0c\u786e\u4fdd\u6d4b\u8bd5\u7684\u4fdd\u771f\u5ea6\u4e0e\u6548\u7387\u3002", "conclusion": "VP-AutoTest\u5e73\u53f0\u6709\u6548\u89e3\u51b3\u4e86\u865a\u62df\u7269\u7406\u878d\u5408\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6d4b\u8bd5\u7684\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.06652", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06652", "abs": "https://arxiv.org/abs/2512.06652", "authors": ["Xiaolei Lu", "Shamim Nemati"], "title": "Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts", "comment": null, "summary": "Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AdaTTT\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6765\u63d0\u5347ICU\u60a3\u8005\u6709\u521b\u673a\u68b0\u901a\u6c14\u9700\u6c42\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u89e3\u51b3\u8de8\u673a\u6784\u90e8\u7f72\u65f6\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\u3002", "motivation": "ICU\u60a3\u8005\u6709\u521b\u673a\u68b0\u901a\u6c14\u9700\u6c42\u9884\u6d4b\u5bf9\u53ca\u65f6\u5e72\u9884\u548c\u8d44\u6e90\u5206\u914d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e0d\u540c\u673a\u6784\u95f4\u7684\u60a3\u8005\u7fa4\u4f53\u3001\u4e34\u5e8a\u5b9e\u8df5\u548c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7cfb\u7edf\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u62ec\u4fe1\u606f\u7406\u8bba\u8fb9\u754c\u5206\u6790\u3001\u81ea\u76d1\u7763\u5b66\u4e60\uff08\u91cd\u6784\u548c\u63a9\u7801\u7279\u5f81\u5efa\u6a21\uff09\u3001\u52a8\u6001\u63a9\u7801\u7b56\u7565\u3001\u539f\u578b\u5b66\u4e60\u548c\u90e8\u5206\u6700\u4f18\u4f20\u8f93\u7279\u5f81\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2d\u5fc3ICU\u961f\u5217\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u6d4b\u8bd5\u65f6\u9002\u5e94\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "AdaTTT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u8de8\u673a\u6784\u90e8\u7f72\u65f6IMV\u9884\u6d4b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u652f\u6301\u3002"}}
{"id": "2512.06424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06424", "abs": "https://arxiv.org/abs/2512.06424", "authors": ["Tianshan Zhang", "Zeyu Zhang", "Hao Tang"], "title": "DragMesh: Interactive 3D Generation Made Easy", "comment": null, "summary": "While generative models have excelled at creating static 3D content, the pursuit of systems that understand how objects move and respond to interactions remains a fundamental challenge. Current methods for articulated motion lie at a crossroads: they are either physically consistent but too slow for real-time use, or generative but violate basic kinematic constraints. We present DragMesh, a robust framework for real-time interactive 3D articulation built around a lightweight motion generation core. Our core contribution is a novel decoupled kinematic reasoning and motion generation framework. First, we infer the latent joint parameters by decoupling semantic intent reasoning (which determines the joint type) from geometric regression (which determines the axis and origin using our Kinematics Prediction Network (KPP-Net)). Second, to leverage the compact, continuous, and singularity-free properties of dual quaternions for representing rigid body motion, we develop a novel Dual Quaternion VAE (DQ-VAE). This DQ-VAE receives these predicted priors, along with the original user drag, to generate a complete, plausible motion trajectory. To ensure strict adherence to kinematics, we inject the joint priors at every layer of the DQ-VAE's non-autoregressive Transformer decoder using FiLM (Feature-wise Linear Modulation) conditioning. This persistent, multi-scale guidance is complemented by a numerically-stable cross-product loss to guarantee axis alignment. This decoupled design allows DragMesh to achieve real-time performance and enables plausible, generative articulation on novel objects without retraining, offering a practical step toward generative 3D intelligence. Code: https://github.com/AIGeeksGroup/DragMesh. Website: https://aigeeksgroup.github.io/DragMesh.", "AI": {"tldr": "DragMesh\u662f\u4e00\u4e2a\u5b9e\u65f6\u4ea4\u4e92\u5f0f3D\u5173\u8282\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8fd0\u52a8\u5b66\u63a8\u7406\u548c\u8fd0\u52a8\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u4e00\u81f4\u6027\u548c\u5b9e\u65f6\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u751f\u6210\u6a21\u578b\u5728\u9759\u6001\u5185\u5bb9\u521b\u5efa\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7406\u89e3\u7269\u4f53\u5982\u4f55\u79fb\u52a8\u548c\u54cd\u5e94\u4ea4\u4e92\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u5173\u8282\u8fd0\u52a8\u65b9\u6cd5\u8981\u4e48\u7269\u7406\u4e00\u81f4\u4f46\u901f\u5ea6\u6162\uff0c\u8981\u4e48\u751f\u6210\u5feb\u901f\u4f46\u8fdd\u53cd\u57fa\u672c\u8fd0\u52a8\u5b66\u7ea6\u675f\u3002", "method": "1) \u901a\u8fc7\u89e3\u8026\u8bed\u4e49\u610f\u56fe\u63a8\u7406\u548c\u51e0\u4f55\u56de\u5f52\u6765\u63a8\u65ad\u6f5c\u5728\u5173\u8282\u53c2\u6570\uff1b2) \u5f00\u53d1\u57fa\u4e8e\u5bf9\u5076\u56db\u5143\u6570\u7684VAE\u6a21\u578b(DQ-VAE)\u751f\u6210\u5b8c\u6574\u8fd0\u52a8\u8f68\u8ff9\uff1b3) \u4f7f\u7528FiLM\u6761\u4ef6\u6ce8\u5165\u5728DQ-VAE\u7684\u6bcf\u4e00\u5c42\u786e\u4fdd\u8fd0\u52a8\u5b66\u7ea6\u675f\uff1b4) \u91c7\u7528\u6570\u503c\u7a33\u5b9a\u7684\u53c9\u79ef\u635f\u5931\u4fdd\u8bc1\u8f74\u5bf9\u9f50\u3002", "result": "DragMesh\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u80fd\u591f\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5bf9\u65b0\u9896\u7269\u4f53\u8fdb\u884c\u5408\u7406\u7684\u751f\u6210\u5f0f\u5173\u8282\u8fd0\u52a8\uff0c\u4e3a\u751f\u6210\u5f0f3D\u667a\u80fd\u8fc8\u51fa\u4e86\u5b9e\u7528\u7684\u4e00\u6b65\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8bbe\u8ba1\u548c\u6301\u7eed\u7684\u8fd0\u52a8\u5b66\u6307\u5bfc\uff0c\u5728\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4ea4\u4e92\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u5f0f3D\u5173\u8282\u8fd0\u52a8\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2512.07582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07582", "abs": "https://arxiv.org/abs/2512.07582", "authors": ["Guangyan Chen", "Meiling Wang", "Qi Shao", "Zichen Zhou", "Weixin Mao", "Te Cui", "Minzhao Zhu", "Yinan Deng", "Luojie Yang", "Zhanqi Zhang", "Yi Yang", "Hua Chen", "Yufeng Yue"], "title": "See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations", "comment": null, "summary": "Developing robust and general-purpose manipulation policies represents a fundamental objective in robotics research. While Vision-Language-Action (VLA) models have demonstrated promising capabilities for end-to-end robot control, existing approaches still exhibit limited generalization to tasks beyond their training distributions. In contrast, humans possess remarkable proficiency in acquiring novel skills by simply observing others performing them once. Inspired by this capability, we propose ViVLA, a generalist robotic manipulation policy that achieves efficient task learning from a single expert demonstration video at test time. Our approach jointly processes an expert demonstration video alongside the robot's visual observations to predict both the demonstrated action sequences and subsequent robot actions, effectively distilling fine-grained manipulation knowledge from expert behavior and transferring it seamlessly to the agent. To enhance the performance of ViVLA, we develop a scalable expert-agent pair data generation pipeline capable of synthesizing paired trajectories from easily accessible human videos, further augmented by curated pairs from publicly available datasets. This pipeline produces a total of 892,911 expert-agent samples for training ViVLA. Experimental results demonstrate that our ViVLA is able to acquire novel manipulation skills from only a single expert demonstration video at test time. Our approach achieves over 30% improvement on unseen LIBERO tasks and maintains above 35% gains with cross-embodiment videos. Real-world experiments demonstrate effective learning from human videos, yielding more than 38% improvement on unseen tasks.", "AI": {"tldr": "ViVLA\u662f\u4e00\u4e2a\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u5728\u6d4b\u8bd5\u65f6\u9ad8\u6548\u5b66\u4e60\u65b0\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eba\u7c7b\u89c6\u9891\u5230\u673a\u5668\u64cd\u4f5c\u7684\u8de8\u5b9e\u4f53\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u8bad\u7ec3\u5206\u5e03\u5916\u7684\u4efb\u52a1\u4e0a\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800c\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u4e00\u6b21\u89c2\u5bdf\u5feb\u901f\u5b66\u4e60\u65b0\u6280\u80fd\uff0c\u8fd9\u542f\u53d1\u4e86\u5f00\u53d1\u80fd\u591f\u4ece\u5355\u4e00\u6f14\u793a\u4e2d\u5b66\u4e60\u65b0\u4efb\u52a1\u7684\u673a\u5668\u4eba\u7b56\u7565\u3002", "method": "ViVLA\u901a\u8fc7\u8054\u5408\u5904\u7406\u4e13\u5bb6\u6f14\u793a\u89c6\u9891\u548c\u673a\u5668\u4eba\u89c6\u89c9\u89c2\u5bdf\uff0c\u540c\u65f6\u9884\u6d4b\u6f14\u793a\u52a8\u4f5c\u5e8f\u5217\u548c\u540e\u7eed\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u4ece\u4e13\u5bb6\u884c\u4e3a\u4e2d\u63d0\u53d6\u7cbe\u7ec6\u64cd\u4f5c\u77e5\u8bc6\u3002\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u4e13\u5bb6-\u667a\u80fd\u4f53\u914d\u5bf9\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u4ece\u4eba\u7c7b\u89c6\u9891\u5408\u6210\u914d\u5bf9\u8f68\u8ff9\u3002", "result": "ViVLA\u5728\u672a\u89c1\u8fc7\u7684LIBERO\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc730%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8de8\u5b9e\u4f53\u89c6\u9891\u4e0a\u4fdd\u630135%\u4ee5\u4e0a\u7684\u589e\u76ca\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u4ece\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u83b7\u5f97\u4e8638%\u4ee5\u4e0a\u7684\u6539\u8fdb\u3002", "conclusion": "ViVLA\u5c55\u793a\u4e86\u4ece\u5355\u4e00\u6f14\u793a\u89c6\u9891\u5b66\u4e60\u65b0\u64cd\u4f5c\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2512.06655", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06655", "abs": "https://arxiv.org/abs/2512.06655", "authors": ["Jehyeok Yeon", "Federico Cinus", "Yifan Wu", "Luca Luceri"], "title": "GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering", "comment": null, "summary": "Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faGraph-Regularized Sparse Autoencoders (GSAEs)\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u6b63\u5219\u5316\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6539\u8fdbLLM\u5b89\u5168\u9632\u5fa1\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5c06\u5b89\u5168\u6982\u5ff5\u5c40\u9650\u4e8e\u5355\u4e00\u7279\u5f81\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u9632\u5fa1\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8981\u4e48\u662f\u9ed1\u76d2\u8f93\u51fa\u8fc7\u6ee4\uff0c\u8981\u4e48\u57fa\u4e8e\u5185\u90e8\u6fc0\u6d3b\u7684\u5355\u7ef4\u5b89\u5168\u7279\u5f81\u5047\u8bbe\u3002\u4f46\u7814\u7a76\u8868\u660e\u62bd\u8c61\u5b89\u5168\u6982\u5ff5\uff08\u5982\u62d2\u7edd\u3001\u65f6\u95f4\u6027\uff09\u5206\u5e03\u5728\u591a\u4e2a\u7279\u5f81\u4e2d\uff0c\u800c\u975e\u5b64\u7acb\u4e8e\u5355\u4e00\u7279\u5f81\u3002", "method": "GSAEs\u5728\u7a00\u758f\u81ea\u7f16\u7801\u5668\u57fa\u7840\u4e0a\u5f15\u5165\u62c9\u666e\u62c9\u65af\u5e73\u6ed1\u60e9\u7f5a\u9879\uff0c\u6784\u5efa\u795e\u7ecf\u5143\u5171\u6fc0\u6d3b\u56fe\uff0c\u6062\u590d\u5e73\u6ed1\u7684\u5206\u5e03\u5f0f\u5b89\u5168\u8868\u793a\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u95e8\u63a7\u673a\u5236\uff0c\u4ec5\u5728\u68c0\u6d4b\u5230\u6709\u5bb3\u63d0\u793a\u6216\u751f\u6210\u5185\u5bb9\u65f6\u6fc0\u6d3b\u5e72\u9884\u3002", "result": "GSAE\u5728\u5b89\u5168\u548cQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u9009\u62e9\u6027\u62d2\u7edd\u7387\u8fbe82%\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6SAE\uff0842%\uff09\uff0c\u540c\u65f6\u5728TriviaQA\uff0870%\uff09\u3001TruthfulQA\uff0865%\uff09\u3001GSM8K\uff0874%\uff09\u4e0a\u4fdd\u6301\u5f3a\u4efb\u52a1\u51c6\u786e\u6027\u3002\u5bf9LLaMA-3\u3001Mistral\u7b49\u6a21\u578b\u5bb6\u65cf\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\uff0c\u5bf9\u6297GCG\u3001AutoDAN\u7b49\u8d8a\u72f1\u653b\u51fb\u65f6\u4fdd\u6301\u226590%\u7684\u6709\u5bb3\u5185\u5bb9\u62d2\u7edd\u7387\u3002", "conclusion": "GSAEs\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u8fd0\u884c\u65f6\u5b89\u5168\u63a7\u5236\uff0c\u5c06\u7279\u5f81\u7ec4\u5408\u6210\u52a0\u6743\u5b89\u5168\u76f8\u5173\u65b9\u5411\uff0c\u81ea\u9002\u5e94\u6267\u884c\u62d2\u7edd\u540c\u65f6\u4fdd\u6301\u826f\u6027\u67e5\u8be2\u7684\u5b9e\u7528\u6027\uff0c\u4e3a\u89e3\u51b3LLM\u5b89\u5168\u6311\u6218\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5206\u5e03\u5f0f\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2512.06426", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06426", "abs": "https://arxiv.org/abs/2512.06426", "authors": ["Nzakiese Mbongo", "Kailash A. Hambarde", "Hugo Proen\u00e7a"], "title": "When Gender is Hard to See: Multi-Attribute Support for Long-Range Recognition", "comment": "12 pages, 9 figures", "summary": "Accurate gender recognition from extreme long-range imagery remains a challenging problem due to limited spatial resolution, viewpoint variability, and loss of facial cues. For such purpose, we present a dual-path transformer framework that leverages CLIP to jointly model visual and attribute-driven cues for gender recognition at a distance. The framework integrates two complementary streams: (1) a direct visual path that refines a pre-trained CLIP image encoder through selective fine-tuning of its upper layers, and (2) an attribute-mediated path that infers gender from a set of soft-biometric prompts (e.g., hairstyle, clothing, accessories) aligned in the CLIP text-image space. Spatial channel attention modules further enhance discriminative localization under occlusion and low resolution. To support large-scale evaluation, we construct U-DetAGReID, a unified long-range gender dataset derived from DetReIDx and AG-ReID.v2, harmonized under a consistent ternary labeling scheme (Male, Female, Unknown). Extensive experiments suggest that the proposed solution surpasses state-of-the-art person-attribute and re-identification baselines across multiple metrics (macro-F1, accuracy, AUC), with consistent robustness to distance, angle, and height variations. Qualitative attention visualizations confirm interpretable attribute localization and responsible abstention behavior. Our results show that language-guided dual-path learning offers a principled, extensible foundation for responsible gender recognition in unconstrained long-range scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84transformer\u6846\u67b6\uff0c\u5229\u7528CLIP\u8054\u5408\u5efa\u6a21\u89c6\u89c9\u548c\u5c5e\u6027\u7ebf\u7d22\uff0c\u7528\u4e8e\u8fdc\u8ddd\u79bb\u6027\u522b\u8bc6\u522b\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8fdc\u8ddd\u79bb\u56fe\u50cf\u4e2d\u7684\u6027\u522b\u8bc6\u522b\u7531\u4e8e\u7a7a\u95f4\u5206\u8fa8\u7387\u6709\u9650\u3001\u89c6\u89d2\u53d8\u5316\u548c\u9762\u90e8\u7ebf\u7d22\u4e22\u5931\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u8def\u5f84\u6846\u67b6\uff1a\u76f4\u63a5\u89c6\u89c9\u8def\u5f84\u901a\u8fc7\u9009\u62e9\u6027\u5fae\u8c03CLIP\u56fe\u50cf\u7f16\u7801\u5668\u4e0a\u5c42\uff0c\u5c5e\u6027\u4ecb\u5bfc\u8def\u5f84\u4ece\u8f6f\u751f\u7269\u7279\u5f81\u63d0\u793a\u63a8\u65ad\u6027\u522b\u3002\u7ed3\u5408\u7a7a\u95f4\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u5224\u522b\u6027\u5b9a\u4f4d\u3002", "result": "\u5728\u6784\u5efa\u7684U-DetAGReID\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728macro-F1\u3001\u51c6\u786e\u7387\u548cAUC\u7b49\u6307\u6807\u4e0a\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5bf9\u8ddd\u79bb\u3001\u89d2\u5ea6\u548c\u9ad8\u5ea6\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u53cc\u8def\u5f84\u5b66\u4e60\u4e3a\u65e0\u7ea6\u675f\u8fdc\u8ddd\u79bb\u573a\u666f\u4e0b\u7684\u8d1f\u8d23\u4efb\u6027\u522b\u8bc6\u522b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2512.07673", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07673", "abs": "https://arxiv.org/abs/2512.07673", "authors": ["Matthias Heyrman", "Chenhao Li", "Victor Klemm", "Dongho Kang", "Stelian Coros", "Marco Hutter"], "title": "Multi-Domain Motion Embedding: Expressive Real-Time Mimicry for Legged Robots", "comment": "15 pages", "summary": "Effective motion representation is crucial for enabling robots to imitate expressive behaviors in real time, yet existing motion controllers often ignore inherent patterns in motion. Previous efforts in representation learning do not attempt to jointly capture structured periodic patterns and irregular variations in human and animal movement. To address this, we present Multi-Domain Motion Embedding (MDME), a motion representation that unifies the embedding of structured and unstructured features using a wavelet-based encoder and a probabilistic embedding in parallel. This produces a rich representation of reference motions from a minimal input set, enabling improved generalization across diverse motion styles and morphologies. We evaluate MDME on retargeting-free real-time motion imitation by conditioning robot control policies on the learned embeddings, demonstrating accurate reproduction of complex trajectories on both humanoid and quadruped platforms. Our comparative studies confirm that MDME outperforms prior approaches in reconstruction fidelity and generalizability to unseen motions. Furthermore, we demonstrate that MDME can reproduce novel motion styles in real-time through zero-shot deployment, eliminating the need for task-specific tuning or online retargeting. These results position MDME as a generalizable and structure-aware foundation for scalable real-time robot imitation.", "AI": {"tldr": "MDME\u662f\u4e00\u79cd\u8fd0\u52a8\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u7f16\u7801\u5668\u548c\u6982\u7387\u5d4c\u5165\u7edf\u4e00\u6355\u6349\u8fd0\u52a8\u4e2d\u7684\u7ed3\u6784\u5316\u5468\u671f\u6a21\u5f0f\u548c\u975e\u89c4\u5219\u53d8\u5316\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u5b9a\u5411\u7684\u5b9e\u65f6\u8fd0\u52a8\u6a21\u4eff\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u63a7\u5236\u5668\u5f80\u5f80\u5ffd\u7565\u8fd0\u52a8\u4e2d\u7684\u56fa\u6709\u6a21\u5f0f\uff0c\u4e14\u73b0\u6709\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u540c\u65f6\u6355\u6349\u4eba\u7c7b\u548c\u52a8\u7269\u8fd0\u52a8\u4e2d\u7684\u7ed3\u6784\u5316\u5468\u671f\u6027\u6a21\u5f0f\u548c\u975e\u89c4\u5219\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u591a\u57df\u8fd0\u52a8\u5d4c\u5165(MDME)\uff0c\u4f7f\u7528\u5c0f\u6ce2\u7f16\u7801\u5668\u548c\u5e76\u884c\u6982\u7387\u5d4c\u5165\u6765\u7edf\u4e00\u8868\u793a\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u7279\u5f81\uff0c\u4ece\u6700\u5c0f\u8f93\u5165\u96c6\u751f\u6210\u4e30\u5bcc\u7684\u53c2\u8003\u8fd0\u52a8\u8868\u793a\u3002", "result": "\u5728\u7c7b\u4eba\u673a\u5668\u4eba\u548c\u56db\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u590d\u6742\u8f68\u8ff9\u7684\u7cbe\u786e\u590d\u5236\uff0c\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u5bf9\u672a\u89c1\u8fd0\u52a8\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u901a\u8fc7\u96f6\u6837\u672c\u90e8\u7f72\u5b9e\u65f6\u91cd\u73b0\u65b0\u9896\u8fd0\u52a8\u98ce\u683c\u3002", "conclusion": "MDME\u4f5c\u4e3a\u53ef\u6269\u5c55\u5b9e\u65f6\u673a\u5668\u4eba\u6a21\u4eff\u7684\u7ed3\u6784\u611f\u77e5\u57fa\u7840\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u6216\u5728\u7ebf\u91cd\u5b9a\u5411\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.06665", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06665", "abs": "https://arxiv.org/abs/2512.06665", "authors": ["Panagiota Kiourti", "Anu Singh", "Preeti Duraipandian", "Weichao Zhou", "Wenchao Li"], "title": "Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods", "comment": null, "summary": "This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u62ec\u65b0\u7684\u76f8\u4f3c\u8f93\u5165\u5b9a\u4e49\u3001\u9c81\u68d2\u6027\u6307\u6807\u548c\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u8f93\u5165\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u6311\u6218\u5f53\u524d\u4e3b\u8981\u5ffd\u7565\u6a21\u578b\u8f93\u51fa\u5dee\u5f02\u7684\u5f52\u56e0\u9c81\u68d2\u6027\u6982\u5ff5\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u7684\u6307\u6807\u6765\u51c6\u786e\u8bc4\u4f30\u5f52\u56e0\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u65b0\u7684\u76f8\u4f3c\u8f93\u5165\u5b9a\u4e49\u3001\u65b0\u7684\u9c81\u68d2\u6027\u6307\u6807\uff0c\u4ee5\u53ca\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u751f\u6210\u8fd9\u4e9b\u8f93\u5165\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u73b0\u6709\u6307\u6807\u548c\u6700\u5148\u8fdb\u5f52\u56e0\u65b9\u6cd5\u7684\u7efc\u5408\u8bc4\u4f30\uff0c\u53d1\u73b0\u9700\u8981\u66f4\u5ba2\u89c2\u7684\u6307\u6807\u6765\u63ed\u793a\u5f52\u56e0\u65b9\u6cd5\u800c\u975e\u795e\u7ecf\u7f51\u7edc\u7684\u5f31\u70b9\u3002", "conclusion": "\u65b0\u6846\u67b6\u80fd\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5f52\u56e0\u65b9\u6cd5\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u7a81\u51fa\u663e\u793a\u5f52\u56e0\u65b9\u6cd5\u672c\u8eab\u800c\u975e\u6a21\u578b\u7684\u95ee\u9898\u3002"}}
{"id": "2512.06434", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06434", "abs": "https://arxiv.org/abs/2512.06434", "authors": ["Lucas R. Mareque", "Ricardo L. Armentano", "Leandro J. Cymberknop"], "title": "Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening", "comment": "8 pages, 2 figures, 3 tables", "summary": "Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5168\u81ea\u52a8\u65b9\u6cd5\uff0c\u4ece2D\u5408\u6210\u4eba\u4f53\u56fe\u50cf\u4e2d\u4f30\u8ba1\u4e94\u4e2a\u5173\u952e\u4eba\u4f53\u6d4b\u91cf\u6307\u6807\uff0c\u7528\u4e8e\u8fd0\u52a8\u5458\u5fc3\u8840\u7ba1\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u5de5\u4eba\u4f53\u6d4b\u91cf\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u4f9d\u8d56\u64cd\u4f5c\u8005\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5927\u89c4\u6a21\u8fd0\u52a8\u5458\u7b5b\u67e5\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u4ece3D\u4eba\u4f53\u7f51\u683c\u751f\u6210\u7684100,000\u5f20\u5408\u6210\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u548c\u8bc4\u4f30\u4e86VGG19\u3001ResNet50\u548cDenseNet121\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u56de\u5f52\u5206\u6790\u3002", "result": "\u6240\u6709\u6a21\u578b\u90fd\u5b9e\u73b0\u4e86\u4e9a\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u5176\u4e2dResNet50\u8868\u73b0\u6700\u4f73\uff0c\u5728\u6240\u6709\u6d4b\u91cf\u6307\u6807\u4e0a\u7684\u5e73\u5747MAE\u4e3a0.668\u5398\u7c73\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u80fd\u591f\u63d0\u4f9b\u5927\u89c4\u6a21\u51c6\u786e\u7684\u4eba\u4f53\u6d4b\u91cf\u6570\u636e\uff0c\u53ef\u4f5c\u4e3a\u8fd0\u52a8\u5458\u7b5b\u67e5\u534f\u8bae\u7684\u6709\u6548\u8865\u5145\u5de5\u5177\u3002"}}
{"id": "2512.07680", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07680", "abs": "https://arxiv.org/abs/2512.07680", "authors": ["P. A. Wigner", "L. Romanello", "A. Hammad", "P. H. Nguyen", "T. Lan", "S. F. Armanini", "B. B. Kocer", "M. Kovac"], "title": "AMBER: Aerial deployable gripping crawler with compliant microspine for canopy manipulation", "comment": null, "summary": "This paper presents an aerially deployable crawler designed for adaptive locomotion and manipulation within tree canopies. The system combines compliant microspine-based tracks, a dual-track rotary gripper, and an elastic tail, enabling secure attachment and stable traversal across branches of varying curvature and inclination.\n  Experiments demonstrate reliable gripping up to 90 degrees of body roll and inclination, while effective climbing on branches inclined up to 67.5 degrees, achieving a maximum speed of 0.55 body lengths per second on horizontal branches. The compliant tracks allow yaw steering of up to 10 degrees, enhancing maneuverability on irregular surfaces.\n  Power measurements show efficient operation with a dimensionless cost of transport over an order of magnitude lower than typical hovering power consumption in aerial robots. Integrated within a drone-tether deployment system, the crawler provides a robust, low-power platform for environmental sampling and in-canopy sensing, bridging the gap between aerial and surface-based ecological robotics.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u5728\u6811\u51a0\u4e2d\u81ea\u9002\u5e94\u79fb\u52a8\u548c\u64cd\u4f5c\u7684\u7a7a\u4e2d\u90e8\u7f72\u722c\u884c\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u4e86\u67d4\u6027\u5fae\u523a\u8f68\u9053\u3001\u53cc\u8f68\u9053\u65cb\u8f6c\u6293\u53d6\u5668\u548c\u5f39\u6027\u5c3e\u90e8\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u66f2\u7387\u548c\u503e\u659c\u5ea6\u7684\u6811\u679d\u4e0a\u7a33\u5b9a\u79fb\u52a8\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u6811\u51a0\u73af\u5883\u4e2d\u8fdb\u884c\u73af\u5883\u91c7\u6837\u548c\u4f20\u611f\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5f25\u8865\u7a7a\u4e2d\u673a\u5668\u4eba\u548c\u5730\u9762\u751f\u6001\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u6280\u672f\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u67d4\u6027\u5fae\u523a\u8f68\u9053\u5b9e\u73b0\u5b89\u5168\u9644\u7740\uff0c\u53cc\u8f68\u9053\u65cb\u8f6c\u6293\u53d6\u5668\u63d0\u4f9b\u6293\u53d6\u80fd\u529b\uff0c\u5f39\u6027\u5c3e\u90e8\u589e\u5f3a\u7a33\u5b9a\u6027\u3002\u7cfb\u7edf\u901a\u8fc7\u65e0\u4eba\u673a-\u7ef3\u7d22\u90e8\u7f72\u7cfb\u7edf\u8fdb\u884c\u7a7a\u4e2d\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u572890\u5ea6\u6eda\u8f6c\u548c\u503e\u659c\u4e0b\u4ecd\u80fd\u53ef\u9760\u6293\u53d6\uff0c\u572867.5\u5ea6\u503e\u659c\u6811\u679d\u4e0a\u6709\u6548\u6500\u722c\uff0c\u6c34\u5e73\u6811\u679d\u4e0a\u6700\u5927\u901f\u5ea60.55\u4f53\u957f/\u79d2\uff0c\u8f6c\u5411\u89d2\u5ea6\u53ef\u8fbe10\u5ea6\u3002\u80fd\u8017\u6bd4\u5178\u578b\u60ac\u505c\u65e0\u4eba\u673a\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6811\u51a0\u73af\u5883\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4f4e\u529f\u8017\u3001\u9c81\u68d2\u7684\u5e73\u53f0\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7a7a\u4e2d\u90e8\u7f72\u4e0e\u8868\u9762\u79fb\u52a8\u7684\u7ed3\u5408\u3002"}}
{"id": "2512.06666", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06666", "abs": "https://arxiv.org/abs/2512.06666", "authors": ["Urav Maniar"], "title": "The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification", "comment": "Link to the repository: https://github.com/urav06/research", "summary": "Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408Hydra\u548cQuant\u4e24\u79cd\u9ad8\u6548\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u7684\u540c\u65f6\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5143\u5b66\u4e60\u7b56\u7565\u5728\u6709\u6548\u5229\u7528\u7b97\u6cd5\u4e92\u8865\u6027\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u65f6\u95f4\u7cfb\u5217\u5206\u7c7b\u4e2d\u9ad8\u7cbe\u5ea6\u7b97\u6cd5\uff08\u5982HIVE-COTE 2.0\uff09\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u901a\u8fc7\u7ec4\u5408\u4e92\u8865\u7684\u9ad8\u6548\u7b97\u6cd5\u6765\u83b7\u5f97\u96c6\u6210\u5b66\u4e60\u4f18\u52bf\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "method": "\u91c7\u7528\u516d\u79cd\u96c6\u6210\u914d\u7f6e\u7ec4\u5408Hydra\uff08\u7ade\u4e89\u5377\u79ef\u6838\uff09\u548cQuant\uff08\u5206\u5c42\u533a\u95f4\u5206\u4f4d\u6570\uff09\u4e24\u79cd\u7b97\u6cd5\uff0c\u572810\u4e2a\u5927\u89c4\u6a21MONSTER\u6570\u636e\u96c6\uff087,898\u52301,168,774\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6700\u5f3a\u914d\u7f6e\u5c06\u5e73\u5747\u51c6\u786e\u7387\u4ece0.829\u63d0\u5347\u52300.836\uff0c\u572810\u4e2a\u6570\u636e\u96c6\u4e2d\u76847\u4e2a\u4e0a\u53d6\u5f97\u6210\u529f\u3002\u4f46\u9884\u6d4b\u7ec4\u5408\u96c6\u6210\u4ec5\u6355\u83b7\u4e8611%\u7684\u7406\u8bba\u6f5c\u529b\uff0c\u7279\u5f81\u62fc\u63a5\u65b9\u6cd5\u901a\u8fc7\u5b66\u4e60\u65b0\u7684\u51b3\u7b56\u8fb9\u754c\u8d85\u8d8a\u4e86\u7406\u8bba\u754c\u9650\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u7684\u6838\u5fc3\u6311\u6218\u5df2\u4ece\u786e\u4fdd\u7b97\u6cd5\u5dee\u5f02\u6027\u8f6c\u5411\u5b66\u4e60\u5982\u4f55\u6709\u6548\u7ec4\u5408\u7b97\u6cd5\u3002\u5f53\u524d\u5143\u5b66\u4e60\u7b56\u7565\u96be\u4ee5\u5145\u5206\u5229\u7528\u7b97\u6cd5\u4e92\u8865\u6027\uff0c\u6539\u8fdb\u7684\u7ec4\u5408\u7b56\u7565\u53ef\u80fd\u5c06\u96c6\u6210\u6536\u76ca\u63d0\u53472-3\u500d\u3002"}}
{"id": "2512.06438", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06438", "abs": "https://arxiv.org/abs/2512.06438", "authors": ["Ramazan Fazylov", "Sergey Zagoruyko", "Aleksandr Parkin", "Stamatis Lefkimmiatis", "Ivan Laptev"], "title": "AGORA: Adversarial Generation Of Real-time Animatable 3D Gaussian Head Avatars", "comment": null, "summary": "The generation of high-fidelity, animatable 3D human avatars remains a core challenge in computer graphics and vision, with applications in VR, telepresence, and entertainment. Existing approaches based on implicit representations like NeRFs suffer from slow rendering and dynamic inconsistencies, while 3D Gaussian Splatting (3DGS) methods are typically limited to static head generation, lacking dynamic control. We bridge this gap by introducing AGORA, a novel framework that extends 3DGS within a generative adversarial network to produce animatable avatars. Our key contribution is a lightweight, FLAME-conditioned deformation branch that predicts per-Gaussian residuals, enabling identity-preserving, fine-grained expression control while allowing real-time inference. Expression fidelity is enforced via a dual-discriminator training scheme leveraging synthetic renderings of the parametric mesh. AGORA generates avatars that are not only visually realistic but also precisely controllable. Quantitatively, we outperform state-of-the-art NeRF-based methods on expression accuracy while rendering at 250+ FPS on a single GPU, and, notably, at $\\sim$9 FPS under CPU-only inference - representing, to our knowledge, the first demonstration of practical CPU-only animatable 3DGS avatar synthesis. This work represents a significant step toward practical, high-performance digital humans. Project website: https://ramazan793.github.io/AGORA/", "AI": {"tldr": "AGORA\u662f\u4e00\u4e2a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u548cGAN\u7684\u52a8\u753b\u53163D\u4eba\u4f53\u5316\u8eab\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53d8\u5f62\u5206\u652f\u5b9e\u73b0\u8eab\u4efd\u4fdd\u6301\u7684\u7cbe\u7ec6\u8868\u60c5\u63a7\u5236\uff0c\u652f\u6301\u5b9e\u65f6CPU\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eNeRF\u7684\u65b9\u6cd5\u6e32\u67d3\u901f\u5ea6\u6162\u4e14\u5b58\u5728\u52a8\u6001\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u800c3DGS\u65b9\u6cd5\u901a\u5e38\u4ec5\u9650\u4e8e\u9759\u6001\u5934\u90e8\u751f\u6210\uff0c\u7f3a\u4e4f\u52a8\u6001\u63a7\u5236\u80fd\u529b\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u9ad8\u8d28\u91cf\u53c8\u80fd\u5b9e\u73b0\u5b9e\u65f6\u52a8\u753b\u63a7\u5236\u7684\u65b9\u6cd5\u3002", "method": "\u5c063D\u9ad8\u65af\u6cfc\u6e85\u6269\u5c55\u5230\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u4e2d\uff0c\u5f15\u5165FLAME\u6761\u4ef6\u5316\u7684\u8f7b\u91cf\u7ea7\u53d8\u5f62\u5206\u652f\u9884\u6d4b\u6bcf\u4e2a\u9ad8\u65af\u7684\u6b8b\u5dee\uff0c\u91c7\u7528\u53cc\u5224\u522b\u5668\u8bad\u7ec3\u65b9\u6848\u5229\u7528\u53c2\u6570\u5316\u7f51\u683c\u7684\u5408\u6210\u6e32\u67d3\u6765\u4fdd\u8bc1\u8868\u60c5\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u8868\u60c5\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684NeRF\u65b9\u6cd5\uff0c\u5728\u5355GPU\u4e0a\u5b9e\u73b0250+ FPS\u6e32\u67d3\uff0c\u5728\u7eafCPU\u63a8\u7406\u4e0b\u8fbe\u5230\u7ea69 FPS\uff0c\u662f\u9996\u4e2a\u5b9e\u7528\u7684\u7eafCPU\u52a8\u753b\u53163DGS\u5316\u8eab\u5408\u6210\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5411\u5b9e\u7528\u9ad8\u6027\u80fd\u6570\u5b57\u4eba\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\uff0c\u5b9e\u73b0\u4e86\u89c6\u89c9\u771f\u5b9e\u4e14\u7cbe\u786e\u53ef\u63a7\u7684\u5316\u8eab\u751f\u6210\u3002"}}
{"id": "2512.07697", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07697", "abs": "https://arxiv.org/abs/2512.07697", "authors": ["Aileen Liao", "Dong-Ki Kim", "Max Olan Smith", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks", "comment": null, "summary": "As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5ef6\u8fdf\u611f\u77e5\u6269\u6563\u7b56\u7565\uff08DA-DP\uff09\uff0c\u901a\u8fc7\u663e\u5f0f\u5730\u5c06\u63a8\u7406\u5ef6\u8fdf\u7eb3\u5165\u7b56\u7565\u5b66\u4e60\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u52a8\u4f5c\u6267\u884c\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u611f\u77e5\u548c\u9009\u62e9\u52a8\u4f5c\u65f6\uff0c\u4e16\u754c\u6301\u7eed\u53d8\u5316\uff0c\u5bfc\u81f4\u89c2\u6d4b\u72b6\u6001\u4e0e\u6267\u884c\u72b6\u6001\u4e4b\u95f4\u5b58\u5728\u6570\u5341\u5230\u6570\u767e\u6beb\u79d2\u7684\u5ef6\u8fdf\uff0c\u8fd9\u5f71\u54cd\u4e86\u7b56\u7565\u7684\u6027\u80fd\u3002", "method": "DA-DP\u5c06\u96f6\u5ef6\u8fdf\u8f68\u8ff9\u6821\u6b63\u4e3a\u5ef6\u8fdf\u8865\u507f\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u5ef6\u8fdf\u6761\u4ef6\u589e\u5f3a\u7b56\u7565\uff0c\u8be5\u6846\u67b6\u4e0e\u67b6\u6784\u65e0\u5173\uff0c\u53ef\u63a8\u5e7f\u5230\u6269\u6563\u7b56\u7565\u4e4b\u5916\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u3001\u673a\u5668\u4eba\u548c\u5ef6\u8fdf\u6761\u4ef6\u4e0b\uff0cDA-DP\u7684\u6210\u529f\u7387\u6bd4\u65e0\u5ef6\u8fdf\u611f\u77e5\u65b9\u6cd5\u66f4\u7a33\u5065\u3002", "conclusion": "DA-DP\u4e3a\u5ef6\u8fdf\u611f\u77e5\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u6a21\u5f0f\uff0c\u5e76\u9f13\u52b1\u8bc4\u4f30\u534f\u8bae\u62a5\u544a\u6027\u80fd\u968f\u6d4b\u91cf\u5ef6\u8fdf\u7684\u53d8\u5316\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4efb\u52a1\u96be\u5ea6\u3002"}}
{"id": "2512.06678", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06678", "abs": "https://arxiv.org/abs/2512.06678", "authors": ["Shrihari Sridharan", "Deepak Ravikumar", "Anand Raghunathan", "Kaushik Roy"], "title": "GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning", "comment": null, "summary": "Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.", "AI": {"tldr": "GradientSpace\u662f\u4e00\u4e2a\u5728\u5b8c\u6574\u68af\u5ea6\u7a7a\u95f4\u4e2d\u76f4\u63a5\u805a\u7c7b\u6837\u672c\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebfSVD\u7b97\u6cd5\u8bc6\u522b\u6f5c\u5728\u6280\u80fd\uff0c\u8bad\u7ec3\u4e13\u7528LoRA\u4e13\u5bb6\u548c\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\uff0c\u4ee5\u89e3\u51b3\u6307\u4ee4\u8c03\u4f18\u4e2d\u68af\u5ea6\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u901a\u5e38\u7531\u591a\u6837\u5316\u4fe1\u606f\u6df7\u5408\u7ec4\u6210\uff0c\u5bfc\u81f4\u68af\u5ea6\u5e72\u6270\u95ee\u9898\uff0c\u5373\u51b2\u7a81\u7684\u68af\u5ea6\u5c06\u6a21\u578b\u62c9\u5411\u76f8\u53cd\u65b9\u5411\uff0c\u964d\u4f4e\u6027\u80fd\u3002\u73b0\u6709\u57fa\u4e8e\u8bed\u4e49\u6216\u5d4c\u5165\u76f8\u4f3c\u6027\u7684\u805a\u7c7b\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u6570\u636e\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u53c2\u6570\uff0c\u800c\u76f4\u63a5\u805a\u7c7b\u68af\u5ea6\u7684\u65b9\u6cd5\u56e0\u968f\u673a\u6295\u5f71\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\uff0c\u4e14\u4f9d\u8d56\u4e13\u5bb6\u96c6\u5408\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "method": "\u63d0\u51faGradientSpace\u6846\u67b6\uff0c\u5728\u5b8c\u6574\u68af\u5ea6\u7a7a\u95f4\u4e2d\u76f4\u63a5\u805a\u7c7b\u6837\u672c\uff1b\u5f15\u5165\u57fa\u4e8e\u5728\u7ebfSVD\u7684\u7b97\u6cd5\uff0c\u64cd\u4f5cLoRA\u68af\u5ea6\u4ee5\u8bc6\u522b\u6f5c\u5728\u6280\u80fd\uff0c\u907f\u514d\u5b58\u50a8\u6240\u6709\u6837\u672c\u68af\u5ea6\u7684\u4e0d\u53ef\u884c\u6210\u672c\uff1b\u4e3a\u6bcf\u4e2a\u805a\u7c7b\u8bad\u7ec3\u4e13\u7528LoRA\u4e13\u5bb6\uff0c\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5728\u63a8\u7406\u65f6\u9009\u62e9\u6700\u4f73\u4e13\u5bb6\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u3001\u91d1\u878d\u548c\u521b\u610f\u5199\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGradientSpace\u5bfc\u81f4\u4e00\u81f4\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u6301\u7eed\u7684\u51c6\u786e\u6027\u63d0\u5347\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u65b9\u6cd5\u548c\u5fae\u8c03\u6280\u672f\uff1b\u8def\u7531\u5230\u5355\u4e2a\u9002\u5f53\u4e13\u5bb6\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u4e13\u5bb6\u96c6\u5408\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "GradientSpace\u901a\u8fc7\u76f4\u63a5\u5728\u5b8c\u6574\u68af\u5ea6\u7a7a\u95f4\u4e2d\u805a\u7c7b\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u68af\u5ea6\u5e72\u6270\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4e13\u5bb6\u4e13\u4e1a\u5316\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u3002"}}
{"id": "2512.06447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06447", "abs": "https://arxiv.org/abs/2512.06447", "authors": ["Jiuyi Chen", "Mingkui Tan", "Haifeng Lu", "Qiuna Xu", "Zhihua Wang", "Runhao Zeng", "Xiping Hu"], "title": "Towards Stable Cross-Domain Depression Recognition under Missing Modalities", "comment": null, "summary": "Depression poses serious public health risks, including suicide, underscoring the urgency of timely and scalable screening. Multimodal automatic depression detection (ADD) offers a promising solution; however, widely studied audio- and video-based ADD methods lack a unified, generalizable framework for diverse depression recognition scenarios and show limited stability to missing modalities, which are common in real-world data. In this work, we propose a unified framework for Stable Cross-Domain Depression Recognition based on Multimodal Large Language Model (SCD-MLLM). The framework supports the integration and processing of heterogeneous depression-related data collected from varied sources while maintaining stability in the presence of incomplete modality inputs. Specifically, SCD-MLLM introduces two key components: (i) Multi-Source Data Input Adapter (MDIA), which employs masking mechanism and task-specific prompts to transform heterogeneous depression-related inputs into uniform token sequences, addressing inconsistency across diverse data sources; (ii) Modality-Aware Adaptive Fusion Module (MAFM), which adaptively integrates audio and visual features via a shared projection mechanism, enhancing resilience under missing modality conditions. e conduct comprehensive experiments under multi-dataset joint training settings on five publicly available and heterogeneous depression datasets from diverse scenarios: CMDC, AVEC2014, DAIC-WOZ, DVlog, and EATD. Across both complete and partial modality settings, SCD-MLLM outperforms state-of-the-art (SOTA) models as well as leading commercial LLMs (Gemini and GPT), demonstrating superior cross-domain generalization, enhanced ability to capture multimodal cues of depression, and strong stability to missing modality cases in real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7a33\u5b9a\u8de8\u57df\u6291\u90c1\u75c7\u8bc6\u522b\u6846\u67b6SCD-MLLM\uff0c\u901a\u8fc7\u591a\u6e90\u6570\u636e\u8f93\u5165\u9002\u914d\u5668\u548c\u6a21\u6001\u611f\u77e5\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff0c\u6709\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u5e76\u589e\u5f3a\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u6291\u90c1\u75c7\u68c0\u6d4b\u5177\u6709\u91cd\u8981\u516c\u5171\u536b\u751f\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u7f3a\u5931\u6a21\u6001\u6570\u636e\u7a33\u5b9a\u6027\u4e0d\u8db3\u3002", "method": "SCD-MLLM\u6846\u67b6\u5305\u542b\uff1a1\uff09\u591a\u6e90\u6570\u636e\u8f93\u5165\u9002\u914d\u5668\uff08MDIA\uff09\uff0c\u4f7f\u7528\u63a9\u7801\u673a\u5236\u548c\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u5c06\u5f02\u6784\u8f93\u5165\u8f6c\u6362\u4e3a\u7edf\u4e00token\u5e8f\u5217\uff1b2\uff09\u6a21\u6001\u611f\u77e5\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff08MAFM\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u6295\u5f71\u673a\u5236\u81ea\u9002\u5e94\u6574\u5408\u89c6\u542c\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u6291\u90c1\u75c7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSCD-MLLM\u5728\u5b8c\u6574\u548c\u90e8\u5206\u6a21\u6001\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8eSOTA\u6a21\u578b\u548c\u4e3b\u6d41\u5546\u4e1aLLM\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u548c\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "SCD-MLLM\u4e3a\u591a\u6a21\u6001\u6291\u90c1\u75c7\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u7a33\u5b9a\u7684\u6846\u67b6\uff0c\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.07765", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07765", "abs": "https://arxiv.org/abs/2512.07765", "authors": ["Gustavo A. Cardona", "Shubham S. Kumbhar", "Panagiotis Artemiadis"], "title": "Toward Seamless Physical Human-Humanoid Interaction: Insights from Control, Intent, and Modeling with a Vision for What Comes Next", "comment": "60 pages, 5 figures, 3 tables", "summary": "Physical Human-Humanoid Interaction (pHHI) is a rapidly advancing field with significant implications for deploying robots in unstructured, human-centric environments. In this review, we examine the current state of the art in pHHI through three core pillars: (i) humanoid modeling and control, (ii) human intent estimation, and (iii) computational human models. For each pillar, we survey representative approaches, identify open challenges, and analyze current limitations that hinder robust, scalable, and adaptive interaction. These include the need for whole-body control strategies capable of handling uncertain human dynamics, real-time intent inference under limited sensing, and modeling techniques that account for variability in human physical states. Although significant progress has been made within each domain, integration across pillars remains limited. We propose pathways for unifying methods across these areas to enable cohesive interaction frameworks. This structure enables us not only to map the current landscape but also to propose concrete directions for future research that aim to bridge these domains. Additionally, we introduce a unified taxonomy of interaction types based on modality, distinguishing between direct interactions (e.g., physical contact) and indirect interactions (e.g., object-mediated), and on the level of robot engagement, ranging from assistance to cooperation and collaboration. For each category in this taxonomy, we provide the three core pillars that highlight opportunities for cross-pillar unification. Our goal is to suggest avenues to advance robust, safe, and intuitive physical interaction, providing a roadmap for future research that will allow humanoid systems to effectively understand, anticipate, and collaborate with human partners in diverse real-world settings.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u4ece\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff08\u4eba\u5f62\u673a\u5668\u4eba\u5efa\u6a21\u4e0e\u63a7\u5236\u3001\u4eba\u7c7b\u610f\u56fe\u4f30\u8ba1\u3001\u8ba1\u7b97\u4eba\u7c7b\u6a21\u578b\uff09\u5206\u6790\u4e86\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4ea4\u4e92\u6a21\u5f0f\u548c\u673a\u5668\u4eba\u53c2\u4e0e\u7a0b\u5ea6\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u8de8\u9886\u57df\u6574\u5408\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a8\u52a8\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u73af\u5883\u4e2d\u7684\u90e8\u7f72\uff0c\u9700\u8981\u89e3\u51b3\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u5305\u62ec\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u3001\u5b9e\u65f6\u610f\u56fe\u63a8\u65ad\u548c\u4eba\u7c7b\u72b6\u6001\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u73b0\u6709\u6587\u732e\uff0c\u5206\u6790\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\u3001\u5f00\u653e\u6311\u6218\u548c\u5f53\u524d\u5c40\u9650\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u6a21\u6001\u548c\u673a\u5668\u4eba\u53c2\u4e0e\u7a0b\u5ea6\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\u3002", "result": "\u8bc6\u522b\u4e86\u5404\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u548c\u6574\u5408\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u8de8\u652f\u67f1\u7edf\u4e00\u7684\u65b9\u6cd5\u8def\u5f84\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u4ea4\u4e92\u5206\u7c7b\u4f53\u7cfb\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5177\u4f53\u65b9\u5411\u3002", "conclusion": "\u9700\u8981\u8de8\u9886\u57df\u6574\u5408\u6765\u6784\u5efa\u9c81\u68d2\u3001\u5b89\u5168\u3001\u76f4\u89c2\u7684\u7269\u7406\u4ea4\u4e92\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u7cfb\u7edf\u80fd\u591f\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u6709\u6548\u7406\u89e3\u3001\u9884\u6d4b\u548c\u534f\u4f5c\u4eba\u7c7b\u4f19\u4f34\u3002"}}
{"id": "2512.06692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06692", "abs": "https://arxiv.org/abs/2512.06692", "authors": ["Shiye Lei", "Zhihao Cheng", "Dacheng Tao"], "title": "State Diversity Matters in Offline Behavior Distillation", "comment": "12 pages, 5 figures, 5 tables", "summary": "Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.", "AI": {"tldr": "\u672c\u6587\u63ed\u793a\u4e86\u79bb\u7ebf\u884c\u4e3a\u84b8\u998f(OBD)\u4e2d\u539f\u59cb\u6570\u636e\u96c6\u4e0e\u5408\u6210\u6570\u636e\u96c6\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u53d1\u73b0\u9ad8\u72b6\u6001\u591a\u6837\u6027\u6bd4\u9ad8\u72b6\u6001\u8d28\u91cf\u5728\u8bad\u7ec3\u635f\u5931\u8f83\u5927\u65f6\u66f4\u91cd\u8981\uff0c\u5e76\u63d0\u51fa\u4e86\u72b6\u6001\u5bc6\u5ea6\u52a0\u6743(SDW)\u7b97\u6cd5\u6765\u589e\u5f3a\u72b6\u6001\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edfOBD\u65b9\u6cd5\u5047\u8bbe\u9ad8\u8d28\u91cf\u539f\u59cb\u6570\u636e\u96c6\u80fd\u4ea7\u751f\u4f18\u8d28\u5408\u6210\u6570\u636e\u96c6\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u5bf9\u5e94\u5173\u7cfb\u5e76\u4e0d\u6210\u7acb\uff0c\u5b58\u5728\u6570\u636e\u96c6\u8d28\u91cf\u4e0e\u6027\u80fd\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5c06\u72b6\u6001\u8d28\u91cf\u548c\u591a\u6837\u6027\u5206\u522b\u5173\u8054\u5230\u5173\u952e\u8bef\u5dee\u548c\u5468\u56f4\u8bef\u5dee\u7684\u51cf\u5c11\uff0c\u63d0\u51fa\u72b6\u6001\u5bc6\u5ea6\u52a0\u6743(SDW)\u7b97\u6cd5\uff0c\u901a\u8fc7\u72b6\u6001\u5bc6\u5ea6\u7684\u5012\u6570\u52a0\u6743\u84b8\u998f\u76ee\u6807\u6765\u589e\u5f3a\u72b6\u6001\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u500bD4RL\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u539f\u59cb\u6570\u636e\u96c6\u72b6\u6001\u591a\u6837\u6027\u6709\u9650\u65f6\uff0cSDW\u80fd\u663e\u8457\u63d0\u5347OBD\u6027\u80fd\u3002", "conclusion": "\u72b6\u6001\u591a\u6837\u6027\u5728OBD\u4e2d\u6bd4\u72b6\u6001\u8d28\u91cf\u66f4\u4e3a\u5173\u952e\uff0c\u7279\u522b\u662f\u5728\u8bad\u7ec3\u635f\u5931\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\uff0cSDW\u7b97\u6cd5\u901a\u8fc7\u5f3a\u8c03\u72b6\u6001\u591a\u6837\u6027\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u96c6\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002"}}
{"id": "2512.06485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06485", "abs": "https://arxiv.org/abs/2512.06485", "authors": ["Kush Revankar", "Shreyas Deshpande", "Araham Sayeed", "Ansh Tandale", "Sarika Bobde"], "title": "Sanvaad: A Multimodal Accessibility Framework for ISL Recognition and Voice-Based Interaction", "comment": null, "summary": "Communication between deaf users, visually im paired users, and the general hearing population often relies on tools that support only one direction of interaction. To address this limitation, this work presents Sanvaad, a lightweight multimodal accessibility framework designed to support real time, two-way communication. For deaf users, Sanvaad includes an ISL recognition module built on MediaPipe landmarks. MediaPipe is chosen primarily for its efficiency and low computational load, enabling the system to run smoothly on edge devices without requiring dedicated hardware. Spoken input from a phone can also be translated into sign representations through a voice-to-sign component that maps detected speech to predefined phrases and produces corresponding GIFs or alphabet-based visualizations. For visually impaired users, the framework provides a screen free voice interface that integrates multilingual speech recognition, text summarization, and text-to-speech generation. These components work together through a Streamlit-based interface, making the system usable on both desktop and mobile environments. Overall, Sanvaad aims to offer a practical and accessible pathway for inclusive communication by combining lightweight computer vision and speech processing tools within a unified framework.", "AI": {"tldr": "Sanvaad\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u65e0\u969c\u788d\u6846\u67b6\uff0c\u652f\u6301\u804b\u54d1\u7528\u6237\u3001\u89c6\u969c\u7528\u6237\u548c\u666e\u901a\u542c\u529b\u4eba\u7fa4\u4e4b\u95f4\u7684\u5b9e\u65f6\u53cc\u5411\u4ea4\u6d41\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u4ec5\u652f\u6301\u5355\u5411\u4ea4\u4e92\u7684\u5c40\u9650\u6027\uff0c\u4fc3\u8fdb\u4e0d\u540c\u80fd\u529b\u4eba\u7fa4\u4e4b\u95f4\u7684\u5305\u5bb9\u6027\u6c9f\u901a\u3002", "method": "\u4f7f\u7528MediaPipe\u5730\u6807\u70b9\u6784\u5efaISL\u8bc6\u522b\u6a21\u5757\uff0c\u96c6\u6210\u8bed\u97f3\u8f6c\u624b\u8bed\u7ec4\u4ef6\u548c\u9762\u5411\u89c6\u969c\u7528\u6237\u7684\u65e0\u5c4f\u5e55\u8bed\u97f3\u754c\u9762\uff0c\u901a\u8fc7Streamlit\u5b9e\u73b0\u7edf\u4e00\u754c\u9762\u3002", "result": "\u5f00\u53d1\u51fa\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6d41\u7545\u8fd0\u884c\u7684\u8f7b\u91cf\u7ea7\u7cfb\u7edf\uff0c\u652f\u6301\u5b9e\u65f6\u53cc\u5411\u6c9f\u901a\u3002", "conclusion": "Sanvaad\u901a\u8fc7\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u8bed\u97f3\u5904\u7406\u5de5\u5177\uff0c\u4e3a\u5305\u5bb9\u6027\u6c9f\u901a\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u65e0\u969c\u788d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07775", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07775", "abs": "https://arxiv.org/abs/2512.07775", "authors": ["David Thorne", "Nathan Chan", "Christa S. Robison", "Philip R. Osteen", "Brett T. Lopez"], "title": "OptMap: Geometric Map Distillation via Submodular Maximization", "comment": null, "summary": "Autonomous robots rely on geometric maps to inform a diverse set of perception and decision-making algorithms. As autonomy requires reasoning and planning on multiple scales of the environment, each algorithm may require a different map for optimal performance. Light Detection And Ranging (LiDAR) sensors generate an abundance of geometric data to satisfy these diverse requirements, but selecting informative, size-constrained maps is computationally challenging as it requires solving an NP-hard combinatorial optimization. In this work we present OptMap: a geometric map distillation algorithm which achieves real-time, application-specific map generation via multiple theoretical and algorithmic innovations. A central feature is the maximization of set functions that exhibit diminishing returns, i.e., submodularity, using polynomial-time algorithms with provably near-optimal solutions. We formulate a novel submodular reward function which quantifies informativeness, reduces input set sizes, and minimizes bias in sequentially collected datasets. Further, we propose a dynamically reordered streaming submodular algorithm which improves empirical solution quality and addresses input order bias via an online approximation of the value of all scans. Testing was conducted on open-source and custom datasets with an emphasis on long-duration mapping sessions, highlighting OptMap's minimal computation requirements. Open-source ROS1 and ROS2 packages are available and can be used alongside any LiDAR SLAM algorithm.", "AI": {"tldr": "OptMap\u662f\u4e00\u4e2a\u5b9e\u65f6\u51e0\u4f55\u5730\u56fe\u84b8\u998f\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b50\u6a21\u4f18\u5316\u6280\u672f\u4eceLiDAR\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\u91cf\u6700\u5927\u3001\u5c3a\u5bf8\u53d7\u9650\u7684\u4e13\u7528\u5730\u56fe\uff0c\u89e3\u51b3\u4e86NP\u96be\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u673a\u5668\u4eba\u9700\u8981\u591a\u5c3a\u5ea6\u7684\u51e0\u4f55\u5730\u56fe\u6765\u652f\u6301\u4e0d\u540c\u7b97\u6cd5\uff0c\u4f46LiDAR\u6570\u636e\u91cf\u5927\u4e14\u9009\u62e9\u6700\u4f18\u5730\u56fe\u662fNP\u96be\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5b9e\u65f6\u5730\u56fe\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u7684\u5b50\u6a21\u5956\u52b1\u51fd\u6570\u91cf\u5316\u4fe1\u606f\u91cf\uff0c\u51cf\u5c11\u8f93\u5165\u96c6\u5927\u5c0f\u5e76\u6700\u5c0f\u5316\u5e8f\u5217\u6570\u636e\u504f\u5dee\uff1b\u91c7\u7528\u52a8\u6001\u91cd\u6392\u5e8f\u6d41\u5f0f\u5b50\u6a21\u7b97\u6cd5\u6539\u8fdb\u89e3\u8d28\u91cf\u5e76\u5904\u7406\u8f93\u5165\u987a\u5e8f\u504f\u5dee\u3002", "result": "\u5728\u5f00\u6e90\u548c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u7279\u522b\u5173\u6ce8\u957f\u65f6\u95f4\u5efa\u56fe\u573a\u666f\uff0c\u663e\u793aOptMap\u5177\u6709\u6781\u4f4e\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5e76\u63d0\u4f9b\u4e86ROS1\u548cROS2\u5f00\u6e90\u5305\u3002", "conclusion": "OptMap\u901a\u8fc7\u7406\u8bba\u521b\u65b0\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u5e94\u7528\u7279\u5b9a\u7684\u5730\u56fe\u751f\u6210\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5730\u56fe\u84b8\u998f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06695", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.06695", "abs": "https://arxiv.org/abs/2512.06695", "authors": ["Haipeng Cao", "Kaining Zhang", "Dacheng Tao", "Zhaofeng Su"], "title": "Mitigating Barren plateaus in quantum denoising diffusion probabilistic models", "comment": "22 pages, 9 figures", "summary": "Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.", "AI": {"tldr": "\u91cf\u5b50\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08QuDDPM\uff09\u5b58\u5728\u8d2b\u7620\u9ad8\u539f\u95ee\u9898\uff0c\u672c\u6587\u901a\u8fc7\u6539\u8fdb\u8f93\u5165\u5206\u5e03\u89e3\u51b3\u4e86\u8be5\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u8bad\u7ec3\u6027\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "QuDDPM\u867d\u7136\u80fd\u9ad8\u6548\u5b66\u4e60\u91cf\u5b50\u6570\u636e\uff0c\u4f46\u7531\u4e8e\u4f7f\u75282-design\u72b6\u6001\u4f5c\u4e3a\u53bb\u566a\u8fc7\u7a0b\u8f93\u5165\uff0c\u51fa\u73b0\u4e86\u8d2b\u7620\u9ad8\u539f\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u6539\u8fdb\u7684QuDDPM\uff0c\u4f7f\u7528\u4e0eHaar\u5206\u5e03\u4fdd\u6301\u4e00\u5b9a\u8ddd\u79bb\u7684\u5206\u5e03\u4f5c\u4e3a\u8f93\u5165\uff0c\u786e\u4fdd\u66f4\u597d\u7684\u53ef\u8bad\u7ec3\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u8d2b\u7620\u9ad8\u539f\u95ee\u9898\uff0c\u751f\u6210\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u6837\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u91cf\u5b50\u751f\u6210\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.06504", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06504", "abs": "https://arxiv.org/abs/2512.06504", "authors": ["Andrii Lysyi", "Anatoliy Sachenko", "Pavlo Radiuk", "Mykola Lysyi", "Oleksandr Melnychenko", "Diana Zahorodnia"], "title": "Method of UAV Inspection of Photovoltaic Modules Using Thermal and RGB Data Fusion", "comment": null, "summary": "The subject of this research is the development of an intelligent, integrated framework for the automated inspection of photovoltaic (PV) infrastructure that addresses the critical shortcomings of conventional methods, including thermal palette bias, data redundancy, and high communication bandwidth requirements. The goal of this study is to design, develop, and validate a comprehensive, multi-modal system that fully automates the monitoring workflow, from data acquisition to the generation of actionable, geo-located maintenance alerts, thereby enhancing plant safety and operational efficiency. The methods employed involve a synergistic architecture that begins with a palette-invariant thermal embedding, learned by enforcing representational consistency, which is fused with a contrast-normalized RGB stream via a gated mechanism. This is supplemented by a closed-loop, adaptive re-acquisition controller that uses Rodrigues-based updates for targeted confirmation of ambiguous anomalies and a geospatial deduplication module that clusters redundant alerts using DBSCAN over the haversine distance. In conclusion, this study establishes a powerful new paradigm for proactive PV inspection, with the proposed system achieving a mean Average Precision (mAP@0.5) of 0.903 on the public PVF-10 benchmark, a significant 12-15% improvement over single-modality baselines. Field validation confirmed the system's readiness, achieving 96% recall, while the de-duplication process reduced duplicate-induced false positives by 15-20%, and relevance-only telemetry cut airborne data transmission by 60-70%.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u667a\u80fd\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5149\u4f0f\u57fa\u7840\u8bbe\u65bd\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u81ea\u9002\u5e94\u91cd\u91c7\u96c6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5149\u4f0f\u68c0\u6d4b\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u70ed\u8c03\u8272\u677f\u504f\u5dee\u3001\u6570\u636e\u5197\u4f59\u548c\u9ad8\u901a\u4fe1\u5e26\u5bbd\u9700\u6c42\u7b49\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4ece\u6570\u636e\u91c7\u96c6\u5230\u7ef4\u62a4\u8b66\u62a5\u751f\u6210\u7684\u5b8c\u5168\u81ea\u52a8\u5316\u76d1\u63a7\u5de5\u4f5c\u6d41\u3002", "method": "\u91c7\u7528\u534f\u540c\u67b6\u6784\uff1a\u9996\u5148\u901a\u8fc7\u5f3a\u5236\u8868\u793a\u4e00\u81f4\u6027\u5b66\u4e60\u8c03\u8272\u677f\u4e0d\u53d8\u7684\u70ed\u5d4c\u5165\uff0c\u7136\u540e\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u4e0e\u5bf9\u6bd4\u5f52\u4e00\u5316\u7684RGB\u6d41\u878d\u5408\uff1b\u7ed3\u5408\u57fa\u4e8e\u7f57\u5fb7\u91cc\u683c\u65af\u66f4\u65b0\u7684\u81ea\u9002\u5e94\u91cd\u91c7\u96c6\u63a7\u5236\u5668\u8fdb\u884c\u5f02\u5e38\u786e\u8ba4\uff0c\u4ee5\u53ca\u4f7f\u7528DBSCAN\u7b97\u6cd5\u5bf9\u5730\u7406\u7a7a\u95f4\u91cd\u590d\u8b66\u62a5\u8fdb\u884c\u53bb\u91cd\u3002", "result": "\u5728\u516c\u5f00PVF-10\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52300.903\u7684mAP@0.5\uff0c\u6bd4\u5355\u6a21\u6001\u57fa\u7ebf\u63d0\u534712-15%\uff1b\u73b0\u573a\u9a8c\u8bc1\u663e\u793a96%\u7684\u53ec\u56de\u7387\uff0c\u53bb\u91cd\u8fc7\u7a0b\u51cf\u5c1115-20%\u7684\u91cd\u590d\u8bef\u62a5\uff0c\u76f8\u5173\u6027\u9065\u6d4b\u4f7f\u7a7a\u4e2d\u6570\u636e\u4f20\u8f93\u51cf\u5c1160-70%\u3002", "conclusion": "\u672c\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u4e3b\u52a8\u5149\u4f0f\u68c0\u6d4b\u65b0\u8303\u5f0f\uff0c\u7cfb\u7edf\u9a8c\u8bc1\u8868\u660e\u5176\u5177\u5907\u5b9e\u9645\u5e94\u7528\u51c6\u5907\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u4f0f\u7535\u7ad9\u7684\u5b89\u5168\u6027\u548c\u8fd0\u8425\u6548\u7387\u3002"}}
{"id": "2512.07813", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07813", "abs": "https://arxiv.org/abs/2512.07813", "authors": ["Hari Prakash Thanabalan", "Lars Bengtsson", "Ugo Lafont", "Giovanni Volpe"], "title": "Inchworm-Inspired Soft Robot with Groove-Guided Locomotion", "comment": null, "summary": "Soft robots require directional control to navigate complex terrains. However, achieving such control often requires multiple actuators, which increases mechanical complexity, complicates control systems, and raises energy consumption. Here, we introduce an inchworm-inspired soft robot whose locomotion direction is controlled passively by patterned substrates. The robot employs a single rolled dielectric elastomer actuator, while groove patterns on a 3D-printed substrate guide its alignment and trajectory. Through systematic experiments, we demonstrate that varying groove angles enables precise control of locomotion direction without the need for complex actuation strategies. This groove-guided approach reduces energy consumption, simplifies robot design, and expands the applicability of bio-inspired soft robots in fields such as search and rescue, pipe inspection, and planetary exploration.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53d7\u5c3a\u8816\u542f\u53d1\u7684\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u56fe\u6848\u5316\u57fa\u5e95\u88ab\u52a8\u63a7\u5236\u8fd0\u52a8\u65b9\u5411\uff0c\u4ec5\u4f7f\u7528\u5355\u4e2a\u5377\u66f2\u4ecb\u7535\u5f39\u6027\u4f53\u81f4\u52a8\u5668\uff0c\u7b80\u5316\u4e86\u673a\u5668\u4eba\u8bbe\u8ba1\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4f53\u673a\u5668\u4eba\u9700\u8981\u591a\u4e2a\u81f4\u52a8\u5668\u6765\u5b9e\u73b0\u65b9\u5411\u63a7\u5236\uff0c\u8fd9\u589e\u52a0\u4e86\u673a\u68b0\u590d\u6742\u6027\u3001\u63a7\u5236\u96be\u5ea6\u548c\u80fd\u8017\u3002", "method": "\u5229\u75283D\u6253\u5370\u57fa\u5e95\u4e0a\u7684\u6c9f\u69fd\u56fe\u6848\u5f15\u5bfc\u673a\u5668\u4eba\u5bf9\u9f50\u548c\u8f68\u8ff9\uff0c\u901a\u8fc7\u6539\u53d8\u6c9f\u69fd\u89d2\u5ea6\u5b9e\u73b0\u7cbe\u786e\u7684\u8fd0\u52a8\u65b9\u5411\u63a7\u5236\uff0c\u65e0\u9700\u590d\u6742\u81f4\u52a8\u7b56\u7565\u3002", "result": "\u7cfb\u7edf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u8fd0\u52a8\u65b9\u5411\uff0c\u540c\u65f6\u964d\u4f4e\u80fd\u8017\u5e76\u7b80\u5316\u673a\u5668\u4eba\u8bbe\u8ba1\u3002", "conclusion": "\u8fd9\u79cd\u6c9f\u69fd\u5f15\u5bfc\u65b9\u6cd5\u6269\u5c55\u4e86\u4eff\u751f\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u641c\u6551\u3001\u7ba1\u9053\u68c0\u6d4b\u548c\u884c\u661f\u63a2\u6d4b\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.06702", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06702", "abs": "https://arxiv.org/abs/2512.06702", "authors": ["Xiangjun Meng", "Zhongjian Wang"], "title": "Pathway to $O(\\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models", "comment": null, "summary": "We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.\n  These assumptions are valid in the flow-based generative model associated with the F\u00f6llmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u5206\u6790\u5de5\u5177\u6765\u4f30\u8ba1\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u5728Wasserstein\u5ea6\u91cf\u4e0b\u7684\u8bef\u5dee\uff0c\u5e76\u5efa\u7acb\u4e86\u6700\u4f18\u91c7\u6837\u8fed\u4ee3\u590d\u6742\u5ea6\u4e3aO(\u221ad)\u7684\u7ef4\u5ea6\u76f8\u5173\u754c\u9650\u3002\u8bef\u5dee\u7531\u4e24\u90e8\u5206\u63a7\u5236\uff1a\u53cd\u5411\u6d41\u63a8\u524d\u6620\u5c04\u7684Lipschitz\u6027\uff08\u4e0e\u7ef4\u5ea6\u65e0\u5173\uff09\u548c\u5c40\u90e8\u79bb\u6563\u5316\u8bef\u5dee\uff08\u4e0e\u221ad\u6210\u6b63\u6bd4\uff09\u3002", "motivation": "\u7814\u7a76\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u7684\u8bef\u5dee\u4f30\u8ba1\u548c\u91c7\u6837\u590d\u6742\u5ea6\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u4f7f\u7528Wasserstein\u5ea6\u91cf\u5206\u6790\u6d41\u6a21\u578b\u7684\u8bef\u5dee\uff0c\u5c06\u8bef\u5dee\u5206\u89e3\u4e3a\u53cd\u5411\u6d41\u63a8\u524d\u6620\u5c04\u7684Lipschitz\u6027\u548c\u5c40\u90e8\u79bb\u6563\u5316\u8bef\u5dee\u4e24\u90e8\u5206\u3002\u5728F\u00f6llmer\u8fc7\u7a0b\u548c1-\u6574\u6d41\u6d41\u6a21\u578b\u4e0b\u9a8c\u8bc1\u5047\u8bbe\u3002", "result": "\u8bc1\u660e\u4e86\u91c7\u6837\u8fed\u4ee3\u590d\u6742\u5ea6\u4e0e\u534f\u65b9\u5dee\u7b97\u5b50\u8ff9\u7684\u5e73\u65b9\u6839\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u5373O(\u221ad)\uff0c\u8fd9\u4e0e\u524d\u5411\u8fc7\u7a0b\u7684\u4e0d\u53d8\u5206\u5e03\u76f8\u5173\u3002", "conclusion": "\u672c\u6587\u4e3a\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u5206\u6790\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5728\u9ad8\u7ef4\u60c5\u51b5\u4e0b\u91c7\u6837\u590d\u6742\u5ea6\u7684\u6700\u4f18\u754c\u9650\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2512.06521", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06521", "abs": "https://arxiv.org/abs/2512.06521", "authors": ["Jens Dede", "Anna F\u00f6rster"], "title": "ShadowWolf -- Automatic Labelling, Evaluation and Model Training Optimised for Camera Trap Wildlife Images", "comment": "31 pages + appendix", "summary": "The continuous growth of the global human population is leading to the expansion of human habitats, resulting in decreasing wildlife spaces and increasing human-wildlife interactions. These interactions can range from minor disturbances, such as raccoons in urban waste bins, to more severe consequences, including species extinction. As a result, the monitoring of wildlife is gaining significance in various contexts. Artificial intelligence (AI) offers a solution by automating the recognition of animals in images and videos, thereby reducing the manual effort required for wildlife monitoring. Traditional AI training involves three main stages: image collection, labelling, and model training. However, the variability, for example, in the landscape (e.g., mountains, open fields, forests), weather (e.g., rain, fog, sunshine), lighting (e.g., day, night), and camera-animal distances presents significant challenges to model robustness and adaptability in real-world scenarios.\n  In this work, we propose a unified framework, called ShadowWolf, designed to address these challenges by integrating and optimizing the stages of AI model training and evaluation. The proposed framework enables dynamic model retraining to adjust to changes in environmental conditions and application requirements, thereby reducing labelling efforts and allowing for on-site model adaptation. This adaptive and unified approach enhances the accuracy and efficiency of wildlife monitoring systems, promoting more effective and scalable conservation efforts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u540d\u4e3aShadowWolf\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4f18\u5316AI\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u9636\u6bb5\uff0c\u89e3\u51b3\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u4e2d\u73af\u5883\u53d8\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u52a8\u6001\u6a21\u578b\u91cd\u8bad\u7ec3\u4ee5\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\u5e76\u63d0\u5347\u76d1\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5168\u7403\u4eba\u53e3\u589e\u957f\u5bfc\u81f4\u91ce\u751f\u52a8\u7269\u6816\u606f\u5730\u51cf\u5c11\uff0c\u4eba\u517d\u51b2\u7a81\u52a0\u5267\uff0c\u9700\u8981\u6709\u6548\u7684\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u65b9\u6cd5\u3002\u4f20\u7edfAI\u8bad\u7ec3\u9762\u4e34\u73af\u5883\u53d8\u5316\uff08\u5982\u5730\u5f62\u3001\u5929\u6c14\u3001\u5149\u7167\u3001\u8ddd\u79bb\uff09\u5e26\u6765\u7684\u6a21\u578b\u9c81\u68d2\u6027\u6311\u6218\u3002", "method": "\u63d0\u51faShadowWolf\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u56fe\u50cf\u91c7\u96c6\u3001\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u9636\u6bb5\uff0c\u652f\u6301\u52a8\u6001\u6a21\u578b\u91cd\u8bad\u7ec3\u4ee5\u9002\u5e94\u73af\u5883\u53d8\u5316\u548c\u5e94\u7528\u9700\u6c42\uff0c\u5b9e\u73b0\u73b0\u573a\u6a21\u578b\u81ea\u9002\u5e94\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u51cf\u5c11\u6807\u6ce8\u5de5\u4f5c\u91cf\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u51c6\u786e\u6027\uff0c\u63d0\u5347\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u7cfb\u7edf\u7684\u6548\u7387\u3002", "conclusion": "ShadowWolf\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u548c\u7edf\u4e00\u7684\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u66f4\u6709\u6548\u7684\u4fdd\u62a4\u5de5\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07819", "abs": "https://arxiv.org/abs/2512.07819", "authors": ["Shubham S. Kumbhar", "Abhijeet M. Kulkarni", "Panagiotis Artemiadis"], "title": "Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation", "comment": null, "summary": "We present a control framework that enables humanoid robots to perform collaborative transportation tasks with a human partner. The framework supports both translational and rotational motions, which are fundamental to co-transport scenarios. It comprises three components: a high-level planner, a low-level controller, and a stiffness modulation mechanism. At the planning level, we introduce the Interaction Linear Inverted Pendulum (I-LIP), which, combined with an admittance model and an MPC formulation, generates dynamically feasible footstep plans. These are executed by a QP-based whole-body controller that accounts for the coupled humanoid-object dynamics. Stiffness modulation regulates robot-object interaction, ensuring convergence to the desired relative configuration defined by the distance between the object and the robot's center of mass. We validate the effectiveness of the framework through real-world experiments conducted on the Digit humanoid platform. To quantify collaboration quality, we propose an efficiency metric that captures both task performance and inter-agent coordination. We show that this metric highlights the role of compliance in collaborative tasks and offers insights into desirable trajectory characteristics across both high- and low-level control layers. Finally, we showcase experimental results on collaborative behaviors, including translation, turning, and combined motions such as semi circular trajectories, representative of naturally occurring co-transportation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u63a7\u5236\u6846\u67b6\u4f7f\u53cc\u8db3\u673a\u5668\u4eba\u80fd\u4e0e\u4eba\u7c7b\u4f19\u4f34\u534f\u4f5c\u642c\u8fd0\u7269\u4f53\uff0c\u652f\u6301\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\uff0c\u5305\u542b\u9ad8\u5c42\u89c4\u5212\u5668\u3001\u4f4e\u5c42\u63a7\u5236\u5668\u548c\u521a\u5ea6\u8c03\u5236\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5b9e\u73b0\u53cc\u8db3\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u7684\u534f\u4f5c\u642c\u8fd0\u4efb\u52a1\uff0c\u89e3\u51b3\u5728\u534f\u4f5c\u8fd0\u8f93\u573a\u666f\u4e2d\u673a\u5668\u4eba\u9700\u8981\u540c\u65f6\u5904\u7406\u5e73\u79fb\u548c\u65cb\u8f6c\u8fd0\u52a8\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u89c4\u5212\u5668\u4f7f\u7528\u4ea4\u4e92\u7ebf\u6027\u5012\u7acb\u6446\u6a21\u578b\u7ed3\u5408\u5bfc\u7eb3\u6a21\u578b\u548cMPC\u751f\u6210\u53ef\u884c\u811a\u6b65\u89c4\u5212\uff1b\u4f4e\u5c42\u63a7\u5236\u5668\u57fa\u4e8eQP\u7684\u5168\u8eab\u63a7\u5236\u5668\u5904\u7406\u8026\u5408\u52a8\u529b\u5b66\uff1b\u521a\u5ea6\u8c03\u5236\u673a\u5236\u8c03\u8282\u673a\u5668\u4eba-\u7269\u4f53\u4ea4\u4e92\u3002", "result": "\u5728Digit\u53cc\u8db3\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u6210\u529f\u5b9e\u73b0\u4e86\u534f\u4f5c\u642c\u8fd0\u884c\u4e3a\uff0c\u5305\u62ec\u5e73\u79fb\u3001\u8f6c\u5411\u548c\u534a\u5706\u5f62\u8f68\u8ff9\u7b49\u81ea\u7136\u534f\u4f5c\u8fd0\u8f93\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86\u91cf\u5316\u534f\u4f5c\u6548\u7387\u7684\u6307\u6807\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u652f\u6301\u4eba\u673a\u534f\u4f5c\u642c\u8fd0\uff0c\u63d0\u51fa\u7684\u6548\u7387\u6307\u6807\u63ed\u793a\u4e86\u67d4\u987a\u6027\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e3a\u9ad8\u4f4e\u5c42\u63a7\u5236\u5c42\u7684\u8f68\u8ff9\u7279\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2512.06708", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06708", "abs": "https://arxiv.org/abs/2512.06708", "authors": ["Waleed Razzaq", "Yun-Bo Zhao"], "title": "A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations", "comment": null, "summary": "Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001RUL\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5229\u7528\u56fe\u50cf\u8868\u793a\u548c\u65f6\u9891\u8868\u793a\u6765\u4f30\u8ba1\u6eda\u52a8\u8f74\u627f\u7684\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff0c\u5728\u51cf\u5c11\u6570\u636e\u9700\u6c42\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709RUL\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u5dee\u3001\u9c81\u68d2\u6027\u4e0d\u8db3\u3001\u6570\u636e\u9700\u6c42\u9ad8\u548c\u53ef\u89e3\u91ca\u6027\u6709\u9650\u7b49\u95ee\u9898\u3002\u6eda\u52a8\u8f74\u627f\u662f\u673a\u68b0\u6545\u969c\u7684\u5e38\u89c1\u539f\u56e0\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684RUL\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u5206\u652f\u67b6\u6784\uff1a\u56fe\u50cf\u8868\u793a\u5206\u652f\u548c\u65f6\u9891\u8868\u793a\u5206\u652f\u5206\u522b\u4f7f\u7528\u6269\u5f20\u5377\u79ef\u5757\u63d0\u53d6\u7a7a\u95f4\u9000\u5316\u7279\u5f81\uff0c\u878d\u5408\u5206\u652f\u5c06\u7279\u5f81\u8fde\u63a5\u540e\u8f93\u5165LSTM\u5efa\u6a21\u65f6\u95f4\u6a21\u5f0f\uff0c\u91c7\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7a81\u51fa\u91cd\u8981\u7279\u5f81\u3002\u4f7f\u7528Bresenham\u7ebf\u7b97\u6cd5\u548c\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\u8fdb\u884c\u4fe1\u53f7\u8f6c\u6362\uff0c\u5e76\u5f15\u5165\u591a\u6a21\u6001\u5c42\u76f8\u5173\u4f20\u64ad\u6280\u672f\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728XJTU-SY\u548cPRONOSTIA\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u5df2\u77e5\u548c\u672a\u77e5\u5de5\u51b5\u4e0b\u5747\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u5728XJTU-SY\u4e0a\u51cf\u5c1128%\u8bad\u7ec3\u6570\u636e\uff0cPRONOSTIA\u4e0a\u51cf\u5c1148%\u3002\u6a21\u578b\u5177\u6709\u5f3a\u566a\u58f0\u9c81\u68d2\u6027\uff0c\u53ef\u89c6\u5316\u8bc1\u5b9e\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86RUL\u4f30\u8ba1\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6570\u636e\u9700\u6c42\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2512.06530", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06530", "abs": "https://arxiv.org/abs/2512.06530", "authors": ["Mohammed Wattad", "Tamir Shor", "Alex Bronstein"], "title": "On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization", "comment": null, "summary": "Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u8868\u660e\u5b66\u4e60\u5230\u7684k\u7a7a\u95f4\u91c7\u6837\u6a21\u5f0f\u5728\u52a0\u901fMRI\u4e2d\u5177\u6709\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f15\u5165\u91c7\u96c6\u4e0d\u786e\u5b9a\u6027\u6765\u589e\u5f3a\u57df\u9c81\u68d2\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5927\u591a\u5173\u6ce8\u9488\u5bf9\u5355\u4e00\u6570\u636e\u96c6\u6216\u6a21\u6001\u4f18\u5316\u7684\u91c7\u96c6\u6a21\u5f0f\uff0c\u800c\u5ffd\u89c6\u4e86\u5b83\u4eec\u5728\u8de8\u6210\u50cf\u57df\u7684\u53ef\u8fc1\u79fb\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5b66\u4e60\u5230\u7684k\u7a7a\u95f4\u91c7\u6837\u5728\u57df\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6570\u636e\u96c6\u548c\u91c7\u96c6\u8303\u5f0f\uff0c\u9a8c\u8bc1\u5b66\u4e60\u91c7\u6837\u6a21\u5f0f\u7684\u8de8\u57df\u6cdb\u5316\u6027\u80fd\uff1b\u63d0\u51fa\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u91c7\u96c6\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u968f\u673a\u6270\u52a8k\u7a7a\u95f4\u8f68\u8ff9\u6765\u6a21\u62df\u626b\u63cf\u4eea\u548c\u6210\u50cf\u6761\u4ef6\u7684\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u5b66\u4e60\u91c7\u6837\u6a21\u5f0f\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8de8\u57df\u8bbe\u7f6e\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u51fa\u7684\u4e0d\u786e\u5b9a\u6027\u589e\u5f3a\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u57df\u9c81\u68d2\u6027\u3002", "conclusion": "k\u7a7a\u95f4\u8f68\u8ff9\u8bbe\u8ba1\u4e0d\u5e94\u4ec5\u4ec5\u88ab\u89c6\u4e3a\u52a0\u901f\u673a\u5236\uff0c\u800c\u5e94\u4f5c\u4e3a\u63d0\u9ad8MRI\u91cd\u5efa\u57df\u6cdb\u5316\u80fd\u529b\u7684\u4e3b\u52a8\u81ea\u7531\u5ea6\u3002"}}
{"id": "2512.06714", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.06714", "abs": "https://arxiv.org/abs/2512.06714", "authors": ["Tony Salloom", "Okyay Kaynak", "Wei He"], "title": "A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting", "comment": null, "summary": "Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGRU\u548cK-means\u7684\u77ed\u671f\u7528\u6c34\u9700\u6c42\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u6269\u5c55\u6280\u672f\u964d\u4f4e\u6781\u7aef\u70b9\u8bef\u5dee\uff0c\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u590d\u6742\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u77ed\u671f\u7528\u6c34\u9700\u6c42\u9884\u6d4b\u4e2d\u867d\u7136\u51c6\u786e\uff0c\u4f46\u5b58\u5728\u53c2\u6570\u8fc7\u591a\u5bfc\u81f4\u7684\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u6781\u7aef\u70b9\u9884\u6d4b\u8bef\u5dee\u8f83\u5927\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u95e8\u63a7\u5faa\u73af\u5355\u5143(GRU)\u5904\u7406\u5386\u53f2\u7528\u6c34\u6570\u636e\u7684\u5e8f\u5217\u5173\u7cfb\uff0c\u5f15\u5165K-means\u65e0\u76d1\u7763\u5206\u7c7b\u65b9\u6cd5\u521b\u5efa\u65b0\u7279\u5f81\uff1b\u63d0\u51fa\u6570\u636e\u6269\u5c55\u6280\u672f\uff0c\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u63d2\u5165\u865a\u62df\u6570\u636e\u4ee5\u7f13\u89e3\u6781\u7aef\u70b9\u9644\u8fd1\u7684\u975e\u7ebf\u6027\u95ee\u9898\u3002", "result": "\u6a21\u578b\u590d\u6742\u5ea6\u964d\u4f4e\u81f3\u6587\u732e\u6c34\u5e73\u7684\u516d\u5206\u4e4b\u4e00\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u7cbe\u5ea6\uff1b\u6570\u636e\u6269\u5c55\u4f7f\u8bef\u5dee\u964d\u4f4e\u7ea630%\uff0c\u4f46\u589e\u52a0\u4e86\u8bad\u7ec3\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7528\u6c34\u9700\u6c42\u9884\u6d4b\u4e2d\u7684\u590d\u6742\u5ea6\u548c\u6781\u7aef\u70b9\u8bef\u5dee\u95ee\u9898\uff0c\u4e3a\u4f9b\u6c34\u7cfb\u7edf\u4f18\u5316\u63a7\u5236\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06531", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06531", "abs": "https://arxiv.org/abs/2512.06531", "authors": ["Sayan Das", "Arghadip Biswas"], "title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images", "comment": null, "summary": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784SAETCN\u548cSAS-Net\uff0c\u7528\u4e8e\u8111\u80bf\u7624\u7684\u81ea\u52a8\u5206\u7c7b\u548c\u5206\u5272\uff0c\u5728\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523099.38%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u548c99.23%\u7684\u50cf\u7d20\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u8111\u80bf\u7624\u5bf9\u4eba\u7c7b\u751f\u547d\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u65e9\u671f\u51c6\u786e\u68c0\u6d4b\u5bf9\u8bca\u65ad\u548c\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u4eba\u5de5\u68c0\u6d4bMRI\u56fe\u50cf\u8017\u65f6\u4e14\u56f0\u96be\uff0c\u73b0\u6709\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5728\u9a8c\u8bc1\u6570\u636e\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff1a(a) SAETCN\uff08\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u80bf\u7624\u5206\u7c7b\u7f51\u7edc\uff09\u7528\u4e8e\u8111\u80bf\u7624\u5206\u7c7b\uff1b(b) SAS-Net\uff08\u81ea\u6ce8\u610f\u529b\u5206\u5272\u7f51\u7edc\uff09\u7528\u4e8e\u8111\u80bf\u7624\u7cbe\u786e\u5206\u5272\u3002\u6a21\u578b\u5728\u5305\u542b3\u79cd\u80bf\u7624\u7c7b\u578b\uff08\u80f6\u8d28\u7624\u3001\u8111\u819c\u7624\u3001\u5782\u4f53\u7624\uff09\u548c\u975e\u80bf\u7624\u75c5\u4f8b\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u3002", "result": "SAETCN\u5728\u9a8c\u8bc1\u6570\u636e\u96c6\u4e0a\u8fbe\u523099.38%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0cSAS-Net\u8fbe\u523099.23%\u7684\u6574\u4f53\u50cf\u7d20\u5206\u5272\u7cbe\u5ea6\uff0c\u6210\u4e3a\u5c11\u6570\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u8111\u80bf\u7624\u7684\u65b0\u578b\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u4e4b\u4e00\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u8111\u80bf\u7624\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u8ba1\u7b97\u673a\u8f85\u52a9\u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u8111\u80bf\u7624\u7684\u65e9\u671f\u81ea\u52a8\u68c0\u6d4b\u3002"}}
{"id": "2512.06725", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.06725", "abs": "https://arxiv.org/abs/2512.06725", "authors": ["Tian Lan"], "title": "Decoding Motor Behavior Using Deep Learning and Reservoir Computing", "comment": "10 pages, 3 figures", "summary": "We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u56de\u58f0\u72b6\u6001\u7f51\u7edc\u7684EEG\u89e3\u7801\u65b0\u65b9\u6cd5ESNNet\uff0c\u7528\u4e8e\u8fd0\u52a8\u884c\u4e3a\u5206\u7c7b\uff0c\u5728\u6ed1\u677f\u6280\u5de7EEG\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u4f20\u7edfCNN\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5377\u79ef\u67b6\u6784\u5982EEGNet\u548cDeepConvNet\u80fd\u6709\u6548\u6355\u6349\u5c40\u90e8\u7a7a\u95f4\u6a21\u5f0f\uff0c\u4f46\u4e0d\u64c5\u957f\u5efa\u6a21\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u548c\u975e\u7ebf\u6027\u52a8\u6001\uff0c\u9700\u8981\u6539\u8fdb\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u5c06\u56de\u58f0\u72b6\u6001\u7f51\u7edc\uff08ESN\uff09\u96c6\u6210\u5230\u89e3\u7801\u6d41\u7a0b\u4e2d\uff0cESN\u6784\u5efa\u9ad8\u7ef4\u7a00\u758f\u8fde\u63a5\u7684\u5faa\u73af\u50a8\u5907\u6c60\uff0c\u64c5\u957f\u8ddf\u8e2a\u65f6\u95f4\u52a8\u6001\uff0c\u4e0eCNN\u7684\u7a7a\u95f4\u8868\u5f81\u80fd\u529b\u5f62\u6210\u4e92\u8865\u3002", "result": "\u5728\u7ecfPREP\u7ba1\u9053\u9884\u5904\u7406\u3001MNE-Python\u5b9e\u73b0\u7684\u6ed1\u677f\u6280\u5de7EEG\u6570\u636e\u96c6\u4e0a\uff0cESNNet\u83b7\u5f97\u4e8683.2%\u7684\u53d7\u8bd5\u8005\u5185\u51c6\u786e\u7387\u548c51.3%\u7684\u7559\u4e00\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u5e7f\u6cdb\u4f7f\u7528\u7684CNN\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ESNNet\u901a\u8fc7\u7ed3\u5408CNN\u7684\u7a7a\u95f4\u8868\u5f81\u80fd\u529b\u548cESN\u7684\u65f6\u95f4\u52a8\u6001\u5efa\u6a21\u80fd\u529b\uff0c\u4e3a\u8111\u673a\u63a5\u53e3\u4e2d\u7684EEG\u89e3\u7801\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8fd0\u52a8\u884c\u4e3a\u5206\u7c7b\u4efb\u52a1\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.06560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06560", "abs": "https://arxiv.org/abs/2512.06560", "authors": ["Dalia Alzu'bi", "A. Ben Hamza"], "title": "Bridging spatial awareness and global context in medical image segmentation", "comment": null, "summary": "Medical image segmentation is a fundamental task in computer-aided diagnosis, requiring models that balance segmentation accuracy and computational efficiency. However, existing segmentation models often struggle to effectively capture local and global contextual information, leading to boundary pixel loss and segmentation errors. In this paper, we propose U-CycleMLP, a novel U-shaped encoder-decoder network designed to enhance segmentation performance while maintaining a lightweight architecture. The encoder learns multiscale contextual features using position attention weight excitation blocks, dense atrous blocks, and downsampling operations, effectively capturing both local and global contextual information. The decoder reconstructs high-resolution segmentation masks through upsampling operations, dense atrous blocks, and feature fusion mechanisms, ensuring precise boundary delineation. To further refine segmentation predictions, channel CycleMLP blocks are incorporated into the decoder along the skip connections, enhancing feature integration while maintaining linear computational complexity relative to input size. Experimental results, both quantitative and qualitative, across three benchmark datasets demonstrate the competitive performance of U-CycleMLP in comparison with state-of-the-art methods, achieving better segmentation accuracy across all datasets, capturing fine-grained anatomical structures, and demonstrating robustness across different medical imaging modalities. Ablation studies further highlight the importance of the model's core architectural components in enhancing segmentation accuracy.", "AI": {"tldr": "U-CycleMLP\u662f\u4e00\u79cd\u65b0\u578bU\u578b\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u901a\u8fc7\u4f4d\u7f6e\u6ce8\u610f\u529b\u6743\u91cd\u6fc0\u52b1\u5757\u3001\u5bc6\u96c6\u7a7a\u6d1e\u5757\u548c\u901a\u9053CycleMLP\u5757\u7b49\u6280\u672f\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u67b6\u6784\u7684\u540c\u65f6\u63d0\u5347\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5206\u5272\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u8fb9\u754c\u50cf\u7d20\u4e22\u5931\u548c\u5206\u5272\u9519\u8bef\uff0c\u9700\u8981\u5e73\u8861\u5206\u5272\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7f16\u7801\u5668\u4f7f\u7528\u4f4d\u7f6e\u6ce8\u610f\u529b\u6743\u91cd\u6fc0\u52b1\u5757\u3001\u5bc6\u96c6\u7a7a\u6d1e\u5757\u548c\u4e0b\u91c7\u6837\u64cd\u4f5c\u5b66\u4e60\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7279\u5f81\uff1b\u89e3\u7801\u5668\u901a\u8fc7\u4e0a\u91c7\u6837\u3001\u5bc6\u96c6\u7a7a\u6d1e\u5757\u548c\u7279\u5f81\u878d\u5408\u673a\u5236\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u5206\u5272\u63a9\u7801\uff1b\u5728\u8df3\u8dc3\u8fde\u63a5\u4e2d\u5f15\u5165\u901a\u9053CycleMLP\u5757\u589e\u5f3a\u7279\u5f81\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cU-CycleMLP\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u4f18\u52bf\uff0c\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u80fd\u6355\u6349\u7ec6\u7c92\u5ea6\u89e3\u5256\u7ed3\u6784\uff0c\u5e76\u5728\u4e0d\u540c\u533b\u5b66\u6210\u50cf\u6a21\u6001\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "U-CycleMLP\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5c40\u90e8-\u5168\u5c40\u4fe1\u606f\u5e73\u8861\u95ee\u9898\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u6838\u5fc3\u7ec4\u4ef6\u5bf9\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.06727", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06727", "abs": "https://arxiv.org/abs/2512.06727", "authors": ["Sourjya Roy", "Shrihari Sridharan", "Surya Selvam", "Anand Raghunathan"], "title": "KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models", "comment": null, "summary": "As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.", "AI": {"tldr": "KV CAR\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u4e0e\u67b6\u6784\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u81ea\u7f16\u7801\u5668\u548c\u76f8\u4f3c\u6027\u9a71\u52a8\u7684\u91cd\u7528\u673a\u5236\uff0c\u663e\u8457\u51cf\u5c11KV\u7f13\u5b58\u5b58\u50a8\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u589e\u5927\u548c\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0cKV\u7f13\u5b58\u7684\u5185\u5b58\u9700\u6c42\u6210\u4e3a\u81ea\u56de\u5f52\u89e3\u7801\u7684\u4e3b\u8981\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6279\u91cf\u5927\u5c0f\u548c\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6269\u5c55\u3002", "method": "\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u6280\u672f\uff1a1\uff09\u8f7b\u91cf\u7ea7\u81ea\u7f16\u7801\u5668\u6cbf\u5d4c\u5165\u7ef4\u5ea6\u5b66\u4e60KV\u5f20\u91cf\u7684\u7d27\u51d1\u8868\u793a\uff1b2\uff09\u76f8\u4f3c\u6027\u9a71\u52a8\u91cd\u7528\u673a\u5236\u8bc6\u522b\u76f8\u90bb\u5c42\u95f4\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u7684KV\u5f20\u91cf\u91cd\u7528\u673a\u4f1a\u3002", "result": "\u5728GPT-2\u548cTinyLLaMA\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cKV CAR\u53ef\u5b9e\u73b0\u9ad8\u8fbe47.85%\u7684KV\u7f13\u5b58\u5185\u5b58\u51cf\u5c11\uff0c\u5bf9\u56f0\u60d1\u5ea6\u548c\u96f6\u6837\u672c\u51c6\u786e\u7387\u5f71\u54cd\u6781\u5c0f\u3002", "conclusion": "KV CAR\u901a\u8fc7\u51cf\u5c11KV\u5f20\u91cf\u7684\u7ef4\u5ea6\u548c\u7ed3\u6784\u5197\u4f59\uff0c\u6709\u6548\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u652f\u6301\u66f4\u957f\u7684\u5e8f\u5217\u957f\u5ea6\u548c\u66f4\u5927\u7684\u6279\u91cf\u5927\u5c0f\u3002"}}
{"id": "2512.06562", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06562", "abs": "https://arxiv.org/abs/2512.06562", "authors": ["Dung Thuy Nguyen", "Quang Nguyen", "Preston K. Robinette", "Eli Jiang", "Taylor T. Johnson", "Kevin Leach"], "title": "SUGAR: A Sweeter Spot for Generative Unlearning of Many Identities", "comment": null, "summary": "Recent advances in 3D-aware generative models have enabled high-fidelity image synthesis of human identities. However, this progress raises urgent questions around user consent and the ability to remove specific individuals from a model's output space. We address this by introducing SUGAR, a framework for scalable generative unlearning that enables the removal of many identities (simultaneously or sequentially) without retraining the entire model. Rather than projecting unwanted identities to unrealistic outputs or relying on static template faces, SUGAR learns a personalized surrogate latent for each identity, diverting reconstructions to visually coherent alternatives while preserving the model's quality and diversity. We further introduce a continual utility preservation objective that guards against degradation as more identities are forgotten. SUGAR achieves state-of-the-art performance in removing up to 200 identities, while delivering up to a 700% improvement in retention utility compared to existing baselines. Our code is publicly available at https://github.com/judydnguyen/SUGAR-Generative-Unlearn.", "AI": {"tldr": "SUGAR\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u751f\u6210\u5f0f\u9057\u5fd8\u6846\u67b6\uff0c\u80fd\u591f\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u540c\u65f6\u6216\u987a\u5e8f\u79fb\u9664\u591a\u4e2a\u8eab\u4efd\uff0c\u5e76\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002", "motivation": "\u968f\u77403D\u611f\u77e5\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u9700\u8981\u89e3\u51b3\u7528\u6237\u540c\u610f\u548c\u4ece\u6a21\u578b\u8f93\u51fa\u7a7a\u95f4\u4e2d\u79fb\u9664\u7279\u5b9a\u4e2a\u4f53\u7684\u80fd\u529b\u95ee\u9898\u3002", "method": "SUGAR\u4e3a\u6bcf\u4e2a\u8eab\u4efd\u5b66\u4e60\u4e2a\u6027\u5316\u7684\u66ff\u4ee3\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u91cd\u5efa\u7ed3\u679c\u8f6c\u5411\u89c6\u89c9\u4e0a\u8fde\u8d2f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u5f15\u5165\u6301\u7eed\u6548\u7528\u4fdd\u6301\u76ee\u6807\u4ee5\u9632\u6b62\u968f\u7740\u66f4\u591a\u8eab\u4efd\u88ab\u9057\u5fd8\u800c\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002", "result": "SUGAR\u5728\u79fb\u9664\u591a\u8fbe200\u4e2a\u8eab\u4efd\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4fdd\u7559\u6548\u7528\u63d0\u9ad8\u4e86700%\u3002", "conclusion": "SUGAR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8eab\u4efd\u79fb\u9664\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2512.06730", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06730", "abs": "https://arxiv.org/abs/2512.06730", "authors": ["Lin Yang", "Xiang Li", "Xin Ma", "Xinxin Zhao"], "title": "Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data", "comment": null, "summary": "Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u5f3a\u73b0\u5b9e\u7684\u7a33\u6001\u89c6\u89c9\u8bf1\u53d1\u7535\u4f4d\uff08AR-SSVEP\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684CNN-BiLSTM\u67b6\u6784\uff0c\u63d0\u9ad8\u8fd0\u52a8\u610f\u56fe\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u5229\u7528SHAP\u65b9\u6cd5\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u8fd0\u52a8\u529f\u80fd\u969c\u788d\u60a3\u8005\u5eb7\u590d\u8bad\u7ec3\u53c2\u4e0e\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f20\u7edfSSVEP-BCI\u7cfb\u7edf\u4f9d\u8d56\u5916\u90e8\u89c6\u89c9\u523a\u6fc0\u8bbe\u5907\u3001\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u9650\u7684\u5c40\u9650\u6027\u3002", "method": "1. \u8bbe\u8ba1\u57fa\u4e8eHoloLens 2\u7684\u56db\u79cdEEG\u7c7b\u522b\u5e76\u6536\u96c6\u4e03\u540d\u5065\u5eb7\u53d7\u8bd5\u8005\u7684EEG\u6570\u636e\uff1b2. \u5728\u4f20\u7edfCNN-BiLSTM\u67b6\u6784\u57fa\u7840\u4e0a\u96c6\u6210\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\uff08MACNN-BiLSTM\uff09\uff1b3. \u63d0\u53d6\u5341\u4e2a\u65f6\u9891EEG\u7279\u5f81\uff0c\u901a\u8fc7CNN\u5b66\u4e60\u9ad8\u5c42\u8868\u793a\uff0c\u4f7f\u7528BiLSTM\u5efa\u6a21\u5e8f\u5217\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e94\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7a81\u51fa\u8fd0\u52a8\u610f\u56fe\u76f8\u5173\u6a21\u5f0f\uff1b4. \u4f7f\u7528SHAP\u65b9\u6cd5\u53ef\u89c6\u5316EEG\u7279\u5f81\u5bf9\u795e\u7ecf\u7f51\u7edc\u51b3\u7b56\u7684\u8d21\u732e\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u8fd0\u52a8\u610f\u56fe\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7AR-SSVEP\u7cfb\u7edf\u548c\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u63d0\u9ad8\u4e86\u8fd0\u52a8\u610f\u56fe\u8bc6\u522b\u7684\u5b9e\u65f6\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u8fd0\u52a8\u969c\u788d\u60a3\u8005\u7684\u5eb7\u590d\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2512.06565", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06565", "abs": "https://arxiv.org/abs/2512.06565", "authors": ["Xiujin Liu"], "title": "GNC-Pose: Geometry-Aware GNC-PnP for Accurate 6D Pose Estimation", "comment": "1 figures, 2 tables, 14pages", "summary": "We present GNC--Pose, a fully learning--free monocular 6D object pose estimation pipeline for textured objects that combines rendering--based initialization, geometry--aware correspondence weighting, and robust GNC optimization. Starting from coarse 2D--3D correspondences obtained through feature matching and rendering--based alignment, our method builds upon the Graduated Non--Convexity (GNC) principle and introduces a geometry--aware, cluster--based weighting mechanism that assigns robust per point confidence based on the 3D structural consistency of the model. This geometric prior and weighting strategy significantly stabilizes the optimization under severe outlier contamination. A final LM refinement further improve accuracy. We tested GNC--Pose on The YCB Object and Model Set, despite requiring no learned features, training data, or category-specific priors, GNC--Pose achieves competitive accuracy compared with both learning-based and learning--free methods, and offers a simple, robust, and practical solution for learning-free 6D pose estimation.", "AI": {"tldr": "GNC-Pose\u662f\u4e00\u79cd\u65e0\u9700\u5b66\u4e60\u7684\u5355\u76ee6D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e32\u67d3\u521d\u59cb\u5316\u3001\u51e0\u4f55\u611f\u77e5\u5bf9\u5e94\u70b9\u52a0\u6743\u548c\u9c81\u68d2GNC\u4f18\u5316\uff0c\u5728YCB\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0e\u5b66\u4e60\u65b9\u6cd5\u548c\u975e\u5b66\u4e60\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7eb9\u7406\u7269\u4f536D\u59ff\u6001\u4f30\u8ba1\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u5b8c\u5168\u65e0\u9700\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4e0d\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u3001\u5b66\u4e60\u7279\u5f81\u6216\u7c7b\u522b\u7279\u5b9a\u5148\u9a8c\uff0c\u63d0\u4f9b\u7b80\u5355\u3001\u9c81\u68d2\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u901a\u8fc7\u7279\u5f81\u5339\u914d\u548c\u6e32\u67d3\u5bf9\u9f50\u83b7\u5f97\u7c97\u7565\u76842D-3D\u5bf9\u5e94\u70b9\uff1b2. \u57fa\u4e8eGNC\u539f\u5219\u5f15\u5165\u51e0\u4f55\u611f\u77e5\u7684\u805a\u7c7b\u52a0\u6743\u673a\u5236\uff0c\u6839\u636e\u6a21\u578b3D\u7ed3\u6784\u4e00\u81f4\u6027\u5206\u914d\u9c81\u68d2\u7684\u9010\u70b9\u7f6e\u4fe1\u5ea6\uff1b3. \u4f7f\u7528LM\u4f18\u5316\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5728YCB\u7269\u4f53\u548c\u6a21\u578b\u96c6\u4e0a\u6d4b\u8bd5\uff0cGNC-Pose\u5c3d\u7ba1\u4e0d\u9700\u8981\u5b66\u4e60\u7279\u5f81\u3001\u8bad\u7ec3\u6570\u636e\u6216\u7c7b\u522b\u7279\u5b9a\u5148\u9a8c\uff0c\u4f46\u4e0e\u57fa\u4e8e\u5b66\u4e60\u548c\u975e\u5b66\u4e60\u7684\u65b9\u6cd5\u76f8\u6bd4\u8fbe\u5230\u4e86\u7ade\u4e89\u6027\u7684\u7cbe\u5ea6\u3002", "conclusion": "GNC-Pose\u901a\u8fc7\u51e0\u4f55\u5148\u9a8c\u548c\u52a0\u6743\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u4f18\u5316\u7a33\u5b9a\u6027\uff0c\u5728\u4e25\u91cd\u5f02\u5e38\u503c\u6c61\u67d3\u4e0b\u4ecd\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4e3a\u65e0\u5b66\u4e606D\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06737", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.06737", "abs": "https://arxiv.org/abs/2512.06737", "authors": ["Nikhil Verma", "Joonas Linnosmaa", "Espinosa-Leal Leonardo", "Napat Vajragupta"], "title": "Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics", "comment": "80 pages, 6 tables, 2 figures, 5 appendices, proof-of-concept", "summary": "The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ArcGD\u4f18\u5316\u5668\uff0c\u5728\u975e\u51f8\u57fa\u51c6\u51fd\u6570\u548c\u771f\u5b9eML\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4Adam\u7b49\u5148\u8fdb\u4f18\u5316\u5668\u5728\u6536\u655b\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3001\u5177\u6709\u66f4\u597d\u6536\u655b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u4f18\u5316\u5668\uff0c\u89e3\u51b3\u73b0\u6709\u4f18\u5316\u5668\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u53ef\u80fd\u51fa\u73b0\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51faArcGD\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u4e24\u79cd\u914d\u7f6e\u8bc4\u4f30\u6d88\u9664\u5b66\u4e60\u7387\u504f\u5dee\uff1a\u4f7f\u7528ArcGD\u6709\u6548\u5b66\u4e60\u7387\u548cAdam\u9ed8\u8ba4\u5b66\u4e60\u7387\u3002\u5728Rosenbrock\u51fd\u6570\uff082D-50,000D\uff09\u548cCIFAR-10\u6570\u636e\u96c6\uff088\u79cdMLP\u67b6\u6784\uff09\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "ArcGD\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8eAdam\uff0c\u5728CIFAR-10\u4e0a\u8fbe\u523050.7%\u7684\u6700\u9ad8\u5e73\u5747\u6d4b\u8bd5\u51c6\u786e\u7387\uff0c\u4f18\u4e8eAdamW\uff0846.6%\uff09\u3001Adam\uff0846.8%\uff09\u3001SGD\uff0849.6%\uff09\u548cLion\uff0843.4%\uff09\u3002ArcGD\u57286/8\u67b6\u6784\u4e0a\u83b7\u80dc\u6216\u6301\u5e73\uff0c\u4e14\u6301\u7eed\u6539\u8fdb\u4e0d\u51fa\u73b0\u56de\u5f52\u3002", "conclusion": "ArcGD\u5728\u51e0\u4f55\u538b\u529b\u6d4b\u8bd5\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u51c6\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002ArcGD\u53d8\u4f53\u53ef\u89c6\u4e3aLion\u4f18\u5316\u5668\u7684\u7279\u4f8b\uff0c\u63ed\u793a\u4e86\u4f18\u5316\u65b9\u6cd5\u5185\u5728\u673a\u5236\u7684\u8054\u7cfb\u3002"}}
{"id": "2512.06575", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06575", "abs": "https://arxiv.org/abs/2512.06575", "authors": ["Fariza Dahes"], "title": "Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules", "comment": "26 pages, 16 figures, 2 tables; proof of concept on mammography classification with compactness/separability modules and interactive dashboard; preprint submitted to arXiv cs.LG", "summary": "This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.", "AI": {"tldr": "\u672c\u7814\u7a76\u9a8c\u8bc1\u5e76\u6269\u5c55\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u65b9\u6cd5\u6846\u67b6\uff0c\u5c06\u6539\u8fdb\u7684ConvNeXt Tiny\u67b6\u6784\u5e94\u7528\u4e8e\u4e73\u817aX\u5149\u5206\u7c7b\uff0c\u8bc4\u4f30\u4e86GAGM\u548cSEVector\u6a21\u5757\u7684\u6709\u6548\u6027\uff0c\u4f46\u53d1\u73b0FSL\u635f\u5931\u51fd\u6570\u5728\u4e73\u817aX\u5149\u5206\u7c7b\u4e2d\u6548\u679c\u4e0d\u660e\u663e\u3002", "motivation": "\u9a8c\u8bc1\u5148\u524d\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c7MRI\u5206\u7c7b\u4e2d\u8868\u73b0\u826f\u597d\u7684\u6539\u8fdbConvNeXt Tiny\u67b6\u6784\u662f\u5426\u53ef\u8fc1\u79fb\u5230\u4e73\u817aX\u5149\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u6269\u5c55\u539f\u6846\u67b6\u7684\u529f\u80fd\u3002", "method": "\u4f7f\u7528Kaggle\u6574\u5408\u7684INbreast\u3001MIAS\u548cDDSM\u4e73\u817aX\u5149\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u57fa\u7ebfCNN\u3001ConvNeXt Tiny\u548cInceptionV3\u9aa8\u5e72\u7f51\u7edc\uff0c\u96c6\u6210GAGM\u548cSEVector\u6a21\u5757\uff0c\u8fdb\u884c\u591a\u6307\u6807\u8bc4\u4f30\u548c\u7279\u5f81\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "GAGM\u548cSEVector\u6a21\u5757\u80fd\u6709\u6548\u589e\u5f3a\u7279\u5f81\u533a\u5206\u5ea6\u5e76\u51cf\u5c11\u5047\u9634\u6027\uff08\u7279\u522b\u662f\u6076\u6027\u75c5\u4f8b\uff09\uff0c\u4f46FSL\u635f\u5931\u51fd\u6570\u5728\u4e73\u817aX\u5149\u5206\u7c7b\u4e2d\u672a\u5e26\u6765\u53ef\u6d4b\u91cf\u7684\u6539\u8fdb\u3002", "conclusion": "\u6210\u529f\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u4e73\u817aX\u5149\u5206\u7c7b\u4e2d\u7684\u90e8\u5206\u9002\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u9700\u8981\u63a2\u7d22\u65b0\u65b9\u6cd5\u6765\u6539\u5584\u6076\u6027\u4e0e\u826f\u6027\u75c5\u4f8b\u533a\u5206\u5ea6\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2512.07437", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07437", "abs": "https://arxiv.org/abs/2512.07437", "authors": ["Chenwei Shi", "Xueyu Luan"], "title": "KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models", "comment": "23 pages, 8 figures, 3 tables", "summary": "DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKAN-Dreamer\uff0c\u5c06KAN\u548cFastKAN\u67b6\u6784\u96c6\u6210\u5230DreamerV3\u6846\u67b6\u4e2d\uff0c\u66ff\u6362\u7279\u5b9aMLP\u548c\u5377\u79ef\u7ec4\u4ef6\uff0c\u5728DeepMind Control Suite\u4e0a\u9a8c\u8bc1\u6027\u80fd\u4e0e\u539f\u59cbMLP\u67b6\u6784\u76f8\u5f53\u3002", "motivation": "\u7ed3\u5408DreamerV3\u7684\u6837\u672c\u6548\u7387\u4f18\u52bf\u548cKAN\u7f51\u7edc\u7684\u53c2\u6570\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\uff0c\u540c\u65f6\u901a\u8fc7FastKAN\u53d8\u4f53\u7f13\u89e3KAN\u7684\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\u3002", "method": "\u5728DreamerV3\u7684\u4e09\u4e2a\u5b50\u7cfb\u7edf\uff08\u89c6\u89c9\u611f\u77e5\u3001\u6f5c\u5728\u9884\u6d4b\u3001\u884c\u4e3a\u5b66\u4e60\uff09\u4e2d\uff0c\u7528KAN\u548cFastKAN\u5c42\u66ff\u6362\u7279\u5b9aMLP\u548c\u5377\u79ef\u7ec4\u4ef6\uff0c\u5e76\u5b9e\u73b0\u9488\u5bf9JAX\u7684\u5b8c\u5168\u5411\u91cf\u5316\u7248\u672c\u548c\u7b80\u5316\u7f51\u683c\u7ba1\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u9002\u914d\u7684FastKAN\u4f5c\u4e3a\u5956\u52b1\u548c\u7ee7\u7eed\u9884\u6d4b\u5668\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u901f\u5ea6\u4e0a\u4e0e\u539f\u59cbMLP\u67b6\u6784\u4fdd\u6301\u540c\u7b49\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8eKAN\u7684\u4e16\u754c\u6a21\u578b\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u521d\u6b65\u63a2\u7d22\uff0c\u8bc1\u660eKAN\u67b6\u6784\u5728\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2512.06752", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06752", "abs": "https://arxiv.org/abs/2512.06752", "authors": ["Chang Liu", "Vivian Li", "Linus Leong", "Vladimir Radenkovic", "Pietro Li\u00f2", "Chaitanya K. Joshi"], "title": "Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets", "comment": "Presented at Machine Learning in Structural Biology, 2025. Open-source code: https://github.com/VirtualProteins/GNN_UNet", "summary": "Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u51e0\u4f55\u56feU-Net\uff0c\u4e00\u79cd\u65b0\u7684\u51e0\u4f55\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u9012\u5f52\u7c97\u5316\u548c\u7ec6\u5316\u86cb\u767d\u8d28\u56fe\u6765\u5b66\u4e60\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709GNN\u548cTransformer\u65e0\u6cd5\u6355\u6349\u86cb\u767d\u8d28\u5c42\u6b21\u7ed3\u6784\u4ea4\u4e92\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u51e0\u4f55GNN\u548cTransformer\u4f9d\u8d56\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u63a7\u5236\u86cb\u767d\u8d28\u529f\u80fd\u7684\u5c42\u6b21\u4ea4\u4e92\uff08\u5982\u5168\u5c40\u7ed3\u6784\u57df\u548c\u957f\u7a0b\u53d8\u6784\u8c03\u8282\uff09\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u53cd\u6620\u751f\u7269\u5c42\u6b21\u7ed3\u6784\u7684\u7f51\u7edc\u67b6\u6784\u3002", "method": "\u5f15\u5165\u51e0\u4f55\u56feU-Nets\uff0c\u901a\u8fc7\u9012\u5f52\u7c97\u5316\u548c\u7ec6\u5316\u86cb\u767d\u8d28\u56fe\u6765\u5b66\u4e60\u591a\u5c3a\u5ea6\u8868\u793a\uff0c\u8be5\u5c42\u6b21\u5316\u8bbe\u8ba1\u7406\u8bba\u4e0a\u6bd4\u6807\u51c6\u51e0\u4f55GNN\u66f4\u5177\u8868\u8fbe\u529b\u3002", "result": "\u5728\u86cb\u767d\u8d28\u6298\u53e0\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u51e0\u4f55U-Nets\u663e\u8457\u4f18\u4e8e\u4e0d\u53d8\u548c\u7b49\u53d8\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u5176\u80fd\u591f\u5b66\u4e60\u5b9a\u4e49\u86cb\u767d\u8d28\u6298\u53e0\u7684\u5168\u5c40\u7ed3\u6784\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8bbe\u8ba1\u80fd\u591f\u5b66\u4e60\u751f\u7269\u5206\u5b50\u591a\u5c3a\u5ea6\u7ed3\u6784\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u51e0\u4f55\u56feU-Nets\u80fd\u591f\u6709\u6548\u6355\u6349\u86cb\u767d\u8d28\u7684\u5c42\u6b21\u5316\u7ed3\u6784\u7279\u5f81\u3002"}}
{"id": "2512.06581", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06581", "abs": "https://arxiv.org/abs/2512.06581", "authors": ["Yuhao Su", "Anwesa Choudhuri", "Zhongpai Gao", "Benjamin Planche", "Van Nguyen Nguyen", "Meng Zheng", "Yuhan Shen", "Arun Innanje", "Terrence Chen", "Ehsan Elhamifar", "Ziyan Wu"], "title": "MedGRPO: Multi-Task Reinforcement Learning for Heterogeneous Medical Video Understanding", "comment": null, "summary": "Large vision-language models struggle with medical video understanding, where spatial precision, temporal reasoning, and clinical semantics are critical. To address this, we first introduce \\textbf{MedVidBench}, a large-scale benchmark of 531,850 video-instruction pairs across 8 medical sources spanning video, segment, and frame-level tasks, curated through a rigorous quality assurance pipeline with expert-guided prompting and dual-model validation. While supervised fine-tuning on MedVidBench yields noticeable gains, standard Reinforcement Learning (RL) fails due to imbalanced reward scales across datasets, which destabilizes optimization and leads to training collapse. To overcome this, we introduce \\textbf{MedGRPO}, a novel RL framework for balanced multi-dataset training with two key innovations: (1) \\emph{cross-dataset reward normalization} that maps each dataset's median performance to a common reward value, ensuring fair optimization regardless of difficulty, and (2) a \\emph{medical LLM judge} that evaluates caption quality on five clinical dimensions through comparative similarity scoring. Supervised fine-tuning Qwen2.5-VL-7B on MedVidBench substantially outperforms GPT-4.1 and Gemini-2.5-Flash across all tasks, demonstrating MedVidBench's efficacy, while our MedGRPO framework further improves upon the SFT baseline across grounding and captioning tasks. Our work establishes a foundational benchmark and robust training methodology for advancing vision-language models in medical domains. Our project website is available at https://yuhaosu.github.io/MedGRPO/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86MedVidBench\u57fa\u51c6\u548cMedGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u7a7a\u95f4\u7cbe\u5ea6\u3001\u65f6\u95f4\u63a8\u7406\u548c\u4e34\u5e8a\u8bed\u4e49\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u89c6\u9891\u7406\u89e3\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u5904\u7406\u7a7a\u95f4\u7cbe\u5ea6\u3001\u65f6\u95f4\u63a8\u7406\u548c\u4e34\u5e8a\u8bed\u4e49\u7b49\u5173\u952e\u6311\u6218\u3002", "method": "1) \u6784\u5efaMedVidBench\u57fa\u51c6\uff0c\u5305\u542b531,850\u4e2a\u89c6\u9891-\u6307\u4ee4\u5bf9\uff1b2) \u63d0\u51faMedGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u8de8\u6570\u636e\u96c6\u5956\u52b1\u5f52\u4e00\u5316\u548c\u533b\u5b66LLM\u8bc4\u4f30\u5668\uff1b3) \u5728Qwen2.5-VL-7B\u6a21\u578b\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u76d1\u7763\u5fae\u8c03\u540e\u7684Qwen2.5-VL-7B\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u663e\u8457\u4f18\u4e8eGPT-4.1\u548cGemini-2.5-Flash\uff0cMedGRPO\u6846\u67b6\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5b9a\u4f4d\u548c\u5b57\u5e55\u751f\u6210\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u533b\u5b66\u9886\u57df\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u57fa\u7840\u57fa\u51c6\u548c\u7a33\u5065\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.07596", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07596", "abs": "https://arxiv.org/abs/2512.07596", "authors": ["Wenzhen Dong", "Jieming Yu", "Yiming Huang", "Hongqiu Wang", "Lei Zhu", "Albert C. S. Chung", "Hongliang Ren", "Long Bai"], "title": "More than Segmentation: Benchmarking SAM 3 for Segmentation, 3D Perception, and Reconstruction in Robotic Surgery", "comment": "Technical Report", "summary": "The recent Segment Anything Model (SAM) 3 has introduced significant advancements over its predecessor, SAM 2, particularly with the integration of language-based segmentation and enhanced 3D perception capabilities. SAM 3 supports zero-shot segmentation across a wide range of prompts, including point, bounding box, and language-based prompts, allowing for more flexible and intuitive interactions with the model. In this empirical evaluation, we assess the performance of SAM 3 in robot-assisted surgery, benchmarking its zero-shot segmentation with point and bounding box prompts and exploring its effectiveness in dynamic video tracking, alongside its newly introduced language prompt segmentation. While language prompts show potential, their performance in the surgical domain is currently suboptimal, highlighting the need for further domain-specific training. Additionally, we investigate SAM 3's 3D reconstruction abilities, demonstrating its capacity to process surgical scene data and reconstruct 3D anatomical structures from 2D images. Through comprehensive testing on the MICCAI EndoVis 2017 and EndoVis 2018 benchmarks, SAM 3 shows clear improvements over SAM and SAM 2 in both image and video segmentation under spatial prompts, while zero-shot evaluations on SCARED, StereoMIS, and EndoNeRF indicate strong monocular depth estimation and realistic 3D instrument reconstruction, yet also reveal remaining limitations in complex, highly dynamic surgical scenes.", "AI": {"tldr": "SAM 3\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4SAM\u548cSAM 2\u5728\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u8bed\u8a00\u63d0\u793a\u5728\u624b\u672f\u9886\u57df\u8868\u73b0\u6b20\u4f73\uff0c\u9700\u8981\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u30023D\u91cd\u5efa\u80fd\u529b\u5728\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u5668\u68b0\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u52a8\u6001\u624b\u672f\u573a\u666f\u4e2d\u4ecd\u6709\u5c40\u9650\u3002", "motivation": "\u8bc4\u4f30SAM 3\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5176\u96f6\u6837\u672c\u5206\u5272\u80fd\u529b\uff08\u5305\u62ec\u70b9\u3001\u8fb9\u754c\u6846\u548c\u8bed\u8a00\u63d0\u793a\uff09\u4ee5\u53ca3D\u611f\u77e5\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u5728\u52a8\u6001\u89c6\u9891\u8ddf\u8e2a\u548c3D\u91cd\u5efa\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u4f7f\u7528MICCAI EndoVis 2017\u548cEndoVis 2018\u57fa\u51c6\u6d4b\u8bd5SAM 3\u7684\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u6027\u80fd\uff0c\u540c\u65f6\u5728SCARED\u3001StereoMIS\u548cEndoNeRF\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6d4b\u8bd5\u5176\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c3D\u5668\u68b0\u91cd\u5efa\u80fd\u529b\u3002", "result": "SAM 3\u5728\u7a7a\u95f4\u63d0\u793a\u4e0b\u7684\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u660e\u663e\u4f18\u4e8eSAM\u548cSAM 2\uff0c\u8bed\u8a00\u63d0\u793a\u5728\u624b\u672f\u9886\u57df\u8868\u73b0\u4e0d\u4f73\u30023D\u91cd\u5efa\u65b9\u9762\u5c55\u793a\u4e86\u826f\u597d\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u5668\u68b0\u91cd\u5efa\u80fd\u529b\uff0c\u4f46\u5728\u590d\u6742\u52a8\u6001\u624b\u672f\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "SAM 3\u5728\u673a\u5668\u4eba\u8f85\u52a9\u624b\u672f\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u8fdb\u6b65\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u63d0\u793a\u5206\u5272\u548c3D\u91cd\u5efa\u65b9\u9762\uff0c\u4f46\u8bed\u8a00\u63d0\u793a\u9700\u8981\u9886\u57df\u7279\u5b9a\u4f18\u5316\uff0c\u590d\u6742\u624b\u672f\u573a\u666f\u76843D\u91cd\u5efa\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2512.06758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06758", "abs": "https://arxiv.org/abs/2512.06758", "authors": ["Zilong Wang", "Shuai Li"], "title": "Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship", "comment": null, "summary": "The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $\u03a9\\left( \\frac{N\\log(T)}{\u0394^2} + \\frac{K\\log(T)}\u0394 \\right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\\geq N)$ is the number of arms, $\u0394$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\\left( \\frac{K\\log(T)}{\u0394^2} \\right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\\left( \\frac{N\\log(T)}{\u0394^2} + \\frac{K\\log(T)}\u0394 \\right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u8fde\u7eed\u9009\u62e9\u7b97\u6cd5\uff0c\u5728\u5e8f\u5217\u72ec\u88c1\u5047\u8bbe\u4e0b\u5b9e\u73b0\u4e86\u4e0e\u7406\u8bba\u4e0b\u754c\u5339\u914d\u7684\u9057\u61be\u4e0a\u754c\uff0c\u89e3\u51b3\u4e86\u53cc\u8fb9\u5339\u914d\u5e02\u573a\u4e2d\u5728\u7ebf\u5b66\u4e60\u504f\u597d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u53cc\u8fb9\u5339\u914d\u5e02\u573a\u7684\u5728\u7ebf\u5b66\u4e60\u95ee\u9898\u4e2d\uff0cSankararaman\u7b49\u4eba\u63d0\u51fa\u4e86\u7406\u8bba\u4e0b\u754c\uff0c\u800cKong\u548cLi\u7684ET-GS\u7b97\u6cd5\u8fbe\u5230\u4e86\u76ee\u524d\u6700\u597d\u7684\u4e0a\u754c\uff0c\u4f46\u4e0a\u4e0b\u754c\u4e4b\u95f4\u5b58\u5728\u4eceN\u5230K\u7684\u5dee\u8ddd\u3002\u9700\u8981\u786e\u5b9a\u662f\u4e0b\u754c\u8fd8\u662f\u4e0a\u754c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u8fde\u7eed\u9009\u62e9\u7b97\u6cd5\uff0c\u5728\u5e8f\u5217\u72ec\u88c1\u5047\u8bbe\u4e0b\u8fd0\u884c\uff0c\u8be5\u5047\u8bbe\u5728\u5e02\u573a\u53c2\u4e0e\u8005\u6709\u4e00\u81f4\u8bc4\u4ef7\u6807\u51c6\u65f6\u5e38\u89c1\u3002", "result": "\u7b97\u6cd5\u83b7\u5f97\u4e86O(Nlog(T)/\u0394\u00b2 + Klog(T)/\u0394)\u7684\u9057\u61be\u4e0a\u754c\uff0c\u4e0e\u7406\u8bba\u4e0b\u754c\u5b8c\u5168\u5339\u914d\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u5e26\u6709\u591a\u81c2\u8001\u864e\u673a\u7684\u5339\u914d\u5e02\u573a\u95ee\u9898\u4e2d\u8fbe\u5230\u7406\u8bba\u4e0b\u754c\u7684\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4e0a\u4e0b\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u95ee\u9898\u3002"}}
{"id": "2512.06598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06598", "abs": "https://arxiv.org/abs/2512.06598", "authors": ["Muhammad Adil", "Patrick J. Clemins", "Andrew W. Schroth", "Panagiotis D. Oikonomou", "Donna M. Rizzo", "Peter D. F. Isles", "Xiaohan Zhang", "Kareem I. Hannoun", "Scott Turnbull", "Noah B. Beckage", "Asim Zia", "Safwan Wshah"], "title": "From Remote Sensing to Multiple Time Horizons Forecasts: Transformers Model for CyanoHAB Intensity in Lake Champlain", "comment": "23 pages, 15 figures", "summary": "Cyanobacterial Harmful Algal Blooms (CyanoHABs) pose significant threats to aquatic ecosystems and public health globally. Lake Champlain is particularly vulnerable to recurring CyanoHAB events, especially in its northern segment: Missisquoi Bay, St. Albans Bay, and Northeast Arm, due to nutrient enrichment and climatic variability. Remote sensing provides a scalable solution for monitoring and forecasting these events, offering continuous coverage where in situ observations are sparse or unavailable. In this study, we present a remote sensing only forecasting framework that combines Transformers and BiLSTM to predict CyanoHAB intensities up to 14 days in advance. The system utilizes Cyanobacterial Index data from the Cyanobacterial Assessment Network and temperature data from Moderate Resolution Imaging Spectroradiometer satellites to capture long range dependencies and sequential dynamics in satellite time series. The dataset is very sparse, missing more than 30% of the Cyanobacterial Index data and 90% of the temperature data. A two stage preprocessing pipeline addressed data gaps by applying forward fill and weighted temporal imputation at the pixel level, followed by smoothing to reduce the discontinuities of CyanoHAB events. The raw dataset is transformed into meaningful features through equal frequency binning for the Cyanobacterial Index values and extracted temperature statistics. Transformer BiLSTM model demonstrates strong forecasting performance across multiple horizons, achieving F1 scores of 89.5%, 86.4%, and 85.5% at one, two, and three-day forecasts, respectively, and maintaining an F1 score of 78.9% with an AUC of 82.6% at the 14-day horizon. These results confirm the model's ability to capture complex spatiotemporal dynamics from sparse satellite data and to provide reliable early warning for CyanoHABs management.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u548cBiLSTM\u7684\u9065\u611f\u9884\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u524d14\u5929\u9884\u6d4b\u84dd\u85fb\u6c34\u534e\u5f3a\u5ea6\uff0c\u5728\u7a00\u758f\u536b\u661f\u6570\u636e\u6761\u4ef6\u4e0b\u53d6\u5f97\u4e86\u826f\u597d\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u84dd\u85fb\u6709\u5bb3\u6c34\u534e\u5bf9\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u548c\u516c\u5171\u5065\u5eb7\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u800c\u4f20\u7edf\u73b0\u573a\u89c2\u6d4b\u6570\u636e\u7a00\u758f\uff0c\u9065\u611f\u6280\u672f\u4e3a\u76d1\u6d4b\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Cyanobacterial Assessment Network\u7684\u84dd\u85fb\u6307\u6570\u6570\u636e\u548cMODIS\u536b\u661f\u6e29\u5ea6\u6570\u636e\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9884\u5904\u7406\u586b\u8865\u6570\u636e\u7f3a\u5931\uff0c\u7136\u540e\u91c7\u7528Transformer-BiLSTM\u6a21\u578b\u6355\u6349\u536b\u661f\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u548c\u5e8f\u5217\u52a8\u6001\u3002", "result": "\u6a21\u578b\u5728\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c1\u5929\u30012\u5929\u30013\u5929\u9884\u6d4b\u7684F1\u5206\u6570\u5206\u522b\u4e3a89.5%\u300186.4%\u300185.5%\uff0c14\u5929\u9884\u6d4b\u7684F1\u5206\u6570\u4e3a78.9%\uff0cAUC\u4e3a82.6%\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u4ece\u7a00\u758f\u536b\u661f\u6570\u636e\u4e2d\u6355\u6349\u590d\u6742\u7684\u65f6\u7a7a\u52a8\u6001\uff0c\u4e3a\u84dd\u85fb\u6c34\u534e\u7ba1\u7406\u63d0\u4f9b\u53ef\u9760\u7684\u65e9\u671f\u9884\u8b66\u3002"}}
{"id": "2512.07698", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07698", "abs": "https://arxiv.org/abs/2512.07698", "authors": ["Arslan Artykov", "Corentin Sautier", "Vincent Lepetit"], "title": "sim2art: Accurate Articulated Object Modeling from a Single Video using Synthetic Training Data Only", "comment": null, "summary": "Understanding articulated objects is a fundamental challenge in robotics and digital twin creation. To effectively model such objects, it is essential to recover both part segmentation and the underlying joint parameters. Despite the importance of this task, previous work has largely focused on setups like multi-view systems, object scanning, or static cameras. In this paper, we present the first data-driven approach that jointly predicts part segmentation and joint parameters from monocular video captured with a freely moving camera. Trained solely on synthetic data, our method demonstrates strong generalization to real-world objects, offering a scalable and practical solution for articulated object understanding. Our approach operates directly on casually recorded video, making it suitable for real-time applications in dynamic environments. Project webpage: https://aartykov.github.io/sim2art/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u9884\u6d4b\u90e8\u4ef6\u5206\u5272\u548c\u5173\u8282\u53c2\u6570\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u81ea\u7531\u79fb\u52a8\u76f8\u673a\u62cd\u6444\u7684\u573a\u666f\u3002", "motivation": "\u7406\u89e3\u94f0\u63a5\u7269\u4f53\u662f\u673a\u5668\u4eba\u548c\u6570\u5b57\u5b6a\u751f\u521b\u5efa\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u591a\u89c6\u89d2\u7cfb\u7edf\u3001\u7269\u4f53\u626b\u63cf\u6216\u9759\u6001\u76f8\u673a\uff0c\u7f3a\u4e4f\u4ece\u5355\u76ee\u89c6\u9891\u4e2d\u76f4\u63a5\u7406\u89e3\u94f0\u63a5\u7269\u4f53\u7684\u5b9e\u7528\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u81ea\u7531\u79fb\u52a8\u76f8\u673a\u62cd\u6444\u7684\u5355\u76ee\u89c6\u9891\u4e2d\u8054\u5408\u9884\u6d4b\u90e8\u4ef6\u5206\u5272\u548c\u5173\u8282\u53c2\u6570\u3002", "result": "\u4ec5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u94f0\u63a5\u7269\u4f53\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u76f4\u63a5\u5904\u7406\u65e5\u5e38\u5f55\u5236\u7684\u89c6\u9891\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u5e94\u7528\uff0c\u662f\u94f0\u63a5\u7269\u4f53\u7406\u89e3\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2512.06782", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06782", "abs": "https://arxiv.org/abs/2512.06782", "authors": ["Weiqi Guan", "Zihao Shi"], "title": "Measuring Over-smoothing beyond Dirichlet energy", "comment": "17 pages, 1 figure", "summary": "While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u9636\u7279\u5f81\u5bfc\u6570\u7684\u8282\u70b9\u76f8\u4f3c\u6027\u5ea6\u91cf\u5bb6\u65cf\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edfDirichlet\u80fd\u91cf\u4ec5\u80fd\u6355\u6349\u4e00\u9636\u5bfc\u6570\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u8fc7\u5e73\u6ed1\u8870\u51cf\u7387\u4e0e\u56fe\u62c9\u666e\u62c9\u65af\u8c31\u9699\u7684\u5185\u5728\u8054\u7cfb\uff0c\u5e76\u5b9e\u8bc1\u8bc1\u660e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684GNN\u5728\u8fd9\u4e9b\u65b0\u5ea6\u91cf\u4e0b\u5b58\u5728\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfDirichlet\u80fd\u91cf\u4f5c\u4e3a\u91cf\u5316\u8fc7\u5e73\u6ed1\u7684\u4e3b\u6d41\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5b83\u53ea\u80fd\u6355\u6349\u4e00\u9636\u7279\u5f81\u5bfc\u6570\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u9ad8\u9636\u5e73\u6ed1\u73b0\u8c61\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u9636\u7279\u5f81\u5bfc\u6570\u80fd\u91cf\u7684\u5e7f\u4e49\u8282\u70b9\u76f8\u4f3c\u6027\u5ea6\u91cf\u5bb6\u65cf\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u7814\u7a76\u8fde\u7eed\u70ed\u6269\u6563\u548c\u79bb\u6563\u805a\u5408\u7b97\u5b50\u4e0b\u7684Dirichlet\u80fd\u91cf\u8870\u51cf\u7387\uff0c\u5e76\u5efa\u7acb\u4e0e\u56fe\u62c9\u666e\u62c9\u65af\u8c31\u9699\u7684\u6570\u5b66\u8054\u7cfb\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86\u8fc7\u5e73\u6ed1\u8870\u51cf\u7387\u4e0e\u56fe\u62c9\u666e\u62c9\u65af\u8c31\u9699\u7684\u5185\u5728\u5173\u7cfb\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u65b0\u63d0\u51fa\u7684\u5ea6\u91cf\u4e0b\u786e\u5b9e\u5b58\u5728\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u9ad8\u9636\u5ea6\u91cf\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8fc7\u5e73\u6ed1\u73b0\u8c61\uff0c\u4e3a\u7406\u89e3\u548c\u6539\u8fdbGNN\u7684\u5e73\u6ed1\u7279\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u548c\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2512.06612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06612", "abs": "https://arxiv.org/abs/2512.06612", "authors": ["Kazuya Nishimura", "Haruka Hirose", "Ryoma Bise", "Kaito Shiku", "Yasuhiro Kojima"], "title": "Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics", "comment": "Neurips 2025", "summary": "Gene expression estimation from pathology images has the potential to reduce the RNA sequencing cost. Point-wise loss functions have been widely used to minimize the discrepancy between predicted and absolute gene expression values. However, due to the complexity of the sequencing techniques and intrinsic variability across cells, the observed gene expression contains stochastic noise and batch effects, and estimating the absolute expression values accurately remains a significant challenge. To mitigate this, we propose a novel objective of learning relative expression patterns rather than absolute levels. We assume that the relative expression levels of genes exhibit consistent patterns across independent experiments, even when absolute expression values are affected by batch effects and stochastic noise in tissue samples. Based on the assumption, we model the relation and propose a novel loss function called STRank that is robust to noise and batch effects. Experiments using synthetic datasets and real datasets demonstrate the effectiveness of the proposed method. The code is available at https://github.com/naivete5656/STRank.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u51fd\u6570STRank\uff0c\u7528\u4e8e\u4ece\u75c5\u7406\u56fe\u50cf\u4e2d\u5b66\u4e60\u57fa\u56e0\u7684\u76f8\u5bf9\u8868\u8fbe\u6a21\u5f0f\u800c\u975e\u7edd\u5bf9\u8868\u8fbe\u503c\uff0c\u4ee5\u89e3\u51b3\u57fa\u56e0\u8868\u8fbe\u4f30\u8ba1\u4e2d\u7684\u968f\u673a\u566a\u58f0\u548c\u6279\u6b21\u6548\u5e94\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6d4b\u5e8f\u6280\u672f\u7684\u590d\u6742\u6027\u548c\u7ec6\u80de\u95f4\u7684\u5185\u5728\u53d8\u5f02\u6027\uff0c\u89c2\u5bdf\u5230\u7684\u57fa\u56e0\u8868\u8fbe\u5305\u542b\u968f\u673a\u566a\u58f0\u548c\u6279\u6b21\u6548\u5e94\uff0c\u51c6\u786e\u4f30\u8ba1\u7edd\u5bf9\u8868\u8fbe\u503c\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u5b66\u4e60\u76f8\u5bf9\u8868\u8fbe\u6a21\u5f0f\u3002", "method": "\u57fa\u4e8e\u76f8\u5bf9\u8868\u8fbe\u6c34\u5e73\u5728\u72ec\u7acb\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4e00\u81f4\u6027\u7684\u5047\u8bbe\uff0c\u63d0\u51faSTRank\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u5efa\u6a21\u57fa\u56e0\u95f4\u5173\u7cfb\u6765\u5b66\u4e60\u76f8\u5bf9\u8868\u8fbe\u6a21\u5f0f\uff0c\u800c\u975e\u7edd\u5bf9\u6570\u503c\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u566a\u58f0\u548c\u6279\u6b21\u6548\u5e94\u7684\u5f71\u54cd\u3002", "conclusion": "STRank\u65b9\u6cd5\u901a\u8fc7\u5173\u6ce8\u76f8\u5bf9\u8868\u8fbe\u6a21\u5f0f\u800c\u975e\u7edd\u5bf9\u6570\u503c\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u57fa\u56e0\u8868\u8fbe\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.07756", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.07756", "abs": "https://arxiv.org/abs/2512.07756", "authors": ["Mayank Anand", "Ujair Alam", "Surya Prakash", "Priya Shukla", "Gora Chand Nandi", "Domenec Puig"], "title": "UltrasODM: A Dual Stream Optical Flow Mamba Network for 3D Freehand Ultrasound Reconstruction", "comment": null, "summary": "Clinical ultrasound acquisition is highly operator-dependent, where rapid probe motion and brightness fluctuations often lead to reconstruction errors that reduce trust and clinical utility. We present UltrasODM, a dual-stream framework that assists sonographers during acquisition through calibrated per-frame uncertainty, saliency-based diagnostics, and actionable prompts. UltrasODM integrates (i) a contrastive ranking module that groups frames by motion similarity, (ii) an optical-flow stream fused with Dual-Mamba temporal modules for robust 6-DoF pose estimation, and (iii) a Human-in-the-Loop (HITL) layer combining Bayesian uncertainty, clinician-calibrated thresholds, and saliency maps highlighting regions of low confidence. When uncertainty exceeds the threshold, the system issues unobtrusive alerts suggesting corrective actions such as re-scanning highlighted regions or slowing the sweep. Evaluated on a clinical freehand ultrasound dataset, UltrasODM reduces drift by 15.2%, distance error by 12.1%, and Hausdorff distance by 10.1% relative to UltrasOM, while producing per-frame uncertainty and saliency outputs. By emphasizing transparency and clinician feedback, UltrasODM improves reconstruction reliability and supports safer, more trustworthy clinical workflows. Our code is publicly available at https://github.com/AnandMayank/UltrasODM.", "AI": {"tldr": "UltrasODM\u662f\u4e00\u4e2a\u53cc\u6d41\u6846\u67b6\uff0c\u901a\u8fc7\u6821\u51c6\u7684\u6bcf\u5e27\u4e0d\u786e\u5b9a\u6027\u3001\u57fa\u4e8e\u663e\u8457\u6027\u7684\u8bca\u65ad\u548c\u53ef\u64cd\u4f5c\u7684\u63d0\u793a\u6765\u8f85\u52a9\u8d85\u58f0\u533b\u5e08\u5728\u91c7\u96c6\u8fc7\u7a0b\u4e2d\uff0c\u51cf\u5c11\u91cd\u5efa\u8bef\u5dee\u5e76\u63d0\u9ad8\u4e34\u5e8a\u5b9e\u7528\u6027\u3002", "motivation": "\u4e34\u5e8a\u8d85\u58f0\u91c7\u96c6\u9ad8\u5ea6\u4f9d\u8d56\u64cd\u4f5c\u8005\uff0c\u5feb\u901f\u63a2\u5934\u8fd0\u52a8\u548c\u4eae\u5ea6\u6ce2\u52a8\u5e38\u5bfc\u81f4\u91cd\u5efa\u8bef\u5dee\uff0c\u964d\u4f4e\u4fe1\u4efb\u5ea6\u548c\u4e34\u5e8a\u6548\u7528\u3002", "method": "UltrasODM\u6574\u5408\u4e86\u4e09\u4e2a\u6a21\u5757\uff1a(i)\u57fa\u4e8e\u8fd0\u52a8\u76f8\u4f3c\u6027\u7684\u5bf9\u6bd4\u6392\u5e8f\u6a21\u5757\uff1b(ii)\u878d\u5408\u53ccMamba\u65f6\u5e8f\u6a21\u5757\u7684\u5149\u6d41\u6d41\u7528\u4e8e\u7a33\u5065\u76846-DoF\u59ff\u6001\u4f30\u8ba1\uff1b(iii)\u7ed3\u5408\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u3001\u4e34\u5e8a\u6821\u51c6\u9608\u503c\u548c\u663e\u8457\u6027\u56fe\u7684HITL\u5c42\u3002", "result": "\u5728\u4e34\u5e8a\u81ea\u7531\u624b\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cUltrasODM\u76f8\u5bf9\u4e8eUltrasOM\u5c06\u6f02\u79fb\u51cf\u5c1115.2%\uff0c\u8ddd\u79bb\u8bef\u5dee\u51cf\u5c1112.1%\uff0cHausdorff\u8ddd\u79bb\u51cf\u5c1110.1%\uff0c\u540c\u65f6\u751f\u6210\u6bcf\u5e27\u4e0d\u786e\u5b9a\u6027\u548c\u663e\u8457\u6027\u8f93\u51fa\u3002", "conclusion": "UltrasODM\u901a\u8fc7\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u533b\u751f\u53cd\u9988\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u53ef\u9760\u6027\uff0c\u652f\u6301\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u4fe1\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2512.06785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06785", "abs": "https://arxiv.org/abs/2512.06785", "authors": ["Vasileios Sevetlidis", "George Pavlidis", "Antonios Gasteratos"], "title": "Angular Regularization for Positive-Unlabeled Learning on the Hypersphere", "comment": "Featured Certification, J2C Certification. Transactions on Machine Learning Research, 2025", "summary": "Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.", "AI": {"tldr": "AngularPU\u662f\u4e00\u79cd\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u89d2\u5ea6\u95f4\u9694\u7684PU\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u539f\u578b\u5411\u91cf\u8868\u793a\u6b63\u7c7b\uff0c\u65e0\u9700\u663e\u5f0f\u8d1f\u7c7b\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u89d2\u5ea6\u6b63\u5219\u5316\u5668\u6765\u6539\u5584\u672a\u6807\u8bb0\u6570\u636e\u7684\u5206\u5e03\u5206\u79bb\u3002", "motivation": "\u73b0\u6709PU\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5f3a\u5206\u5e03\u5047\u8bbe\uff0c\u8981\u4e48\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u5bb9\u6613\u5d29\u6e83\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u4e14\u65e0\u9700\u663e\u5f0f\u8d1f\u7c7b\u5efa\u6a21\u7684\u65b9\u6cd5\u3002", "method": "\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u89d2\u5ea6\u95f4\u9694\uff0c\u6b63\u7c7b\u7531\u53ef\u5b66\u4e60\u7684\u539f\u578b\u5411\u91cf\u8868\u793a\uff0c\u5206\u7c7b\u7b80\u5316\u4e3a\u8ba1\u7b97\u5d4c\u5165\u4e0e\u539f\u578b\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u9608\u503c\u3002\u5f15\u5165\u89d2\u5ea6\u6b63\u5219\u5316\u5668\u4fc3\u4f7f\u672a\u6807\u8bb0\u5d4c\u5165\u5728\u8d85\u7403\u9762\u4e0a\u5206\u6563\uff0c\u6539\u5584\u5206\u79bb\u6548\u679c\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAngularPU\u5728\u6b63\u6837\u672c\u7a00\u7f3a\u548c\u9ad8\u7ef4\u5d4c\u5165\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709PU\u65b9\u6cd5\uff0c\u5177\u6709\u51e0\u4f55\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "AngularPU\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u8d1f\u7c7b\u5efa\u6a21\u7684PU\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u4f18\u52bf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u7ef4\u548c\u6b63\u6837\u672c\u7a00\u7f3a\u7684\u573a\u666f\u3002"}}
{"id": "2512.06613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06613", "abs": "https://arxiv.org/abs/2512.06613", "authors": ["Yueying Ke"], "title": "Hierarchical Deep Learning for Diatom Image Classification: A Multi-Level Taxonomic Approach", "comment": "10 pages, 6 figures, 2 tables, IEEE conference format. Submitted as course project", "summary": "Accurate taxonomic identification of diatoms is essential for aquatic ecosystem monitoring, yet conventional methods depend heavily on expert taxonomists. Recent deep learning approaches improve automation, but most treat diatom recognition as flat classification predicting only one taxonomic rank. We investigate whether embedding taxonomic hierarchy into neural network architectures can improve both accuracy and error locality.\n  We introduce a hierarchical convolutional network with five cascaded heads that jointly predict class, order, family, genus, and species. Each head receives shared backbone features and probability distributions from higher levels, with binary masks restricting predictions to valid descendants during training and inference. Using a filtered dataset of 1,456 diatom images covering 82 species, we compare hierarchical and flat models under identical settings.\n  The hierarchical model matches flat baselines at species level (69.4% accuracy) while outperforming at all upper taxonomic levels. When species predictions fail, errors remain taxonomically local: 92.5 % of misclassified species are correctly predicted at genus level, versus 67.2% for flat baselines. The hierarchical model reduces mean taxonomic distance by 38.2% (1.209 vs. 1.955).\n  Progressive training reveals bidirectional mechanisms: hierarchical constraint masks operate top-down to constrain prediction space, while gradients from fine-grained levels propagate bottom-up through the shared backbone, refining features. This improves class accuracy from 96.2% to 99.5% and yields 6-8% gains at upper levels, producing more robust, interpretable, and biologically aligned predictions for multi-level taxonomic classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5377\u79ef\u7f51\u7edc\uff0c\u901a\u8fc7\u5c06\u5206\u7c7b\u5b66\u5c42\u6b21\u7ed3\u6784\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u8054\u5408\u9884\u6d4b\u7845\u85fb\u7684\u7c7b\u522b\u3001\u76ee\u3001\u79d1\u3001\u5c5e\u548c\u79cd\uff0c\u5728\u4fdd\u6301\u7269\u79cd\u7ea7\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u4e0a\u5c42\u5206\u7c7b\u7ea7\u522b\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u9519\u8bef\u5206\u7c7b\u66f4\u5177\u751f\u7269\u5b66\u610f\u4e49\u3002", "motivation": "\u4f20\u7edf\u7845\u85fb\u5206\u7c7b\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u591a\u91c7\u7528\u5e73\u9762\u5206\u7c7b\u4ec5\u9884\u6d4b\u5355\u4e00\u5206\u7c7b\u7ea7\u522b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c06\u5206\u7c7b\u5b66\u5c42\u6b21\u7ed3\u6784\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u662f\u5426\u80fd\u540c\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9519\u8bef\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u5177\u6709\u4e94\u4e2a\u7ea7\u8054\u5934\u90e8\u7684\u5206\u5c42\u5377\u79ef\u7f51\u7edc\uff0c\u6bcf\u4e2a\u5934\u90e8\u63a5\u6536\u5171\u4eab\u9aa8\u5e72\u7279\u5f81\u548c\u6765\u81ea\u66f4\u9ad8\u5c42\u7ea7\u7684\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u7528\u4e8c\u8fdb\u5236\u63a9\u7801\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9650\u5236\u9884\u6d4b\u5230\u6709\u6548\u540e\u4ee3\u3002\u4f7f\u7528\u5305\u542b1,456\u5f20\u7845\u85fb\u56fe\u50cf\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5206\u5c42\u6a21\u578b\u5728\u7269\u79cd\u7ea7\u8fbe\u523069.4%\u51c6\u786e\u7387\uff08\u4e0e\u5e73\u9762\u57fa\u7ebf\u76f8\u5f53\uff09\uff0c\u4f46\u5728\u6240\u6709\u4e0a\u5c42\u5206\u7c7b\u7ea7\u522b\u5747\u4f18\u4e8e\u57fa\u7ebf\u300292.5%\u7684\u9519\u8bef\u7269\u79cd\u5206\u7c7b\u5728\u5c5e\u7ea7\u88ab\u6b63\u786e\u9884\u6d4b\uff08\u5e73\u9762\u57fa\u7ebf\u4e3a67.2%\uff09\uff0c\u5e73\u5747\u5206\u7c7b\u8ddd\u79bb\u51cf\u5c1138.2%\u3002", "conclusion": "\u5206\u5c42\u6a21\u578b\u901a\u8fc7\u81ea\u4e0a\u800c\u4e0b\u7684\u7ea6\u675f\u63a9\u7801\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u68af\u5ea6\u4f20\u64ad\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u4e0e\u751f\u7269\u5b66\u5bf9\u9f50\u7684\u591a\u5c42\u6b21\u5206\u7c7b\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2512.06791", "categories": ["cs.LG", "cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.06791", "abs": "https://arxiv.org/abs/2512.06791", "authors": ["Vedansh Sharma"], "title": "Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games", "comment": null, "summary": "Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faSmall-Gain Nash\uff08SGN\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5757\u52a0\u6743\u51e0\u4f55\u4e2d\u7684\u5757\u5c0f\u589e\u76ca\u6761\u4ef6\uff0c\u4e3a\u5177\u6709\u5f3a\u4ea4\u53c9\u73a9\u5bb6\u8026\u5408\u7684\u975e\u5355\u8c03\u535a\u5f08\u63d0\u4f9b\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u68af\u5ea6\u7684\u535a\u5f08\u5b66\u4e60\u6536\u655b\u6027\u5206\u6790\u9700\u8981\u4f2a\u68af\u5ea6\u5728\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u4e2d\u6ee1\u8db3\uff08\u5f3a\uff09\u5355\u8c03\u6027\u6761\u4ef6\uff0c\u4f46\u8fd9\u4e00\u6761\u4ef6\u5728\u5177\u6709\u5f3a\u4ea4\u53c9\u73a9\u5bb6\u8026\u5408\u7684\u7b80\u5355\u535a\u5f08\u4e2d\u5e38\u5e38\u4e0d\u6210\u7acb\u3002", "method": "\u5f15\u5165SGN\u65b9\u6cd5\uff0c\u5c06\u5c40\u90e8\u66f2\u7387\u548c\u4ea4\u53c9\u73a9\u5bb6Lipschitz\u8026\u5408\u8fb9\u754c\u8f6c\u6362\u4e3a\u53ef\u5904\u7406\u7684\u6536\u7f29\u8bc1\u4e66\uff0c\u6784\u5efa\u52a0\u6743\u5757\u5ea6\u91cf\u4f7f\u4f2a\u68af\u5ea6\u5728\u8be5\u5ea6\u91cf\u4e0b\u5f3a\u5355\u8c03\u3002\u4f7f\u7528\u6295\u5f71Euler\u548cRK4\u79bb\u6563\u5316\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8eSGN\u8fb9\u9645\u548c\u5c40\u90e8Lipschitz\u5e38\u6570\u63a8\u5bfc\u663e\u5f0f\u6b65\u957f\u8fb9\u754c\u3002", "result": "\u5728\u6b27\u51e0\u91cc\u5f97\u5355\u8c03\u6027\u5206\u6790\u65e0\u6cd5\u9884\u6d4b\u6536\u655b\u7684\u4e8c\u6b21\u535a\u5f08\u4e2d\uff0cSGN\u6210\u529f\u8ba4\u8bc1\u4e86\u6536\u655b\u6027\u3002\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u5230\u955c\u50cf/Fisher\u51e0\u4f55\uff0c\u7528\u4e8e\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u4e2d\u7684\u71b5\u6b63\u5219\u5316\u7b56\u7565\u68af\u5ea6\u3002", "conclusion": "SGN\u63d0\u4f9b\u4e86\u4e00\u4e2a\u79bb\u7ebf\u8ba4\u8bc1\u6d41\u7a0b\uff0c\u53ef\u5728\u7d27\u51d1\u533a\u57df\u4f30\u8ba1\u66f2\u7387\u3001\u8026\u5408\u548cLipschitz\u53c2\u6570\uff0c\u4f18\u5316\u5757\u6743\u91cd\u4ee5\u6269\u5927SGN\u8fb9\u9645\uff0c\u4e3a\u975e\u5355\u8c03\u535a\u5f08\u8fd4\u56de\u5305\u542b\u5ea6\u91cf\u3001\u6536\u7f29\u7387\u548c\u5b89\u5168\u6b65\u957f\u7684\u7ed3\u6784\u6027\u53ef\u8ba1\u7b97\u6536\u655b\u8bc1\u4e66\u3002"}}
{"id": "2512.06642", "categories": ["cs.CV", "astro-ph.CO", "astro-ph.IM", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06642", "abs": "https://arxiv.org/abs/2512.06642", "authors": ["Achmad Ardani Prasha", "Clavino Ourizqi Rachmadi", "Muhamad Fauzan Ibnu Syahlan", "Naufal Rahfi Anugerah", "Nanda Garin Raditya", "Putri Amelia", "Sabrina Laila Mutiara", "Hilman Syachr Ramadhan"], "title": "Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution", "comment": "21 pages, 7 figures, 3 table", "summary": "Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u4ece\u6a21\u62df\u7684\u5f3a\u5f15\u529b\u900f\u955c\u56fe\u50cf\u4e2d\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u8868\u793a\uff0c\u4ee5\u652f\u6301\u6697\u7269\u8d28\u6a21\u578b\u5206\u7c7b\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u5f3a\u5f15\u529b\u900f\u955c\u53ef\u4ee5\u63ed\u793a\u661f\u7cfb\u4e2d\u6697\u7269\u8d28\u5b50\u7ed3\u6784\u7684\u5f71\u54cd\uff0c\u4f46\u4ece\u566a\u58f0\u5927\u3001\u5206\u8fa8\u7387\u4f4e\u7684\u56fe\u50cf\u4e2d\u5206\u6790\u8fd9\u4e9b\u6548\u5e94\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u7269\u7406\u4e30\u5bcc\u7684\u6a21\u62df\u6570\u636e\u4e2d\u5b66\u4e60\u901a\u7528\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u4ee5\u652f\u6301\u591a\u79cd\u5206\u6790\u4efb\u52a1\u3002", "method": "\u4f7f\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08MAE\uff09\u5728DeepLense ML4SCI\u57fa\u51c6\u7684\u6a21\u62df\u5f3a\u900f\u955c\u56fe\u50cf\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u9488\u5bf9\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08\u6697\u7269\u8d28\u6a21\u578b\u5206\u7c7b\u548c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff09\u5206\u522b\u5bf9\u7f16\u7801\u5668\u8fdb\u884c\u5fae\u8c03\u3002\u901a\u8fc7\u8c03\u6574\u63a9\u7801\u6bd4\u4f8b\u6765\u4f18\u5316\u6027\u80fd\u3002", "result": "\u572890%\u63a9\u7801\u6bd4\u4f8b\u4e0b\uff0c\u5fae\u8c03\u540e\u7684\u5206\u7c7b\u5668\u8fbe\u5230\u5b8f\u89c2AUC 0.968\u548c\u51c6\u786e\u738788.65%\uff0c\u4f18\u4e8e\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec3\u7684ViT\uff08AUC 0.957\uff0c\u51c6\u786e\u738782.46%\uff09\u3002\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff0816x16\u523064x64\uff09\u4e2d\uff0cMAE\u9884\u8bad\u7ec3\u6a21\u578b\u7684PSNR\u7ea6\u4e3a33 dB\uff0cSSIM\u4e3a0.961\uff0c\u7565\u4f18\u4e8e\u4ece\u96f6\u8bad\u7ec3\u3002", "conclusion": "MAE\u9884\u8bad\u7ec3\u5728\u7269\u7406\u4e30\u5bcc\u7684\u6a21\u62df\u6570\u636e\u4e0a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u91cd\u7528\u7684\u7f16\u7801\u5668\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5f3a\u900f\u955c\u5206\u6790\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u63a9\u7801\u6bd4\u4f8b\u5728\u5206\u7c7b\u548c\u91cd\u5efa\u4efb\u52a1\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002"}}
{"id": "2512.06813", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06813", "abs": "https://arxiv.org/abs/2512.06813", "authors": ["Agung Nugraha", "Heungjun Im", "Jihwan Lee"], "title": "Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation", "comment": "19 pages, 12 figures", "summary": "High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u6df7\u51dd\u571f\u7684\u90e8\u5206\u9006\u5411\u8bbe\u8ba1\uff0c\u80fd\u591f\u5728\u5355\u4e00\u524d\u5411\u4f20\u9012\u4e2d\u751f\u6210\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\u4e14\u6027\u80fd\u4e00\u81f4\u7684\u914d\u5408\u6bd4\u8bbe\u8ba1\u3002", "motivation": "\u9ad8\u6027\u80fd\u6df7\u51dd\u571f\u9700\u8981\u590d\u6742\u7684\u914d\u5408\u6bd4\u8bbe\u8ba1\uff0c\u6d89\u53ca\u591a\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u53d8\u91cf\u548c\u5b9e\u9645\u7ea6\u675f\u3002\u867d\u7136\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u6b63\u5411\u8bbe\u8ba1\u9884\u6d4b\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u9006\u5411\u8bbe\u8ba1\uff08\u7279\u522b\u662f\u90e8\u5206\u53d8\u91cf\u56fa\u5b9a\u7684\u7ea6\u675f\u60c5\u51b5\u4e0b\uff09\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u534f\u4f5c\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u4e2a\u8026\u5408\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1a\u4e00\u4e2a\u63d2\u8865\u6a21\u578b\u63a8\u65ad\u672a\u786e\u5b9a\u53d8\u91cf\uff0c\u4e00\u4e2a\u66ff\u4ee3\u6a21\u578b\u9884\u6d4b\u6297\u538b\u5f3a\u5ea6\u3002\u901a\u8fc7\u534f\u4f5c\u5b66\u4e60\uff0c\u6a21\u578b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u7ea6\u675f\u7ec4\u5408\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e860.87-0.92\u7684\u7a33\u5b9aR\u5e73\u65b9\u503c\uff0c\u4e0e\u81ea\u7f16\u7801\u5668\u57fa\u7ebf\u76f8\u6bd4\u5e73\u5747\u51cf\u5c1150%\u7684\u5747\u65b9\u8bef\u5dee\uff0c\u4e0e\u8d1d\u53f6\u65af\u63a8\u7406\u76f8\u6bd4\u5e73\u5747\u51cf\u5c1170%\u7684\u5747\u65b9\u8bef\u5dee\u3002", "conclusion": "\u534f\u4f5c\u795e\u7ecf\u7f51\u7edc\u4e3a\u6df7\u51dd\u571f\u5de5\u7a0b\u4e2d\u7684\u7ea6\u675f\u611f\u77e5\u3001\u6570\u636e\u9a71\u52a8\u914d\u5408\u6bd4\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u7a33\u5065\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u57fa\u7840\u3002"}}
{"id": "2512.06657", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06657", "abs": "https://arxiv.org/abs/2512.06657", "authors": ["Qiyan Zhao", "Yue Yan", "Da-Han Wang"], "title": "TextMamba: Scene Text Detector with Mamba", "comment": null, "summary": "In scene text detection, Transformer-based methods have addressed the global feature extraction limitations inherent in traditional convolution neural network-based methods. However, most directly rely on native Transformer attention layers as encoders without evaluating their cross-domain limitations and inherent shortcomings: forgetting important information or focusing on irrelevant representations when modeling long-range dependencies for text detection. The recently proposed state space model Mamba has demonstrated better long-range dependencies modeling through a linear complexity selection mechanism. Therefore, we propose a novel scene text detector based on Mamba that integrates the selection mechanism with attention layers, enhancing the encoder's ability to extract relevant information from long sequences. We adopt the Top\\_k algorithm to explicitly select key information and reduce the interference of irrelevant information in Mamba modeling. Additionally, we design a dual-scale feed-forward network and an embedding pyramid enhancement module to facilitate high-dimensional hidden state interactions and multi-scale feature fusion. Our method achieves state-of-the-art or competitive performance on various benchmarks, with F-measures of 89.7\\%, 89.2\\%, and 78.5\\% on CTW1500, TotalText, and ICDAR19ArT, respectively. Codes will be available.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMamba\u7684\u573a\u666f\u6587\u672c\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u96c6\u6210\u9009\u62e9\u673a\u5236\u548c\u6ce8\u610f\u529b\u5c42\u589e\u5f3a\u957f\u5e8f\u5217\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u89e3\u51b3Transformer\u5728\u573a\u666f\u6587\u672c\u68c0\u6d4b\u4e2d\u5efa\u6a21\u957f\u8ddd\u79bb\u4f9d\u8d56\u65f6\u7684\u4fe1\u606f\u9057\u5fd8\u548c\u65e0\u5173\u8868\u5f81\u5173\u6ce8\u95ee\u9898\uff0c\u5229\u7528Mamba\u7684\u9009\u62e9\u673a\u5236\u6539\u8fdb\u7279\u5f81\u63d0\u53d6", "method": "\u96c6\u6210Mamba\u7684\u9009\u62e9\u673a\u5236\u4e0e\u6ce8\u610f\u529b\u5c42\uff0c\u91c7\u7528Top_k\u7b97\u6cd5\u9009\u62e9\u5173\u952e\u4fe1\u606f\uff0c\u8bbe\u8ba1\u53cc\u5c3a\u5ea6\u524d\u9988\u7f51\u7edc\u548c\u5d4c\u5165\u91d1\u5b57\u5854\u589e\u5f3a\u6a21\u5757\u8fdb\u884c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408", "result": "\u5728CTW1500\u3001TotalText\u548cICDAR19ArT\u4e0a\u5206\u522b\u8fbe\u523089.7%\u300189.2%\u548c78.5%\u7684F-measure", "conclusion": "\u63d0\u51fa\u7684Mamba-based\u65b9\u6cd5\u5728\u573a\u666f\u6587\u672c\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2512.06837", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06837", "abs": "https://arxiv.org/abs/2512.06837", "authors": ["Zhenhao Li", "Xu Cheng", "Yi Zhou"], "title": "Neural Factorization-based Bearing Fault Diagnosis", "comment": null, "summary": "This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u5206\u89e3\u7684\u5206\u7c7b\u6846\u67b6NFC\u7528\u4e8e\u9ad8\u94c1\u8f74\u627f\u6545\u969c\u8bca\u65ad\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u5d4c\u5165\u548c\u795e\u7ecf\u5206\u89e3\u878d\u5408\u6280\u672f\uff0c\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u7684\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u9ad8\u94c1\u8f74\u627f\u4f5c\u4e3a\u5217\u8f66\u8fd0\u884c\u7cfb\u7edf\u7684\u6838\u5fc3\u90e8\u4ef6\uff0c\u5176\u5065\u5eb7\u72b6\u51b5\u76f4\u63a5\u5f71\u54cd\u8fd0\u884c\u5b89\u5168\u3002\u4f20\u7edf\u8bca\u65ad\u65b9\u6cd5\u5728\u590d\u6742\u5de5\u51b5\u4e0b\u8bca\u65ad\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faNFC\u6846\u67b6\uff1a1\uff09\u5c06\u632f\u52a8\u65f6\u95f4\u5e8f\u5217\u5d4c\u5165\u4e3a\u591a\u4e2a\u6a21\u6001\u7279\u5f81\u5411\u91cf\u4ee5\u6355\u6349\u591a\u6837\u5316\u6545\u969c\u6a21\u5f0f\uff1b2\uff09\u5229\u7528\u795e\u7ecf\u5206\u89e3\u539f\u7406\u878d\u5408\u8fd9\u4e9b\u5411\u91cf\u4e3a\u7edf\u4e00\u632f\u52a8\u8868\u793a\u3002\u5177\u4f53\u5b9e\u73b0CP-NFC\u548cTucker-NFC\u4e24\u79cd\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCP-NFC\u548cTucker-NFC\u6a21\u578b\u5747\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u4f18\u8d8a\u7684\u8bca\u65ad\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u94c1\u8f74\u627f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bca\u65ad\u7b56\u7565\u9009\u62e9\u4f9d\u636e\uff0c\u901a\u8fc7\u6bd4\u8f83\u5206\u6790\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5b9e\u8bc1\u8bc1\u636e\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2512.06662", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06662", "abs": "https://arxiv.org/abs/2512.06662", "authors": ["Ruoyu Xue", "Hieu Le", "Jingyi Xu", "Sounak Mondal", "Abe Leite", "Gregory Zelinsky", "Minh Hoai", "Dimitris Samaras"], "title": "Personalized Image Descriptions from Attention Sequences", "comment": "10 pages, 4 figures", "summary": "People can view the same image differently: they focus on different regions, objects, and details in varying orders and describe them in distinct linguistic styles. This leads to substantial variability in image descriptions. However, existing models for personalized image description focus on linguistic style alone, with no prior work leveraging individual viewing patterns. We address this gap by explicitly modeling personalized viewing behavior as a core factor in description generation. Our method, DEPER (DEscription-PERception persona encoder), learns a subject embedding that captures both linguistic style and viewing behavior, guided by an auxiliary attention-prediction task. A lightweight adapter aligns these embeddings with a frozen vision-language model, enabling few-shot personalization without retraining. Across four datasets spanning diverse viewing tasks and both short and detailed descriptions, DEPER achieves a 24% average improvement, showing that modeling personalized attention produces more human-aligned and high-quality descriptions. We posit that understanding how people see helps predict what they say; modeling human diversity in perception can improve both performance and human alignment in multimodal systems.", "AI": {"tldr": "DEPER\u6a21\u578b\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4e2a\u6027\u5316\u89c2\u770b\u884c\u4e3a\uff08\u5305\u62ec\u89c6\u89c9\u6ce8\u610f\u529b\u548c\u8bed\u8a00\u98ce\u683c\uff09\u6765\u751f\u6210\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u56fe\u50cf\u63cf\u8ff0\uff0c\u76f8\u6bd4\u4ec5\u5173\u6ce8\u8bed\u8a00\u98ce\u683c\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u5347\u4e8624%\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u63cf\u8ff0\u4e2a\u6027\u5316\u6a21\u578b\u4ec5\u5173\u6ce8\u8bed\u8a00\u98ce\u683c\uff0c\u800c\u5ffd\u7565\u4e86\u4e0d\u540c\u4eba\u89c2\u770b\u56fe\u50cf\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u6a21\u5f0f\u5dee\u5f02\u3002\u4eba\u4eec\u89c2\u770b\u540c\u4e00\u56fe\u50cf\u65f6\u4f1a\u5173\u6ce8\u4e0d\u540c\u533a\u57df\u3001\u5bf9\u8c61\u548c\u7ec6\u8282\uff0c\u5e76\u4ee5\u4e0d\u540c\u7684\u8bed\u8a00\u98ce\u683c\u63cf\u8ff0\uff0c\u8fd9\u5bfc\u81f4\u4e86\u63cf\u8ff0\u7684\u5de8\u5927\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51faDEPER\u65b9\u6cd5\uff0c\u5b66\u4e60\u4e00\u4e2a\u4e3b\u4f53\u5d4c\u5165\u6765\u540c\u65f6\u6355\u6349\u8bed\u8a00\u98ce\u683c\u548c\u89c2\u770b\u884c\u4e3a\uff0c\u901a\u8fc7\u8f85\u52a9\u6ce8\u610f\u529b\u9884\u6d4b\u4efb\u52a1\u8fdb\u884c\u6307\u5bfc\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5c06\u8fd9\u4e9b\u5d4c\u5165\u4e0e\u51bb\u7ed3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u4e2a\u6027\u5316\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u6db5\u76d6\u4e0d\u540c\u89c2\u770b\u4efb\u52a1\u548c\u957f\u77ed\u63cf\u8ff0\u7684\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDEPER\u5b9e\u73b0\u4e8624%\u7684\u5e73\u5747\u63d0\u5347\uff0c\u8868\u660e\u5efa\u6a21\u4e2a\u6027\u5316\u6ce8\u610f\u529b\u53ef\u4ee5\u4ea7\u751f\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u63cf\u8ff0\u3002", "conclusion": "\u7406\u89e3\u4eba\u4eec\u5982\u4f55\u89c2\u770b\u56fe\u50cf\u6709\u52a9\u4e8e\u9884\u6d4b\u4ed6\u4eec\u4f1a\u8bf4\u4ec0\u4e48\uff1b\u5728\u611f\u77e5\u4e2d\u5efa\u6a21\u4eba\u7c7b\u591a\u6837\u6027\u53ef\u4ee5\u63d0\u5347\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u4eba\u7c7b\u5bf9\u9f50\u5ea6\u3002"}}
{"id": "2512.06917", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06917", "abs": "https://arxiv.org/abs/2512.06917", "authors": ["Clifford F", "Devika Jay", "Abhishek Sarkar", "Satheesh K Perepu", "Santhosh G S", "Kaushik Dey", "Balaraman Ravindran"], "title": "Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis", "comment": "Accepted at 4th Deployable AI Workshop at AAAI 2026", "summary": "As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a \"radical term\" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful \"Why this, and not that?\" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u548c\u805a\u5408\u72b6\u6001\u91cd\u8981\u6027\u5ea6\u91cf\u6765\u5bf9\u5b8c\u6574\u8f68\u8ff9\u8fdb\u884c\u6392\u540d\uff0c\u4ece\u800c\u89e3\u91ca\u667a\u80fd\u4f53\u7684\u957f\u671f\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u884c\u4e3a\u900f\u660e\u53ef\u4fe1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u5355\u6b65\u51b3\u7b56\uff0c\u800c\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u957f\u671f\u884c\u4e3a\u7684\u89e3\u91ca\u3002", "method": "\u5f15\u5165\u4e00\u4e2a\u7ed3\u5408Q\u503c\u5dee\u5f02\u548c\"\u6fc0\u8fdb\u9879\"\u7684\u65b0\u72b6\u6001\u91cd\u8981\u6027\u5ea6\u91cf\uff0c\u8be5\u6fc0\u8fdb\u9879\u6355\u6349\u667a\u80fd\u4f53\u8fbe\u5230\u76ee\u6807\u7684\u4eb2\u548c\u529b\u3002\u901a\u8fc7\u8be5\u5ea6\u91cf\u5bf9\u5b8c\u6574\u8f68\u8ff9\u8fdb\u884c\u6392\u540d\uff0c\u5e76\u4ece\u5173\u952e\u72b6\u6001\u751f\u6210\u53cd\u4e8b\u5b9e\u63a8\u6f14\u6765\u9a8c\u8bc1\u8def\u5f84\u9009\u62e9\u7684\u4f18\u8d8a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6210\u529f\u4ece\u5f02\u6784\u667a\u80fd\u4f53\u7ecf\u9a8c\u4e2d\u8bc6\u522b\u6700\u4f18\u8f68\u8ff9\uff0c\u4e14\u76f8\u6bd4\u7ecf\u5178\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u8bc6\u522b\u6700\u4f18\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\"\u4e3a\u4ec0\u4e48\u9009\u62e9\u8fd9\u6761\u8def\u5f84\u800c\u4e0d\u662f\u5176\u4ed6\"\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u89e3\u91ca\uff0c\u662f\u5b9e\u73b0\u53ef\u4fe1\u81ea\u4e3b\u7cfb\u7edf\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.06663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06663", "abs": "https://arxiv.org/abs/2512.06663", "authors": ["Yu Qi", "Yumeng Zhang", "Chenting Gong", "Xiao Tan", "Weiming Zhang", "Wei Zhang", "Jingdong Wang"], "title": "CoT4Det: A Chain-of-Thought Framework for Perception-Oriented Vision-Language Tasks", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable success in a broad range of vision-language tasks, such as general visual question answering and optical character recognition (OCR). However, their performance on perception-centric tasks -- such as object detection, semantic segmentation, and depth estimation -- remains significantly inferior to that of task-specific expert models. For example, Qwen2.5-VL-7B-Instruct achieves only 19% mAP on COCO2017 val, particularly struggling with dense scenes and small object recall. In this work, we introduce Chain-of-Thought for Detection (CoT4Det), a simple but efficient strategy that reformulates perception tasks into three interpretable steps: classification, counting, and grounding -- each more naturally aligned with the reasoning capabilities of LVLMs. Extensive experiments demonstrate that our method significantly improves perception performance without compromising general vision language capabilities. With a standard Qwen2.5-VL-7B-Instruct, CoT4Det boosts mAP from 19.0% to 33.0% on COCO2017 val and achieves competitive results across a variety of perception benchmarks, outperforming baselines by +2% on RefCOCO series and 19% on Flickr30k entities.", "AI": {"tldr": "CoT4Det\u662f\u4e00\u79cd\u5c06\u611f\u77e5\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u5206\u7c7b\u3001\u8ba1\u6570\u548c\u5b9a\u4f4d\u4e09\u4e2a\u53ef\u89e3\u91ca\u6b65\u9aa4\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u3001\u8bed\u4e49\u5206\u5272\uff09\u4e0a\u7684\u8868\u73b0\u8fdc\u4e0d\u5982\u4efb\u52a1\u4e13\u7528\u4e13\u5bb6\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u573a\u666f\u548c\u5c0f\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "method": "\u63d0\u51faChain-of-Thought for Detection (CoT4Det)\u65b9\u6cd5\uff0c\u5c06\u611f\u77e5\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u6b65\u9aa4\uff1a\u5206\u7c7b\uff08\u8bc6\u522b\u7269\u4f53\u7c7b\u522b\uff09\u3001\u8ba1\u6570\uff08\u7edf\u8ba1\u7269\u4f53\u6570\u91cf\uff09\u548c\u5b9a\u4f4d\uff08\u786e\u5b9a\u7269\u4f53\u4f4d\u7f6e\uff09\uff0c\u8fd9\u4e9b\u6b65\u9aa4\u66f4\u7b26\u5408\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u6807\u51c6Qwen2.5-VL-7B-Instruct\u6a21\u578b\u4e0a\uff0cCoT4Det\u5c06COCO2017 val\u6570\u636e\u96c6\u7684mAP\u4ece19.0%\u63d0\u5347\u81f333.0%\uff0c\u5728RefCOCO\u7cfb\u5217\u4e0a\u8d85\u8d8a\u57fa\u7ebf2%\uff0c\u5728Flickr30k entities\u4e0a\u8d85\u8d8a\u57fa\u7ebf19%\u3002", "conclusion": "CoT4Det\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u6027\u80fd\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5176\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\uff0c\u5728\u5404\u79cd\u611f\u77e5\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002"}}
{"id": "2512.06920", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06920", "abs": "https://arxiv.org/abs/2512.06920", "authors": ["Alexandr Plashchinsky"], "title": "Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models", "comment": null, "summary": "We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.", "AI": {"tldr": "PGSRM\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u6846\u67b6\uff0c\u4f7f\u7528\u7236\u6a21\u578b\u53c2\u8003\u8f93\u51fa\u4e0e\u5b50\u6a21\u578b\u751f\u6210\u8f93\u51fa\u7684\u5d4c\u5165\u4f59\u5f26\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u8bed\u4e49\u5956\u52b1\u4fe1\u53f7\uff0c\u66ff\u4ee3\u4f20\u7edf\u4e8c\u5143\u6b63\u786e\u6027\u4fe1\u53f7\u6216\u4eba\u7c7b\u504f\u597d\u6570\u636e\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8c\u5143\u6b63\u786e\u6027\u4fe1\u53f7\u3001\u4eba\u7c7b\u504f\u597d\u6570\u636e\u6216\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u3002PGSRM\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3001\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5956\u52b1\u6846\u67b6\u3002", "method": "PGSRM\u901a\u8fc7\u8ba1\u7b97\u7236\u6a21\u578b\u53c2\u8003\u8f93\u51fa\u5d4c\u5165\u4e0e\u5b50\u6a21\u578b\u751f\u6210\u8f93\u51fa\u5d4c\u5165\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6765\u751f\u6210\u5bc6\u96c6\u7684\u8bed\u4e49\u5956\u52b1\u4fe1\u53f7\uff0c\u5b8c\u5168\u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u4e0d\u4f9d\u8d56\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u6216\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2a\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPGSRM\u76f8\u6bd4\u4e8c\u5143\u5956\u52b1\u57fa\u7ebf\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u5956\u52b1\u6539\u8fdb\u548c\u66f4\u7a33\u5b9a\u7684PPO\u52a8\u6001\uff0c\u8bc1\u660e\u57fa\u4e8e\u5d4c\u5165\u7684\u8bed\u4e49\u5956\u52b1\u662fRLHF\u5f0f\u5956\u52b1\u5efa\u6a21\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "PGSRM\u4e3a\u5c0f\u578bTransformer\u6a21\u578b\u7684\u7236\u5f15\u5bfc\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u8bed\u4e49\u5956\u52b1\u65b9\u6cd5\uff0c\u65e0\u9700\u4eba\u7c7b\u6807\u6ce8\u6216\u989d\u5916\u6a21\u578b\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u8bed\u8a00\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.06673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06673", "abs": "https://arxiv.org/abs/2512.06673", "authors": ["Shida Gao", "Feng Xue", "Xiangfeng Wang", "Anlong Ming", "Teng Long", "Yihua Shao", "Haozhe Wang", "Zhaowen Lin", "Wei Wang", "Nicu Sebe"], "title": "1 + 1 > 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning", "comment": null, "summary": "Spatio-temporal grounding and reasoning aims to locate the temporal segment and spatial region of an event in a video given a user query, while also reasoning about semantics such as causality, temporal order, and action relationships. To achieve this, current MLLMs primarily treats bounding boxes as text tokens and generates them autoregressively. However, such autoregressive spatial decoding leads to very-long output sequences, causing spatial errors to accumulated over time and the localization results to progressively drift across a video. To address this, we present a Detector-Empowered Video LLM, short for DEViL, which couples a Video LLM with an open-vocabulary detector (OVD). Specifically, the MLLM and detector are connected via a reference-semantic token (RST) that distills the user query into a rich semantic representation. Unlike tokens that merely serve as spatial prompts or segmentor switches, the RST functions as both a control signal and a replacement for the OVD's text embedding, enabling end-to-end learning of both referential understanding and spatial localization. Furthermore, we propose a tube-mined temporal regularization (TTReg) within OVD, which drives the OVD to generate temporally-consistent queries for target objects, thereby ensuring effective temporal association. Experiments demonstrate that DEViL achieves strong performance across various fine-grained video understanding tasks, particularly STVG and GroundedVQA. Code will be released on https://github.com/gaostar123/DeViL.", "AI": {"tldr": "DEViL\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u7528\u8bed\u4e49\u4ee4\u724c\u5b9e\u73b0\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u56de\u5f52\u7a7a\u95f4\u89e3\u7801\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u5e76\u5728\u65f6\u7a7a\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u65f6\u7a7a\u5b9a\u4f4d\u4efb\u52a1\u65f6\uff0c\u5c06\u8fb9\u754c\u6846\u4f5c\u4e3a\u6587\u672c\u4ee4\u724c\u8fdb\u884c\u81ea\u56de\u5f52\u751f\u6210\uff0c\u5bfc\u81f4\u8f93\u51fa\u5e8f\u5217\u8fc7\u957f\u3001\u7a7a\u95f4\u8bef\u5dee\u968f\u65f6\u95f4\u7d2f\u79ef\u4ee5\u53ca\u5b9a\u4f4d\u7ed3\u679c\u6f02\u79fb\u7684\u95ee\u9898\u3002", "method": "DEViL\u6846\u67b6\u5c06\u89c6\u9891LLM\u4e0e\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff08OVD\uff09\u8026\u5408\uff0c\u901a\u8fc7\u5f15\u7528\u8bed\u4e49\u4ee4\u724c\uff08RST\uff09\u5c06\u7528\u6237\u67e5\u8be2\u8f6c\u5316\u4e3a\u4e30\u5bcc\u7684\u8bed\u4e49\u8868\u793a\uff0c\u5e76\u63d0\u51fa\u4e86\u7ba1\u9053\u6316\u6398\u65f6\u95f4\u6b63\u5219\u5316\uff08TTReg\uff09\u6765\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDEViL\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\uff08\u7279\u522b\u662fSTVG\u548cGroundedVQA\uff09\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "DEViL\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u65f6\u7a7a\u5b9a\u4f4d\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4ee3\u7801\u5c06\u5728GitHub\u4e0a\u53d1\u5e03\u3002"}}
{"id": "2512.06925", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.06925", "abs": "https://arxiv.org/abs/2512.06925", "authors": ["Aseer Al Faisal"], "title": "Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features", "comment": null, "summary": "Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RoBERTa\u8bed\u4e49\u5d4c\u5165\u548c\u624b\u5de5\u8bcd\u6c47\u7279\u5f81\u7684QR-DQN\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u7f51\u7edc\u9493\u9c7c\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u5e76\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8699.86%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u7f51\u7edc\u9493\u9c7c\u653b\u51fb\u901a\u8fc7\u6b3a\u8bc8\u6d88\u606f\u548c\u7be1\u6539\u7f51\u7ad9\u7b49\u624b\u6bb5\u6b3a\u9a97\u7528\u6237\u6cc4\u9732\u4e2a\u4eba\u4fe1\u606f\uff0c\u9020\u6210\u7ecf\u6d4e\u635f\u5931\u3002\u4f20\u7edfDQN\u65b9\u6cd5\u4ec5\u4f30\u8ba1\u5355\u4e00\u6807\u91cfQ\u503c\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u8981\u6539\u8fdb\u68c0\u6d4b\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5206\u4f4d\u6570\u56de\u5f52\u6df1\u5ea6Q\u7f51\u7edc(QR-DQN)\uff0c\u6574\u5408RoBERTa\u8bed\u4e49\u5d4c\u5165\u548c\u624b\u5de5\u8bcd\u6c47\u7279\u5f81\uff0c\u901a\u8fc7\u5efa\u6a21\u56de\u62a5\u5206\u5e03\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u4f7f\u7528\u6765\u81eaPhishTank\u3001OpenPhish\u7b49\u6e90\u7684105,000\u4e2aURL\u6570\u636e\u96c6\uff0c\u91c7\u752880/20\u7684\u8bad\u7ec3\u6d4b\u8bd5\u5212\u5206\u3002", "result": "QR-DQN\u6846\u67b6\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523099.86%\u51c6\u786e\u7387\u300199.75%\u7cbe\u786e\u7387\u300199.96%\u53ec\u56de\u7387\u548c99.85% F1\u5206\u6570\u3002\u76f8\u6bd4\u4ec5\u4f7f\u7528\u8bcd\u6c47\u7279\u5f81\u7684\u6807\u51c6DQN\uff0c\u6df7\u5408\u65b9\u6cd5\u5c06\u6cdb\u5316\u5dee\u8ddd\u4ece1.66%\u964d\u4f4e\u52300.04%\u3002\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u663e\u793a\u5e73\u5747\u51c6\u786e\u7387\u4e3a99.90%\uff0c\u6807\u51c6\u5dee0.04%\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u7f51\u7edc\u9493\u9c7c\u5a01\u80c1\uff0c\u9002\u5e94\u4e0d\u65ad\u6f14\u53d8\u7684\u653b\u51fb\u7b56\u7565\uff0c\u5e76\u5bf9\u672a\u89c1\u6570\u636e\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.06674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06674", "abs": "https://arxiv.org/abs/2512.06674", "authors": ["Songping Wang", "Rufan Qian", "Yueming Lyu", "Qinglong Liu", "Linzhuang Zou", "Jie Qin", "Songhua Liu", "Caifeng Shan"], "title": "RunawayEvil: Jailbreaking the Image-to-Video Generative Models", "comment": null, "summary": "Image-to-Video (I2V) generation synthesizes dynamic visual content from image and text inputs, providing significant creative control. However, the security of such multimodal systems, particularly their vulnerability to jailbreak attacks, remains critically underexplored. To bridge this gap, we propose RunawayEvil, the first multimodal jailbreak framework for I2V models with dynamic evolutionary capability. Built on a \"Strategy-Tactic-Action\" paradigm, our framework exhibits self-amplifying attack through three core components: (1) Strategy-Aware Command Unit that enables the attack to self-evolve its strategies through reinforcement learning-driven strategy customization and LLM-based strategy exploration; (2) Multimodal Tactical Planning Unit that generates coordinated text jailbreak instructions and image tampering guidelines based on the selected strategies; (3) Tactical Action Unit that executes and evaluates the multimodal coordinated attacks. This self-evolving architecture allows the framework to continuously adapt and intensify its attack strategies without human intervention. Extensive experiments demonstrate RunawayEvil achieves state-of-the-art attack success rates on commercial I2V models, such as Open-Sora 2.0 and CogVideoX. Specifically, RunawayEvil outperforms existing methods by 58.5 to 79 percent on COCO2017. This work provides a critical tool for vulnerability analysis of I2V models, thereby laying a foundation for more robust video generation systems.", "AI": {"tldr": "RunawayEvil\u662f\u9996\u4e2a\u9488\u5bf9\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u591a\u6a21\u6001\u8d8a\u72f1\u6846\u67b6\uff0c\u5177\u6709\u52a8\u6001\u8fdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\"\u7b56\u7565-\u6218\u672f-\u884c\u52a8\"\u8303\u5f0f\u5b9e\u73b0\u81ea\u6211\u589e\u5f3a\u653b\u51fb\uff0c\u5728\u5546\u4e1aI2V\u6a21\u578b\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u8106\u5f31\u6027\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u6f0f\u6d1e\u5206\u6790\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\"\u7b56\u7565-\u6218\u672f-\u884c\u52a8\"\u8303\u5f0f\u6784\u5efa\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u7b56\u7565\u611f\u77e5\u547d\u4ee4\u5355\u5143\uff08\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548cLLM\u5b9e\u73b0\u7b56\u7565\u81ea\u8fdb\u5316\uff09\u3001\u591a\u6a21\u6001\u6218\u672f\u89c4\u5212\u5355\u5143\uff08\u751f\u6210\u534f\u8c03\u7684\u6587\u672c\u8d8a\u72f1\u6307\u4ee4\u548c\u56fe\u50cf\u7be1\u6539\u6307\u5357\uff09\u3001\u6218\u672f\u884c\u52a8\u5355\u5143\uff08\u6267\u884c\u548c\u8bc4\u4f30\u591a\u6a21\u6001\u534f\u8c03\u653b\u51fb\uff09\u3002", "result": "\u5728Open-Sora 2.0\u548cCogVideoX\u7b49\u5546\u4e1aI2V\u6a21\u578b\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u653b\u51fb\u6210\u529f\u7387\uff0c\u5728COCO2017\u6570\u636e\u96c6\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad8\u51fa58.5%\u523079%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aI2V\u6a21\u578b\u7684\u6f0f\u6d1e\u5206\u6790\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.06926", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06926", "abs": "https://arxiv.org/abs/2512.06926", "authors": ["Salma Albelali", "Moataz Ahmed"], "title": "Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise", "comment": null, "summary": "Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u53cc\u5411LSTM\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u53d7\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u548c\u52a0\u6027\u566a\u58f0\u7684\u663e\u8457\u5f71\u54cd\uff0c\u8fc7\u957f\u5e8f\u5217\u4f1a\u589e\u52a0\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u566a\u58f0\u4f1a\u964d\u4f4e\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e24\u8005\u540c\u65f6\u5b58\u5728\u65f6\u6a21\u578b\u7a33\u5b9a\u6027\u6700\u5dee\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u8f93\u5165\u6570\u636e\u7279\u6027\uff08\u5982\u5e8f\u5217\u957f\u5ea6\u548c\u566a\u58f0\uff09\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684\u5173\u6ce8\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u53ef\u590d\u73b0\u7684\u9884\u6d4b\u6d41\u7a0b\uff0c\u5728\u4e09\u4e2a\u4e0d\u540c\u91c7\u6837\u9891\u7387\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u63a7\u5236\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8bc4\u4f30BiLSTM\u6a21\u578b\u5728\u4e0d\u540c\u8f93\u5165\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\uff1a1\uff09\u957f\u8f93\u5165\u5e8f\u5217\u663e\u8457\u589e\u52a0\u8fc7\u62df\u5408\u548c\u6570\u636e\u6cc4\u9732\u98ce\u9669\uff1b2\uff09\u52a0\u6027\u566a\u58f0\u6301\u7eed\u964d\u4f4e\u9884\u6d4b\u7cbe\u5ea6\uff1b3\uff09\u4e24\u4e2a\u56e0\u7d20\u540c\u65f6\u5b58\u5728\u65f6\u6a21\u578b\u7a33\u5b9a\u6027\u4e0b\u964d\u6700\u4e25\u91cd\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9884\u6d4b\u6d41\u7a0b\u5b58\u5728\u91cd\u8981\u5c40\u9650\u6027\uff0c\u9700\u8981\u91c7\u7528\u6570\u636e\u611f\u77e5\u7684\u8bbe\u8ba1\u7b56\u7565\u6765\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u6cdb\u5316\u7684\u9884\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2512.06684", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06684", "abs": "https://arxiv.org/abs/2512.06684", "authors": ["Yumeng He", "Zanwei Zhou", "Yekun Zheng", "Chen Liang", "Yunbo Wang", "Xiaokang Yang"], "title": "EMGauss: Continuous Slice-to-3D Reconstruction via Dynamic Gaussian Modeling in Volume Electron Microscopy", "comment": null, "summary": "Volume electron microscopy (vEM) enables nanoscale 3D imaging of biological structures but remains constrained by acquisition trade-offs, leading to anisotropic volumes with limited axial resolution. Existing deep learning methods seek to restore isotropy by leveraging lateral priors, yet their assumptions break down for morphologically anisotropic structures. We present EMGauss, a general framework for 3D reconstruction from planar scanned 2D slices with applications in vEM, which circumvents the inherent limitations of isotropy-based approaches. Our key innovation is to reframe slice-to-3D reconstruction as a 3D dynamic scene rendering problem based on Gaussian splatting, where the progression of axial slices is modeled as the temporal evolution of 2D Gaussian point clouds. To enhance fidelity in data-sparse regimes, we incorporate a Teacher-Student bootstrapping mechanism that uses high-confidence predictions on unobserved slices as pseudo-supervisory signals. Compared with diffusion- and GAN-based reconstruction methods, EMGauss substantially improves interpolation quality, enables continuous slice synthesis, and eliminates the need for large-scale pretraining. Beyond vEM, it potentially provides a generalizable slice-to-3D solution across diverse imaging domains.", "AI": {"tldr": "EMGauss\u662f\u4e00\u4e2a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u76843D\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8f74\u5411\u5207\u7247\u5e8f\u5217\u5efa\u6a21\u4e3a2D\u9ad8\u65af\u70b9\u4e91\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u89e3\u51b3\u4e86\u4f53\u79ef\u7535\u5b50\u663e\u5fae\u955c\u4e2d\u5404\u9879\u5f02\u6027\u7ed3\u6784\u7684\u91cd\u5efa\u95ee\u9898\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u8fde\u7eed\u5207\u7247\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u57fa\u4e8e\u5404\u9879\u540c\u6027\u5047\u8bbe\uff0c\u4f46\u5728\u5f62\u6001\u5b66\u4e0a\u5404\u9879\u5f02\u6027\u7684\u751f\u7269\u7ed3\u6784\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u4f53\u79ef\u7535\u5b50\u663e\u5fae\u955c\u5b58\u5728\u91c7\u96c6\u6743\u8861\uff0c\u5bfc\u81f4\u8f74\u5411\u5206\u8fa8\u7387\u6709\u9650\u7684\u5404\u9879\u5f02\u6027\u4f53\u79ef\uff0c\u9700\u8981\u65b0\u7684\u91cd\u5efa\u65b9\u6cd5\u3002", "method": "\u5c06\u5207\u7247\u52303D\u91cd\u5efa\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u76843D\u52a8\u6001\u573a\u666f\u6e32\u67d3\u95ee\u9898\uff0c\u4f7f\u7528\u6559\u5e08-\u5b66\u751f\u81ea\u4e3e\u673a\u5236\u5728\u6570\u636e\u7a00\u758f\u533a\u57df\u589e\u5f3a\u4fdd\u771f\u5ea6\uff0c\u5229\u7528\u672a\u89c2\u6d4b\u5207\u7247\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u4f5c\u4e3a\u4f2a\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u4e0e\u57fa\u4e8e\u6269\u6563\u548cGAN\u7684\u91cd\u5efa\u65b9\u6cd5\u76f8\u6bd4\uff0cEMGauss\u663e\u8457\u63d0\u9ad8\u4e86\u63d2\u503c\u8d28\u91cf\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u5207\u7247\u5408\u6210\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3002", "conclusion": "EMGauss\u4e0d\u4ec5\u89e3\u51b3\u4e86vEM\u4e2d\u7684\u5404\u9879\u5f02\u6027\u91cd\u5efa\u95ee\u9898\uff0c\u8fd8\u4e3a\u8de8\u4e0d\u540c\u6210\u50cf\u9886\u57df\u7684\u5207\u7247\u52303D\u91cd\u5efa\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06929", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06929", "abs": "https://arxiv.org/abs/2512.06929", "authors": ["MinCheol Jeon"], "title": "Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding", "comment": null, "summary": "Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.", "AI": {"tldr": "AdaMamba\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u3001\u591a\u5c3a\u5ea6\u8d8b\u52bf\u63d0\u53d6\u548c\u4e0a\u4e0b\u6587\u5e8f\u5217\u5efa\u6a21\u6765\u89e3\u51b3\u975e\u5e73\u7a33\u6027\u3001\u591a\u5c3a\u5ea6\u65f6\u95f4\u6a21\u5f0f\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5728\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edfTransformer\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u975e\u5e73\u7a33\u6027\u3001\u591a\u5c3a\u5ea6\u65f6\u95f4\u6a21\u5f0f\u548c\u5206\u5e03\u504f\u79fb\u7b49\u6311\u6218\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f1a\u964d\u4f4e\u6a21\u578b\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "1. \u81ea\u9002\u5e94\u5f52\u4e00\u5316\u5757\uff1a\u901a\u8fc7\u591a\u5c3a\u5ea6\u5377\u79ef\u8d8b\u52bf\u63d0\u53d6\u548c\u901a\u9053\u91cd\u6821\u51c6\u53bb\u9664\u975e\u5e73\u7a33\u6210\u5206\uff1b2. \u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff1a\u7ed3\u5408\u8865\u4e01\u5d4c\u5165\u3001\u4f4d\u7f6e\u7f16\u7801\u548cMamba\u589e\u5f3a\u7684Transformer\u5c42\uff1b3. \u8f7b\u91cf\u7ea7\u9884\u6d4b\u5934\u751f\u6210\u591a\u6b65\u9884\u6d4b\uff1b4. \u53cd\u5f52\u4e00\u5316\u673a\u5236\u901a\u8fc7\u91cd\u65b0\u6574\u5408\u5c40\u90e8\u8d8b\u52bf\u786e\u4fdd\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cAdaMamba\u5728\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u76f8\u6bd4\u4f20\u7edfTransformer\u57fa\u7ebf\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u534f\u53d8\u91cf\u504f\u79fb\u5e76\u589e\u5f3a\u4e86\u9884\u6d4b\u53ef\u9760\u6027\u3002", "conclusion": "AdaMamba\u901a\u8fc7\u81ea\u9002\u5e94\u5f52\u4e00\u5316\u548c\u4e13\u5bb6\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u4e3a\u5f02\u6784\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5177\u6709\u6a21\u5757\u5316\u53ef\u6269\u5c55\u6027\uff0c\u652f\u6301\u786e\u5b9a\u6027\u9884\u6d4b\u548c\u6982\u7387\u6269\u5c55\u3002"}}
{"id": "2512.06689", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2512.06689", "abs": "https://arxiv.org/abs/2512.06689", "authors": ["Jisoo Park", "Seonghak Lee", "Guisik Kim", "Taewoo Kim", "Junseok Kwon"], "title": "Lightweight Wasserstein Audio-Visual Model for Unified Speech Enhancement and Separation", "comment": "Accepted to ASRU 2025", "summary": "Speech Enhancement (SE) and Speech Separation (SS) have traditionally been treated as distinct tasks in speech processing. However, real-world audio often involves both background noise and overlapping speakers, motivating the need for a unified solution. While recent approaches have attempted to integrate SE and SS within multi-stage architectures, these approaches typically involve complex, parameter-heavy models and rely on supervised training, limiting scalability and generalization. In this work, we propose UniVoiceLite, a lightweight and unsupervised audio-visual framework that unifies SE and SS within a single model. UniVoiceLite leverages lip motion and facial identity cues to guide speech extraction and employs Wasserstein distance regularization to stabilize the latent space without requiring paired noisy-clean data. Experimental results demonstrate that UniVoiceLite achieves strong performance in both noisy and multi-speaker scenarios, combining efficiency with robust generalization. The source code is available at https://github.com/jisoo-o/UniVoiceLite.", "AI": {"tldr": "UniVoiceLite\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u65e0\u76d1\u7763\u97f3\u89c6\u9891\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u8bed\u97f3\u589e\u5f3a\u548c\u8bed\u97f3\u5206\u79bb\u4efb\u52a1\uff0c\u5229\u7528\u5507\u90e8\u8fd0\u52a8\u548c\u9762\u90e8\u8eab\u4efd\u7279\u5f81\uff0c\u65e0\u9700\u914d\u5bf9\u566a\u58f0-\u5e72\u51c0\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u7a33\u5b9a\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u97f3\u9891\u901a\u5e38\u540c\u65f6\u5305\u542b\u80cc\u666f\u566a\u58f0\u548c\u91cd\u53e0\u8bf4\u8bdd\u4eba\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u8bed\u97f3\u589e\u5f3a\u548c\u8bed\u97f3\u5206\u79bb\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u53c2\u6570\u590d\u6742\u7684\u591a\u9636\u6bb5\u67b6\u6784\uff0c\u4f9d\u8d56\u76d1\u7763\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faUniVoiceLite\u6846\u67b6\uff0c\u5229\u7528\u5507\u90e8\u8fd0\u52a8\u548c\u9762\u90e8\u8eab\u4efd\u7279\u5f81\u5f15\u5bfc\u8bed\u97f3\u63d0\u53d6\uff0c\u91c7\u7528Wasserstein\u8ddd\u79bb\u6b63\u5219\u5316\u7a33\u5b9a\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eUniVoiceLite\u5728\u566a\u58f0\u548c\u591a\u8bf4\u8bdd\u4eba\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u5b9e\u73b0\u4e86\u8bed\u97f3\u589e\u5f3a\u548c\u8bed\u97f3\u5206\u79bb\u7684\u7edf\u4e00\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u76d1\u7763\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.06932", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06932", "abs": "https://arxiv.org/abs/2512.06932", "authors": ["Salma Albelali", "Moataz Ahmed"], "title": "Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies", "comment": null, "summary": "Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u636e\u6cc4\u6f0f\u5bf9LSTM\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u9a8c\u8bc1\u65b9\u6cd5\u7684\u9009\u62e9\u3001\u8f93\u5165\u7a97\u53e3\u5927\u5c0f\u548c\u6ede\u540e\u6b65\u957f\u90fd\u4f1a\u663e\u8457\u5f71\u54cd\u6cc4\u6f0f\u654f\u611f\u6027\uff0c\u5176\u4e2d10\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6700\u5bb9\u6613\u53d7\u6cc4\u6f0f\u5f71\u54cd\uff0c\u800c2-way\u548c3-way\u5206\u5272\u66f4\u7a33\u5065\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u8bc4\u4f30\u5b8c\u6574\u6027\u5e38\u56e0\u6570\u636e\u6cc4\u6f0f\u800c\u53d7\u635f\u3002\u6570\u636e\u6cc4\u6f0f\u6307\u5728\u6570\u636e\u96c6\u5212\u5206\u524d\u6784\u5efa\u8f93\u5165\u8f93\u51fa\u5e8f\u5217\uff0c\u5bfc\u81f4\u672a\u6765\u4fe1\u606f\u65e0\u610f\u4e2d\u5f71\u54cd\u8bad\u7ec3\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u6570\u636e\u6cc4\u6f0f\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u9a8c\u8bc1\u65b9\u6cd5\u5bf9\u6cc4\u6f0f\u654f\u611f\u6027\u7684\u8c03\u8282\u4f5c\u7528\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u5e38\u7528\u9a8c\u8bc1\u6280\u672f\uff082-way\u5206\u5272\u30013-way\u5206\u5272\u548c10\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff09\u5728\u6cc4\u6f0f\uff08\u9884\u5206\u5272\u5e8f\u5217\u751f\u6210\uff09\u548c\u6e05\u6d01\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002\u6e05\u6d01\u6761\u4ef6\u901a\u8fc7\u5728\u6570\u636e\u5206\u5272\u540e\u6784\u5efa\u5e8f\u5217\u6765\u51cf\u8f7b\u6cc4\u6f0f\u98ce\u9669\u3002\u4f7f\u7528RMSE\u589e\u76ca\uff08\u6cc4\u6f0f\u4e0e\u6e05\u6d01\u8bbe\u7f6e\u7684RMSE\u767e\u5206\u6bd4\u5dee\u5f02\uff09\u6765\u8bc4\u4f30\u6cc4\u6f0f\u5f71\u54cd\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c10\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5728\u8f83\u957f\u6ede\u540e\u6b65\u957f\u4e0bRMSE\u589e\u76ca\u9ad8\u8fbe20.5%\uff0c\u800c2-way\u548c3-way\u5206\u5272\u66f4\u7a33\u5065\uff0cRMSE\u589e\u76ca\u901a\u5e38\u4fdd\u6301\u57285%\u4ee5\u4e0b\u3002\u8f83\u5c0f\u7684\u8f93\u5165\u7a97\u53e3\u548c\u8f83\u957f\u7684\u6ede\u540e\u6b65\u957f\u4f1a\u589e\u52a0\u6cc4\u6f0f\u98ce\u9669\uff0c\u800c\u8f83\u5927\u7684\u7a97\u53e3\u6709\u52a9\u4e8e\u51cf\u5c11\u6cc4\u6f0f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u914d\u7f6e\u611f\u77e5\u3001\u6297\u6cc4\u6f0f\u7684\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4ee5\u786e\u4fdd\u6027\u80fd\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002\u9a8c\u8bc1\u65b9\u6cd5\u7684\u9009\u62e9\u3001\u7a97\u53e3\u5927\u5c0f\u548c\u6ede\u540e\u6b65\u957f\u914d\u7f6e\u90fd\u5bf9\u9632\u6b62\u6570\u636e\u6cc4\u6f0f\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2512.06726", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06726", "abs": "https://arxiv.org/abs/2512.06726", "authors": ["Shuo Li", "Jiajun Sun", "Zhihao Zhang", "Xiaoran Fan", "Senjie Jin", "Hui Li", "Yuming Yang", "Junjie Ye", "Lixing Shen", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "The Role of Entropy in Visual Grounding: Analysis and Optimization", "comment": null, "summary": "Recent advances in fine-tuning multimodal large language models (MLLMs) using reinforcement learning have achieved remarkable progress, particularly with the introduction of various entropy control techniques. However, the role and characteristics of entropy in perception-oriented tasks like visual grounding, as well as effective strategies for controlling it, remain largely unexplored. To address this issue, we focus on the visual grounding task and analyze the role and characteristics of entropy in comparison to reasoning tasks. Building on these findings, we introduce ECVGPO (Entropy Control Visual Grounding Policy Optimization), an interpretable algorithm designed for effective entropy regulation. Through entropy control, the trade-off between exploration and exploitation is better balanced. Experiments show that ECVGPO achieves broad improvements across various benchmarks and models.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u71b5\u7684\u4f5c\u7528\u4e0e\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86ECVGPO\u7b97\u6cd5\u8fdb\u884c\u6709\u6548\u71b5\u8c03\u63a7\uff0c\u5b9e\u73b0\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u66f4\u597d\u5e73\u8861\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5e7f\u6cdb\u6539\u8fdb\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u5728\u611f\u77e5\u5bfc\u5411\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u5b9a\u4f4d\uff09\u4e2d\uff0c\u71b5\u7684\u4f5c\u7528\u7279\u6027\u548c\u6709\u6548\u63a7\u5236\u7b56\u7565\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faECVGPO\uff08\u71b5\u63a7\u5236\u89c6\u89c9\u5b9a\u4f4d\u7b56\u7565\u4f18\u5316\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u71b5\u63a7\u5236\u6765\u66f4\u597d\u5730\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660eECVGPO\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u4e0a\u90fd\u53d6\u5f97\u4e86\u5e7f\u6cdb\u7684\u6539\u8fdb\u3002", "conclusion": "ECVGPO\u7b97\u6cd5\u901a\u8fc7\u6709\u6548\u7684\u71b5\u8c03\u63a7\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2512.06944", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.06944", "abs": "https://arxiv.org/abs/2512.06944", "authors": ["Munshi Mahbubur Rahman", "Shimei Pan", "James R. Foulds"], "title": "A Unifying Human-Centered AI Fairness Framework", "comment": null, "summary": "The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u4eba\u7c7b\u4e2d\u5fc3\u516c\u5e73\u6027\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u5730\u6574\u5408\u4e86\u516b\u79cd\u4e0d\u540c\u7684\u516c\u5e73\u6027\u6307\u6807\uff0c\u5e2e\u52a9\u5229\u76ca\u76f8\u5173\u8005\u6839\u636e\u81ea\u8eab\u4ef7\u503c\u89c2\u548c\u60c5\u5883\u9009\u62e9\u9002\u5f53\u7684\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u3002", "motivation": "AI\u5728\u5173\u952e\u793e\u4f1a\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u52a0\u5267\u4e86\u516c\u5e73\u6027\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u4e0d\u540c\u516c\u5e73\u6027\u6982\u5ff5\u4e0e\u9884\u6d4b\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u963b\u788d\u4e86\u516c\u5e73AI\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u516c\u5e73\u6027\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u4e2a\u4f53\u4e0e\u7fa4\u4f53\u516c\u5e73\u6027\u3001\u8fb9\u9645\u5185\u4e0e\u4ea4\u53c9\u6027\u5047\u8bbe\u3001\u7ed3\u679c\u5bfc\u5411\u4e0e\u673a\u4f1a\u5747\u7b49\u89c6\u89d2\uff0c\u4f7f\u7528\u4e00\u81f4\u7684\u516c\u5f0f\u5316\u8868\u8fbe\uff0c\u5141\u8bb8\u5229\u76ca\u76f8\u5173\u8005\u4e3a\u591a\u4e2a\u516c\u5e73\u6027\u76ee\u6807\u5206\u914d\u6743\u91cd\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\uff08UCI\u6210\u4eba\u666e\u67e5\u3001COMPAS\u3001\u5fb7\u56fd\u4fe1\u8d37\u3001MEPS\uff09\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0c\u8c03\u6574\u6743\u91cd\u53ef\u4ee5\u63ed\u793a\u4e0d\u540c\u516c\u5e73\u6027\u6307\u6807\u4e4b\u95f4\u7684\u7ec6\u5fae\u6743\u8861\u3002\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u53f8\u6cd5\u51b3\u7b56\u548c\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u516c\u5e73AI\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u4ef7\u503c\u654f\u611f\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u5229\u76ca\u76f8\u5173\u8005\u59a5\u534f\uff0c\u4fc3\u8fdb\u516c\u5e73AI\u7cfb\u7edf\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u3002"}}
{"id": "2512.06736", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06736", "abs": "https://arxiv.org/abs/2512.06736", "authors": ["Jiaxing Fan", "Jiaojiao Liu", "Wenkong Wang", "Yang Zhang", "Xin Ma", "Jichen Zhang"], "title": "Graph Convolutional Long Short-Term Memory Attention Network for Post-Stroke Compensatory Movement Detection Based on Skeleton Data", "comment": null, "summary": "Most stroke patients experience upper limb motor dysfunction. Compensatory movements are prevalent during rehabilitation training, which is detrimental to patients' long-term recovery. Therefore, detecting compensatory movements is of great significance. In this study, a Graph Convolutional Long Short-Term Memory Attention Network (GCN-LSTM-ATT) based on skeleton data is proposed for the detection of compensatory movements after stroke. Sixteen stroke patients were selected in the research. The skeleton data of the patients performing specific rehabilitation movements were collected using the Kinect depth camera. After data processing, detection models were constructed respectively using the GCN-LSTM-ATT model, the Support Vector Machine(SVM), the K-Nearest Neighbor algorithm(KNN), and the Random Forest(RF). The results show that the detection accuracy of the GCN-LSTM-ATT model reaches 0.8580, which is significantly higher than that of traditional machine learning algorithms. Ablation experiments indicate that each component of the model contributes significantly to the performance improvement. These findings provide a more precise and powerful tool for the detection of compensatory movements after stroke, and are expected to facilitate the optimization of rehabilitation training strategies for stroke patients.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9aa8\u9abc\u6570\u636e\u7684\u56fe\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u6ce8\u610f\u529b\u7f51\u7edc\uff08GCN-LSTM-ATT\uff09\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e2d\u98ce\u60a3\u8005\u7684\u4ee3\u507f\u6027\u8fd0\u52a8\uff0c\u5176\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u52300.8580\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u5927\u591a\u6570\u4e2d\u98ce\u60a3\u8005\u5b58\u5728\u4e0a\u80a2\u8fd0\u52a8\u529f\u80fd\u969c\u788d\uff0c\u5eb7\u590d\u8bad\u7ec3\u4e2d\u666e\u904d\u5b58\u5728\u4ee3\u507f\u6027\u8fd0\u52a8\uff0c\u8fd9\u5bf9\u60a3\u8005\u7684\u957f\u671f\u6062\u590d\u4e0d\u5229\uff0c\u56e0\u6b64\u68c0\u6d4b\u4ee3\u507f\u6027\u8fd0\u52a8\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f7f\u7528Kinect\u6df1\u5ea6\u76f8\u673a\u91c7\u96c616\u540d\u4e2d\u98ce\u60a3\u8005\u6267\u884c\u7279\u5b9a\u5eb7\u590d\u52a8\u4f5c\u7684\u9aa8\u9abc\u6570\u636e\uff0c\u6784\u5efaGCN-LSTM-ATT\u6a21\u578b\uff0c\u5e76\u4e0e\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u3001K\u8fd1\u90bb\u7b97\u6cd5\uff08KNN\uff09\u548c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "GCN-LSTM-ATT\u6a21\u578b\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u52300.8580\uff0c\u663e\u8457\u9ad8\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u7684\u6bcf\u4e2a\u7ec4\u4ef6\u90fd\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e2d\u98ce\u540e\u4ee3\u507f\u6027\u8fd0\u52a8\u7684\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u548c\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u6709\u671b\u4fc3\u8fdb\u4e2d\u98ce\u60a3\u8005\u5eb7\u590d\u8bad\u7ec3\u7b56\u7565\u7684\u4f18\u5316\u3002"}}
{"id": "2512.06969", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06969", "abs": "https://arxiv.org/abs/2512.06969", "authors": ["Adrian Przybysz", "Miko\u0142aj Ko\u0142ek", "Franciszek Sobota", "Jarek Duda"], "title": "Comparing BFGS and OGR for Second-Order Optimization", "comment": null, "summary": "Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86BFGS\u65b9\u6cd5\u4e2d\u7684Sherman-Morrison\u66f4\u65b0\u4e0e\u65b0\u578bOnline Gradient Regression\uff08OGR\uff09\u65b9\u6cd5\u5728Hessian\u77e9\u9635\u4f30\u8ba1\u4e2d\u7684\u8868\u73b0\uff0cOGR\u5728\u975e\u51f8\u4f18\u5316\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684Hessian\u77e9\u9635\u4f30\u8ba1\u9762\u4e34\u9ad8\u7ef4\u5ea6\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5982BFGS\u5728\u975e\u51f8\u4f18\u5316\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "OGR\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u5bf9\u68af\u5ea6\u76f8\u5bf9\u4e8e\u4f4d\u7f6e\u8fdb\u884c\u56de\u5f52\uff0c\u5728\u7ebf\u4f30\u8ba1\u4e8c\u9636\u5bfc\u6570\u800c\u65e0\u9700Hessian\u77e9\u9635\u6c42\u9006\uff0c\u80fd\u591f\u5904\u7406\u975e\u51f8\u7ed3\u6784\u3002", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u51fd\u6570\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cOGR\u6bd4BFGS\u6536\u655b\u66f4\u5feb\u4e14\u635f\u5931\u66f4\u4f4e\uff0c\u7279\u522b\u662f\u5728\u975e\u51f8\u8bbe\u7f6e\u4e2d\u3002", "conclusion": "OGR\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7075\u6d3b\u6709\u6548\u7684Hessian\u4f30\u8ba1\u65b9\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002"}}
{"id": "2512.06738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06738", "abs": "https://arxiv.org/abs/2512.06738", "authors": ["M Yashwanth", "Sampath Koti", "Arunabh Singh", "Shyam Marjit", "Anirban Chakraborty"], "title": "FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation", "comment": "Accepted to Winter Conference on Applications of Computer Vision (WACV) 2026, Round 1", "summary": "We address the Federated source-Free Domain Adaptation (FFreeDA) problem, with clients holding unlabeled data with significant inter-client domain gaps. The FFreeDA setup constrains the FL frameworks to employ only a pre-trained server model as the setup restricts access to the source dataset during the training rounds. Often, this source domain dataset has a distinct distribution to the clients' domains. To address the challenges posed by the FFreeDA setup, adaptation of the Source-Free Domain Adaptation (SFDA) methods to FL struggles with client-drift in real-world scenarios due to extreme data heterogeneity caused by the aforementioned domain gaps, resulting in unreliable pseudo-labels. In this paper, we introduce FedSCAl, an FL framework leveraging our proposed Server-Client Alignment (SCAl) mechanism to regularize client updates by aligning the clients' and server model's predictions. We observe an improvement in the clients' pseudo-labeling accuracy post alignment, as the SCAl mechanism helps to mitigate the client-drift. Further, we present extensive experiments on benchmark vision datasets showcasing how FedSCAl consistently outperforms state-of-the-art FL methods in the FFreeDA setup for classification tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FedSCAl\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u5bf9\u9f50\u673a\u5236\u6765\u7f13\u89e3\u5ba2\u6237\u7aef\u6f02\u79fb\uff0c\u63d0\u9ad8\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\u3002", "motivation": "\u8054\u90a6\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u8bbe\u7f6e\u4e2d\uff0c\u5ba2\u6237\u7aef\u5b58\u5728\u663e\u8457\u7684\u57df\u95f4\u5dee\u5f02\uff0c\u4e14\u65e0\u6cd5\u8bbf\u95ee\u6e90\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5728\u6781\u7aef\u6570\u636e\u5f02\u6784\u4e0b\u4ea7\u751f\u4e0d\u53ef\u9760\u7684\u4f2a\u6807\u7b7e\u548c\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faFedSCAl\u6846\u67b6\uff0c\u91c7\u7528\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u5bf9\u9f50\u673a\u5236\u6765\u6b63\u5219\u5316\u5ba2\u6237\u7aef\u66f4\u65b0\uff0c\u901a\u8fc7\u5bf9\u9f50\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u6a21\u578b\u7684\u9884\u6d4b\u6765\u7f13\u89e3\u5ba2\u6237\u7aef\u6f02\u79fb\u3002", "result": "\u5728\u57fa\u51c6\u89c6\u89c9\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedSCAl\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d consistently \u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "FedSCAl\u901a\u8fc7\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u5bf9\u9f50\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u65e0\u6e90\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u51c6\u786e\u6027\u3002"}}
{"id": "2512.06971", "categories": ["cs.LG", "cs.CR", "cs.DS", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.06971", "abs": "https://arxiv.org/abs/2512.06971", "authors": ["Ben Jacobsen", "Kassem Fawaz"], "title": "Prediction with Expert Advice under Local Differential Privacy", "comment": "19 pages, 3 figures", "summary": "We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \\textit{central} DP algorithm by 1.5-3$\\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5c40\u90e8\u5dee\u5206\u9690\u79c1(LDP)\u7ea6\u675f\u4e0b\u7684\u4e13\u5bb6\u9884\u6d4b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u6539\u8fdb\u7b97\u6cd5RW-AdaBatch\u548cRW-Meta\uff0c\u901a\u8fc7\u9690\u79c1\u653e\u5927\u548c\u5143\u5b66\u4e60\u6280\u672f\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u5728COVID-19\u533b\u9662\u6570\u636e\u9884\u6d4b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u4e13\u5bb6\u9884\u6d4b\u7b97\u6cd5\u5728LDP\u7ea6\u675f\u4e0b\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u65b0\u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728\u533b\u7597\u6570\u636e\u7b49\u654f\u611f\u573a\u666f\u4e2d\uff0c\u9690\u79c1\u4fdd\u62a4\u81f3\u5173\u91cd\u8981\u3002", "method": "1. RW-AdaBatch\u7b97\u6cd5\u5229\u7528LDP\u8bf1\u5bfc\u7684\u6709\u9650\u5207\u6362\u884c\u4e3a\u5b9e\u73b0\u9690\u79c1\u653e\u5927\uff0c\u7c7b\u4f3c\u79bb\u7ebf\u5b66\u4e60\u4e2d\u7684\u6df7\u6d17\u6a21\u578b\uff1b2. RW-Meta\u7b97\u6cd5\u5f00\u53d1\u4e86\u5728LDP\u7ea6\u675f\u4e0b\u9009\u62e9\u975e\u5e73\u51e1\u5b66\u4e60\u4e13\u5bb6\u7684\u4e00\u822c\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u9690\u79c1\u6210\u672c\uff1b3. \u57fa\u4e8e\u968f\u673a\u6e38\u8d70\u7406\u8bba\u5206\u6790\u7b97\u6cd5\u6027\u80fd\u3002", "result": "1. RW-AdaBatch\u5728\u6613\u5904\u7406\u6570\u636e\u4e0a\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u66f4\u5f3a\u4e14\u65e0\u6548\u7528\u635f\u5931\uff1b2. RW-Meta\u5728COVID-19\u533b\u9662\u6570\u636e\u9884\u6d4b\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u4e2d\u5fc3\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u6027\u80fd\u63d0\u53471.5-3\u500d\uff1b3. \u63a8\u5bfc\u4e86\u4e0e\u4e13\u5bb6\u72ec\u7acb\u6027\u7a0b\u5ea6\u6210\u53cd\u6bd4\u7684\u9057\u61be\u754c\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u79cd\u7b97\u6cd5\u5728LDP\u7ea6\u675f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u4e13\u5bb6\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u6570\u636e\u7b49\u654f\u611f\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u5728\u7ebf\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06746", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06746", "abs": "https://arxiv.org/abs/2512.06746", "authors": ["Ruoxin Chen", "Jiahui Gao", "Kaiqing Lin", "Keyue Zhang", "Yandan Zhao", "Isabel Guan", "Taiping Yao", "Shouhong Ding"], "title": "Task-Model Alignment: A Simple Path to Generalizable AI-Generated Image Detection", "comment": null, "summary": "Vision Language Models (VLMs) are increasingly adopted for AI-generated images (AIGI) detection, yet converting VLMs into detectors requires substantial resource, while the resulting models still exhibit severe hallucinations. To probe the core issue, we conduct an empirical analysis and observe two characteristic behaviors: (i) fine-tuning VLMs on high-level semantic supervision strengthens semantic discrimination and well generalize to unseen data; (ii) fine-tuning VLMs on low-level pixel-artifact supervision yields poor transfer. We attribute VLMs' underperformance to task-model misalignment: semantics-oriented VLMs inherently lack sensitivity to fine-grained pixel artifacts, and semantically non-discriminative pixel artifacts thus exceeds their inductive biases. In contrast, we observe that conventional pixel-artifact detectors capture low-level pixel artifacts yet exhibit limited semantic awareness relative to VLMs, highlighting that distinct models are better matched to distinct tasks. In this paper, we formalize AIGI detection as two complementary tasks--semantic consistency checking and pixel-artifact detection--and show that neglecting either induces systematic blind spots. Guided by this view, we introduce the Task-Model Alignment principle and instantiate it as a two-branch detector, AlignGemini, comprising a VLM fine-tuned exclusively with pure semantic supervision and a pixel-artifact expert trained exclusively with pure pixel-artifact supervision. By enforcing orthogonal supervision on two simplified datasets, each branch trains to its strengths, producing complementary discrimination over semantic and pixel cues. On five in-the-wild benchmarks, AlignGemini delivers a +9.5 gain in average accuracy, supporting task-model alignment as an effective path to generalizable AIGI detection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u539f\u5219\uff0c\u5c06AIGI\u68c0\u6d4b\u5f62\u5f0f\u5316\u4e3a\u8bed\u4e49\u4e00\u81f4\u6027\u68c0\u67e5\u548c\u50cf\u7d20\u4f2a\u5f71\u68c0\u6d4b\u4e24\u4e2a\u4e92\u8865\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86AlignGemini\u53cc\u5206\u652f\u68c0\u6d4b\u5668\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86+9.5%\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684AIGI\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8d44\u6e90\u6d88\u8017\u5927\u548c\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\uff0c\u6838\u5fc3\u95ee\u9898\u5728\u4e8e\u8bed\u4e49\u5bfc\u5411\u7684VLM\u7f3a\u4e4f\u5bf9\u7ec6\u7c92\u5ea6\u50cf\u7d20\u4f2a\u5f71\u7684\u654f\u611f\u6027\uff0c\u800c\u4f20\u7edf\u50cf\u7d20\u4f2a\u5f71\u68c0\u6d4b\u5668\u53c8\u7f3a\u4e4f\u8bed\u4e49\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u539f\u5219\uff0c\u8bbe\u8ba1AlignGemini\u53cc\u5206\u652f\u68c0\u6d4b\u5668\uff1a\u4e00\u4e2a\u5206\u652f\u4f7f\u7528\u7eaf\u8bed\u4e49\u76d1\u7763\u5fae\u8c03VLM\uff0c\u53e6\u4e00\u4e2a\u5206\u652f\u4f7f\u7528\u7eaf\u50cf\u7d20\u4f2a\u5f71\u76d1\u7763\u8bad\u7ec3\u50cf\u7d20\u4f2a\u5f71\u4e13\u5bb6\u3002\u901a\u8fc7\u6b63\u4ea4\u76d1\u7763\u5728\u4e24\u4e2a\u7b80\u5316\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e92\u8865\u68c0\u6d4b\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAlignGemini\u5b9e\u73b0\u4e86\u5e73\u5747\u51c6\u786e\u7387+9.5%\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4efb\u52a1-\u6a21\u578b\u5bf9\u9f50\u662f\u6784\u5efa\u53ef\u6cdb\u5316AIGI\u68c0\u6d4b\u5668\u7684\u6709\u6548\u8def\u5f84\uff0c\u901a\u8fc7\u5c06\u68c0\u6d4b\u4efb\u52a1\u5206\u89e3\u4e3a\u4e92\u8865\u7684\u8bed\u4e49\u548c\u50cf\u7d20\u5c42\u9762\u4efb\u52a1\uff0c\u5e76\u5206\u522b\u5339\u914d\u6700\u9002\u5408\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u514b\u670d\u5355\u4e00\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.06982", "categories": ["cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.06982", "abs": "https://arxiv.org/abs/2512.06982", "authors": ["Yu Yu", "Qian Xie", "Nairen Cao", "Li Jin"], "title": "LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding", "comment": "NeurIPS 2025 Workshop on Bridging Language, Agent, and World Models for Reasoning and Planning", "summary": "Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u8bbe\u8ba1\u591a\u6e90\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u590d\u5408\u72b6\u6001\u7f16\u7801\u5668\u3002", "motivation": "\u591a\u6e90\u4fe1\u606f\uff08\u5982\u4f20\u611f\u5668\u6d4b\u91cf\u3001\u65f6\u95f4\u5e8f\u5217\u4fe1\u53f7\u3001\u56fe\u50cf\u89c2\u6d4b\u548c\u6587\u672c\u6307\u4ee4\uff09\u7684\u72b6\u6001\u7f16\u7801\u5668\u8bbe\u8ba1\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u901a\u5e38\u9700\u8981\u624b\u52a8\u8bbe\u8ba1\u3002\u73b0\u6709NAS\u65b9\u6cd5\u5ffd\u7565\u4e86\u6a21\u5757\u4e2d\u95f4\u8f93\u51fa\u7684\u6709\u7528\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u591a\u6e90RL\u8bbe\u7f6e\u4e2d\u7684\u6837\u672c\u6548\u7387\u3002", "method": "\u5c06\u591a\u6e90\u72b6\u6001\u7f16\u7801\u5668\u8bbe\u8ba1\u5f62\u5f0f\u5316\u4e3a\u590d\u5408\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u95ee\u9898\uff0c\u63d0\u51faLLM\u9a71\u52a8\u7684NAS\u6d41\u7a0b\uff0c\u5229\u7528\u8bed\u8a00\u6a21\u578b\u5148\u9a8c\u548c\u4e2d\u95f4\u8f93\u51fa\u4fe1\u53f7\u6765\u6307\u5bfc\u9ad8\u6548\u641c\u7d22\u9ad8\u6027\u80fd\u590d\u5408\u72b6\u6001\u7f16\u7801\u5668\u3002", "result": "\u5728\u6df7\u5408\u81ea\u4e3b\u4ea4\u901a\u63a7\u5236\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfNAS\u57fa\u7ebf\u548c\u57fa\u4e8eLLM\u7684GENIUS\u6846\u67b6\u4f7f\u7528\u66f4\u5c11\u7684\u5019\u9009\u8bc4\u4f30\u53d1\u73b0\u4e86\u66f4\u9ad8\u6027\u80fd\u7684\u67b6\u6784\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684LLM\u9a71\u52a8NAS\u65b9\u6cd5\u80fd\u591f\u66f4\u9ad8\u6548\u5730\u8bbe\u8ba1\u591a\u6e90\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u590d\u5408\u72b6\u6001\u7f16\u7801\u5668\uff0c\u63d0\u9ad8\u4e86\u641c\u7d22\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2512.06750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06750", "abs": "https://arxiv.org/abs/2512.06750", "authors": ["Weiqi Li", "Xuanyu Zhang", "Bin Chen", "Jingfen Xie", "Yan Wang", "Kexin Zhang", "Junlin Li", "Li Zhang", "Jian Zhang", "Shijie Zhao"], "title": "UARE: A Unified Vision-Language Model for Image Quality Assessment, Restoration, and Enhancement", "comment": null, "summary": "Image quality assessment (IQA) and image restoration are fundamental problems in low-level vision. Although IQA and restoration are closely connected conceptually, most existing work treats them in isolation. Recent advances in unified multimodal understanding-generation models demonstrate promising results and indicate that stronger understanding can improve generative performance. This motivates a single model that unifies IQA and restoration and explicitly studies how IQA can guide restoration, a setting that remains largely underexplored yet highly valuable. In this paper, we propose UARE, to our knowledge the first Unified vision-language model for image quality Assessment, Restoration, and Enhancement. Built on pretrained unified understanding and generation models, we introduce a two-stage training framework. First, a progressive, easy-to-hard schedule expands from single-type distortions to higher-order mixed degradations, enabling UARE to handle multiple degradations. Second, we perform unified fine-tuning of quality understanding and restoration with interleaved text-image data, aligning IQA signals with restoration objectives. Through multi-task co-training, UARE leverages IQA to boost restoration and enhancement performance. Extensive experiments across IQA, restoration, and enhancement tasks demonstrate the effectiveness of UARE. The code and models will be available at https://github.com/lwq20020127/UARE.", "AI": {"tldr": "UARE\u662f\u9996\u4e2a\u7edf\u4e00\u5904\u7406\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3001\u4fee\u590d\u548c\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u591a\u4efb\u52a1\u534f\u540c\u8bad\u7ec3\uff0c\u5229\u7528IQA\u6307\u5bfc\u63d0\u5347\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u901a\u5e38\u5c06\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u548c\u56fe\u50cf\u4fee\u590d\u5206\u5f00\u5904\u7406\uff0c\u4f46\u4e24\u8005\u5728\u6982\u5ff5\u4e0a\u7d27\u5bc6\u76f8\u5173\u3002\u7814\u7a76\u8868\u660e\u66f4\u5f3a\u7684\u7406\u89e3\u80fd\u529b\u53ef\u4ee5\u63d0\u5347\u751f\u6210\u6027\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u7528IQA\u6307\u5bfc\u56fe\u50cf\u4fee\u590d\u3002", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u7edf\u4e00\u7406\u89e3\u548c\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u6e10\u8fdb\u5f0f\u4ece\u5355\u7c7b\u578b\u5931\u771f\u5230\u9ad8\u9636\u6df7\u5408\u9000\u5316\u8bad\u7ec3\uff1b2\uff09\u901a\u8fc7\u4ea4\u9519\u6587\u672c\u56fe\u50cf\u6570\u636e\u8fdb\u884c\u7edf\u4e00\u5fae\u8c03\uff0c\u5c06IQA\u4fe1\u53f7\u4e0e\u4fee\u590d\u76ee\u6807\u5bf9\u9f50\u3002", "result": "\u5728IQA\u3001\u4fee\u590d\u548c\u589e\u5f3a\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86UARE\u7684\u6709\u6548\u6027\uff0c\u6a21\u578b\u80fd\u591f\u5904\u7406\u591a\u79cd\u9000\u5316\u60c5\u51b5\u5e76\u63d0\u5347\u6062\u590d\u6027\u80fd\u3002", "conclusion": "UARE\u6210\u529f\u5b9e\u73b0\u4e86\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u4e0e\u4fee\u590d\u7684\u7edf\u4e00\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u534f\u540c\u8bad\u7ec3\u8bc1\u660e\u4e86IQA\u5bf9\u56fe\u50cf\u6062\u590d\u7684\u6307\u5bfc\u4f5c\u7528\uff0c\u4e3a\u4f4e\u5c42\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06987", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.06987", "abs": "https://arxiv.org/abs/2512.06987", "authors": ["Emily Jin", "Andrei Cristian Nica", "Mikhail Galkin", "Jarrid Rector-Brooks", "Kin Long Kelvin Lee", "Santiago Miret", "Frances H. Arnold", "Michael Bronstein", "Avishek Joey Bose", "Alexander Tong", "Cheng-Hao Liu"], "title": "OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction", "comment": null, "summary": "Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\\text{RMSD}_1<0.5$ \u00c5 and attains over 80\\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.", "AI": {"tldr": "OXtal\u662f\u4e00\u4e2a100M\u53c2\u6570\u7684\u5168\u539f\u5b50\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u5206\u5b50\u5185\u6784\u8c61\u548c\u5468\u671f\u6027\u5806\u79ef\u7684\u6761\u4ef6\u8054\u5408\u5206\u5e03\uff0c\u5b9e\u73b0\u4e86\u4ece2D\u5316\u5b66\u56fe\u52303D\u5206\u5b50\u6676\u4f53\u7ed3\u6784\u7684\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "\u5206\u5b50\u6676\u4f53\u7ed3\u6784\u9884\u6d4b\u662f\u8ba1\u7b97\u5316\u5b66\u4e2d\u957f\u671f\u5b58\u5728\u7684\u5f00\u653e\u6311\u6218\uff0c\u5bf9\u836f\u7269\u548c\u6709\u673a\u534a\u5bfc\u4f53\u7b49\u9886\u57df\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3a\u6676\u4f53\u5806\u79ef\u76f4\u63a5\u5f71\u54cd\u6709\u673a\u56fa\u4f53\u7684\u7269\u7406\u5316\u5b66\u6027\u8d28\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u7b56\u7565\u66ff\u4ee3\u663e\u5f0f\u7b49\u53d8\u67b6\u6784\uff0c\u63d0\u51fa\u7ed3\u6676\u542f\u53d1\u7684\u65e0\u6676\u683c\u8bad\u7ec3\u65b9\u6848S^4\uff0c\u901a\u8fc7\u968f\u673a\u5316\u5b66\u8ba1\u91cf\u58f3\u91c7\u6837\u9ad8\u6548\u6355\u83b7\u957f\u7a0b\u76f8\u4e92\u4f5c\u7528\uff0c\u907f\u514d\u663e\u5f0f\u6676\u683c\u53c2\u6570\u5316\u3002", "result": "\u572860\u4e07\u5b9e\u9a8c\u9a8c\u8bc1\u6676\u4f53\u7ed3\u6784\u6570\u636e\u96c6\u4e0a\uff0cOXtal\u76f8\u6bd4\u5148\u524d\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5b9e\u73b0\u6570\u91cf\u7ea7\u6539\u8fdb\uff0c\u6062\u590d\u5b9e\u9a8c\u7ed3\u6784\u7684\u6784\u8c61RMSD<0.5\u00c5\uff0c\u5806\u79ef\u76f8\u4f3c\u7387\u8fbe\u523080%\u4ee5\u4e0a\u3002", "conclusion": "OXtal\u80fd\u591f\u6709\u6548\u5efa\u6a21\u5206\u5b50\u7ed3\u6676\u7684\u70ed\u529b\u5b66\u548c\u52a8\u529b\u5b66\u89c4\u5f8b\uff0c\u5728\u4fdd\u6301\u5168\u539f\u5b50\u5206\u8fa8\u7387\u7684\u540c\u65f6\u6bd4\u4f20\u7edf\u91cf\u5b50\u5316\u5b66\u65b9\u6cd5\u4fbf\u5b9c\u6570\u4e2a\u6570\u91cf\u7ea7\u3002"}}
{"id": "2512.06759", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06759", "abs": "https://arxiv.org/abs/2512.06759", "authors": ["Wenbo Lyu", "Yingjun Du", "Jinglin Zhao", "Xianton Zhen", "Ling Shao"], "title": "VisChainBench: A Benchmark for Multi-Turn, Multi-Image Visual Reasoning Beyond Language Priors", "comment": "12 pages,13figures", "summary": "Understanding multi-image, multi-turn scenarios is a critical yet underexplored capability for Large Vision-Language Models (LVLMs). Existing benchmarks predominantly focus on static or horizontal comparisons -- e.g., spotting visual differences or assessing appropriateness -- while relying heavily on language cues. Such settings overlook progressive, context-dependent reasoning and the challenge of visual-to-visual inference. To bridge this gap, we present VisChainBench, a large-scale benchmark designed to rigorously evaluate LVLMs' ability to perform multi-step visual reasoning across sequential, interdependent tasks with minimal language guidance. VisChainBench contains 1,457 tasks spanning over 20,000 images across three diverse domains (e.g., daily scenarios, engineering troubleshooting), structured to mimic real-world decision-making processes. Uniquely, the benchmark is constructed using a multi-agent generation pipeline, ensuring high visual diversity and controlled language bias. All the benchmark data and code for benchmark construction are available for viewing and download via following Link: https://huggingface.co/datasets/eyehole/VisChainBench", "AI": {"tldr": "VisChainBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u50cf\u3001\u591a\u8f6e\u6b21\u573a\u666f\u4e2d\u8fdb\u884c\u591a\u6b65\u89c6\u89c9\u63a8\u7406\u7684\u80fd\u529b\uff0c\u5305\u542b1,457\u4e2a\u4efb\u52a1\u548c\u8d85\u8fc720,000\u5f20\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u6216\u6c34\u5e73\u6bd4\u8f83\uff0c\u4f9d\u8d56\u8bed\u8a00\u7ebf\u7d22\uff0c\u5ffd\u89c6\u4e86\u6e10\u8fdb\u5f0f\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u63a8\u7406\u548c\u89c6\u89c9\u5230\u89c6\u89c9\u7684\u63a8\u7406\u6311\u6218\u3002", "method": "\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u751f\u6210\u6d41\u6c34\u7ebf\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\uff08\u65e5\u5e38\u573a\u666f\u3001\u5de5\u7a0b\u6545\u969c\u6392\u9664\u7b49\uff09\uff0c\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b1,457\u4e2a\u4efb\u52a1\u548c\u8d85\u8fc720,000\u5f20\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5177\u6709\u9ad8\u89c6\u89c9\u591a\u6837\u6027\u548c\u53d7\u63a7\u7684\u8bed\u8a00\u504f\u89c1\u3002", "conclusion": "VisChainBench\u586b\u8865\u4e86\u591a\u56fe\u50cf\u591a\u8f6e\u6b21\u573a\u666f\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u8bc4\u4f30LVLMs\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2512.06989", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06989", "abs": "https://arxiv.org/abs/2512.06989", "authors": ["Minshen Zhang", "Xiang Hu", "Jianguo Li", "Wei Wu", "Kewei Tu"], "title": "Flash Multi-Head Feed-Forward Network", "comment": "17 pages, 8 figures", "summary": "We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFlash Multi-Head FFN (FlashMHF)\u4f5c\u4e3aTransformer\u4e2dFFN\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684I/O\u611f\u77e5\u878d\u5408\u5185\u6838\u548c\u52a8\u6001\u52a0\u6743\u5e76\u884c\u5b50\u7f51\u7edc\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u591a\u5934FFN\u7684\u5185\u5b58\u6d88\u8017\u548c\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u53d7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u5c06\u591a\u5934\u8bbe\u8ba1\u5e94\u7528\u4e8eFFN\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u9762\u4e34\u5185\u5b58\u6d88\u8017\u968f\u5934\u6570\u589e\u52a0\u800c\u589e\u957f\uff0c\u4ee5\u53ca\u4e2d\u95f4\u5c3a\u5bf8\u4e0e\u5934\u7ef4\u5ea6\u6bd4\u4f8b\u5931\u8861\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFlashMHF\uff1a1\uff09\u7c7b\u4f3cFlashAttention\u7684I/O\u611f\u77e5\u878d\u5408\u5185\u6838\uff0c\u5728SRAM\u4e2d\u5728\u7ebf\u8ba1\u7b97\u8f93\u51fa\uff1b2\uff09\u4f7f\u7528\u52a8\u6001\u52a0\u6743\u7684\u5e76\u884c\u5b50\u7f51\u7edc\u8bbe\u8ba1\uff0c\u4fdd\u6301\u4e2d\u95f4\u5c3a\u5bf8\u4e0e\u5934\u7ef4\u5ea6\u7684\u5e73\u8861\u6bd4\u4f8b\u3002", "result": "\u57281.28\u4ebf\u523013\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cFlashMHF\u76f8\u6bd4SwiGLU FFN\u6301\u7eed\u6539\u5584\u56f0\u60d1\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5c06\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\u964d\u4f4e3-5\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u6700\u9ad81.08\u500d\u3002", "conclusion": "\u591a\u5934\u8bbe\u8ba1\u662fFFN\u7684\u4f18\u8d8a\u67b6\u6784\u539f\u5219\uff0cFlashMHF\u4f5c\u4e3aTransformer\u4e2dFFN\u7684\u5f3a\u5927\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.06763", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06763", "abs": "https://arxiv.org/abs/2512.06763", "authors": ["Chengyang Yan", "Mitch Bryson", "Donald G. Dansereau"], "title": "JOCA: Task-Driven Joint Optimisation of Camera Hardware and Adaptive Camera Control Algorithms", "comment": null, "summary": "The quality of captured images strongly influences the performance of downstream perception tasks. Recent works on co-designing camera systems with perception tasks have shown improved task performance. However, most prior approaches focus on optimising fixed camera parameters set at manufacturing, while many parameters, such as exposure settings, require adaptive control at runtime. This paper introduces a method that jointly optimises camera hardware and adaptive camera control algorithms with downstream vision tasks. We present a unified optimisation framework that integrates gradient-based and derivative-free methods, enabling support for both continuous and discrete parameters, non-differentiable image formation processes, and neural network-based adaptive control algorithms. To address non-differentiable effects such as motion blur, we propose DF-Grad, a hybrid optimisation strategy that trains adaptive control networks using signals from a derivative-free optimiser alongside unsupervised task-driven learning. Experiments show that our method outperforms baselines that optimise static and dynamic parameters separately, particularly under challenging conditions such as low light and fast motion. These results demonstrate that jointly optimising hardware parameters and adaptive control algorithms improves perception performance and provides a unified approach to task-driven camera system design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u76f8\u673a\u786c\u4ef6\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7b97\u6cd5\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u68af\u5ea6\u4f18\u5316\u548c\u5bfc\u6570\u65e0\u5173\u65b9\u6cd5\uff0c\u63d0\u5347\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u5236\u9020\u65f6\u56fa\u5b9a\u7684\u76f8\u673a\u53c2\u6570\uff0c\u4f46\u8bb8\u591a\u53c2\u6570\uff08\u5982\u66dd\u5149\u8bbe\u7f6e\uff09\u9700\u8981\u5728\u8fd0\u884c\u65f6\u81ea\u9002\u5e94\u63a7\u5236\u3002\u5355\u72ec\u4f18\u5316\u9759\u6001\u548c\u52a8\u6001\u53c2\u6570\u9650\u5236\u4e86\u611f\u77e5\u6027\u80fd\u7684\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u4f18\u5316\u6846\u67b6DF-Grad\uff0c\u7ed3\u5408\u68af\u5ea6\u4f18\u5316\u548c\u5bfc\u6570\u65e0\u5173\u65b9\u6cd5\uff0c\u652f\u6301\u8fde\u7eed/\u79bb\u6563\u53c2\u6570\u3001\u975e\u53ef\u5fae\u56fe\u50cf\u5f62\u6210\u8fc7\u7a0b\u548c\u795e\u7ecf\u7f51\u7edc\u81ea\u9002\u5e94\u63a7\u5236\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f4e\u5149\u7167\u548c\u5feb\u901f\u8fd0\u52a8\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u4f18\u4e8e\u5355\u72ec\u4f18\u5316\u9759\u6001\u548c\u52a8\u6001\u53c2\u6570\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8054\u5408\u4f18\u5316\u786c\u4ef6\u53c2\u6570\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7b97\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u611f\u77e5\u6027\u80fd\uff0c\u4e3a\u4efb\u52a1\u9a71\u52a8\u7684\u76f8\u673a\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7edf\u4e00\u65b9\u6cd5\u3002"}}
{"id": "2512.06993", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06993", "abs": "https://arxiv.org/abs/2512.06993", "authors": ["Ali Ebrahimpour-Boroojeny"], "title": "Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation", "comment": null, "summary": "We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.\n  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e86AMUN\u548cTRW\u4e24\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\uff0c\u5206\u522b\u5728\u6837\u672c\u9057\u5fd8\u548c\u7c7b\u522b\u9057\u5fd8\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\u548c\u5206\u5e03\u91cd\u52a0\u6743\u5b9e\u73b0\u66f4\u597d\u7684\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5728\u91cd\u73b0\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\u884c\u4e3a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u6837\u672c\u9057\u5fd8\u548c\u7c7b\u522b\u9057\u5fd8\u4efb\u52a1\u4e2d\uff0c\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u6a21\u578b\u5bf9\u5df2\u9057\u5fd8\u6837\u672c\u7684\u7f6e\u4fe1\u5ea6\u3002", "method": "AMUN\u65b9\u6cd5\u901a\u8fc7\u5bf9\u9057\u5fd8\u6837\u672c\u7684\u5bf9\u6297\u6837\u672c\u8fdb\u884c\u5fae\u8c03\u6765\u964d\u4f4e\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff1bTRW\u65b9\u6cd5\u901a\u8fc7\u4f30\u8ba1\u7c7b\u95f4\u76f8\u4f3c\u6027\u5e76\u76f8\u5e94\u8c03\u6574\u76ee\u6807\u5206\u5e03\u6765\u5b9e\u73b0\u7c7b\u522b\u9057\u5fd8\u3002\u8fd8\u63d0\u51fa\u4e86FastClip\u65b9\u6cd5\u63a7\u5236\u6a21\u578b\u5e73\u6ed1\u5ea6\u3002", "result": "AMUN\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u57fa\u4e8eSOTA MIA\u5f97\u5206\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff1bTRW\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u9057\u5fd8\u65b9\u6cd5\u901a\u8fc7\u5173\u6ce8\u6a21\u578b\u9884\u6d4b\u76f8\u4f3c\u6027\u548c\u5206\u5e03\u8fd1\u4f3c\uff0c\u5728\u6837\u672c\u548c\u7c7b\u522b\u9057\u5fd8\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06769", "abs": "https://arxiv.org/abs/2512.06769", "authors": ["Hang Yin", "Xiaomin He", "PeiWen Yuan", "Yiwei Li", "Jiayi Shi", "Wenxiao Fan", "Shaoxiong Feng", "Kan Li"], "title": "Stitch and Tell: A Structured Multimodal Data Augmentation Method for Spatial Understanding", "comment": null, "summary": "Existing vision-language models often suffer from spatial hallucinations, i.e., generating incorrect descriptions about the relative positions of objects in an image. We argue that this problem mainly stems from the asymmetric properties between images and text. To enrich the spatial understanding ability of vision-language models, we propose a simple, annotation-free, plug-and-play method named $\\text{Stitch and Tell}$ (abbreviated as SiTe), which injects structured spatial supervision into data. It constructs stitched image-text pairs by stitching images along a spatial axis and generating spatially-aware captions or question answer pairs based on the layout of stitched image, without relying on costly advanced models or human involvement. We evaluate SiTe across three architectures including LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B and HALVA-7B, two training datasets, and eight benchmarks. Experiments show that SiTe improves spatial understanding tasks such as $\\text{MME}_{\\text{Position}}$ (+5.50%) and Spatial-MM (+4.19%), while maintaining or improving performance on general vision-language benchmarks including COCO-QA (+1.02%) and MMBench (+4.76%). Our findings suggest that explicitly injecting spatially-aware structure into training data offers an effective way to mitigate spatial hallucinations and improve spatial understanding, while preserving general vision-language capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSiTe\u7684\u7b80\u5355\u3001\u65e0\u9700\u6807\u6ce8\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u6cbf\u7a7a\u95f4\u8f74\u62fc\u63a5\u5e76\u751f\u6210\u7a7a\u95f4\u611f\u77e5\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6ce8\u5165\u7ed3\u6784\u5316\u7a7a\u95f4\u76d1\u7763\uff0c\u4ee5\u89e3\u51b3\u7a7a\u95f4\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u51fa\u73b0\u7a7a\u95f4\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u5bf9\u56fe\u50cf\u4e2d\u7269\u4f53\u76f8\u5bf9\u4f4d\u7f6e\u751f\u6210\u9519\u8bef\u63cf\u8ff0\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e3b\u8981\u6e90\u4e8e\u56fe\u50cf\u548c\u6587\u672c\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u6027\u3002", "method": "SiTe\u65b9\u6cd5\u901a\u8fc7\u62fc\u63a5\u56fe\u50cf\u6784\u9020\u7f1d\u5408\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u5e76\u57fa\u4e8e\u7f1d\u5408\u56fe\u50cf\u7684\u5e03\u5c40\u751f\u6210\u7a7a\u95f4\u611f\u77e5\u7684\u6807\u9898\u6216\u95ee\u7b54\u5bf9\uff0c\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u7684\u5148\u8fdb\u6a21\u578b\u6216\u4eba\u5de5\u53c2\u4e0e\u3002", "result": "\u5728\u4e09\u4e2a\u67b6\u6784\u3001\u4e24\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSiTe\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u95f4\u7406\u89e3\u4efb\u52a1\uff08\u5982MME_Position +5.50%\uff0cSpatial-MM +4.19%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u6539\u8fdb\u4e86\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u57fa\u51c6\u7684\u6027\u80fd\u3002", "conclusion": "\u660e\u786e\u5c06\u7a7a\u95f4\u611f\u77e5\u7ed3\u6784\u6ce8\u5165\u8bad\u7ec3\u6570\u636e\u662f\u7f13\u89e3\u7a7a\u95f4\u5e7b\u89c9\u3001\u63d0\u9ad8\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u4fdd\u6301\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\u3002"}}
{"id": "2512.07010", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07010", "abs": "https://arxiv.org/abs/2512.07010", "authors": ["Kevin Lee", "Pablo Millan Arias"], "title": "Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation", "comment": "Work in progress, (12 pages manuscript, 6 figures, 6 tables, 3 pages references, 14 pages appendix)", "summary": "Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\\% and 95.06\\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.", "AI": {"tldr": "DynamicLRP\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u5c42\u95f4\u76f8\u5173\u6027\u4f20\u64ad\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f52\u56e0\u5206\u89e3\u4e3a\u8ba1\u7b97\u56fe\u4e2d\u7684\u5355\u4e2a\u5f20\u91cf\u64cd\u4f5c\uff0c\u5e76\u5f15\u5165Promise System\u5b9e\u73b0\u5ef6\u8fdf\u6fc0\u6d3b\u89e3\u6790\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u67b6\u6784\u65e0\u5173\u6027\u3002", "motivation": "\u73b0\u6709LRP\u5b9e\u73b0\u57fa\u4e8e\u6a21\u5757\u7ea7\u522b\uff0c\u9700\u8981\u67b6\u6784\u7279\u5b9a\u7684\u4f20\u64ad\u89c4\u5219\u548c\u4fee\u6539\uff0c\u9650\u5236\u4e86\u76ee\u6807\u6a21\u578b\u7684\u901a\u7528\u6027\u548c\u5b9e\u73b0\u7684\u53ef\u7ef4\u62a4\u6027\u3002", "method": "\u5728\u8ba1\u7b97\u56fe\u7684\u64cd\u4f5c\u7ea7\u522b\u5206\u89e3\u5f52\u56e0\uff0c\u5f15\u5165Promise System\u8fdb\u884c\u5ef6\u8fdf\u6fc0\u6d3b\u89e3\u6790\uff0c\u72ec\u7acb\u4e8e\u53cd\u5411\u4f20\u64ad\u673a\u5236\uff0c\u65e0\u9700\u6a21\u578b\u4fee\u6539\u5373\u53ef\u5728\u4efb\u610f\u8ba1\u7b97\u56fe\u4e0a\u8fd0\u884c\u3002", "result": "\u5728\u591a\u79cd\u67b6\u6784\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u4e13\u7528\u5b9e\u73b0\u7684\u6027\u80fd\uff08VGG ABPC 1.77 vs 1.69\uff0cViT\u7b49\u6548\u6027\u80fd\uff0cRoBERTa-large\u548cFlan-T5-large\u5728SQuADv2\u4e0a\u7684\u5f52\u56e0\u51c6\u786e\u7387\u5206\u522b\u4e3a93.70%\u548c95.06%\uff09\uff0c\u8986\u76d631,465\u4e2a\u8ba1\u7b97\u56fe\u8282\u70b9\u768499.92%\u3002", "conclusion": "DynamicLRP\u7684\u64cd\u4f5c\u7ea7\u522b\u5206\u89e3\u548cPromise System\u4e3aLRP\u5728\u6f14\u8fdb\u67b6\u6784\u4e2d\u5efa\u7acb\u4e86\u53ef\u6301\u7eed\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2512.06774", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06774", "abs": "https://arxiv.org/abs/2512.06774", "authors": ["Longjie Zhao", "Ziming Hong", "Zhenyang Ren", "Runnan Chen", "Mingming Gong", "Tongliang Liu"], "title": "RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.", "AI": {"tldr": "RDSplat\u662f\u4e00\u79cd\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684\u9c81\u68d2\u6c34\u5370\u6280\u672f\uff0c\u4e13\u95e8\u8bbe\u8ba1\u6765\u62b5\u6297\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u653b\u51fb\uff0c\u901a\u8fc7\u5c06\u6c34\u5370\u5d4c\u5165\u5230\u4f4e\u9891\u9ad8\u65af\u5206\u91cf\u4e2d\u5e76\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u6765\u63d0\u9ad8\u6c34\u5370\u7684\u751f\u5b58\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u76843DGS\u6c34\u5370\u65b9\u6cd5\u5bf9\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u653b\u51fb\u975e\u5e38\u8106\u5f31\uff0c\u8fd9\u79cd\u653b\u51fb\u53ef\u4ee5\u8f7b\u6613\u64e6\u9664\u5d4c\u5165\u7684\u6765\u6e90\u4fe1\u606f\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5185\u5728\u62b5\u6297\u6b64\u7c7b\u653b\u51fb\u76843DGS\u6c34\u5370\u6280\u672f\u3002", "method": "RDSplat\u91c7\u7528\u591a\u57df\u6846\u67b6\uff0c\u57283DGS\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7\u534f\u8c03\u534f\u65b9\u5dee\u6b63\u5219\u5316\u548c2D\u6ee4\u6ce2\u5c06\u6c34\u5370\u5d4c\u5165\u5230\u6269\u6563\u7f16\u8f91\u4fdd\u7559\u7684\u4f4e\u9891\u9ad8\u65af\u5206\u91cf\u4e2d\uff0c\u5e76\u4f7f\u7528\u9ad8\u65af\u6a21\u7cca\u4f5c\u4e3a\u8bad\u7ec3\u4ee3\u7406\u8fdb\u884c\u5bf9\u6297\u6027\u5fae\u8c03\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5168\u9762\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0cRDSplat\u5728\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u4e0b\u4fdd\u6301\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6c34\u5370\u4e0d\u53ef\u89c1\u6027\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RDSplat\u4e3a3DGS\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7248\u6743\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u62b5\u6297\u6269\u6563\u7f16\u8f91\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u6c34\u5370\u4e0d\u53ef\u89c1\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6c34\u5370\u7684\u751f\u5b58\u80fd\u529b\u3002"}}
{"id": "2512.07011", "categories": ["cs.LG", "cs.CL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.07011", "abs": "https://arxiv.org/abs/2512.07011", "authors": ["Daniel Ohayon", "Itay Lamprecht", "Itay Hubara", "Israel Cohen", "Daniel Soudry", "Noam Elata"], "title": "Block Sparse Flash Attention", "comment": "10 pages, 5 figures. Code: https://github.com/Danielohayon/Block-Sparse-Flash-Attention", "summary": "Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention", "AI": {"tldr": "Block-Sparse FlashAttention (BSFA) \u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u7cbe\u786e\u7684\u67e5\u8be2-\u952e\u76f8\u4f3c\u5ea6\u6765\u9009\u62e9\u6700\u91cd\u8981\u7684\u503c\u5757\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u5c06\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u6700\u9ad81.24\u500d\u3002", "motivation": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u4f46\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u9020\u6210\u4e86\u4e25\u91cd\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "BSFA\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u67e5\u8be2\u4e0e\u952e\u5757\u7684\u7cbe\u786e\u76f8\u4f3c\u5ea6\uff0c\u9009\u62e9top-k\u6700\u91cd\u8981\u7684\u503c\u5757\uff0c\u4f7f\u7528\u6821\u51c6\u9608\u503c\u8df3\u8fc7\u7ea650%\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u4f20\u8f93\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e00\u6b21\u6027\u9608\u503c\u6821\u51c6\u3002", "result": "\u5728Llama-3.1-8B\u4e0a\uff0cBSFA\u5728\u771f\u5b9e\u4e16\u754c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u9ad81.10\u500d\u52a0\u901f\uff0c\u5728\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u9ad81.24\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u630199%\u4ee5\u4e0a\u7684\u57fa\u7ebf\u51c6\u786e\u7387\u3002", "conclusion": "BSFA\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bad\u7ec3\u65e0\u5173\u7684\u6ce8\u610f\u529b\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u53ef\u4f5c\u4e3aFlashAttention\u7684\u76f4\u63a5\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.06783", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06783", "abs": "https://arxiv.org/abs/2512.06783", "authors": ["Tobias Leuthold", "Michele Xiloyannis", "Yves Zimmermann"], "title": "Physics Informed Human Posture Estimation Based on 3D Landmarks from Monocular RGB-Videos", "comment": "16 pages, 5 figures", "summary": "Applications providing automated coaching for physical training are increasing in popularity, for example physical therapy. These applications rely on accurate and robust pose estimation using monocular video streams. State-of-the-art models like BlazePose excel in real-time pose tracking, but their lack of anatomical constraints indicates improvement potential by including physical knowledge. We present a real-time post-processing algorithm fusing the strengths of BlazePose 3D and 2D estimations using a weighted optimization, penalizing deviations from expected bone length and biomechanical models. Bone length estimations are refined to the individual anatomy using a Kalman filter with adapting measurement trust. Evaluation using the Physio2.2M dataset shows a 10.2 percent reduction in 3D MPJPE and a 16.6 percent decrease in errors of angles between body segments compared to BlazePose 3D estimation. Our method provides a robust, anatomically consistent pose estimation based on a computationally efficient video-to-3D pose estimation, suitable for automated physiotherapy, healthcare, and sports coaching on consumer-level laptops and mobile devices. The refinement runs on the backend with anonymized data only.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5b9e\u65f6\u540e\u5904\u7406\u7b97\u6cd5\uff0c\u878d\u5408BlazePose 3D\u548c2D\u4f30\u8ba1\uff0c\u901a\u8fc7\u52a0\u6743\u4f18\u5316\u7ed3\u5408\u9aa8\u9abc\u957f\u5ea6\u7ea6\u675f\u548c\u751f\u7269\u529b\u5b66\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u89e3\u5256\u5b66\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\uff08\u5982BlazePose\uff09\u7f3a\u4e4f\u89e3\u5256\u5b66\u7ea6\u675f\uff0c\u5728\u7269\u7406\u6cbb\u7597\u7b49\u81ea\u52a8\u6559\u7ec3\u5e94\u7528\u4e2d\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u63d0\u5347\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027", "method": "\u4f7f\u7528\u52a0\u6743\u4f18\u5316\u7b97\u6cd5\u878d\u5408BlazePose 3D\u548c2D\u4f30\u8ba1\uff0c\u901a\u8fc7\u60e9\u7f5a\u504f\u79bb\u9884\u671f\u9aa8\u9abc\u957f\u5ea6\u548c\u751f\u7269\u529b\u5b66\u6a21\u578b\u6765\u7ea6\u675f\u59ff\u6001\uff1b\u91c7\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6839\u636e\u6d4b\u91cf\u4fe1\u4efb\u5ea6\u81ea\u9002\u5e94\u4f18\u5316\u4e2a\u4f53\u9aa8\u9abc\u957f\u5ea6\u4f30\u8ba1", "result": "\u5728Physio2.2M\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c3D MPJPE\u964d\u4f4e10.2%\uff0c\u8eab\u4f53\u6bb5\u95f4\u89d2\u5ea6\u8bef\u5dee\u51cf\u5c1116.6%\uff0c\u76f8\u6bd4BlazePose 3D\u4f30\u8ba1\u6709\u660e\u663e\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u3001\u89e3\u5256\u5b66\u4e00\u81f4\u7684\u5b9e\u65f6\u59ff\u6001\u4f30\u8ba1\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6d88\u8d39\u7ea7\u8bbe\u5907\u7684\u81ea\u52a8\u7269\u7406\u6cbb\u7597\u3001\u533b\u7597\u4fdd\u5065\u548c\u8fd0\u52a8\u6559\u7ec3\u5e94\u7528\uff0c\u540e\u7aef\u8fd0\u884c\u4ec5\u4f7f\u7528\u533f\u540d\u6570\u636e"}}
{"id": "2512.07021", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07021", "abs": "https://arxiv.org/abs/2512.07021", "authors": ["Jose Geraldo Fernandes", "Luiz Facury de Souza", "Pedro Robles Dutenhefner", "Gisele L. Pappa", "Wagner Meira"], "title": "Transferring Clinical Knowledge into ECGs Representation", "comment": null, "summary": "Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \\emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\u589e\u5f3aECG\u7f16\u7801\u5668\u6027\u80fd\uff0c\u63d0\u9ad8\u5fc3\u7535\u56fe\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4ec5\u9700ECG\u4fe1\u53f7\u8fdb\u884c\u63a8\u7406\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5fc3\u7535\u56fe\u5206\u7c7b\u4e2d\u5177\u6709\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u5176\u9ed1\u7bb1\u7279\u6027\u963b\u788d\u4e86\u4e34\u5e8a\u5e94\u7528\uff0c\u7f3a\u4e4f\u53ef\u4fe1\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u7684\u8054\u5408\u5d4c\u5165\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5c06\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\uff08\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u3001\u751f\u547d\u4f53\u5f81\u3001\u751f\u7269\u7279\u5f81\uff09\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230ECG\u7f16\u7801\u5668\u4e2d\uff0c\u540c\u65f6\u8bad\u7ec3\u6a21\u578b\u4eceECG\u5d4c\u5165\u9884\u6d4b\u76f8\u5173\u7684\u5b9e\u9a8c\u5ba4\u5f02\u5e38\u503c\u4f5c\u4e3a\u95f4\u63a5\u89e3\u91ca\u3002", "result": "\u5728MIMIC-IV-ECG\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u6807\u7b7e\u8bca\u65ad\u5206\u7c7b\u4e2d\u4f18\u4e8e\u4ec5\u4f7f\u7528\u4fe1\u53f7\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u9700\u8981\u6240\u6709\u6570\u636e\u7684\u5168\u591a\u6a21\u6001\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u62bd\u8c61\u9884\u6d4b\u8f6c\u5316\u4e3a\u57fa\u4e8e\u751f\u7406\u5b66\u7684\u89e3\u91ca\uff0c\u4e3aAI\u66f4\u5b89\u5168\u5730\u878d\u5165\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2512.06793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06793", "abs": "https://arxiv.org/abs/2512.06793", "authors": ["Jiaxin Liu", "Gangwei Xu", "Xianqi Wang", "Chengliang Zhang", "Xin Yang"], "title": "Generalized Geometry Encoding Volume for Real-time Stereo Matching", "comment": "Accepted by AAAI 2026", "summary": "Real-time stereo matching methods primarily focus on enhancing in-domain performance but often overlook the critical importance of generalization in real-world applications. In contrast, recent stereo foundation models leverage monocular foundation models (MFMs) to improve generalization, but typically suffer from substantial inference latency. To address this trade-off, we propose Generalized Geometry Encoding Volume (GGEV), a novel real-time stereo matching network that achieves strong generalization. We first extract depth-aware features that encode domain-invariant structural priors as guidance for cost aggregation. Subsequently, we introduce a Depth-aware Dynamic Cost Aggregation (DDCA) module that adaptively incorporates these priors into each disparity hypothesis, effectively enhancing fragile matching relationships in unseen scenes. Both steps are lightweight and complementary, leading to the construction of a generalized geometry encoding volume with strong generalization capability. Experimental results demonstrate that our GGEV surpasses all existing real-time methods in zero-shot generalization capability, and achieves state-of-the-art performance on the KITTI 2012, KITTI 2015, and ETH3D benchmarks.", "AI": {"tldr": "\u63d0\u51faGGEV\u7f51\u7edc\u89e3\u51b3\u5b9e\u65f6\u7acb\u4f53\u5339\u914d\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0e\u63a8\u7406\u901f\u5ea6\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u548c\u52a8\u6001\u6210\u672c\u805a\u5408\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u5b9e\u65f6\u7acb\u4f53\u5339\u914d\u65b9\u6cd5\u8fc7\u4e8e\u5173\u6ce8\u57df\u5185\u6027\u80fd\u800c\u5ffd\u89c6\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u5355\u76ee\u57fa\u7840\u6a21\u578b\u7684\u7acb\u4f53\u57fa\u7840\u6a21\u578b\u867d\u7136\u6cdb\u5316\u597d\u4f46\u63a8\u7406\u5ef6\u8fdf\u9ad8", "method": "1) \u63d0\u53d6\u7f16\u7801\u57df\u4e0d\u53d8\u7ed3\u6784\u5148\u9a8c\u7684\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u4f5c\u4e3a\u6210\u672c\u805a\u5408\u6307\u5bfc\uff1b2) \u63d0\u51fa\u6df1\u5ea6\u611f\u77e5\u52a8\u6001\u6210\u672c\u805a\u5408(DDCA)\u6a21\u5757\uff0c\u5c06\u5148\u9a8c\u81ea\u9002\u5e94\u878d\u5165\u6bcf\u4e2a\u89c6\u5dee\u5047\u8bbe", "result": "\u5728KITTI 2012\u3001KITTI 2015\u548cETH3D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u5b9e\u65f6\u65b9\u6cd5", "conclusion": "GGEV\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e92\u8865\u6b65\u9aa4\u6784\u5efa\u4e86\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u51e0\u4f55\u7f16\u7801\u4f53\u79ef\uff0c\u6210\u529f\u5e73\u8861\u4e86\u5b9e\u65f6\u6027\u80fd\u4e0e\u6cdb\u5316\u80fd\u529b"}}
{"id": "2512.07040", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07040", "abs": "https://arxiv.org/abs/2512.07040", "authors": ["Sakib Mostafa", "Lei Xing", "Md. Tauhidul Islam"], "title": "Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis", "comment": null, "summary": "Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.", "AI": {"tldr": "Graph2Image\u5c06\u5927\u578b\u751f\u7269\u7f51\u7edc\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u56fe\u50cf\uff0c\u4f7f\u7528CNN\u8fdb\u884c\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u957f\u7a0b\u4f9d\u8d56\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u751f\u7269\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\u9762\u4e34\u53ef\u6269\u5c55\u6027\u5dee\u3001\u957f\u7a0b\u4f9d\u8d56\u6355\u6349\u56f0\u96be\u3001\u591a\u6a21\u6001\u6574\u5408\u96be\u3001\u8868\u8fbe\u6027\u6709\u9650\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06\u7f51\u7edc\u8282\u70b9\u7a7a\u95f4\u6392\u5217\u57282D\u7f51\u683c\u4e0a\uff0c\u5c06\u751f\u7269\u7f51\u7edc\u8f6c\u6362\u4e3a\u56fe\u50cf\u96c6\uff0c\u7136\u540e\u4f7f\u7528\u5177\u6709\u5168\u5c40\u611f\u53d7\u91ce\u548c\u591a\u5c3a\u5ea6\u91d1\u5b57\u5854\u7684CNN\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u591a\u4e2a\u5927\u89c4\u6a21\u751f\u7269\u7f51\u7edc\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u8fbe67.2%\uff0c\u80fd\u591f\u5206\u6790\u8d85\u8fc710\u4ebf\u8282\u70b9\u7684\u7f51\u7edc\uff0c\u5e76\u5728\u4e2a\u4eba\u8ba1\u7b97\u673a\u4e0a\u8fd0\u884c\u3002", "conclusion": "Graph2Image\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u4e14\u652f\u6301\u591a\u6a21\u6001\u7684\u751f\u7269\u7f51\u7edc\u5206\u6790\u65b9\u6cd5\uff0c\u4e3a\u75be\u75c5\u8bca\u65ad\u548c\u590d\u6742\u751f\u7269\u7cfb\u7edf\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.06802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06802", "abs": "https://arxiv.org/abs/2512.06802", "authors": ["Yutong Wang", "Haiyu Zhang", "Tianfan Xue", "Yu Qiao", "Yaohui Wang", "Chang Xu", "Xinyuan Chen"], "title": "VDOT: Efficient Unified Video Creation via Optimal Transport Distillation", "comment": null, "summary": "The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7edf\u4e00\u7684\u89c6\u9891\u751f\u6210\u6a21\u578bVDOT\uff0c\u91c7\u7528\u5206\u5e03\u5339\u914d\u84b8\u998f\uff08DMD\uff09\u8303\u5f0f\u548c\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u548c\u8d28\u91cf\uff0c\u57284\u6b65\u751f\u6210\u4e2d\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a100\u6b65\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u8981\u4e48\u53ea\u80fd\u5904\u7406\u5c11\u6570\u7279\u5b9a\u6761\u4ef6\uff0c\u8981\u4e48\u56e0\u590d\u6742\u7684\u6a21\u578b\u63a8\u7406\u5bfc\u81f4\u751f\u6210\u65f6\u95f4\u8fc7\u957f\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b9e\u7528\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89c6\u9891\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u91c7\u7528\u5206\u5e03\u5339\u914d\u84b8\u998f\uff08DMD\uff09\u8bad\u7ec3\u8303\u5f0f\uff1b2. \u5f15\u5165\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u6280\u672f\u4f18\u5316\u771f\u5b9e\u4e0e\u751f\u6210\u5206\u6570\u5206\u5e03\u4e4b\u95f4\u7684\u5dee\u5f02\uff1b3. \u96c6\u6210\u5224\u522b\u5668\u4f7f\u6a21\u578b\u611f\u77e5\u771f\u5b9e\u89c6\u9891\u6570\u636e\uff1b4. \u5f00\u53d1\u5168\u81ea\u52a8\u89c6\u9891\u6570\u636e\u6807\u6ce8\u548c\u8fc7\u6ee4\u6d41\u7a0b\uff1b5. \u6784\u5efa\u7edf\u4e00\u6d4b\u8bd5\u57fa\u51c6UVCBench\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c4\u6b65\u751f\u6210\u7684VDOT\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8a\u4e86\u9700\u8981100\u6b65\u53bb\u566a\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6548\u7387\u3002", "conclusion": "VDOT\u901a\u8fc7\u521b\u65b0\u7684OT\u6280\u672f\u548cDMD\u8303\u5f0f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\u548c\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07064", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.07064", "abs": "https://arxiv.org/abs/2512.07064", "authors": ["Jiannan Yang", "Veronika Thost", "Tengfei Ma"], "title": "Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design", "comment": null, "summary": "Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u7387\u6846\u67b6\u6765\u5206\u6790\u5206\u5b50\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u63a9\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u53d1\u73b0\uff1a\u5bf9\u4e8e\u5e38\u89c1\u7684\u8282\u70b9\u7ea7\u9884\u6d4b\u4efb\u52a1\uff0c\u590d\u6742\u7684\u63a9\u7801\u5206\u5e03\u76f8\u6bd4\u5747\u5300\u91c7\u6837\u5e76\u65e0\u660e\u663e\u4f18\u52bf\uff1b\u9884\u6d4b\u76ee\u6807\u7684\u9009\u62e9\u53ca\u5176\u4e0e\u7f16\u7801\u5668\u67b6\u6784\u7684\u534f\u540c\u4f5c\u7528\u66f4\u4e3a\u5173\u952e\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u63a9\u7801\u7684\u5206\u5b50\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u96be\u4ee5\u786e\u5b9a\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u771f\u6b63\u6709\u6548\u3002", "method": "\u6784\u5efa\u7edf\u4e00\u7684\u6982\u7387\u6846\u67b6\u5206\u6790\u9884\u8bad\u7ec3-\u5fae\u8c03\u6d41\u7a0b\uff0c\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u7814\u7a76\u4e09\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u7ef4\u5ea6\uff1a\u63a9\u7801\u5206\u5e03\u3001\u9884\u6d4b\u76ee\u6807\u548c\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5e76\u4f7f\u7528\u4fe1\u606f\u8bba\u6307\u6807\u8bc4\u4f30\u9884\u8bad\u7ec3\u4fe1\u53f7\u7684\u4fe1\u606f\u91cf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u590d\u6742\u63a9\u7801\u5206\u5e03\u5728\u8282\u70b9\u7ea7\u9884\u6d4b\u4efb\u52a1\u4e2d\u65e0\u4f18\u52bf\uff1b2\uff09\u9884\u6d4b\u76ee\u6807\u7684\u9009\u62e9\u548c\u4e0e\u7f16\u7801\u5668\u67b6\u6784\u7684\u534f\u540c\u4f5c\u7528\u66f4\u4e3a\u5173\u952e\uff1b3\uff09\u4f7f\u7528\u8bed\u4e49\u66f4\u4e30\u5bcc\u7684\u9884\u6d4b\u76ee\u6807\u7ed3\u5408\u56feTransformer\u7f16\u7801\u5668\u53ef\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u4e3a\u5206\u5b50\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5f3a\u8c03\u5e94\u5173\u6ce8\u9884\u6d4b\u76ee\u6807\u548c\u7f16\u7801\u5668\u67b6\u6784\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u800c\u975e\u8fc7\u5ea6\u4f18\u5316\u63a9\u7801\u7b56\u7565\u3002"}}
{"id": "2512.06810", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06810", "abs": "https://arxiv.org/abs/2512.06810", "authors": ["Yueqian Wang", "Songxiang Liu", "Disong Wang", "Nuo Xu", "Guanglu Wan", "Huishuai Zhang", "Dongyan Zhao"], "title": "MMDuet2: Enhancing Proactive Interaction of Video MLLMs with Multi-Turn Reinforcement Learning", "comment": null, "summary": "Recent advances in video multimodal large language models (Video MLLMs) have significantly enhanced video understanding and multi-modal interaction capabilities. While most existing systems operate in a turn-based manner where the model can only reply after user turns, proactively deciding when to reply during video playback presents a promising yet challenging direction for real-time applications. In this work, we propose a novel text-to-text approach to proactive interaction, where the model autonomously determines whether to respond or remain silent at each turn based on dialogue history and visual context up to current frame of an streaming video. To overcome difficulties in previous methods such as manually tuning response decision thresholds and annotating precise reply times, we introduce a multi-turn RL based training method that encourages timely and accurate responses without requiring precise response time annotations. We train our model MMDuet2 on a dataset of 52k videos with two types of dialogues via SFT and RL. Experimental results demonstrate that MMDuet2 outperforms existing proactive Video MLLM baselines in response timing and quality, achieving state-of-the-art performance on the ProactiveVideoQA benchmark.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6587\u672c\u5230\u6587\u672c\u7684\u4e3b\u52a8\u4ea4\u4e92\u65b9\u6cd5MMDuet2\uff0c\u901a\u8fc7\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f7f\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u51b3\u5b9a\u5728\u89c6\u9891\u64ad\u653e\u8fc7\u7a0b\u4e2d\u4f55\u65f6\u56de\u590d\uff0c\u65e0\u9700\u7cbe\u786e\u56de\u590d\u65f6\u95f4\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5927\u591a\u91c7\u7528\u8f6e\u6b21\u5f0f\u4ea4\u4e92\uff0c\u800c\u5b9e\u65f6\u5e94\u7528\u9700\u8981\u6a21\u578b\u80fd\u591f\u5728\u89c6\u9891\u64ad\u653e\u8fc7\u7a0b\u4e2d\u4e3b\u52a8\u51b3\u5b9a\u4f55\u65f6\u56de\u590d\u3002\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u624b\u52a8\u8c03\u6574\u56de\u590d\u51b3\u7b56\u9608\u503c\u548c\u6807\u6ce8\u7cbe\u786e\u56de\u590d\u65f6\u95f4\u7684\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u6587\u672c\u5230\u6587\u672c\u7684\u4e3b\u52a8\u4ea4\u4e92\u65b9\u6cd5\uff0c\u6a21\u578b\u57fa\u4e8e\u5bf9\u8bdd\u5386\u53f2\u548c\u5f53\u524d\u5e27\u89c6\u89c9\u4e0a\u4e0b\u6587\u81ea\u4e3b\u51b3\u5b9a\u56de\u590d\u6216\u4fdd\u6301\u6c89\u9ed8\u3002\u91c7\u7528\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6cd5\uff0c\u65e0\u9700\u7cbe\u786e\u56de\u590d\u65f6\u95f4\u6807\u6ce8\u3002\u5728\u5305\u542b52k\u89c6\u9891\u548c\u4e24\u7c7b\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\u4e0a\u901a\u8fc7SFT\u548cRL\u8bad\u7ec3\u3002", "result": "MMDuet2\u5728ProactiveVideoQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u56de\u590d\u65f6\u673a\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u4e3b\u52a8\u89c6\u9891MLLM\u57fa\u7ebf\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u52a8\u4ea4\u4e92\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07079", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07079", "abs": "https://arxiv.org/abs/2512.07079", "authors": ["Anton Morgunov", "Victor S. Batista"], "title": "Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation", "comment": "11 pages + 7 pages of SI. RetroCast is available on GitHub, see https://github.com/ischemist/project-procrustes. SynthArena is publicly available, see https://syntharena.ischemist.com/", "summary": "Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between \"solvability\" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a \"complexity cliff\" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.06811", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.06811", "abs": "https://arxiv.org/abs/2512.06811", "authors": ["Xiang Lin", "Weixin Li", "Shu Guo", "Lihong Wang", "Di Huang"], "title": "RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models", "comment": "Accepted by AAAI 2026(Oral)", "summary": "Pre-trained Vision-Language Models (VLMs), \\textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.", "AI": {"tldr": "RMAdapter\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u9002\u914d\u5668\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u67b6\u6784\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5e73\u8861\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u5fae\u8c03\u4e2d\u5b58\u5728\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u4e0e\u6cdb\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e14\u57fa\u4e8e\u9002\u914d\u5668\u7684\u65b9\u6cd5\u7814\u7a76\u4e0d\u8db3\uff0c\u6027\u80fd\u5b58\u5728\u660e\u663e\u5dee\u8ddd\u3002", "method": "\u63d0\u51faRMAdapter\u53cc\u5206\u652f\u67b6\u6784\uff1a\u9002\u5e94\u5206\u652f\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6ce8\u5165\u4efb\u52a1\u7279\u5b9a\u77e5\u8bc6\uff0c\u91cd\u5efa\u5206\u652f\u901a\u8fc7\u5c06\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\u91cd\u5efa\u56de\u539f\u59cb\u7279\u5f81\u7a7a\u95f4\u6765\u4fdd\u7559\u901a\u7528\u77e5\u8bc6\u3002\u91c7\u7528\u5c40\u90e8\u91cd\u5efa\u635f\u5931\u8ba1\u7b97\u548c\u5171\u4eab\u6295\u5f71\u6a21\u5757\u4fdd\u6301\u8f7b\u91cf\u5316\uff0c\u5e76\u52a0\u5165\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u5e73\u8861\u5224\u522b\u6027\u548c\u6cdb\u5316\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u4efb\u52a1\uff08\u65b0\u7c7b\u522b\u6cdb\u5316\u3001\u65b0\u76ee\u6807\u6570\u636e\u96c6\u6cdb\u5316\u3001\u9886\u57df\u6cdb\u5316\uff09\u4e0a\u5168\u9762\u8bc4\u4f30\uff0cRMAdapter\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u4e0d\u4f9d\u8d56\u6570\u636e\u589e\u5f3a\u6216\u91cd\u590d\u63d0\u793a\u8bbe\u8ba1\u3002", "conclusion": "RMAdapter\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u5206\u652f\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c11\u6837\u672c\u5fae\u8c03\u4e2d\u7684\u9002\u5e94-\u6cdb\u5316\u5e73\u8861\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.07082", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07082", "abs": "https://arxiv.org/abs/2512.07082", "authors": ["Yuan-Ting Zhong", "Ting Huang", "Xiaolin Xiao", "Yue-Jiao Gong"], "title": "TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization", "comment": "Accepted by AAAI 2026", "summary": "Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.", "AI": {"tldr": "TRACE\u662f\u4e00\u79cd\u53ef\u8fc1\u79fb\u7684\u6982\u5ff5\u6f02\u79fb\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u5b66\u4e60\u68c0\u6d4b\u6d41\u6570\u636e\u4e2d\u7684\u5206\u5e03\u53d8\u5316\uff0c\u5177\u6709\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u96c6\u6210\u5230\u6d41\u4f18\u5316\u5668\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u6d41\u6570\u636e\u9a71\u52a8\u4f18\u5316\u65b9\u6cd5\u5728\u56fa\u5b9a\u6f02\u79fb\u95f4\u9694\u548c\u5b8c\u5168\u73af\u5883\u53ef\u89c2\u6d4b\u6027\u7b49\u9650\u5236\u6027\u5047\u8bbe\u4e0b\u5de5\u4f5c\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u7684\u52a8\u6001\u73af\u5883\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faTRACE\u65b9\u6cd5\uff0c\u91c7\u7528\u539f\u5219\u6027\u6807\u8bb0\u5316\u7b56\u7565\u4ece\u6570\u636e\u6d41\u4e2d\u63d0\u53d6\u7edf\u8ba1\u7279\u5f81\uff0c\u4f7f\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u5b66\u4e60\u5efa\u6a21\u6f02\u79fb\u6a21\u5f0f\uff0c\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u7684\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6d41\u6570\u636e\u9a71\u52a8\u4f18\u5316\u573a\u666f\u4e2d\u5177\u6709\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u3001\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "TRACE\u5c55\u793a\u4e86\u5b66\u4e60\u5230\u7684\u6f02\u79fb\u6a21\u5f0f\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5176\u5373\u63d2\u5373\u7528\u7279\u6027\u4f7f\u5176\u80fd\u591f\u96c6\u6210\u5230\u6d41\u4f18\u5316\u5668\u4e2d\uff0c\u4fc3\u8fdb\u672a\u77e5\u6f02\u79fb\u4e0b\u7684\u81ea\u9002\u5e94\u4f18\u5316\u3002"}}
{"id": "2512.06818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06818", "abs": "https://arxiv.org/abs/2512.06818", "authors": ["Jan Held", "Sanghyun Son", "Renaud Vandeghen", "Daniel Rebain", "Matheus Gadelha", "Yi Zhou", "Anthony Cioppa", "Ming C. Lin", "Marc Van Droogenbroeck", "Andrea Tagliasacchi"], "title": "MeshSplatting: Differentiable Rendering with Opaque Meshes", "comment": null, "summary": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/.", "AI": {"tldr": "MeshSplatting\u662f\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u8054\u5408\u4f18\u5316\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u5b9e\u73b0\u5b9e\u65f63D\u5f15\u64ce\u4e2d\u7684\u9ad8\u6548\u6e32\u67d3\uff0c\u5728Mip-NeRF360\u6570\u636e\u96c6\u4e0aPSNR\u63d0\u53470.69dB\uff0c\u8bad\u7ec3\u901f\u5ea6\u5feb2\u500d\u4e14\u5185\u5b58\u4f7f\u7528\u51cf\u5c112\u500d\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u57fa\u5143\u7684\u65b9\u6cd5\uff08\u59823D\u9ad8\u65af\u6cfc\u6e85\uff09\u4e0eAR/VR\u548c\u6e38\u620f\u5f15\u64ce\u4e2d\u57fa\u4e8e\u7f51\u683c\u7684\u6d41\u7a0b\u4e0d\u517c\u5bb9\u7684\u95ee\u9898\uff0c\u6865\u63a5\u795e\u7ecf\u6e32\u67d3\u548c\u4ea4\u4e92\u5f0f3D\u56fe\u5f62\u4ee5\u5b9e\u73b0\u65e0\u7f1d\u5b9e\u65f6\u573a\u666f\u4ea4\u4e92\u3002", "method": "\u901a\u8fc7\u53d7\u9650Delaunay\u4e09\u89d2\u5256\u5206\u5f3a\u5236\u8fde\u63a5\u6027\u5e76\u4f18\u5316\u8868\u9762\u4e00\u81f4\u6027\uff0c\u8054\u5408\u4f18\u5316\u51e0\u4f55\u548c\u5916\u89c2\u7684\u53ef\u5fae\u5206\u6e32\u67d3\u65b9\u6cd5\u3002", "result": "\u5728Mip-NeRF360\u6570\u636e\u96c6\u4e0a\uff0cMeshSplatting\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684MiLo\u65b9\u6cd5PSNR\u63d0\u53470.69dB\uff0c\u8bad\u7ec3\u901f\u5ea6\u5feb2\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c112\u500d\u3002", "conclusion": "MeshSplatting\u521b\u5efa\u4e86\u7aef\u5230\u7aef\u5e73\u6ed1\u3001\u89c6\u89c9\u8d28\u91cf\u9ad8\u7684\u7f51\u683c\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u5728\u5b9e\u65f63D\u5f15\u64ce\u4e2d\u6e32\u67d3\uff0c\u6210\u529f\u6865\u63a5\u4e86\u795e\u7ecf\u6e32\u67d3\u548c\u4ea4\u4e92\u5f0f3D\u56fe\u5f62\u3002"}}
{"id": "2512.07092", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07092", "abs": "https://arxiv.org/abs/2512.07092", "authors": ["Zhixiang Wang"], "title": "The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models", "comment": "10 pages, 3 figures, 1 table. Code and dataset available at https://huggingface.co/Zx93/Soul-Engine-Qwen2.5-0.5B", "summary": "Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an \"alignment tax\" -- degrading general reasoning capabilities.\n  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.\n  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for \"Zero-Shot Personality Injection\" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.\n  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSoul Engine\u6846\u67b6\uff0c\u57fa\u4e8e\u7ebf\u6027\u8868\u793a\u5047\u8bbe\uff0c\u901a\u8fc7\u63d0\u53d6\u89e3\u8026\u7684\u4eba\u683c\u5411\u91cf\u5b9e\u73b0\u4e2a\u6027\u5316LLM\uff0c\u907f\u514d\u4e86\u5fae\u8c03\u5e26\u6765\u7684\u5bf9\u9f50\u7a0e\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4e2a\u6027\u5316\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u907f\u514d\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982\u76d1\u7763\u5fae\u8c03\uff09\u5bfc\u81f4\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0b\u964d\u7684'\u5bf9\u9f50\u7a0e'\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u7ebf\u6027\u8868\u793a\u5047\u8bbe\uff0c\u4f7f\u7528\u53cc\u5934\u67b6\u6784\u5728\u51bb\u7ed3\u7684Qwen-2.5\u57fa\u7840\u4e0a\uff0c\u901a\u8fc7SoulBench\u6570\u636e\u96c6\u548c\u52a8\u6001\u4e0a\u4e0b\u6587\u91c7\u6837\u63d0\u53d6\u89e3\u8026\u7684\u4eba\u683c\u5411\u91cf\uff0c\u4e0d\u4fee\u6539\u4e3b\u5e72\u6743\u91cd\u3002", "result": "\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4eba\u683c\u5206\u6790\uff08MSE=0.011\uff09\u3001\u51e0\u4f55\u6b63\u4ea4\u6027\u9a8c\u8bc1\uff08\u4eba\u683c\u6d41\u5f62\u8fde\u7eed\u4e14\u72ec\u7acb\uff09\u3001\u96f6\u6837\u672c\u4eba\u683c\u6ce8\u5165\u548c\u786e\u5b9a\u6027\u884c\u4e3a\u63a7\u5236\uff0c\u4fdd\u6301\u539f\u59cb\u6a21\u578b\u667a\u80fd\u3002", "conclusion": "\u6311\u6218\u4e86\u5fae\u8c03\u5728\u4e2a\u6027\u5316\u4e2d\u7684\u5fc5\u8981\u6027\uff0c\u4ece\u6982\u7387\u63d0\u793a\u8f6c\u5411\u786e\u5b9a\u6027\u6f5c\u5728\u5e72\u9884\uff0c\u4e3a\u5b89\u5168\u53ef\u63a7\u7684AI\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u6570\u5b66\u4e25\u8c28\u7684\u57fa\u7840\u3002"}}
{"id": "2512.06838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06838", "abs": "https://arxiv.org/abs/2512.06838", "authors": ["Jiahao Wang", "Zhongwei Jiang", "Wenchao Sun", "Jiaru Zhong", "Haibao Yu", "Yuner Zhang", "Chenyang Lu", "Chuang Zhang", "Lei He", "Shaobing Xu", "Jianqiang Wang"], "title": "SparseCoop: Cooperative Perception with Kinematic-Grounded Queries", "comment": "Accepted by AAAI 2026", "summary": "Cooperative perception is critical for autonomous driving, overcoming the inherent limitations of a single vehicle, such as occlusions and constrained fields-of-view. However, current approaches sharing dense Bird's-Eye-View (BEV) features are constrained by quadratically-scaling communication costs and the lack of flexibility and interpretability for precise alignment across asynchronous or disparate viewpoints. While emerging sparse query-based methods offer an alternative, they often suffer from inadequate geometric representations, suboptimal fusion strategies, and training instability. In this paper, we propose SparseCoop, a fully sparse cooperative perception framework for 3D detection and tracking that completely discards intermediate BEV representations. Our framework features a trio of innovations: a kinematic-grounded instance query that uses an explicit state vector with 3D geometry and velocity for precise spatio-temporal alignment; a coarse-to-fine aggregation module for robust fusion; and a cooperative instance denoising task to accelerate and stabilize training. Experiments on V2X-Seq and Griffin datasets show SparseCoop achieves state-of-the-art performance. Notably, it delivers this with superior computational efficiency, low transmission cost, and strong robustness to communication latency. Code is available at https://github.com/wang-jh18-SVM/SparseCoop.", "AI": {"tldr": "SparseCoop\u662f\u4e00\u4e2a\u5b8c\u5168\u7a00\u758f\u7684\u534f\u540c\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e3D\u68c0\u6d4b\u548c\u8ddf\u8e2a\uff0c\u901a\u8fc7\u4e22\u5f03BEV\u8868\u793a\u89e3\u51b3\u4e86\u901a\u4fe1\u6210\u672c\u9ad8\u548c\u89c6\u89d2\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5355\u8f66\u8f86\u611f\u77e5\u7684\u5c40\u9650\u6027\uff08\u5982\u906e\u6321\u548c\u89c6\u91ce\u53d7\u9650\uff09\uff0c\u540c\u65f6\u514b\u670d\u73b0\u6709\u5bc6\u96c6BEV\u7279\u5f81\u5171\u4eab\u65b9\u6cd5\u7684\u9ad8\u901a\u4fe1\u6210\u672c\u548c\u7a00\u758f\u67e5\u8be2\u65b9\u6cd5\u51e0\u4f55\u8868\u793a\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u521b\u65b0\uff1a\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u5b9e\u4f8b\u67e5\u8be2\uff08\u4f7f\u75283D\u51e0\u4f55\u548c\u901f\u5ea6\u7684\u663e\u5f0f\u72b6\u6001\u5411\u91cf\uff09\u3001\u7c97\u5230\u7cbe\u805a\u5408\u6a21\u5757\u3001\u534f\u540c\u5b9e\u4f8b\u53bb\u566a\u4efb\u52a1\u3002", "result": "\u5728V2X-Seq\u548cGriffin\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5177\u6709\u9ad8\u8ba1\u7b97\u6548\u7387\u3001\u4f4e\u4f20\u8f93\u6210\u672c\u548c\u5f3a\u901a\u4fe1\u5ef6\u8fdf\u9c81\u68d2\u6027\u3002", "conclusion": "SparseCoop\u6846\u67b6\u5728\u534f\u540c\u611f\u77e5\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07100", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07100", "abs": "https://arxiv.org/abs/2512.07100", "authors": ["Hong Wang", "Yinglong Zhang", "Hanhan Guo", "Xuewen Xia", "Xing Xu"], "title": "Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph", "comment": null, "summary": "Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.\n  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.\n  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.", "AI": {"tldr": "DRCL\u662f\u4e00\u4e2a\u5b8c\u5168\u65e0\u76d1\u7763\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u548c\u6587\u672c\u8bed\u4e49\u7684\u53cc\u5411\u7cbe\u70bc\u5faa\u73af\uff0c\u5728\u65e0\u6807\u7b7e\u7684\u6587\u672c\u5c5e\u6027\u7f51\u7edc\u4e0a\u5b9e\u73b0\u793e\u533a\u68c0\u6d4b\u548c\u6587\u672c\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u96be\u4ee5\u90e8\u7f72\uff0c\u4f20\u7edf\u793e\u533a\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u6587\u672c\u8bed\u4e49\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7GCN\u793e\u533a\u68c0\u6d4b\u6a21\u5757\u548c\u6587\u672c\u8bed\u4e49\u5efa\u6a21\u6a21\u5757\u7684\u53cc\u5411\u7cbe\u70bc\u5faa\u73af\uff0c\u8fed\u4ee3\u4ea4\u6362\u4f2a\u6807\u7b7e\uff0c\u5b9e\u73b0\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u7684\u76f8\u4e92\u589e\u5f3a\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u5c5e\u6027\u56fe\u6570\u636e\u96c6\u4e0a\uff0cDRCL\u663e\u8457\u63d0\u5347\u4e86\u53d1\u73b0\u793e\u533a\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u8d28\u91cf\uff0c\u57fa\u4e8e\u5176\u793e\u533a\u4fe1\u53f7\u8bad\u7ec3\u7684Mamba\u5206\u7c7b\u5668\u8fbe\u5230\u63a5\u8fd1\u6709\u76d1\u7763\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "DRCL\u5c55\u793a\u4e86\u5728\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u7684\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u76d1\u7763\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06840", "abs": "https://arxiv.org/abs/2512.06840", "authors": ["Satoshi Hashimoto", "Tatsuya Konishi", "Tomoya Kaichi", "Kazunori Matsumoto", "Mori Kurokawa"], "title": "CADE: Continual Weakly-supervised Video Anomaly Detection with Ensembles", "comment": "Accepted to WACV 2026", "summary": "Video anomaly detection (VAD) has long been studied as a crucial problem in public security and crime prevention. In recent years, weakly-supervised VAD (WVAD) have attracted considerable attention due to their easy annotation process and promising research results. While existing WVAD methods tackle mainly on static datasets, the possibility that the domain of data can vary has been neglected. To adapt such domain-shift, the continual learning (CL) perspective is required because otherwise additional training only with new coming data could easily cause performance degradation for previous data, i.e., forgetting. Therefore, we propose a brand-new approach, called Continual Anomaly Detection with Ensembles (CADE) that is the first work combining CL and WVAD viewpoints. Specifically, CADE uses the Dual-Generator(DG) to address data imbalance and label uncertainty in WVAD. We also found that forgetting exacerbates the \"incompleteness'' where the model becomes biased towards certain anomaly modes, leading to missed detections of various anomalies. To address this, we propose to ensemble Multi-Discriminator (MD) that capture missed anomalies in past scenes due to forgetting, using multiple models. Extensive experiments show that CADE significantly outperforms existing VAD methods on the common multi-scene VAD datasets, such as ShanghaiTech and Charlotte Anomaly datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CADE\u65b9\u6cd5\uff0c\u9996\u6b21\u5c06\u6301\u7eed\u5b66\u4e60\u4e0e\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u53cc\u751f\u6210\u5668\u548c\u591a\u5224\u522b\u5668\u96c6\u6210\u6765\u89e3\u51b3\u9886\u57df\u504f\u79fb\u548c\u9057\u5fd8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u9759\u6001\u6570\u636e\u96c6\uff0c\u5ffd\u89c6\u4e86\u6570\u636e\u9886\u57df\u53ef\u80fd\u53d8\u5316\u7684\u60c5\u51b5\u3002\u4e3a\u9002\u5e94\u9886\u57df\u504f\u79fb\uff0c\u9700\u8981\u6301\u7eed\u5b66\u4e60\u89c6\u89d2\uff0c\u5426\u5219\u4ec5\u7528\u65b0\u6570\u636e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u5bf9\u5148\u524d\u6570\u636e\u7684\u6027\u80fd\u4e0b\u964d\uff08\u9057\u5fd8\uff09\u3002", "method": "\u63d0\u51faCADE\u65b9\u6cd5\uff1a\u4f7f\u7528\u53cc\u751f\u6210\u5668\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u548c\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff1b\u901a\u8fc7\u591a\u5224\u522b\u5668\u96c6\u6210\u6355\u83b7\u56e0\u9057\u5fd8\u800c\u9057\u6f0f\u7684\u5f02\u5e38\u6a21\u5f0f\u3002", "result": "\u5728ShanghaiTech\u548cCharlotte Anomaly\u7b49\u591a\u573a\u666fVAD\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCADE\u663e\u8457\u4f18\u4e8e\u73b0\u6709VAD\u65b9\u6cd5\u3002", "conclusion": "CADE\u662f\u9996\u4e2a\u7ed3\u5408\u6301\u7eed\u5b66\u4e60\u548c\u5f31\u76d1\u7763\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u7684\u5de5\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9886\u57df\u504f\u79fb\u548c\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u591a\u573a\u666f\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.07112", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07112", "abs": "https://arxiv.org/abs/2512.07112", "authors": ["Ziqing Wen", "Jiahuan Wang", "Ping Luo", "Dongsheng Li", "Tao Sun"], "title": "FOAM: Blocked State Folding for Memory-Efficient LLM Training", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\\%, eliminates up to 90\\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.", "AI": {"tldr": "FOAM\u662f\u4e00\u79cd\u65b0\u7684\u5185\u5b58\u9ad8\u6548\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u5757\u68af\u5ea6\u5747\u503c\u538b\u7f29\u548c\u6b8b\u5dee\u6821\u6b63\u6765\u51cf\u5c11Adam\u4f18\u5316\u5668\u7684\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\u5185\u5b58\u74f6\u9888\u4e25\u91cd\uff0c\u73b0\u6709\u5185\u5b58\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u989d\u5916\u5185\u5b58\u9700\u6c42\u6216\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFOAM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u5757\u68af\u5ea6\u5747\u503c\u538b\u7f29\u4f18\u5316\u5668\u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u6b8b\u5dee\u6821\u6b63\u6765\u6062\u590d\u4e22\u5931\u7684\u4fe1\u606f\u3002", "result": "FOAM\u51cf\u5c11\u603b\u8bad\u7ec3\u5185\u5b58\u7ea650%\uff0c\u6d88\u9664\u9ad8\u8fbe90%\u7684\u4f18\u5316\u5668\u72b6\u6001\u5185\u5b58\u5f00\u9500\uff0c\u52a0\u901f\u6536\u655b\uff0c\u6027\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u73b0\u6709\u57fa\u51c6\u3002", "conclusion": "FOAM\u5728\u7406\u8bba\u6536\u655b\u7387\u548c\u5b9e\u9645\u6027\u80fd\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5bb9\u5176\u4ed6\u5185\u5b58\u9ad8\u6548\u4f18\u5316\u5668\uff0c\u662f\u89e3\u51b3LLM\u8bad\u7ec3\u5185\u5b58\u74f6\u9888\u7684\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.06845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06845", "abs": "https://arxiv.org/abs/2512.06845", "authors": ["Satoshi Hashimoto", "Hitoshi Nishimura", "Yanan Wang", "Mori Kurokawa"], "title": "Pseudo Anomalies Are All You Need: Diffusion-Based Generation for Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Deploying video anomaly detection in practice is hampered by the scarcity and collection cost of real abnormal footage. We address this by training without any real abnormal videos while evaluating under the standard weakly supervised split, and we introduce PA-VAD, a generation-driven approach that learns a detector from synthesized pseudo-abnormal videos paired with real normal videos, using only a small set of real normal images to drive synthesis. For synthesis, we select class-relevant initial images with CLIP and refine textual prompts with a vision-language model to improve fidelity and scene consistency before invoking a video diffusion model. For training, we mitigate excessive spatiotemporal magnitude in synthesized anomalies by an domain-aligned regularized module that combines domain alignment and memory usage-aware updates. Extensive experiments show that our approach reaches 98.2% on ShanghaiTech and 82.5% on UCF-Crime, surpassing the strongest real-abnormal method on ShanghaiTech by +0.6% and outperforming the UVAD state-of-the-art on UCF-Crime by +1.9%. The results demonstrate that high-accuracy anomaly detection can be obtained without collecting real anomalies, providing a practical path toward scalable deployment.", "AI": {"tldr": "PA-VAD\u662f\u4e00\u79cd\u65e0\u9700\u771f\u5b9e\u5f02\u5e38\u89c6\u9891\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u4f2a\u5f02\u5e38\u89c6\u9891\u4e0e\u771f\u5b9e\u6b63\u5e38\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u6807\u51c6\u5f31\u76d1\u7763\u5206\u5272\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9645\u90e8\u7f72\u4e2d\u771f\u5b9e\u5f02\u5e38\u89c6\u9891\u7a00\u7f3a\u4e14\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4ec5\u4f7f\u7528\u6b63\u5e38\u89c6\u9891\u8bad\u7ec3\u5f02\u5e38\u68c0\u6d4b\u5668\u7684\u53ef\u884c\u8def\u5f84\u3002", "method": "1) \u4f7f\u7528CLIP\u9009\u62e9\u7c7b\u522b\u76f8\u5173\u521d\u59cb\u56fe\u50cf\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u6587\u672c\u63d0\u793a\u4ee5\u63d0\u5347\u5408\u6210\u8d28\u91cf\uff1b2) \u8c03\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u4f2a\u5f02\u5e38\u89c6\u9891\uff1b3) \u901a\u8fc7\u57df\u5bf9\u9f50\u6b63\u5219\u5316\u6a21\u5757\u7f13\u89e3\u5408\u6210\u5f02\u5e38\u4e2d\u7684\u65f6\u7a7a\u5e45\u5ea6\u8fc7\u5927\u95ee\u9898\u3002", "result": "\u5728ShanghaiTech\u4e0a\u8fbe\u523098.2%\uff0c\u5728UCF-Crime\u4e0a\u8fbe\u523082.5%\uff0c\u5206\u522b\u6bd4\u6700\u5f3a\u7684\u771f\u5b9e\u5f02\u5e38\u65b9\u6cd5\u9ad8\u51fa0.6%\u548c\u6bd4UVAD SOTA\u65b9\u6cd5\u9ad8\u51fa1.9%\u3002", "conclusion": "\u8bc1\u660e\u65e0\u9700\u6536\u96c6\u771f\u5b9e\u5f02\u5e38\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e3a\u53ef\u6269\u5c55\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2512.07113", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2512.07113", "abs": "https://arxiv.org/abs/2512.07113", "authors": ["Kepeng Lin", "Qizhe Zhang", "Rui Wang", "Xuehai Hu", "Wei Xu"], "title": "PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes", "comment": "6 pages, 5 figures, accept to BIBM", "summary": "Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE", "AI": {"tldr": "PlantBiMoE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u690d\u7269\u57fa\u56e0\u7ec4\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5411Mamba\u548c\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u53c2\u6570\u8fc7\u591a\u548c\u53cc\u5411\u5efa\u6a21\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u572831\u4e2a\u6570\u636e\u96c6\u4e2d\u768420\u4e2a\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u690d\u7269\u57fa\u56e0\u7ec4\u6a21\u578b\u5982AgroNT\u548cPDLLMs\u5b58\u5728\u53c2\u6570\u89c4\u6a21\u8fc7\u5927\u548c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21DNA\u53cc\u94fe\u53cc\u5411\u6027\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faPlantBiMoE\u6a21\u578b\uff0c\u96c6\u6210\u53cc\u5411Mamba\u6765\u6355\u6349DNA\u6b63\u53cd\u94fe\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6846\u67b6\u51cf\u5c11\u6fc0\u6d3b\u53c2\u6570\u6570\u91cf\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728\u589e\u5f3a\u7684\u57fa\u56e0\u7ec4\u57fa\u51c6\u6d4b\u8bd5MPGB\uff08\u5305\u542b31\u4e2a\u6570\u636e\u96c6\u300111\u4e2a\u4efb\u52a1\uff09\u4e0a\uff0cPlantBiMoE\u572820\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u5e73\u5747\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "PlantBiMoE\u80fd\u6709\u6548\u8868\u793a\u690d\u7269\u57fa\u56e0\u7ec4\u5e8f\u5217\uff0c\u4e3a\u57fa\u56e0\u7ec4\u4efb\u52a1\u63d0\u4f9b\u5f3a\u5927\u8ba1\u7b97\u5de5\u5177\uff0c\u5bf9\u690d\u7269\u57fa\u56e0\u7ec4\u5b66\u3001\u57fa\u56e0\u7f16\u8f91\u548c\u5408\u6210\u751f\u7269\u5b66\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2512.06849", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06849", "abs": "https://arxiv.org/abs/2512.06849", "authors": ["Matan Atad", "Alexander W. Marka", "Lisa Steinhelfer", "Anna Curto-Vilalta", "Yannik Leonhardt", "Sarah C. Foreman", "Anna-Sophia Walburga Dietrich", "Robert Graf", "Alexandra S. Gersing", "Bjoern Menze", "Daniel Rueckert", "Jan S. Kirschke", "Hendrik M\u00f6ller"], "title": "Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT", "comment": "In submission", "summary": "Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u690e\u4f53\u7ea7\u522b\u7684\u5065\u5eb7/\u6076\u6027\u6807\u7b7e\uff08\u65e0\u9700\u75c5\u53d8\u63a9\u7801\uff09\u6765\u5206\u5272CT\u4e2d\u7684\u690e\u4f53\u8f6c\u79fb\u7076\uff0c\u901a\u8fc7\u6269\u6563\u81ea\u7f16\u7801\u5668\u548c\u50cf\u7d20\u7ea7\u5dee\u5f02\u56fe\u7ed3\u5408\u9009\u62e9\u6027\u906e\u6321\u6280\u672f\u5b9e\u73b0\u51c6\u786e\u5206\u5272\u3002", "motivation": "\u690e\u4f53\u8f6c\u79fb\u7076\u7684\u7cbe\u786e\u5206\u5272\u5728\u4e34\u5e8a\u4e0a\u5f88\u91cd\u8981\u4f46\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u56e0\u4e3a\u4f53\u7d20\u7ea7\u6ce8\u91ca\u7a00\u7f3a\uff0c\u4e14\u6eb6\u9aa8\u6027\u548c\u6210\u9aa8\u6027\u75c5\u53d8\u5e38\u4e0e\u826f\u6027\u9000\u884c\u6027\u53d8\u5316\u76f8\u4f3c\u3002", "method": "\u7ed3\u5408\u6269\u6563\u81ea\u7f16\u7801\u5668\u751f\u6210\u5065\u5eb7\u7f16\u8f91\u7248\u672c\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u5dee\u5f02\u56fe\u63d0\u51fa\u5019\u9009\u75c5\u53d8\u533a\u57df\uff0c\u4f7f\u7528Hide-and-Seek Attribution\u6280\u672f\u9010\u4e2a\u8bc4\u4f30\u5019\u9009\u533a\u57df\u7684\u6076\u6027\u8d21\u732e\u5ea6\uff0c\u9ad8\u8bc4\u5206\u533a\u57df\u5f62\u6210\u6700\u7ec8\u5206\u5272\u3002", "result": "\u5728\u4fdd\u7559\u7684\u653e\u5c04\u79d1\u533b\u751f\u6ce8\u91ca\u4e0a\uff0c\u65e0\u9700\u63a9\u7801\u76d1\u7763\u5373\u8fbe\u5230\u5f3a\u6027\u80fd\uff08\u6210\u9aa8\u6027F1:0.91/Dice:0.87\uff1b\u6eb6\u9aa8\u6027F1:0.85/Dice:0.78\uff09\uff0c\u8d85\u8fc7\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u690e\u4f53\u7ea7\u6807\u7b7e\u53ef\u8f6c\u5316\u4e3a\u53ef\u9760\u7684\u75c5\u53d8\u63a9\u7801\uff0c\u751f\u6210\u7f16\u8f91\u4e0e\u9009\u62e9\u6027\u906e\u6321\u76f8\u7ed3\u5408\u652f\u6301CT\u4e2d\u51c6\u786e\u7684\u5f31\u76d1\u7763\u5206\u5272\u3002"}}
{"id": "2512.07142", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.07142", "abs": "https://arxiv.org/abs/2512.07142", "authors": ["Tanay Arora", "Christof Teuscher"], "title": "Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search", "comment": "This work plans to be submitted to the IEEE for possible publication", "summary": "The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faConcrete Ticket Search (CTS)\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u5b50\u7f51\u7edc\u53d1\u73b0\u5efa\u6a21\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528Concrete\u677e\u5f1b\u548c\u68af\u5ea6\u5e73\u8861\u65b9\u6848\uff0c\u9ad8\u6548\u8bc6\u522b\u9ad8\u6027\u80fd\u7a00\u758f\u5b50\u7f51\u7edc\uff0c\u5728\u4fdd\u6301\u9ad8\u7a00\u758f\u5ea6\u7684\u540c\u65f6\u8fbe\u5230\u6216\u8d85\u8fc7Lottery Ticket Rewinding\u7684\u7cbe\u5ea6\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u5927\u5e45\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u7684\u5f69\u7968\u7968\u5047\u8bbe\u65b9\u6cd5\u4e2d\uff0cLTR\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u800c\u57fa\u4e8e\u663e\u8457\u6027\u7684\u521d\u59cb\u5316\u526a\u679d\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6-\u7a00\u758f\u5ea6\u6743\u8861\u95ee\u9898\u4e14\u65e0\u6cd5\u901a\u8fc7\u57fa\u672c\u9a8c\u8bc1\u6d4b\u8bd5\u3002\u4f5c\u8005\u8ba4\u4e3aPaI\u65b9\u6cd5\u4f9d\u8d56\u4e00\u9636\u663e\u8457\u6027\u6307\u6807\u5ffd\u7565\u4e86\u6743\u91cd\u95f4\u4f9d\u8d56\u5173\u7cfb\u662f\u6027\u80fd\u5dee\u8ddd\u7684\u4e3b\u8981\u539f\u56e0\u3002", "method": "\u63d0\u51faCTS\u7b97\u6cd5\uff0c\u5c06\u5b50\u7f51\u7edc\u53d1\u73b0\u5efa\u6a21\u4e3a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528Concrete\u677e\u5f1b\u5904\u7406\u79bb\u6563\u641c\u7d22\u7a7a\u95f4\uff0c\u5f15\u5165GRADBALANCE\u68af\u5ea6\u5e73\u8861\u65b9\u6848\u63a7\u5236\u7a00\u758f\u5ea6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u526a\u679d\u76ee\u6807\uff08\u7279\u522b\u662f\u6700\u5c0f\u5316\u7a00\u758f\u4e0e\u7a20\u5bc6\u7f51\u7edc\u8f93\u51fa\u7684\u53cd\u5411KL\u6563\u5ea6CTS-KL\uff09\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cCTS\u751f\u6210\u7684\u5b50\u7f51\u7edc\u80fd\u7a33\u5b9a\u901a\u8fc7\u9a8c\u8bc1\u6d4b\u8bd5\uff0c\u7cbe\u5ea6\u8fbe\u5230\u6216\u8d85\u8fc7LTR\uff0c\u8ba1\u7b97\u6210\u672c\u4ec5\u4e3aLTR\u7684\u4e00\u5c0f\u90e8\u5206\u3002\u4f8b\u5982\u5728ResNet-20/CIFAR10\u4e0a\uff0c99.3%\u7a00\u758f\u5ea6\u4e0b\u8fbe\u523074.0%\u7cbe\u5ea6\uff08LTR\u4e3a68.3%\uff09\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ece95.2\u5206\u949f\u964d\u81f37.9\u5206\u949f\u3002", "conclusion": "CTS\u65b9\u6cd5\u5728\u9ad8\u5ea6\u7a00\u758f\u533a\u57df\u4f18\u52bf\u660e\u663e\uff0c\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u9ad8\u6027\u80fd\u5b50\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7cbe\u5ea6-\u7a00\u758f\u5ea6\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5f69\u7968\u7968\u5047\u8bbe\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2512.06862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06862", "abs": "https://arxiv.org/abs/2512.06862", "authors": ["Qiancheng Zheng", "Yunhang Shen", "Gen Luo", "Baiyang Song", "Xing Sun", "Xiaoshuai Sun", "Yiyi Zhou", "Rongrong Ji"], "title": "Omni-Referring Image Segmentation", "comment": null, "summary": "In this paper, we propose a novel task termed Omni-Referring Image Segmentation (OmniRIS) towards highly generalized image segmentation. Compared with existing unimodally conditioned segmentation tasks, such as RIS and visual RIS, OmniRIS supports the input of text instructions and reference images with masks, boxes or scribbles as omni-prompts. This property makes it can well exploit the intrinsic merits of both text and visual modalities, i.e., granular attribute referring and uncommon object grounding, respectively. Besides, OmniRIS can also handle various segmentation settings, such as one v.s. many and many v.s. many, further facilitating its practical use. To promote the research of OmniRIS, we also rigorously design and construct a large dataset termed OmniRef, which consists of 186,939 omni-prompts for 30,956 images, and establish a comprehensive evaluation system. Moreover, a strong and general baseline termed OmniSegNet is also proposed to tackle the key challenges of OmniRIS, such as omni-prompt encoding. The extensive experiments not only validate the capability of OmniSegNet in following omni-modal instructions, but also show the superiority of OmniRIS for highly generalized image segmentation.", "AI": {"tldr": "\u63d0\u51faOmniRIS\u4efb\u52a1\uff0c\u652f\u6301\u6587\u672c\u6307\u4ee4\u548c\u89c6\u89c9\u53c2\u8003\u56fe\u50cf\u4f5c\u4e3aomni-prompts\uff0c\u5b9e\u73b0\u9ad8\u5ea6\u6cdb\u5316\u7684\u56fe\u50cf\u5206\u5272\u3002\u6784\u5efa\u4e86OmniRef\u6570\u636e\u96c6\u548cOmniSegNet\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5206\u5272\u4efb\u52a1\u591a\u4e3a\u5355\u6a21\u6001\u8f93\u5165\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u4f18\u52bf\u3002OmniRIS\u65e8\u5728\u7ed3\u5408\u6587\u672c\u7684\u7ec6\u7c92\u5ea6\u5c5e\u6027\u63cf\u8ff0\u548c\u89c6\u89c9\u7684\u7f55\u89c1\u7269\u4f53\u5b9a\u4f4d\u80fd\u529b\u3002", "method": "\u63d0\u51faOmniRIS\u4efb\u52a1\u6846\u67b6\uff0c\u652f\u6301\u6587\u672c\u6307\u4ee4\u548c\u5e26\u63a9\u7801/\u6846/\u6d82\u9e26\u7684\u53c2\u8003\u56fe\u50cf\u4f5c\u4e3a\u591a\u6a21\u6001\u63d0\u793a\u3002\u6784\u5efaOmniRef\u6570\u636e\u96c6\uff08186,939\u4e2a\u63d0\u793a\uff0c30,956\u5f20\u56fe\u50cf\uff09\uff0c\u5f00\u53d1OmniSegNet\u57fa\u7ebf\u6a21\u578b\u5904\u7406omni-prompt\u7f16\u7801\u7b49\u5173\u952e\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86OmniSegNet\u80fd\u591f\u6709\u6548\u9075\u5faa\u591a\u6a21\u6001\u6307\u4ee4\uff0c\u5e76\u8bc1\u660e\u4e86OmniRIS\u5728\u9ad8\u5ea6\u6cdb\u5316\u56fe\u50cf\u5206\u5272\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "OmniRIS\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u66f4\u901a\u7528\u548c\u5b9e\u7528\u7684\u56fe\u50cf\u5206\u5272\uff0c\u4e3a\u9ad8\u5ea6\u6cdb\u5316\u5206\u5272\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.07150", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07150", "abs": "https://arxiv.org/abs/2512.07150", "authors": ["Jonghyun Park", "Jong Chul Ye"], "title": "FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers", "comment": null, "summary": "Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.", "AI": {"tldr": "\u63d0\u51fa\u4e86FlowLPS\u6846\u67b6\uff0c\u901a\u8fc7Langevin Proximal Sampling\u7b56\u7565\u89e3\u51b3\u9884\u8bad\u7ec3\u6d41\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u6536\u655b\u548c\u6d41\u5f62\u504f\u5dee\u95ee\u9898", "motivation": "\u73b0\u6709\u8bad\u7ec3\u81ea\u7531\u65b9\u6cd5\u5728\u5e94\u7528\u4e8e\u6f5c\u5728\u6d41\u6a21\u578b\u65f6\uff0c\u5e38\u65e0\u6cd5\u6536\u655b\u5230\u540e\u9a8c\u6a21\u5f0f\u6216\u906d\u53d7\u6f5c\u5728\u7a7a\u95f4\u5185\u7684\u6d41\u5f62\u504f\u5dee", "method": "\u7ed3\u5408Langevin\u52a8\u529b\u5b66\u8fdb\u884c\u6d41\u5f62\u4e00\u81f4\u6027\u63a2\u7d22\u548c\u8fd1\u7aef\u4f18\u5316\u8fdb\u884c\u7cbe\u786e\u6a21\u5f0f\u641c\u7d22\u7684Langevin Proximal Sampling\u7b56\u7565", "result": "\u5728FFHQ\u548cDIV2K\u7684\u591a\u4e2a\u9006\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u7684\u4f18\u8d8a\u5e73\u8861\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u7684\u9006\u6c42\u89e3\u5668", "conclusion": "FlowLPS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6f5c\u5728\u6d41\u6a21\u578b\u5728\u9006\u95ee\u9898\u6c42\u89e3\u4e2d\u7684\u6536\u655b\u548c\u6d41\u5f62\u504f\u5dee\u95ee\u9898"}}
{"id": "2512.06864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06864", "abs": "https://arxiv.org/abs/2512.06864", "authors": ["Kaixuan Lu", "Mehmet Onurcan Kaya", "Dim P. Papadopoulos"], "title": "Boosting Unsupervised Video Instance Segmentation with Automatic Quality-Guided Self-Training", "comment": "Accepted to WACV 2026. arXiv admin note: substantial text overlap with arXiv:2508.19808", "summary": "Video Instance Segmentation (VIS) faces significant annotation challenges due to its dual requirements of pixel-level masks and temporal consistency labels. While recent unsupervised methods like VideoCutLER eliminate optical flow dependencies through synthetic data, they remain constrained by the synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised framework that bridges this gap through quality-guided self-training. Our approach establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos. Experiments demonstrate state-of-the-art performance with 52.6 $\\text{AP}_{50}$ on YouTubeVIS-2019 $\\texttt{val}$ set, surpassing the previous state-of-the-art VideoCutLER by 4.4%, while requiring no human annotations. This demonstrates the viability of quality-aware self-training for unsupervised VIS. We will release the code at https://github.com/wcbup/AutoQ-VIS.", "AI": {"tldr": "AutoQ-VIS\u662f\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u7684\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\u89e3\u51b3\u5408\u6210\u6570\u636e\u5230\u771f\u5b9e\u89c6\u9891\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\uff0c\u5728YouTubeVIS-2019\u6570\u636e\u96c6\u4e0a\u8fbe\u523052.6 AP50\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u9762\u4e34\u50cf\u7d20\u7ea7\u63a9\u7801\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\u6807\u7b7e\u7684\u53cc\u91cd\u6807\u6ce8\u6311\u6218\u3002\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u5982VideoCutLER\u867d\u7136\u901a\u8fc7\u5408\u6210\u6570\u636e\u6d88\u9664\u4e86\u5149\u6d41\u4f9d\u8d56\uff0c\u4f46\u4ecd\u53d7\u9650\u4e8e\u5408\u6210\u5230\u771f\u5b9e\u9886\u57df\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u8d28\u91cf\u5f15\u5bfc\u7684\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u5efa\u7acb\u4f2a\u6807\u7b7e\u751f\u6210\u548c\u81ea\u52a8\u8d28\u91cf\u8bc4\u4f30\u4e4b\u95f4\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4ece\u5408\u6210\u89c6\u9891\u5230\u771f\u5b9e\u89c6\u9891\u7684\u6e10\u8fdb\u5f0f\u9002\u5e94\u3002", "result": "\u5728YouTubeVIS-2019\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u523052.6 AP50\uff0c\u8d85\u8d8a\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5VideoCutLER 4.4%\uff0c\u4e14\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "\u8bc1\u660e\u4e86\u8d28\u91cf\u611f\u77e5\u81ea\u8bad\u7ec3\u5728\u65e0\u76d1\u7763\u89c6\u9891\u5b9e\u4f8b\u5206\u5272\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u65e0\u76d1\u7763VIS\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07173", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07173", "abs": "https://arxiv.org/abs/2512.07173", "authors": ["Jucheng Shen", "Gaurav Sarkar", "Yeonju Ro", "Sharath Nittur Sridhar", "Zhangyang Wang", "Aditya Akella", "Souvik Kundu"], "title": "Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration", "comment": "8 pages, 3 figures. Preprint under review", "summary": "We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.", "AI": {"tldr": "CadLLM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u751f\u6210\u5757\u5927\u5c0f\u3001\u6b65\u957f\u548c\u9608\u503c\u6765\u52a0\u901f\u57fa\u4e8e\u6269\u6563\u7684LLMs\u63a8\u7406\u541e\u5410\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u52a8\u6001\u8bcd\u6c47\u5b50\u96c6\u51cf\u5c11softmax\u5f00\u9500\uff0c\u5b9e\u73b0\u6700\u9ad82.28\u500d\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u57fa\u4e8e\u6269\u6563\u7684LLMs\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u7279\u522b\u662ftoken unmasking\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u53d8\u5316\u7279\u6027\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u65b9\u6cd5\u6765\u4f18\u5316\u63a8\u7406\u6027\u80fd\u3002", "method": "1. \u5206\u6790token unmasking\u7f6e\u4fe1\u5ea6\u5728\u5757\u548c\u6b65\u9aa4\u95f4\u7684\u52a8\u6001\u7279\u6027\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u5e73\u5747\u7f6e\u4fe1\u5ea6\u7684\u8f7b\u91cf\u7ea7\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u63a7\u5236\u751f\u6210\u5757\u5927\u5c0f\u3001\u6b65\u957f\u548c\u9608\u503c\uff1b3. \u901a\u8fc7\u52a8\u6001\u8bcd\u6c47\u5b50\u96c6\u51cf\u5c11softmax\u8ba1\u7b97\u5f00\u9500\uff1b4. \u8bbe\u8ba1\u4e3a\u5373\u63d2\u5373\u7528\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff0c\u517c\u5bb9\u57fa\u4e8eKV\u7f13\u5b58\u7684dLLMs\u3002", "result": "\u5728\u56db\u4e2a\u6d41\u884c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCadLLM\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad82.28\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u3002", "conclusion": "CadLLM\u8bc1\u660e\u4e86\u901a\u8fc7\u52a8\u6001\u7f6e\u4fe1\u5ea6\u5206\u6790\u548c\u81ea\u9002\u5e94\u53c2\u6570\u8c03\u6574\u53ef\u4ee5\u6709\u6548\u52a0\u901fdLLMs\u63a8\u7406\uff0c\u4e3a\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u63a8\u7406\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06865", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06865", "abs": "https://arxiv.org/abs/2512.06865", "authors": ["Xiaosong Jia", "Chenhe Zhang", "Yule Jiang", "Songbur Wong", "Zhiyuan Zhang", "Chen Chen", "Shaofeng Zhang", "Xuanhe Zhou", "Xue Yang", "Junchi Yan", "Yu-Gang Jiang"], "title": "Spatial Retrieval Augmented Autonomous Driving", "comment": "Demo Page: https://spatialretrievalad.github.io/ with open sourced code, dataset, and checkpoints", "summary": "Existing autonomous driving systems rely on onboard sensors (cameras, LiDAR, IMU, etc) for environmental perception. However, this paradigm is limited by the drive-time perception horizon and often fails under limited view scope, occlusion or extreme conditions such as darkness and rain. In contrast, human drivers are able to recall road structure even under poor visibility. To endow models with this ``recall\" ability, we propose the spatial retrieval paradigm, introducing offline retrieved geographic images as an additional input. These images are easy to obtain from offline caches (e.g, Google Maps or stored autonomous driving datasets) without requiring additional sensors, making it a plug-and-play extension for existing AD tasks.\n  For experiments, we first extend the nuScenes dataset with geographic images retrieved via Google Maps APIs and align the new data with ego-vehicle trajectories. We establish baselines across five core autonomous driving tasks: object detection, online mapping, occupancy prediction, end-to-end planning, and generative world modeling. Extensive experiments show that the extended modality could enhance the performance of certain tasks. We will open-source dataset curation code, data, and benchmarks for further study of this new autonomous driving paradigm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7a7a\u95f4\u68c0\u7d22\u8303\u5f0f\uff0c\u901a\u8fc7\u5f15\u5165\u79bb\u7ebf\u68c0\u7d22\u7684\u5730\u7406\u56fe\u50cf\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\uff0c\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\uff0c\u7a81\u7834\u4f20\u7edf\u8f66\u8f7d\u4f20\u611f\u5668\u7684\u89c6\u91ce\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u8f66\u8f7d\u4f20\u611f\u5668\uff0c\u5b58\u5728\u611f\u77e5\u89c6\u91ce\u6709\u9650\u3001\u6613\u53d7\u906e\u6321\u548c\u6076\u52a3\u5929\u6c14\u5f71\u54cd\u7684\u95ee\u9898\u3002\u4eba\u7c7b\u9a7e\u9a76\u5458\u80fd\u591f\u56de\u5fc6\u9053\u8def\u7ed3\u6784\uff0c\u56e0\u6b64\u5e0c\u671b\u8d4b\u4e88\u6a21\u578b\u7c7b\u4f3c\u7684\"\u56de\u5fc6\"\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u68c0\u7d22\u8303\u5f0f\uff0c\u4ece\u79bb\u7ebf\u7f13\u5b58\uff08\u5982Google Maps\u6216\u5b58\u50a8\u7684\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\uff09\u83b7\u53d6\u5730\u7406\u56fe\u50cf\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\u3002\u6269\u5c55nuScenes\u6570\u636e\u96c6\uff0c\u901a\u8fc7Google Maps API\u68c0\u7d22\u5730\u7406\u56fe\u50cf\u5e76\u4e0e\u81ea\u8f66\u8f68\u8ff9\u5bf9\u9f50\u3002", "result": "\u5728\u4e94\u4e2a\u6838\u5fc3\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\uff08\u76ee\u6807\u68c0\u6d4b\u3001\u5728\u7ebf\u5efa\u56fe\u3001\u5360\u636e\u9884\u6d4b\u3001\u7aef\u5230\u7aef\u89c4\u5212\u548c\u751f\u6210\u5f0f\u4e16\u754c\u5efa\u6a21\uff09\u4e0a\u5efa\u7acb\u57fa\u7ebf\uff0c\u5b9e\u9a8c\u8868\u660e\u6269\u5c55\u6a21\u6001\u80fd\u591f\u63d0\u5347\u67d0\u4e9b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u7a7a\u95f4\u68c0\u7d22\u8303\u5f0f\u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u6269\u5c55\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u5373\u53ef\u589e\u5f3a\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u3002\u5c06\u5f00\u6e90\u6570\u636e\u96c6\u6784\u5efa\u4ee3\u7801\u3001\u6570\u636e\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4fc3\u8fdb\u8fd9\u4e00\u65b0\u8303\u5f0f\u7684\u7814\u7a76\u3002"}}
{"id": "2512.07175", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07175", "abs": "https://arxiv.org/abs/2512.07175", "authors": ["Yibo Wang", "Qing-Guo Chen", "Zhao Xu", "Weihua Luo", "Kaifu Zhang", "Lijun Zhang"], "title": "SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models", "comment": "NeurIPS 2025", "summary": "Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.", "AI": {"tldr": "SPACE\u662f\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u5bf9\u5f08\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u6765\u6355\u6349\u771f\u5b9e\u6570\u636e\u5206\u5e03\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8e\u5956\u52b1\u5dee\u8ddd\u7684\u65b9\u6cd5\u5b58\u5728\u7684\u4e0d\u7a33\u5b9a\u6f14\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u81ea\u5bf9\u5f08\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u771f\u5b9e\u6837\u672c\u548c\u5408\u6210\u6837\u672c\u4e4b\u95f4\u7684\u5956\u52b1\u5dee\u8ddd\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u7684\u7edd\u5bf9\u503c\uff0c\u5bfc\u81f4\u76ee\u6807\u51fd\u6570\u53ef\u80fd\u9000\u5316\uff0c\u5f15\u53d1\u4e0d\u7a33\u5b9a\u6f14\u5316\u3002", "method": "SPACE\u5c06\u5408\u6210\u6837\u672c\u89c6\u4e3a\u8f85\u52a9\u6210\u5206\uff0c\u4ee5\u4e8c\u5143\u5206\u7c7b\u65b9\u5f0f\u533a\u5206\u5b83\u4eec\u4e0e\u771f\u5b9e\u6837\u672c\uff0c\u72ec\u7acb\u4f18\u5316\u6bcf\u7c7b\u6570\u636e\u7684\u7edd\u5bf9\u5956\u52b1\u503c\uff0c\u786e\u4fdd\u76ee\u6807\u51fd\u6570\u59cb\u7ec8\u6709\u610f\u4e49\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eSPACE\u7684\u6700\u4f18\u89e3\u4e0e\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e00\u81f4\uff0c\u4e14\u4fdd\u8bc1\u7a33\u5b9a\u6536\u655b\u3002\u5b9e\u8bc1\u663e\u793aSPACE\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f18\u4e8e\u4f7f\u7528\u66f4\u591a\u771f\u5b9e\u6837\u672c\u7684\u76d1\u7763\u5fae\u8c03\u3002", "conclusion": "SPACE\u901a\u8fc7\u566a\u58f0\u5bf9\u6bd4\u4f30\u8ba1\u89e3\u51b3\u4e86\u81ea\u5bf9\u5f08\u5fae\u8c03\u7684\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u548c\u7a33\u5b9a\u6f14\u5316\u3002"}}
{"id": "2512.06866", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06866", "abs": "https://arxiv.org/abs/2512.06866", "authors": ["Yulin Li", "Haokun Gui", "Ziyang Fan", "Junjie Wang", "Bin Kang", "Bin Chen", "Zhuotao Tian"], "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior", "comment": "Accepted by NeurIPS 2025", "summary": "Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .", "AI": {"tldr": "DyToK\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u4ee4\u724c\u538b\u7f29\u65b9\u6cd5\uff0c\u5229\u7528VLLM\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u89c6\u9891\u5185\u5bb9\u7684\u9ad8\u6548\u5904\u7406\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u589e\u957f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u5173\u952e\u5e27\u91c7\u6837\u65b9\u6cd5\u5b58\u5728\u989d\u5916\u8ba1\u7b97\u6210\u672c\u548c\u6b21\u4f18\u7684\u4e8c\u5143\u5e27\u9009\u62e9\u8303\u5f0f\u3002", "method": "\u901a\u8fc7\u5206\u6790VLLM\u6ce8\u610f\u529b\u5c42\u81ea\u7136\u7f16\u7801\u7684\u67e5\u8be2\u6761\u4ef6\u5173\u952e\u5e27\u5148\u9a8c\uff0c\u52a8\u6001\u8c03\u6574\u6bcf\u5e27\u4ee4\u724c\u4fdd\u7559\u6bd4\u4f8b\uff0c\u4f18\u5148\u4fdd\u7559\u8bed\u4e49\u4e30\u5bcc\u7684\u5e27\u800c\u6291\u5236\u5197\u4f59\u5185\u5bb9\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDyToK\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\uff0c\u4e0e\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u517c\u5bb9\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53474.3\u500d\u7684\u540c\u65f6\u4fdd\u6301\u7cbe\u5ea6\u3002", "conclusion": "DyToK\u63d0\u4f9b\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u9ad8\u6548\u89c6\u9891\u5904\u7406\u65b9\u6848\uff0c\u4e3a\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ba1\u7b97\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2512.07184", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07184", "abs": "https://arxiv.org/abs/2512.07184", "authors": ["Da Zhang", "Bingyu Li", "Zhuyuan Zhao", "Junyu Gao", "Feiping Nie", "Xuelong Li"], "title": "UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting", "comment": null, "summary": "As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.", "AI": {"tldr": "UniDiff\u662f\u4e00\u4e2a\u7edf\u4e00\u6269\u6563\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u548c\u65b0\u578b\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u673a\u5236\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\u591a\u6a21\u6001\u6570\u636e\u6fc0\u589e\uff0c\u4f46\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u6570\u503c\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u590d\u6742\u5f02\u6784\u6570\u636e\u4e2d\u7684\u8de8\u6a21\u6001\u4fe1\u53f7\u3002", "method": "1) \u5c06\u65f6\u95f4\u5e8f\u5217\u5206\u5757\u5e76\u6620\u5c04\u5230\u5d4c\u5165\u7a7a\u95f4\uff1b2) \u4f7f\u7528\u7edf\u4e00\u7684\u5e76\u884c\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u5355\u4e00\u8de8\u6ce8\u610f\u529b\u673a\u5236\u6574\u5408\u65f6\u95f4\u6233\u7684\u7ed3\u6784\u4fe1\u606f\u548c\u6587\u672c\u7684\u8bed\u4e49\u4fe1\u606f\uff1b3) \u5f15\u5165\u591a\u6e90\u6761\u4ef6\u7684\u65b0\u578b\u5206\u7c7b\u5668\u65e0\u5173\u5f15\u5bfc\u673a\u5236\u3002", "result": "\u5728\u516b\u4e2a\u9886\u57df\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cUniDiff\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "UniDiff\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u878d\u5408\u548c\u7075\u6d3b\u7684\u5f15\u5bfc\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2512.06870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06870", "abs": "https://arxiv.org/abs/2512.06870", "authors": ["Wangkai Li", "Rui Sun", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective", "comment": "Accepted by Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Pseudo-label learning is widely used in semantic segmentation, particularly in label-scarce scenarios such as unsupervised domain adaptation (UDA) and semisupervised learning (SSL). Despite its success, this paradigm can generate erroneous pseudo-labels, which are further amplified during training due to utilization of one-hot encoding. To address this issue, we propose ECOCSeg, a novel perspective for segmentation models that utilizes error-correcting output codes (ECOC) to create a fine-grained encoding for each class. ECOCSeg offers several advantages. First, an ECOC-based classifier is introduced, enabling model to disentangle classes into attributes and handle partial inaccurate bits, improving stability and generalization in pseudo-label learning. Second, a bit-level label denoising mechanism is developed to generate higher-quality pseudo-labels, providing adequate and robust supervision for unlabeled images. ECOCSeg can be easily integrated with existing methods and consistently demonstrates significant improvements on multiple UDA and SSL benchmarks across different segmentation architectures. Code is available at https://github.com/Woof6/ECOCSeg.", "AI": {"tldr": "ECOCSeg\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea0\u9519\u8f93\u51fa\u7801\u7684\u8bed\u4e49\u5206\u5272\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7f16\u7801\u548c\u4f4d\u7ea7\u6807\u7b7e\u53bb\u566a\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4f2a\u6807\u7b7e\u5b66\u4e60\u4e2d\u7684\u9519\u8bef\u4f20\u64ad\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4f2a\u6807\u7b7e\u5b66\u4e60\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u5b58\u5728\u9519\u8bef\u6807\u7b7e\u653e\u5927\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728UDA\u548cSSL\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u4f7f\u7528one-hot\u7f16\u7801\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u3002", "method": "\u5f15\u5165ECOC\u5206\u7c7b\u5668\u5c06\u7c7b\u522b\u5206\u89e3\u4e3a\u5c5e\u6027\uff0c\u5f00\u53d1\u4f4d\u7ea7\u6807\u7b7e\u53bb\u566a\u673a\u5236\uff0c\u80fd\u591f\u5904\u7406\u90e8\u5206\u4e0d\u51c6\u786e\u7684\u6bd4\u7279\u4f4d\uff0c\u63d0\u9ad8\u4f2a\u6807\u7b7e\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2aUDA\u548cSSL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cECOCSeg\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u540e\u5747\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u5206\u5272\u67b6\u6784\u3002", "conclusion": "ECOCSeg\u4e3a\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5b9a\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u4f2a\u6807\u7b7e\u5b66\u4e60\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.07200", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07200", "abs": "https://arxiv.org/abs/2512.07200", "authors": ["Zhen Huang", "Jiaxin Deng", "Jiayu Xu", "Junbiao Pang", "Haitao Yu"], "title": "Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction", "comment": null, "summary": "In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u975e\u5747\u5300\u9053\u8def\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u516c\u4ea4\u5230\u7ad9\u65f6\u95f4\u9884\u6d4b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89e3\u8026\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u9884\u6d4b", "motivation": "\u4f20\u7edf\u5747\u5300\u5206\u5272\u65b9\u6cd5\u65e0\u6cd5\u8003\u8651\u9053\u8def\u7269\u7406\u7ea6\u675f\uff08\u8def\u51b5\u3001\u4ea4\u53c9\u53e3\u3001\u5174\u8da3\u70b9\u7b49\uff09\u7684\u5dee\u5f02\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u6548\u7387", "method": "1) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6839\u636e\u5f71\u54cd\u5206\u6570\u63d0\u53d6\u975e\u5747\u5300\u9053\u8def\u6bb5\uff1b2) \u5728\u7ebf\u6027\u9884\u6d4b\u6a21\u578b\u4e2d\u5bf9\u9009\u5b9a\u6bb5\u8fdb\u884c\u9884\u6d4b", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u8fd8\u63d0\u5347\u4e86\u5b66\u4e60\u6027\u80fd\uff0c\u7ebf\u6027\u65b9\u6cd5\u751a\u81f3\u4f18\u4e8e\u590d\u6742\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u4f18\u6bb5\u9009\u62e9\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u5747\u5300\u65b9\u6cd5\u6709\u663e\u8457\u6539\u8fdb"}}
{"id": "2512.06877", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06877", "abs": "https://arxiv.org/abs/2512.06877", "authors": ["Mohammed Q. Alkhatib", "Ali Jamali", "Swalpa Kumar Roy"], "title": "SceneMixer: Exploring Convolutional Mixing Networks for Remote Sensing Scene Classification", "comment": "Accepted and presented in ICSPIS", "summary": "Remote sensing scene classification plays a key role in Earth observation by enabling the automatic identification of land use and land cover (LULC) patterns from aerial and satellite imagery. Despite recent progress with convolutional neural networks (CNNs) and vision transformers (ViTs), the task remains challenging due to variations in spatial resolution, viewpoint, orientation, and background conditions, which often reduce the generalization ability of existing models. To address these challenges, this paper proposes a lightweight architecture based on the convolutional mixer paradigm. The model alternates between spatial mixing through depthwise convolutions at multiple scales and channel mixing through pointwise operations, enabling efficient extraction of both local and contextual information while keeping the number of parameters and computations low. Extensive experiments were conducted on the AID and EuroSAT benchmarks. The proposed model achieved overall accuracy, average accuracy, and Kappa values of 74.7%, 74.57%, and 73.79 on the AID dataset, and 93.90%, 93.93%, and 93.22 on EuroSAT, respectively. These results demonstrate that the proposed approach provides a good balance between accuracy and efficiency compared with widely used CNN- and transformer-based models. Code will be publicly available on: https://github.com/mqalkhatib/SceneMixer", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u6df7\u5408\u5668\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\uff0c\u7528\u4e8e\u9065\u611f\u573a\u666f\u5206\u7c7b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u79ef\u548c\u9010\u70b9\u64cd\u4f5c\u4ea4\u66ff\u8fdb\u884c\u7a7a\u95f4\u548c\u901a\u9053\u6df7\u5408\uff0c\u5728\u4fdd\u6301\u4f4e\u53c2\u6570\u548c\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u3002", "motivation": "\u9065\u611f\u573a\u666f\u5206\u7c7b\u5728\u571f\u5730\u5229\u7528\u548c\u571f\u5730\u8986\u76d6\u8bc6\u522b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709CNN\u548cViT\u6a21\u578b\u7531\u4e8e\u7a7a\u95f4\u5206\u8fa8\u7387\u3001\u89c6\u89d2\u3001\u65b9\u5411\u548c\u80cc\u666f\u6761\u4ef6\u7684\u53d8\u5316\u800c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5377\u79ef\u6df7\u5408\u5668\u67b6\u6784\uff0c\u4ea4\u66ff\u4f7f\u7528\u591a\u5c3a\u5ea6\u6df1\u5ea6\u5377\u79ef\u8fdb\u884c\u7a7a\u95f4\u6df7\u5408\u548c\u9010\u70b9\u64cd\u4f5c\u8fdb\u884c\u901a\u9053\u6df7\u5408\uff0c\u4ee5\u4f4e\u53c2\u6570\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\u63d0\u53d6\u5c40\u90e8\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "result": "\u5728AID\u6570\u636e\u96c6\u4e0a\u8fbe\u523074.7%\u603b\u4f53\u51c6\u786e\u7387\u300174.57%\u5e73\u5747\u51c6\u786e\u7387\u548c73.79 Kappa\u503c\uff1b\u5728EuroSAT\u6570\u636e\u96c6\u4e0a\u8fbe\u523093.90%\u603b\u4f53\u51c6\u786e\u7387\u300193.93%\u5e73\u5747\u51c6\u786e\u7387\u548c93.22 Kappa\u503c\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4f18\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684CNN\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002"}}
{"id": "2512.07208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07208", "abs": "https://arxiv.org/abs/2512.07208", "authors": ["Fei Luo", "Ziwei Zhao", "Mingxuan Wang", "Duoyang Li", "Zhe Qian", "Jiayi Tuo", "Chenyue Zhou", "Yanbiao Ma"], "title": "Geometric Prior-Guided Federated Prompt Calibration", "comment": null, "summary": "Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($\u03b2$=0.1), it outperforms the state-of-the-art by 2.15\\%. Under extreme skew ($\u03b2$=0.01), it improves upon the baseline by 9.17\\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.", "AI": {"tldr": "GGTPC\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u5f15\u5bfc\u7684\u6587\u672c\u63d0\u793a\u6821\u51c6\u6765\u76f4\u63a5\u7ea0\u6b63\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u672c\u5730\u8bad\u7ec3\u504f\u5dee\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u805a\u5408\u6216\u6b63\u5219\u5316\uff0c\u4f46\u672a\u80fd\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u672c\u5730\u8bad\u7ec3\u504f\u5dee\u8fd9\u4e00\u6839\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faGeometry-Guided Text Prompt Calibration (GGTPC)\u6846\u67b6\uff1a1\uff09\u670d\u52a1\u5668\u7aef\u4ece\u534f\u65b9\u5dee\u77e9\u9635\u91cd\u6784\u5168\u5c40\u51e0\u4f55\u5148\u9a8c\uff1b2\uff09\u5ba2\u6237\u7aef\u4f7f\u7528Geometry-Prior Calibration Layer (GPCL)\u5c06\u672c\u5730\u7279\u5f81\u5206\u5e03\u4e0e\u5168\u5c40\u5148\u9a8c\u5bf9\u9f50\u3002", "result": "\u5728\u6807\u7b7e\u504f\u659c\u7684CIFAR-100\u6570\u636e\u96c6\u4e0a\uff08\u03b2=0.1\uff09\uff0c\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u53472.15%\uff1b\u5728\u6781\u7aef\u504f\u659c\uff08\u03b2=0.01\uff09\u4e0b\u6bd4\u57fa\u7ebf\u63d0\u53479.17%\uff1b\u5728\u57df\u504f\u659c\u7684Office-Home\u6570\u636e\u96c6\u4e0a\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff0c\u5c06FedAvg\u6027\u80fd\u63d0\u53474.60%\u3002", "conclusion": "GGTPC\u901a\u8fc7\u7ea0\u6b63\u6839\u672c\u7684\u672c\u5730\u8bad\u7ec3\u504f\u5dee\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u53ef\u4f5c\u4e3a\u589e\u5f3a\u5404\u79cd\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\u7684\u901a\u7528\u6a21\u5757\u3002"}}
{"id": "2512.06882", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06882", "abs": "https://arxiv.org/abs/2512.06882", "authors": ["Yu Zhu", "Naoya Chiba", "Koichi Hashimoto"], "title": "Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion", "comment": "Accepted to BMVC 2025 (Sheffield, UK, Nov 24-27, 2025). Supplementary video and poster available upon request", "summary": "Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level. Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u56fe\u50cf\u5f15\u5bfc\u76843D\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5b9e\u4f8b\u7ea7\u5230\u90e8\u4ef6\u7ea7\u7684\u6e10\u8fdb\u7ec6\u5316\u6765\u89e3\u51b3\u5de5\u4e1a\u573a\u666f\u4e2d\u906e\u6321\u548c\u5c3a\u5ea6\u5dee\u5f02\u95ee\u9898\u3002", "motivation": "\u5de5\u4e1a\u73af\u5883\u4e2d\u5bc6\u96c6\u5e03\u5c40\u548c\u591a\u5c3a\u5ea6\u7269\u4f53\u7684\u590d\u6742\u573a\u666f\u9700\u8981\u53ef\u9760\u76843D\u5206\u5272\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u8bed\u4e49\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5206\u5c42\u65b9\u6cd5\uff1a\u5148\u901a\u8fc7\u9876\u89c6\u56fe\u6e32\u67d3\u548cSAM+YOLO-World\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\uff0c\u7136\u540e\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u8fdb\u884c\u591a\u89c6\u56fe\u6e32\u67d3\uff0c\u5e94\u75282D\u5206\u5272\u548c\u8d1d\u53f6\u65af\u66f4\u65b0\u878d\u5408\u5b9e\u73b0\u90e8\u4ef6\u7ea7\u5206\u5272\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u5382\u6570\u636e\u4e0a\u6709\u6548\u5904\u7406\u906e\u6321\u548c\u7ed3\u6784\u590d\u6742\u6027\uff0c\u83b7\u5f97\u9ad8mIoU\u5206\u6570\uff1b\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u591a\u6837\u53163D\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u6807\u6ce8\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u5de5\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5206\u5272\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07222", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07222", "abs": "https://arxiv.org/abs/2512.07222", "authors": ["Qiwei Tian", "Chenhao Lin", "Zhengyu Zhao", "Chao Shen"], "title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models", "comment": null, "summary": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faFunction-word De-Attention (FDA)\u65b9\u6cd5\u6765\u89e3\u51b3VLM\u5728\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u4e2d\u7684\u9c81\u68d2\u6027\u4e0e\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u51cf\u5c11\u529f\u80fd\u8bcd\u7684\u5f71\u54cd\u6765\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3VLM\u5728\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u4e2d\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4f5c\u8005\u89c2\u5bdf\u5230\u529f\u80fd\u8bcd\u662f\u5bfc\u81f4VLM\u6613\u53d7\u653b\u51fb\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u63d0\u51faFunction-word De-Attention (FDA)\u65b9\u6cd5\uff0c\u7c7b\u4f3c\u4e8e\u5dee\u5206\u653e\u5927\u5668\uff0c\u5728\u6ce8\u610f\u529b\u5934\u4e2d\u8ba1\u7b97\u539f\u59cb\u8de8\u6ce8\u610f\u529b\u548c\u529f\u80fd\u8bcd\u8de8\u6ce8\u610f\u529b\uff0c\u5e76\u5c06\u540e\u8005\u4ece\u524d\u8005\u4e2d\u5dee\u5206\u51cf\u9664\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u5bf9\u9f50\u548c\u9c81\u68d2\u7684VLM\u3002", "result": "\u57283\u4e2a\u6a21\u578b\u4e0a\u7684\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cFDA\u5e73\u5747\u964d\u4f4e\u4e8618/13/53%\u7684\u653b\u51fb\u6210\u529f\u7387(ASR)\uff0c\u6027\u80fd\u4ec5\u4e0b\u964d0.2/0.3/0.6%\uff1b\u5728\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\u4e2d\uff0cASR\u964d\u4f4e90%\u7684\u540c\u65f6\u6027\u80fd\u8fd8\u63d0\u5347\u4e860.3%\u3002", "conclusion": "FDA\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u3001\u6cdb\u5316\u6027\u548c\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u5728GitHub\u4e0a\u3002"}}
{"id": "2512.06885", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06885", "abs": "https://arxiv.org/abs/2512.06885", "authors": ["Wancheng Feng", "Chen An", "Zhenliang He", "Meina Kan", "Shiguang Shan", "Lukun Wang"], "title": "JoPano: Unified Panorama Generation via Joint Modeling", "comment": "Code: https://github.com/VIPL-GENUN/JoPano", "summary": "Panorama generation has recently attracted growing interest in the research community, with two core tasks, text-to-panorama and view-to-panorama generation. However, existing methods still face two major challenges: their U-Net-based architectures constrain the visual quality of the generated panoramas, and they usually treat the two core tasks independently, which leads to modeling redundancy and inefficiency. To overcome these challenges, we propose a joint-face panorama (JoPano) generation approach that unifies the two core tasks within a DiT-based model. To transfer the rich generative capabilities of existing DiT backbones learned from natural images to the panorama domain, we propose a Joint-Face Adapter built on the cubemap representation of panoramas, which enables a pretrained DiT to jointly model and generate different views of a panorama. We further apply Poisson Blending to reduce seam inconsistencies that often appear at the boundaries between cube faces. Correspondingly, we introduce Seam-SSIM and Seam-Sobel metrics to quantitatively evaluate the seam consistency. Moreover, we propose a condition switching mechanism that unifies text-to-panorama and view-to-panorama tasks within a single model. Comprehensive experiments show that JoPano can generate high-quality panoramas for both text-to-panorama and view-to-panorama generation tasks, achieving state-of-the-art performance on FID, CLIP-FID, IS, and CLIP-Score metrics.", "AI": {"tldr": "JoPano\u662f\u4e00\u79cd\u57fa\u4e8eDiT\u7684\u7edf\u4e00\u5168\u666f\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u9762\u9002\u914d\u5668\u548c\u6761\u4ef6\u5207\u6362\u673a\u5236\uff0c\u540c\u65f6\u652f\u6301\u6587\u672c\u5230\u5168\u666f\u56fe\u548c\u89c6\u56fe\u5230\u5168\u666f\u56fe\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u4efb\u52a1\u72ec\u7acb\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5168\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u57fa\u4e8eU-Net\u7684\u67b6\u6784\u9650\u5236\u4e86\u751f\u6210\u5168\u666f\u56fe\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u4e14\u901a\u5e38\u5c06\u6587\u672c\u5230\u5168\u666f\u56fe\u548c\u89c6\u56fe\u5230\u5168\u666f\u56fe\u4e24\u4e2a\u6838\u5fc3\u4efb\u52a1\u72ec\u7acb\u5904\u7406\uff0c\u5bfc\u81f4\u5efa\u6a21\u5197\u4f59\u548c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eDiT\u7684\u8054\u5408\u9762\u5168\u666f\u56fe\u751f\u6210\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a1\uff09\u57fa\u4e8e\u7acb\u65b9\u4f53\u8d34\u56fe\u8868\u793a\u7684\u8054\u5408\u9762\u9002\u914d\u5668\uff0c\u5c06\u9884\u8bad\u7ec3DiT\u7684\u751f\u6210\u80fd\u529b\u8fc1\u79fb\u5230\u5168\u666f\u56fe\u9886\u57df\uff1b2\uff09\u6cca\u677e\u6df7\u5408\u51cf\u5c11\u9762\u8fb9\u754c\u63a5\u7f1d\u4e0d\u4e00\u81f4\uff1b3\uff09\u6761\u4ef6\u5207\u6362\u673a\u5236\u7edf\u4e00\u4e24\u4e2a\u4efb\u52a1\uff1b4\uff09\u5f15\u5165Seam-SSIM\u548cSeam-Sobel\u6307\u6807\u8bc4\u4f30\u63a5\u7f1d\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eJoPano\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u5168\u666f\u56fe\uff0c\u5728FID\u3001CLIP-FID\u3001IS\u548cCLIP-Score\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "JoPano\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u7684DiT\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5168\u666f\u56fe\u751f\u6210\uff0c\u4e3a\u5168\u666f\u56fe\u751f\u6210\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07244", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07244", "abs": "https://arxiv.org/abs/2512.07244", "authors": ["Elizaveta Kovtun", "Maksim Makarenko", "Natalia Semenova", "Alexey Zaytsev", "Semen Budennyy"], "title": "PINE: Pipeline for Important Node Exploration in Attributed Networks", "comment": null, "summary": "A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPINE\u7684\u65e0\u76d1\u7763\u56fe\u8282\u70b9\u91cd\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u8282\u70b9\u8bed\u4e49\u7279\u5f81\u548c\u7f51\u7edc\u7ed3\u6784\u7279\u6027\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u6765\u8bc6\u522b\u5173\u952e\u8282\u70b9\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u8282\u70b9\u5ea6\u3001PageRank\uff09\u4ec5\u8003\u8651\u7f51\u7edc\u7ed3\u6784\u800c\u5ffd\u7565\u8282\u70b9\u5c5e\u6027\uff0c\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u9700\u8981\u76d1\u7763\u5b66\u4e60\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u65e0\u76d1\u7763\u4e14\u5c5e\u6027\u611f\u77e5\u7684\u65b9\u6cd5\u7a7a\u767d\u3002", "method": "\u63d0\u51faPINE\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u56fe\u6a21\u578b\uff0c\u5c06\u8282\u70b9\u8bed\u4e49\u7279\u5f81\u878d\u5165\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5206\u5e03\u751f\u6210\u8282\u70b9\u91cd\u8981\u6027\u8bc4\u5206\u3002", "result": "\u5728\u591a\u79cd\u540c\u8d28\u548c\u5f02\u8d28\u5c5e\u6027\u7f51\u7edc\u4e0a\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u4f01\u4e1a\u56fe\u4e2d\u7684\u5173\u952e\u5b9e\u4f53\u8bc6\u522b\u3002", "conclusion": "PINE\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u5229\u7528\u7f51\u7edc\u7ed3\u6784\u548c\u8282\u70b9\u5c5e\u6027\u6765\u8bc6\u522b\u91cd\u8981\u8282\u70b9\uff0c\u5177\u6709\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.06886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06886", "abs": "https://arxiv.org/abs/2512.06886", "authors": ["Wangkai Li", "Rui Sun", "Bohao Liao", "Zhaoyang Li", "Tianzhu Zhang"], "title": "Balanced Learning for Domain Adaptive Semantic Segmentation", "comment": "Accepted by International Conference on Machine Learning (ICML 2025)", "summary": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Despite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains. To address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift. First, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits. Subsequently, we introduce a post-hoc approach to align the logits distributions across different classes using shared anchor distributions. To further consider the network's need to generate unbiased pseudo-labels during self-training, we estimate logits distributions online and incorporate logits correction terms into the loss function. Moreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains. Extensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into various existing methods. Code is available at https://github.com/Woof6/BLDA.", "AI": {"tldr": "BLDA\u63d0\u51fa\u4e86\u4e00\u79cd\u5e73\u8861\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u6790\u9884\u6d4blogits\u5206\u5e03\u6765\u8bc6\u522b\u8fc7\u9884\u6d4b\u548c\u6b20\u9884\u6d4b\u7c7b\u522b\uff0c\u5e76\u4f7f\u7528\u5171\u4eab\u951a\u5206\u5e03\u8fdb\u884c\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u8bad\u7ec3\u6280\u672f\u5728UDA\u4e2d\u7531\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u57df\u95f4\u5206\u5e03\u504f\u79fb\uff0c\u96be\u4ee5\u5e73\u8861\u5b66\u4e60\u5404\u4e2a\u7c7b\u522b\uff0c\u5bfc\u81f4\u6a21\u578b\u5bf9\u67d0\u4e9b\u7c7b\u522b\u9884\u6d4b\u504f\u5dee\u8f83\u5927\u3002", "method": "1\uff09\u901a\u8fc7\u5206\u6790\u9884\u6d4blogits\u5206\u5e03\u8bc6\u522b\u8fc7\u9884\u6d4b\u548c\u6b20\u9884\u6d4b\u7c7b\u522b\uff1b2\uff09\u4f7f\u7528\u5171\u4eab\u951a\u5206\u5e03\u8fdb\u884c\u540e\u5904\u7406\u5bf9\u9f50\uff1b3\uff09\u5728\u7ebf\u4f30\u8ba1logits\u5206\u5e03\u5e76\u52a0\u5165\u635f\u5931\u51fd\u6570\u4fee\u6b63\u9879\uff1b4\uff09\u5229\u7528\u7d2f\u79ef\u5bc6\u5ea6\u4f5c\u4e3a\u57df\u95f4\u5171\u4eab\u7684\u7ed3\u6784\u77e5\u8bc6\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6UDA\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBLDA\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u7279\u522b\u662f\u5bf9\u6b20\u9884\u6d4b\u7c7b\u522b\u6709\u663e\u8457\u6539\u5584\uff0c\u4e14\u53ef\u96c6\u6210\u5230\u591a\u79cd\u73b0\u6709\u65b9\u6cd5\u4e2d\u3002", "conclusion": "BLDA\u901a\u8fc7\u5e73\u8861\u5b66\u4e60\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86UDA\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u7c7b\u522b\u504f\u5dee\u95ee\u9898\uff0c\u65e0\u9700\u5148\u9a8c\u5206\u5e03\u77e5\u8bc6\uff0c\u5177\u6709\u8f83\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.07249", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07249", "abs": "https://arxiv.org/abs/2512.07249", "authors": ["Jingran Yang", "Min Zhang", "Lingfeng Zhang", "Zhaohui Wang", "Yonggang Zhang"], "title": "IFFair: Influence Function-driven Sample Reweighting for Fair Classification", "comment": null, "summary": "Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f71\u54cd\u51fd\u6570\u7684\u9884\u5904\u7406\u516c\u5e73\u6027\u4f18\u5316\u65b9\u6cd5IFFair\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6837\u672c\u6743\u91cd\u6765\u51cf\u8f7b\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e2d\u7684\u504f\u89c1\uff0c\u800c\u4e0d\u6539\u53d8\u7f51\u7edc\u7ed3\u6784\u6216\u51b3\u7b56\u8fb9\u754c\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5728\u8f85\u52a9\u6216\u66ff\u4ee3\u4eba\u7c7b\u51b3\u7b56\u65f6\uff0c\u4f1a\u5b66\u4e60\u5e76\u52a0\u5267\u6837\u672c\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u5bf9\u5f31\u52bf\u7fa4\u4f53\u7684\u6b67\u89c6\u6027\u51b3\u7b56\uff0c\u635f\u5bb3\u793e\u4f1a\u798f\u7949\u5e76\u963b\u788d\u76f8\u5173\u5e94\u7528\u53d1\u5c55\u3002", "method": "IFFair\u65b9\u6cd5\u4ec5\u4f7f\u7528\u8bad\u7ec3\u6837\u672c\u5bf9\u4e0d\u540c\u7fa4\u4f53\u5f71\u54cd\u5dee\u5f02\u4f5c\u4e3a\u6307\u5bfc\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u6837\u672c\u6743\u91cd\uff0c\u65e0\u9700\u4fee\u6539\u7f51\u7edc\u7ed3\u6784\u3001\u6570\u636e\u7279\u5f81\u548c\u51b3\u7b56\u8fb9\u754c\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u548c\u6307\u6807\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIFFair\u80fd\u591f\u51cf\u8f7b\u5206\u7c7b\u8bbe\u7f6e\u4e2d\u7684\u591a\u79cd\u516c\u5e73\u6027\u6307\u6807\u504f\u89c1\uff0c\u5305\u62ec\u4eba\u53e3\u7edf\u8ba1\u5747\u7b49\u3001\u5747\u7b49\u5316\u673a\u4f1a\u3001\u673a\u4f1a\u5e73\u7b49\u548c\u9519\u8bef\u7387\u5747\u7b49\uff0c\u4e14\u65e0\u51b2\u7a81\u3002", "conclusion": "IFFair\u5728\u591a\u4e2a\u6548\u7528\u548c\u516c\u5e73\u6027\u6307\u6807\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6bd4\u5148\u524d\u9884\u5904\u7406\u65b9\u6cd5\u66f4\u597d\u7684\u6743\u8861\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u7b97\u6cd5\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2512.06888", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06888", "abs": "https://arxiv.org/abs/2512.06888", "authors": ["Liyang Song", "Hardik Bishnoi", "Sai Kumar Reddy Manne", "Sarah Ostadabbas", "Briana J. Taylor", "Michael Wan"], "title": "Overcoming Small Data Limitations in Video-Based Infant Respiration Estimation", "comment": null, "summary": "The development of contactless respiration monitoring for infants could enable advances in the early detection and treatment of breathing irregularities, which are associated with neurodevelopmental impairments and conditions like sudden infant death syndrome (SIDS). But while respiration estimation for adults is supported by a robust ecosystem of computer vision algorithms and video datasets, only one small public video dataset with annotated respiration data for infant subjects exists, and there are no reproducible algorithms which are effective for infants. We introduce the annotated infant respiration dataset of 400 videos (AIR-400), contributing 275 new, carefully annotated videos from 10 recruited subjects to the public corpus. We develop the first reproducible pipelines for infant respiration estimation, based on infant-specific region-of-interest detection and spatiotemporal neural processing enhanced by optical flow inputs. We establish, through comprehensive experiments, the first reproducible benchmarks for the state-of-the-art in vision-based infant respiration estimation. We make our dataset, code repository, and trained models available for public use.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AIR-400\u5a74\u513f\u547c\u5438\u76d1\u6d4b\u6570\u636e\u96c6\u548c\u9996\u4e2a\u53ef\u590d\u73b0\u7684\u5a74\u513f\u547c\u5438\u4f30\u8ba1\u7b97\u6cd5\uff0c\u586b\u8865\u4e86\u5a74\u513f\u547c\u5438\u76d1\u6d4b\u9886\u57df\u7684\u6570\u636e\u548c\u7b97\u6cd5\u7a7a\u767d\u3002", "motivation": "\u5a74\u513f\u547c\u5438\u5f02\u5e38\u4e0e\u795e\u7ecf\u53d1\u80b2\u969c\u788d\u548c\u5a74\u513f\u731d\u6b7b\u7efc\u5408\u5f81\u76f8\u5173\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u5a74\u513f\u7684\u547c\u5438\u76d1\u6d4b\u6570\u636e\u96c6\u548c\u6709\u6548\u7b97\u6cd5\uff0c\u800c\u6210\u4eba\u547c\u5438\u76d1\u6d4b\u5df2\u6709\u6210\u719f\u6280\u672f\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5a74\u513f\u7279\u5b9a\u611f\u5174\u8da3\u533a\u57df\u68c0\u6d4b\u548c\u5149\u6d41\u589e\u5f3a\u7684\u65f6\u7a7a\u795e\u7ecf\u5904\u7406\u7b97\u6cd5\uff0c\u6784\u5efa\u4e86\u5305\u542b400\u4e2a\u6807\u6ce8\u89c6\u9891\u7684AIR-400\u6570\u636e\u96c6\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u53ef\u590d\u73b0\u7684\u5a74\u513f\u547c\u5438\u4f30\u8ba1\u57fa\u51c6\uff0c\u7b97\u6cd5\u5728400\u4e2a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5a74\u513f\u547c\u5438\u76d1\u6d4b\u9886\u57df\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6570\u636e\u96c6\u548c\u7b97\u6cd5\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u53d1\u73b0\u548c\u6cbb\u7597\u547c\u5438\u5f02\u5e38\u3002"}}
{"id": "2512.07287", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07287", "abs": "https://arxiv.org/abs/2512.07287", "authors": ["Sijia Li", "Yuchen Huang", "Zifan Liu", "Zijian Li", "Jingjing fu", "Lei Song", "Jiang Bian", "Jun Zhang", "Rui Wang"], "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents", "comment": null, "summary": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u72b6\u6001\u96c6\u6210\u5de5\u5177\u56fe\uff08SIT-Graph\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u90e8\u5206\u91cd\u53e0\u7684\u7ecf\u9a8c\u6765\u589e\u5f3a\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u5f53\u524dLLM\u667a\u80fd\u4f53\u5728\u5904\u7406\u6e10\u8fdb\u610f\u56fe\u548c\u73af\u5883\u53d8\u5316\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\uff0c\u610f\u56fe\u4f1a\u9010\u6b65\u660e\u786e\u4e14\u73af\u5883\u968f\u6bcf\u6b21\u5de5\u5177\u8c03\u7528\u800c\u53d8\u5316\u3002\u73b0\u6709LLM\u667a\u80fd\u4f53\u8981\u4e48\u5c06\u6574\u4e2a\u8f68\u8ff9\u6216\u9884\u5b9a\u4e49\u5b50\u4efb\u52a1\u89c6\u4e3a\u4e0d\u53ef\u5206\u5272\u5355\u5143\uff0c\u8981\u4e48\u4ec5\u5229\u7528\u5de5\u5177\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u96be\u4ee5\u9002\u5e94\u72b6\u6001\u548c\u4fe1\u606f\u5728\u8f6e\u6b21\u95f4\u7684\u6f14\u53d8\u3002", "method": "SIT-Graph\u4ece\u5386\u53f2\u8f68\u8ff9\u4e2d\u6355\u83b7\u7d27\u51d1\u7684\u72b6\u6001\u8868\u793a\uff08\u7c7b\u4f3c\u60c5\u666f\u8bb0\u5fc6\u7247\u6bb5\uff09\u548c\u5de5\u5177\u95f4\u4f9d\u8d56\u5173\u7cfb\uff08\u7c7b\u4f3c\u7a0b\u5e8f\u8bb0\u5fc6\u4f8b\u7a0b\uff09\u3002\u9996\u5148\u4ece\u7d2f\u79ef\u7684\u5de5\u5177\u4f7f\u7528\u5e8f\u5217\u6784\u5efa\u5de5\u5177\u56fe\uff0c\u7136\u540e\u5728\u6bcf\u6761\u8fb9\u4e0a\u589e\u5f3a\u5bf9\u8bdd\u548c\u5de5\u5177\u5386\u53f2\u7684\u7d27\u51d1\u72b6\u6001\u6458\u8981\u3002\u63a8\u7406\u65f6\uff0c\u667a\u80fd\u4f53\u5728\u9700\u8981\u56de\u5fc6\u5148\u524d\u4e0a\u4e0b\u6587\u65f6\u68c0\u7d22\u76f8\u5173\u8fb9\u4e0a\u7684\u72b6\u6001\u6458\u8981\u6765\u6307\u5bfc\u884c\u52a8\uff0c\u5728\u5e38\u89c4\u6b65\u9aa4\u65f6\u5219\u9075\u5faa\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5de5\u5177\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728\u591a\u4e2a\u6709\u72b6\u6001\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIT-Graph\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u8bb0\u5fc6\u548c\u56fe\u7684\u57fa\u7840\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684\u5de5\u5177\u9009\u62e9\u548c\u66f4\u6709\u6548\u7684\u7ecf\u9a8c\u8fc1\u79fb\u3002", "conclusion": "SIT-Graph\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u4e2d\u60c5\u666f\u8bb0\u5fc6\u548c\u7a0b\u5e8f\u8bb0\u5fc6\u7684\u6574\u5408\uff0c\u4e3a\u591a\u8f6e\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u548c\u6e10\u8fdb\u660e\u786e\u7684\u610f\u56fe\u3002"}}
{"id": "2512.06905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06905", "abs": "https://arxiv.org/abs/2512.06905", "authors": ["Zijian Zhou", "Shikun Liu", "Haozhe Liu", "Haonan Qiu", "Zhaochong An", "Weiming Ren", "Zhiheng Liu", "Xiaoke Huang", "Kam Woh Ng", "Tian Xie", "Xiao Han", "Yuren Cong", "Hang Li", "Chuyan Zhu", "Aditya Patel", "Tao Xiang", "Sen He"], "title": "Scaling Zero-Shot Reference-to-Video Generation", "comment": "Website: https://franciszzj.github.io/Saber/", "summary": "Reference-to-video (R2V) generation aims to synthesize videos that align with a text prompt while preserving the subject identity from reference images. However, current R2V methods are hindered by the reliance on explicit reference image-video-text triplets, whose construction is highly expensive and difficult to scale. We bypass this bottleneck by introducing Saber, a scalable zero-shot framework that requires no explicit R2V data. Trained exclusively on video-text pairs, Saber employs a masked training strategy and a tailored attention-based model design to learn identity-consistent and reference-aware representations. Mask augmentation techniques are further integrated to mitigate copy-paste artifacts common in reference-to-video generation. Moreover, Saber demonstrates remarkable generalization capabilities across a varying number of references and achieves superior performance on the OpenS2V-Eval benchmark compared to methods trained with R2V data.", "AI": {"tldr": "Saber\u662f\u4e00\u4e2a\u65e0\u9700\u53c2\u8003\u56fe\u50cf-\u89c6\u9891-\u6587\u672c\u4e09\u5143\u7ec4\u6570\u636e\u7684\u96f6\u6837\u672c\u53c2\u8003\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u548c\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\uff0c\u5728\u4ec5\u4f7f\u7528\u89c6\u9891-\u6587\u672c\u5bf9\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8eab\u4efd\u4e00\u81f4\u548c\u53c2\u8003\u611f\u77e5\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u53c2\u8003\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u663e\u5f0f\u53c2\u8003\u56fe\u50cf-\u89c6\u9891-\u6587\u672c\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u8fd9\u4e9b\u6570\u636e\u7684\u6784\u5efa\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u627e\u5230\u7ed5\u8fc7\u8fd9\u4e00\u74f6\u9888\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u548c\u5b9a\u5236\u7684\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u8bbe\u8ba1\uff0c\u4ec5\u4f7f\u7528\u89c6\u9891-\u6587\u672c\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u6574\u5408\u63a9\u7801\u589e\u5f3a\u6280\u672f\u6765\u51cf\u8f7b\u53c2\u8003\u89c6\u9891\u751f\u6210\u4e2d\u5e38\u89c1\u7684\u590d\u5236\u7c98\u8d34\u4f2a\u5f71\u3002", "result": "\u5728OpenS2V-Eval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u4f7f\u7528R2V\u6570\u636e\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0cSaber\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u6570\u91cf\u53c2\u8003\u4e0b\u7684\u663e\u8457\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Saber\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u65e0\u9700\u663e\u5f0fR2V\u6570\u636e\u7684\u53ef\u6269\u5c55\u96f6\u6837\u672c\u53c2\u8003\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u4f9d\u8d56\u74f6\u9888\u95ee\u9898\uff0c\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.07310", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07310", "abs": "https://arxiv.org/abs/2512.07310", "authors": ["Andrei V. Konstantinov", "Valerii A. Zuev", "Lev V. Utkin"], "title": "Towards a Relationship-Aware Transformer for Tabular Data", "comment": null, "summary": "Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u8868\u683c\u6570\u636e\u4e2d\u7684\u5916\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u56fe\u573a\u666f\u4e0b\u7684\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8868\u683c\u6570\u636e\u6837\u672c\u95f4\u7684\u5916\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u800c\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7a00\u758f\u56fe\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8003\u8651\u6570\u636e\u70b9\u95f4\u53ef\u80fd\u5173\u7cfb\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4fee\u6539\u6ce8\u610f\u529b\u673a\u5236\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5728\u6ce8\u610f\u529b\u77e9\u9635\u4e2d\u6dfb\u52a0\u989d\u5916\u9879\u6765\u8003\u8651\u6570\u636e\u70b9\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u56de\u5f52\u4efb\u52a1\u4ee5\u53caIHDP\u6570\u636e\u96c6\u4e0a\u7684\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0c\u4e0e\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u548c\u5176\u4ed6\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u9a8c\u8bc1\u3002", "conclusion": "\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u6709\u6548\u5904\u7406\u8868\u683c\u6570\u636e\u4e2d\u7684\u5916\u90e8\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u7a00\u758f\u56fe\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2512.06921", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.06921", "abs": "https://arxiv.org/abs/2512.06921", "authors": ["Ziyang Song", "Zelin Zang", "Xiaofan Ye", "Boqiang Xu", "Long Bai", "Jinlin Wu", "Hongliang Ren", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "title": "NeuroABench: A Multimodal Evaluation Benchmark for Neurosurgical Anatomy Identification", "comment": "Accepted by IEEE ICIA 2025", "summary": "Multimodal Large Language Models (MLLMs) have shown significant potential in surgical video understanding. With improved zero-shot performance and more effective human-machine interaction, they provide a strong foundation for advancing surgical education and assistance. However, existing research and datasets primarily focus on understanding surgical procedures and workflows, while paying limited attention to the critical role of anatomical comprehension. In clinical practice, surgeons rely heavily on precise anatomical understanding to interpret, review, and learn from surgical videos. To fill this gap, we introduce the Neurosurgical Anatomy Benchmark (NeuroABench), the first multimodal benchmark explicitly created to evaluate anatomical understanding in the neurosurgical domain. NeuroABench consists of 9 hours of annotated neurosurgical videos covering 89 distinct procedures and is developed using a novel multimodal annotation pipeline with multiple review cycles. The benchmark evaluates the identification of 68 clinical anatomical structures, providing a rigorous and standardized framework for assessing model performance. Experiments on over 10 state-of-the-art MLLMs reveal significant limitations, with the best-performing model achieving only 40.87% accuracy in anatomical identification tasks. To further evaluate the benchmark, we extract a subset of the dataset and conduct an informative test with four neurosurgical trainees. The results show that the best-performing student achieves 56% accuracy, with the lowest scores of 28% and an average score of 46.5%. While the best MLLM performs comparably to the lowest-scoring student, it still lags significantly behind the group's average performance. This comparison underscores both the progress of MLLMs in anatomical understanding and the substantial gap that remains in achieving human-level performance.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86NeuroABench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u795e\u7ecf\u5916\u79d1\u9886\u57df\u89e3\u5256\u5b66\u7406\u89e3\u7684\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5305\u542b9\u5c0f\u65f6\u6807\u6ce8\u89c6\u9891\u548c68\u4e2a\u89e3\u5256\u7ed3\u6784\uff0c\u5b9e\u9a8c\u663e\u793a\u5f53\u524dMLLMs\u5728\u89e3\u5256\u8bc6\u522b\u4efb\u52a1\u4e0a\u6700\u9ad8\u4ec5\u8fbe40.87%\u51c6\u786e\u7387\uff0c\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u624b\u672f\u6d41\u7a0b\u7406\u89e3\uff0c\u800c\u5ffd\u89c6\u4e86\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u89e3\u5256\u5b66\u8ba4\u77e5\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30MLLMs\u5728\u795e\u7ecf\u5916\u79d1\u89e3\u5256\u7406\u89e3\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u65b0\u578b\u591a\u6a21\u6001\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\u5305\u542b89\u79cd\u4e0d\u540c\u624b\u672f\u76849\u5c0f\u65f6\u6807\u6ce8\u89c6\u9891\u6570\u636e\u96c6\uff0c\u8bc4\u4f3068\u4e2a\u4e34\u5e8a\u89e3\u5256\u7ed3\u6784\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u5bf910\u591a\u4e2a\u5148\u8fdbMLLMs\u548c4\u540d\u795e\u7ecf\u5916\u79d1\u57f9\u8bad\u751f\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "\u6700\u4f73MLLM\u4ec5\u8fbe\u523040.87%\u51c6\u786e\u7387\uff0c\u800c\u4eba\u7c7b\u57f9\u8bad\u751f\u5e73\u5747\u51c6\u786e\u7387\u4e3a46.5%\uff08\u6700\u9ad856%\uff0c\u6700\u4f4e28%\uff09\uff0cMLLMs\u8868\u73b0\u63a5\u8fd1\u6700\u5dee\u4eba\u7c7b\u4f46\u660e\u663e\u843d\u540e\u4e8e\u5e73\u5747\u6c34\u5e73\u3002", "conclusion": "MLLMs\u5728\u89e3\u5256\u5b66\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\u4f46\u4ecd\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0cNeuroABench\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\u3002"}}
{"id": "2512.07313", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.07313", "abs": "https://arxiv.org/abs/2512.07313", "authors": ["Bosun Kang", "Hyejun Park", "Chenglin Fan"], "title": "Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach", "comment": "7 pages", "summary": "We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u51b3\u7b56\u7684\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ef4\u62a4\u7cbe\u786e\u540e\u9a8c\u5206\u5e03\u6765\u7edf\u4e00\u4f20\u7edf\u6700\u574f\u60c5\u51b5\u5206\u6790\u548c\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u4e13\u5bb6\u5148\u9a8c\u7684\u65e0\u7f1d\u6574\u5408\u3002", "motivation": "\u4f20\u7edf\u6ed1\u96ea\u79df\u8d41\u7b97\u6cd5\u53ea\u5173\u6ce8\u6700\u574f\u60c5\u51b5\u6210\u672c\uff0c\u800c\u8fd1\u671f\u5b66\u4e60\u589e\u5f3a\u65b9\u6cd5\u867d\u7136\u5229\u7528\u9884\u6d4b\u4f46\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002\u672c\u6587\u65e8\u5728\u7edf\u4e00\u8fd9\u4e24\u79cd\u89c6\u89d2\uff0c\u63d0\u4f9b\u66f4\u539f\u5219\u6027\u7684\u51b3\u7b56\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u79bb\u6563\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u7ef4\u62a4\u65f6\u95f4\u8303\u56f4\u5185\u7684\u7cbe\u786e\u540e\u9a8c\u5206\u5e03\uff0c\u652f\u6301\u4e13\u5bb6\u5148\u9a8c\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u7b97\u6cd5\u80fd\u591f\u4f18\u96c5\u5730\u5728\u6700\u574f\u60c5\u51b5\u548c\u5b8c\u5168\u4fe1\u606f\u8bbe\u7f6e\u95f4\u63d2\u503c\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u591a\u6837\u5316\u573a\u666f\u4e0b\u5177\u6709\u4f18\u8d8a\u7684\u5b9e\u8bc1\u6027\u80fd\uff0c\u5728\u51c6\u786e\u5148\u9a8c\u4e0b\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u7684\u6700\u574f\u60c5\u51b5\u4fdd\u8bc1\u3002", "conclusion": "\u8d1d\u53f6\u65af\u6846\u67b6\u81ea\u7136\u652f\u6301\u591a\u9884\u6d4b\u3001\u975e\u5747\u5300\u5148\u9a8c\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u51f8\u663e\u4e86\u8d1d\u53f6\u65af\u63a8\u7406\u5728\u5177\u6709\u4e0d\u5b8c\u7f8e\u9884\u6d4b\u7684\u5728\u7ebf\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2512.06949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.06949", "abs": "https://arxiv.org/abs/2512.06949", "authors": ["Shravan Venkatraman", "Muthu Subash Kavitha", "Joe Dhanith P R", "V Manikandarajan", "Jia Wu"], "title": "Can We Go Beyond Visual Features? Neural Tissue Relation Modeling for Relational Graph Analysis in Non-Melanoma Skin Histology", "comment": "19 pages, 5 figures, 2 tables", "summary": "Histopathology image segmentation is essential for delineating tissue structures in skin cancer diagnostics, but modeling spatial context and inter-tissue relationships remains a challenge, especially in regions with overlapping or morphologically similar tissues. Current convolutional neural network (CNN)-based approaches operate primarily on visual texture, often treating tissues as independent regions and failing to encode biological context. To this end, we introduce Neural Tissue Relation Modeling (NTRM), a novel segmentation framework that augments CNNs with a tissue-level graph neural network to model spatial and functional relationships across tissue types. NTRM constructs a graph over predicted regions, propagates contextual information via message passing, and refines segmentation through spatial projection. Unlike prior methods, NTRM explicitly encodes inter-tissue dependencies, enabling structurally coherent predictions in boundary-dense zones. On the benchmark Histopathology Non-Melanoma Skin Cancer Segmentation Dataset, NTRM outperforms state-of-the-art methods, achieving a robust Dice similarity coefficient that is 4.9\\% to 31.25\\% higher than the best-performing models among the evaluated approaches. Our experiments indicate that relational modeling offers a principled path toward more context-aware and interpretable histological segmentation, compared to local receptive-field architectures that lack tissue-level structural awareness. Our code is available at https://github.com/shravan-18/NTRM.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNTRM\u7684\u65b0\u578b\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u7ec4\u7ec7\u95f4\u7684\u7a7a\u95f4\u548c\u529f\u80fd\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCNN\u65b9\u6cd5\u5728\u5904\u7406\u91cd\u53e0\u6216\u5f62\u6001\u76f8\u4f3c\u7ec4\u7ec7\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eCNN\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u7eb9\u7406\u7279\u5f81\uff0c\u5c06\u7ec4\u7ec7\u89c6\u4e3a\u72ec\u7acb\u533a\u57df\uff0c\u7f3a\u4e4f\u5bf9\u7ec4\u7ec7\u95f4\u751f\u7269\u5b66\u4e0a\u4e0b\u6587\u5173\u7cfb\u7684\u5efa\u6a21\uff0c\u7279\u522b\u662f\u5728\u8fb9\u754c\u5bc6\u96c6\u533a\u57df\u96be\u4ee5\u5b9e\u73b0\u7ed3\u6784\u4e00\u81f4\u7684\u9884\u6d4b\u3002", "method": "NTRM\u6846\u67b6\u5728CNN\u57fa\u7840\u4e0a\u5f15\u5165\u7ec4\u7ec7\u7ea7\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u6784\u5efa\u9884\u6d4b\u533a\u57df\u56fe\uff0c\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u4f20\u64ad\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u7a7a\u95f4\u6295\u5f71\u7ec6\u5316\u5206\u5272\u7ed3\u679c\uff0c\u663e\u5f0f\u7f16\u7801\u7ec4\u7ec7\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728Histopathology Non-Melanoma Skin Cancer Segmentation Dataset\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNTRM\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cDice\u76f8\u4f3c\u7cfb\u6570\u6bd4\u6700\u4f73\u6a21\u578b\u9ad8\u51fa4.9%\u81f331.25%\u3002", "conclusion": "\u5173\u7cfb\u5efa\u6a21\u4e3a\u7ec4\u7ec7\u75c5\u7406\u5b66\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u6761\u66f4\u6ce8\u91cd\u4e0a\u4e0b\u6587\u548c\u53ef\u89e3\u91ca\u6027\u7684\u8def\u5f84\uff0c\u76f8\u6bd4\u7f3a\u4e4f\u7ec4\u7ec7\u7ea7\u7ed3\u6784\u611f\u77e5\u7684\u5c40\u90e8\u611f\u53d7\u91ce\u67b6\u6784\u5177\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2512.07332", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07332", "abs": "https://arxiv.org/abs/2512.07332", "authors": ["Zhengquan Luo", "Guy Tadmor", "Or Amar", "David Zeevi", "Zhiqiang Xu"], "title": "Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach", "comment": null, "summary": "Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.", "AI": {"tldr": "RicciKGE\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5d4c\u5165\u635f\u5931\u68af\u5ea6\u4e0e\u5c40\u90e8\u66f2\u7387\u8026\u5408\u5728\u6269\u5c55\u7684Ricci\u6d41\u4e2d\uff0c\u4f7f\u5b9e\u4f53\u5d4c\u5165\u4e0e\u5e95\u5c42\u6d41\u5f62\u51e0\u4f55\u5171\u540c\u6f14\u5316\uff0c\u5b9e\u73b0\u76f8\u4e92\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6240\u6709\u5b9e\u4f53\u653e\u7f6e\u5728\u5355\u4e00\u540c\u8d28\u6d41\u5f62\u4e0a\uff0c\u65e0\u6cd5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u56fe\u8c31\u5728\u4e0d\u540c\u5c40\u90e8\u533a\u57df\u8868\u73b0\u51fa\u7684\u663e\u8457\u53d8\u5316\u66f2\u7387\uff0c\u8fd9\u79cd\u51e0\u4f55\u4e0d\u5339\u914d\u4f1a\u626d\u66f2\u5b9e\u4f53\u95f4\u8ddd\u79bb\u5e76\u635f\u5bb3\u5d4c\u5165\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u5c06KGE\u635f\u5931\u68af\u5ea6\u4e0e\u5c40\u90e8\u66f2\u7387\u8026\u5408\u5728\u6269\u5c55\u7684Ricci\u6d41\u4e2d\uff0c\u4f7f\u5b9e\u4f53\u5d4c\u5165\u548c\u6d41\u5f62\u51e0\u4f55\u5171\u540c\u52a8\u6001\u6f14\u5316\u3002\u7406\u8bba\u8bc1\u660e\u5f53\u8026\u5408\u7cfb\u6570\u6709\u754c\u4e14\u9002\u5f53\u9009\u62e9\u65f6\uff0c\u8fb9\u66f2\u7387\u5448\u6307\u6570\u8870\u51cf\uff0c\u6d41\u5f62\u8d8b\u5411\u6b27\u51e0\u91cc\u5f97\u5e73\u5766\uff0c\u540c\u65f6KGE\u8ddd\u79bb\u4e25\u683c\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u3002", "result": "\u5728\u94fe\u63a5\u9884\u6d4b\u548c\u8282\u70b9\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6539\u8fdb\u6548\u679c\uff0c\u8bc1\u660eRicciKGE\u80fd\u591f\u6709\u6548\u9002\u5e94\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u3002", "conclusion": "RicciKGE\u901a\u8fc7\u51e0\u4f55\u5e73\u5766\u5316\u548c\u5d4c\u5165\u4f18\u5316\u7684\u76f8\u4e92\u4fc3\u8fdb\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u9002\u5e94\u5f02\u6784\u56fe\u8c31\u7ed3\u6784\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.06981", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06981", "abs": "https://arxiv.org/abs/2512.06981", "authors": ["Yuemin Wang", "Ian Stavness"], "title": "Selective Masking based Self-Supervised Learning for Image Semantic Segmentation", "comment": null, "summary": "This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u6548\u679c\uff0c\u76f8\u6bd4\u4f20\u7edf\u968f\u673a\u63a9\u7801\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u968f\u673a\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u65f6\u6548\u7387\u4e0d\u9ad8\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u9009\u62e9\u6027\u63a9\u7801\u7b56\u7565\uff0c\u66f4\u667a\u80fd\u5730\u9009\u62e9\u9700\u8981\u91cd\u5efa\u7684\u56fe\u50cf\u5757\uff0c\u63d0\u9ad8\u9884\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u5c06\u56fe\u50cf\u91cd\u5efa\u9884\u8bad\u7ec3\u5206\u89e3\u4e3a\u8fed\u4ee3\u6b65\u9aa4\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u9009\u62e9\u91cd\u5efa\u635f\u5931\u6700\u9ad8\u7684\u56fe\u50cf\u5757\u8fdb\u884c\u63a9\u7801\uff0c\u5229\u7528\u5df2\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u6765\u6307\u5bfc\u63a9\u7801\u9009\u62e9\u8fc7\u7a0b\u3002", "result": "\u5728\u4e24\u4e2a\u901a\u7528\u6570\u636e\u96c6\uff08Pascal VOC\u548cCityscapes\uff09\u548c\u4e24\u4e2a\u6742\u8349\u5206\u5272\u6570\u636e\u96c6\u4e0a\uff0c\u9009\u62e9\u6027\u63a9\u7801\u65b9\u6cd5\u6bd4\u4f20\u7edf\u968f\u673a\u63a9\u7801\u65b9\u6cd5\u548c\u6709\u76d1\u7763ImageNet\u9884\u8bad\u7ec3\u5206\u522b\u63d0\u9ad8\u4e862.9%\u548c2.5%\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u4f4e\u6027\u80fd\u7c7b\u522b\u4e0a\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u9009\u62e9\u6027\u63a9\u7801\u56fe\u50cf\u91cd\u5efa\u65b9\u6cd5\u4e3a\u7aef\u5230\u7aef\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u6709\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6709\u9650\u6a21\u578b\u5bb9\u91cf\u4ee5\u6ee1\u8db3\u63a8\u7406\u901f\u5ea6\u548c\u8ba1\u7b97\u8d44\u6e90\u8981\u6c42\u7684\u573a\u666f\u3002"}}
{"id": "2512.07374", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07374", "abs": "https://arxiv.org/abs/2512.07374", "authors": ["Yezi Liu", "Hanning Chen", "Wenjun Huang", "Yang Ni", "Mohsen Imani"], "title": "Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning", "comment": null, "summary": "Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.", "AI": {"tldr": "R2F\u662f\u4e00\u79cd\u57fa\u4e8eLoRA\u9002\u914d\u5668\u66f4\u65b0\u7684\u9ad8\u6548\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u4f4e\u79e9\u53c2\u6570\u91cd\u5efa\u5168\u6a21\u578b\u68af\u5ea6\u65b9\u5411\uff0c\u907f\u514d\u5168\u6a21\u578b\u5fae\u8c03\u6216\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e", "motivation": "\u89e3\u51b3\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u9700\u8981\u5168\u6a21\u578b\u5fae\u8c03\u6216\u8bbf\u95ee\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u66f4\u65b0\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027", "method": "\u4f7f\u7528\u591a\u7ec4\u8f6c\u8ff0\u63d0\u793a\u8ba1\u7b97LoRA\u53c2\u6570\u7684\u68af\u5ea6\uff0c\u8bad\u7ec3\u68af\u5ea6\u89e3\u7801\u5668\u8fd1\u4f3c\u5bf9\u5e94\u7684\u5168\u6a21\u578b\u68af\u5ea6\uff1b\u901a\u8fc7\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u89e3\u7801\u5668\u5e76\u8fc1\u79fb\u5230\u76ee\u6807\u6a21\u578b", "result": "R2F\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u6548\u9057\u5fd8\uff0c\u4e3a\u9884\u8bad\u7ec3LLM\u63d0\u4f9b\u65e0\u9700\u5168\u91cd\u8bad\u7ec3\u6216\u5185\u90e8\u53c2\u6570\u8bbf\u95ee\u7684\u8f7b\u91cf\u7ea7\u9057\u5fd8\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u8f7b\u91cf\u7ea7\u7684\u9057\u5fd8\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6709\u6548\u6027"}}
{"id": "2512.07034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07034", "abs": "https://arxiv.org/abs/2512.07034", "authors": ["Tuan-Anh Vu", "Hai Nguyen-Truong", "Ziqiang Zheng", "Binh-Son Hua", "Qing Guo", "Ivor Tsang", "Sai-Kit Yeung"], "title": "Power of Boundary and Reflection: Semantic Transparent Object Segmentation using Pyramid Vision Transformer with Transparent Cues", "comment": "Accepted to WACV 2026", "summary": "Glass is a prevalent material among solid objects in everyday life, yet segmentation methods struggle to distinguish it from opaque materials due to its transparency and reflection. While it is known that human perception relies on boundary and reflective-object features to distinguish glass objects, the existing literature has not yet sufficiently captured both properties when handling transparent objects. Hence, we propose incorporating both of these powerful visual cues via the Boundary Feature Enhancement and Reflection Feature Enhancement modules in a mutually beneficial way. Our proposed framework, TransCues, is a pyramidal transformer encoder-decoder architecture to segment transparent objects. We empirically show that these two modules can be used together effectively, improving overall performance across various benchmark datasets, including glass object semantic segmentation, mirror object semantic segmentation, and generic segmentation datasets. Our method outperforms the state-of-the-art by a large margin, achieving +4.2% mIoU on Trans10K-v2, +5.6% mIoU on MSD, +10.1% mIoU on RGBD-Mirror, +13.1% mIoU on TROSD, and +8.3% mIoU on Stanford2D3D, showing the effectiveness of our method against glass objects.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faTransCues\u6846\u67b6\uff0c\u901a\u8fc7\u8fb9\u754c\u7279\u5f81\u589e\u5f3a\u548c\u53cd\u5c04\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u7ed3\u5408\u91d1\u5b57\u5854Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73bb\u7483\u7269\u4f53\u5206\u5272\u7684\u6311\u6218\u3002", "motivation": "\u73bb\u7483\u7269\u4f53\u56e0\u5176\u900f\u660e\u6027\u548c\u53cd\u5c04\u6027\uff0c\u73b0\u6709\u5206\u5272\u65b9\u6cd5\u96be\u4ee5\u5c06\u5176\u4e0e\u4e0d\u900f\u660e\u6750\u6599\u533a\u5206\u5f00\u3002\u4eba\u7c7b\u611f\u77e5\u4f9d\u8d56\u8fb9\u754c\u548c\u53cd\u5c04\u7269\u4f53\u7279\u5f81\u6765\u8bc6\u522b\u73bb\u7483\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u6355\u6349\u8fd9\u4e24\u79cd\u7279\u6027\u3002", "method": "\u63d0\u51faTransCues\u6846\u67b6\uff0c\u5305\u542b\u8fb9\u754c\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u548c\u53cd\u5c04\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u91c7\u7528\u91d1\u5b57\u5854Transformer\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5c06\u4e24\u79cd\u89c6\u89c9\u7ebf\u7d22\u4ee5\u4e92\u5229\u65b9\u5f0f\u7ed3\u5408\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff1aTrans10K-v2\u63d0\u53474.2% mIoU\uff0cMSD\u63d0\u53475.6% mIoU\uff0cRGBD-Mirror\u63d0\u534710.1% mIoU\uff0cTROSD\u63d0\u534713.1% mIoU\uff0cStanford2D3D\u63d0\u53478.3% mIoU\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u73bb\u7483\u7269\u4f53\u5206\u5272\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8fb9\u754c\u548c\u53cd\u5c04\u7279\u5f81\u589e\u5f3a\u6a21\u5757\u7684\u7ed3\u5408\u80fd\u591f\u663e\u8457\u63d0\u5347\u900f\u660e\u7269\u4f53\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2512.07375", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07375", "abs": "https://arxiv.org/abs/2512.07375", "authors": ["Yezi Liu", "Hanning Chen", "Wenjun Huang", "Yang Ni", "Mohsen Imani"], "title": "LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples", "comment": null, "summary": "Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.", "AI": {"tldr": "LUNE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u66f4\u65b0\u4f4e\u79e9\u9002\u914d\u5668\u6765\u9ad8\u6548\u79fb\u9664\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7279\u5b9a\u77e5\u8bc6\uff0c\u8ba1\u7b97\u6210\u672c\u6bd4\u5168\u5fae\u8c03\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u79fb\u9664\u7279\u5b9a\u4fe1\u606f\uff0c\u4f20\u7edf\u9057\u5fd8\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u9ad8\u6548\u7684\u9057\u5fd8\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eLoRA\u7684\u8d1f\u5411\u9057\u5fd8\u6846\u67b6\uff0c\u4ec5\u66f4\u65b0\u4f4e\u79e9\u9002\u914d\u5668\u800c\u51bb\u7ed3\u4e3b\u5e72\u7f51\u7edc\uff0c\u901a\u8fc7\u6291\u5236\u6216\u66ff\u6362\u4e2d\u95f4\u8868\u793a\u6765\u5b9e\u73b0\u77e5\u8bc6\u9057\u5fd8\u3002", "result": "\u5728\u591a\u4e2a\u4e8b\u5b9e\u9057\u5fd8\u4efb\u52a1\u4e0a\uff0cLUNE\u7684\u6548\u679c\u4e0e\u5168\u5fae\u8c03\u548c\u5185\u5b58\u7f16\u8f91\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "LUNE\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6548\u679c\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2512.07037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07037", "abs": "https://arxiv.org/abs/2512.07037", "authors": ["Josep M. Rocafort", "Shaolin Su", "Javier Vazquez-Corral", "Alexandra Gomez-Villa"], "title": "Evaluating and Preserving High-level Fidelity in Super-Resolution", "comment": null, "summary": "Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8861\u91cf\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u9ad8\u4fdd\u771f\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u6784\u5efa\u4e86\u9996\u4e2a\u5e26\u6ce8\u91ca\u7684\u9ad8\u4fdd\u771f\u5ea6\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e86\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0e\u9ad8\u4fdd\u771f\u5ea6\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u9ad8\u4fdd\u771f\u5ea6\u53cd\u9988\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u867d\u7136\u80fd\u751f\u6210\u89c6\u89c9\u4e0a\u4ee4\u4eba\u6109\u60a6\u7684\u7ed3\u679c\uff0c\u4f46\u6709\u65f6\u4f1a\u4ea7\u751f\u5e7b\u89c9\u6548\u5e94\u6539\u53d8\u56fe\u50cf\u5185\u5bb9\u3002\u73b0\u6709\u4f4e\u5c42\u6b21\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u8fd9\u79cd\u9ad8\u5c42\u6b21\u7684\u5185\u5bb9\u53d8\u5316\uff0c\u9700\u8981\u5efa\u7acb\u9ad8\u4fdd\u771f\u5ea6\u8bc4\u4f30\u6807\u51c6\u6765\u63ed\u793a\u751f\u6210\u5f0f\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u9996\u4e2a\u5e26\u6ce8\u91ca\u7684\u9ad8\u4fdd\u771f\u5ea6\u8bc4\u5206\u6570\u636e\u96c6\uff1b\u8bc4\u4f30SOTA\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u5ea6\u8868\u73b0\uff1b\u5206\u6790\u73b0\u6709\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u4e0e\u9ad8\u4fdd\u771f\u5ea6\u7684\u76f8\u5173\u6027\uff1b\u5229\u7528\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u9ad8\u5c42\u6b21\u4efb\u52a1\uff1b\u901a\u8fc7\u9ad8\u4fdd\u771f\u5ea6\u53cd\u9988\u5fae\u8c03\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u3002", "result": "\u5efa\u7acb\u4e86\u9ad8\u4fdd\u771f\u5ea6\u8bc4\u4f30\u6807\u51c6\uff1b\u5c55\u793a\u4e86\u9ad8\u4fdd\u771f\u5ea6\u53cd\u9988\u53ef\u4ee5\u540c\u65f6\u63d0\u5347\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\uff1b\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u80fd\u66f4\u597d\u5730\u5904\u7406\u9ad8\u5c42\u6b21\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u9ad8\u4fdd\u771f\u5ea6\u6807\u51c6\u5728\u6a21\u578b\u8bc4\u4f30\u548c\u4f18\u5316\u4e2d\u5177\u6709\u6f5c\u5728\u4ef7\u503c\uff0c\u4e3a\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u751f\u6210\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.07390", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07390", "abs": "https://arxiv.org/abs/2512.07390", "authors": ["Gilhyun Nam", "Taewon Kim", "Joonhyun Jeong", "Eunho Yang"], "title": "Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood", "comment": "Accepted to WACV 2026", "summary": "Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.", "AI": {"tldr": "SICL\u662f\u4e00\u4e2a\u57fa\u4e8e\u98ce\u683c\u4e0d\u53d8\u6027\u7684\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u91cf\u98ce\u683c\u53d8\u6362\u540e\u9884\u6d4b\u4e00\u81f4\u6027\u6765\u4f30\u8ba1\u5b9e\u4f8b\u7ea7\u6b63\u786e\u6027\u6982\u7387\uff0c\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u5373\u53ef\u5b9e\u73b0\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u6a21\u578b\u6216\u9759\u6001\u5206\u5e03\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u6d4b\u8bd5\u65f6\u81ea\u9002\u5e94\u6a21\u578b\u5f80\u5f80\u5b58\u5728\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u4e0d\u826f\u7684\u95ee\u9898\u3002", "method": "SICL\u5229\u7528\u98ce\u683c\u4e0d\u53d8\u6027\u539f\u7406\uff0c\u901a\u8fc7\u751f\u6210\u98ce\u683c\u53d8\u6362\u7684\u53d8\u4f53\u5e76\u6d4b\u91cf\u9884\u6d4b\u4e00\u81f4\u6027\u6765\u4f30\u8ba1\u5b9e\u4f8b\u7ea7\u6b63\u786e\u6027\u4f3c\u7136\uff0c\u4ec5\u9700\u6a21\u578b\u524d\u5411\u4f20\u64ad\uff0c\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u57284\u4e2a\u57fa\u7ebf\u30015\u79cdTTA\u65b9\u6cd5\u548c2\u4e2a\u73b0\u5b9e\u573a\u666f\u30013\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0cSICL\u76f8\u6bd4\u4f20\u7edf\u6821\u51c6\u65b9\u6cd5\u5e73\u5747\u51cf\u5c1113\u4e2a\u767e\u5206\u70b9\u7684\u6821\u51c6\u8bef\u5dee\u3002", "conclusion": "SICL\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6821\u51c6\u6a21\u5757\uff0c\u517c\u5bb9\u4efb\u4f55TTA\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u6d4b\u8bd5\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07051", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07051", "abs": "https://arxiv.org/abs/2512.07051", "authors": ["Adnan Munir", "Shujaat Khan"], "title": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation", "comment": "11 pages, 7 figures", "summary": "Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.", "AI": {"tldr": "DAUNet\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7UNet\u53d8\u4f53\uff0c\u96c6\u6210\u4e86\u53ef\u53d8\u5f62\u5377\u79ef\u548c\u53c2\u6570\u65e0\u5173\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4e14\u53c2\u6570\u9ad8\u6548\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6a21\u578b\u5bf9\u51e0\u4f55\u53d8\u5316\u9002\u5e94\u80fd\u529b\u4e0d\u8db3\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u878d\u5408\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u4ee5\u9002\u5e94\u4e34\u5e8a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "method": "\u4f7f\u7528Deformable V2 Convolutions\u5904\u7406\u51e0\u4f55\u53d8\u5316\uff0cSimAM\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u89e3\u7801\u5668\u548c\u8df3\u8dc3\u8fde\u63a5\uff0c\u5b9e\u73b0\u7a7a\u95f4\u81ea\u9002\u5e94\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7279\u5f81\u878d\u5408\u3002", "result": "\u5728FH-PS-AoP\u548cFUMPE\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDAUNet\u5728Dice\u5206\u6570\u3001HD95\u548cASD\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "DAUNet\u901a\u8fc7\u53ef\u53d8\u5f62\u5377\u79ef\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684\u534f\u540c\u4f5c\u7528\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u9002\u5408\u5b9e\u65f6\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2512.07393", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07393", "abs": "https://arxiv.org/abs/2512.07393", "authors": ["Yann Bourdin", "Pierrick Legrand", "Fanny Roche"], "title": "Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects", "comment": null, "summary": "This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u622a\u65ad\u65f6\u95f4\u53cd\u5411\u4f20\u64ad(TBPTT)\u5728\u6570\u5b57\u97f3\u9891\u6548\u679c\u5efa\u6a21\u4e2d\u7684\u4f18\u5316\uff0c\u91cd\u70b9\u5173\u6ce8\u52a8\u6001\u8303\u56f4\u538b\u7f29\uff0c\u901a\u8fc7\u8c03\u6574\u5173\u952e\u8d85\u53c2\u6570\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f18\u5316TBPTT\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6570\u5b57\u97f3\u9891\u6548\u679c\u5efa\u6a21\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u8303\u56f4\u538b\u7f29\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528\u5377\u79ef-\u5faa\u73af\u67b6\u6784\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30TBPTT\u7684\u5173\u952e\u8d85\u53c2\u6570\uff08\u5e8f\u5217\u6570\u3001\u6279\u6b21\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\uff09\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5f71\u54cd\u3002", "result": "\u4f18\u5316\u540e\u7684TBPTT\u53c2\u6570\u8bbe\u7f6e\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7cbe\u5ea6\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u5ba2\u89c2\u8bc4\u4f30\u548c\u4e3b\u89c2\u542c\u611f\u6d4b\u8bd5\u5747\u8bc1\u5b9e\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u7cbe\u5fc3\u8c03\u6574TBPTT\u8d85\u53c2\u6570\u53ef\u4ee5\u6709\u6548\u6539\u5584\u6570\u5b57\u97f3\u9891\u6548\u679c\u5efa\u6a21\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u9ad8\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2512.07052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07052", "abs": "https://arxiv.org/abs/2512.07052", "authors": ["Hoang-Nhat Tran", "Francesco Di Sario", "Gabriele Spadaro", "Giuseppe Valenzise", "Enzo Tartaglione"], "title": "RAVE: Rate-Adaptive Visual Encoding for 3D Gaussian Splatting", "comment": null, "summary": "Recent advances in neural scene representations have transformed immersive multimedia, with 3D Gaussian Splatting (3DGS) enabling real-time photorealistic rendering. Despite its efficiency, 3DGS suffers from large memory requirements and costly training procedures, motivating efforts toward compression. Existing approaches, however, operate at fixed rates, limiting adaptability to varying bandwidth and device constraints. In this work, we propose a flexible compression scheme for 3DGS that supports interpolation at any rate between predefined bounds. Our method is computationally lightweight, requires no retraining for any rate, and preserves rendering quality across a broad range of operating points. Experiments demonstrate that the approach achieves efficient, high-quality compression while offering dynamic rate control, making it suitable for practical deployment in immersive applications. The code will be provided open-source upon acceptance of the work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u5728\u9884\u5b9a\u4e49\u8fb9\u754c\u4e4b\u95f4\u4efb\u610f\u901f\u7387\u63d2\u503c\u7684\u7075\u6d3b3D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u65b9\u6848\uff0c\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u901f\u7387\u91cd\u65b0\u8bad\u7ec3\uff0c\u8ba1\u7b97\u8f7b\u91cf\u4e14\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u80fd\u5b9e\u73b0\u5b9e\u65f6\u903c\u771f\u6e32\u67d3\uff0c\u4f46\u5b58\u5728\u5185\u5b58\u9700\u6c42\u5927\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u53ea\u80fd\u5728\u56fa\u5b9a\u901f\u7387\u4e0b\u5de5\u4f5c\uff0c\u65e0\u6cd5\u9002\u5e94\u53d8\u5316\u7684\u5e26\u5bbd\u548c\u8bbe\u5907\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u538b\u7f29\u65b9\u6848\uff0c\u652f\u6301\u5728\u9884\u5b9a\u4e49\u8fb9\u754c\u4e4b\u95f4\u8fdb\u884c\u4efb\u610f\u901f\u7387\u7684\u63d2\u503c\uff0c\u8be5\u65b9\u6cd5\u8ba1\u7b97\u8f7b\u91cf\uff0c\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u538b\u7f29\u901f\u7387\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5e7f\u6cdb\u7684\u64cd\u4f5c\u70b9\u4e0a\u90fd\u80fd\u5b9e\u73b0\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u538b\u7f29\uff0c\u540c\u65f6\u63d0\u4f9b\u52a8\u6001\u901f\u7387\u63a7\u5236\uff0c\u4fdd\u6301\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u5728\u6c89\u6d78\u5f0f\u5e94\u7528\u4e2d\u5b9e\u9645\u90e8\u7f72\uff0c\u4ee3\u7801\u5c06\u5728\u5de5\u4f5c\u88ab\u63a5\u53d7\u540e\u5f00\u6e90\u63d0\u4f9b\u3002"}}
{"id": "2512.07400", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07400", "abs": "https://arxiv.org/abs/2512.07400", "authors": ["Giulia Lanzillotta", "Damiano Meier", "Thomas Hofmann"], "title": "Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse", "comment": null, "summary": "A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the \"strong collapse\" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u6df1\u5ea6\u7279\u5f81\u7a7a\u95f4\u9057\u5fd8\u4e0e\u6d45\u5c42\u5206\u7c7b\u5668\u9057\u5fd8\u4e4b\u95f4\u7684\u4e0d\u5bf9\u79f0\u6027\uff0c\u6307\u51fa\u5373\u4f7f\u5c0f\u7f13\u51b2\u533a\u4e5f\u80fd\u9632\u6b62\u6df1\u5ea6\u9057\u5fd8\uff0c\u4f46\u7f13\u89e3\u6d45\u5c42\u9057\u5fd8\u9700\u8981\u66f4\u5927\u5bb9\u91cf\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u795e\u7ecf\u7f51\u7edc\u80fd\u4fdd\u6301\u8fc7\u53bb\u4efb\u52a1\u7684\u7ebf\u6027\u53ef\u5206\u8868\u793a\u4f46\u8f93\u51fa\u9884\u6d4b\u5931\u8d25\u8fd9\u4e00\u77db\u76fe\u73b0\u8c61\u3002", "method": "\u5c06\u795e\u7ecf\u5d29\u6e83\u6846\u67b6\u6269\u5c55\u5230\u987a\u5e8f\u8bbe\u7f6e\uff0c\u5206\u6790\u7279\u5f81\u51e0\u4f55\u6f02\u79fb\u548c\u534f\u65b9\u5dee\u77e9\u9635\u79e9\u4e0d\u8db3\u95ee\u9898\uff0c\u5c06\u6301\u7eed\u5b66\u4e60\u4e0e\u5206\u5e03\u5916\u68c0\u6d4b\u7edf\u4e00\u3002", "result": "\u8bc1\u660e\u4efb\u4f55\u975e\u96f6\u91cd\u64ad\u5206\u6570\u90fd\u80fd\u6e10\u8fd1\u4fdd\u8bc1\u7ebf\u6027\u53ef\u5206\u6027\u7684\u4fdd\u6301\uff0c\u4f46\u5c0f\u7f13\u51b2\u533a\u4f1a\u5bfc\u81f4\u5f3a\u5d29\u6e83\u548c\u7edf\u8ba1\u4f2a\u5f71\u3002", "conclusion": "\u6311\u6218\u4f9d\u8d56\u5927\u7f13\u51b2\u533a\u7684\u4f20\u7edf\u505a\u6cd5\uff0c\u5efa\u8bae\u901a\u8fc7\u663e\u5f0f\u6821\u6b63\u7edf\u8ba1\u4f2a\u5f71\u6765\u5b9e\u73b0\u6700\u5c0f\u91cd\u64ad\u4e0b\u7684\u9c81\u68d2\u6027\u80fd\u3002"}}
{"id": "2512.07062", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07062", "abs": "https://arxiv.org/abs/2512.07062", "authors": ["Changliang Xia", "Chengyou Jia", "Minnan Luo", "Zhuohang Dang", "Xin Shen", "Bowen Ping"], "title": "$\\mathrm{D}^{\\mathrm{3}}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction", "comment": null, "summary": "Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\\mathrm{D}^{\\mathrm{3}}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\\mathrm{D}^{\\mathrm{3}}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\\mathrm{D}^{\\mathrm{3}}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86D\u00b3-Predictor\uff0c\u4e00\u79cd\u65e0\u566a\u58f0\u7684\u786e\u5b9a\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6765\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u968f\u673a\u566a\u58f0\u5bf9\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u4e0d\u5229\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u5c06\u6269\u6563\u6a21\u578b\u89c6\u4e3a\u65f6\u95f4\u6b65\u76f8\u5173\u7684\u89c6\u89c9\u4e13\u5bb6\u96c6\u5408\uff0c\u5e76\u81ea\u76d1\u7763\u5730\u805a\u5408\u5176\u5f02\u6784\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u9ad8\u6548\u7684\u5355\u6b65\u63a8\u7406\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u89c6\u89c9\u5148\u9a8c\u80fd\u529b\uff0c\u4f46\u5176\u6838\u5fc3\u7684\u968f\u673a\u566a\u58f0\u4e0e\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u6240\u9700\u7684\u786e\u5b9a\u6027\u56fe\u50cf\u5230\u51e0\u4f55\u6620\u5c04\u5b58\u5728\u6839\u672c\u6027\u51b2\u7a81\u3002\u968f\u673a\u566a\u58f0\u4f1a\u7834\u574f\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ebf\u7d22\uff0c\u4f7f\u6a21\u578b\u504f\u5411\u65f6\u95f4\u6b65\u7279\u5b9a\u7684\u566a\u58f0\u76ee\u6807\uff0c\u4ece\u800c\u7834\u574f\u6709\u610f\u4e49\u7684\u51e0\u4f55\u7ed3\u6784\u6620\u5c04\u3002", "method": "D\u00b3-Predictor\u901a\u8fc7\u91cd\u65b0\u6784\u5efa\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u53bb\u9664\u968f\u673a\u6027\u566a\u58f0\u3002\u8be5\u65b9\u6cd5\u5c06\u6269\u6563\u7f51\u7edc\u89c6\u4e3a\u65f6\u95f4\u6b65\u76f8\u5173\u7684\u89c6\u89c9\u4e13\u5bb6\u96c6\u5408\uff0c\u81ea\u76d1\u7763\u5730\u805a\u5408\u8fd9\u4e9b\u5f02\u6784\u5148\u9a8c\u5f62\u6210\u4e00\u4e2a\u5e72\u51c0\u5b8c\u6574\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u5e76\u5229\u7528\u4efb\u52a1\u7279\u5b9a\u76d1\u7763\u5c06\u5176\u9002\u914d\u5230\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u3002", "result": "\u5728\u591a\u79cd\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cD\u00b3-Predictor\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u8fbe\u5230\u4e86\u7ade\u4e89\u6027\u6216\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u4ec5\u9700\u4e4b\u524d\u4e00\u534a\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u80fd\u4ee5\u5355\u6b65\u9ad8\u6548\u5b8c\u6210\u63a8\u7406\u3002", "conclusion": "D\u00b3-Predictor\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u968f\u673a\u566a\u58f0\u4e0e\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u6570\u636e\u9700\u6c42\u5c11\u7684\u786e\u5b9a\u6027\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2512.07417", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07417", "abs": "https://arxiv.org/abs/2512.07417", "authors": ["Giray \u00d6n\u00fcr", "Azita Dabiri", "Bart De Schutter"], "title": "Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning", "comment": null, "summary": "Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u8c03\u6574\u72b6\u6001\u53cd\u9988\u4ea4\u901a\u63a7\u5236\u5668\u7684\u53c2\u6570\uff0c\u7ed3\u5408\u4e86\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u53cd\u5e94\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u7ba1\u7406\u7b56\u7565\uff08\u5982\u8def\u7ebf\u5f15\u5bfc\u3001\u531d\u9053\u8ba1\u91cf\u548c\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff09\u901a\u5e38\u4f9d\u8d56\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\uff0c\u867d\u7136\u7b80\u5355\u4e14\u53cd\u5e94\u8fc5\u901f\uff0c\u4f46\u7f3a\u4e4f\u5e94\u5bf9\u590d\u6742\u65f6\u53d8\u4ea4\u901a\u52a8\u6001\u7684\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4ee5\u8f83\u4f4e\u9891\u7387\u81ea\u9002\u5e94\u8c03\u6574\u72b6\u6001\u53cd\u9988\u63a7\u5236\u5668\u7684\u53c2\u6570\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u9ad8\u9891\u786e\u5b9a\u63a7\u5236\u52a8\u4f5c\u3002\u591a\u667a\u80fd\u4f53\u7ed3\u6784\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u5c40\u90e8\u63a7\u5236\u5668\u5728\u90e8\u5206\u6545\u969c\u65f6\u53ef\u72ec\u7acb\u8fd0\u884c\u3002", "result": "\u5728\u6a21\u62df\u7684\u591a\u7c7b\u4ea4\u901a\u7f51\u7edc\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u6846\u67b6\u5728\u65e0\u63a7\u5236\u548c\u56fa\u5b9a\u53c2\u6570\u72b6\u6001\u53cd\u9988\u63a7\u5236\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4e0e\u5355\u667a\u80fd\u4f53RL\u81ea\u9002\u5e94\u72b6\u6001\u53cd\u9988\u63a7\u5236\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u5bf9\u90e8\u5206\u6545\u969c\u7684\u6062\u590d\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u72b6\u6001\u53cd\u9988\u63a7\u5236\u7684\u53cd\u5e94\u6027\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u5e76\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65f6\u53d8\u7684\u4ea4\u901a\u73af\u5883\u3002"}}
{"id": "2512.07065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07065", "abs": "https://arxiv.org/abs/2512.07065", "authors": ["Anil Chintapalli", "Peter Tenholder", "Henry Chen", "Arjun Rao"], "title": "Persistent Homology-Guided Frequency Filtering for Image Compression", "comment": "17 pages, 8 figures, code available at github.com/RMATH3/persistent-homology-compression", "summary": "Feature extraction in noisy image datasets presents many challenges in model reliability. In this paper, we use the discrete Fourier transform in conjunction with persistent homology analysis to extract specific frequencies that correspond with certain topological features of an image. This method allows the image to be compressed and reformed while ensuring that meaningful data can be differentiated. Our experimental results show a level of compression comparable to that of using JPEG using six different metrics. The end goal of persistent homology-guided frequency filtration is its potential to improve performance in binary classification tasks (when augmenting a Convolutional Neural Network) compared to traditional feature extraction and compression methods. These findings highlight a useful end result: enhancing the reliability of image compression under noisy conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u548c\u6301\u7eed\u6027\u540c\u8c03\u5206\u6790\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u566a\u58f0\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u7279\u5b9a\u9891\u7387\u5bf9\u5e94\u7684\u62d3\u6251\u7279\u5f81\uff0c\u5b9e\u73b0\u56fe\u50cf\u538b\u7f29\u548c\u91cd\u6784\u3002", "motivation": "\u89e3\u51b3\u566a\u58f0\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u7279\u5f81\u63d0\u53d6\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u7ed3\u5408\u6301\u7eed\u6027\u540c\u8c03\u5206\u6790\uff0c\u63d0\u53d6\u4e0e\u56fe\u50cf\u62d3\u6251\u7279\u5f81\u5bf9\u5e94\u7684\u7279\u5b9a\u9891\u7387\uff0c\u5b9e\u73b0\u9891\u7387\u8fc7\u6ee4\u548c\u56fe\u50cf\u538b\u7f29\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u516d\u79cd\u6307\u6807\u4e0b\u8fbe\u5230\u4e0eJPEG\u76f8\u5f53\u7684\u538b\u7f29\u6c34\u5e73\uff0c\u5e76\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u6709\u6f5c\u5728\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u589e\u5f3a\u566a\u58f0\u6761\u4ef6\u4e0b\u56fe\u50cf\u538b\u7f29\u7684\u53ef\u9760\u6027\uff0c\u4e3a\u56fe\u50cf\u5904\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u7279\u5f81\u5f15\u5bfc\u7684\u9891\u7387\u8fc7\u6ee4\u65b9\u6cd5\u3002"}}
{"id": "2512.07419", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07419", "abs": "https://arxiv.org/abs/2512.07419", "authors": ["Haidong Kang", "Jun Du", "Lihong Lin"], "title": "Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models", "comment": null, "summary": "Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u514d\u8d39\u81ea\u52a8\u4ee3\u7406\u53d1\u73b0\u6846\u67b6TAP\uff0c\u7528\u4e8e\u89e3\u51b3\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u4e2d\u4ee3\u7406\u8bbe\u8ba1\u7684\u4eba\u5de5\u4f9d\u8d56\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63d0\u793a\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u4ee3\u7406\u53d1\u73b0\u3002", "motivation": "\u4f20\u7edf\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u53ef\u5fae\u5206\u4f18\u5316\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u7075\u6d3b\u6027\uff1b\u8981\u4e48\u9700\u8981\u4eba\u5de5\u4e13\u5bb6\u8bbe\u8ba1\u7684\u4ee3\u7406\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u3002\u80fd\u5426\u8bbe\u8ba1\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u4e13\u5bb6\u53c2\u4e0e\u548c\u8bad\u7ec3\u7684\u4ee3\u7406\uff1f", "method": "\u63d0\u51faTAP\u6846\u67b6\uff1a1\uff09\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u53d1\u73b0\u9002\u5408\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7684\u4ee3\u7406\uff1b2\uff09\u8bbe\u8ba1\u57fa\u4e8e\u76f4\u63a5\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u63d0\u793a\uff0c\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\uff0c\u6784\u5efaLLM\u4e0eMPQ\u4efb\u52a1\u4e4b\u95f4\u7684\u6b63\u5411\u53cd\u9988\u5faa\u73af\u3002", "result": "\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTAP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TAP\u4e3a\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684LLM\u9a71\u52a8\u8bbe\u8ba1\u7b97\u6cd5\u89c6\u89d2\uff0c\u5c06\u663e\u8457\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.07076", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07076", "abs": "https://arxiv.org/abs/2512.07076", "authors": ["Chen-Yang Wang", "Gepeng Ji", "Song Shao", "Ming-Ming Cheng", "Deng-Ping Fan"], "title": "Context-measure: Contextualizing Metric for Camouflage", "comment": "Technical Report", "summary": "Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8bc4\u4f30\u8303\u5f0fContext-measure\uff0c\u7528\u4e8e\u8bc4\u4f30\u4f2a\u88c5\u573a\u666f\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6307\u6807\u5ffd\u7565\u7a7a\u95f4\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4f2a\u88c5\u573a\u666f\u8bc4\u4f30\u6307\u6807\u4e3b\u8981\u9488\u5bf9\u4e00\u822c\u6216\u663e\u8457\u7269\u4f53\u8bbe\u8ba1\uff0c\u5047\u8bbe\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4e0d\u76f8\u5173\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4f2a\u88c5\u6548\u679c\u7684\u5b9e\u9645\u611f\u77e5\u3002", "method": "\u57fa\u4e8e\u6982\u7387\u50cf\u7d20\u611f\u77e5\u76f8\u5173\u6846\u67b6\uff0c\u7ed3\u5408\u7a7a\u95f4\u4f9d\u8d56\u6027\u548c\u50cf\u7d20\u7ea7\u4f2a\u88c5\u91cf\u5316\uff0c\u6784\u5efa\u4e0a\u4e0b\u6587\u5316\u8bc4\u4f30\u65b9\u6cd5\u3002", "result": "\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4f2a\u88c5\u7269\u4f53\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cContext-measure\u6bd4\u73b0\u6709\u4e0a\u4e0b\u6587\u65e0\u5173\u6307\u6807\u66f4\u53ef\u9760\u3002", "conclusion": "Context-measure\u4e3a\u6d89\u53ca\u4f2a\u88c5\u6a21\u5f0f\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u8bc4\u4f30\u57fa\u51c6\uff0c\u9002\u7528\u4e8e\u519c\u4e1a\u3001\u5de5\u4e1a\u548c\u533b\u7597\u7b49\u573a\u666f\u3002"}}
{"id": "2512.07430", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07430", "abs": "https://arxiv.org/abs/2512.07430", "authors": ["Yangle Li", "Danli Luo", "Haifeng Hu"], "title": "MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis", "comment": null, "summary": "Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9886\u57df\u6cdb\u5316\u6846\u67b6MIDG\uff0c\u901a\u8fc7\u6df7\u5408\u4e0d\u53d8\u4e13\u5bb6\u6a21\u578b\u548c\u8de8\u6a21\u6001\u9002\u914d\u5668\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u6a21\u6001\u95f4\u534f\u540c\u5173\u7cfb\u4e0d\u8db3\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u788e\u7247\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u5728\u63d0\u53d6\u4e0d\u53d8\u7279\u5f81\u65f6\u5ffd\u89c6\u4e86\u6a21\u6001\u95f4\u7684\u534f\u540c\u5173\u7cfb\uff0c\u4e14\u77e5\u8bc6\u6ce8\u5165\u6280\u672f\u5b58\u5728\u8de8\u6a21\u6001\u77e5\u8bc6\u788e\u7247\u5316\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u591a\u6a21\u6001\u6570\u636e\u7684\u4e30\u5bcc\u8bed\u4e49\u4fe1\u606f\u3002", "method": "1) \u6df7\u5408\u4e0d\u53d8\u4e13\u5bb6\u6a21\u578b\uff1a\u63d0\u53d6\u9886\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u589e\u5f3a\u6a21\u6001\u95f4\u534f\u540c\u5173\u7cfb\u7684\u5b66\u4e60\u80fd\u529b\uff1b2) \u8de8\u6a21\u6001\u9002\u914d\u5668\uff1a\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u6ce8\u5165\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\u7684\u8bed\u4e49\u4e30\u5bcc\u5ea6\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u9886\u57df\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MIDG\u6846\u67b6\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u6a21\u6001\u95f4\u534f\u540c\u5173\u7cfb\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u6ce8\u5165\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u5728\u9886\u57df\u6cdb\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.07078", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07078", "abs": "https://arxiv.org/abs/2512.07078", "authors": ["Bo Gao", "Jingcheng Tong", "Xingsheng Chen", "Han Yu", "Zichen Li"], "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection", "comment": "16 pages", "summary": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.\n  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.\n  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.", "AI": {"tldr": "DFIR-DETR\u662f\u4e00\u79cd\u9488\u5bf9\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u7279\u5f81\u805a\u5408\u548c\u9891\u57df\u5904\u7406\u89e3\u51b3\u7279\u5f81\u9000\u5316\u3001\u957f\u8ddd\u79bb\u4f9d\u8d56\u548c\u7279\u5f81\u56fe\u81a8\u80c0\u95ee\u9898\uff0c\u5728\u65e0\u4eba\u673a\u9065\u611f\u56fe\u50cf\u548c\u5de5\u4e1a\u7f3a\u9677\u68c0\u6d4b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u7279\u5f81\u968f\u7f51\u7edc\u4e0b\u91c7\u6837\u4e25\u91cd\u9000\u5316\u3001\u7a7a\u95f4\u5377\u79ef\u65e0\u6cd5\u6709\u6548\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u6807\u51c6\u4e0a\u91c7\u6837\u65b9\u6cd5\u5bfc\u81f4\u7279\u5f81\u56fe\u4e0d\u5fc5\u8981\u81a8\u80c0\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aDCFA\u6a21\u5757\u4f7f\u7528\u52a8\u6001K\u7a00\u758f\u6ce8\u610f\u529b\u964d\u4f4e\u590d\u6742\u5ea6\uff0cDFPN\u6a21\u5757\u5e94\u7528\u5e45\u5ea6\u5f52\u4e00\u5316\u4e0a\u91c7\u6837\u9632\u6b62\u7279\u5f81\u81a8\u80c0\uff0cFIRC3\u6a21\u5757\u5728\u9891\u57df\u64cd\u4f5c\u5b9e\u73b0\u5168\u5c40\u611f\u53d7\u91ce\u3002", "result": "\u5728NEU-DET\u548cVisDrone\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523092.9%\u548c51.6%\u7684mAP50\uff0c\u53c2\u6570\u91cf\u4ec511.7M\uff0c\u8ba1\u7b97\u91cf41.2 GFLOPs\uff0c\u5747\u4e3a\u5f53\u524d\u6700\u4f18\u3002", "conclusion": "DFIR-DETR\u5728\u4e24\u4e2a\u4e0d\u540c\u9886\u57df\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u5176\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u8de8\u573a\u666f\u5c0f\u76ee\u6807\u68c0\u6d4b\u3002"}}
{"id": "2512.07433", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.07433", "abs": "https://arxiv.org/abs/2512.07433", "authors": ["Yezi Liu", "William Youngwoo Chung", "Yang Ni", "Hanning Chen", "Mohsen Imani"], "title": "Mitigating Bias in Graph Hyperdimensional Computing", "comment": null, "summary": "Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\\approx 10\\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u56fe\u8d85\u7ef4\u8ba1\u7b97\uff08HDC\uff09\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86FairGHDC\u6846\u67b6\u6765\u7f13\u89e3\u504f\u89c1\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u5e76\u5b9e\u73b0\u4e86\u7ea610\u500d\u7684\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u3002", "motivation": "\u867d\u7136\u56feHDC\u5728\u8ba4\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u516c\u5e73\u6027\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u6570\u636e\u8868\u793a\u548c\u51b3\u7b56\u89c4\u5219\u4e2d\u7684\u504f\u89c1\u53ef\u80fd\u5bfc\u81f4\u5bf9\u4e0d\u540c\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u5bf9\u5f85\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u7f13\u89e3\u8fd9\u79cd\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86FairGHDC\u6846\u67b6\uff0c\u5f15\u5165\u57fa\u4e8e\u5dee\u8ddd\u7684\u4eba\u53e3\u7edf\u8ba1\u5947\u5076\u6b63\u5219\u5316\u5668\u7684\u504f\u89c1\u6821\u6b63\u9879\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u6807\u91cf\u516c\u5e73\u56e0\u5b50\uff0c\u76f4\u63a5\u5728\u8d85\u5411\u91cf\u7a7a\u95f4\u4e2d\u7f29\u653e\u771f\u5b9e\u6807\u7b7e\u7c7b\u522b\u8d85\u5411\u91cf\u7684\u66f4\u65b0\uff0c\u65e0\u9700\u4fee\u6539\u56fe\u7f16\u7801\u5668\u6216\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFairGHDC\u663e\u8457\u51cf\u5c11\u4e86\u4eba\u53e3\u7edf\u8ba1\u5947\u5076\u548c\u673a\u4f1a\u5747\u7b49\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6807\u51c6GNN\u548c\u516c\u5e73\u611f\u77e5GNN\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u7ea610\u500d\u7684\u8bad\u7ec3\u52a0\u901f\u3002", "conclusion": "FairGHDC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u56feHDC\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u516c\u5e73\u6027\u63d0\u5347\uff0c\u4e3a\u8111\u542f\u53d1\u5f0f\u8ba1\u7b97\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.07107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07107", "abs": "https://arxiv.org/abs/2512.07107", "authors": ["Jaeyoon Lee", "Hojoon Jung", "Sungtae Hwang", "Jihyong Oh", "Jongwon Choi"], "title": "COREA: Coarse-to-Fine 3D Representation Alignment Between Relightable 3D Gaussians and SDF via Bidirectional 3D-to-3D Supervision", "comment": "Project page: https://vilab-cau.github.io/COREA/", "summary": "We present COREA, the first unified framework that jointly learns relightable 3D Gaussians and a Signed Distance Field (SDF) for accurate geometry reconstruction and faithful relighting. While recent 3D Gaussian Splatting (3DGS) methods have extended toward mesh reconstruction and physically-based rendering (PBR), their geometry is still learned from 2D renderings, leading to coarse surfaces and unreliable BRDF-lighting decomposition. To address these limitations, COREA introduces a coarse-to-fine bidirectional 3D-to-3D alignment strategy that allows geometric signals to be learned directly in 3D space. Within this strategy, depth provides coarse alignment between the two representations, while depth gradients and normals refine fine-scale structure, and the resulting geometry supports stable BRDF-lighting decomposition. A density-control mechanism further stabilizes Gaussian growth, balancing geometric fidelity with memory efficiency. Experiments on standard benchmarks demonstrate that COREA achieves superior performance in novel-view synthesis, mesh reconstruction, and PBR within a unified framework.", "AI": {"tldr": "COREA\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u9996\u6b21\u8054\u5408\u5b66\u4e60\u53ef\u91cd\u7167\u660e\u76843D\u9ad8\u65af\u548cSDF\uff0c\u5b9e\u73b0\u7cbe\u786e\u51e0\u4f55\u91cd\u5efa\u548c\u5fe0\u5b9e\u91cd\u7167\u660e\u3002\u901a\u8fc73D\u52303D\u7684\u7c97\u5230\u7ec6\u53cc\u5411\u5bf9\u9f50\u7b56\u7565\uff0c\u76f4\u63a5\u57283D\u7a7a\u95f4\u4e2d\u5b66\u4e60\u51e0\u4f55\u4fe1\u53f7\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u51e0\u4f55\u7c97\u7cd9\u548cBRDF-\u5149\u7167\u5206\u89e3\u4e0d\u53ef\u9760\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\u867d\u7136\u6269\u5c55\u5230\u7f51\u683c\u91cd\u5efa\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\uff0c\u4f46\u5176\u51e0\u4f55\u4ecd\u4ece2D\u6e32\u67d3\u4e2d\u5b66\u4e60\uff0c\u5bfc\u81f4\u8868\u9762\u7c97\u7cd9\u548cBRDF-\u5149\u7167\u5206\u89e3\u4e0d\u53ef\u9760\u3002\u9700\u8981\u76f4\u63a5\u57283D\u7a7a\u95f4\u4e2d\u5b66\u4e60\u51e0\u4f55\u4fe1\u53f7\u4ee5\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u3002", "method": "1\uff09\u5f15\u5165\u7c97\u5230\u7ec6\u53cc\u54113D\u52303D\u5bf9\u9f50\u7b56\u7565\uff1a\u6df1\u5ea6\u63d0\u4f9b\u7c97\u5bf9\u9f50\uff0c\u6df1\u5ea6\u68af\u5ea6\u548c\u6cd5\u5411\u91cf\u7ec6\u5316\u7cbe\u7ec6\u7ed3\u6784\uff1b2\uff09\u5bc6\u5ea6\u63a7\u5236\u673a\u5236\u7a33\u5b9a\u9ad8\u65af\u589e\u957f\uff0c\u5e73\u8861\u51e0\u4f55\u4fdd\u771f\u5ea6\u548c\u5185\u5b58\u6548\u7387\uff1b3\uff09\u8054\u5408\u5b66\u4e60\u53ef\u91cd\u7167\u660e3D\u9ad8\u65af\u548cSDF\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOREA\u5728\u65b0\u89c6\u89d2\u5408\u6210\u3001\u7f51\u683c\u91cd\u5efa\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6e32\u67d3\u65b9\u9762\u5747\u53d6\u5f97\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "COREA\u901a\u8fc73D\u52303D\u7684\u76f4\u63a5\u51e0\u4f55\u5b66\u4e60\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u91cd\u5efa\u548c\u91cd\u7167\u660e\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a3D\u91cd\u5efa\u548c\u6e32\u67d3\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07110", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07110", "abs": "https://arxiv.org/abs/2512.07110", "authors": ["Liangwei Jiang", "Jinluo Xie", "Yecheng Huang", "Hua Zhang", "Hongyu Yang", "Di Huang"], "title": "MSN: Multi-directional Similarity Network for Hand-crafted and Deep-synthesized Copy-Move Forgery Detection", "comment": null, "summary": "Copy-move image forgery aims to duplicate certain objects or to hide specific contents with copy-move operations, which can be achieved by a sequence of manual manipulations as well as up-to-date deep generative network-based swapping. Its detection is becoming increasingly challenging for the complex transformations and fine-tuned operations on the tampered regions. In this paper, we propose a novel two-stream model, namely Multi-directional Similarity Network (MSN), to accurate and efficient copy-move forgery detection. It addresses the two major limitations of existing deep detection models in \\textbf{representation} and \\textbf{localization}, respectively. In representation, an image is hierarchically encoded by a multi-directional CNN network, and due to the diverse augmentation in scales and rotations, the feature achieved better measures the similarity between sampled patches in two streams. In localization, we design a 2-D similarity matrix based decoder, and compared with the current 1-D similarity vector based one, it makes full use of spatial information in the entire image, leading to the improvement in detecting tampered regions. Beyond the method, a new forgery database generated by various deep neural networks is presented, as a new benchmark for detecting the growing deep-synthesized copy-move. Extensive experiments are conducted on two classic image forensics benchmarks, \\emph{i.e.} CASIA CMFD and CoMoFoD, and the newly presented one. The state-of-the-art results are reported, which demonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u6d41\u6a21\u578bMSN\uff0c\u7528\u4e8e\u51c6\u786e\u9ad8\u6548\u7684\u590d\u5236-\u79fb\u52a8\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6df1\u5ea6\u68c0\u6d4b\u6a21\u578b\u5728\u8868\u793a\u548c\u5b9a\u4f4d\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u590d\u5236-\u79fb\u52a8\u56fe\u50cf\u4f2a\u9020\u68c0\u6d4b\u9762\u4e34\u590d\u6742\u53d8\u6362\u548c\u7cbe\u7ec6\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u73b0\u6709\u6df1\u5ea6\u68c0\u6d4b\u6a21\u578b\u5728\u7279\u5f81\u8868\u793a\u548c\u533a\u57df\u5b9a\u4f4d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u591a\u65b9\u5411\u76f8\u4f3c\u6027\u7f51\u7edc(MSN)\uff1a1\uff09\u4f7f\u7528\u591a\u65b9\u5411CNN\u7f51\u7edc\u8fdb\u884c\u5206\u5c42\u7f16\u7801\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u65cb\u8f6c\u589e\u5f3a\u5b9e\u73b0\u66f4\u597d\u7684\u7279\u5f81\u76f8\u4f3c\u6027\u5ea6\u91cf\uff1b2\uff09\u8bbe\u8ba1\u57fa\u4e8e2-D\u76f8\u4f3c\u6027\u77e9\u9635\u7684\u89e3\u7801\u5668\uff0c\u5145\u5206\u5229\u7528\u56fe\u50cf\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728CASIA CMFD\u3001CoMoFoD\u548c\u65b0\u63d0\u51fa\u7684\u6df1\u5ea6\u5408\u6210\u4f2a\u9020\u6570\u636e\u5e93\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u68c0\u6d4b\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MSN\u6a21\u578b\u901a\u8fc7\u6539\u8fdb\u7684\u7279\u5f81\u8868\u793a\u548c\u5b9a\u4f4d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u5236-\u79fb\u52a8\u4f2a\u9020\u68c0\u6d4b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u65b0\u6570\u636e\u5e93\u4e3a\u68c0\u6d4b\u6df1\u5ea6\u5408\u6210\u4f2a\u9020\u63d0\u4f9b\u4e86\u57fa\u51c6\u3002"}}
{"id": "2512.07450", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07450", "abs": "https://arxiv.org/abs/2512.07450", "authors": ["Imran Ahsan", "Hyunwook Yu", "Jinsung Kim", "Mucheol Kim"], "title": "Forget and Explain: Transparent Verification of GNN Unlearning", "comment": "To appear in WSDM 2026 (ACM International Conference on Web Search and Data Mining). Code is available at https://github.com/ImranAhsan23/F-E", "summary": "Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to \"forget\" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684GNN\u9057\u5fd8\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5220\u9664\u524d\u540e\u7684\u6a21\u578b\u89e3\u91ca\u5dee\u5f02\u6765\u900f\u660e\u9a8c\u8bc1\u9057\u5fd8\u6548\u679c\u3002", "motivation": "\u73b0\u6709GNN\u9057\u5fd8\u65b9\u6cd5\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u96be\u4ee5\u9a8c\u8bc1\u4fe1\u606f\u662f\u5426\u771f\u6b63\u88ab\u9057\u5fd8\uff0c\u7279\u522b\u662f\u5728GDPR\u7b49\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\u4e0b\u3002", "method": "\u4f7f\u7528\u5f52\u56e0\u504f\u79fb\u548c\u5c40\u90e8\u7ed3\u6784\u53d8\u5316\uff08\u5982\u56fe\u7f16\u8f91\u8ddd\u79bb\uff09\u4f5c\u4e3a\u900f\u660e\u8bc1\u636e\uff0c\u63d0\u51fa\u4e94\u79cd\u53ef\u89e3\u91ca\u6027\u6307\u6807\u6765\u9a8c\u8bc1\u9057\u5fd8\u6548\u679c\u3002", "result": "\u8bc4\u4f30\u663e\u793aRetrain\u548cGNNDelete\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u5168\u9057\u5fd8\uff0cGraphEditor\u63d0\u4f9b\u90e8\u5206\u64e6\u9664\uff0cIDEA\u5b58\u5728\u6b8b\u7559\u4fe1\u53f7\u3002", "conclusion": "\u89e3\u91ca\u5dee\u5f02\u63d0\u4f9b\u4e86\u4e3b\u8981\u7684\u4eba\u7c7b\u53ef\u8bfb\u9057\u5fd8\u8bc1\u636e\uff0c\u6210\u5458\u63a8\u65adROC-AUC\u53ef\u4f5c\u4e3a\u8865\u5145\u7684\u56fe\u8303\u56f4\u9690\u79c1\u4fe1\u53f7\u3002"}}
{"id": "2512.07126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07126", "abs": "https://arxiv.org/abs/2512.07126", "authors": ["Shengjie Lu", "Zhibin Wan", "Jiejie Liu", "Quan Zhang", "Mingjie Sun"], "title": "Training-free Clothing Region of Interest Self-correction for Virtual Try-On", "comment": "16 pages, 8 figures", "summary": "VTON (Virtual Try-ON) aims at synthesizing the target clothing on a certain person, preserving the details of the target clothing while keeping the rest of the person unchanged. Existing methods suffer from the discrepancies between the generated clothing results and the target ones, in terms of the patterns, textures and boundaries. Therefore, we propose to use an energy function to impose constraints on the attention map extracted through the generation process. Thus, at each generation step, the attention can be more focused on the clothing region of interest, thereby influencing the generation results to be more consistent with the target clothing details. Furthermore, to address the limitation that existing evaluation metrics concentrate solely on image realism and overlook the alignment with target elements, we design a new metric, Virtual Try-on Inception Distance (VTID), to bridge this gap and ensure a more comprehensive assessment. On the VITON-HD and DressCode datasets, our approach has outperformed the previous state-of-the-art (SOTA) methods by 1.4%, 2.3%, 12.3%, and 5.8% in the traditional metrics of LPIPS, FID, KID, and the new VTID metrics, respectively. Additionally, by applying the generated data to downstream Clothing-Change Re-identification (CC-Reid) methods, we have achieved performance improvements of 2.5%, 1.1%, and 1.6% on the LTCC, PRCC, VC-Clothes datasets in the metrics of Rank-1. The code of our method is public at https://github.com/MrWhiteSmall/CSC-VTON.git.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u51fd\u6570\u7684\u6ce8\u610f\u529b\u7ea6\u675f\u65b9\u6cd5\uff0c\u63d0\u5347\u865a\u62df\u8bd5\u8863\u751f\u6210\u6548\u679c\uff0c\u5e76\u8bbe\u8ba1\u65b0\u8bc4\u4f30\u6307\u6807VTID\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u8863\u65b9\u6cd5\u5728\u751f\u6210\u670d\u88c5\u7684\u56fe\u6848\u3001\u7eb9\u7406\u548c\u8fb9\u754c\u65b9\u9762\u4e0e\u76ee\u6807\u670d\u88c5\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u4ec5\u5173\u6ce8\u56fe\u50cf\u771f\u5b9e\u6027\u800c\u5ffd\u7565\u4e0e\u76ee\u6807\u5143\u7d20\u7684\u5bf9\u9f50\u3002", "method": "\u4f7f\u7528\u80fd\u91cf\u51fd\u6570\u5bf9\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6ce8\u610f\u529b\u56fe\u65bd\u52a0\u7ea6\u675f\uff0c\u4f7f\u6ce8\u610f\u529b\u66f4\u805a\u7126\u4e8e\u670d\u88c5\u533a\u57df\uff1b\u63d0\u51faVTID\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u3002", "result": "\u5728VITON-HD\u548cDressCode\u6570\u636e\u96c6\u4e0a\uff0cLPIPS\u3001FID\u3001KID\u548cVTID\u6307\u6807\u5206\u522b\u63d0\u53471.4%\u30012.3%\u300112.3%\u548c5.8%\uff1b\u5728\u4e0b\u6e38CC-Reid\u4efb\u52a1\u4e2d\uff0cRank-1\u6307\u6807\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u53472.5%\u30011.1%\u548c1.6%\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u865a\u62df\u8bd5\u8863\u751f\u6210\u8d28\u91cf\uff0c\u65b0\u8bc4\u4f30\u6307\u6807VTID\u80fd\u66f4\u5168\u9762\u8bc4\u4f30\u751f\u6210\u6548\u679c\uff0c\u4e14\u751f\u6210\u6570\u636e\u5bf9\u4e0b\u6e38\u4efb\u52a1\u6709\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2512.07463", "categories": ["cs.LG", "stat.AP", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.07463", "abs": "https://arxiv.org/abs/2512.07463", "authors": ["Rongmei Liang", "Zizheng Liu", "Xiaofei Wu", "Jingwen Tu"], "title": "Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification", "comment": null, "summary": "In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u8bc6\u7ed3\u6784\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5206\u5e03\u5f0f\u5b58\u50a8\u5927\u6570\u636e\u4e2d\u7684\u7ec4\u5408\u6b63\u5219\u5316\u652f\u6301\u5411\u91cf\u673a\uff08CR-SVMs\uff09\u8ba1\u7b97\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u5206\u5e03\u5f0f\u5e76\u884cADMM\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u9ad8\u65af\u56de\u4ee3\u6cd5\u786e\u4fdd\u6536\u655b\u3002", "motivation": "\u5728\u4eba\u5de5\u667a\u80fd\u5feb\u901f\u53d1\u5c55\u65f6\u4ee3\uff0c\u7ec4\u5408\u6b63\u5219\u5316\u652f\u6301\u5411\u91cf\u673a\uff08CR-SVMs\uff09\u80fd\u6709\u6548\u5904\u7406\u6570\u636e\u7279\u5f81\u7ed3\u6784\u4fe1\u606f\uff0c\u4f46\u5728\u5206\u5e03\u5f0f\u5b58\u50a8\u5927\u6570\u636e\u573a\u666f\u4e0b\u7f3a\u4e4f\u9ad8\u6548\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5171\u8bc6\u7ed3\u6784\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u5f00\u53d1\u5206\u5e03\u5f0f\u5e76\u884c\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08ADMM\uff09\u7b97\u6cd5\uff0c\u5f15\u5165\u9ad8\u65af\u56de\u4ee3\u6cd5\u786e\u4fdd\u6536\u655b\uff0c\u5e76\u63d0\u51fa\u7a00\u758f\u7fa4\u5957\u7d22\u652f\u6301\u5411\u91cf\u673a\uff08SGL-SVM\uff09\u6a21\u578b\u5e94\u7528\u4e8e\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u7b97\u6cd5\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0d\u53d7\u6b63\u5219\u5316\u9879\u548c\u635f\u5931\u51fd\u6570\u5f71\u54cd\uff0c\u5b9e\u9a8c\u5728\u5408\u6210\u548c\u81ea\u7531\u97f3\u4e50\u6863\u6848\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u53ef\u9760\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u5177\u6709\u5f3a\u6269\u5c55\u6027\uff0c\u53ef\u9002\u7528\u4e8e\u5404\u79cd\u635f\u5931\u51fd\u6570\u548c\u7ec4\u5408\u6b63\u5219\u5316\u9879\uff0c\u5e76\u884c\u7b97\u6cd5\u5177\u6709\u666e\u9002\u6027\uff0c\u5728\u5206\u5e03\u5f0f\u5927\u6570\u636e\u73af\u5883\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.07128", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07128", "abs": "https://arxiv.org/abs/2512.07128", "authors": ["Chau Truong", "Hieu Ta Quang", "Dung D. Le"], "title": "MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP", "comment": null, "summary": "Vision-language models like CLIP show impressive ability to align images and text, but their training on short, concise captions makes them struggle with lengthy, detailed descriptions. Recent advances mitigate this challenge by leveraging region-proposal information to map visual regions with corresponding sentences from lengthy captions, yet incurring notable deployment costs. We introduce MulCLIP, a novel end-to-end multi-level alignment framework that bridges natural long-text structures with image components. MulCLIP first preserves global contrastive alignment between images and both summary and long captions, while extending positional embeddings for longer text sequences. To further enhance fine-grained understanding, we propose two novel strategies: (1) a token reconstruction alignment over locally calibrated features to strengthen semantic connections between words and image patches, and (2) a subcaption-aggregated patch alignment that automatically extracts and aggregates context-rich patches for each subcaption. Experimental results across diverse benchmarks demonstrate our method consistently improves downstream performance, while ablation studies confirm its multi-scale alignment is the key factor driving better fine-grained capability than region-proposal-assisted approaches, making it particularly suitable for diverse real-world applications.", "AI": {"tldr": "MulCLIP\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u591a\u5c42\u6b21\u5bf9\u9f50\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u6587\u672c\u63cf\u8ff0\u65f6\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5168\u5c40\u5bf9\u6bd4\u5bf9\u9f50\u3001\u5c40\u90e8\u7279\u5f81\u91cd\u5efa\u548c\u5b50\u6807\u9898\u805a\u5408\u8865\u4e01\u5bf9\u9f50\u6765\u63d0\u5347\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u77ed\u6587\u672c\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u957f\u800c\u8be6\u7ec6\u7684\u63cf\u8ff0\u65f6\u6548\u679c\u4e0d\u4f73\u3002\u867d\u7136\u5df2\u6709\u65b9\u6cd5\u5229\u7528\u533a\u57df\u5efa\u8bae\u4fe1\u606f\u6765\u6620\u5c04\u89c6\u89c9\u533a\u57df\u548c\u957f\u6587\u672c\u53e5\u5b50\uff0c\u4f46\u90e8\u7f72\u6210\u672c\u8f83\u9ad8\u3002", "method": "1. \u4fdd\u6301\u56fe\u50cf\u4e0e\u6458\u8981/\u957f\u6807\u9898\u7684\u5168\u5c40\u5bf9\u6bd4\u5bf9\u9f50\uff0c\u6269\u5c55\u4f4d\u7f6e\u5d4c\u5165\u4ee5\u652f\u6301\u66f4\u957f\u6587\u672c\u5e8f\u5217\n2. \u63d0\u51fa\u5c40\u90e8\u7279\u5f81\u6821\u51c6\u7684token\u91cd\u5efa\u5bf9\u9f50\uff0c\u589e\u5f3a\u8bcd\u4e0e\u56fe\u50cf\u5757\u4e4b\u95f4\u7684\u8bed\u4e49\u8fde\u63a5\n3. \u63d0\u51fa\u5b50\u6807\u9898\u805a\u5408\u8865\u4e01\u5bf9\u9f50\uff0c\u81ea\u52a8\u63d0\u53d6\u548c\u805a\u5408\u6bcf\u4e2a\u5b50\u6807\u9898\u7684\u4e0a\u4e0b\u6587\u4e30\u5bcc\u8865\u4e01", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e00\u81f4\u63d0\u9ad8\u4e86\u4e0b\u6e38\u6027\u80fd\uff0c\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u5176\u591a\u5c3a\u5ea6\u5bf9\u9f50\u662f\u9a71\u52a8\u6bd4\u533a\u57df\u5efa\u8bae\u65b9\u6cd5\u66f4\u597d\u7ec6\u7c92\u5ea6\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "MulCLIP\u7684\u591a\u5c42\u6b21\u5bf9\u9f50\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u957f\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u9002\u5408\u591a\u6837\u5316\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u4e14\u76f8\u6bd4\u533a\u57df\u5efa\u8bae\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u90e8\u7f72\u4f18\u52bf\u3002"}}
{"id": "2512.07486", "categories": ["cs.LG", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2512.07486", "abs": "https://arxiv.org/abs/2512.07486", "authors": ["Niklas Dobberstein", "Jan Hamaekers"], "title": "Materium: An Autoregressive Approach for Material Generation", "comment": null, "summary": "We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.", "AI": {"tldr": "Materium\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u6676\u4f53\u7ed3\u6784\u7684\u81ea\u56de\u5f52\u53d8\u538b\u5668\uff0c\u901a\u8fc7\u5c063D\u6750\u6599\u8868\u793a\u8f6c\u6362\u4e3a\u4ee4\u724c\u5e8f\u5217\u6765\u5b9e\u73b0\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u6676\u4f53\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u7684\u6269\u6563\u65b9\u6cd5\u9700\u8981\u591a\u6b21\u8fed\u4ee3\u53bb\u566a\u6b65\u9aa4\u6765\u7ec6\u5316\u539f\u5b50\u4f4d\u7f6e\uff0c\u800cMaterium\u65e8\u5728\u901a\u8fc7\u7cbe\u786e\u653e\u7f6e\u539f\u5b50\u5750\u6807\u5b9e\u73b0\u66f4\u5feb\u901f\u3001\u9ad8\u6548\u7684\u6676\u4f53\u7ed3\u6784\u751f\u6210\u3002", "method": "\u5c063D\u6750\u6599\u8868\u793a\uff08\u5305\u62ec\u5143\u7d20\u6c27\u5316\u6001\u3001\u5206\u6570\u5750\u6807\u548c\u6676\u683c\u53c2\u6570\uff09\u8f6c\u6362\u4e3a\u4ee4\u724c\u5e8f\u5217\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u53d8\u538b\u5668\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u751f\u6210\u3002", "result": "\u6a21\u578b\u5728\u5355GPU\u4e0a\u4ec5\u9700\u51e0\u5c0f\u65f6\u5373\u53ef\u5b8c\u6210\u8bad\u7ec3\uff0c\u751f\u6210\u901f\u5ea6\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u5feb\u5f97\u591a\uff0c\u5728\u5355\u6761\u4ef6\u548c\u7ec4\u5408\u6761\u4ef6\u4e0b\u90fd\u80fd\u4ea7\u751f\u4e0e\u8f93\u5165\u8981\u6c42\u4e00\u81f4\u7684\u5019\u9009\u7ed3\u6784\u3002", "conclusion": "Materium\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u6676\u4f53\u7ed3\u6784\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u5c5e\u6027\u6761\u4ef6\u7ea6\u675f\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u6750\u6599\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\u3002"}}
{"id": "2512.07135", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07135", "abs": "https://arxiv.org/abs/2512.07135", "authors": ["Zebin Xing", "Pengxuan Yang", "Linbo Wang", "Yichen Zhang", "Yiming Hu", "Yupeng Zheng", "Junli Wang", "Yinfeng Gao", "Guang Li", "Kun Ma", "Long Chen", "Zhongpu Xia", "Qichao Zhang", "Hangjun Ye", "Dongbin Zhao"], "title": "TrajMoE: Scene-Adaptive Trajectory Planning with Mixture of Experts and Reinforcement Learning", "comment": null, "summary": "Current autonomous driving systems often favor end-to-end frameworks, which take sensor inputs like images and learn to map them into trajectory space via neural networks. Previous work has demonstrated that models can achieve better planning performance when provided with a prior distribution of possible trajectories. However, these approaches often overlook two critical aspects: 1) The appropriate trajectory prior can vary significantly across different driving scenarios. 2) Their trajectory evaluation mechanism lacks policy-driven refinement, remaining constrained by the limitations of one-stage supervised training. To address these issues, we explore improvements in two key areas. For problem 1, we employ MoE to apply different trajectory priors tailored to different scenarios. For problem 2, we utilize Reinforcement Learning to fine-tune the trajectory scoring mechanism. Additionally, we integrate models with different perception backbones to enhance perceptual features. Our integrated model achieved a score of 51.08 on the navsim ICCV benchmark, securing third place.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u9002\u5e94\u4e0d\u540c\u573a\u666f\u7684\u8f68\u8ff9\u5148\u9a8c\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8f68\u8ff9\u8bc4\u5206\u673a\u5236\uff0c\u5728navsim ICCV\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f9751.08\u5206\uff0c\u6392\u540d\u7b2c\u4e09\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u4e0d\u540c\u9a7e\u9a76\u573a\u666f\u9700\u8981\u4e0d\u540c\u7684\u8f68\u8ff9\u5148\u9a8c\u5206\u5e03\uff1b2\uff09\u8f68\u8ff9\u8bc4\u4f30\u673a\u5236\u7f3a\u4e4f\u7b56\u7565\u9a71\u52a8\u7684\u7ec6\u5316\uff0c\u53d7\u9650\u4e8e\u5355\u9636\u6bb5\u76d1\u7763\u8bad\u7ec3\u7684\u5c40\u9650\u6027\u3002", "method": "1\uff09\u4f7f\u7528\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u4e3a\u4e0d\u540c\u573a\u666f\u5e94\u7528\u5b9a\u5236\u5316\u7684\u8f68\u8ff9\u5148\u9a8c\uff1b2\uff09\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5bf9\u8f68\u8ff9\u8bc4\u5206\u673a\u5236\u8fdb\u884c\u5fae\u8c03\uff1b3\uff09\u96c6\u6210\u4e0d\u540c\u611f\u77e5\u9aa8\u5e72\u7f51\u7edc\u4ee5\u589e\u5f3a\u611f\u77e5\u7279\u5f81\u3002", "result": "\u5728navsim ICCV\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u83b7\u5f97\u4e8651.08\u5206\u7684\u6210\u7ee9\uff0c\u6392\u540d\u7b2c\u4e09\u3002", "conclusion": "\u901a\u8fc7\u573a\u666f\u81ea\u9002\u5e94\u7684\u8f68\u8ff9\u5148\u9a8c\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u8bc4\u5206\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u591a\u9636\u6bb5\u4f18\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.07490", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.07490", "abs": "https://arxiv.org/abs/2512.07490", "authors": ["Zhiyu Liu", "Zhi Han", "Yandong Tang", "Jun Fan", "Yao Wang"], "title": "Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent", "comment": null, "summary": "The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u66ff\u9884\u6761\u4ef6\u68af\u5ea6\u4e0b\u964d\uff08APGD\uff09\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4f4e\u7ba1\u79e9\u5f20\u91cf\u4f30\u8ba1\u95ee\u9898\uff0c\u8be5\u7b97\u6cd5\u5728\u8fc7\u53c2\u6570\u5316\u8bbe\u7f6e\u4e0b\u901a\u8fc7\u9884\u6761\u4ef6\u9879\u52a0\u901f\u6536\u655b\uff0c\u5e76\u5177\u6709\u7ebf\u6027\u6536\u655b\u4fdd\u8bc1\u3002", "motivation": "\u4f20\u7edf\u5f20\u91cf\u5947\u5f02\u503c\u5206\u89e3\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5f20\u91cf\uff1b\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u56e0\u5b50\u5206\u89e3\u65b9\u6cd5\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u5f20\u91cf\u79e9\uff0c\u5f53\u79e9\u88ab\u9ad8\u4f30\u65f6\u6536\u655b\u901f\u5ea6\u663e\u8457\u4e0b\u964d\u751a\u81f3\u53d1\u6563\u3002", "method": "APGD\u7b97\u6cd5\u901a\u8fc7\u5411\u539f\u59cb\u68af\u5ea6\u6dfb\u52a0\u9884\u6761\u4ef6\u9879\uff0c\u5e76\u4ea4\u66ff\u66f4\u65b0\u4e24\u4e2a\u8f83\u5c0f\u7684\u56e0\u5b50\u5f20\u91cf\uff0c\u4ece\u800c\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u52a0\u901f\u6536\u655b\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eAPGD\u5728\u8fc7\u53c2\u6570\u5316\u60c5\u51b5\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u7ebf\u6027\u6536\u655b\uff0c\u4e14\u6536\u655b\u901f\u7387\u4e0e\u5f20\u91cf\u6761\u4ef6\u6570\u65e0\u5173\uff1b\u5927\u91cf\u5408\u6210\u6570\u636e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u7ed3\u679c\u3002", "conclusion": "APGD\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u7ba1\u79e9\u5f20\u91cf\u4f30\u8ba1\u4e2d\u7684\u8fc7\u53c2\u6570\u5316\u95ee\u9898\uff0c\u5177\u6709\u7ebf\u6027\u6536\u655b\u6027\u548c\u5bf9\u6761\u4ef6\u6570\u7684\u72ec\u7acb\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5f20\u91cf\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07136", "abs": "https://arxiv.org/abs/2512.07136", "authors": ["Siyang Jiang", "Mu Yuan", "Xiang Ji", "Bufang Yang", "Zeyu Liu", "Lilin Xu", "Yang Li", "Yuting He", "Liran Dong", "Wenrui Lu", "Zhenyu Yan", "Xiaofan Jiang", "Wei Gao", "Hongkai Chen", "Guoliang Xing"], "title": "A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning", "comment": null, "summary": "Multimodal human action recognition (HAR) leverages complementary sensors for activity classification. Beyond recognition, recent advances in large language models (LLMs) enable detailed descriptions and causal reasoning, motivating new tasks: human action understanding (HAU) and human action reasoning (HARn). However, most LLMs, especially large vision language models (LVLMs), struggle with non-RGB modalities such as depth, IMU, and mmWave due to the lack of large-scale data-caption resources. Existing HAR datasets mainly provide coarse data-label annotations, which are insufficient to capture fine-grained action dynamics needed for HAU and HARn. We consider two ground-truth pair types: (1) data label (discrete category) and (2) data caption (textual description). Naively generating captions from labels often lacks logical and spatiotemporal consistency. We introduce CUHK-X, a large-scale multimodal dataset and benchmark suite for HAR, HAU, and HARn. CUHK-X contains 58,445 samples covering 40 actions performed by 30 participants across two indoor environments. To improve caption consistency, we propose a prompt-based scene creation method that leverages LLMs to generate logically connected activity sequences, followed by human validation. CUHK-X includes three benchmarks with six evaluation tasks. Experiments report average accuracies of 76.52% (HAR), 40.76% (HAU), and 70.25% (HARn). CUHK-X aims to enable the community to apply and develop data-intensive learning methods for robust, multimodal human activity analysis. Project page and code: https://openaiotlab.github.io/CUHK-X/ and https://github.com/openaiotlab/CUHK-X.", "AI": {"tldr": "CUHK-X\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u4eba\u7c7b\u52a8\u4f5c\u8bc6\u522b\u3001\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u6587\u672c\u63cf\u8ff0\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u975eRGB\u6a21\u6001\u6570\u636e\uff08\u5982\u6df1\u5ea6\u3001IMU\u3001\u6beb\u7c73\u6ce2\uff09\uff0c\u4e14\u73b0\u6709\u6570\u636e\u96c6\u4e3b\u8981\u63d0\u4f9b\u7c97\u7c92\u5ea6\u7684\u6570\u636e\u6807\u7b7e\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u7406\u89e3\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u63d0\u793a\u7684\u573a\u666f\u521b\u5efa\u65b9\u6cd5\uff0c\u5229\u7528LLM\u751f\u6210\u903b\u8f91\u8fde\u8d2f\u7684\u6d3b\u52a8\u5e8f\u5217\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u786e\u4fdd\u4e00\u81f4\u6027\u3002\u6570\u636e\u96c6\u5305\u542b58,445\u4e2a\u6837\u672c\uff0c\u8986\u76d640\u4e2a\u52a8\u4f5c\uff0c\u753130\u540d\u53c2\u4e0e\u8005\u5728\u4e24\u4e2a\u5ba4\u5185\u73af\u5883\u4e2d\u5b8c\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u516d\u4e2a\u8bc4\u4f30\u4efb\u52a1\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u5206\u522b\u4e3a\uff1a\u52a8\u4f5c\u8bc6\u522b76.52%\uff0c\u52a8\u4f5c\u7406\u89e340.76%\uff0c\u52a8\u4f5c\u63a8\u740670.25%\u3002", "conclusion": "CUHK-X\u6570\u636e\u96c6\u65e8\u5728\u63a8\u52a8\u793e\u533a\u5e94\u7528\u548c\u53d1\u5c55\u6570\u636e\u5bc6\u96c6\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u6a21\u6001\u4eba\u7c7b\u6d3b\u52a8\u5206\u6790\u3002"}}
{"id": "2512.07509", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07509", "abs": "https://arxiv.org/abs/2512.07509", "authors": ["Nikita Gabdullin"], "title": "Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces", "comment": "9 pages, 5 figures, 1 table, 4 equations", "summary": "The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u9884\u5b9a\u4e49\u5411\u91cf\u7cfb\u7edf\uff08\u5982An\u6839\u7cfb\u7edf\u5411\u91cf\uff09\u4f5c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u914d\u7f6e\u76ee\u6807\u7684\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u5206\u5e03\uff0c\u4ece\u800c\u52a0\u901f\u5927\u89c4\u6a21\u5206\u7c7b\u4efb\u52a1\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u7c7b\u522b\u6570\u636e\u96c6\uff08\u5982ImageNet-1K\u548c50k-600k\u7c7b\u522b\uff09\u4e2d\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u6765\u907f\u514d\u4f7f\u7528\u5206\u7c7b\u5c42\uff0c\u7b80\u5316\u7f51\u7edc\u67b6\u6784\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u5411\u91cf\u7cfb\u7edf\uff08\u5982An\u6839\u7cfb\u7edf\uff09\u4f5c\u4e3a\u6f5c\u5728\u7a7a\u95f4\u914d\u7f6e\u7684\u76ee\u6807\uff0c\u6784\u5efa\u9002\u5408\u7684\u5411\u91cf\u7cfb\u7edf\uff0c\u5e76\u5728\u7f16\u7801\u5668\u548c\u89c6\u89c9\u53d8\u6362\u5668\u4e2d\u5e94\u7528\u8fd9\u4e9b\u7cfb\u7edf\uff0c\u4ee5\u6700\u5c0f\u5316\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u52a0\u5feb\u4e86ImageNet-1K\u548c50k-600k\u7c7b\u522b\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u901f\u5ea6\uff0c\u5e76\u4e14\u4f7f\u7528\u6700\u5c0f\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u3002", "conclusion": "\u7ed3\u8bba\u662f\u9884\u5b9a\u4e49\u5411\u91cf\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u6709\u52a9\u4e8e\u51cf\u5c11\u5d4c\u5165\u5b58\u50a8\u7684\u5411\u91cf\u6570\u636e\u5e93\u5927\u5c0f\u3002"}}
{"id": "2512.07141", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07141", "abs": "https://arxiv.org/abs/2512.07141", "authors": ["Fenghua Weng", "Chaochao Lu", "Xia Hu", "Wenqi Shao", "Wenjie Wang"], "title": "Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models", "comment": null, "summary": "As multimodal reasoning improves the overall capabilities of Large Vision Language Models (LVLMs), recent studies have begun to explore safety-oriented reasoning, aiming to enhance safety awareness by analyzing potential safety risks during the reasoning process before generating the final response. Although such approaches improve safety awareness and interpretability, this single-pass think-then-answer paradigm remains vulnerable to contextual or visual jailbreak attacks. This reveals a critical flaw: single-pass reasoning may overlook explicit harmful content in its own output. Our key insight is to exploit this wasted signal through reflection, which can effectively leverage the malicious content revealed in the first-pass reasoning to enable genuine self-correction and prevent unsafe generations. Motivated by this, we propose Think-Reflect-Revise (TRR), a three-stage training framework designed to enhance the safety alignment of LVLMs through policy-guided self-reflection. We first build a Reflective Safety Reasoning (ReSafe) dataset with 5,000 examples that follow a think-reflect-revise process. We then fine-tune the target model using the ReSafe dataset to initialize reflective behavior, and finally reinforce policy-guided reflection through reinforcement learning. Experimental results show that TRR substantially improves the safety performance of LVLMs across both safety-awareness benchmarks and jailbreak attack evaluations, increasing the overall safe response rate from 42.8% to 87.7% on Qwen2.5-VL-7B, while preserving stable performance on general benchmarks such as MMMU and MMStar. The project page is available at https://think-reflect-revise.github.io/.", "AI": {"tldr": "\u63d0\u51faThink-Reflect-Revise (TRR)\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u5f15\u5bfc\u7684\u81ea\u6211\u53cd\u601d\u589e\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u5bf9\u9f50\uff0c\u5c06\u5b89\u5168\u54cd\u5e94\u7387\u4ece42.8%\u63d0\u5347\u81f387.7%\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u8f6e\u601d\u8003-\u56de\u7b54\u8303\u5f0f\u5bb9\u6613\u53d7\u5230\u4e0a\u4e0b\u6587\u6216\u89c6\u89c9\u8d8a\u72f1\u653b\u51fb\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u81ea\u8eab\u8f93\u51fa\u4e2d\u7684\u6709\u5bb3\u5185\u5bb9\uff0c\u9700\u8981\u5f15\u5165\u53cd\u601d\u673a\u5236\u5b9e\u73b0\u771f\u6b63\u7684\u81ea\u6211\u4fee\u6b63\u3002", "method": "1) \u6784\u5efa\u5305\u542b5000\u4e2a\u6837\u4f8b\u7684Reflective Safety Reasoning\u6570\u636e\u96c6\uff1b2) \u4f7f\u7528ReSafe\u6570\u636e\u96c6\u5fae\u8c03\u76ee\u6807\u6a21\u578b\u521d\u59cb\u5316\u53cd\u601d\u884c\u4e3a\uff1b3) \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f3a\u5316\u7b56\u7565\u5f15\u5bfc\u7684\u53cd\u601d\u3002", "result": "TRR\u663e\u8457\u63d0\u5347LVLMs\u5728\u5b89\u5168\u610f\u8bc6\u548c\u8d8a\u72f1\u653b\u51fb\u8bc4\u4f30\u4e2d\u7684\u5b89\u5168\u6027\u8868\u73b0\uff0cQwen2.5-VL-7B\u7684\u5b89\u5168\u54cd\u5e94\u7387\u4ece42.8%\u63d0\u9ad8\u523087.7%\uff0c\u540c\u65f6\u5728MMMU\u548cMMStar\u7b49\u901a\u7528\u57fa\u51c6\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u53cd\u601d\u673a\u5236\u5229\u7528\u9996\u6b21\u63a8\u7406\u4e2d\u66b4\u9732\u7684\u6076\u610f\u5185\u5bb9\uff0cTRR\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u7684\u81ea\u6211\u4fee\u6b63\uff0c\u663e\u8457\u589e\u5f3a\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2512.07519", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07519", "abs": "https://arxiv.org/abs/2512.07519", "authors": ["Alexander Gammerman"], "title": "Machine Learning: Progress and Prospects", "comment": "Inaugural Lecture. 18 pages, 13 figures, Published in 1997 by Royal Holloway, University of London, ISBN 0 900145 93 5", "summary": "This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.\n  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of \"simplicity\" known as \"Ockham's razor\" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that \"we learn some things only by doing things\".\n  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u7684\u8d77\u6e90\u548c\u53d1\u5c55\uff0c\u4ece1949\u5e74\u9999\u519c\u7684\u7b97\u6cd5\u5230\u4e9a\u91cc\u58eb\u591a\u5fb7\u7684\u5b66\u4e60\u7406\u8bba\uff0c\u6db5\u76d6\u4e86\u591a\u4e2a\u5386\u53f2\u65f6\u671f\u548c\u4e0d\u540c\u5b66\u79d1\u5bf9\u673a\u5668\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u65e8\u5728\u68b3\u7406\u673a\u5668\u5b66\u4e60\u7684\u8d77\u6e90\u548c\u53d1\u5c55\u5386\u7a0b\uff0c\u63a2\u8ba8\u5176\u591a\u5b66\u79d1\u80cc\u666f\u548c\u4e0d\u540c\u7814\u7a76\u65b9\u5411\u7684\u878d\u5408\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u56de\u987e\u548c\u7406\u8bba\u5206\u6790\uff0c\u8ffd\u6eaf\u673a\u5668\u5b66\u4e60\u601d\u60f3\u7684\u8d77\u6e90\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u5b66\u79d1\u5bf9\u673a\u5668\u5b66\u4e60\u53d1\u5c55\u7684\u8d21\u732e\u3002", "result": "\u660e\u786e\u4e86\u673a\u5668\u5b66\u4e60\u8d77\u6e90\u4e8e\u591a\u4e2a\u5386\u53f2\u65f6\u671f\u548c\u4e0d\u540c\u5b66\u79d1\uff0c\u5305\u62ec\u54f2\u5b66\u3001\u7edf\u8ba1\u5b66\u548c\u8ba1\u7b97\u673a\u79d1\u5b66\u7b49\uff0c\u5f62\u6210\u4e86\u591a\u4e2a\u5e76\u884c\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u662f\u4e00\u4e2a\u591a\u5b66\u79d1\u878d\u5408\u7684\u9886\u57df\uff0c\u5176\u601d\u60f3\u6839\u6e90\u53ef\u4ee5\u8ffd\u6eaf\u5230\u53e4\u4ee3\uff0c\u73b0\u4ee3\u53d1\u5c55\u5219\u53d7\u76ca\u4e8e\u591a\u4e2a\u5b66\u79d1\u7684\u4ea4\u53c9\u5f71\u54cd\u3002"}}
{"id": "2512.07155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07155", "abs": "https://arxiv.org/abs/2512.07155", "authors": ["Dahyeon Kye", "Jeahun Sung", "MinKyu Jeon", "Jihyong Oh"], "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics", "comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/", "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.", "AI": {"tldr": "CHIMERA\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u96f6\u6837\u672c\u56fe\u50cf\u53d8\u5f62\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7f13\u5b58\u6ce8\u5165\u548c\u8bed\u4e49\u951a\u70b9\u63d0\u793a\u5b9e\u73b0\u5e73\u6ed1\u81ea\u7136\u7684\u56fe\u50cf\u8fc7\u6e21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u5bf9\u9f50\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u53d8\u5f62\u4efb\u52a1\u4e2d\u5f80\u5f80\u4ea7\u751f\u7a81\u5140\u7684\u8fc7\u6e21\u6216\u8fc7\u9971\u548c\u7684\u5916\u89c2\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u81ea\u9002\u5e94\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\u3002", "method": "\u63d0\u51faCHIMERA\u6846\u67b6\uff1a1\uff09\u81ea\u9002\u5e94\u7f13\u5b58\u6ce8\u5165(ACI)\u5728DDIM\u53cd\u6f14\u8fc7\u7a0b\u4e2d\u7f13\u5b58\u8f93\u5165\u56fe\u50cf\u7684\u7279\u5f81\u5e76\u81ea\u9002\u5e94\u91cd\u6ce8\u5165\uff1b2\uff09\u8bed\u4e49\u951a\u70b9\u63d0\u793a(SAP)\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5171\u4eab\u8bed\u4e49\u951a\u70b9\uff1b3\uff09\u63d0\u51fa\u5168\u5c40-\u5c40\u90e8\u4e00\u81f4\u6027\u8bc4\u5206(GLCS)\u4f5c\u4e3a\u53d8\u5f62\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u548c\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cCHIMERA\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u5e73\u6ed1\u3001\u8bed\u4e49\u66f4\u4e00\u81f4\u7684\u56fe\u50cf\u8fc7\u6e21\uff0c\u5728\u56fe\u50cf\u53d8\u5f62\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "CHIMERA\u901a\u8fc7\u521b\u65b0\u7684\u7f13\u5b58\u6ce8\u5165\u548c\u8bed\u4e49\u951a\u70b9\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u53d8\u5f62\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u9ad8\u8d28\u91cf\u56fe\u50cf\u53d8\u5f62\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07528", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07528", "abs": "https://arxiv.org/abs/2512.07528", "authors": ["Nishanth Venkatesh", "Andreas A. Malikopoulos"], "title": "Model-Based Reinforcement Learning Under Confounding", "comment": "9 pages, 2 figures - decompressed draft", "summary": "We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e0a\u4e0b\u6587\u672a\u89c2\u5bdf\u5230\u7684\u6df7\u6dc6\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08C-MDPs\uff09\uff0c\u901a\u8fc7\u4ee3\u7406\u53d8\u91cf\u548c\u6700\u5927\u56e0\u679c\u71b5\u6846\u67b6\u5b9e\u73b0\u4e00\u81f4\u7684\u653f\u7b56\u8bc4\u4f30\u3002", "motivation": "\u5728\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\uff0c\u672a\u89c2\u5bdf\u5230\u7684\u4e0a\u4e0b\u6587\u4f1a\u5bfc\u81f4\u6df7\u6dc6\u95ee\u9898\uff0c\u4f20\u7edf\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u5728\u8fd9\u79cd\u8bbe\u7f6e\u4e0b\u5b58\u5728\u6839\u672c\u6027\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u56e0\u4e3a\u884c\u4e3a\u7b56\u7565\u751f\u6210\u7684\u8f6c\u79fb\u548c\u5956\u52b1\u673a\u5236\u4e0e\u8bc4\u4f30\u72b6\u6001\u7b56\u7565\u6240\u9700\u7684\u5e72\u9884\u91cf\u4e0d\u5bf9\u5e94\u3002", "method": "\u91c7\u7528\u8fd1\u7aef\u79bb\u7b56\u7565\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5229\u7528\u4ee3\u7406\u53d8\u91cf\u7684\u53ef\u9006\u6027\u6761\u4ef6\u8bc6\u522b\u6df7\u6dc6\u5956\u52b1\u671f\u671b\uff0c\u7ed3\u5408\u884c\u4e3a\u5e73\u5747\u8f6c\u79fb\u6a21\u578b\u6784\u5efa\u66ff\u4ee3MDP\uff0c\u5e76\u4e0e\u6700\u5927\u56e0\u679c\u71b5\u6a21\u578b\u5b66\u4e60\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u6240\u6784\u5efa\u7684\u66ff\u4ee3MDP\u5177\u6709\u660e\u786e\u5b9a\u4e49\u4e14\u4e00\u81f4\u7684\u72b6\u6001\u7b56\u7565\u8d1d\u5c14\u66fc\u7b97\u5b50\uff0c\u80fd\u591f\u5728\u4e0a\u4e0b\u6587\u4fe1\u606f\u672a\u89c2\u5bdf\u3001\u4e0d\u53ef\u7528\u6216\u6536\u96c6\u4e0d\u5207\u5b9e\u9645\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u6709\u539f\u5219\u7684\u6a21\u578b\u5b66\u4e60\u548c\u89c4\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b58\u5728\u6df7\u6dc6\u7684\u5f3a\u5316\u5b66\u4e60\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u7406\u8bba\u4e25\u8c28\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6269\u5c55\u4e86\u6a21\u578b\u5b66\u4e60\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.07165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07165", "abs": "https://arxiv.org/abs/2512.07165", "authors": ["Muyu Xu", "Fangneng Zhan", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "MuSASplat: Efficient Sparse-View 3D Gaussian Splats via Lightweight Multi-Scale Adaptation", "comment": null, "summary": "Sparse-view 3D Gaussian splatting seeks to render high-quality novel views of 3D scenes from a limited set of input images. While recent pose-free feed-forward methods leveraging pre-trained 3D priors have achieved impressive results, most of them rely on full fine-tuning of large Vision Transformer (ViT) backbones and incur substantial GPU costs. In this work, we introduce MuSASplat, a novel framework that dramatically reduces the computational burden of training pose-free feed-forward 3D Gaussian splats models with little compromise of rendering quality. Central to our approach is a lightweight Multi-Scale Adapter that enables efficient fine-tuning of ViT-based architectures with only a small fraction of training parameters. This design avoids the prohibitive GPU overhead associated with previous full-model adaptation techniques while maintaining high fidelity in novel view synthesis, even with very sparse input views. In addition, we introduce a Feature Fusion Aggregator that integrates features across input views effectively and efficiently. Unlike widely adopted memory banks, the Feature Fusion Aggregator ensures consistent geometric integration across input views and meanwhile mitigates the memory usage, training complexity, and computational costs significantly. Extensive experiments across diverse datasets show that MuSASplat achieves state-of-the-art rendering quality but has significantly reduced parameters and training resource requirements as compared with existing methods.", "AI": {"tldr": "MuSASplat\u662f\u4e00\u4e2a\u7528\u4e8e\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6e85\u5c04\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u9002\u914d\u5668\u548c\u7279\u5f81\u878d\u5408\u805a\u5408\u5668\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9884\u8bad\u7ec33D\u5148\u9a8c\u7684\u65e0\u59ff\u6001\u524d\u9988\u65b9\u6cd5\u867d\u7136\u6548\u679c\u51fa\u8272\uff0c\u4f46\u9700\u8981\u5b8c\u6574\u5fae\u8c03\u5927\u578bViT\u4e3b\u5e72\u7f51\u7edc\uff0c\u5bfc\u81f4GPU\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u964d\u4f4e\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u9002\u914d\u5668\uff0c\u4ec5\u9700\u5fae\u8c03\u5c11\u91cf\u53c2\u6570\u5373\u53ef\u9ad8\u6548\u9002\u914dViT\u67b6\u6784\uff1b\u8bbe\u8ba1\u7279\u5f81\u878d\u5408\u805a\u5408\u5668\uff0c\u6709\u6548\u6574\u5408\u591a\u89c6\u56fe\u7279\u5f81\u5e76\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMuSASplat\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548c\u8bad\u7ec3\u8d44\u6e90\u9700\u6c42\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "MuSASplat\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u9002\u914d\u5668\u548c\u805a\u5408\u5668\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u89c6\u56fe3D\u9ad8\u65af\u6e85\u5c04\u8bad\u7ec3\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07539", "abs": "https://arxiv.org/abs/2512.07539", "authors": ["Qingyuan Yang", "Shizhuo", "Dongyue Chen", "Da Teng", "Zehua Gan"], "title": "FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting", "comment": null, "summary": "Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.", "AI": {"tldr": "FRWKV\u662f\u4e00\u4e2a\u7ed3\u5408\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u9891\u57df\u5206\u6790\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfTransformer\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86O(T)\u7684\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u57288\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u5747\u6392\u540d\u3002", "motivation": "\u4f20\u7edfTransformer\u5728\u957f\u5e8f\u5217\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b58\u5728\u4e8c\u6b21\u590d\u6742\u5ea6O(T\u00b2)\u7684\u74f6\u9888\uff0c\u4e14\u96be\u4ee5\u6709\u6548\u5229\u7528\u9891\u57df\u4fe1\u606f\u3002\u53d7\u5230RWKV\u7ebf\u6027\u6ce8\u610f\u529b\u7684\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u53c8\u80fd\u5145\u5206\u5229\u7528\u9891\u57df\u4fe1\u606f\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faFRWKV\u6846\u67b6\uff0c\u5c06\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u9891\u57df\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u5728\u6ce8\u610f\u529b\u8def\u5f84\u4e0a\u5b9e\u73b0O(T)\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u5229\u7528\u9891\u8c31\u4fe1\u606f\u589e\u5f3a\u65f6\u5e8f\u7279\u5f81\u8868\u793a\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u3002", "result": "\u57288\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFRWKV\u53d6\u5f97\u4e86\u7b2c\u4e00\u540d\u7684\u5e73\u5747\u6392\u540d\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u9891\u57df\u7f16\u7801\u5668\u4e24\u4e2a\u7ec4\u4ef6\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c55\u793a\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u4e0e\u9891\u57df\u5206\u6790\u4e4b\u95f4\u7684\u5f3a\u5927\u534f\u540c\u4f5c\u7528\uff0c\u4e3a\u53ef\u6269\u5c55\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2512.07166", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07166", "abs": "https://arxiv.org/abs/2512.07166", "authors": ["Siyuan Xu", "Yibing Liu", "Peilin Chen", "Yung-Hui Li", "Shiqi Wang", "Sam Kwong"], "title": "When Privacy Meets Recovery: The Overlooked Half of Surrogate-Driven Privacy Preservation for MLLM Editing", "comment": "9 pages,7figures", "summary": "Privacy leakage in Multimodal Large Language Models (MLLMs) has long been an intractable problem. Existing studies, though effectively obscure private information in MLLMs, often overlook the evaluation of the authenticity and recovery quality of user privacy. To this end, this work uniquely focuses on the critical challenge of how to restore surrogate-driven protected data in diverse MLLM scenarios. We first bridge this research gap by contributing the SPPE (Surrogate Privacy Protected Editable) dataset, which includes a wide range of privacy categories and user instructions to simulate real MLLM applications. This dataset offers protected surrogates alongside their various MLLM-edited versions, thus enabling the direct assessment of privacy recovery quality. By formulating privacy recovery as a guided generation task conditioned on complementary multimodal signals, we further introduce a unified approach that reliably reconstructs private content while preserving the fidelity of MLLM-generated edits. The experiments on both SPPE and InstructPix2Pix further show that our approach generalizes well across diverse visual content and editing tasks, achieving a strong balance between privacy protection and MLLM usability.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u66ff\u4ee3\u9690\u79c1\u4fdd\u62a4\u6570\u636e\u7684\u9690\u79c1\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efaSPPE\u6570\u636e\u96c6\u548c\u7edf\u4e00\u6062\u590d\u6846\u67b6\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301MLLM\u7f16\u8f91\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709MLLM\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u867d\u7136\u80fd\u6709\u6548\u6a21\u7cca\u9690\u79c1\u4fe1\u606f\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7528\u6237\u9690\u79c1\u771f\u5b9e\u6027\u548c\u6062\u590d\u8d28\u91cf\u7684\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u8bc1MLLM\u7684\u53ef\u7528\u6027\u3002", "method": "\u6784\u5efaSPPE\u6570\u636e\u96c6\u5305\u542b\u591a\u79cd\u9690\u79c1\u7c7b\u522b\u548c\u7528\u6237\u6307\u4ee4\uff0c\u5c06\u9690\u79c1\u6062\u590d\u5efa\u6a21\u4e3a\u57fa\u4e8e\u4e92\u8865\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u5f15\u5bfc\u751f\u6210\u4efb\u52a1\uff0c\u63d0\u51fa\u7edf\u4e00\u6062\u590d\u65b9\u6cd5\u53ef\u9760\u91cd\u5efa\u9690\u79c1\u5185\u5bb9\u5e76\u4fdd\u6301MLLM\u7f16\u8f91\u4fdd\u771f\u5ea6\u3002", "result": "\u5728SPPE\u548cInstructPix2Pix\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u89c6\u89c9\u5185\u5bb9\u548c\u7f16\u8f91\u4efb\u52a1\uff0c\u5728\u9690\u79c1\u4fdd\u62a4\u548cMLLM\u53ef\u7528\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aMLLM\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u7684\u8bc4\u4f30\u89c6\u89d2\u548c\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u66ff\u4ee3\u9a71\u52a8\u7684\u9690\u79c1\u6062\u590d\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u53ef\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002"}}
{"id": "2512.07542", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07542", "abs": "https://arxiv.org/abs/2512.07542", "authors": ["Jad Mounayer", "Sebastian Rodriguez", "Jerome Tomezyk", "Chady Ghnatios", "Francisco Chinesta"], "title": "RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems", "comment": null, "summary": "Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.", "AI": {"tldr": "RRAEDy\u662f\u4e00\u79cd\u65b0\u578b\u7684\u6f5c\u7a7a\u95f4\u52a8\u6001\u7cfb\u7edf\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u53d1\u73b0\u5408\u9002\u7684\u6f5c\u7ef4\u5ea6\uff0c\u5728\u6f5c\u7a7a\u95f4\u4e2d\u5f3a\u5236\u6267\u884c\u6b63\u5219\u5316\u548c\u7ebf\u6027\u5316\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u9700\u8981\u9884\u5148\u56fa\u5b9a\u6f5c\u7ef4\u5ea6\u3001\u4f9d\u8d56\u590d\u6742\u635f\u5931\u5e73\u8861\u548c\u7f3a\u4e4f\u6f5c\u53d8\u91cf\u6b63\u5219\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6f5c\u7a7a\u95f4\u52a8\u6001\u7cfb\u7edf\u6a21\u578b\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a\u9700\u8981\u9884\u5148\u56fa\u5b9a\u6f5c\u7ef4\u5ea6\u3001\u4f9d\u8d56\u590d\u6742\u635f\u5931\u5e73\u8861\u6765\u8fd1\u4f3c\u7ebf\u6027\u52a8\u6001\u3001\u4ee5\u53ca\u7f3a\u4e4f\u5bf9\u6f5c\u53d8\u91cf\u7684\u6b63\u5219\u5316\u3002\u8fd9\u4e9b\u9650\u5236\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u57fa\u4e8e\u79e9\u7ea6\u51cf\u81ea\u7f16\u7801\u5668\uff08RRAEs\uff09\uff0cRRAEDy\u901a\u8fc7\u5947\u5f02\u503c\u81ea\u52a8\u6392\u5e8f\u548c\u526a\u679d\u6f5c\u53d8\u91cf\uff0c\u540c\u65f6\u5b66\u4e60\u63a7\u5236\u5176\u65f6\u95f4\u6f14\u5316\u7684\u6f5c\u52a8\u6001\u6a21\u6001\u5206\u89e3\uff08DMD\uff09\u7b97\u5b50\u3002\u8fd9\u79cd\u65e0\u7ed3\u6784\u4f46\u7ebf\u6027\u7ea6\u675f\u7684\u516c\u5f0f\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u7a33\u5b9a\u4e14\u4f4e\u7ef4\u7684\u52a8\u6001\uff0c\u65e0\u9700\u8f85\u52a9\u635f\u5931\u6216\u624b\u52a8\u8c03\u53c2\u3002", "result": "\u5728\u7ecf\u5178\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecVan der Pol\u632f\u8361\u5668\u3001Burgers\u65b9\u7a0b\u30012D Navier-Stokes\u548c\u65cb\u8f6c\u9ad8\u65af\u5206\u5e03\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRRAEDy\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u9884\u6d4b\u3002\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u5b66\u4e60\u7b97\u5b50\u7684\u7a33\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5904\u7406\u53c2\u6570ODE\u7684\u6269\u5c55\u5c55\u793a\u4e86\u6a21\u578b\u7684\u901a\u7528\u6027\u3002", "conclusion": "RRAEDy\u901a\u8fc7\u6d88\u9664\u73b0\u6709\u6a21\u578b\u7684\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u548c\u7a33\u5b9a\u7684\u6f5c\u7a7a\u95f4\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u4f18\u8d8a\u6027\u80fd\u3002\u6a21\u578b\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.07170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07170", "abs": "https://arxiv.org/abs/2512.07170", "authors": ["Jiayang Li", "Chengjie Jiang", "Junjun Jiang", "Pengwei Liang", "Jiayi Ma", "Liqiang Nie"], "title": "Towards Unified Semantic and Controllable Image Fusion: A Diffusion Transformer Approach", "comment": null, "summary": "Image fusion aims to blend complementary information from multiple sensing modalities, yet existing approaches remain limited in robustness, adaptability, and controllability. Most current fusion networks are tailored to specific tasks and lack the ability to flexibly incorporate user intent, especially in complex scenarios involving low-light degradation, color shifts, or exposure imbalance. Moreover, the absence of ground-truth fused images and the small scale of existing datasets make it difficult to train an end-to-end model that simultaneously understands high-level semantics and performs fine-grained multimodal alignment. We therefore present DiTFuse, instruction-driven Diffusion-Transformer (DiT) framework that performs end-to-end, semantics-aware fusion within a single model. By jointly encoding two images and natural-language instructions in a shared latent space, DiTFuse enables hierarchical and fine-grained control over fusion dynamics, overcoming the limitations of pre-fusion and post-fusion pipelines that struggle to inject high-level semantics. The training phase employs a multi-degradation masked-image modeling strategy, so the network jointly learns cross-modal alignment, modality-invariant restoration, and task-aware feature selection without relying on ground truth images. A curated, multi-granularity instruction dataset further equips the model with interactive fusion capabilities. DiTFuse unifies infrared-visible, multi-focus, and multi-exposure fusion-as well as text-controlled refinement and downstream tasks-within a single architecture. Experiments on public IVIF, MFF, and MEF benchmarks confirm superior quantitative and qualitative performance, sharper textures, and better semantic retention. The model also supports multi-level user control and zero-shot generalization to other multi-image fusion scenarios, including instruction-conditioned segmentation.", "AI": {"tldr": "DiTFuse\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563-Transformer\u7684\u6307\u4ee4\u9a71\u52a8\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u6846\u67b6\uff0c\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7075\u6d3b\u63a7\u5236\u878d\u5408\u8fc7\u7a0b\uff0c\u7edf\u4e00\u5904\u7406\u7ea2\u5916-\u53ef\u89c1\u5149\u3001\u591a\u805a\u7126\u3001\u591a\u66dd\u5149\u7b49\u591a\u79cd\u878d\u5408\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7f3a\u4e4f\u7528\u6237\u610f\u56fe\u7684\u7075\u6d3b\u878d\u5165\u80fd\u529b\uff0c\u4e14\u53d7\u9650\u4e8e\u7f3a\u4e4f\u771f\u5b9e\u878d\u5408\u56fe\u50cf\u6807\u7b7e\u548c\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8054\u5408\u7f16\u7801\u53cc\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u5171\u4eab\u6f5c\u7a7a\u95f4\u7b56\u7565\uff0c\u4f7f\u7528\u591a\u9000\u5316\u63a9\u7801\u56fe\u50cf\u5efa\u6a21\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u6a21\u6001\u4e0d\u53d8\u6062\u590d\u548c\u4efb\u52a1\u611f\u77e5\u7279\u5f81\u9009\u62e9\u3002", "result": "\u5728\u516c\u5f00\u7684IVIF\u3001MFF\u548cMEF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u6027\u80fd\uff0c\u5177\u6709\u66f4\u6e05\u6670\u7684\u7eb9\u7406\u548c\u66f4\u597d\u7684\u8bed\u4e49\u4fdd\u7559\u80fd\u529b\u3002", "conclusion": "DiTFuse\u5728\u5355\u4e00\u67b6\u6784\u4e2d\u7edf\u4e00\u4e86\u591a\u79cd\u56fe\u50cf\u878d\u5408\u4efb\u52a1\uff0c\u652f\u6301\u591a\u7ea7\u7528\u6237\u63a7\u5236\u548c\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4e3a\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07558", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07558", "abs": "https://arxiv.org/abs/2512.07558", "authors": ["Shimin Zhang", "Xianwei Chen", "Yufan Shen", "Ziyuan Ye", "Jibin Wu"], "title": "ReLaX: Reasoning with Latent Exploration for Large Reasoning Models", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faReLaX\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790LRMs\u7684\u6f5c\u5728\u52a8\u6001\u6765\u89e3\u51b3RLVR\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd", "motivation": "RLVR\u867d\u7136\u80fd\u589e\u5f3a\u5927\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5e38\u5bfc\u81f4\u71b5\u5d29\u6e83\uff0c\u9020\u6210\u7b56\u7565\u8fc7\u65e9\u6536\u655b\u548c\u6027\u80fd\u9971\u548c\u3002\u4f5c\u8005\u8ba4\u4e3atoken\u751f\u6210\u80cc\u540e\u7684\u6f5c\u5728\u52a8\u6001\u5305\u542b\u66f4\u4e30\u5bcc\u7684\u8ba1\u7b97\u7ed3\u6784\uff0c\u53ef\u7528\u4e8e\u5f15\u5bfc\u7b56\u7565\u4f18\u5316", "method": "\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u83b7\u5f97\u9690\u85cf\u72b6\u6001\u52a8\u6001\u7684\u7ebf\u6027\u8868\u793a\uff0c\u63d0\u51fa\u52a8\u6001\u8c31\u5206\u6563\u5ea6(DSD)\u6307\u6807\u91cf\u5316\u6f5c\u5728\u52a8\u6001\u5f02\u8d28\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faReLaX\u8303\u5f0f\u6765\u8c03\u63a7\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u63a2\u7d22\u4e0e\u5229\u7528", "result": "\u5728\u591a\u6a21\u6001\u548c\u7eaf\u6587\u672c\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cReLaX\u663e\u8457\u7f13\u89e3\u4e86\u8fc7\u65e9\u6536\u655b\u95ee\u9898\uff0c\u5e76\u6301\u7eed\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "ReLaX\u901a\u8fc7\u663e\u5f0f\u6574\u5408\u6f5c\u5728\u52a8\u6001\u6765\u8c03\u63a7\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u4e2d\u7684\u71b5\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u63d0\u5347\u5927\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2512.07171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07171", "abs": "https://arxiv.org/abs/2512.07171", "authors": ["Shravan Venkatraman", "Rakesh Raj Madavan", "Pavan Kumar S", "Muthu Subash Kavitha"], "title": "TIDE: Two-Stage Inverse Degradation Estimation with Guided Prior Disentanglement for Underwater Image Restoration", "comment": "21 pages, 11 figures, 5 tables", "summary": "Underwater image restoration is essential for marine applications ranging from ecological monitoring to archaeological surveys, but effectively addressing the complex and spatially varying nature of underwater degradations remains a challenge. Existing methods typically apply uniform restoration strategies across the entire image, struggling to handle multiple co-occurring degradations that vary spatially and with water conditions. We introduce TIDE, a $\\underline{t}$wo stage $\\underline{i}$nverse $\\underline{d}$egradation $\\underline{e}$stimation framework that explicitly models degradation characteristics and applies targeted restoration through specialized prior decomposition. Our approach disentangles the restoration process into multiple specialized hypotheses that are adaptively fused based on local degradation patterns, followed by a progressive refinement stage that corrects residual artifacts. Specifically, TIDE decomposes underwater degradations into four key factors, namely color distortion, haze, detail loss, and noise, and designs restoration experts specialized for each. By generating specialized restoration hypotheses, TIDE balances competing degradation factors and produces natural results even in highly degraded regions. Extensive experiments across both standard benchmarks and challenging turbid water conditions show that TIDE achieves competitive performance on reference based fidelity metrics while outperforming state of the art methods on non reference perceptual quality metrics, with strong improvements in color correction and contrast enhancement. Our code is available at: https://rakesh-123-cryp.github.io/TIDE.", "AI": {"tldr": "TIDE\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u9000\u5316\u7279\u5f81\u5e76\u5e94\u7528\u9488\u5bf9\u6027\u6062\u590d\u7b56\u7565\uff0c\u6709\u6548\u5904\u7406\u7a7a\u95f4\u53d8\u5316\u7684\u590d\u6742\u6c34\u4e0b\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6c34\u4e0b\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u901a\u5e38\u5bf9\u6574\u4e2a\u56fe\u50cf\u5e94\u7528\u7edf\u4e00\u7684\u6062\u590d\u7b56\u7565\uff0c\u96be\u4ee5\u5904\u7406\u7a7a\u95f4\u53d8\u5316\u4e14\u5171\u5b58\u7684\u591a\u79cd\u9000\u5316\u95ee\u9898\u3002\u6c34\u4e0b\u9000\u5316\u968f\u7a7a\u95f4\u4f4d\u7f6e\u548c\u6c34\u8d28\u6761\u4ef6\u53d8\u5316\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u6062\u590d\u65b9\u6cd5\u3002", "method": "TIDE\u5c06\u6062\u590d\u8fc7\u7a0b\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u5c06\u6c34\u4e0b\u9000\u5316\u5206\u89e3\u4e3a\u56db\u4e2a\u5173\u952e\u56e0\u7d20\uff08\u989c\u8272\u5931\u771f\u3001\u96fe\u973e\u3001\u7ec6\u8282\u4e22\u5931\u548c\u566a\u58f0\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e13\u95e8\u7684\u6062\u590d\u4e13\u5bb6\uff1b\u7136\u540e\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u548c\u6e10\u8fdb\u7ec6\u5316\u9636\u6bb5\u6765\u5e73\u8861\u7ade\u4e89\u6027\u9000\u5316\u56e0\u7d20\u5e76\u4fee\u6b63\u6b8b\u4f59\u4f2a\u5f71\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u548c\u6311\u6218\u6027\u6d51\u6d4a\u6c34\u6761\u4ef6\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTIDE\u5728\u57fa\u4e8e\u53c2\u8003\u7684\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u5728\u975e\u53c2\u8003\u611f\u77e5\u8d28\u91cf\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u989c\u8272\u6821\u6b63\u548c\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "TIDE\u6846\u67b6\u901a\u8fc7\u4e13\u95e8\u7684\u5148\u9a8c\u5206\u89e3\u548c\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u591a\u53d8\u7684\u6c34\u4e0b\u56fe\u50cf\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u6d77\u6d0b\u5e94\u7528\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u6062\u590d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07569", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07569", "abs": "https://arxiv.org/abs/2512.07569", "authors": ["Joel Ekstrand", "Tor Mattsson", "Zahra Taghiyarrenani", "Slawomir Nowaczyk", "Jens Lundstr\u00f6m", "Mikael Lind\u00e9n"], "title": "Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting", "comment": null, "summary": "Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.", "AI": {"tldr": "WECA\u65b9\u6cd5\u901a\u8fc7\u52a0\u6743\u5bf9\u6bd4\u9002\u5e94\u76ee\u6807\uff0c\u5728ATM\u73b0\u91d1\u7269\u6d41\u7b49\u5e94\u7528\u4e2d\u63d0\u9ad8\u5f02\u5e38\u6761\u4ef6\u4e0b\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u5728\u5f02\u5e38\u6570\u636e\u4e0aSMAPE\u6307\u6807\u63d0\u53476.1\u4e2a\u767e\u5206\u70b9\uff0c\u6b63\u5e38\u6570\u636e\u6027\u80fd\u65e0\u663e\u8457\u4e0b\u964d", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u9884\u6d4b\u6a21\u578b\u5728\u6b63\u5e38\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5206\u5e03\u504f\u79fb\uff08\u5f02\u5e38\u6761\u4ef6\uff09\u4e0b\u5bb9\u6613\u5931\u6548\uff0c\u800c\u53ef\u9760\u7684\u5f02\u5e38\u6761\u4ef6\u4e0b\u9884\u6d4b\u5bf9ATM\u73b0\u91d1\u7269\u6d41\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51fa\u52a0\u6743\u5bf9\u6bd4\u9002\u5e94\uff08WECA\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u52a0\u6743\u5bf9\u6bd4\u76ee\u6807\u5bf9\u9f50\u6b63\u5e38\u548c\u5f02\u5e38\u589e\u5f3a\u8868\u793a\uff0c\u4fdd\u7559\u5f02\u5e38\u76f8\u5173\u4fe1\u606f\u7684\u540c\u65f6\u4fdd\u6301\u826f\u6027\u53d8\u5316\u4e0b\u7684\u4e00\u81f4\u6027", "result": "\u5728\u5168\u56fdATM\u4ea4\u6613\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cWECA\u76f8\u6bd4\u6b63\u5e38\u8bad\u7ec3\u57fa\u7ebf\u5728\u5f02\u5e38\u5f71\u54cd\u6570\u636e\u4e0aSMAPE\u63d0\u53476.1\u4e2a\u767e\u5206\u70b9\uff0c\u6b63\u5e38\u6570\u636e\u6027\u80fd\u51e0\u4e4e\u65e0\u9000\u5316", "conclusion": "WECA\u80fd\u591f\u5728\u4fdd\u6301\u6b63\u5e38\u64cd\u4f5c\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u589e\u5f3a\u5f02\u5e38\u6761\u4ef6\u4e0b\u7684\u9884\u6d4b\u53ef\u9760\u6027"}}
{"id": "2512.07186", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07186", "abs": "https://arxiv.org/abs/2512.07186", "authors": ["Zhuoming Liu", "Xiaofeng Gao", "Feiyang Niu", "Qiaozi Gao", "Liu Liu", "Robinson Piramuthu"], "title": "START: Spatial and Textual Learning for Chart Understanding", "comment": "WACV2026 Camera Ready", "summary": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available.", "AI": {"tldr": "START\u662f\u4e00\u4e2a\u7528\u4e8e\u56fe\u8868\u7406\u89e3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u548c\u6587\u672c\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u56fe\u8868\u89c6\u89c9\u5e03\u5c40\u548c\u5e95\u5c42\u6570\u636e\u7ed3\u6784\u7684\u7cbe\u786e\u7406\u89e3\u3002", "motivation": "\u56fe\u8868\u7406\u89e3\u5728\u79d1\u5b66\u8bba\u6587\u548c\u6280\u672f\u62a5\u544a\u5206\u6790\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u56fe\u8868\u7ed3\u5408\u4e86\u7ed3\u6784\u5316\u89c6\u89c9\u5e03\u5c40\uff08\u7a7a\u95f4\u5c5e\u6027\uff09\u548c\u5e95\u5c42\u6570\u636e\u8868\u793a\uff08\u6587\u672c\u5c5e\u6027\uff09\uff0c\u9700\u8981\u540c\u65f6\u7406\u89e3\u8fd9\u4e24\u4e2a\u65b9\u9762\u624d\u80fd\u5b9e\u73b0\u7cbe\u786e\u7684\u56fe\u8868\u63a8\u7406\u3002", "method": "\u63d0\u51faSTART\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a(1) \u56fe\u8868\u5143\u7d20\u5b9a\u4f4d - \u589e\u5f3a\u5bf9\u56fe\u8868\u89c6\u89c9\u5e03\u5c40\u7684\u7406\u89e3\uff1b(2) \u56fe\u8868\u5230\u4ee3\u7801\u751f\u6210 - \u5f3a\u5316\u5bf9\u6570\u636e\u7ec6\u8282\u7684\u638c\u63e1\u3002\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\u521b\u5efaSTART\u6570\u636e\u96c6\uff0c\u5229\u7528MLLM\u5c06\u771f\u5b9e\u56fe\u8868\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u518d\u4f7f\u7528LLM\u6f14\u5316\u4ee3\u7801\u4ee5\u786e\u5b9a\u56fe\u8868\u5143\u7d20\u4f4d\u7f6e\u3002", "result": "START\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\uff0c\u660e\u663e\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "START\u901a\u8fc7\u7a7a\u95f4\u548c\u6587\u672c\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u8868\u7406\u89e3\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u56fe\u8868\u7a7a\u95f4\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6CS-Bench\uff0c\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002"}}
{"id": "2512.07624", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07624", "abs": "https://arxiv.org/abs/2512.07624", "authors": ["Yongbo Yu", "Jari Peeperkorn", "Johannes De Smedt", "Jochen De Weerdt"], "title": "Time Series Foundation Models for Process Model Forecasting", "comment": null, "summary": "Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\u5728\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\uff08PMF\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5fae\u8c03\u5e26\u6765\u7684\u6539\u8fdb\u6709\u9650\uff0c\u5c55\u793a\u4e86TSFMs\u5728\u8fc7\u7a0b\u76f8\u5173\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\u4e2d\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u76f4\u63a5\u8ddf\u968f\u5173\u7cfb\u65f6\u95f4\u5e8f\u5217\u7684\u7a00\u758f\u6027\u548c\u5f02\u8d28\u6027\u3002\u7814\u7a76\u63a2\u7d22\u9884\u8bad\u7ec3\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u5229\u7528\u5176\u4ece\u975e\u8fc7\u7a0b\u9886\u57df\u5b66\u4e60\u7684\u65f6\u95f4\u7ed3\u6784\u77e5\u8bc6\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u4e8b\u4ef6\u65e5\u5fd7\u751f\u6210\u7684\u76f4\u63a5\u8ddf\u968f\u5173\u7cfb\u65f6\u95f4\u5e8f\u5217\uff0c\u6bd4\u8f83\u96f6\u6837\u672c\u4f7f\u7528\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e0e\u5728PMF\u6570\u636e\u4e0a\u5fae\u8c03\u7684\u53d8\u4f53\uff0c\u5e76\u4e0e\u4f20\u7edf\u548c\u4e13\u95e8\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u901a\u5e38\u6bd4\u4f20\u7edf\u65b9\u6cd5\u83b7\u5f97\u66f4\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\uff08MAE\u548cRMSE\uff09\uff0c\u8868\u660e\u4ece\u975e\u8fc7\u7a0b\u9886\u57df\u6709\u6548\u8f6c\u79fb\u4e86\u65f6\u95f4\u7ed3\u6784\u77e5\u8bc6\u3002\u5fae\u8c03\u867d\u7136\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u6539\u8fdb\u6709\u9650\u4e14\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u53ef\u80fd\u6d88\u5931\u3002", "conclusion": "\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u5728\u8fc7\u7a0b\u6a21\u578b\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387\uff0c\u96f6\u6837\u672c\u4f7f\u7528\u662f\u5f3a\u5927\u7684\u9ed8\u8ba4\u9009\u62e9\u3002\u8fd9\u662f\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u65f6\u95f4\u57fa\u7840\u6a21\u578b\u5728PMF\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2512.07190", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07190", "abs": "https://arxiv.org/abs/2512.07190", "authors": ["Pengfei Gu", "Huimin Li", "Haoteng Tang", "Dongkuan", "Xu", "Erik Enriquez", "DongChul Kim", "Bin Fu", "Danny Z. Chen"], "title": "Integrating Multi-scale and Multi-filtration Topological Features for Medical Image Classification", "comment": null, "summary": "Modern deep neural networks have shown remarkable performance in medical image classification. However, such networks either emphasize pixel-intensity features instead of fundamental anatomical structures (e.g., those encoded by topological invariants), or they capture only simple topological features via single-parameter persistence. In this paper, we propose a new topology-guided classification framework that extracts multi-scale and multi-filtration persistent topological features and integrates them into vision classification backbones. For an input image, we first compute cubical persistence diagrams (PDs) across multiple image resolutions/scales. We then develop a ``vineyard'' algorithm that consolidates these PDs into a single, stable diagram capturing signatures at varying granularities, from global anatomy to subtle local irregularities that may indicate early-stage disease. To further exploit richer topological representations produced by multiple filtrations, we design a cross-attention-based neural network that directly processes the consolidated final PDs. The resulting topological embeddings are fused with feature maps from CNNs or Transformers. By integrating multi-scale and multi-filtration topologies into an end-to-end architecture, our approach enhances the model's capacity to recognize complex anatomical structures. Evaluations on three public datasets show consistent, considerable improvements over strong baselines and state-of-the-art methods, demonstrating the value of our comprehensive topological perspective for robust and interpretable medical image classification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u5f15\u5bfc\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u3001\u591a\u8fc7\u6ee4\u7684\u6301\u4e45\u62d3\u6251\u7279\u5f81\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8981\u4e48\u8fc7\u4e8e\u5173\u6ce8\u50cf\u7d20\u5f3a\u5ea6\u7279\u5f81\u800c\u5ffd\u89c6\u57fa\u672c\u89e3\u5256\u7ed3\u6784\uff0c\u8981\u4e48\u53ea\u80fd\u901a\u8fc7\u5355\u53c2\u6570\u6301\u4e45\u6027\u6355\u83b7\u7b80\u5355\u7684\u62d3\u6251\u7279\u5f81\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u533b\u5b66\u56fe\u50cf\u4e2d\u590d\u6742\u7684\u89e3\u5256\u7ed3\u6784\u4fe1\u606f\u3002", "method": "1. \u8ba1\u7b97\u591a\u5206\u8fa8\u7387/\u591a\u5c3a\u5ea6\u7684\u7acb\u65b9\u6301\u4e45\u56fe\uff1b2. \u5f00\u53d1\"\u8461\u8404\u56ed\"\u7b97\u6cd5\u5c06\u591a\u4e2a\u6301\u4e45\u56fe\u6574\u5408\u4e3a\u5355\u4e00\u7a33\u5b9a\u56fe\uff1b3. \u8bbe\u8ba1\u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u5904\u7406\u6574\u5408\u540e\u7684\u6301\u4e45\u56fe\uff1b4. \u5c06\u62d3\u6251\u5d4c\u5165\u4e0eCNN\u6216Transformer\u7279\u5f81\u56fe\u878d\u5408\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u5c06\u591a\u5c3a\u5ea6\u548c\u591a\u8fc7\u6ee4\u62d3\u6251\u6574\u5408\u5230\u7aef\u5230\u7aef\u67b6\u6784\u4e2d\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u6a21\u578b\u8bc6\u522b\u590d\u6742\u89e3\u5256\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u4e3a\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u62d3\u6251\u89c6\u89d2\u3002"}}
{"id": "2512.07647", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07647", "abs": "https://arxiv.org/abs/2512.07647", "authors": ["Georgios Tzachristas", "Lei Deng", "Ioannis Tzachristas", "Gong Zhang", "Renhai Chen"], "title": "A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance", "comment": null, "summary": "We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\\mathrm{TV}(P,\\hat P)=1-e^{-\\mathrm{KL}(\\hat P\\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\\mathrm{TV}(P,\\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2=\u03c4\\|\u03bc_{\\mathrm{tail}}-\u03bc_{\\mathrm{head}}\\|_2$ with $\u03c4=\\mathrm{TV}(P,\\hat P)$, yielding a new head-tail diameter bound $\\|\\mathrm{Attn}(q,K,V)-\\mathrm{Attn}_k(q,K,V)\\|_2\\le\u03c4\\,\\mathrm{diam}_{H,T}$ and refinements linking the error to $\\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\\sim\\mathcal N(\u03bc,\u03c3^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\\varepsilon$ ensuring $\\mathrm{TV}(P,\\hat P)\\le\\varepsilon$, namely $k_\\varepsilon/n\\approx\u03a6_c(\u03c3+\u03a6^{-1}(\\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\\times$ on average while meeting the prescribed total-variation budget.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u8ba4\u8bc1Top-k\u6ce8\u610f\u529b\u622a\u65ad\uff0c\u91cf\u5316\u5206\u5e03\u548c\u8f93\u51fa\u5c42\u9762\u7684\u8fd1\u4f3c\u8bef\u5dee\u3002\u901a\u8fc7\u603b\u53d8\u5dee\u8ddd\u79bb\u548cKL\u6563\u5ea6\u7684\u5173\u7cfb\uff0c\u63a8\u5bfc\u51fa\u975e\u6e10\u8fd1\u786e\u5b9a\u6027\u8fb9\u754c\uff0c\u5e76\u8bc1\u660e\u8f93\u51fa\u8bef\u5dee\u53ef\u4ee5\u5206\u89e3\u4e3a\u5934\u5c3e\u8ddd\u79bb\u7684\u4e58\u79ef\u3002\u5728i.i.d.\u9ad8\u65af\u8bc4\u5206\u6a21\u578b\u4e0b\uff0c\u7ed9\u51fa\u4e86\u5c01\u95ed\u5f62\u5f0f\u7684\u5c3e\u8d28\u91cf\u548c\u6700\u5c0fk\u7684\u9009\u62e9\u89c4\u5219\u3002", "motivation": "\u73b0\u6709\u7684\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0cTop-k\u622a\u65ad\u867d\u7136\u80fd\u52a0\u901f\u8ba1\u7b97\u4f46\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u3002\u672c\u6587\u65e8\u5728\u4e3aTop-k\u6ce8\u610f\u529b\u622a\u65ad\u63d0\u4f9b\u4e25\u683c\u7684\u6570\u5b66\u8ba4\u8bc1\u6846\u67b6\uff0c\u91cf\u5316\u622a\u65ad\u5e26\u6765\u7684\u8bef\u5dee\u3002", "method": "1) \u5efa\u7acb\u603b\u53d8\u5dee\u8ddd\u79bb\u4e0e\u4e22\u5f03\u7684softmax\u5c3e\u8d28\u91cf\u7684\u5173\u7cfb\uff1b2) \u63a8\u5bfc\u57fa\u4e8e\u6709\u5e8flogits\u7684TV\u8fb9\u754c\uff1b3) \u63d0\u51fa\u5934\u5c3e\u5206\u89e3\u65b9\u6cd5\u5206\u6790\u8f93\u51fa\u8bef\u5dee\uff1b4) \u5728\u9ad8\u65af\u8bc4\u5206\u6a21\u578b\u4e0b\u63a8\u5bfc\u5c01\u95ed\u89e3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728bert-base-uncased\u548c\u5408\u6210logits\u4e0a\uff0c\u8ba4\u8bc1Top-k\u53ef\u4ee5\u5c06\u8bc4\u5206\u952e\u51cf\u5c112-4\u500d\uff0c\u540c\u65f6\u6ee1\u8db3\u9884\u8bbe\u7684\u603b\u53d8\u5dee\u9884\u7b97\u3002\u9884\u6d4b\u7684k_\u03b5/n\u7f29\u653e\u89c4\u5f8b\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u6587\u4e3aTop-k\u6ce8\u610f\u529b\u622a\u65ad\u63d0\u4f9b\u4e86\u7406\u8bba\u8ba4\u8bc1\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u5728\u63a7\u5236\u8bef\u5dee\u7684\u524d\u63d0\u4e0b\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u4e3a\u9ad8\u6548Transformer\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u3002"}}
{"id": "2512.07191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07191", "abs": "https://arxiv.org/abs/2512.07191", "authors": ["Wenqi Zhao", "Jiacheng Sang", "Fenghua Cheng", "Yonglu Shu", "Dong Li", "Xiaofeng Yang"], "title": "RefLSM: Linearized Structural-Prior Reflectance Model for Medical Image Segmentation and Bias-Field Correction", "comment": null, "summary": "Medical image segmentation remains challenging due to intensity inhomogeneity, noise, blurred boundaries, and irregular structures. Traditional level set methods, while effective in certain cases, often depend on approximate bias field estimations and therefore struggle under severe non-uniform imaging conditions. To address these limitations, we propose a novel variational Reflectance-based Level Set Model (RefLSM), which explicitly integrates Retinex-inspired reflectance decomposition into the segmentation framework. By decomposing the observed image into reflectance and bias field components, RefLSM directly segments the reflectance, which is invariant to illumination and preserves fine structural details. Building on this foundation, we introduce two key innovations for enhanced precision and robustness. First, a linear structural prior steers the smoothed reflectance gradients toward a data-driven reference, providing reliable geometric guidance in noisy or low-contrast scenes. Second, a relaxed binary level-set is embedded in RefLSM and enforced via convex relaxation and sign projection, yielding stable evolution and avoiding reinitialization-induced diffusion. The resulting variational problem is solved efficiently using an ADMM-based optimization scheme. Extensive experiments on multiple medical imaging datasets demonstrate that RefLSM achieves superior segmentation accuracy, robustness, and computational efficiency compared to state-of-the-art level set methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRetinex\u53cd\u5c04\u7387\u5206\u89e3\u7684\u53d8\u5206\u6c34\u5e73\u96c6\u6a21\u578bRefLSM\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u53cd\u5c04\u7387\u548c\u504f\u7f6e\u573a\u5206\u91cf\uff0c\u76f4\u63a5\u5bf9\u5149\u7167\u4e0d\u53d8\u7684\u53cd\u5c04\u7387\u8fdb\u884c\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5f3a\u5ea6\u4e0d\u5747\u5300\u6027\u3001\u566a\u58f0\u548c\u8fb9\u754c\u6a21\u7cca\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6c34\u5e73\u96c6\u65b9\u6cd5\u5728\u4e25\u91cd\u975e\u5747\u5300\u6210\u50cf\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u8fd1\u4f3c\u7684\u504f\u7f6e\u573a\u4f30\u8ba1\u3002\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9762\u4e34\u5f3a\u5ea6\u4e0d\u5747\u5300\u6027\u3001\u566a\u58f0\u3001\u6a21\u7cca\u8fb9\u754c\u548c\u4e0d\u89c4\u5219\u7ed3\u6784\u7684\u6311\u6218\u3002", "method": "RefLSM\u6a21\u578b\u5c06\u89c2\u5bdf\u56fe\u50cf\u5206\u89e3\u4e3a\u53cd\u5c04\u7387\u548c\u504f\u7f6e\u573a\u5206\u91cf\uff0c\u5f15\u5165\u7ebf\u6027\u7ed3\u6784\u5148\u9a8c\u6307\u5bfc\u5e73\u6ed1\u53cd\u5c04\u7387\u68af\u5ea6\uff0c\u5e76\u91c7\u7528\u677e\u5f1b\u4e8c\u5143\u6c34\u5e73\u96c6\u901a\u8fc7\u51f8\u677e\u5f1b\u548c\u7b26\u53f7\u6295\u5f71\u5b9e\u73b0\u7a33\u5b9a\u6f14\u5316\u3002\u4f7f\u7528ADMM\u4f18\u5316\u65b9\u6848\u9ad8\u6548\u6c42\u89e3\u53d8\u5206\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRefLSM\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u96c6\u65b9\u6cd5\uff0c\u5728\u5206\u5272\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "RefLSM\u901a\u8fc7\u53cd\u5c04\u7387\u5206\u89e3\u548c\u7ed3\u6784\u5148\u9a8c\u7684\u6709\u6548\u7ed3\u5408\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u3001\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u6311\u6218\u6027\u7684\u6210\u50cf\u6761\u4ef6\u3002"}}
{"id": "2512.07667", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07667", "abs": "https://arxiv.org/abs/2512.07667", "authors": ["Gracjan G\u00f3ral", "Marysia Winkels", "Steven Basart"], "title": "Depth-Wise Activation Steering for Honest Language Models", "comment": "See \\url{https://github.com/marysia/gaussian-activation-steering}. for code and experiments", "summary": "Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u65af\u8c03\u5ea6\u5728\u795e\u7ecf\u7f51\u7edc\u6df1\u5ea6\u4e0a\u5206\u914d\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u4ee5\u6539\u5584\u8bed\u8a00\u6a21\u578b\u7684\u8bda\u5b9e\u6027\u800c\u975e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u4e8b\u5b9e\u6b63\u786e\u6027\u6216\u4f9d\u8d56\u91cd\u65b0\u8bad\u7ec3\u548c\u8106\u5f31\u7684\u5355\u5c42\u7f16\u8f91\uff0c\u5bf9\u771f\u5b9e\u62a5\u544a\u7684\u63a7\u5236\u6709\u9650\u3002\u8bed\u8a00\u6a21\u578b\u6709\u65f6\u4f1a\u5728\u5185\u90e8\u8868\u793a\u6b63\u786e\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\u4ecd\u65ad\u8a00\u9519\u8bef\u4fe1\u606f\uff0c\u8fd9\u662f\u8bda\u5b9e\u6027\u800c\u975e\u51c6\u786e\u6027\u7684\u5931\u8d25\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8c03\u5ea6\u5728\u795e\u7ecf\u7f51\u7edc\u6df1\u5ea6\u4e0a\u52a0\u6743\u5f15\u5bfc\u5f3a\u5ea6\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u4e14\u4e0d\u9700\u8981\u5fae\u8c03\u3002", "result": "\u5728MASK\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u9ad8\u65af\u8c03\u5ea6\u5728\u4e03\u4e2a\u6a21\u578b\u4e2d\u7684\u516d\u4e2a\u6a21\u578b\u4e0a\u6bd4\u65e0\u5f15\u5bfc\u548c\u5355\u5c42\u57fa\u7ebf\u63d0\u9ad8\u4e86\u8bda\u5b9e\u6027\u3002\u5728LLaMA-3.1-8B-Instruct\u548cQwen-2.5-7B-Instruct\u4e0a\u7684\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u9ad8\u65af\u8c03\u5ea6\u4f18\u4e8e\u968f\u673a\u3001\u5747\u5300\u548c\u76d2\u5f0f\u6ee4\u6ce2\u6df1\u5ea6\u5206\u914d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u4e3a\u4ece\u6a21\u578b\u73b0\u6709\u80fd\u529b\u4e2d\u5f15\u51fa\u771f\u5b9e\u62a5\u544a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u63a7\u5236\u624b\u6bb5\uff0c\u4e14\u5e72\u9884\u5728\u6df1\u5ea6\u4e0a\u7684\u5206\u5e03\u65b9\u5f0f\u5bf9\u7ed3\u679c\u6709\u5b9e\u8d28\u6027\u5f71\u54cd\u3002"}}
{"id": "2512.07192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07192", "abs": "https://arxiv.org/abs/2512.07192", "authors": ["Niu Yi", "Xu Tianyi", "Ma Mingming", "Wang Xinkun"], "title": "HVQ-CGIC: Enabling Hyperprior Entropy Modeling for VQ-Based Controllable Generative Image Compression", "comment": "12 pages, 7 figures", "summary": "Generative learned image compression methods using Vector Quantization (VQ) have recently shown impressive potential in balancing distortion and perceptual quality. However, these methods typically estimate the entropy of VQ indices using a static, global probability distribution, which fails to adapt to the specific content of each image. This non-adaptive approach leads to untapped bitrate potential and challenges in achieving flexible rate control. To address this challenge, we introduce a Controllable Generative Image Compression framework based on a VQ Hyperprior, termed HVQ-CGIC. HVQ-CGIC rigorously derives the mathematical foundation for introducing a hyperprior to the VQ indices entropy model. Based on this foundation, through novel loss design, to our knowledge, this framework is the first to introduce RD balance and control into vector quantization-based Generative Image Compression. Cooperating with a lightweight hyper-prior estimation network, HVQ-CGIC achieves a significant advantage in rate-distortion (RD) performance compared to current state-of-the-art (SOTA) generative compression methods. On the Kodak dataset, we achieve the same LPIPS as Control-GIC, CDC and HiFiC with an average of 61.3% fewer bits. We posit that HVQ-CGIC has the potential to become a foundational component for VQGAN-based image compression, analogous to the integral role of the HyperPrior framework in neural image compression.", "AI": {"tldr": "HVQ-CGIC\u662f\u4e00\u4e2a\u57fa\u4e8eVQ\u8d85\u5148\u9a8c\u7684\u53ef\u63a7\u751f\u6210\u56fe\u50cf\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u8d85\u5148\u9a8c\u6a21\u578b\u6765\u52a8\u6001\u4f30\u8ba1VQ\u7d22\u5f15\u7684\u71b5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7387\u5931\u771f\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u76f8\u540cLPIPS\u6307\u6807\u4e0b\u53ef\u8282\u770161.3%\u7684\u6bd4\u7279\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eVQ\u7684\u751f\u6210\u56fe\u50cf\u538b\u7f29\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u5168\u5c40\u6982\u7387\u5206\u5e03\u4f30\u8ba1VQ\u7d22\u5f15\u7684\u71b5\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u56fe\u50cf\u5185\u5bb9\uff0c\u5bfc\u81f4\u6bd4\u7279\u7387\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\u4e14\u96be\u4ee5\u5b9e\u73b0\u7075\u6d3b\u7684\u7801\u7387\u63a7\u5236\u3002", "method": "\u63d0\u51faVQ\u8d85\u5148\u9a8c\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u5f15\u5165\u8d85\u5148\u9a8c\u5230VQ\u7d22\u5f15\u71b5\u6a21\u578b\uff0c\u8bbe\u8ba1\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\uff0c\u914d\u5408\u8f7b\u91cf\u7ea7\u8d85\u5148\u9a8c\u4f30\u8ba1\u7f51\u7edc\u5b9e\u73b0\u7387\u5931\u771f\u5e73\u8861\u4e0e\u63a7\u5236\u3002", "result": "\u5728Kodak\u6570\u636e\u96c6\u4e0a\uff0c\u4e0eControl-GIC\u3001CDC\u548cHiFiC\u76f8\u6bd4\uff0c\u5728\u76f8\u540cLPIPS\u8d28\u91cf\u4e0b\u5e73\u5747\u8282\u770161.3%\u7684\u6bd4\u7279\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u751f\u6210\u538b\u7f29\u65b9\u6cd5\u3002", "conclusion": "HVQ-CGIC\u6709\u671b\u6210\u4e3aVQGAN-based\u56fe\u50cf\u538b\u7f29\u7684\u57fa\u7840\u7ec4\u4ef6\uff0c\u7c7b\u4f3c\u4e8eHyperPrior\u6846\u67b6\u5728\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\u3002"}}
{"id": "2512.07676", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07676", "abs": "https://arxiv.org/abs/2512.07676", "authors": ["Hongjian Lan", "Yucong Liu", "Florian Sch\u00e4fer"], "title": "A Bootstrap Perspective on Stochastic Gradient Descent", "comment": null, "summary": "Machine learning models trained with \\emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.", "AI": {"tldr": "SGD\u901a\u8fc7\u5229\u7528\u6279\u6b21\u91c7\u6837\u7684\u68af\u5ea6\u53d8\u5f02\u6027\u4f5c\u4e3a\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u968f\u673a\u6027\u7684\u4ee3\u7406\uff0c\u9690\u5f0f\u6b63\u5219\u5316\u68af\u5ea6\u534f\u65b9\u5dee\u77e9\u9635\u7684\u8ff9\uff0c\u4ece\u800c\u63a7\u5236\u7b97\u6cd5\u53d8\u5f02\u6027\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76SGD\u76f8\u6bd4\u786e\u5b9a\u6027\u68af\u5ea6\u4e0b\u964d\u5728\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4f18\u52bf\uff0c\u63a2\u8ba8SGD\u5982\u4f55\u901a\u8fc7\u68af\u5ea6\u53d8\u5f02\u6027\u6765\u6a21\u62df\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u7684\u968f\u673a\u6027\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7ecf\u9a8c\u98ce\u9669\u6700\u5c0f\u5316\u7684\u7406\u60f3\u5316\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660eSGD\u88ab\u5438\u5f15\u5230\u5728\u91cd\u91c7\u6837\u4e0b\u7a33\u5065\u7684\u53c2\u6570\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5b9e\u9a8c\u5728\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u9a8c\u8bc1\u7b97\u6cd5\u53d8\u5f02\u6027\u4f5c\u4e3a\u6b63\u5219\u5668\u7684\u6548\u679c\u3002", "result": "SGD\u9690\u5f0f\u6b63\u5219\u5316\u68af\u5ea6\u534f\u65b9\u5dee\u77e9\u9635\u7684\u8ff9\uff0c\u63a7\u5236\u7b97\u6cd5\u53d8\u5f02\u6027\uff0c\u4f7f\u89e3\u5bf9\u91c7\u6837\u566a\u58f0\u66f4\u4e0d\u654f\u611f\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u6027\u80fd\u3002\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u5b9e\u9a8c\u8868\u660e\uff0c\u663e\u5f0f\u52a0\u5165\u7b97\u6cd5\u53d8\u5f02\u6027\u4f30\u8ba1\u4f5c\u4e3a\u6b63\u5219\u5668\u53ef\u63d0\u5347\u6d4b\u8bd5\u6027\u80fd\u3002", "conclusion": "SGD\u7684\u6cdb\u5316\u4f18\u52bf\u6e90\u4e8e\u5176\u901a\u8fc7\u68af\u5ea6\u53d8\u5f02\u6027\u8fdb\u884c\u81ea\u52a9\u6cd5\u4f30\u8ba1\uff0c\u9690\u5f0f\u6b63\u5219\u5316\u7b97\u6cd5\u53d8\u5f02\u6027\uff0c\u4f7f\u6a21\u578b\u907f\u514d\u865a\u5047\u89e3\u5e76\u63d0\u9ad8\u5bf9\u6570\u636e\u91c7\u6837\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07197", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07197", "abs": "https://arxiv.org/abs/2512.07197", "authors": ["Seokhyun Youn", "Soohyun Lee", "Geonho Kim", "Weeyoung Kwon", "Sung-Ho Bae", "Jihyong Oh"], "title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting", "comment": "The first three authors contributed equally to this work. The last two authors are co-corresponding authors. Please visit our project page at https://cmlab-korea.github.io/Awesome-Efficient-GS/", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u9ad8\u65483D\u548c4D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u7684\u7efc\u8ff0\uff0c\u7cfb\u7edf\u5730\u5206\u7c7b\u4e86\u53c2\u6570\u538b\u7f29\u548c\u7ed3\u6784\u538b\u7f29\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\uff0c\u5e76\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "3D\u9ad8\u65af\u6e85\u5c04\u867d\u7136\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f3D\u91cd\u5efa\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff0c\u7279\u522b\u662f\u57284D\u52a8\u6001\u573a\u666f\u4e2d\u66f4\u4e3a\u4e25\u91cd\u3002\u9700\u8981\u7814\u7a76\u9ad8\u6548\u65b9\u6cd5\u6765\u51cf\u5c11\u5197\u4f59\u540c\u65f6\u4fdd\u6301\u91cd\u5efa\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5c06\u73b0\u6709\u6280\u672f\u5206\u4e3a\u53c2\u6570\u538b\u7f29\u548c\u7ed3\u6784\u538b\u7f29\u4e24\u5927\u65b9\u5411\uff0c\u5bf9\u6bcf\u79cd\u65b9\u6cd5\u7684\u6838\u5fc3\u601d\u60f3\u548c\u65b9\u6cd5\u8d8b\u52bf\u8fdb\u884c\u5168\u9762\u603b\u7ed3\uff0c\u5e76\u6db5\u76d6\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u57fa\u51c6\u6bd4\u8f83\u3002", "result": "\u63d0\u4f9b\u4e86\u9996\u4e2a\u7edf\u4e00\u7684\u9ad8\u65483D\u548c4D\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u6982\u89c8\uff0c\u5efa\u7acb\u4e86\u7cfb\u7edf\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u53d1\u5c55\u73b0\u72b6\u548c\u6280\u672f\u7279\u70b9\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u65e8\u5728\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u7d27\u51d1\u548c\u5b9e\u65f6\u7684\u9759\u6001\u4e0e\u52a8\u60013D\u573a\u666f\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u3002"}}
{"id": "2512.07705", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07705", "abs": "https://arxiv.org/abs/2512.07705", "authors": ["Saroj Gopali", "Bipin Chhetri", "Deepika Giri", "Sima Siami-Namini", "Akbar Siami Namin"], "title": "In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models", "comment": null, "summary": "Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.\n  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.\n  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\uff0c\u53d1\u73b0Google\u7684TimesFM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u5b9e\u65f6\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u7814\u7a76\u8fd9\u4e9b\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u662f\u5426\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f7f\u7528in-context\u5b66\u4e60\u3001\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u8bad\u7ec3LLM\u6a21\u578b\uff0c\u5bf9\u6bd4TimesFM\u3001TCN\u548cLSTM\u7b49\u6a21\u578b\u6027\u80fd\u3002", "result": "TimesFM\u8868\u73b0\u6700\u4f73\uff0cRMSE\u6700\u4f4e\uff080.3023\uff09\uff0c\u63a8\u7406\u65f6\u95f4\u6709\u7ade\u4e89\u529b\uff08266\u79d2\uff09\u3002OpenAI o4-mini\u5728\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u9884\u8bad\u7ec3\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u662f\u5b9e\u65f6\u9884\u6d4b\u7684\u6709\u524d\u666f\u65b9\u5411\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u6a21\u578b\u9002\u5e94\u5b9e\u73b0\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u90e8\u7f72\u3002"}}
{"id": "2512.07198", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07198", "abs": "https://arxiv.org/abs/2512.07198", "authors": ["Xiujie Song", "Qi Jia", "Shota Watanabe", "Xiaoyi Pang", "Ruijie Chen", "Mengyue Wu", "Kenny Q. Zhu"], "title": "Generating Storytelling Images with Rich Chains-of-Reasoning", "comment": null, "summary": "An image can convey a compelling story by presenting rich, logically connected visual clues. These connections form Chains-of-Reasoning (CoRs) within the image, enabling viewers to infer events, causal relationships, and other information, thereby understanding the underlying story. In this paper, we focus on these semantically rich images and define them as Storytelling Images. Such images have diverse applications beyond illustration creation and cognitive screening, leveraging their ability to convey multi-layered information visually and inspire active interpretation. However, due to their complex semantic nature, Storytelling Images are inherently challenging to create, and thus remain relatively scarce. To address this challenge, we introduce the Storytelling Image Generation task, which explores how generative AI models can be leveraged to create such images. Specifically, we propose a two-stage pipeline, StorytellingPainter, which combines the creative reasoning abilities of Large Language Models (LLMs) with the visual synthesis capabilities of Text-to-Image (T2I) models to generate Storytelling Images. Alongside this pipeline, we develop a dedicated evaluation framework comprising three main evaluators: a Semantic Complexity Evaluator, a KNN-based Diversity Evaluator and a Story-Image Alignment Evaluator. Given the critical role of story generation in the Storytelling Image Generation task and the performance disparity between open-source and proprietary LLMs, we further explore tailored training strategies to reduce this gap, resulting in a series of lightweight yet effective models named Mini-Storytellers. Experimental results demonstrate the feasibility and effectiveness of our approaches. The code is available at https://github.com/xiujiesong/StorytellingImageGeneration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Storytelling Image Generation\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u4f18\u52bf\u6765\u751f\u6210\u5177\u6709\u590d\u6742\u8bed\u4e49\u548c\u6545\u4e8b\u6027\u7684\u56fe\u50cf\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u751f\u6210\u6280\u672f\u96be\u4ee5\u521b\u5efa\u5177\u6709\u4e30\u5bcc\u903b\u8f91\u8fde\u63a5\u548c\u6545\u4e8b\u6027\u7684\u56fe\u50cf\uff0c\u8fd9\u7c7b\u56fe\u50cf\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\u4f46\u6570\u91cf\u7a00\u7f3a\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebfStorytellingPainter\uff0c\u5229\u7528LLMs\u7684\u63a8\u7406\u80fd\u529b\u751f\u6210\u6545\u4e8b\u63cf\u8ff0\uff0c\u518d\u901a\u8fc7T2I\u6a21\u578b\u751f\u6210\u56fe\u50cf\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u8bad\u7ec3\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578bMini-Storytellers\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u5177\u6709\u6545\u4e8b\u6027\u56fe\u50cf\u65b9\u9762\u662f\u53ef\u884c\u4e14\u6709\u6548\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u751f\u6210\u5177\u6709\u590d\u6742\u8bed\u4e49\u7684\u6545\u4e8b\u6027\u56fe\u50cf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u786e\u4fdd\u4e86\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2512.07723", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07723", "abs": "https://arxiv.org/abs/2512.07723", "authors": ["Yonggeon Lee", "Jibin Hwang", "Alfred Malengo Kondoro", "Juhyun Song", "Youngtae Noh"], "title": "Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity", "comment": "16 pages, 9 figures, AAAI'26 (accepted)", "summary": "Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \\ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \\ours algorithm and its contribution to sustainable transportation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u5b9e\u65f6\u4e8b\u4ef6\u9884\u6d4b\u6a21\u578b\uff0c\u7528\u4e8e\u51c6\u786e\u9884\u6d4b\u7535\u52a8\u6c7d\u8f66\u7528\u6237\u7684\u51fa\u53d1\u65f6\u95f4\uff0c\u4ee5\u4f18\u5316\u5145\u7535\u7b56\u7565\u5e76\u5ef6\u957f\u7535\u6c60\u5bff\u547d\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u7684\u9502\u79bb\u5b50\u7535\u6c60\u5728\u957f\u65f6\u95f4\u9ad8\u7535\u91cf\u72b6\u6001\u4e0b\u4f1a\u52a0\u901f\u9000\u5316\uff0c\u901a\u8fc7\u5ef6\u8fdf\u5145\u6ee1\u7535\u81f3\u51fa\u53d1\u524d\u53ef\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4f46\u9700\u8981\u51c6\u786e\u9884\u6d4b\u7528\u6237\u51fa\u53d1\u65f6\u95f4\u3002", "method": "\u91c7\u7528Transformer\u67b6\u6784\u7684\u5b9e\u65f6\u4e8b\u4ef6\u9884\u6d4b\u6a21\u578b\uff0c\u5c06\u6bcf\u5929\u7684\u65f6\u95f4\u79bb\u6563\u5316\u4e3a\u57fa\u4e8e\u7f51\u683c\u7684token\u5e8f\u5217\uff0c\u5229\u7528\u6d41\u5f0f\u4e0a\u4e0b\u6587\u4fe1\u606f\u800c\u975e\u4ec5\u4f9d\u8d56\u5386\u53f2\u6a21\u5f0f\u6765\u9884\u6d4b\u51fa\u53d1\u65f6\u95f4\u3002", "result": "\u572893\u540d\u7528\u6237\u7684\u771f\u5b9e\u667a\u80fd\u624b\u673a\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u4e2a\u4f53\u65e5\u5e38\u4e2d\u7684\u4e0d\u89c4\u5219\u51fa\u53d1\u6a21\u5f0f\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u6301\u7eed\u4ea4\u901a\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2512.07201", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07201", "abs": "https://arxiv.org/abs/2512.07201", "authors": ["Cheng Yu"], "title": "Understanding Diffusion Models via Code Execution", "comment": null, "summary": "Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ea6300\u884c\u4ee3\u7801\u7684\u7b80\u6d01\u6269\u6563\u6a21\u578b\u5b9e\u73b0\uff0c\u4ece\u4ee3\u7801\u6267\u884c\u89d2\u5ea6\u89e3\u91ca\u6269\u6563\u6a21\u578b\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u586b\u8865\u4e86\u7406\u8bba\u63a8\u5bfc\u4e0e\u5b9e\u9645\u5b9e\u73b0\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7406\u8bba\u590d\u6742\u4e14\u8bba\u6587\u4e2d\u7684\u6570\u5b66\u516c\u5f0f\u4e0e\u5f00\u6e90\u5b9e\u73b0\u4e4b\u95f4\u5b58\u5728\u8f83\u5927\u5dee\u8ddd\u3002\u73b0\u6709\u6559\u7a0b\u4e3b\u8981\u5173\u6ce8\u65b9\u7a0b\u63a8\u5bfc\uff0c\u5bf9\u4ee3\u7801\u5b9e\u73b0\u6307\u5bfc\u6709\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u5b9e\u73b0\uff0c\u5305\u542b\u524d\u5411\u6269\u6563\u3001\u53cd\u5411\u91c7\u6837\u3001\u566a\u58f0\u9884\u6d4b\u7f51\u7edc\u548c\u8bad\u7ec3\u5faa\u73af\u7b49\u6838\u5fc3\u7ec4\u4ef6\uff0c\u53bb\u9664\u4e86\u4e0d\u5fc5\u8981\u7684\u5de5\u7a0b\u7ec6\u8282\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u3001\u5b9e\u73b0\u4f18\u5148\u7684\u7406\u89e3\u65b9\u5f0f\uff0c\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u5728\u5b9e\u8df5\u4e2d\u7684\u5de5\u4f5c\u539f\u7406\u4ee5\u53ca\u4ee3\u7801\u4e0e\u7406\u8bba\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u8be5\u6280\u672f\u62a5\u544a\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4ece\u4ee3\u7801\u89d2\u5ea6\u7406\u89e3\u6269\u6563\u6a21\u578b\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.07741", "categories": ["cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07741", "abs": "https://arxiv.org/abs/2512.07741", "authors": ["Agnes Norbury", "George Fairs", "Alexandra L. Georgescu", "Matthew M. Nour", "Emilia Molimpakis", "Stefano Goria"], "title": "A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data", "comment": null, "summary": "During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u8d1d\u53f6\u65af\u7f51\u7edc\u5efa\u6a21\u6765\u89e3\u51b3\u7cbe\u795e\u79d1\u8bc4\u4f30\u4e2d\u591a\u6a21\u6001\u4fe1\u606f\u6574\u5408\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5206\u6790\u8bed\u97f3\u7279\u5f81\u9884\u6d4b\u6291\u90c1\u548c\u7126\u8651\u75c7\u72b6\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7cbe\u795e\u79d1\u8bc4\u4f30\u4e2d\uff0c\u4e34\u5e8a\u533b\u751f\u9700\u8981\u540c\u65f6\u89c2\u5bdf\u60a3\u8005\u7684\u8a00\u8bed\u5185\u5bb9\u548c\u975e\u8bed\u8a00\u4fe1\u53f7\uff08\u5982\u8bed\u8c03\u3001\u8bed\u901f\u3001\u8eab\u4f53\u8bed\u8a00\u7b49\uff09\uff0c\u6574\u5408\u8fd9\u4e9b\u591a\u6e90\u4fe1\u606f\u662f\u4e00\u9879\u6311\u6218\u6027\u4efb\u52a1\uff0c\u9700\u8981\u667a\u80fd\u5de5\u5177\u7684\u652f\u6301\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u7f51\u7edc\u6a21\u578b\uff0c\u57fa\u4e8e\u8bed\u97f3\u7279\u5f81\u9884\u6d4b\u6291\u90c1\u548c\u7126\u8651\u75c7\u72b6\uff0c\u5728\u5305\u542b30,135\u540d\u72ec\u7279\u8bf4\u8bdd\u8005\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u6291\u90c1\u548c\u7126\u8651\u9884\u6d4b\u4e0a\u8868\u73b0\u826f\u597d\uff08ROC-AUC\u5206\u522b\u4e3a0.842\u548c0.831\uff09\uff0c\u6838\u5fc3\u4e2a\u4f53\u75c7\u72b6\u7684ROC-AUC\u5747\u8d85\u8fc70.74\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u4eba\u53e3\u7edf\u8ba1\u516c\u5e73\u6027\u548c\u4e0d\u540c\u8f93\u5165\u6a21\u6001\u7684\u6574\u5408\u4e0e\u5197\u4f59\u3002", "conclusion": "\u5f53\u63d0\u4f9b\u8db3\u591f\u4e30\u5bcc\u7684\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u6d41\uff0c\u5e76\u5728\u75c7\u72b6\u5c42\u9762\u800c\u975e\u75be\u75c5\u5c42\u9762\u5efa\u6a21\u65f6\uff0c\u8d1d\u53f6\u65af\u7f51\u7edc\u6a21\u578b\u662f\u6784\u5efa\u7a33\u5065\u8bc4\u4f30\u652f\u6301\u5de5\u5177\u7684\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u900f\u660e\u53ef\u89e3\u91ca\u7684\u683c\u5f0f\u63d0\u4f9b\u4e34\u5e8a\u76f8\u5173\u8f93\u51fa\uff0c\u4fbf\u4e8e\u4e13\u5bb6\u4e34\u5e8a\u76d1\u7763\u3002"}}
{"id": "2512.07203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07203", "abs": "https://arxiv.org/abs/2512.07203", "authors": ["Xuhui Zheng", "Kang An", "Ziliang Wang", "Yuhang Wang", "Faqiang Qian", "Yichao Wu"], "title": "MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning", "comment": "7 pages, 1 figures", "summary": "Multimodal pre-training remains constrained by the descriptive bias of image-caption pairs, leading models to favor surface linguistic cues over grounded visual understanding. We introduce MMRPT, a masked multimodal reinforcement pre-training framework that strengthens visual reasoning in MLLMs. We are the first to incorporate reinforcement learning directly into the pre-training of large vision-language models, enabling learning signals that reward visual grounding rather than caption imitation. MMRPT constructs masked multimodal data by estimating sentence-level visual dependency via attention over visual tokens and masking highly vision-dependent segments; the model reconstructs these spans through vision-grounded reasoning guided by a semantic-visual reward. Experiments show consistent zero-shot gains across diverse benchmarks and substantially improved robustness under supervised fine-tuning, demonstrating that reinforcement-driven masked reasoning provides a more reliable and generalizable pre-training objective for multimodal models.", "AI": {"tldr": "MMRPT\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u63a9\u7801\u591a\u6a21\u6001\u6570\u636e\u548c\u89c6\u89c9\u8bed\u4e49\u5956\u52b1\u673a\u5236\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u56fe\u50cf-\u6587\u672c\u5bf9\u9884\u8bad\u7ec3\u4e2d\u7684\u63cf\u8ff0\u6027\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u53d7\u9650\u4e8e\u56fe\u50cf-\u6587\u672c\u5bf9\u7684\u63cf\u8ff0\u6027\u504f\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u5b66\u4e60\u8868\u9762\u8bed\u8a00\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u7684\u89c6\u89c9\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5f3a\u5316\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u63a9\u7801\u591a\u6a21\u6001\u5f3a\u5316\u9884\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u89c6\u89c9token\u6ce8\u610f\u529b\u4f30\u8ba1\u53e5\u5b50\u7ea7\u89c6\u89c9\u4f9d\u8d56\u5ea6\uff1b2\uff09\u63a9\u7801\u9ad8\u5ea6\u89c6\u89c9\u4f9d\u8d56\u7684\u6587\u672c\u7247\u6bb5\uff1b3\uff09\u4f7f\u7528\u57fa\u4e8e\u89c6\u89c9\u8bed\u4e49\u7684\u5956\u52b1\u673a\u5236\u6307\u5bfc\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u91cd\u5efa\u3002\u9996\u6b21\u5c06\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u878d\u5165\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u4e2d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u83b7\u5f97\u4e00\u81f4\u7684\u96f6\u6837\u672c\u6027\u80fd\u63d0\u5347\uff0c\u5728\u6709\u76d1\u7763\u5fae\u8c03\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u63a9\u7801\u63a8\u7406\u4e3a\u591a\u6a21\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u6cdb\u5316\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2512.07766", "categories": ["cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.07766", "abs": "https://arxiv.org/abs/2512.07766", "authors": ["Matteo Cipollina", "Michail Karatarakis", "Freek Wiedijk"], "title": "Formalized Hopfield Networks and Boltzmann Machines", "comment": null, "summary": "Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u795e\u7ecf\u7f51\u7edc\u5728Lean 4\u4e2d\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u786e\u5b9a\u6027\u6a21\u578b\uff08Hopfield\u7f51\u7edc\uff09\u548c\u968f\u673a\u6a21\u578b\uff08Boltzmann\u673a\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u76f8\u5173\u6536\u655b\u6027\u548c\u6b63\u786e\u6027\u5b9a\u7406\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u5e94\u7528\u5e7f\u6cdb\u4f46\u5206\u6790\u9a8c\u8bc1\u56f0\u96be\uff0c\u9700\u8981\u5f62\u5f0f\u5316\u65b9\u6cd5\u6765\u786e\u4fdd\u5176\u7406\u8bba\u6027\u8d28\u7684\u6b63\u786e\u6027\u3002", "method": "\u4f7f\u7528Lean 4\u5b9a\u7406\u8bc1\u660e\u5668\u5f62\u5f0f\u5316\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff1a\u5bf9Hopfield\u7f51\u7edc\u8bc1\u660e\u6536\u655b\u6027\u548cHebbian\u5b66\u4e60\u6b63\u786e\u6027\uff1b\u5bf9Boltzmann\u673a\u8bc1\u660e\u904d\u5386\u6027\u548c\u5e73\u7a33\u5206\u5e03\u6536\u655b\u6027\uff0c\u5e76\u65b0\u5f62\u5f0f\u5316\u4e86Perron-Frobenius\u5b9a\u7406\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u795e\u7ecf\u7f51\u7edc\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\uff1aHopfield\u7f51\u7edc\u5728\u6b63\u4ea4\u6a21\u5f0f\u4e0b\u7684\u6536\u655b\u6027\u5f97\u5230\u8bc1\u660e\uff1bBoltzmann\u673a\u7684\u904d\u5386\u6027\u548c\u552f\u4e00\u5e73\u7a33\u5206\u5e03\u6536\u655b\u6027\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u5b9a\u7406\u8bc1\u660e\u5668\u5728\u795e\u7ecf\u7f51\u7edc\u7406\u8bba\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2512.07206", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07206", "abs": "https://arxiv.org/abs/2512.07206", "authors": ["Boyang Pan", "Zeyu Zhang", "Hongyu Meng", "Bin Cui", "Yingying Zhang", "Wenli Hou", "Junhao Li", "Langdi Zhong", "Xiaoxiao Chen", "Xiaoyu Xu", "Changjin Zuo", "Chao Cheng", "Nan-Jie Gong"], "title": "AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT", "comment": null, "summary": "Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.", "AI": {"tldr": "AutoLugano\u662f\u4e00\u4e2a\u5168\u81ea\u52a8\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u80fd\u591f\u4eceFDG-PET/CT\u626b\u63cf\u4e2d\u81ea\u52a8\u5b8c\u6210\u6dcb\u5df4\u7624\u5206\u7c7b\uff0c\u5305\u62ec\u75c5\u7076\u5206\u5272\u3001\u89e3\u5256\u5b9a\u4f4d\u548cLugano\u5206\u671f\uff0c\u5728\u5916\u90e8\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u6dcb\u5df4\u7624\u7684\u521d\u59cb\u5206\u671f\u3001\u6cbb\u7597\u5206\u5c42\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a1\uff09\u57fa\u4e8e3D nnU-Net\u7684\u89e3\u5256\u611f\u77e5\u75c5\u7076\u5206\u5272\uff1b2\uff09\u4f7f\u7528TotalSegmentator\u5de5\u5177\u5305\u8fdb\u884c\u57fa\u4e8e\u56fe\u8c31\u7684\u89e3\u5256\u5b9a\u4f4d\uff1b3\uff09\u5c06\u53d7\u7d2f\u533a\u57df\u7a7a\u95f4\u5206\u5e03\u8f6c\u6362\u4e3aLugano\u5206\u671f\u548c\u6cbb\u7597\u5206\u7ec4\u3002", "result": "\u5728\u5916\u90e8\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u533a\u57df\u53d7\u7d2f\u68c0\u6d4b\u51c6\u786e\u738788.31%\uff0c\u654f\u611f\u602774.47%\uff0c\u7279\u5f02\u602794.21%\uff0cF1\u5206\u657080.80%\uff1b\u6cbb\u7597\u5206\u5c42\u51c6\u786e\u738785.07%\uff0c\u654f\u611f\u602782.61%\uff0c\u7279\u5f02\u602790.48%\u3002", "conclusion": "AutoLugano\u662f\u9996\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u80fd\u591f\u5c06\u5355\u6b21FDG-PET/CT\u626b\u63cf\u8f6c\u6362\u4e3a\u5b8c\u6574\u7684Lugano\u5206\u671f\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.07782", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07782", "abs": "https://arxiv.org/abs/2512.07782", "authors": ["Jiaxu Liu", "Yuhe Bai", "Christos-Savvas Bouganis"], "title": "GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory", "comment": null, "summary": "Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \\textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \\emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \\emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\\underline{Gated} (\\underline{F}lash) \\underline{W}indowed \\underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.", "AI": {"tldr": "GatedFWA\u662f\u4e00\u79cd\u5185\u5b58\u95e8\u63a7\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u4fdd\u6301SWA\u7ebf\u6027\u6548\u7387\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8870\u51cf\u504f\u7f6e\u7a33\u5b9a\u5185\u5b58\u66f4\u65b0\u5e76\u63a7\u5236\u68af\u5ea6\u6d41\u52a8\u3002", "motivation": "\u89e3\u51b3Softmax\u6ce8\u610f\u529b\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\u548cSWA\u8bad\u7ec3\u76ee\u6807\u65e0\u754c\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u5185\u5b58\u6536\u7f29\u548c\u68af\u5ea6\u6d88\u5931\u3002", "method": "\u901a\u8fc7\u6bcf\u4e2atoken/head\u7684\u95e8\u63a7\u79ef\u7d2f\u6210\u8870\u51cf\u504f\u7f6e\u6dfb\u52a0\u5230\u6ce8\u610f\u529blogits\u4e2d\uff0c\u5b9e\u73b0\u53ef\u5b66\u4e60\u7684\u8bb0\u5fc6\u6536\u7f29\uff0c\u5e76\u5f00\u53d1\u4e86\u878d\u5408\u5355\u6b21\u95e8\u9884\u5904\u7406\u548cFlashAttention\u517c\u5bb9\u5185\u6838\u3002", "result": "\u5728\u8bed\u8a00\u5efa\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGatedFWA\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u541e\u5410\u91cf\uff0c\u5177\u6709\u53ef\u5ffd\u7565\u7684\u5f00\u9500\u548c\u66f4\u597d\u7684\u5168\u5c40\u4e0a\u4e0b\u6587\u5229\u7528\u80fd\u529b\u3002", "conclusion": "GatedFWA\u6210\u529f\u5e73\u8861\u4e86\u6548\u7387\u4e0e\u7a33\u5b9a\u6027\uff0c\u5e76\u80fd\u4e0eNSA\u7b49\u538b\u7f29\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u81ea\u56de\u5f52\u9886\u57df\u3002"}}
{"id": "2512.07211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07211", "abs": "https://arxiv.org/abs/2512.07211", "authors": ["Frederik Hagelskj\u00e6r", "Dimitrios Arapis", "Steffen Madsen", "Thorbj\u00f8rn Mosekj\u00e6r Iversen"], "title": "Object Pose Distribution Estimation for Determining Revolution and Reflection Uncertainty in Point Clouds", "comment": "8 pages, 8 figures, 5 tables, ICCR 2025", "summary": "Object pose estimation is crucial to robotic perception and typically provides a single-pose estimate. However, a single estimate cannot capture pose uncertainty deriving from visual ambiguity, which can lead to unreliable behavior. Existing pose distribution methods rely heavily on color information, often unavailable in industrial settings.\n  We propose a novel neural network-based method for estimating object pose uncertainty using only 3D colorless data. To the best of our knowledge, this is the first approach that leverages deep learning for pose distribution estimation without relying on RGB input. We validate our method in a real-world bin picking scenario with objects of varying geometric ambiguity. Our current implementation focuses on symmetries in reflection and revolution, but the framework is extendable to full SE(3) pose distribution estimation. Source code available at opde3d.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u75283D\u65e0\u8272\u6570\u636e\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u662f\u9996\u4e2a\u4e0d\u4f9d\u8d56RGB\u8f93\u5165\u7684\u6df1\u5ea6\u5b66\u4e60\u59ff\u6001\u5206\u5e03\u4f30\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5355\u59ff\u6001\u4f30\u8ba1\u65e0\u6cd5\u6355\u6349\u89c6\u89c9\u6a21\u7cca\u6027\u5e26\u6765\u7684\u59ff\u6001\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u73b0\u6709\u59ff\u6001\u5206\u5e03\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u989c\u8272\u4fe1\u606f\uff0c\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u5f80\u5f80\u4e0d\u53ef\u7528\u3002", "method": "\u5f00\u53d1\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5229\u75283D\u51e0\u4f55\u6570\u636e\u4f30\u8ba1\u7269\u4f53\u59ff\u6001\u5206\u5e03\uff0c\u91cd\u70b9\u5173\u6ce8\u53cd\u5c04\u548c\u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u4f46\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5b8c\u6574\u7684SE(3)\u59ff\u6001\u5206\u5e03\u4f30\u8ba1\u3002", "result": "\u5728\u5177\u6709\u4e0d\u540c\u51e0\u4f55\u6a21\u7cca\u5ea6\u7684\u771f\u5b9e\u4e16\u754c\u7bb1\u5f0f\u62e3\u9009\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u4e0d\u4f9d\u8d56\u989c\u8272\u4fe1\u606f\u7684\u53ef\u9760\u59ff\u6001\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u6e90\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
{"id": "2512.07805", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.07805", "abs": "https://arxiv.org/abs/2512.07805", "authors": ["Yifan Zhang", "Zixiang Chen", "Yifeng Liu", "Zhen Qin", "Huizhuo Yuan", "Kangping Xu", "Yang Yuan", "Quanquan Gu", "Andrew Chi-Chih Yao"], "title": "Group Representational Position Encoding", "comment": "Project Page: https://github.com/model-architectures/GRAPE", "summary": "We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\\mathrm{GL}$. In Multiplicative GRAPE, a position $n \\in \\mathbb{Z}$ (or $t \\in \\mathbb{R}$) acts as $\\mathbf{G}(n)=\\exp(n\\,\u03c9\\,\\mathbf{L})$ with a rank-2 skew generator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.", "AI": {"tldr": "GRAPE\u662f\u4e00\u4e2a\u57fa\u4e8e\u7fa4\u4f5c\u7528\u7684\u7edf\u4e00\u4f4d\u7f6e\u7f16\u7801\u6846\u67b6\uff0c\u5305\u542b\u4e58\u6cd5\u65cb\u8f6c\u548c\u52a0\u6cd5logit\u504f\u5dee\u4e24\u79cd\u673a\u5236\uff0c\u5c06RoPE\u548cALiBi\u7b49\u73b0\u6709\u65b9\u6cd5\u4f5c\u4e3a\u7279\u4f8b\u7edf\u4e00\u8d77\u6765\u3002", "motivation": "\u73b0\u6709\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\uff08\u5982RoPE\u3001ALiBi\uff09\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0cGRAPE\u65e8\u5728\u901a\u8fc7\u7fa4\u4f5c\u7528\u7406\u8bba\u63d0\u4f9b\u4e00\u4e2a\u539f\u5219\u6027\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u7edf\u4e00\u4e0d\u540c\u7684\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u3002", "method": "\u57fa\u4e8e\u7fa4\u4f5c\u7528\u7406\u8bba\uff0c\u63d0\u51fa\u4e58\u6cd5GRAPE\uff08\u4f7f\u7528SO(d)\u7fa4\u7684\u65cb\u8f6c\uff09\u548c\u52a0\u6cd5GRAPE\uff08\u4f7f\u7528GL\u7fa4\u7684\u5355\u80fd\u4f5c\u7528\uff09\uff0c\u5206\u522b\u5bf9\u5e94\u65cb\u8f6c\u548c\u52a0\u6cd5\u504f\u5dee\u4e24\u79cd\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\u3002", "result": "GRAPE\u80fd\u591f\u7cbe\u786e\u6062\u590dRoPE\u548cALiBi\u7b49\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u5b50\u7a7a\u95f4\u548c\u975e\u4ea4\u6362\u6df7\u5408\u6269\u5c55\u4e86\u51e0\u4f55\u8868\u8fbe\u80fd\u529b\uff0c\u4fdd\u6301\u4e86\u76f8\u5bf9\u4f4d\u7f6e\u5173\u7cfb\u548c\u6d41\u5f0f\u7f13\u5b58\u80fd\u529b\u3002", "conclusion": "GRAPE\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u591a\u79cd\u4f4d\u7f6e\u7f16\u7801\u65b9\u6cd5\u7eb3\u5165\u5176\u4e2d\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u7684\u4f4d\u7f6e\u51e0\u4f55\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6307\u5bfc\u3002"}}
{"id": "2512.07215", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07215", "abs": "https://arxiv.org/abs/2512.07215", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation", "comment": null, "summary": "Vision Foundation Models (VFMs) and Vision Language Models (VLMs) have revolutionized computer vision by providing rich semantic and geometric representations. This paper presents a comprehensive visual comparison between CLIP based and DINOv2 based approaches for 3D pose estimation in hand object grasping scenarios. We evaluate both models on the task of 6D object pose estimation and demonstrate their complementary strengths: CLIP excels in semantic understanding through language grounding, while DINOv2 provides superior dense geometric features. Through extensive experiments on benchmark datasets, we show that CLIP based methods achieve better semantic consistency, while DINOv2 based approaches demonstrate competitive performance with enhanced geometric precision. Our analysis provides insights for selecting appropriate vision models for robotic manipulation and grasping, picking applications.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u57fa\u4e8eCLIP\u548cDINOv2\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u57283D\u624b\u90e8\u7269\u4f53\u6293\u53d6\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0CLIP\u5728\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u800cDINOv2\u5728\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u66f4\u4f18\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b(VFMs)\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u8bed\u4e49\u548c\u51e0\u4f55\u8868\u793a\uff0c\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6bd4\u8f83CLIP\u548cDINOv2\u57283D\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u9762\u7684\u89c6\u89c9\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u8bc4\u4f30CLIP\u548cDINOv2\u57286D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u4e24\u8005\u7684\u8bed\u4e49\u7406\u89e3\u548c\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1aCLIP\u57fa\u4e8e\u65b9\u6cd5\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0cDINOv2\u57fa\u4e8e\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u65b9\u9762\u5177\u6709\u7ade\u4e89\u4f18\u52bf\uff0c\u4e24\u8005\u57283D\u59ff\u6001\u4f30\u8ba1\u4e2d\u5c55\u73b0\u51fa\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u673a\u5668\u4eba\u548c\u6293\u53d6\u5e94\u7528\u4e2d\u9009\u62e9\u5408\u9002\u7684\u89c6\u89c9\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u5efa\u8bae\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\u9009\u62e9CLIP\uff08\u8bed\u4e49\u7406\u89e3\uff09\u6216DINOv2\uff08\u51e0\u4f55\u7cbe\u5ea6\uff09\u6a21\u578b\u3002"}}
{"id": "2512.07818", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.07818", "abs": "https://arxiv.org/abs/2512.07818", "authors": ["Xinyuan Cao", "Santosh S. Vempala"], "title": "Provable Long-Range Benefits of Next-Token Prediction", "comment": "66 pages, 5 figures", "summary": "Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5bf9\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e0b\u4e00\u8bcd\u9884\u6d4b\u4f18\u5316\u53ef\u4ee5\u5b66\u4e60\u5230\u957f\u8ddd\u79bb\u7ed3\u6784\uff0c\u5b9e\u73b0\u8bad\u7ec3\u5206\u5e03\u7684\u826f\u597d\u8fd1\u4f3c\u3002", "motivation": "\u89e3\u91ca\u4e3a\u4ec0\u4e48\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e0b\u4e00\u8bcd\u9884\u6d4b\u8bad\u7ec3\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u6587\u6863\u5e76\u6355\u6349\u957f\u8ddd\u79bb\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4e0b\u4e00\u8bcd\u9884\u6d4b\u4f18\u5316\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u7684\u7406\u8bba\u6709\u6548\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u76f8\u540c\u524d\u7f00\u4e0b\uff0c\u4efb\u4f55\u6709\u754c\u63cf\u8ff0\u957f\u5ea6\u7b97\u6cd5\u90fd\u65e0\u6cd5\u533a\u5206\u8bad\u7ec3\u6587\u6863\u548c\u6a21\u578b\u751f\u6210\u7684\u8fde\u7eedk\u4e2a\u6807\u8bb0\u3002", "conclusion": "\u4e0b\u4e00\u8bcd\u9884\u6d4b\u662f\u5b66\u4e60\u957f\u8ddd\u79bb\u7ed3\u6784\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u957f\u8ddd\u79bb\u8fde\u8d2f\u6027\u63d0\u4f9b\u4e86\u590d\u6742\u6027\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2512.07228", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07228", "abs": "https://arxiv.org/abs/2512.07228", "authors": ["Hengyang Yao", "Lin Li", "Ke Sun", "Jianing Qiu", "Huiping Chen"], "title": "Towards Robust Protective Perturbation against DeepFake Face Swapping", "comment": null, "summary": "DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EOLT\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u5b66\u4e60\u6700\u4f18\u53d8\u6362\u5206\u5e03\u6765\u751f\u6210\u66f4\u9c81\u68d2\u7684DeepFake\u9632\u62a4\u6270\u52a8\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u5e73\u5747\u9c81\u68d2\u6027\u4e0a\u63d0\u534726%\u3002", "motivation": "\u73b0\u6709DeepFake\u9632\u62a4\u65b9\u6cd5\u5d4c\u5165\u7684\u9690\u5f62\u6270\u52a8\u6613\u88ab\u57fa\u672c\u56fe\u50cf\u53d8\u6362\u7834\u574f\uff0c\u4e14\u4f20\u7edfEOT\u6846\u67b6\u7684\u5747\u5300\u91c7\u6837\u53d8\u6362\u5206\u5e03\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faEOLT\u6846\u67b6\uff0c\u5c06\u53d8\u6362\u5206\u5e03\u4f5c\u4e3a\u53ef\u5b66\u4e60\u7ec4\u4ef6\uff0c\u4f7f\u7528\u7b56\u7565\u7f51\u7edc\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u81ea\u52a8\u4f18\u5148\u5173\u952e\u53d8\u6362\u5e76\u751f\u6210\u5b9e\u4f8b\u7279\u5b9a\u7684\u6270\u52a8\u3002", "result": "\u57286\u7c7b30\u79cd\u53d8\u6362\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5e73\u5747\u9c81\u68d2\u6027\u63d0\u534726%\uff0c\u5728\u6311\u6218\u6027\u53d8\u6362\u7c7b\u522b\u4e0a\u589e\u76ca\u8fbe30%\u3002", "conclusion": "EOLT\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u53d8\u6362\u5206\u5e03\u6709\u6548\u5efa\u6a21\u9632\u5fa1\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86DeepFake\u9632\u62a4\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.07828", "categories": ["cs.LG", "econ.GN"], "pdf": "https://arxiv.org/pdf/2512.07828", "abs": "https://arxiv.org/abs/2512.07828", "authors": ["Jeremy Yang", "Noah Yonack", "Kate Zyskowski", "Denis Yarats", "Johnny Ho", "Jerry Ma"], "title": "The Adoption and Usage of AI Agents: Early Evidence from Perplexity", "comment": null, "summary": "This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u5728\u5f00\u653e\u7f51\u7edc\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u901a\u7528AI\u4ee3\u7406\u7684\u91c7\u7528\u3001\u4f7f\u7528\u5f3a\u5ea6\u548c\u4f7f\u7528\u573a\u666f\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u5b9e\u5730\u7814\u7a76\uff0c\u57fa\u4e8eComet\u6d4f\u89c8\u5668\u7684\u6570\u4ebf\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u63ed\u793a\u4e86AI\u4ee3\u7406\u5728\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u4e2d\u7684\u5f02\u8d28\u6027\u91c7\u7528\u6a21\u5f0f\u548c\u4f7f\u7528\u7279\u5f81\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u80fd\u529b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u4e86\u89e3\u5b9e\u9645\u7528\u6237\u5982\u4f55\u91c7\u7528\u548c\u4f7f\u7528\u8fd9\u4e9b\u667a\u80fd\u4ee3\u7406\uff0c\u4ee5\u53ca\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u4f7f\u7528\u6a21\u5f0f\u5dee\u5f02\uff0c\u4e3a\u7814\u7a76\u8005\u3001\u4f01\u4e1a\u3001\u653f\u7b56\u5236\u5b9a\u8005\u548c\u6559\u80b2\u5de5\u4f5c\u8005\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u7814\u7a76\u57fa\u4e8ePerplexity\u5f00\u53d1\u7684Comet\u6d4f\u89c8\u5668\u53ca\u5176\u96c6\u6210\u4ee3\u7406Comet Assistant\u7684\u6570\u4ebf\u6761\u533f\u540d\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u901a\u8fc7\u5206\u5c42\u4ee3\u7406\u5206\u7c7b\u6cd5\u5bf9\u4f7f\u7528\u573a\u666f\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\uff0c\u5206\u6790\u7528\u6237\u91c7\u7528\u6a21\u5f0f\u3001\u4f7f\u7528\u5f3a\u5ea6\u548c\u5177\u4f53\u4f7f\u7528\u573a\u666f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u65e9\u671f\u91c7\u7528\u8005\u3001\u9ad8GDP\u56fd\u5bb6\u7528\u6237\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u884c\u4e1a\u4ece\u4e1a\u8005\u66f4\u503e\u5411\u4e8e\u4f7f\u7528AI\u4ee3\u7406\uff1b57%\u7684\u67e5\u8be2\u96c6\u4e2d\u5728\u751f\u4ea7\u529b\u4e0e\u5de5\u4f5c\u6d41\u3001\u5b66\u4e60\u4e0e\u7814\u7a76\u4e24\u5927\u4e3b\u9898\uff1b55%\u4e3a\u4e2a\u4eba\u7528\u9014\uff0c30%\u4e3a\u4e13\u4e1a\u7528\u9014\uff0c16%\u4e3a\u6559\u80b2\u7528\u9014\uff1b\u4f7f\u7528\u573a\u666f\u5177\u6709\u77ed\u671f\u7c98\u6027\u4f46\u957f\u671f\u5411\u8ba4\u77e5\u5bfc\u5411\u8f6c\u53d8\u3002", "conclusion": "AI\u4ee3\u7406\u7684\u6269\u6563\u5bf9\u591a\u65b9\u5229\u76ca\u76f8\u5173\u8005\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684AI\u80fd\u529b\u7c7b\u522b\uff0c\u4e3a\u672a\u6765\u7684\u6280\u672f\u53d1\u5c55\u548c\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2512.07229", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07229", "abs": "https://arxiv.org/abs/2512.07229", "authors": ["Fang Zhou", "Zhiqiang Chen", "Martin Pavlovski", "Yizhong Zhang"], "title": "ReLKD: Inter-Class Relation Learning with Knowledge Distillation for Generalized Category Discovery", "comment": "Accepted to the Main Track of the 28th European Conference on Artificial Intelligence (ECAI 2025). To appear in the proceedings published by IOS Press (DOI: 10.3233/FAIA413)", "summary": "Generalized Category Discovery (GCD) faces the challenge of categorizing unlabeled data containing both known and novel classes, given only labels for known classes. Previous studies often treat each class independently, neglecting the inherent inter-class relations. Obtaining such inter-class relations directly presents a significant challenge in real-world scenarios. To address this issue, we propose ReLKD, an end-to-end framework that effectively exploits implicit inter-class relations and leverages this knowledge to enhance the classification of novel classes. ReLKD comprises three key modules: a target-grained module for learning discriminative representations, a coarse-grained module for capturing hierarchical class relations, and a distillation module for transferring knowledge from the coarse-grained module to refine the target-grained module's representation learning. Extensive experiments on four datasets demonstrate the effectiveness of ReLKD, particularly in scenarios with limited labeled data. The code for ReLKD is available at https://github.com/ZhouF-ECNU/ReLKD.", "AI": {"tldr": "ReLKD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9690\u5f0f\u7c7b\u522b\u95f4\u5173\u7cfb\u6765\u589e\u5f3a\u65b0\u7c7b\u522b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u3002", "motivation": "\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0(GCD)\u9762\u4e34\u5728\u53ea\u6709\u5df2\u77e5\u7c7b\u522b\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u5bf9\u5305\u542b\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u7684\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u5206\u7c7b\u7684\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u72ec\u7acb\u5904\u7406\u6bcf\u4e2a\u7c7b\u522b\uff0c\u5ffd\u7565\u4e86\u56fa\u6709\u7684\u7c7b\u522b\u95f4\u5173\u7cfb\uff0c\u800c\u76f4\u63a5\u83b7\u53d6\u8fd9\u4e9b\u5173\u7cfb\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u6311\u6218\u3002", "method": "ReLKD\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u76ee\u6807\u7c92\u5ea6\u6a21\u5757\u7528\u4e8e\u5b66\u4e60\u5224\u522b\u6027\u8868\u793a\uff0c\u7c97\u7c92\u5ea6\u6a21\u5757\u7528\u4e8e\u6355\u83b7\u5c42\u6b21\u5316\u7c7b\u522b\u5173\u7cfb\uff0c\u84b8\u998f\u6a21\u5757\u7528\u4e8e\u5c06\u77e5\u8bc6\u4ece\u7c97\u7c92\u5ea6\u6a21\u5757\u8f6c\u79fb\u5230\u76ee\u6807\u7c92\u5ea6\u6a21\u5757\u4ee5\u4f18\u5316\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86ReLKD\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6807\u8bb0\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ReLKD\u901a\u8fc7\u6709\u6548\u5229\u7528\u9690\u5f0f\u7c7b\u522b\u95f4\u5173\u7cfb\uff0c\u6210\u529f\u63d0\u5347\u4e86\u65b0\u7c7b\u522b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07230", "abs": "https://arxiv.org/abs/2512.07230", "authors": ["Abhinav Raundhal", "Gaurav Behera", "P J Narayanan", "Ravi Kiran Sarvadevabhatla", "Makarand Tapaswi"], "title": "STRinGS: Selective Text Refinement in Gaussian Splatting", "comment": "Accepted to WACV 2026. Project Page, see https://STRinGS-official.github.io", "summary": "Text as signs, labels, or instructions is a critical element of real-world scenes as they can convey important contextual information. 3D representations such as 3D Gaussian Splatting (3DGS) struggle to preserve fine-grained text details, while achieving high visual fidelity. Small errors in textual element reconstruction can lead to significant semantic loss. We propose STRinGS, a text-aware, selective refinement framework to address this issue for 3DGS reconstruction. Our method treats text and non-text regions separately, refining text regions first and merging them with non-text regions later for full-scene optimization. STRinGS produces sharp, readable text even in challenging configurations. We introduce a text readability measure OCR Character Error Rate (CER) to evaluate the efficacy on text regions. STRinGS results in a 63.6% relative improvement over 3DGS at just 7K iterations. We also introduce a curated dataset STRinGS-360 with diverse text scenarios to evaluate text readability in 3D reconstruction. Our method and dataset together push the boundaries of 3D scene understanding in text-rich environments, paving the way for more robust text-aware reconstruction methods.", "AI": {"tldr": "STRinGS\u662f\u4e00\u4e2a\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85(3DGS)\u7684\u6587\u672c\u611f\u77e5\u9009\u62e9\u6027\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u522b\u5904\u7406\u6587\u672c\u548c\u975e\u6587\u672c\u533a\u57df\u6765\u89e3\u51b3\u6587\u672c\u7ec6\u8282\u91cd\u5efa\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u53ef\u8bfb\u6027\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6587\u672c\u5143\u7d20\u627f\u8f7d\u91cd\u8981\u8bed\u4e49\u4fe1\u606f\uff0c\u4f46\u73b0\u67093D\u8868\u793a\u65b9\u6cd5\u59823DGS\u96be\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u6587\u672c\u7ec6\u8282\uff0c\u5c0f\u7684\u6587\u672c\u91cd\u5efa\u9519\u8bef\u4f1a\u5bfc\u81f4\u4e25\u91cd\u8bed\u4e49\u635f\u5931\u3002", "method": "\u91c7\u7528\u6587\u672c\u611f\u77e5\u7684\u9009\u62e9\u6027\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u6587\u672c\u548c\u975e\u6587\u672c\u533a\u57df\u5206\u5f00\u5904\u7406\uff1a\u5148\u4f18\u5316\u6587\u672c\u533a\u57df\uff0c\u7136\u540e\u4e0e\u4f18\u5316\u540e\u7684\u975e\u6587\u672c\u533a\u57df\u5408\u5e76\u8fdb\u884c\u5168\u573a\u666f\u4f18\u5316\u3002", "result": "STRinGS\u5728\u4ec57K\u6b21\u8fed\u4ee3\u4e0b\u76f8\u5bf93DGS\u5b9e\u73b0\u4e8663.6%\u7684\u6539\u8fdb\uff0c\u80fd\u591f\u751f\u6210\u6e05\u6670\u53ef\u8bfb\u7684\u6587\u672c\uff0c\u5373\u4f7f\u5728\u6311\u6218\u6027\u914d\u7f6e\u4e0b\u4e5f\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0e\u914d\u5957\u7684STRinGS-360\u6570\u636e\u96c6\u5171\u540c\u63a8\u52a8\u4e86\u6587\u672c\u4e30\u5bcc\u73af\u5883\u4e2d3D\u573a\u666f\u7406\u89e3\u7684\u8fb9\u754c\uff0c\u4e3a\u66f4\u9c81\u68d2\u7684\u6587\u672c\u611f\u77e5\u91cd\u5efa\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.07234", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07234", "abs": "https://arxiv.org/abs/2512.07234", "authors": ["Biao Chen", "Lin Zuo", "Mengmeng Jing", "Kunbin He", "Yuchen Wang"], "title": "Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models", "comment": null, "summary": "Dropout is a widely used regularization technique which improves the generalization ability of a model by randomly dropping neurons. In light of this, we propose Dropout Prompt Learning, which aims for applying dropout to improve the robustness of the vision-language models. Different from the vanilla dropout, we apply dropout on the tokens of the textual and visual branches, where we evaluate the token significance considering both intra-modal context and inter-modal alignment, enabling flexible dropout probabilities for each token. Moreover, to maintain semantic alignment for general knowledge transfer while encouraging the diverse representations that dropout introduces, we further propose residual entropy regularization. Experiments on 15 benchmarks show our method's effectiveness in challenging scenarios like low-shot learning, long-tail classification, and out-of-distribution generalization. Notably, our method surpasses regularization-based methods including KgCoOp by 5.10% and PromptSRC by 2.13% in performance on base-to-novel generalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDropout Prompt Learning\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u89c6\u89c9\u548c\u6587\u672c\u5206\u652f\u7684token\u4e0a\u5e94\u7528\u57fa\u4e8e\u91cd\u8981\u6027\u7684dropout\uff0c\u5e76\u7ed3\u5408\u6b8b\u5dee\u71b5\u6b63\u5219\u5316\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfdropout\u65b9\u6cd5\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u4e0d\u591f\u7075\u6d3b\uff0c\u65e0\u6cd5\u8003\u8651token\u7684\u91cd\u8981\u6027\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u66f4\u667a\u80fd\u7684dropout\u7b56\u7565\u63d0\u5347\u6a21\u578b\u5728\u4f4e\u6837\u672c\u5b66\u4e60\u3001\u957f\u5c3e\u5206\u7c7b\u548c\u5206\u5e03\u5916\u6cdb\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "1) \u5728\u6587\u672c\u548c\u89c6\u89c9\u5206\u652f\u7684token\u4e0a\u5e94\u7528dropout\uff0c\u6839\u636etoken\u7684\u6a21\u6001\u5185\u4e0a\u4e0b\u6587\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u5173\u7cfb\u8bc4\u4f30\u91cd\u8981\u6027\uff1b2) \u63d0\u51fa\u6b8b\u5dee\u71b5\u6b63\u5219\u5316\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u5bf9\u9f50\u7684\u540c\u65f6\u9f13\u52b1dropout\u5e26\u6765\u7684\u591a\u6837\u6027\u8868\u793a\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u57fa\u7840\u5230\u65b0\u7c7b\u6cdb\u5316\u4efb\u52a1\u4e0a\uff0c\u6027\u80fd\u8d85\u8fc7KgCoOp 5.10%\u548cPromptSRC 2.13%\u3002", "conclusion": "Dropout Prompt Learning\u901a\u8fc7\u667a\u80fd\u5316\u7684token\u7ea7dropout\u548c\u6b8b\u5dee\u71b5\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.07237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07237", "abs": "https://arxiv.org/abs/2512.07237", "authors": ["Cheng Zhang", "Boying Li", "Meng Wei", "Yan-Pei Cao", "Camilo Cruz Gambardella", "Dinh Phung", "Jianfei Cai"], "title": "Unified Camera Positional Encoding for Controlled Video Generation", "comment": "Code: https://github.com/chengzhag/UCPE", "summary": "Transformers have emerged as a universal backbone across 3D perception, video generation, and world models for autonomous driving and embodied AI, where understanding camera geometry is essential for grounding visual observations in three-dimensional space. However, existing camera encoding methods often rely on simplified pinhole assumptions, restricting generalization across the diverse intrinsics and lens distortions in real-world cameras. We introduce Relative Ray Encoding, a geometry-consistent representation that unifies complete camera information, including 6-DoF poses, intrinsics, and lens distortions. To evaluate its capability under diverse controllability demands, we adopt camera-controlled text-to-video generation as a testbed task. Within this setting, we further identify pitch and roll as two components effective for Absolute Orientation Encoding, enabling full control over the initial camera orientation. Together, these designs form UCPE (Unified Camera Positional Encoding), which integrates into a pretrained video Diffusion Transformer through a lightweight spatial attention adapter, adding less than 1% trainable parameters while achieving state-of-the-art camera controllability and visual fidelity. To facilitate systematic training and evaluation, we construct a large video dataset covering a wide range of camera motions and lens types. Extensive experiments validate the effectiveness of UCPE in camera-controllable video generation and highlight its potential as a general camera representation for Transformers across future multi-view, video, and 3D tasks. Code will be available at https://github.com/chengzhag/UCPE.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UCPE\uff08\u7edf\u4e00\u76f8\u673a\u4f4d\u7f6e\u7f16\u7801\uff09\uff0c\u4e00\u79cd\u51e0\u4f55\u4e00\u81f4\u7684\u76f8\u673a\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u5bf9\u5149\u7ebf\u7f16\u7801\u548c\u7edd\u5bf9\u65b9\u5411\u7f16\u7801\u5b9e\u73b0\u76f8\u673a\u63a7\u5236\u7684\u89c6\u9891\u751f\u6210\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u76f8\u673a\u63a7\u5236\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u76f8\u673a\u7f16\u7801\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u7b80\u5316\u7684\u9488\u5b54\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u76f8\u673a\u591a\u6837\u5185\u53c2\u548c\u955c\u5934\u7578\u53d8\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u5b8c\u6574\u76f8\u673a\u4fe1\u606f\u7684\u51e0\u4f55\u4e00\u81f4\u8868\u793a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u76f8\u5bf9\u5149\u7ebf\u7f16\u7801\uff08Relative Ray Encoding\uff09\u7edf\u4e00\u76f8\u673a\u4f4d\u59ff\u3001\u5185\u53c2\u548c\u955c\u5934\u7578\u53d8\u4fe1\u606f\uff0c\u7ed3\u5408\u7edd\u5bf9\u65b9\u5411\u7f16\u7801\u63a7\u5236\u521d\u59cb\u76f8\u673a\u65b9\u5411\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u6ce8\u610f\u529b\u9002\u914d\u5668\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u6269\u6563Transformer\u4e2d\u3002", "result": "UCPE\u4ec5\u589e\u52a0\u4e0d\u52301%\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u76f8\u673a\u63a7\u5236\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u76f8\u673a\u63a7\u5236\u80fd\u529b\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "conclusion": "UCPE\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u7684\u76f8\u673a\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u76f8\u673a\u63a7\u5236\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5177\u6709\u5728\u591a\u89c6\u56fe\u3001\u89c6\u9891\u548c3D\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u5e94\u7528Transformer\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.07241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07241", "abs": "https://arxiv.org/abs/2512.07241", "authors": ["Md. Srabon Chowdhury", "Syeda Fahmida Tanzim", "Sheekar Banerjee", "Ishtiak Al Mamoon", "AKM Muzahidul Islam"], "title": "Squeezed-Eff-Net: Edge-Computed Boost of Tomography Based Brain Tumor Classification leveraging Hybrid Neural Network Architecture", "comment": null, "summary": "Brain tumors are one of the most common and dangerous neurological diseases which require a timely and correct diagnosis to provide the right treatment procedures. Even with the promotion of magnetic resonance imaging (MRI), the process of tumor delineation is difficult and time-consuming, which is prone to inter-observer error. In order to overcome these limitations, this work proposes a hybrid deep learning model based on SqueezeNet v1 which is a lightweight model, and EfficientNet-B0, which is a high-performing model, and is enhanced with handcrafted radiomic descriptors, including Histogram of Oriented Gradients (HOG), Local Binary Patterns (LBP), Gabor filters and Wavelet transforms. The framework was trained and tested only on publicly available Nickparvar Brain Tumor MRI dataset, which consisted of 7,023 contrast-enhanced T1-weighted axial MRI slices which were categorized into four groups: glioma, meningioma, pituitary tumor, and no tumor. The testing accuracy of the model was 98.93% that reached a level of 99.08% with Test Time Augmentation (TTA) showing great generalization and power. The proposed hybrid network offers a compromise between computation efficiency and diagnostic accuracy compared to current deep learning structures and only has to be trained using fewer than 2.1 million parameters and less than 1.2 GFLOPs. The handcrafted feature addition allowed greater sensitivity in texture and the EfficientNet-B0 backbone represented intricate hierarchical features. The resulting model has almost clinical reliability in automated MRI-based classification of tumors highlighting its possibility of use in clinical decision-support systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSqueezeNet v1\u548cEfficientNet-B0\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u624b\u5de5\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff0c\u7528\u4e8e\u8111\u80bf\u7624MRI\u56fe\u50cf\u7684\u81ea\u52a8\u5206\u7c7b\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523098.93%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "motivation": "\u8111\u80bf\u7624\u8bca\u65ad\u9700\u8981\u53ca\u65f6\u51c6\u786e\uff0c\u4f46MRI\u56fe\u50cf\u4e2d\u80bf\u7624\u52fe\u753b\u8fc7\u7a0b\u56f0\u96be\u4e14\u8017\u65f6\uff0c\u5bb9\u6613\u4ea7\u751f\u89c2\u5bdf\u8005\u95f4\u8bef\u5dee\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7SqueezeNet v1\u548c\u9ad8\u6027\u80fdEfficientNet-B0\uff0c\u589e\u5f3aHOG\u3001LBP\u3001Gabor\u6ee4\u6ce2\u5668\u548c\u5c0f\u6ce2\u53d8\u6362\u7b49\u624b\u5de5\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff0c\u57287,023\u5f20MRI\u5207\u7247\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u6d4b\u8bd5\u3002", "result": "\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe98.93%\uff0c\u4f7f\u7528\u6d4b\u8bd5\u65f6\u95f4\u589e\u5f3a\u540e\u63d0\u5347\u81f399.08%\uff0c\u4ec5\u9700210\u4e07\u53c2\u6570\u548c1.2 GFLOPs\u8ba1\u7b97\u91cf\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6df7\u5408\u7f51\u7edc\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8bca\u65ad\u51c6\u786e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5177\u6709\u4e34\u5e8a\u53ef\u9760\u6027\uff0c\u53ef\u7528\u4e8e\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2512.07245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07245", "abs": "https://arxiv.org/abs/2512.07245", "authors": ["Toshinori Yamauchi", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Zero-Shot Textual Explanations via Translating Decision-Critical Features", "comment": "11+6 pages, 8 figures, 4 tables", "summary": "Textual explanations make image classifier decisions transparent by describing the prediction rationale in natural language. Large vision-language models can generate captions but are designed for general visual understanding, not classifier-specific reasoning. Existing zero-shot explanation methods align global image features with language, producing descriptions of what is visible rather than what drives the prediction. We propose TEXTER, which overcomes this limitation by isolating decision-critical features before alignment. TEXTER identifies the neurons contributing to the prediction and emphasizes the features encoded in those neurons -- i.e., the decision-critical features. It then maps these emphasized features into the CLIP feature space to retrieve textual explanations that reflect the model's reasoning. A sparse autoencoder further improves interpretability, particularly for Transformer architectures. Extensive experiments show that TEXTER generates more faithful and interpretable explanations than existing methods. The code will be publicly released.", "AI": {"tldr": "TEXTER\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u5206\u7c7b\u5668\u89e3\u91ca\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u51b3\u7b56\u5173\u952e\u7279\u5f81\u5e76\u6620\u5c04\u5230CLIP\u7279\u5f81\u7a7a\u95f4\u6765\u751f\u6210\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u6587\u672c\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u89e3\u91ca\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5168\u5c40\u56fe\u50cf\u7279\u5f81\uff0c\u751f\u6210\u7684\u662f\u53ef\u89c1\u5185\u5bb9\u7684\u63cf\u8ff0\u800c\u975e\u9a71\u52a8\u9884\u6d4b\u7684\u5173\u952e\u56e0\u7d20\u3002\u9700\u8981\u4e00\u79cd\u80fd\u53cd\u6620\u5206\u7c7b\u5668\u7279\u5b9a\u63a8\u7406\u8fc7\u7a0b\u7684\u89e3\u91ca\u65b9\u6cd5\u3002", "method": "1) \u8bc6\u522b\u5bf9\u9884\u6d4b\u8d21\u732e\u6700\u5927\u7684\u795e\u7ecf\u5143\uff1b2) \u5f3a\u8c03\u8fd9\u4e9b\u795e\u7ecf\u5143\u7f16\u7801\u7684\u51b3\u7b56\u5173\u952e\u7279\u5f81\uff1b3) \u5c06\u5f3a\u8c03\u7684\u7279\u5f81\u6620\u5c04\u5230CLIP\u7279\u5f81\u7a7a\u95f4\u68c0\u7d22\u6587\u672c\u89e3\u91ca\uff1b4) \u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u9ad8Transformer\u67b6\u6784\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTEXTER\u6bd4\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u66f4\u5fe0\u5b9e\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\uff0c\u80fd\u66f4\u597d\u5730\u53cd\u6620\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "TEXTER\u901a\u8fc7\u9694\u79bb\u51b3\u7b56\u5173\u952e\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u65e0\u6cd5\u53cd\u6620\u5206\u7c7b\u5668\u7279\u5b9a\u63a8\u7406\u7684\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u66f4\u900f\u660e\u7684\u51b3\u7b56\u89e3\u91ca\u3002"}}
{"id": "2512.07247", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07247", "abs": "https://arxiv.org/abs/2512.07247", "authors": ["Ziming Hong", "Tianyu Huang", "Runnan Chen", "Shanshan Ye", "Mingming Gong", "Bo Han", "Tongliang Liu"], "title": "AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing", "comment": "40 pages, 34 figures, 18 tables", "summary": "Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.", "AI": {"tldr": "AdLift\u662f\u9996\u4e2a\u9488\u5bf93D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u7f16\u8f91\u4fdd\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e25\u683c\u53d7\u9650\u76842D\u5bf9\u6297\u6027\u6270\u52a8\u63d0\u5347\u4e3a3D\u9ad8\u65af\u8868\u793a\u7684\u4fdd\u62a4\u5c42\uff0c\u9632\u6b62\u8de8\u4efb\u610f\u89c6\u89d2\u548c\u7ef4\u5ea6\u7684\u6307\u4ee4\u9a71\u52a8\u7f16\u8f91\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6307\u4ee4\u9a71\u52a82D\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u6269\u5c55\u52303DGS\u9886\u57df\uff0c3DGS\u8d44\u4ea7\u9762\u4e34\u672a\u7ecf\u6388\u6743\u7f16\u8f91\u548c\u6076\u610f\u7be1\u6539\u7684\u98ce\u9669\u3002\u73b0\u67092D\u56fe\u50cf\u7684\u5bf9\u6297\u6027\u6270\u52a8\u4fdd\u62a4\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e3DGS\uff0c\u9700\u8981\u89e3\u51b3\u89c6\u89d2\u901a\u7528\u6027\u4fdd\u62a4\u548c\u53ef\u89c1\u6027\u4e0e\u4fdd\u62a4\u80fd\u529b\u5e73\u8861\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faAdLift\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7684Lifted PGD\u7b97\u6cd5\uff0c\u5728\u8bad\u7ec3\u89c6\u89d2\u4e0a\u9010\u6b65\u4f18\u5316\u4fdd\u62a4\u9ad8\u65af\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u68af\u5ea6\u622a\u65ad\u548c\u56fe\u50cf\u5230\u9ad8\u65af\u62df\u5408\u4e24\u4e2a\u4ea4\u66ff\u6b65\u9aa4\uff1a\u9996\u5148\u5728\u6e32\u67d3\u56fe\u50cf\u4e0a\u5bf9\u7f16\u8f91\u6a21\u578b\u8fdb\u884c\u68af\u5ea6\u622a\u65ad\uff0c\u7136\u540e\u901a\u8fc7\u56fe\u50cf\u5230\u9ad8\u65af\u62df\u5408\u64cd\u4f5c\u5c06\u6270\u52a8\u53cd\u5411\u4f20\u64ad\u5230\u4fdd\u62a4\u9ad8\u65af\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAdLift\u80fd\u6709\u6548\u5bf9\u6297\u6700\u5148\u8fdb\u7684\u6307\u4ee4\u9a71\u52a82D\u56fe\u50cf\u548c3DGS\u7f16\u8f91\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u89c6\u89d2\u4e0b\u63d0\u4f9b\u4e00\u81f4\u7684\u57fa\u4e8e\u5bf9\u6297\u6027\u6270\u52a8\u7684\u4fdd\u62a4\u6027\u80fd\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u65b0\u89c6\u89d2\u3002", "conclusion": "AdLift\u662f\u9996\u4e2a\u9488\u5bf93DGS\u7684\u7f16\u8f91\u4fdd\u62a4\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e863D\u573a\u666f\u4fdd\u62a4\u4e2d\u7684\u89c6\u89d2\u901a\u7528\u6027\u548c\u53ef\u89c1\u6027-\u4fdd\u62a4\u80fd\u529b\u5e73\u8861\u95ee\u9898\uff0c\u4e3a3D\u5185\u5bb9\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07251", "abs": "https://arxiv.org/abs/2512.07251", "authors": ["Junqi Liu", "Zejun Wu", "Pedro R. A. S. Bassi", "Xinze Zhou", "Wenxuan Li", "Ibrahim E. Hamamci", "Sezgin Er", "Tianyu Lin", "Yi Luo", "Szymon P\u0142otka", "Bjoern Menze", "Daguang Xu", "Kai Ding", "Kang Wang", "Yang Yang", "Yucheng Tang", "Alan L. Yuille", "Zongwei Zhou"], "title": "See More, Change Less: Anatomy-Aware Diffusion for Contrast Enhancement", "comment": null, "summary": "Image enhancement improves visual quality and helps reveal details that are hard to see in the original image. In medical imaging, it can support clinical decision-making, but current models often over-edit. This can distort organs, create false findings, and miss small tumors because these models do not understand anatomy or contrast dynamics. We propose SMILE, an anatomy-aware diffusion model that learns how organs are shaped and how they take up contrast. It enhances only clinically relevant regions while leaving all other areas unchanged. SMILE introduces three key ideas: (1) structure-aware supervision that follows true organ boundaries and contrast patterns; (2) registration-free learning that works directly with unaligned multi-phase CT scans; (3) unified inference that provides fast and consistent enhancement across all contrast phases. Across six external datasets, SMILE outperforms existing methods in image quality (14.2% higher SSIM, 20.6% higher PSNR, 50% better FID) and in clinical usefulness by producing anatomically accurate and diagnostically meaningful images. SMILE also improves cancer detection from non-contrast CT, raising the F1 score by up to 10 percent.", "AI": {"tldr": "SMILE\u662f\u4e00\u4e2a\u89e3\u5256\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\uff0c\u901a\u8fc7\u7406\u89e3\u5668\u5b98\u5f62\u72b6\u548c\u5bf9\u6bd4\u5ea6\u52a8\u6001\uff0c\u4ec5\u5728\u4e34\u5e8a\u76f8\u5173\u533a\u57df\u8fdb\u884c\u589e\u5f3a\uff0c\u907f\u514d\u8fc7\u5ea6\u7f16\u8f91\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u56fe\u50cf\u589e\u5f3a\u6a21\u578b\u5bb9\u6613\u8fc7\u5ea6\u7f16\u8f91\uff0c\u5bfc\u81f4\u5668\u5b98\u53d8\u5f62\u3001\u4ea7\u751f\u5047\u9633\u6027\u7ed3\u679c\u548c\u9057\u6f0f\u5c0f\u80bf\u7624\uff0c\u56e0\u4e3a\u5b83\u4eec\u7f3a\u4e4f\u5bf9\u89e3\u5256\u7ed3\u6784\u548c\u5bf9\u6bd4\u5ea6\u52a8\u6001\u7684\u7406\u89e3\u3002", "method": "SMILE\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a\u7ed3\u6784\u611f\u77e5\u76d1\u7763\uff08\u9075\u5faa\u771f\u5b9e\u5668\u5b98\u8fb9\u754c\u548c\u5bf9\u6bd4\u5ea6\u6a21\u5f0f\uff09\u3001\u65e0\u9700\u914d\u51c6\u5b66\u4e60\uff08\u76f4\u63a5\u5904\u7406\u672a\u5bf9\u9f50\u7684\u591a\u671f\u76f8CT\u626b\u63cf\uff09\u3001\u7edf\u4e00\u63a8\u65ad\uff08\u5728\u6240\u6709\u5bf9\u6bd4\u5ea6\u671f\u76f8\u63d0\u4f9b\u5feb\u901f\u4e00\u81f4\u7684\u589e\u5f3a\uff09\u3002", "result": "\u5728\u516d\u4e2a\u5916\u90e8\u6570\u636e\u96c6\u4e0a\uff0cSMILE\u5728\u56fe\u50cf\u8d28\u91cf\uff08SSIM\u63d0\u9ad814.2%\uff0cPSNR\u63d0\u9ad820.6%\uff0cFID\u6539\u558450%\uff09\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u5c06\u975e\u5bf9\u6bd4CT\u7684\u764c\u75c7\u68c0\u6d4bF1\u5206\u6570\u63d0\u9ad8\u4e8610%\u3002", "conclusion": "SMILE\u901a\u8fc7\u89e3\u5256\u611f\u77e5\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u8d28\u91cf\u548c\u4e34\u5e8a\u8bca\u65ad\u4ef7\u503c\u3002"}}
{"id": "2512.07253", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07253", "abs": "https://arxiv.org/abs/2512.07253", "authors": ["Handing Xu", "Zhenguo Nie", "Tairan Peng", "Huimin Pan", "Xin-Jun Liu"], "title": "DGGAN: Degradation Guided Generative Adversarial Network for Real-time Endoscopic Video Enhancement", "comment": "18 pages, 8 figures, and 7 tables", "summary": "Endoscopic surgery relies on intraoperative video, making image quality a decisive factor for surgical safety and efficacy. Yet, endoscopic videos are often degraded by uneven illumination, tissue scattering, occlusions, and motion blur, which obscure critical anatomical details and complicate surgical manipulation. Although deep learning-based methods have shown promise in image enhancement, most existing approaches remain too computationally demanding for real-time surgical use. To address this challenge, we propose a degradation-aware framework for endoscopic video enhancement, which enables real-time, high-quality enhancement by propagating degradation representations across frames. In our framework, degradation representations are first extracted from images using contrastive learning. We then introduce a fusion mechanism that modulates image features with these representations to guide a single-frame enhancement model, which is trained with a cycle-consistency constraint between degraded and restored images to improve robustness and generalization. Experiments demonstrate that our framework achieves a superior balance between performance and efficiency compared with several state-of-the-art methods. These results highlight the effectiveness of degradation-aware modeling for real-time endoscopic video enhancement. Nevertheless, our method suggests that implicitly learning and propagating degradation representation offer a practical pathway for clinical application.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9000\u5316\u611f\u77e5\u7684\u5b9e\u65f6\u5185\u7aa5\u955c\u89c6\u9891\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5e27\u4f20\u64ad\u9000\u5316\u8868\u793a\u6765\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u589e\u5f3a", "motivation": "\u5185\u7aa5\u955c\u624b\u672f\u4f9d\u8d56\u672f\u4e2d\u89c6\u9891\uff0c\u4f46\u89c6\u9891\u8d28\u91cf\u5e38\u56e0\u5149\u7167\u4e0d\u5747\u3001\u7ec4\u7ec7\u6563\u5c04\u3001\u906e\u6321\u548c\u8fd0\u52a8\u6a21\u7cca\u800c\u4e0b\u964d\uff0c\u5f71\u54cd\u624b\u672f\u5b89\u5168\u6027\u548c\u6548\u679c\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u624b\u672f\u9700\u6c42", "method": "\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u9000\u5316\u8868\u793a\uff0c\u5f15\u5165\u878d\u5408\u673a\u5236\u5c06\u9000\u5316\u8868\u793a\u4e0e\u56fe\u50cf\u7279\u5f81\u7ed3\u5408\u6765\u6307\u5bfc\u5355\u5e27\u589e\u5f3a\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u9000\u5316\u4e0e\u6062\u590d\u56fe\u50cf\u95f4\u7684\u5faa\u73af\u4e00\u81f4\u6027\u7ea6\u675f\u8fdb\u884c\u8bad\u7ec3", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u95f4\u8fbe\u5230\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5e73\u8861", "conclusion": "\u901a\u8fc7\u9690\u5f0f\u5b66\u4e60\u548c\u4f20\u64ad\u9000\u5316\u8868\u793a\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u4e2d\u5b9e\u65f6\u5185\u7aa5\u955c\u89c6\u9891\u589e\u5f3a\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84"}}
{"id": "2512.07269", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07269", "abs": "https://arxiv.org/abs/2512.07269", "authors": ["Mike Diessner", "Yannick Tarant"], "title": "A graph generation pipeline for critical infrastructures based on heuristics, images and depth data", "comment": null, "summary": "Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6444\u5f71\u6d4b\u91cf\u7684\u56fe\u751f\u6210\u7ba1\u9053\uff0c\u7528\u4e8e\u521b\u5efa\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u865a\u62df\u8868\u793a\uff0c\u76f8\u6bd4\u6fc0\u5149\u626b\u63cf\u66f4\u7ecf\u6d4e\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528\u6fc0\u5149\u626b\u63cf\u4eea\u83b7\u53d63D\u70b9\u4e91\u7684\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u6613\u7528\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u7acb\u4f53\u76f8\u673a\u83b7\u53d6RGB\u56fe\u50cf\u548c\u6df1\u5ea6\u6570\u636e\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\uff0c\u7ed3\u5408\u7528\u6237\u5b9a\u4e49\u7684\u542f\u53d1\u5f0f\u89c4\u5219\u63a8\u65ad\u5bf9\u8c61\u5173\u7cfb\u3002", "result": "\u5728\u4e24\u4e2a\u6db2\u538b\u7cfb\u7edf\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u56fe\u63a5\u8fd1\u771f\u5b9e\u60c5\u51b5\uff0c\u5177\u6709\u7075\u6d3b\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u7684\u9ad8\u98ce\u9669\u51b3\u7b56\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u3001\u7075\u6d3b\u4e14\u900f\u660e\u7684\u865a\u62df\u8868\u793a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07273", "abs": "https://arxiv.org/abs/2512.07273", "authors": ["Zhi Rao", "Yucheng Zhou", "Benjia Zhou", "Yiqing Huang", "Sergio Escalera", "Jun Wan"], "title": "RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation", "comment": null, "summary": "Gloss-free sign language translation (SLT) is hindered by two key challenges: **inadequate sign representation** that fails to capture nuanced visual cues, and **sentence-level semantic misalignment** in current LLM-based methods, which limits translation quality. To address these issues, we propose a three-stage **r**einforcing **v**ision-**l**anguage **f**ramework (**RVLF**). We build a large vision-language model (LVLM) specifically designed for sign language, and then combine it with reinforcement learning (RL) to adaptively enhance translation performance. First, for a sufficient representation of sign language, RVLF introduces an effective semantic representation learning mechanism that fuses skeleton-based motion cues with semantically rich visual features extracted via DINOv2, followed by instruction tuning to obtain a strong SLT-SFT baseline. Then, to improve sentence-level semantic misalignment, we introduce a GRPO-based optimization strategy that fine-tunes the SLT-SFT model with a reward function combining translation fidelity (BLEU) and sentence completeness (ROUGE), yielding the optimized model termed SLT-GRPO. Our conceptually simple framework yields substantial gains under the gloss-free SLT setting without pre-training on any external large-scale sign language datasets, improving BLEU-4 scores by +5.1, +1.11, +1.4, and +1.61 on the CSL-Daily, PHOENIX-2014T, How2Sign, and OpenASL datasets, respectively. To the best of our knowledge, this is the first work to incorporate GRPO into SLT. Extensive experiments and ablation studies validate the effectiveness of GRPO-based optimization in enhancing both translation quality and semantic consistency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRVLF\u7684\u4e09\u9636\u6bb5\u5f3a\u5316\u89c6\u89c9\u8bed\u8a00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u6ce8\u91ca\u624b\u8bed\u7ffb\u8bd1\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u624b\u8bed\u8868\u793a\u4e0d\u8db3\u548c\u53e5\u5b50\u7ea7\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u3001\u6307\u4ee4\u5fae\u8c03\u548cGRPO\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u624b\u8bed\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u65e0\u6ce8\u91ca\u624b\u8bed\u7ffb\u8bd1\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u624b\u8bed\u8868\u793a\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6355\u6349\u7ec6\u5fae\u7684\u89c6\u89c9\u7ebf\u7d22\uff1b2\uff09\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5b58\u5728\u53e5\u5b50\u7ea7\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "RVLF\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a1\uff09\u6784\u5efa\u4e13\u95e8\u7684\u624b\u8bed\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u878d\u5408\u57fa\u4e8e\u9aa8\u9abc\u7684\u8fd0\u52a8\u7ebf\u7d22\u548cDINOv2\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\uff1b2\uff09\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u83b7\u5f97SLT-SFT\u57fa\u7ebf\u6a21\u578b\uff1b3\uff09\u5f15\u5165GRPO\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408BLEU\u548cROUGE\u5956\u52b1\u51fd\u6570\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff1aCSL-Daily\uff08+5.1 BLEU-4\uff09\u3001PHOENIX-2014T\uff08+1.11\uff09\u3001How2Sign\uff08+1.4\uff09\u548cOpenASL\uff08+1.61\uff09\u3002\u8fd9\u662f\u9996\u4e2a\u5c06GRPO\u5e94\u7528\u4e8e\u624b\u8bed\u7ffb\u8bd1\u7684\u5de5\u4f5c\u3002", "conclusion": "\u5b9e\u9a8c\u8bc1\u660eGRPO\u4f18\u5316\u80fd\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u8be5\u6846\u67b6\u5728\u4e0d\u4f7f\u7528\u5916\u90e8\u5927\u89c4\u6a21\u624b\u8bed\u6570\u636e\u96c6\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u6539\u5584\u4e86\u65e0\u6ce8\u91ca\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\u3002"}}
{"id": "2512.07275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07275", "abs": "https://arxiv.org/abs/2512.07275", "authors": ["Siyu Wang", "Hua Wang", "Huiyu Li", "Fan Zhang"], "title": "Effective Attention-Guided Multi-Scale Medical Network for Skin Lesion Segmentation", "comment": "The paper has been accepted by BIBM 2025", "summary": "In the field of healthcare, precise skin lesion segmentation is crucial for the early detection and accurate diagnosis of skin diseases. Despite significant advances in deep learning for image processing, existing methods have yet to effectively address the challenges of irregular lesion shapes and low contrast. To address these issues, this paper proposes an innovative encoder-decoder network architecture based on multi-scale residual structures, capable of extracting rich feature information from different receptive fields to effectively identify lesion areas. By introducing a Multi-Resolution Multi-Channel Fusion (MRCF) module, our method captures cross-scale features, enhancing the clarity and accuracy of the extracted information. Furthermore, we propose a Cross-Mix Attention Module (CMAM), which redefines the attention scope and dynamically calculates weights across multiple contexts, thus improving the flexibility and depth of feature capture and enabling deeper exploration of subtle features. To overcome the information loss caused by skip connections in traditional U-Net, an External Attention Bridge (EAB) is introduced, facilitating the effective utilization of information in the decoder and compensating for the loss during upsampling. Extensive experimental evaluations on several skin lesion segmentation datasets demonstrate that the proposed model significantly outperforms existing transformer and convolutional neural network-based models, showcasing exceptional segmentation accuracy and robustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u6b8b\u5dee\u7ed3\u6784\u7684\u521b\u65b0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u4e2d\u4e0d\u89c4\u5219\u5f62\u72b6\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u7684\u6311\u6218\uff0c\u901a\u8fc7MRCF\u6a21\u5757\u3001CMAM\u6a21\u5757\u548cEAB\u6865\u63a5\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u4e2d\u96be\u4ee5\u6709\u6548\u5904\u7406\u4e0d\u89c4\u5219\u75c5\u53d8\u5f62\u72b6\u548c\u4f4e\u5bf9\u6bd4\u5ea6\u95ee\u9898\uff0c\u4f20\u7edfU-Net\u7684\u8df3\u8dc3\u8fde\u63a5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u6b8b\u5dee\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u5305\u542bMRCF\u6a21\u5757\u5b9e\u73b0\u8de8\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0cCMAM\u6a21\u5757\u901a\u8fc7\u52a8\u6001\u6743\u91cd\u8ba1\u7b97\u589e\u5f3a\u7279\u5f81\u6355\u83b7\u6df1\u5ea6\uff0cEAB\u6865\u63a5\u673a\u5236\u8865\u507f\u4e0a\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u635f\u5931\u3002", "result": "\u5728\u591a\u4e2a\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u548cCNN\u7684\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u521b\u65b0\u7f51\u7edc\u67b6\u6784\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u6865\u63a5\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u76ae\u80a4\u75c5\u53d8\u5206\u5272\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u533b\u7597\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07276", "abs": "https://arxiv.org/abs/2512.07276", "authors": ["Mai Tsujimoto", "Junjue Wang", "Weihao Xuan", "Naoto Yokoya"], "title": "Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery", "comment": "Accepted to WACV 2026. Camera-ready-based version with minor edits for readability (no change in the contents)", "summary": "Three-dimensional geospatial analysis is critical to applications in urban planning, climate adaptation, and environmental assessment. Current methodologies depend on costly, specialized sensors (e.g., LiDAR and multispectral), which restrict global accessibility. Existing sensor-based and rule-driven methods further struggle with tasks requiring the integration of multiple 3D cues, handling diverse queries, and providing interpretable reasoning. We hereby present Geo3DVQA, a comprehensive benchmark for evaluating vision-language models (VLMs) in height-aware, 3D geospatial reasoning using RGB-only remote sensing imagery. Unlike conventional sensor-based frameworks, Geo3DVQA emphasizes realistic scenarios that integrate elevation, sky view factors, and land cover patterns. The benchmark encompasses 110k curated question-answer pairs spanning 16 task categories across three complexity levels: single-feature inference, multi-feature reasoning, and application-level spatial analysis. The evaluation of ten state-of-the-art VLMs highlights the difficulty of RGB-to-3D reasoning. GPT-4o and Gemini-2.5-Flash achieved only 28.6% and 33.0% accuracy respectively, while domain-specific fine-tuning of Qwen2.5-VL-7B achieved 49.6% (+24.8 points). These results reveal both the limitations of current VLMs and the effectiveness of domain adaptation. Geo3DVQA introduces new challenge frontiers for scalable, accessible, and holistic 3D geospatial analysis. The dataset and code will be released upon publication at https://github.com/mm1129/Geo3DVQA.", "AI": {"tldr": "Geo3DVQA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57283D\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4ec5RGB\u9065\u611f\u56fe\u50cf\uff0c\u6db5\u76d611\u4e07\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524dVLMs\u5728RGB\u52303D\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d3D\u5730\u7406\u7a7a\u95f4\u5206\u6790\u4f9d\u8d56\u6602\u8d35\u4e13\u4e1a\u4f20\u611f\u5668\uff0c\u9650\u5236\u4e86\u5168\u7403\u53ef\u8bbf\u95ee\u6027\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6574\u5408\u591a\u79cd3D\u7ebf\u7d22\u548c\u5904\u7406\u591a\u6837\u5316\u67e5\u8be2\u3002", "method": "\u5f00\u53d1Geo3DVQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b16\u4e2a\u4efb\u52a1\u7c7b\u522b\u3001\u4e09\u4e2a\u590d\u6742\u5ea6\u7ea7\u522b\u768411\u4e07\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8bc4\u4f3010\u4e2a\u6700\u5148\u8fdbVLMs\u7684\u6027\u80fd\u3002", "result": "GPT-4o\u548cGemini-2.5-Flash\u51c6\u786e\u7387\u5206\u522b\u4e3a28.6%\u548c33.0%\uff0c\u800c\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u7684Qwen2.5-VL-7B\u8fbe\u523049.6%\u51c6\u786e\u7387\u3002", "conclusion": "Geo3DVQA\u63ed\u793a\u4e86\u5f53\u524dVLMs\u7684\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u4e86\u9886\u57df\u9002\u5e94\u7684\u6709\u6548\u6027\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u8bbf\u95ee\u76843D\u5730\u7406\u7a7a\u95f4\u5206\u6790\u8bbe\u7acb\u4e86\u65b0\u7684\u6311\u6218\u524d\u6cbf\u3002"}}
{"id": "2512.07302", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07302", "abs": "https://arxiv.org/abs/2512.07302", "authors": ["Mingning Guo", "Mengwei Wu", "Shaoxian Li", "Haifeng Li", "Chao Tao"], "title": "Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts", "comment": null, "summary": "Existing image perception methods based on VLMs generally follow a paradigm wherein models extract and analyze image content based on user-provided textual task prompts. However, such methods face limitations when applied to UAV imagery, which presents challenges like target confusion, scale variations, and complex backgrounds. These challenges arise because VLMs' understanding of image content depends on the semantic alignment between visual and textual tokens. When the task prompt is simplistic and the image content is complex, achieving effective alignment becomes difficult, limiting the model's ability to focus on task-relevant information. To address this issue, we introduce AerialVP, the first agent framework for task prompt enhancement in UAV image perception. AerialVP proactively extracts multi-dimensional auxiliary information from UAV images to enhance task prompts, overcoming the limitations of traditional VLM-based approaches. Specifically, the enhancement process includes three stages: (1) analyzing the task prompt to identify the task type and enhancement needs, (2) selecting appropriate tools from the tool repository, and (3) generating enhanced task prompts based on the analysis and selected tools. To evaluate AerialVP, we introduce AerialSense, a comprehensive benchmark for UAV image perception that includes Aerial Visual Reasoning, Aerial Visual Question Answering, and Aerial Visual Grounding tasks. AerialSense provides a standardized basis for evaluating model generalization and performance across diverse resolutions, lighting conditions, and both urban and natural scenes. Experimental results demonstrate that AerialVP significantly enhances task prompt guidance, leading to stable and substantial performance improvements in both open-source and proprietary VLMs. Our work will be available at https://github.com/lostwolves/AerialVP.", "AI": {"tldr": "AerialVP\u662f\u4e00\u4e2a\u4e13\u4e3a\u65e0\u4eba\u673a\u56fe\u50cf\u611f\u77e5\u8bbe\u8ba1\u7684\u4efb\u52a1\u63d0\u793a\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u63d0\u53d6\u591a\u7ef4\u8f85\u52a9\u4fe1\u606f\u6765\u589e\u5f3a\u4efb\u52a1\u63d0\u793a\uff0c\u89e3\u51b3\u4f20\u7edfVLM\u65b9\u6cd5\u5728\u590d\u6742\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eVLM\u7684\u56fe\u50cf\u611f\u77e5\u65b9\u6cd5\u5728\u5904\u7406\u65e0\u4eba\u673a\u56fe\u50cf\u65f6\u9762\u4e34\u76ee\u6807\u6df7\u6dc6\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u590d\u6742\u80cc\u666f\u7b49\u6311\u6218\uff0c\u56e0\u4e3aVLM\u5bf9\u56fe\u50cf\u5185\u5bb9\u7684\u7406\u89e3\u4f9d\u8d56\u4e8e\u89c6\u89c9\u548c\u6587\u672c\u6807\u8bb0\u7684\u8bed\u4e49\u5bf9\u9f50\u3002\u5f53\u4efb\u52a1\u63d0\u793a\u7b80\u5355\u800c\u56fe\u50cf\u5185\u5bb9\u590d\u6742\u65f6\uff0c\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u5bf9\u9f50\u3002", "method": "AerialVP\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u9636\u6bb5\uff1a(1)\u5206\u6790\u4efb\u52a1\u63d0\u793a\u4ee5\u786e\u5b9a\u4efb\u52a1\u7c7b\u578b\u548c\u589e\u5f3a\u9700\u6c42\uff1b(2)\u4ece\u5de5\u5177\u5e93\u4e2d\u9009\u62e9\u9002\u5f53\u5de5\u5177\uff1b(3)\u57fa\u4e8e\u5206\u6790\u548c\u9009\u5b9a\u5de5\u5177\u751f\u6210\u589e\u5f3a\u7684\u4efb\u52a1\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAerialVP\u663e\u8457\u589e\u5f3a\u4e86\u4efb\u52a1\u63d0\u793a\u7684\u6307\u5bfc\u80fd\u529b\uff0c\u5728\u5f00\u6e90\u548c\u4e13\u6709VLM\u4e2d\u90fd\u5e26\u6765\u4e86\u7a33\u5b9a\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "AerialVP\u662f\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u65e0\u4eba\u673a\u56fe\u50cf\u611f\u77e5\u7684\u4efb\u52a1\u63d0\u793a\u589e\u5f3a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165AerialSense\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u65e0\u4eba\u673a\u56fe\u50cf\u611f\u77e5\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u7840\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u65e0\u4eba\u673a\u56fe\u50cf\u4e2d\u7684\u611f\u77e5\u6311\u6218\u3002"}}
{"id": "2512.07305", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07305", "abs": "https://arxiv.org/abs/2512.07305", "authors": ["Tobias Abraham Haider"], "title": "Reevaluating Automated Wildlife Species Detection: A Reproducibility Study on a Custom Image Dataset", "comment": null, "summary": "This study revisits the findings of Carl et al., who evaluated the pre-trained Google Inception-ResNet-v2 model for automated detection of European wild mammal species in camera trap images. To assess the reproducibility and generalizability of their approach, we reimplemented the experiment from scratch using openly available resources and a different dataset consisting of 900 images spanning 90 species. After minimal preprocessing, we obtained an overall classification accuracy of 62%, closely aligning with the 71% reported in the original work despite differences in datasets. As in the original study, per-class performance varied substantially, as indicated by a macro F1 score of 0.28,highlighting limitations in generalization when labels do not align directly with ImageNet classes. Our results confirm that pretrained convolutional neural networks can provide a practical baseline for wildlife species identification but also reinforce the need for species-specific adaptation or transfer learning to achieve consistent, high-quality predictions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u590d\u73b0\u4e86Carl\u7b49\u4eba\u7684\u5de5\u4f5c\uff0c\u4f7f\u7528\u516c\u5f00\u8d44\u6e90\u548c\u4e0d\u540c\u6570\u636e\u96c6\u8bc4\u4f30\u9884\u8bad\u7ec3Google Inception-ResNet-v2\u6a21\u578b\u5728\u76f8\u673a\u9677\u9631\u56fe\u50cf\u4e2d\u8bc6\u522b\u6b27\u6d32\u91ce\u751f\u54fa\u4e73\u52a8\u7269\u7269\u79cd\u7684\u6027\u80fd\uff0c\u83b7\u5f97\u4e8662%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u4e0e\u539f\u59cb\u7814\u7a76\u768471%\u76f8\u8fd1\uff0c\u4f46\u5f3a\u8c03\u4e86\u7269\u79cd\u7279\u5f02\u6027\u9002\u5e94\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u8bc4\u4f30Carl\u7b49\u4eba\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u91ce\u751f\u52a8\u7269\u7269\u79cd\u8bc6\u522b\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u91cd\u65b0\u5b9e\u73b0\u539f\u59cb\u5b9e\u9a8c\uff0c\u4f7f\u7528\u516c\u5f00\u53ef\u7528\u8d44\u6e90\u548c\u5305\u542b90\u4e2a\u7269\u79cd\u7684900\u5f20\u56fe\u50cf\u7684\u4e0d\u540c\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u6700\u5c0f\u9884\u5904\u7406\u540e\u5e94\u7528\u9884\u8bad\u7ec3Google Inception-ResNet-v2\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u603b\u4f53\u5206\u7c7b\u51c6\u786e\u7387\u4e3a62%\uff0c\u4e0e\u539f\u59cb\u7814\u7a76\u768471%\u76f8\u8fd1\uff1b\u5b8fF1\u5206\u6570\u4e3a0.28\uff0c\u8868\u660e\u4e0d\u540c\u7c7b\u522b\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5f53\u6807\u7b7e\u4e0eImageNet\u7c7b\u522b\u4e0d\u5b8c\u5168\u5bf9\u5e94\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u53ef\u4f5c\u4e3a\u91ce\u751f\u52a8\u7269\u7269\u79cd\u8bc6\u522b\u7684\u5b9e\u7528\u57fa\u7ebf\uff0c\u4f46\u9700\u8981\u7269\u79cd\u7279\u5f02\u6027\u9002\u5e94\u6216\u8fc1\u79fb\u5b66\u4e60\u4ee5\u5b9e\u73b0\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u9884\u6d4b\u3002"}}
{"id": "2512.07328", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07328", "abs": "https://arxiv.org/abs/2512.07328", "authors": ["Ziyang Mai", "Yu-Wing Tai"], "title": "ContextAnyone: Context-Aware Diffusion for Character-Consistent Text-to-Video Generation", "comment": null, "summary": "Text-to-video (T2V) generation has advanced rapidly, yet maintaining consistent character identities across scenes remains a major challenge. Existing personalization methods often focus on facial identity but fail to preserve broader contextual cues such as hairstyle, outfit, and body shape, which are critical for visual coherence. We propose \\textbf{ContextAnyone}, a context-aware diffusion framework that achieves character-consistent video generation from text and a single reference image. Our method jointly reconstructs the reference image and generates new video frames, enabling the model to fully perceive and utilize reference information. Reference information is effectively integrated into a DiT-based diffusion backbone through a novel Emphasize-Attention module that selectively reinforces reference-aware features and prevents identity drift across frames. A dual-guidance loss combines diffusion and reference reconstruction objectives to enhance appearance fidelity, while the proposed Gap-RoPE positional embedding separates reference and video tokens to stabilize temporal modeling. Experiments demonstrate that ContextAnyone outperforms existing reference-to-video methods in identity consistency and visual quality, generating coherent and context-preserving character videos across diverse motions and scenes. Project page: \\href{https://github.com/ziyang1106/ContextAnyone}{https://github.com/ziyang1106/ContextAnyone}.", "AI": {"tldr": "ContextAnyone\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u5f20\u53c2\u8003\u56fe\u50cf\u5b9e\u73b0\u89d2\u8272\u4e00\u81f4\u6027\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u53d1\u578b\u3001\u670d\u88c5\u548c\u4f53\u578b\u7b49\u4e0a\u4e0b\u6587\u7ebf\u7d22\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9762\u90e8\u8eab\u4efd\uff0c\u4f46\u65e0\u6cd5\u4fdd\u6301\u53d1\u578b\u3001\u670d\u88c5\u548c\u4f53\u578b\u7b49\u66f4\u5e7f\u6cdb\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u8fd9\u4e9b\u5bf9\u4e8e\u89c6\u89c9\u8fde\u8d2f\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u8054\u5408\u91cd\u5efa\u53c2\u8003\u56fe\u50cf\u548c\u751f\u6210\u65b0\u89c6\u9891\u5e27\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7Emphasize-Attention\u6a21\u5757\u9009\u62e9\u6027\u589e\u5f3a\u53c2\u8003\u611f\u77e5\u7279\u5f81\uff0c\u4f7f\u7528\u53cc\u5f15\u5bfc\u635f\u5931\u7ed3\u5408\u6269\u6563\u548c\u53c2\u8003\u91cd\u5efa\u76ee\u6807\uff0c\u91c7\u7528Gap-RoPE\u4f4d\u7f6e\u5d4c\u5165\u5206\u79bb\u53c2\u8003\u548c\u89c6\u9891\u6807\u8bb0\u3002", "result": "\u5b9e\u9a8c\u8868\u660eContextAnyon\u5728\u8eab\u4efd\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u53c2\u8003\u5230\u89c6\u9891\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u540c\u52a8\u4f5c\u548c\u573a\u666f\u4e2d\u751f\u6210\u8fde\u8d2f\u4e14\u4fdd\u6301\u4e0a\u4e0b\u6587\u7684\u89d2\u8272\u89c6\u9891\u3002", "conclusion": "ContextAnyon\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u89d2\u8272\u8eab\u4efd\u4e00\u81f4\u6027\u6311\u6218\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u89c6\u89c9\u8fde\u8d2f\u6027\u3002"}}
{"id": "2512.07331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07331", "abs": "https://arxiv.org/abs/2512.07331", "authors": ["Kanishk Awadhiya"], "title": "The Inductive Bottleneck: Data-Driven Emergence of Representational Sparsity in Vision Transformers", "comment": null, "summary": "Vision Transformers (ViTs) lack the hierarchical inductive biases inherent to Convolutional Neural Networks (CNNs), theoretically allowing them to maintain high-dimensional representations throughout all layers. However, recent observations suggest ViTs often spontaneously manifest a \"U-shaped\" entropy profile-compressing information in middle layers before expanding it for the final classification. In this work, we demonstrate that this \"Inductive Bottleneck\" is not an architectural artifact, but a data-dependent adaptation. By analyzing the layer-wise Effective Encoding Dimension (EED) of DINO-trained ViTs across datasets of varying compositional complexity (UC Merced, Tiny ImageNet, and CIFAR-100), we show that the depth of the bottleneck correlates strongly with the semantic abstraction required by the task. We find that while texture-heavy datasets preserve high-rank representations throughout, object-centric datasets drive the network to dampen high-frequency information in middle layers, effectively \"learning\" a bottleneck to isolate semantic features.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0Vision Transformers\uff08ViTs\uff09\u4e2d\u7684\"\u5f52\u7eb3\u74f6\u9888\"\u73b0\u8c61\u662f\u6570\u636e\u4f9d\u8d56\u7684\u9002\u5e94\u6027\u884c\u4e3a\uff0c\u800c\u975e\u67b6\u6784\u7f3a\u9677\u3002\u74f6\u9888\u6df1\u5ea6\u4e0e\u4efb\u52a1\u6240\u9700\u7684\u8bed\u4e49\u62bd\u8c61\u7a0b\u5ea6\u76f8\u5173\uff0c\u5bf9\u8c61\u4e2d\u5fc3\u6570\u636e\u96c6\u4f1a\u9a71\u52a8\u7f51\u7edc\u5728\u4e2d\u95f4\u5c42\u6291\u5236\u9ad8\u9891\u4fe1\u606f\u4ee5\u5206\u79bb\u8bed\u4e49\u7279\u5f81\u3002", "motivation": "\u5c3d\u7ba1ViTs\u7406\u8bba\u4e0a\u53ef\u4ee5\u5728\u6240\u6709\u5c42\u4fdd\u6301\u9ad8\u7ef4\u8868\u793a\uff0c\u4f46\u5b9e\u9645\u89c2\u5bdf\u663e\u793a\u5b83\u4eec\u7ecf\u5e38\u81ea\u53d1\u5f62\u6210\"U\u5f62\"\u71b5\u5206\u5e03\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\"\u5f52\u7eb3\u74f6\u9888\"\u73b0\u8c61\u7684\u672c\u8d28\u539f\u56e0\u3002", "method": "\u901a\u8fc7\u5206\u6790DINO\u8bad\u7ec3\u7684ViTs\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u6570\u636e\u96c6\uff08UC Merced\u3001Tiny ImageNet\u548cCIFAR-100\uff09\u4e0a\u7684\u9010\u5c42\u6709\u6548\u7f16\u7801\u7ef4\u5ea6\uff08EED\uff09\uff0c\u7814\u7a76\u74f6\u9888\u6df1\u5ea6\u4e0e\u8bed\u4e49\u62bd\u8c61\u9700\u6c42\u7684\u76f8\u5173\u6027\u3002", "result": "\u7eb9\u7406\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u5728\u6574\u4e2a\u7f51\u7edc\u4e2d\u4fdd\u6301\u9ad8\u79e9\u8868\u793a\uff0c\u800c\u5bf9\u8c61\u4e2d\u5fc3\u6570\u636e\u96c6\u5219\u9a71\u52a8\u7f51\u7edc\u5728\u4e2d\u95f4\u5c42\u6291\u5236\u9ad8\u9891\u4fe1\u606f\uff0c\u6709\u6548\"\u5b66\u4e60\"\u51fa\u4e00\u4e2a\u74f6\u9888\u6765\u5206\u79bb\u8bed\u4e49\u7279\u5f81\u3002", "conclusion": "ViTs\u4e2d\u7684\u5f52\u7eb3\u74f6\u9888\u662f\u6570\u636e\u9a71\u52a8\u7684\u9002\u5e94\u6027\u673a\u5236\uff0c\u5176\u6df1\u5ea6\u4e0e\u4efb\u52a1\u8bed\u4e49\u590d\u6742\u5ea6\u6b63\u76f8\u5173\uff0c\u8868\u660eViTs\u80fd\u591f\u6839\u636e\u6570\u636e\u7279\u6027\u52a8\u6001\u8c03\u6574\u8868\u793a\u7b56\u7565\u3002"}}
{"id": "2512.07338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07338", "abs": "https://arxiv.org/abs/2512.07338", "authors": ["Lu\u00eds Marnoto", "Alexandre Bernardino", "Bruno Martins"], "title": "Generalized Referring Expression Segmentation on Aerial Photos", "comment": "Submitted to IEEE J-STARS", "summary": "Referring expression segmentation is a fundamental task in computer vision that integrates natural language understanding with precise visual localization of target regions. Considering aerial imagery (e.g., modern aerial photos collected through drones, historical photos from aerial archives, high-resolution satellite imagery, etc.) presents unique challenges because spatial resolution varies widely across datasets, the use of color is not consistent, targets often shrink to only a few pixels, and scenes contain very high object densities and objects with partial occlusions. This work presents Aerial-D, a new large-scale referring expression segmentation dataset for aerial imagery, comprising 37,288 images with 1,522,523 referring expressions that cover 259,709 annotated targets, spanning across individual object instances, groups of instances, and semantic regions covering 21 distinct classes that range from vehicles and infrastructure to land coverage types. The dataset was constructed through a fully automatic pipeline that combines systematic rule-based expression generation with a Large Language Model (LLM) enhancement procedure that enriched both the linguistic variety and the focus on visual details within the referring expressions. Filters were additionally used to simulate historic imaging conditions for each scene. We adopted the RSRefSeg architecture, and trained models on Aerial-D together with prior aerial datasets, yielding unified instance and semantic segmentation from text for both modern and historical images. Results show that the combined training achieves competitive performance on contemporary benchmarks, while maintaining strong accuracy under monochrome, sepia, and grainy degradations that appear in archival aerial photography. The dataset, trained models, and complete software pipeline are publicly available at https://luispl77.github.io/aerial-d .", "AI": {"tldr": "Aerial-D\u662f\u4e00\u4e2a\u65b0\u7684\u7528\u4e8e\u822a\u7a7a\u56fe\u50cf\u7684\u5927\u89c4\u6a21\u53c2\u8003\u8868\u8fbe\u5f0f\u5206\u5272\u6570\u636e\u96c6\uff0c\u5305\u542b37,288\u5f20\u56fe\u50cf\u548c1,522,523\u4e2a\u53c2\u8003\u8868\u8fbe\u5f0f\uff0c\u6db5\u76d6259,709\u4e2a\u6807\u6ce8\u76ee\u6807\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u81ea\u52a8\u6d41\u6c34\u7ebf\u6784\u5efa\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u8868\u8fbe\u5f0f\u751f\u6210\u548cLLM\u589e\u5f3a\uff0c\u5e76\u6a21\u62df\u4e86\u5386\u53f2\u6210\u50cf\u6761\u4ef6\u3002", "motivation": "\u822a\u7a7a\u56fe\u50cf\uff08\u5982\u65e0\u4eba\u673a\u62cd\u6444\u7684\u73b0\u4ee3\u822a\u7a7a\u7167\u7247\u3001\u5386\u53f2\u822a\u7a7a\u6863\u6848\u3001\u9ad8\u5206\u8fa8\u7387\u536b\u661f\u56fe\u50cf\u7b49\uff09\u5728\u53c2\u8003\u8868\u8fbe\u5f0f\u5206\u5272\u4efb\u52a1\u4e2d\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u7a7a\u95f4\u5206\u8fa8\u7387\u5dee\u5f02\u5927\u3001\u8272\u5f69\u4f7f\u7528\u4e0d\u4e00\u81f4\u3001\u76ee\u6807\u50cf\u7d20\u5c11\u3001\u573a\u666f\u5bf9\u8c61\u5bc6\u5ea6\u9ad8\u4e14\u5b58\u5728\u90e8\u5206\u906e\u6321\u3002", "method": "\u91c7\u7528\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u7cfb\u7edf\u5316\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u8868\u8fbe\u5f0f\u751f\u6210\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u589e\u5f3a\u7a0b\u5e8f\uff0c\u4e30\u5bcc\u53c2\u8003\u8868\u8fbe\u5f0f\u7684\u8bed\u8a00\u591a\u6837\u6027\u548c\u89c6\u89c9\u7ec6\u8282\u5173\u6ce8\u3002\u4f7f\u7528\u8fc7\u6ee4\u5668\u6a21\u62df\u5386\u53f2\u6210\u50cf\u6761\u4ef6\u3002\u91c7\u7528RSRefSeg\u67b6\u6784\uff0c\u5728Aerial-D\u548c\u5148\u524d\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u8054\u5408\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u8054\u5408\u8bad\u7ec3\u5728\u5f53\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6863\u6848\u822a\u7a7a\u6444\u5f71\u4e2d\u51fa\u73b0\u7684\u5355\u8272\u3001\u6df1\u8910\u8272\u548c\u9897\u7c92\u72b6\u9000\u5316\u6761\u4ef6\u4e0b\u4fdd\u6301\u5f3a\u51c6\u786e\u6027\u3002", "conclusion": "Aerial-D\u6570\u636e\u96c6\u53ca\u5176\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u7edf\u4e00\u5b9e\u73b0\u73b0\u4ee3\u548c\u5386\u53f2\u56fe\u50cf\u7684\u5b9e\u4f8b\u548c\u8bed\u4e49\u5206\u5272\uff0c\u4e3a\u822a\u7a7a\u56fe\u50cf\u7684\u53c2\u8003\u8868\u8fbe\u5f0f\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002\u6570\u636e\u96c6\u3001\u8bad\u7ec3\u6a21\u578b\u548c\u5b8c\u6574\u8f6f\u4ef6\u6d41\u6c34\u7ebf\u5df2\u516c\u5f00\u3002"}}
{"id": "2512.07345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07345", "abs": "https://arxiv.org/abs/2512.07345", "authors": ["Shilong Jin", "Haoran Duan", "Litao Hua", "Wentao Huang", "Yuan Zhou"], "title": "Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting", "comment": "15 pages, 8 figures, 5 tables, 2 algorithms, Accepted by AAAI 2026", "summary": "Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.", "AI": {"tldr": "TD-Attn\u662f\u4e00\u4e2a\u89e3\u51b3T2I\u6269\u6563\u6a21\u578b\u4e2d\u5148\u9a8c\u89c6\u89d2\u504f\u5dee\u5bfc\u81f4\u591a\u89c6\u89d2\u4e0d\u4e00\u81f4\u95ee\u9898\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc73D\u611f\u77e5\u6ce8\u610f\u529b\u5f15\u5bfc\u548c\u5206\u5c42\u6ce8\u610f\u529b\u8c03\u5236\u6765\u63d0\u53473D\u4efb\u52a1\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "motivation": "T2I\u6a21\u578b\u5b58\u5728\u5148\u9a8c\u89c6\u89d2\u504f\u5dee\uff0c\u5bfc\u81f4\u4e0d\u540c\u89c6\u89d2\u4e0b\u7269\u4f53\u5916\u89c2\u51b2\u7a81\uff0c\u5f71\u54cd3D\u751f\u6210\u548c\u7f16\u8f91\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51faTD-Attn\u6846\u67b6\uff0c\u5305\u542b3D-AAG\u6a21\u5757\u6784\u5efa\u89c6\u56fe\u4e00\u81f4\u76843D\u6ce8\u610f\u529b\u9ad8\u65af\u5206\u5e03\uff0c\u4ee5\u53caHAM\u6a21\u5757\u901a\u8fc7\u8bed\u4e49\u5f15\u5bfc\u6811\u5b9a\u4f4d\u548c\u8c03\u5236\u5bf9\u89c6\u89d2\u6761\u4ef6\u654f\u611f\u7684CA\u5c42\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTD-Attn\u80fd\u663e\u8457\u63d0\u5347\u591a\u89c6\u89d2\u4e00\u81f4\u6027\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u63d2\u4ef6\u589e\u5f3a\u5404\u79cd3D\u4efb\u52a1\u3002", "conclusion": "TD-Attn\u901a\u8fc7\u6570\u5b66\u5206\u6790\u548c\u5206\u5c42\u8c03\u5236\u6709\u6548\u89e3\u51b3\u4e86T2I\u6a21\u578b\u7684\u89c6\u89d2\u504f\u5dee\u95ee\u9898\uff0c\u4e3a3D\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89c6\u56fe\u4e00\u81f4\u6027\u4fdd\u969c\u3002"}}
{"id": "2512.07348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07348", "abs": "https://arxiv.org/abs/2512.07348", "authors": ["Xinyu Wei", "Kangrui Cen", "Hongyang Wei", "Zhen Guo", "Bairui Li", "Zeqing Wang", "Jinrui Zhang", "Lei Zhang"], "title": "MICo-150K: A Comprehensive Dataset Advancing Multi-Image Composition", "comment": "Project Page: https://MICo-150K.github.io/", "summary": "In controllable image generation, synthesizing coherent and consistent images from multiple reference inputs, i.e., Multi-Image Composition (MICo), remains a challenging problem, partly hindered by the lack of high-quality training data. To bridge this gap, we conduct a systematic study of MICo, categorizing it into 7 representative tasks and curate a large-scale collection of high-quality source images and construct diverse MICo prompts. Leveraging powerful proprietary models, we synthesize a rich amount of balanced composite images, followed by human-in-the-loop filtering and refinement, resulting in MICo-150K, a comprehensive dataset for MICo with identity consistency. We further build a Decomposition-and-Recomposition (De&Re) subset, where 11K real-world complex images are decomposed into components and recomposed, enabling both real and synthetic compositions. To enable comprehensive evaluation, we construct MICo-Bench with 100 cases per task and 300 challenging De&Re cases, and further introduce a new metric, Weighted-Ref-VIEScore, specifically tailored for MICo evaluation. Finally, we fine-tune multiple models on MICo-150K and evaluate them on MICo-Bench. The results show that MICo-150K effectively equips models without MICo capability and further enhances those with existing skills. Notably, our baseline model, Qwen-MICo, fine-tuned from Qwen-Image-Edit, matches Qwen-Image-2509 in 3-image composition while supporting arbitrary multi-image inputs beyond the latter's limitation. Our dataset, benchmark, and baseline collectively offer valuable resources for further research on Multi-Image Composition.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MICo-150K\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u56fe\u50cf\u7ec4\u5408\u751f\u6210\u4e2d\u7684\u6311\u6218\uff0c\u5305\u62ec\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u57fa\u51c6\u548c\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u591a\u56fe\u50cf\u7ec4\u5408\u751f\u6210\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u963b\u788d\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b7\u79cd\u4ee3\u8868\u6027\u4efb\u52a1\uff0c\u5229\u7528\u4e13\u6709\u6a21\u578b\u5408\u6210\u5e73\u8861\u7684\u590d\u5408\u56fe\u50cf\uff0c\u7ed3\u5408\u4eba\u5de5\u7b5b\u9009\u548c\u7cbe\u4fee\uff0c\u6784\u5efaMICo-150K\u6570\u636e\u96c6\uff1b\u5efa\u7acb\u5206\u89e3\u4e0e\u91cd\u7ec4\u5b50\u96c6\u548cMICo-Bench\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "MICo-150K\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7ec4\u5408\u751f\u6210\u80fd\u529b\uff0c\u57fa\u7ebf\u6a21\u578bQwen-MICo\u57283\u56fe\u50cf\u7ec4\u5408\u4e0a\u8fbe\u5230Qwen-Image-2509\u6c34\u5e73\uff0c\u5e76\u652f\u6301\u4efb\u610f\u591a\u56fe\u50cf\u8f93\u5165\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u56fe\u50cf\u7ec4\u5408\u751f\u6210\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u57fa\u51c6\u548c\u57fa\u7ebf\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2512.07351", "categories": ["cs.CV", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.07351", "abs": "https://arxiv.org/abs/2512.07351", "authors": ["Sayeem Been Zaman", "Wasimul Karim", "Arefin Ittesafun Abian", "Reem E. Mohamed", "Md Rafiqul Islam", "Asif Karim", "Sami Azam"], "title": "DeepAgent: A Dual Stream Multi Agent Fusion for Robust Multimodal Deepfake Detection", "comment": null, "summary": "The increasing use of synthetic media, particularly deepfakes, is an emerging challenge for digital content verification. Although recent studies use both audio and visual information, most integrate these cues within a single model, which remains vulnerable to modality mismatches, noise, and manipulation. To address this gap, we propose DeepAgent, an advanced multi-agent collaboration framework that simultaneously incorporates both visual and audio modalities for the effective detection of deepfakes. DeepAgent consists of two complementary agents. Agent-1 examines each video with a streamlined AlexNet-based CNN to identify the symbols of deepfake manipulation, while Agent-2 detects audio-visual inconsistencies by combining acoustic features, audio transcriptions from Whisper, and frame-reading sequences of images through EasyOCR. Their decisions are fused through a Random Forest meta-classifier that improves final performance by taking advantage of the different decision boundaries learned by each agent. This study evaluates the proposed framework using three benchmark datasets to demonstrate both component-level and fused performance. Agent-1 achieves a test accuracy of 94.35% on the combined Celeb-DF and FakeAVCeleb datasets. On the FakeAVCeleb dataset, Agent-2 and the final meta-classifier attain accuracies of 93.69% and 81.56%, respectively. In addition, cross-dataset validation on DeepFakeTIMIT confirms the robustness of the meta-classifier, which achieves a final accuracy of 97.49%, and indicates a strong capability across diverse datasets. These findings confirm that hierarchy-based fusion enhances robustness by mitigating the weaknesses of individual modalities and demonstrate the effectiveness of a multi-agent approach in addressing diverse types of manipulations in deepfakes.", "AI": {"tldr": "DeepAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u548c\u97f3\u9891\u53cc\u6a21\u6001\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\uff0c\u91c7\u7528\u5206\u5c42\u878d\u5408\u7b56\u7565\u63d0\u5347\u68c0\u6d4b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5c06\u97f3\u9891\u548c\u89c6\u89c9\u4fe1\u606f\u96c6\u6210\u5728\u5355\u4e00\u6a21\u578b\u4e2d\uff0c\u5bb9\u6613\u53d7\u5230\u6a21\u6001\u4e0d\u5339\u914d\u3001\u566a\u58f0\u548c\u64cd\u7eb5\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u53cc\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff1aAgent-1\u4f7f\u7528AlexNet-CNN\u68c0\u6d4b\u89c6\u89c9\u4f2a\u9020\u75d5\u8ff9\uff0cAgent-2\u7ed3\u5408\u58f0\u5b66\u7279\u5f81\u3001Whisper\u97f3\u9891\u8f6c\u5f55\u548cEasyOCR\u5e27\u5e8f\u5217\u68c0\u6d4b\u97f3\u89c6\u9891\u4e0d\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u968f\u673a\u68ee\u6797\u5143\u5206\u7c7b\u5668\u878d\u5408\u51b3\u7b56\u3002", "result": "\u5728Celeb-DF\u548cFakeAVCeleb\u6570\u636e\u96c6\u4e0aAgent-1\u51c6\u786e\u7387\u8fbe94.35%\uff1bFakeAVCeleb\u4e0aAgent-2\u548c\u5143\u5206\u7c7b\u5668\u51c6\u786e\u7387\u5206\u522b\u4e3a93.69%\u548c81.56%\uff1b\u8de8\u6570\u636e\u96c6\u9a8c\u8bc1\u5728DeepFakeTIMIT\u4e0a\u5143\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8fbe97.49%\u3002", "conclusion": "\u5206\u5c42\u878d\u5408\u7b56\u7565\u901a\u8fc7\u7f13\u89e3\u5355\u6a21\u6001\u5f31\u70b9\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\uff0c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u6df1\u5ea6\u4f2a\u9020\u4e2d\u7684\u591a\u6837\u5316\u64cd\u7eb5\u7c7b\u578b\u3002"}}
{"id": "2512.07360", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07360", "abs": "https://arxiv.org/abs/2512.07360", "authors": ["Qiming Huang", "Hao Ai", "Jianbo Jiao"], "title": "Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "Accepted to WACV2026", "summary": "Benefiting from the inductive biases learned from large-scale datasets, open-vocabulary semantic segmentation (OVSS) leverages the power of vision-language models, such as CLIP, to achieve remarkable progress without requiring task-specific training. However, due to CLIP's pre-training nature on image-text pairs, it tends to focus on global semantic alignment, resulting in suboptimal performance when associating fine-grained visual regions with text. This leads to noisy and inconsistent predictions, particularly in local areas. We attribute this to a dispersed bias stemming from its contrastive training paradigm, which is difficult to alleviate using CLIP features alone. To address this, we propose a structure-aware feature rectification approach that incorporates instance-specific priors derived directly from the image. Specifically, we construct a region adjacency graph (RAG) based on low-level features (e.g., colour and texture) to capture local structural relationships and use it to refine CLIP features by enhancing local discrimination. Extensive experiments show that our method effectively suppresses segmentation noise, improves region-level consistency, and achieves strong performance on multiple open-vocabulary segmentation benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u7279\u5f81\u4fee\u6b63\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u533a\u57df\u90bb\u63a5\u56fe\u6765\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u5173\u7cfb\uff0c\u4ece\u800c\u6539\u8fdbCLIP\u7279\u5f81\u5728\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5bf9\u9884\u8bad\u7ec3\u4e2d\u4fa7\u91cd\u4e8e\u5168\u5c40\u8bed\u4e49\u5bf9\u9f50\uff0c\u5bfc\u81f4\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u533a\u57df\u4e0e\u6587\u672c\u5173\u8054\u65f6\u6027\u80fd\u4e0d\u4f73\uff0c\u4ea7\u751f\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u3002\u8fd9\u6e90\u4e8e\u5bf9\u6bd4\u8bad\u7ec3\u8303\u5f0f\u5e26\u6765\u7684\u5206\u6563\u504f\u5dee\u3002", "method": "\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u4fee\u6b63\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4f4e\u5c42\u7279\u5f81\uff08\u5982\u989c\u8272\u548c\u7eb9\u7406\uff09\u6784\u5efa\u533a\u57df\u90bb\u63a5\u56fe\uff08RAG\uff09\u6765\u6355\u6349\u5c40\u90e8\u7ed3\u6784\u5173\u7cfb\uff0c\u5e76\u7528\u5176\u589e\u5f3aCLIP\u7279\u5f81\u7684\u5c40\u90e8\u5224\u522b\u80fd\u529b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6291\u5236\u4e86\u5206\u5272\u566a\u58f0\uff0c\u63d0\u9ad8\u4e86\u533a\u57df\u7ea7\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u5f3a\u52b2\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5b9e\u4f8b\u7279\u5b9a\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u7f13\u89e3\u4e86CLIP\u7279\u5f81\u5728\u5c40\u90e8\u533a\u57df\u7684\u5206\u5272\u95ee\u9898\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07379", "abs": "https://arxiv.org/abs/2512.07379", "authors": ["Mahila Moghadami", "Mohammad Ali Keyvanrad", "Melika Sabaghian"], "title": "Enhancing Small Object Detection with YOLO: A Novel Framework for Improved Accuracy and Efficiency", "comment": "22 pages, 16 figures", "summary": "This paper investigates and develops methods for detecting small objects in large-scale aerial images. Current approaches for detecting small objects in aerial images often involve image cropping and modifications to detector network architectures. Techniques such as sliding window cropping and architectural enhancements, including higher-resolution feature maps and attention mechanisms, are commonly employed. Given the growing importance of aerial imagery in various critical and industrial applications, the need for robust frameworks for small object detection becomes imperative. To address this need, we adopted the base SW-YOLO approach to enhance speed and accuracy in small object detection by refining cropping dimensions and overlap in sliding window usage and subsequently enhanced it through architectural modifications. we propose a novel model by modifying the base model architecture, including advanced feature extraction modules in the neck for feature map enhancement, integrating CBAM in the backbone to preserve spatial and channel information, and introducing a new head to boost small object detection accuracy. Finally, we compared our method with SAHI, one of the most powerful frameworks for processing large-scale images, and CZDet, which is also based on image cropping, achieving significant improvements in accuracy. The proposed model achieves significant accuracy gains on the VisDrone2019 dataset, outperforming baseline YOLOv5L detection by a substantial margin. Specifically, the final proposed model elevates the mAP .5.5 accuracy on the VisDrone2019 dataset from the base accuracy of 35.5 achieved by the YOLOv5L detector to 61.2. Notably, the accuracy of CZDet, which is another classic method applied to this dataset, is 58.36. This research demonstrates a significant improvement, achieving an increase in accuracy from 35.5 to 61.2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684SW-YOLO\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u6ed1\u52a8\u7a97\u53e3\u88c1\u526a\u7b56\u7565\u548c\u7f51\u7edc\u67b6\u6784\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u822a\u62cd\u56fe\u50cf\u4e2d\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5728VisDrone2019\u6570\u636e\u96c6\u4e0amAP\u4ece35.5\u63d0\u5347\u81f361.2\u3002", "motivation": "\u968f\u7740\u822a\u62cd\u56fe\u50cf\u5728\u5173\u952e\u5de5\u4e1a\u548c\u519b\u4e8b\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5c0f\u76ee\u6807\u7684\u9c81\u68d2\u6846\u67b6\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u68c0\u6d4b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8eSW-YOLO\u65b9\u6cd5\uff0c\u4f18\u5316\u6ed1\u52a8\u7a97\u53e3\u7684\u88c1\u526a\u5c3a\u5bf8\u548c\u91cd\u53e0\u7b56\u7565\uff0c\u5e76\u5728\u7f51\u7edc\u67b6\u6784\u4e2d\u5f15\u5165\u9ad8\u7ea7\u7279\u5f81\u63d0\u53d6\u6a21\u5757\u3001CBAM\u6ce8\u610f\u529b\u673a\u5236\u548c\u65b0\u7684\u68c0\u6d4b\u5934\u8bbe\u8ba1\u3002", "result": "\u5728VisDrone2019\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u6a21\u578b\u5c06mAP .5.5\u7cbe\u5ea6\u4eceYOLOv5L\u768435.5\u63d0\u5347\u81f361.2\uff0c\u663e\u8457\u4f18\u4e8eSAHI\u548cCZDet\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u5728\u822a\u62cd\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u80fd\u591f\u5b9e\u73b0\u663e\u8457\u7cbe\u5ea6\u63d0\u5347\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07381", "abs": "https://arxiv.org/abs/2512.07381", "authors": ["Shuohan Tao", "Boyao Zhou", "Hanzhang Tu", "Yuwang Wang", "Yebin Liu"], "title": "Tessellation GS: Neural Mesh Gaussians for Robust Monocular Reconstruction of Dynamic Objects", "comment": null, "summary": "3D Gaussian Splatting (GS) enables highly photorealistic scene reconstruction from posed image sequences but struggles with viewpoint extrapolation due to its anisotropic nature, leading to overfitting and poor generalization, particularly in sparse-view and dynamic scene reconstruction. We propose Tessellation GS, a structured 2D GS approach anchored on mesh faces, to reconstruct dynamic scenes from a single continuously moving or static camera. Our method constrains 2D Gaussians to localized regions and infers their attributes via hierarchical neural features on mesh faces. Gaussian subdivision is guided by an adaptive face subdivision strategy driven by a detail-aware loss function. Additionally, we leverage priors from a reconstruction foundation model to initialize Gaussian deformations, enabling robust reconstruction of general dynamic objects from a single static camera, previously extremely challenging for optimization-based methods. Our method outperforms previous SOTA method, reducing LPIPS by 29.1% and Chamfer distance by 49.2% on appearance and mesh reconstruction tasks.", "AI": {"tldr": "Tessellation GS\u662f\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u9762\u76842D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u6444\u50cf\u5934\u91cd\u5efa\u52a8\u6001\u573a\u666f\uff0c\u89e3\u51b3\u4e863D GS\u5728\u89c6\u89d2\u5916\u63a8\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u5728\u7a00\u758f\u89c6\u89d2\u548c\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e2d\u5b58\u5728\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u89c6\u89d2\u5916\u63a8\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5c062D\u9ad8\u65af\u7ea6\u675f\u5728\u5c40\u90e8\u533a\u57df\uff0c\u901a\u8fc7\u7f51\u683c\u9762\u4e0a\u7684\u5206\u5c42\u795e\u7ecf\u7279\u5f81\u63a8\u65ad\u5176\u5c5e\u6027\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u9762\u7ec6\u5206\u7b56\u7565\u548c\u7ec6\u8282\u611f\u77e5\u635f\u5931\u51fd\u6570\u5f15\u5bfc\u9ad8\u65af\u7ec6\u5206\uff1b\u5229\u7528\u91cd\u5efa\u57fa\u7840\u6a21\u578b\u7684\u5148\u9a8c\u521d\u59cb\u5316\u9ad8\u65af\u53d8\u5f62\u3002", "result": "\u5728\u8868\u89c2\u548c\u7f51\u683c\u91cd\u5efa\u4efb\u52a1\u4e0a\uff0cLPIPS\u964d\u4f4e\u4e8629.1%\uff0cChamfer\u8ddd\u79bb\u964d\u4f4e\u4e8649.2%\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684SOTA\u65b9\u6cd5\u3002", "conclusion": "Tessellation GS\u80fd\u591f\u4ece\u5355\u4e2a\u9759\u6001\u6444\u50cf\u5934\u7a33\u5065\u5730\u91cd\u5efa\u4e00\u822c\u52a8\u6001\u7269\u4f53\uff0c\u89e3\u51b3\u4e86\u4f18\u5316\u65b9\u6cd5\u6b64\u524d\u9762\u4e34\u7684\u6781\u7aef\u6311\u6218\u3002"}}
{"id": "2512.07383", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07383", "abs": "https://arxiv.org/abs/2512.07383", "authors": ["Deepika SN Vemuri", "Gautham Bellamkonda", "Aditya Pola", "Vineeth N Balasubramanian"], "title": "LogicCBMs: Logic-Enhanced Concept-Based Learning", "comment": "18 pages, 19 figures, WACV 2026", "summary": "Concept Bottleneck Models (CBMs) provide a basis for semantic abstractions within a neural network architecture. Such models have primarily been seen through the lens of interpretability so far, wherein they offer transparency by inferring predictions as a linear combination of semantic concepts. However, a linear combination is inherently limiting. So we propose the enhancement of concept-based learning models through propositional logic. We introduce a logic module that is carefully designed to connect the learned concepts from CBMs through differentiable logic operations, such that our proposed LogicCBM can go beyond simple weighted combinations of concepts to leverage various logical operations to yield the final predictions, while maintaining end-to-end learnability. Composing concepts using a set of logic operators enables the model to capture inter-concept relations, while simultaneously improving the expressivity of the model in terms of logic operations. Our empirical studies on well-known benchmarks and synthetic datasets demonstrate that these models have better accuracy, perform effective interventions and are highly interpretable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLogicCBM\uff0c\u901a\u8fc7\u53ef\u5fae\u903b\u8f91\u64cd\u4f5c\u589e\u5f3a\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u7ebf\u6027\u6982\u5ff5\u7ec4\u5408\uff0c\u63d0\u9ad8\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u4ec5\u901a\u8fc7\u7ebf\u6027\u7ec4\u5408\u6982\u5ff5\u8fdb\u884c\u9884\u6d4b\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u5e0c\u671b\u5f15\u5165\u903b\u8f91\u64cd\u4f5c\u6765\u6355\u6349\u6982\u5ff5\u95f4\u5173\u7cfb\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u5728\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u57fa\u7840\u4e0a\u5f15\u5165\u903b\u8f91\u6a21\u5757\uff0c\u4f7f\u7528\u53ef\u5fae\u903b\u8f91\u64cd\u4f5c\u8fde\u63a5\u5b66\u4e60\u5230\u7684\u6982\u5ff5\uff0c\u652f\u6301\u591a\u79cd\u903b\u8f91\u8fd0\u7b97\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5728\u77e5\u540d\u57fa\u51c6\u6d4b\u8bd5\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLogicCBM\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u6709\u6548\u7684\u5e72\u9884\u80fd\u529b\u548c\u826f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u903b\u8f91\u64cd\u4f5c\u589e\u5f3a\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7aef\u5230\u7aef\u53ef\u5b66\u4e60\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.07385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07385", "abs": "https://arxiv.org/abs/2512.07385", "authors": ["Chunhui Zhang", "Li Liu", "Zhipeng Zhang", "Yong Wang", "Hao Wen", "Xi Zhou", "Shiming Ge", "Yanfeng Wang"], "title": "How Far are Modern Trackers from UAV-Anti-UAV? A Million-Scale Benchmark and New Baseline", "comment": "https://github.com/983632847/Awesome-Multimodal-Object-Tracking", "summary": "Unmanned Aerial Vehicles (UAVs) offer wide-ranging applications but also pose significant safety and privacy violation risks in areas like airport and infrastructure inspection, spurring the rapid development of Anti-UAV technologies in recent years. However, current Anti-UAV research primarily focuses on RGB, infrared (IR), or RGB-IR videos captured by fixed ground cameras, with little attention to tracking target UAVs from another moving UAV platform. To fill this gap, we propose a new multi-modal visual tracking task termed UAV-Anti-UAV, which involves a pursuer UAV tracking a target adversarial UAV in the video stream. Compared to existing Anti-UAV tasks, UAV-Anti-UAV is more challenging due to severe dual-dynamic disturbances caused by the rapid motion of both the capturing platform and the target. To advance research in this domain, we construct a million-scale dataset consisting of 1,810 videos, each manually annotated with bounding boxes, a language prompt, and 15 tracking attributes. Furthermore, we propose MambaSTS, a Mamba-based baseline method for UAV-Anti-UAV tracking, which enables integrated spatial-temporal-semantic learning. Specifically, we employ Mamba and Transformer models to learn global semantic and spatial features, respectively, and leverage the state space model's strength in long-sequence modeling to establish video-level long-term context via a temporal token propagation mechanism. We conduct experiments on the UAV-Anti-UAV dataset to validate the effectiveness of our method. A thorough experimental evaluation of 50 modern deep tracking algorithms demonstrates that there is still significant room for improvement in the UAV-Anti-UAV domain. The dataset and codes will be available at {\\color{magenta}https://github.com/983632847/Awesome-Multimodal-Object-Tracking}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aUAV-Anti-UAV\u7684\u65b0\u578b\u591a\u6a21\u6001\u89c6\u89c9\u8ddf\u8e2a\u4efb\u52a1\uff0c\u6d89\u53ca\u4ece\u79fb\u52a8\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u8ddf\u8e2a\u76ee\u6807\u654c\u5bf9\u65e0\u4eba\u673a\uff0c\u5e76\u6784\u5efa\u4e86\u767e\u4e07\u7ea7\u6570\u636e\u96c6\u548c\u57fa\u4e8eMamba\u7684\u57fa\u7ebf\u65b9\u6cd5MambaSTS\u3002", "motivation": "\u5f53\u524d\u53cd\u65e0\u4eba\u673a\u7814\u7a76\u4e3b\u8981\u57fa\u4e8e\u56fa\u5b9a\u5730\u9762\u6444\u50cf\u5934\uff0c\u7f3a\u4e4f\u4ece\u79fb\u52a8\u65e0\u4eba\u673a\u5e73\u53f0\u8ddf\u8e2a\u76ee\u6807\u65e0\u4eba\u673a\u7684\u7814\u7a76\u3002\u65e0\u4eba\u673a\u5bf9\u6297\u65e0\u4eba\u673a\u573a\u666f\u9762\u4e34\u53cc\u91cd\u52a8\u6001\u5e72\u6270\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faMambaSTS\u65b9\u6cd5\uff0c\u7ed3\u5408Mamba\u548cTransformer\u6a21\u578b\u5206\u522b\u5b66\u4e60\u5168\u5c40\u8bed\u4e49\u548c\u7a7a\u95f4\u7279\u5f81\uff0c\u5229\u7528\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u957f\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u901a\u8fc7\u65f6\u95f4\u4ee4\u724c\u4f20\u64ad\u673a\u5236\u5efa\u7acb\u89c6\u9891\u7ea7\u957f\u671f\u4e0a\u4e0b\u6587\u3002", "result": "\u5728UAV-Anti-UAV\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5bf950\u79cd\u73b0\u4ee3\u6df1\u5ea6\u8ddf\u8e2a\u7b97\u6cd5\u7684\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\u8be5\u9886\u57df\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "UAV-Anti-UAV\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u65b9\u6cd5\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8be5\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u6280\u672f\u521b\u65b0\u3002"}}
{"id": "2512.07391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07391", "abs": "https://arxiv.org/abs/2512.07391", "authors": ["\u0110or\u0111e Nedeljkovi\u0107"], "title": "GlimmerNet: A Lightweight Grouped Dilated Depthwise Convolutions for UAV-Based Emergency Monitoring", "comment": null, "summary": "Convolutional Neural Networks (CNNs) have proven highly effective for edge and mobile vision tasks due to their computational efficiency. While many recent works seek to enhance CNNs with global contextual understanding via self-attention-based Vision Transformers, these approaches often introduce significant computational overhead. In this work, we demonstrate that it is possible to retain strong global perception without relying on computationally expensive components. We present GlimmerNet, an ultra-lightweight convolutional network built on the principle of separating receptive field diversity from feature recombination. GlimmerNet introduces Grouped Dilated Depthwise Convolutions(GDBlocks), which partition channels into groups with distinct dilation rates, enabling multi-scale feature extraction at no additional parameter cost. To fuse these features efficiently, we design a novel Aggregator module that recombines cross-group representations using grouped pointwise convolution, significantly lowering parameter overhead. With just 31K parameters and 29% fewer FLOPs than the most recent baseline, GlimmerNet achieves a new state-of-the-art weighted F1-score of 0.966 on the UAV-focused AIDERv2 dataset. These results establish a new accuracy-efficiency trade-off frontier for real-time emergency monitoring on resource-constrained UAV platforms. Our implementation is publicly available at https://github.com/djordjened92/gdd-cnn.", "AI": {"tldr": "GlimmerNet\u662f\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u5377\u79ef\u7f51\u7edc\uff0c\u901a\u8fc7\u5206\u7ec4\u6269\u5f20\u6df1\u5ea6\u5377\u79ef\u548c\u805a\u5408\u5668\u6a21\u5757\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u9ad8\u6548\u7279\u5f81\u878d\u5408\uff0c\u5728\u4ec531K\u53c2\u6570\u4e0b\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684Vision Transformer\u65b9\u6cd5\u867d\u7136\u80fd\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u4f46\u5f15\u5165\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u4e0d\u4f9d\u8d56\u8ba1\u7b97\u6602\u8d35\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u5f3a\u5927\u7684\u5168\u5c40\u611f\u77e5\u80fd\u529b\u3002", "method": "\u63d0\u51faGlimmerNet\uff0c\u91c7\u7528\u5206\u7ec4\u6269\u5f20\u6df1\u5ea6\u5377\u79ef(GDBlocks)\u5c06\u901a\u9053\u5206\u7ec4\u5e76\u5e94\u7528\u4e0d\u540c\u6269\u5f20\u7387\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u8bbe\u8ba1\u805a\u5408\u5668\u6a21\u5757\u4f7f\u7528\u5206\u7ec4\u70b9\u5377\u79ef\u9ad8\u6548\u91cd\u7ec4\u8de8\u7ec4\u8868\u793a\u3002", "result": "\u4ec531K\u53c2\u6570\uff0c\u6bd4\u6700\u65b0\u57fa\u7ebf\u51cf\u5c1129% FLOPs\uff0c\u5728AIDERv2\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.966\u7684\u52a0\u6743F1\u5206\u6570\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002", "conclusion": "GlimmerNet\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u7684\u5b9e\u65f6\u7d27\u6025\u76d1\u6d4b\u5efa\u7acb\u4e86\u65b0\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u524d\u6cbf\u3002"}}
{"id": "2512.07394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07394", "abs": "https://arxiv.org/abs/2512.07394", "authors": ["Zhifan Zhu", "Siddhant Bansal", "Shashank Tripathi", "Dima Damen"], "title": "Reconstructing Objects along Hand Interaction Timelines in Egocentric Video", "comment": "webpage: https://zhifanzhu.github.io/objects-along-hit", "summary": "We introduce the task of Reconstructing Objects along Hand Interaction Timelines (ROHIT). We first define the Hand Interaction Timeline (HIT) from a rigid object's perspective. In a HIT, an object is first static relative to the scene, then is held in hand following contact, where its pose changes. This is usually followed by a firm grip during use, before it is released to be static again w.r.t. to the scene. We model these pose constraints over the HIT, and propose to propagate the object's pose along the HIT enabling superior reconstruction using our proposed Constrained Optimisation and Propagation (COP) framework. Importantly, we focus on timelines with stable grasps - i.e. where the hand is stably holding an object, effectively maintaining constant contact during use. This allows us to efficiently annotate, study, and evaluate object reconstruction in videos without 3D ground truth. We evaluate our proposed task, ROHIT, over two egocentric datasets, HOT3D and in-the-wild EPIC-Kitchens. In HOT3D, we curate 1.2K clips of stable grasps. In EPIC-Kitchens, we annotate 2.4K clips of stable grasps including 390 object instances across 9 categories from videos of daily interactions in 141 environments. Without 3D ground truth, we utilise 2D projection error to assess the reconstruction. Quantitatively, COP improves stable grasp reconstruction by 6.2-11.3% and HIT reconstruction by up to 24.5% with constrained pose propagation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ROHIT\u4efb\u52a1\uff0c\u7528\u4e8e\u91cd\u5efa\u624b\u4ea4\u4e92\u65f6\u95f4\u8f74\u4e2d\u7684\u7269\u4f53\u59ff\u6001\u3002\u901a\u8fc7\u5b9a\u4e49\u624b\u4ea4\u4e92\u65f6\u95f4\u8f74(HIT)\u5e76\u5efa\u6a21\u59ff\u6001\u7ea6\u675f\uff0c\u63d0\u51faCOP\u6846\u67b6\u8fdb\u884c\u7ea6\u675f\u4f18\u5316\u548c\u4f20\u64ad\uff0c\u5728\u7a33\u5b9a\u6293\u53d6\u573a\u666f\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u7269\u4f53\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u624b\u4e0e\u7269\u4f53\u4ea4\u4e92\u65f6\u7684\u59ff\u6001\u53d8\u5316\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u7269\u4f53\u4ece\u9759\u6b62\u5230\u88ab\u624b\u6301\u3001\u4f7f\u7528\u518d\u5230\u91ca\u653e\u7684\u5b8c\u6574\u65f6\u95f4\u8f74\u4e0a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5efa\u6a21\u8fd9\u4e9b\u59ff\u6001\u7ea6\u675f\u5e76\u5b9e\u73b0\u8fde\u7eed\u91cd\u5efa\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCOP(Constrained Optimisation and Propagation)\u6846\u67b6\uff1a1) \u5b9a\u4e49\u624b\u4ea4\u4e92\u65f6\u95f4\u8f74(HIT)\u7684\u56db\u4e2a\u9636\u6bb5\uff1b2) \u5bf9\u6bcf\u4e2a\u9636\u6bb5\u5efa\u6a21\u59ff\u6001\u7ea6\u675f\uff1b3) \u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u548c\u4f20\u64ad\u5b9e\u73b0\u59ff\u6001\u7684\u8fde\u7eed\u91cd\u5efa\uff1b4) \u4e13\u6ce8\u4e8e\u7a33\u5b9a\u6293\u53d6\u573a\u666f\u4ee5\u7b80\u5316\u6807\u6ce8\u548c\u8bc4\u4f30\u3002", "result": "\u5728HOT3D\u6570\u636e\u96c6\u4e0a\u6807\u6ce8\u4e861.2K\u4e2a\u7a33\u5b9a\u6293\u53d6\u7247\u6bb5\uff0c\u5728EPIC-Kitchens\u6570\u636e\u96c6\u4e0a\u6807\u6ce8\u4e862.4K\u4e2a\u7247\u6bb5\uff08390\u4e2a\u7269\u4f53\u5b9e\u4f8b\uff0c9\u4e2a\u7c7b\u522b\uff0c141\u4e2a\u73af\u5883\uff09\u3002COP\u65b9\u6cd5\u5c06\u7a33\u5b9a\u6293\u53d6\u91cd\u5efa\u7cbe\u5ea6\u63d0\u9ad8\u4e866.2-11.3%\uff0cHIT\u91cd\u5efa\u7cbe\u5ea6\u63d0\u9ad8\u4e8624.5%\u3002", "conclusion": "ROHIT\u4efb\u52a1\u548cCOP\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u624b\u4ea4\u4e92\u65f6\u95f4\u8f74\u4e2d\u7684\u7269\u4f53\u91cd\u5efa\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7a33\u5b9a\u6293\u53d6\u573a\u666f\u4e0b\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u548c\u59ff\u6001\u4f20\u64ad\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u65e03D\u771f\u503c\u6807\u6ce8\u7684\u89c6\u9891\u7269\u4f53\u91cd\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.07564", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07564", "abs": "https://arxiv.org/abs/2512.07564", "authors": ["Kassoum Sanogo", "Renzo Ardiccioni"], "title": "Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models", "comment": "24 pages, 3 figures, 2 tables. Training-free self-correction framework for vision-language models. Code and implementation details will be released at: https://github.com/kassoumsanogo1/self-correcting-vlm-re-Attention.git", "summary": "Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u6821\u6b63\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u89c6\u89c9\u91cd\u6ce8\u610f\u673a\u5236\u6765\u51cf\u5c11\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u751f\u6210", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u7ea0\u6b63\u8fd9\u4e9b\u9519\u8bef", "method": "\u7ed3\u5408\u591a\u7ef4\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08\u6807\u8bb0\u71b5\u3001\u6ce8\u610f\u529b\u5206\u6563\u5ea6\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u3001\u58f0\u660e\u7f6e\u4fe1\u5ea6\uff09\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u88c1\u526a\uff0c\u5bf9\u672a\u5145\u5206\u63a2\u7d22\u533a\u57df\u8fdb\u884c\u91cd\u65b0\u5173\u6ce8\uff0c\u5b8c\u5168\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u9700\u68af\u5ea6\u66f4\u65b0", "result": "\u5728POPE\u548cMMHAL BENCH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e7b\u89c9\u7387\u6bd4\u57fa\u7ebf\u964d\u4f4e9.8\u4e2a\u767e\u5206\u70b9\uff0c\u5bf9\u6297\u6027\u5206\u5272\u4e2d\u7684\u5bf9\u8c61\u5b58\u5728\u51c6\u786e\u6027\u63d0\u9ad84.7\u4e2a\u767e\u5206\u70b9", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u7684\u91cd\u6ce8\u610f\u673a\u5236\u80fd\u591f\u6210\u529f\u5c06\u6821\u6b63\u57fa\u4e8e\u89c6\u89c9\u8bc1\u636e\uff0c\u4e3a\u53ef\u4fe1\u8d56\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5"}}
{"id": "2512.07410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07410", "abs": "https://arxiv.org/abs/2512.07410", "authors": ["Bin Li", "Ruichi Zhang", "Han Liang", "Jingyan Zhang", "Juze Zhang", "Xin Chen", "Lan Xu", "Jingyi Yu", "Jingya Wang"], "title": "InterAgent: Physics-based Multi-agent Command Execution via Diffusion on Interaction Graphs", "comment": "Project page: https://binlee26.github.io/InterAgent-Page", "summary": "Humanoid agents are expected to emulate the complex coordination inherent in human social behaviors. However, existing methods are largely confined to single-agent scenarios, overlooking the physically plausible interplay essential for multi-agent interactions. To bridge this gap, we propose InterAgent, the first end-to-end framework for text-driven physics-based multi-agent humanoid control. At its core, we introduce an autoregressive diffusion transformer equipped with multi-stream blocks, which decouples proprioception, exteroception, and action to mitigate cross-modal interference while enabling synergistic coordination. We further propose a novel interaction graph exteroception representation that explicitly captures fine-grained joint-to-joint spatial dependencies to facilitate network learning. Additionally, within it we devise a sparse edge-based attention mechanism that dynamically prunes redundant connections and emphasizes critical inter-agent spatial relations, thereby enhancing the robustness of interaction modeling. Extensive experiments demonstrate that InterAgent consistently outperforms multiple strong baselines, achieving state-of-the-art performance. It enables producing coherent, physically plausible, and semantically faithful multi-agent behaviors from only text prompts. Our code and data will be released to facilitate future research.", "AI": {"tldr": "InterAgent\u662f\u9996\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u6587\u672c\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u4eba\u5f62\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u6269\u6563\u53d8\u6362\u5668\u548c\u591a\u6d41\u5757\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u95f4\u7684\u7269\u7406\u5408\u7406\u4ea4\u4e92\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u6240\u9700\u7684\u7269\u7406\u5408\u7406\u534f\u8c03\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u590d\u6742\u591a\u667a\u80fd\u4f53\u793e\u4f1a\u884c\u4e3a\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u81ea\u56de\u5f52\u6269\u6563\u53d8\u6362\u5668\uff0c\u91c7\u7528\u591a\u6d41\u5757\u89e3\u8026\u672c\u4f53\u611f\u77e5\u3001\u5916\u90e8\u611f\u77e5\u548c\u52a8\u4f5c\uff1b\u5f15\u5165\u4ea4\u4e92\u56fe\u5916\u90e8\u611f\u77e5\u8868\u793a\u6355\u83b7\u5173\u8282\u95f4\u7a7a\u95f4\u4f9d\u8d56\uff1b\u8bbe\u8ba1\u7a00\u758f\u8fb9\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u526a\u679d\u5197\u4f59\u8fde\u63a5\u3002", "result": "\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\uff0cInterAgent\u59cb\u7ec8\u4f18\u4e8e\u591a\u4e2a\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u80fd\u591f\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u8fde\u8d2f\u3001\u7269\u7406\u5408\u7406\u4e14\u8bed\u4e49\u51c6\u786e\u7684\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u3002", "conclusion": "InterAgent\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u7269\u7406\u4ea4\u4e92\u7684\u6311\u6218\uff0c\u4e3a\u6587\u672c\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u4eba\u5f62\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2512.07415", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07415", "abs": "https://arxiv.org/abs/2512.07415", "authors": ["Gabriele Galatolo", "Mirco Nanni"], "title": "Data-driven Exploration of Mobility Interaction Patterns", "comment": null, "summary": "Understanding the movement behaviours of individuals and the way they react to the external world is a key component of any problem that involves the modelling of human dynamics at a physical level. In particular, it is crucial to capture the influence that the presence of an individual can have on the others. Important examples of applications include crowd simulation and emergency management, where the simulation of the mass of people passes through the simulation of the individuals, taking into consideration the others as part of the general context. While existing solutions basically start from some preconceived behavioural model, in this work we propose an approach that starts directly from the data, adopting a data mining perspective. Our method searches the mobility events in the data that might be possible evidences of mutual interactions between individuals, and on top of them looks for complex, persistent patterns and time evolving configurations of events. The study of these patterns can provide new insights on the mechanics of mobility interactions between individuals, which can potentially help in improving existing simulation models. We instantiate the general methodology on two real case studies, one on cars and one on pedestrians, and a full experimental evaluation is performed, both in terms of performances, parameter sensitivity and interpretation of sample results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u6316\u6398\u7684\u65b9\u6cd5\u6765\u7814\u7a76\u4e2a\u4f53\u95f4\u7684\u79fb\u52a8\u884c\u4e3a\u4ea4\u4e92\uff0c\u901a\u8fc7\u4ece\u6570\u636e\u4e2d\u76f4\u63a5\u53d1\u73b0\u79fb\u52a8\u4e8b\u4ef6\u4e2d\u7684\u76f8\u4e92\u5f71\u54cd\u6a21\u5f0f\uff0c\u800c\u975e\u4f9d\u8d56\u9884\u5b9a\u4e49\u7684\u884c\u4e3a\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u7c7b\u52a8\u6001\u5efa\u6a21\u65b9\u6cd5\u901a\u5e38\u57fa\u4e8e\u9884\u5b9a\u4e49\u7684\u884c\u4e3a\u6a21\u578b\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u4e2a\u4f53\u95f4\u76f8\u4e92\u5f71\u54cd\u7684\u590d\u6742\u6027\u3002\u7279\u522b\u662f\u5728\u4eba\u7fa4\u6a21\u62df\u548c\u5e94\u6025\u7ba1\u7406\u7b49\u5e94\u7528\u4e2d\uff0c\u51c6\u786e\u7406\u89e3\u4e2a\u4f53\u95f4\u7684\u4ea4\u4e92\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u6570\u636e\u6316\u6398\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u641c\u7d22\u53ef\u80fd\u53cd\u6620\u4e2a\u4f53\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u79fb\u52a8\u4e8b\u4ef6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5bfb\u627e\u590d\u6742\u3001\u6301\u4e45\u7684\u6a21\u5f0f\u548c\u968f\u65f6\u95f4\u6f14\u5316\u7684\u4e8b\u4ef6\u914d\u7f6e\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u7814\u7a76\uff08\u6c7d\u8f66\u548c\u884c\u4eba\uff09\u4e0a\u8fdb\u884c\u4e86\u5b8c\u6574\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5305\u62ec\u6027\u80fd\u3001\u53c2\u6570\u654f\u611f\u6027\u548c\u7ed3\u679c\u89e3\u91ca\u7b49\u65b9\u9762\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u4ea4\u4e92\u6a21\u5f0f\uff0c\u53ef\u4ee5\u4e3a\u6539\u8fdb\u73b0\u6709\u6a21\u62df\u6a21\u578b\u63d0\u4f9b\u65b0\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u4e2a\u4f53\u79fb\u52a8\u4ea4\u4e92\u673a\u5236\u3002"}}
{"id": "2512.07703", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07703", "abs": "https://arxiv.org/abs/2512.07703", "authors": ["Leo Fillioux", "Enzo Ferrante", "Paul-Henry Courn\u00e8de", "Maria Vakalopoulou", "Stergios Christodoulidis"], "title": "PVeRA: Probabilistic Vector-Based Random Matrix Adaptation", "comment": null, "summary": "Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.", "AI": {"tldr": "PVeRA\u662f\u4e00\u79cd\u6982\u7387\u7248\u672c\u7684VeRA\u9002\u914d\u5668\uff0c\u901a\u8fc7\u6982\u7387\u5316\u4fee\u6539\u4f4e\u79e9\u77e9\u9635\u6765\u5904\u7406\u8f93\u5165\u4e2d\u7684\u6a21\u7cca\u6027\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65f6\u652f\u6301\u4e0d\u540c\u91c7\u6837\u914d\u7f6e\uff0c\u5728VTAB-1k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8eVeRA\u548c\u5176\u4ed6\u9002\u914d\u5668\u3002", "motivation": "\u5927\u578b\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u548c\u5fae\u8c03\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u53c2\u6570\u9ad8\u6548\u9002\u914d\u65b9\u6cd5\u80fd\u5728\u5c11\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\u4e0b\u5fae\u8c03\u6a21\u578b\u3002VeRA\u9002\u914d\u5668\u901a\u8fc7\u5171\u4eab\u51bb\u7ed3\u968f\u673a\u4f4e\u79e9\u77e9\u9635\u5728\u53c2\u6570\u9ad8\u6548\u9002\u914d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u5904\u7406\u8f93\u5165\u6a21\u7cca\u6027\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faPVeRA\uff08\u6982\u7387VeRA\u9002\u914d\u5668\uff09\uff0c\u5bf9VeRA\u7684\u4f4e\u79e9\u77e9\u9635\u8fdb\u884c\u6982\u7387\u5316\u4fee\u6539\uff0c\u4f7f\u5176\u80fd\u591f\u81ea\u7136\u5904\u7406\u8f93\u5165\u4e2d\u7684\u56fa\u6709\u6a21\u7cca\u6027\uff0c\u5e76\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\u652f\u6301\u4e0d\u540c\u7684\u91c7\u6837\u914d\u7f6e\u3002", "result": "\u5728VTAB-1k\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5bf97\u79cd\u9002\u914d\u5668\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0cPVeRA\u7684\u8868\u73b0\u4f18\u4e8eVeRA\u548c\u5176\u4ed6\u9002\u914d\u5668\u3002", "conclusion": "PVeRA\u901a\u8fc7\u6982\u7387\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u53c2\u6570\u9ad8\u6548\u9002\u914d\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8f93\u5165\u6a21\u7cca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u4e86\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07426", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07426", "abs": "https://arxiv.org/abs/2512.07426", "authors": ["Karel Moens", "Matthew B. Blaschko", "Tinne Tuytelaars", "Bart Diricx", "Jonas De Vylder", "Mustafa Yousif"], "title": "When normalization hallucinates: unseen risks in AI-powered whole slide image processing", "comment": "4 pages, accepted for oral presentation at SPIE Medical Imaging, 2026", "summary": "Whole slide image (WSI) normalization remains a vital preprocessing step in computational pathology. Increasingly driven by deep learning, these models learn to approximate data distributions from training examples. This often results in outputs that gravitate toward the average, potentially masking diagnostically important features. More critically, they can introduce hallucinated content, artifacts that appear realistic but are not present in the original tissue, posing a serious threat to downstream analysis. These hallucinations are nearly impossible to detect visually, and current evaluation practices often overlook them. In this work, we demonstrate that the risk of hallucinations is real and underappreciated. While many methods perform adequately on public datasets, we observe a concerning frequency of hallucinations when these same models are retrained and evaluated on real-world clinical data. To address this, we propose a novel image comparison measure designed to automatically detect hallucinations in normalized outputs. Using this measure, we systematically evaluate several well-cited normalization methods retrained on real-world data, revealing significant inconsistencies and failures that are not captured by conventional metrics. Our findings underscore the need for more robust, interpretable normalization techniques and stricter validation protocols in clinical deployment.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\u5168\u73bb\u7247\u56fe\u50cf\uff08WSI\uff09\u5f52\u4e00\u5316\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f1a\u4ea7\u751f\u5e7b\u89c9\u5185\u5bb9\uff08\u865a\u5047\u4f46\u903c\u771f\u7684\u4f2a\u5f71\uff09\uff0c\u8fd9\u4e9b\u4f2a\u5f71\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e2d\u5c24\u4e3a\u4e25\u91cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u6bd4\u8f83\u65b9\u6cd5\u6765\u68c0\u6d4b\u8fd9\u4e9b\u5e7b\u89c9\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684WSI\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u503e\u5411\u4e8e\u8f93\u51fa\u5e73\u5747\u503c\uff0c\u53ef\u80fd\u63a9\u76d6\u8bca\u65ad\u91cd\u8981\u7279\u5f81\uff0c\u66f4\u4e25\u91cd\u7684\u662f\u4f1a\u4ea7\u751f\u96be\u4ee5\u89c6\u89c9\u68c0\u6d4b\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u8fd9\u5bf9\u4e0b\u6e38\u5206\u6790\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u6bd4\u8f83\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5f52\u4e00\u5316\u8f93\u51fa\u4e2d\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u591a\u4e2a\u5f15\u7528\u5e7f\u6cdb\u7684\u5f52\u4e00\u5316\u65b9\u6cd5\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u8fd9\u4e9b\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u4e0a\u91cd\u65b0\u8bad\u7ec3\u548c\u8bc4\u4f30\u65f6\uff0c\u5e7b\u89c9\u51fa\u73b0\u7684\u9891\u7387\u4ee4\u4eba\u62c5\u5fe7\uff0c\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u6355\u6349\u5230\u663e\u8457\u7684\u5dee\u5f02\u6027\u548c\u5931\u8d25\u6848\u4f8b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u4e34\u5e8a\u90e8\u7f72\u4e2d\u9700\u8981\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u7684\u5f52\u4e00\u5316\u6280\u672f\u548c\u66f4\u4e25\u683c\u7684\u9a8c\u8bc1\u534f\u8bae\u3002"}}
{"id": "2512.07833", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07833", "abs": "https://arxiv.org/abs/2512.07833", "authors": ["Thao Nguyen", "Sicheng Mo", "Krishna Kumar Singh", "Yilin Wang", "Jing Shi", "Nicholas Kolkin", "Eli Shechtman", "Yong Jae Lee", "Yuheng Li"], "title": "Relational Visual Similarity", "comment": "Project page, data, and code: https://thaoshibe.github.io/relsim", "summary": "Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u6355\u6349\u4eba\u7c7b\u611f\u77e5\u7684\u5173\u7cfb\u76f8\u4f3c\u6027\u800c\u975e\u4f20\u7edf\u7684\u5916\u89c2\u5c5e\u6027\u76f8\u4f3c\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u76f8\u4f3c\u6027\u5ea6\u91cf\u65b9\u6cd5\uff08\u5982LPIPS\u3001CLIP\u3001DINO\uff09\u53ea\u5173\u6ce8\u611f\u77e5\u5c5e\u6027\u76f8\u4f3c\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u80fd\u591f\u611f\u77e5\u7684\u4e30\u5bcc\u5173\u7cfb\u76f8\u4f3c\u6027\u3002", "method": "\u9996\u5148\u5c06\u5173\u7cfb\u56fe\u50cf\u76f8\u4f3c\u6027\u5b9a\u4e49\u4e3a\u53ef\u6d4b\u91cf\u95ee\u9898\uff0c\u7136\u540e\u6784\u5efa\u5305\u542b11.4\u4e07\u5f20\u56fe\u50cf-\u533f\u540d\u6807\u9898\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6765\u6d4b\u91cf\u56fe\u50cf\u95f4\u7684\u5173\u7cfb\u76f8\u4f3c\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u9996\u4e2a\u80fd\u591f\u57fa\u4e8e\u5173\u7cfb\u7ed3\u6784\u800c\u975e\u8868\u9762\u5916\u89c2\u6765\u8fde\u63a5\u56fe\u50cf\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u73b0\u6709\u56fe\u50cf\u76f8\u4f3c\u6027\u6a21\u578b\u5728\u6355\u6349\u5173\u7cfb\u76f8\u4f3c\u6027\u65b9\u9762\u5b58\u5728\u91cd\u8981\u7f3a\u9677\u3002", "conclusion": "\u5173\u7cfb\u76f8\u4f3c\u6027\u5177\u6709\u91cd\u8981\u73b0\u5b9e\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u5f53\u524d\u89c6\u89c9\u8ba1\u7b97\u9886\u57df\u5728\u8fd9\u65b9\u9762\u5b58\u5728\u5173\u952e\u7a7a\u767d\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u3002"}}
{"id": "2512.07469", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07469", "abs": "https://arxiv.org/abs/2512.07469", "authors": ["Xiangpeng Yang", "Ji Xie", "Yiyuan Yang", "Yan Huang", "Min Xu", "Qiang Wu"], "title": "Unified Video Editing with Temporal Reasoner", "comment": "Project Page: https://videocof.github.io/", "summary": "Existing video editing methods face a critical trade-off: expert models offer precision but rely on task-specific priors like masks, hindering unification; conversely, unified temporal in-context learning models are mask-free but lack explicit spatial cues, leading to weak instruction-to-region mapping and imprecise localization. To resolve this conflict, we propose VideoCoF, a novel Chain-of-Frames approach inspired by Chain-of-Thought reasoning. VideoCoF enforces a ``see, reason, then edit\" procedure by compelling the video diffusion model to first predict reasoning tokens (edit-region latents) before generating the target video tokens. This explicit reasoning step removes the need for user-provided masks while achieving precise instruction-to-region alignment and fine-grained video editing. Furthermore, we introduce a RoPE alignment strategy that leverages these reasoning tokens to ensure motion alignment and enable length extrapolation beyond the training duration. We demonstrate that with a minimal data cost of only 50k video pairs, VideoCoF achieves state-of-the-art performance on VideoCoF-Bench, validating the efficiency and effectiveness of our approach. Our code, weight, data are available at https://github.com/knightyxp/VideoCoF.", "AI": {"tldr": "VideoCoF\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684Chain-of-Frames\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f3a\u5236\u89c6\u9891\u6269\u6563\u6a21\u578b\u5148\u9884\u6d4b\u7f16\u8f91\u533a\u57df\u6f5c\u5728\u8868\u793a\uff0c\u518d\u8fdb\u884c\u89c6\u9891\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u7528\u6237\u63d0\u4f9b\u63a9\u7801\u7684\u7cbe\u786e\u89c6\u9891\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u9762\u4e34\u7cbe\u786e\u6027\u4e0e\u901a\u7528\u6027\u7684\u77db\u76fe\uff1a\u4e13\u5bb6\u6a21\u578b\u4f9d\u8d56\u7279\u5b9a\u4efb\u52a1\u5148\u9a8c\uff08\u5982\u63a9\u7801\uff09\uff0c\u96be\u4ee5\u7edf\u4e00\uff1b\u800c\u7edf\u4e00\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u7a7a\u95f4\u7ebf\u7d22\uff0c\u5bfc\u81f4\u6307\u4ee4\u5230\u533a\u57df\u6620\u5c04\u4e0d\u7cbe\u786e\u3002", "method": "\u53d7Chain-of-Thought\u542f\u53d1\uff0cVideoCoF\u91c7\u7528\"\u89c2\u5bdf-\u63a8\u7406-\u7f16\u8f91\"\u6d41\u7a0b\uff0c\u5f3a\u5236\u6a21\u578b\u5148\u9884\u6d4b\u7f16\u8f91\u533a\u57df\u6f5c\u5728\u8868\u793a\uff08\u63a8\u7406\u4ee4\u724c\uff09\uff0c\u518d\u751f\u6210\u76ee\u6807\u89c6\u9891\u4ee4\u724c\u3002\u540c\u65f6\u5f15\u5165RoPE\u5bf9\u9f50\u7b56\u7565\u786e\u4fdd\u8fd0\u52a8\u5bf9\u9f50\u548c\u957f\u5ea6\u5916\u63a8\u3002", "result": "\u4ec5\u4f7f\u75285\u4e07\u5bf9\u89c6\u9891\u6570\u636e\uff0cVideoCoF\u5728VideoCoF-Bench\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "VideoCoF\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u89e3\u51b3\u4e86\u89c6\u9891\u7f16\u8f91\u4e2d\u7cbe\u786e\u6027\u4e0e\u901a\u7528\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u63a9\u7801\u7684\u7cbe\u786e\u6307\u4ee4\u5230\u533a\u57df\u5bf9\u9f50\u548c\u7ec6\u7c92\u5ea6\u89c6\u9891\u7f16\u8f91\u3002"}}
{"id": "2512.07480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07480", "abs": "https://arxiv.org/abs/2512.07480", "authors": ["Naifu Xue", "Zhaoyang Jia", "Jiahao Li", "Bin Li", "Zihan Zheng", "Yuan Zhang", "Yan Lu"], "title": "Single-step Diffusion-based Video Coding with Semantic-Temporal Guidance", "comment": null, "summary": "While traditional and neural video codecs (NVCs) have achieved remarkable rate-distortion performance, improving perceptual quality at low bitrates remains challenging. Some NVCs incorporate perceptual or adversarial objectives but still suffer from artifacts due to limited generation capacity, whereas others leverage pretrained diffusion models to improve quality at the cost of heavy sampling complexity. To overcome these challenges, we propose S2VC, a Single-Step diffusion based Video Codec that integrates a conditional coding framework with an efficient single-step diffusion generator, enabling realistic reconstruction at low bitrates with reduced sampling cost. Recognizing the importance of semantic conditioning in single-step diffusion, we introduce Contextual Semantic Guidance to extract frame-adaptive semantics from buffered features. It replaces text captions with efficient, fine-grained conditioning, thereby improving generation realism. In addition, Temporal Consistency Guidance is incorporated into the diffusion U-Net to enforce temporal coherence across frames and ensure stable generation. Extensive experiments show that S2VC delivers state-of-the-art perceptual quality with an average 52.73% bitrate saving over prior perceptual methods, underscoring the promise of single-step diffusion for efficient, high-quality video compression.", "AI": {"tldr": "S2VC\u662f\u4e00\u4e2a\u57fa\u4e8e\u5355\u6b65\u6269\u6563\u7684\u89c6\u9891\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u6761\u4ef6\u7f16\u7801\u6846\u67b6\u548c\u9ad8\u6548\u7684\u5355\u6b65\u6269\u6563\u751f\u6210\u5668\uff0c\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u903c\u771f\u91cd\u5efa\u5e76\u51cf\u5c11\u91c7\u6837\u590d\u6742\u5ea6\u3002", "motivation": "\u4f20\u7edf\u548c\u795e\u7ecf\u89c6\u9891\u7f16\u89e3\u7801\u5668\u5728\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u611f\u77e5\u8d28\u91cf\u63d0\u5347\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5b58\u5728\u751f\u6210\u80fd\u529b\u9650\u5236\u5bfc\u81f4\u7684\u4f2a\u5f71\uff0c\u8981\u4e48\u4f9d\u8d56\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51faS2VC\uff0c\u7ed3\u5408\u6761\u4ef6\u7f16\u7801\u6846\u67b6\u548c\u5355\u6b65\u6269\u6563\u751f\u6210\u5668\uff1b\u5f15\u5165\u4e0a\u4e0b\u6587\u8bed\u4e49\u6307\u5bfc\u4ece\u7f13\u51b2\u7279\u5f81\u4e2d\u63d0\u53d6\u5e27\u81ea\u9002\u5e94\u8bed\u4e49\uff1b\u52a0\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u6307\u5bfc\u4ee5\u786e\u4fdd\u5e27\u95f4\u65f6\u5e8f\u8fde\u8d2f\u6027\u3002", "result": "S2VC\u5728\u611f\u77e5\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u611f\u77e5\u65b9\u6cd5\u5e73\u5747\u8282\u770152.73%\u7684\u6bd4\u7279\u7387\u3002", "conclusion": "\u5355\u6b65\u6269\u6563\u65b9\u6cd5\u4e3a\u9ad8\u6548\u9ad8\u8d28\u91cf\u89c6\u9891\u538b\u7f29\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07498", "abs": "https://arxiv.org/abs/2512.07498", "authors": ["Chih-Chung Hsu", "Shao-Ning Chen", "Chia-Ming Lee", "Yi-Fang Wang", "Yi-Shiuan Chou"], "title": "Towards Robust DeepFake Detection under Unstable Face Sequences: Adaptive Sparse Graph Embedding with Order-Free Representation and Explicit Laplacian Spectral Prior", "comment": "16 pages (including appendix)", "summary": "Ensuring the authenticity of video content remains challenging as DeepFake generation becomes increasingly realistic and robust against detection. Most existing detectors implicitly assume temporally consistent and clean facial sequences, an assumption that rarely holds in real-world scenarios where compression artifacts, occlusions, and adversarial attacks destabilize face detection and often lead to invalid or misdetected faces. To address these challenges, we propose a Laplacian-Regularized Graph Convolutional Network (LR-GCN) that robustly detects DeepFakes from noisy or unordered face sequences, while being trained only on clean facial data. Our method constructs an Order-Free Temporal Graph Embedding (OF-TGE) that organizes frame-wise CNN features into an adaptive sparse graph based on semantic affinities. Unlike traditional methods constrained by strict temporal continuity, OF-TGE captures intrinsic feature consistency across frames, making it resilient to shuffled, missing, or heavily corrupted inputs. We further impose a dual-level sparsity mechanism on both graph structure and node features to suppress the influence of invalid faces. Crucially, we introduce an explicit Graph Laplacian Spectral Prior that acts as a high-pass operator in the graph spectral domain, highlighting structural anomalies and forgery artifacts, which are then consolidated by a low-pass GCN aggregation. This sequential design effectively realizes a task-driven spectral band-pass mechanism that suppresses background information and random noise while preserving manipulation cues. Extensive experiments on FF++, Celeb-DFv2, and DFDC demonstrate that LR-GCN achieves state-of-the-art performance and significantly improved robustness under severe global and local disruptions, including missing faces, occlusions, and adversarially perturbed face detections.", "AI": {"tldr": "\u63d0\u51faLR-GCN\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u6b63\u5219\u5316\u56fe\u5377\u79ef\u7f51\u7edc\u68c0\u6d4bDeepFake\u89c6\u9891\uff0c\u80fd\u591f\u5728\u566a\u58f0\u6216\u65e0\u5e8f\u4eba\u8138\u5e8f\u5217\u4e2d\u5b9e\u73b0\u9c81\u68d2\u68c0\u6d4b\uff0c\u4ec5\u9700\u5728\u5e72\u51c0\u4eba\u8138\u6570\u636e\u4e0a\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u68c0\u6d4b\u5668\u5047\u8bbe\u65f6\u95f4\u4e00\u81f4\u4e14\u5e72\u51c0\u7684\u4eba\u8138\u5e8f\u5217\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u4e2d\u5b58\u5728\u538b\u7f29\u4f2a\u5f71\u3001\u906e\u6321\u548c\u5bf9\u6297\u653b\u51fb\uff0c\u5bfc\u81f4\u4eba\u8138\u68c0\u6d4b\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u65e0\u5e8f\u65f6\u5e8f\u56fe\u5d4c\u5165(OF-TGE)\uff0c\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u5c06\u5e27\u7ea7CNN\u7279\u5f81\u7ec4\u7ec7\u6210\u81ea\u9002\u5e94\u7a00\u758f\u56fe\uff1b\u5f15\u5165\u53cc\u91cd\u7a00\u758f\u673a\u5236\u548c\u56fe\u62c9\u666e\u62c9\u65af\u9891\u8c31\u5148\u9a8c\uff0c\u5b9e\u73b0\u9891\u8c31\u5e26\u901a\u673a\u5236\u3002", "result": "\u5728FF++\u3001Celeb-DFv2\u548cDFDC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLR-GCN\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4e25\u91cd\u5168\u5c40\u548c\u5c40\u90e8\u5e72\u6270\u4e0b\u5177\u6709\u663e\u8457\u6539\u8fdb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LR-GCN\u901a\u8fc7\u56fe\u7ed3\u6784\u548c\u9891\u8c31\u57df\u5904\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u5b9e\u573a\u666f\u4e2dDeepFake\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u4e3a\u566a\u58f0\u548c\u5e72\u6270\u6761\u4ef6\u4e0b\u7684\u89c6\u9891\u771f\u5b9e\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07500", "abs": "https://arxiv.org/abs/2512.07500", "authors": ["Penghui Liu", "Jiangshan Wang", "Yutong Shen", "Shanhui Mo", "Chenyang Qi", "Yue Ma"], "title": "MultiMotion: Multi Subject Video Motion Transfer via Video Diffusion Transformer", "comment": null, "summary": "Multi-object video motion transfer poses significant challenges for Diffusion Transformer (DiT) architectures due to inherent motion entanglement and lack of object-level control. We present MultiMotion, a novel unified framework that overcomes these limitations. Our core innovation is Maskaware Attention Motion Flow (AMF), which utilizes SAM2 masks to explicitly disentangle and control motion features for multiple objects within the DiT pipeline. Furthermore, we introduce RectPC, a high-order predictor-corrector solver for efficient and accurate sampling, particularly beneficial for multi-entity generation. To facilitate rigorous evaluation, we construct the first benchmark dataset specifically for DiT-based multi-object motion transfer. MultiMotion demonstrably achieves precise, semantically aligned, and temporally coherent motion transfer for multiple distinct objects, maintaining DiT's high quality and scalability. The code is in the supp.", "AI": {"tldr": "MultiMotion\u662f\u4e00\u4e2a\u9488\u5bf9\u591a\u5bf9\u8c61\u89c6\u9891\u8fd0\u52a8\u8fc1\u79fb\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7Mask-aware Attention Motion Flow (AMF)\u6280\u672f\u89e3\u51b3\u4e86DiT\u67b6\u6784\u4e2d\u7684\u8fd0\u52a8\u7ea0\u7f20\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86RectPC\u91c7\u6837\u5668\u63d0\u9ad8\u751f\u6210\u6548\u7387\u3002", "motivation": "\u89e3\u51b3Diffusion Transformer (DiT)\u5728\u591a\u5bf9\u8c61\u89c6\u9891\u8fd0\u52a8\u8fc1\u79fb\u4e2d\u5b58\u5728\u7684\u8fd0\u52a8\u7ea0\u7f20\u95ee\u9898\u548c\u7f3a\u4e4f\u5bf9\u8c61\u7ea7\u63a7\u5236\u7684\u5c40\u9650\u6027\u3002", "method": "\u6838\u5fc3\u521b\u65b0\u662fAMF\u6280\u672f\uff0c\u5229\u7528SAM2\u63a9\u7801\u5728DiT\u6d41\u6c34\u7ebf\u4e2d\u663e\u5f0f\u89e3\u7f20\u548c\u63a7\u5236\u591a\u4e2a\u5bf9\u8c61\u7684\u8fd0\u52a8\u7279\u5f81\uff1b\u540c\u65f6\u5f15\u5165RectPC\u9ad8\u9636\u9884\u6d4b-\u6821\u6b63\u91c7\u6837\u5668\u3002", "result": "\u6784\u5efa\u4e86\u9996\u4e2a\u9488\u5bf9DiT\u591a\u5bf9\u8c61\u8fd0\u52a8\u8fc1\u79fb\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0cMultiMotion\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u8bed\u4e49\u5bf9\u9f50\u4e14\u65f6\u95f4\u4e00\u81f4\u7684\u591a\u5bf9\u8c61\u8fd0\u52a8\u8fc1\u79fb\uff0c\u4fdd\u6301\u4e86DiT\u7684\u9ad8\u8d28\u91cf\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MultiMotion\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5bf9\u8c61\u89c6\u9891\u8fd0\u52a8\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u4e3aDiT\u67b6\u6784\u63d0\u4f9b\u4e86\u5bf9\u8c61\u7ea7\u63a7\u5236\u80fd\u529b\uff0c\u5728\u591a\u5bf9\u8c61\u751f\u6210\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.07503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07503", "abs": "https://arxiv.org/abs/2512.07503", "authors": ["Yao Teng", "Zhihuan Jiang", "Han Shi", "Xian Liu", "Xuefei Ning", "Guohao Dai", "Yu Wang", "Zhenguo Li", "Xihui Liu"], "title": "SJD++: Improved Speculative Jacobi Decoding for Training-free Acceleration of Discrete Auto-regressive Text-to-Image Generation", "comment": null, "summary": "Large autoregressive models can generate high-quality, high-resolution images but suffer from slow generation speed, because these models require hundreds to thousands of sequential forward passes for next-token prediction during inference. To accelerate autoregressive text-to-image generation, we propose Speculative Jacobi Decoding++ (SJD++), a training-free probabilistic parallel decoding algorithm. Unlike traditional next-token prediction, SJD++ performs multi-token prediction in each forward pass, drastically reducing generation steps. Specifically, it integrates the iterative multi-token prediction mechanism from Jacobi decoding, with the probabilistic drafting-and-verification mechanism from speculative sampling. More importantly, for further acceleration, SJD++ reuses high-confidence draft tokens after each verification phase instead of resampling them all. We conduct extensive experiments on several representative autoregressive text-to-image generation models and demonstrate that SJD++ achieves $2\\times$ to $3\\times$ inference latency reduction and $2\\times$ to $7\\times$ step compression, while preserving visual quality with no observable degradation.", "AI": {"tldr": "SJD++\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6982\u7387\u5e76\u884c\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u4ee4\u724c\u9884\u6d4b\u548c\u9a8c\u8bc1\u673a\u5236\uff0c\u5c06\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e2-3\u500d\uff0c\u6b65\u6570\u538b\u7f292-7\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5927\u578b\u81ea\u56de\u5f52\u6a21\u578b\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u8d28\u91cf\u597d\u4f46\u901f\u5ea6\u6162\uff0c\u9700\u8981\u6570\u767e\u5230\u6570\u5343\u6b21\u987a\u5e8f\u524d\u5411\u4f20\u9012\u8fdb\u884c\u4e0b\u4e00\u4e2a\u4ee4\u724c\u9884\u6d4b\u3002", "method": "\u7ed3\u5408Jacobi\u89e3\u7801\u7684\u8fed\u4ee3\u591a\u4ee4\u724c\u9884\u6d4b\u673a\u5236\u548c\u63a8\u6d4b\u91c7\u6837\u7684\u6982\u7387\u8349\u7a3f-\u9a8c\u8bc1\u673a\u5236\uff0c\u5728\u9a8c\u8bc1\u9636\u6bb5\u540e\u91cd\u7528\u9ad8\u7f6e\u4fe1\u5ea6\u8349\u7a3f\u4ee4\u724c\u800c\u975e\u91cd\u65b0\u91c7\u6837\u3002", "result": "\u5728\u591a\u4e2a\u4ee3\u8868\u6027\u81ea\u56de\u5f52\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e0a\uff0cSJD++\u5b9e\u73b02-3\u500d\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u548c2-7\u500d\u6b65\u6570\u538b\u7f29\uff0c\u89c6\u89c9\u8d28\u91cf\u65e0\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "SJD++\u6709\u6548\u52a0\u901f\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\uff0c\u4e3a\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07504", "abs": "https://arxiv.org/abs/2512.07504", "authors": ["Ryota Okumura", "Kaede Shiohara", "Toshihiko Yamasaki"], "title": "ControlVP: Interactive Geometric Refinement of AI-Generated Images with Consistent Vanishing Points", "comment": "Accepted to WACV 2026, 8 pages, supplementary included. Dataset and code: https://github.com/RyotaOkumura/ControlVP", "summary": "Recent text-to-image models, such as Stable Diffusion, have achieved impressive visual quality, yet they often suffer from geometric inconsistencies that undermine the structural realism of generated scenes. One prominent issue is vanishing point inconsistency, where projections of parallel lines fail to converge correctly in 2D space. This leads to structurally implausible geometry that degrades spatial realism, especially in architectural scenes. We propose ControlVP, a user-guided framework for correcting vanishing point inconsistencies in generated images. Our approach extends a pre-trained diffusion model by incorporating structural guidance derived from building contours. We also introduce geometric constraints that explicitly encourage alignment between image edges and perspective cues. Our method enhances global geometric consistency while maintaining visual fidelity comparable to the baselines. This capability is particularly valuable for applications that require accurate spatial structure, such as image-to-3D reconstruction. The dataset and source code are available at https://github.com/RyotaOkumura/ControlVP .", "AI": {"tldr": "ControlVP\u662f\u4e00\u4e2a\u7528\u6237\u5f15\u5bfc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u7ea0\u6b63\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u6d88\u5931\u70b9\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5f15\u5bfc\u548c\u51e0\u4f55\u7ea6\u675f\u6765\u63d0\u5347\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u5b58\u5728\u51e0\u4f55\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7279\u522b\u662f\u6d88\u5931\u70b9\u4e0d\u4e00\u81f4\uff0c\u8fd9\u7834\u574f\u4e86\u751f\u6210\u573a\u666f\u7684\u7ed3\u6784\u771f\u5b9e\u611f\u3002", "method": "\u6269\u5c55\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5efa\u7b51\u8f6e\u5ed3\u7684\u7ed3\u6784\u5f15\u5bfc\uff0c\u5e76\u5f15\u5165\u51e0\u4f55\u7ea6\u675f\u4ee5\u9f13\u52b1\u56fe\u50cf\u8fb9\u7f18\u4e0e\u900f\u89c6\u7ebf\u7d22\u7684\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u5168\u5c40\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u51c6\u786e\u7a7a\u95f4\u7ed3\u6784\u7684\u5e94\u7528\uff08\u5982\u56fe\u50cf\u52303D\u91cd\u5efa\uff09\u3002", "conclusion": "ControlVP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u56fe\u50cf\u4e2d\u7684\u6d88\u5931\u70b9\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7a7a\u95f4\u771f\u5b9e\u611f\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2512.07514", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07514", "abs": "https://arxiv.org/abs/2512.07514", "authors": ["Junkai Lin", "Hang Long", "Huipeng Guo", "Jielei Zhang", "JiaYi Yang", "Tianle Guo", "Yang Yang", "Jianwen Li", "Wenxiao Zhang", "Matthias Nie\u00dfner", "Wei Yang"], "title": "MeshRipple: Structured Autoregressive Generation of Artist-Meshes", "comment": null, "summary": "Meshes serve as a primary representation for 3D assets. Autoregressive mesh generators serialize faces into sequences and train on truncated segments with sliding-window inference to cope with memory limits. However, this mismatch breaks long-range geometric dependencies, producing holes and fragmented components. To address this critical limitation, we introduce MeshRipple, which expands a mesh outward from an active generation frontier, akin to a ripple on a surface.MeshRipple rests on three key innovations: a frontier-aware BFS tokenization that aligns the generation order with surface topology; an expansive prediction strategy that maintains coherent, connected surface growth; and a sparse-attention global memory that provides an effectively unbounded receptive field to resolve long-range topological dependencies.This integrated design enables MeshRipple to generate meshes with high surface fidelity and topological completeness, outperforming strong recent baselines.", "AI": {"tldr": "MeshRipple\u662f\u4e00\u79cd\u65b0\u7684\u7f51\u683c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u524d\u6cbf\u611f\u77e5\u7684BFS\u6807\u8bb0\u5316\u3001\u6269\u5c55\u9884\u6d4b\u7b56\u7565\u548c\u7a00\u758f\u6ce8\u610f\u529b\u5168\u5c40\u5185\u5b58\u6765\u89e3\u51b3\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u4e2d\u7684\u957f\u8ddd\u79bb\u51e0\u4f55\u4f9d\u8d56\u95ee\u9898\uff0c\u751f\u6210\u5177\u6709\u9ad8\u8868\u9762\u4fdd\u771f\u5ea6\u548c\u62d3\u6251\u5b8c\u6574\u6027\u7684\u7f51\u683c\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u65b9\u6cd5\u7531\u4e8e\u5185\u5b58\u9650\u5236\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u63a8\u7406\uff0c\u5bfc\u81f4\u957f\u8ddd\u79bb\u51e0\u4f55\u4f9d\u8d56\u88ab\u7834\u574f\uff0c\u4ea7\u751f\u5b54\u6d1e\u548c\u788e\u7247\u5316\u7ec4\u4ef6\u3002", "method": "1) \u524d\u6cbf\u611f\u77e5\u7684BFS\u6807\u8bb0\u5316\uff0c\u4f7f\u751f\u6210\u987a\u5e8f\u4e0e\u8868\u9762\u62d3\u6251\u5bf9\u9f50\uff1b2) \u6269\u5c55\u9884\u6d4b\u7b56\u7565\uff0c\u4fdd\u6301\u8fde\u8d2f\u3001\u8fde\u63a5\u7684\u8868\u9762\u751f\u957f\uff1b3) \u7a00\u758f\u6ce8\u610f\u529b\u5168\u5c40\u5185\u5b58\uff0c\u63d0\u4f9b\u6709\u6548\u65e0\u754c\u7684\u611f\u53d7\u91ce\u6765\u89e3\u51b3\u957f\u8ddd\u79bb\u62d3\u6251\u4f9d\u8d56\u3002", "result": "MeshRipple\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u8868\u9762\u4fdd\u771f\u5ea6\u548c\u62d3\u6251\u5b8c\u6574\u6027\u7684\u7f51\u683c\uff0c\u5728\u5f3a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MeshRipple\u7684\u96c6\u6210\u8bbe\u8ba1\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7f51\u683c\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2512.07527", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07527", "abs": "https://arxiv.org/abs/2512.07527", "authors": ["Fei Yu", "Yu Liu", "Luyang Tang", "Mingchao Sun", "Zengye Ge", "Rui Bu", "Yuchao Jin", "Haisen Zhao", "He Sun", "Yangyan Li", "Mu Xu", "Wenzheng Chen", "Baoquan Chen"], "title": "From Orbit to Ground: Generative City Photogrammetry from Extreme Off-Nadir Satellite Images", "comment": null, "summary": "City-scale 3D reconstruction from satellite imagery presents the challenge of extreme viewpoint extrapolation, where our goal is to synthesize ground-level novel views from sparse orbital images with minimal parallax. This requires inferring nearly $90^\\circ$ viewpoint gaps from image sources with severely foreshortened facades and flawed textures, causing state-of-the-art reconstruction engines such as NeRF and 3DGS to fail.\n  To address this problem, we propose two design choices tailored for city structures and satellite inputs. First, we model city geometry as a 2.5D height map, implemented as a Z-monotonic signed distance field (SDF) that matches urban building layouts from top-down viewpoints. This stabilizes geometry optimization under sparse, off-nadir satellite views and yields a watertight mesh with crisp roofs and clean, vertically extruded facades. Second, we paint the mesh appearance from satellite images via differentiable rendering techniques. While the satellite inputs may contain long-range, blurry captures, we further train a generative texture restoration network to enhance the appearance, recovering high-frequency, plausible texture details from degraded inputs.\n  Our method's scalability and robustness are demonstrated through extensive experiments on large-scale urban reconstruction. For example, in our teaser figure, we reconstruct a $4\\,\\mathrm{km}^2$ real-world region from only a few satellite images, achieving state-of-the-art performance in synthesizing photorealistic ground views. The resulting models are not only visually compelling but also serve as high-fidelity, application-ready assets for downstream tasks like urban planning and simulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u57ce\u5e02\u89c4\u6a213D\u91cd\u5efa\u7684\u65b9\u6cd5\uff0c\u901a\u8fc72.5D\u9ad8\u5ea6\u56fe\u548c\u7eb9\u7406\u6062\u590d\u7f51\u7edc\u89e3\u51b3\u4ece\u7a00\u758f\u536b\u661f\u56fe\u50cf\u5408\u6210\u5730\u9762\u89c6\u89d2\u7684\u6781\u7aef\u89c6\u89d2\u5916\u63a8\u95ee\u9898\u3002", "motivation": "\u4ece\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u57ce\u5e02\u89c4\u6a213D\u91cd\u5efa\u9762\u4e34\u6781\u7aef\u89c6\u89d2\u5916\u63a8\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5982NeRF\u548c3DGS\u5728\u7a00\u758f\u8f68\u9053\u56fe\u50cf\u548c\u4e25\u91cd\u900f\u89c6\u7f29\u77ed\u7684\u7acb\u9762\u4e0b\u65e0\u6cd5\u6709\u6548\u5de5\u4f5c\u3002", "method": "1. \u4f7f\u75282.5D\u9ad8\u5ea6\u56fe\u5efa\u6a21\u57ce\u5e02\u51e0\u4f55\uff0c\u5b9e\u73b0\u4e3aZ\u5355\u8c03\u6709\u7b26\u53f7\u8ddd\u79bb\u573a(SDF)\uff1b2. \u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u6280\u672f\u4ece\u536b\u661f\u56fe\u50cf\u7ed8\u5236\u7f51\u683c\u5916\u89c2\uff1b3. \u8bad\u7ec3\u751f\u6210\u5f0f\u7eb9\u7406\u6062\u590d\u7f51\u7edc\u589e\u5f3a\u5916\u89c2\u7ec6\u8282\u3002", "result": "\u57284\u5e73\u65b9\u516c\u91cc\u771f\u5b9e\u533a\u57df\u4e0a\u4ec5\u4f7f\u7528\u5c11\u91cf\u536b\u661f\u56fe\u50cf\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u903c\u771f\u5730\u9762\u89c6\u56fe\u5408\u6210\uff0c\u751f\u6210\u5177\u6709\u6e05\u6670\u5c4b\u9876\u548c\u5782\u76f4\u7acb\u9762\u6c34\u5bc6\u7f51\u683c\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u57ce\u5e02\u91cd\u5efa\u4e2d\u5c55\u73b0\u51fa\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u751f\u6210\u7684\u9ad8\u4fdd\u771f\u6a21\u578b\u53ef\u76f4\u63a5\u7528\u4e8e\u57ce\u5e02\u89c4\u5212\u3001\u4eff\u771f\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2512.07568", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.07568", "abs": "https://arxiv.org/abs/2512.07568", "authors": ["Xuecheng Li", "Weikuan Jia", "Alisher Kurbonaliev", "Qurbonaliev Alisher", "Khudzhamkulov Rustam", "Ismoilov Shuhratjon", "Eshmatov Javhariddin", "Yuanjie Zheng"], "title": "Dual-Stream Cross-Modal Representation Learning via Residual Semantic Decorrelation", "comment": null, "summary": "Cross-modal learning has become a fundamental paradigm for integrating heterogeneous information sources such as images, text, and structured attributes. However, multimodal representations often suffer from modality dominance, redundant information coupling, and spurious cross-modal correlations, leading to suboptimal generalization and limited interpretability. In particular, high-variance modalities tend to overshadow weaker but semantically important signals, while na\u00efve fusion strategies entangle modality-shared and modality-specific factors in an uncontrolled manner. This makes it difficult to understand which modality actually drives a prediction and to maintain robustness when some modalities are noisy or missing. To address these challenges, we propose a Dual-Stream Residual Semantic Decorrelation Network (DSRSD-Net), a simple yet effective framework that disentangles modality-specific and modality-shared information through residual decomposition and explicit semantic decorrelation constraints. DSRSD-Net introduces: (1) a dual-stream representation learning module that separates intra-modal (private) and inter-modal (shared) latent factors via residual projection; (2) a residual semantic alignment head that maps shared factors from different modalities into a common space using a combination of contrastive and regression-style objectives; and (3) a decorrelation and orthogonality loss that regularizes the covariance structure of the shared space while enforcing orthogonality between shared and private streams, thereby suppressing cross-modal redundancy and preventing feature collapse. Experimental results on two large-scale educational benchmarks demonstrate that DSRSD-Net consistently improves next-step prediction and final outcome prediction over strong single-modality, early-fusion, late-fusion, and co-attention baselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDSRSD-Net\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u5206\u89e3\u548c\u8bed\u4e49\u53bb\u76f8\u5173\u7ea6\u675f\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u6a21\u6001\u4e3b\u5bfc\u3001\u4fe1\u606f\u5197\u4f59\u548c\u865a\u5047\u76f8\u5173\u7b49\u95ee\u9898\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u8868\u793a\u5e38\u9762\u4e34\u6a21\u6001\u4e3b\u5bfc\u3001\u5197\u4f59\u4fe1\u606f\u8026\u5408\u548c\u865a\u5047\u8de8\u6a21\u6001\u76f8\u5173\u6027\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3002\u9ad8\u65b9\u5dee\u6a21\u6001\u5bb9\u6613\u63a9\u76d6\u8bed\u4e49\u91cd\u8981\u4fe1\u53f7\uff0c\u800c\u7b80\u5355\u878d\u5408\u7b56\u7565\u4f1a\u65e0\u63a7\u5236\u5730\u7ea0\u7f20\u6a21\u6001\u5171\u4eab\u548c\u6a21\u6001\u7279\u5b9a\u56e0\u7d20\u3002", "method": "DSRSD-Net\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u53cc\u6d41\u8868\u793a\u5b66\u4e60\u6a21\u5757\u901a\u8fc7\u6b8b\u5dee\u6295\u5f71\u5206\u79bb\u6a21\u6001\u5185\u548c\u6a21\u6001\u95f4\u6f5c\u5728\u56e0\u7d20\uff1b(2)\u6b8b\u5dee\u8bed\u4e49\u5bf9\u9f50\u5934\u7ed3\u5408\u5bf9\u6bd4\u548c\u56de\u5f52\u76ee\u6807\u5c06\u4e0d\u540c\u6a21\u6001\u7684\u5171\u4eab\u56e0\u7d20\u6620\u5c04\u5230\u5171\u540c\u7a7a\u95f4\uff1b(3)\u53bb\u76f8\u5173\u548c\u6b63\u4ea4\u6027\u635f\u5931\u6b63\u5219\u5316\u5171\u4eab\u7a7a\u95f4\u7684\u534f\u65b9\u5dee\u7ed3\u6784\uff0c\u540c\u65f6\u5f3a\u5236\u5171\u4eab\u6d41\u548c\u79c1\u6709\u6d41\u4e4b\u95f4\u7684\u6b63\u4ea4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u6559\u80b2\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDSRSD-Net\u5728\u4e0b\u4e00\u6b65\u9884\u6d4b\u548c\u6700\u7ec8\u7ed3\u679c\u9884\u6d4b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u5355\u6a21\u6001\u3001\u65e9\u671f\u878d\u5408\u3001\u665a\u671f\u878d\u5408\u548c\u5171\u540c\u6ce8\u610f\u529b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DSRSD-Net\u901a\u8fc7\u6709\u6548\u7684\u6a21\u6001\u4fe1\u606f\u89e3\u8026\u548c\u8bed\u4e49\u53bb\u76f8\u5173\u7ea6\u675f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u548c\u6a21\u578b\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.07580", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07580", "abs": "https://arxiv.org/abs/2512.07580", "authors": ["Yahong Wang", "Juncheng Wu", "Zhangkai Ni", "Longzhen Yang", "Yihang Liu", "Chengmei Yang", "Ying Wen", "Xianfeng Tang", "Hui Liu", "Yuyin Zhou", "Lianghua He"], "title": "All You Need Are Random Visual Tokens? Demystifying Token Pruning in VLLMs", "comment": null, "summary": "Vision Large Language Models (VLLMs) incur high computational costs due to their reliance on hundreds of visual tokens to represent images. While token pruning offers a promising solution for accelerating inference, this paper, however, identifies a key observation: in deeper layers (e.g., beyond the 20th), existing training-free pruning methods perform no better than random pruning. We hypothesize that this degradation is caused by \"vanishing token information\", where visual tokens progressively lose their salience with increasing network depth. To validate this hypothesis, we quantify a token's information content by measuring the change in the model output probabilities upon its removal. Using this proposed metric, our analysis of the information of visual tokens across layers reveals three key findings: (1) As layers deepen, the information of visual tokens gradually becomes uniform and eventually vanishes at an intermediate layer, which we term as \"information horizon\", beyond which the visual tokens become redundant; (2) The position of this horizon is not static; it extends deeper for visually intensive tasks, such as Optical Character Recognition (OCR), compared to more general tasks like Visual Question Answering (VQA); (3) This horizon is also strongly correlated with model capacity, as stronger VLLMs (e.g., Qwen2.5-VL) employ deeper visual tokens than weaker models (e.g., LLaVA-1.5). Based on our findings, we show that simple random pruning in deep layers efficiently balances performance and efficiency. Moreover, integrating random pruning consistently enhances existing methods. Using DivPrune with random pruning achieves state-of-the-art results, maintaining 96.9% of Qwen-2.5-VL-7B performance while pruning 50% of visual tokens. The code will be publicly available at https://github.com/YahongWang1/Information-Horizon.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5c42\u5b58\u5728'\u4fe1\u606f\u6d88\u5931'\u73b0\u8c61\uff0c\u63d0\u51fa\u4fe1\u606f\u5730\u5e73\u7ebf\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u6df1\u5c42\u968f\u673a\u526a\u679d\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u5728\u6df1\u5c42\u8868\u73b0\u4e0d\u4f73\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u89c6\u89c9\u4ee4\u724c\u4fe1\u606f\u968f\u7f51\u7edc\u6df1\u5ea6\u589e\u52a0\u800c\u9010\u6e10\u6d88\u5931\u5bfc\u81f4\u7684\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u4ee4\u724c\u4fe1\u606f\u542b\u91cf\uff08\u79fb\u9664\u4ee4\u724c\u540e\u6a21\u578b\u8f93\u51fa\u6982\u7387\u7684\u53d8\u5316\uff09\uff0c\u5206\u6790\u89c6\u89c9\u4ee4\u724c\u4fe1\u606f\u5728\u6df1\u5ea6\u5c42\u4e2d\u7684\u5206\u5e03\u89c4\u5f8b\uff0c\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u5730\u5e73\u7ebf\u7684\u968f\u673a\u526a\u679d\u7b56\u7565\u3002", "result": "\u53d1\u73b0\u4fe1\u606f\u5730\u5e73\u7ebf\u73b0\u8c61\uff0c\u6df1\u5c42\u968f\u673a\u526a\u679d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002DivPrune\u7ed3\u5408\u968f\u673a\u526a\u679d\u53ef\u526a\u966450%\u89c6\u89c9\u4ee4\u724c\uff0c\u4fdd\u6301Qwen-2.5-VL-7B 96.9%\u7684\u6027\u80fd\u3002", "conclusion": "\u89c6\u89c9\u4ee4\u724c\u5728\u6df1\u5c42\u53d8\u5f97\u5197\u4f59\uff0c\u968f\u673a\u526a\u679d\u662f\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002\u4fe1\u606f\u5730\u5e73\u7ebf\u4f4d\u7f6e\u53d6\u51b3\u4e8e\u4efb\u52a1\u96be\u5ea6\u548c\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2512.07584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07584", "abs": "https://arxiv.org/abs/2512.07584", "authors": ["Meituan LongCat Team", "Hanghang Ma", "Haoxian Tan", "Jiale Huang", "Junqiang Wu", "Jun-Yan He", "Lishuai Gao", "Songlin Xiao", "Xiaoming Wei", "Xiaoqi Ma", "Xunliang Cai", "Yayong Guan", "Jie Hu"], "title": "LongCat-Image Technical Report", "comment": null, "summary": "We introduce LongCat-Image, a pioneering open-source and bilingual (Chinese-English) foundation model for image generation, designed to address core challenges in multilingual text rendering, photorealism, deployment efficiency, and developer accessibility prevalent in current leading models. 1) We achieve this through rigorous data curation strategies across the pre-training, mid-training, and SFT stages, complemented by the coordinated use of curated reward models during the RL phase. This strategy establishes the model as a new state-of-the-art (SOTA), delivering superior text-rendering capabilities and remarkable photorealism, and significantly enhancing aesthetic quality. 2) Notably, it sets a new industry standard for Chinese character rendering. By supporting even complex and rare characters, it outperforms both major open-source and commercial solutions in coverage, while also achieving superior accuracy. 3) The model achieves remarkable efficiency through its compact design. With a core diffusion model of only 6B parameters, it is significantly smaller than the nearly 20B or larger Mixture-of-Experts (MoE) architectures common in the field. This ensures minimal VRAM usage and rapid inference, significantly reducing deployment costs. Beyond generation, LongCat-Image also excels in image editing, achieving SOTA results on standard benchmarks with superior editing consistency compared to other open-source works. 4) To fully empower the community, we have established the most comprehensive open-source ecosystem to date. We are releasing not only multiple model versions for text-to-image and image editing, including checkpoints after mid-training and post-training stages, but also the entire toolchain of training procedure. We believe that the openness of LongCat-Image will provide robust support for developers and researchers, pushing the frontiers of visual content creation.", "AI": {"tldr": "LongCat-Image\u662f\u4e00\u4e2a\u5f00\u521b\u6027\u7684\u5f00\u6e90\u53cc\u8bed\uff08\u4e2d\u82f1\uff09\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6570\u636e\u7b56\u7565\u548c\u7d27\u51d1\u8bbe\u8ba1\uff0c\u5728\u6587\u672c\u6e32\u67d3\u3001\u771f\u5b9e\u611f\u3001\u90e8\u7f72\u6548\u7387\u7b49\u65b9\u9762\u8fbe\u5230\u65b0SOTA\u6c34\u5e73\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5728\u591a\u8bed\u8a00\u6587\u672c\u6e32\u67d3\u3001\u7167\u7247\u771f\u5b9e\u611f\u3001\u90e8\u7f72\u6548\u7387\u548c\u5f00\u53d1\u8005\u53ef\u8bbf\u95ee\u6027\u65b9\u9762\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u5728\u9884\u8bad\u7ec3\u3001\u4e2d\u671f\u8bad\u7ec3\u548cSFT\u9636\u6bb5\u91c7\u7528\u4e25\u683c\u7684\u6570\u636e\u7b5b\u9009\u7b56\u7565\uff0c\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u6a21\u578b\u8fdb\u884cRL\u9636\u6bb5\u8bad\u7ec3\uff1b\u91c7\u7528\u4ec56B\u53c2\u6570\u7684\u7d27\u51d1\u6269\u6563\u6a21\u578b\u8bbe\u8ba1\u3002", "result": "\u5728\u6587\u672c\u6e32\u67d3\u80fd\u529b\u3001\u7167\u7247\u771f\u5b9e\u611f\u548c\u7f8e\u5b66\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u65b0SOTA\uff1b\u5728\u4e2d\u6587\u6c49\u5b57\u6e32\u67d3\u65b9\u9762\u5efa\u7acb\u65b0\u884c\u4e1a\u6807\u51c6\uff1b\u5728\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u4e5f\u8fbe\u5230SOTA\u7ed3\u679c\uff0c\u5177\u6709\u4f18\u8d8a\u7684\u7f16\u8f91\u4e00\u81f4\u6027\u3002", "conclusion": "LongCat-Image\u901a\u8fc7\u5176\u5168\u9762\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u4e3a\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5f3a\u5927\u652f\u6301\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u5185\u5bb9\u521b\u4f5c\u7684\u524d\u6cbf\u53d1\u5c55\u3002"}}
{"id": "2512.07590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07590", "abs": "https://arxiv.org/abs/2512.07590", "authors": ["Kaili Qi", "Zhongyi Huang", "Wenli Yang"], "title": "Robust Variational Model Based Tailored UNet: Leveraging Edge Detector and Mean Curvature for Improved Image Segmentation", "comment": null, "summary": "To address the challenge of segmenting noisy images with blurred or fragmented boundaries, this paper presents a robust version of Variational Model Based Tailored UNet (VM_TUNet), a hybrid framework that integrates variational methods with deep learning. The proposed approach incorporates physical priors, an edge detector and a mean curvature term, into a modified Cahn-Hilliard equation, aiming to combine the interpretability and boundary-smoothing advantages of variational partial differential equations (PDEs) with the strong representational ability of deep neural networks. The architecture consists of two collaborative modules: an F module, which conducts efficient frequency domain preprocessing to alleviate poor local minima, and a T module, which ensures accurate and stable local computations, backed by a stability estimate. Extensive experiments on three benchmark datasets indicate that the proposed method achieves a balanced trade-off between performance and computational efficiency, which yields competitive quantitative results and improved visual quality compared to pure convolutional neural network (CNN) based models, while achieving performance close to that of transformer-based method with reasonable computational expense.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7248\u672c\u7684VM_TUNet\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u53d8\u5206\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6765\u5206\u5272\u566a\u58f0\u56fe\u50cf\uff0c\u901a\u8fc7\u5f15\u5165\u7269\u7406\u5148\u9a8c\u3001\u8fb9\u7f18\u68c0\u6d4b\u5668\u548c\u5e73\u5747\u66f2\u7387\u9879\uff0c\u5728\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u566a\u58f0\u56fe\u50cf\u5206\u5272\u95ee\u9898\uff0c\u7279\u522b\u662f\u8fb9\u754c\u6a21\u7cca\u6216\u65ad\u88c2\u7684\u60c5\u51b5\uff0c\u65e8\u5728\u7ed3\u5408\u53d8\u5206PDE\u7684\u53ef\u89e3\u91ca\u6027\u548c\u8fb9\u754c\u5e73\u6ed1\u4f18\u52bf\u4e0e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5f3a\u5927\u8868\u793a\u80fd\u529b\u3002", "method": "\u63d0\u51faVM_TUNet\u6df7\u5408\u6846\u67b6\uff0c\u5305\u542bF\u6a21\u5757\uff08\u9891\u57df\u9884\u5904\u7406\uff09\u548cT\u6a21\u5757\uff08\u5c40\u90e8\u8ba1\u7b97\uff09\uff0c\u5728\u6539\u8fdb\u7684Cahn-Hilliard\u65b9\u7a0b\u4e2d\u5f15\u5165\u7269\u7406\u5148\u9a8c\u3001\u8fb9\u7f18\u68c0\u6d4b\u5668\u548c\u5e73\u5747\u66f2\u7387\u9879\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u76f8\u6bd4\u7eafCNN\u6a21\u578b\u83b7\u5f97\u7ade\u4e89\u6027\u5b9a\u91cf\u7ed3\u679c\u548c\u66f4\u597d\u7684\u89c6\u89c9\u8d28\u91cf\uff0c\u540c\u65f6\u4ee5\u5408\u7406\u8ba1\u7b97\u6210\u672c\u8fbe\u5230\u63a5\u8fd1\u57fa\u4e8eTransformer\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u53d8\u5206\u65b9\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u4e3a\u566a\u58f0\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07599", "abs": "https://arxiv.org/abs/2512.07599", "authors": ["Hanshi Wang", "Zijian Cai", "Jin Gao", "Yiwei Zhang", "Weiming Hu", "Ke Wang", "Zhipeng Zhang"], "title": "Online Segment Any 3D Thing as Instance Tracking", "comment": "NeurIPS 2025, Code is at https://github.com/AutoLab-SAI-SJTU/AutoSeg3D", "summary": "Online, real-time, and fine-grained 3D segmentation constitutes a fundamental capability for embodied intelligent agents to perceive and comprehend their operational environments. Recent advancements employ predefined object queries to aggregate semantic information from Vision Foundation Models (VFMs) outputs that are lifted into 3D point clouds, facilitating spatial information propagation through inter-query interactions. Nevertheless, perception is an inherently dynamic process, rendering temporal understanding a critical yet overlooked dimension within these prevailing query-based pipelines. Therefore, to further unlock the temporal environmental perception capabilities of embodied agents, our work reconceptualizes online 3D segmentation as an instance tracking problem (AutoSeg3D). Our core strategy involves utilizing object queries for temporal information propagation, where long-term instance association promotes the coherence of features and object identities, while short-term instance update enriches instant observations. Given that viewpoint variations in embodied robotics often lead to partial object visibility across frames, this mechanism aids the model in developing a holistic object understanding beyond incomplete instantaneous views. Furthermore, we introduce spatial consistency learning to mitigate the fragmentation problem inherent in VFMs, yielding more comprehensive instance information for enhancing the efficacy of both long-term and short-term temporal learning. The temporal information exchange and consistency learning facilitated by these sparse object queries not only enhance spatial comprehension but also circumvent the computational burden associated with dense temporal point cloud interactions. Our method establishes a new state-of-the-art, surpassing ESAM by 2.8 AP on ScanNet200 and delivering consistent gains on ScanNet, SceneNN, and 3RScan datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAutoSeg3D\u65b9\u6cd5\uff0c\u5c06\u5728\u7ebf3D\u5206\u5272\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5b9e\u4f8b\u8ddf\u8e2a\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u8c61\u67e5\u8be2\u5b9e\u73b0\u65f6\u7a7a\u4fe1\u606f\u4f20\u64ad\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u67e5\u8be2\u76843D\u5206\u5272\u65b9\u6cd5\u5ffd\u89c6\u4e86\u65f6\u95f4\u7ef4\u5ea6\u7684\u91cd\u8981\u6027\uff0c\u800c\u611f\u77e5\u662f\u4e00\u4e2a\u52a8\u6001\u8fc7\u7a0b\uff0c\u9700\u8981\u65f6\u95f4\u7406\u89e3\u80fd\u529b\u3002\u673a\u5668\u4eba\u89c6\u89d2\u53d8\u5316\u5bfc\u81f4\u7269\u4f53\u90e8\u5206\u53ef\u89c1\u6027\uff0c\u9700\u8981\u8d85\u8d8a\u77ac\u65f6\u89c6\u56fe\u7684\u5b8c\u6574\u7269\u4f53\u7406\u89e3\u3002", "method": "1) \u5c06\u5728\u7ebf3D\u5206\u5272\u91cd\u6784\u4e3a\u5b9e\u4f8b\u8ddf\u8e2a\u95ee\u9898\uff1b2) \u4f7f\u7528\u5bf9\u8c61\u67e5\u8be2\u8fdb\u884c\u65f6\u95f4\u4fe1\u606f\u4f20\u64ad\uff1a\u957f\u671f\u5b9e\u4f8b\u5173\u8054\u4fdd\u6301\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u77ed\u671f\u5b9e\u4f8b\u66f4\u65b0\u4e30\u5bcc\u77ac\u65f6\u89c2\u6d4b\uff1b3) \u5f15\u5165\u7a7a\u95f4\u4e00\u81f4\u6027\u5b66\u4e60\u7f13\u89e3VFMs\u7684\u788e\u7247\u5316\u95ee\u9898\u3002", "result": "\u5728ScanNet200\u4e0a\u8d85\u8d8aESAM\u65b9\u6cd52.8 AP\uff0c\u5728ScanNet\u3001SceneNN\u548c3RScan\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff0c\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "conclusion": "\u901a\u8fc7\u7a00\u758f\u5bf9\u8c61\u67e5\u8be2\u5b9e\u73b0\u7684\u65f6\u95f4\u4fe1\u606f\u4ea4\u6362\u548c\u4e00\u81f4\u6027\u5b66\u4e60\u4e0d\u4ec5\u589e\u5f3a\u4e86\u7a7a\u95f4\u7406\u89e3\uff0c\u8fd8\u907f\u514d\u4e86\u5bc6\u96c6\u65f6\u95f4\u70b9\u4e91\u4ea4\u4e92\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u89e3\u9501\u4e86\u66f4\u5f3a\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2512.07606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07606", "abs": "https://arxiv.org/abs/2512.07606", "authors": ["Jingna Qiu", "Frauke Wilm", "Mathias \u00d6ttl", "Jonas Utz", "Maja Schlereth", "Moritz Schillinger", "Marc Aubreville", "Katharina Breininger"], "title": "Decomposition Sampling for Efficient Region Annotations in Active Learning", "comment": null, "summary": "Active learning improves annotation efficiency by selecting the most informative samples for annotation and model training. While most prior work has focused on selecting informative images for classification tasks, we investigate the more challenging setting of dense prediction, where annotations are more costly and time-intensive, especially in medical imaging. Region-level annotation has been shown to be more efficient than image-level annotation for these tasks. However, existing methods for representative annotation region selection suffer from high computational and memory costs, irrelevant region choices, and heavy reliance on uncertainty sampling. We propose decomposition sampling (DECOMP), a new active learning sampling strategy that addresses these limitations. It enhances annotation diversity by decomposing images into class-specific components using pseudo-labels and sampling regions from each class. Class-wise predictive confidence further guides the sampling process, ensuring that difficult classes receive additional annotations. Across ROI classification, 2-D segmentation, and 3-D segmentation, DECOMP consistently surpasses baseline methods by better sampling minority-class regions and boosting performance on these challenging classes. Code is in https://github.com/JingnaQiu/DECOMP.git.", "AI": {"tldr": "DECOMP\u662f\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u7c7b\u522b\u7279\u5b9a\u7ec4\u4ef6\u5e76\u57fa\u4e8e\u7c7b\u522b\u7f6e\u4fe1\u5ea6\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u4e2d\u533a\u57df\u7ea7\u6ce8\u91ca\u7684\u9ad8\u6210\u672c\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff08\u7279\u522b\u662f\u533b\u5b66\u5f71\u50cf\uff09\u4e2d\u6ce8\u91ca\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u533a\u57df\u9009\u62e9\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u533a\u57df\u9009\u62e9\u4e0d\u76f8\u5173\u3001\u8fc7\u5ea6\u4f9d\u8d56\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u7b49\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4f2a\u6807\u7b7e\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u7c7b\u522b\u7279\u5b9a\u7ec4\u4ef6\uff0c\u4ece\u6bcf\u4e2a\u7c7b\u522b\u4e2d\u91c7\u6837\u533a\u57df\uff0c\u5e76\u901a\u8fc7\u7c7b\u522b\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6307\u5bfc\u91c7\u6837\u8fc7\u7a0b\uff0c\u786e\u4fdd\u56f0\u96be\u7c7b\u522b\u83b7\u5f97\u66f4\u591a\u6ce8\u91ca\u3002", "result": "\u5728ROI\u5206\u7c7b\u30012D\u5206\u5272\u548c3D\u5206\u5272\u4efb\u52a1\u4e2d\uff0cDECOMP\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u91c7\u6837\u5c11\u6570\u7c7b\u533a\u57df\u5e76\u63d0\u5347\u8fd9\u4e9b\u56f0\u96be\u7c7b\u522b\u7684\u6027\u80fd\u3002", "conclusion": "DECOMP\u901a\u8fc7\u589e\u5f3a\u6ce8\u91ca\u591a\u6837\u6027\u548c\u9488\u5bf9\u6027\u91c7\u6837\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\u7684\u6ce8\u91ca\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\u65f6\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.07628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07628", "abs": "https://arxiv.org/abs/2512.07628", "authors": ["Zhiqi Li", "Wenhuan Li", "Tengfei Wang", "Zhenwei Wang", "Junta Wu", "Haoyuan Wang", "Yunhan Yang", "Zehuan Huang", "Yang Li", "Peidong Liu", "Chunchao Guo"], "title": "MoCA: Mixture-of-Components Attention for Scalable Compositional 3D Generation", "comment": null, "summary": "Compositionality is critical for 3D object and scene generation, but existing part-aware 3D generation methods suffer from poor scalability due to quadratic global attention costs when increasing the number of components. In this work, we present MoCA, a compositional 3D generative model with two key designs: (1) importance-based component routing that selects top-k relevant components for sparse global attention, and (2) unimportant components compression that preserve contextual priors of unselected components while reducing computational complexity of global attention. With these designs, MoCA enables efficient, fine-grained compositional 3D asset creation with scalable number of components. Extensive experiments show MoCA outperforms baselines on both compositional object and scene generation tasks. Project page: https://lizhiqi49.github.io/MoCA", "AI": {"tldr": "MoCA\u662f\u4e00\u4e2a\u7ec4\u5408\u5f0f3D\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u7ec4\u4ef6\u8def\u7531\u548c\u672a\u9009\u7ec4\u4ef6\u538b\u7f29\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u589e\u52a0\u7ec4\u4ef6\u6570\u91cf\u65f6\u56e0\u5168\u5c40\u6ce8\u610f\u529b\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u800c\u5bfc\u81f4\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u90e8\u5206\u76843D\u751f\u6210\u65b9\u6cd5\u5728\u589e\u52a0\u7ec4\u4ef6\u6570\u91cf\u65f6\uff0c\u7531\u4e8e\u5168\u5c40\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u4e8c\u6b21\u65b9\u6210\u672c\uff0c\u5bfc\u81f4\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5173\u952e\u8bbe\u8ba1\uff1a(1) \u57fa\u4e8e\u91cd\u8981\u6027\u7684\u7ec4\u4ef6\u8def\u7531\uff0c\u9009\u62e9\u524dk\u4e2a\u76f8\u5173\u7ec4\u4ef6\u8fdb\u884c\u7a00\u758f\u5168\u5c40\u6ce8\u610f\u529b\u8ba1\u7b97\uff1b(2) \u4e0d\u91cd\u8981\u7ec4\u4ef6\u538b\u7f29\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u4fdd\u7559\u672a\u9009\u7ec4\u4ef6\u7684\u4e0a\u4e0b\u6587\u5148\u9a8c\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoCA\u5728\u7ec4\u5408\u5f0f\u7269\u4f53\u548c\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "MoCA\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7ec6\u7c92\u5ea6\u7684\u7ec4\u5408\u5f0f3D\u8d44\u6e90\u521b\u5efa\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u7ec4\u4ef6\u6570\u91cf\u3002"}}
{"id": "2512.07651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07651", "abs": "https://arxiv.org/abs/2512.07651", "authors": ["Yuanye Liu", "Hanxiao Zhang", "Nannan Shi", "Yuxin Shi", "Arif Mahmood", "Murtaza Taj", "Xiahai Zhuang"], "title": "Liver Fibrosis Quantification and Analysis: The LiQA Dataset and Baseline Method", "comment": null, "summary": "Liver fibrosis represents a significant global health burden, necessitating accurate staging for effective clinical management. This report introduces the LiQA (Liver Fibrosis Quantification and Analysis) dataset, established as part of the CARE 2024 challenge. Comprising $440$ patients with multi-phase, multi-center MRI scans, the dataset is curated to benchmark algorithms for Liver Segmentation (LiSeg) and Liver Fibrosis Staging (LiFS) under complex real-world conditions, including domain shifts, missing modalities, and spatial misalignment. We further describe the challenge's top-performing methodology, which integrates a semi-supervised learning framework with external data for robust segmentation, and utilizes a multi-view consensus approach with Class Activation Map (CAM)-based regularization for staging. Evaluation of this baseline demonstrates that leveraging multi-source data and anatomical constraints significantly enhances model robustness in clinical settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86LiQA\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u809d\u810f\u5206\u5272\u548c\u7ea4\u7ef4\u5316\u5206\u671f\u7684\u591a\u4e2d\u5fc3MRI\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u548c\u591a\u89c6\u56fe\u5171\u8bc6\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u809d\u810f\u7ea4\u7ef4\u5316\u662f\u5168\u7403\u91cd\u5927\u5065\u5eb7\u8d1f\u62c5\uff0c\u9700\u8981\u51c6\u786e\u5206\u671f\u4ee5\u8fdb\u884c\u6709\u6548\u4e34\u5e8a\u7ba1\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u73b0\u5b9e\u6761\u4ef6\u4e0b\uff08\u5982\u57df\u504f\u79fb\u3001\u6a21\u6001\u7f3a\u5931\u548c\u7a7a\u95f4\u9519\u4f4d\uff09\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\u5916\u90e8\u6570\u636e\u8fdb\u884c\u9c81\u68d2\u5206\u5272\uff0c\u91c7\u7528\u591a\u89c6\u56fe\u5171\u8bc6\u65b9\u6cd5\u548c\u57fa\u4e8e\u7c7b\u6fc0\u6d3b\u56fe\uff08CAM\uff09\u7684\u6b63\u5219\u5316\u8fdb\u884c\u5206\u671f\u3002", "result": "\u8bc4\u4f30\u8868\u660e\uff0c\u5229\u7528\u591a\u6e90\u6570\u636e\u548c\u89e3\u5256\u7ea6\u675f\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LiQA\u6570\u636e\u96c6\u4e3a\u809d\u810f\u7ea4\u7ef4\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u590d\u6742\u4e34\u5e8a\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.07652", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07652", "abs": "https://arxiv.org/abs/2512.07652", "authors": ["Hamad Almazrouei", "Mariam Al Nasseri", "Maha Alzaabi"], "title": "An AI-Powered Autonomous Underwater System for Sea Exploration and Scientific Research", "comment": null, "summary": "Traditional sea exploration faces significant challenges due to extreme conditions, limited visibility, and high costs, resulting in vast unexplored ocean regions. This paper presents an innovative AI-powered Autonomous Underwater Vehicle (AUV) system designed to overcome these limitations by automating underwater object detection, analysis, and reporting. The system integrates YOLOv12 Nano for real-time object detection, a Convolutional Neural Network (CNN) (ResNet50) for feature extraction, Principal Component Analysis (PCA) for dimensionality reduction, and K-Means++ clustering for grouping marine objects based on visual characteristics. Furthermore, a Large Language Model (LLM) (GPT-4o Mini) is employed to generate structured reports and summaries of underwater findings, enhancing data interpretation. The system was trained and evaluated on a combined dataset of over 55,000 images from the DeepFish and OzFish datasets, capturing diverse Australian marine environments. Experimental results demonstrate the system's capability to detect marine objects with a mAP@0.5 of 0.512, a precision of 0.535, and a recall of 0.438. The integration of PCA effectively reduced feature dimensionality while preserving 98% variance, facilitating K-Means clustering which successfully grouped detected objects based on visual similarities. The LLM integration proved effective in generating insightful summaries of detections and clusters, supported by location data. This integrated approach significantly reduces the risks associated with human diving, increases mission efficiency, and enhances the speed and depth of underwater data analysis, paving the way for more effective scientific research and discovery in challenging marine environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684AI\u9a71\u52a8\u81ea\u4e3b\u6c34\u4e0b\u822a\u884c\u5668\u7cfb\u7edf\uff0c\u901a\u8fc7\u96c6\u6210YOLOv12 Nano\u3001CNN\u3001PCA\u3001K-Means++\u548cGPT-4o Mini\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u6c34\u4e0b\u7269\u4f53\u7684\u5b9e\u65f6\u68c0\u6d4b\u3001\u7279\u5f81\u63d0\u53d6\u3001\u964d\u7ef4\u805a\u7c7b\u548c\u667a\u80fd\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u6d77\u6d0b\u52d8\u63a2\u9762\u4e34\u6781\u7aef\u6761\u4ef6\u3001\u80fd\u89c1\u5ea6\u4f4e\u548c\u6210\u672c\u9ad8\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u5927\u91cf\u6d77\u6d0b\u533a\u57df\u672a\u88ab\u63a2\u7d22\u3002\u9700\u8981\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u964d\u4f4e\u4eba\u7c7b\u6f5c\u6c34\u98ce\u9669\uff0c\u63d0\u9ad8\u52d8\u63a2\u6548\u7387\u3002", "method": "\u7cfb\u7edf\u96c6\u6210YOLOv12 Nano\u8fdb\u884c\u5b9e\u65f6\u7269\u4f53\u68c0\u6d4b\uff0cResNet50\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0cPCA\u964d\u7ef4\uff0cK-Means++\u805a\u7c7b\uff0cGPT-4o Mini\u751f\u6210\u7ed3\u6784\u5316\u62a5\u544a\u3002\u5728\u5305\u542b55,000\u591a\u5f20\u56fe\u50cf\u7684DeepFish\u548cOzFish\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u5728\u6d77\u6d0b\u7269\u4f53\u68c0\u6d4b\u4e0a\u8fbe\u5230mAP@0.5\u4e3a0.512\uff0c\u7cbe\u786e\u5ea60.535\uff0c\u53ec\u56de\u73870.438\u3002PCA\u964d\u7ef4\u4fdd\u755998%\u65b9\u5dee\uff0cK-Means\u805a\u7c7b\u6210\u529f\u6309\u89c6\u89c9\u7279\u5f81\u5206\u7ec4\uff0cLLM\u80fd\u751f\u6210\u6709\u6d1e\u5bdf\u529b\u7684\u62a5\u544a\u3002", "conclusion": "\u8be5\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4eba\u7c7b\u6f5c\u6c34\u98ce\u9669\uff0c\u63d0\u9ad8\u4efb\u52a1\u6548\u7387\uff0c\u589e\u5f3a\u6c34\u4e0b\u6570\u636e\u5206\u6790\u7684\u6df1\u5ea6\u548c\u901f\u5ea6\uff0c\u4e3a\u6311\u6218\u6027\u6d77\u6d0b\u73af\u5883\u4e2d\u7684\u79d1\u5b66\u7814\u7a76\u5f00\u8f9f\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.07661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07661", "abs": "https://arxiv.org/abs/2512.07661", "authors": ["Shiaho Li", "Naisheng Ye", "Tianyu Li", "Kashyap Chitta", "Tuo An", "Peng Su", "Boyang Wang", "Haiou Liu", "Chen Lv", "Hongyang Li"], "title": "Optimization-Guided Diffusion for Interactive Scene Generation", "comment": null, "summary": "Realistic and diverse multi-agent driving scenes are crucial for evaluating autonomous vehicles, but safety-critical events which are essential for this task are rare and underrepresented in driving datasets. Data-driven scene generation offers a low-cost alternative by synthesizing complex traffic behaviors from existing driving logs. However, existing models often lack controllability or yield samples that violate physical or social constraints, limiting their usability. We present OMEGA, an optimization-guided, training-free framework that enforces structural consistency and interaction awareness during diffusion-based sampling from a scene generation model. OMEGA re-anchors each reverse diffusion step via constrained optimization, steering the generation towards physically plausible and behaviorally coherent trajectories. Building on this framework, we formulate ego-attacker interactions as a game-theoretic optimization in the distribution space, approximating Nash equilibria to generate realistic, safety-critical adversarial scenarios. Experiments on nuPlan and Waymo show that OMEGA improves generation realism, consistency, and controllability, increasing the ratio of physically and behaviorally valid scenes from 32.35% to 72.27% for free exploration capabilities, and from 11% to 80% for controllability-focused generation. Our approach can also generate $5\\times$ more near-collision frames with a time-to-collision under three seconds while maintaining the overall scene realism.", "AI": {"tldr": "OMEGA\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f18\u5316\u7684\u8bad\u7ec3\u81ea\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u5728\u6269\u6563\u91c7\u6837\u8fc7\u7a0b\u4e2d\u589e\u5f3a\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u4ea4\u4e92\u610f\u8bc6\uff0c\u751f\u6210\u7269\u7406\u5408\u7406\u4e14\u884c\u4e3a\u4e00\u81f4\u7684\u4ea4\u901a\u573a\u666f\uff0c\u7279\u522b\u9488\u5bf9\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u9a7e\u9a76\u573a\u666f\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u53ef\u63a7\u6027\uff0c\u751f\u6210\u7684\u6837\u672c\u7ecf\u5e38\u8fdd\u53cd\u7269\u7406\u6216\u793e\u4f1a\u7ea6\u675f\uff0c\u5bfc\u81f4\u53ef\u7528\u6027\u53d7\u9650\u3002\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\u5728\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u4e2d\u7f55\u89c1\u4e14\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u63d0\u51faOMEGA\u6846\u67b6\uff0c\u5728\u6269\u6563\u6a21\u578b\u7684\u9006\u6269\u6563\u6b65\u9aa4\u4e2d\u901a\u8fc7\u7ea6\u675f\u4f18\u5316\u91cd\u65b0\u951a\u5b9a\uff0c\u786e\u4fdd\u751f\u6210\u7684\u8f68\u8ff9\u7269\u7406\u5408\u7406\u4e14\u884c\u4e3a\u4e00\u81f4\u3002\u5c06ego-attacker\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5206\u5e03\u7a7a\u95f4\u4e2d\u7684\u535a\u5f08\u8bba\u4f18\u5316\u95ee\u9898\uff0c\u8fd1\u4f3c\u7eb3\u4ec0\u5747\u8861\u6765\u751f\u6210\u5b89\u5168\u5173\u952e\u5bf9\u6297\u573a\u666f\u3002", "result": "\u5728nuPlan\u548cWaymo\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cOMEGA\u5c06\u7269\u7406\u548c\u884c\u4e3a\u6709\u6548\u573a\u666f\u7684\u6bd4\u4f8b\u4ece32.35%\u63d0\u5347\u523072.27%\uff08\u81ea\u7531\u63a2\u7d22\uff09\uff0c\u4ece11%\u63d0\u5347\u523080%\uff08\u53ef\u63a7\u751f\u6210\uff09\uff0c\u5e76\u80fd\u751f\u62105\u500d\u591a\u7684\u8fd1\u78b0\u649e\u5e27\uff08TTC<3\u79d2\uff09\u540c\u65f6\u4fdd\u6301\u573a\u666f\u771f\u5b9e\u6027\u3002", "conclusion": "OMEGA\u901a\u8fc7\u4f18\u5316\u5f15\u5bfc\u7684\u6269\u6563\u91c7\u6837\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u73b0\u5b9e\u6027\u3001\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2512.07668", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07668", "abs": "https://arxiv.org/abs/2512.07668", "authors": ["Ronan John", "Aditya Kesari", "Vincenzo DiMatteo", "Kristin Dana"], "title": "EgoCampus: Egocentric Pedestrian Eye Gaze Model and Dataset", "comment": null, "summary": "We address the challenge of predicting human visual attention during real-world navigation by measuring and modeling egocentric pedestrian eye gaze in an outdoor campus setting. We introduce the EgoCampus dataset, which spans 25 unique outdoor paths over 6 km across a university campus with recordings from more than 80 distinct human pedestrians, resulting in a diverse set of gaze-annotated videos. The system used for collection, Meta's Project Aria glasses, integrates eye tracking, front-facing RGB cameras, inertial sensors, and GPS to provide rich data from the human perspective. Unlike many prior egocentric datasets that focus on indoor tasks or exclude eye gaze information, our work emphasizes visual attention while subjects walk in outdoor campus paths. Using this data, we develop EgoCampusNet, a novel method to predict eye gaze of navigating pedestrians as they move through outdoor environments. Our contributions provide both a new resource for studying real-world attention and a resource for future work in gaze prediction models for navigation. Dataset and code are available upon request, and will be made publicly available at a later date at https://github.com/ComputerVisionRutgers/EgoCampus .", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86EgoCampus\u6570\u636e\u96c6\u548cEgoCampusNet\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u884c\u4eba\u5728\u6237\u5916\u6821\u56ed\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u771f\u5b9e\u4e16\u754c\u5bfc\u822a\u4e2d\u9884\u6d4b\u4eba\u7c7b\u89c6\u89c9\u6ce8\u610f\u529b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6237\u5916\u6821\u56ed\u73af\u5883\u4e2d\u7684\u884c\u4eba\u773c\u52a8\u6ce8\u89c6\u3002", "method": "\u4f7f\u7528Meta's Project Aria\u773c\u955c\u6536\u96c6\u6570\u636e\uff0c\u6574\u5408\u773c\u52a8\u8ffd\u8e2a\u3001RGB\u76f8\u673a\u3001\u60ef\u6027\u4f20\u611f\u5668\u548cGPS\uff0c\u5f00\u53d1\u4e86EgoCampusNet\u65b9\u6cd5\u6765\u9884\u6d4b\u884c\u4eba\u5bfc\u822a\u65f6\u7684\u773c\u52a8\u6ce8\u89c6\u3002", "result": "\u521b\u5efa\u4e86EgoCampus\u6570\u636e\u96c6\uff0c\u5305\u542b25\u6761\u72ec\u7279\u6237\u5916\u8def\u5f84\u30016\u516c\u91cc\u8ddd\u79bb\u300180\u591a\u540d\u884c\u4eba\u7684\u773c\u52a8\u6807\u6ce8\u89c6\u9891\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7814\u7a76\u771f\u5b9e\u4e16\u754c\u6ce8\u610f\u529b\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\uff0c\u5e76\u4e3a\u5bfc\u822a\u4e2d\u7684\u6ce8\u89c6\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u63d0\u4f9b\u3002"}}
{"id": "2512.07674", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07674", "abs": "https://arxiv.org/abs/2512.07674", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "DIST-CLIP: Arbitrary Metadata and Image Guided MRI Harmonization via Disentangled Anatomy-Contrast Representations", "comment": null, "summary": "Deep learning holds immense promise for transforming medical image analysis, yet its clinical generalization remains profoundly limited. A major barrier is data heterogeneity. This is particularly true in Magnetic Resonance Imaging, where scanner hardware differences, diverse acquisition protocols, and varying sequence parameters introduce substantial domain shifts that obscure underlying biological signals. Data harmonization methods aim to reduce these instrumental and acquisition variability, but existing approaches remain insufficient. When applied to imaging data, image-based harmonization approaches are often restricted by the need for target images, while existing text-guided methods rely on simplistic labels that fail to capture complex acquisition details or are typically restricted to datasets with limited variability, failing to capture the heterogeneity of real-world clinical environments. To address these limitations, we propose DIST-CLIP (Disentangled Style Transfer with CLIP Guidance), a unified framework for MRI harmonization that flexibly uses either target images or DICOM metadata for guidance. Our framework explicitly disentangles anatomical content from image contrast, with the contrast representations being extracted using pre-trained CLIP encoders. These contrast embeddings are then integrated into the anatomical content via a novel Adaptive Style Transfer module. We trained and evaluated DIST-CLIP on diverse real-world clinical datasets, and showed significant improvements in performance when compared against state-of-the-art methods in both style translation fidelity and anatomical preservation, offering a flexible solution for style transfer and standardizing MRI data. Our code and weights will be made publicly available upon publication.", "AI": {"tldr": "DIST-CLIP\u662f\u4e00\u4e2a\u57fa\u4e8eCLIP\u6307\u5bfc\u7684\u89e3\u8026\u98ce\u683c\u8fc1\u79fb\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3MRI\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\uff0c\u652f\u6301\u56fe\u50cf\u6216DICOM\u5143\u6570\u636e\u4f5c\u4e3a\u5f15\u5bfc\u8f93\u5165\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u98ce\u683c\u8f6c\u6362\u3002", "motivation": "\u73b0\u6709MRI\u6570\u636e\u534f\u8c03\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u56fe\u50cf\u7684\u65b9\u6cd5\u9700\u8981\u76ee\u6807\u56fe\u50cf\uff0c\u57fa\u4e8e\u6587\u672c\u7684\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5316\u7684\u6807\u7b7e\u4e14\u65e0\u6cd5\u6355\u6349\u590d\u6742\u91c7\u96c6\u7ec6\u8282\u3002\u4e34\u5e8aMRI\u6570\u636e\u56e0\u626b\u63cf\u4eea\u786c\u4ef6\u5dee\u5f02\u3001\u91c7\u96c6\u534f\u8bae\u591a\u6837\u6027\u548c\u5e8f\u5217\u53c2\u6570\u53d8\u5316\u5bfc\u81f4\u663e\u8457\u7684\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faDIST-CLIP\u6846\u67b6\uff0c\u660e\u786e\u89e3\u8026\u89e3\u5256\u5185\u5bb9\u548c\u56fe\u50cf\u5bf9\u6bd4\u5ea6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3CLIP\u7f16\u7801\u5668\u63d0\u53d6\u5bf9\u6bd4\u5ea6\u8868\u793a\uff0c\u901a\u8fc7\u65b0\u578b\u81ea\u9002\u5e94\u98ce\u683c\u8f6c\u79fb\u6a21\u5757\u5c06\u5bf9\u6bd4\u5ea6\u5d4c\u5165\u6574\u5408\u5230\u89e3\u5256\u5185\u5bb9\u4e2d\u3002", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u98ce\u683c\u8f6c\u6362\u4fdd\u771f\u5ea6\u548c\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u65b9\u9762\u5747\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "DIST-CLIP\u4e3aMRI\u6570\u636e\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u7075\u6d3b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u98ce\u683c\u8f6c\u79fb\u548c\u6807\u51c6\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u548c\u6743\u91cd\u5c06\u5728\u53d1\u8868\u540e\u516c\u5f00\u3002"}}
{"id": "2512.07702", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07702", "abs": "https://arxiv.org/abs/2512.07702", "authors": ["Sangha Park", "Eunji Kim", "Yeongtak Oh", "Jooyoung Choi", "Sungroh Yoon"], "title": "Guiding What Not to Generate: Automated Negative Prompting for Text-Image Alignment", "comment": "WACV 2026", "summary": "Despite substantial progress in text-to-image generation, achieving precise text-image alignment remains challenging, particularly for prompts with rich compositional structure or imaginative elements. To address this, we introduce Negative Prompting for Image Correction (NPC), an automated pipeline that improves alignment by identifying and applying negative prompts that suppress unintended content. We begin by analyzing cross-attention patterns to explain why both targeted negatives-those directly tied to the prompt's alignment error-and untargeted negatives-tokens unrelated to the prompt but present in the generated image-can enhance alignment. To discover useful negatives, NPC generates candidate prompts using a verifier-captioner-proposer framework and ranks them with a salient text-space score, enabling effective selection without requiring additional image synthesis. On GenEval++ and Imagine-Bench, NPC outperforms strong baselines, achieving 0.571 vs. 0.371 on GenEval++ and the best overall performance on Imagine-Bench. By guiding what not to generate, NPC provides a principled, fully automated route to stronger text-image alignment in diffusion models. Code is released at https://github.com/wiarae/NPC.", "AI": {"tldr": "NPC\u662f\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u8d1f\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u5e94\u7528\u6291\u5236\u610f\u5916\u5185\u5bb9\u7684\u8d1f\u63d0\u793a\u6765\u6539\u8fdb\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50", "motivation": "\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7cbe\u786e\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u4e30\u5bcc\u7ec4\u5408\u7ed3\u6784\u6216\u60f3\u8c61\u5143\u7d20\u7684\u63d0\u793a", "method": "\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5f0f\u5206\u6790\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668-\u6807\u6ce8\u5668-\u63d0\u8bae\u5668\u6846\u67b6\u751f\u6210\u5019\u9009\u8d1f\u63d0\u793a\uff0c\u5e76\u4f7f\u7528\u663e\u8457\u6587\u672c\u7a7a\u95f4\u8bc4\u5206\u8fdb\u884c\u6392\u5e8f", "result": "\u5728GenEval++\u4e0a\u8fbe\u52300.571\uff08vs 0.371\u57fa\u7ebf\uff09\uff0c\u5728Imagine-Bench\u4e0a\u83b7\u5f97\u6700\u4f73\u6574\u4f53\u6027\u80fd", "conclusion": "NPC\u901a\u8fc7\u6307\u5bfc\u6a21\u578b\u4e0d\u751f\u6210\u4ec0\u4e48\u5185\u5bb9\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u3001\u5168\u81ea\u52a8\u7684\u5f3a\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8def\u5f84"}}
{"id": "2512.07712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07712", "abs": "https://arxiv.org/abs/2512.07712", "authors": ["Sayak Dutta", "Harish Katti", "Shashikant Verma", "Shanmuganathan Raman"], "title": "UnCageNet: Tracking and Pose Estimation of Caged Animal", "comment": "9 pages, 2 figures, 2 tables. Accepted to the Indian Conference on Computer Vision, Graphics, and Image Processing (ICVGIP 2025), Mandi, India", "summary": "Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u9884\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u7b3c\u5b50\u5206\u5272\u3001\u5185\u5bb9\u611f\u77e5\u4fee\u590d\u548c\u8bc4\u4f30\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u7269\u8ffd\u8e2a\u548c\u59ff\u6001\u4f30\u8ba1\u7cfb\u7edf\u5728\u7b3c\u5b50\u906e\u6321\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u52a8\u7269\u8ffd\u8e2a\u548c\u59ff\u6001\u4f30\u8ba1\u7cfb\u7edf\uff08\u5982STEP\u3001ViTPose\uff09\u5728\u5904\u7406\u542b\u6709\u7b3c\u5b50\u7ed3\u6784\u548c\u7cfb\u7edf\u6027\u906e\u6321\u7684\u56fe\u50cf\u89c6\u9891\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u4e09\u9636\u6bb5\u9884\u5904\u7406\u6d41\u6c34\u7ebf\uff1a1\uff09\u4f7f\u7528Gabor\u589e\u5f3a\u7684ResNet-UNet\u67b6\u6784\u8fdb\u884c\u7b3c\u5b50\u5206\u5272\uff1b2\uff09\u4f7f\u7528CRFill\u8fdb\u884c\u5185\u5bb9\u611f\u77e5\u7684\u7b3c\u5b50\u4fee\u590d\uff1b3\uff09\u5728\u4fee\u590d\u540e\u7684\u65e0\u7b3c\u5e27\u4e0a\u8bc4\u4f30\u59ff\u6001\u4f30\u8ba1\u548c\u8ffd\u8e2a\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u901a\u8fc7\u8be5\u6d41\u6c34\u7ebf\u79fb\u9664\u7b3c\u5b50\u906e\u6321\u540e\uff0c\u59ff\u6001\u4f30\u8ba1\u548c\u8ffd\u8e2a\u6027\u80fd\u53ef\u8fbe\u5230\u65e0\u906e\u6321\u73af\u5883\u7684\u6c34\u5e73\uff0c\u5173\u952e\u70b9\u68c0\u6d4b\u7cbe\u5ea6\u548c\u8f68\u8ff9\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u9884\u5904\u7406\u6d41\u6c34\u7ebf\u80fd\u6709\u6548\u514b\u670d\u7b3c\u5b50\u906e\u6321\u5bf9\u52a8\u7269\u8ffd\u8e2a\u548c\u59ff\u6001\u4f30\u8ba1\u7cfb\u7edf\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07720", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07720", "abs": "https://arxiv.org/abs/2512.07720", "authors": ["Fan Yang", "Heyuan Li", "Peihao Li", "Weihao Yuan", "Lingteng Qiu", "Chaoyue Song", "Cheng Chen", "Yisheng He", "Shifeng Zhang", "Xiaoguang Han", "Steven Hoi", "Guosheng Lin"], "title": "ViSA: 3D-Aware Video Shading for Real-Time Upper-Body Avatar Creation", "comment": null, "summary": "Generating high-fidelity upper-body 3D avatars from one-shot input image remains a significant challenge. Current 3D avatar generation methods, which rely on large reconstruction models, are fast and capable of producing stable body structures, but they often suffer from artifacts such as blurry textures and stiff, unnatural motion. In contrast, generative video models show promising performance by synthesizing photorealistic and dynamic results, but they frequently struggle with unstable behavior, including body structural errors and identity drift. To address these limitations, we propose a novel approach that combines the strengths of both paradigms. Our framework employs a 3D reconstruction model to provide robust structural and appearance priors, which in turn guides a real-time autoregressive video diffusion model for rendering. This process enables the model to synthesize high-frequency, photorealistic details and fluid dynamics in real time, effectively reducing texture blur and motion stiffness while preventing the structural inconsistencies common in video generation methods. By uniting the geometric stability of 3D reconstruction with the generative capabilities of video models, our method produces high-fidelity digital avatars with realistic appearance and dynamic, temporally coherent motion. Experiments demonstrate that our approach significantly reduces artifacts and achieves substantial improvements in visual quality over leading methods, providing a robust and efficient solution for real-time applications such as gaming and virtual reality. Project page: https://lhyfst.github.io/visa", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u91cd\u5efa\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u4e0a\u534a\u8eab3D\u865a\u62df\u5f62\u8c61\u3002\u8be5\u65b9\u6cd5\u901a\u8fc73D\u91cd\u5efa\u63d0\u4f9b\u7ed3\u6784\u5148\u9a8c\uff0c\u6307\u5bfc\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9e\u65f6\u6e32\u67d3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7eb9\u7406\u6a21\u7cca\u3001\u8fd0\u52a8\u50f5\u786c\u548c\u7ed3\u6784\u4e0d\u7a33\u5b9a\u65b9\u9762\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d3D\u865a\u62df\u5f62\u8c61\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u57fa\u4e8e\u5927\u91cd\u5efa\u6a21\u578b\u7684\u65b9\u6cd5\u867d\u7136\u901f\u5ea6\u5feb\u3001\u7ed3\u6784\u7a33\u5b9a\uff0c\u4f46\u5e38\u51fa\u73b0\u7eb9\u7406\u6a21\u7cca\u548c\u8fd0\u52a8\u50f5\u786c\uff1b\u800c\u751f\u6210\u5f0f\u89c6\u9891\u6a21\u578b\u80fd\u4ea7\u751f\u903c\u771f\u52a8\u6001\u6548\u679c\uff0c\u4f46\u5bb9\u6613\u51fa\u73b0\u8eab\u4f53\u7ed3\u6784\u9519\u8bef\u548c\u8eab\u4efd\u6f02\u79fb\u7b49\u4e0d\u7a33\u5b9a\u884c\u4e3a\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u6765\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u878d\u5408\u6846\u67b6\uff1a\u4f7f\u75283D\u91cd\u5efa\u6a21\u578b\u63d0\u4f9b\u7a33\u5065\u7684\u7ed3\u6784\u548c\u5916\u89c2\u5148\u9a8c\uff0c\u8fd9\u4e9b\u5148\u9a8c\u6307\u5bfc\u4e00\u4e2a\u5b9e\u65f6\u81ea\u56de\u5f52\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6e32\u67d3\u3002\u901a\u8fc7\u5c063D\u91cd\u5efa\u7684\u51e0\u4f55\u7a33\u5b9a\u6027\u4e0e\u89c6\u9891\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u865a\u62df\u5f62\u8c61\u7684\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4f2a\u5f71\uff0c\u5728\u89c6\u89c9\u8d28\u91cf\u4e0a\u76f8\u6bd4\u9886\u5148\u65b9\u6cd5\u6709\u5b9e\u8d28\u6027\u63d0\u5347\u3002\u80fd\u591f\u5408\u6210\u9ad8\u9891\u3001\u903c\u771f\u7684\u7ec6\u8282\u548c\u6d41\u7545\u52a8\u6001\uff0c\u540c\u65f6\u907f\u514d\u4e86\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u7ed3\u6784\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u54083D\u91cd\u5efa\u548c\u89c6\u9891\u751f\u6210\u7684\u5404\u81ea\u4f18\u52bf\uff0c\u4e3a\u5b9e\u65f6\u5e94\u7528\uff08\u5982\u6e38\u620f\u548c\u865a\u62df\u73b0\u5b9e\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u5177\u6709\u903c\u771f\u5916\u89c2\u548c\u52a8\u6001\u65f6\u5e8f\u4e00\u81f4\u8fd0\u52a8\u7684\u9ad8\u4fdd\u771f\u6570\u5b57\u865a\u62df\u5f62\u8c61\u3002"}}
{"id": "2512.07729", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07729", "abs": "https://arxiv.org/abs/2512.07729", "authors": ["Aidas Aglinskas", "Stefano Anzellotti"], "title": "Improving action classification with brain-inspired deep networks", "comment": null, "summary": "Action recognition is also key for applications ranging from robotics to healthcare monitoring. Action information can be extracted from the body pose and movements, as well as from the background scene. However, the extent to which deep neural networks (DNNs) make use of information about the body and information about the background remains unclear. Since these two sources of information may be correlated within a training dataset, DNNs might learn to rely predominantly on one of them, without taking full advantage of the other. Unlike DNNs, humans have domain-specific brain regions selective for perceiving bodies, and regions selective for perceiving scenes. The present work tests whether humans are thus more effective at extracting information from both body and background, and whether building brain-inspired deep network architectures with separate domain-specific streams for body and scene perception endows them with more human-like performance. We first demonstrate that DNNs trained using the HAA500 dataset perform almost as accurately on versions of the stimuli that show both body and background and on versions of the stimuli from which the body was removed, but are at chance-level for versions of the stimuli from which the background was removed. Conversely, human participants (N=28) can recognize the same set of actions accurately with all three versions of the stimuli, and perform significantly better on stimuli that show only the body than on stimuli that show only the background. Finally, we implement and test a novel architecture patterned after domain specificity in the brain with separate streams to process body and background information. We show that 1) this architecture improves action recognition performance, and 2) its accuracy across different versions of the stimuli follows a pattern that matches more closely the pattern of accuracy observed in human participants.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u4eba\u7c7b\u5728\u52a8\u4f5c\u8bc6\u522b\u4e2d\u5bf9\u8eab\u4f53\u548c\u80cc\u666f\u4fe1\u606f\u7684\u5229\u7528\u5dee\u5f02\uff0c\u53d1\u73b0DNNs\u4e3b\u8981\u4f9d\u8d56\u80cc\u666f\u4fe1\u606f\uff0c\u800c\u4eba\u7c7b\u80fd\u540c\u65f6\u6709\u6548\u5229\u7528\u8eab\u4f53\u548c\u80cc\u666f\u4fe1\u606f\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u7814\u7a76\u8005\u6784\u5efa\u4e86\u6a21\u62df\u5927\u8111\u9886\u57df\u7279\u5f02\u6027\u5904\u7406\u7684\u53cc\u6d41\u7f51\u7edc\u67b6\u6784\uff0c\u63d0\u9ad8\u4e86\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u5e76\u4f7f\u5176\u8868\u73b0\u66f4\u63a5\u8fd1\u4eba\u7c7b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u52a8\u4f5c\u8bc6\u522b\u4efb\u52a1\u4e2d\u5982\u4f55\u5229\u7528\u8eab\u4f53\u59ff\u6001\u4fe1\u606f\u548c\u80cc\u666f\u573a\u666f\u4fe1\u606f\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5229\u7528\u65b9\u5f0f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5dee\u5f02\u3002\u7531\u4e8eDNNs\u53ef\u80fd\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u8fc7\u5ea6\u4f9d\u8d56\u76f8\u5173\u4fe1\u606f\u8f83\u5f3a\u7684\u7279\u5f81\uff08\u5982\u80cc\u666f\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u8eab\u4f53\u4fe1\u606f\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5927\u8111\u7684\u9886\u57df\u7279\u5f02\u6027\u5904\u7406\u673a\u5236\u6765\u6539\u8fdbDNNs\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u90e8\u5206\uff1a1\uff09\u4f7f\u7528HAA500\u6570\u636e\u96c6\u8bad\u7ec3\u6807\u51c6DNNs\uff0c\u6d4b\u8bd5\u5176\u5728\u5b8c\u6574\u523a\u6fc0\u3001\u79fb\u9664\u8eab\u4f53\u7684\u523a\u6fc0\u548c\u79fb\u9664\u80cc\u666f\u7684\u523a\u6fc0\u4e0a\u7684\u8868\u73b0\uff1b2\uff09\u8fdb\u884c\u4eba\u7c7b\u884c\u4e3a\u5b9e\u9a8c\uff08N=28\uff09\uff0c\u6bd4\u8f83\u4eba\u7c7b\u5728\u4e09\u79cd\u523a\u6fc0\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff1b3\uff09\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u65b0\u578b\u53cc\u6d41\u7f51\u7edc\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u6a21\u62df\u5927\u8111\u7684\u9886\u57df\u7279\u5f02\u6027\uff0c\u5206\u522b\u5904\u7406\u8eab\u4f53\u4fe1\u606f\u548c\u80cc\u666f\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u6807\u51c6DNNs\u5728\u5b8c\u6574\u523a\u6fc0\u548c\u79fb\u9664\u8eab\u4f53\u7684\u523a\u6fc0\u4e0a\u8868\u73b0\u76f8\u4f3c\u4e14\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u79fb\u9664\u80cc\u666f\u7684\u523a\u6fc0\u4e0a\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\uff1b2\uff09\u4eba\u7c7b\u5728\u6240\u6709\u4e09\u79cd\u523a\u6fc0\u6761\u4ef6\u4e0b\u90fd\u80fd\u51c6\u786e\u8bc6\u522b\u52a8\u4f5c\uff0c\u4e14\u5728\u4ec5\u663e\u793a\u8eab\u4f53\u7684\u523a\u6fc0\u4e0a\u8868\u73b0\u4f18\u4e8e\u4ec5\u663e\u793a\u80cc\u666f\u7684\u523a\u6fc0\uff1b3\uff09\u65b0\u578b\u53cc\u6d41\u67b6\u6784\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0c\u5176\u5728\u4e0d\u540c\u523a\u6fc0\u6761\u4ef6\u4e0b\u7684\u51c6\u786e\u7387\u6a21\u5f0f\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u8868\u73b0\u6a21\u5f0f\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\uff0c\u4eba\u7c7b\u5728\u52a8\u4f5c\u8bc6\u522b\u4e2d\u80fd\u66f4\u5747\u8861\u5730\u5229\u7528\u8eab\u4f53\u548c\u80cc\u666f\u4fe1\u606f\uff0c\u800c\u6807\u51c6DNNs\u8fc7\u5ea6\u4f9d\u8d56\u80cc\u666f\u4fe1\u606f\u3002\u901a\u8fc7\u6a21\u62df\u5927\u8111\u7684\u9886\u57df\u7279\u5f02\u6027\u5904\u7406\u673a\u5236\u6784\u5efa\u7684\u53cc\u6d41\u7f51\u7edc\u67b6\u6784\uff0c\u80fd\u591f\u4f7fDNNs\u7684\u8868\u73b0\u66f4\u52a0\u4eba\u7c7b\u5316\uff0c\u63d0\u9ad8\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u667a\u80fd\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2512.07730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07730", "abs": "https://arxiv.org/abs/2512.07730", "authors": ["Sangha Park", "Seungryong Yoo", "Jisoo Mok", "Sungroh Yoon"], "title": "SAVE: Sparse Autoencoder-Driven Visual Information Enhancement for Mitigating Object Hallucination", "comment": "WACV 2026", "summary": "Although Multimodal Large Language Models (MLLMs) have advanced substantially, they remain vulnerable to object hallucination caused by language priors and visual information loss. To address this, we propose SAVE (Sparse Autoencoder-Driven Visual Information Enhancement), a framework that mitigates hallucination by steering the model along Sparse Autoencoder (SAE) latent features. A binary object-presence question-answering probe identifies the SAE features most indicative of the model's visual information processing, referred to as visual understanding features. Steering the model along these identified features reinforces grounded visual understanding and effectively reduces hallucination. With its simple design, SAVE outperforms state-of-the-art training-free methods on standard benchmarks, achieving a 10\\%p improvement in CHAIR\\_S and consistent gains on POPE and MMHal-Bench. Extensive evaluations across multiple models and layers confirm the robustness and generalizability of our approach. Further analysis reveals that steering along visual understanding features suppresses the generation of uncertain object tokens and increases attention to image tokens, mitigating hallucination. Code is released at https://github.com/wiarae/SAVE.", "AI": {"tldr": "SAVE\u6846\u67b6\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u7279\u5f81\u5f15\u5bfc\uff0c\u6709\u6548\u7f13\u89e3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7531\u8bed\u8a00\u5148\u9a8c\u548c\u89c6\u89c9\u4fe1\u606f\u4e22\u5931\u5bfc\u81f4\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5", "method": "\u63d0\u51faSAVE\u6846\u67b6\uff0c\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522b\u89c6\u89c9\u7406\u89e3\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u7279\u5f81\u5f15\u5bfc\u589e\u5f3a\u6a21\u578b\u7684\u89c6\u89c9\u57fa\u7840\u7406\u89e3\u80fd\u529b", "result": "\u5728CHAIR_S\u6307\u6807\u4e0a\u63d0\u534710%\uff0c\u5728POPE\u548cMMHal-Bench\u57fa\u51c6\u4e0a\u6301\u7eed\u83b7\u5f97\u6539\u8fdb\uff0c\u8bc1\u660e\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027", "conclusion": "SAVE\u901a\u8fc7\u5f15\u5bfc\u89c6\u89c9\u7406\u89e3\u7279\u5f81\uff0c\u6709\u6548\u6291\u5236\u4e0d\u786e\u5b9a\u7269\u4f53\u6807\u8bb0\u7684\u751f\u6210\uff0c\u589e\u5f3a\u5bf9\u56fe\u50cf\u6807\u8bb0\u7684\u5173\u6ce8\uff0c\u4e3a\u7f13\u89e3\u591a\u6a21\u6001\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.07733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07733", "abs": "https://arxiv.org/abs/2512.07733", "authors": ["Meng Cao", "Xingyu Li", "Xue Liu", "Ian Reid", "Xiaodan Liang"], "title": "SpatialDreamer: Incentivizing Spatial Reasoning via Active Mental Imagery", "comment": null, "summary": "Despite advancements in Multi-modal Large Language Models (MLLMs) for scene understanding, their performance on complex spatial reasoning tasks requiring mental simulation remains significantly limited. Current methods often rely on passive observation of spatial data, failing to internalize an active mental imagery process. To bridge this gap, we propose SpatialDreamer, a reinforcement learning framework that enables spatial reasoning through a closedloop process of active exploration, visual imagination via a world model, and evidence-grounded reasoning. To address the lack of fine-grained reward supervision in longhorizontal reasoning tasks, we propose Geometric Policy Optimization (GeoPO), which introduces tree-structured sampling and step-level reward estimation with geometric consistency constraints. Extensive experiments demonstrate that SpatialDreamer delivers highly competitive results across multiple challenging benchmarks, signifying a critical advancement in human-like active spatial mental simulation for MLLMs.", "AI": {"tldr": "SpatialDreamer\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a2\u7d22\u3001\u89c6\u89c9\u60f3\u8c61\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u6765\u89e3\u51b3MLLMs\u5728\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u5fc3\u7406\u6a21\u62df\u7684\u590d\u6742\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u4e3b\u8981\u4f9d\u8d56\u88ab\u52a8\u89c2\u5bdf\u7a7a\u95f4\u6570\u636e\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u7684\u5fc3\u7406\u610f\u8c61\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faSpatialDreamer\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e3b\u52a8\u63a2\u7d22\u3001\u4e16\u754c\u6a21\u578b\u7684\u89c6\u89c9\u60f3\u8c61\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u63a8\u7406\u95ed\u73af\u8fc7\u7a0b\u3002\u4e3a\u89e3\u51b3\u957f\u5e8f\u5217\u63a8\u7406\u4efb\u52a1\u4e2d\u7ec6\u7c92\u5ea6\u5956\u52b1\u76d1\u7763\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u51e0\u4f55\u7b56\u7565\u4f18\u5316(GeoPO)\uff0c\u5f15\u5165\u6811\u7ed3\u6784\u91c7\u6837\u548c\u5e26\u51e0\u4f55\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u6b65\u7ea7\u5956\u52b1\u4f30\u8ba1\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpatialDreamer\u53d6\u5f97\u4e86\u6781\u5177\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6807\u5fd7\u7740MLLMs\u5728\u7c7b\u4eba\u4e3b\u52a8\u7a7a\u95f4\u5fc3\u7406\u6a21\u62df\u65b9\u9762\u7684\u5173\u952e\u8fdb\u5c55\u3002"}}
{"id": "2512.07738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07738", "abs": "https://arxiv.org/abs/2512.07738", "authors": ["Dengjia Zhang", "Charles Weng", "Katherine Guerrerio", "Yi Lu", "Kenton Murray", "Alexander Martin", "Reno Kriz", "Benjamin Van Durme"], "title": "HLTCOE Evaluation Team at TREC 2025: VQA Track", "comment": "7 pages, 1 figure", "summary": "The HLTCOE Evaluation team participated in TREC VQA's Answer Generation (AG) task, for which we developed a listwise learning framework that aims to improve semantic precision and ranking consistency in answer generation. Given a video-question pair, a base multimodal model first generates multiple candidate answers, which are then reranked using a model trained with a novel Masked Pointer Cross-Entropy Loss with Rank Weights. This objective integrates pointer-based candidate selection, rank-dependent weighting, and masked cross-entropy under vocabulary restriction, enabling stable and interpretable listwise optimization. By bridging generative modeling with discriminative ranking, our method produces coherent, fine-grained answer lists. Experiments reveal consistent gains in accuracy and ranking stability, especially for questions requiring temporal reasoning and semantic disambiguation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5217\u8868\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u5f0f\u5efa\u6a21\u548c\u5224\u522b\u5f0f\u6392\u5e8f\uff0c\u63d0\u5347\u89c6\u9891\u95ee\u7b54\u4e2d\u7b54\u6848\u751f\u6210\u7684\u8bed\u4e49\u7cbe\u5ea6\u548c\u6392\u5e8f\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u89c6\u9891\u95ee\u7b54\u7684\u7b54\u6848\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5f80\u5f80\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u8bed\u4e49\u51c6\u786e\u6027\u548c\u6392\u5e8f\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u65f6\u95f4\u63a8\u7406\u548c\u8bed\u4e49\u6d88\u6b67\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u4f7f\u7528\u57fa\u7840\u591a\u6a21\u6001\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5019\u9009\u7b54\u6848\uff0c\u7136\u540e\u901a\u8fc7\u5e26\u6709\u63a9\u7801\u6307\u9488\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u6392\u5e8f\u6743\u91cd\u7684\u6a21\u578b\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u8be5\u76ee\u6807\u6574\u5408\u4e86\u6307\u9488\u5f0f\u5019\u9009\u9009\u62e9\u3001\u6392\u5e8f\u4f9d\u8d56\u6743\u91cd\u548c\u8bcd\u6c47\u9650\u5236\u4e0b\u7684\u63a9\u7801\u4ea4\u53c9\u71b5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6392\u5e8f\u7a33\u5b9a\u6027\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u9700\u8981\u65f6\u95f4\u63a8\u7406\u548c\u8bed\u4e49\u6d88\u6b67\u7684\u95ee\u9898\u4e0a\u6548\u679c\u660e\u663e\u3002", "conclusion": "\u901a\u8fc7\u5c06\u751f\u6210\u5f0f\u5efa\u6a21\u4e0e\u5224\u522b\u5f0f\u6392\u5e8f\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u751f\u6210\u8fde\u8d2f\u4e14\u7ec6\u7c92\u5ea6\u7684\u7b54\u6848\u5217\u8868\uff0c\u4e3a\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07745", "abs": "https://arxiv.org/abs/2512.07745", "authors": ["Jialv Zou", "Shaoyu Chen", "Bencheng Liao", "Zhiyu Zheng", "Yuehao Song", "Lefei Zhang", "Qian Zhang", "Wenyu Liu", "Xinggang Wang"], "title": "DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving", "comment": null, "summary": "Generative diffusion models for end-to-end autonomous driving often suffer from mode collapse, tending to generate conservative and homogeneous behaviors. While DiffusionDrive employs predefined anchors representing different driving intentions to partition the action space and generate diverse trajectories, its reliance on imitation learning lacks sufficient constraints, resulting in a dilemma between diversity and consistent high quality. In this work, we propose DiffusionDriveV2, which leverages reinforcement learning to both constrain low-quality modes and explore for superior trajectories. This significantly enhances the overall output quality while preserving the inherent multimodality of its core Gaussian Mixture Model. First, we use scale-adaptive multiplicative noise, ideal for trajectory planning, to promote broad exploration. Second, we employ intra-anchor GRPO to manage advantage estimation among samples generated from a single anchor, and inter-anchor truncated GRPO to incorporate a global perspective across different anchors, preventing improper advantage comparisons between distinct intentions (e.g., turning vs. going straight), which can lead to further mode collapse. DiffusionDriveV2 achieves 91.2 PDMS on the NAVSIM v1 dataset and 85.5 EPDMS on the NAVSIM v2 dataset in closed-loop evaluation with an aligned ResNet-34 backbone, setting a new record. Further experiments validate that our approach resolves the dilemma between diversity and consistent high quality for truncated diffusion models, achieving the best trade-off. Code and model will be available at https://github.com/hustvl/DiffusionDriveV2", "AI": {"tldr": "DiffusionDriveV2\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6a21\u5f0f\u574d\u584c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u591a\u6837\u6027\u7684\u540c\u65f6\u63d0\u5347\u8f68\u8ff9\u8d28\u91cf", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u5b58\u5728\u6a21\u5f0f\u574d\u584c\u95ee\u9898\uff0c\u503e\u5411\u4e8e\u751f\u6210\u4fdd\u5b88\u4e14\u540c\u8d28\u5316\u7684\u884c\u4e3a\u3002DiffusionDrive\u867d\u7136\u4f7f\u7528\u9884\u5b9a\u4e49\u951a\u70b9\u6765\u5212\u5206\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4f46\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\u7f3a\u4e4f\u8db3\u591f\u7ea6\u675f\uff0c\u5bfc\u81f4\u591a\u6837\u6027\u548c\u9ad8\u8d28\u91cf\u4e4b\u95f4\u7684\u4e24\u96be\u56f0\u5883", "method": "1. \u4f7f\u7528\u5c3a\u5ea6\u81ea\u9002\u5e94\u4e58\u6027\u566a\u58f0\u4fc3\u8fdb\u5e7f\u6cdb\u63a2\u7d22\uff1b2. \u91c7\u7528\u951a\u70b9\u5185GRPO\u7ba1\u7406\u540c\u4e00\u951a\u70b9\u5185\u6837\u672c\u7684\u4f18\u52bf\u4f30\u8ba1\uff1b3. \u91c7\u7528\u951a\u70b9\u95f4\u622a\u65adGRPO\u6574\u5408\u4e0d\u540c\u951a\u70b9\u7684\u5168\u5c40\u89c6\u89d2\uff0c\u9632\u6b62\u4e0d\u540c\u610f\u56fe\u95f4\u7684\u4e0d\u5f53\u4f18\u52bf\u6bd4\u8f83", "result": "\u5728NAVSIM v1\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.2 PDMS\uff0c\u5728NAVSIM v2\u6570\u636e\u96c6\u4e0a\u8fbe\u523085.5 EPDMS\uff0c\u521b\u9020\u4e86\u65b0\u7eaa\u5f55", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u622a\u65ad\u6269\u6563\u6a21\u578b\u4e2d\u591a\u6837\u6027\u548c\u4e00\u81f4\u9ad8\u8d28\u91cf\u4e4b\u95f4\u7684\u4e24\u96be\u56f0\u5883\uff0c\u5b9e\u73b0\u4e86\u6700\u4f73\u6743\u8861"}}
{"id": "2512.07747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07747", "abs": "https://arxiv.org/abs/2512.07747", "authors": ["Shihao Zhao", "Yitong Chen", "Zeyinzi Jiang", "Bojia Zi", "Shaozhe Hao", "Yu Liu", "Chaojie Mao", "Kwan-Yee K. Wong"], "title": "Unison: A Fully Automatic, Task-Universal, and Low-Cost Framework for Unified Understanding and Generation", "comment": null, "summary": "Unified understanding and generation is a highly appealing research direction in multimodal learning. There exist two approaches: one trains a transformer via an auto-regressive paradigm, and the other adopts a two-stage scheme connecting pre-trained understanding and generative models for alignment fine-tuning. The former demands massive data and computing resources unaffordable for ordinary researchers. Though the latter requires a lower training cost, existing works often suffer from limited task coverage or poor generation quality. Both approaches lack the ability to parse input meta-information (such as task type, image resolution, video duration, etc.) and require manual parameter configuration that is tedious and non-intelligent. In this paper, we propose Unison which adopts the two-stage scheme while preserving the capabilities of the pre-trained models well. With an extremely low training cost, we cover a variety of multimodal understanding tasks, including text, image, and video understanding, as well as diverse generation tasks, such as text-to-visual content generation, editing, controllable generation, and IP-based reference generation. We also equip our model with the ability to automatically parse user intentions, determine the target task type, and accurately extract the meta-information required for the corresponding task. This enables full automation of various multimodal tasks without human intervention. Experiments demonstrate that, under a low-cost setting of only 500k training samples and 50 GPU hours, our model can accurately and automatically identify tasks and extract relevant parameters, and achieve superior performance across a variety of understanding and generation tasks.", "AI": {"tldr": "Unison\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u52a8\u89e3\u6790\u7528\u6237\u610f\u56fe\u548c\u4efb\u52a1\u53c2\u6570\uff0c\u5728\u4f4e\u8bad\u7ec3\u6210\u672c\u4e0b\u5b9e\u73b0\u591a\u79cd\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u4efb\u52a1\u8986\u76d6\u6709\u9650\u3001\u751f\u6210\u8d28\u91cf\u5dee\u3001\u7f3a\u4e4f\u81ea\u52a8\u53c2\u6570\u89e3\u6790\u80fd\u529b\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff1a1\uff09\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u529b\uff1b2\uff09\u901a\u8fc7\u5c11\u91cf\u6837\u672c\uff0850\u4e07\uff09\u548c\u4f4e\u8ba1\u7b97\u6210\u672c\uff0850 GPU\u5c0f\u65f6\uff09\u8fdb\u884c\u5bf9\u9f50\u5fae\u8c03\uff0c\u5b9e\u73b0\u81ea\u52a8\u4efb\u52a1\u8bc6\u522b\u548c\u53c2\u6570\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUnison\u80fd\u591f\u51c6\u786e\u81ea\u52a8\u8bc6\u522b\u4efb\u52a1\u7c7b\u578b\u548c\u63d0\u53d6\u76f8\u5173\u53c2\u6570\uff0c\u5728\u591a\u79cd\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "Unison\u8bc1\u660e\u4e86\u5728\u6781\u4f4e\u8bad\u7ec3\u6210\u672c\u4e0b\u5b9e\u73b0\u5168\u81ea\u52a8\u591a\u6a21\u6001\u4efb\u52a1\u5904\u7406\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u667a\u80fd\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07760", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07760", "abs": "https://arxiv.org/abs/2512.07760", "authors": ["Menglin Wang", "Xiaojin Gong", "Jiachen Li", "Genlin Ji"], "title": "Modality-Aware Bias Mitigation and Invariance Learning for Unsupervised Visible-Infrared Person Re-Identification", "comment": "Accepted to AAAI 2026", "summary": "Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match individuals across visible and infrared cameras without relying on any annotation. Given the significant gap across visible and infrared modality, estimating reliable cross-modality association becomes a major challenge in USVI-ReID. Existing methods usually adopt optimal transport to associate the intra-modality clusters, which is prone to propagating the local cluster errors, and also overlooks global instance-level relations. By mining and attending to the visible-infrared modality bias, this paper focuses on addressing cross-modality learning from two aspects: bias-mitigated global association and modality-invariant representation learning. Motivated by the camera-aware distance rectification in single-modality re-ID, we propose modality-aware Jaccard distance to mitigate the distance bias caused by modality discrepancy, so that more reliable cross-modality associations can be estimated through global clustering. To further improve cross-modality representation learning, a `split-and-contrast' strategy is designed to obtain modality-specific global prototypes. By explicitly aligning these prototypes under global association guidance, modality-invariant yet ID-discriminative representation learning can be achieved. While conceptually simple, our method obtains state-of-the-art performance on benchmark VI-ReID datasets and outperforms existing methods by a significant margin, validating its effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u4eba\u5458\u91cd\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5Jaccard\u8ddd\u79bb\u548c\u5206\u88c2\u5bf9\u6bd4\u7b56\u7565\u6765\u89e3\u51b3\u8de8\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u6700\u4f18\u4f20\u8f93\u6765\u5173\u8054\u8de8\u6a21\u6001\u805a\u7c7b\uff0c\u4f46\u5bb9\u6613\u4f20\u64ad\u5c40\u90e8\u805a\u7c7b\u9519\u8bef\u4e14\u5ffd\u7565\u5168\u5c40\u5b9e\u4f8b\u7ea7\u5173\u7cfb\u3002\u672c\u6587\u901a\u8fc7\u6316\u6398\u548c\u5173\u6ce8\u53ef\u89c1\u5149-\u7ea2\u5916\u6a21\u6001\u504f\u5dee\uff0c\u4ece\u504f\u5dee\u7f13\u89e3\u7684\u5168\u5c40\u5173\u8054\u548c\u6a21\u6001\u4e0d\u53d8\u8868\u793a\u5b66\u4e60\u4e24\u65b9\u9762\u89e3\u51b3\u8de8\u6a21\u6001\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6a21\u6001\u611f\u77e5Jaccard\u8ddd\u79bb\u6765\u7f13\u89e3\u7531\u6a21\u6001\u5dee\u5f02\u5f15\u8d77\u7684\u8ddd\u79bb\u504f\u5dee\uff0c\u901a\u8fc7\u5168\u5c40\u805a\u7c7b\u4f30\u8ba1\u66f4\u53ef\u9760\u7684\u8de8\u6a21\u6001\u5173\u8054\uff1b\u8bbe\u8ba1\u5206\u88c2\u5bf9\u6bd4\u7b56\u7565\u83b7\u5f97\u6a21\u6001\u7279\u5b9a\u5168\u5c40\u539f\u578b\uff0c\u5728\u5168\u5c40\u5173\u8054\u6307\u5bfc\u4e0b\u663e\u5f0f\u5bf9\u9f50\u8fd9\u4e9b\u539f\u578b\uff0c\u5b9e\u73b0\u6a21\u6001\u4e0d\u53d8\u4e14\u8eab\u4efd\u53ef\u533a\u5206\u7684\u8868\u793a\u5b66\u4e60\u3002", "result": "\u5728\u57fa\u51c6VI-ReID\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u89e3\u51b3\u8de8\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5728\u65e0\u76d1\u7763\u53ef\u89c1\u5149-\u7ea2\u5916\u4eba\u5458\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.07776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07776", "abs": "https://arxiv.org/abs/2512.07776", "authors": ["Maximilian Schall", "Felix Leonard Kn\u00f6fel", "Noah Elias K\u00f6nig", "Jan Jonas Kubeler", "Maximilian von Klinski", "Joan Wilhelm Linnemann", "Xiaoshi Liu", "Iven Jelle Schlegelmilch", "Ole Woyciniuk", "Alexandra Schild", "Dante Wasmuht", "Magdalena Bermejo Espinet", "German Illera Basas", "Gerard de Melo"], "title": "GorillaWatch: An Automated System for In-the-Wild Gorilla Re-Identification and Population Monitoring", "comment": "Accepted at WACV 2026", "summary": "Monitoring critically endangered western lowland gorillas is currently hampered by the immense manual effort required to re-identify individuals from vast archives of camera trap footage. The primary obstacle to automating this process has been the lack of large-scale, \"in-the-wild\" video datasets suitable for training robust deep learning models. To address this gap, we introduce a comprehensive benchmark with three novel datasets: Gorilla-SPAC-Wild, the largest video dataset for wild primate re-identification to date; Gorilla-Berlin-Zoo, for assessing cross-domain re-identification generalization; and Gorilla-SPAC-MoT, for evaluating multi-object tracking in camera trap footage. Building on these datasets, we present GorillaWatch, an end-to-end pipeline integrating detection, tracking, and re-identification. To exploit temporal information, we introduce a multi-frame self-supervised pretraining strategy that leverages consistency in tracklets to learn domain-specific features without manual labels. To ensure scientific validity, a differentiable adaptation of AttnLRP verifies that our model relies on discriminative biometric traits rather than background correlations. Extensive benchmarking subsequently demonstrates that aggregating features from large-scale image backbones outperforms specialized video architectures. Finally, we address unsupervised population counting by integrating spatiotemporal constraints into standard clustering to mitigate over-segmentation. We publicly release all code and datasets to facilitate scalable, non-invasive monitoring of endangered species", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u6fd2\u5371\u897f\u90e8\u4f4e\u5730\u5927\u7329\u7329\u81ea\u52a8\u91cd\u8bc6\u522b\u7684\u7aef\u5230\u7aef\u7cfb\u7edfGorillaWatch\uff0c\u5305\u542b\u4e09\u4e2a\u65b0\u6570\u636e\u96c6\u548c\u521b\u65b0\u7684\u591a\u5e27\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u5408\u56fe\u50cf\u4e3b\u5e72\u7279\u5f81\u4f18\u4e8e\u4e13\u7528\u89c6\u9891\u67b6\u6784\uff0c\u5e76\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u79cd\u7fa4\u8ba1\u6570\u95ee\u9898\u3002", "motivation": "\u76ee\u524d\u6fd2\u5371\u897f\u90e8\u4f4e\u5730\u5927\u7329\u7329\u76d1\u6d4b\u9762\u4e34\u5de8\u5927\u7684\u4eba\u5de5\u91cd\u8bc6\u522b\u5de5\u4f5c\u91cf\uff0c\u7f3a\u4e4f\u9002\u5408\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5927\u89c4\u6a21\u91ce\u5916\u89c6\u9891\u6570\u636e\u96c6\u662f\u4e3b\u8981\u969c\u788d\u3002", "method": "\u63d0\u51faGorillaWatch\u7aef\u5230\u7aef\u7ba1\u9053\uff0c\u6574\u5408\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u91cd\u8bc6\u522b\uff1b\u5f15\u5165\u591a\u5e27\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7b56\u7565\u5229\u7528\u8f68\u8ff9\u4e00\u81f4\u6027\u5b66\u4e60\u9886\u57df\u7279\u5b9a\u7279\u5f81\uff1b\u4f7f\u7528AttnLRP\u9a8c\u8bc1\u6a21\u578b\u4f9d\u8d56\u751f\u7269\u7279\u5f81\u800c\u975e\u80cc\u666f\u76f8\u5173\u6027\uff1b\u901a\u8fc7\u805a\u5408\u5927\u89c4\u6a21\u56fe\u50cf\u4e3b\u5e72\u7279\u5f81\u4f18\u5316\u6027\u80fd\u3002", "result": "\u521b\u5efa\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u7684\u91ce\u751f\u7075\u957f\u7c7b\u52a8\u7269\u91cd\u8bc6\u522b\u89c6\u9891\u6570\u636e\u96c6\uff1b\u591a\u5e27\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6709\u6548\u5b66\u4e60\u9886\u57df\u7279\u5f81\uff1b\u56fe\u50cf\u4e3b\u5e72\u7279\u5f81\u805a\u5408\u4f18\u4e8e\u4e13\u7528\u89c6\u9891\u67b6\u6784\uff1b\u65e0\u76d1\u7763\u79cd\u7fa4\u8ba1\u6570\u65b9\u6cd5\u7f13\u89e3\u4e86\u8fc7\u5206\u5272\u95ee\u9898\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6fd2\u5371\u7269\u79cd\u7684\u975e\u4fb5\u5165\u5f0f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53d1\u5e03\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2512.07778", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07778", "abs": "https://arxiv.org/abs/2512.07778", "authors": ["Sen Ye", "Jianning Pei", "Mengde Xu", "Shuyang Gu", "Chunyu Wang", "Liwei Wang", "Han Hu"], "title": "Distribution Matching Variational AutoEncoder", "comment": null, "summary": "Most visual generative models compress images into a latent space before applying diffusion or autoregressive modelling. Yet, existing approaches such as VAEs and foundation model aligned encoders implicitly constrain the latent space without explicitly shaping its distribution, making it unclear which types of distributions are optimal for modeling. We introduce \\textbf{Distribution-Matching VAE} (\\textbf{DMVAE}), which explicitly aligns the encoder's latent distribution with an arbitrary reference distribution via a distribution matching constraint. This generalizes beyond the Gaussian prior of conventional VAEs, enabling alignment with distributions derived from self-supervised features, diffusion noise, or other prior distributions. With DMVAE, we can systematically investigate which latent distributions are more conducive to modeling, and we find that SSL-derived distributions provide an excellent balance between reconstruction fidelity and modeling efficiency, reaching gFID equals 3.2 on ImageNet with only 64 training epochs. Our results suggest that choosing a suitable latent distribution structure (achieved via distribution-level alignment), rather than relying on fixed priors, is key to bridging the gap between easy-to-model latents and high-fidelity image synthesis. Code is avaliable at https://github.com/sen-ye/dmvae.", "AI": {"tldr": "DMVAE\u901a\u8fc7\u663e\u5f0f\u5206\u5e03\u5339\u914d\u7ea6\u675f\u5c06\u7f16\u7801\u5668\u6f5c\u5728\u5206\u5e03\u4e0e\u4efb\u610f\u53c2\u8003\u5206\u5e03\u5bf9\u9f50\uff0c\u7a81\u7834\u4e86\u4f20\u7edfVAE\u7684\u9ad8\u65af\u5148\u9a8c\u9650\u5236\uff0c\u53d1\u73b0SSL\u884d\u751f\u5206\u5e03\u5728\u5efa\u6a21\u6548\u7387\u4e0e\u91cd\u5efa\u8d28\u91cf\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u751f\u6210\u6a21\u578b\uff08\u5982VAE\u548c\u57fa\u7840\u6a21\u578b\u5bf9\u9f50\u7f16\u7801\u5668\uff09\u9690\u5f0f\u7ea6\u675f\u6f5c\u5728\u7a7a\u95f4\u800c\u672a\u660e\u786e\u5851\u9020\u5176\u5206\u5e03\uff0c\u4e0d\u6e05\u695a\u54ea\u79cd\u5206\u5e03\u6700\u9002\u5408\u5efa\u6a21\u3002", "method": "\u63d0\u51faDistribution-Matching VAE (DMVAE)\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u7ea6\u675f\u663e\u5f0f\u5bf9\u9f50\u7f16\u7801\u5668\u6f5c\u5728\u5206\u5e03\u4e0e\u4efb\u610f\u53c2\u8003\u5206\u5e03\uff0c\u652f\u6301\u81ea\u76d1\u7763\u7279\u5f81\u3001\u6269\u6563\u566a\u58f0\u7b49\u591a\u79cd\u5148\u9a8c\u5206\u5e03\u3002", "result": "SSL\u884d\u751f\u5206\u5e03\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff0c\u5728ImageNet\u4e0a\u4ec5\u752864\u4e2a\u8bad\u7ec3\u5468\u671f\u83b7\u5f97gFID=3.2\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u9009\u62e9\u5408\u9002\u7684\u6f5c\u5728\u5206\u5e03\u7ed3\u6784\uff08\u901a\u8fc7\u5206\u5e03\u7ea7\u5bf9\u9f50\u5b9e\u73b0\uff09\u800c\u975e\u4f9d\u8d56\u56fa\u5b9a\u5148\u9a8c\uff0c\u662f\u5f25\u5408\u6613\u5efa\u6a21\u6f5c\u5728\u7a7a\u95f4\u4e0e\u9ad8\u4fdd\u771f\u56fe\u50cf\u5408\u6210\u4e4b\u95f4\u5dee\u8ddd\u7684\u5173\u952e\u3002"}}
{"id": "2512.07802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07802", "abs": "https://arxiv.org/abs/2512.07802", "authors": ["Zhaochong An", "Menglin Jia", "Haonan Qiu", "Zijian Zhou", "Xiaoke Huang", "Zhiheng Liu", "Weiming Ren", "Kumara Kahatapitiya", "Ding Liu", "Sen He", "Chenyang Zhang", "Tao Xiang", "Fanny Yang", "Serge Belongie", "Tian Xie"], "title": "OneStory: Coherent Multi-Shot Video Generation with Adaptive Memory", "comment": "Project Page: https://zhaochongan.github.io/projects/OneStory", "summary": "Storytelling in real-world videos often unfolds through multiple shots -- discontinuous yet semantically connected clips that together convey a coherent narrative. However, existing multi-shot video generation (MSV) methods struggle to effectively model long-range cross-shot context, as they rely on limited temporal windows or single keyframe conditioning, leading to degraded performance under complex narratives. In this work, we propose OneStory, enabling global yet compact cross-shot context modeling for consistent and scalable narrative generation. OneStory reformulates MSV as a next-shot generation task, enabling autoregressive shot synthesis while leveraging pretrained image-to-video (I2V) models for strong visual conditioning. We introduce two key modules: a Frame Selection module that constructs a semantically-relevant global memory based on informative frames from prior shots, and an Adaptive Conditioner that performs importance-guided patchification to generate compact context for direct conditioning. We further curate a high-quality multi-shot dataset with referential captions to mirror real-world storytelling patterns, and design effective training strategies under the next-shot paradigm. Finetuned from a pretrained I2V model on our curated 60K dataset, OneStory achieves state-of-the-art narrative coherence across diverse and complex scenes in both text- and image-conditioned settings, enabling controllable and immersive long-form video storytelling.", "AI": {"tldr": "OneStory\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u6784\u95ee\u9898\u4e3a\u4e0b\u4e00\u4e2a\u955c\u5934\u751f\u6210\u4efb\u52a1\uff0c\u5f15\u5165\u5168\u5c40\u5185\u5b58\u548c\u81ea\u9002\u5e94\u6761\u4ef6\u6a21\u5757\uff0c\u5b9e\u73b0\u8fde\u8d2f\u7684\u957f\u7bc7\u53d9\u4e8b\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u957f\u8ddd\u79bb\u8de8\u955c\u5934\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u4f9d\u8d56\u6709\u9650\u65f6\u95f4\u7a97\u53e3\u6216\u5355\u5173\u952e\u5e27\u6761\u4ef6\uff0c\u5bfc\u81f4\u590d\u6742\u53d9\u4e8b\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5c06\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u91cd\u6784\u4e3a\u4e0b\u4e00\u4e2a\u955c\u5934\u751f\u6210\u4efb\u52a1\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u56fe\u50cf\u8f6c\u89c6\u9891\u6a21\u578b\uff0c\u5f15\u5165\u5e27\u9009\u62e9\u6a21\u5757\u6784\u5efa\u5168\u5c40\u5185\u5b58\u548c\u81ea\u9002\u5e94\u6761\u4ef6\u6a21\u5757\u8fdb\u884c\u91cd\u8981\u6027\u5f15\u5bfc\u7684\u8865\u4e01\u5316\u3002", "result": "\u572860K\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u540e\uff0cOneStory\u5728\u6587\u672c\u548c\u56fe\u50cf\u6761\u4ef6\u8bbe\u7f6e\u4e0b\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u652f\u6301\u53ef\u63a7\u7684\u6c89\u6d78\u5f0f\u957f\u7bc7\u89c6\u9891\u53d9\u4e8b\u3002", "conclusion": "OneStory\u901a\u8fc7\u6709\u6548\u7684\u8de8\u955c\u5934\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u7684\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u4e3a\u957f\u7bc7\u53d9\u4e8b\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.07806", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07806", "abs": "https://arxiv.org/abs/2512.07806", "authors": ["Gyeongjin Kang", "Seungkwon Yang", "Seungtae Nam", "Younggeun Lee", "Jungwoo Kim", "Eunbyung Park"], "title": "Multi-view Pyramid Transformer: Look Coarser to See Broader", "comment": "Project page: see https://gynjn.github.io/MVP/", "summary": "We propose Multi-view Pyramid Transformer (MVP), a scalable multi-view transformer architecture that directly reconstructs large 3D scenes from tens to hundreds of images in a single forward pass. Drawing on the idea of ``looking broader to see the whole, looking finer to see the details,\" MVP is built on two core design principles: 1) a local-to-global inter-view hierarchy that gradually broadens the model's perspective from local views to groups and ultimately the full scene, and 2) a fine-to-coarse intra-view hierarchy that starts from detailed spatial representations and progressively aggregates them into compact, information-dense tokens. This dual hierarchy achieves both computational efficiency and representational richness, enabling fast reconstruction of large and complex scenes. We validate MVP on diverse datasets and show that, when coupled with 3D Gaussian Splatting as the underlying 3D representation, it achieves state-of-the-art generalizable reconstruction quality while maintaining high efficiency and scalability across a wide range of view configurations.", "AI": {"tldr": "MVP\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u591a\u89c6\u89d2Transformer\u67b6\u6784\uff0c\u80fd\u591f\u4e00\u6b21\u6027\u4ece\u6570\u5341\u5230\u6570\u767e\u5f20\u56fe\u50cf\u91cd\u5efa\u5927\u578b3D\u573a\u666f\uff0c\u7ed3\u5408\u5c40\u90e8\u5230\u5168\u5c40\u7684\u89c6\u89d2\u5c42\u6b21\u548c\u7ec6\u5230\u7c97\u7684\u7a7a\u95f4\u8868\u793a\u5c42\u6b21\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\uff0c\u65e8\u5728\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u5927\u578b\u590d\u6742\u573a\u666f\u91cd\u5efa\u3002", "method": "\u57fa\u4e8e\u4e24\u4e2a\u6838\u5fc3\u8bbe\u8ba1\u539f\u5219\uff1a1\uff09\u5c40\u90e8\u5230\u5168\u5c40\u7684\u89c6\u89d2\u5c42\u6b21\u7ed3\u6784\uff0c\u4ece\u5c40\u90e8\u89c6\u56fe\u9010\u6b65\u6269\u5c55\u5230\u5b8c\u6574\u573a\u666f\uff1b2\uff09\u7ec6\u5230\u7c97\u7684\u7a7a\u95f4\u8868\u793a\u5c42\u6b21\uff0c\u4ece\u8be6\u7ec6\u7a7a\u95f4\u8868\u793a\u9010\u6b65\u805a\u5408\u4e3a\u7d27\u51d1\u7684\u4fe1\u606f\u5bc6\u96c6\u4ee4\u724c\u3002\u7ed3\u54083D Gaussian Splatting\u4f5c\u4e3a\u5e95\u5c423D\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u8868\u660e\uff0cMVP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u91cd\u5efa\u8d28\u91cf\uff0c\u540c\u65f6\u5728\u5404\u79cd\u89c6\u89d2\u914d\u7f6e\u4e0b\u4fdd\u6301\u9ad8\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MVP\u901a\u8fc7\u53cc\u5c42\u6b21\u7ed3\u6784\u8bbe\u8ba1\u6210\u529f\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8868\u793a\u4e30\u5bcc\u6027\uff0c\u4e3a\u5927\u89c4\u6a213D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07807", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.07807", "abs": "https://arxiv.org/abs/2512.07807", "authors": ["Shai Krakovsky", "Gal Fiebelman", "Sagie Benaim", "Hadar Averbuch-Elor"], "title": "Lang3D-XL: Language Embedded 3D Gaussians for Large-scale Scenes", "comment": "Accepted to SIGGRAPH Asia 2025. Project webpage: https://tau-vailab.github.io/Lang3D-XL", "summary": "Embedding a language field in a 3D representation enables richer semantic understanding of spatial environments by linking geometry with descriptive meaning. This allows for a more intuitive human-computer interaction, enabling querying or editing scenes using natural language, and could potentially improve tasks like scene retrieval, navigation, and multimodal reasoning. While such capabilities could be transformative, in particular for large-scale scenes, we find that recent feature distillation approaches cannot effectively learn over massive Internet data due to challenges in semantic feature misalignment and inefficiency in memory and runtime. To this end, we propose a novel approach to address these challenges. First, we introduce extremely low-dimensional semantic bottleneck features as part of the underlying 3D Gaussian representation. These are processed by rendering and passing them through a multi-resolution, feature-based, hash encoder. This significantly improves efficiency both in runtime and GPU memory. Second, we introduce an Attenuated Downsampler module and propose several regularizations addressing the semantic misalignment of ground truth 2D features. We evaluate our method on the in-the-wild HolyScenes dataset and demonstrate that it surpasses existing approaches in both performance and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57283D\u8868\u793a\u4e2d\u5d4c\u5165\u8bed\u8a00\u573a\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6781\u4f4e\u7ef4\u8bed\u4e49\u74f6\u9888\u7279\u5f81\u548c\u57fa\u4e8e\u54c8\u5e0c\u7f16\u7801\u5668\u7684\u591a\u5206\u8fa8\u7387\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u4e49\u7279\u5f81\u5bf9\u9f50\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u57283D\u8868\u793a\u4e2d\u5d4c\u5165\u8bed\u8a00\u573a\u53ef\u4ee5\u5b9e\u73b0\u66f4\u4e30\u5bcc\u7684\u7a7a\u95f4\u73af\u5883\u8bed\u4e49\u7406\u89e3\uff0c\u5c06\u51e0\u4f55\u4e0e\u63cf\u8ff0\u6027\u610f\u4e49\u8054\u7cfb\u8d77\u6765\uff0c\u4ece\u800c\u652f\u6301\u66f4\u76f4\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\uff08\u5982\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u548c\u7f16\u8f91\u573a\u666f\uff09\u5e76\u6539\u8fdb\u573a\u666f\u68c0\u7d22\u3001\u5bfc\u822a\u548c\u591a\u6a21\u6001\u63a8\u7406\u7b49\u4efb\u52a1\u3002", "method": "1) \u5728\u5e95\u5c423D\u9ad8\u65af\u8868\u793a\u4e2d\u5f15\u5165\u6781\u4f4e\u7ef4\u8bed\u4e49\u74f6\u9888\u7279\u5f81\uff1b2) \u901a\u8fc7\u6e32\u67d3\u5e76\u5c06\u8fd9\u4e9b\u7279\u5f81\u4f20\u9012\u7ed9\u57fa\u4e8e\u7279\u5f81\u7684\u591a\u5206\u8fa8\u7387\u54c8\u5e0c\u7f16\u7801\u5668\u8fdb\u884c\u5904\u7406\uff1b3) \u5f15\u5165\u8870\u51cf\u4e0b\u91c7\u6837\u5668\u6a21\u5757\u548c\u591a\u79cd\u6b63\u5219\u5316\u65b9\u6cd5\u6765\u89e3\u51b32D\u7279\u5f81\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u5728HolyScenes\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u65b9\u9762\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u4e92\u8054\u7f51\u6570\u636e\u5b66\u4e60\u4e2d\u7684\u8bed\u4e49\u7279\u5f81\u5bf9\u9f50\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u8bed\u8a00\u589e\u5f3a\u76843D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07821", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07821", "abs": "https://arxiv.org/abs/2512.07821", "authors": ["Shaoheng Fang", "Hanwen Jiang", "Yunpeng Bai", "Niloy J. Mitra", "Qixing Huang"], "title": "WorldReel: 4D Video Generation with Consistent Geometry and Motion Modeling", "comment": null, "summary": "Recent video generators achieve striking photorealism, yet remain fundamentally inconsistent in 3D. We present WorldReel, a 4D video generator that is natively spatio-temporally consistent. WorldReel jointly produces RGB frames together with 4D scene representations, including pointmaps, camera trajectory, and dense flow mapping, enabling coherent geometry and appearance modeling over time. Our explicit 4D representation enforces a single underlying scene that persists across viewpoints and dynamic content, yielding videos that remain consistent even under large non-rigid motion and significant camera movement. We train WorldReel by carefully combining synthetic and real data: synthetic data providing precise 4D supervision (geometry, motion, and camera), while real videos contribute visual diversity and realism. This blend allows WorldReel to generalize to in-the-wild footage while preserving strong geometric fidelity. Extensive experiments demonstrate that WorldReel sets a new state-of-the-art for consistent video generation with dynamic scenes and moving cameras, improving metrics of geometric consistency, motion coherence, and reducing view-time artifacts over competing methods. We believe that WorldReel brings video generation closer to 4D-consistent world modeling, where agents can render, interact, and reason about scenes through a single and stable spatiotemporal representation.", "AI": {"tldr": "WorldReel\u662f\u4e00\u4e2a4D\u89c6\u9891\u751f\u6210\u5668\uff0c\u901a\u8fc7\u8054\u5408\u751f\u6210RGB\u5e27\u548c4D\u573a\u666f\u8868\u793a\uff08\u70b9\u4e91\u56fe\u3001\u76f8\u673a\u8f68\u8ff9\u3001\u5bc6\u96c6\u5149\u6d41\u6620\u5c04\uff09\uff0c\u5b9e\u73b0\u4e86\u539f\u751f\u65f6\u7a7a\u4e00\u81f4\u6027\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u5668\u867d\u7136\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u903c\u771f\u5ea6\uff0c\u4f46\u57283D\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\u3002WorldReel\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u573a\u666f\u548c\u79fb\u52a8\u76f8\u673a\u4e0b\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u7684\u8bad\u7ec3\u7b56\u7565\uff1a\u5408\u6210\u6570\u636e\u63d0\u4f9b\u7cbe\u786e\u76844D\u76d1\u7763\uff08\u51e0\u4f55\u3001\u8fd0\u52a8\u548c\u76f8\u673a\uff09\uff0c\u771f\u5b9e\u89c6\u9891\u8d21\u732e\u89c6\u89c9\u591a\u6837\u6027\u548c\u771f\u5b9e\u611f\u3002\u901a\u8fc7\u663e\u5f0f4D\u8868\u793a\u5f3a\u5236\u4fdd\u6301\u5e95\u5c42\u573a\u666f\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eWorldReel\u5728\u52a8\u6001\u573a\u666f\u548c\u79fb\u52a8\u76f8\u673a\u4e0b\u7684\u89c6\u9891\u751f\u6210\u65b9\u9762\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u3001\u8fd0\u52a8\u8fde\u8d2f\u6027\u548c\u51cf\u5c11\u89c6\u89d2-\u65f6\u95f4\u4f2a\u5f71\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u7ade\u4e89\u65b9\u6cd5\u3002", "conclusion": "WorldReel\u5c06\u89c6\u9891\u751f\u6210\u63a8\u54114D\u4e00\u81f4\u7684\u4e16\u754c\u5efa\u6a21\uff0c\u4f7f\u5f97\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u7a33\u5b9a\u7684\u65f6\u7a7a\u8868\u793a\u6765\u6e32\u67d3\u3001\u4ea4\u4e92\u548c\u63a8\u7406\u573a\u666f\u3002"}}
{"id": "2512.07826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07826", "abs": "https://arxiv.org/abs/2512.07826", "authors": ["Haoyang He", "Jie Wang", "Jiangning Zhang", "Zhucun Xue", "Xingyuan Bu", "Qiangpeng Yang", "Shilei Wen", "Lei Xie"], "title": "OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing", "comment": "38 pages", "summary": "The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://github.com/lewandofskee/OpenVE.", "AI": {"tldr": "\u63d0\u51fa\u4e86OpenVE-3M\uff0c\u4e00\u4e2a\u5f00\u6e90\u3001\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u89c6\u9891\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u5e76\u6784\u5efa\u4e86\u7edf\u4e00\u57fa\u51c6OpenVE-Bench\uff0c\u540c\u65f6\u5f00\u53d1\u4e865B\u53c2\u6570\u6a21\u578bOpenVE-Edit\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6307\u4ee4\u89c6\u9891\u7f16\u8f91\u9886\u57df\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u7edf\u4e00\u57fa\u51c6\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u6d41\u6c34\u7ebf\u751f\u6210\u4e24\u7c7b\u7f16\u8f91\u7c7b\u578b\uff08\u7a7a\u95f4\u5bf9\u9f50\u548c\u975e\u7a7a\u95f4\u5bf9\u9f50\uff09\uff0c\u5e76\u8fdb\u884c\u4e25\u683c\u8d28\u91cf\u8fc7\u6ee4\u3002\u6784\u5efa\u5305\u542b431\u4e2a\u89c6\u9891\u7f16\u8f91\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "result": "OpenVE-3M\u5728\u89c4\u6a21\u3001\u7f16\u8f91\u7c7b\u578b\u591a\u6837\u6027\u3001\u6307\u4ee4\u957f\u5ea6\u548c\u6574\u4f53\u8d28\u91cf\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6570\u636e\u96c6\u3002OpenVE-Edit\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6240\u6709\u5148\u524d\u5f00\u6e90\u6a21\u578b\uff0c\u5305\u62ec14B\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6307\u4ee4\u89c6\u9891\u7f16\u8f91\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u7edf\u4e00\u57fa\u51c6\u548c\u9ad8\u6548\u6a21\u578b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.07829", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07829", "abs": "https://arxiv.org/abs/2512.07829", "authors": ["Yuan Gao", "Chen Chen", "Tianrong Chen", "Jiatao Gu"], "title": "One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation", "comment": null, "summary": "Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.", "AI": {"tldr": "FAE\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u89e3\u7801\u5668\u7ed3\u6784\u5c06\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u5f81\u9002\u914d\u5230\u751f\u6210\u6a21\u578b\u7684\u4f4e\u7ef4\u6f5c\u7a7a\u95f4\uff0c\u5728\u4fdd\u6301\u7406\u89e3\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u89c6\u89c9\u8868\u5f81\u4e0e\u751f\u6210\u6a21\u578b\u6f5c\u7a7a\u95f4\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u67b6\u6784\u548c\u76ee\u6807\u51fd\u6570\u3002", "method": "\u4f7f\u7528\u53cc\u89e3\u7801\u5668\u7ed3\u6784\uff1a\u4e00\u4e2a\u89e3\u7801\u5668\u91cd\u5efa\u539f\u59cb\u7279\u5f81\u7a7a\u95f4\uff0c\u53e6\u4e00\u4e2a\u89e3\u7801\u5668\u5c06\u91cd\u5efa\u7279\u5f81\u4f5c\u4e3a\u8f93\u5165\u8fdb\u884c\u56fe\u50cf\u751f\u6210\u3002\u53ea\u9700\u5355\u5c42\u6ce8\u610f\u529b\u5373\u53ef\u9002\u914d\u9884\u8bad\u7ec3\u8868\u5f81\u3002", "result": "\u5728ImageNet 256x256\u4e0a\uff0c\u6269\u6563\u6a21\u578b\u8fbe\u5230FID 1.29\uff08800\u8f6e\uff09\u548c1.70\uff0880\u8f6e\uff09\u7684\u4f18\u5f02\u6027\u80fd\uff0c\u65e0\u9700CFG\u65f6\u4e5f\u80fd\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "conclusion": "FAE\u6846\u67b6\u7b80\u5355\u901a\u7528\uff0c\u53ef\u9002\u914d\u591a\u79cd\u81ea\u76d1\u7763\u7f16\u7801\u5668\u548c\u751f\u6210\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u5b66\u4e60\u3002"}}
{"id": "2512.07831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07831", "abs": "https://arxiv.org/abs/2512.07831", "authors": ["Jiehui Huang", "Yuechen Zhang", "Xu He", "Yuan Gao", "Zhi Cen", "Bin Xia", "Yan Zhou", "Xin Tao", "Pengfei Wan", "Jiaya Jia"], "title": "UnityVideo: Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation", "comment": "Project Website https://jackailab.github.io/Projects/UnityVideo", "summary": "Recent video generation models demonstrate impressive synthesis capabilities but remain limited by single-modality conditioning, constraining their holistic world understanding. This stems from insufficient cross-modal interaction and limited modal diversity for comprehensive world knowledge representation. To address these limitations, we introduce UnityVideo, a unified framework for world-aware video generation that jointly learns across multiple modalities (segmentation masks, human skeletons, DensePose, optical flow, and depth maps) and training paradigms. Our approach features two core components: (1) dynamic noising to unify heterogeneous training paradigms, and (2) a modality switcher with an in-context learner that enables unified processing via modular parameters and contextual learning. We contribute a large-scale unified dataset with 1.3M samples. Through joint optimization, UnityVideo accelerates convergence and significantly enhances zero-shot generalization to unseen data. We demonstrate that UnityVideo achieves superior video quality, consistency, and improved alignment with physical world constraints. Code and data can be found at: https://github.com/dvlab-research/UnityVideo", "AI": {"tldr": "UnityVideo\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u591a\u79cd\u6a21\u6001\uff08\u5206\u5272\u63a9\u7801\u3001\u4eba\u4f53\u9aa8\u67b6\u3001DensePose\u3001\u5149\u6d41\u3001\u6df1\u5ea6\u56fe\uff09\u548c\u8bad\u7ec3\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u5355\u6a21\u6001\u6761\u4ef6\u9650\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u53d7\u9650\u4e8e\u5355\u6a21\u6001\u6761\u4ef6\u9650\u5236\uff0c\u7f3a\u4e4f\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u6a21\u6001\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u5b9e\u73b0\u5168\u9762\u7684\u4e16\u754c\u77e5\u8bc6\u8868\u793a\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u566a\u58f0\u6ce8\u5165\u7edf\u4e00\u5f02\u6784\u8bad\u7ec3\u8303\u5f0f\uff0c\u4ee5\u53ca\u5e26\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u5668\u7684\u6a21\u6001\u5207\u6362\u5668\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u53c2\u6570\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u7edf\u4e00\u5904\u7406\u3002\u6784\u5efa\u4e86130\u4e07\u6837\u672c\u7684\u5927\u89c4\u6a21\u7edf\u4e00\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\uff0cUnityVideo\u52a0\u901f\u4e86\u6536\u655b\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u672a\u89c1\u6570\u636e\u4e0a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u89c6\u9891\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u548c\u7269\u7406\u4e16\u754c\u7ea6\u675f\u5bf9\u9f50\u3002", "conclusion": "UnityVideo\u6846\u67b6\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u4e16\u754c\u611f\u77e5\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u7edf\u4e00\u8bad\u7ec3\u5728\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2512.07834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.07834", "abs": "https://arxiv.org/abs/2512.07834", "authors": ["Yi-Chuan Huang", "Jiewen Chan", "Hao-Jen Chien", "Yu-Lun Liu"], "title": "Voxify3D: Pixel Art Meets Volumetric Rendering", "comment": "Project page: https://yichuanh.github.io/Voxify-3D/", "summary": "Voxel art is a distinctive stylization widely used in games and digital media, yet automated generation from 3D meshes remains challenging due to conflicting requirements of geometric abstraction, semantic preservation, and discrete color coherence. Existing methods either over-simplify geometry or fail to achieve the pixel-precise, palette-constrained aesthetics of voxel art. We introduce Voxify3D, a differentiable two-stage framework bridging 3D mesh optimization with 2D pixel art supervision. Our core innovation lies in the synergistic integration of three components: (1) orthographic pixel art supervision that eliminates perspective distortion for precise voxel-pixel alignment; (2) patch-based CLIP alignment that preserves semantics across discretization levels; (3) palette-constrained Gumbel-Softmax quantization enabling differentiable optimization over discrete color spaces with controllable palette strategies. This integration addresses fundamental challenges: semantic preservation under extreme discretization, pixel-art aesthetics through volumetric rendering, and end-to-end discrete optimization. Experiments show superior performance (37.12 CLIP-IQA, 77.90\\% user preference) across diverse characters and controllable abstraction (2-8 colors, 20x-50x resolutions). Project page: https://yichuanh.github.io/Voxify-3D/", "AI": {"tldr": "Voxify3D\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u53ef\u5fae\u5206\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u50cf\u7d20\u827a\u672f\u76d1\u7763\u3001\u57fa\u4e8e\u8865\u4e01\u7684CLIP\u5bf9\u9f50\u548c\u8c03\u8272\u677f\u7ea6\u675f\u7684Gumbel-Softmax\u91cf\u5316\uff0c\u5b9e\u73b0\u4ece3D\u7f51\u683c\u5230\u4f53\u7d20\u827a\u672f\u7684\u9ad8\u8d28\u91cf\u81ea\u52a8\u5316\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u4f53\u7d20\u827a\u672f\u751f\u6210\u4e2d\u51e0\u4f55\u62bd\u8c61\u8fc7\u5ea6\u7b80\u5316\u3001\u8bed\u4e49\u4fdd\u7559\u4e0d\u8db3\u548c\u79bb\u6563\u989c\u8272\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u548c\u8c03\u8272\u677f\u7ea6\u675f\u7684\u7f8e\u5b66\u8981\u6c42\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u6b63\u4ea4\u50cf\u7d20\u827a\u672f\u76d1\u7763\u6d88\u9664\u900f\u89c6\u5931\u771f\uff1b2\uff09\u57fa\u4e8e\u8865\u4e01\u7684CLIP\u5bf9\u9f50\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff1b3\uff09\u8c03\u8272\u677f\u7ea6\u675f\u7684Gumbel-Softmax\u91cf\u5316\u5b9e\u73b0\u79bb\u6563\u989c\u8272\u7a7a\u95f4\u7684\u53ef\u5fae\u5206\u4f18\u5316\u3002", "result": "\u5728\u591a\u6837\u5316\u89d2\u8272\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff08CLIP-IQA\u5f97\u520637.12\uff0c\u7528\u6237\u504f\u597d\u738777.90%\uff09\uff0c\u652f\u6301\u53ef\u63a7\u62bd\u8c61\uff082-8\u79cd\u989c\u8272\uff0c20x-50x\u5206\u8fa8\u7387\uff09\u3002", "conclusion": "Voxify3D\u6210\u529f\u89e3\u51b3\u4e86\u6781\u7aef\u79bb\u6563\u5316\u4e0b\u7684\u8bed\u4e49\u4fdd\u7559\u3001\u50cf\u7d20\u827a\u672f\u7f8e\u5b66\u548c\u7aef\u5230\u7aef\u79bb\u6563\u4f18\u5316\u7b49\u6838\u5fc3\u6311\u6218\uff0c\u4e3a3D\u4f53\u7d20\u827a\u672f\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
