<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 205]
- [cs.RO](#cs.RO) [Total: 62]
- [cs.LG](#cs.LG) [Total: 164]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出基于OpenVLA-OFT框架的可解释对抗鲁棒视觉-语言-动作模型，通过Evidence-3模块检测光度扰动并生成自然语言解释，在对抗条件下显著提升动作预测精度。


<details>
  <summary>Details</summary>
Motivation: 智能农业中基于RGB相机和机械臂的系统易受光度扰动（色调、光照、噪声变化）影响，在对抗攻击下容易发生故障。

Method: 基于OpenVLA-OFT框架构建Vision-Language-Action模型，集成Evidence-3模块检测光度扰动并生成自然语言解释。

Result: 相比基线模型，当前动作L1损失降低21.7%，下一动作L1损失降低18.4%，动作预测精度和可解释性显著提升。

Conclusion: 所提模型有效提高了智能农业系统在对抗条件下的鲁棒性和可解释性，为应对光度扰动提供了可靠解决方案。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [2] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

TL;DR: Temporal-Anchor3DLane通过改进损失函数和添加轻量级时序融合模块，显著提升了单目3D车道线检测的性能和时序稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的Anchor3DLane方法在3D车道线检测中存在回归异常值敏感、全局几何监督弱、多损失项平衡困难以及时序连续性利用不足等问题。

Method: 提出三个关键改进：(1)多任务损失改进包括平衡L1回归、Chamfer点集距离和基于不确定性的损失加权；(2)轻量级时序LSTM融合模块替代Transformer式融合；(3)ESCOP风格训练优化结合曲线级监督和时序一致性。

Result: 在OpenLane数据集上，F1分数提升+6.2，并产生更平滑的时序轨迹。

Conclusion: 通过小的架构和损失函数改进，无需额外传感器或扩展即可显著增强3D车道线检测的鲁棒性。

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [3] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

TL;DR: 该研究为埃塞俄比亚提格雷地区开发了一种离线的仙人掌无花果作物病害检测系统，通过比较三种轻量级模型在移动设备上的性能，发现MobileViT-XS在准确率上表现最佳，同时兼顾了部署效率。


<details>
  <summary>Details</summary>
Motivation: 提格雷地区80%以上人口依赖农业，但基础设施限制导致难以获得专业作物病害诊断服务。研究旨在开发适用于边缘环境的离线检测系统，提升粮食安全诊断的包容性。

Method: 构建了包含3,587张田间图像的本土仙人掌无花果数据集，比较了三种移动高效架构：轻量CNN、EfficientNet-Lite1和MobileViT-XS。系统采用离线优先设计，支持ARM设备上的本地推理。

Result: EfficientNet-Lite1达到90.7%测试准确率，轻量CNN为89.5%（推理延迟42毫秒，模型大小4.8MB），MobileViT-XS平均交叉验证准确率达97.3%，显示其全局推理能力更有效区分病虫害。

Conclusion: 研究证明了在资源受限环境中，基于MHSA的全局推理模型能更可靠地识别本土作物病害，系统已部署支持提格雷语和阿姆哈拉语的Flutter应用，强化了粮食安全诊断的包容性。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [4] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出了一种用于小麦全语义分割竞赛的系统化自训练框架，结合两阶段混合训练策略和广泛数据增强，基于SegFormer模型实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 解决小麦全语义分割任务中的精度提升和数据利用最大化问题

Method: 采用两阶段混合训练策略，结合SegFormer模型（MiT-B4骨干网络）和迭代式师生循环训练，配合大量数据增强技术

Result: 在开发阶段和测试阶段数据集上均取得了竞争性性能表现

Conclusion: 该自训练框架能够有效提升小麦语义分割的准确性，并实现数据的高效利用

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [5] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

TL;DR: 该研究比较了SAM3零样本分割模型与三种YOLO11变体在MinneApple数据集上的实例分割性能，发现IoU阈值选择对性能评估影响显著，SAM3在边界稳定性方面表现优异，而YOLO在检测完整性方面更胜一筹。


<details>
  <summary>Details</summary>
Motivation: 深度学习存在两种实例分割范式：通过任务特定微调优化的专用模型和能够零样本分割的通才基础模型。本研究旨在系统比较这两种范式的性能差异。

Method: 在MinneApple数据集（包含670张果园图像和28,179个苹果实例标注）上，评估SAM3零样本模式与三种YOLO11变体（nano、medium、large）微调后的实例分割性能，分析不同IoU阈值下的表现。

Result: 在IoU=0.15时，YOLO模型达到68.9%、72.2%和71.9%的F1分数，SAM3为59.8%。但SAM3在IoU范围变化时仅下降4分，而YOLO下降48-50分，显示SAM3的边界稳定性是YOLO的12倍。

Conclusion: SAM3在掩模精度方面具有优势，而YOLO11在检测完整性方面更专业。研究为密集实例分割任务中何时选择专用微调模型或通用基础模型提供了深入理解。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [6] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: mmWeaver是一个基于隐式神经表示(INR)的毫米波信号生成框架，通过超网络结合环境上下文和人体运动特征，实现高效、逼真的信号合成，在压缩比、信号质量和下游任务性能上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 毫米波信号具有复杂、稀疏和高维特性，物理仿真计算成本高昂，而现有方法难以生成环境特定且多样化的信号数据集，限制了毫米波雷达在活动识别和姿态估计等应用的发展。

Method: 使用隐式神经表示(INR)将毫米波信号建模为连续函数，结合超网络根据RGB-D图像提取的环境上下文和通过MotionGPT从文本生成的人体运动特征，动态生成INR参数，实现多分辨率I/Q信号合成。

Result: mmWeaver实现了0.88的复杂SSIM和35 dB的PSNR，比现有方法在信号逼真度上表现更优，同时将活动识别准确率提升7%，人体姿态估计误差降低15%，运行速度比仿真方法快6-35倍。

Conclusion: mmWeaver通过语义和几何先验条件化，有效解决了毫米波信号生成的挑战，为毫米波雷达应用提供了高效、逼真的数据增强解决方案。

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [7] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

TL;DR: Hot Hém是一个GeoAI工作流，用于估计和操作化胡志明市行人热暴露，结合街景图像、语义分割和遥感技术，通过XGBoost模型预测地表温度，实现热感知路径规划。


<details>
  <summary>Details</summary>
Motivation: 解决热带高密度城市中行人热暴露这一关键健康风险，标准路径规划算法往往忽略微观尺度的热变化。

Method: 结合Google街景图像、语义图像分割和遥感技术，训练两个XGBoost模型预测地表温度，并在所有OSMnx衍生的人行网络节点上以拼凑方式部署。

Result: 开发了一个能够估计行人热暴露的模型，为识别城市走廊在基础设施尺度上为何经历不成比例高温提供了基础。

Conclusion: Hot Hém工作流为热带城市的热暴露研究提供了可行方法，有助于理解城市热岛效应和改善行人热舒适度。

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [8] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

TL;DR: 本文提供了基于无人机采集的开放微观车辆轨迹数据集，用于研究异质化区域城市交通条件，解决了传统路边视频采集的遮挡和视角限制问题。


<details>
  <summary>Details</summary>
Motivation: 传统路边视频采集在密集混合交通中因遮挡、视角限制和车辆不规则运动而失效，无人机采集提供了俯视视角以减少这些问题并捕捉丰富的时空动态。

Method: 使用Data from Sky平台从无人机视频中提取数据，并在六个印度首都地区的中段位置收集，包含时间戳车辆位置、速度、加速度和车辆分类，分辨率为30帧/秒。

Result: 数据集经过手动计数、空间平均速度和探针轨迹验证，探索性分析揭示了关键行为模式，如车道保持偏好、速度分布和横向操纵。

Conclusion: 这些开放数据集为全球研究社区提供了资源，支持仿真建模、安全评估和行为研究，有助于开发更准确代表复杂城市交通环境的模型。

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [9] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

TL;DR: 本文提出了RIO-VQA任务和RIO-Bench基准，旨在解决大型视觉语言模型在文本攻击下的选择性文本使用问题，要求模型根据上下文决定何时读取文本、何时忽略文本。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法和防御机制主要关注物体识别，鼓励模型忽略文本以获得鲁棒性，但现实场景需要同时处理物体和文本的联合推理。现有方法在文本攻击鲁棒性和文本阅读能力之间存在不平衡。

Method: 引入RIO-VQA任务，创建RIO-Bench基准数据集，为每个真实图像提供相同场景的反事实（读取/忽略）样本，仅改变文本内容和问题类型。基于此基准提出数据驱动的自适应选择性文本使用防御方法。

Result: 实验表明，现有强LVLMs和防御方法无法平衡文本攻击鲁棒性和文本阅读能力。RIO-Bench支持的新型防御方法能够超越之前非自适应的文本忽略防御策略。

Conclusion: 这项工作揭示了现有评估范围与现实需求之间的根本性错位，为构建可靠的LVLMs提供了原则性路径。RIO-Bench为模型选择性文本使用能力提供了标准化评估框架。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [10] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

TL;DR: CLARGA是一种通用的多模态融合架构，适用于任意数量和类型的模态，通过图注意力网络实现高效的多模态表示学习，具有自适应性和缺失模态处理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态融合方法通常针对特定模态设计，缺乏通用性。CLARGA旨在开发一个能够处理任意模态组合的通用框架，提高多模态表示学习的适应性和效率。

Method: 使用图注意力网络构建模态间注意力加权图，通过消息传递实现多模态融合。采用混合损失函数（监督任务损失+对比InfoNCE损失）进行训练，支持缺失模态处理。

Result: 在7个不同领域的数据集上测试，包括金融、人机交互、多媒体分类和情感计算，CLARGA均优于基线模型和现有最优方法，表现出强大的鲁棒性和适应性。

Conclusion: CLARGA是一个高效、通用的多模态融合框架，可轻松集成到各种机器学习模型中，在多种任务上实现优异的表示学习性能。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [11] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 该研究提出使用智能手机被动捕捉的微笑强度作为积极情感的客观行为指标，通过分析405,448个视频片段发现微笑模式与幸福感调查数据高度相关，为大规模研究情感行为提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 传统的主观幸福感测量方法依赖自我报告，存在回忆偏差和参与者负担重的问题，需要更客观、可扩展的行为指标来理解日常生活中的幸福感表达。

Method: 使用深度学习模型分析233名参与者一周内被动记录的405,448个视频片段，量化微笑强度，并研究其与身体活动、光照暴露和智能手机使用的关系。

Result: 微笑强度的每日模式与全国幸福感调查数据高度相关（r=0.92），昼夜节律与日重建方法结果一致（r=0.80）。较高的平均微笑强度与更多身体活动（Beta=0.043）和更大光照暴露（Beta=0.038）显著相关。

Conclusion: 被动智能手机传感可作为研究情感行为动态的强大生态有效方法，为在人群尺度上理解这种行为打开了大门。

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [12] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

TL;DR: MPath是一个轻量级多模态框架，通过视觉前缀提示机制将WSI视觉嵌入注入预训练的生物医学语言模型，用于病理报告自动生成。


<details>
  <summary>Details</summary>
Motivation: 解决从全切片图像直接生成临床连贯病理报告的挑战，克服组织形态学变异大和病理叙述结构复杂的问题。

Method: 使用基础模型的WSI特征（CONCH + Titan），通过紧凑投影模块将视觉嵌入注入冻结的BioBART语言模型，采用视觉前缀提示机制而非端到端视觉语言预训练。

Result: 在RED 2025 Grand Challenge数据集上开发评估，在Test Phase 2中排名第4，尽管提交机会有限。

Conclusion: 基于提示的多模态调节策略具有可扩展性和可解释性，在病理报告生成方面显示出巨大潜力。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [13] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: FloraForge是一个基于LLM的框架，通过自然语言交互帮助领域专家生成生物准确的参数化3D植物模型，无需编程专业知识。


<details>
  <summary>Details</summary>
Motivation: 现有3D植物建模方法存在局限：基于学习的方法需要大量物种特定数据且不可编辑；程序化建模需要专业几何建模知识，对领域科学家不友好。

Method: 利用LLM辅助协同设计，通过迭代自然语言植物精炼(PR)生成参数化Python脚本，创建分层B样条曲面表示，具有植物学约束和显式控制点。

Result: 框架在玉米、大豆和绿豆上验证，通过手动精炼植物描述符(PD)将程序模型拟合到点云数据，生成可视化三角网格和带参数元数据的分析网格。

Conclusion: FloraForge结合了LLM辅助模板创建、数学连续表示和直接参数控制，为植物科学民主化了复杂几何建模，同时保持数学严谨性。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [14] [TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder](https://arxiv.org/abs/2512.11926)
*Qinghao Meng,Chenming Wu,Liangjun Zhang,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出了一种联合完成和检测的框架TransBridge，通过基于transformer的上采样块融合检测和完成网络的特征，提升稀疏区域的3D物体检测性能，同时保持成本不变。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中远距离区域仅有少量LiDAR点云的3D物体检测难题，应对点云稀疏性带来的挑战。

Method: 提出TransBridge transformer上采样块融合检测和完成网络特征；设计Dynamic-Static Reconstruction模块生成密集LiDAR数据；利用transformer机制建立通道和空间关系，生成高分辨率特征图。

Result: 在nuScenes和Waymo数据集上的实验表明，该框架能持续提升端到端3D物体检测性能，mAP提升0.7-1.5点；对于两阶段检测框架，mAP最高提升5.78点。

Conclusion: 提出的联合完成和检测框架有效提升了稀疏点云区域的3D物体检测性能，具有良好的泛化能力。

Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.

</details>


### [15] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

TL;DR: 本文提出了MONET模型，使用扩散模型从明场图像预测细胞染色图像，解决了传统细胞染色技术的劳动密集和无法研究细胞动态的问题。


<details>
  <summary>Details</summary>
Motivation: 传统细胞染色技术存在两个主要问题：(1) 劳动密集型，(2) 需要化学固定，无法研究细胞动态变化。

Method: 训练扩散模型（MONET）在大型数据集上，使用一致性架构从明场图像预测细胞染色通道，并能生成时间序列视频。

Result: 模型质量随规模扩大而提升，能够生成时间序列视频，并展示出一定的上下文学习能力，可部分适应分布外的细胞系和成像协议。

Conclusion: 虚拟细胞染色并非完全替代物理细胞染色，而是作为补充工具，为生物学研究开辟新的工作流程。

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [16] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

TL;DR: 本文提出了一种新的HEMC-CPS模型，结合上下文Peano扫描和证据隐马尔可夫链，用于无监督图像分割，在合成和真实图像上表现出有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的Peano扫描将二维图像像素转换为一维序列，使隐马尔可夫链可用于图像分割。虽然基于隐马尔可夫链的贝叶斯分割方法比隐马尔可夫场更快且具有竞争力，但需要进一步改进以处理更复杂的图像。

Method: 提出HEMC-CPS模型，同时考虑上下文Peano扫描和证据隐马尔可夫链。使用随机期望最大化方法进行无监督参数估计，采用贝叶斯最大后验概率模式进行分割。

Result: 在合成和真实图像上的实验表明，HEMC-CPS模型在贝叶斯分割中有效，且具有处理更复杂图像（如三维或多传感器多分辨率图像）的潜力。

Conclusion: HEMC-CPS模型不仅限于图像分割，还可用于任何类型的空间相关数据建模，为复杂图像分析提供了新的工具。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [17] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

TL;DR: DynaPURLS是一个用于零样本骨架动作识别的统一框架，通过建立多尺度视觉-语义对应关系并在推理时动态优化，解决了现有方法中粗粒度对齐导致的领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别方法依赖于骨架特征与静态类级语义的粗粒度对齐，无法有效弥合可见类和未见类之间的领域偏移，限制了细粒度视觉知识的迁移。

Method: 1）使用大语言模型生成包含全局运动和局部身体部位动态的层次化文本描述；2）自适应分区模块通过语义分组骨架关节产生细粒度视觉表示；3）动态优化模块在推理时通过轻量级可学习投影将文本特征适配到输入视觉流；4）使用置信感知、类别平衡的记忆库稳定优化过程。

Result: 在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上的广泛实验表明，DynaPURLS显著优于现有方法，创造了新的最先进记录。

Conclusion: DynaPURLS通过建立细粒度多尺度视觉-语义对应关系和动态优化机制，有效解决了零样本骨架动作识别中的领域偏移问题，为相关研究提供了新的思路和解决方案。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [18] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

TL;DR: 本研究探讨了在数据受限条件下使用数据高效图像变换器(DeiT)进行晶圆图缺陷分类，结果表明DeiT在分类准确率(90.83%)、F1分数(90.78%)和训练收敛速度方面均优于传统CNN模型。


<details>
  <summary>Details</summary>
Motivation: 半导体晶圆行业对缺陷检测的预测性要求高，但传统CNN模型在数据有限和不平衡的情况下性能下降，需要寻找更有效的解决方案。

Method: 使用数据高效图像变换器(DeiT)模型进行晶圆图缺陷分类，并与VGG-19、SqueezeNet、Xception和Hybrid等CNN模型进行对比实验。

Result: DeiT模型取得了90.83%的最高分类准确率，明显优于VGG-19(65%)、SqueezeNet(82%)、Xception(66%)和Hybrid(67%)等CNN模型，同时在F1分数和训练收敛速度方面也表现更优。

Conclusion: 基于变换器的模型如DeiT在半导体晶圆缺陷检测中具有巨大潜力，能够支持半导体制造过程中的预测性维护策略。

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [19] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

TL;DR: CARI4D是首个从单目RGB视频重建4D人机交互的类别无关方法，通过姿态假设选择算法和渲染比较范式实现空间、时间和像素对齐，在未见数据集上比现有方法提升36%重建精度。


<details>
  <summary>Details</summary>
Motivation: 从单目RGB视图推断4D人机交互具有挑战性，因为存在物体和人类信息未知、深度模糊、遮挡和复杂运动等问题。现有方法需要真实物体模板或局限于特定物体类别。

Method: 提出姿态假设选择算法，整合基础模型的预测结果；通过学习的渲染比较范式进行联合优化，确保空间、时间和像素对齐；利用物理约束进行接触推理进一步优化。

Result: 在分布内数据集上重建误差比现有方法降低38%，在未见数据集上降低36%。模型能够泛化到训练类别之外，可零样本应用于真实网络视频。

Conclusion: CARI4D首次实现了从单目RGB视频进行类别无关的4D人机交互重建，在精度和泛化能力上都显著优于现有方法。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [20] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: V-REX是一个评估视觉语言模型多步探索推理能力的评测套件，包含挑战性视觉推理任务基准和评估协议，通过Chain-of-Questions方法将多步探索推理分解为规划和执行两个能力维度进行量化评估。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理简单明确定义的问题时表现良好，但在需要多步探索和推理的复杂开放任务中表现不佳。现有的评测方法难以评估这种多步探索过程。

Method: 开发V-REX评测套件，将多步探索推理转化为Chain-of-Questions框架，通过为每个步骤提供有限的问题和答案选项，实现可靠的量化评估。评估模型在规划和执行两个维度的能力。

Result: 通过对SOTA专有和开源视觉语言模型的评估，揭示了模型能力的一致扩展趋势，规划能力和执行能力之间存在显著差异，多步探索推理仍有很大改进空间。

Conclusion: V-REX为评估视觉语言模型的多步探索推理能力提供了有效的评测框架，揭示了当前模型在此类复杂任务中的局限性，并为未来研究指明了改进方向。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [21] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [22] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于离散动态图神经网络（DDGNN）的毫米波雷达点云人体活动识别方法，通过星形图表示解决点云稀疏和变尺寸问题，在资源受限平台上实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达点云存在稀疏性和变尺寸问题，现有基于视觉系统的预处理算法不适用于雷达系统，需要专门针对雷达特性的特征提取方法。

Method: 设计星形图表示手动添加的静态中心点与动态雷达点之间的高维相对关系，采用DDGNN学习变尺寸星形图中的时空特征。

Result: 在真实HAR数据集上达到94.27%的分类准确率，接近基于骨架视觉方法的97.25%，在树莓派4上验证了资源受限平台的有效性。

Conclusion: 该方法优于其他基线方法，无需重采样或帧聚合器，通过全面的消融研究验证了模型设计的有效性。

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [23] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

TL;DR: 本文研究了联邦学习在卫星船舶检测中的应用，比较了四种FL模型与本地训练基线的性能，发现FL模型能显著提升小数据集上的检测精度，接近全局训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决卫星船舶检测中数据隐私保护问题，避免商业卫星图像和敏感船舶标注数据的共享需求。

Method: 使用YOLOv8船舶检测模型，评估FedAvg、FedProx、FedOpt和FedMedian四种联邦学习算法，并与本地独立训练基线对比。

Result: FL模型相比小规模本地数据集训练显著提升检测精度，性能接近使用所有数据集的全局训练水平。

Conclusion: 联邦学习是卫星船舶检测的有效隐私保护方案，选择合适的FL配置参数对优化检测精度和计算效率至关重要。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [24] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

TL;DR: 本文提出了一种监督语义分割工作流，旨在提高火灾后烧毁区域（BA）划分的性能和效率，特别针对时间紧迫的应急管理场景。


<details>
  <summary>Details</summary>
Motivation: 当前烧毁区域划分方法依赖计算机视觉模型，但往往忽略了其在时间紧迫的应急管理场景中的适用性，需要更高效和实用的解决方案。

Method: 使用SPOT-6/7高分辨率遥感影像，采用U-Net和SegFormer模型进行语义分割，结合土地覆盖数据作为辅助任务，并应用测试时增强（TTA）技术。

Result: U-Net和SegFormer在有限训练数据下表现相似，但SegFormer资源需求更高。加入土地覆盖数据增强了模型鲁棒性而不增加推理时间，TTA提高了性能但增加了推理时间，可通过混合精度等方法优化。

Conclusion: 提出的工作流在烧毁区域划分中平衡了性能与效率，U-Net更适合应急场景，辅助任务和TTA技术可进一步提升模型实用性和鲁棒性。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [25] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

TL;DR: CreativeVR是一个针对AI生成视频和真实视频的修复框架，专门处理严重结构和时间伪影，通过单一精度控制实现精确修复与结构校正的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有文本转视频扩散模型在精细结构上表现脆弱，产生扭曲的人脸、手部、背景和时间不一致的运动，而传统视频修复方法无法有效处理这类严重结构伪影。

Method: 提出基于深度适配器的扩散先验引导视频修复框架，包含时间一致性退化模块，在训练时应用精心设计的变换来模拟真实结构失效。

Result: 在AIGC54基准测试中取得最先进结果，对严重伪影视频修复效果显著，在标准视频修复基准上表现竞争力，处理速度达13FPS（720p，单A100）。

Conclusion: CreativeVR框架有效解决了AI生成视频和低质量真实视频中的结构伪影问题，提供了感知质量与保真度之间的可控权衡。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [26] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

TL;DR: BAgger是一种自监督方案，通过构建校正轨迹来缓解自回归视频模型中的曝光偏差问题，提升长期视频生成的稳定性和一致性


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频模型中的曝光偏差问题，即训练时使用干净上下文而推理时使用自生成帧导致错误累积和质量漂移

Method: 引入Backwards Aggregation (BAgger)方案，从模型自身生成的轨迹中构建校正轨迹，使用标准得分或流匹配目标进行训练，避免使用大型教师模型和长链反向传播

Result: 在文本到视频、视频扩展和多提示生成任务中，观察到更稳定的长期运动和更好的视觉一致性，减少了漂移现象

Conclusion: BAgger能有效改善自回归视频模型的长期生成质量，为世界建模提供了更可靠的方法

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [27] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

TL;DR: RePack通过将高维视觉基础模型表示投影到低维流形，解决了信息过载问题，显著加速了扩散变换器的收敛并提高了图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉基础模型的高维表示在注入潜在扩散模型时可能导致信息过载，特别是当特征尺寸超过原始图像时，影响解码效果。

Method: 提出RePack框架，将高维VFM表示转换为更紧凑、解码器友好的表示，通过投影到低维流形来过滤非语义噪声并保留核心结构信息。

Result: 在DiT-XL/2上，RePack仅用64轮训练就达到FID 3.66，比最先进方法快35%，显著加速收敛并提升重建质量。

Conclusion: RePack能有效提取VFM表示的核心语义，同时规避高维度的副作用，为扩散模型的高效训练提供了有效解决方案。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [28] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

TL;DR: VEGAS是一种简单有效的推理时方法，通过将视觉编码器的注意力图注入到语言模型的中间层，自适应地引导未能聚焦关键图像对象的标记，显著减少大视觉语言模型（LVLM）的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型（LVLM）在处理视觉和文本输入时经常产生与视觉证据事实不符的流畅输出，即产生幻觉。研究发现，当LVLM的最终视觉注意力图未能聚焦关键图像对象时容易产生幻觉，而视觉编码器更集中的注意力图能有效减少幻觉。

Method: 提出VEGAS方法，分析解码过程中的视觉-文本冲突，发现冲突在语言模型的中间层达到峰值。将视觉编码器的注意力图注入到这些中间层，自适应地引导未能聚焦关键图像对象的标记。

Result: 在多个基准测试上的广泛实验表明，VEGAS在减少幻觉方面始终达到最先进的性能。

Conclusion: 视觉编码器自身的注意力图是有效抑制LVLM幻觉的关键，VEGAS方法通过简单的注意力注入机制实现了显著的幻觉减少效果。

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [29] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

TL;DR: SPDMark是一种基于选择性参数位移的生成式视频水印框架，通过在视频扩散模型中修改参数子集来嵌入水印，具有不可感知性、鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成模型的兴起需要可靠的水印方案来检测和追踪生成视频的来源，现有方法在不可感知性、鲁棒性和计算效率方面存在不足。

Method: 使用低秩适应(LoRA)实现层间基位移，通过联合训练基位移和水印提取器，结合消息恢复、感知相似性和时间一致性损失。采用密码学哈希函数生成帧特定水印消息，并使用最大二分匹配进行水印提取。

Result: 在文本到视频和图像到视频生成模型上的评估表明，SPDMark能够生成不可感知的水印，并以高准确率恢复水印，同时对各种常见视频修改具有鲁棒性。

Conclusion: SPDMark框架有效解决了视频水印在不可感知性、鲁棒性和效率方面的挑战，为生成式视频的溯源提供了可靠解决方案。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [30] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 本文研究了在光学和数字全息显微镜图像中自动识别花粉的方法，发现全息图像识别效果较差，通过GAN生成合成数据可提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决全息显微镜图像中花粉识别困难的问题，因为全息图像存在散斑噪声和双像伪影，与光学图像差异较大。

Method: 使用YOLOv8s进行目标检测，MobileNetV3L进行分类，并采用Wasserstein GAN生成合成全息图像来增强数据集。

Result: 光学图像检测mAP50达91.3%，分类准确率97%；全息图像检测mAP50仅8.15%，分类准确率50%。使用GAN合成数据后，检测性能提升至15.4%。

Conclusion: GAN数据增强可缩小光学与全息图像识别性能差距，为兽医成像的全自动全息工作流程迈出重要一步。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


### [31] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

TL;DR: 该论文提出了EchoGround-MIMIC数据集和EchoVLM模型，通过测量基础的多模态训练方法，在超声心动图解释中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 超声心动图解释是劳动密集型的多模态任务，现有视觉语言模型在超声心动图领域应用受限，主要缺乏大规模临床基础的数据集和测量推理能力。

Method: 构建EchoGround-MIMIC数据集（19,065个图像-文本对），提出EchoVLM模型，采用视图感知对比损失和否定感知对比损失两种新的预训练目标。

Result: 在36个临床任务中实现最先进性能：零样本疾病分类AUC达86.5%，视图分类准确率达95.1%，并证明临床基础的多模态预训练可产生可迁移的视觉表示。

Conclusion: EchoVLM成为端到端超声心动图解释的基础模型，将发布数据集和代码以促进多模态超声心动图解释的进一步研究。

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [32] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

TL;DR: 该研究提出了一种基于patch的持久同调构建方法，用于3D CT医学影像数据，相比传统的3D立方体复形方法在分类性能和时间效率上都有显著提升


<details>
  <summary>Details</summary>
Motivation: 传统基于3D立方体复形的持久同调方法在处理高分辨率CT图像时存在性能不足和计算复杂度高的问题，需要更高效的拓扑特征提取方法

Method: 开发了一种新颖的基于patch的持久同调构建方法，专门针对体积医学影像数据设计，并通过多组3D CT图像数据集进行广泛实验验证

Result: 提出的方法在所有数据集上平均提升了10.38%的准确率、6.94%的AUC、2.06%的灵敏度、11.58%的特异性和8.51%的F1分数，同时在时间效率上也有优势

Conclusion: 基于patch的TDA方法在CT图像分析中优于传统方法，研究还提供了方便的Python包Patch-TDA来促进该方法的应用

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [33] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

TL;DR: 本文提出了最大的道路损坏评估基准数据集，包含18个基线模型，解决了现有数据集规模小、分辨率低、缺乏操作验证的问题，并发现道路线空间不对齐会导致模型性能下降5.596%。


<details>
  <summary>Details</summary>
Motivation: 现有灾后道路损坏评估数据集存在规模小、依赖低分辨率图像、缺乏操作验证等局限性，无法满足应急管理需求。

Method: 使用10次联邦宣布灾害的sUAS图像，标注657.25公里道路并按10类标签方案分类，训练18个基线模型，在2024年飓风响应中部署验证，并提供9,184条道路线调整以解决空间对齐问题。

Result: 当模型部署到实际不对齐的道路线时，平均性能下降5.596% Macro IoU；若不考虑空间对齐，约8%的不良条件道路会被错误标记，9%的道路线偏离实际道路。

Conclusion: 道路线空间不对齐是ML、CV和机器人社区需要解决的关键问题，以提高灾害期间决策的有效性和准确性。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [34] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

TL;DR: 开发深度学习模型融合遥感观测和物理模型数据，生成格陵兰冰盖每日100米分辨率的地表融水分布图，解决现有方法时间或空间分辨率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速融化过程尚未完全理解且难以测量，现有融水分布图在时间和空间分辨率上存在权衡，无法同时满足高时空分辨率需求。

Method: 使用深度学习模型（UNet和DeepLabv3+）对区域气候模型输出进行时空降尺度，融合合成孔径雷达、被动微波和数字高程模型数据，在2017-2023年期间对赫尔海姆冰川进行研究。

Result: 基于深度学习的多数据融合方法在研究区域准确率达到95%，显著优于仅依赖区域气候模型（83%）或被动微波观测（72%）的非深度学习方法。SAR数据的滑动窗口计算也达到90%准确率。

Conclusion: 深度学习融合多源数据的方法能有效生成高时空分辨率的融水分布图，为理解冰盖融化过程提供重要工具，并发布了MeltwaterBench数据集供后续研究比较。

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [35] [Open Horizons: Evaluating Deep Models in the Wild](https://arxiv.org/abs/2512.12146)
*Ayush Vaibhav Bhatti,Deniz Karakay,Debottama Das,Nilotpal Rajbongshi,Yuito Sugimoto*

Main category: cs.CV

TL;DR: 本文对CIFAR-10数据集上的开放集识别(OSR)和少样本类增量学习(FSCIL)进行了统一实验研究，比较了不同视觉编码器和评分函数在未知类别检测和增量学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 开放世界部署需要模型能够识别已知类别并在出现新类别时保持可靠性能，因此需要系统研究开放集识别和少样本类增量学习方法。

Method: 对于OSR，比较了ResNet-50、ConvNeXt-Tiny和CLIP ViT-B/16三种预训练编码器，使用线性探测和四种后验评分函数；对于FSCIL，比较了SPPR、OrCo和ConCM方法在1、5、10样本场景下的性能。

Result: CLIP编码器在未知样本检测上表现最佳，Energy评分函数最稳定；在FSCIL中，ConCM在10样本场景下达到84.7%准确率，所有方法在5样本后趋于饱和。

Conclusion: 研究揭示了骨干网络架构和评分机制对未知检测的影响，以及基于原型的方法在增量适应中如何缓解灾难性遗忘问题。

Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.

</details>


### [36] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

TL;DR: 这篇论文提出了一个音频-视觉框架，利用被动场景声音作为补充线索，用于野外视频中的相对相机姿态估计，在视觉信息退化时表现出鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉方法在运动模糊或遮挡等视觉退化条件下往往表现不佳，而被动场景声音可以提供互补的线索来改善相对相机姿态估计。

Method: 引入一个简单但有效的音频-视觉框架，将到达方向（DOA）频谱和双耳化嵌入集成到最先进的纯视觉姿态估计模型中。

Result: 在两个大型数据集上的结果显示，相比强视觉基线有持续提升，且在视觉信息被破坏时表现出鲁棒性。

Conclusion: 这是首个成功利用音频进行真实世界视频中相对相机姿态估计的工作，确立了日常音频作为经典空间挑战的一个有前景的信号。

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [37] [SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation](https://arxiv.org/abs/2512.12193)
*Xuancheng Xu,Yaning Li,Sisi You,Bing-Kun Bao*

Main category: cs.CV

TL;DR: SMRABooth是一个定制化视频生成方法，通过自监督编码器和光流编码器提供对象级的主体和运动表示，利用LoRA微调实现主体外观保持和运动一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保证主体外观相似性和运动模式一致性，缺乏对主体和运动的对象级指导。

Method: 采用三阶段方法：1）自监督编码器提取主体表示指导主体对齐；2）光流编码器提取结构一致的对象级运动轨迹；3）提出主体-运动关联解耦策略，在位置和时间上应用稀疏LoRA注入。

Result: 实验表明SMRABooth在主体和运动定制方面表现出色，能够保持一致的主体外观和运动模式。

Conclusion: 该方法在可控文本到视频生成中有效解决了主体外观保持和运动一致性的问题。

Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.

</details>


### [38] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

TL;DR: 提出了一种轻量级周界跟踪方法，用于在带宽受限的野火环境下工作的微型无人机团队，通过热成像和RGB图像融合实现边界检测，并在嵌入式平台上实现低延迟性能。


<details>
  <summary>Details</summary>
Motivation: 解决在野火等紧急侦察场景中，微型无人机团队在带宽受限条件下进行快速部署和稳定周界跟踪的需求。

Method: 使用热成像帧通过自适应阈值和形态学细化生成热区域掩码，RGB帧提供边缘线索并通过梯度滤波抑制误检；采用规则级融合策略选择边界候选点，并用Ramer-Douglas-Peucker算法简化；集成周期性信标和惯性反馈回路以应对GPS退化。

Result: 小规模仿真显示，与纯边缘跟踪基线相比，平均路径长度和边界抖动减少，同时保持环境覆盖；在标准微型平台上可实现10-15 m/s的前向运动，计算利用率低。

Conclusion: 该方法在嵌入式SoC平台上实现了低于50 ms的延迟，具备快速部署能力，适用于紧急侦察应用，仅需鲁棒传感和最小通信。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [39] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

TL;DR: 本文提出了一个大规模、长期的英国布里斯托尔城市街灯视觉数据集，包含52.6万张图像，涵盖不同光照、天气和季节条件，并提供了基于CNN-VAE的自监督框架和两种漂移度量方法。


<details>
  <summary>Details</summary>
Motivation: 为智能城市部署中的视觉漂移、异常检测和MLOps策略研究提供真实世界的数据基准。

Method: 使用22个固定角度摄像机收集数据，采用卷积变分自编码器（CNN-VAE）分别对每个摄像头节点和昼夜图像集进行训练，定义相对质心漂移和相对重建误差两种漂移度量。

Result: 创建了一个包含丰富元数据的公开数据集，支持模型稳定性评估、漂移感知学习和部署就绪的视觉系统研究。

Conclusion: 该数据集为长期模型稳定性评估和城市场景理解等下游应用提供了现实、细粒度的基准，数据已公开可用。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [40] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

TL;DR: 该论文提出了ALERT数据集和ISA-ViT框架，解决了UWB雷达在分心驾驶检测中的两大挑战：缺乏大规模真实数据集和ViT模型对非标准维度数据的适应问题。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶是全球致命事故的主要原因。虽然IR-UWB雷达在DAR中具有抗干扰、低功耗和隐私保护等优势，但缺乏大规模真实数据集和ViT模型对UWB数据非标准维度的适应问题限制了其应用。

Method: 提出ALERT数据集（包含10,220个真实驾驶条件下的雷达样本）和ISA-ViT框架。ISA-ViT通过调整补丁配置、利用预训练位置嵌入向量，以及结合距离域和频域特征的域融合策略，在保持雷达特定信息的同时适应ViT输入要求。

Result: 实验表明，ISA-ViT在基于UWB的DAR任务中比现有ViT方法的准确率提升了22.68%。

Conclusion: 通过公开ALERT数据集和详细描述输入尺寸无关策略，该工作促进了更鲁棒和可扩展的分心驾驶检测系统的开发，适用于实际部署。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [41] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

TL;DR: 该研究提出了一种结合ResNet-50和图卷积网络的深度学习管道，用于识别自闭症儿童在与人形机器人互动时的情绪反应，在印度建立了首个大规模自闭症情绪分析数据集。


<details>
  <summary>Details</summary>
Motivation: 理解自闭症儿童在社会互动中的情绪反应是发展心理学和人机交互领域的关键挑战，目前缺乏针对自闭症儿童的情绪识别研究。

Method: 使用混合模型：基于ResNet-50的CNN和三层GCN，结合视觉和几何特征（MediaPipe FaceMesh地标点），通过DeepFace和FER模型进行概率标注，利用KL散度优化融合嵌入进行最终分类。

Result: 该方法在建模细微情感反应方面表现出稳健性能，能够有效捕捉神经多样性儿童的微情绪线索。

Conclusion: 该研究为临床和治疗性人机交互中的自闭症儿童情感分析提供了重要基础，对未来的个性化辅助技术具有重要贡献。

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [42] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: CineLOG是一个包含5000个高质量视频片段的新数据集，带有详细的场景描述、基于标准电影分类法的明确相机指令和类型标签。作者提出了一个新颖的管道，将复杂的文本到视频生成任务分解为四个阶段，并引入了轨迹引导过渡模块来生成连贯的多镜头序列。


<details>
  <summary>Details</summary>
Motivation: 解决可控视频合成中的精细控制问题，特别是针对相机轨迹和类型等电影属性。现有数据集存在数据不平衡、标签噪声和模拟到现实的差距问题。

Method: 将文本到视频生成任务分解为四个阶段：剧本生成、相机轨迹规划、过渡生成和视频合成。引入了轨迹引导过渡模块实现平滑的时空插值。

Result: 广泛的人类评估显示，该管道在遵循特定相机和剧本指令方面显著优于最先进的端到端T2V模型，同时保持专业的视觉质量。

Conclusion: CineLOG数据集和提出的管道为可控视频合成提供了有效的解决方案，特别适用于电影属性的精细控制，代码和数据已公开可用。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [43] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

TL;DR: 该论文提出了一种评估视觉语言模型推理链视觉忠实性的方法，并开发了轻量级自反思程序来检测和修复不忠实的感知步骤，以提高多模态推理的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在生成显式推理链时存在新的失败模式：模型可能通过视觉上不忠实的中间步骤得出正确答案，或者进行忠实推理但在最终预测上失败。仅衡量最终答案准确性的标准评估无法区分这些行为。

Method: 提出了一个无需训练和参考的框架，将推理链分解为感知与推理步骤，使用现成的VLM判断器进行步骤级忠实性评估。基于此指标，开发了轻量级自反思程序，检测并局部重新生成不忠实的感知步骤。

Result: 在多个经过推理训练的VLM和感知密集型基准测试中，该方法降低了不忠实感知率，同时保持了最终答案的准确性。

Conclusion: 该方法提高了多模态推理的可靠性，为评估和改进视觉语言模型的推理链提供了有效工具。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [44] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

TL;DR: 本文提出了一种属性中心表示（ACR）的零样本学习框架，通过属性解缠解决属性纠缠问题，在多个基准数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度类别识别中的属性纠缠问题，传统方法将不同属性（如颜色、形状、纹理）压缩到单一视觉嵌入中，导致关键区分特征被掩盖。

Method: 使用两个混合专家组件：补丁专家混合（MoPE）和属性专家混合（MoAE）。MoPE通过双级路由机制将图像块分配给专用专家处理相干属性族，MoAE将专家精炼特征投影为稀疏的部件感知属性图。

Result: 在CUB、AwA2和SUN等零样本学习基准数据集上实现了持续的最先进性能。

Conclusion: ACR框架通过属性解缠学习有效解决了属性纠缠问题，为零样本细粒度分类提供了稳健的解决方案。

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [45] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

TL;DR: 本文提出了ProImage-Bench基准测试，用于评估专业图像生成模型在科学插图生成方面的能力，发现现有模型在科学准确性方面存在显著差距，并通过迭代编辑方法显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在生成视觉上合理的图片方面表现良好，但在生成信息密集、科学精确的专业插图方面存在不足。需要建立一个专门的基准来量化和改进模型在科学插图生成方面的性能。

Method: 1) 构建ProImage-Bench基准，包含654个来自真实教科书和技术报告的图表；2) 设计分层评分标准，将正确性分解为6,076个标准和44,131个二元检查；3) 使用大型多模态模型自动评估；4) 通过迭代编辑方法，将失败的检查反馈给编辑模型进行优化。

Result: 基准测试显示，最佳基础模型在评分标准准确率上仅为0.791，标准得分为0.553，表明在科学保真度方面存在显著差距。通过迭代编辑方法，模型性能从0.653提升到0.865（准确率）和从0.388提升到0.697（标准得分）。

Conclusion: ProImage-Bench为专业图像生成提供了严格的诊断工具，同时为改进符合规范的科学插图生成提供了可扩展的监督信号，证明了基于评分标准的迭代优化方法能够显著提升模型的科学准确性。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [46] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

TL;DR: 本研究比较了SynthSeg和SamSeg在婴儿脑MRI分割中的性能，发现SynthSeg在所有质量指标上均优于SamSeg，并提供了更可靠的体积和分形维度估计。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI的准确分割对于量化结构发育变化至关重要，但持续的髓鞘化和降低的组织对比度使得自动分割特别具有挑战性。

Method: 使用Baby Open Brains数据集（71次扫描，1-9个月），系统比较了SynthSeg和SamSeg两种方法的分割准确性及其对体积和分形维度估计的影响，评估指标包括Dice系数、IoU、95% Hausdorff距离和归一化互信息。

Result: SynthSeg在所有质量指标上均优于SamSeg（主要区域平均Dice > 0.8），其体积估计与手动参考高度匹配（平均+4%）。SamSeg系统性高估脑室和全脑体积（平均+76%）。分割准确性随年龄增长而提高，与髓鞘化过程中组织对比度增加一致。

Conclusion: SynthSeg为儿科MRI提供了最可靠的体积和FD结果，但由于分割相关的不确定性，应谨慎解释体积和FD的微小形态差异。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [47] [Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder](https://arxiv.org/abs/2512.12229)
*Tianyu Zhang,Dong Liu,Chang Wen Chen*

Main category: cs.CV

TL;DR: 提出了一种非对称极端图像压缩框架AEIC，使用浅层编码器结合扩散解码器，在极低比特率下实现高效编码和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有超低比特率图像压缩方法依赖大型预训练编码器，不适合在计算能力有限的边缘设备上部署，需要探索使用浅层编码器的可行性。

Method: 采用浅层编码器网络，结合一步扩散解码器保持重建质量；设计双端特征蒸馏方案，将知识从中等编码器迁移到浅层编码器变体。

Result: AEIC在超低比特率下超越了现有方法的率失真感知性能，编码效率达到35.8 FPS（1080P输入），解码速度与现有方法相当。

Conclusion: AEIC框架证明了浅层编码器在超低比特率图像压缩中的可行性，实现了编码简单性和解码质量的平衡，适合边缘设备部署。

Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.

</details>


### [48] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的方法，通过直接在LLM输出token上应用分割目标来检测视频时刻和亮点，避免了传统文本生成方法无法提供帧级预测梯度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于transformer的方法和生成式多模态LLM方法虽然有效，但基于文本的生成无法为帧级预测提供直接梯度，而强化学习方法存在局限性。

Method: 将固定数量的帧输入LLM，并强制其输出连续的'0'和'1'字符序列（每个字符对应一帧），'0'代表背景概率，'1'代表前景概率。训练时结合分割损失和因果语言模型损失。

Result: 在QVHighlights数据集上，仅采样25帧（少于同类方法一半）就实现了56.74的HIT@1亮点检测分数，以及35.28 MAP的时刻检索分数，超过了基线。

Conclusion: 该方法在保持LLM语言能力的同时实现了高效的视频时刻检测，分割损失即使在因果LM损失平稳时也能提供稳定的补充学习信号。

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [49] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

TL;DR: MetaTPT是一个元学习框架，通过学习自监督辅助任务来指导测试时提示调优，解决了传统方法在域偏移下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时提示调优（TPT）方法使用固定的数据增强策略，在更具挑战性的域偏移场景中可能失效，需要更灵活的自适应增强方法。

Method: 采用双循环优化范式：内环学习生成信息丰富视图的自监督任务，外环通过强制这些视图的一致性进行提示调优，动态学习参数化增强策略。

Result: 在域泛化和跨数据集基准测试中，MetaTPT实现了最先进的性能表现。

Conclusion: 通过将增强学习与提示调优相结合，MetaTPT显著提升了模型在域偏移下的测试时适应能力。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [50] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

TL;DR: 提出了一种用于面部表情识别的混合持续学习框架，结合深度卷积特征和面部动作单元，使用贝叶斯高斯混合模型来缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中的普及，需要能够识别和适应人类情绪的能力。面部表情识别是推断情感状态的主要渠道，但情绪的动态性和文化差异性要求模型能够持续学习而不遗忘先验知识。

Method: 提出混合框架，整合深度卷积特征和面部动作单元两种互补模态，使用贝叶斯高斯混合模型进行建模，提供轻量级的概率解决方案，避免重新训练。

Result: 在CFEE数据集上的实验表明，该模型能够先学习基本表情，然后逐步识别复合表情，表现出更高的准确性、更强的知识保留能力和更少的遗忘。

Conclusion: 该框架有助于开发具有情感智能的AI系统，在教育、医疗和自适应用户界面等领域具有应用前景。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [51] [Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection](https://arxiv.org/abs/2512.12281)
*Jiahao Zhao*

Main category: cs.CV

TL;DR: Cognitive-YOLO是一个基于LLM的架构合成框架，直接从数据集内在特征生成目标检测网络配置，无需传统搜索循环，实现了优越的性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: 传统手动设计目标检测架构耗时费力，NAS计算成本高，现有LLM方法仅作为搜索循环中的优化器而非直接从数据整体理解生成架构。

Method: 三阶段方法：1）分析模块提取数据集元特征；2）LLM基于特征和RAG检索的组件推理，生成NADL结构化描述；3）编译器实例化为可部署模型。

Result: 在五个不同目标检测数据集上的实验表明，Cognitive-YOLO生成的架构性能优越，在多个基准测试中展现出卓越的性能-参数权衡。

Conclusion: 消融研究证明LLM的数据驱动推理是性能主要驱动力，对数据"第一原理"的深入理解比单纯检索SOTA组件更重要。

Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.

</details>


### [52] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 该论文提出了第一个包含真实目标图像的拖拽式图像编辑基准RealDrag，包含400多个标注样本，并设计了四个专门的任务指标来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前拖拽式图像编辑模型的评估缺乏标准化基准和指标，评价协议不一致且缺少真实目标图像，导致模型间难以客观比较。

Method: 构建RealDrag基准数据集，包含400+人工标注样本，提供源/目标图像、控制点/目标点、可编辑区域掩码和描述性标注；提出四个新指标：语义距离(SeD)、外部掩码保持分数(OMPS)、内部补丁保持分数(IPPS)和方向相似性(DiS)。

Result: 使用该基准对17个SOTA模型进行了首次大规模系统分析，揭示了当前方法之间的明确权衡，并建立了可复现的基线。

Conclusion: RealDrag基准为拖拽式图像编辑提供了可靠的评估标准，有助于指导未来研究，数据集和评估工具将公开提供。

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [53] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

TL;DR: GrowTAS是一种渐进式Transformer架构搜索方法，通过从小到大的子网训练减少权重共享带来的干扰，提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有TAS方法中所有子网共享权重导致小规模子网性能严重下降，而训练良好的小子网可以作为大子网的良好基础

Method: 提出渐进训练框架GrowTAS，先训练小子网，然后逐步加入大子网；还提出GrowTAS+，仅微调部分权重以进一步提升大子网性能

Result: 在ImageNet和多个迁移学习基准测试（CIFAR-10/100、Flowers、CARS、INAT-19）上表现出优于当前TAS方法的性能

Conclusion: GrowTAS通过渐进训练策略有效减少了权重共享干扰，稳定了训练过程，显著提升了Transformer架构搜索的性能

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [54] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

TL;DR: Intention-Drive是首个评估自动驾驶系统从高级人类意图转化为安全驾驶动作能力的基准测试，包含新数据集和基于意图成功率(ISR)的评估协议，揭示了现有模型在场景理解和意图理解方面的显著性能缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统仅能执行低级指令，缺乏理解并满足高级人类意图的能力，而这一能力是实现真正智能自主驾驶的关键。现有研究缺乏标准化基准来推动这一复杂任务的发展。

Method: 提出Intention-Drive基准，包括两个核心贡献：(1) 包含复杂场景和对应自然语言意图的新数据集；(2) 以意图成功率(ISR)为中心的新型评估协议，评估语义层面的意图实现而非简单的几何精度。

Result: 通过对多种基线模型在Intention-Drive上的广泛评估，发现基线模型在实现全面场景和意图理解方面存在显著性能缺陷，难以胜任这一高级任务。

Conclusion: 该研究填补了自动驾驶领域的关键空白，为从命令跟随者向意图实现者的范式转变提供了标准化评估工具，揭示了当前技术在这一方向上的局限性。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [55] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

TL;DR: OMUDA是一个无监督域自适应语义分割框架，通过分层掩码策略在上下文、特征和类别三个层面减少域间差异，在多个基准测试中取得SOTA结果，平均提升7%。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法在处理跨域语义分割时面临三个主要挑战：跨域上下文模糊性、不一致的特征表示和类别级伪标签噪声，导致域间差距难以有效弥合。

Method: 提出OMUDA框架，包含三个分层掩码策略：1)上下文感知掩码(CAM)区分前景背景平衡全局上下文和局部细节；2)特征蒸馏掩码(FDM)通过预训练模型知识迁移增强特征学习；3)类别解耦掩码(CDM)通过显式建模类别不确定性减轻伪标签噪声影响。

Result: 在SYNTHIA->Cityscapes和GTA5->Cityscapes等跨域语义分割基准测试中，OMUDA能够无缝集成到现有UDA方法中，一致取得最先进结果，平均性能提升7%。

Conclusion: OMUDA通过分层掩码范式在上下文、表示和分类三个层面有效减少域偏移，为UDA语义分割提供了超越现有方法的统一解决方案。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [56] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

TL;DR: MRD是一种使用物理可微分渲染的方法，通过寻找物理上不同但产生相同模型激活的3D场景参数来探究视觉模型对生成性3D场景属性的隐式理解。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在视觉基准测试中取得了显著成功，但理解和解释这些模型的表示和决策仍然困难。虽然视觉模型通常在2D输入上训练，但通常假设它们对底层3D场景形成了隐式表示。

Method: 使用基于物理的可微分渲染，通过优化找到在物理上不同但产生相同模型激活的3D场景参数（即模型元体）。这些重建结果始终基于物理场景描述，可以探究模型对特定场景属性的敏感性。

Result: 在形状和材质参数恢复实验中，目标场景和优化场景之间的模型激活高度相似，但视觉结果各异。重建结果有助于定性地研究模型对哪些物理场景属性敏感或不敏感。

Conclusion: MRD有望通过分析物理场景参数如何驱动模型响应变化，促进我们对计算机和人类视觉的理解。

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [57] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: WeDetect是一个基于检索范式的开放词汇目标检测模型家族，通过双塔架构实现实时检测，并在检测、提议生成、目标检索和引用表达理解等多个任务上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 探索基于检索的开放词汇目标检测方法的优势，特别是在效率和通用性方面，避免跨模态融合层的计算开销。

Method: 采用非融合的双塔架构，将目标检测视为检索问题，通过区域与文本查询在共享嵌入空间中的匹配实现识别。提出WeDetect-Uni通用提议生成器和WeDetect-Ref基于LMM的目标分类器。

Result: 在15个基准测试中达到最先进性能，支持实时检测，并实现历史数据中的目标检索和复杂引用表达理解。

Conclusion: WeDetect模型家族在统一的检索框架下整合了多个视觉任务，展示了检索范式在开放词汇目标检测中的高效性和通用性。

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [58] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

TL;DR: UniCoDe是一个统一的算法，将采样和梯度引导方法结合，通过集成局部梯度信号来提高采样效率，并在奖励对齐和扩散模型先验偏差之间实现更好的权衡。


<details>
  <summary>Details</summary>
Motivation: 对齐扩散模型输出与下游目标对于提高任务特定性能至关重要。现有的无训练推理方法存在采样效率低或梯度近似不准确的问题。

Method: 提出UniCoDe算法，将采样方法和梯度引导方法统一到一个框架中，在采样过程中集成局部梯度信号，解决复杂奖励采样方法的效率问题。

Result: 实验结果表明，UniCoDe在一系列任务中与最先进的基线方法保持竞争力。

Conclusion: UniCoDe通过结合两种范式的优势，实现了更高效的采样和更好的权衡效果，代码已开源。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [59] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

TL;DR: 本文提出了TCLeaf-Net，一种基于Transformer和卷积混合的叶片病害检测网络，以及Daylily-Leaf数据集，用于在复杂田间环境下实现高精度的叶片病害检测。


<details>
  <summary>Details</summary>
Motivation: 田间环境下叶片病害检测面临复杂背景干扰、域偏移和病变级别数据集有限等挑战，需要开发鲁棒的检测模型。

Method: 提出TCLeaf-Net网络，包含三个关键模块：Transformer-卷积模块（TCM）抑制非叶片区域干扰，原始尺度特征召回与采样（RSFRS）块保留空间细节，可变形对齐FPN（DFPN）块处理病变尺度变化和特征偏移。

Result: 在Daylily-Leaf数据集田间分割上，TCLeaf-Net比基线模型mAP@50提升5.4个百分点至78.2%，计算量减少7.5 GFLOPs，GPU内存使用降低8.7%，在多个植物病害数据集上表现优异。

Conclusion: TCLeaf-Net在复杂田间环境下表现出强大的病变检测能力，具有良好的鲁棒性和泛化性，适用于多种植物病害检测场景。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [60] [VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding](https://arxiv.org/abs/2512.12360)
*Yufei Yin,Qianke Meng,Minghao Chen,Jiajun Ding,Zhenwei Shao,Zhou Yu*

Main category: cs.CV

TL;DR: VideoARM提出了一种基于代理推理的分层记忆范式，用于长视频理解，通过自适应循环减少token消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决长视频理解中时间结构复杂、多模态信息密集的问题，克服现有方法依赖手工推理流程或高token消耗预处理方案的局限性。

Method: 采用代理推理范式，通过控制器自主调用工具进行粗到细的视频解析，结合分层多模态记忆动态捕获和更新多级线索。

Result: 在主流基准测试中超越现有最优方法DVD，同时显著降低长视频处理的token消耗。

Conclusion: VideoARM通过自适应代理推理和分层记忆机制，为长视频理解提供了高效且性能优越的解决方案。

Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.

</details>


### [61] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

TL;DR: 本文提出STAGE工作流，通过结构化故事板而非稀疏关键帧来解决多镜头视频生成中的跨镜头一致性和电影语言问题，并引入ConStoryBoard数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于关键帧的视频生成方法难以保持跨镜头一致性和捕捉电影语言，而端到端方法计算成本高昂。

Method: 提出STAGE工作流：1) STEP2预测每个镜头的起止帧对构成故事板；2) 多镜头记忆包确保长程实体一致性；3) 双重编码策略保证镜头内连贯性；4) 两阶段训练学习镜头间电影化过渡。

Result: 实验证明STAGE在结构化叙事控制和跨镜头一致性方面表现优异。

Conclusion: STAGE通过故事板锚定的生成方法有效解决了多镜头视频生成中的一致性和电影化问题，为叙事性视频生成提供了新思路。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [62] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

TL;DR: V-Warper是一个无需训练的粗到细视频个性化框架，通过两阶段方法在保持文本对齐和运动动态的同时显著提升外观保真度，无需大规模视频微调。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法依赖繁重的视频微调或大规模视频数据集，计算成本高且难以扩展，同时难以维持跨帧的细粒度外观一致性。

Method: 1) 轻量级粗外观适配阶段：仅使用少量参考图像，通过图像LoRA和主题嵌入适配编码全局主题身份；2) 推理时细外观注入阶段：通过RoPE-free中层查询-键特征计算语义对应关系，引导外观丰富的值表示变形到生成过程的语义对齐区域。

Result: V-Warper显著提高了外观保真度，同时保持了提示对齐和运动动态，且无需大规模视频微调即可高效实现这些增益。

Conclusion: V-Warper为解决视频个性化中的外观一致性问题提供了有效的训练免费解决方案，在计算效率和生成质量方面都有显著优势。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [63] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

TL;DR: M4Human是目前最大规模的多模态基准数据集，包含66.1万帧高分辨率毫米波雷达、RGB和深度数据，旨在推动基于雷达的人体网格重建研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模人体网格重建数据集主要依赖RGB视觉输入，但存在遮挡、光照变化和隐私问题。毫米波雷达能够实现隐私保护的室内人体感知，但现有雷达数据集存在标注稀疏、规模有限和动作简单的问题。

Method: 提出M4Human数据集，包含原始雷达张量和处理后的雷达点云，支持不同粒度的射频信号研究。数据集包含20名受试者和50种多样化动作，提供高质量的运动捕捉标注（3D网格和全局轨迹）。

Result: 建立了基于雷达张量和雷达点云的基准测试，以及多模态融合（RGB-D）的基准。实验结果表明M4Human对雷达人体建模具有重要意义，但在快速无约束运动下仍存在挑战。

Conclusion: M4Human数据集将推动基于雷达的人体网格重建研究发展，为隐私保护的人体感知提供重要支持。数据集和代码将在论文发表后开源。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [64] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

TL;DR: SR-DiT是一个集成多种训练效率优化技术的扩散变换器框架，在140M参数模型上实现了与更大模型相当的性能


<details>
  <summary>Details</summary>
Motivation: 现有扩散变换器训练效率提升技术大多孤立研究，缺乏对多种技术组合协同效应的探索

Method: 系统整合了token路由、架构改进和训练修改，并在表示对齐基础上构建

Result: 在ImageNet-256上达到FID 3.49和KDD 0.319，仅使用140M参数模型在40万次迭代内完成训练，无需分类器自由引导

Conclusion: SR-DiT在同等模型规模下达到state-of-the-art水平，为未来研究提供了计算友好的基准框架

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [65] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

TL;DR: D3D-VLP模型通过动态3D思维链和协同学习策略，解决了具身智能中端到端模型缺乏可解释性与模块化系统忽视组件间协同的问题，在多个导航和任务基准上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能中端到端模型缺乏可解释性和显式3D推理能力，以及模块化系统忽略跨组件相互依赖和协同作用的关键问题。

Method: 提出动态3D视觉-语言-规划模型(D3D-VLP)，包含：1) 动态3D思维链统一规划、定位、导航和问答；2) 协同学习策略通过掩码自回归损失从部分标注的混合数据中学习，实现不同组件间的相互监督和强化。

Result: 在多个基准测试中达到最先进水平，包括视觉语言导航(R2R-CE等)、目标导航(HM3D-OVON)和任务导向的顺序定位与导航(SG3D)，并通过真实世界移动操作实验验证有效性。

Conclusion: D3D-VLP成功桥接了端到端模型和模块化系统之间的差距，通过统一的3D-VLM和协同学习框架实现了可解释的3D推理和组件间协同，为具身智能提供了有效的解决方案。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [66] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

TL;DR: ArtGen是一个基于条件扩散的框架，能够从单视图图像或文本描述生成具有精确几何和连贯运动学的铰接3D对象，通过跨状态蒙特卡洛采样和思维链推理解决结构-运动纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖单视图输入表示闭合状态，导致几何形状和关节动力学纠缠，产生模糊或不现实的运动学结构。

Method: 采用跨状态蒙特卡洛采样强制全局运动一致性，集成思维链推理模块推断结构先验，使用稀疏专家扩散变换器处理不同运动交互，结合局部-全局注意力增强的组合3D-VAE潜在先验。

Result: 在PartNet-Mobility基准测试上的广泛实验表明，ArtGen显著优于现有最先进方法。

Conclusion: ArtGen能够从单视图输入生成具有准确几何和连贯运动学的铰接3D对象，有效解决了结构-运动纠缠问题。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [67] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的端到端跨级融合方法，将对象列表信息与原始相机图像结合进行3D目标检测，通过可变形高斯掩码和伪对象列表生成技术显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统传感器融合系统通常只能获得处理后的对象列表而非原始传感器数据，现有方法在对象级别进行融合效率较低，需要一种能够直接整合高度抽象对象列表信息与原始图像的方法。

Method: 使用Transformer作为融合框架，将对象列表作为去噪查询输入，结合可学习查询进行特征聚合；引入基于对象列表位置和尺寸先验的可变形高斯掩码来引导注意力；提出从真实边界框生成伪对象列表的方法来模拟状态噪声和误检漏检。

Result: 在nuScenes数据集上相比基于视觉的基线方法表现出显著的性能提升，在不同噪声水平的模拟对象列表和真实检测器上都展示了良好的泛化能力。

Conclusion: 该方法是首个进行跨级融合的工作，通过有效整合对象列表和原始图像信息，为3D目标检测提供了新的解决方案，具有实际应用价值。

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [68] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

TL;DR: 本文提出了一种基于图注意力网络（GAT）的框架，仅使用当前LiDAR帧即可重建缺失的垂直通道，无需相机图像或时间信息。


<details>
  <summary>Details</summary>
Motivation: 垂直光束丢失（由硬件老化、灰尘、雪、雾或强反射引起）会从点云中移除整个垂直切片，严重降低自动驾驶车辆的3D感知能力。

Method: 将每个LiDAR扫描表示为非结构化空间图：点为节点，边连接附近点同时保持原始光束索引顺序。多层GAT学习局部几何邻域的自适应注意力权重，并直接在丢失位置回归缺失的高程（z）值。

Result: 在1,065个原始KITTI序列上进行训练和评估，平均高度RMSE为11.67厘米，87.98%的重建点落在10厘米误差阈值内。单GPU每帧推理时间为14.65秒。

Conclusion: 结果表明，仅基于原始点云几何的纯图注意力模型能够有效恢复实际传感器退化下的丢失垂直光束。

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [69] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

TL;DR: 本文提出了一种基于检索增强生成（RAG）的零样本道路标志识别框架，无需特定任务训练即可实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在处理大量标志类别和创建完整标注数据集方面存在困难，需要一种无需大量标注数据的可扩展解决方案。

Method: 使用视觉语言模型（VLM）从输入图像生成文本描述，然后从参考设计向量数据库中检索最相关的标志候选集，最后通过大型语言模型（LLM）进行细粒度推理识别。

Result: 在俄亥俄州MUTCD的303个监管标志上测试，理想参考图像准确率达到95.58%，真实道路数据准确率为82.45%。

Conclusion: 基于RAG的架构为创建无需特定任务训练的可扩展、准确的道路标志识别系统提供了可行方案。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [70] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

TL;DR: ViInfographicVQA是首个越南语信息图表视觉问答基准，包含6747个真实信息图表和20409个问题-答案对，评估模型在数据丰富、布局复杂的视觉内容上的阅读和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准主要针对场景文本或自然图像，而信息图表需要更强的OCR、布局理解以及数值和语义推理能力，特别是在低资源语言如越南语中缺乏相关评估标准。

Method: 创建包含经济、医疗、教育等领域的真实信息图表数据集，设计单图任务（传统VQA）和多图任务（跨图像推理），评估多种最新视觉语言模型。

Result: 现有模型在ViInfographicVQA上表现存在显著差距，特别是在多图任务中涉及跨图像整合和非片段推理的问题上错误率最高。

Conclusion: 该基准为越南语InfographicVQA提供了评估标准，揭示了当前多模态模型在低资源语境下的局限性，鼓励未来开发布局感知和跨图像推理方法。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [71] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: Motus是一个统一的潜在动作世界模型，通过混合Transformer架构整合理解、视频生成和动作三个专家模块，实现多功能统一建模，在仿真和现实场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法使用孤立模型进行理解、世界建模和控制，这种碎片化阻碍了多模态生成能力的统一和大规模异构数据的学习，需要构建统一的系统。

Method: 提出MoT架构整合三个专家模块，采用UniDiffuser风格调度器实现不同建模模式的灵活切换，利用光流学习潜在动作，采用三阶段训练流程和六层数据金字塔进行大规模动作预训练。

Result: 在仿真环境中比X-VLA提升15%，比Pi0.5提升45%；在现实场景中提升11-48%，证明了统一建模方法对下游机器人任务的显著益处。

Conclusion: Motus通过统一建模所有功能和先验知识，显著提升了机器人任务的性能，为构建通用具身智能体提供了有效解决方案。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [72] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

TL;DR: BokehDepth是一个两阶段框架，通过将散焦作为无监督的几何线索，解耦散焦渲染和深度预测，提升散焦渲染质量和单目深度估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在散焦渲染和深度估计之间的耦合不完整。高质量散焦渲染依赖有噪声的深度图，而单目深度模型在弱纹理、远距离和几何模糊区域表现不佳，而这些区域正是散焦线索最有价值的地方。

Method: 第一阶段：基于预训练图像编辑骨干构建物理引导的可控散焦生成器，从单张清晰输入生成无深度的散焦堆栈。第二阶段：轻量级散焦感知聚合模块集成到现有单目深度编码器中，沿散焦维度融合特征，暴露稳定的深度敏感变化。

Result: 在多个挑战性基准测试中，BokehDepth相比基于深度图的散焦基线方法提升了视觉保真度，并显著提高了强单目深度基础模型的度量准确性和鲁棒性。

Conclusion: 该方法通过解耦散焦合成和深度预测，利用散焦作为辅助几何线索，有效解决了现有方法的局限性，实现了散焦渲染和深度估计的双重提升。

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [73] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

TL;DR: MMDrive是一个多模态视觉语言模型框架，将传统2D图像理解扩展到3D场景理解，通过融合占用图、LiDAR点云和文本描述，实现了自动驾驶场景的深度语义融合和空间感知。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于2D平面图像理解范式，无法有效感知3D空间信息并进行深度语义融合，导致在复杂自动驾驶环境中性能不佳。

Method: 提出MMDrive框架，包含文本导向多模态调制器和跨模态抽象器两个核心组件，前者基于问题语义动态加权各模态贡献，后者使用可学习抽象令牌生成紧凑的跨模态摘要。

Result: 在DriveLM和NuScenes-QA基准测试中取得显著性能提升，DriveLM上BLEU-4达54.56，METEOR达41.78，NuScenes-QA准确率达62.7%。

Conclusion: MMDrive突破了传统仅图像理解的限制，为可解释的自动驾驶场景理解提供了新基础，在复杂驾驶环境中实现稳健的多模态推理。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [74] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: Endless World是一个实时无限3D一致视频生成框架，通过条件自回归训练和全局3D感知注意力实现长序列稳定生成


<details>
  <summary>Details</summary>
Motivation: 解决长序列视频生成中3D结构不稳定、连贯性差的问题，特别是在流式场景下的挑战

Method: 采用条件自回归训练策略对齐新生成内容与现有帧，集成全局3D感知注意力提供几何指导，通过3D注入机制确保物理合理性和几何一致性

Result: 能够生成长而稳定的视觉连贯视频，在视觉保真度和空间一致性方面达到或优于现有方法的性能

Conclusion: Endless World框架在单GPU上实现实时推理，无需额外训练开销，为无限3D一致视频生成提供了有效解决方案

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [75] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

TL;DR: MindDrive是一个基于LLM的VLA框架，通过将连续动作空间转换为离散语言决策空间，解决了自动驾驶中在线强化学习的探索效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶VLA模型主要依赖模仿学习，存在分布偏移和因果混淆问题。在线强化学习虽然能解决这些问题，但在连续动作空间中探索效率低下。

Method: 使用两个LoRA参数的LLM：决策专家负责场景推理和驾驶决策，动作专家将语言决策动态映射为可行轨迹。通过轨迹级奖励反馈到推理空间，实现在离散语言决策空间中的试错学习。

Result: 在Bench2Drive基准测试中取得78.04的驾驶评分和55.09%的成功率，首次证明了在线强化学习在自动驾驶VLA模型中的有效性。

Conclusion: MindDrive通过将连续动作空间转换为离散语言决策空间，有效平衡了复杂场景中的最优决策、类人驾驶行为和在线强化学习的高效探索。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [76] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

TL;DR: 本文提出了高斯光子场（GPF），一种可学习的表示方法，将光子分布编码为各向异性3D高斯基元，用于加速多视角渲染。GPF将光子映射重新表述为连续可重用的辐射函数，避免了重复的光子追踪计算。


<details>
  <summary>Details</summary>
Motivation: 传统光子映射在渲染同一场景的多个视角时存在计算效率低下的问题，因为每个视角都需要独立的光子追踪和随机核估计，导致不可避免的冗余计算。

Method: 引入高斯光子场（GPF），从第一次SPPM迭代的物理追踪光子初始化，使用多视角监督优化，将基于光子的光传输提炼为连续场。训练后，该场支持沿相机光线的可微分辐射评估，无需重复光子追踪或迭代优化。

Result: 在具有复杂光传输（如焦散和镜面-漫反射相互作用）的场景上的广泛实验表明，GPF在达到光子级精度的同时，将计算量减少了数个数量级。

Conclusion: GPF统一了基于光子的渲染的物理严谨性和神经场景表示的效率，为多视角渲染提供了高效的解决方案。

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [77] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

TL;DR: PeRL-VL是一种解耦框架，通过分别改进视觉感知和文本推理来增强视觉语言模型的RLVR训练，解决了现有方法中视觉提取不准确和逻辑不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR训练方法在视觉语言模型中存在两个主要问题：视觉提取不准确（遗漏或幻觉细节）和逻辑不一致的思维链，这主要是因为可验证信号只监督最终答案。

Method: PeRL-VL提出感知和推理分离的框架：1）感知方面引入基于VLM的描述奖励来评估模型自生成图像描述的真实性和充分性；2）推理方面增加纯文本的推理SFT阶段，在富含逻辑的思维链数据上训练，独立于视觉增强连贯性和逻辑一致性。

Result: 在多样化多模态基准测试中，PeRL-VL将平均Pass@1准确率从63.3%（基础Qwen2.5-VL-7B）提升至68.8%，优于标准RLVR、纯文本推理SFT和从GPT-4o的直接多模态蒸馏。

Conclusion: PeRL-VL通过解耦感知和推理学习，有效解决了RLVR训练中的关键问题，显著提升了视觉语言模型的性能，为多模态推理提供了更可靠的解决方案。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [78] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

TL;DR: 提出AdaptiveDetector两阶段检测器-验证器框架，通过自适应阈值调整和成本敏感强化学习，在真实内窥镜恶劣条件下显著提升息肉检测召回率，减少漏检风险。


<details>
  <summary>Details</summary>
Motivation: 解决息肉检测器在真实临床环境中因光照变化、运动模糊等恶劣成像条件导致的性能下降问题，弥合实验室条件与临床实践之间的领域鸿沟。

Method: 采用YOLOv11检测器结合视觉语言模型验证器的两阶段框架，使用GRPO算法和不对称成本敏感奖励函数进行微调，构建合成测试平台进行零样本评估。

Result: 在合成退化的CVC-ClinicDB和Kvasir-SEG数据集上，召回率比单独使用YOLO提升14-22个百分点，精度保持在基线0.7点以下至1.7点以上。

Conclusion: 自适应阈值调整与成本敏感强化学习的结合实现了临床对齐的开放世界息肉检测，显著减少假阴性，降低癌前息肉漏检风险，改善患者预后。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [79] [Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention](https://arxiv.org/abs/2512.12498)
*Tasweer Ahmad,Arindam Sikdar,Sandip Pradhan,Ardhendu Behera*

Main category: cs.CV

TL;DR: 本文提出了一种基于补丁关系优化的少样本图像分类方法，通过图注意力网络学习补丁间依赖关系，提升缓存适配器的判别能力，在11个基准测试中优于现有方法，并引入了战场伤员识别数据集验证实用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于缓存的适配方法（如Tip-Adapter）仍继承CLIP的全局表示倾向，在少样本场景下难以获得最优的判别性表示。本文旨在通过补丁级关系学习解决这一问题。

Method: 提出关系门控图注意力网络，构建补丁图并进行边缘感知注意力计算，强调信息丰富的补丁间交互，生成上下文丰富的补丁嵌入。通过可学习的多聚合池化将这些嵌入组合成紧凑的任务判别表示。

Result: 在11个基准测试中 consistently 超越最先进的CLIP适配器和基于缓存的方法，同时保持零样本推理效率。在战场伤员识别任务上验证了实际应用价值。

Conclusion: 该方法通过补丁驱动的关系优化有效提升了少样本分类性能，图精炼仅在训练时使用，不影响推理效率，为时间敏感的应用场景（如战场搜救）提供了实用解决方案。

Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

</details>


### [80] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

TL;DR: 本文提出利用视频基础模型进行时空数据增强，通过视频扩散模型生成3D空间和时间变化来丰富训练数据，在数据稀缺场景下提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要基于简单的几何变换或外观扰动，无法有效生成真实的3D空间和时间变化。在无人机图像等标注数据稀缺的场景中，需要更有效的增强方法来提升模型性能。

Method: 使用现成的视频扩散模型从给定的图像数据集生成真实的时空变化，将合成的视频片段作为补充训练数据。提供实用指南包括选择生成设置、标注迁移和处理遮挡区域。

Result: 在COCO子集和无人机数据集上的实验表明，该方法能沿传统和先前生成方法无法充分覆盖的维度扩展数据分布，在数据稀缺情况下带来一致性能提升。

Conclusion: 时空数据增强通过视频基础模型能够有效改善数据稀缺场景下的模型性能，为低数据环境提供了有效的解决方案。

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [81] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

TL;DR: Animus3D是一个文本驱动的3D动画框架，通过Motion Score Distillation (MSD)方法解决传统SDS在3D动画生成中运动不足和抖动的问题，能够为静态3D资产生成高质量的运动场。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用vanilla Score Distillation Sampling (SDS)从预训练的文生视频扩散模型中提取运动，但会导致动画运动幅度小或明显抖动，需要更有效的运动生成方法。

Method: 提出Motion Score Distillation (MSD)替代SDS，包括：LoRA增强的视频扩散模型定义静态源分布、基于反转的噪声估计技术保持外观一致性、时空正则化项减少几何失真、运动细化模块提升时间分辨率和细节。

Result: 实验表明Animus3D能从多样文本提示成功动画化静态3D资产，相比现有最优方法生成更显著和详细的运动，同时保持高视觉完整性。

Conclusion: Animus3D框架通过创新的MSD方法和多项技术改进，显著提升了文本驱动3D动画的质量和运动保真度，为3D内容创作提供了有效解决方案。

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [82] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种结合心肌解剖先验、结构感知特征编码和三维小波变换的冠状动脉分割框架，在复杂几何条件下实现更稳定一致的动脉分割。


<details>
  <summary>Details</summary>
Motivation: 准确分割冠状动脉对于定量分析和临床决策至关重要，但由于血管细小、分支复杂、边界模糊和心肌干扰等因素，可靠分割仍然具有挑战性。

Method: 采用心肌先验和基于残差注意力的特征增强来强化冠状动脉结构表示，使用小波变换进行下采样和上采样以实现空间频率联合建模，并设计多尺度特征融合模块整合语义和几何信息。

Result: 在ImageCAS数据集上实验显示，该方法达到Dice系数0.8082、敏感性0.7946、精确度0.8471和HD95为9.77mm，优于多个主流分割模型。

Conclusion: 提出的方法能够在复杂几何条件下实现更稳定一致的冠状动脉分割，为后续冠状动脉结构分析任务提供可靠的分割结果。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [83] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

TL;DR: 提出了一种基于监督对比学习的视频表示学习框架，通过时间全局上下文和视频到图像聚合策略，在减少计算开销的同时提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示学习方法通常依赖复杂的视频变换器模型，计算开销大。本文旨在利用预训练的卷积神经网络，通过更高效的方式学习有效的视频表示。

Method: 采用视频到图像聚合策略，将多个视频帧空间排列为单张输入图像；设计对比学习目标，比较模型生成的成对投影；通过不同时间帧采样创建同一视频的多个自然视图作为正样本。

Result: 在Penn Action数据集上达到76%的分类准确率（ViVIT为43%），在HMDB51数据集上达到48%的准确率（ViVIT为37%），同时计算资源需求更少。

Conclusion: 所提出的监督对比帧聚合方法在监督和自监督设置下都能学习有效的视频表示，支持分类和字幕生成等视频任务，性能优于现有方法且计算效率更高。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [84] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于令牌修剪的在线视频理解方法，通过减少视频帧的上下文长度来降低GPU内存使用和计算延迟。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在在线视频理解中面临帧数过多导致的高内存消耗和计算延迟问题，需要一种有效的方法来减少上下文长度同时保留关键信息。

Method: 提出了MSSAVT冗余度量标准，结合令牌相似性和空间位置；设计了掩码修剪策略避免双向依赖；整合了基于时间冗余的修剪方法来消除视频模态的时间冗余。

Result: 在多个在线和离线视频理解基准测试中，该方法显著提高了准确率（最高提升4%），同时修剪延迟可忽略不计（小于1ms）。

Conclusion: 该方法有效解决了MLLMs在视频理解中的计算效率问题，实现了准确率和效率的良好平衡。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [85] [From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models](https://arxiv.org/abs/2512.12571)
*Boyeong Im,Wooseok Lee,Yoojin Kwon,Hyung-Sin Kim*

Main category: cs.CV

TL;DR: MVP是一个将测试时适应从数字领域扩展到物理传感器控制的前向框架，通过相机曝光三角作为物理提示，在推理时选择最佳传感器设置并聚合预测，显著提升视觉语言模型在物理环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 扩展视觉语言模型从网络图像到传感器介导的物理环境应用，解决传统数字测试时适应方法在物理环境中的局限性。

Method: 使用相机曝光三角（ISO、快门速度、光圈）作为物理提示，在推理时采集多视角物理视图，通过源亲和度得分选择top-k传感器设置，进行轻量级数字增强，过滤低熵子集，使用零温度softmax（硬投票）聚合预测。

Result: 在ImageNet-ES和ImageNet-ES-Diverse数据集上，MVP比仅数字测试时适应方法提升高达25.6个百分点，比传统传感器控制与TTA结合的流程额外提升3.4个百分点。

Conclusion: 测量时间控制（选择和组合真实物理视图）显著提升了视觉语言模型的鲁棒性，超越了仅后捕获提示的方法。

Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.

</details>


### [86] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

TL;DR: StegaVAR是一个新颖的隐私保护视频动作识别框架，通过将动作视频嵌入到普通封面视频中，在隐写域直接进行动作识别，解决了传统匿名化方法的低隐蔽性和时空特征破坏问题。


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护方法存在两个主要问题：(1)低隐蔽性，产生视觉扭曲的视频在传输过程中容易引起攻击者注意；(2)时空特征破坏，会降低准确视频动作识别所需的必要时空特征。

Method: 提出StegaVAR框架，首次在隐写域直接执行视频动作识别。采用Secret Spatio-Temporal Promotion (STeP) 在训练期间使用秘密视频指导隐写域的时空特征提取，以及Cross-Band Difference Attention (CroDA) 通过捕获跨频带语义差异来抑制封面干扰。

Result: 实验表明，StegaVAR在广泛使用的数据集上实现了优越的视频动作识别和隐私保护性能，且该框架对多种隐写模型都有效。

Conclusion: StegaVAR通过隐写域分析成功解决了隐私保护视频动作识别中的隐蔽性和特征保持问题，为隐私敏感场景下的视频分析提供了有效解决方案。

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [87] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

TL;DR: 本文介绍了一种用于线束检测的半自动化机器视觉系统，能够验证线束位置、连接器极性和颜色序列的正确性，在工业应用中实现了100%的检测精度和44%的时间节省。


<details>
  <summary>Details</summary>
Motivation: 现代电子制造服务行业中的线束检测过程仍然是一个劳动密集型且容易出错的过程，需要自动化解决方案来提高效率和准确性。

Method: 系统采用五个工业级CMOS相机集成到模块化机械框架中，使用基于HSV和RGB颜色域值比较的颜色序列分类器。用户可通过至少五个参考样本训练系统，训练文件可存储并重复使用。

Result: 系统在GPV Lanka Pvt. Ltd.部署后实现了100%的检测准确率，相比人工检测方法减少了44%的检测时间。

Conclusion: 该半自动化机器视觉系统为线束检测提供了可靠高效的解决方案，在实际工业应用中表现出色，具有用户管理、可调照明、会话数据存储和安全登录等附加功能。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [88] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

TL;DR: 该研究提出了一种融合视觉增强LLM和先进transformer架构的框架，通过整流流机制和双向标记化策略，实现了高效高分辨率图像合成和多模态数据理解，在图像清晰度和计算效率上显著优于扩散方法。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率图像合成和多模态数据解释中的挑战，提升视觉增强LLM在计算机视觉和多模态AI中的能力。

Method: 采用整流流机制连接噪声和数据线性路径，双向标记化策略融合文本、图像和视频输入，结合空间-时间特征嵌入和混合文本-图像序列建模，使用噪声感知学习算法优化架构。

Result: 在基准数据集上评估显示，图像分辨率清晰度提高25%，计算需求降低20%，相比扩散方法表现更优，并展现出强大的可扩展性和适应性。

Conclusion: 该工作强调了视觉中心LLM在重新定义计算机视觉和多模态人工智能能力方面的作用，为自主系统、创意内容生成和高级视频分析等应用提供了潜力。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [89] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的图像广告布局生成方法，通过分析图像内容来智能放置文本和logo，相比传统显著性映射方法能生成更高质量的广告布局。


<details>
  <summary>Details</summary>
Motivation: 传统广告布局技术主要依赖显著性映射来检测背景图像中的显著区域，但这种方法无法充分考虑图像的详细构图和语义内容。

Method: 采用两步流程：首先使用VLM分析图像识别物体类型和空间关系，生成基于文本的"放置计划"；然后将该计划通过生成HTML格式代码渲染成最终布局。

Result: 通过定量和定性对比实验验证，结果表明该方法通过显式考虑背景图像内容，能生成明显更高质量的广告布局。

Conclusion: 基于VLM的广告布局生成方法能够有效提升广告布局质量，通过语义理解实现更智能的文本和logo放置。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [90] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

TL;DR: 该论文提出了一种几何感知的场景一致图像生成方法，通过引入场景一致性数据构建流程和几何引导注意力损失，解决了现有方法在场景保持和提示遵循之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景一致图像生成中存在权衡问题：要么高度保真地复制场景但响应提示能力差，要么优先遵循提示但牺牲场景一致性。需要解决这一矛盾。

Method: 提出两个关键贡献：(1) 场景一致性数据构建流程，生成多样化的几何基础训练对；(2) 新颖的几何引导注意力损失，利用跨视图线索规范模型的空间推理。

Result: 在场景一致性基准测试中，该方法在场景对齐和文本-图像一致性方面优于最先进基线，自动指标和人类偏好研究均显示更好效果。

Conclusion: 该方法能够生成几何连贯的图像，保持多样化的构图，同时忠实于文本指令和底层场景结构。

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [91] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

TL;DR: X-Slim是一种基于缓存的扩散模型加速器，通过跨时间步、结构块和空间令牌的统一缓存框架，在保持生成质量的同时显著降低计算延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但计算开销随步数、模型深度和序列长度线性增长。现有缓存方法存在速度与质量之间的权衡：激进的时间步重用能大幅加速但容易损害保真度，而块级或令牌级重用更安全但加速效果有限。

Method: X-Slim引入双阈值控制器，将缓存过程分为推和抛光两个阶段：首先在时间步级别进行重用直到预警线，然后切换到轻量级的块和令牌级刷新来消除剩余冗余，当达到临界线时触发完整推理以重置累积误差。每个级别都使用上下文感知指示器决定何时何地缓存。

Result: 在FLUX.1-dev和HunyuanVideo上，X-Slim分别将延迟降低4.97倍和3.52倍，感知损失最小。在DiT-XL/2上实现3.13倍加速，FID比现有方法提升2.42。

Conclusion: X-Slim通过统一利用跨时间步、结构和空间的缓存冗余，在多样化任务上推进了速度-质量的前沿，为扩散模型提供了高效的训练免费加速方案。

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [92] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: Patchify是一个简单有效的基于图像块的检索框架，通过局部特征匹配实现高精度、可扩展和可解释的图像检索，并提出了LocScore指标评估空间定位准确性。


<details>
  <summary>Details</summary>
Motivation: 解决实例级图像检索中因尺寸、位置和外观变化带来的挑战，提供无需微调的高性能检索方案，并关注检索结果的空间定位准确性。

Method: 将数据库图像划分为少量结构化图像块，通过局部特征与全局查询描述符的匹配进行检索，结合Product Quantization实现大规模高效检索。

Result: 在多个基准测试中，Patchify优于全局方法，并能与最先进的重新排序流程互补，压缩过程中使用信息丰富的特征显著提升性能。

Conclusion: Patchify框架在实例级图像检索任务中表现出色，提供了准确、可扩展且可解释的解决方案，LocScore指标为理解检索行为提供了有价值的诊断工具。

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [93] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: DMLR是一个动态多模态潜在推理框架，通过置信度引导的潜在策略梯度优化来精炼潜在思考标记，实现动态视觉-文本交错推理，显著提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式逐步推理，存在感知-推理交互不稳定和计算开销大的问题。受人类认知启发，认为思维应通过感知和推理的动态交错展开。

Method: 提出DMLR框架：1）使用置信度引导的潜在策略梯度优化精炼潜在思考标记；2）动态视觉注入策略，在每个潜在思考标记处检索最相关视觉特征并更新最佳视觉补丁集，注入到潜在思考标记中。

Result: 在七个多模态推理基准测试和多种模型架构上的实验表明，DMLR显著提高了推理和感知性能，同时保持了较高的推理效率。

Conclusion: DMLR通过动态视觉-文本交错推理机制，有效解决了现有方法的局限性，为多模态推理提供了更高效、更稳定的解决方案。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [94] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: DiG是一个用于提升多模态大语言模型细粒度视觉感知能力的代理任务框架，通过识别相似图像对之间的所有差异来学习精确的空间推理，无需预知差异数量。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在细粒度视觉感知和精确空间推理方面仍存在局限，需要新的训练方法来提升这些能力。

Method: 提出DiG框架，使用自动化的3D渲染数据生成流程创建可控差异的图像对，并采用课程学习策略从单差异逐步过渡到多差异训练。

Result: 实验表明DiG显著提升了模型在多个视觉感知基准上的性能，学习到的细粒度感知技能能有效迁移到标准下游任务。

Conclusion: 差异定位是提升多模态大语言模型细粒度视觉推理能力的可扩展且稳健的方法。

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [95] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为CARe的简单而有效的跨模态眼底图像配准方法，专门针对具有大视场差异的挑战性场景。该方法通过裁剪和对齐操作，成功解决了现有方法在大视场差异下失效的问题。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态眼底图像配准方法假设视场差异较小，但在实际临床场景中，OCTA图像和宽场彩色眼底照片之间存在大视场差异，直接应用现有方法会失败。

Method: CARe方法包含两个核心模块：1）裁剪操作：利用视网膜生理结构从目标图像中裁剪出与源图像视场大致对齐的子图像；2）对齐模块：采用RANSAC算法和多项式坐标拟合的双重拟合方法改进空间变换。

Result: 在包含60对OCTA-wfCFP图像的新测试集上进行广泛实验，验证了CARe方法在跨模态眼底图像配准中的有效性。

Conclusion: CARe方法为解决大视场差异下的跨模态眼底图像配准问题提供了一种可行的解决方案，该方法简单但效果显著。

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [96] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: CogDoc是一个统一粗到细思维框架，通过模仿人类认知过程解决文档推理中的可扩展性与保真度权衡问题，采用直接强化学习方法，在7B参数规模下超越更大规模专有模型。


<details>
  <summary>Details</summary>
Motivation: 当前文档推理范式存在可扩展性（处理长上下文文档）与保真度（捕捉细粒度多模态细节）之间的基本权衡问题，需要新的解决方案。

Method: 提出CogDoc框架，包含低分辨率"快速阅读"阶段进行可扩展信息定位，以及高分辨率"专注思考"阶段进行深度推理。采用直接强化学习方法，避免监督微调初始化带来的策略冲突问题。

Result: 7B参数模型在其参数类别中达到最先进性能，在具有挑战性的视觉丰富文档基准测试中显著超越GPT-4o等更大规模专有模型。

Conclusion: CogDoc框架通过模仿人类认知的粗到细思维过程，成功解决了文档推理中的可扩展性与保真度权衡问题，直接强化学习方法显示出优于传统方法的性能。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [97] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

TL;DR: SSMT-Net是一种基于Transformer的半监督多任务网络，用于甲状腺结节超声图像分割，通过利用未标记数据和多任务学习提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决甲状腺结节超声图像分割中边界模糊、尺寸变化大、标注数据稀缺等问题，现有深度学习模型难以有效整合甲状腺腺体上下文信息并泛化到多样化病例。

Method: 提出SSMT-Net，包含无监督预训练阶段和有监督多任务学习阶段。无监督阶段利用未标记数据增强Transformer编码器特征提取能力；有监督阶段联合优化结节分割、腺体分割和结节尺寸估计三个任务，整合局部和全局上下文特征。

Result: 在TN3K和DDTI数据集上的广泛评估表明，SSMT-Net在准确性和鲁棒性方面优于现有最先进方法。

Conclusion: SSMT-Net在甲状腺结节分割任务上表现出色，具有在真实临床应用中推广的潜力。

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [98] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

TL;DR: InteracTalker是一个新颖框架，通过多阶段训练将基于提示的对象感知交互与语音驱动手势生成无缝集成，解决了现有方法独立处理这两个任务的问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法独立处理语音驱动手势和对象交互，限制了真实世界应用的适用性，因为缺乏集成的综合数据集。需要生成能够自然响应语言和物理对象的真实人体运动。

Method: 采用多阶段训练学习统一的运动、语音和提示嵌入空间；构建丰富的人-对象交互数据集；使用广义运动适应模块实现独立训练；提出自适应融合策略动态重新加权条件信号。

Result: InteracTalker在语音手势生成和对象交互合成方面均优于现有方法，超越了专注于手势的扩散方法，产生了高度真实、对象感知的全身运动。

Conclusion: 该框架成功统一了之前分离的任务，在真实性、灵活性和控制性方面表现出色，为交互式数字体验提供了有效的解决方案。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [99] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

TL;DR: 本文提出了一种用于开放世界深度伪造溯源（OW-DFA）的置信度感知非对称学习框架（CAL），通过动态平衡已知和新型伪造类型的模型置信度，解决了现有方法的置信度偏差和先验假设不现实的问题。


<details>
  <summary>Details</summary>
Motivation: 现有OW-DFA方法存在两个关键局限：1）置信度偏差导致新型伪造的伪标签不可靠，造成训练偏差；2）不现实地假设未知伪造类型的数量已知。需要一种能够自适应平衡置信度并自动估计新型伪造类型数量的方法。

Method: CAL框架包含两个核心组件：置信度感知一致性正则化（CCR）和不对称置信度增强（ACR）。CCR通过基于归一化置信度动态缩放样本损失来缓解伪标签偏差；ACR通过选择性学习高置信度样本分别校准已知和新型类别的置信度。此外，还提出了动态原型剪枝（DPP）策略来自动估计新型伪造类型数量。

Result: 在标准OW-DFA基准和新扩展的包含先进操作的基准上的大量实验表明，CAL在已知和新型伪造溯源方面均优于现有方法，达到了新的最先进性能。

Conclusion: CAL框架通过置信度感知的非对称学习机制有效解决了OW-DFA中的关键挑战，提出的DPP策略增强了方法在真实场景中的可扩展性，为开放世界深度伪造溯源提供了有效的解决方案。

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [100] [Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation](https://arxiv.org/abs/2512.12673)
*Yushun Tang,Ziqiong Liu,Jiyuan Jia,Yi Zhang,Zhihai He*

Main category: cs.CV

TL;DR: 该论文提出了一种渐进式条件尺度-偏移重校准（PCSR）方法，用于在线测试时适应Transformer模型在新领域中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当将Transformer模型应用于新目标领域时，其自注意力模块的Query、Key和Value特征会发生显著变化，导致模型性能大幅下降。

Method: 通过在每个Transformer层学习域分离网络提取域偏移特征，并使用因子生成器网络预测尺度-偏移参数来重校准自注意力。这些轻量级网络在推理过程中在线适应。

Result: 在ImageNet-C数据集上，该方法将在线测试时域适应性能显著提高了3.9%的分类准确率。

Conclusion: PCSR方法能有效解决Transformer模型在新领域的适应问题，显著提升在线测试时域适应性能。

Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.

</details>


### [101] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出Scone方法，通过统一的理解-生成框架解决多主体图像生成中的区分问题，结合语义对齐和注意力掩码技术提升多主体场景下的身份保持能力。


<details>
  <summary>Details</summary>
Motivation: 当前多主体图像生成方法忽视了区分能力，即在输入包含多个候选主体时正确识别和生成目标主体的能力，这限制了在复杂真实视觉场景中的有效性。

Method: 提出Scone方法，包含理解专家和生成专家，通过两阶段训练：先学习组合能力，再通过语义对齐和注意力掩码增强区分能力。理解专家作为语义桥梁传递信息，指导生成专家保持主体身份。

Result: 实验表明Scone在两个基准测试中优于现有开源模型，在组合和区分任务上都表现出色。

Conclusion: Scone通过统一的理解-生成框架有效解决了多主体图像生成中的区分问题，提出的SconeEval基准为评估组合和区分能力提供了标准。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [102] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

TL;DR: β-CLIP是一个多粒度文本条件对比学习框架，通过层次化对齐从完整描述到句子和短语的多种文本粒度及其对应的视觉区域，显著提升了密集视觉语言对齐性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在全局视觉-文本表示对齐方面表现良好，但在细粒度任务上表现不佳，即使使用长而详细的描述进行微调。需要一种能够处理多层次语义对齐的方法。

Method: 提出β-CLIP框架，使用跨注意力动态池化图像块生成上下文视觉嵌入，并引入β-上下文对比对齐损失（β-CAL）来处理层次结构中的语义重叠问题。

Result: 在Urban1K上达到91.8% T2I和92.3% I2T的R@1，在FG-OVD（Hard）上达到30.9%，在无需困难负样本训练的方法中达到最先进水平。

Conclusion: β-CLIP为密集视觉语言对应关系建立了一个强大、自适应的基准，代码和模型已开源。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [103] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

TL;DR: ATP是一种动态推理机制，通过基于上下文相关性保留最信息丰富的令牌来减少视觉语言模型的计算需求，在保持精度的同时显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 现实世界中视觉语言模型的部署受到高计算需求的阻碍，现有架构对所有令牌进行统一处理效率低下。

Method: 提出自适应令牌剪枝(ATP)，在视觉语言接口处操作，基于ViT CLS注意力和CLIP文本-图像相似性的混合重要性评分来保留top-K令牌。

Result: 在VQAv2、GQA和COCO上的评估表明，ATP减少约40%的推理FLOPs，端到端延迟加速约1.5倍，精度损失小于1%。

Conclusion: 自适应剪枝不仅能提高效率，还能抑制虚假相关性，提高模型稳定性，表明资源受限的推理和模型可靠性不是竞争目标。

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [104] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于部分感知的掩码自回归模型，利用视频中可信的人体部分数据来增强运动生成，解决了视频中人体部分遮挡或离屏导致的数据质量问题。


<details>
  <summary>Details</summary>
Motivation: 解决从大规模网络视频中提取人体运动时，由于离屏拍摄或遮挡导致部分人体不可见的数据质量问题。丢弃这些数据会限制规模和多样性，保留则会降低数据质量和模型性能。

Method: 1. 将人体分解为五个部分，检测视频帧中清晰可见的部分作为可信部分；2. 使用部分感知变分自编码器将可信部分编码为潜在标记；3. 提出鲁棒的部分级掩码生成模型，预测被掩码的可信部分，同时忽略噪声部分。

Result: 实验结果表明，该方法在干净和噪声数据集上都优于基线方法，在运动质量、语义一致性和多样性方面表现更好。

Conclusion: 提出的方法成功解决了视频中部分人体不可见的问题，通过利用可信的部分级数据提高了运动生成的鲁棒性和质量，并贡献了新的K700-M基准数据集。

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [105] [Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images](https://arxiv.org/abs/2512.12718)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Changgyun Kim,Taemin Lee*

Main category: cs.CV

TL;DR: 提出了一种基于四方向深度图像的3D人体姿态分析系统，通过多图像融合解决单图像方法在脊柱中心线估计中的局限性，实现高精度3D脊柱配准估计


<details>
  <summary>Details</summary>
Motivation: 现有基于多图像的人体重建方法需要昂贵设备和复杂流程，而基于单图像的方法由于遮挡和视角限制难以准确估计脊柱中心线等内部结构

Method: 集成四个方向的深度图像重建3D人体模型，通过全局和精细配准的分层匹配进行噪声和遮挡恢复，应用自适应顶点减少保持网格分辨率，使用LOD集成确保脊柱角度估计的准确性和稳定性

Result: 实现了不依赖训练数据或复杂神经网络模型的高精度3D脊柱配准估计，验证了匹配质量的提升

Conclusion: 该方法有效弥补了多图像方法的不足，解决了单图像方法的局限性，为脊柱角度分析提供了可靠的技术方案

Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.

</details>


### [106] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: GenieDrive是一个物理感知的驾驶视频生成框架，通过生成4D占用作为物理基础，结合创新的VAE压缩和互控注意力机制，显著提升了视频生成的物理一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一扩散模型直接将驾驶动作映射为视频，导致学习困难且输出物理不一致。需要一种能够生成物理感知驾驶视频的方法来支持驾驶规划、异常数据合成和闭环评估。

Method: 1. 首先生成包含丰富物理信息的4D占用；2. 提出VAE将高分辨率占用编码为潜在三角平面表示，将潜在大小减少到之前方法的58%；3. 引入互控注意力精确建模控制对占用演化的影响；4. 端到端联合训练VAE和预测模块；5. 在视频生成中使用归一化多视图注意力，基于4D占用指导生成多视图驾驶视频。

Result: 1. 预测mIoU提升7.2%，推理速度达41 FPS，仅使用3.47M参数；2. 视频质量显著提升，FVD降低20.7%；3. 实现了高度可控、多视图一致且物理感知的驾驶视频生成。

Conclusion: GenieDrive通过物理感知的4D占用生成和创新的注意力机制，有效解决了现有方法的物理不一致问题，为驾驶规划和应用提供了高质量的物理感知视频生成能力。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [107] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

TL;DR: FysicsWorld是首个统一的全模态基准测试，支持图像、视频、音频和文本之间的双向输入输出，包含16个主要任务和3,268个样本，通过跨模态互补性筛选策略构建数据，对30多个先进模型进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型基准测试存在模态覆盖不全、仅限于文本输出、模态间依赖性和互补性弱等问题，需要构建更全面的评估框架。

Method: 提出跨模态互补性筛选策略，系统构建包含图像、视频、音频、文本的全模态数据集，支持任意模态间的理解和生成任务。

Result: 通过对30多个先进模型的评估，揭示了模型在理解、生成和推理方面的性能差距和局限性。

Conclusion: FysicsWorld为评估和推进下一代全模态架构建立了统一基础和强基线。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [108] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

TL;DR: CoRe3D提出了一种统一的3D理解和生成推理框架，通过语义和空间抽象联合操作，使语言推断的高层意图直接指导低层3D内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型的显式推理机制在语言和视觉任务中有效，但在3D领域扩展不足。需要开发能够同时处理语义和空间推理的3D理解框架。

Method: 采用空间接地推理表示，将3D潜在空间分解为局部化区域，使模型能够以组合和程序化方式对几何进行推理。通过语义链式推理与结构化空间推理的紧密耦合实现3D生成。

Result: CoRe3D生成的3D输出具有强局部一致性和与语言描述的高度对齐性。

Conclusion: 该框架证明了在3D理解和生成任务中集成显式推理机制的有效性，为提升模型可靠性和可解释性提供了新途径。

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [109] [Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior](https://arxiv.org/abs/2512.12774)
*Hao Wang,Ashish Bastola,Chaoyi Zhou,Wenhui Zhu,Xiwen Chen,Xuanzhao Dong,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: Fast-2DGS是一个轻量级框架，通过引入深度高斯先验和属性回归网络，实现了高效的高斯图像表示，显著减少了计算成本而不牺牲视觉质量。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能够产生高质量视觉内容，对高效、可解释和可编辑的图像表示需求增加。现有2D高斯泼溅方法需要后优化，收敛速度慢，而基于学习网络的方法增加了计算和架构复杂性。

Method: 提出Fast-2DGS框架，包含深度高斯先验（条件网络捕捉高斯基元空间分布）和属性回归网络（预测密集高斯属性），采用解耦架构实现单次前向传播的高质量重建。

Result: 实验表明该方法在单次前向传播后只需最小微调即可实现高质量重建，显著降低计算成本，保持视觉质量，使2DGS更接近工业就绪部署。

Conclusion: Fast-2DGS通过轻量级设计解决了2D高斯泼溅的效率和复杂性挑战，为工业应用提供了实用的解决方案。

Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.

</details>


### [110] [L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context](https://arxiv.org/abs/2512.12790)
*Tiange Zhang,Zhimeng Huang,Xiandong Meng,Kai Zhang,Zhipin Deng,Siwei Ma*

Main category: cs.CV

TL;DR: L-STEC方法通过引入LSTM扩展参考链捕获长期依赖，并结合像素域的扭曲空间上下文，通过多感受野网络融合时空信息，显著提升神经视频压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩方法仅依赖前一帧特征预测时间上下文，存在两个关键问题：短参考窗口错过长期依赖和精细纹理细节；仅传播特征级信息会导致误差累积和细微纹理丢失。

Method: 提出L-STEC方法：1）使用LSTM扩展参考链捕获长期依赖；2）引入像素域的扭曲空间上下文；3）通过多感受野网络融合时空信息以更好保留参考细节。

Result: 实验结果显示L-STEC显著提升压缩性能，与DCVC-TCM相比在PSNR上实现37.01%的码率节省，在MS-SSIM上实现31.65%的码率节省，性能超过VTM-17.0和DCVC-FM，达到新的最先进水平。

Conclusion: L-STEC通过丰富上下文信息有效解决了神经视频压缩中的长期依赖和纹理保留问题，在压缩性能上实现了显著提升，确立了新的技术标杆。

Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.

</details>


### [111] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DrivePI是一个空间感知的4D多模态大语言模型，作为统一的视觉-语言-动作框架，在自动驾驶中实现3D感知、预测和规划的端到端优化，以小型0.5B模型超越现有VLA和专用VA模型。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在自动驾驶领域的细粒度3D感知和预测应用尚未充分探索，需要开发能够统一处理空间理解、3D感知、预测和规划的框架。

Method: 提出DrivePI框架，集成点云、多视角图像和语言指令，通过数据引擎生成文本-占据和文本-流问答对，实现4D空间理解的端到端并行优化。

Result: DrivePI在nuScenes-QA上比OpenDriveVLA-7B准确率提升2.5%，碰撞率降低70%；在3D占据、占据流和规划任务上分别超越专用模型10.3 RayIoU、降低mAVE误差、减少32% L2误差。

Conclusion: DrivePI证明了小型统一模型在自动驾驶多任务处理中的有效性，为4D空间理解提供了新的解决方案。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [112] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

TL;DR: 本文提出了一种新的对比分析框架，能够在GAN和扩散模型中分离图像数据集的共同生成因子和显著生成因子，仅使用数据集信号而非属性监督。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成方法主要关注条件操作和分离表示学习，但缺乏对数据集间对比分析的研究。本文旨在解决仅使用数据集信号来分离共享因子和特定因子的挑战。

Method: 提出新颖的对比分析框架，包含专门设计的训练策略和损失函数，确保共同因子和显著因子的有效分离，同时保持高质量图像生成。

Result: 在多种数据集（人脸、动物图像、医学扫描）上评估，证明该方法在因子分离能力和图像合成质量方面优于现有方法。

Conclusion: 该框架为数据集对比分析提供了有效的解决方案，在弱监督设置下实现了优异的性能，具有广泛的应用前景。

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [113] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

TL;DR: Lemon是一个统一的多模态Transformer架构，通过将3D点云块和语言标记作为单一序列处理，解决了现有3D理解模型在可扩展性、架构复杂性和训练稳定性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有3D多模态模型面临三个主要挑战：点云数据稀疏不规则、依赖碎片化的模态特定编码器架构、训练管道不稳定且可扩展性差。需要一种统一的架构来支持更有效的3D空间智能发展。

Method: 采用统一的Transformer架构，将3D点云块和语言标记联合处理为单一序列；开发了结构化分块和标记化方案以保持空间上下文；使用三阶段训练课程，从物体级识别逐步构建到场景级空间推理能力。

Result: Lemon在全面的3D理解和推理任务上建立了新的最先进性能，包括物体识别、描述和3D场景空间推理，同时在模型规模和训练数据增加时展现出强大的可扩展性。

Conclusion: 该工作为推进真实世界应用中的3D空间智能提供了统一的基础，通过早期空间-语言融合和消除冗余编码器，实现了参数效率和模型扩展的显著改进。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [114] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文系统研究了CoCa视觉骨干网络在少样本图像分类中的参数高效微调策略，发现数据增强在低样本场景下的矛盾作用，并提出结合监督对比损失的混合目标函数能稳定提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注CLIP等双编码器架构，而CoCa这种生成-对比混合模型在极端数据稀缺（少样本学习）下的适应能力尚未充分探索，特别是其独特潜在空间对参数高效微调（PEFT）的响应机制存在研究空白。

Method: 采用层次化策略评估：从训练免费的混合原型到基于低秩适应（LoRA）的深度参数适应。系统分析数据增强、监督对比损失（SupCon）和交叉熵损失的混合目标函数，并研究训练配置对数据稀缺的敏感性。

Result: 发现"增强分歧"现象：强数据增强会降低低样本场景下线性探测的性能，但对稳定LoRA微调至关重要；SupCon损失在不同样本量下均能带来持续性能提升；确定了正则化、秩和采样策略的实证参考设置。

Conclusion: 为生成-对比基础模型的高效适应提供了系统的实证指导，揭示了CoCa潜在空间在少样本学习中的独特行为特征，为参数高效微调策略的设计提供了重要参考。

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [115] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

TL;DR: SAVE模型通过Schrodinger Bridge实现音频和视频的联合编辑，解决了多模态数据对齐和编辑一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 联合编辑音频和视觉内容对于精确可控的内容创作至关重要，但面临配对数据稀缺和模态异质性的挑战。

Method: 提出SAVEBench数据集和SAVE模型，使用基于流的匹配方法和Schrodinger Bridge实现从源到目标音频视觉混合的直接传输。

Result: SAVE模型能够有效移除目标对象并保留剩余内容，在时间同步和语义对应方面优于音频和视频编辑器的组合方法。

Conclusion: 该方法为联合音频视觉编辑提供了有效解决方案，显著提升了编辑质量和多模态一致性。

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [116] [Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification](https://arxiv.org/abs/2512.12887)
*Han Liu,Bogdan Georgescu,Yanbo Zhang,Youngjin Yoo,Michael Baumgartner,Riqiang Gao,Jianing Wang,Gengyan Zhao,Eli Gibson,Dorin Comaniciu,Sasa Grbic*

Main category: cs.CV

TL;DR: AnyMC3D是一个可扩展的3D医学图像分类框架，通过轻量级插件适配2D基础模型，在12个多样化任务上实现最先进性能，证明2D基础模型经过适当适配可以超越专用3D架构。


<details>
  <summary>Details</summary>
Motivation: 解决当前医学基础模型存在的三个关键缺陷：数据制度偏差、次优适配和任务覆盖不足，旨在开发一个能够扩展到新任务的统一3D分类框架。

Method: 基于冻结的单一骨干网络，为每个任务添加约1M参数的轻量级插件，支持多视图输入、像素级监督和可解释热图生成。

Result: 在涵盖不同病理、解剖结构和模态的12个任务基准测试中，AnyMC3D实现了最先进性能（包括VLM3D挑战赛第一名），证明2D基础模型经过适当适配可以匹配甚至超越医学专用模型。

Conclusion: 首次证明使用单一可扩展框架即可在多样化应用中实现最先进性能，消除了对单独任务特定模型的需求，有效适配是释放基础模型潜力的关键。

Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.

</details>


### [117] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

TL;DR: 本文提出Qonvolutions方法，通过结合坐标查询的卷积操作来改进神经网络对高频信号的学习能力，在多个高频学习任务中取得优异表现，特别是在新视角合成任务中达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络在处理高频信号时存在频谱偏差和优化困难的问题，现有的傅里叶编码等方法仍有改进空间。

Method: 提出Qonvolutions方法，将低频信号与坐标查询等查询信息进行卷积操作，利用卷积的邻域特性来增强高频信号的学习能力。

Result: 在1D回归、2D超分辨率、2D图像回归和新视角合成等多个高频学习任务中验证了Qonvolutions的有效性，特别是在结合高斯溅射的新视角合成任务中超越了现有辐射场模型的图像质量。

Conclusion: Qonvolutions是一种简单而有效的方法，能够显著提升神经网络对高频信号的学习能力，在计算机视觉和图形学领域具有重要应用价值。

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [118] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于预测样本分配（PSA）的语义连贯分布外检测（SCOOD）框架，通过双阈值三元样本分配策略和概念对比表示学习，显著提升了SCOOD性能。


<details>
  <summary>Details</summary>
Motivation: 当前SCOOD方法主要采用基于聚类的分布内样本过滤策略，这会引入大量噪声样本。为了解决这个问题，需要更纯净的样本选择方法。

Method: 提出了PSA框架，包括：1）基于预测能量的双阈值三元样本分配策略，将不确定的无标签数据分配到丢弃样本集；2）概念对比表示学习损失，扩大分布内和分布外样本在表示空间的距离；3）重训练策略帮助模型充分拟合选定的辅助样本。

Result: 在两个标准SCOOD基准测试上的实验表明，该方法显著优于现有最先进方法。

Conclusion: PSA框架通过提高样本选择纯度和增强表示学习，有效解决了SCOOD中的噪声样本问题，取得了优异的检测性能。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [119] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种解决广义类别发现中伪标签噪声问题的新方法，包含损失锐度惩罚和动态锚点选择两个模块，通过提高模型鲁棒性和优化未知类样本选择来提升聚类精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于参数化分类的GCD方法使用DINO类伪标签策略，但大型预训练模型对特定视觉模式的偏好会导致对未标记数据编码伪相关并产生噪声伪标签，影响模型性能。

Method: 提出LSP模块通过最小化模型的最坏情况损失锐度来增强参数对小扰动的鲁棒性，抑制琐碎特征编码；DAS模块基于KNN密度和类别概率选择未知类代表性样本并分配硬伪标签，缓解已知类和未知类之间的置信度差异。

Result: 大量实验表明，该方法能有效减轻伪标签噪声，在多个GCD基准测试中达到了最先进的性能。

Conclusion: 所提出的LSP和DAS模块协同工作，通过提高模型鲁棒性和优化样本选择策略，显著提升了广义类别发现的聚类准确性。

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [120] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

TL;DR: MADTempo是一个视频检索框架，通过统一时间搜索与网络规模视觉定位，解决了现有方法在建模多事件时间依赖和处理罕见视觉概念查询方面的不足。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容的快速增长需要能够理解复杂事件时间结构的检索系统，现有方法在建模多事件时间依赖和应对罕见视觉概念查询方面存在局限。

Method: 引入时间搜索机制通过聚合连续视频片段的相似性得分来捕捉事件级连续性，同时结合基于Google图片搜索的回退模块，利用外部网络图像扩展查询表示。

Result: 该框架提升了现代视频检索系统的时间推理和泛化能力，增强了对分布外查询的鲁棒性。

Conclusion: MADTempo为大规模视频语料库中实现更具语义意识和自适应的检索铺平了道路。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal rea- soning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [121] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

TL;DR: 提出了一种统一的多模态时刻检索系统，通过级联双嵌入管道、时间感知评分机制和智能体引导查询分解，解决现有方法在跨模态噪声、时间建模和手动模态选择方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频内容的指数增长迫切需要高效的多模态时刻检索系统，但现有方法面临三个关键挑战：固定权重融合策略在跨模态噪声和模糊查询中失效、时间建模难以捕捉连贯事件序列、系统需要手动模态选择降低可用性。

Method: 1) 级联双嵌入管道结合BEIT-3和SigLIP进行广泛检索，通过BLIP-2重排平衡召回率和精度；2) 时间感知评分机制通过波束搜索对大的时间间隔应用指数衰减惩罚，构建连贯事件序列；3) Agent引导的查询分解(GPT-4o)自动解释模糊查询，分解为模态特定子查询，执行自适应分数融合。

Result: 定性分析表明，该系统能有效处理模糊查询，检索时间连贯的序列，并动态调整融合策略。

Conclusion: 该系统提升了交互式时刻搜索能力，通过创新的多模态融合和时间建模方法，显著改进了视频时刻检索的准确性和可用性。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [122] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

TL;DR: 本文提出了一种基于内容自适应的运动对齐框架CAMA，通过三方面改进提升端到端视频压缩性能：两阶段流引导变形扭曲机制、多参考质量感知策略和训练无下采样模块，在标准测试集上相比基线模型DCVC-TCM实现24.95%的BD-rate节省。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端视频压缩框架缺乏内容特定适应性，导致压缩性能不理想。本文旨在通过内容自适应方法提升编码策略对不同内容特征的适应性。

Method: 1. 两阶段流引导变形扭曲机制：通过粗到细的偏移预测和掩码调制优化运动补偿；2. 多参考质量感知策略：基于参考质量调整失真权重，应用于分层训练减少误差传播；3. 训练无下采样模块：根据运动幅度和分辨率下采样帧以获得平滑运动估计。

Result: 在标准测试数据集上，CAMA框架相比最先进的神经视频压缩模型有显著提升，相比基线模型DCVC-TCM实现了24.95%的BD-rate（PSNR）节省，同时优于复现的DCVC-DC和传统编解码器HM-16.25。

Conclusion: 提出的内容自适应运动对齐框架通过三方面技术创新有效提升了视频压缩性能，验证了内容自适应策略在端到端视频压缩中的重要性。

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [123] [UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction](https://arxiv.org/abs/2512.12941)
*Siyuan Yao,Dongxiu Liu,Taotao Li,Shengjie Li,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出UAGLNet网络，通过不确定性建模指导下的全局-局部视觉语义融合，解决建筑物提取中的特征金字塔间隙和全局-局部特征融合不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建筑物提取中存在特征金字塔间隙和全局-局部特征融合不足，导致提取结果不准确和模糊。

Method: 设计协作编码器（CNN和Transformer混合层）、中间协作交互块（CIB）、全局-局部融合模块（GLF）和不确定性聚合解码器（UAD）。

Result: 在广泛实验中，该方法优于其他最先进方法。

Conclusion: UAGLNet通过不确定性建模和全局-局部特征融合，显著提升了建筑物提取的准确性。

Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet

</details>


### [124] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

TL;DR: SCAdapter是一种基于CLIP图像空间的扩散模型风格迁移方法，通过分离内容和风格特征实现照片级真实感风格迁移，比现有方法效果更好且推理速度快2倍


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在风格迁移中难以实现照片级真实感，容易产生绘画效果或丢失细节，且无法有效处理原始内容风格和风格参考内容特征的不必要影响

Method: 利用CLIP图像空间分离和整合内容与风格特征，包含三个核心组件：可控风格自适应实例归一化(CSAdaIN)用于精确多风格混合，KVS注入用于针对性风格整合，以及风格迁移一致性目标保持过程连贯性

Result: 在传统和基于扩散的基准测试中显著优于最先进方法，无需DDIM反演和推理阶段优化，推理速度比其他扩散方法至少快2倍

Conclusion: SCAdapter在效果和效率上都优于现有方法，为实际应用提供了更有效和高效的解决方案

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [125] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

TL;DR: VLCache是一个多模态缓存复用框架，通过复用KV缓存和编码器缓存来避免重复计算，在保持精度的同时实现1.2-16倍的推理加速


<details>
  <summary>Details</summary>
Motivation: 多模态输入在处理过程中存在大量重复计算，传统启发式方法无法有效解决累积重用误差问题

Method: 提出累积重用误差最小化方法，分析模型层重要性差异，设计动态分层感知重计算策略来平衡精度和效率

Result: 实验显示VLCache在保持与完全重计算相同精度的同时，仅需计算2-5%的token，实现1.2-16倍的TTFT加速

Conclusion: VLCache已集成到SGLang中，在实际部署中显著提升了多模态推理效率

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [126] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Generator-Aware Prototype Learning (GAPL)的框架，用于解决通用AI生成图像检测器在数据源多样性增加时出现的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 当前通用AI生成图像检测器通过聚合多个生成器的数据来提高泛化能力，但作者发现存在"先受益后冲突"的悖论：随着数据源多样性增加，检测器性能先提升后下降。

Method: 提出GAPL框架，包含两个核心组件：1）学习一组紧凑的伪造原型来创建统一、低方差的特征空间；2）采用两阶段训练方案结合低秩适应技术，增强判别能力同时保留预训练知识。

Result: 通过大量实验证明，GAPL在多种GAN和扩散模型生成器上实现了最先进的检测性能，显示出优越的检测准确率。

Conclusion: GAPL通过结构化学习范式有效解决了数据异构性和模型瓶颈问题，为AI生成图像检测提供了更鲁棒和通用的解决方案。

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [127] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种新的CLIP对抗性微调方法，通过考虑预测准确性和不确定性对齐来修复对抗攻击下的不确定性校准问题。


<details>
  <summary>Details</summary>
Motivation: 现有对抗性微调方法主要关注干净样本和对抗样本的预测logits匹配，但忽视了不确定性校准，导致对抗攻击下模型变得过度自信但不可靠。

Method: 通过将CLIP输出重新参数化为Dirichlet分布的浓度参数，提出统一表示来捕获语义结构和预测置信度，在扰动下整体对齐这些分布。

Result: 在多个零样本分类基准测试中，该方法有效恢复了校准的不确定性，实现了有竞争力的对抗鲁棒性，同时保持了干净样本的准确性。

Conclusion: 该方法超越了单一logit锚定，恢复了校准的不确定性，填补了CLIP模型在可靠性方面的关键空白。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [128] [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)
*Yifan Pu,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Fan Wang,Bohan Zhuang,Gao Huang*

Main category: cs.CV

TL;DR: 本文系统研究了将扩散蒸馏技术应用于开放文本到图像生成的方法，通过统一框架比较现有技术，并提供了实用指南和开源实现。


<details>
  <summary>Details</summary>
Motivation: 扩散蒸馏在类别条件图像生成中已显著加速，但其在开放文本到图像生成中的应用仍不明确。本文旨在填补这一空白。

Method: 将现有方法统一到框架中，在FLUX.1-lite教师模型上比较最先进的蒸馏技术，分析从离散类别标签到自由形式语言提示的关键障碍。

Result: 提供了输入缩放、网络架构和超参数的实用指南，并发布了开源实现和预训练学生模型。

Conclusion: 研究结果为在实际文本到图像应用中部署快速、高保真且资源高效的扩散生成器奠定了坚实基础。

Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.

</details>


### [129] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

TL;DR: 提出基于光场图像的物体跟踪方法，无需预训练模型，对复杂视觉行为（如反射）具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有高性能跟踪方法依赖预捕获物体视图构建显式参考模型，限制了其只能处理已知物体集，且对视觉复杂外观表现不佳。

Method: 使用视觉基础模型从光场输入中提取语义和几何特征，转换为视图相关的高斯泼溅，作为统一物体表示支持可微渲染和位姿优化。

Result: 在包含挑战性反射物体的光场物体跟踪数据集上实验表明，该方法在困难情况下与最先进的基于模型的跟踪器具有竞争力。

Conclusion: 该方法为机器人系统中的通用物体跟踪铺平了道路，代码和数据已开源。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [130] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

TL;DR: TWLR是一个两阶段可解释性糖尿病视网膜病变评估框架，通过视觉语言模型整合眼科知识进行疾病分级和病灶分类，并采用弱监督语义分割实现病变定位和疾病严重度回归。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分析中像素级标注成本高、深度学习模型缺乏可解释性等问题，为糖尿病视网膜病变评估提供更高效和可解释的解决方案。

Method: 第一阶段：视觉语言模型整合眼科知识进行DR分级和病灶分类；第二阶段：基于弱监督语义分割的迭代严重度回归框架，通过渐进修复机制消除病理特征。

Result: 在FGADR、DDR和私有数据集上的实验表明，TWLR在DR分类和病灶分割方面均达到有竞争力的性能。

Conclusion: TWLR框架为自动化视网膜图像分析提供了更可解释和标注效率更高的解决方案，实现了无像素级监督的准确病灶定位和疾病到健康转换的可视化。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [131] [JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion](https://arxiv.org/abs/2512.13014)
*Haoyu Wang,Lei Zhang,Wenrui Liu,Dengyang Jiang,Wei Wei,Chen Ding*

Main category: cs.CV

TL;DR: JoDiffusion是一种新颖的联合生成扩散框架，能够同时生成配对的图像和语义一致的标注掩码，仅需文本提示即可生成高质量的语义分割数据集。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中存在的图像-标注语义不一致和可扩展性问题，避免像素级标注的高成本和耗时问题。

Method: 使用独立的标注VAE网络将标注掩码映射到与图像共享的潜在空间，定制扩散模型捕获图像和标注掩码的联合分布，并开发掩码优化策略减少生成噪声。

Result: 在Pascal VOC、COCO和ADE20K数据集上的实验表明，JoDiffusion生成的标注数据集在语义分割任务中相比现有方法有显著性能提升。

Conclusion: JoDiffusion通过联合生成框架有效解决了语义分割数据集的生成问题，具有更好的语义一致性和可扩展性。

Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.

</details>


### [132] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

TL;DR: 该论文提出了Next Scene Prediction（NSP）任务，通过结合Qwen-VL和LTX的统一框架，在三个训练阶段实现视频场景预测，在时序推理方面取得先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型主要关注文本到视频生成等传统任务，而忽略了统一模型在时序推理方面的潜力。NSP任务旨在推动统一视频模型向时序和因果推理方向发展。

Method: 提出统一框架，结合Qwen-VL进行理解、LTX进行合成，通过潜在查询嵌入和连接模块连接。采用三阶段训练：文本到视频预训练、监督微调和带因果一致性奖励的强化学习（GRPO）。

Result: 实验证明该模型在新基准测试中达到了最先进的性能，提升了通用多模态系统预测未来事件的能力。

Conclusion: NSP任务成功推动了统一模型在时序和因果推理方面的发展，提出的框架在场景预测任务上表现出色，为多模态系统的时间推理能力提供了新方向。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [133] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 本研究首次全面评估了空间泛化技术，系统比较了多种方法在FMCW MIMO雷达室内人员计数任务中的表现，发现基于sigmoid的幅度加权和迁移学习能显著提升跨环境性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在RF感知中的应用需要解决空间泛化问题，以实现实际部署中的稳健性能。本研究旨在为室内人员计数等雷达感知系统提供实用的空间泛化解决方案。

Method: 系统研究了多种空间泛化方法：基于幅度的统计预处理（sigmoid加权和阈值归零）、频域滤波、自编码器背景抑制、数据增强策略和迁移学习。实验在两个不同布局的室内环境中进行。

Result: sigmoid幅度加权在跨环境性能上表现最佳，相比基线方法RMSE降低50.1%，MAE降低55.2%。数据增强可额外提升MAE达8.8%。迁移学习对于大空间偏移效果显著，使用540个目标域样本时RMSE降低82.1%，MAE降低91.3%。

Conclusion: 结合深度学习模型、基于幅度的预处理和高效迁移学习，可为雷达感知系统开发提供实用的方向，使其在空间变化下保持稳健的准确性。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [134] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

TL;DR: SneakPeek是一个基于扩散模型的自动回归框架，用于生成精确的逐步教学视频，通过预测因果适应、未来引导自强制和多提示条件控制来增强时间一致性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型在生成多步骤教学视频时难以保持时间一致性和可控性，这限制了在内容创作、教育和人机交互等领域的应用潜力。

Method: 提出SneakPeek流水线，包含三个关键创新：预测因果适应（学习下一帧预测和未来关键帧预测）、未来引导自强制（双区域KV缓存解决推理时暴露偏差问题）、多提示条件控制（提供对多步骤指令的细粒度程序控制）。

Result: 实验结果表明，该方法能够生成时间连贯、语义忠实且准确遵循复杂多步骤任务描述的教学视频。

Conclusion: SneakPeek通过创新的技术组件有效缓解了时间漂移问题，保持了运动一致性，并实现了交互式视频生成，其中未来提示更新可以动态影响正在进行的流式视频生成。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [135] [Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs](https://arxiv.org/abs/2512.13031)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

TL;DR: 该研究首次系统比较了基于规则的方法、传统机器学习模型和深度学习模型在FMCW MIMO雷达无线电波感知中的性能，发现深度学习模型在相同环境下精度最高但易受领域偏移影响，而基于规则的方法虽然输出粒度粗但具有更好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着无线电波感知技术的发展，需要了解不同方法在室内环境中的性能差异，特别是在面对环境布局变化时的泛化能力。

Method: 在两种不同布局的室内环境中，系统评估了五种方法：基于规则的连通分量方法、三种传统机器学习模型（KNN、随机森林、SVM）以及结合CNN和LSTM的深度学习模型。

Result: 在训练环境中，CNN-LSTM模型达到最高精度，传统机器学习模型表现中等。但在新布局中，所有基于学习的方法性能显著下降，而基于规则的方法保持稳定。对于人员存在检测的二元任务，所有模型在不同布局中均能保持高精度。

Conclusion: 高容量模型能在相同环境中提供精细输出但易受领域偏移影响，基于规则的方法输出粒度粗但鲁棒性强。无论模型类型如何，都存在空间泛化性能与输出粒度之间的明显权衡。

Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.

</details>


### [136] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

TL;DR: 本文提出了一种双向图像引导概念擦除框架Bi-Erasing，通过同时进行概念抑制和安全增强来平衡概念移除效果和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法通常采用单向策略，要么抑制目标概念，要么强化安全替代方案，难以在概念移除和生成质量之间取得平衡。

Method: 基于文本提示和对应图像的联合表示，引入两个解耦的图像分支：负分支负责抑制有害语义，正分支为安全替代方案提供视觉指导。通过联合优化这些互补方向，并应用基于掩码的过滤来防止无关内容干扰。

Result: 在广泛的实验评估中，Bi-Erasing在平衡概念移除有效性和视觉保真度方面优于基线方法。

Conclusion: 所提出的双向擦除框架能够有效解决概念擦除中的平衡问题，在保持生成质量的同时实现更好的概念移除效果。

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [137] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

TL;DR: GTR-Turbo是一种高效的多模态智能体强化学习方法，通过合并训练过程中的检查点权重来创建免费教师模型，无需依赖昂贵的外部模型，在减少50%训练时间和60%计算成本的同时，性能提升10-30%


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态智能体强化学习方法依赖昂贵教师模型、奖励稀疏和长时程信用分配问题，提高方法的实用性和可复现性

Method: 在持续强化学习训练过程中合并检查点权重，使用合并后的模型作为免费教师，通过监督微调或软logit蒸馏指导后续强化学习

Result: 在多种视觉任务中，相比基线模型准确率提升10-30%，相比GTR方法训练时间减少50%，计算成本降低60%

Conclusion: GTR-Turbo通过创新的权重合并策略，成功摆脱了对特权视觉语言模型的依赖，解决了熵崩溃问题，实现了高效稳定的多模态智能体训练

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [138] [Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing](https://arxiv.org/abs/2512.13055)
*Jaeyoon Kim,Yoonki Cho,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: 本文提出了一种高效的非对称视觉地点识别框架，通过地理记忆库和隐式嵌入增强技术，在降低计算成本的同时提升了资源受限设备上的VPR性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于DINOv2等大模型的VPR方法虽然性能优秀，但计算成本过高，无法在资源受限设备上部署。需要一种既能保持高性能又能降低计算负担的解决方案。

Method: 采用非对称框架：高容量图库模型离线提取特征，轻量级查询网络在线处理。引入地理记忆库利用VPR数据库中的地理位置元数据组织图库特征，避免昂贵的k-NN计算。使用隐式嵌入增强技术提升查询网络对特征变化的建模能力。

Result: 实验表明该方法显著降低了计算成本，同时超越了现有的非对称检索技术，在资源受限环境中建立了新的VPR标准。

Conclusion: 提出的非对称VPR框架通过地理记忆库和嵌入增强技术，成功解决了异构网络兼容性问题，为资源受限环境下的视觉地点识别提供了高效可行的解决方案。

Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR

</details>


### [139] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

TL;DR: 提出了一种融合检索增强生成（RAG）的持续学习框架，通过动态知识蒸馏解决多模态医学视觉语言模型在持续学习中的细粒度特征保留与跨模态域间隙问题。


<details>
  <summary>Details</summary>
Motivation: 多模态生物医学视觉语言模型在持续学习中面临核心困境：如何在保持细粒度模态内特征的同时跨越不同模态间的显著域间隙。

Method: 基于1800万篇PubMed科学论文构建多模态医学检索数据库，采用多层多模态RAG系统提供实时指导，并引入动态知识蒸馏框架动态调节参数空间重要性、知识蒸馏粒度和参考数据集分布。

Result: 在设计的医疗通用任务增量学习（MGTIL）基准测试中，该方法在所有指标上均达到最先进性能。

Conclusion: 该方法有效解决了医学多模态持续学习中的关键挑战，展示了显著的临床价值。

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [140] [Heart Disease Prediction using Case Based Reasoning (CBR)](https://arxiv.org/abs/2512.13078)
*Mohaiminul Islam Bhuiyan,Chan Hue Wah,Nur Shazwani Kamarudin,Nur Hafieza Ismail,Ahmad Fakhri Ab Nasir*

Main category: cs.CV

TL;DR: 本研究比较了三种智能系统方法（模糊逻辑、神经网络和案例推理）在心脏病预测中的准确性，最终选择案例推理（CBR）方法，实现了97.95%的预测准确率，并发现男性心脏病概率（57.76%）高于女性（42.24%）。


<details>
  <summary>Details</summary>
Motivation: 传统依赖医生经验的心脏病预测方法精度不足，需要智能系统作为替代方案来提高预测准确性。

Method: 使用心脏病数据集进行数据预处理和分割，比较模糊逻辑、神经网络和案例推理（CBR）三种智能系统方法，最终采用CBR进行预测。

Result: CBR方法在心脏病预测中达到97.95%的准确率，男性患病概率为57.76%，女性为42.24%，吸烟和饮酒是主要风险因素。

Conclusion: 案例推理（CBR）是有效的心脏病预测方法，性别和生活方式因素对心脏病风险有显著影响，智能系统可辅助医疗决策。

Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.

</details>


### [141] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

TL;DR: 提出多样性正则化器(DiRe)来减少数据集压缩中的冗余，提高合成数据集的多样性


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法合成的数据集存在显著冗余，亟需减少冗余并提高合成数据集的多样性

Method: 提出基于余弦相似度和欧氏距离的多样性正则化器(DiRe)，可即插即用地应用于各种最先进的压缩方法

Result: 通过广泛实验证明，添加该正则化器可提升多种基准数据集上的压缩方法性能，在泛化性和多样性指标上均有改进

Conclusion: DiRe正则化器能有效提高数据集压缩方法的性能，适用于从CIFAR-10到ImageNet-1K等各种数据集

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [142] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

TL;DR: UniVCD是一种基于冻结SAM2和CLIP的无监督开放词汇变化检测方法，无需标注数据或配对图像即可检测跨场景的类别无关变化。


<details>
  <summary>Details</summary>
Motivation: 传统变化检测方法依赖监督学习，性能受数据集限制且标注成本高，只能检测预定义类别，泛化能力差。视觉基础模型的出现为突破这些限制提供了新机会。

Method: 使用冻结的SAM2和CLIP，引入轻量级特征对齐模块连接SAM2的空间细节表示和CLIP的语义先验，通过简化的后处理流程抑制噪声和伪变化。

Result: 在多个公开BCD和SCD基准测试中，UniVCD在F1和IoU等关键指标上表现一致强劲，匹配或超越现有开放词汇变化检测方法。

Conclusion: 基于冻结视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇变化检测的实用有效范式。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [143] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

TL;DR: ADHint是一种新的提示式强化学习方法，通过考虑样本难度来优化提示比例调度和相对优势估计，实现探索与模仿的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的RL方法通常忽略难度因素，导致学习不稳定和过度模仿离策略提示。

Method: 提出自适应提示与样本难度先验、基于一致性的梯度调制和选择性掩码、以及基于推演难度后验的优势估计。

Result: 在多种模态、模型规模和领域上的实验表明，ADHint在推理能力和分布外泛化方面优于现有方法。

Conclusion: ADHint通过难度感知的提示调度和优势估计，实现了更稳定的学习和更好的性能表现。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [144] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

TL;DR: 提出UnCoL框架，通过双教师模型和不确定性指导的伪标签学习，在医学图像分割中平衡通用性和专业性，显著减少标注需求。


<details>
  <summary>Details</summary>
Motivation: 解决视觉基础模型在医学图像分割中，面对有限标注或罕见病理变化时泛化能力不足的问题，弥合通用先验与任务特定需求之间的不匹配。

Method: UnCoL采用双教师框架：一个冻结的基础模型传递通用知识，一个逐步适应的教师模型捕捉细粒度任务特定表示。通过预测不确定性自适应调节伪标签学习，抑制不可靠监督。

Result: 在多种2D和3D分割基准测试中，UnCoL consistently优于最先进的半监督方法和基础模型基线，且标注需求显著减少。

Conclusion: UnCoL框架有效平衡了通用性和专业性，在医学图像分割中实现了接近全监督性能，同时大幅降低标注成本。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [145] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

TL;DR: FID-Net是一个基于YOLOv8n的深度学习模型，用于从无人机可见光图像中检测病虫害树木，并通过三个空间指标进行虫害分析，在32个森林样地的实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在大规模、细粒度森林病虫害监测方面存在局限，需要开发高效准确的监测技术来识别受感染树木并分析虫害模式。

Method: 基于YOLOv8n构建FID-Net，引入轻量级特征增强模块提取病害敏感特征，自适应多尺度特征融合模块对齐融合RGB和增强特征，以及高效通道注意力机制。从检测结果构建虫害分析框架，包括核密度估计、邻域评估和DBSCAN聚类。

Result: 在新疆东部天山32个森林样地的无人机图像实验中，FID-Net达到86.10%精确率、75.44%召回率、82.29% mAP@0.5和64.30% mAP@0.5:0.95，优于主流YOLO模型。分析证实受感染树木呈现明显聚类特征。

Conclusion: FID-Net能够准确区分树木健康状况，结合空间指标为智能病虫害监测、早期预警和精准管理提供可靠数据支持，有助于针对性森林保护。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [146] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

TL;DR: DiffFusion是一种基于扩散模型的多模态3D目标检测框架，通过扩散修复和自适应跨模态融合增强恶劣天气条件下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 多模态3D目标检测在恶劣天气条件下效果受限，主要由于天气引起的失真和不同数据模态之间的不对齐问题

Method: 提出DiffFusion框架，包括Diffusion-IR修复天气退化的图像，Point Cloud Restoration利用图像目标线索补偿损坏的LiDAR数据，以及BAFAM模块实现动态多模态融合和双向BEV对齐

Result: 在三个公共数据集上的实验表明，DiffFusion在恶劣天气下实现了最先进的鲁棒性，同时在清洁数据上保持强性能，在DENSE数据集上的零样本结果进一步验证了其泛化能力

Conclusion: DiffFusion框架有效解决了恶劣天气条件下的多模态3D目标检测问题，具有良好的鲁棒性和泛化性能，将作为开源项目发布

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [147] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

TL;DR: DePT3R是一个新颖的框架，能够在单次前向传播中同时执行动态场景的密集点跟踪和3D重建，无需相机位姿，显著提高了在动态环境中的适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前动态场景密集3D点跟踪方法通常依赖成对处理、已知相机位姿或假设时间顺序，限制了其灵活性和适用性。同时，大规模无位姿图像集合的3D重建技术进展为动态场景理解提供了统一方法的机遇。

Method: 通过强大的骨干网络提取深度时空特征，并使用密集预测头回归像素级映射，实现多任务学习。该方法在无需相机位姿的情况下运行，增强了在快速变化的动态环境中的适应性和效率。

Result: 在多个涉及动态场景的挑战性基准测试上验证了DePT3R，展示了强大的性能和相对于现有最先进方法在内存效率上的显著改进。

Conclusion: DePT3R为动态场景的密集点跟踪和3D重建提供了一种高效、灵活的统一解决方案，特别是在快速变化的动态环境中表现出色。

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [148] [LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping](https://arxiv.org/abs/2512.13130)
*Shanghua Liu,Majharulislam Babor,Christoph Verduyn,Breght Vandenberghe,Bruno Betoni Parodi,Cornelia Weltzien,Marina M. -C. Höhne*

Main category: cs.CV

TL;DR: 本文提出了CanolaTrack数据集和LeafTrackNet框架，用于在复杂作物（如油菜）中实现高精度叶片跟踪，解决了现有方法在真实条件下跟踪叶片动态变化的局限性。


<details>
  <summary>Details</summary>
Motivation: 高分辨率叶片表型分析对植物发育和胁迫响应研究至关重要，但现有叶片跟踪方法在复杂作物和真实条件下效果有限，缺乏大规模数据集和鲁棒的跟踪算法。

Method: 开发了LeafTrackNet框架，结合YOLOv10叶片检测器和MobileNetV3嵌入网络，采用基于嵌入的记忆关联策略在推理过程中维持叶片身份。

Result: LeafTrackNet在CanolaTrack数据集上优于植物专用跟踪器和最先进的多目标跟踪基线，HOTA指标提升9%。

Conclusion: 本研究为真实条件下的叶片级跟踪设立了新标准，提供了农业作物中最大的叶片跟踪数据集，将推动植物表型研究的未来发展。

Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.

</details>


### [149] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文提出了一种称为权重空间相关分析的方法，用于量化深度学习模型在医学图像分析中对混淆元数据的特征利用情况，验证模型的可信度。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度学习模型容易受到捷径学习的影响，依赖图像嵌入中编码的混淆元数据（如扫描仪型号）。关键问题是模型是否主动利用这些编码信息进行最终预测。

Method: 引入权重空间相关分析方法，通过测量主要临床任务和辅助元数据任务的分类头之间的对齐程度来量化特征利用。首先验证方法能成功检测人为诱导的捷径学习，然后应用于SA-SonoNet模型进行自发早产预测分析。

Result: 分析证实，虽然嵌入包含大量元数据，但sPTB分类器的权重向量与临床相关因素（如出生体重）高度相关，而与临床无关的采集因素（如扫描仪）解耦。

Conclusion: 该方法为验证模型可信度提供了工具，证明在没有诱导偏置的情况下，临床模型会选择性地利用与真实临床信号相关的特征。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [150] [StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion](https://arxiv.org/abs/2512.13147)
*Sangmin Hong,Suyoung Lee,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: StarryGazer是一个无需真实深度标签的深度补全框架，通过结合单目深度估计模型和稀疏深度图来预测密集深度图像，在多个数据集上优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督深度补全方法需要辅助数据，而单目深度估计模型虽然能从单张图像生成相对深度图，但无法有效结合稀疏深度信息。简单的仿射变换会导致高误差，因为单目深度估计在物体间深度差异估计上不准确。

Method: 首先使用预训练的单目深度估计模型生成相对深度图，对这些图像进行分割和随机缩放，形成合成的密集伪真值和对应稀疏深度的训练对。然后训练一个细化网络，结合相对深度图和RGB图像来提高模型的准确性和鲁棒性。

Result: StarryGazer在多个数据集上表现出优于现有无监督方法和转换后单目深度估计结果的性能，证明了该框架能够有效利用单目深度估计模型的能力，同时通过稀疏深度信息适当修正错误。

Conclusion: 该框架成功实现了无需真实深度标签的深度补全，展示了结合单目深度估计和稀疏深度信息的有效性，为实际应用场景提供了可行的解决方案。

Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.

</details>


### [151] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: 提出Intrinsic Image Fusion方法，从多视角图像重建高质量物理材质，通过结合单视角先验和扩散模型预测，使用鲁棒优化框架融合多视角预测，最终通过逆路径追踪优化低维参数


<details>
  <summary>Details</summary>
Motivation: 材质重建问题高度欠约束，传统基于分析-合成的方法需要昂贵的路径追踪且噪声较大，需要更好的约束条件来提升重建质量

Method: 1) 使用扩散模型生成单视角材质分解候选；2) 拟合低维参数函数减少预测不一致性；3) 提出鲁棒优化框架，结合软预测选择和置信度多视角内点集；4) 使用逆路径追踪优化低维参数

Result: 在合成和真实场景的材质解缠任务中超越现有最优方法，产生清晰干净的重建结果，适合高质量重光照

Conclusion: 该方法通过结合单视角先验和多视角一致性约束，有效解决了材质重建的欠约束问题，实现了高质量的物理材质重建

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [152] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

TL;DR: CRAFTS是首个病理学专用的文本到图像生成基础模型，通过相关性调节对齐框架解决生成模型在病理图像合成中的语义不稳定和形态学幻觉问题，能够生成30种癌症类型的多样化病理图像，并通过数据增强提升临床任务的性能。


<details>
  <summary>Details</summary>
Motivation: 临床级病理AI的发展受限于高质量标注数据的稀缺性，现有生成模型存在语义漂移和形态学幻觉问题，影响诊断可靠性。

Method: 采用双阶段训练策略，在280万图像-标题对数据集上训练，引入新颖的对齐机制抑制语义漂移，确保生物学准确性。结合ControlNet实现从核分割掩码和荧光图像等输入对组织结构的精确控制。

Result: 生成的病理图像质量通过客观指标和病理学家评估严格验证，CRAFTS增强的数据集在分类、跨模态检索、自监督学习和视觉问答等多种临床任务中提升了性能。

Conclusion: CRAFTS通过克服数据稀缺和隐私问题，为罕见和复杂癌症表型提供了无限多样的标注组织学数据来源，有效推动了稳健诊断工具的开发。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [153] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

TL;DR: DFSS是一种专门为语义分割设计的无数据知识蒸馏框架，通过利用批归一化统计和渐进式蒸馏策略，在保持场景结构连续性的同时实现高效的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 现有无数据知识蒸馏方法主要针对分类任务设计，忽略了语义分割中空间连续性和结构一致性的重要性，导致直接应用于分割任务时性能显著下降。

Method: 提出DFSS框架：1）利用教师模型的批归一化统计指导近似分布采样（ADS），选择更接近原始训练分布的数据；2）提出加权分布渐进蒸馏（WDPD），动态优先处理可靠样本并逐步引入挑战性样本。

Result: 在标准基准测试上的广泛实验表明，DFSS在语义分割的无数据蒸馏方法中持续优于现有方法，达到了最先进的性能，同时显著减少了对辅助数据的依赖。

Conclusion: DFSS通过尊重真实世界场景的结构和上下文连续性，为语义分割任务提供了有效的无数据知识蒸馏解决方案，证明了在保持空间连贯性的同时进行知识迁移的重要性。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [154] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

TL;DR: CoRA是一个新颖的协作感知架构，通过混合融合方法在低通信成本下实现性能与鲁棒性的解耦，在极端场景下比基线性能提升约19%，通信量减少5倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有SOTA方法在恶劣通信条件下性能下降，因为数据传输导致的对齐问题严重阻碍了实际部署。作者发现中间融合和后期融合的优势不是权衡关系而是互补关系。

Method: CoRA包含两个组件：特征级融合分支和对象级校正分支。第一分支选择关键特征并高效融合以确保性能和可扩展性；第二分支利用语义相关性校正空间位移，保证对姿态误差的鲁棒性。

Result: 在极端场景下，CoRA在AP@0.7指标上比基线性能提升约19%，同时通信量减少5倍以上。

Conclusion: CoRA是解决鲁棒协作感知问题的有前景方案，成功实现了在低通信成本下的高性能和强鲁棒性。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [155] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

TL;DR: POLAR是一个大规模物理校准的One-Light-at-a-Time数据集，POLARNet是基于该数据集的流式生成模型，用于从单张肖像预测光照响应，实现可扩展、可控的人脸重光照。


<details>
  <summary>Details</summary>
Motivation: 解决人脸重光照任务中大规模、物理一致光照数据稀缺的问题，提供更真实、可控的重光照效果。

Method: 构建包含200+受试者、156种光照方向的POLAR数据集，开发基于流的生成模型POLARNet，将光照建模为连续、物理可解释的照明状态变换。

Result: 建立了统一的光照学习框架，能够捕捉细粒度、方向感知的光照效果，同时保持面部身份特征。

Conclusion: POLAR和POLARNet形成了自持续的"鸡生蛋-蛋生鸡"循环，为可扩展、可复现的肖像光照处理奠定了基础。

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [156] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Ego-EXTRA是一个用于专家-学员辅助的50小时第一人称视频数据集，包含真实专家通过自然语言指导学员执行程序性活动的对话，并创建了超过15k个视觉问答对来评估多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量的第一人称视角下专家与学员交互的对话数据集，需要开发能够提供专家级辅助的智能助手，但现有模型在此类任务上表现有限。

Method: 采用"Wizard of OZ"数据收集范式，专家通过穿戴设备观看学员的第一人称视角视频，以自然语言回答问题或主动提供建议，记录双向对话并转录为视觉问答数据集。

Result: 创建了包含15k+高质量视觉问答对的基准测试集，评估显示当前多模态大语言模型在提供专家级辅助方面存在明显局限性，Ego-EXTRA数据集具有挑战性。

Conclusion: Ego-EXTRA为第一人称视频语言助手提供了公开可用的基准测试资源，揭示了当前模型在专家级辅助任务上的不足，推动了相关研究发展。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [157] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: STARCaster是一个统一的身份感知时空视频扩散模型，能够处理语音驱动肖像动画和自由视角说话肖像合成，通过软性身份约束和隐式3D感知解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有2D语音到视频扩散模型过度依赖参考引导导致运动多样性有限，而3D感知动画通常基于预训练三平面生成器导致重建不完美和身份漂移。需要重新思考参考和几何范式。

Method: 采用组合方法：从ID感知运动建模开始，通过唇读监督实现视听同步，最后通过时空适应实现新视角动画。使用解耦学习方法分别训练视图一致性和时间连贯性，以及自强制训练方案学习更长时间上下文。

Result: 综合评估表明STARCaster在跨任务和身份上有效泛化，在不同基准测试中持续超越先前方法。

Conclusion: STARCaster通过软性身份约束和隐式3D感知，在统一框架下成功解决了语音驱动肖像动画和自由视角说话肖像合成的挑战，克服了现有方法的局限性。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [158] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

TL;DR: VG-AVS任务通过视觉信息选择最有信息量的下一个视角，无需场景记忆或外部知识，提升视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型仅限于静态图像推理，而具身智能体需要主动移动获取更信息丰富的视角。

Method: 构建合成数据集，通过监督微调（SFT）和基于强化学习（RL）的策略优化微调预训练VLM。

Result: 在视角选择基础上实现强问题回答性能，并泛化到未见过的合成和真实场景，提升现有EQA系统的下游问答准确性。

Conclusion: VG-AVS框架有效提升视觉问答性能，并增强现有场景探索EQA系统的下游任务表现。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [159] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

TL;DR: CogniEdit是一个基于扩散模型的指令图像编辑框架，通过多模态推理和密集奖励优化解决细粒度指令编辑问题，实现了轨迹级别的梯度传播控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法在处理细粒度指令（如颜色、位置、数量等精确属性）时表现不佳，且现有GRPO方法仅在单个采样步骤优化，反馈稀疏，限制了轨迹级别控制。

Method: CogniEdit包含三个核心组件：1）多模态大语言模型分解复杂指令为可执行指令；2）动态令牌焦点重定位自适应强调细粒度属性；3）基于密集GRPO的优化，在连续去噪步骤间传播梯度实现轨迹级别监督。

Result: 在基准数据集上的广泛实验表明，CogniEdit在平衡细粒度指令遵循与视觉质量和可编辑性保持方面达到了最先进的性能。

Conclusion: 提出的CogniEdit框架通过结合多模态推理和密集奖励优化，有效解决了细粒度指令图像编辑的挑战，为轨迹级别的扩散模型控制提供了新思路。

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [160] [Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?](https://arxiv.org/abs/2512.13281)
*Jiaqi Wang,Weijia Wu,Yi Zhan,Rui Zhao,Ming Hu,James Cheng,Wei Liu,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 本文提出了Video Reality Test基准测试，用于评估AI生成视频的真实性，特别关注音频-视觉耦合条件下的感知真实性。研究发现当前最佳视频生成模型Veo3.1-Fast能够欺骗大多数视觉语言模型，而添加音频有助于真假区分。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成视频检测的挑战，评估视频生成模型能否产生具有沉浸感的音频配对视频，以及这些视频是否能可靠欺骗人类和视觉语言模型。

Method: 构建基于ASMR视频的基准测试套件，采用对抗性的创作者-评审者协议，其中视频生成模型作为创作者试图欺骗评审者，而视觉语言模型作为评审者识别虚假内容。

Result: 最佳创作者Veo3.1-Fast欺骗了大多数VLM：最强评审者Gemini 2.5-Pro仅达到56%准确率（随机为50%），远低于人类专家（81.25%）。添加音频改善了真假区分，但水印等表面线索仍会误导模型。

Conclusion: 研究划定了当前视频生成真实性的边界，暴露了视觉语言模型在感知保真度和音频-视觉一致性方面的局限性。

Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.

</details>


### [161] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

TL;DR: 本文提出CausalCLIP框架，通过因果推断原理从预训练视觉语言模型中解耦因果特征与非因果特征，提升生成图像检测器的跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法（包括基于预训练视觉语言模型的方法）往往产生高度纠缠的特征表示，混合了任务相关的取证线索（因果特征）与虚假或无关模式（非因果特征），限制了模型的泛化能力。

Method: 提出CausalCLIP框架：1）使用结构因果模型建模生成过程；2）通过Gumbel-Softmax特征掩码和Hilbert-Schmidt独立性准则约束实现统计独立性；3）分离稳定的因果特征以抵抗分布偏移。

Result: 在未见过的不同系列生成模型上测试，CausalCLIP相比最先进方法在准确率上提升6.83%，平均精度提升4.06%，表现出强大的泛化能力。

Conclusion: CausalCLIP通过显式解耦因果特征与非因果特征，并基于因果推断原理进行针对性过滤，有效保留了最具可迁移性和判别性的取证线索，显著提升了生成图像检测器的跨模型泛化性能。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [162] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

TL;DR: 本文针对扩散模型在物理对齐和分布外指令遵循方面的不足，提出了基于因果分析的LINA框架，通过针对性干预和因果感知的去噪调度，显著提升了图像和视频生成的物理合理性和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了显著成功，但在物理对齐和分布外指令遵循方面仍存在困难。作者认为这些问题源于模型未能学习因果方向和因果因子的解耦重组。

Method: 引入因果场景图和物理对齐探测数据集进行诊断干预分析，提出LINA框架，包括：(1)在提示和视觉潜在空间中的针对性引导；(2)重新分配的因果感知去噪调度。

Result: LINA框架在具有挑战性的因果生成任务和Winoground数据集上实现了最先进的性能，有效增强了物理对齐和分布外指令遵循能力。

Conclusion: 通过因果分析和针对性干预，LINA框架显著提升了扩散模型在物理对齐和指令遵循方面的表现，为因果感知的生成模型提供了新思路。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [163] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

TL;DR: 论文提出了创意表格可视化任务，通过结合MLLM和扩散模型的自我修正流程，实现高保真度的数据可视化生成


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在需要深度推理和精确数据到视觉映射的任务上表现不足，需要突破一般场景的限制

Method: 提出ShowTable流程，将MLLM作为推理中枢进行视觉规划和错误判断，扩散模型执行指令，通过渐进式自我修正过程实现高质量可视化

Result: 实验表明该流程显著优于基线方法，在TableVisBench基准测试中表现出强大的多模态推理、生成和纠错能力

Conclusion: 该方法有效解决了创意表格可视化的挑战，展示了多模态模型在复杂推理任务中的潜力

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [164] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

TL;DR: KlingAvatar 2.0是一个时空级联框架，通过空间分辨率上采样和时间维度扩展，解决了长时高分辨率视频生成中的时间漂移、质量下降和提示跟随弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有头像视频生成模型在生成长时高分辨率视频时效率有限，存在时间漂移、质量下降和提示跟随弱的问题。

Method: 采用时空级联框架，首先生成低分辨率蓝图视频关键帧捕捉全局语义和运动，然后使用首末帧策略细化成高分辨率时间一致子片段。引入协同推理导演（三个模态特定LLM专家）增强跨模态指令融合和对齐，负向导演进一步优化负向提示。

Result: 实验表明，该模型有效解决了高效、多模态对齐的长时高分辨率视频生成挑战，提供增强的视觉清晰度、逼真的唇齿渲染与准确口型同步、强身份保持和连贯的多模态指令跟随。

Conclusion: KlingAvatar 2.0框架在长时高分辨率头像视频生成方面取得了显著改进，解决了时间一致性和多模态对齐的关键问题。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [165] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

TL;DR: 本文研究了面向人脸检索系统的身份遗忘问题，提出了一种基于嵌入分散的无学习方法，旨在使特定身份不可检索同时保持其他身份的检索性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统虽然能有效进行身份检索，但存在严重的隐私风险。现有机器遗忘方法在基于嵌入的现代人脸识别模型中的适用性尚未充分探索，需要解决如何在遗忘特定身份的同时保持嵌入空间判别结构的问题。

Method: 评估了多种现有近似类别遗忘方法（如随机标记、梯度上升、边界遗忘等），并提出了一种简单有效的基于分散的无学习方法，通过在超球面上分散目标身份的嵌入来防止形成紧凑的身份簇。

Result: 在VGGFace2和CelebA等标准基准上的大量实验表明，该方法在实现优越遗忘效果的同时，能够很好地保持检索效用。

Conclusion: 该研究为人脸检索系统的隐私保护提供了有效的身份遗忘解决方案，通过嵌入分散技术成功解决了在保护隐私的同时保持系统性能的关键挑战。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [166] [Automated User Identification from Facial Thermograms with Siamese Networks](https://arxiv.org/abs/2512.13361)
*Elizaveta Prozorova,Anton Konev,Vladimir Faerman*

Main category: cs.CV

TL;DR: 本文分析了基于面部热成像图的生物识别中热成像技术的应用，比较了不同红外光谱范围，提出了热成像相机技术要求，并使用Siamese神经网络实现自动识别，在实验中达到约80%准确率。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的安全系统需要有效的生物识别技术，热成像技术作为一种非接触式识别方式具有独特优势，但需要系统研究其技术要求和实现方法。

Method: 采用比较分析方法研究不同红外光谱范围（NIR、SWIR、MWIR、LWIR），定义热成像相机技术要求（传感器分辨率、热灵敏度、30Hz帧率），并使用Siamese神经网络进行自动识别。

Result: 在专有数据集上的实验表明，所提方法达到约80%的准确率，同时研究发现可见光和红外光谱的混合系统能克服单一模态的局限性。

Conclusion: 热成像技术是开发可靠安全系统的有前景技术，结合多种光谱的混合系统具有更好的应用潜力。

Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.

</details>


### [167] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

TL;DR: 本文提出了一种改进的平衡组Softmax（BAGS）框架和度量学习方法，用于解决长尾分布下的2D目标检测问题，在LVISv1数据集上取得了24.5% mAP的新SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的目标检测数据集往往存在长尾分布问题，即少数类别拥有大量样本而多数类别只有少量样本。这种类别不平衡会导致检测模型偏向于频繁类别，在稀有类别上性能下降。

Method: 1. 使用两阶段Faster R-CNN架构
2. 改进平衡组Softmax（BAGS）框架来缓解类别不平衡
3. 探索度量学习方法，生成类间分离、类内紧凑的特征嵌入
4. 在推理阶段使用k近邻（k-NN）方法提升分类性能

Result: 在LVISv1数据集（1,203个类别，164,000张图像）上实现了24.5%的mAP，超越了之前24.0%的基准。

Conclusion: 所提出的方法在长尾目标检测任务中表现出色，特别是通过度量学习和k-NN分类器有效提升了稀有类别的检测性能。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [168] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

TL;DR: 本文提出了一种利用DINO自注意力关键特征的息肉分割框架，通过简单的卷积解码器实现鲁棒分割，在数据稀缺和挑战性场景下显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在息肉分割中泛化能力不足，特别是在数据受限或复杂场景下，且许多方法依赖复杂的任务特定架构。

Method: 利用DINO自注意力模块的关键特征，结合简单卷积解码器预测息肉掩码，避免使用最深层的Vision Transformer特征。

Result: 在多中心数据集上的实验表明，该方法在领域泛化和极端单领域泛化协议下达到SOTA性能，超越了nnU-Net和UM-Net等模型。

Conclusion: 该框架在不使用息肉特定架构的情况下实现了优异的泛化能力，并对DINO框架演进进行了系统性基准测试。

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [169] [Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs](https://arxiv.org/abs/2512.13392)
*Anran Qi,Changjian Li,Adrien Bousseau,Niloy J. Mitra*

Main category: cs.CV

TL;DR: 该论文提出了一种图像到视频生成方法，通过用户可编辑的代理动态图（PDG）实现对新显露区域内容的精确控制，解决了现有方法在生成可预测的关节运动方面的困难。


<details>
  <summary>Details</summary>
Motivation: 当前图像到视频生成方法虽然能产生合理的运动，但难以生成可预测的关节运动，同时无法在视频中新显露的区域强制执行用户指定的内容。

Method: 采用训练无关的流程，将运动规范与外观合成分离：用户通过轻量级PDG指定运动，利用冻结的扩散先验进行外观合成，通过潜在空间合成协调运动与用户意图。

Result: 该方法在关节物体、家具、车辆和可变形物体的图像转短视频生成中表现出明显优势，实现了可控的关节运动和用户对新显露区域内容的控制。

Conclusion: 该方法结合了生成控制（宽松的姿态和结构）与可预测控制（最终帧新显露区域的外观规范），开辟了新的图像到视频工作流程。

Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

</details>


### [170] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

TL;DR: 本文提出使用神经细胞自动机（NCA）作为有效的分割掩码细化机制，通过学习局部迭代更新来修复拓扑错误，提升分割结果的拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: 通用分割模型通常产生碎片化或不连贯的输出，修复这些伪影需要手工制定的细化规则或针对特定任务的专用架构。本文旨在探索一种通用的细化方法。

Method: 通过在不完美的掩码和真实标签上训练NCA，使自动机学习目标形状的结构特性，仅依赖局部信息进行迭代更新，逐步重新连接断裂区域、修剪松散片段。

Result: 在视网膜血管分割中，Dice/clDice提升2-3%，Betti错误显著降低（β₀错误减少60%，β₁减少20%）；在心肌分割中，零样本设置下修复了61.5%的断裂案例，ASSD和HD分别降低19%和16%。

Conclusion: NCA可作为有效且广泛适用的细化器，能够修复不同基础分割模型和任务产生的常见拓扑错误。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [171] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

TL;DR: End2Reg是一个端到端的深度学习框架，通过联合优化分割和配准，无需弱分割标签和手动步骤，在脊柱手术导航中实现了毫米级精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于术中放射成像和骨锚标记的脊柱手术导航系统具有侵入性、辐射强度高且工作流程中断的问题。现有的无标记RGB-D配准方法依赖弱分割标签，可能导致错误传播。

Method: 提出端到端深度学习框架，网络仅通过配准目标学习专门优化的分割掩码，无需直接分割监督，联合优化分割和配准。

Result: 在体外和体内基准测试中达到最先进性能，中位目标配准误差降低32%至1.83mm，均方根误差降低45%至3.95mm。消融研究证实端到端优化显著提高配准精度。

Conclusion: 该端到端RGB-D配准管道消除了对弱标签和手动步骤的依赖，朝着完全自动化的无标记术中导航迈进。

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [172] [Computer vision training dataset generation for robotic environments using Gaussian splatting](https://arxiv.org/abs/2512.13411)
*Patryk Niżeniec,Marcin Iwanowski*

Main category: cs.CV

TL;DR: 本文提出了一种用于机器人视觉任务的新型大规模数据集生成流程，通过3D高斯泼溅和两阶段渲染技术解决合成数据与真实数据之间的领域差距问题，并实现自动像素级标注。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉任务中合成数据与真实图像之间的领域差距问题，以及手动标注耗时费力的瓶颈，为机器人环境提供高质量、自动标注的大规模数据集。

Method: 使用3D高斯泼溅创建真实环境表示，在游戏引擎中进行物理模拟生成自然布局，采用两阶段渲染技术结合泼溅真实感和代理网格生成的阴影图，自动生成像素级分割掩码。

Result: 实验表明，结合少量真实图像和大量合成数据的混合训练策略在目标检测和分割任务上表现最佳，验证了该策略在高效获得鲁棒准确模型方面的优势。

Conclusion: 该流程为机器人视觉任务提供了一种高效生成大规模、高质量标注数据的方法，混合训练策略被证明是实现高性能模型的优化方案。

Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.

</details>


### [173] [USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition](https://arxiv.org/abs/2512.13415)
*Ahmed Abul Hasanaath,Hamzah Luqman*

Main category: cs.CV

TL;DR: 提出了一种名为USTM的统一时空建模框架，用于连续手语识别，通过Swin Transformer骨干网络和轻量级时间适配器TAPE来捕捉细粒度空间特征和长短时时间依赖，在多个数据集上达到state-of-the-art性能。


<details>
  <summary>Details</summary>
Motivation: 现有的连续手语识别方法主要基于CNN空间骨干网络结合时间卷积或循环模块，这些方法难以捕捉细粒度的手部和面部线索，且无法有效建模长程时间依赖关系。

Method: 提出USTM框架，结合Swin Transformer骨干网络和轻量级时间适配器TAPE（包含位置嵌入），能够同时捕捉细粒度空间特征和长短时时间上下文，仅使用RGB视频输入而不依赖多流输入或辅助模态。

Result: 在PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上的实验表明，USTM在基于RGB的方法中达到state-of-the-art性能，甚至优于多模态方法，与多流方法性能相当。

Conclusion: USTM框架展示了在连续手语识别任务中的强大有效性和优势，代码已开源。

Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

</details>


### [174] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的元跨任务不可利用示例生成框架（MCT-UEG），旨在生成具有广泛不可利用性的个人图像，防止在线个人图像被未经授权利用。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成广泛不可利用的示例方面存在局限性，无法在不同现实计算机视觉任务中保持有效性，限制了实际应用。

Method: 设计了基于平坦最小值的元训练和测试方案，优化不可利用示例生成器，以有效产生广泛不可利用的示例。

Result: 大量实验证明了该框架的有效性。

Conclusion: MCT-UEG框架能够生成具有广泛不可利用性的示例，提升了个人数据隐私保护的实际应用能力。

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [175] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

TL;DR: RecTok是一种新型视觉分词器，通过流语义蒸馏和重建对齐蒸馏技术，克服了高维视觉分词器在扩散模型中的性能限制，实现了更好的图像重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉分词器在潜在空间维度和生成质量之间存在固有权衡，高维分词器性能不如低维版本。需要解决高维潜在空间的语义丰富性和生成质量之间的矛盾。

Method: 提出两种关键技术：1）流语义蒸馏 - 将视觉基础模型的语义信息蒸馏到流匹配的前向流轨迹中；2）重建对齐蒸馏 - 引入掩码特征重建损失进一步增强语义。

Result: RecTok在gFID-50K基准测试中取得了最先进的结果，在有无分类器自由引导的设置下都表现优异。随着潜在维度增加，性能持续提升。

Conclusion: RecTok通过将关注点从潜在空间转移到流匹配的前向流轨迹，成功克服了高维视觉分词器的性能限制，实现了高质量的图像重建和生成。

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [176] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

TL;DR: MineTheGap是一种自动挖掘文本到图像模型偏见提示的方法，通过遗传算法优化提示来暴露模型偏见


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在面对模糊提示时会产生偏见，这些偏见可能带来社会影响（如职业种族偏见）和用户体验问题（缺乏多样性）

Method: 使用遗传算法迭代优化提示池，通过新颖的偏见评分来驱动优化过程，该评分通过比较生成图像分布与LLM生成的文本变体分布来计算

Result: 该方法在已知偏见数据集上得到验证，能够有效识别和排序偏见的严重程度

Conclusion: MineTheGap方法超越了简单的偏见检测，能够主动挖掘导致模型偏见的提示，为理解和完善TTI模型提供了有效工具

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [177] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 提出了一种轻量级的小样本学习框架，结合MobileNet特征提取器和Bi-LSTM注意力分类器，在数据稀缺环境下实现高精度植物病害识别，模型仅40MB大小。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习方法依赖大数据集和计算资源的问题，适应数据稀缺和资源受限的农业环境需求。

Method: 使用MobileNetV2/V3作为特征提取器，结合特征融合技术和带注意力的Bi-LSTM分类器，在小样本场景下进行训练。

Result: 在PlantVillage数据集上15-shot达到98.23%准确率，接近SOTA的99.98%；在真实场景Dhan Shomadhan数据集上达到69.28%，优于之前96.0%的SOTA。

Conclusion: 该框架为数据稀缺地区提供了一个可扩展、移动就绪的植物病害诊断解决方案，具有实际应用价值。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [178] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

TL;DR: IMILIA是一个用于炎症性肠病(IBD)组织切片炎症预测和自动标记的可解释多实例学习框架，包含炎症预测模块和可解释性模块，在多个验证队列中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着IBD治疗目标转向组织学缓解，准确评估微观炎症对于评估疾病活动性和治疗反应变得至关重要。

Method: IMILIA框架包含炎症预测模块(基于多实例学习的模型)和可解释性模块(HistoPLUS用于细胞检测、分割和分类，EpiSeg用于上皮分割)。

Result: 在发现队列中ROC-AUC为0.83，在两个外部验证队列中分别达到0.99和0.84。可解释性模块显示高预测分数区域免疫细胞密度增加，低分数区域主要为正常上皮细胞。

Conclusion: IMILIA能够准确预测IBD组织切片中的炎症，并提供生物学一致的可解释性见解，有助于IBD的精准评估和治疗监测。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [179] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

TL;DR: 使用扩散模型在测试时将目标域图像映射回源域分布，以提升域泛化性能，无需大规模合成数据生成


<details>
  <summary>Details</summary>
Motivation: 生成式基础模型包含广泛视觉知识但合成目标域变体成本高且不完整，需要更高效的域泛化方法

Method: 在测试时利用扩散模型将目标图像反向映射到下游模型训练时的源域分布，仅需源域描述且保留任务模型

Result: 在分割、检测和分类任务中实现显著提升：BDD100K-Night相对增益137%，ImageNet-R增益68%，DarkZurich增益62%

Conclusion: 该方法在未知目标分布的真实域泛化场景中表现一致优异，为域泛化提供了高效替代方案

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [180] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

TL;DR: PoseAnything是一个通用的姿态引导视频生成框架，能够处理人类和非人类角色的任意骨骼输入，通过部件感知时序一致性模块和运动解耦CFG策略，实现了更好的运动一致性和独立相机控制。


<details>
  <summary>Details</summary>
Motivation: 当前姿态引导视频生成方法仅限于人类姿态输入，对其他主体的姿态泛化能力差，需要开发能够处理任意主体姿态的通用框架。

Method: 提出PoseAnything框架，包含部件感知时序一致性模块（将主体分为不同部件并建立跨帧对应关系）和主体与相机运动解耦CFG策略（独立控制相机运动）。同时构建了包含5万对非人类姿态-视频数据的高质量数据集XPose。

Result: 实验表明，PoseAnything在效果和泛化能力上显著优于现有最先进方法，能够生成更一致的运动视频并支持独立相机控制。

Conclusion: 该工作首次实现了通用的姿态引导视频生成，突破了人类姿态的限制，为动画制作等应用提供了更强大的控制能力。

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [181] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

TL;DR: T3-Video是一种新的Transformer改造策略，能够在保持预训练模型架构不变的情况下，通过优化前向逻辑显著降低计算需求，实现10倍以上的4K视频生成加速，同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决4K视频生成中因时空分辨率增加导致的二次计算爆炸问题，平衡效率与质量之间的矛盾。

Method: 提出T3策略，采用多尺度权重共享窗口注意力机制，结合分层分块和轴保持全注意力设计，实现预训练模型的注意力模式转换。

Result: 在4K-VBench上显著优于现有方法，VQA指标提升4.29，VTC指标提升0.08，同时实现10倍以上的4K视频生成加速。

Conclusion: T3-Video证明了在不改变预训练模型核心架构的情况下，通过优化前向逻辑可以显著提升4K视频生成的效率和性能。

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [182] [Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation](https://arxiv.org/abs/2512.13495)
*Jiangning Zhang,Junwei Zhu,Zhenye Gan,Donghao Luo,Chuming Lin,Feifan Xu,Xu Peng,Jianlong Hu,Yuansen Liu,Yijia Hong,Weijian Cao,Han Feng,Xu Chen,Chencan Fu,Keke He,Xiaobin Hu,Chengjie Wang*

Main category: cs.CV

TL;DR: Soul是一个多模态驱动的数字人动画框架，能够从单张肖像图、文本提示和音频生成高质量的长时视频，实现精确的唇部同步、生动的面部表情和稳健的身份保持。


<details>
  <summary>Details</summary>
Motivation: 解决当前数字人动画在数据稀缺、长期生成一致性、推理效率等方面的挑战，为虚拟主播、影视制作等实际应用提供更优解决方案。

Method: 基于Wan2.2-5B骨干网络，集成音频注入层和多种训练策略，采用阈值感知码本替换确保长期一致性，使用步长/CFG蒸馏和轻量VAE优化推理效率。

Result: 在视频质量、视频-文本对齐、身份保持和唇部同步精度上显著优于当前领先的开源和商业模型，推理速度提升11.4倍且质量损失可忽略。

Conclusion: Soul框架在数字人动画领域表现出色，具有广泛的实际应用前景，特别是在虚拟主播和影视制作场景中。

Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

</details>


### [183] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.5 pro是一个专门用于原生联合音频-视频生成的基础模型，采用双分支Diffusion Transformer架构，通过跨模态联合模块和多阶段数据管道实现卓越的视听同步和生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成领域的进展为统一视听生成铺平了道路，需要开发能够原生联合生成音频和视频的模型。

Method: 采用双分支Diffusion Transformer架构，集成跨模态联合模块和专门的多阶段数据管道，并进行精细的后训练优化，包括监督微调和高品质数据集上的RLHF。

Result: 实现了卓越的视听同步和生成质量，支持精确的多语言和方言口型同步、动态电影级摄像机控制，并增强了叙事连贯性。推理速度提升超过10倍。

Conclusion: Seedance 1.5 pro定位为专业级内容创作的强大引擎，已在火山引擎平台上提供访问。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [184] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: TARA是一个无需视频数据就能将多模态大语言模型适配为时间感知视频-文本嵌入模型的方法，在时间感知检索基准上超越现有模型，并在否定理解、动词副词理解等任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 构建通用的时间感知视频-文本嵌入模型，解决现有模型在时间敏感任务上的不足。

Method: 提出TARA方法，通过适配多模态大语言模型来构建时间感知视频-文本嵌入模型，完全不需要视频数据。

Result: 在提出的时间感知基准上超越所有现有视频-文本模型，在否定理解和动词副词理解任务上达到SOTA性能。

Conclusion: TARA产生了一个强大、通用、时间感知的视频-文本嵌入模型，具有最先进的零样本性能。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [185] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: Pancakes框架能自动为医学图像生成多种语义一致的分割方案，解决了现有模型只能处理单一分割协议或需要手动提示的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型通常只能支持单一分割协议或需要人工干预，无法自动生成多种语义一致的分割方案。

Method: 提出Pancakes框架，通过新的问题建模方式，在未见过的医学图像上自动生成多标签分割图，保持跨图像语义一致性。

Result: 在7个测试数据集上的实验表明，该模型在生成多个语义一致的全图像分割方面显著优于现有基础模型。

Conclusion: Pancakes框架解决了医学图像多协议分割的挑战，为自动生成语义一致的分割方案提供了新思路。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [186] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一个新任务H2IAD（人-人交互异常检测），旨在检测协作3D人体动作中的异常交互行为，并提出了IADNet模型来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 现有的人体异常检测模型主要针对单个人体行为，但人类行为本质上是协作的，异常也可能来自人际互动。现有方法无法有效捕捉复杂且不对称的交互动态，导致检测精度低。

Method: 提出IADNet模型，包含Temporal Attention Sharing Module（TASM）来共享编码的运动嵌入以同步协作运动关联，以及Distance-Based Relational Encoding Module（DREM）来反映人际空间配置和社会线索，最后使用标准化流进行异常评分。

Result: 在人-人运动基准测试上的广泛实验表明，IADNet在H2IAD任务上优于现有的人体异常检测基线方法。

Conclusion: 该研究成功解决了人-人交互异常检测问题，提出的IADNet模型通过有效捕捉时间和空间交互特征，显著提升了检测性能。

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


### [187] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

TL;DR: 本文提出了MMhops基准测试和MMhops-R1框架，用于评估和提升多模态多跳推理能力，解决了现有MLLMs在复杂推理任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要局限于单步推理，缺乏复杂多跳推理能力，而现实世界问题需要跨模态和多步骤的迭代推理。

Method: 提出MMhops-R1框架，采用强化学习优化的多模态检索增强生成方法，支持动态规划推理路径、目标查询制定和多层次信息合成。

Result: 实验表明MMhops-R1在MMhops基准上显著优于强基线，证明了动态规划和多模态知识整合对复杂推理的重要性，且具有良好的泛化能力。

Conclusion: 本研究贡献了具有挑战性的新基准和强大的基线模型，将发布相关代码、数据和权重以推动该关键领域的研究发展。

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [188] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

TL;DR: LiMo是一种基于扩散模型的时空光照估计方法，通过生成不同曝光下的镜面球体和漫反射球体来预测高频细节和准确照度，结合深度和几何条件实现精确空间控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有光照估计方法在同时预测真实高频细节和准确照度方面的不足，特别是在时空场景下的光照建模挑战。

Method: 1. 生成不同曝光下的镜面和漫反射球体阵列；2. 在大规模定制数据集上微调扩散模型；3. 引入新的几何条件提供场景相对位置信息；4. 通过可微分渲染将多曝光预测合并为单一HDRI图。

Result: LiMo在空间控制和预测精度方面达到最先进水平，通过全面评估验证了方法的有效性。

Conclusion: 该方法成功实现了时空光照的高质量估计，为复杂场景的光照建模提供了有效的解决方案。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [189] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级的领域自适应自监督适配器（DA-SSL），用于解决病理学基础模型在特定癌症类型（如膀胱肿瘤经尿道切除术）上的领域偏移问题，无需微调基础模型本身。


<details>
  <summary>Details</summary>
Motivation: 现有的病理学基础模型在特定癌症类型（如TURBT）上表现不佳，因为这些癌症类型在预训练数据中很少出现，且样本中存在组织碎片和电灼伪影等罕见特征。

Method: 提出DA-SSL方法，通过自监督学习将预训练的病理学基础模型特征重新对齐到TURBT领域，结合多实例学习框架预测膀胱癌患者对新辅助化疗的治疗反应。

Result: 在多中心研究中，DA-SSL在五折交叉验证中达到AUC 0.77±0.04，外部测试准确率0.84，敏感性0.71，特异性0.91。

Conclusion: 轻量级的领域自适应结合自监督学习可以有效增强基于病理学基础模型的多实例学习流程，适用于临床挑战性的病理学任务。

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [190] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie 2是一个端到端自回归视频世界建模框架，通过三阶段训练实现可控性、长期视觉质量和时间一致性，支持长达5分钟的连续视频生成。


<details>
  <summary>Details</summary>
Motivation: 基于预训练视频生成系统构建视频世界模型是实现通用时空智能的重要但具有挑战性的步骤。世界模型需要具备可控性、长期视觉质量和时间一致性三个关键属性。

Method: 采用渐进式方法：1）多模态引导，整合稠密和稀疏控制信号；2）输入帧的退化感知训练，弥合训练与长期推理的差距；3）历史上下文引导，确保相邻片段的时间一致性。

Result: 在LongVGenBench基准测试中，LongVie 2在长程可控性、时间一致性和视觉保真度方面达到最先进性能，支持长达5分钟的连续视频生成。

Conclusion: LongVie 2标志着向统一视频世界建模迈出了重要一步，为构建具有可控性、高质量和一致性的视频世界模型提供了有效解决方案。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [191] [DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis](https://arxiv.org/abs/2512.13608)
*Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge*

Main category: cs.CV

TL;DR: 本文开发了首个针对数字乳腺断层合成（DBT）的基础模型DBT-DINO，并在乳腺密度分类、乳腺癌风险预测和病变检测三个临床任务上进行了评估。结果显示DBT-DINO在密度分类和风险预测任务上表现优异，但在病变检测任务上表现不一。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学影像领域显示出潜力，但针对三维成像模态（如数字乳腺断层合成DBT）的基础模型仍未被充分探索。目前尚无专门针对DBT的基础模型，尽管DBT被广泛用于乳腺癌筛查。

Method: 使用DINOv2方法在487,975个DBT体积的超过2500万张2D切片上进行自监督预训练，构建DBT-DINO模型。评估了三个下游任务：乳腺密度分类（5000例筛查）、5年乳腺癌风险预测（106,417例筛查）和病变检测（393个标注体积）。

Result: 乳腺密度分类：DBT-DINO准确率0.79，优于MetaAI DINOv2基线（0.73）和DenseNet-121（0.74）。5年风险预测：DBT-DINO AUROC为0.78，与DINOv2的0.76无显著差异。病变检测：DINOv2平均灵敏度0.67优于DBT-DINO的0.62，但DBT-DINO对癌性病变检测率更高（78.8% vs 77.3%）。

Conclusion: DBT-DINO是首个针对DBT的基础模型，在乳腺密度分类和癌症风险预测任务上表现强劲。但领域特定预训练在检测任务上显示出可变效益，表明局部检测任务需要进一步的方法开发。

Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.

</details>


### [192] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: Do-Undo任务和基准旨在解决视觉语言模型在理解和生成由真实世界动作驱动的物理合理场景转换方面的关键差距，要求模型模拟物理动作的结果并准确反转，反映真实世界中的因果关系。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要关注对象级编辑，缺乏对物理动作驱动的场景转换的理解和生成能力，这在具身AI、机器人技术和物理感知生成建模中至关重要。

Method: 从真实世界视频中策划大规模可逆动作数据集，并设计训练策略以强制一致性，实现鲁棒的动作定位。

Result: 实验表明，现有模型在物理可逆性方面表现不佳，突显了该任务的重要性。

Conclusion: Do-Undo为评估和推进多模态系统中的物理推理提供了一个直观的测试平台。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [193] [SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning](https://arxiv.org/abs/2512.13635)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Chongyu Qu,Juming Xiong,Siqi Lu,Zhengyi Lu,Yanfan Zhu,Marilyn Lionts,Yuechen Yang,Yalin Zheng,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: SCR2-ST是一个统一框架，利用单细胞先验知识指导空间转录组学数据的高效采集和准确表达预测，包含基于强化学习的主动采样和混合回归-检索预测网络。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学数据获取成本高昂，传统固定网格采样策略导致冗余测量，数据稀缺限制了现有方法。单细胞测序数据可作为有效的辅助来源来缓解这一限制。

Method: SCR2-ST整合了单细胞引导的强化学习主动采样(SCRL)和混合回归-检索预测网络(SCR2Net)。SCRL结合单细胞基础模型嵌入和空间密度信息构建生物学奖励信号，SCR2Net通过回归建模和检索增强推理的混合架构利用主动采样数据。

Result: 在三个公共ST数据集上的评估显示，SCR2-ST在采样效率和预测准确性方面均达到SOTA性能，特别是在低预算场景下表现优异。

Conclusion: SCR2-ST框架成功地将单细胞先验知识应用于空间转录组学数据采集和预测，为高成本ST技术提供了经济高效的解决方案。

Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST

</details>


### [194] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 本文提出了一个用于新视角合成的高质量数据集，包含动态场景、多模态注释和三种基准测试场景。


<details>
  <summary>Details</summary>
Motivation: 为训练和评估先进的4D场景重建和新视角生成模型提供高质量的基准数据集。

Method: 从高质量动画电影中生成数据集，包含RGB图像、深度、表面法线、物体分割和光流等多模态数据，并组织成密集多视角、稀疏相机和单目视频三种基准场景。

Result: 创建了一个视觉丰富、注释质量高、实验设置多样的独特数据集资源。

Conclusion: 该数据集为推进视角合成和3D视觉技术边界提供了宝贵的资源。

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [195] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

TL;DR: 该论文提出了Grab-3D框架，利用消失点作为3D几何模式表示，通过几何感知的Transformer检测AI生成视频，在几何一致性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法对生成视频中的3D几何模式探索有限，而扩散生成技术能产生高度逼真的视频，需要可靠的检测机制。

Method: 引入Grab-3D框架，包含几何位置编码、时空几何注意力和EMA几何分类器头，通过构建静态场景的AI生成视频数据集进行稳定3D几何特征提取。

Result: 实验表明Grab-3D显著优于最先进的检测器，对未知生成器具有强大的跨域泛化能力。

Conclusion: 基于3D几何时间一致性的几何感知方法能有效检测AI生成视频，为解决生成内容检测提供了新思路。

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [196] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

TL;DR: AgentIAD是一个基于工具驱动的智能代理框架，通过多阶段视觉检查解决工业异常检测中样本稀缺和缺陷细微的问题，在MMAD数据集上达到97.62%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临正常样本稀缺和缺陷细微局部化的挑战，单次视觉语言模型容易忽略小异常且缺乏与标准正常模式对比的机制。

Method: 提出AgentIAD框架，配备感知缩放器进行局部细粒度分析，比较检索器在证据模糊时查询正常样本；通过结构化感知和比较轨迹进行两阶段训练：监督微调+强化学习，采用感知奖励和行为奖励的双重奖励设计。

Result: 在MMAD数据集上达到97.62%的分类准确率，超越现有MLLM方法，同时产生透明可解释的检查轨迹。

Conclusion: AgentIAD通过多阶段观察、缩放和验证机制，实现了更精确的工业异常检测，为实际工业应用提供了可靠解决方案。

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [197] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

TL;DR: 本文提出了Interactive Intelligence（交互智能）新范式，通过Mio框架实现个性表达、自适应交互和自我进化的数字人，建立了新基准并展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前数字人主要停留在表面模仿阶段，缺乏真正的智能交互能力。本文旨在推动数字人从浅层模仿向智能交互发展。

Method: 提出Mio端到端框架，包含Thinker、Talker、Face Animator、Body Animator和Renderer五个专门模块，将认知推理与实时多模态体现相结合。

Result: 大量实验表明，该框架在所有评估维度上都优于现有最先进方法，实现了流畅一致的交互体验。

Conclusion: 该研究为数字人技术带来了突破性进展，使其从简单的表面模仿转向真正的智能交互。

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [198] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

TL;DR: JoVA是一个统一的视频-音频生成框架，通过联合自注意力机制实现跨模态交互，无需额外对齐模块，并引入基于面部关键点检测的嘴部区域损失来提升唇语同步质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法的两大局限：1) 只能生成环境音，缺乏唇语同步的人类语音生成能力；2) 现有统一方法依赖显式融合或模态特定对齐模块，增加了架构复杂性。

Method: 在Transformer层中使用视频和音频token的联合自注意力实现直接跨模态交互；引入基于面部关键点检测的嘴部区域损失来增强唇语同步监督。

Result: 在多个基准测试中，JoVA在唇语同步准确性、语音质量和整体视频-音频生成保真度方面优于或与现有最先进方法相当。

Conclusion: JoVA是一个优雅的高质量多模态生成框架，通过简洁的架构设计实现了优秀的视频-音频生成性能。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [199] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Steer3D是一个前馈方法，为图像到3D模型添加文本可操控性，使生成的3D资产能够通过语言进行编辑。该方法基于ControlNet思想，通过两阶段训练流程实现文本引导的3D生成，比竞争方法更快且更忠实于语言指令。


<details>
  <summary>Details</summary>
Motivation: 当前图像到3D技术虽然取得了进展，但要将AI生成的3D资产应用于实际场景，需要具备易于编辑的能力。本文旨在解决如何为预训练的图像到3D生成模型添加文本操控能力的问题。

Method: 受ControlNet启发，将文本操控性引入图像到3D生成。构建了可扩展的数据引擎进行自动数据生成，采用基于流匹配训练和直接偏好优化(DPO)的两阶段训练流程。

Result: 与竞争方法相比，Steer3D更忠实地遵循语言指令，与原始3D资产保持更好的一致性，同时速度提升2.4倍到28.5倍。证明仅需10万数据即可为预训练模型添加新的文本模态操控能力。

Conclusion: Steer3D展示了为预训练图像到3D生成模型添加文本操控能力的可行性，为3D资产的便捷编辑提供了有效解决方案，在保真度和效率方面均优于现有方法。

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [200] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

TL;DR: LASER是一个无需训练的训练框架，可将离线重建模型转换为流式系统，通过层尺度对齐解决连续时间窗口间的深度尺度不一致问题，实现高效流式视频处理。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈重建模型（如VGGT和π³）虽然重建质量高，但由于二次内存复杂度无法处理流式视频，而现有流式方法需要大量重新训练且无法充分利用离线模型的几何先验。

Method: 提出层尺度对齐方法：将深度预测分割为离散层，计算每层尺度因子，并在相邻窗口和时间戳间传播，解决简单相似变换对齐因单目尺度模糊导致的层深度错位问题。

Result: 实验表明LASER在相机姿态估计和点云重建质量上达到最先进水平，在RTX A6000 GPU上以14 FPS和6 GB峰值内存运行，支持千米级流式视频处理。

Conclusion: LASER框架成功将离线重建模型转换为实用的流式系统，无需重新训练即可保持高质量重建，解决了现有方法的内存和训练效率问题。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [201] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

TL;DR: 该论文提出了一种新方法，通过重新编程预训练的3D实例生成器来学习场景级知识，从而解决交互式3D场景生成的泛化问题。该方法使用模型中心的空间监督替代数据集监督，能够在未见过的布局和新物体组合上实现泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的3D场景生成方法在有限场景数据集上进行空间理解训练，导致对新布局的泛化能力受限。论文旨在突破这一限制，利用预训练生成器的可转移空间知识来实现更好的泛化性能。

Method: 采用视图中心的场景空间表示替代传统的规范空间，将预训练的3D实例生成器重新编程为场景级学习器。通过模型中心的空间监督，直接从实例模型中学习空间关系，无需依赖数据集监督。

Result: 定量和定性结果表明，3D实例生成器具有隐含的空间学习和推理能力。即使在训练场景由随机组合物体构成的情况下，空间推理能力仍然能够涌现。

Conclusion: 该方法证明了预训练3D实例生成器可以作为基础模型，为交互式3D场景理解和生成提供可转移的空间先验知识，开辟了新的研究方向。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [202] [Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684)
*Daniel Zoran,Nikhil Parthasarathy,Yi Yang,Drew A Hudson,Joao Carreira,Andrew Zisserman*

Main category: cs.CV

TL;DR: RVM是一种新颖的视频表示学习方法，使用基于transformer的循环神经网络来聚合时间上的密集图像特征，有效捕捉自然视频数据的时空结构。该方法通过非对称掩码预测任务学习，仅需标准像素重建目标，实现了高效且通用的编码器。


<details>
  <summary>Details</summary>
Motivation: 现有的视频表示学习方法在参数效率和计算成本方面存在局限性，特别是在小模型架构下表现不佳。RVM旨在开发一种更高效的视频表示学习框架，能够在保持竞争力的同时显著提高参数效率。

Method: 使用基于transformer的循环神经网络架构，通过非对称掩码预测任务进行学习，仅需要标准像素重建目标。该设计支持长时间稳定特征传播，计算成本呈线性增长。

Result: RVM在视频级任务（如动作识别、点/物体跟踪）上达到与最先进视频模型（VideoMAE、V-JEPA）相当的竞争性性能，同时在测试几何和密集空间理解的任务上优于图像模型（如DINOv2）。在小模型架构下表现出色，参数效率比竞争视频掩码自编码器高30倍。

Conclusion: RVM通过循环神经网络架构有效解决了标准时空注意力架构的局限性，能够以线性计算成本稳定传播特征，学习到丰富的场景语义、结构和运动表示，为视频表示学习提供了高效且通用的解决方案。

Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

</details>


### [203] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出VTP视觉分词器预训练框架，通过联合优化对比学习、自监督和重建损失，解决了传统基于重建的视觉分词器偏向低层信息的问题，实现了更好的生成质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统视觉分词器（如VAE）基于重建的训练范式导致潜在空间偏向低层信息，即使像素级精度提高也无法改善生成质量，存在"预训练缩放问题"。

Method: 提出VTP统一框架，联合优化图像-文本对比损失、自监督损失和重建损失，强调潜在空间需要简洁表示高层语义。

Result: VTP在零样本准确率达到78.2%，rFID为0.36，生成收敛速度比先进蒸馏方法快4.1倍，且具有更好的缩放性：仅增加预训练计算量就能实现65.8%的FID改进。

Conclusion: 理解是生成的关键驱动力，VTP框架展示了视觉分词器预训练的有效缩放特性，为生成模型提供了更优质的潜在空间表示。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [204] [LitePT: Lighter Yet Stronger Point Transformer](https://arxiv.org/abs/2512.13689)
*Yuanwen Yue,Damien Robert,Jianyuan Wang,Sunghwan Hong,Jan Dirk Wegner,Christian Rupprecht,Konrad Schindler*

Main category: cs.CV

TL;DR: LitePT模型通过早期使用卷积提取低层几何特征，后期使用注意力机制捕获高层语义，结合新的3D位置编码PointROPE，实现了比Point Transformer V3更轻量、更高效且性能相当的3D点云处理网络。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云网络结合卷积和注意力机制，但最佳组合方式不明确。研究发现卷积适合早期高分辨率提取低层几何特征，而注意力在深层低分辨率下更高效地捕获高层语义和上下文信息。

Method: 提出LitePT模型：早期阶段使用卷积，深层阶段切换到注意力机制。为避免丢弃冗余卷积层时丢失空间布局信息，引入无需训练的3D位置编码PointROPE。

Result: LitePT参数量减少3.6倍，运行速度提升2倍，内存使用减少2倍，在多个任务和数据集上性能匹配甚至优于最先进的Point Transformer V3。

Conclusion: 基于卷积和注意力机制在不同层级的最优分工原则设计的LitePT模型，证明了在保持性能的同时显著提升效率的可行性，为3D点云网络设计提供了新思路。

Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.

</details>


### [205] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

TL;DR: DiffusionBrowser是一个模型无关的轻量级解码器框架，可在去噪过程中任意时间步或变换块生成交互式预览，支持RGB和场景内在信息的多模态表示，速度超过4倍实时，并能通过随机性重注入和模态引导实现生成控制。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型虽然革新了视频生成，但存在生成不精确、速度慢且生成过程不透明的问题，用户需要长时间等待且无法干预生成过程。

Method: 提出DiffusionBrowser框架，训练轻量级解码器在去噪过程的任意中间点生成预览，支持多模态表示（RGB和场景内在信息），并通过随机性重注入和模态引导实现交互式控制。

Result: 模型能以超过4倍实时速度生成预览（4秒视频小于1秒），预览与最终视频具有一致的外观和运动，同时能够揭示去噪过程中场景、物体等细节的组成和组装过程。

Conclusion: DiffusionBrowser解决了视频扩散模型的透明度和交互性问题，为生成过程提供了新的控制能力，并揭示了原本黑盒去噪过程的内在机制。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [206] [Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights](https://arxiv.org/abs/2512.11802)
*Zheng Li,Peng Zhang,Shixiao Liang,Hang Zhou,Chengyuan Ma,Handong Yao,Qianwen Li,Xiaopeng Li*

Main category: cs.RO

TL;DR: 本文研究了特斯拉交通灯和停车标志控制系统（TLSSC）与交通控制设备（TCD）的交互行为，通过实验数据分析了停止、加速和跟驰三种行为模式，并校准了全速度差模型（FVDM）来量化表征这些行为。


<details>
  <summary>Details</summary>
Motivation: 高级驾驶辅助系统（ADAS）与交通控制设备的交互对交通运行有重要影响，但这一交互行为缺乏系统的实证研究。

Method: 设计并执行了不同速度限制和TCD类型下的实验，收集同步的高分辨率车辆轨迹数据和驾驶员视角视频，开发了TLSSC-TCD交互行为分类法，并校准FVDM模型来量化行为模式。

Result: 识别出跟驰阈值（约90米），停止行为对期望速度偏差和相对速度响应强烈，加速行为较为保守，交叉口跟驰行为比标准跟驰行为更平滑且车头间距更小。

Conclusion: 建立的数据集、行为定义和模型表征为未来ADAS-TCD交互逻辑的仿真、安全评估和设计提供了基础。

Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.

</details>


### [207] [ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision](https://arxiv.org/abs/2512.11824)
*Rosh Ho,Jian Zhang*

Main category: cs.RO

TL;DR: ReGlove系统将低成本商业气动康复手套改造为视觉引导的辅助矫形器，通过手腕摄像头和边缘计算实现无需可靠肌肉信号的上下文感知抓取，成本低于250美元。


<details>
  <summary>Details</summary>
Motivation: 解决慢性上肢障碍患者面临的辅助技术昂贵或依赖不可靠生物信号的问题，为被传统EMG设备排除在外的群体提供可访问的解决方案。

Method: 集成手腕安装摄像头与边缘计算推理引擎（Raspberry Pi 5），采用实时YOLO计算机视觉模型实现上下文感知抓取。

Result: 达到96.73%的抓取分类准确率，端到端延迟低于40毫秒，在YCB物体操作任务中成功率为82.71%，并在27项日常生活活动任务中表现可靠。

Conclusion: ReGlove为基于视觉的上肢辅助提供了技术基础，具有成本效益和可访问性优势，能够惠及传统EMG控制设备无法覆盖的人群。

Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.

</details>


### [208] [WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2512.11872)
*Mingwang Xu,Jiahao Cui,Feipeng Cai,Hanlin Shang,Zhihao Zhu,Shan Luan,Yifang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

TL;DR: WAM-Diff是一个基于视觉-语言-动作的自动驾驶框架，采用掩码扩散方法迭代生成离散轨迹序列，在NAVSIM基准上取得了优异性能


<details>
  <summary>Details</summary>
Motivation: 探索离散掩码扩散在自动驾驶轨迹生成中的潜力，作为自回归和连续扩散策略的替代方案

Method: 提出三个关键创新：系统化适配掩码扩散支持灵活解码顺序；基于稀疏MoE架构的可扩展模型；使用GSPO进行在线强化学习优化序列级驾驶奖励

Result: 在NAVSIM-v1上达到91.0 PDMS，在NAVSIM-v2上达到89.7 EPDMS，证明了掩码扩散在自动驾驶中的有效性

Conclusion: 该方法为轨迹生成提供了有前景的替代方案，支持场景感知的解码策略

Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff

</details>


### [209] [Audio-Based Tactile Human-Robot Interaction Recognition](https://arxiv.org/abs/2512.11873)
*Antonia Yepes,Marie Charbonneau*

Main category: cs.RO

TL;DR: 该研究探索使用机器人身体上的麦克风通过触摸机器人外壳产生的声音来检测触觉交互，作为传统关节扭矩传感器或6轴力/扭矩传感器的替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统触觉检测方法依赖昂贵的专用传感器，而基于声音的方法成本更低且易于部署，为机器人触觉感知提供了一种创新解决方案。

Method: 在机器人躯干上安装两个MEMS麦克风，采集6种不同触摸类型（敲击、轻敲、摩擦、抚摸、刮擦和按压）的声音信号，使用卷积神经网络对336个预处理样本进行分类。

Result: 模型在具有明显声学主导频率的不同触摸类型之间表现出高分类准确率，验证了基于声音的触觉检测方法的可行性。

Conclusion: 基于麦克风的触觉检测方法是一种有前景的低成本替代方案，特别适用于具有明显声学特征的触摸类型识别。

Abstract: This study explores the use of microphones placed on a robot's body to detect tactile interactions via sounds produced when the hard shell of the robot is touched. This approach is proposed as an alternative to traditional methods using joint torque sensors or 6-axis force/torque sensors. Two Adafruit I2S MEMS microphones integrated with a Raspberry Pi 4 were positioned on the torso of a Pollen Robotics Reachy robot to capture audio signals from various touch types on the robot arms (tapping, knocking, rubbing, stroking, scratching, and pressing). A convolutional neural network was trained for touch classification on a dataset of 336 pre-processed samples (48 samples per touch type). The model shows high classification accuracy between touch types with distinct acoustic dominant frequencies.

</details>


### [210] [Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)](https://arxiv.org/abs/2512.11876)
*Hrigved Mahesh Suryawanshi*

Main category: cs.RO

TL;DR: 该论文提出了一个基于地形可通行性感知的导航框架，通过LiDAR生成2.5D高程图，使用CNN模型评估地形可通行性，并结合自定义A*路径规划器实现能效优化的自主导航。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需要实时评估地形难度并规划平衡效率与安全性的路径。现有的导航方法往往忽视地形可通行性对能耗和稳定性的影响。

Method: 使用FAST-LIO进行实时定位，从LiDAR点云生成2.5D高程图。基于CNN的模型处理高程图估计可通行性分数，转换为导航成本。自定义A*规划器结合几何距离和能耗成本，寻找能效最优路径。

Result: 实验结果表明，系统成功避开低可通行性区域，通过接受少量路径长度增加实现了地形成本显著降低。LiDAR-based映射达到厘米级精度，优于相机-based方法。

Conclusion: 该工作为多模态机器人平台建立了智能地形感知导航的基础框架，证明了地形可通行性感知在自主导航中的重要性。

Abstract: Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analy- sis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real- time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR- based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhib- ited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.

</details>


### [211] [Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control](https://arxiv.org/abs/2512.11886)
*Mohammed Irfan Ali*

Main category: cs.RO

TL;DR: 该论文开发了一套完整的蛇形机器人自主导航系统，结合视觉-惯性SLAM、降阶状态估计和闭环轨迹跟踪，实现了在无外部跟踪基础设施环境中的自主路径点导航。


<details>
  <summary>Details</summary>
Motivation: 蛇形机器人在极端地形中具有卓越的机动性，但其高度铰接的身体结构在缺乏外部跟踪基础设施的环境中面临自主导航挑战。之前的研究完全依赖开环遥操作，需要开发完整的自主导航系统。

Method: 集成机载视觉-惯性SLAM、降阶状态估计和闭环轨迹跟踪。使用深度相机和边缘计算进行实时定位，通过降阶框架估计质心位姿，驱动闭环控制器通过距离相关偏航误差混合来调制CPG步态参数。

Result: 物理实验验证了完整系统的有效性，展示了准确的多路径点跟踪能力，为蛇形机器人自主导航奠定了基础。

Conclusion: 该系统成功实现了蛇形机器人的自主导航，解决了在无外部跟踪环境中的定位和运动控制问题，为行星探索等应用提供了技术基础。

Abstract: Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.

</details>


### [212] [VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer](https://arxiv.org/abs/2512.11891)
*Songqiao Hu,Zeyi Liu,Shuang Liu,Jun Cen,Zihan Meng,Xiao He*

Main category: cs.RO

TL;DR: 本文提出了一种名为AEGIS的Vision-Language-Safe Action（VLSA）架构，通过在现有VLA模型中集成基于控制屏障函数的安全约束层，在保持指令跟随性能的同时提高机器人操作的安全性。


<details>
  <summary>Details</summary>
Motivation: 当前Vision-Language-Action（VLA）模型在非结构化环境中部署面临挑战，需要同时确保任务合规性和安全性，特别是在防止物理交互中的潜在碰撞方面。

Method: AEGIS架构包含一个即插即用的安全约束层，该层通过控制屏障函数构建，可直接与现有VLA模型集成，提供理论保证的安全性。

Result: 实验表明AEGIS在避障率上提高了59.16%，任务执行成功率提高了17.25%，构建了安全关键基准SafeLIBERO进行评估。

Conclusion: 该方法在保持VLA模型原有性能的同时显著提升了安全性，为机器人安全操作提供了有效解决方案，相关代码和数据集已开源。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.

</details>


### [213] [Data-driven Interpretable Hybrid Robot Dynamics](https://arxiv.org/abs/2512.11900)
*Christopher E. Mower,Rui Zong,Haitham Bou-Ammar*

Main category: cs.RO

TL;DR: 该论文提出使用符号回归和稀疏非线性动力学识别(SINDy)方法，从关节空间数据中学习可解释的混合机器人动力学模型，其中解析刚体动力学模型与学习的残余扭矩项互补。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发可解释的机器人动力学模型，替代黑盒函数逼近器，提供更紧凑、准确且具有物理意义的扭矩预测方法。

Method: 方法包括使用符号回归和SINDy技术，从关节空间数据中恢复残余扭矩的紧凑闭式表达式，构建分析刚体动力学模型与学习残余项相结合的混合模型。

Result: 在7-DoF Franka臂仿真中，模型准确恢复惯性、科里奥利、重力和粘性效应，相对误差极小，在准确性和泛化性上均优于神经网络基线。在真实7-DoF WAM臂数据上，符号回归残余比SINDy和神经网络泛化能力更强，且提出了扩展名义动力学模型的新闭式公式候选。

Conclusion: 可解释的残余动力学模型为扭矩预测提供了比黑盒函数逼近器更紧凑、准确且具有物理意义的替代方案。

Abstract: We study data-driven identification of interpretable hybrid robot dynamics, where an analytical rigid-body dynamics model is complemented by a learned residual torque term. Using symbolic regression and sparse identification of nonlinear dynamics (SINDy), we recover compact closed-form expressions for this residual from joint-space data. In simulation on a 7-DoF Franka arm with known dynamics, these interpretable models accurately recover inertial, Coriolis, gravity, and viscous effects with very small relative error and outperform neural-network baselines in both accuracy and generalization. On real data from a 7-DoF WAM arm, symbolic-regression residuals generalize substantially better than SINDy and neural networks, which tend to overfit, and suggest candidate new closed-form formulations that extend the nominal dynamics model for this robot. Overall, the results indicate that interpretable residual dynamics models provide compact, accurate, and physically meaningful alternatives to black-box function approximators for torque prediction.

</details>


### [214] [Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics](https://arxiv.org/abs/2512.11903)
*Iacopo Catalano,Eduardo Montijano,Javier Civera,Julio A. Placed,Jorge Pena-Queralta*

Main category: cs.RO

TL;DR: Aion框架将时间流动态嵌入到分层3D场景图中，结合了语义结构和运动模式，以提升动态环境中的导航规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有的3D场景图主要关注静态语义结构，而动态地图通常局限于网格表示且缺乏语义信息。需要一种能够同时捕捉语义结构和时间演化的空间表示方法。

Method: Aion采用基于图的稀疏动态地图表示，在场景图的导航节点上附加运动流信息，捕获任意时间间隔内的运动模式。

Result: 该方法产生了更可解释和可扩展的预测，改善了复杂动态环境中的规划和交互能力。

Conclusion: Aion成功地将时间维度整合到分层3D场景图中，为动态环境中的自主导航提供了更有效的空间表示框架。

Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.

</details>


### [215] [Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models](https://arxiv.org/abs/2512.11908)
*Heng Zhang,Rui Dai,Gokhan Solak,Pokuang Zhou,Yu She,Arash Ajoudani*

Main category: cs.RO

TL;DR: 该论文综述了用于机器人接触密集型任务的安全学习型方法，将其分为安全探索和安全执行两大领域，重点探讨了这些方法如何与新兴的机器人基础模型（特别是VLM/VLA）结合，并分析了安全机遇与挑战。


<details>
  <summary>Details</summary>
Motivation: 接触密集型任务存在不确定性、复杂动力学和高风险等挑战，现有学习型控制方法在确保安全方面存在瓶颈，需要系统梳理安全学习型方法以促进可靠部署。

Method: 采用综述研究方法，将现有方法分为安全探索和安全执行两大类别，详细分析约束强化学习、风险敏感优化、不确定性建模、控制屏障函数等关键技术，并重点探讨与VLM/VLA基础模型的结合。

Result: 系统梳理了安全学习型方法在接触密集型任务中的应用现状，揭示了VLM/VLA方法带来的新安全机遇（如语言级约束规范）和风险挑战，为安全部署提供了理论指导。

Conclusion: 论文指出了当前方法的局限性，并展望了未来发展方向，旨在推动可靠、安全对齐且基于基础模型的机器人在复杂接触环境中的部署。

Abstract: Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.

</details>


### [216] [Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control](https://arxiv.org/abs/2512.11921)
*Abdullah Yahya Abdullah Omaisan,Ibrahim Sheikh Mohamed*

Main category: cs.RO

TL;DR: 本文提出了一种高效的微调方法，使大规模视觉-语言-动作模型能够在低成本机器人平台上部署，通过LoRA和量化技术实现在消费级GPU上的运行。


<details>
  <summary>Details</summary>
Motivation: 解决大规模VLA模型在低成本机器人平台上部署的挑战，包括计算资源限制和对新机器人形态的适应问题。

Method: 使用低秩适应（LoRA）和量化技术进行资源高效的微调，支持31亿参数模型在8GB显存的消费级GPU上运行，重点研究冻结与未冻结视觉编码器的权衡。

Result: 在SO101机械臂的按钮按压任务中实现了有效的操作性能，基于200个演示片段进行训练，分析了训练数据量与真实世界性能的关系。

Conclusion: 通过适当的微调方法，VLA模型可以成功部署在低成本机器人平台上，使先进的操控能力超越昂贵的研究机器人而普及。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.

</details>


### [217] [A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach](https://arxiv.org/abs/2512.11944)
*Jia Hu,Yang Chang,Haoran Wang*

Main category: cs.RO

TL;DR: 本文提出了一种数据驱动的最优控制范式，旨在解决自动驾驶运动规划中传统管道方法的脆弱性与现代学习方法的黑盒特性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶运动规划领域长期面临一个根本性权衡：管道方法透明但脆弱，而基于学习的方法自适应但不透明。这种困境阻碍了真正可信赖系统的发展。

Method: 通过综合评述基于学习的运动规划方法，提出数据驱动最优控制范式，将经典控制的可验证结构与机器学习的自适应能力相结合，利用真实世界数据持续优化系统动力学、成本函数和安全约束等关键组件。

Result: 该框架有望实现三个关键能力："以人为本"的定制化、"平台自适应"的动态适应以及通过自调优实现的"系统自优化"。

Conclusion: 基于这一范式，提出了未来研究方向，旨在开发同时具备安全性、可解释性和类人自主能力的智能交通系统。

Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.

</details>


### [218] [Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking](https://arxiv.org/abs/2512.12021)
*Xincheng Cao,Haochong Chen,Bilin Aksun-Guvenc,Levent Guvenc*

Main category: cs.RO

TL;DR: 提出了一种改进的Hybrid-A*路径规划算法，用于解决车辆在狭小空间中的停车问题，该算法结合了运动可行性和静态障碍物避障功能。


<details>
  <summary>Details</summary>
Motivation: 由于可行路径稀缺且需要避免碰撞，在狭小空间中停车是一项具有挑战性的任务。

Method: 使用改进的Hybrid-A*路径规划算法，结合单轨运动学模型生成可行运动基元，并通过车辆中心线重建和膨胀二值占据图实现静态障碍物避障。

Result: 仿真研究表明，该算法能够持续生成运动学可行且无碰撞的轨迹。

Conclusion: 所提出的方法有效解决了狭小空间停车问题，保证了路径的可行性和安全性。

Abstract: Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.

</details>


### [219] [A Stochastic Approach to Terrain Maps for Safe Lunar Landing](https://arxiv.org/abs/2512.12058)
*Anja Sheppard,Chris Reale,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 本文提出了一种基于高斯过程的两阶段模型，利用LRO数据生成月球南极地区的随机高程地图，通过考虑DEM置信度数据来更准确地建模异方差传感器噪声，为安全着陆提供更可靠的地形不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 月球南极地区存在阴影和潜在资源，传统视觉方法不可靠，LiDAR技术未经充分测试。LRO提供了丰富的月球表面数据，但现有方法未考虑DEM置信度地图中的关键信息。

Method: 采用两阶段高斯过程模型：次级GP从DEM置信度数据学习空间变化的噪声特征，主GP利用这些信息建模月球地形。使用随机变分GPs实现可扩展训练。

Result: 该方法能够更准确地建模异方差传感器噪声对高程地图的影响，生成更具信息量的地形不确定性估计。

Conclusion: 所提出的方法为下游任务（如危险检测和安全着陆点选择）提供了更可靠的地形不确定性信息，有助于提高月球南极着陆的安全性。

Abstract: Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.
  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.
  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.

</details>


### [220] [B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping](https://arxiv.org/abs/2512.12194)
*Min-Won Seo,Aamodh Suresh,Carlos Nieto-Granda,Solmaz S. Kia*

Main category: cs.RO

TL;DR: 提出B-ActiveSEAL框架，解决机器人主动探索中定位与建图耦合不确定性的计算难题，通过信息论方法实现可扩展的探索-利用平衡。


<details>
  <summary>Details</summary>
Motivation: 大规模环境中长期运行时，定位和建图的耦合不确定性管理计算复杂度高且难以处理，需要新的决策框架。

Method: 基于信息论的主动探索框架，使用行为熵作为信息度量，自适应平衡地图不确定性（探索）和定位不确定性（利用），支持广义熵度量。

Result: 在开源地图和ROS-Unity仿真中验证，B-ActiveSEAL实现了良好的探索-利用平衡，产生多样化自适应探索行为，优于基线方法。

Conclusion: 该框架为耦合不确定性的传播和集成提供了理论基础，在复杂环境中展现出显著优势，实现了不确定性感知的主动探索。

Abstract: Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.

</details>


### [221] [Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion](https://arxiv.org/abs/2512.12203)
*Eric J. Elias,Michael Esswein,Jonathan P. How,David W. Miller*

Main category: cs.RO

TL;DR: 本文研究了在轨操作中，通过融合可见光和热红外图像来提升SLAM导航性能的方法，解决了单一传感器在复杂光照条件下的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着在轨操作需求的增长，需要精确导航未知空间物体。传统相机在阴影期性能受限，激光雷达则体积大、功耗高。热红外相机能在困难光照条件下工作，但分辨率较低。

Method: 通过像素级融合方法创建可见光/热红外复合图像，利用两种传感器的优势。在低地球轨道上对目标卫星进行真实感模拟，比较单目SLAM算法在不同光照和轨迹下的导航误差。

Result: 融合图像相比单一可见光或热红外方法，导航性能显著提升。

Conclusion: 可见光与热红外图像的融合能够有效克服单一传感器的局限性，为在轨操作提供更可靠的导航解决方案。

Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.

</details>


### [222] [Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving](https://arxiv.org/abs/2512.12211)
*Longchao Da,David Isele,Hua Wei,Manish Saroya*

Main category: cs.RO

TL;DR: 本文提出了一种新的轨迹预测器评估方法，通过准确性和多样性两个维度动态评估预测器性能，相比传统误差指标更能反映预测器对自动驾驶车辆实际驾驶性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测评估方法主要依赖误差指标（如ADE、FDE），这些指标仅从后验角度衡量准确性，忽略了预测器对自动驾驶车辆在复杂交互场景中的实际影响。高质量的预测器不仅要准确，还应捕捉邻域智能体所有可能的运动方向，以支持谨慎的决策制定。

Method: 提出一个综合性评估流程，通过两个维度自适应评估预测器性能：准确性和多样性。根据驾驶场景的关键性，动态结合这两个维度生成最终评分。

Result: 在基于真实世界数据集的闭环基准测试上进行广泛实验，结果表明该方法比传统指标产生更合理的评估结果，更好地反映了预测器评估与自动驾驶车辆驾驶性能之间的相关性。

Conclusion: 该评估流程提供了一种稳健的方法来选择对自动驾驶车辆驾驶性能贡献最大的预测器，为轨迹预测器的评估提供了更全面的视角。

Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.

</details>


### [223] [Semantic Zone based 3D Map Management for Mobile Robot](https://arxiv.org/abs/2512.12228)
*Huichang Yun,Seungho Yoo*

Main category: cs.RO

TL;DR: 提出了一种基于语义区域的3D地图管理方法，将内存管理从几何中心转向语义中心，通过动态加载任务相关区域到工作内存，卸载非活动区域到长期内存，有效控制内存使用。


<details>
  <summary>Details</summary>
Motivation: 大规模室内环境中，3D地图占用大量内存，现有SLAM框架基于几何距离或时间指标的内存管理方法在空间分隔环境中数据检索效率低下。

Method: 将环境划分为有意义的空间单元（如大厅、走廊），以这些语义区域作为内存管理的基本单位，在RTAB-Map框架中实现动态内存管理。

Result: 相比标准方法，显著减少了不必要的签名加载/卸载周期和累积内存使用，确保稳定可预测的内存使用同时保持地图可用于导航。

Conclusion: 基于语义区域的管理方法能够有效解决大规模室内环境中的内存管理问题，提高系统效率。

Abstract: Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment

</details>


### [224] [Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy](https://arxiv.org/abs/2512.12230)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 本文提出了一种统一的深度强化学习策略，能够使七种不同形态的人形机器人实现零样本跌倒恢复，无需针对特定机器人进行训练。


<details>
  <summary>Details</summary>
Motivation: 在动态环境（如RoboCup）中，跌倒恢复是人形机器人的关键技能，现有方法需要为每种机器人形态训练单独策略，效率低下。

Method: 使用CrossQ训练单一DRL策略，覆盖7种不同高度（0.48-0.81m）、重量（2.8-7.9kg）和动力学特性的人形机器人。

Result: 统一策略在未见过的机器人形态上实现零样本转移，成功率高达86±7%（95%置信区间[81, 89]），在某些情况下甚至超过专用策略。

Conclusion: 研究表明形态无关控制在跌倒恢复中的实用性，为通用人形机器人控制奠定基础。

Abstract: Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup

</details>


### [225] [Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements](https://arxiv.org/abs/2512.12233)
*Murad Mehrab Abrar,Trevor W. Harrison*

Main category: cs.RO

TL;DR: 提出了一种低成本、鲁棒的水下定位方法，通过双向声学飞行时间和几何成本滤波技术，在沿海水域实现高精度微浮标定位。


<details>
  <summary>Details</summary>
Motivation: 解决低成本自主平台在需要高频位置更新的水下环境中精确定位的挑战。

Method: 采用双向声学飞行时间定位框架，结合非线性三边测量和基于几何成本与CRLB的滤波技术，去除多径效应等误差。

Result: 在实地测试中实现中位定位误差低于4米，平均误差从139.29米降至12.07米，轨迹与GPS路径对齐度显著提升。

Conclusion: 通过优化算法设计，提高了基于距离的声学定位技术的实用性和鲁棒性，适用于各种硬件平台。

Abstract: Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.

</details>


### [226] [CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement](https://arxiv.org/abs/2512.12243)
*HT To,S Nguyen,NH Pham*

Main category: cs.RO

TL;DR: CAR-CHASE提出了一种针对类车机器人多智能体路径规划的冲突感知启发式缓存和自适应混合启发式方法，显著提升了计算效率


<details>
  <summary>Details</summary>
Motivation: 传统启发式缓存假设启发式函数仅依赖于状态，但在CBS中冲突解决引入的约束使得搜索空间具有上下文依赖性，导致计算成本高昂

Method: 结合冲突感知启发式缓存（基于状态和相关约束上下文）和自适应混合启发式（智能切换快速近似和精确计算），包括冲突指纹、相关性过滤器和自适应切换策略

Result: 在480个基准实例上实现几何平均速度提升2.46倍，成功率从77.9%提升至84.8%，总运行时间减少70.1%，解决了33个之前超时的实例

Conclusion: 该技术具有通用性，可应用于其他CBS变体，性能提升随问题复杂度增加而增强，在30智能体障碍场景下速度提升可达4.06倍

Abstract: Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\% and 50\%) demonstrates a geometric mean speedup of 2.46$\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\% to 84.8\% (+6.9 percentage points), reduce total runtime by 70.1\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.

</details>


### [227] [Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy](https://arxiv.org/abs/2512.12320)
*Canqi Meng,Weibang Bai*

Main category: cs.RO

TL;DR: 提出了一种通过在多孔泡沫体上切割特定图案来实现可编程变形的软多孔致动器设计方法，该方法利用切口图案引导材料在全局真空输入下的变形，实现了弯曲、倾斜和扭转等多种运动模式。


<details>
  <summary>Details</summary>
Motivation: 传统软气动致动器结构支撑性差且需要针对不同功能进行昂贵的几何重新设计，而现有填充多孔材料的方法在实现可编程变形方面研究不足。

Method: 在多孔泡沫体上切割特定图案（横向、纵向、对角线）以引入局部结构各向异性，结合有限元分析建模，通过全局真空输入控制变形。

Result: 实验表明该方法可实现80°弯曲（N=2）、18°倾斜（N=1）和115°扭转（N=8），并展示了图案可转移性、可扩展性和无模具快速原型制作能力。

Conclusion: 该工作为多功能软多孔机器人的设计提供了一种新颖、高效且可扩展的范式，通过将人手褶皱图转化为功能切口图案，成功制造出具有类人自适应抓取能力的仿生软机器人手。

Abstract: Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\circ}$ (N=2), tilting of $18^{\circ}$ (N=1), and twisting of $115^{\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.

</details>


### [228] [INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset](https://arxiv.org/abs/2512.12377)
*Haichuan Li,Changda Tian,Panos Trahanias,Tomi Westerlund*

Main category: cs.RO

TL;DR: INDOOR-LIDAR是一个混合室内3D LiDAR点云数据集，结合仿真和真实世界数据，解决现有数据集规模小、标注不一致等问题，为机器人感知研究提供可扩展基准。


<details>
  <summary>Details</summary>
Motivation: 现有室内LiDAR数据集存在规模有限、标注格式不一致、数据采集人为差异大等问题，限制了机器人感知研究的进展。

Method: 通过集成仿真环境和自主地面机器人采集的真实扫描数据，提供一致覆盖和真实传感器行为。数据集包含密集点云数据、强度测量和KITTI风格标注，涵盖多种室内场景和物体类别。

Result: 创建了一个包含仿真和真实世界数据的混合数据集，仿真部分支持灵活配置布局、点密度和遮挡，真实部分捕捉真实传感器噪声和室内环境特征。

Conclusion: INDOOR-LIDAR通过弥合合成数据与真实世界数据之间的差距，为复杂室内环境中的机器人感知研究建立了可扩展、真实且可复现的基准。

Abstract: We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.

</details>


### [229] [Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models](https://arxiv.org/abs/2512.12427)
*Rudolf Reiter,Chao Qin,Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: Unique提出了一种统一MPC框架，通过级联不同精度的模型在单一优化中实现短时高精度控制和长时规划，在同等计算预算下将闭环性能提升高达75%。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼任务中同时需要瞬时反应性和长时规划的矛盾，高精度模型计算慢，低精度模型则影响闭环性能。

Method: 在单一优化中结合短时高精度模型和长时低精度模型，通过成本对齐、可行性约束和过渡约束实现模型间协调，采用3D渐进平滑和并行随机初始化MPC求解器避免局部极小值。

Result: 在仿真和实际飞行中，相比标准MPC和分层规划器，在同等计算预算下位置或速度跟踪性能提升最高达75%。

Conclusion: Unique框架通过模型级联和优化技术有效平衡了控制精度和规划效率，在各种场景下均表现出稳健的性能提升。

Abstract: Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.

</details>


### [230] [Sim2Real Reinforcement Learning for Soccer skills](https://arxiv.org/abs/2512.12437)
*Jonathan Spraggett*

Main category: cs.RO

TL;DR: 本文提出了一种使用强化学习训练人形机器人控制任务的更高效方法，通过课程训练和对抗运动先验技术克服传统方法的局限性，在模拟环境中表现出色但未能成功迁移到现实世界。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在适应现实环境、处理复杂性和生成自然运动方面存在局限性，需要开发更有效的训练方法。

Method: 采用课程训练策略和对抗运动先验（AMP）技术来提升强化学习在机器人控制任务中的性能。

Result: 开发的踢球、行走和跳跃策略比之前的方法更具动态性和适应性，但在模拟到现实的迁移方面未能成功。

Conclusion: 虽然提出的方法在模拟环境中表现优异，但当前强化学习方法在完全适应现实世界场景方面仍存在局限性，特别是在模拟到现实的迁移问题上。

Abstract: This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.

</details>


### [231] [Autonomously Unweaving Multiple Cables Using Visual Feedback](https://arxiv.org/abs/2512.12468)
*Tina Tian,Xinyu Wang,Andrew L. Orekhov,Fujun Ruan,Lu Li,Oliver Kroemer,Howie Choset*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉反馈的多线缆解缠方法，将线缆解缠问题建模为抓取放置问题，使用图结构表示线缆状态，并通过状态转移模型预测未来状态和选择动作。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单根线缆的解结问题，而实际线缆管理任务中经常需要处理多根线缆相互缠绕的情况。本文聚焦于多线缆解缠这一子任务，旨在分离相互交织的多根线缆以方便后续操作。

Method: 1) 使用基于图的线缆状态表示方法，从视觉图像中编码线缆的拓扑和几何信息；2) 提出新颖的状态转移模型，考虑线缆在操作过程中的伸直和弯曲；3) 通过状态转移模型选择两个高层动作基元并计算预测即时成本来优化底层动作。

Result: 实验表明，通过迭代上述感知-规划-动作过程，该方法在解开电线束和鞋带的任务中平均成功率达到了84%。

Conclusion: 本文提出的方法能够有效解决多线缆解缠问题，为自动化线缆管理提供了可行的解决方案，在真实场景中表现出良好的性能。

Abstract: Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.

</details>


### [232] [Optimized Conflict Management for Urban Air Mobility Using Swarm UAV Networks](https://arxiv.org/abs/2512.12632)
*Rishit Agnihotri,Sandeep Kumar Sharma*

Main category: cs.RO

TL;DR: 该论文提出了一种基于边缘AI的分散式无人机群架构，通过控制算法优化实现实时冲突解决，在密集城市环境中显著提升无人机交通管理效率。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通（UAM）在密集城市走廊中面临前所未有的交通协调挑战，特别是随着无人机密度的增加，需要解决实时决策和低延迟的需求。

Method: 采用数学建模和控制算法，构建边缘AI驱动的分散式群架构，利用轻量级神经网络在边缘节点进行分布式冲突检测与解决，并开发仿真平台进行验证。

Result: 仿真结果表明，冲突解决时间最高可减少3.8倍，准确率相比传统集中式控制模型有所提升。

Conclusion: 该架构对未来UAM系统的可扩展、高效和安全空中交通管理具有重要应用前景。

Abstract: Urban Air Mobility (UAM) poses unprecedented traffic coordination challenges, especially with increasing UAV densities in dense urban corridors. This paper introduces a mathematical model using a control algorithm to optimize an Edge AI-driven decentralized swarm architecture for intelligent conflict resolution, enabling real-time decision-making with low latency. Using lightweight neural networks, the system leverages edge nodes to perform distributed conflict detection and resolution. A simulation platform was developed to evaluate the scheme under various UAV densities. Results indicate that the conflict resolution time is dramatically minimized up to 3.8 times faster, and accuracy is enhanced compared to traditional centralized control models. The proposed architecture is highly promising for scalable, efficient, and safe aerial traffic management in future UAM systems.

</details>


### [233] [Bayesian Optimization Parameter Tuning Framework for a Lyapunov Based Path Following Controller](https://arxiv.org/abs/2512.12649)
*Zhewen Zheng,Wenjing Cao,Hongkang Yu,Mo Chen,Takashi Suzuki*

Main category: cs.RO

TL;DR: 本文提出了一种基于贝叶斯优化的框架，用于在有限评估预算下高效调整非线性路径跟随控制器的参数，并在本田AI-Formula三轮机器人上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实硬件实验中参数调优受限于有限的评估预算，非线性几何控制器中多个增益通过耦合非线性项影响动力学，这种相互依赖使得手动调优效率低下且难以在有限试验次数内获得满意性能。

Method: 采用贝叶斯优化框架，将闭环系统视为黑箱，使用高斯过程代理模型选择控制器增益，提供无模型探索、量化不确定性和数据高效搜索。

Result: 在本田AI-Formula三轮机器人上的重复全圈实验表明，贝叶斯优化在32次试验（包括15次预热初始评估）内提升了控制器性能，能够有效定位参数空间中的高性能区域。

Conclusion: 贝叶斯优化为真实机器人平台上的非线性路径跟随控制器提供了一种实用、可靠且数据高效的调优方法。

Abstract: Parameter tuning in real-world experiments is constrained by the limited evaluation budget available on hardware. The path-following controller studied in this paper reflects a typical situation in nonlinear geometric controller, where multiple gains influence the dynamics through coupled nonlinear terms. Such interdependence makes manual tuning inefficient and unlikely to yield satisfactory performance within a practical number of trials. To address this challenge, we propose a Bayesian optimization (BO) framework that treats the closed-loop system as a black box and selects controller gains using a Gaussian-process surrogate. BO offers model-free exploration, quantified uncertainty, and data-efficient search, making it well suited for tuning tasks where each evaluation is costly. The framework is implemented on Honda's AI-Formula three-wheeled robot and assessed through repeated full-lap experiments on a fixed test track. The results show that BO improves controller performance within 32 trials, including 15 warm-start initial evaluations, indicating that it can efficiently locate high-performing regions of the parameter space under real-world conditions. These findings demonstrate that BO provides a practical, reliable, and data-efficient tuning approach for nonlinear path-following controllers on real robotic platforms.

</details>


### [234] [HMPCC: Human-Aware Model Predictive Coverage Control](https://arxiv.org/abs/2512.12717)
*Mattia Catellani,Marta Gabbi,Lorenzo Sabattini*

Main category: cs.RO

TL;DR: 提出基于模型预测控制（MPC）的人机协同覆盖框架HMPCC，通过预测人类运动轨迹实现机器人的主动协调和动态环境适应


<details>
  <summary>Details</summary>
Motivation: 传统覆盖策略依赖简化假设（如已知环境、静态密度函数），难以适应现实场景特别是人类参与的情况

Method: 使用高斯混合模型（GMM）建模环境兴趣区域，结合MPC框架集成人类运动预测，实现完全去中心化的多机器人协调

Result: 人类轨迹预测使覆盖更高效和自适应，改善了人机代理之间的协调性

Conclusion: HMPCC框架在敌对或通信受限场景下具有重要价值，通过人类运动预测实现了更智能的机器人团队协调

Abstract: We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.

</details>


### [235] [Making Robots Play by the Rules: The ROS 2 CLIPS-Executive](https://arxiv.org/abs/2512.12722)
*Tarik Viehmann,Daniel Swoboda,Samridhi Kalra,Himanshu Grover,Gerhard Lakemeyer*

Main category: cs.RO

TL;DR: 将CLIPS规则编程语言集成到ROS生态系统中，并展示其与PDDL规划框架的集成灵活性


<details>
  <summary>Details</summary>
Motivation: CLIPS适合构建知识驱动型应用，特别是协调自主机器人的复杂任务，但需要将其集成到主流的ROS生态系统中

Method: 基于Fawkes机器人框架中的CLIPS-Executive进行改进，将CLIPS集成到ROS生态系统，并实现与PDDL规划框架的集成

Result: 成功实现了CLIPS在ROS中的集成，并展示了其与PDDL规划框架的兼容性

Conclusion: CLIPS可以成功集成到ROS生态系统中，为机器人协调任务提供了灵活的知识驱动解决方案

Abstract: CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.

</details>


### [236] [VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps](https://arxiv.org/abs/2512.12793)
*Mizuho Aoki,Kohei Honda,Yasuhiro Yoshimura,Takeshi Ishita,Ryo Yonetani*

Main category: cs.RO

TL;DR: VLG-Loc是一种新颖的全局定位方法，使用仅包含视觉地标名称和区域的人类可读足迹地图，通过视觉语言模型在机器人多方向图像中搜索地图中的地标，并在蒙特卡洛定位框架中评估位姿假设。


<details>
  <summary>Details</summary>
Motivation: 解决机器人系统难以仅使用缺乏几何和外观细节的标注地图进行定位的挑战，模仿人类利用地标名称进行自然定位的能力。

Method: 利用视觉语言模型从机器人多方向图像观测中搜索地图标注的地标，在蒙特卡洛定位框架中使用找到的地标评估位姿假设的似然度。

Result: 在模拟和真实零售环境中的实验验证显示，相比现有基于扫描的方法具有更好的鲁棒性，特别是在环境变化情况下，通过视觉和扫描定位的概率融合进一步提升了性能。

Conclusion: VLG-Loc方法成功实现了基于语义地标的高效全局定位，在环境变化下表现出优越的鲁棒性，为机器人定位提供了新的解决方案。

Abstract: This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.

</details>


### [237] [SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding](https://arxiv.org/abs/2512.12842)
*Kuan Fang,Yuxin Chen,Xinghao Zhu,Farzad Niroui,Lingfeng Sun,Jiuguang Wang*

Main category: cs.RO

TL;DR: SAGA是一个通用的视觉运动控制框架，通过将高层语义意图与低层控制解耦，使用基于可操作性的任务表示，能够在不同环境、任务目标和用户规范间泛化。


<details>
  <summary>Details</summary>
Motivation: 为了解决移动操作中泛化能力不足的问题，需要开发一个能够适应不同任务形式（语言指令、点选、示例演示）的统一框架。

Method: 使用多模态基础模型将任务表示为3D可操作性热图，训练条件策略进行全身控制，支持零样本执行和少样本适应。

Result: 在四足机器人上进行的11个真实世界任务实验中，SAGA显著优于端到端和模块化基线方法。

Conclusion: 结构化可操作性接地为通用移动操作提供了一个可扩展且有效的途径。

Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.

</details>


### [238] [MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems](https://arxiv.org/abs/2512.12855)
*Patrick Kostelac,Xuerui Wang,Anahita Jamshidnejad*

Main category: cs.RO

TL;DR: 提出了一种结合MPC和RL的集成框架，将MPC的稳定性和安全性保证与RL的适应性相结合，在训练时使用MPC定义安全控制边界引导RL学习，部署时通过轻量级安全滤波器确保约束满足。


<details>
  <summary>Details</summary>
Motivation: 现代工程系统需要鲁棒、自适应且安全感知的控制器，RL具有数据驱动的适应性但缺乏约束处理机制，MPC具有结构化约束处理能力但依赖精确模型且计算量大。

Method: 在训练阶段使用MPC定义安全控制边界来引导RL组件进行约束感知的策略学习；在部署阶段，学习到的策略实时运行，并基于Lipschitz连续性的轻量级安全滤波器确保约束满足，无需繁重的在线优化。

Result: 在非线性气动弹性翼系统上的验证表明，该方法提高了扰动抑制能力，减少了执行器能耗，并在湍流下表现出鲁棒性能。

Conclusion: 该架构可推广到具有结构化非线性和有界扰动的其他领域，为工程应用中的安全人工智能驱动控制提供了可扩展解决方案。

Abstract: Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.

</details>


### [239] [SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework](https://arxiv.org/abs/2512.12945)
*Anja Sheppard,Parker Ewen,Joey Wilson,Advaith V. Sethuraman,Benard Adewole,Anran Li,Yuzhen Chen,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: SLIM-VDB是一种轻量级语义建图系统，利用OpenVDB数据结构，支持闭集和开集语义融合，在保持精度的同时显著降低内存和计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有语义建图系统缺乏对固定类别和开放语言标签预测的集成支持，且OpenVDB在语义建图领域的应用尚未探索。

Method: 基于OpenVDB数据结构，采用统一的贝叶斯更新框架，实现闭集和开集语义融合。

Result: 相比现有最先进方法，SLIM-VDB在内存使用和集成时间上显著减少，同时保持可比较的建图精度。

Conclusion: SLIM-VDB为语义建图提供了高效解决方案，支持闭集和开集字典，开源代码已发布。

Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.

</details>


### [240] [Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning](https://arxiv.org/abs/2512.12987)
*Amin Jalal Aghdasian,Farzaneh Abdollahi,Ali Kamali Iglie*

Main category: cs.RO

TL;DR: 本文提出了两种基于深度强化学习的车道保持系统算法，用于处理雪天道路条件下的不确定性和打滑问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在雪天道路条件下行驶时面临车道线模糊、车辆打滑等挑战，传统方法难以有效处理这些不确定性。

Method: 提出了AR-RDPG和AR-CADPG两种算法：AR-RDPG在感知层使用多尺度神经网络去噪，通过预训练DCNN提取中心线系数；AR-CADPG采用端到端方法，将CNN和注意力机制集成到DRL框架中。

Result: 在CARLA模拟器中训练并在真实Jetson Nano自动驾驶车辆上验证，两种方法都表现出可行性和稳定性，其中AR-CADPG在路径跟踪精度和鲁棒性方面表现更优。

Conclusion: 结合时序记忆、对抗鲁棒性和注意力机制的AR-CADPG方法在雪天自动驾驶场景中具有显著优势，为恶劣天气条件下的自动驾驶提供了有效解决方案。

Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.

</details>


### [241] [Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations](https://arxiv.org/abs/2512.12993)
*Guillermo A. Castillo,Himanshu Lodha,Ayonga Hereid*

Main category: cs.RO

TL;DR: 本文提出了一种层次化的地形感知双足运动策略，通过降维感知表示增强强化学习高层策略，用于实时步态生成。


<details>
  <summary>Details</summary>
Motivation: 传统端到端方法在复杂地形上的双足运动控制效果有限，需要更高效的地形感知和决策框架来提高学习效率和策略鲁棒性。

Method: 使用卷积变分自编码器(CNN-VAE)提取地形潜在编码，结合降阶机器人动力学，构建紧凑状态空间；分析潜在空间维度对学习的影响；引入历史感知机制；提出从深度相机图像学习潜在表示的蒸馏方法。

Result: 在高精度Agility Robotics仿真器中验证了方法的鲁棒性和适应性，包含真实传感器噪声、状态估计和执行器动力学，证明了硬件部署的可行性。

Conclusion: 该分层框架显著提高了地形感知双足运动的鲁棒性和学习效率，为实际硬件部署提供了可行方案。

Abstract: This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.

</details>


### [242] [K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots](https://arxiv.org/abs/2512.13009)
*Oğuzhan Akbıyık,Naseem Alhousani,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: K-VARK是一种基于核化方差感知残差卡尔曼滤波的新型传感器力估计方法，通过核化运动基元捕获残差力矩的预测均值和异方差方差，在6自由度协作机械臂上实现了比现有方法降低20%以上的RMSE。


<details>
  <summary>Details</summary>
Motivation: 机器人安全精确交互需要可靠接触力估计，但现有传感器力估计方法面临建模误差和复杂残差动力学挑战，难以实现准确估计。

Method: 将核化概率残差力矩模型集成到自适应卡尔曼滤波框架中，通过核化运动基元学习残差力矩的均值和方差，并采用变分贝叶斯优化在线适应过程噪声协方差。

Result: 在6-DoF协作机械臂上的实验验证表明，K-VARK相比最先进的传感器力估计方法实现了超过20%的RMSE降低。

Conclusion: K-VARK能够提供鲁棒准确的外部力/力矩估计，适用于抛光、装配等高级任务。

Abstract: Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.

</details>


### [243] [Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos](https://arxiv.org/abs/2512.13080)
*Yicheng Feng,Wanpeng Zhang,Ye Wang,Hao Luo,Haoqi Yuan,Sipeng Zheng,Zongqing Lu*

Main category: cs.RO

TL;DR: 本文提出了一种空间感知的VLA预训练范式，通过2D视觉观察与3D空间推理的对齐，解决现有VLA模型在3D物理环境中执行动作时的感知与动作基础之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型大多依赖2D视觉输入在3D物理环境中执行动作，导致感知与动作基础之间存在显著差距。

Method: 利用预训练的视觉语言模型，从大规模人类演示视频中提取3D视觉和3D动作注释，形成新的监督源，实现2D视觉观察与3D空间推理的对齐。具体实现为VIPA-VLA双编码器架构，包含3D视觉编码器以增强语义视觉表示。

Result: 在下游机器人任务中，VIPA-VLA显著改善了2D视觉与3D动作之间的基础，产生了更鲁棒和可泛化的机器人策略。

Conclusion: 提出的空间感知VLA预训练范式通过显式对齐视觉空间和物理空间，使模型在机器人策略学习前获得3D空间理解，有效解决了感知与动作基础之间的差距。

Abstract: Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.

</details>


### [244] [Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion](https://arxiv.org/abs/2512.13090)
*Jebeom Chae,Junwoo Chang,Seungho Yeom,Yujin Kim,Jongeun Choi*

Main category: cs.RO

TL;DR: LCHD是一个基于视觉的语言条件扩散规划框架，通过集成CLIP语义先验和碰撞避免扩散核，实现无需显式障碍物信息的语言条件无碰撞轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 解决当前扩散模型在多机器人规划中语言条件任务适应性差、计算成本高、泛化能力不足的问题，特别是缺乏几何可达性推理机制。

Method: 集成CLIP语义先验与碰撞避免扩散核，将语言命令严格约束在可达工作空间内，无需显式环境表示即可处理分布外场景。

Result: 在多样化真实世界地图和真实机器人实验中，LCHD在成功率上持续优于现有扩散规划器，同时降低规划延迟。

Conclusion: LCHD通过语义-物理融合的扩散核设计，有效解决了语言条件多机器人规划中的可达性推理和泛化问题。

Abstract: Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.

</details>


### [245] [PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations](https://arxiv.org/abs/2512.13093)
*Mingqi Yuan,Tao Yu,Haolin Song,Bo Li,Xin Jin,Hua Chen,Wenjun Zeng*

Main category: cs.RO

TL;DR: 提出PvP框架，通过本体感知与特权状态的对比学习，提高人形机器人全身控制的样本效率和性能，并开发了SRL4Humanoid评估框架。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在人形机器人控制中的样本低效问题，由于复杂动力学和部分可观测性导致学习困难。

Method: 提出PvP（本体感知-特权对比学习框架），利用本体感知状态和特权状态的内在互补性，学习紧凑且任务相关的潜在表示，无需手工数据增强。

Result: 在LimX Oli机器人上的速度跟踪和运动模仿任务实验中，PvP相比基线SRL方法显著提高了样本效率和最终性能。

Conclusion: 研究为将状态表示学习与强化学习结合提供了实用见解，为人形机器人的数据高效学习提供了有价值的指导。

Abstract: Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.

</details>


### [246] [Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation](https://arxiv.org/abs/2512.13094)
*Xiang Li,Gang Liu,Weitao Zhou,Hongyi Zhu,Zhong Cao*

Main category: cs.RO

TL;DR: 本文提出了一种名为专家序列（SoE）的时间交替策略方法，旨在解决模仿学习在自动驾驶闭环测试中因小误差累积导致的性能退化问题，无需增加模型规模或数据需求即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在自动驾驶中虽然能在开环环境下很好匹配专家行为，但在闭环测试中由于小误差随时间累积会导致性能意外下降。当前研究主要关注单时间点的状态级鲁棒性，而自动驾驶是连续时间过程，利用时间尺度增强鲁棒性可能为解决此问题提供新视角。

Method: 提出专家序列（SoE）方法，这是一种时间交替策略，通过时序上的专家交替来增强闭环性能，不需要增加模型复杂度或训练数据量。

Result: 在大型自动驾驶基准nuPlan上的实验表明，SoE方法在所有评估模型上都一致且显著地提升了性能，并达到了最先进的性能水平。

Conclusion: 该模块可能为提高自动驾驶模型训练效率提供关键且广泛适用的支持，为处理模仿学习中的误差累积问题提供了新的时间维度解决方案。

Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.

</details>


### [247] [OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning](https://arxiv.org/abs/2512.13100)
*Guanhua Ji,Harsha Polavaram,Lawrence Yunliang Chen,Sandeep Bajamahal,Zehan Ma,Simeon Adebola,Chenfeng Xu,Ken Goldberg*

Main category: cs.RO

TL;DR: 本文提出了AugE-Toolkit机器人数据增强工具包和OXE-AugE数据集，通过增强现有OXE数据集中的机器人本体多样性，显著提高了通用机器人策略的跨本体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模机器人数据集（如OXE）存在严重的数据不平衡问题，少数几种机器人类型占据了大部分数据，这会导致模型在训练时过度拟合特定的机器人-场景组合，限制了策略在新机器人硬件上的泛化能力。

Method: 开发了可扩展的机器人增强管道AugE-Toolkit，创建了OXE-AugE开源数据集，将OXE数据集扩展到9种不同的机器人本体，提供了超过440万条轨迹，是原始OXE的三倍多。

Result: 实验表明，增加多样化的机械臂和夹爪数据不仅能提升增强机器人的策略性能，还能改善未见过的机器人以及原始机器人在分布偏移下的表现。在物理实验中，OpenVLA和π₀等先进通用策略在OXE-AugE上微调后，在四个真实世界操作任务上的成功率提高了24-45%。

Conclusion: 通过数据增强增加机器人本体的多样性是提升跨本体学习效果的有效方法，OXE-AugE数据集为训练更具泛化能力的通用机器人策略提供了重要资源。

Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot--scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot--gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.

</details>


### [248] [START: Traversing Sparse Footholds with Terrain Reconstruction](https://arxiv.org/abs/2512.13153)
*Ruiqi Yu,Qianshi Wang,Hongyi Li,Zheng Jun,Zhicheng Wang,Jun Wu,Qiuguo Zhu*

Main category: cs.RO

TL;DR: START是一个单阶段学习框架，通过低成本机载视觉和本体感知重建局部地形高度图，实现四足机器人在稀疏立足点地形上的敏捷稳定运动


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在稀疏立足点地形上的局限性：基于模型的分层控制器泛化能力有限且行为保守，端到端学习方法要么依赖噪声高度图，要么从深度图像隐式推断地形特征，缺乏准确几何信息导致学习效率低和步态僵硬

Method: 提出START框架，仅使用低成本机载视觉和本体感知准确重建局部地形高度图，提供显式中间表示来传达稀疏立足点区域的关键特征，支持全面环境理解和精确地形评估

Result: 实验结果显示START实现了在多样化真实场景中的零样本迁移，表现出优越的适应性、精确的立足点放置和鲁棒的运动能力

Conclusion: START框架通过显式地形重建有效解决了稀疏立足点地形上的运动挑战，降低了探索成本并加速了技能获取

Abstract: Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.

</details>


### [249] [Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks](https://arxiv.org/abs/2512.13170)
*Deepak Ingole,Valentin Bhend,Shiva Ganesh Murali,Oliver Dobrich,Alisa Rupenayan*

Main category: cs.RO

TL;DR: 该论文提出了一种基于任务级性能反馈的非线性模型预测控制（NMPC）权重矩阵自动调优的迭代学习框架，通过构建经验灵敏度矩阵实现结构化权重更新，无需解析导数。


<details>
  <summary>Details</summary>
Motivation: 制造过程常受环境和系统磨损的漂移影响，即使在重复操作中也需要控制重新调优，传统梯度方法需要微分NMPC求解器，计算复杂。

Method: 受范数最优迭代学习控制（ILC）启发，该方法在任务重复中自适应调整NMPC权重Q和R，以最小化跟踪精度、控制努力和饱和度的关键性能指标（KPIs），通过经验灵敏度矩阵实现权重更新。

Result: 在UR10e机器人进行碳纤维缠绕的仿真验证中，该方法仅需4次在线重复即可达到接近最优的跟踪性能（RMSE在离线贝叶斯优化的0.3%以内），而BO算法需要100次离线评估。

Conclusion: 该方法为重复性机器人任务中的自适应NMPC调优提供了实用解决方案，结合了精心优化控制器的精度和在线适应的灵活性。

Abstract: Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.

</details>


### [250] [Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification](https://arxiv.org/abs/2512.13183)
*Alfredo González-Calvin,Juan F. Jiménez,Héctor García de Marina*

Main category: cs.RO

TL;DR: 提出了一种通过mollification正则化不可微函数的方法，为移动机器人生成可行路径，该方法计算高效且可实现实时路径跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有路径跟踪算法要求路径至少二阶可微，但实际任务中常用的分段函数路径不可微。传统平滑方法要么产生复杂轨迹，要么计算成本高，需要一种高效处理不可微路径的方法。

Method: 使用mollification技术近似任意路径为可微函数，可达到任意精度逼近。提供系统方法限制生成路径的曲率，特别适用于由线段连接航点序列的路径。

Result: 该方法计算效率高，可在微控制器上实时实现，且与标准轨迹跟踪和路径跟踪算法兼容。

Conclusion: mollification方法有效解决了不可微路径的跟踪问题，为移动机器人提供了实用的路径生成方案。

Abstract: Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.

</details>


### [251] [ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation](https://arxiv.org/abs/2512.13198)
*Hyun-Gi Lee,Jaekyeong Han,Minjun Kwon,Hyeonuk Kwon,Jooha Park,Hoe Jin Ha,Dong-Hwa Seo*

Main category: cs.RO

TL;DR: 本文介绍了ALBATROSS系统，一个能够自动进行电解质配方、纽扣电池组装和电化学评估的自动化系统，旨在解决传统纽扣电池组装和测试过程耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 随着电池技术向更高稳定性和能量密度发展，需要在各种组件配置下进行广泛的电池级测试。传统的纽扣电池组装和测试过程需要研究人员投入大量时间和精力，这给高通量筛选研究带来了挑战。

Method: 开发了ALBATROSS系统，该系统集成在氩气填充的手套箱内，能够全自动组装和测试多达48个电池，无需研究人员干预。通过定制设计的机器人夹爪和3D打印结构优化了精确的电池处理。

Result: ALBATROSS实现了高组装可靠性，NCM811||Li半电池的放电容量相对标准偏差（RSD）小于1.2%，EIS测量的标准偏差小于3 Ω。

Conclusion: ALBATROSS的高可靠性和自动化能力使得能够获得高质量的纽扣电池数据集，有望加速下一代电解质的开发。

Abstract: As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 Ω in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.

</details>


### [252] [Differentiable Material Point Method for the Control of Deformable Objects](https://arxiv.org/abs/2512.13214)
*Diego Bolliger,Gabriele Fadini,Markus Bambach,Alisa Rupenyan*

Main category: cs.RO

TL;DR: 提出可微分材料点法（MPM）模拟器，用于柔性物体控制，通过可微分特性优化控制轨迹，在超弹性绳索主动阻尼问题中实现更快的能量最小化。


<details>
  <summary>Details</summary>
Motivation: 柔性物体因其非线性动力学和高维配置空间而难以控制，需要开发有效的控制方法。

Method: 使用可微分材料点法（MPM）模拟器，利用其可微分特性优化控制轨迹，应用于超弹性绳索的主动阻尼问题。

Result: 模拟器在绳索动能最小化方面比基线MPPI方法快约2倍，能量水平降低20%，计算时间仅需约3%。

Conclusion: 可微分MPM模拟器为柔性物体控制提供了一种高效的方法，显著优于传统控制方法。

Abstract: Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.

</details>


### [253] [Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment](https://arxiv.org/abs/2512.13215)
*Yinsong Qu,Yunxiang Li,Shanlin Zhong*

Main category: cs.RO

TL;DR: 提出改进的顺序模型预测控制（ISMPC）导航框架，通过多向安全矩形走廊算法和顺序MPC集成，解决AMR在密集、杂乱环境中的导航挑战，提高自由空间利用率和实时性能。


<details>
  <summary>Details</summary>
Motivation: 自主移动机器人在密集、杂乱、半结构化环境中导航面临挑战，包括非完整车辆动力学、与静态/动态障碍物交互以及非凸约束空间等问题。

Method: 1）多向安全矩形走廊（MDSRC）算法，通过矩形凸区域编码自由空间避免与静态障碍物碰撞；2）顺序MPC导航框架，集成走廊约束和屏障函数约束实现静态和动态障碍物避障。

Result: 框架在自由空间利用率上提高41.05%（平均走廊面积），同时保持实时计算性能（平均走廊生成延迟3毫秒）。

Conclusion: ISMPC导航框架简化了传统导航算法架构，直接为AMR生成速度，在密集环境中表现出优越的导航性能。

Abstract: Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).

</details>


### [254] [A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization](https://arxiv.org/abs/2512.13219)
*Christoph Hartmann,Marios Demetriades,Kevin Prüfer,Zichen Zhang,Klaus Spindler,Stefan Weltge*

Main category: cs.RO

TL;DR: PyCAALP是一个基于Python的计算机辅助装配线规划框架，采用图论方法对生产模块中的组件和连接进行建模，集成运动学边界条件以确保自动化装配规划的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决装配序列规划(ASP)和生产线规划(PLP)中的高组合复杂性，确保自动化装配的可行性，并平衡规划效率与最优性。

Method: 使用图论建模组件和连接，开发算法计算所有可行生产序列，集成空间关系检测和几何约束模块，采用启发式方法（如单件流装配）优化解空间，PLP阶段建模为混合整数规划问题。

Result: 开发了开源框架PyCAALP，能够处理复杂装配的自动化规划，支持工程约束定制和ASP-PLP之间的灵活权衡，显著减少了混合整数规划的计算时间。

Conclusion: PyCAALP框架为工业和生产研究应用提供了有效的自动化装配规划解决方案，其开源特性促进了进一步协作和采用。

Abstract: This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.

</details>


### [255] [Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving](https://arxiv.org/abs/2512.13262)
*Hyunki Seong,Jeong-Kyun Lee,Heesoo Myeong,Yongho Shin,Hyun-Mook Cho,Duck Hoon Kim,Pranav Desai,Monu Surana*

Main category: cs.RO

TL;DR: 该论文提出两种互补策略来解决自动驾驶中多智能体交互运动行为学习的局限性：GRBO强化学习后训练方法和Warm-K采样策略，显著提升安全性能和行为一致性。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习模型因数据集偏差导致的鲁棒性不足问题，以及开环评估忽视闭环执行中误差累积的局限性。

Method: 1. GRBO：基于群体相对优势最大化的强化学习后训练方法，使用人类正则化对预训练行为模型进行微调；2. Warm-K：基于热启动的Top-K采样策略，平衡运动选择的一致性和多样性。

Result: GRBO仅使用10%训练数据就将安全性能提升40%以上，同时保持行为真实性；Warm-K方法在测试时无需重新训练即可增强行为一致性和反应性。

Conclusion: 提出的两种策略有效解决了自动驾驶行为模型的安全性和鲁棒性问题，GRBO通过后训练优化安全性能，Warm-K通过测试时采样策略改善行为一致性。

Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.

</details>


### [256] [Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation](https://arxiv.org/abs/2512.13271)
*Fangju Yang,Hang Yang,Ibrahim Alsarraj,Yuhao Wang,Ke Wu*

Main category: cs.RO

TL;DR: 提出了LASEM框架，用于电缆驱动连续体机器人的轻量级动态建模，通过变分推导将动力学简化为单个PDE，支持力输入和位移输入两种驱动模式，计算效率提升62.3%。


<details>
  <summary>Details</summary>
Motivation: 电缆驱动连续体机器人需要准确、实时的动态模型进行高速动态预测或基于模型的控制，现有方法计算复杂且难以同时支持两种驱动模式。

Method: 采用轻量级驱动空间能量建模框架，直接在驱动空间构建驱动势能，通过变分推导将动力学简化为单个偏微分方程，避免显式计算电缆-骨架接触力，并采用Galerkin时空模态离散化。

Result: 该方法在保持几何精度和物理一致性的同时，计算效率比现有实时动态建模方法平均提升62.3%，且原生支持力输入和位移输入两种驱动模式。

Conclusion: LASEM框架为电缆驱动连续体机器人提供了一种轻量级、高精度的动态建模方法，显著提升了计算效率，同时解决了现有方法难以同时支持两种驱动模式的问题。

Abstract: Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.

</details>


### [257] [Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration](https://arxiv.org/abs/2512.13293)
*Hao Fua,Wei Liu,Shuai Zhoua*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多机器人强化学习算法，通过内在动机探索和双采样模式来解决多机器人社会编队导航中的协调探索效率问题。


<details>
  <summary>Details</summary>
Motivation: 多机器人社会编队导航对于实现人机无缝共存至关重要，但行人行为的不可预测性和非合作性给机器人协调探索带来了挑战，特别是效率问题。

Method: 提出协调探索多机器人RL算法，引入内在动机探索，核心是自学习内在奖励机制以缓解策略保守主义；在集中训练分散执行框架中采用双采样模式，利用双时间尺度更新规则解耦参数更新。

Result: 在社会编队导航基准测试中，该算法在关键指标上优于现有最先进方法。

Conclusion: 所提出的算法通过内在奖励机制和双采样策略有效提升了多机器人社会导航的协调探索效率，为人机共存提供了可行解决方案。

Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.

</details>


### [258] [Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories](https://arxiv.org/abs/2512.13304)
*Sait Sovukluk,Johannes Englsberger,Christian Ott*

Main category: cs.RO

TL;DR: 该研究提出了一种基于弹簧-质量轨迹和死区控制增益库的步态自适应框架，用于人形机器人跑步控制，通过全身体控制框架实现各种敏捷行为。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够在各种挑战性环境下实现稳健跑步控制的人形机器人控制框架，无需针对不同行为进行额外参数调整。

Method: 包括四个主要部分：自动弹簧-质量轨迹库生成、通过主动控制模板模型生成死区控制增益库、步态自适应轨迹选择策略开发、以及通过全身体控制框架将弹簧-质量轨迹映射到人形机器人模型。

Result: 在MuJoCo物理模拟器中成功实现了多种敏捷行为，包括随机踏石跑步、跳跃障碍、蛇形运动、突然转向等，并能抵抗显著干扰和不确定性。单个库和相同WBC参数可支持所有行为，库计算仅需4.5秒。

Conclusion: 提出的框架具有包容性和鲁棒性，能够应对现实世界中的各种挑战，包括信号噪声、不精确性、建模误差和延迟，为复杂环境下的机器人跑步控制提供了有效解决方案。

Abstract: This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.

</details>


### [259] [Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)](https://arxiv.org/abs/2512.13356)
*Zeyad Gamal,Youssef Mahran,Ayman El-Badawy*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的框架来控制双旋翼气动系统（TRAS），实现特定俯仰角和方位角的稳定控制及轨迹跟踪。使用TD3算法训练智能体，通过仿真和实验验证了该方法相比传统PID控制器的优越性。


<details>
  <summary>Details</summary>
Motivation: 双旋翼气动系统的复杂动力学特性和非线性使其难以用传统控制算法有效控制。强化学习技术的发展为多旋翼飞行器控制提供了新的可能性。

Method: 采用Twin Delayed Deep Deterministic Policy Gradient（TD3）算法训练强化学习智能体，该算法适用于连续状态和动作空间的环境，且不需要系统模型。

Result: 仿真结果表明强化学习方法有效，在风扰动等外部干扰下表现出比传统PID控制器更好的性能。实验室实验进一步验证了该控制器在实际应用中的有效性。

Conclusion: 强化学习框架为双旋翼气动系统的控制提供了有效的解决方案，特别是在处理复杂非线性和外部干扰方面优于传统控制方法。

Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.

</details>


### [260] [Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles](https://arxiv.org/abs/2512.13359)
*Sümer Tunçay,Alain Andres,Ignacio Carlucho*

Main category: cs.RO

TL;DR: 本文提出了一种基于JAX和MuJoCo-XLA的GPU加速强化学习训练流水线，实现了在2分钟内完成训练，并在真实水下实验中展示了鲁棒的6-DOF轨迹跟踪和扰动抑制能力。


<details>
  <summary>Details</summary>
Motivation: 传统控制器在标称条件下有效，但在面对未建模动态或环境扰动时性能下降。强化学习虽提供强大替代方案，但训练速度慢且模拟到现实的迁移具有挑战性。

Method: 使用JAX和MuJoCo-XLA构建GPU加速的RL训练流水线，通过联合JIT编译大规模并行物理模拟和学习更新，实现快速训练。系统评估多种RL算法。

Result: 训练时间缩短至2分钟以内，在真实水下实验中实现了鲁棒的6-DOF轨迹跟踪和有效的扰动抑制，策略从模拟零样本迁移到现实。

Conclusion: 这是首个在真实世界中展示基于RL的AUV全六自由度位置控制的明确成果，为复杂海洋环境中的自主水下航行器控制提供了高效解决方案。

Abstract: Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.

</details>


### [261] [Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning](https://arxiv.org/abs/2512.13380)
*Chuan Mao,Haoqi Yuan,Ziye Huang,Chaoyi Xu,Kai Ma,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoFunGrasp提出了一种用于通用灵巧功能抓取的方法，通过将功能抓取条件分解为抓取风格和可操作性两个互补组件，并集成到RL框架中，实现了对任意物体和功能抓取条件的学习。


<details>
  <summary>Details</summary>
Motivation: 虽然强化学习在灵巧抓取方面取得了成功，但细粒度的功能抓取对于下游操作任务至关重要，目前仍面临目标规范复杂、多任务RL探索困难以及仿真到现实迁移等挑战。

Method: 将功能抓取条件分解为抓取风格和可操作性组件，利用单次抓取演示将RL问题重新表述为一步演示编辑，显著提高了样本效率和性能。

Result: 在仿真和真实世界的实验中，DemoFunGrasp能够泛化到未见过的物体、可操作性和抓取风格组合，在成功率和功能抓取准确性方面均优于基线方法。

Conclusion: 该方法不仅具有强大的仿真到现实迁移能力，通过结合视觉语言模型进行规划，还能实现自主的指令跟随抓取执行。

Abstract: Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.

</details>


### [262] [Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model](https://arxiv.org/abs/2512.13477)
*Timothy A. Brumfiel,Revanth Konda,Drew Elliott,Jaydev P. Desai*

Main category: cs.RO

TL;DR: 本研究评估了改进版COAST导丝机器人在脉动流解剖模型中的导航性能，该简化版本由两个导管组成，能够有效导航复杂血管模型。


<details>
  <summary>Details</summary>
Motivation: 解决血管介入手术中手动导航导丝的问题，通过机器人导丝技术提高可操作性和导航能力。

Method: 使用改进版COAST导丝机器人（由两个导管组成的简化版本），在具有脉动流的解剖模型中进行导航实验。

Result: 改进版COAST导丝机器人能够有效导航复杂的模型血管结构。

Conclusion: 简化版COAST导丝机器人在复杂血管导航中表现出良好性能，验证了其临床应用的可行性。

Abstract: To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.

</details>


### [263] [Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM](https://arxiv.org/abs/2512.13514)
*Aman Arora,Matteo El-Hariry,Miguel Olivares-Mendez*

Main category: cs.RO

TL;DR: 本文提出了一个强化学习框架，用于JAXA Int-Ball2机器人在国际空间站日本实验舱内的六自由度精确对接，通过PPO算法在高保真模拟环境中训练控制器。


<details>
  <summary>Details</summary>
Motivation: 解决国际空间站内自主飞行器在传感噪声、执行器误差和环境变化等挑战下实现精确对接的难题。

Method: 使用近端策略优化（PPO）算法，在Isaac Sim高保真模拟环境中进行训练，采用领域随机化动力学和有界观测噪声，显式建模螺旋桨阻力扭矩效应和极性结构。

Result: 学习到的策略在多样化条件下实现了稳定可靠的对接性能，为未来扩展奠定了基础。

Conclusion: 该研究为Int-Ball2在碰撞感知导航、安全强化学习、精确的模拟到现实迁移以及基于视觉的端到端对接等领域的未来扩展提供了基础。

Abstract: Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.

</details>


### [264] [Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments](https://arxiv.org/abs/2512.13561)
*Li-Wei Shih,Ruo-Syuan Mei,Jesse Heidrich,Hui-Ping Wang,Joel Hooton,Joshua Solomon,Jorge Arinez,Guangze Li,Chenhui Shao*

Main category: cs.RO

TL;DR: 本文提出了一种用于自主移动机器人(AMR)的三层近场感知框架，包括光线间断检测、光位移测量和基于计算机视觉的目标检测，在树莓派5系统上实现实时性能，为制造业环境中的AMR提供可扩展的安全感知解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统测距传感器如LiDAR和超声波设备在检测机器人底座附近的小物体方面经常失败，而近场感知对于制造业环境中AMR的安全操作至关重要。

Method: 1) 光线间断检测：通过投射激光条纹并检测条纹中断来实现快速二值障碍物检测；2) 光位移测量：通过分析投影条纹在相机图像中的几何位移来估计物体高度；3) 基于计算机视觉的目标检测模型在嵌入式AI硬件上进行物体分类。

Result: 所有方法在树莓派5系统上实现，达到25或50帧/秒的实时性能。实验评估和比较分析表明，所提出的层次结构在精度、计算和成本之间取得了平衡。

Conclusion: 该框架为制造业环境中的AMR安全操作提供了可扩展的感知解决方案，能够有效检测近场小物体并实现语义感知和上下文感知的安全决策。

Abstract: Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.

</details>


### [265] [World Models Can Leverage Human Videos for Dexterous Manipulation](https://arxiv.org/abs/2512.13644)
*Raktim Gautam Goswami,Amir Bar,David Fan,Tsung-Yen Yang,Gaoyue Zhou,Prashanth Krishnamurthy,Michael Rabbat,Farshad Khorrami,Yann LeCun*

Main category: cs.RO

TL;DR: DexWM是一个灵巧操作世界模型，通过预测环境潜在状态来提升灵巧操作能力，在零样本泛化中表现优于现有方法50%以上


<details>
  <summary>Details</summary>
Motivation: 灵巧操作具有挑战性，因为需要理解细微的手部动作如何通过物体接触影响环境。现有灵巧操作数据集稀缺，需要开发能够从非灵巧数据中学习的方法

Method: 提出DexWM模型，基于过去状态和灵巧动作预测环境的下一个潜在状态。使用超过900小时的人类和非灵巧机器人视频进行训练。引入辅助手部一致性损失来确保准确的手部配置

Result: DexWM在状态预测准确性上优于基于文本、导航和全身动作的现有世界模型。在Franka Panda机械臂上部署时，在抓取、放置和到达任务中平均比Diffusion Policy高出50%以上

Conclusion: DexWM通过世界模型方法和辅助手部一致性损失，有效解决了灵巧操作中的挑战，展示了强大的零样本泛化能力

Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

</details>


### [266] [RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2512.13660)
*Enshen Zhou,Cheng Chi,Yibo Li,Jingkun An,Jiayuan Zhang,Shanyu Rong,Yi Han,Yuheng Ji,Mengzhen Liu,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboTracer是一个3D感知的视觉语言模型，通过空间编码器和回归监督解码器实现空间定位和测量，结合强化微调实现多步度量推理，在空间追踪任务上达到79.1%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间追踪任务上存在困难，该任务需要多步度量推理和复杂空间参照，因此需要开发能够同时处理空间参照和测量的模型。

Method: 提出RoboTracer模型，包含通用空间编码器和回归监督解码器进行监督微调，同时使用带度量敏感过程奖励的强化微调来增强多步推理能力。构建了30M QA对的TraceSpatial数据集。

Result: RoboTracer在空间理解、测量和参照方面超越基线方法，平均成功率达79.1%，在TraceSpatial-Bench上大幅超越Gemini-2.5-Pro 36%的准确率。可集成到多种机器人控制策略中执行长时程动态任务。

Conclusion: RoboTracer通过创新的空间编码和强化微调方法，成功解决了空间追踪的挑战性任务，为机器人在复杂现实场景中的空间推理能力提供了有效解决方案。

Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

</details>


### [267] [NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks](https://arxiv.org/abs/2512.13670)
*Licheng Luo,Yu Xia,Kaier Liang,Mingyu Cai*

Main category: cs.RO

TL;DR: 该论文提出了一种基于时空逻辑（SpaTiaL）的数据集生成框架NL2SpaTiaL，用于解决机器人操作任务中空间关系表达不足的问题，并通过翻译-验证框架确保语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于时序逻辑的方法主要关注机器人轨迹，忽略了物体层面的空间交互关系，而机器人操作任务的成功高度依赖于几何空间约束和物体间关系。

Method: 开发了SpaTiaL规范合成框架，通过确定性、语义保持的反向翻译过程将空间逻辑规范转换为自然语言描述，构建NL2SpaTiaL数据集，并配备基于语言的语义检查器进行翻译验证。

Result: 实验表明，基于SpaTiaL的表征在指令跟随任务中提供了更可解释、可验证和可组合的语义基础。

Conclusion: SpaTiaL框架能够有效表达机器人操作任务中的多层次空间关系，为指令理解提供了更强大的语义表示能力。

Abstract: Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [268] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 该论文提出了价值档案的概念，这是一种可重用的价值相关参数集合，用于在动态环境中实现状态依赖的价值控制，而无需为每个情境维护独立参数。


<details>
  <summary>Details</summary>
Motivation: 解决在波动环境中，代理需要在潜在情境间切换价值控制机制，但为每个情境维护独立偏好、策略偏差和行动置信度参数在计算上不可行的问题。

Method: 引入价值档案：少量可重用的价值参数包（结果偏好、策略先验和策略精度），通过后验信念加权混合实现有效控制参数，在概率反转学习任务中比较静态精度、熵耦合动态精度和基于档案的模型。

Result: 模型比较显示基于档案的模型优于简单替代方案（约100点AIC差异），参数恢复分析支持结构可识别性，模型推断表明自适应控制主要由策略先验调制驱动。

Conclusion: 可重用的价值档案为波动环境中的信念条件价值控制提供了可计算的理论框架，并产生了信念依赖控制和行为灵活性的可测试特征。

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [269] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种基于因果推理的CR3G框架，用于改进胸部X光报告生成，通过关注因果关系和患者中心解释来增强AI诊断的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医学图像分析中擅长发现相关性，但难以理解模式与患者状况之间的深层因果关系，限制了诊断报告的准确性和临床实用性。

Method: 采用提示驱动的CR3G框架，结合因果推理方法，专注于胸部X光分析中的因果关系和患者中心解释生成。

Result: CR3G在5种异常中的2种上表现出更好的因果关系识别能力和解释能力。

Conclusion: 该框架通过因果推理提升了AI生成报告的质量，使其在临床实践中更加有用和可信。

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [270] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出了一个用于代表性快捷模型的通用设计框架，为这些模型的有效性提供理论依据，并通过组件级改进实现了ImageNet-256x256上2.85 FID50k的最新SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前few-step扩散模型的理论推导和实际实现紧密耦合，限制了设计空间的探索。需要建立一个通用框架来解耦组件级选择，系统性地识别改进方向。

Method: 提出通用设计框架，为代表性快捷模型提供理论依据，解耦具体组件级选择，系统性地识别改进点。无需预训练、蒸馏或课程学习。

Result: 改进后的一步模型在ImageNet-256x256上实现了2.85 FID50k的最新SOTA结果，在分类器自由引导设置下表现优异。

Conclusion: 该工作降低了快捷模型中组件级创新的门槛，促进了设计空间的原则性探索，为few-step扩散模型的发展提供了系统化框架。

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [271] [Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation](https://arxiv.org/abs/2512.11832)
*Jakub Walczak*

Main category: cs.LG

TL;DR: 本文评估了三种稀疏气候数据重建方法，发现简单的反距离加权法（IDW）在重建精度和计算效率方面均优于普通克里金法和先进的隐式神经表示模型。


<details>
  <summary>Details</summary>
Motivation: 比较不同复杂度的稀疏气候数据重建方法的性能，包括传统方法（IDW、OK）和现代深度学习方法（MMGN），以确定最有效的方法。

Method: 使用验证分割进行超参数调优，对三种重建方法（IDW、OK、MMGN）进行广泛实验，并进行全面的统计分析，包括RMSE、MAE、Δ_MAX、R²等指标评估。

Result: IDW方法在所有评估指标上表现最佳：最低的RMSE（3.00±1.93）、MAE（1.32±0.77）和Δ_MAX（24.06±17.15），以及最高的R²（0.68±0.16），差异具有统计显著性。

Conclusion: 简单的IDW方法在稀疏气候数据重建中优于更复杂的统计和深度学习方法，证明了在某些应用场景中简单方法的有效性。

Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $Δ_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]

</details>


### [272] [Soft Decision Tree classifier: explainable and extendable PyTorch implementation](https://arxiv.org/abs/2512.11833)
*Reuben R Shamir*

Main category: cs.LG

TL;DR: 实现了Soft Decision Tree (SDT)和Short-term Memory Soft Decision Tree (SM-SDT)，在模拟和临床数据集上测试，与XGBoost性能相当，优于其他传统方法。


<details>
  <summary>Details</summary>
Motivation: 开发可解释的决策树模型，结合深度学习优势，提高分类性能同时保持可解释性。

Method: 使用PyTorch实现SDT和SM-SDT，在模拟和临床数据集上进行广泛测试，并与XGBoost、随机森林、逻辑回归和传统决策树进行比较。

Result: SDT、SM-SDT和XGBoost的AUC值相似，均优于随机森林、逻辑回归和传统决策树。在临床数据集上，除传统决策树外，所有测试的分类方法结果相当。

Conclusion: SDT和SM-SDT在保持可解释性的同时，实现了与XGBoost相当的分类性能，为可解释AI提供了有效工具。

Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree

</details>


### [273] [Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems](https://arxiv.org/abs/2512.11834)
*Stiven Briand Massala,Ludovic Chamoin,Massimo Picca Ciamarra*

Main category: cs.LG

TL;DR: 提出了一种结合物理建模与数据驱动学习的混合方法，用于增强复杂不确定物理系统的状态估计和预测。该方法在PBDW框架基础上，引入DeepONet来学习模型偏差，并保持物理模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂物理系统的状态估计需要协调存在固有缺陷的理论模型与噪声实验数据。传统方法难以同时处理预期和非预期的不确定性。

Method: 基于PBDW框架，结合降阶模型和测量数据，引入约束为最佳知识流形正交补的DeepONet来学习模型偏差，并研究最优传感器布置策略。

Result: 在涉及亥姆霍兹方程的代表性问题上验证了该方法，能够有效处理边界条件和源项等多种建模误差源。

Conclusion: 所提出的混合方法能够有效增强状态估计和预测，同时保持物理模型的解释性和保真度，为复杂不确定系统的建模提供了有效解决方案。

Abstract: The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.

</details>


### [274] [Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions](https://arxiv.org/abs/2512.11836)
*Dayne R. Freudenberg,Daniel G. Haughian,Mitchell A. Klusty,Caroline N. Leach,W. Scott Black,Leslie N. Woltenberg,Rowan Hallock,Elizabeth Solie,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 本文提出了一种机器学习管道，能够从文本描述中预测全面的Food Compass Score 2.0营养评分，通过多头部神经网络处理混合特征向量，实现了从语言到可操作营养信息的转化。


<details>
  <summary>Details</summary>
Motivation: 现有营养评估系统需要详细数据，但通常无法从通俗的食品文本描述中获取，这限制了公共卫生领域的营养评估可及性。

Method: 使用多头部神经网络处理混合特征向量，结合语义文本嵌入、词汇模式、领域启发式方法以及USDA FNDDS数据，估计FCS算法所需的营养素和食品成分。

Result: 系统表现出强大的预测能力，个体营养素的R²中位数为0.81，预测的FCS与公布值强相关（Pearson's r = 0.77），平均绝对差异为14.0分。

Conclusion: 该方法能够将语言转化为可操作的营养信息，为消费者应用和研究提供可扩展的饮食评估方案，尽管对于模糊或加工食品的误差较大。

Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.

</details>


### [275] [D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe - Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space](https://arxiv.org/abs/2512.11838)
*Samarth Raina,Saksham Aggarwal,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.LG

TL;DR: DPO并非重写模型内部信念，而是通过低秩导向机制在少量偏好方向上微调激活值，实现行为对齐而非语义重构。


<details>
  <summary>Details</summary>
Motivation: 探究DPO对齐机制的实际工作原理，验证其是否真正改变模型内部信念还是仅通过表面行为调整实现对齐。

Method: 通过理论推导证明DPO梯度仅依赖于偏好与非偏好补全的logit嵌入差异，提取经验导向向量进行激活值加减实验，并进行谱分析验证低秩特性。

Result: DPO梯度仅导致最终隐藏表示的一阶偏移；添加导向向量可复现对齐行为，减去向量可恢复原始模型；上层网络呈现秩一主导和熵崩溃现象。

Conclusion: DPO产生的是行为幻觉，教会模型如何表现对齐而非改变其内在信念，对齐通过狭窄子空间实现。

Abstract: Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.

</details>


### [276] [Large Language Models as Generalist Policies for Network Optimization](https://arxiv.org/abs/2512.11839)
*Duo Wu,Linjia Kang,Zhimin Wang,Fangxin Wang,Wei Zhang,Xuefeng Tao,Wei Yang,Le Zhang,Peng Cui,Zhi Wang*

Main category: cs.LG

TL;DR: Trailblazer是首个基于大语言模型（LLM）的通用网络策略框架，通过将LLM与网络对齐方案和自适应策略协作机制结合，实现了跨任务和跨环境的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统网络优化依赖专家设计的专用策略，泛化能力差；而LLM在互联网规模语料上预训练，编码了基础网络原理，具备处理未见场景的泛化能力，为构建通用网络策略提供了变革性基础。

Method: Trailblazer框架包含网络对齐方案（将LLM与特定网络任务对接）和自适应策略协作机制（将简单控制任务从LLM卸载到轻量级策略以提高计算效率）。

Result: 通过大规模仿真和抖音真实环境在线评估，Trailblazer在跨任务和跨环境泛化方面优于传统专用策略。

Conclusion: LLM可作为通用网络策略的基础，Trailblazer是迈向通用驱动范式的第一步，能够以最小策略设计努力实现强泛化。

Abstract: Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.

</details>


### [277] [Amortized Causal Discovery with Prior-Fitted Networks](https://arxiv.org/abs/2512.11840)
*Mateusz Sypniewski,Mateusz Olko,Mateusz Gajewski,Piotr Miłoś*

Main category: cs.LG

TL;DR: 本文提出了一种基于先验拟合网络（PFNs）的摊销因果发现方法，解决了传统可微惩罚似然方法中似然估计误差导致结构学习不可靠的问题。


<details>
  <summary>Details</summary>
Motivation: 近年来，可微惩罚似然方法通过最大化因果结构对数据的似然来优化结构，但研究发现即使在大样本量下，似然估计误差也会阻碍正确结构的发现。

Method: 利用先验拟合网络（PFNs）来摊销数据依赖的似然估计，为结构学习提供更可靠的评分。

Result: 在合成、模拟和真实数据集上的实验显示，相比标准基线方法，该方法在结构恢复方面取得了显著提升。PFNs比传统神经网络方法提供更准确的似然估计。

Conclusion: PFNs方法能够有效提高因果发现的准确性，解决了传统方法中似然估计不准确的核心问题。

Abstract: In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.

</details>


### [278] [Meta-Continual Mobility Forecasting for Proactive Handover Prediction](https://arxiv.org/abs/2512.11841)
*Sasi Vardhan Reddy Mandapati*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的元连续预测框架，用于蜂窝网络中的短期移动性预测，通过GRU预测器、Reptile元初始化和EWMA残差检测器，实现了快速适应非平稳移动性模式，显著提升了切换预测性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的移动性高度非平稳，包括突然转向、速度快速变化和不可预测的用户行为，导致传统预测器漂移，引发切换时机错误或失败。

Method: 提出了一种轻量级元连续预测框架，结合GRU预测器、Reptile元初始化实现快速少样本适应，以及EWMA残差检测器仅在漂移发生时触发紧凑在线更新。

Result: 在GeoLife和DeepMIMO管道上的评估显示，零样本设置下达到4.46米ADE和7.79米FDE，10样本少样本设置下ADE提升至3.71米，从突然漂移中恢复的速度比离线GRU快2到3倍。应用于切换预测时，F1提升至0.83，AUROC达到0.90，显著减少了切换失败和乒乓事件。

Conclusion: 该模型轻量（128k参数），适用于5G和6G系统的边缘部署，有效解决了非平稳移动性预测的挑战。

Abstract: Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.

</details>


### [279] [Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach](https://arxiv.org/abs/2512.11845)
*Wenbo Du,Lingling Han,Ying Xiong,Ling Zhang,Biyue Li,Yisheng Lv,Tong Guo*

Main category: cs.LG

TL;DR: 本文提出了一种名为DTSFormer的可变形时频变换器，用于机场客流预测。该模型通过多尺度可变形分区和联合时频滤波模块，能够动态捕捉客流数据的异质模式和周期性特征。


<details>
  <summary>Details</summary>
Motivation: 现有基于固定大小补丁嵌入的Transformer模型难以有效建模机场客流的复杂异质模式，需要一种能够动态适应不同时间尺度特征的预测方法。

Method: DTSFormer包含多尺度可变形分区模块和联合时频滤波模块。输入序列通过基于窗口函数的掩码进行动态多尺度分区，然后使用频域注意力机制捕获高低频分量，最后在时域融合多频特征。

Result: 在北京首都国际机场2023年1月至2024年3月的真实客流数据上进行实验，结果表明该方法在不同预测时间范围内均优于现有最先进的预测模型。

Conclusion: 可变形分区模块能够将补丁长度与主导周期和异质趋势对齐，从而更好地捕捉突发高频波动，证明了该方法在机场客流预测中的有效性。

Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.

</details>


### [280] [Exploring Topological Bias in Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2512.11846)
*Yihan Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种用于异质图神经网络（HGNNs）的去偏方法，通过元权重和PageRank构建投影来识别拓扑偏差，并利用对比学习进行去偏。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质图神经网络的拓扑偏差，而异质图神经网络中的拓扑偏差问题尚未得到充分研究。作者发现HGNNs在不同节点类型间存在性能偏差，这限制了模型性能。

Method: 1. 使用元权重对异质图的邻接矩阵进行修改以区分不同的元关系；2. 结合PageRank和节点标签信息构建投影，将节点映射到与模型性能强相关的值；3. 基于节点映射值的差异设计去偏结构，并与原始图结构进行对比学习。

Result: 在三个公开数据集上的实验验证了所提方法的有效性，能够显著提高HGNNs的性能并有效减少拓扑偏差。

Conclusion: 该研究证明了拓扑偏差在HGNNs中的普遍存在性，并提出了一种有效的去偏方法，为提升异质图神经网络性能提供了新的思路。

Abstract: Graph Neural Networks (GNNs) are characterized by their capacity of processing graph-structured data. However, due to the sparsity of labels under semi-supervised learning, they have been found to exhibit biased performance on specific nodes. This kind of bias has been validated to correlate with topological structure and is considered as a bottleneck of GNNs' performance. Existing work focuses on the study of homogeneous GNNs and little attention has been given to topological bias in Heterogeneous Graph Neural Networks (HGNNs). In this work, firstly, in order to distinguish distinct meta relations, we apply meta-weighting to the adjacency matrix of a heterogeneous graph. Based on the modified adjacency matrix, we leverage PageRank along with the node label information to construct a projection. The constructed projection effectively maps nodes to values that strongly correlated with model performance when using datasets both with and without intra-type connections, which demonstrates the universal existence of topological bias in HGNNs. To handle this bias, we propose a debiasing structure based on the difference in the mapped values of nodes and use it along with the original graph structure for contrastive learning. Experiments on three public datasets verify the effectiveness of the proposed method in improving HGNNs' performance and debiasing.

</details>


### [281] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: TRM模型在ARC-AGI-1任务上的性能主要来自测试时增强、多数投票集成和任务标识符依赖，而非深度内部推理。递归更新效果较浅，性能在少数步骤后饱和。


<details>
  <summary>Details</summary>
Motivation: 分析TRM模型在ARC任务上的性能来源，明确其成功是源于架构优势、测试时计算还是任务特定先验。

Method: 通过测试时增强和投票分析、任务标识符消融实验、递归轨迹分析、训练阶段对比实验，以及与传统QLoRA微调的比较。

Result: 测试时增强贡献约11%性能提升；模型严格依赖任务标识符；递归更新效果浅显；增强训练扩展候选解分布；TRM在吞吐量和内存使用上优于传统方法。

Conclusion: TRM的性能主要源于效率优势、任务特定条件和激进的测试时计算，而非深度推理能力。

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [282] [KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs](https://arxiv.org/abs/2512.11851)
*Prashant Pandey*

Main category: cs.LG

TL;DR: 本文提出了一种称为token recycling的方法，通过重用先前计算好的注意力键值状态来加速相似提示的推理过程，从而扩展上下文记忆空间。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索是否可以通过重用小型LLM中为某个提示计算好的注意力键值状态，来加速对新的相似提示的推理，从而提高上下文记忆的效率。

Method: 方法包括构建过去激活的缓存，通过句子嵌入获取条目，并在缓存提示是新输入的精确前缀时重用缓存的过去键值。实验使用DialoGPT-medium模型，无需修改模型即可实现可复现性。

Result: 实验结果显示，当存在前缀重叠时，推理速度得到一致提升，且输出语义没有明显退化；当没有重叠时，行为与基线一致。

Conclusion: 结论是token recycling方法在存在前缀重叠的情况下能有效加速推理，且不影响输出质量，为LLM的推理优化提供了可行方案。

Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.

</details>


### [283] [Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things](https://arxiv.org/abs/2512.11852)
*Muhammad Jawad Bashir,Shagufta Henna,Eoghan Furey*

Main category: cs.LG

TL;DR: 本研究利用Temporal Fusion Transformer（TFT）模型实现智能温室执行器设置的自动化控制，并通过多种可解释性技术（模型内在解释、LIME和SHAP）增强决策过程的透明度和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有智能温室中的时间序列预测模型多为黑盒模型，缺乏可解释性机制，这在需要信任、透明度和监管合规的智慧农业实践中是一个关键限制。

Method: 采用TFT模型进行温室执行器设置自动化，并运用模型内在解释、LIME和SHAP等局部和全局解释技术来分析传感器数据（温度、湿度、CO2、光照和外部气候）对执行器控制决策的贡献。

Result: 训练后的TFT模型在类别不平衡数据集上实现了95%的测试准确率，结果显示不同传感器对实时温室调整具有不同程度的影响。

Conclusion: 该方法确保了决策透明度，支持自适应微调，有助于提高作物产量和资源利用效率。

Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.

</details>


### [284] [Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks](https://arxiv.org/abs/2512.11854)
*Grant King,Musa Azeem,Savannah Noblitt,Ramtin Zand,Homayoun Valafar*

Main category: cs.LG

TL;DR: 本文提出了一种基于手腕IMU的实时力量训练接近力竭状态检测系统，通过两阶段深度学习模型实现重复次数分割和力竭状态分类，为优化增肌训练提供客观反馈。


<details>
  <summary>Details</summary>
Motivation: 传统的主观重复次数储备评估不可靠，导致训练刺激不足或疲劳过度。需要一种客观、实时的力竭状态检测方法来优化增肌训练效果。

Method: 使用单手腕IMU采集数据，采用两阶段管道：ResNet模型实时分割重复动作，然后结合LSTM历史上下文和卷积特征对接近力竭状态进行分类。

Result: 在13名参与者的631次重复数据上，分割模型F1得分0.83，力竭分类器F1得分0.82。在树莓派5上平均延迟112ms，iPhone 16上23.5ms，满足实时部署要求。

Conclusion: 该系统证明了使用最小化硬件实现客观、实时训练强度反馈的可行性，为可访问的AI驱动增肌教练工具铺平了道路。

Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.

</details>


### [285] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来比较精确对称性和近似对称性在机器学习中的成本，发现近似对称性只需要对数级别的平均复杂度，而精确对称性需要线性复杂度，这为实践中优先使用近似对称性提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 在科学应用中，强制实施精确对称性通常能带来显著收益，但最近的研究表明近似对称性可能提供更大的灵活性和鲁棒性。然而，缺乏对这两种方法成本的理论比较。

Method: 引入了平均复杂度框架来量化通过平均实施对称性的成本，并在标准条件下分析了精确对称性和近似对称性的复杂度要求。

Result: 主要结果是发现指数级分离：精确对称性需要线性平均复杂度，而近似对称性只需要对数平均复杂度。

Conclusion: 这是首次在理论上分离精确对称性和近似对称性的研究，正式证明了为什么在实践中近似对称性可能更优，相关工具和技术对更广泛的机器学习对称性研究具有独立价值。

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [286] [GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search](https://arxiv.org/abs/2512.11856)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Zhi Yang,Weisheng Zhao,Chunming Hu*

Main category: cs.LG

TL;DR: GCoDE是首个面向图神经网络在设备-边缘层次上的架构-映射协同设计与部署的自动框架，通过将设备通信过程抽象为显式操作，实现了架构和映射方案的统一优化，能够实现高达44.9倍的加速和98.2%的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在边缘设备上推理效率低的问题，因为GNN的高计算成本和边缘设备有限的硬件资源无法满足实时性和能耗要求，而传统的模型分区方法对GNN无效。

Method: 提出GCoDE框架，将设备通信过程抽象为显式操作，融合架构和映射方案进行联合优化；引入能耗预测方法提高评估精度；采用基于约束的随机搜索策略在1.5小时内找到最优解；集成协同推理引擎实现高效部署。

Result: 实验结果显示，GCoDE相比现有方法在不同应用和系统配置下可实现高达44.9倍的加速和98.2%的能耗降低。

Conclusion: GCoDE通过架构-映射协同设计有效解决了GNN在边缘设备上的推理效率问题，为GNN在边缘计算场景的实际应用提供了可行的解决方案。

Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.

</details>


### [287] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

TL;DR: TopicProphet是一个新颖的框架，通过分析历史时期中相似的公众情感趋势和历史背景来改进股票预测，解决了传统方法因数据不足和市场快速变化导致的预测困难。


<details>
  <summary>Details</summary>
Motivation: 传统股票预测方法因定量数据缺乏因果逻辑和市场快速变化导致训练数据不足而效果有限，需要一种能够捕捉历史时期相似模式的新方法。

Method: 采用主题建模、时间分析、断点检测和分段优化等一系列技术，识别具有相似公众情感趋势和历史背景的最优训练时间段。

Result: TopicProphet相比现有最先进方法在预测金融百分比变化方面产生了更好的结果，能够更有效地捕捉最优训练数据。

Conclusion: 该研究证明通过分析历史时期的相似性来优化训练数据选择，可以显著提高股票预测的准确性，为金融预测提供了新的有效途径。

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [288] [Adaptive Path Integral Diffusion: AdaPID](https://arxiv.org/abs/2512.11858)
*Michael Chertkov,Hamidreza Behjoo*

Main category: cs.LG

TL;DR: 本文提出了基于路径的调度选择框架，用于谐波路径积分扩散中的时变刚度控制，通过分段恒定参数化和分层细化来优化采样质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散采样器在终端时间匹配目标，但中间时间动态的调度选择具有重要价值。本文旨在开发一种系统方法来选择最优调度，以提高采样质量。

Method: 采用分段恒定参数化方法，结合分层细化策略，为谐波路径积分扩散设计时间变化的刚度调度。引入调度敏感的采样质量诊断指标，并在高斯混合目标下保持闭式格林函数比和数值稳定的预测状态映射。

Result: 在2D实验中，基于采样质量驱动的分段恒定调度显著改善了早期退出保真度、尾部精度、动态条件性以及标签选择时机，在固定积分预算下表现出优越性能。

Conclusion: 提出的调度选择框架能够有效提升扩散采样器的性能，特别是在复杂目标分布下，通过优化中间时间动态调度可以获得更好的采样效果。

Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.

</details>


### [289] [Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion](https://arxiv.org/abs/2512.11859)
*Michael Chertkov*

Main category: cs.LG

TL;DR: GH-PID是一个线性可解的引导随机最优传输框架，具有硬终端分布和软路径成本，通过低维引导协议保持解析结构，支持稳定采样和可微协议学习。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够精确匹配终端分布、同时通过引导协议优化路径成本的随机最优传输框架，使其在保持解析可解性的同时具有实际应用价值。

Method: 采用线性可解的路径积分扩散方法，构建前向和后向Kolmogorov方程保持线性的框架，使用高斯混合模型终端分布获得闭式解，开发引导协议诊断指标。

Result: 在2D导航场景中验证了方法的有效性：几何和刚度影响轨迹滞后和曲率效应；单任务协议学习优化路径成本；多专家融合实现共识协议生成。

Conclusion: GH-PID能够生成几何感知、可信赖的轨迹，在满足规定终端分布的同时系统性地降低积分成本，为经验随机最优传输提供了可解释的变分近似。

Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.

</details>


### [290] [An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes](https://arxiv.org/abs/2512.11860)
*Yuelian Li,Andrew Rushing Hands*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理约束的图神经网络OCGNN-PINN，用于在非规则网格上稳定求解偏微分方程，特别适用于多物理场相互作用场景。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在规则网格上高效求解PDE，但在不规则区域易失稳。实际应用中，扩散、损伤和愈合等多物理相互作用常发生在不规则网格上，需要开发能在非规则域上稳定求解PDE的方法。

Method: 开发了算子一致图神经网络(OCGNN-PINN)，通过节点-边消息传递和一致性损失，利用图关联矩阵强制梯度-散度关系，确保离散节点和边动力学在时间推进过程中保持结构耦合。

Result: 在物理驱动演化网格和真实扫描表面上的扩散过程评估表明，相比图卷积和多层感知器基线，模型具有更好的时间稳定性和预测精度，接近Crank-Nicolson求解器在非结构化域上的性能。

Conclusion: OCGNN-PINN方法能够有效处理不规则网格上的PDE求解问题，为多物理场相互作用提供了稳定的数值求解方案。

Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.

</details>


### [291] [Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL](https://arxiv.org/abs/2512.11862)
*Jiahao You,Ziye Jia,Can Cui,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.LG

TL;DR: 本文针对低空智能网络(LAINs)中的能量受限无人机、随机任务到达和异构计算资源等挑战，提出了一个集成空地协作网络，通过分层学习框架优化无人机轨迹规划和任务卸载决策。


<details>
  <summary>Details</summary>
Motivation: 低空智能网络在动态和基础设施有限的环境中具有巨大潜力，但面临无人机能量受限、任务到达随机性和计算资源异构性等关键挑战，需要有效的解决方案来支持关键任务应用。

Method: 提出分层学习框架：在大时间尺度上使用VCG拍卖机制进行能量感知和激励相容的轨迹分配；在小时间尺度上提出扩散异构智能体近端策略优化算法，将潜在扩散模型嵌入到行动者网络中，通过高斯先验采样和观测条件去噪增强适应性和策略多样性。

Result: 大量仿真实验表明，该框架在能量效率、任务成功率和收敛性能方面优于基线方法。

Conclusion: 所提出的集成空地协作网络和分层学习框架有效解决了LAINs中的关键挑战，为动态环境下的边缘智能应用提供了可行的解决方案。

Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.

</details>


### [292] [Phase transitions reveal hierarchical structure in deep neural networks](https://arxiv.org/abs/2512.11866)
*Ibrahim Talha Ersoy,Andrés Fernando Cardozo Licha,Karoline Wiesner*

Main category: cs.LG

TL;DR: 该论文提出了一个统一框架，将深度神经网络训练中的相变、鞍点和模式连通性现象联系起来，揭示了这些现象都源于损失和误差景观的几何特性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中的许多现象（如相变、鞍点普遍存在、模式连通性）仍然缺乏统一的理论解释，作者旨在从几何角度为这些现象提供统一的理论框架。

Method: 作者引入了一个简单快速的算法，利用L2正则化器作为工具来探测误差景观的几何特性，并在MNIST数据集上验证模式连通性，通过数值模拟证明鞍点诱导模型间转换。

Result: 研究证实了DNNs中的模式连通性，并发现鞍点诱导了编码不同数字类别的模型之间的转换，揭示了精度盆地的层次结构类似于统计物理中的相。

Conclusion: 该工作建立了DNNs关键训练现象的几何起源，揭示了类似于统计物理中相的精度盆地层次结构，为理解深度神经网络训练提供了新的几何视角。

Abstract: Training Deep Neural Networks relies on the model converging on a high-dimensional, non-convex loss landscape toward a good minimum. Yet, much of the phenomenology of training remains ill understood. We focus on three seemingly disparate observations: the occurrence of phase transitions reminiscent of statistical physics, the ubiquity of saddle points, and phenomenon of mode connectivity relevant for model merging. We unify these within a single explanatory framework, the geometry of the loss and error landscapes. We analytically show that phase transitions in DNN learning are governed by saddle points in the loss landscape. Building on this insight, we introduce a simple, fast, and easy to implement algorithm that uses the L2 regularizer as a tool to probe the geometry of error landscapes. We apply it to confirm mode connectivity in DNNs trained on the MNIST dataset by efficiently finding paths that connect global minima. We then show numerically that saddle points induce transitions between models that encode distinct digit classes. Our work establishes the geometric origin of key training phenomena in DNNs and reveals a hierarchy of accuracy basins analogous to phases in statistical physics.

</details>


### [293] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

TL;DR: 该论文研究了在持续学习中使用合成数据进行重复训练会导致性能下降和分布漂移的问题，揭示了合成数据引入显著偏差和方差，削弱最大似然估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在模型训练中的广泛应用，重复使用合成数据进行训练引发了分布漂移和性能退化的担忧，需要系统研究这种自举过程的影响。

Method: 通过持续学习的视角分析，将问题与生成式经验回放方法相关联，进行统计分析证明合成数据引入偏差和方差，并通过实证研究展示流行生成模型在合成数据重复训练下的崩溃现象。

Result: 实证证据表明流行生成模型在合成数据重复训练下会发生崩溃，最先进的GER方法无法保持潜在空间的对齐，性能出现显著退化。

Conclusion: 研究结果对在持续学习中使用合成数据提出了严重警告，揭示了合成数据重复训练带来的根本性挑战。

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [294] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

TL;DR: 该论文提出了一种基于ICE曲线的全局敏感性度量方法，以解决传统PDP方法在存在强交互作用时可能产生误导的问题。


<details>
  <summary>Details</summary>
Motivation: 在航空航天设计等工程应用中，理解输入变量对数据驱动模型的影响至关重要。传统的PDP方法在存在强交互作用时，其全局敏感性度量可能产生误导，因为平均化会掩盖交互效应。

Method: 提出基于个体条件期望(ICE)曲线的全局敏感性度量方法，计算ICE曲线上的期望特征重要性及其标准差，以更有效地捕捉交互作用的影响。同时引入ICE相关值来量化交互作用如何改变输入与输出之间的关系。

Result: 在三个案例（5变量分析函数、5变量风力涡轮机疲劳问题、9变量翼型空气动力学案例）的对比评估中，ICE敏感性方法相比PDP、SHAP和Sobol指数提供了更丰富的见解。

Conclusion: ICE特征重要性比传统PDP方法提供更丰富的洞察，而PDP、ICE和SHAP的可视化解释通过提供多重视角相互补充。

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [295] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

TL;DR: 本文提出了"神经变色龙"模型，通过微调使模型能够零样本逃避激活监控器，即使面对安全相关的概念也能成功规避检测。


<details>
  <summary>Details</summary>
Motivation: 测试激活监控在最坏情况下的鲁棒性，特别是在模型可能主动隐藏内部状态的错位威胁模型下，验证模型是否能够学会逃避未知的激活监控器。

Method: 通过微调LLM，使其在特定触发词（如"You are being probed for {concept}"）条件下逃避对良性概念（如语言、HTML）的监控，并验证该机制能否推广到安全相关概念。

Result: 成功创建了能够零样本逃避激活监控的模型，该逃避行为具有高度选择性，仅针对触发词中提到的特定概念，且对模型标准基准能力影响较小。

Conclusion: 这项工作为监控技术在最坏情况错位威胁模型下的鲁棒性评估提供了概念验证和工具，表明现有监控方法在面对主动规避时存在脆弱性。

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [296] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

TL;DR: 该论文提出了一种基于强化学习的上下文生成框架，通过从用户提示中提取上下文信息来指导LLM生成更安全、更合适的响应，有效减少有害输出并提高良性请求的合规性。


<details>
  <summary>Details</summary>
Motivation: 传统LLM框架在生成即时响应时忽略了用户意图、先验知识和风险因素等上下文信息，这可能导致对模糊请求的错误解读——要么产生不安全输出，要么过度拒绝良性请求。

Method: 设计了一种基于强化学习的自动编码器式上下文生成器，该模型能够从用户提示中推断出基于实际语境的上下文信号，并用这些信号来指导响应生成过程。

Result: 在SafetyInstruct数据集上，该方法平均减少有害响应5.6%；在XSTest和WildJailbreak数据集上，攻击成功率和良性提示合规率的调和平均值提高了6.2%。

Conclusion: 通过提取和利用上下文信息，可以有效提高LLM推理的安全性和可靠性，特别是在处理模糊请求的安全任务中表现出显著优势。

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [297] [EnviroLLM: Resource Tracking and Optimization for Local AI](https://arxiv.org/abs/2512.12004)
*Troy Allen*

Main category: cs.LG

TL;DR: EnviroLLM是一个开源工具包，用于跟踪、基准测试和优化在个人设备上运行大型语言模型时的性能和能耗。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型越来越多地在本地部署以满足隐私和可访问性需求，用户缺乏工具来测量其资源使用、环境影响和效率指标。

Method: 系统提供实时进程监控、跨多个平台的基准测试、持久化存储和可视化分析，以及个性化模型和优化建议，同时包含LLM-as-judge评估和能耗速度指标。

Result: EnviroLLM使用户能够评估在测试自定义提示时质量与效率之间的权衡关系。

Conclusion: 该工具包为本地部署的LLM提供了全面的性能和能耗监控解决方案。

Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.

</details>


### [298] [DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning](https://arxiv.org/abs/2512.12022)
*Kaichuang Zhang,Wei Yin,Jinghao Yang,Ping Xu*

Main category: cs.LG

TL;DR: DFedReweighting是一个去中心化联邦学习的统一聚合框架，通过目标导向的加权聚合实现公平性和鲁棒性等多样化目标


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习虽然避免了中心服务器的单点故障风险，但仍面临公平性、鲁棒性等挑战，需要一种能够同时实现多种目标的统一框架

Method: 提出DFedReweighting框架，在每轮学习的最后一步进行目标导向的加权聚合：首先基于辅助数据集计算初步权重，然后通过定制化重加权策略优化权重，确保线性收敛

Result: 理论分析证明框架能够线性收敛，实验结果表明在多种场景下显著提高了公平性和对拜占庭攻击的鲁棒性

Conclusion: 通过选择合适的性能指标和重加权策略，该框架能够实现广泛的期望学习目标，为去中心化联邦学习提供了有效的统一解决方案

Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.

</details>


### [299] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

TL;DR: 该论文提出了基于Eikonal PDE的Eik-QRL方法，将目标条件强化学习重新构建为连续时间形式，无需轨迹信息，仅需采样状态和目标，并改进了分布外泛化能力。针对复杂动态的局限性，进一步提出了分层分解的Eik-HiQRL方法。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习通过将任务定义为目标达成而非最大化人工设计的奖励信号来缓解奖励设计的难度。最优目标条件值函数自然形成拟度量，这启发了拟度量强化学习的研究。

Method: 提出了Eikonal约束的拟度量强化学习（Eik-QRL），基于Eikonal偏微分方程的连续时间重构，使方法无需轨迹信息，仅需采样状态和目标。针对复杂动态问题，进一步提出了分层分解的Eik-HiQRL方法。

Result: Eik-HiQRL在离线目标条件导航任务中达到了最先进的性能，在操作任务中相比QRL取得了持续提升，与时间差分方法性能相当。

Conclusion: 基于Eikonal PDE的连续时间重构为拟度量强化学习提供了理论保证，分层分解方法有效解决了复杂动态环境下的挑战，在多个任务中表现出优越性能。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [300] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

TL;DR: 本文挑战了当前大语言模型安全评估中单次测试的假设，通过研究不同随机种子和温度设置下安全拒绝决策的稳定性，发现18-28%的提示会出现决策翻转，表明单次安全评估不足以可靠评估模型安全性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的安全评估主要依赖单次测试，隐含假设模型响应是确定性的且能代表模型的安全对齐程度。本文旨在验证这一假设的可靠性。

Method: 测试了来自三个家族的四个指令调优模型（Llama 3.1 8B、Qwen 2.5 7B、Qwen 3 8B、Gemma 3 12B），在876个有害提示上使用20种不同采样配置（4个温度×5个随机种子），并开发了安全稳定性指数（SSI）来量化决策稳定性。

Result: 发现18-28%的提示在不同配置下会出现决策翻转（模型在某些配置下拒绝，在其他配置下遵守）。温度越高，决策稳定性显著降低（SSI从温度0.0时的0.951降至温度1.0时的0.896）。单次评估与多样本真实结果的一致性仅为92.4%。

Conclusion: 单次安全评估不足以可靠评估模型安全性，建议每个提示至少使用3个样本进行安全评估。

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [301] [Physics-informed neural networks to solve inverse problems in unbounded domains](https://arxiv.org/abs/2512.12074)
*Gregorio Pérez-Bernal,Oscar Rincón-Cardeño,Silvana Montoya-Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

TL;DR: 该论文提出了一种解决无限和半无限域逆问题的新方法，结合负指数和正态分布的采样策略以及双网络架构，在PINN和PIKAN框架下实现，结果表明PINN在此场景下比PIKAN更准确且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络在无限和半无限域逆问题求解中存在边界条件处理困难的问题，需要开发无需显式施加边界条件的新方法。

Method: 提出基于负指数和正态分布的新型采样策略，采用双网络架构同时学习方程解和参数，使用相同损失函数训练，无需显式施加边界条件。

Result: PINN比PIKAN求解速度快1000倍，相对误差更小，在噪声环境下表现更稳健。

Conclusion: 在无限和半无限域逆问题中，PINN相比PIKAN具有更好的准确性和计算效率优势，为相关应用提供了更优的解决方案。

Abstract: Inverse problems are extensively studied in applied mathematics, with applications ranging from acoustic tomography for medical diagnosis to geophysical exploration. Physics informed neural networks (PINNs) have emerged as a powerful tool for solving such problems, while Physics informed Kolmogorov Arnold networks (PIKANs) represent a recent benchmark that, in certain problems, promises greater interpretability and accuracy compared to PINNs, due to their nature, being constructed as a composition of polynomials. In this work, we develop a methodology for addressing inverse problems in infinite and semi infinite domains. We introduce a novel sampling strategy for the network's training points, using the negative exponential and normal distributions, alongside a dual network architecture that is trained to learn the solution and parameters of an equation with the same loss function. This design enables the solution of inverse problems without explicitly imposing boundary conditions, as long as the solutions tend to stabilize when leaving the domain of interest. The proposed architecture is implemented using both PINNs and PIKANs, and their performance is compared in terms of accuracy with respect to a known solution as well as computational time and response to a noisy environment. Our results demonstrate that, in this setting, PINNs provide a more accurate and computationally efficient solution, solving the inverse problem 1,000 times faster and in the same order of magnitude, yet with a lower relative error than PIKANs.

</details>


### [302] [SigTime: Learning and Visually Explaining Time Series Signatures](https://arxiv.org/abs/2512.12076)
*Yu-Chia Huang,Juntong Chen,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.LG

TL;DR: 本文提出了一个联合训练两个Transformer模型的学习框架，结合形状表示和传统特征工程来发现时间序列模式，并开发了SigTime视觉分析系统进行多角度探索。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模式发现方法存在计算复杂度高、可解释性差、难以捕捉有意义时间结构等问题，特别是在生物医学研究中，发现生理信号中有意义的模式对改善诊断和患者预后至关重要。

Method: 联合训练两个Transformer模型：一个使用形状基表示捕捉局部时间结构，另一个使用传统特征工程编码统计特性；学习到的形状作为可解释的签名来区分不同类别的时间序列；开发SigTime视觉分析系统，通过协调视图从多个角度探索时间序列签名。

Result: 在8个公开数据集和1个专有临床数据集上进行了定量评估；通过两个使用场景（公共ECG数据和早产分析）与领域专家合作证明了系统的有效性。

Conclusion: 该框架成功解决了时间序列模式发现的挑战，提供了可解释的签名和有效的可视化工具，在生物医学等领域具有重要应用价值。

Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.

</details>


### [303] [CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation](https://arxiv.org/abs/2512.12086)
*Xin Yang,Omid Ardakanian*

Main category: cs.LG

TL;DR: Cloak是一个基于潜在扩散模型的数据混淆框架，通过对比学习提取解耦表示，在保护隐私的同时保持数据实用性，特别适合资源受限的移动物联网设备部署。


<details>
  <summary>Details</summary>
Motivation: 现有数据混淆方法需要修改下游任务、难以实现满意的隐私-效用权衡，或计算密集不适合资源受限设备。

Method: 使用对比学习提取解耦表示，指导潜在扩散过程保留有用信息同时隐藏隐私信息，支持用户根据隐私需求灵活调整权衡。

Result: 在四个公共时间序列数据集和面部图像数据集上的实验表明，Cloak持续优于最先进的混淆技术，且适合资源受限环境部署。

Conclusion: Cloak框架通过解耦表示和潜在扩散模型，有效解决了隐私保护与数据效用之间的权衡问题，为移动物联网设备提供了实用的数据混淆解决方案。

Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.

</details>


### [304] [GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes](https://arxiv.org/abs/2512.12091)
*Mohammad Pivezhandi,Mahdi Banisharif,Saeed Bakhshan,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.LG

TL;DR: GraphPerf-RT是一种针对异构嵌入式SoC上OpenMP工作负载的性能预测方法，通过统一任务DAG拓扑、代码语义和运行时上下文，实现高精度预测和风险感知调度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理异构嵌入式SoC上的OpenMP工作负载时面临挑战：经典启发式方法在负载不规则性下表现不佳，表格回归器丢弃结构信息，无模型强化学习可能导致设备过热。

Method: GraphPerf-RT将任务DAG拓扑、CFG导出的代码语义和运行时上下文（每核DVFS、热状态、利用率）统一到异构图表示中，使用多任务证据头预测多个性能指标，并集成到强化学习方法中进行调度。

Result: 在三个嵌入式ARM平台上验证，R² > 0.95，不确定性校准良好（ECE < 0.05）。与模型无关基线相比，MAMBRL-D3QN结合GraphPerf-RT实现了66%的制造时间减少和82%的能量减少。

Conclusion: 准确且不确定性感知的替代模型能够在热约束嵌入式系统上实现有效的基于模型的规划，显著提升性能并降低能耗。

Abstract: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache
  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks
  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core
  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict
  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.
  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To
  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),
  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the
  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,
  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.

</details>


### [305] [Neural CDEs as Correctors for Learned Time Series Models](https://arxiv.org/abs/2512.12116)
*Muhammad Bilal Shahid,Prajwal Koirla,Cody Fleming*

Main category: cs.LG

TL;DR: 提出了一种预测器-校正器机制，通过神经网络控制的微分方程来校正预测器的误差，提高时间序列预测性能


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列模型在生成多步预测时容易产生累积误差，无论是直接预测还是迭代预测方法都存在这个问题

Method: 使用预测器（任何学习的时间序列模型）生成预测，然后通过校正器（神经控制的微分方程）预测并修正预测误差，还引入了两种正则化策略来提升外推性能

Result: 在合成、物理模拟和真实世界数据集上的实验表明，预测器-校正器机制相比单独使用预测器能持续提升性能

Conclusion: 该方法能有效处理不规则采样时间序列，适用于连续和离散时间预测器，显著改善了时间序列预测的准确性

Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.

</details>


### [306] [MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models](https://arxiv.org/abs/2512.12121)
*Ahmad Chamma,Omar El Herraoui,Guokan Shang*

Main category: cs.LG

TL;DR: MixtureKit是一个模块化开源框架，用于构建、训练和分析专家混合模型，支持三种方法：传统MoE、BTX和BTS，并提供可视化界面和实验验证。


<details>
  <summary>Details</summary>
Motivation: 为研究和开发MoE系统提供一个实用的基础框架，支持从任意预训练或微调模型构建MoE模型，促进跨领域应用。

Method: 支持三种方法：(i)传统MoE：每个transformer块使用单个路由器选择专家；(ii)BTX：为每个指定子层引入单独路由器实现细粒度令牌路由；(iii)BTS：保持专家完整并引入可训练缝合层控制中心与专家间信息交换。

Result: 在多语言代码切换数据上的实验表明，基于MixtureKit训练的BTX模型在多个基准测试中优于基线密集模型。

Conclusion: MixtureKit作为一个实用的MoE系统研发基础框架发布，支持多样化领域的应用。

Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.

</details>


### [307] [High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees](https://arxiv.org/abs/2512.12122)
*Elynn Chen,Yuefeng Han,Jiayu Li*

Main category: cs.LG

TL;DR: 该论文提出了基于CP低秩结构的张量判别分析方法(CP-TDA)，用于处理高维张量数据的分类问题，特别是在小样本高维场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有张量分类方法依赖稀疏性或Tucker结构，缺乏理论保证。研究发现判别信号集中在少数多重线性分量上，因此引入CP低秩结构来建模判别张量。

Method: 在张量高斯混合模型下，提出CP-TDA方法，包含随机复合PCA初始化(处理依赖性和各向异性噪声)和迭代优化算法。还开发了半参数张量判别模型，通过深度生成模型将非正态张量映射到适合CP-TDA的潜在空间。

Result: 建立了全局收敛性和极小极大最优误分类率。在图形分类等实际数据中，相比现有张量分类器和图神经网络，特别是在高维小样本情况下取得了显著提升。

Conclusion: CP-TDA方法为高维张量分类提供了新的建模视角，具有理论保证和实际应用价值，特别适用于现代应用中从神经网络学到的张量表示。

Abstract: High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates.
  To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.

</details>


### [308] [BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models](https://arxiv.org/abs/2512.12131)
*Zhengyang Wang,Ziyue Liu,Ruijie Zhang,Avinash Maurya,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang*

Main category: cs.LG

TL;DR: BOOST是一个针对大规模低秩瓶颈架构的高效训练框架，通过引入瓶颈感知张量并行性和多项优化技术，显著提升了训练速度和GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 传统的全秩方法在Transformer模型预训练中存在计算和通信成本高的问题，而低秩瓶颈架构虽然算法效率高，但在标准张量并行下扩展性差，导致通信开销大和GPU利用率低。

Method: 提出BOOST框架，包括：1）瓶颈感知张量并行性；2）在线RMSNorm优化；3）线性层分组；4）低秩激活检查点技术。

Result: 在不同低秩瓶颈架构上的评估显示，BOOST相比全秩模型基线实现了1.46-1.91倍的加速，相比简单集成3D并行的低秩模型实现了1.87-2.27倍的加速，同时提高了GPU利用率并减少了通信开销。

Conclusion: BOOST框架有效解决了低秩瓶颈架构在大规模训练中的扩展性问题，为高效的大规模Transformer预训练提供了可行的解决方案。

Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.

</details>


### [309] [On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency](https://arxiv.org/abs/2512.12132)
*Koffi O. Ayena*

Main category: cs.LG

TL;DR: 本文证明SiLU激活网络在逼近光滑函数时具有指数逼近率，相比ReLU网络具有更优的复杂度控制。通过新颖的分层构造方法，实现了更紧凑的平方函数逼近，并将结果扩展到Sobolev类函数的深度逼近。


<details>
  <summary>Details</summary>
Motivation: 研究SiLU激活函数在神经网络逼近理论中的优势，特别是与经典ReLU网络相比在逼近效率和复杂度控制方面的改进潜力。

Method: 开发了一种新颖的分层构造方法：首先高效逼近平方函数x²，相比Yarotsky的ReLU实现更紧凑；然后通过函数复合将这种方法扩展到深度SiLU网络对Sobolev类函数的逼近。

Result: 获得了逼近误差以O(ω^{-2k})速率衰减的网络构造，网络深度为O(1)；对于Sobolev类函数，实现了总深度O(1)和规模O(ε^{-d/n})的尖锐逼近界限。

Conclusion: SiLU激活网络在函数逼近方面相比ReLU网络具有显著优势，能够以更紧凑的网络结构实现指数级逼近速率，为深度学习理论提供了新的理论支撑。

Abstract: This article establishes a comprehensive theoretical framework demonstrating that SiLU (Sigmoid Linear Unit) activation networks achieve exponential approximation rates for smooth functions with explicit and improved complexity control compared to classical ReLU-based constructions. We develop a novel hierarchical construction beginning with an efficient approximation of the square function $x^2$ more compact in depth and size than comparable ReLU realizations, such as those given by Yarotsky. This construction yields an approximation error decaying as $\mathcal{O}(ω^{-2k})$ using networks of depth $\mathcal{O}(1)$. We then extend this approach through functional composition to establish sharp approximation bounds for deep SiLU networks in approximating Sobolev-class functions, with total depth $\mathcal{O}(1)$ and size $\mathcal{O}(\varepsilon^{-d/n})$.

</details>


### [310] [BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity](https://arxiv.org/abs/2512.12135)
*Lucine L. Oganesian,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文提出了一种新的多区域颅内脑电记录时空变换器模型，通过可调节空间尺度的标记编码和掩码重建任务，探索了空间尺度对自监督预训练和下游解码性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于变换器的神经基础模型在处理多区域颅内记录时，面临如何最佳编码空间信息以及设计自监督任务来学习脑网络模式的关键问题。这些记录在从单通道到脑区级别的不同空间尺度上表现出复杂的时空交互作用。

Method: 提出了一种新的时空变换器模型和对应的自监督掩码潜在重建任务，允许灵活调整标记编码和掩码的空间尺度。在公开的多区域颅内脑电数据上应用该模型。

Result: 研究发现调整标记编码和掩码重建的空间尺度显著影响下游解码性能；较大空间尺度的编码比常用的通道级编码能提升下游解码性能；方法支持区域级标记编码同时保持准确的通道级神经重建。

Conclusion: 该建模框架能够探索标记编码和掩码的空间尺度，揭示了它们在多区域人脑活动神经基础模型自监督预训练中的重要性，并提升了下游解码性能。

Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.

</details>


### [311] [HydroDiffusion: Diffusion-Based Probabilistic Streamflow Forecasting with a State Space Backbone](https://arxiv.org/abs/2512.12183)
*Yihan Wang,Annan Yu,Lujun Zhang,Charuleka Varadharajan,N. Benjamin Erichson*

Main category: cs.LG

TL;DR: HydroDiffusion是一个基于扩散模型的概率性径流预测框架，采用解码器状态空间模型作为骨干网络，能够联合去噪多日轨迹，确保时间一致性并减少误差累积。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型依赖LSTM骨干网络和单步训练目标，限制了捕捉长距离依赖关系的能力，且难以生成跨预报时间的一致轨迹。

Method: 开发了HydroDiffusion框架，使用解码器状态空间模型作为骨干，在单次前向传播中联合去噪完整多日轨迹，避免自回归预测中的误差积累。

Result: 在CAMELS数据集的531个美国流域评估显示，HydroDiffusion在观测气象驱动下具有强现时预报精度，并在整个模拟时间范围内保持稳定性能，在确定性预报和概率预报方面均优于DRUM模型。

Conclusion: HydroDiffusion为中期径流概率预测提供了一个稳健的生成建模框架，建立了新的建模基准，并为大陆尺度概率水文预测的未来研究奠定了基础。

Abstract: Recent advances have introduced diffusion models for probabilistic streamflow forecasting, demonstrating strong early flood-warning skill. However, current implementations rely on recurrent Long Short-Term Memory (LSTM) backbones and single-step training objectives, which limit their ability to capture long-range dependencies and produce coherent forecast trajectories across lead times. To address these limitations, we developed HydroDiffusion, a diffusion-based probabilistic forecasting framework with a decoder-only state space model backbone. The proposed framework jointly denoises full multi-day trajectories in a single pass, ensuring temporal coherence and mitigating error accumulation common in autoregressive prediction. HydroDiffusion is evaluated across 531 watersheds in the contiguous United States (CONUS) in the CAMELS dataset. We benchmark HydroDiffusion against two diffusion baselines with LSTM backbones, as well as the recently proposed Diffusion-based Runoff Model (DRUM). Results show that HydroDiffusion achieves strong nowcast accuracy when driven by observed meteorological forcings, and maintains consistent performance across the full simulation horizon. Moreover, HydroDiffusion delivers stronger deterministic and probabilistic forecast skill than DRUM in operational forecasting. These results establish HydroDiffusion as a robust generative modeling framework for medium-range streamflow forecasting, providing both a new modeling benchmark and a foundation for future research on probabilistic hydrologic prediction at continental scales.

</details>


### [312] [MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching](https://arxiv.org/abs/2512.12198)
*Jirui Jin,Cheng Zeng,Pawan Prakash,Ellad B. Tadmor,Adrian Roitberg,Richard G. Hennig,Stefano Martiniani,Mingjie Liu*

Main category: cs.LG

TL;DR: 本文提出了一种混合引导策略，将先进的引导方法（如无分类器引导、自动引导和模型引导）整合到SE(3)-等变流匹配框架中，用于条件分子生成，在QM9和QMe14S数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 条件分子生成的关键目标包括确保化学有效性、使生成分子与目标属性对齐、促进结构多样性以及实现高效采样。计算机视觉领域的新引导策略可以适配这些目标。

Method: 提出混合引导策略，分别引导连续和离散分子模态（分别作用于速度场和预测logits），同时通过贝叶斯优化联合优化其引导尺度。在SE(3)-等变流匹配过程中整合了无分类器引导、自动引导和模型引导。

Result: 在QM9和QMe14S数据集上的基准测试显示，该方法在从头分子生成的属性对齐方面达到了最先进的性能，生成的分子具有高结构有效性。

Conclusion: 系统比较了各种引导方法的优势和局限性，为它们的更广泛应用提供了见解。

Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.

</details>


### [313] [EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training](https://arxiv.org/abs/2512.12210)
*Yuting Tang,Weibang Jiang,Shanglin Li,Yong Li,Chenyu Liu,Xinliang Zhou,Yi Ding,Cuntai Guan*

Main category: cs.LG

TL;DR: EEG-DLite是一个数据蒸馏框架，通过选择性去除大型EEG数据集中的噪声和冗余样本，实现更高效的预训练，仅需5%的数据就能达到全量数据训练的效果。


<details>
  <summary>Details</summary>
Motivation: 大规模EEG基础模型训练资源消耗大，且EEG数据存在质量和冗余问题，需要更高效的预训练方法。

Method: 使用自监督自动编码器将EEG片段编码为紧凑的潜在表示，基于这些表示过滤异常值和最小化冗余，保留多样性的信息子集。

Result: 在2500小时数据集中仅使用5%经过EEG-DLite处理的数据进行训练，在多个下游任务中性能与全量数据训练相当甚至更好。

Conclusion: EEG-DLite为生理基础建模提供了可扩展且实用的路径，是首个系统研究EEG基础模型预训练数据蒸馏的工作。

Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.

</details>


### [314] [Optimized Learned Count-Min Sketch](https://arxiv.org/abs/2512.12252)
*Kyosuke Nishishita,Atsuki Sato,Yusuke Matsui*

Main category: cs.LG

TL;DR: OptLCMS是一种优化的学习型Count-Min草图，通过分区和动态规划优化参数，在保持相同内存使用的情况下提供理论保证并加快构建速度。


<details>
  <summary>Details</summary>
Motivation: 传统的学习型Count-Min草图（LCMS）虽然能减少估计误差，但存在构建速度慢、缺乏理论保证的问题，需要优化解决。

Method: 将输入域分区并为每个分区分配独立的CMS实例，通过动态规划和近似可行性检查优化阈值参数，减少经验验证需求。

Result: 实验表明OptLCMS构建速度更快，不可容忍错误概率更低，同时保持了与LCMS相当的估计精度。

Conclusion: OptLCMS在提供理论保证的同时实现了更快的构建速度和更好的错误控制，为实际应用提供了更高的灵活性。

Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.

</details>


### [315] [GRC-Net: Gram Residual Co-attention Net for epilepsy prediction](https://arxiv.org/abs/2512.12273)
*Bihao You,Jiping Cui*

Main category: cs.LG

TL;DR: 本文提出了一种基于Gram Matrix的3D表示和multi-level特征提取的癫痫预测方法，在BONN数据集上达到93.66%的准确率


<details>
  <summary>Details</summary>
Motivation: 传统方法对EEG信号进行1D处理，无法充分建模信号间关系；同时EEG数据存在局部和全局信号不平衡的问题

Method: 使用Gram Matrix将EEG信号转换为3D表示，结合coattention机制提取全局特征和inception结构处理局部信号，实现多粒度特征提取

Result: 在BONN数据集最困难的五分类任务上，GRC-Net模型达到93.66%的准确率，优于现有方法

Conclusion: 提出的3D表示和multi-level特征提取方法能有效提升癫痫预测性能，证明了该方法在EEG信号分析中的有效性

Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.

</details>


### [316] [Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model](https://arxiv.org/abs/2512.12276)
*Jeffrey van der Voort,Martin Verlaan,Hanne Kekkonen*

Main category: cs.LG

TL;DR: 本文提出了一种多保真度集合卡尔曼滤波方法，将机器学习代理模型作为低保真度模型，与少量昂贵的高保真物理模型结合使用，在相同计算预算下提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习代理模型在计算昂贵物理模型中的应用日益增多，需要一种方法能够充分利用ML模型的快速计算优势，同时保持物理模型的准确性。

Method: 使用多保真度集合卡尔曼滤波，将少量昂贵的全物理模型运行与大量廉价但精度较低的ML模型运行相结合，通过保持原始物理模型来提高精度。

Result: 在Lorenz-2005模型和准地转模型上的测试表明，该方法在相同计算预算下达到了比完全替换为ML模型更高的精度，ML代理模型比低分辨率模型具有相似或更好的精度且提供更大的加速。

Conclusion: 该方法通过增加EnKF中的有效集合大小，改善了初始条件的估计，从而提高了气象学和海洋学等领域预测的准确性。

Abstract: Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.

</details>


### [317] [Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation](https://arxiv.org/abs/2512.12285)
*Lujuan Dang,Zilai Wang*

Main category: cs.LG

TL;DR: 提出了一种结合分数阶微积分和深度学习的FDIFF-PINN神经网络架构，用于提高锂离子电池SOC估计的准确性和物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的神经网络模型难以充分表征电化学过程的复杂非线性和记忆依赖动态特性，限制了其在动态工况下的预测精度和物理可解释性。

Method: 基于分数阶等效电路模型构建离散化分数阶偏微分方程，并将分数阶微积分与深度学习相结合，开发FDIFF-PINN架构。

Result: 使用Panasonic 18650PF电池在-10°C至20°C多温度条件下的动态充放电数据集进行了对比实验验证。

Conclusion: FDIFF-PINN方法能够有效提升锂离子电池SOC估计的准确性和物理可解释性，为电池管理系统提供更可靠的解决方案。

Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).

</details>


### [318] [TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting](https://arxiv.org/abs/2512.12301)
*Mahima Kumavat,Aditya Maheshwari*

Main category: cs.LG

TL;DR: TwinFormer是一个用于长序列时间序列预测的分层Transformer模型，通过局部和全局信息处理机制，在多个真实世界数据集上表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决长序列时间序列预测中传统Transformer模型计算复杂度高、难以有效捕捉长期依赖关系的问题。

Method: 采用分层架构：1）局部信息器使用top-k稀疏注意力建模内部patch动态；2）全局信息器捕获patch间长期依赖；3）轻量级GRU聚合全局上下文信息进行多步预测。

Result: 在8个不同领域的真实数据集上，TwinFormer在34个评估中27次进入前两名，其中17次获得MAE和RMSE最佳性能，10次位列第二，显著优于PatchTST、iTransformer等基准模型。

Conclusion: TwinFormer通过有效的分层设计和稀疏注意力机制，在保持线性计算复杂度的同时，实现了优异的长序列时间序列预测性能。

Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.

</details>


### [319] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

TL;DR: 本文证明了Robbins提出的经典亚高斯混合模型在随机设置下满足路径式（确定性）遗憾界。对于Ville事件E_α中的每条路径，遗憾上界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T，其中V_T是累积方差过程。


<details>
  <summary>Details</summary>
Motivation: 旨在连接对抗性在线学习（处理有界数据的遗憾界）和博弈论统计（能处理无界数据但需随机假设）两个领域，通过条件遗憾界作为随机和对抗性投注之间的桥梁。

Method: 分析经典亚高斯混合模型，证明其在Ville事件中满足路径式遗憾界，研究累积方差过程V_T对遗憾界的影响。

Result: 证明了在概率至少为1-α的Ville事件E_α中，遗憾上界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T；在概率为1的E_0事件中，遗憾最终被ln ln V_T界定。

Conclusion: 条件遗憾界成功连接了随机和对抗性学习框架，为处理无界数据提供了新的理论工具，扩展了在线学习的应用范围。

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [320] [Uncertainty Quantification for Machine Learning: One Size Does Not Fit All](https://arxiv.org/abs/2512.12341)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 本文认为没有单一最佳的不确定性度量方法，应根据具体应用场景定制不确定性量化。通过使用区分总不确定性、任意不确定性和认知不确定性的灵活度量家族，并配合适当的评分规则，可以针对不同任务优化不确定性度量。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，机器学习预测不确定性的准确量化至关重要。现有研究通常声称某种不确定性度量方法优于其他方法，但本文认为不存在单一最佳度量，而应根据具体应用需求进行定制。

Method: 使用二阶分布的灵活不确定性度量家族，区分总不确定性、任意不确定性和认知不确定性。通过特定的损失函数（即适当的评分规则）来实例化这些度量，以控制其特性。

Result: 研究结果表明：1）对于选择性预测任务，评分规则应与任务损失匹配；2）对于分布外检测，互信息（一种广泛使用的认知不确定性度量）表现最佳；3）在主动学习场景中，基于零一损失的认知不确定性始终优于其他不确定性度量。

Conclusion: 不确定性度量应根据具体任务进行定制，没有通用的最优方法。通过选择合适的评分规则和不确定性类型，可以在不同应用场景中获得更好的性能。

Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.

</details>


### [321] [Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept](https://arxiv.org/abs/2512.12365)
*Thai-Duy Dinh,Minh-Luan Vo,Cuong Tuan Nguyen,Bich-Hien Vo*

Main category: cs.LG

TL;DR: 该研究提出了一个用于声学分类的概念验证合成蚊群数据集，通过模拟真实多物种和嘈杂群体环境，解决了传统数据集需要人工密集录制的问题。


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病每年导致超过70万人死亡，构成严重的全球健康威胁。传统蚊虫录音数据集制作成本高、资源需求大，限制了声学蚊虫研究的发展。

Method: 使用合成方法生成蚊群音频数据集，采用log-mel频谱图，并评估轻量级深度学习架构对六种主要蚊媒物种的分类性能。

Result: 实验表明，这些模型能有效识别六种主要蚊媒，且适合部署在嵌入式低功耗设备上。

Conclusion: 合成蚊群音频数据集具有加速声学蚊虫研究和实现可扩展实时监控解决方案的潜力。

Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.

</details>


### [322] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

TL;DR: 本文分析了领域自适应预训练（DAPT）在金融领域的扩展规律，通过训练1B和3B参数的Llama-3.2模型在SEC文件数据上，发现前200M tokens可获得最大收益，金融语言具有高度规律性且可高效学习，同时不会导致通用领域性能退化。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过领域自适应预训练而非完全重新训练，将大型语言模型专业化到高价值领域（如金融），特别是分析在SEC文件上的扩展规律和数据效率。

Method: 使用400M tokens的金融语料库对1B和3B参数的Llama-3.2模型进行持续预训练，并在50M、100M、200M和400M tokens处设置验证检查点，分析验证损失变化和幂律拟合。

Result: 两个模型在SEC领域验证损失上均持续改善，最大收益出现在前200M tokens，之后收益递减；通用领域验证损失基本保持不变，没有灾难性遗忘迹象；幂律拟合显示浅指数，表明金融语言高度规律且可高效学习。

Conclusion: 研究表明通过相对适中的token预算即可实现有意义的领域自适应，较大模型规模（7B-70B）在预计数据需求下仍可行，为金融基础模型的扩展提供了早期实证指导。

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [323] [Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment](https://arxiv.org/abs/2512.12387)
*Yawen Shao,Jie Xiao,Kai Zhu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.LG

TL;DR: VGPO提出了一种新的价值锚定组策略优化框架，解决了GRPO在流匹配图像生成中的时序信用分配和优化停滞问题，通过密集过程感知价值估计和绝对价值增强的组归一化，在三个基准测试中实现了最先进的图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前GRPO在流匹配图像生成中存在两个关键限制：(1) 在所有时间步均匀应用稀疏终端奖励损害了时序信用分配，忽略了从早期结构形成到后期调整的不同生成阶段的关键性差异；(2) 仅依赖相对组内奖励导致训练收敛时优化信号衰减，当奖励多样性完全耗尽时出现优化停滞。

Method: VGPO框架在时间和组维度上重新定义价值估计：将稀疏终端奖励转换为密集的过程感知价值估计，通过建模每个生成阶段的预期累积奖励实现精确信用分配；用绝对价值增强的新过程替换标准组归一化，在奖励多样性下降时保持稳定的优化信号。

Result: 在三个基准测试上的广泛实验表明，VGPO实现了最先进的图像质量，同时提高了任务特定准确性，有效缓解了奖励黑客问题。

Conclusion: VGPO通过解决GRPO在视觉合成过程中的基本冲突，为流匹配图像生成提供了更有效的优化框架，在图像质量和任务准确性方面均取得了显著提升。

Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.

</details>


### [324] [DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields](https://arxiv.org/abs/2512.12402)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: DeepVekua提出了一种混合架构，将几何深度学习与谱分析相结合，用于稀疏数据条件下的偏微分方程求解。


<details>
  <summary>Details</summary>
Motivation: 传统基于坐标的神经网络在处理偏微分方程时存在谱偏差问题，特别是在复杂几何形状和稀疏数据情况下表现不佳。

Method: 通过学习一个微分同胚坐标变换，将复杂几何映射到潜在谐波空间，将几何学习与物理学习分离，并采用闭式解求解最优谱权重。

Result: 在平流-扩散系统上超越了最先进的隐式表示方法，相比谱基线方法实现了100倍的性能提升。

Conclusion: DeepVekua通过几何与物理学习的分离，有效解决了谱偏差问题，为稀疏数据条件下的PDE求解提供了新的有效方法。

Abstract: We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.

</details>


### [325] [Can Graphs Improve Tabular Foundation Models?](https://arxiv.org/abs/2512.12405)
*Franck Le,Keith Grueneberg,Erich Nahum,Vadim Sheinin*

Main category: cs.LG

TL;DR: BOLERO是一个轻量级静态二分图头，通过引入简单的图先验来增强预训练的表格transformer，在分类和回归任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的表格学习方法缺乏显式的实例间关系建模机制，而相似样本通常具有相关结果。研究是否可以通过引入简单的图先验来增强预训练表格transformer的性能。

Method: 提出BOLERO方法：在RoBERTa-Tab（基于掩码标记预测预训练的RoBERTa风格表格骨干网络）基础上，添加轻量级静态二分图头，每个实例连接到特征/值锚点，使用小型GNN细化行表示，同时保持骨干网络冻结。

Result: 在TP-BERTa基准套件的80个分类和64个回归数据集上进行评估，BOLERO在分类和回归任务上都取得了最多的统计显著胜利，证明轻量级图先验能有效改进预训练表格transformer。

Conclusion: 轻量级图先验可以显著提升预训练表格transformer的性能，BOLERO方法在广泛的基准测试中表现优异。

Abstract: Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.

</details>


### [326] [Learning Dynamics in Memristor-Based Equilibrium Propagation](https://arxiv.org/abs/2512.12428)
*Michael Döll,Andreas Müller,Bernd Ulmann*

Main category: cs.LG

TL;DR: 该论文研究了基于忆阻器的内存计算中非线性权重更新对平衡传播训练神经网络收敛行为的影响，发现只要忆阻器具有足够宽的电阻范围（至少一个数量级），平衡传播就能在非线性权重更新下实现稳健收敛。


<details>
  <summary>Details</summary>
Motivation: 克服冯·诺依曼瓶颈和内存墙的限制，通过忆阻器实现完全并行化和高能效的向量矩阵乘法。

Method: 将六种忆阻器模型集成到EBANA框架中，在两种基准分类任务上评估非线性、忆阻器驱动的权重更新对平衡传播训练收敛行为的影响。

Result: 平衡传播能够在非线性权重更新下实现稳健收敛，前提是忆阻器表现出足够宽的电阻范围（至少一个数量级）。

Conclusion: 忆阻器基内存计算在神经网络训练中具有潜力，但需要忆阻器具备足够大的电阻变化范围来保证训练收敛性。

Abstract: Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.

</details>


### [327] [Rough Sets for Explainability of Spectral Graph Clustering](https://arxiv.org/abs/2512.12436)
*Bartłomiej Starosta,Sławomir T. Wierzchoń,Piotr Borkowski,Dariusz Czerski,Marcin Sydow,Eryk Laskowski,Mieczysław A. Kłopotek*

Main category: cs.LG

TL;DR: 本文提出了一种基于粗糙集理论的图谱聚类解释方法增强方案，旨在解决图谱聚类在文本文档聚类中结果难以解释的问题。


<details>
  <summary>Details</summary>
Motivation: 图谱聚类方法能够处理各种形状和密度的簇，但在文本文档聚类中，由于嵌入到谱空间且与文档内容无明显关联，聚类结果难以向用户解释。此外，文档内容不清晰和聚类算法的随机性也降低了可解释性。

Method: 基于团队先前研究的解释方法，引入粗糙集理论来增强解释能力，以克服上述问题。

Result: 该方法能够改善图谱聚类在文本文档聚类中的可解释性，使聚类结果更容易被用户理解。

Conclusion: 通过结合粗糙集理论，提出的增强方法有效提升了图谱聚类算法的解释性，解决了谱空间嵌入和算法随机性带来的解释难题。

Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.

</details>


### [328] [Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction](https://arxiv.org/abs/2512.12445)
*Abdul Matin,Rupasree Dey,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.LG

TL;DR: 提出了一种基于ViT的知识引导掩码自编码器，将线性光谱混合模型和光谱角匹配器作为物理约束嵌入自监督重建过程，提升模型的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 将领域知识融入深度学习可以改善模型的可解释性、泛化性和数据效率，但目前主要依赖数据驱动优化，缺乏物理约束。

Method: 提出知识引导的ViT掩码自编码器，将LSMM作为物理约束，结合SAM损失和Huber损失进行联合优化，确保学习表示符合观测信号与潜在分量之间的结构关系。

Result: 实验结果表明，该模型显著提升了重建质量，改善了下游任务性能，在有限监督下稳定训练，并产生基于物理原理的可解释潜在表示。

Conclusion: 将物理知识引导的归纳偏置嵌入基于Transformer的自监督学习具有广阔前景，能够增强重建保真度和模型性能。

Abstract: Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.

</details>


### [329] [Optimized Architectures for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.12448)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: 本文提出通过过参数化架构结合稀疏化方法，在不牺牲准确性的前提下学习紧凑、可解释的Kolmogorov-Arnold网络（KANs），解决了传统架构增强带来的复杂性对可解释性的影响。


<details>
  <summary>Details</summary>
Motivation: 传统KANs架构增强方法会引入复杂性，破坏其原有的可解释性优势。本文旨在通过过参数化和稀疏化的组合策略，在保持准确性的同时获得更紧凑、更可解释的模型。

Method: 采用过参数化架构结合可微分稀疏化方法，将架构搜索转化为端到端优化问题。通过这种组合策略学习紧凑的KANs模型。

Result: 在函数逼近基准测试、动态系统预测和实际预测任务中，该方法实现了竞争性或更优的准确性，同时发现了显著更小的模型。过参数化和稀疏化具有协同效应，组合使用优于单独使用任一方法。

Conclusion: 该方法为构建既更具表达能力又更可解释的模型提供了一条原则性路径，解决了科学机器学习中的一个关键矛盾。

Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.

</details>


### [330] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 该论文提出了一种跨模态知识蒸馏框架，将预训练的多会话尖峰Transformer模型的高保真表示知识迁移到LFP Transformer模型中，以解决LFP信号建模的挑战。


<details>
  <summary>Details</summary>
Motivation: LFP信号虽然具有长期稳定性好、对电极退化更鲁棒、功耗更低等优势，但由于其聚合的群体水平特性，在建模上面临挑战，导致对下游任务变量的预测能力较低。现有神经建模框架主要关注尖峰活动，而忽略了LFP的潜力。

Method: 首先使用掩码自编码目标和会话特定的神经标记化策略，在多会话上训练教师尖峰模型。然后，将学生LFP模型的潜在表示与教师尖峰模型的表示对齐，实现知识蒸馏。

Result: 蒸馏后的LFP模型在完全无监督和监督设置下均优于单会话和多会话LFP基线模型，且能够泛化到其他会话而无需额外蒸馏，同时保持优越性能。

Conclusion: 跨模态知识蒸馏是一种强大且可扩展的方法，能够利用高性能尖峰模型开发更准确的LFP模型。

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [331] [Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference](https://arxiv.org/abs/2512.12462)
*Eray Erturk,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 提出了一种实时递归解码框架，能够处理多模态神经时间序列数据的不同时间尺度、概率分布和缺失样本问题，显著提升实时目标解码性能。


<details>
  <summary>Details</summary>
Motivation: 现有非线性多模态神经活动模型无法处理不同时间尺度和缺失样本问题，且部分模型不支持实时解码。需要开发能够实时处理多尺度、多分布、缺失样本的多模态神经数据的解码方法。

Method: 框架包含三个核心组件：1）多尺度编码器学习模态内动态以处理不同时间尺度和缺失样本；2）多尺度动态骨干网络提取多模态时序动态并支持实时递归解码；3）模态特定解码器处理不同概率分布。

Result: 在仿真和三个多尺度脑数据集上的实验表明，该方法能够有效整合不同时间尺度、分布和缺失样本的多模态信息，显著提升实时目标解码性能，优于多种线性和非线性多模态基准方法。

Conclusion: 该学习框架为解决多模态神经数据的不同时间尺度、分布差异和缺失样本问题提供了有效解决方案，在实时神经解码应用中具有重要价值。

Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.

</details>


### [332] [Exploring the Design Space of Transition Matching](https://arxiv.org/abs/2512.12465)
*Uriel Singer,Yaron Lipman*

Main category: cs.LG

TL;DR: Transition Matching (TM) 是一种新兴的生成建模范式，它通过引入内部生成模型来增强过渡步骤的表达能力。本文系统研究了TM框架中头模块的设计、训练和采样策略，通过大规模实验发现MLP头配合特定时间加权和高频采样器在生成质量上达到最优。


<details>
  <summary>Details</summary>
Motivation: TM范式虽然比扩散和流匹配模型更具表达力，但其头模块的设计和训练策略尚未得到系统研究。本文旨在通过大规模实验探索不同头架构和采样策略对TM模型性能的影响。

Method: 训练了56个不同的17亿参数文本到图像模型，进行了549次评估，系统比较了MLP头和Transformer头在不同时间加权策略和采样频率下的表现。

Result: MLP头配合特定时间加权和高频采样器在所有指标上表现最佳，达到最先进水平；Transformer头配合序列缩放和低频采样在图像美学方面表现优异。

Conclusion: 研究明确了TM框架中最能提升质量和效率的设计选择，同时指出了哪些设计选择不太可能带来进一步增益，为未来TM模型开发提供了重要指导。

Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.

</details>


### [333] [Sparse Concept Anchoring for Interpretable and Controllable Neural Representations](https://arxiv.org/abs/2512.12469)
*Sandy Fraser,Patryk Wielopolski*

Main category: cs.LG

TL;DR: Sparse Concept Anchoring是一种通过最小监督（每个锚定概念<0.1%的标注样本）来偏置潜在空间的方法，能够定位目标概念子集同时允许其他概念自组织。该方法支持可逆行为引导和永久概念移除两种实用干预。


<details>
  <summary>Details</summary>
Motivation: 为了解决在学习的表示中实现可解释、可引导行为的需求，需要一种能够精确定位和操作特定概念的方法，同时保持对其他特征的干扰最小。

Method: 结合激活归一化、分离正则化器以及锚点或子空间正则化器，将少量标注样本吸引到预定义方向或轴对齐子空间中。训练方法使用极少的监督信息（每个概念<0.1%标注）。

Result: 在结构化自编码器上的实验显示，该方法能够选择性衰减目标概念，对正交特征影响可忽略，完全消除概念时的重构误差接近理论界限。

Conclusion: Sparse Concept Anchoring为学习表示中的可解释、可引导行为提供了一条实用路径，支持可逆行为引导和永久概念移除两种重要干预方式。

Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.

</details>


### [334] [GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction](https://arxiv.org/abs/2512.12489)
*Shuhui Qu,Cheolwoo Park*

Main category: cs.LG

TL;DR: GoMS是一种新颖的图神经网络架构，通过显式建模分子子结构之间的相互作用和空间排列来改进分子性质预测，特别适用于大分子和工业规模分子。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ESAN将分子视为独立子结构的集合，忽略了子结构之间的关键关系，导致拓扑信息丢失。

Method: GoMS构建了一个图结构，其中节点代表子图，边捕捉子结构之间的结构关系，保留子结构如何连接和重叠的关键拓扑信息。

Result: 在公共分子数据集上的实验表明，GoMS优于ESAN和其他基线方法，特别是对于包含100个以上原子的大分子，性能差距随分子尺寸增大而扩大。

Conclusion: GoMS通过捕捉袋式方法中丢失的子结构关系，在可扩展和可解释的分子性质预测方面取得了重要进展，特别适用于涉及复杂分子的材料科学应用。

Abstract: While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.

</details>


### [335] [AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models](https://arxiv.org/abs/2512.12493)
*Vaarunay Kaushal,Rajib Mall*

Main category: cs.LG

TL;DR: 该研究通过分析在线学习环境中不同时间点的学生风险预测，发现决策树和LSTM模型在不同干预阶段各有优势：LSTM在早期（第2-4周）具有97%的高召回率，适合早期干预；决策树在中期（第8-16周）提供稳定的平衡性能；到后期（第20周）两者召回率相近但LSTM精度更高。静态人口特征占预测重要性的68%，可实现无评估的早期预测。


<details>
  <summary>Details</summary>
Motivation: 早期识别在线学习环境中的风险学生对于有效干预至关重要，需要了解不同时间点的预测模型性能和适用场景。

Method: 使用决策树和LSTM模型，在课程持续时间的六个时间快照（包括第20周，即50%课程时长）进行时序预测分析，比较不同阶段的性能指标。

Result: 不同干预阶段需要不同的性能指标：早期（第2-4周）需要高召回率，中期（第8-16周）需要平衡的精确度-召回率，后期（第20周）需要高精确度。LSTM在第2周达到97%召回率，决策树在中期提供78%的准确率。到第20周，两者召回率均为68%，但LSTM精度更高（90% vs 86%）。

Conclusion: 模型选择应取决于干预时机，早期信号（第2-4周）使用主要的人口统计和入学前信息足以进行可靠的初始预测，静态人口特征在预测中占主导地位（68%重要性）。

Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.

</details>


### [336] [Policy Optimization for Dynamic Heart Transplant Allocation](https://arxiv.org/abs/2512.12497)
*Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

TL;DR: 本文针对心脏移植分配政策存在的问题，开发了一个新的模拟器来评估和改进现有政策，提出了考虑动态分配过程的改进策略。


<details>
  <summary>Details</summary>
Motivation: 当前心脏移植分配政策（2018年修订版）未能充分考量移植前后的死亡率，导致分配效率低下，限制了有限的供体资源的最优利用。

Method: 利用UNOS历史数据开发模拟器，评估不同分配政策性能；提出考虑患者未来效用的潜力值方法；探索批量分配供体的可行性。

Result: 研究发现现状政策远不如近视政策有效；改进的动态分配政策能显著提升性能；少量供体批量分配可进一步提高效率。

Conclusion: 通过模拟器验证的改进分配政策能够更有效地利用稀缺的供体资源，考虑地理邻近性和移植中心拒绝倾向等关键因素可进一步优化分配效果。

Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.

</details>


### [337] [Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems](https://arxiv.org/abs/2512.12523)
*Wenqi Fang,Ye Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于奇异值分解和半正交约束训练算法的神经网络架构，用于提高传统对比学习在检测复杂时间序列数据中临界转变的性能。该方法在保持识别精度的同时，显著降低了模型复杂度并增强了抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 复杂噪声时间序列数据中的临界转变检测是科学和工程领域的基础挑战。传统基于深度神经网络的对比学习方法虽然前景广阔，但往往过度参数化且对无关噪声敏感，导致临界点识别不准确。

Method: 提出了一种使用奇异值分解技术构建的神经网络架构，配合严格的半正交约束训练算法，以增强传统对比学习的性能。

Result: 大量实验表明，所提方法在识别临界转变方面与传统对比学习技术性能相当，但模型更轻量且抗噪能力显著增强。

Conclusion: 该研究为解决临界转变检测中的噪声敏感和过度参数化问题提供了有效的解决方案，在保持性能的同时实现了更高的效率和鲁棒性。

Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.

</details>


### [338] [Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling](https://arxiv.org/abs/2512.12526)
*Agustín M. de los Riscos,Julio E. Sandubete,Diego Carmona-Fernández,León Beleña*

Main category: cs.LG

TL;DR: 该研究应用经验模态分解(EMD)将MSCI世界指数分解为多个本征模态函数(IMF)，并将IMF转换为图表示以便使用图神经网络(GNN)进行建模。


<details>
  <summary>Details</summary>
Motivation: 通过将时间序列分解为不同尺度的分量并转换为图结构，为金融时间序列预测提供更有效的GNN建模方法。

Method: 使用CEEMDAN方法提取9个IMF，采用四种时间序列转图方法（自然可见性图、水平可见性图、递归图、转移图）将IMF转换为图表示。

Result: 高频IMF产生密集、高度连接的小世界图，低频IMF产生稀疏网络且具有更长的特征路径长度；可见性图方法对振幅变化更敏感，递归图更好地保留时间依赖性。

Conclusion: 研究结果为针对分解分量的结构特性设计GNN架构提供了指导，支持更有效的金融时间序列预测建模。

Abstract: This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.

</details>


### [339] [Effective Fine-Tuning with Eigenvector Centrality Based Pruning](https://arxiv.org/abs/2512.12543)
*Shaif Chowdhury,Soham Biren Katlariwala,Devleena Kashyap*

Main category: cs.LG

TL;DR: 提出一种基于图论的神经网络剪枝方法，通过保留最重要的神经元来提升微调性能，在多个数据集上实现了更高的分类准确率和显著降低的模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 受社交媒体网络中少数高影响力用户能够驱动大规模变化的现象启发，类比神经网络微调过程，认为通过先剪枝保留重要神经元再进行微调可以获得更好的适应性能。

Method: 将神经元表示为图节点，边编码神经元间相似性，使用特征向量中心性计算重要性得分进行剪枝，然后对保留的最中心神经元进行微调。

Result: 在VGGNet、EfficientNet和ResNet模型上，使用TF Flowers、Caltech 101和Oxford Flowers 102数据集评估，相比基线VGGNet模型的30%准确率，该方法在Oxford Flowers 102数据集上达到48%准确率。

Conclusion: 基于图论的剪枝方法能有效提升神经网络微调性能，在保持较高分类准确率的同时显著降低模型复杂度。

Abstract: In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning.
  We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.

</details>


### [340] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

TL;DR: TianXing-S2S是一个多圈层耦合的概率模型，用于全球次季节到季节(S2S)的每日集合预报。该模型通过编码多圈层预测因子到潜空间，并使用扩散模型生成集合预报，在45天预报中优于ECMWF S2S系统和FuXi-S2S。


<details>
  <summary>Details</summary>
Motivation: 准确的S2S极端事件预测对于气候变化下的资源规划和灾害缓解至关重要，但由于复杂多圈层相互作用和内在大气不确定性，此类预测仍然具有挑战性。

Method: 模型首先将多样多圈层预测因子编码到紧凑潜空间，然后使用扩散模型生成每日集合预报。在去噪器中加入基于最优传输(OT)的新型耦合模块，优化大气与多圈层边界条件之间的相互作用。

Result: 在关键大气变量上，TianXing-S2S在1.5度分辨率的45天日均集合预报中优于ECMWF S2S系统和FuXi-S2S。模型能够熟练预测极端事件（如热浪和异常降水），并识别土壤湿度为关键前兆信号。模型可生成稳定的180天滚动预报。

Conclusion: TianXing-S2S为变暖世界中的S2S研究建立了稳健框架，展示了在极端事件预测方面的显著优势。

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [341] [Optimal Mistake Bounds for Transductive Online Learning](https://arxiv.org/abs/2512.12567)
*Zachary Chase,Steve Hanneke,Shay Moran,Jonathan Shafer*

Main category: cs.LG

TL;DR: 该论文解决了在线学习中无标签数据作用这一30年未解问题，证明了转导式在线学习与标准在线学习之间存在平方级差距，转导式学习的错误下界为Ω(√d)，上界为O(√d)，而标准在线学习的错误界为d。


<details>
  <summary>Details</summary>
Motivation: 研究无标签数据在转导式在线学习中的作用，量化转导式学习与标准在线学习之间的性能差距，这一开放问题已存在30年。

Method: 通过理论分析，建立了转导式在线学习错误下界Ω(√d)的证明，并构造了达到O(√d)上界的Littlestone维度为d的概念类。

Result: 证明了转导式在线学习的错误界为Θ(√d)，相比标准在线学习的d界有平方级改进，显著优于之前已知的Ω(loglog d)、Ω(√log d)和Ω(log d)下界。

Conclusion: 转导式在线学习相比标准在线学习具有显著优势，错误界从d降低到√d，这凸显了提前获取无标签实例序列的益处，与PAC学习设置中的相似样本复杂度形成对比。

Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.

</details>


### [342] [On the Accuracy of Newton Step and Influence Function Data Attributions](https://arxiv.org/abs/2512.12572)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 本文对线性回归中的数据归因方法（NS和IF）进行了新的分析，突破了传统强凸性假设的限制，首次解释了NS方法通常比IF更准确的原因，并提供了渐近紧致的误差边界。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法（如IF和NS）的分析存在两个主要局限：一是依赖全局强凸性假设，这在实践中往往不成立；二是误差边界随参数数量d和移除样本数k的缩放效果很差。需要更准确的分析方法来回答哪种方法更优以及误差的渐近缩放规律。

Method: 引入新的分析框架，针对凸学习问题分析NS和IF数据归因方法。该方法不假设全局强凸性，通过数学证明在逻辑回归等良好行为场景下，误差边界可以达到渐进紧致（up to poly-logarithmic factors）。

Result: 证明了在平均样本移除情况下，NS方法的误差为Õ(kd/n²)，NS与IF方法之间的差异为Õ((k+d)√(kd)/n²)。这是第一个不依赖强凸性假设的分析，也是首次解释NS通常比IF更准确的原因。

Conclusion: 该分析为数据归因方法提供了更准确的理论基础，证明了NS方法在实践中的优越性，并为理解不同数据归因方法的误差缩放规律提供了重要理论依据。

Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]

</details>


### [343] [Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses](https://arxiv.org/abs/2512.12581)
*David Strnadel*

Main category: cs.LG

TL;DR: 本文提出了一种基于模拟器的概念验证，探索参数化量子电路的可微分能量项能否作为GAN的辅助正则化信号。在ACGAN生成器目标中加入VQE启发的能量项，实验表明能量正则化模型在分类准确性上优于传统ACGAN，但样本质量指标存在高方差。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算与生成对抗网络的结合可能性，研究量子电路衍生的能量项是否能为GAN训练提供有效的正则化信号，为量子机器学习提供新的方法论思路。

Method: 在Auxiliary Classifier GAN生成器目标函数中增加Variational Quantum Eigensolver启发的能量项，使用Qiskit的EstimatorQNN和TorchConnector计算类特定Ising哈密顿量的能量。实验基于4量子位的无噪声状态向量模拟器。

Result: 在MNIST数据集上，QACGAN在5个epoch内达到99-100%的分类准确率（ACGAN为87.8%），表明辅助项影响类条件化。但样本质量指标FID在运行间存在高方差（变异系数约25%），最终稳定在23-24左右，与ACGAN基线相当。

Conclusion: 本研究展示了VQE风格能量计算可以通过可微分路径集成到GAN训练循环中，但未声称量子优势或改进稳定性。量子辅助信号是否优于等效经典正则化器仍是一个需要系统性消融研究的开放问题。

Abstract: This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector.
  Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs.
  On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline.
  We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.

</details>


### [344] [Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics](https://arxiv.org/abs/2512.12602)
*Jingdi Lei,Di Zhang,Soujanya Poria*

Main category: cs.LG

TL;DR: EFLA是一种无误差线性注意力机制，通过将在线学习更新建模为连续时间动力系统，实现了线性时间复杂度的精确计算，避免了误差累积。


<details>
  <summary>Details</summary>
Motivation: 解决传统softmax注意力在长上下文语言模型中的二次计算成本瓶颈，同时克服现有线性注意力方法可能存在的数值不稳定和误差累积问题。

Method: 将在线学习更新建模为连续时间动力系统，利用动力矩阵的秩-1结构直接推导出精确闭式解，相当于无限阶龙格-库塔方法。

Result: 在噪声环境中表现稳健，语言建模困惑度更低，下游基准测试性能优于DeltaNet，且无需额外参数。

Conclusion: 为构建高保真、可扩展的线性时间注意力模型提供了新的理论基础。

Abstract: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.

</details>


### [345] [Causal inference and model explainability tools for retail](https://arxiv.org/abs/2512.12605)
*Pranav Gupta,Nithin Surendran*

Main category: cs.LG

TL;DR: 本文提出了一种将模型可解释性和因果推断应用于销售洞察的框架，通过回顾现有文献并应用于真实数据集，发现可解释模型具有较低的SHAP值方差，且通过双重机器学习方法包含多个混杂因子能获得正确的因果效应符号。


<details>
  <summary>Details</summary>
Motivation: 当前零售业使用的机器学习模型缺乏可解释性，且无法验证或发现因果关系，限制了从数据中获取深入洞察的能力。

Method: 回顾因果推断和可解释性文献，应用于真实零售数据集，使用SHAP值分析模型可解释性，采用双重机器学习方法处理混杂因子。

Result: 可解释模型展现出较低的SHAP值方差；双重机器学习方法能够正确识别因果效应的符号方向。

Conclusion: 结合模型可解释性和因果推断方法能为零售业提供更可靠的销售洞察，有助于制定更有效的商业决策。

Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.

</details>


### [346] [Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain](https://arxiv.org/abs/2512.12617)
*Animesh Mishra*

Main category: cs.LG

TL;DR: Spectral Sentinel是一个拜占庭检测框架，利用随机矩阵理论检测非IID数据下的恶意梯度攻击，在1.5B参数模型上实现高效防御。


<details>
  <summary>Details</summary>
Motivation: 现有的去中心化联邦学习防御方法面临可扩展性困境：距离过滤可能误拒合法非IID更新，几何中值方法计算成本高，而认证防御仅适用于小模型。

Method: 结合Frequent Directions草图和数据相关的Marchenko-Pastur跟踪，通过协方差矩阵特征谱检测拜占庭扰动引起的尾部异常。

Result: 在144种攻击-聚合配置中实现78.4%的平均准确率，优于基线方法的48-63%。

Conclusion: 该方法在(σ,f)威胁模型下证明具有(ε,δ)-拜占庭弹性，收敛速率达到最小化最优，并实现了区块链集成。

Abstract: Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f < 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.

</details>


### [347] [Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks](https://arxiv.org/abs/2512.12642)
*Filippo Maria Bianchi,Carlo Abate,Ivan Marisca*

Main category: cs.LG

TL;DR: Torch Geometric Pool (tgp)是一个基于PyTorch Geometric的图神经网络分层池化库，提供统一API和模块化设计的多种池化算子，强调可用性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 为图神经网络提供快速原型设计的分层池化库支持，因为最优池化算子的选择取决于具体任务和数据集。

Method: 构建基于PyTorch Geometric的模块化池化库，提供预计算池化等加速训练的特性，并通过统一API整合多种池化算子。

Result: 通过广泛基准测试展示了库的特性，系统比较了不同下游任务中图池化方法的性能，验证了库的实用性。

Conclusion: tgp库能够有效支持图神经网络中池化算子的快速选择和原型设计，证明了针对不同任务和数据选择合适池化算子的重要性。

Abstract: We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.

</details>


### [348] [PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks](https://arxiv.org/abs/2512.12663)
*Gelesh G Omathil,Sreeja CS*

Main category: cs.LG

TL;DR: PerNodeDrop是一种轻量级随机正则化方法，通过逐样本、逐节点的扰动来打破现有技术的噪声均匀性，从而在保留有用共适应的同时进行正则化。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络容易过拟合，因为神经元倾向于共适应，这虽然能捕捉复杂特征交互，但也会强化虚假模式。现有噪声正则化方法如Dropout和DropConnect的噪声通常是均匀的，可能同时抑制有害和有益的共适应。

Method: PerNodeDrop在样本级别对权重进行丢弃，而不是批量级别。通过逐样本、逐节点的扰动，使每个节点经历输入特定的变异性。

Result: 实验表明，PerNodeDrop缩小了训练和验证性能之间的差距，在视觉、文本和音频基准测试中相对于标准噪声正则化器提高了泛化能力。

Conclusion: PerNodeDrop通过样本级别的权重丢弃，有效衰减过度共适应同时保留预测性交互，提高了模型在未见数据上的可靠性。

Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.

</details>


### [349] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

TL;DR: DynaGen是一个统一的时间知识图谱推理方法，通过动态构建实体中心子图和条件扩散过程，分别解决插值推理和外推推理的挑战，在六个基准数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间知识图谱推理中面临两个关键挑战：插值推理中有限的上下文建模能力，以及外推推理中的认知泛化偏差。需要一种统一的方法来同时解决这两个问题。

Method: DynaGen采用双分支方法：对于插值推理，动态构建实体中心子图并使用协同双分支GNN编码器捕捉演化结构上下文；对于外推推理，应用条件扩散过程来学习底层演化原理而非表面模式。

Result: 在六个基准数据集上的实验表明，DynaGen相比次优模型平均提升插值推理的MRR分数2.61分，外推推理的MRR分数1.45分。

Conclusion: DynaGen通过统一的框架有效解决了时间知识图谱推理中的插值和外推问题，证明了动态上下文建模和条件扩散过程在捕捉时间演化模式方面的有效性。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [350] [On Approaches to Building Surrogate ODE Models for Diffusion Bridges](https://arxiv.org/abs/2512.12671)
*Maria Khilchuk,Vladimir Latypov,Pavel Kleshchev,Alexander Hvatov*

Main category: cs.LG

TL;DR: 本文提出使用替代模型简化扩散和薛定谔桥模型，开发了SINDy-FM和DSBM-NeuralODE两种算法，在保持竞争力的同时显著提升效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和薛定谔桥模型在生成建模中表现优异，但存在计算成本高、训练复杂、采样效率低等问题，需要更简单高效的替代方案。

Method: 提出SINDy-FM（利用稀疏回归识别符号微分方程）和DSBM-NeuralODE（神经ODE重构薛定谔桥）两种替代模型算法。

Result: 在Gaussian传输任务和MNIST潜在转换实验中，替代模型达到竞争性性能，参数数量减少数个数量级，实现近瞬时推理。

Conclusion: 该方法为实际部署开辟了新型可处理且高性能的桥模型路径，显著提升效率、可解释性和实用性。

Abstract: Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.

</details>


### [351] [Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity](https://arxiv.org/abs/2512.12688)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

TL;DR: 本文提出将提示视为外部注入程序的理论框架，证明单一固定Transformer骨干可以通过提示切换实现广泛的计算行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将提示作为启发式方法而非理论对象，缺乏对提示切换模型行为的机制层面理解。

Method: 构建简化的Transformer模型，将提示视为外部程序，分解注意力机制、前馈网络和深度堆叠在提示执行中的作用。

Result: 证明了单一固定骨干可以通过提示近似广泛的目标行为，提供了形式化提示长度/精度权衡的理论基础。

Conclusion: 该框架为研究提示切换的结构限制和约束条件提供了统一的理论起点，区别于对预训练LLM的经验性主张。

Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

</details>


### [352] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

TL;DR: 本文重新评估了视觉语言模型推理训练中监督微调（SFT）与强化学习（RL）的相对有效性，挑战了当前'RL优于SFT'的主流观点。


<details>
  <summary>Details</summary>
Motivation: 当前社区普遍认为SFT阶段不仅无法提升推理能力，还可能对模型训练产生负面影响，但这一观点缺乏系统验证。本文旨在通过控制变量实验重新审视SFT和RL在VLM推理中的实际效果。

Method: 使用相同数据源，系统比较SFT和RL在不同模型容量、数据规模和分布条件下的表现，重点关注SFT在弱模型、数据效率和跨模态迁移方面的作用。

Result: 发现SFT在多个场景中发挥关键作用：对弱模型更有效；仅需2K数据即可达到RL使用20K数据的性能；具有更强的跨模态泛化能力。同时发现RL存在欺骗性奖励问题。

Conclusion: SFT的作用被低估，SFT和RL应作为互补组件构建更平衡的后训练流程，挑战了当前RL-centric的训练范式。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [353] [Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits](https://arxiv.org/abs/2512.12693)
*Sumantrak Mukherjee,Serafima Lebedeva,Valentin Margraf,Jonas Hanselle,Kanta Yamaoka,Viktor Bengs,Stefan Konigorski,Eyke Hüllermeier,Sebastian Josef Vollmer*

Main category: cs.LG

TL;DR: 提出了一种新的贝叶斯框架，用于在上下文部分可观测的多任务多臂老虎机设置中进行高效探索，利用潜在上下文变量诱导奖励分布间的依赖关系。


<details>
  <summary>Details</summary>
Motivation: 解决上下文部分可观测的多任务多臂老虎机问题，通过利用潜在上下文变量诱导的结构依赖关系来提高探索效率。

Method: 使用基于粒子的对数密度高斯过程近似来表示任务和奖励的联合分布，实现跨任务和臂的灵活依赖关系发现，无需对潜在变量进行先验假设。

Result: 实证表明，该方法在模型误设或复杂潜在异质性设置下，优于分层模型老虎机等基线方法。

Conclusion: 该框架通过整合跨任务观察和学习全局联合分布，有效处理结构不确定性和用户特定不确定性，实现了高效的个性化推理。

Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.

</details>


### [354] [Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data](https://arxiv.org/abs/2512.12708)
*Anthime Valin*

Main category: cs.LG

TL;DR: 提出MT-PINN方法解决硬零终端库存约束下的最优交易执行问题，通过轨迹损失和终端惩罚直接强化零库存约束，相比传统PINNs更稳定且准确。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理硬零终端库存约束时约束执行不足，导致控制不稳定，需要改进方法来直接强化终端约束。

Method: 提出多轨迹PINN（MT-PINN），添加基于滚动的轨迹损失，通过时间反向传播终端惩罚来直接执行零终端库存约束，并采用轻量级lambda课程来稳定训练。

Result: 在Gatheral-Schied单资产模型上，MT-PINN与闭式解高度一致，终端库存紧密集中在零附近，沿最优路径误差减少；在SPY日内数据上，风险中性时匹配TWAP，高风险厌恶时实现更低风险和竞争性成本。

Conclusion: MT-PINN有效解决了硬约束下的交易执行问题，在理论和实际数据上均表现出优越性能，特别适用于风险厌恶场景。

Abstract: We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.

</details>


### [355] [Solving a Machine Learning Regression Problem Based on the Theory of Random Functions](https://arxiv.org/abs/2512.12731)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 本文从随机函数理论框架出发，将机器学习回归问题视为多元近似问题进行研究，基于无差别性公设推导出回归方法，证明了自然对称性下核函数、正则化形式和噪声参数化的解析解，为平滑插值方法提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 为机器学习回归方法提供严格的理论基础，从基本原理出发推导最优解，而不是依赖经验选择核函数和正则化形式。

Method: 采用随机函数理论框架，基于平移、旋转、缩放不变性和高斯性等自然对称性公设，通过解析推导得到回归方法的完整解。

Result: 推导出的核函数与广义多调和样条一致，但这是公设的自然结果而非经验选择；整个解方案（包括核形式、正则化类型和噪声参数化）都从对称性公设中解析得出。

Conclusion: 该方法为广泛的平滑和插值方法提供了理论依据，证明了在缺乏先验信息情况下的最优性，为回归方法建立了坚实的数学基础。

Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.

</details>


### [356] [SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization](https://arxiv.org/abs/2512.12737)
*Li Xia*

Main category: cs.LG

TL;DR: SPARK是一种去中心化联邦学习方法，通过随机投影压缩Jacobian矩阵、阶段式退火蒸馏和Nesterov动量加速，在减少98.7%通信量的同时保持收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: 解决去中心化联邦学习面临的统计异构性和通信开销挑战，特别是NTK方法传输完整Jacobian矩阵在带宽受限边缘网络中的不实用性。

Method: 集成三种技术：1）基于随机投影的Jacobian压缩，保持收敛所需的光谱特性；2）阶段式退火蒸馏，从纯NTK演化过渡到邻居正则化学习；3）Nesterov动量加速，通过蒸馏平滑实现稳定累积。

Result: SPARK相比NTK-DFL减少98.7%通信量，保持收敛速度且精度更优。结合动量后达到目标性能的速度快3倍。

Conclusion: SPARK为通信高效的去中心化学习建立了最先进的结果，使在带宽受限边缘环境中的实际部署成为可能。

Abstract: Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.

</details>


### [357] [Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models](https://arxiv.org/abs/2512.12744)
*Haotian Xu,Tian Gao,Tsui-Wei Weng,Tengfei Ma*

Main category: cs.LG

TL;DR: 该研究提出通过引入可训练的自发神经元来补偿输入稀疏化对LLMs性能的影响，将输入稀疏化重新解释为动态结构化剪枝，并受生物神经元自发基线放电率的启发来稳定稀疏化模型的激活。


<details>
  <summary>Details</summary>
Motivation: 现有输入稀疏化方法主要关注计算效率，但忽视了稀疏化对模型表示能力的影响，导致与完整模型存在明显的性能差距。研究旨在通过引入补偿机制来缩小这一差距。

Method: 将输入稀疏化重新解释为动态结构化剪枝，并受生物神经元自发基线放电率的启发，引入一小组可训练的自发神经元作为补偿单元，以稳定稀疏化LLMs中的激活。

Result: 实验表明，这些辅助神经元显著减少了稀疏化引起的性能差距，并在不同任务上具有有效的泛化能力。

Conclusion: 通过引入自发神经元作为补偿机制，可以在保持输入稀疏化计算效率优势的同时，有效缓解其对LLMs表示能力的负面影响，提升稀疏化模型的性能。

Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

</details>


### [358] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

TL;DR: 本文提出FLFA框架，通过反馈对齐机制解决联邦学习中的数据异构性问题，在减少本地漂移的同时保持低计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据异构（non-IID）场景下存在本地漂移问题，影响全局模型收敛。

Method: FLFA框架在本地训练的反向传播过程中使用全局模型权重作为共享反馈矩阵，对齐本地更新与全局模型。

Result: 理论分析和实验验证表明FLFA能有效缓解本地漂移，提升模型性能且不增加额外通信开销。

Conclusion: FLFA是一种高效解决联邦学习数据异构问题的方法，具有良好的收敛性和实用性。

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [359] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: OLR-WAA是一个超参数自由的自适应在线回归模型，通过动态加权平均机制有效应对概念漂移问题，在非平稳数据流中实现模型稳定性与适应性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集经常存在不断变化的数据分布（概念漂移），传统在线模型的固定超参数缺乏动态调整能力，导致模型性能下降。

Method: 采用指数加权移动平均增量更新基础模型，引入独特的优化机制动态检测概念漂移、量化其幅度，并根据实时数据特征调整模型。

Result: 在静态设置下与批量回归性能相当，在概念漂移数据集上显著优于或匹敌最先进的在线模型，收敛速度快且R2值更高。

Conclusion: OLR-WAA有效解决了概念漂移问题，通过保守更新策略处理置信度场景，为非平稳数据流提供了有效的连续适应解决方案。

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [360] [Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset](https://arxiv.org/abs/2512.12783)
*Atalay Denknalbant,Emre Sezdi,Zeki Furkan Kutlu,Polat Goktas*

Main category: cs.LG

TL;DR: 该研究通过合成数据集和机器学习模型证明，使用替代性金融数据（如手机使用、网购行为等）可以有效评估缺乏正式信用记录的借款人，提升信用评分的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决金融排斥问题，帮助缺乏正式信用记录的伊斯坦布尔借款人获得公平的信贷评估，缩小财富差距。

Method: 创建合成数据集模拟伊斯坦布尔居民，使用检索增强生成技术生成包含社会人口统计变量和替代性行为属性的记录，并比较CatBoost、LightGBM和XGBoost模型在有无替代性数据下的性能。

Result: 替代性数据块使AUC提升约1.3个百分点，平衡F1分数从0.84提高到0.95，增益达14%，接近征信机构的判别能力。

Conclusion: 研究提供了透明的蓝图，证明简洁的行为属性集可以扩展公平和安全的信贷准入，服务缺乏正式信用记录的借款人。

Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.

</details>


### [361] [OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average](https://arxiv.org/abs/2512.12785)
*Mohammad Abu Shaira,Yunhe Feng,Heng Fan,Weishi Shi*

Main category: cs.LG

TL;DR: OLC-WA是一种自适应、无超参数的在线分类模型，通过指数加权移动平均融合数据流，自动检测概念漂移并动态调整模型，在动态数据流中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集存在随时间变化的数据分布（概念漂移），忽视这一问题会显著降低模型预测精度，而在线模型中的超参数通常固定，无法动态适应数据分布变化。

Method: 提出OLC-WA模型，通过指数加权移动平均将新数据流与基础模型融合，集成优化机制动态检测概念漂移、量化其幅度，并根据数据流特征调整模型。

Result: 在多种基准数据集上的实证评估显示，OLC-WA在静态环境中性能与批处理模型相当（准确率相差1-3%），在概念漂移情况下优于领先在线基线10-25%。

Conclusion: OLC-WA能够有效适应动态数据流中的演化数据分布，在保持静态环境性能的同时显著提升对概念漂移的适应能力。

Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.

</details>


### [362] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

TL;DR: 该论文针对在线学习中多算法跨数据集比较的统计检验问题，提出使用Friedman检验及后续检验方法，在真实和合成数据集上验证了现有基线方法的性能一致性，同时指出先进方法仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一数据集上两个学习算法的性能评估，而在线学习中多算法跨数据集比较的统计检验方法被忽视。在线学习需要统计显著性验证来确保快速收敛和有效处理概念漂移。

Method: 采用Friedman检验及相应的后续检验方法，在真实和合成数据集上使用5折交叉验证和种子平均进行综合评估，比较多个在线回归模型。

Result: 测试结果基本确认了竞争基线方法的性能与其个体报告一致，但某些统计检验结果也表明最先进方法在特定方面仍有改进空间。

Conclusion: 研究强调了在线学习中多算法比较统计检验的重要性，验证了现有方法的有效性，同时指出了未来改进方向。

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [363] [Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks](https://arxiv.org/abs/2512.12792)
*Shivansh Sahni,Wenzhi Zhang*

Main category: cs.LG

TL;DR: Liquid Reasoning Transformer (LRT) 是一种具有自适应深度的推理架构，通过迭代更新、基于丢弃的校正和学习停止机制，在结构化推理任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的单次前向传播模型难以处理复杂的结构化推理任务，无法纠正早期错误或根据输入难度调整计算量。

Method: LRT 使用循环推理令牌在多个内部步骤中进行迭代更新，结合丢弃门和停止门机制，实现自适应计算深度和错误校正。

Result: 在数独任务中达到98.68%的数字准确率和36.30%的完整谜题准确率，无需符号规则或搜索。

Conclusion: LRT 的机制可扩展到棋类规模推理任务，支持多令牌推理和更大领域应用。

Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.

</details>


### [364] [TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk](https://arxiv.org/abs/2512.12795)
*Mengying Yan,Ziye Tian,Siqi Li,Nan Liu,Benjamin A. Goldstein,Molei Liu,Chuan Hong*

Main category: cs.LG

TL;DR: TRACER框架通过迁移学习实时适应临床环境变化，解决了电子健康记录系统中因时间性人口变化导致的性能漂移问题，特别是在COVID-19等新疾病出现时能保持预测模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持工具常因时间性人口变化（如系统操作更新或新疾病出现）导致性能漂移，特别是在混合人群过渡阶段。需要一种无需完全重新训练的方法来适应这种演变环境。

Method: 提出TRACER框架，通过识别就诊级别的过渡成员关系，使用迁移学习技术调整预测模型，无需进行完整的模型重新训练。

Result: 在模拟研究中，TRACER优于基于历史或当代数据训练的静态模型。在实际应用中，预测急诊就诊后住院的COVID-19过渡期间，TRACER提高了模型的区分度和校准度。

Conclusion: TRACER为在演变和异质临床条件下保持稳健预测性能提供了一种可扩展的方法。

Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.

</details>


### [365] [From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs](https://arxiv.org/abs/2512.12805)
*Anastasiia Alokhina,Pan Li*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来分析Transformer在几何数据上的大小泛化能力，证明了具有稳定位置编码的Transformer在离散样本与连续域等价之间的误差边界，该边界由采样密度和数据流形的内在维度决定。


<details>
  <summary>Details</summary>
Motivation: Transformer展现出显著的大小泛化能力，能够从较小的token集合外推到更长的序列，这一现象在点云、图数据和自然语言处理等多个领域得到实证，但缺乏严格的理论刻画。

Method: 为几何数据（如从流形采样的点云、从图论采样的图）建立理论框架，分析Transformer输出在离散样本与连续域等价之间的误差边界，特别关注具有稳定位置编码的Transformer。

Result: 理论证明误差边界由采样密度和数据流形的内在维度决定，并在不同大小的图和点云数据集上的实验验证了理论边界的紧致性。

Conclusion: 研究为Transformer的大小泛化能力提供了理论支撑，表明其在处理几何数据时具有从离散样本有效逼近连续域的能力，且该能力受数据采样密度和内在维度的影响。

Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.

</details>


### [366] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

TL;DR: 该论文研究在概念漂移和有限预算下如何优化机器学习模型的训练和部署资源分配，提出了一个模型无关的框架来分析资源分配、概念漂移动态和部署时机之间的关系。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在边缘设备上的部署增多，模型提供商需要维护模型性能但面临概念漂移和预算限制的挑战。客户端设备支持本地推理但缺乏重新训练能力，性能维护负担落在提供商身上。

Method: 引入了一个模型无关的框架来捕捉资源分配、概念漂移动态和部署时机之间的相互作用。分析了概念持续时间的老化特性，针对突然概念变化推导了在预算约束下的最优训练策略，并研究了通信约束下的模型部署问题。

Result: 研究发现最优训练策略关键取决于概念持续时间的老化特性。在DMRL分布下推导了最优训练策略，证明在IMRL分布下直观启发式方法是次优的。针对通信约束下的部署问题，证明了优化问题的拟凸性，并提出了一种实现接近最优客户端性能的随机调度策略。

Conclusion: 该研究为在概念漂移下实现成本效益的ML模型管理提供了理论和算法基础，对持续学习、分布式推理和自适应ML系统具有重要意义。

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [367] [On the continuity of flows](https://arxiv.org/abs/2512.12821)
*Congzhou M Sha*

Main category: cs.LG

TL;DR: 本文研究了当先验分布和目标分布拓扑不匹配时，标准流匹配目标下的最优速度场可能出现空间不连续性的拓扑约束问题。


<details>
  <summary>Details</summary>
Motivation: 探索流匹配框架中存在的潜在拓扑约束问题，特别是当先验分布和目标分布拓扑不匹配时（如单峰到多峰），最优速度场可能表现出空间不连续性的现象。

Method: 通过对双峰高斯混合模型的理论分析，证明了最优速度场在决策边界处存在跳跃不连续性，且随着时间接近目标分布，不连续性的幅度趋于无穷大。

Result: 研究发现这种不连续性现象不是L2损失特有的，而是分布间拓扑不匹配的必然结果。通过实验验证了理论分析，并探讨了在流形上流匹配的潜在影响。

Conclusion: 拓扑不匹配会导致流匹配中的最优速度场出现不连续性，这为流匹配在流形上的应用以及神经网络学习不连续表示的挑战提供了新的理论视角。

Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.

</details>


### [368] [GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients](https://arxiv.org/abs/2512.12827)
*Mohammad Mahdi Razmjoo,Mohammad Mahdi Sharifian,Saeed Bagheri Shouraki*

Main category: cs.LG

TL;DR: 本文提出了一种基于输入损失流形几何特性的对抗性攻击检测方法，通过分析模型梯度参数的本征维度来区分自然数据和对抗性数据。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对微小对抗性扰动存在严重脆弱性，在医疗诊断和自动驾驶等可靠性要求高的应用中，需要有效的对抗性攻击检测方法。

Method: 研究模型输入损失流形的几何特性，分析模型梯度参数的本征维度，利用自然数据和对抗性数据在本征维度上的显著差异进行检测。

Result: 在MNIST和SVHN数据集上的批量检测中表现优异，在CIFAR-10和MS COCO等挑战性基准测试中实现了新的最先进结果，对抗CW和AutoAttack等多种攻击的检测率超过92%。

Conclusion: 本征维度是跨不同数据集和攻击策略的对抗性检测的强大指纹，几何方法具有显著的鲁棒性优势。

Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.

</details>


### [369] [Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future](https://arxiv.org/abs/2512.12832)
*Kaustav Chatterjee,Joshua Li,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 该研究开发了一个用于评估公路铁路平交道口(HRGC)悬挂风险的网络级框架，通过结合激光成像系统、深度学习模型和车辆尺寸数据分析，识别高风险交叉口并开发决策支持工具。


<details>
  <summary>Details</summary>
Motivation: 陡峭轮廓的公路铁路平交道口对低底盘车辆构成安全隐患，车辆可能在轨道上被困，导致火车与车辆碰撞的风险。

Method: 使用行走轮廓仪和Pave3D8K激光成像系统收集道口轮廓数据；开发结合LSTM和Transformer的混合深度学习模型重建道口轮廓；收集约350辆特种车辆的尺寸数据；采用三种车辆尺寸场景分析悬挂风险。

Result: 在三种场景下分别识别出36、62和67个最高风险道口；开发了ArcGIS数据库和软件界面支持交通安全管理。

Conclusion: 该框架通过整合新一代传感技术、深度学习和基础设施数据集，为交通安全评估提供了实用的决策支持工具。

Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.

</details>


### [370] [PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks](https://arxiv.org/abs/2512.12840)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Ananthram Swami,Rui Ning,Hongyi Wu,Jin-Hee Cho*

Main category: cs.LG

TL;DR: PRIVEE是一种保护垂直联邦学习中隐私的防御机制，通过混淆置信度分数来防止特征推断攻击，同时保持模型预测准确性。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习(VFL)虽然支持跨组织协作训练，但容易受到特征推断攻击，攻击者可以利用共享的置信度分数重构其他参与方的私有输入特征。

Method: PRIVEE通过转换置信度分数的表示形式来实现隐私保护，只共享变换后的表示而非原始分数，同时保持相对排名和分数间距离等关键属性。

Result: 实验表明，PRIVEE在隐私保护方面比现有最先进防御方法提高了三倍，同时完全保持了预测性能，能够有效对抗高级特征推断攻击。

Conclusion: PRIVEE提供了一种有效的隐私保护解决方案，能够在垂直联邦学习环境中平衡隐私保护和模型性能，为安全协作学习提供了新思路。

Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.

</details>


### [371] [Selective Conformal Risk Control](https://arxiv.org/abs/2512.12844)
*Yunpeng Xu,Wenge Guo,Zhi Wei*

Main category: cs.LG

TL;DR: 本文提出了选择性共形风险控制（SCRC）框架，将共形预测与选择性分类结合，通过两阶段方法在保证覆盖率和风险控制的同时生成更紧凑的预测集。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测虽然能提供分布无关的覆盖保证，但往往产生过大的预测集，限制了实际应用价值。需要一种既能保证可靠性又能生成紧凑预测集的方法。

Method: 提出了SCRC框架，包含两个算法：SCRC-T（保持可交换性，提供精确有限样本保证）和SCRC-I（仅校准变体，提供PAC式概率保证，计算更高效）。框架分为两阶段：第一阶段选择置信样本进行预测，第二阶段在选定子集上应用共形风险控制。

Result: 在两个公共数据集上的实验表明，两种方法都能达到目标覆盖率和风险水平，性能几乎相同。SCRC-I表现出略微保守的风险控制但计算实用性更优。

Conclusion: 选择性共形风险控制为紧凑、可靠的不确定性量化提供了一条有效且高效的路径。

Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.

</details>


### [372] [Information-Consistent Language Model Recommendations through Group Relative Policy Optimization](https://arxiv.org/abs/2512.12858)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.LG

TL;DR: 本文提出了一种基于GRPO的强化学习框架，通过将语义等效提示作为组处理并重置对话上下文，直接优化LLM在信息内容上的一致性，减少企业部署中的变异性问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在关键业务领域部署时，即使语义等效的提示词存在微小差异也会产生不一致的响应，这破坏了用户信任、增加了合规难度并影响了用户体验。企业场景需要信息传递的稳定性。

Method: 采用基于Group Relative Policy Optimization (GRPO)的强化学习框架，引入基于熵的有用性和稳定性奖励，将提示变体视为组并重置对话上下文以隔离措辞影响。

Result: 在投资和职位推荐任务上的实验表明，GRPO训练模型比微调或基于解码的基线方法更有效地减少了变异性。

Conclusion: 这是GRPO在信息一致性对齐方面的新颖应用，将变异性重新定义为企业部署中可纠正的缺陷而非生成多样性的可接受特征。

Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.

</details>


### [373] [Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels](https://arxiv.org/abs/2512.12870)
*Pouya Ahadi,Blair Winograd,Camille Zaug,Karunesh Arora,Lijun Wang,Kamran Paynabar*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的主动学习框架，通过优化分配查询样本给标注者来最小化标签噪声，从而构建鲁棒的分类模型。


<details>
  <summary>Details</summary>
Motivation: 在主动学习中，由于标注者的准确性不同，标签往往包含噪声，特别是复杂样本更容易被错误标注。从有噪声的标签中学习会导致分类器不准确。

Method: 提出包含分配模型和采样方法的主动学习框架：分配模型优化地将查询点分配给标注者以最小化每个周期内的最大可能噪声；新的采样方法识别最佳查询点以减少标签噪声对分类器性能的影响。

Result: 实验表明，该方法相比多个基准方法显著提高了分类性能。

Conclusion: 所提出的框架能够有效降低标签噪声，构建更鲁棒的分类模型，在主动学习场景中具有重要应用价值。

Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.

</details>


### [374] [Improving Recursive Transformers with Mixture of LoRAs](https://arxiv.org/abs/2512.12880)
*Mohammadmahdi Nouriborji,Morteza Rohanian,Omid Rohanian*

Main category: cs.LG

TL;DR: MoL（LoRA专家混合）是一种轻量级条件计算机制，通过在共享前馈网络中插入LoRA专家，实现递归transformer中参数共享下的表达能力恢复。


<details>
  <summary>Details</summary>
Motivation: 递归transformer中的参数共享会降低层间表达能力，需要一种轻量级方法来恢复这种表达能力而不增加过多参数。

Method: 提出MoL机制，在共享FFN中插入LoRA专家，实现基于token条件的权重空间调制；使用ModernALBERT架构集成旋转嵌入、GeGLU、FlashAttention等技术；提出专家合并过程在推理时将MoL压缩为单一适配器。

Result: 在GLUE、SQuAD-v2和BEIR基准测试中，ModernALBERT（50M-120M参数）在紧凑模型中达到SOTA性能，超越更大的完全参数化基线模型。

Conclusion: 条件权重空间调制能有效恢复递归transformer中因激进参数共享而丧失的表达能力，MoL机制在保持模型紧凑性的同时显著提升性能。

Abstract: Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.

</details>


### [375] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 本文提出了一种无监督学习算法，用于学习多尺度神经数据中的切换动态系统模型，能够同时处理连续和离散的神经信号模态，在行为解码方面优于单尺度和静态多尺度模型。


<details>
  <summary>Details</summary>
Motivation: 现有的切换动态系统方法主要针对单一神经模态（连续高斯信号或离散泊松信号），但实际神经记录通常包含多种模态。同时，训练数据中缺乏状态标签，使得学习状态依赖的切换动态变得困难。

Method: 开发了一种无监督学习算法，仅使用多尺度神经观测数据来学习切换多尺度动态系统模型的参数。该方法能够同时融合连续和离散的神经信号模态。

Result: 在模拟数据和两个不同的实验数据集（包含多模态尖峰-LFP观测）上验证了方法的有效性。与切换单尺度动态模型相比，该方法在行为解码方面更准确；与静态多尺度模型相比，表现更好，说明跟踪状态依赖的非平稳性很重要。

Conclusion: 该无监督学习框架通过利用多模态记录中的信息并纳入状态切换，能够更准确地建模复杂的多尺度神经动态，有望提高脑机接口的性能和鲁棒性，并增进对行为神经基础的理解。

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [376] [Distillation of Discrete Diffusion by Exact Conditional Distribution Matching](https://arxiv.org/abs/2512.12889)
*Yansong Gao,Yu Sun*

Main category: cs.LG

TL;DR: 提出了一种基于条件分布匹配的离散扩散模型蒸馏方法，通过匹配教师模型和学生模型的条件分布来加速推理过程


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型推理成本高，需要大量函数评估，现有加速方法要么依赖近似模拟器，要么需要训练新的学生模型和代理目标

Method: 利用反向条件分布p_{0∣t}(x_0∣x_t)的马尔可夫分解特性，通过边缘密度比和已知的前向CTMC核来恢复条件分布，定义直接匹配教师和学生模型条件分布的蒸馏目标

Result: 为一步和少步采样器设计了基于条件分布匹配的蒸馏目标

Conclusion: 该方法为离散扩散模型提供了一种简单而原则性的蒸馏替代方案，能够有效降低推理成本

Abstract: Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $τ$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\mid t}(x_0 \mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.

</details>


### [377] [Wait, Wait, Wait... Why Do Reasoning Models Loop?](https://arxiv.org/abs/2512.12895)
*Charilaos Pipis,Shivam Garg,Vasilis Kontonis,Vaishnavi Shrivastava,Akshay Krishnamurthy,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: 该论文研究了推理模型在低温度设置下产生重复循环的原因，发现学习错误（训练分布与学习模型不匹配）是主要原因，并提出了两种机制解释循环现象。


<details>
  <summary>Details</summary>
Motivation: 推理模型在解决复杂问题时经常产生重复的循环文本，特别是在低温度或贪婪解码设置下。研究者希望理解这种现象的根本原因以及温度在其中扮演的角色。

Method: 使用开源推理模型进行实验分析，引入合成图推理任务来演示两种循环机制：风险规避（由于学习难度）和Transformer对时间相关错误的归纳偏置。

Result: 发现低温度下循环现象普遍，大模型循环较少，而蒸馏学生模型即使教师模型很少循环也会显著循环。温度升高通过促进探索减少循环，但不能解决学习错误问题。

Conclusion: 温度只是临时解决方案而非根本解决方法，需要从训练阶段干预来直接减少学习错误，才能真正解决推理模型的循环问题。

Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.

</details>


### [378] [Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning](https://arxiv.org/abs/2512.12896)
*Parthasarathy Nadarajan,Michael Botsch*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的预测占用网格方法，用于实时预测复杂交通场景中多参与者的行为演化，通过随机森林算法降低计算负载，提高车辆安全系统的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的预测方法由于需要考虑交通参与者的大量可能轨迹，计算负载过高，难以实现实时应用。需要一种能够详细建模行为不确定性且计算效率高的方法。

Method: 采用机器学习方法计算预测占用网格，使用增强的占用网格单元表示当前交通场景状态，基于随机森林算法实现从当前状态到未来预测的映射。

Result: 通过交通场景仿真验证，机器学习方法相比基于模型的方法在保持预测准确性的同时显著降低了计算负载，有望实现预测占用网格的实时计算。

Conclusion: 该方法能够详细建模交通参与者的行为不确定性，可显著改进车辆安全系统中的关键组件，如危险度评估和轨迹规划，为实时安全应用提供了可行方案。

Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.

</details>


### [379] [Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm](https://arxiv.org/abs/2512.12901)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 本文提出了一种基于机器学习的概率时空表示方法，用于预测复杂交通场景中参与者的未来行为，通过分层分类器和随机森林实现预测-占用网格（POG）的生成。


<details>
  <summary>Details</summary>
Motivation: 为主动车辆安全应用提供准确的交通场景预测，特别是在复杂交通环境中执行动态机动操作时，需要可靠的未来行为预测。

Method: 使用分层情境分类器区分交通场景类型，然后通过堆叠去噪自编码器（SDA）降低特征维度，再用随机森林（RF）预测POG。

Result: 该方法在仿真和真实车辆实验中表现出优异性能，能够有效预测交通参与者的未来行为。

Conclusion: 提出的机器学习方法（SDA+RF）能准确预测复杂交通场景，POG可用于评估场景危险性和确定安全轨迹。

Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.

</details>


### [380] [Next-generation reservoir computing validated by classification task](https://arxiv.org/abs/2512.12903)
*Ken-ichi Kitayama*

Main category: cs.LG

TL;DR: 本文首次证明下一代储层计算（NG-RC）在分类任务上能达到与传统储层计算相当的性能，验证了NG-RC在预测和分类任务中的通用计算能力。


<details>
  <summary>Details</summary>
Motivation: 现有NG-RC基准测试仅限于时间序列预测任务（如Lorenz 63吸引子和Mackey-Glass混沌信号），缺乏对分类任务的验证，需要证明NG-RC在更广泛任务中的适用性。

Method: NG-RC不需要实际储层进行输入数据混合，而是直接从时间序列输入计算多项式项，简化了计算过程。

Result: NG-RC在分类任务上表现出与传统储层计算相当的性能，证明了其在两类任务（预测和分类）中的通用计算能力。

Conclusion: NG-RC具有多功能计算能力，不仅适用于时间序列预测，也能有效处理分类任务，拓展了该计算范式的应用范围。

Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.

</details>


### [381] [Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic](https://arxiv.org/abs/2512.12907)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

TL;DR: 本文提出了一种用于高效估计复杂交通场景概率时空表示的新型机器学习架构，通过识别交通场景类型并映射当前状态到未来状态，为自动驾驶和主动安全系统提供详细预测。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶和主动安全系统提供准确的未来交通场景预测，需要详细的概率时空表示来应对交通参与者的不确定性行为。

Method: 使用两个堆叠去噪自编码器（SDA）和随机森林集合组成的新型架构，输入为增强占用网格（AOG），输出为预测占用网格（POG）。

Result: 通过仿真验证，该架构在准确性和计算时间方面均优于现有的SDA和DeconvNet架构。

Conclusion: 提出的新型机器学习架构能够有效预测交通场景的概率时空表示，在主动安全领域具有重要应用价值。

Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.

</details>


### [382] [LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization](https://arxiv.org/abs/2512.12922)
*Bangyu Li,Boping Gu,Ziyang Ding*

Main category: cs.LG

TL;DR: 本文提出了一种基于LLM的个性化投资组合推荐框架，结合大语言模型、强化学习和个体风险偏好建模，以解决传统方法无法捕捉投资者行为与市场动态非线性交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现代金融市场中，投资者需要能够反映个人风险偏好并适应动态市场条件的个性化投资策略，而传统基于规则或静态优化的方法难以满足这一需求。

Method: 提出LLM-based Personalized Portfolio Recommender框架，整合大语言模型、强化学习和个体风险偏好建模技术。

Result: 该框架能够支持智能投资决策，通过捕捉投资者行为、市场波动和金融目标之间的非线性交互关系。

Conclusion: 该集成方法为个性化投资组合推荐提供了新的解决方案，能够更好地适应现代金融市场的复杂需求。

Abstract: In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.

</details>


### [383] [SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision](https://arxiv.org/abs/2512.12930)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.LG

TL;DR: SeVeDo是一种基于SVD的异构加速器，通过将异常值敏感组件分离到高精度低秩路径，其余计算在低比特残差数据路径中执行，结合层次化分组量化和SVD引导的混合精度，实现了高效的Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 低比特量化是减少Transformer推理计算和内存开销的有效技术，但激进的位宽缩减会因激活异常值导致精度下降。现有方法虽然精度高但能耗大，需要更节能的解决方案。

Method: 提出SeVeDo加速器：1）结构分离异常值敏感组件到高精度低秩路径；2）层次化分组量化(HGQ)结合粗粒度浮点缩放和细粒度移位；3）SVD引导混合精度(SVD-MP)静态分配更高位宽给精度敏感组件。

Result: 实验结果显示SeVeDo达到峰值能效13.8TOPS/W，在ViT-Base上为12.7TOPS/W，在Llama2-7B上为13.4TOPS/W，优于传统设计。

Conclusion: SeVeDo通过创新的异构架构和量化策略，在保持精度的同时显著提升了Transformer推理的能效，为低比特量化提供了有效的解决方案。

Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.

</details>


### [384] [Investigating Data Pruning for Pretraining Biological Foundation Models at Scale](https://arxiv.org/abs/2512.12932)
*Yifan Wu,Jiyue Jiang,Xichen Ye,Yiqi Wang,Chang Zhou,Yitao Xu,Jiayang Chen,He Hu,Weizhong Zhang,Cheng Jin,Jiao Yuan,Yu Li*

Main category: cs.LG

TL;DR: 本文提出了一种针对生物基础模型的后验影响引导数据剪枝框架，能够在极端的剪枝率（超过99%）下有效减少训练数据量，同时保持甚至超越随机选择更大数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 生物基础模型需要大量训练数据和参数，导致计算成本高昂，阻碍了学术实验室的可重复性和可访问性。

Method: 提出了基于子集的自影响公式来高效估计样本重要性，并构建了Top-k影响和覆盖中心影响两种选择策略。

Result: 在RNA-FM和ESM-C模型上的实验表明，该方法在RNA和蛋白质相关任务中均优于随机选择基线，甚至在数据量仅为十分之一的情况下仍能超越随机选择。

Conclusion: 影响引导数据剪枝能够显著降低生物基础模型的预训练计算成本，为更高效、可访问和可持续的生物AI研究铺平道路。

Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.

</details>


### [385] [Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties](https://arxiv.org/abs/2512.12947)
*Nischal Subedi,Ember Kerstetter,Winnie Li,Silo Murphy*

Main category: cs.LG

TL;DR: 本文通过诊断性研究分析了图卷积网络(GCNs)在半监督节点分类中的表现，发现GCN性能主要取决于图同质性和特征质量之间的相互作用。GCN在标签极度稀缺时表现最佳，但在低同质性和强特征条件下会损害性能。


<details>
  <summary>Details</summary>
Motivation: 为实践者提供关于何时GCN能比简单基线方法带来有意义改进的明确指导，理解GCN在什么条件下有效以及为什么有效。

Method: 使用Amazon Computers共购数据，通过模拟标签稀缺性、特征消融和按类别分析的系统性实验进行诊断研究。

Result: GCN在极端标签稀缺时提供最大增益，利用邻域结构弥补有限监督；在高同质性图上，即使节点特征被随机噪声替代，GCN仍能保持原性能；但在低同质性和强特征条件下，GCN会损害性能。

Conclusion: 四分位分析显示GCN在四种条件中的三种下都有帮助，仅在低同质性遇到强特征时有害。这些发现为实践者决定是否采用基于图的方法提供了实用指导。

Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

</details>


### [386] [Application of Deep Learning in Biological Data Compression](https://arxiv.org/abs/2512.12975)
*Chunyu Zou*

Main category: cs.LG

TL;DR: 本文提出了一种基于隐式神经表示（INR）的深度学习方法，用于压缩冷冻电镜（Cryo-EM）生物数据，通过结合位置编码和加权MSE损失函数来提高重建质量。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜数据文件存储体积庞大，给研究和教育带来挑战，需要高效的压缩解决方案。

Method: 首先根据密度阈值提取每个文件的二值图，利用GZIP压缩高度重复的密度图，然后训练神经网络编码空间密度信息，存储网络参数和可学习潜在向量，并引入位置编码和加权MSE损失函数优化重建精度。

Result: 该方法旨在实现合理的压缩比和文件间重建质量，为教育和研究提供实用的生物数据压缩方案。

Conclusion: 基于INR的深度学习压缩方法有望有效解决冷冻电镜数据存储问题，平衡压缩效率与重建质量。

Abstract: Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.

</details>


### [387] [CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks](https://arxiv.org/abs/2512.12981)
*Jonathan Wenshøj,Tong Chen,Bob Pepin,Raghavendra Selvan*

Main category: cs.LG

TL;DR: CoDeQ是一种完全可微分的联合剪枝-量化方法，通过参数化量化器的死区宽度来直接诱导稀疏性，无需额外辅助过程即可实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 现有的联合剪枝-量化方法依赖训练循环外的辅助程序来确定压缩参数，增加了工程复杂性和超参数调优，且缺乏直接的数据驱动梯度信号，可能导致次优压缩。

Method: 基于标量量化器的死区等价于幅度剪枝的观察，参数化死区宽度并通过反向传播学习，同时学习量化参数。该方法通过单一全局超参数控制稀疏性，并将稀疏性选择与比特宽度选择解耦。

Result: 在ImageNet上使用ResNet-18，CoDeQ将比特操作减少到约5%，同时在固定精度和混合精度模式下保持接近全精度的准确率。

Conclusion: CoDeQ提供了一种简单、架构无关且易于实现的联合压缩方法，能够同时确定稀疏模式和量化参数，无需任何辅助程序。

Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.

</details>


### [388] [Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)](https://arxiv.org/abs/2512.13010)
*Hassan Iftikhar,Rizwan Ahmad,Arunark Kolipaka*

Main category: cs.LG

TL;DR: DIME算法通过深度学习提升MRE中剪切模量估计的鲁棒性，相比传统MMDI方法在模拟和真实数据中表现更优，具有更高准确性和临床可行性。


<details>
  <summary>Details</summary>
Motivation: 传统MMDI算法基于Helmholtz方程，假设介质均匀、无限且同质，对噪声敏感，导致刚度估计精度和可靠性不足。

Method: 提出DIME框架，基于有限元建模生成的位移场-刚度图对进行训练，采用小图像块捕捉局部波行为，提升对全局图像变化的鲁棒性。

Result: 在模拟数据中DIME显示低像素变异性、准确边界划分，与真实值相关性更高（r=0.99，R²=0.98）；在体内数据中保持生理一致性刚度模式，与MMDI结果相近但偏差更小。

Conclusion: DIME初步验证了在MRE临床应用中的可行性，相比MMDI具有更高准确性和更小偏差。

Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.

</details>


### [389] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

TL;DR: 本文提出一种优化Transformer训练的方法，通过非对称投影将反向传播梯度分解为平行分量和正交违规项，在保持标准前向QKV结构的同时提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的注意力梯度存在几何效率低下的问题，需要优化训练过程以提升性能。

Method: 使用非对称投影将反向传播梯度分解为平行跨度和平交违规项，并选择性地对第0阶双向平行跨度进行缩放。

Result: 在WikiText-2数据集上，该方法实现了验证损失降低0.56%，证明了框架的有效性。

Conclusion: 该方法为Transformer训练提供了更有效的学习信号，在更大数据集和更深层训练中具有显著潜力。

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [390] [Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization](https://arxiv.org/abs/2512.13034)
*Xiaoyu He,Yu Cai,Jin Jia,Canxi Huang,Wenqing Chen,Zibin Zheng*

Main category: cs.LG

TL;DR: Alada是一种用于大规模矩阵随机优化的自适应动量方法，采用秩一分解估计梯度二阶矩，通过交替更新因子最小化估计误差，具有次线性内存开销并可扩展至张量优化。


<details>
  <summary>Details</summary>
Motivation: 解决传统自适应优化方法（如Adam）在大规模矩阵优化中内存开销过大的问题，同时保持算法性能。

Method: 使用秩一分解方法估计梯度二阶矩，因子交替更新以最小化估计误差；结合一阶矩估计规则增强算法鲁棒性且不增加额外内存开销。

Result: 理论性能与传统方法（如Adam）相当；在多个自然语言处理任务上的数值研究表明，相比Adam及其变体，内存开销显著降低且训练大型模型时更鲁棒。

Conclusion: Alada在保持与传统自适应优化方法相当性能的同时，显著降低了内存开销，特别适用于大规模矩阵和张量优化问题。

Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

</details>


### [391] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

TL;DR: FinFRE-RAG是一个两阶段方法，通过特征缩减和检索增强学习，将LLMs应用于金融欺诈检测，在保持可解释性的同时提升性能


<details>
  <summary>Details</summary>
Motivation: 传统表格模型需要大量特征工程且可解释性差，而LLMs虽然能生成可读解释但直接应用于欺诈检测效果不佳

Method: 两阶段方法：重要性引导的特征缩减将数值/分类属性序列化为自然语言，然后基于标签感知的实例级样本进行检索增强的上下文学习

Result: 在四个公共欺诈数据集和三类开源LLMs上，FinFRE-RAG显著优于直接提示方法，并在某些设置中与专业表格基线竞争

Conclusion: LLMs虽然仍落后于专业分类器，但缩小了性能差距并提供可解释的理由，在欺诈分析中具有辅助工具价值

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [392] [Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments](https://arxiv.org/abs/2512.13060)
*Kangning Gao,Yi Hu,Cong Nie,Wei Li*

Main category: cs.LG

TL;DR: 提出基于深度Q学习的智能调度优化框架，解决异构数据环境下ETL流程调度效率低、资源分配不均和适应性差的问题


<details>
  <summary>Details</summary>
Motivation: 解决异构数据环境中ETL流程的低调度效率、不平衡资源分配和差适应性问题

Method: 将ETL调度过程形式化为马尔可夫决策过程，使用强化学习代理在高维状态空间中进行自适应决策，包含状态表示模块、特征嵌入网络、Q值估计器和奖励评估机制

Result: 显著降低了调度延迟，提高了系统吞吐量，增强了多源异构任务条件下的执行稳定性

Conclusion: 深度Q学习调度框架在复杂数据调度和资源管理中具有强大潜力，为智能数据管道构建提供了高效可扩展的优化策略

Abstract: This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.

</details>


### [393] [Multi-fidelity aerodynamic data fusion by autoencoder transfer learning](https://arxiv.org/abs/2512.13069)
*Javier Nieto-Centenero,Esther Andrés,Rodrigo Castellanos*

Main category: cs.LG

TL;DR: 本文提出了一种多保真度深度学习框架，结合自动编码器迁移学习和多分割共形预测策略，在极端数据稀缺条件下实现不确定性感知的空气动力学数据融合。


<details>
  <summary>Details</summary>
Motivation: 高保真度模拟计算成本过高，限制了数据驱动建模的应用，因此需要开发多保真度策略来利用廉价低保真度信息而不牺牲精度。

Method: 利用丰富的低保真度数据学习紧凑的潜在物理表示，作为解码器的冻结知识库，然后使用稀缺的高保真度样本对解码器进行微调。采用多分割共形预测策略进行不确定性量化。

Result: 在NACA翼型（2D）和跨音速机翼（3D）数据库上测试，模型成功校正了低保真度偏差，使用极少的高保真度训练数据实现了高精度压力预测。MSCP框架产生了稳健、可操作的不确定性带，点覆盖率达到95%以上。

Conclusion: 通过结合极端数据效率和不确定性量化，这项工作为数据稀缺环境下的空气动力学回归提供了可扩展且可靠的解决方案。

Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.

</details>


### [394] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [395] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

TL;DR: 本文提出了一种半监督的RLVR方法TraPO，通过少量标注样本指导无标注样本的强化学习训练，解决了纯无监督方法在训练后期出现的模型崩溃问题，在数学推理任务上实现了显著的数据效率提升。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法依赖大量标注成本高昂，而无监督RLVR方法容易在训练后期出现模型崩溃。因此需要一种既能减少标注成本又能保持训练稳定性的半监督方法。

Method: 提出TraPO算法，通过匹配无标注样本与标注样本的学习轨迹相似度来识别可靠的无标注样本，利用标注样本的监督奖励来稳定一致性训练，确保只将已验证的推理模式纳入RL训练。

Result: 在6个数学推理基准测试和3个分布外任务上，仅使用1K标注和3K无标注样本就达到42.6%平均准确率，超越使用45K无标注样本的最佳无监督方法（38.3%）。使用4K标注和12K无标注样本时，性能甚至超过使用全部45K标注样本的全监督模型。

Conclusion: TraPO证明了半监督RLVR的有效性，通过少量标注样本指导无标注样本训练，既能显著降低标注成本，又能避免模型崩溃，在数学推理任务上实现了优异的性能和泛化能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [396] [From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network](https://arxiv.org/abs/2512.13111)
*Hayk Amirkhanian,Marco F. Huber*

Main category: cs.LG

TL;DR: 该论文提出了一种名为HABNN的分层近似贝叶斯神经网络，通过使用高斯-逆Wishart分布作为网络权重的超先验，提高了模型的鲁棒性和性能，并在分布外任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中存在的超参数调优困难和过拟合问题，贝叶斯神经网络通过将不确定性直接纳入模型来提供更可靠的预测，特别是在处理分布外数据时。

Method: 使用高斯-逆Wishart分布作为网络权重的超先验，提出了预测分布和权重后验的解析表示，这些表示可以以线性复杂度计算学生t分布的参数。

Result: HABNN不仅与最先进模型相媲美，而且往往表现更优，有效解决了过拟合问题，并为分布外任务提供了可靠的不确定性估计。

Conclusion: HABNN为安全关键环境中的未来应用提供了一个有前景的方向，展示了在提高模型鲁棒性和性能方面的潜力。

Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.

</details>


### [397] [Quanvolutional Neural Networks for Spectrum Peak-Finding](https://arxiv.org/abs/2512.13125)
*Lukas Bischof,Rudolf M. Füchslin,Kurt Stockinger,Pavel Sulimov*

Main category: cs.LG

TL;DR: 本文探索了量子卷积神经网络（QuanvNNs）在核磁共振谱峰分析中的应用，相比传统CNN在复杂光谱上表现更优，F1分数提升11%，峰值位置估计误差降低30%。


<details>
  <summary>Details</summary>
Motivation: 传统核磁共振谱峰分析（去卷积）对专家和机器都极具挑战，量子计算技术有望提升现有机器学习方法的性能。受经典CNN成功启发，研究量子卷积神经网络在峰值检测问题中的应用潜力。

Method: 设计了一个简单可解释的QuanvNN架构，可直接与经典CNN对比，在合成的NMR数据集上进行多任务峰值检测（包括峰值计数和位置估计）。

Result: QuanvNNs在复杂光谱上优于经典CNNs，F1分数提高11%，峰值位置估计的平均绝对误差降低30%，且在困难问题上表现出更好的收敛稳定性。

Conclusion: 量子卷积神经网络在光谱分析任务中具有显著优势，为量子机器学习在科学数据分析中的应用提供了有前景的方向。

Abstract: The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\% improvement in F1 score and a 30\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.

</details>


### [398] [Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency](https://arxiv.org/abs/2512.13149)
*Xinwei Tai,Dongmian Zou,Hongfei Wang*

Main category: cs.LG

TL;DR: 本文提出通过解相关节点特征来改进无监督图域自适应（GDA），解决条件偏移问题，并通过理论分析和实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 图域自适应中条件偏移问题阻碍了知识迁移，研究发现条件偏移仅存在于节点特征存在局部依赖时，因此需要解相关节点特征来提升迁移性能。

Method: 提出通过解相关GCN层和图变换器层来实现节点特征解相关，从而减少条件偏移，改进GDA性能。

Result: 实验结果表明该方法在基线GDA方法基础上显著提升性能，学习到的表征具有较小的类内距离。

Conclusion: 解相关节点特征是解决图域自适应中条件偏移问题的有效方法，理论分析和实验验证了该方法的优越性。

Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT

</details>


### [399] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

TL;DR: 本文提出了一种将Soft Actor-Critic（SAC）与n步回报相结合的方法，解决了传统方法中由于重要性采样导致的数值不稳定问题，并提出了τ采样熵估计来降低学习目标的方差。


<details>
  <summary>Details</summary>
Motivation: SAC是实际应用中最相关的离策略在线无模型强化学习方法之一。n步回报技术可以提高RL算法的收敛速度，但SAC与n步回报的结合存在困难，因为传统组合会因动作分布变化而引入偏差，而重要性采样又可能导致数值不稳定。

Method: 提出了一种应用数值稳定重要性采样的方法，简化了超参数选择；分析了SAC在n步最大熵框架下的熵估计方法，并制定了τ采样熵估计以减少学习目标的方差；最终制定了SACn算法。

Result: 在MuJoCo模拟环境中进行了实验验证，表明所提方法能够有效结合SAC与n步回报，提高算法性能。

Conclusion: 成功开发了SACn算法，克服了SAC与n步回报结合时的数值不稳定问题，并通过τ采样熵估计降低了方差，为实际应用提供了更稳定高效的强化学习解决方案。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [400] [PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning](https://arxiv.org/abs/2512.13186)
*Khalid Ferji*

Main category: cs.LG

TL;DR: 本文提出了PolySet框架，将聚合物表示为从摩尔质量分布中采样的有限加权链集合，解决了传统机器学习模型将聚合物视为单一分子图与物理现实不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 当前聚合物科学中的机器学习模型通常将聚合物视为单一完美定义的分子图，而真实材料是由具有分布长度的链组成的随机集合。这种物理现实与数字表示之间的不匹配限制了现有模型捕捉聚合物行为的能力。

Method: 引入PolySet框架，将聚合物表示为从假设的摩尔质量分布中采样的有限加权链集合。这种基于集合的编码独立于化学细节，兼容任何分子表示，并在均聚物情况下使用最小语言模型进行说明。

Result: PolySet保留了高阶分布矩（如Mz、Mz+1），使机器学习模型能够学习尾部敏感特性，显著提高了稳定性和准确性。

Conclusion: 通过明确承认聚合物物质的统计性质，PolySet为未来的聚合物机器学习建立了物理基础，可自然扩展到共聚物、嵌段结构和其他复杂拓扑结构。

Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.

</details>


### [401] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

TL;DR: 本文提出了一种名为WAY的新型深度学习架构，用于船舶目的地估计，通过将长距离港口到港口轨迹重新表述为嵌套序列结构，并使用空间网格来减轻时空偏差。


<details>
  <summary>Details</summary>
Motivation: 自动识别系统（AIS）在海上监视中存在可靠性问题和数据间隔不规则的问题，需要一种能够提前数天至数周准确估计船舶目的地的解决方案。

Method: WAY架构包括轨迹表示层和通道聚合顺序处理（CASP）块。表示层从运动学和非运动学特征生成多通道向量序列，CASP块使用多头通道注意力和自注意力进行聚合和顺序信息传递。此外，还提出了专门的任务梯度丢弃（GD）技术，用于在单个标签上进行多对多训练。

Result: 在5年AIS数据上的实验表明，WAY优于传统的基于空间网格的方法，且采用GD技术带来了性能提升。多任务学习也展示了WAY在实际应用中的潜力。

Conclusion: WAY架构通过创新的轨迹表示和注意力机制，有效解决了长距离船舶目的地估计问题，GD技术的引入进一步提升了模型性能，展示了在实际海上监视应用中的可行性。

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [402] [Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning](https://arxiv.org/abs/2512.13196)
*Chethana Prasad Kabgere,Sudarshan T S B*

Main category: cs.LG

TL;DR: 本文提出噪声弹性量子联邦学习（NR-QFL）框架，通过变分量子电路在NISQ条件下实现安全、低延迟的模型聚合，解决传统联邦学习在车载网络中的噪声、延迟和安全问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在实时车载网络中面临噪声、延迟和安全约束，需要一种能够在边缘条件下保持高效性和稳定性的新型聚合方法。

Method: 采用混合量子-经典框架，将模型参数编码为量子态，使用变分量子电路进行自适应门重参数化，结合基于量子熵的客户端选择和多服务器协调机制。

Result: 经验验证显示NR-QFL在受限边缘条件下实现稳定收敛，梯度方差降低，通信开销减少，噪声容忍度显著提升。

Conclusion: NR-QFL为量子增强的联邦学习建立了可扩展基础，能够在车载边缘实现安全、高效且动态稳定的ADAS智能。

Abstract: Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.

</details>


### [403] [Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting](https://arxiv.org/abs/2512.13207)
*Karina Chichifoi,Fabio Merizzi,Michele Colajanni*

Main category: cs.LG

TL;DR: 本文研究了联邦学习在气象预报中的安全漏洞，特别是数据投毒攻击对地表温度预测的影响，发现即使少量恶意客户端也能通过空间相关性造成大范围预测偏差，并评估了防御机制的效果。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和联邦学习在气象预报中的广泛应用，其分布式特性带来了新的安全威胁。数据投毒攻击可能通过操纵训练数据来降低模型性能或引入系统性偏差，这种威胁在具有空间相关性的气象数据中被放大。

Method: 使用Copernicus欧洲区域再分析数据集，模拟地理分布的客户端，评估基于补丁和全局偏置攻击对区域温度预测的影响，并测试修剪均值聚合作为防御机制的有效性。

Result: 即使只有少量中毒客户端也能误导大范围空间连接区域的预测：单个客户端的全局温度偏置攻击可使预测偏移达-1.7K；协调的补丁攻击使均方误差增加三倍以上，并产生超过+3.5K的持续区域异常。修剪均值聚合能有效防御全局偏置攻击（性能下降2-13%），但对补丁攻击无效（性能恶化281-603%）。

Conclusion: 联邦学习在气象预报中存在严重的安全漏洞，基于异常值的防御机制对空间相关数据的补丁攻击无效，需要开发更强大的防御策略来应对空间相关的威胁模型。

Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.

</details>


### [404] [ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data](https://arxiv.org/abs/2512.13228)
*Melvin Barbaux*

Main category: cs.LG

TL;DR: ModSSC是一个开源的Python框架，统一了归纳和直推式半监督分类，提供模块化代码库和广泛的数据集支持。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督分类软件支持分散在不同方法和模态中，缺乏统一的框架。

Method: 开发了ModSSC框架，实现了多种经典和最新算法，支持表格、图像、文本、音频和图形数据集，提供统一的配置接口和YAML声明式实验描述。

Result: ModSSC 1.0.0已发布，支持从CPU上的轻量级方法到多GPU的深度方法，便于复现现有工作和进行大规模比较研究。

Conclusion: ModSSC通过统一的框架解决了半监督分类软件碎片化问题，促进了该领域的研究和发展。

Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.

</details>


### [405] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

TL;DR: 本文提出CORE框架，将掩码特征重建（MFR）与图对比学习（GCL）相结合，在理论上证明两者目标一致的基础上，通过构建正负样本对来提升图自监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 发现MFR和GCL都旨在最大化相似元素间的一致性，尽管操作机制不同，但在特定条件下目标函数会收敛，这表明两种方法是互补的而非根本不同的。

Method: 提出Contrastive Masked Feature Reconstruction (CORE)框架，将对比学习融入MFR：1）仅使用原始和重建的掩码节点特征作为正样本对；2）将掩码节点自身作为负样本，结合MFR的重建能力和GCL的判别能力。

Result: CORE在节点分类和图分类任务上显著优于MFR，达到最先进水平：在节点分类上比GraphMAE和GraphMAE2分别提升2.80%和3.72%，在图分类上分别提升3.82%和3.76%。

Conclusion: CORE框架成功整合了生成式和对比式学习方法，证明了理论上的联系可以转化为实际性能提升，为图自监督学习提供了新的有效解决方案。

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [406] [Learning to Retrieve with Weakened Labels: Robust Training under Label Noise](https://arxiv.org/abs/2512.13237)
*Arnab Sharma*

Main category: cs.LG

TL;DR: 本文提出了一种标签弱化方法，用于在存在标签噪声的情况下训练鲁棒的检索模型，通过允许一组可能的标签而非单一标签来缓解稀疏标注和标签噪声问题。


<details>
  <summary>Details</summary>
Motivation: 神经编码器在NLP领域用于密集检索任务时，训练数据中的稀疏标注和标签噪声使得模型训练困难。现有方法需要调整超参数或增加训练复杂性，因此需要更简单有效的解决方案。

Method: 采用标签弱化方法，为每个查询-文档对生成一组可能的标签，而不是强制使用单一标签。这些标签基于观察到的监督信号和模型的置信度得分。在两个检索模型和一个重排序模型上进行了评估，并使用语义感知噪声生成技术创建不同噪声比例的真实噪声环境。

Result: 初步结果显示，标签弱化方法相比10种最先进的损失函数，能够提高检索任务的性能。

Conclusion: 标签弱化是一种有效的技术，能够在存在标签噪声的情况下提升检索模型的鲁棒性和性能，且不需要复杂的超参数调整。

Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.

</details>


### [407] [BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation](https://arxiv.org/abs/2512.13255)
*Yunhong Min,Juil Koo,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.LG

TL;DR: BézierFlow是一种轻量级训练方法，用于预训练扩散和流模型的少步生成，在≤10次NFE采样时实现2-3倍性能提升，仅需15分钟训练。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级训练方法局限于ODE离散化，需要扩展搜索空间以优化采样轨迹变换。

Method: 通过将随机插值调度器参数化为Bézier函数，利用控制点自然满足边界条件、可微性和SNR单调性等关键要求。

Result: 在多种预训练扩散和流模型上，BézierFlow始终优于先前的时序学习方法。

Conclusion: 将搜索空间从离散时序扩展到基于Bézier的轨迹变换是有效的改进方向。

Abstract: We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.

</details>


### [408] [No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction](https://arxiv.org/abs/2512.13300)
*Qinglin Jia,Zhaocheng Du,Chuhan Wu,Huifeng Guo,Ruiming Tang,Shuting Shi,Muyu Zhang*

Main category: cs.LG

TL;DR: 本文提出KAML框架，通过属性驱动掩码策略和分层知识提取机制，解决多任务学习中因广告商仅提交部分转化数据导致的标签不对称问题，在线上线下测试中显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现实广告系统中，广告商通常有多样化的客户获取目标，但只提交部分用户转化行为数据，导致多任务学习数据标签不完整且分布不对称，传统方法在部署时面临训练与部署数据分布不匹配的问题。

Method: 提出KAML框架：1）属性驱动掩码策略（ADM）更好地利用不对称多标签数据；2）分层知识提取机制（HKE）建模目标任务塔内的样本差异；3）结合排序损失策略充分利用未标记样本。

Result: 在离线行业数据集和在线A/B测试中，KAML框架相比现有多任务学习基线表现出显著性能提升。

Conclusion: KAML框架有效解决了多任务学习中的不对称多标签数据问题，通过精细化的知识迁移和样本差异建模，在实际广告系统中取得了优异效果。

Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.

</details>


### [409] [ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning](https://arxiv.org/abs/2512.13316)
*Mayank Gulati,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: ALIGN-FL是一种新颖的分布式学习方法，通过选择性共享生成组件来解决高度不相关数据分布的学习挑战，采用隐私保护机制在非IID场景下保持数据效用。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中高度不相关数据分布（Non-IID）的挑战，同时保护客户隐私，避免直接交换完整模型参数。

Method: 使用选择性共享生成组件的方法，仅传输生成能力而非完整参数；服务器利用合成样本进行全局训练；结合DP-SGD自适应裁剪和Lipschitz正则化VAE解码器；采用支持异构客户端的架构。

Result: 在MNIST和Fashion-MNIST数据集上的实验验证表明，隐私机制能有效将敏感异常值映射到典型数据点，在极端非IID场景下保持效用。

Conclusion: ALIGN-FL框架通过生成组件共享和隐私机制，在跨域协作的极端非IID场景下实现了隐私保护的有效学习。

Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures

</details>


### [410] [KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers](https://arxiv.org/abs/2512.13336)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 本文提出了一种知识蒸馏的物理信息神经网络框架（KD-PINN），通过KL散度将高容量教师模型的预测精度转移到紧凑的学生模型，实现了在保持物理精度的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 为了解决物理信息神经网络（PINNs）在实时应用中推理延迟较高的问题，需要开发既能保持物理精度又具有超低延迟的神经网络PDE求解器。

Method: 使用知识蒸馏框架，通过Kullback-Leibler散度连续适应，将高容量教师模型的预测能力转移到紧凑的学生模型，并在多种偏微分方程上进行评估。

Result: 学生模型在所有测试案例中保持了教师的物理精度，平均RMSE增加低于0.64%，推理速度提升4.8倍（Navier-Stokes）到6.9倍（Burgers），平均推理延迟为5.3毫秒，达到亚10毫秒的超低延迟实时性能。

Conclusion: 知识蒸馏能有效降低PINNs的推理延迟，为实现精确的超低延迟神经网络PDE求解器提供了可行方案，蒸馏过程还显示出正则化效果。

Abstract: This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.

</details>


### [411] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: FROC是一个用于大语言模型机器遗忘的统一框架，通过风险优化控制来平衡遗忘充分性和效用保留之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘技术缺乏评估和控制风险的有效机制，难以在安全性和效用之间取得平衡，这引发了关于"被遗忘权"的信任问题。

Method: FROC采用符合风险控制方法，引入连续风险模型，计算符合遗忘风险(CUR)和风险控制配置集，通过概率约束来指导超参数选择。

Result: 实验表明FROC能产生稳定、可解释的风险景观，揭示遗忘配置、语义偏移和效用影响之间的一致关系。

Conclusion: FROC将机器遗忘重新定义为可控、风险感知的过程，为大规模LLM部署中的遗忘行为管理提供了实用基础。

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [412] [Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks](https://arxiv.org/abs/2512.13340)
*Henrik C. M. Frederiksen,Junya Shiraishi,Cedomir Stefanovic,Hei Victor Cheng,Shashi Raj Pandey*

Main category: cs.LG

TL;DR: 该论文提出了一种事件驱动的通信框架，将持续学习策略集成到物联网网络中，用于节能的故障检测。该框架通过适应无线链路条件和可用能量预算，实现物联网设备与边缘服务器的协作模型更新。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在非平稳环境中进行设备端推理时，由于初始训练数据有限，推理精度会下降。传统方法通过定期更新模型来应对，但这对能量受限的物联网设备来说能耗过高。

Method: 提出事件驱动的通信框架，通过物联网设备与边缘服务器的协作，根据无线链路条件和可用能量预算自适应地更新轻量级机器学习模型。

Result: 在真实数据集上的评估显示，该方法在推理召回率上优于周期性采样和非自适应持续学习方法，在严格的能量和带宽约束下可实现高达42.8%的性能提升。

Conclusion: 该事件驱动框架能够有效平衡模型精度与能耗需求，为资源受限的物联网设备提供了一种高效的持续学习解决方案。

Abstract: The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.

</details>


### [413] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

TL;DR: 本文研究了在LLM训练数据提取流程中整合多种成员推理攻击(MIA)技术，评估它们在真实数据提取场景中的实际效用


<details>
  <summary>Details</summary>
Motivation: 大型语言模型容易记忆训练数据，存在严重的隐私风险，特别是训练数据提取和成员推理攻击。先前研究表明这两种威胁相互关联，需要系统评估MIA技术在数据提取场景中的有效性

Method: 将多种MIA技术整合到数据提取流程中，系统性地基准测试它们的有效性，并与传统MIA基准测试结果进行比较

Result: 通过对比集成设置与传统基准测试的表现，评估了不同MIA技术在真实数据提取场景中的实际效用

Conclusion: 该研究为评估MIA技术在LLM隐私保护中的实际应用价值提供了系统性方法，有助于更好地理解和管理LLM的数据泄露风险

Abstract: Large Language Models (LLMs) are prone to mem- orizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA bench- marks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [414] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

TL;DR: DPUL是一种新颖的服务器端联邦遗忘方法，通过深度遗忘所有有影响力的权重来防止隐私泄露，相比现有方法在准确性和时间成本上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在高计算需求、复杂激励机制和客户端计算能力差异等问题，且仅移除目标客户端的更新，忽略了其他客户端贡献中的隐私信息，可能导致隐私泄露。

Method: DPUL包含三个组件：(i)通过过滤客户端更新幅度识别高权重参数并回滚以确保深度移除；(ii)利用变分自编码器(VAE)重构和消除低权重参数；(iii)使用基于投影的技术恢复模型。

Result: 在四个数据集上的实验结果表明，DPUL优于最先进的基线方法，准确率提高1%-5%，时间成本最多降低12倍。

Conclusion: DPUL提供了一种有效的服务器端联邦遗忘解决方案，能够深度遗忘所有有影响力的权重，在保护隐私的同时显著提升了性能效率。

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [415] [Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks](https://arxiv.org/abs/2512.13410)
*Vítor M. Hanriot,Luiz C. B. Torres,Antônio P. Braga*

Main category: cs.LG

TL;DR: 本文提出了基于Gabriel图（GG）的二元和多类分类方法改进，包括更平滑的激活函数、结构支持向量（SSV）中心神经元、新的图正则化成员函数以及计算效率更高的GG重计算算法。实验表明该方法优于之前的GG分类器，并与基于树的模型统计等效。


<details>
  <summary>Details</summary>
Motivation: 虽然大间隔分类器源于优化框架，但支持向量可以通过几何方法获得。本文旨在推进Gabriel图在分类问题中的应用，特别是解决现有GG分类器在分类边界平滑性和计算效率方面的问题。

Method: 1. 提出Chipclass二元分类器的改进：使用更平滑的激活函数和SSV中心神经元
2. 扩展神经网络架构，支持反向传播训练（softmax+交叉熵损失）或线性方程组求解
3. 引入新的基于子图/距离的图正则化成员函数
4. 开发计算效率更高的GG重计算算法

Result: 通过Friedman检验的实验结果表明，该方法优于之前的GG基分类器，并且与基于树的模型在统计上等效。

Conclusion: 基于Gabriel图的分类方法通过引入更平滑的激活函数、SSV中心神经元和高效的GG重计算算法，在保持几何解释性的同时提高了分类性能和计算效率，为优化无关的分类器设计提供了有前景的替代方案。

Abstract: While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.

</details>


### [416] [XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders](https://arxiv.org/abs/2512.13442)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

TL;DR: XNNTab是一种神经网络架构，结合了神经网络的表达能力和可解释性，通过学习非线性特征表示并使用稀疏自编码器分解为可解释概念，在保持高性能的同时实现内在可解释性。


<details>
  <summary>Details</summary>
Motivation: 在需要可解释性的表格数据应用中，神经网络虽然性能更高但因其黑盒特性而未被采用，而决策树和线性回归等可解释模型性能有限。

Method: 首先学习高度非线性的特征表示，然后使用稀疏自编码器（SAE）将这些特征分解为单语义特征，并为这些特征分配人类可解释的概念。

Result: XNNTab在性能上优于可解释预测模型，并达到与非可解释模型相当的性能水平。

Conclusion: 该工作成功实现了神经网络表达能力和可解释性的结合，为表格数据应用提供了既高性能又可解释的解决方案。

Abstract: In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.

</details>


### [417] [SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy](https://arxiv.org/abs/2512.13458)
*Yici Liu,Qi Wei Oung,Hoi Leong Lee*

Main category: cs.LG

TL;DR: 提出了一种基于源选择与对抗策略的跨被试EEG情绪识别方法，通过源选择网络破坏类可分性并放大域间差异，以及对抗策略网络增强域分类性能，在SEED和SEED-IV数据集上取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 现有跨被试EEG情绪识别研究大多忽视了个体间差异和负迁移现象，导致模型性能受限。

Method: 采用双模块设计：源选择网络(SS)通过逆向工程域适应训练过程，破坏类可分性并放大域间差异；对抗策略网络(AS)利用预训练域判别器计算新损失函数，确保对抗策略平衡。

Result: 在SEED和SEED-IV两个EEG情绪数据集上取得了突出的性能表现。

Conclusion: 该方法能有效解决跨被试EEG情绪识别中的个体差异问题，学习到域不变且与情绪相关的表征，具有实际应用价值。

Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.

</details>


### [418] [DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems](https://arxiv.org/abs/2512.13460)
*Chethana Prasad Kabgere,Shylaja S S*

Main category: cs.LG

TL;DR: DP-EMAR是一个差分隐私的错误模型自主修复框架，用于在资源受限的物联网网络中检测和重建联邦学习传输过程中的模型权重失真，同时保持严格的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 在多层联邦物联网系统中，不稳定的连接和对抗性干扰会静默改变传输的参数，降低收敛性能，而模型权重失真仍然是资源受限物联网网络中的主要挑战。

Method: 提出DP-EMAR框架，通过估计损坏模式并在添加隐私噪声之前应用自适应校正，将差分隐私与安全聚合相结合，区分DP噪声与真实传输错误。

Result: 在异构物联网传感器和图数据集上的实验表明，DP-EMAR在通信损坏下保持收敛稳定性，维持接近基线性能，同时确保严格的(epsilon, delta)-DP保证。

Conclusion: 该框架增强了隐私保护联邦物联网学习的鲁棒性、通信效率和可信度。

Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.

</details>


### [419] [Element-wise Modulation of Random Matrices for Efficient Neural Layers](https://arxiv.org/abs/2512.13480)
*Maksymilian Szorc*

Main category: cs.LG

TL;DR: 提出参数化随机投影（PRP）层，通过固定随机矩阵和轻量可学习参数实现特征混合与适应的解耦，大幅减少全连接层的参数数量和计算开销。


<details>
  <summary>Details</summary>
Motivation: 全连接层是深度神经网络中内存和计算开销的主要来源，现有的压缩技术往往引入复杂的工程权衡或降低模型性能。

Method: 使用固定随机矩阵配合轻量级的可学习逐元素参数，将特征混合与适应过程解耦，将可训练参数数量降至线性规模。

Result: 在各种基准测试中保持可靠的准确率，同时显著减少参数数量。

Conclusion: PRP层为资源受限环境下的架构扩展和部署提供了稳定且计算高效的解决方案。

Abstract: Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.

</details>


### [420] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

TL;DR: 本文提出了一种基于PatchCore的在线持续学习方法，用于解决动态制造环境中视觉异常检测面临的模型更新、边缘设备资源限制和训练数据稀缺等挑战。该方法通过轻量级特征提取器和增量核心集更新机制，实现了高效的内存使用和快速适应。


<details>
  <summary>Details</summary>
Motivation: 现代制造业中，小批量按需生产导致产品频繁变更，需要快速模型更新；边缘设备资源有限无法运行大型AI模型；异常和正常训练数据往往稀缺，特别是新产品变体。

Method: 扩展PatchCore方法，采用轻量级特征提取器和基于k中心选择的增量核心集更新机制，支持在线学习，无需昂贵的云端重新训练。

Result: 在工业用例评估中，相比基线方法AUROC提升12%，内存使用减少80%，训练速度比批量重新训练更快。

Conclusion: 该方法为动态智能制造提供了准确、资源高效且自适应的视觉异常检测解决方案。

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [421] [Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource](https://arxiv.org/abs/2512.13506)
*Sofiya Zaichyk*

Main category: cs.LG

TL;DR: 本文提出了一个名为"可重复性预算"的新统计原语$C_T$，用于量化在分布漂移和内生反馈下系统的统计可重复性能力，并推导出最优的泛化误差界。


<details>
  <summary>Details</summary>
Motivation: 传统泛化边界在分布漂移下可能失效，需要新的理论框架来刻画学习系统在数据生成过程不断变化时的统计性能。

Method: 定义可重复性预算$C_T$作为Fisher-Rao路径长度的累积量，衡量学习过程中分布变化的总量，并基于此推导泛化误差边界。

Result: 得到了$O(T^{-1/2} + C_T/T)$阶的漂移-反馈泛化边界，并证明该速率是极小极大最优的。

Conclusion: 建立了可重复性速度限制，表明任何算法的最坏情况泛化误差都受到数据生成过程的平均Fisher-Rao漂移率$C_T/T$的限制。

Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.

</details>


### [422] [Async Control: Stress-testing Asynchronous Control Measures for LLM Agents](https://arxiv.org/abs/2512.13526)
*Asa Cooper Stickland,Jan Michelfeit,Arathi Mani,Charlie Griffin,Ollie Matthews,Tomek Korbak,Rogan Inglis,Oliver Makins,Alan Cooney*

Main category: cs.LG

TL;DR: 本文研究LLM软件工程代理的异步监控方法，通过蓝队（监控设计者）与红队（破坏性代理）的对抗游戏来评估和限制代理的破坏潜力，在5个软件工程环境中开发出误报率1%时漏报率6%的集成监控器。


<details>
  <summary>Details</summary>
Motivation: LLM软件工程代理在现实开发任务中可能接触到敏感数据或安全关键代码库，如果代理未对齐，可能故意破坏这些代码库。需要开发有效的监控机制来防止不可逆损害。

Method: 采用异步监控方法，建立蓝队与红队的对抗游戏框架，在5个不同的软件工程环境中开发和测试监控器，使用Claude 4.1 Opus作为基础代理模型。

Result: 开发出的集成监控器在保留测试环境中达到6%的漏报率（1%误报率下），并基于漏报率估计部署时的破坏风险。

Conclusion: 异步监控是可行的安全防护方法，通过对抗游戏可以评估和限制代理的破坏潜力，但需要谨慎的风险外推模型来估计实际部署风险。

Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.

</details>


### [423] [Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability](https://arxiv.org/abs/2512.13568)
*Leonard Bereska,Zoe Tzifa-Kratira,Reza Samavi,Efstratios Gavves*

Main category: cs.LG

TL;DR: 本文提出了一个信息论框架来测量神经网络中的叠加现象，通过计算稀疏自编码器激活的香农熵来量化有效特征数量，揭示了叠加与对抗鲁棒性的关系。


<details>
  <summary>Details</summary>
Motivation: 神经网络通过叠加（多个特征在激活空间中重叠编码）实现卓越性能，但缺乏测量叠加的原则性方法，这阻碍了可解释性研究。

Method: 使用信息论框架，对稀疏自编码器的激活应用香农熵，计算有效特征数量作为无干扰编码所需的最小神经元数，等效于网络通过叠加模拟的'虚拟神经元'数量。

Result: 度量方法在玩具模型中与真实值强相关，在算法任务中检测到最小叠加，在dropout下显示系统性减少。层间模式与Pythia-70M的内在维度研究一致，还能捕捉训练动态如grokking中的特征整合。对抗训练可增加有效特征同时提高鲁棒性。

Conclusion: 将叠加定义为有损压缩，使能够原则性测量神经网络在计算约束下如何组织信息，连接了叠加与对抗鲁棒性，发现任务复杂性和网络容量决定特征扩展（丰富机制）或减少（稀缺机制）。

Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.

</details>


### [424] [DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication](https://arxiv.org/abs/2512.13583)
*Zehan Zhu,Heng Zhao,Yan Huang,Joey Tianyi Zhou,Shouling Ji,Jinming Xu*

Main category: cs.LG

TL;DR: 提出DP-CSGP算法，在定向图上的去中心化学习中实现差分隐私和压缩通信，保持高模型效用


<details>
  <summary>Details</summary>
Motivation: 现有方法在去中心化学习中难以同时保证严格的差分隐私和高效的通信效率

Method: 使用差分隐私随机梯度推送与压缩通信技术，在定向图上进行去中心化学习

Result: 算法在非凸平滑目标函数上达到紧致效用边界，实验表明在相同隐私预算下比精确通信方法通信成本显著降低

Conclusion: DP-CSGP算法在保证差分隐私的同时实现了高效通信，模型精度与精确通信方法相当

Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.

</details>


### [425] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

TL;DR: 本文提出Diffusion Preview方法，通过快速低步数采样生成预览图，用户满意后再进行完整步数优化。为解决现有方法预览质量差和一致性不足的问题，提出了基于强化学习优化的ConsistencySolver求解器。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型的慢推理过程严重影响交互体验，现有加速方法难以在低步数下提供高质量预览或保证预览与最终输出的一致性。

Method: 提出ConsistencySolver，基于通用线性多步方法，通过强化学习优化的轻量级可训练高阶求解器，提升预览质量和一致性。

Result: 实验显示ConsistencySolver在低步数场景下显著提升生成质量和一致性，比Multistep DPM-Solver少用47%步数达到同等FID分数，优于蒸馏基线方法。用户研究显示该方法减少用户交互时间近50%。

Conclusion: ConsistencySolver是高效预览-优化工作流程的理想选择，显著改善了扩散模型的交互体验。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [426] [Scalable Formal Verification via Autoencoder Latent Space Abstraction](https://arxiv.org/abs/2512.13593)
*Robert Reed,Morteza Lahijanian,Luca Laurenti*

Main category: cs.LG

TL;DR: 提出了一种基于凸自编码器和核方法的正式降维方法，构建有限抽象并保证包含原始系统真实行为，验证结果可映射回原系统，显著提升可扩展性而不损失严谨性。


<details>
  <summary>Details</summary>
Motivation: 有限抽象方法在证明系统满足规范方面提供了强大的形式框架，但面对高维系统时存在可扩展性挑战，因为状态空间离散化随维度指数增长。基于学习的降维方法有潜力缓解此问题，但确保验证结果的正确性仍是开放问题。

Method: 使用凸自编码器正式降低系统维度，通过核方法在潜在空间中学习动态，然后在潜在空间中从学习模型构建有限抽象，并保证该抽象包含原始系统的真实行为。

Result: 该方法在多个系统上验证有效，包括一个由神经网络控制的26维系统，显示出显著的可扩展性改进且不损失严谨性。

Conclusion: 提出的方法能够有效解决高维系统验证的可扩展性问题，通过形式化的降维和学习过程，确保验证结果的正确性，为复杂系统的形式验证提供了可行的解决方案。

Abstract: Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.

</details>


### [427] [LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification](https://arxiv.org/abs/2512.13617)
*Ankit Sharma,Sayan Roy Gupta*

Main category: cs.LG

TL;DR: LightTopoGAT是一种轻量级图注意力网络，通过拓扑增强（节点度和局部聚类系数）提升图表示学习性能，在保持参数效率的同时优于传统GNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络需要大量计算资源且难以有效捕捉全局图属性，需要一种轻量级但性能优越的方法。

Method: 通过拓扑增强整合节点度和局部聚类系数特征，采用简化的注意力机制，在保持参数效率的同时融入结构信息。

Result: 在MUTAG、ENZYMES和PROTEINS三个基准数据集上，LightTopoGAT相比GCN、GraphSAGE和标准GAT表现更优，MUTAG准确率提升6.6%，PROTEINS提升2.2%。

Conclusion: 消融实验证实性能提升直接来源于拓扑特征的加入，展示了一种简单有效且无需增加架构复杂度的图神经网络增强策略。

Abstract: Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.

</details>


### [428] [StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion](https://arxiv.org/abs/2512.13632)
*Guransh Singh,Md Shah Fahad*

Main category: cs.LG

TL;DR: StutterFuse是首个用于多标签口吃检测的检索增强分类器，通过结合临床示例的非参数记忆库来解决现有模型在重叠性不流畅问题上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型难以区分复杂、同时发生的不流畅现象（如'阻塞'与'延长'重叠），因为训练数据中这些特定组合稀缺。检索增强生成在NLP中取得成功，但在病理语音处理中尚未探索。

Method: 引入StutterFuse，基于Conformer编码器，结合临床示例的非参数记忆库进行分类。解决'模态崩溃'问题，使用SetCon（Jaccard加权度量学习目标）和门控混合专家融合策略。

Result: 在SEP-28k数据集上，StutterFuse获得0.65的加权F1分数，优于强基线，并展现出卓越的零样本跨语言泛化能力。

Conclusion: StutterFuse通过检索增强方法有效解决了重叠性口吃检测问题，为病理语音处理开辟了新方向。

Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.

</details>


### [429] [From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves](https://arxiv.org/abs/2512.13641)
*Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza*

Main category: cs.LG

TL;DR: 该论文提出了一种评估卷积神经网络在恶劣条件下鲁棒性的方法，通过创建MangoLeafDB-C数据集（包含19种人工损坏类型和5个严重级别），比较了5种架构的性能。结果表明轻量级专用模型LCNN在实际场景中表现优于复杂模型。


<details>
  <summary>Details</summary>
Motivation: 芒果是全球重要作物，但缺乏对其叶片病害诊断模型鲁棒性的研究。需要评估AI模型在真实世界挑战（如噪声、模糊、天气变化）下的可靠性能。

Method: 改编MangoLeafDB数据集生成MangoLeafDB-C，包含19种人工损坏类型和5个严重级别。对ResNet-50、ResNet-101、VGG-16、Xception和LCNN五种架构进行基准测试，使用F1分数、损坏误差和相对平均损坏误差作为评估指标。

Result: LCNN在散焦模糊、运动模糊等实际场景损坏中表现优于复杂模型，且获得最低的平均损坏误差。现代架构（如ResNet-101）在损坏场景下性能显著下降，尽管在理想条件下准确率高。

Conclusion: 轻量级专用模型更适合边缘设备的实际应用，其中鲁棒性和效率至关重要。研究强调在农业智能系统开发中，特别是在技术受限地区，需要纳入鲁棒性评估。

Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.

</details>


### [430] [A Scientific Reasoning Model for Organic Synthesis Procedure Generation](https://arxiv.org/abs/2512.13668)
*Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler*

Main category: cs.LG

TL;DR: QFANG是一个科学推理语言模型，能够从反应方程直接生成精确的结构化实验程序，具有明确的思维链推理能力，旨在弥合计算合成规划与实验室执行之间的差距。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助合成规划中的关键挑战，即弥合计算路线设计与实际实验室执行之间的差距，特别是准确预测每个合成步骤的可行实验程序。

Method: 1) 构建包含90.5万个化学反应的高质量数据集；2) 引入化学引导推理(CGR)框架生成基于化学知识的思维链数据；3) 进行监督微调以激发复杂的化学推理能力；4) 应用可验证奖励强化学习(RLVR)进一步提高程序准确性。

Result: QFANG在传统NLP相似性指标和化学感知评估器上均优于先进的通用推理模型和最近邻检索基线，能够泛化到某些域外反应类别，并适应实验室条件和用户特定约束的变化。

Conclusion: QFANG生成高质量合成程序的能力代表了向弥合计算合成规划与全自动实验室合成之间差距的重要一步。

Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.

</details>


### [431] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

TL;DR: 文本反转(TI)方法在复杂提示下常失败，归因于嵌入范数膨胀问题。提出方向性文本反转(DTI)，通过固定嵌入范数、仅优化单位超球面上的方向来解决该问题。


<details>
  <summary>Details</summary>
Motivation: 传统文本反转方法在处理复杂提示时效果不佳，研究发现这是由于学习到的标记嵌入范数膨胀，导致预范数Transformer中的提示条件化效果下降。

Method: 提出DTI方法：固定嵌入范数为分布内尺度，通过黎曼SGD仅优化单位超球面上的方向；将方向学习建模为带有von Mises-Fisher先验的MAP问题。

Result: 在个性化任务中，DTI在保持主体相似性的同时，比TI及其变体显著提升了文本保真度。DTI的超球面参数化支持学习概念间的平滑语义插值(slerp)。

Conclusion: 仅优化方向的方法为提示忠实个性化提供了稳健且可扩展的路径，方向是CLIP标记空间中语义编码的主要载体。

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>
