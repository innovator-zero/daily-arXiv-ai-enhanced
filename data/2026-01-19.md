<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 49]
- [cs.RO](#cs.RO) [Total: 25]
- [cs.LG](#cs.LG) [Total: 60]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Future Optical Flow Prediction Improves Robot Control & Video Generation](https://arxiv.org/abs/2601.10781)
*Kanchana Ranasinghe,Honglu Zhou,Yu Fang,Luyu Yang,Le Xue,Ran Xu,Caiming Xiong,Silvio Savarese,Michael S Ryoo,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: FOFPred是一个基于语言条件的光流预测模型，结合VLM和Diffusion架构，从大规模网络视频数据中学习未来运动表示，并应用于机器人操控和视频生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预测通用化、空间密集的运动表示方面存在挑战，且从噪声真实数据中学习此类预测尚未充分探索。光流等未来运动表示对控制和生成任务具有重要价值。

Method: 提出FOFPred模型，采用统一的视觉语言模型和扩散架构，结合关键数据预处理技术，从大规模网络人类活动数据中训练，利用强图像预训练提取有意义信号。

Result: 模型在语言驱动的机器人操控和视频生成任务中表现出色，验证了统一VLM-Diffusion架构的跨域通用性。

Conclusion: 统一的VLM-Diffusion架构和从多样化网络数据中可扩展学习对未来光流预测具有重要价值，FOFPred展示了该方法的有效性。

Abstract: Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.

</details>


### [2] [ICONIC-444: A 3.1-Million-Image Dataset for OOD Detection Research](https://arxiv.org/abs/2601.10802)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: ICONIC-444是一个专门为OOD检测研究设计的大规模工业图像数据集，包含310万张RGB图像和444个类别，旨在解决现有数据集在OOD检测研究中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前OOD检测研究的进展受到缺乏大规模、高质量数据集的限制，这些数据集需要明确定义不同难度级别的OOD类别，并支持细粒度和粗粒度的计算机视觉任务。

Method: 通过原型工业分拣机采集图像，构建ICONIC-444数据集，包含444个类别的310万张RGB图像，并定义了四个参考任务来基准测试OOD检测方法。

Result: 提供了22种最先进的后期OOD检测方法的基线结果，为OOD检测研究提供了结构化和多样化的数据支持。

Conclusion: ICONIC-444数据集通过提供结构化、多样化的数据，支持严格的OOD评估，有助于推动OOD检测研究的发展。

Abstract: Current progress in out-of-distribution (OOD) detection is limited by the lack of large, high-quality datasets with clearly defined OOD categories across varying difficulty levels (near- to far-OOD) that support both fine- and coarse-grained computer vision tasks. To address this limitation, we introduce ICONIC-444 (Image Classification and OOD Detection with Numerous Intricate Complexities), a specialized large-scale industrial image dataset containing over 3.1 million RGB images spanning 444 classes tailored for OOD detection research. Captured with a prototype industrial sorting machine, ICONIC-444 closely mimics real-world tasks. It complements existing datasets by offering structured, diverse data suited for rigorous OOD evaluation across a spectrum of task complexities. We define four reference tasks within ICONIC-444 to benchmark and advance OOD detection research and provide baseline results for 22 state-of-the-art post-hoc OOD detection methods.

</details>


### [3] [A Unified 3D Object Perception Framework for Real-Time Outside-In Multi-Camera Systems](https://arxiv.org/abs/2601.10819)
*Yizhou Wang,Sameer Pusegaonkar,Yuxing Wang,Anqi Li,Vishal Kumar,Chetan Sethi,Ganapathy Aiyer,Yun He,Kartikay Thakkar,Swapnil Rathi,Bhushan Rupde,Zheng Tang,Sujit Biswas*

Main category: cs.CV

TL;DR: 本文提出了一个针对大规模基础设施环境优化的Sparse4D框架，通过世界坐标几何先验和遮挡感知ReID模块解决静态相机网络中的多目标多相机跟踪问题，并在AI City Challenge 2025基准测试中达到45.22 HOTA的先进水平。


<details>
  <summary>Details</summary>
Motivation: 解决从自动驾驶模型到静态相机网络转换中的异构相机布局和极端遮挡问题，实现工业基础设施的数字转型。

Method: 采用绝对世界坐标几何先验，引入遮挡感知ReID嵌入模块，使用NVIDIA COSMOS框架进行生成式数据增强以缩小Sim2Real域差距，并开发了针对多尺度可变形聚合的TensorRT插件进行硬件加速。

Result: 在AI City Challenge 2025基准测试中达到45.22 HOTA的先进性能，硬件加速实现2.15倍速度提升，单个Blackwell级GPU可支持超过64个并发相机流。

Conclusion: 该框架成功解决了静态相机网络中的3D目标感知和MTMC跟踪挑战，通过几何先验、遮挡感知和硬件优化实现了高性能和实时部署能力。

Abstract: Accurate 3D object perception and multi-target multi-camera (MTMC) tracking are fundamental for the digital transformation of industrial infrastructure. However, transitioning "inside-out" autonomous driving models to "outside-in" static camera networks presents significant challenges due to heterogeneous camera placements and extreme occlusion. In this paper, we present an adapted Sparse4D framework specifically optimized for large-scale infrastructure environments. Our system leverages absolute world-coordinate geometric priors and introduces an occlusion-aware ReID embedding module to maintain identity stability across distributed sensor networks. To bridge the Sim2Real domain gap without manual labeling, we employ a generative data augmentation strategy using the NVIDIA COSMOS framework, creating diverse environmental styles that enhance the model's appearance-invariance. Evaluated on the AI City Challenge 2025 benchmark, our camera-only framework achieves a state-of-the-art HOTA of $45.22$. Furthermore, we address real-time deployment constraints by developing an optimized TensorRT plugin for Multi-Scale Deformable Aggregation (MSDA). Our hardware-accelerated implementation achieves a $2.15\times$ speedup on modern GPU architectures, enabling a single Blackwell-class GPU to support over 64 concurrent camera streams.

</details>


### [4] [Can Vision-Language Models Understand Construction Workers? An Exploratory Study](https://arxiv.org/abs/2601.10835)
*Hieu Bui,Nathaniel E. Chodosh,Arash Tavakoli*

Main category: cs.CV

TL;DR: 本研究评估了三种领先的视觉语言模型在建筑工地图像中识别工人行为和情绪的能力，发现GPT-4o表现最佳，但所有模型在语义相近类别上仍有困难。


<details>
  <summary>Details</summary>
Motivation: 随着机器人在建筑工作流程中的集成度提高，其理解和响应人类行为的能力对于安全有效的协作至关重要。视觉语言模型为视觉理解任务提供了有前景的工具，特别是在建筑领域标注数据稀缺的情况下。

Method: 使用包含1,000张图像的数据集，评估GPT-4o、Florence 2和LLaVa-1.5三种模型在十个行为类别和十个情绪类别上的识别性能，通过标准化推理流程和多种评估指标进行分析。

Result: GPT-4o在行为识别和情绪识别任务中均表现最佳，平均F1分数分别为0.756和0.712；Florence 2表现中等；LLaVa-1.5表现最差。所有模型在区分语义相近类别时都存在困难。

Conclusion: 通用视觉语言模型可为建筑环境中的人类行为识别提供基础能力，但需要领域适应、时间建模或多模态感知等进一步改进才能实现实际应用的可靠性。

Abstract: As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model's outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.

</details>


### [5] [One Model, Many Behaviors: Training-Induced Effects on Out-of-Distribution Detection](https://arxiv.org/abs/2601.10836)
*Gerhard Krumpl,Henning Avenhaus,Horst Possegger*

Main category: cs.CV

TL;DR: 本文通过实证研究发现，在ImageNet训练模型中，OOD检测性能与ID准确率存在非单调关系，且训练策略、检测器选择和OOD性能之间存在强相互依赖关系。


<details>
  <summary>Details</summary>
Motivation: 研究OOD检测与现代训练流程（旨在最大化ID准确率和泛化能力）之间的相互作用，这一领域尚未得到充分探索。

Method: 使用ResNet-50架构，对56个通过不同训练策略获得的ImageNet训练模型，评估21种最先进的OOD检测方法，并在8个OOD测试集上进行测试。

Result: 发现OOD性能与ID准确率呈非单调关系：初始随准确率提升而改善，但当高级训练方法使准确率超过基线后反而下降。不同训练策略和检测器组合的OOD性能差异显著。

Conclusion: 没有单一的OOD检测方法在所有情况下都是最优的，训练策略、检测器选择和OOD性能之间存在强相互依赖关系，挑战了"更高ID准确率意味着更好OOD检测性能"的常见假设。

Abstract: Out-of-distribution (OOD) detection is crucial for deploying robust and reliable machine-learning systems in open-world settings. Despite steady advances in OOD detectors, their interplay with modern training pipelines that maximize in-distribution (ID) accuracy and generalization remains under-explored. We investigate this link through a comprehensive empirical study. Fixing the architecture to the widely adopted ResNet-50, we benchmark 21 post-hoc, state-of-the-art OOD detection methods across 56 ImageNet-trained models obtained via diverse training strategies and evaluate them on eight OOD test sets. Contrary to the common assumption that higher ID accuracy implies better OOD detection performance, we uncover a non-monotonic relationship: OOD performance initially improves with accuracy but declines once advanced training recipes push accuracy beyond the baseline. Moreover, we observe a strong interdependence between training strategy, detector choice, and resulting OOD performance, indicating that no single method is universally optimal.

</details>


### [6] [Effects of Different Attention Mechanisms Applied on 3D Models in Video Classification](https://arxiv.org/abs/2601.10854)
*Mohammad Rasras,Iuliana Marin,Serban Radu,Irina Mocanu*

Main category: cs.CV

TL;DR: 本文研究了在3D ResNet模型（MC3、R3D、R(2+1)D）中减少时间数据知识的同时增加帧分辨率的影响，通过添加注意力机制（如CBAM、TCN、多头注意力等）来提升受限时间模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探索在人类动作识别中，当减少时间数据信息时，通过增加帧分辨率和引入不同注意力机制对模型性能的影响，特别是在受限时间模型中的表现。

Method: 基于三种3D ResNet模型创建了类似设计，添加了dropout层，并为每种模型开发了10个新版本，这些变体包含了CBAM、TCN、多头注意力和通道注意力等特殊注意力块。

Result: 在UCF101数据集上的测试结果显示，修改后的R(2+1)D模型添加多头注意力后达到了88.98%的准确率。变体模型在类别级准确率上表现出不同行为，尽管整体性能提升相似。

Conclusion: 研究证实了缺失时间特征对新增高分辨率模型性能的重要性，不同注意力机制对模型在类别级别的表现有差异化影响。

Abstract: Human action recognition has become an important research focus in computer vision due to the wide range of applications where it is used. 3D Resnet-based CNN models, particularly MC3, R3D, and R(2+1)D, have different convolutional filters to extract spatiotemporal features. This paper investigates the impact of reducing the captured knowledge from temporal data, while increasing the resolution of the frames. To establish this experiment, we created similar designs to the three originals, but with a dropout layer added before the final classifier. Secondly, we then developed ten new versions for each one of these three designs. The variants include special attention blocks within their architecture, such as convolutional block attention module (CBAM), temporal convolution networks (TCN), in addition to multi-headed and channel attention mechanisms. The purpose behind that is to observe the extent of the influence each of these blocks has on performance for the restricted-temporal models. The results of testing all the models on UCF101 have shown accuracy of 88.98% for the variant with multiheaded attention added to the modified R(2+1)D. This paper concludes the significance of missing temporal features in the performance of the newly created increased resolution models. The variants had different behavior on class-level accuracy, despite the similarity of their enhancements to the overall performance.

</details>


### [7] [Medical SAM3: A Foundation Model for Universal Prompt-Driven Medical Image Segmentation](https://arxiv.org/abs/2601.10880)
*Chongcong Jiang,Tianxingjian Ding,Chuhan Song,Jiachen Tu,Ziyang Yan,Yihua Shao,Zhenyi Wang,Yuzhang Shang,Tianyu Han,Yu Tian*

Main category: cs.CV

TL;DR: Medical SAM3是通过在33个医学影像数据集上微调SAM3得到的通用提示驱动医学图像分割基础模型，解决了SAM3在医学领域因域偏移和缺乏空间提示而性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: SAM3等可提示分割基础模型在医学图像分割中直接应用受限，主要由于严重的域偏移、缺乏特权空间提示以及需要处理复杂解剖和体积结构。

Method: 在包含10种医学成像模态的33个大规模异构2D和3D医学影像数据集上对SAM3进行全模型微调，使用配对的掩码和文本提示。

Result: 实验表明Medical SAM3在器官、成像模态和维度上均获得一致且显著的性能提升，特别是在语义模糊、复杂形态和长距离3D上下文等挑战性场景中表现优异。

Conclusion: Medical SAM3成为医学成像的通用文本引导分割基础模型，证明了在严重域偏移下实现鲁棒提示驱动分割需要整体模型适应而非仅提示工程。

Abstract: Promptable segmentation foundation models such as SAM3 have demonstrated strong generalization capabilities through interactive and concept-based prompting. However, their direct applicability to medical image segmentation remains limited by severe domain shifts, the absence of privileged spatial prompts, and the need to reason over complex anatomical and volumetric structures. Here we present Medical SAM3, a foundation model for universal prompt-driven medical image segmentation, obtained by fully fine-tuning SAM3 on large-scale, heterogeneous 2D and 3D medical imaging datasets with paired segmentation masks and text prompts. Through a systematic analysis of vanilla SAM3, we observe that its performance degrades substantially on medical data, with its apparent competitiveness largely relying on strong geometric priors such as ground-truth-derived bounding boxes. These findings motivate full model adaptation beyond prompt engineering alone. By fine-tuning SAM3's model parameters on 33 datasets spanning 10 medical imaging modalities, Medical SAM3 acquires robust domain-specific representations while preserving prompt-driven flexibility. Extensive experiments across organs, imaging modalities, and dimensionalities demonstrate consistent and significant performance gains, particularly in challenging scenarios characterized by semantic ambiguity, complex morphology, and long-range 3D context. Our results establish Medical SAM3 as a universal, text-guided segmentation foundation model for medical imaging and highlight the importance of holistic model adaptation for achieving robust prompt-driven segmentation under severe domain shift. Code and model will be made available at https://github.com/AIM-Research-Lab/Medical-SAM3.

</details>


### [8] [FrankenMotion: Part-level Human Motion Generation and Composition](https://arxiv.org/abs/2601.10909)
*Chuqiao Li,Xianghui Xie,Yong Cao,Andreas Geiger,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 该论文提出了一个具有细粒度、时间感知的部分级文本标注的高质量运动数据集，并基于此开发了一个扩散模型框架FrankenMotion，实现了对身体部位和原子动作的时空控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖序列级或动作级描述，缺乏细粒度的部分级运动标注，限制了单个身体部位的可控性。

Method: 利用大语言模型构建具有原子化、时间感知的部分级文本标注的运动数据集，并开发基于扩散的部分感知运动生成框架，每个身体部位由其自身的时间结构化文本提示引导。

Result: FrankenMotion在实验中的表现优于所有先前为当前设置调整和重新训练的基线模型，并且能够生成训练中未见过的运动组合。

Conclusion: 这是首个提供原子化、时间感知部分级运动标注的工作，实现了运动生成的时空控制，代码和数据集将在发表后公开。

Abstract: Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.

</details>


### [9] [Classification of Chest XRay Diseases through image processing and analysis techniques](https://arxiv.org/abs/2601.10913)
*Santiago Martínez Novoa,María Catalina Ibáñez,Lina Gómez Mesa,Jeremias Kramer*

Main category: cs.CV

TL;DR: 本文概述了多分类胸部X射线图像诊断方法，包括DenseNet121等模型，并开发了开源Web应用进行方法比较和性能评估


<details>
  <summary>Details</summary>
Motivation: 胸部X射线是最常见的放射学检查方法之一，需要有效的方法来辅助诊断胸部疾病

Method: 使用DenseNet121等多种方法进行多分类胸部X射线图像分析，并开发了开源Web应用进行测试比较

Result: 对提出的方法进行了测试比较，分析了各种方法的性能表现

Conclusion: 识别了所提方法的弱点，并提出了未来改进的建议

Abstract: Multi-Classification Chest X-Ray Images are one of the most prevalent forms of radiological examination used for diagnosing thoracic diseases. In this study, we offer a concise overview of several methods employed for tackling this task, including DenseNet121. In addition, we deploy an open-source web-based application. In our study, we conduct tests to compare different methods and see how well they work. We also look closely at the weaknesses of the methods we propose and suggest ideas for making them better in the future. Our code is available at: https://github.com/AML4206-MINE20242/Proyecto_AML

</details>


### [10] [Self-learned representation-guided latent diffusion model for breast cancer classification in deep ultraviolet whole surface images](https://arxiv.org/abs/2601.10917)
*Pouya Afshin,David Helminiak,Tianling Niu,Julie M. Jorns,Tina Yen,Bing Yu,Dong Hye Ye*

Main category: cs.CV

TL;DR: 提出基于自监督学习的潜在扩散模型来生成高质量的合成训练数据，以解决乳腺癌保乳手术中深层紫外荧光扫描显微镜数据稀缺问题，通过结合真实和合成数据训练视觉Transformer模型，在WSI级别分类上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌保乳手术需要精确的术中边缘评估，但深层紫外荧光扫描显微镜的标注数据稀缺限制了深度学习模型的训练效果。

Method: 使用自监督学习引导的潜在扩散模型生成合成训练补丁，通过微调的DINO教师模型注入细胞结构语义细节，结合真实和合成数据微调视觉Transformer模型，采用补丁预测聚合进行WSI级别分类。

Result: 在5折交叉验证实验中，该方法达到96.47%的准确率，FID分数降至45.72，显著优于类别条件基线方法。

Conclusion: 所提出的SSL引导LDM方法能有效解决医学成像数据稀缺问题，为乳腺癌手术中的精确边缘评估提供了可靠的技术支持。

Abstract: Breast-Conserving Surgery (BCS) requires precise intraoperative margin assessment to preserve healthy tissue. Deep Ultraviolet Fluorescence Scanning Microscopy (DUV-FSM) offers rapid, high-resolution surface imaging for this purpose; however, the scarcity of annotated DUV data hinders the training of robust deep learning models. To address this, we propose an Self-Supervised Learning (SSL)-guided Latent Diffusion Model (LDM) to generate high-quality synthetic training patches. By guiding the LDM with embeddings from a fine-tuned DINO teacher, we inject rich semantic details of cellular structures into the synthetic data. We combine real and synthetic patches to fine-tune a Vision Transformer (ViT), utilizing patch prediction aggregation for WSI-level classification. Experiments using 5-fold cross-validation demonstrate that our method achieves 96.47 % accuracy and reduces the FID score to 45.72, significantly outperforming class-conditioned baselines.

</details>


### [11] [RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions](https://arxiv.org/abs/2601.10921)
*Tasneem Shaffee,Sherief Reda*

Main category: cs.CV

TL;DR: RobuMTL是一种新颖的多任务学习架构，通过动态选择任务特定的分层LoRA模块和LoRA专家小组，在恶劣天气条件下提高模型鲁棒性，在PASCAL和NYUD-v2数据集上表现出显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决现实环境中恶劣天气条件对多任务学习模型性能的严重影响，提高自主系统在复杂环境下的可靠性和鲁棒性。

Method: 采用基于输入扰动的混合专家方法，动态选择任务特定的分层LoRA模块和LoRA专家小组，实现基于输入特征的自适应专业化。

Result: 在PASCAL数据集上，相对于MTL基线，单扰动条件下平均相对提升+2.8%，混合天气条件下最高提升+44.4%；在NYUD-v2数据集上，各任务平均相对提升+9.7%。

Conclusion: RobuMTL框架通过自适应专业化机制有效提升了多任务学习模型在恶劣天气条件下的鲁棒性，在多个基准测试中均优于现有方法。

Abstract: Robust Multi-Task Learning (MTL) is crucial for autonomous systems operating in real-world environments, where adverse weather conditions can severely degrade model performance and reliability. In this paper, we introduce RobuMTL, a novel architecture designed to adaptively address visual degradation by dynamically selecting task-specific hierarchical Low-Rank Adaptation (LoRA) modules and a LoRA expert squad based on input perturbations in a mixture-of-experts fashion. Our framework enables adaptive specialization based on input characteristics, improving robustness across diverse real-world conditions. To validate our approach, we evaluated it on the PASCAL and NYUD-v2 datasets and compared it against single-task models, standard MTL baselines, and state-of-the-art methods. On the PASCAL benchmark, RobuMTL delivers a +2.8% average relative improvement under single perturbations and up to +44.4% under mixed weather conditions compared to the MTL baseline. On NYUD-v2, RobuMTL achieves a +9.7% average relative improvement across tasks. The code is available at GitHub.

</details>


### [12] [Sparse Data Tree Canopy Segmentation: Fine-Tuning Leading Pretrained Models on Only 150 Images](https://arxiv.org/abs/2601.10931)
*David Szczecina,Hudson Sun,Anthony Bertnyk,Niloofar Azad,Kyle Gao,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 该论文评估了五种深度学习架构在仅有150张标注图像的小数据集上进行树冠检测的性能，发现基于卷积的预训练模型（YOLOv11和Mask R-CNN）比基于Transformer的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 在真实数据标注稀缺的情况下，研究如何在极少量数据上训练深度学习模型进行树冠检测，避免过拟合问题。

Method: 使用Solafune树冠检测竞赛提供的150张标注图像，评估YOLOv11、Mask R-CNN、DeepLabv3、Swin-UNet和DINOv2五种架构的性能，分析训练策略、数据增强和模型行为。

Result: 预训练的卷积模型（特别是YOLOv11和Mask R-CNN）泛化能力显著优于Transformer模型。DeepLabv3、Swin-UNet和DINOv2表现不佳，主要由于语义分割与实例分割任务差异、Vision Transformer的高数据需求以及缺乏强归纳偏置。

Conclusion: 在数据稀缺情况下，基于CNN的轻量级方法仍然是树冠检测最可靠的选择，Transformer架构在低数据环境下表现不佳，除非有大量预训练或数据增强。

Abstract: Tree canopy detection from aerial imagery is an important task for environmental monitoring, urban planning, and ecosystem analysis. Simulating real-life data annotation scarcity, the Solafune Tree Canopy Detection competition provides a small and imbalanced dataset of only 150 annotated images, posing significant challenges for training deep models without severe overfitting. In this work, we evaluate five representative architectures, YOLOv11, Mask R-CNN, DeepLabv3, Swin-UNet, and DINOv2, to assess their suitability for canopy segmentation under extreme data scarcity. Our experiments show that pretrained convolution-based models, particularly YOLOv11 and Mask R-CNN, generalize significantly better than pretrained transformer-based models. DeeplabV3, Swin-UNet and DINOv2 underperform likely due to differences between semantic and instance segmentation tasks, the high data requirements of Vision Transformers, and the lack of strong inductive biases. These findings confirm that transformer-based architectures struggle in low-data regimes without substantial pretraining or augmentation and that differences between semantic and instance segmentation further affect model performance. We provide a detailed analysis of training strategies, augmentation policies, and model behavior under the small-data constraint and demonstrate that lightweight CNN-based methods remain the most reliable for canopy detection on limited imagery.

</details>


### [13] [PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis](https://arxiv.org/abs/2601.10945)
*K Lokesh,Abhirama Subramanyam Penamakuri,Uday Agarwal,Apoorva Challa,Shreya K Gowda,Somesh Gupta,Anand Mishra*

Main category: cs.CV

TL;DR: 提出了一种预咨询对话框架(PCDF)，通过模拟医生与患者之间的多轮对话来提升医学诊断准确性，相比仅依赖图像分析的方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统AI医学诊断主要依赖图像分析，但缺乏患者报告的症状信息限制了诊断准确性。真实诊断过程中医生会通过询问症状来辅助诊断，因此需要模拟这种对话交互过程。

Method: 设计了DocVLM和PatientVLM两个视觉语言模型：DocVLM基于图像和对话历史生成后续问题，PatientVLM根据真实诊断生成的症状档案进行回答。通过这种对话模拟生成包含图像和诊断的多轮咨询数据，用于微调DocVLM。

Result: 小规模临床验证表明合成症状具有临床相关性、症状覆盖性和整体真实性。对话监督训练相比仅图像训练带来了显著性能提升。

Conclusion: 基于对话的监督训练能够有效提升诊断准确性，证明了现实症状采集在医学诊断中的重要性。

Abstract: Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.

</details>


### [14] [MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement](https://arxiv.org/abs/2601.10949)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Haiqin Zhong,Xiaoling Luo,Wenting Chen,Linlin Shen*

Main category: cs.CV

TL;DR: MMedExpert-R1是一个针对医学视觉语言模型的推理增强框架，通过领域特定适应和临床指南强化来解决现有方法在复杂临床推理中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉语言模型在感知任务上表现优秀，但在复杂临床推理方面存在困难，主要挑战包括深度推理数据稀缺、多专科对齐的冷启动问题，以及标准强化学习算法无法建模临床推理的多样性。

Method: 构建了包含10K样本的高质量MMedExpert数据集；采用领域特定适应创建专科特定的LoRA模块提供多样化初始化；通过指南基础优势明确建模不同临床推理视角；最后使用冲突感知能力集成将专科专家统一为单一代理。

Result: 在MedXpert-MM上达到27.50分，在OmniMedVQA上达到83.03分，实现了最先进的性能表现。

Conclusion: MMedExpert-R1为可靠的多模态医学推理系统建立了坚实基础，有效提升了医学视觉语言模型的临床推理能力。

Abstract: Medical Vision-Language Models (MedVLMs) excel at perception tasks but struggle with complex clinical reasoning required in real-world scenarios. While reinforcement learning (RL) has been explored to enhance reasoning capabilities, existing approaches face critical mismatches: the scarcity of deep reasoning data, cold-start limits multi-specialty alignment, and standard RL algorithms fail to model clinical reasoning diversity. We propose MMedExpert-R1, a novel reasoning MedVLM that addresses these challenges through domain-specific adaptation and clinical guideline reinforcement. We construct MMedExpert, a high-quality dataset of 10K samples across four specialties with step-by-step reasoning traces. Our Domain-Specific Adaptation (DSA) creates specialty-specific LoRA modules to provide diverse initialization, while Guideline-Based Advantages (GBA) explicitly models different clinical reasoning perspectives to align with real-world diagnostic strategies. Conflict-Aware Capability Integration then merges these specialized experts into a unified agent, ensuring robust multi-specialty alignment. Comprehensive experiments demonstrate state-of-the-art performance, with our 7B model achieving 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, establishing a robust foundation for reliable multimodal medical reasoning systems.

</details>


### [15] [IDDR-NGP: Incorporating Detectors for Distractor Removal with Instant Neural Radiance Field](https://arxiv.org/abs/2601.11030)
*Xianliang Huang,Jiajie Gou,Shuhang Chen,Zhizhou Zhong,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: IDDR-NGP是首个统一的分心物去除方法，可直接在Instant-NGP上操作，能够去除3D场景中的多种分心物（如雪花、彩纸、落叶等），而现有方法通常只针对特定类型。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景分心物去除方法通常只针对特定类型的分心物，缺乏统一的解决方案。需要开发一种能够处理多种分心物的通用方法，以更高效地恢复被污染的3D场景。

Method: 结合隐式3D表示和2D检测器，设计LPIPS损失和多视角补偿损失(MVCL)联合优化渲染结果，可端到端训练合成高质量3D场景。构建了包含合成和真实分心物的新基准数据集。

Result: 实验结果表明IDDR-NGP在去除多种分心物方面具有有效性和鲁棒性，与现有SOTA去雪方法效果相当，能够准确去除真实和合成分心物。

Conclusion: IDDR-NGP首次实现了统一的分心物去除框架，通过多视角信息聚合和联合优化策略，在3D场景恢复方面取得了显著进展，为隐式3D表示中的分心物去除研究提供了新的基准。

Abstract: This paper presents the first unified distractor removal method, named IDDR-NGP, which directly operates on Instant-NPG. The method is able to remove a wide range of distractors in 3D scenes, such as snowflakes, confetti, defoliation and petals, whereas existing methods usually focus on a specific type of distractors. By incorporating implicit 3D representations with 2D detectors, we demonstrate that it is possible to efficiently restore 3D scenes from multiple corrupted images. We design the learned perceptual image patch similarity~( LPIPS) loss and the multi-view compensation loss (MVCL) to jointly optimize the rendering results of IDDR-NGP, which could aggregate information from multi-view corrupted images. All of them can be trained in an end-to-end manner to synthesize high-quality 3D scenes. To support the research on distractors removal in implicit 3D representations, we build a new benchmark dataset that consists of both synthetic and real-world distractors. To validate the effectiveness and robustness of IDDR-NGP, we provide a wide range of distractors with corresponding annotated labels added to both realistic and synthetic scenes. Extensive experimental results demonstrate the effectiveness and robustness of IDDR-NGP in removing multiple types of distractors. In addition, our approach achieves results comparable with the existing SOTA desnow methods and is capable of accurately removing both realistic and synthetic distractors.

</details>


### [16] [Your One-Stop Solution for AI-Generated Video Detection](https://arxiv.org/abs/2601.11035)
*Long Ma,Zihao Xue,Yan Wang,Zhiyuan Yan,Jin Xu,Xiaorui Jiang,Haiyang Yu,Yong Liao,Zhen Bi*

Main category: cs.CV

TL;DR: 该论文提出了AIGVDBench基准测试，旨在解决现有AI生成视频检测领域在数据集规模和模型覆盖范围上的不足，通过覆盖31个最先进生成模型和44万多个视频，对33个现有检测器进行1500多次评估，提供了8个深入分析和4个新发现。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频检测领域存在两大限制：数据集规模有限且使用过时生成模型，缺乏语义多样性和技术代表性；现有基准测试停留在数据集创建阶段，缺乏系统性深入分析。

Method: 构建AIGVDBench基准测试，涵盖31个最先进生成模型和超过44万视频，对33个现有检测器（分属4个类别）进行1500多次评估，从多个角度进行8个深入分析。

Result: 通过大规模评估发现了4个新发现，为未来研究提供了宝贵见解，证明了现有检测器在面对现代生成技术时的局限性。

Conclusion: AIGVDBench为AI生成视频检测领域提供了坚实基础，有助于推动该领域的发展，基准测试已开源。

Abstract: Recent advances in generative modeling can create remarkably realistic synthetic videos, making it increasingly difficult for humans to distinguish them from real ones and necessitating reliable detection methods.
  However, two key limitations hinder the development of this field.
  \textbf{From the dataset perspective}, existing datasets are often limited in scale and constructed using outdated or narrowly scoped generative models, making it difficult to capture the diversity and rapid evolution of modern generative techniques. Moreover, the dataset construction process frequently prioritizes quantity over quality, neglecting essential aspects such as semantic diversity, scenario coverage, and technological representativeness.
  \textbf{From the benchmark perspective}, current benchmarks largely remain at the stage of dataset creation, leaving many fundamental issues and in-depth analysis yet to be systematically explored.
  Addressing this gap, we propose AIGVDBench, a benchmark designed to be comprehensive and representative, covering \textbf{31} state-of-the-art generation models and over \textbf{440,000} videos. By executing more than \textbf{1,500} evaluations on \textbf{33} existing detectors belonging to four distinct categories. This work presents \textbf{8 in-depth analyses} from multiple perspectives and identifies \textbf{4 novel findings} that offer valuable insights for future research. We hope this work provides a solid foundation for advancing the field of AI-generated video detection.
  Our benchmark is open-sourced at https://github.com/LongMa-2025/AIGVDBench.

</details>


### [17] [M3DDM+: An improved video outpainting by a modified masking strategy](https://arxiv.org/abs/2601.11048)
*Takuya Murakawa,Takumi Fukuzawa,Ning Ding,Toru Tamaki*

Main category: cs.CV

TL;DR: M3DDM+通过改进M3DDM的掩码策略，解决了视频外推在信息有限场景下的质量下降问题，显著提升了视觉保真度和时间一致性。


<details>
  <summary>Details</summary>
Motivation: M3DDM在相机运动有限或外推区域较大的挑战性场景中会出现明显的空间模糊和时间不一致问题，这是由于训练和推理阶段掩码策略不匹配导致的。

Method: 提出M3DDM+，在训练阶段对所有帧应用统一的掩码方向和宽度，然后对预训练的M3DDM模型进行微调，以解决训练-推理不匹配问题。

Result: 实验表明M3DDM+在信息有限场景下显著改善了视觉保真度和时间一致性，同时保持了计算效率。

Conclusion: M3DDM+通过统一训练和推理的掩码策略，有效解决了M3DDM在挑战性场景下的质量退化问题，为视频外推提供了更可靠的解决方案。

Abstract: M3DDM provides a computationally efficient framework for video outpainting via latent diffusion modeling. However, it exhibits significant quality degradation -- manifested as spatial blur and temporal inconsistency -- under challenging scenarios characterized by limited camera motion or large outpainting regions, where inter-frame information is limited. We identify the cause as a training-inference mismatch in the masking strategy: M3DDM's training applies random mask directions and widths across frames, whereas inference requires consistent directional outpainting throughout the video. To address this, we propose M3DDM+, which applies uniform mask direction and width across all frames during training, followed by fine-tuning of the pretrained M3DDM model. Experiments demonstrate that M3DDM+ substantially improves visual fidelity and temporal coherence in information-limited scenarios while maintaining computational efficiency. The code is available at https://github.com/tamaki-lab/M3DDM-Plus.

</details>


### [18] [PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models](https://arxiv.org/abs/2601.11087)
*Qiyuan Zhang,Biao Gong,Shuai Tan,Zheng Zhang,Yujun Shen,Xing Zhu,Yuyuan Li,Kelu Yao,Chunhua Shen,Changqing Zou*

Main category: cs.CV

TL;DR: 本文提出了首个基于物理感知强化学习的视频生成模型范式，通过在高维空间中直接强制执行物理碰撞规则，解决了transformer视频生成中物理原理缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有transformer视频生成方法在像素级全局去噪过程中丢弃了物体刚性的概念，即使数学约束正确，在模型优化中也被视为次优解，严重限制了生成视频的物理真实性。

Method: 引入物理感知强化学习范式，直接在高维空间强制执行物理碰撞规则；提出Mimicry-Discovery Cycle（MDcycle）统一框架，支持大规模微调同时完全保留模型利用物理基础反馈的能力。

Result: 构建了新的基准测试PhysRVGBench，并通过广泛的定性和定量实验验证了方法的有效性。

Conclusion: 该方法首次将物理规则严格应用于视频生成过程，而非仅作为条件，显著提升了生成视频的物理真实感。

Abstract: Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.

</details>


### [19] [CoDance: An Unbind-Rebind Paradigm for Robust Multi-Subject Animation](https://arxiv.org/abs/2601.11096)
*Shuai Tan,Biao Gong,Ke Ma,Yutong Feng,Qiyuan Zhang,Yan Wang,Yujun Shen,Hengshuang Zhao*

Main category: cs.CV

TL;DR: CoDance提出了一种Unbind-Rebind框架，用于解决多角色图像动画中任意数量、类型和空间配置的问题，通过解绑刚性空间绑定和重新绑定运动到目标角色来实现更好的动画效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单人动画方面表现良好，但难以处理任意数量的角色、多样化的角色类型以及参考图像与驱动姿态之间的空间错位问题。这些限制源于过于刚性的空间绑定和无法将运动一致地重新绑定到目标角色。

Method: CoDance采用Unbind-Rebind框架：Unbind模块使用姿态偏移编码器通过引入随机扰动打破姿态与参考之间的刚性空间绑定；Rebind模块利用文本提示的语义指导和主体掩码的空间指导将学习到的运动引导到目标角色。

Result: 在新建的CoDanceBench和现有数据集上的大量实验表明，CoDance实现了最先进的性能，在不同角色和空间布局上表现出显著的泛化能力。

Conclusion: CoDance通过创新的Unbind-Rebind框架有效解决了多角色动画中的关键挑战，代码和权重将开源。

Abstract: Character image animation is gaining significant importance across various domains, driven by the demand for robust and flexible multi-subject rendering. While existing methods excel in single-person animation, they struggle to handle arbitrary subject counts, diverse character types, and spatial misalignment between the reference image and the driving poses. We attribute these limitations to an overly rigid spatial binding that forces strict pixel-wise alignment between the pose and reference, and an inability to consistently rebind motion to intended subjects. To address these challenges, we propose CoDance, a novel Unbind-Rebind framework that enables the animation of arbitrary subject counts, types, and spatial configurations conditioned on a single, potentially misaligned pose sequence. Specifically, the Unbind module employs a novel pose shift encoder to break the rigid spatial binding between the pose and the reference by introducing stochastic perturbations to both poses and their latent features, thereby compelling the model to learn a location-agnostic motion representation. To ensure precise control and subject association, we then devise a Rebind module, leveraging semantic guidance from text prompts and spatial guidance from subject masks to direct the learned motion to intended characters. Furthermore, to facilitate comprehensive evaluation, we introduce a new multi-subject CoDanceBench. Extensive experiments on CoDanceBench and existing datasets show that CoDance achieves SOTA performance, exhibiting remarkable generalization across diverse subjects and spatial layouts. The code and weights will be open-sourced.

</details>


### [20] [Graph Smoothing for Enhanced Local Geometry Learning in Point Cloud Analysis](https://arxiv.org/abs/2601.11102)
*Shangbo Yuan,Jie Xu,Ping Hu,Xiaofeng Zhu,Na Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种结合图平滑模块和增强局部几何学习模块的新方法，用于解决3D点云分析中传统图结构在边界点和连接区域存在的稀疏连接和噪声连接问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于图的方法在3D点云分析中虽然有效，但常常因边界点的稀疏连接和连接区域的噪声连接而导致图结构不理想，影响分析性能。

Method: 提出图平滑模块来优化图结构，减少不可靠连接的影响；基于优化后的图结构，通过自适应几何描述符（基于特征向量）和圆柱坐标变换来增强局部几何特征提取。

Result: 在真实世界数据集上的实验表明，该方法在点云分类、部件分割和语义分割等任务中均表现出有效性。

Conclusion: 通过图结构优化和局部几何特征增强，该方法能够有效提升3D点云分析任务的性能。

Abstract: Graph-based methods have proven to be effective in capturing relationships among points for 3D point cloud analysis. However, these methods often suffer from suboptimal graph structures, particularly due to sparse connections at boundary points and noisy connections in junction areas. To address these challenges, we propose a novel method that integrates a graph smoothing module with an enhanced local geometry learning module. Specifically, we identify the limitations of conventional graph structures, particularly in handling boundary points and junction areas. In response, we introduce a graph smoothing module designed to optimize the graph structure and minimize the negative impact of unreliable sparse and noisy connections. Based on the optimized graph structure, we improve the feature extract function with local geometry information. These include shape features derived from adaptive geometric descriptors based on eigenvectors and distribution features obtained through cylindrical coordinate transformation. Experimental results on real-world datasets validate the effectiveness of our method in various point cloud learning tasks, i.e., classification, part segmentation, and semantic segmentation.

</details>


### [21] [Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning](https://arxiv.org/abs/2601.11109)
*Shaofeng Yin,Jiaxin Ge,Zora Zhiruo Wang,Xiuyu Li,Michael J. Black,Trevor Darrell,Angjoo Kanazawa,Haiwen Feng*

Main category: cs.CV

TL;DR: VIGA是一个通过迭代执行和验证实现视觉逆向图形化的智能体系统，能够在空场景中重建或编辑图形程序，显著提升单次推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型缺乏细粒度空间和物理基础能力，无法实现一次性视觉逆向图形化的问题。

Method: 采用闭环的写-运行-渲染-比较-修订流程，结合技能库和演进上下文记忆，实现任务无关和模型无关的多模态推理。

Result: 在BlenderGym上提升35.32%，SlideBench上提升117.17%，BlenderBench上提升124.70%，显著优于单次推理基线。

Conclusion: VIGA通过迭代多模态推理有效解决了视觉逆向图形化的挑战，为异构基础模型的统一评估提供了可行方案。

Abstract: Vision-as-inverse-graphics, the concept of reconstructing an image as an editable graphics program is a long-standing goal of computer vision. Yet even strong VLMs aren't able to achieve this in one-shot as they lack fine-grained spatial and physical grounding capability. Our key insight is that closing this gap requires interleaved multimodal reasoning through iterative execution and verification. Stemming from this, we present VIGA (Vision-as-Inverse-Graphic Agent) that starts from an empty world and reconstructs or edits scenes through a closed-loop write-run-render-compare-revise procedure. To support long-horizon reasoning, VIGA combines (i) a skill library that alternates generator and verifier roles and (ii) an evolving context memory that contains plans, code diffs, and render history. VIGA is task-agnostic as it doesn't require auxiliary modules, covering a wide range of tasks such as 3D reconstruction, multi-step scene editing, 4D physical interaction, and 2D document editing, etc. Empirically, we found VIGA substantially improves one-shot baselines on BlenderGym (35.32%) and SlideBench (117.17%). Moreover, VIGA is also model-agnostic as it doesn't require finetuning, enabling a unified protocol to evaluate heterogeneous foundation VLMs. To better support this protocol, we introduce BlenderBench, a challenging benchmark that stress-tests interleaved multimodal reasoning with graphics engine, where VIGA improves by 124.70%.

</details>


### [22] [SoLA-Vision: Fine-grained Layer-wise Linear Softmax Hybrid Attention](https://arxiv.org/abs/2601.11164)
*Ruibang Li,Guan Luo,Yiwei Zhang,Jin Gao,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: 该论文提出SoLA-Vision，一种层级的软注意力与线性注意力混合架构，通过精细控制软注意力层的插入位置，在保持线性注意力计算效率的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力在视觉任务中表现出色但计算复杂度高，线性注意力计算效率高但建模能力受限。需要找到平衡计算效率和模型性能的混合方案。

Method: 通过分析软注意力和线性注意力的层级堆叠特性，系统研究不同层级混合模式，提出SoLA-Vision框架，通过策略性插入少量全局softmax层实现精细混合。

Result: 在ImageNet-1K上优于纯线性和其他混合注意力模型，在密集预测任务中显著超越强基线模型。

Conclusion: 层级细粒度混合注意力设计能够实现精度和计算成本的良好权衡，SoLA-Vision为高分辨率视觉任务提供了高效解决方案。

Abstract: Standard softmax self-attention excels in vision tasks but incurs quadratic complexity O(N^2), limiting high-resolution deployment. Linear attention reduces the cost to O(N), yet its compressed state representations can impair modeling capacity and accuracy. We present an analytical study that contrasts linear and softmax attention for visual representation learning from a layer-stacking perspective. We further conduct systematic experiments on layer-wise hybridization patterns of linear and softmax attention. Our results show that, compared with rigid intra-block hybrid designs, fine-grained layer-wise hybridization can match or surpass performance while requiring fewer softmax layers. Building on these findings, we propose SoLA-Vision (Softmax-Linear Attention Vision), a flexible layer-wise hybrid attention backbone that enables fine-grained control over how linear and softmax attention are integrated. By strategically inserting a small number of global softmax layers, SoLA-Vision achieves a strong trade-off between accuracy and computational cost. On ImageNet-1K, SoLA-Vision outperforms purely linear and other hybrid attention models. On dense prediction tasks, it consistently surpasses strong baselines by a considerable margin. Code will be released.

</details>


### [23] [Democratizing planetary-scale analysis: An ultra-lightweight Earth embedding database for accurate and flexible global land monitoring](https://arxiv.org/abs/2601.11183)
*Shuang Chen,Jie Wang,Shuai Yuan,Jiayang Li,Yu Xia,Yuanhong Liao,Junbo Wei,Jincheng Yuan,Xiaoqing Xu,Xiaolin Zhu,Peng Zhu,Hongsheng Zhang,Yuyu Zhou,Haohuan Fu,Huabing Huang,Bin Chen,Fan Dai,Peng Gong*

Main category: cs.CV

TL;DR: ESD是一个超轻量级的30米全球地球嵌入数据库，通过将多传感器观测数据压缩为信息密集的量化潜在向量，实现了340倍的数据压缩，使全球地表分析能够在标准工作站上进行。


<details>
  <summary>Details</summary>
Motivation: 解决卫星地球观测系统数据量庞大、计算和存储需求高的问题，促进全球尺度研究的普及化。

Method: 使用ESDNet架构和有限标量量化(FSQ)，将Landsat系列和MODIS Terra的高维多传感器观测数据转换为信息密集的量化潜在向量，将年度物候周期压缩为12个时间步。

Result: 数据体积减少约340倍，单年全球陆地表面数据约2.4TB；重建保真度高(MAE:0.0130, RMSE:0.0179, CC:0.8543)；土地覆盖分类准确率达79.74%，优于原始融合数据的76.92%。

Conclusion: ESD为普及化行星尺度研究和推进下一代地理空间人工智能提供了多功能基础，具有强大的少样本学习能力和纵向一致性。

Abstract: The rapid evolution of satellite-borne Earth Observation (EO) systems has revolutionized terrestrial monitoring, yielding petabyte-scale archives. However, the immense computational and storage requirements for global-scale analysis often preclude widespread use, hindering planetary-scale studies. To address these barriers, we present Embedded Seamless Data (ESD), an ultra-lightweight, 30-m global Earth embedding database spanning the 25-year period from 2000 to 2024. By transforming high-dimensional, multi-sensor observations from the Landsat series (5, 7, 8, and 9) and MODIS Terra into information-dense, quantized latent vectors, ESD distills essential geophysical and semantic features into a unified latent space. Utilizing the ESDNet architecture and Finite Scalar Quantization (FSQ), the dataset achieves a transformative ~340-fold reduction in data volume compared to raw archives. This compression allows the entire global land surface for a single year to be encapsulated within approximately 2.4 TB, enabling decadal-scale global analysis on standard local workstations. Rigorous validation demonstrates high reconstructive fidelity (MAE: 0.0130; RMSE: 0.0179; CC: 0.8543). By condensing the annual phenological cycle into 12 temporal steps, the embeddings provide inherent denoising and a semantically organized space that outperforms raw reflectance in land-cover classification, achieving 79.74% accuracy (vs. 76.92% for raw fusion). With robust few-shot learning capabilities and longitudinal consistency, ESD provides a versatile foundation for democratizing planetary-scale research and advancing next-generation geospatial artificial intelligence.

</details>


### [24] [ATATA: One Algorithm to Align Them All](https://arxiv.org/abs/2601.11194)
*Boyi Pang,Savva Ignatyev,Vladimir Ippolitov,Ramil Khafizov,Yurii Melnik,Oleg Voynov,Maksim Nakhodnov,Aibek Alanov,Xiaopeng Fan,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: 提出一种基于Rectified Flow模型的多模态联合推理算法，用于生成结构对齐的样本对，相比现有方法具有更快的推理速度和更好的结构对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有联合生成方法缺乏结构对齐视角，而基于Score Distillation Sampling的方法存在计算耗时、模式崩溃和卡通化结果等问题。

Method: 基于Rectified Flow模型，在结构化潜在空间中进行联合传输，可构建在任意Rectified Flow模型之上。

Result: 在图像、视频和3D形状生成领域验证了方法的有效性，相比编辑基和联合推理基方法，实现了高度结构对齐和高质量样本生成。

Conclusion: 该方法在图像和视频生成方面达到最先进水平，在3D生成方面质量相当但速度快几个数量级。

Abstract: We suggest a new multi-modal algorithm for joint inference of paired structurally aligned samples with Rectified Flow models. While some existing methods propose a codependent generation process, they do not view the problem of joint generation from a structural alignment perspective. Recent work uses Score Distillation Sampling to generate aligned 3D models, but SDS is known to be time-consuming, prone to mode collapse, and often provides cartoonish results. By contrast, our suggested approach relies on the joint transport of a segment in the sample space, yielding faster computation at inference time. Our approach can be built on top of an arbitrary Rectified Flow model operating on the structured latent space. We show the applicability of our method to the domains of image, video, and 3D shape generation using state-of-the-art baselines and evaluate it against both editing-based and joint inference-based competing approaches. We demonstrate a high degree of structural alignment for the sample pairs obtained with our method and a high visual quality of the samples. Our method improves the state-of-the-art for image and video generation pipelines. For 3D generation, it is able to show comparable quality while working orders of magnitude faster.

</details>


### [25] [Bio-inspired fine-tuning for selective transfer learning in image classification](https://arxiv.org/abs/2601.11235)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: BioTune是一种基于进化优化的自适应微调技术，通过优化选择冻结层和调整学习率来提升迁移学习效果，在多个图像分类数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 迁移学习虽然能利用预训练模型解决标注数据不足的问题，但源域和目标域之间的差异会阻碍有效迁移，需要更有效的自适应微调方法。

Method: 使用进化优化算法，自适应地选择需要冻结的层，并为未冻结层调整学习率，实现最优的迁移学习配置。

Result: 在9个图像分类数据集（包括自然图像和医学影像）上的评估显示，BioTune在准确性和效率上均优于AutoRGN和LoRA等先进方法，并在4种不同CNN架构上保持稳定性能。

Conclusion: BioTune是一种灵活高效的迁移学习微调技术，能够适应不同的数据特性和分布变化，消融研究验证了其关键组件的有效性。

Abstract: Deep learning has significantly advanced image analysis across diverse domains but often depends on large, annotated datasets for success. Transfer learning addresses this challenge by utilizing pre-trained models to tackle new tasks with limited labeled data. However, discrepancies between source and target domains can hinder effective transfer learning. We introduce BioTune, a novel adaptive fine-tuning technique utilizing evolutionary optimization. BioTune enhances transfer learning by optimally choosing which layers to freeze and adjusting learning rates for unfrozen layers. Through extensive evaluation on nine image classification datasets, spanning natural and specialized domains such as medical imaging, BioTune demonstrates superior accuracy and efficiency over state-of-the-art fine-tuning methods, including AutoRGN and LoRA, highlighting its adaptability to various data characteristics and distribution changes. Additionally, BioTune consistently achieves top performance across four different CNN architectures, underscoring its flexibility. Ablation studies provide valuable insights into the impact of BioTune's key components on overall performance. The source code is available at https://github.com/davilac/BioTune.

</details>


### [26] [Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification](https://arxiv.org/abs/2601.11243)
*Zhiqi Pang,Lingling Zhao,Yang Liu,Chunyu Wang,Gaurav Sharma*

Main category: cs.CV

TL;DR: 本文提出了无监督多场景行人重识别任务，并开发了图像-文本知识建模框架，通过三个阶段有效利用视觉语言模型的表示能力，在多个场景下实现了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 传统行人重识别方法通常针对单一场景设计，缺乏处理多场景（如跨分辨率、换装等）的统一框架。本文旨在开发一个能够同时处理多种场景的通用行人重识别框架。

Method: 提出三阶段ITKM框架：第一阶段在图像编码器中引入场景嵌入并微调；第二阶段优化文本嵌入与伪标签关联，并引入多场景分离损失；第三阶段通过异构匹配模块获取可靠正样本对，并动态更新文本表示。

Result: 实验结果表明ITKM在多个场景下不仅优于现有的场景特定方法，还能通过整合多场景知识提升整体性能。

Conclusion: ITKM框架成功解决了无监督多场景行人重识别问题，展示了视觉语言模型在多场景学习中的强大潜力，为未来相关研究提供了新思路。

Abstract: We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.

</details>


### [27] [Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval](https://arxiv.org/abs/2601.11248)
*Fangke Chen,Tianhao Dong,Sirry Chen,Guobin Zhang,Yishu Zhang,Yining Chen*

Main category: cs.CV

TL;DR: 提出轻量级非对称双编码器框架，用于跨语言手写词检索，通过联合优化实例级对齐和类级语义一致性，实现风格不变视觉嵌入。


<details>
  <summary>Details</summary>
Motivation: 手写词检索在数字档案中至关重要，但由于手写变异性大和跨语言语义鸿沟，现有方法面临挑战。大型视觉语言模型计算成本高，难以在边缘设备部署。

Method: 采用轻量级非对称双编码器框架，学习统一的风格不变视觉嵌入。通过联合优化实例级对齐和类级语义一致性，将视觉嵌入锚定到语言无关的语义原型上。

Result: 在28个基线方法中表现最优，在跨语言检索任务中达到最先进准确率，仅需现有模型参数的一小部分即可实现强性能。

Conclusion: 该框架实现了准确且资源高效的跨脚本手写检索，为边缘设备部署提供了可行的解决方案。

Abstract: Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.

</details>


### [28] [FTDMamba: Frequency-Assisted Temporal Dilation Mamba for Unmanned Aerial Vehicle Video Anomaly Detection](https://arxiv.org/abs/2601.11254)
*Cheng-Zhuang Liu,Si-Bao Chen,Qing-Ling Shu,Chris Ding,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出FTDMamba网络用于无人机视频异常检测，通过频率解耦和时序扩张Mamba模块解决动态背景下的多源运动耦合问题，并在新构建的MUVAD数据集上验证了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法主要针对静态背景，而无人机动态背景视频存在多源运动耦合问题，容易误判正常无人机运动为异常或漏检真实异常。

Method: 提出FTDMamba网络，包含频率解耦时空关联模块（通过频率分析解耦运动模式）和时序扩张Mamba模块（联合学习多时间尺度的细粒度时序动态和局部空间结构）。

Result: 在两个公开静态基准测试和新构建的MUVAD数据集（包含222,736帧、240个异常事件、12种异常类型）上实现了最先进的性能。

Conclusion: FTDMamba能有效处理无人机动态背景视频的异常检测问题，提出的方法在多个数据集上表现出优越性能，代码和数据集将开源。

Abstract: Recent advances in video anomaly detection (VAD) mainly focus on ground-based surveillance or unmanned aerial vehicle (UAV) videos with static backgrounds, whereas research on UAV videos with dynamic backgrounds remains limited. Unlike static scenarios, dynamically captured UAV videos exhibit multi-source motion coupling, where the motion of objects and UAV-induced global motion are intricately intertwined. Consequently, existing methods may misclassify normal UAV movements as anomalies or fail to capture true anomalies concealed within dynamic backgrounds. Moreover, many approaches do not adequately address the joint modeling of inter-frame continuity and local spatial correlations across diverse temporal scales. To overcome these limitations, we propose the Frequency-Assisted Temporal Dilation Mamba (FTDMamba) network for UAV VAD, including two core components: (1) a Frequency Decoupled Spatiotemporal Correlation Module, which disentangles coupled motion patterns and models global spatiotemporal dependencies through frequency analysis; and (2) a Temporal Dilation Mamba Module, which leverages Mamba's sequence modeling capability to jointly learn fine-grained temporal dynamics and local spatial structures across multiple temporal receptive fields. Additionally, unlike existing UAV VAD datasets which focus on static backgrounds, we construct a large-scale Moving UAV VAD dataset (MUVAD), comprising 222,736 frames with 240 anomaly events across 12 anomaly types. Extensive experiments demonstrate that FTDMamba achieves state-of-the-art (SOTA) performance on two public static benchmarks and the new MUVAD dataset. The code and MUVAD dataset will be available at: https://github.com/uavano/FTDMamba.

</details>


### [29] [X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning](https://arxiv.org/abs/2601.11269)
*Maanping Shao,Feihong Zhang,Gu Zhang,Baiye Cheng,Zhengrong Xue,Huazhe Xu*

Main category: cs.CV

TL;DR: X-Distill是一种通过跨架构知识蒸馏将大型ViT教师模型的视觉表示迁移到紧凑CNN学生模型的方法，在数据稀缺的机器人学习场景中实现了优异的性能


<details>
  <summary>Details</summary>
Motivation: 解决在数据稀缺的机器人学习环境中，大型预训练ViT模型数据需求高而紧凑CNN模型难以获得强大泛化能力之间的权衡问题

Method: 采用离线跨架构知识蒸馏，将冻结的DINOv2教师模型的丰富视觉表示迁移到ResNet-18学生模型上，然后在目标操作任务上联合微调蒸馏后的编码器和扩散策略头

Result: 在34个模拟基准和5个真实世界任务上的实验表明，X-Distill始终优于从头训练的ResNet或微调DINOv2编码器的策略，甚至超过使用特权点云观测或更大视觉语言模型的3D编码器

Conclusion: 简单而有效的知识蒸馏策略可以在数据高效的机器人操作中实现最先进的性能，为数据稀缺场景下的机器人学习提供了实用解决方案

Abstract: Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.

</details>


### [30] [Efficient On-Board Processing of Oblique UAV Video for Rapid Flood Extent Mapping](https://arxiv.org/abs/2601.11290)
*Vishisht Sharma,Sam Leroux,Lisa Landuyt,Nick Witvrouwen,Pieter Simoens*

Main category: cs.CV

TL;DR: 提出Temporal Token Reuse (TTR)框架，通过利用航空视频的时空冗余性，在嵌入式设备上加速视频分割，实现30%的延迟降低且精度损失可忽略


<details>
  <summary>Details</summary>
Motivation: 无人机在灾害响应中需要处理高分辨率倾斜视频流，但受限于SWaP约束，标准边缘硬件难以实现低延迟推理

Method: TTR将图像块视为token，使用轻量级相似度度量动态识别静态区域并传播预计算的深度特征，避免冗余的骨干网络计算

Result: 在边缘级硬件上，TTR实现推理延迟降低30%，分割精度损失小于0.5% mIoU

Conclusion: TTR有效提升了操作帕累托边界，为时间关键的遥感任务实现了高保真、实时的倾斜视频理解

Abstract: Effective disaster response relies on rapid disaster response, where oblique aerial video is the primary modality for initial scouting due to its ability to maximize spatial coverage and situational awareness in limited flight time. However, the on-board processing of high-resolution oblique streams is severely bottlenecked by the strict Size, Weight, and Power (SWaP) constraints of Unmanned Aerial Vehicles (UAVs). The computational density required to process these wide-field-of-view streams precludes low-latency inference on standard edge hardware. To address this, we propose Temporal Token Reuse (TTR), an adaptive inference framework capable of accelerating video segmentation on embedded devices. TTR exploits the intrinsic spatiotemporal redundancy of aerial video by formulating image patches as tokens; it utilizes a lightweight similarity metric to dynamically identify static regions and propagate their precomputed deep features, thereby bypassing redundant backbone computations. We validate the framework on standard benchmarks and a newly curated Oblique Floodwater Dataset designed for hydrological monitoring. Experimental results on edge-grade hardware demonstrate that TTR achieves a 30% reduction in inference latency with negligible degradation in segmentation accuracy (< 0.5% mIoU). These findings confirm that TTR effectively shifts the operational Pareto frontier, enabling high-fidelity, real-time oblique video understanding for time-critical remote sensing missions

</details>


### [31] [SAMannot: A Memory-Efficient, Local, Open-source Framework for Interactive Video Instance Segmentation based on SAM2](https://arxiv.org/abs/2601.11301)
*Gergely Dinya,András Gelencsér,Krisztina Kupán,Clemens Küpper,Kristóf Karacs,Anna Gelencsér-Horváth*

Main category: cs.CV

TL;DR: SAMannot是一个开源本地视频实例分割框架，集成SAM2模型到人机协作工作流，解决手动标注瓶颈和云服务隐私问题


<details>
  <summary>Details</summary>
Motivation: 当前视频分割研究面临手动标注耗时、商业平台昂贵、云服务隐私泄露等问题，需要一种高效、私密、成本可控的解决方案

Method: 修改SAM2依赖并实现处理层以减少计算开销，采用持久实例身份管理、自动锁定优化工作流和基于掩码骨架化的自动提示机制

Result: 通过动物行为追踪和LVOS、DAVIS基准数据集验证，该工具能生成YOLO和PNG格式的研究就绪数据集及结构化交互日志

Conclusion: SAMannot为复杂视频标注任务提供了可扩展、私密且经济高效的商业平台替代方案

Abstract: Current research workflows for precise video segmentation are often forced into a compromise between labor-intensive manual curation, costly commercial platforms, and/or privacy-compromising cloud-based services. The demand for high-fidelity video instance segmentation in research is often hindered by the bottleneck of manual annotation and the privacy concerns of cloud-based tools. We present SAMannot, an open-source, local framework that integrates the Segment Anything Model 2 (SAM2) into a human-in-the-loop workflow. To address the high resource requirements of foundation models, we modified the SAM2 dependency and implemented a processing layer that minimizes computational overhead and maximizes throughput, ensuring a highly responsive user interface. Key features include persistent instance identity management, an automated ``lock-and-refine'' workflow with barrier frames, and a mask-skeletonization-based auto-prompting mechanism. SAMannot facilitates the generation of research-ready datasets in YOLO and PNG formats alongside structured interaction logs. Verified through animal behavior tracking use-cases and subsets of the LVOS and DAVIS benchmark datasets, the tool provides a scalable, private, and cost-effective alternative to commercial platforms for complex video annotation tasks.

</details>


### [32] [Context-Aware Semantic Segmentation via Stage-Wise Attention](https://arxiv.org/abs/2601.11310)
*Antoine Carreaud,Elias Naha,Arthur Chansel,Nina Lahellec,Jan Skaloud,Adrien Gressin*

Main category: cs.CV

TL;DR: CASWiT是一种双分支Swin Transformer架构，用于解决超高分辨率图像分割中Transformer内存消耗过大的问题，通过上下文感知和跨尺度融合提升分割性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在超高分辨率图像分割中面临内存消耗随token数量平方增长的问题，限制了上下文范围或空间分辨率。

Method: 提出双分支架构：上下文编码器处理下采样邻域捕获长程依赖，高分辨率编码器提取细节特征；使用跨尺度融合模块结合交叉注意力和门控特征注入；采用SimMIM风格预训练，掩码75%高分辨率token和低分辨率中心区域进行重建训练。

Result: 在IGN FLAIR-HUB数据集上达到65.83% mIoU，比RGB基线提升1.78个百分点；在URUR数据集上达到49.1% mIoU，超越当前SOTA 0.9%。

Conclusion: CASWiT通过上下文感知的分阶段Transformer架构和创新的预训练策略，有效解决了超高分辨率图像分割中的内存约束问题，在多个数据集上实现了state-of-the-art性能。

Abstract: Semantic ultra high resolution image (UHR) segmentation is essential in remote sensing applications such as aerial mapping and environmental monitoring. Transformer-based models struggle in this setting because memory grows quadratically with token count, constraining either the contextual scope or the spatial resolution. We introduce CASWiT (Context-Aware Stage-Wise Transformer), a dual-branch, Swin-based architecture that injects global cues into fine-grained UHR features. A context encoder processes a downsampled neighborhood to capture long-range dependencies, while a high resolution encoder extracts detailed features from UHR patches. A cross-scale fusion module, combining cross-attention and gated feature injection, enriches high-resolution tokens with context. Beyond architecture, we propose a SimMIM-style pretraining. We mask 75% of the high-resolution image tokens and the low-resolution center region that spatially corresponds to the UHR patch, then train the shared dual-encoder with small decoder to reconstruct the UHR initial image. Extensive experiments on the large-scale IGN FLAIR-HUB aerial dataset demonstrate the effectiveness of CASWiT. Our method achieves 65.83% mIoU, outperforming RGB baselines by 1.78 points. On URUR, CASWiT achieves 49.1% mIoU, surpassing the current SoTA by +0.9% under the official evaluation protocol. All codes are provided on: https://huggingface.co/collections/heig-vd-geo/caswit.

</details>


### [33] [Enhancing Vision Language Models with Logic Reasoning for Situational Awareness](https://arxiv.org/abs/2601.11322)
*Pavana Pradeep,Krishna Kant,Suya Yu*

Main category: cs.CV

TL;DR: 本文提出了一种将视觉语言模型与传统计算机视觉方法通过显式逻辑推理相结合的方法，以增强情境感知能力，包括细粒度事件细节提取、智能微调策略和推理过程中的输出合理性判断。


<details>
  <summary>Details</summary>
Motivation: 在情境感知应用中，需要可靠准确地识别低频但重要的事件，同时提取细粒度细节并评估识别质量。视觉语言模型虽然能生成可解释的描述，但在这些关键需求上仍有改进空间。

Method: 通过将视觉语言模型与传统计算机视觉方法集成，采用显式逻辑推理，实现细粒度事件细节提取、智能微调策略（比无指导选择显著提高准确性）以及推理过程中生成VLM输出的合理性判断。

Result: 智能微调机制提高了准确性，并在推理过程中提供了确认VLM输出有效性或指出其可能存在问题的有价值手段。

Conclusion: 该方法通过逻辑推理的集成策略有效增强了视觉语言模型在情境感知应用中的性能，特别是在处理低频重要事件和细粒度细节识别方面表现出色。

Abstract: Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.

</details>


### [34] [Beer-Lambert Autoencoder for Unsupervised Stain Representation Learning and Deconvolution in Multi-immunohistochemical Brightfield Histology Images](https://arxiv.org/abs/2601.11336)
*Mark Eastwood,Thomas McKee,Zedong Hu,Sabine Tejpar,Fayyaz Minhas*

Main category: cs.CV

TL;DR: 提出了一种用于多路免疫组化（mIHC）RGB全玻片图像的染色分离方法，通过数据驱动的编码器-解码器架构学习特定队列的染色特征，生成清晰分离的染色浓度图。


<details>
  <summary>Details</summary>
Motivation: 传统Beer-Lambert颜色反卷积方法在K>3染色剂的多路免疫组化中变得欠定且不稳定，需要一种更稳健的染色分离技术。

Method: 使用紧凑的U-Net编码器预测K个非负浓度通道，结合可学习的染色矩阵进行可微分Beer-Lambert前向模型解码，采用无监督训练和感知重建目标。

Result: 在包含5种染色剂的结直肠mIHC面板上，该方法显示出优异的RGB重建效果，与基于矩阵的反卷积相比显著减少了通道间串扰。

Conclusion: 该方法为多路免疫组化提供了有效的染色分离解决方案，代码和模型已开源。

Abstract: Separating the contributions of individual chromogenic stains in RGB histology whole slide images (WSIs) is essential for stain normalization, quantitative assessment of marker expression, and cell-level readouts in immunohistochemistry (IHC). Classical Beer-Lambert (BL) color deconvolution is well-established for two- or three-stain settings, but becomes under-determined and unstable for multiplex IHC (mIHC) with K>3 chromogens. We present a simple, data-driven encoder-decoder architecture that learns cohort-specific stain characteristics for mIHC RGB WSIs and yields crisp, well-separated per-stain concentration maps. The encoder is a compact U-Net that predicts K nonnegative concentration channels; the decoder is a differentiable BL forward model with a learnable stain matrix initialized from typical chromogen hues. Training is unsupervised with a perceptual reconstruction objective augmented by loss terms that discourage unnecessary stain mixing. On a colorectal mIHC panel comprising 5 stains (H, CDX2, MUC2, MUC5, CD8) we show excellent RGB reconstruction, and significantly reduced inter-channel bleed-through compared with matrix-based deconvolution. Code and model are available at https://github.com/measty/StainQuant.git.

</details>


### [35] [Assessing Building Heat Resilience Using UAV and Street-View Imagery with Coupled Global Context Vision Transformer](https://arxiv.org/abs/2601.11357)
*Steffen Knoblauch,Ram Kumar Muthusamy,Hao Li,Iddy Chazua,Benedcto Adamu,Innocent Maholi,Alexander Zipf*

Main category: cs.CV

TL;DR: 该论文提出了一种结合无人机和街景图像的机器学习框架，用于评估与热相关的建筑属性，揭示建筑材料与热暴露健康风险之间的关系。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧了全球南方城市中心的热暴露风险，但缺乏可扩展的方法来评估与热相关的建筑属性。

Method: 使用耦合全局上下文视觉变换器（CGCViT）融合无人机和街景图像，结合HotSat-1的热红外测量数据量化建筑属性与热风险的关系。

Result: 双模态学习方法比单模态模型性能提升9.3%，发现植被覆盖、浅色屋顶以及混凝土/粘土/木材屋顶与较低热红外值显著相关。

Conclusion: 该框架能够识别家庭层面的热暴露不平等问题，为制定公平的气候适应策略提供数据驱动的风险评估方法。

Abstract: Climate change is intensifying human heat exposure, particularly in densely built urban centers of the Global South. Low-cost construction materials and high thermal-mass surfaces further exacerbate this risk. Yet scalable methods for assessing such heat-relevant building attributes remain scarce. We propose a machine learning framework that fuses openly available unmanned aerial vehicle (UAV) and street-view (SV) imagery via a coupled global context vision transformer (CGCViT) to learn heat-relevant representations of urban structures. Thermal infrared (TIR) measurements from HotSat-1 are used to quantify the relationship between building attributes and heat-associated health risks. Our dual-modality cross-view learning approach outperforms the best single-modality models by up to $9.3\%$, demonstrating that UAV and SV imagery provide valuable complementary perspectives on urban structures. The presence of vegetation surrounding buildings (versus no vegetation), brighter roofing (versus darker roofing), and roofing made of concrete, clay, or wood (versus metal or tarpaulin) are all significantly associated with lower HotSat-1 TIR values. Deployed across the city of Dar es Salaam, Tanzania, the proposed framework illustrates how household-level inequalities in heat exposure - often linked to socio-economic disadvantage and reflected in building materials - can be identified and addressed using machine learning. Our results point to the critical role of localized, data-driven risk assessment in shaping climate adaptation strategies that deliver equitable outcomes.

</details>


### [36] [Think-Clip-Sample: Slow-Fast Frame Selection for Video Understanding](https://arxiv.org/abs/2601.11359)
*Wenhui Tan,Ruihua Song,Jiaze Li,Jianzhong Ju,Zhenbo Luo*

Main category: cs.CV

TL;DR: TCS是一个无需训练的长视频理解框架，通过多查询推理和片段级慢快采样技术，显著提升多模态大语言模型在长视频上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在长视频理解方面存在计算限制和帧选择不佳的问题，需要更高效的解决方案。

Method: 提出TCS框架，包含两个核心组件：多查询推理（生成多个查询捕捉问题的不同方面）和片段级慢快采样（自适应平衡局部细节和全局上下文）。

Result: 在MLVU、LongVideoBench和VideoMME数据集上的实验表明，TCS可将准确率提升最高6.9%，同时减少50%推理时间成本。

Conclusion: TCS框架在长视频理解任务上展现出高效性和有效性，能够显著提升现有模型的性能表现。

Abstract: Recent progress in multi-modal large language models (MLLMs) has significantly advanced video understanding. However, their performance on long-form videos remains limited by computational constraints and suboptimal frame selection. We present Think-Clip-Sample (TCS), a training-free framework that enhances long video understanding through two key components: (i) Multi-Query Reasoning, which generates multiple queries to capture complementary aspects of the question and video; and (ii) Clip-level Slow-Fast Sampling, which adaptively balances dense local details and sparse global context. Extensive experiments on MLVU, LongVideoBench, and VideoMME demonstrate that TCS consistently improves performance across different MLLMs, boosting up to 6.9% accuracy, and is capable of achieving comparable accuracy with 50% fewer inference time cost, highlighting both efficiency and efficacy of TCS on long video understanding.

</details>


### [37] [Heterogeneous Uncertainty-Guided Composed Image Retrieval with Fine-Grained Probabilistic Learning](https://arxiv.org/abs/2601.11393)
*Haomiao Tang,Jinpeng Wang,Minyi Zhao,Guanghao Meng,Ruisheng Luo,Long Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种异构不确定性引导（HUG）范式来解决组合图像检索中的内在噪声问题，通过细粒度概率学习框架和不确定性估计机制提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 组合图像检索（CIR）中存在的内在噪声会导致内在不确定性，威胁模型鲁棒性。现有概率学习方法在CIR中效果有限，因为它们采用实例级整体建模且对查询和目标进行同质处理。

Method: HUG采用细粒度概率学习框架，使用高斯嵌入表示查询和目标以捕获详细概念和不确定性。为多模态查询和单模态目标定制异构不确定性估计，包括单模态内容质量不确定性和多模态协调不确定性，并通过可证明的动态加权机制获得综合查询不确定性。设计了不确定性引导目标，包括查询-目标整体对比和细粒度对比，结合全面的负采样策略。

Result: 在基准测试上的实验表明，HUG超越了最先进的基线方法，有效性得到验证。

Conclusion: HUG范式通过异构不确定性估计和细粒度概率学习，有效解决了CIR中的内在噪声问题，提升了检索模型的鲁棒性和性能。

Abstract: Composed Image Retrieval (CIR) enables image search by combining a reference image with modification text. Intrinsic noise in CIR triplets incurs intrinsic uncertainty and threatens the model's robustness. Probabilistic learning approaches have shown promise in addressing such issues; however, they fall short for CIR due to their instance-level holistic modeling and homogeneous treatment of queries and targets. This paper introduces a Heterogeneous Uncertainty-Guided (HUG) paradigm to overcome these limitations. HUG utilizes a fine-grained probabilistic learning framework, where queries and targets are represented by Gaussian embeddings that capture detailed concepts and uncertainties. We customize heterogeneous uncertainty estimations for multi-modal queries and uni-modal targets. Given a query, we capture uncertainties not only regarding uni-modal content quality but also multi-modal coordination, followed by a provable dynamic weighting mechanism to derive comprehensive query uncertainty. We further design uncertainty-guided objectives, including query-target holistic contrast and fine-grained contrasts with comprehensive negative sampling strategies, which effectively enhance discriminative learning. Experiments on benchmarks demonstrate HUG's effectiveness beyond state-of-the-art baselines, with faithful analysis justifying the technical contributions.

</details>


### [38] [SUG-Occ: An Explicit Semantics and Uncertainty Guided Sparse Learning Framework for Real-Time 3D Occupancy Prediction](https://arxiv.org/abs/2601.11396)
*Hanlin Wu,Pengfei Lin,Ehsan Javanmardi,Nanren Bao,Bo Qian,Hao Si,Manabu Tsukada*

Main category: cs.CV

TL;DR: SUG-Occ是一个基于语义和不确定性引导的稀疏学习3D占据预测框架，通过利用3D场景的固有稀疏性来减少冗余计算，同时保持几何和语义完整性。


<details>
  <summary>Details</summary>
Motivation: 3D语义占据预测在自动驾驶中至关重要，但传统方法存在计算和内存开销过大的问题，阻碍了实时部署。

Method: 1. 利用语义和不确定性先验抑制自由空间投影，使用无符号距离编码增强几何一致性；2. 设计级联稀疏补全模块实现粗到细的推理；3. 基于OCR的掩码解码器通过轻量级查询-上下文交互优化体素预测。

Result: 在SemanticKITTI基准测试中，该方法相比基线在准确率上提升7.34%，效率提升57.8%。

Conclusion: SUG-Occ框架有效解决了3D语义占据预测的计算效率问题，在保持高精度的同时显著提升了推理速度。

Abstract: As autonomous driving moves toward full scene understanding, 3D semantic occupancy prediction has emerged as a crucial perception task, offering voxel-level semantics beyond traditional detection and segmentation paradigms. However, such a refined representation for scene understanding incurs prohibitive computation and memory overhead, posing a major barrier to practical real-time deployment. To address this, we propose SUG-Occ, an explicit Semantics and Uncertainty Guided Sparse Learning Enabled 3D Occupancy Prediction Framework, which exploits the inherent sparsity of 3D scenes to reduce redundant computation while maintaining geometric and semantic completeness. Specifically, we first utilize semantic and uncertainty priors to suppress projections from free space during view transformation while employing an explicit unsigned distance encoding to enhance geometric consistency, producing a structurally consistent sparse 3D representation. Secondly, we design an cascade sparse completion module via hyper cross sparse convolution and generative upsampling to enable efficiently coarse-to-fine reasoning. Finally, we devise an object contextual representation (OCR) based mask decoder that aggregates global semantic context from sparse features and refines voxel-wise predictions via lightweight query-context interactions, avoiding expensive attention operations over volumetric features. Extensive experiments on SemanticKITTI benchmark demonstrate that the proposed approach outperforms the baselines, achieving a 7.34/% improvement in accuracy and a 57.8\% gain in efficiency.

</details>


### [39] [Wetland mapping from sparse annotations with satellite image time series and temporal-aware segment anything model](https://arxiv.org/abs/2601.11400)
*Shuai Yuan,Tianwu Lin,Shuang Chen,Yu Xia,Peng Qin,Xiangyu Liu,Xiaoqing Xu,Nan Xu,Hongsheng Zhang,Jie Wang,Peng Gong*

Main category: cs.CV

TL;DR: WetSAM是一个基于SAM的湿地制图框架，通过整合卫星时间序列图像和稀疏点监督，采用双分支设计解决湿地动态变化带来的分割难题，在8个全球区域实现85.58%的平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统湿地制图需要密集像素级标注成本高昂，而稀疏点标签下深度学习模型性能差；湿地强烈的季节性和年际动态变化使得单日期图像不足，导致显著制图误差；现有基础模型如SAM虽然能从点提示泛化，但专为静态图像设计，无法建模时间信息。

Method: 提出WetSAM框架：1）时间提示分支通过分层适配器和动态时间聚合扩展SAM，从物候变化中分离湿地特征；2）空间分支采用时间约束区域生长策略生成可靠密集伪标签；3）双向一致性正则化联合优化两个分支。

Result: 在8个全球区域（每个约5000平方公里）的广泛实验表明，WetSAM显著优于最先进方法，平均F1-score达到85.58%，能以最小标注工作量实现准确且结构一致的湿地分割。

Conclusion: WetSAM展示了强大的泛化能力和可扩展性，为低成本、高分辨率的湿地制图提供了有效解决方案，特别适用于动态湿地环境的精确监测。

Abstract: Accurate wetland mapping is essential for ecosystem monitoring, yet dense pixel-level annotation is prohibitively expensive and practical applications usually rely on sparse point labels, under which existing deep learning models perform poorly, while strong seasonal and inter-annual wetland dynamics further render single-date imagery inadequate and lead to significant mapping errors; although foundation models such as SAM show promising generalization from point prompts, they are inherently designed for static images and fail to model temporal information, resulting in fragmented masks in heterogeneous wetlands. To overcome these limitations, we propose WetSAM, a SAM-based framework that integrates satellite image time series for wetland mapping from sparse point supervision through a dual-branch design, where a temporally prompted branch extends SAM with hierarchical adapters and dynamic temporal aggregation to disentangle wetland characteristics from phenological variability, and a spatial branch employs a temporally constrained region-growing strategy to generate reliable dense pseudo-labels, while a bidirectional consistency regularization jointly optimizes both branches. Extensive experiments across eight global regions of approximately 5,000 km2 each demonstrate that WetSAM substantially outperforms state-of-the-art methods, achieving an average F1-score of 85.58%, and delivering accurate and structurally consistent wetland segmentation with minimal labeling effort, highlighting its strong generalization capability and potential for scalable, low-cost, high-resolution wetland mapping.

</details>


### [40] [SME-YOLO: A Real-Time Detector for Tiny Defect Detection on PCB Surfaces](https://arxiv.org/abs/2601.11402)
*Meng Han*

Main category: cs.CV

TL;DR: SME-YOLO是一个基于YOLOv11n的PCB缺陷检测框架，通过NWDLoss、EUCB上采样和MSFA注意力模块，有效解决了微小缺陷检测的挑战，在PKU-PCB数据集上实现了2.2%的mAP提升和4%的精度提升。


<details>
  <summary>Details</summary>
Motivation: PCB表面缺陷直接影响产品可靠性和安全性，但由于缺陷尺寸微小、纹理相似度高、尺度分布不均匀，传统检测方法难以实现高精度检测。

Method: 1. 使用Normalized Wasserstein Distance Loss替代IoU，降低对微小目标位置偏差的敏感性；2. 用Efficient Upsampling Convolution Block替换原始上采样模块，通过多尺度卷积恢复空间分辨率；3. 提出Multi-Scale Focused Attention模块，自适应增强关键尺度区间的感知能力。

Result: 在PKU-PCB数据集上，SME-YOLO相比基线YOLOv11n，mAP提升2.2%，精度提升4%，达到state-of-the-art性能。

Conclusion: SME-YOLO通过创新的损失函数、上采样模块和注意力机制，有效解决了PCB微小缺陷检测的挑战，为高精度工业缺陷检测提供了有效解决方案。

Abstract: Surface defects on Printed Circuit Boards (PCBs) directly compromise product reliability and safety. However, achieving high-precision detection is challenging because PCB defects are typically characterized by tiny sizes, high texture similarity, and uneven scale distributions. To address these challenges, this paper proposes a novel framework based on YOLOv11n, named SME-YOLO (Small-target Multi-scale Enhanced YOLO). First, we employ the Normalized Wasserstein Distance Loss (NWDLoss). This metric effectively mitigates the sensitivity of Intersection over Union (IoU) to positional deviations in tiny objects. Second, the original upsampling module is replaced by the Efficient Upsampling Convolution Block (EUCB). By utilizing multi-scale convolutions, the EUCB gradually recovers spatial resolution and enhances the preservation of edge and texture details for tiny defects. Finally, this paper proposes the Multi-Scale Focused Attention (MSFA) module. Tailored to the specific spatial distribution of PCB defects, this module adaptively strengthens perception within key scale intervals, achieving efficient fusion of local fine-grained features and global context information. Experimental results on the PKU-PCB dataset demonstrate that SME-YOLO achieves state-of-the-art performance. Specifically, compared to the baseline YOLOv11n, SME-YOLO improves mAP by 2.2% and Precision by 4%, validating the effectiveness of the proposed method.

</details>


### [41] [Topology-Guaranteed Image Segmentation: Enforcing Connectivity, Genus, and Width Constraints](https://arxiv.org/abs/2601.11409)
*Wenxiao Li,Xue-Cheng Tai,Jun Liu*

Main category: cs.CV

TL;DR: 本文提出了一种新的数学框架，将宽度信息显式整合到拓扑结构表征中，解决了传统拓扑方法缺乏维度宽度信息的问题，从而在图像分割中更好地保持连通性和亏格等拓扑特征。


<details>
  <summary>Details</summary>
Motivation: 现有研究强调拓扑先验在图像分割中的关键作用，但传统拓扑结构定义缺乏宽度信息（如厚度和长度），限制了持久同调等方法在实际分割需求中的应用。

Method: 结合持久同调和偏微分方程中的平滑概念，修改上层集的局部极值，使拓扑结构能够捕获宽度属性。将增强的拓扑描述整合到变分图像分割模型中，并通过适当的损失函数设计神经网络。

Result: 通过数值实验验证了方法的有效性，能够成功保持拓扑不变性（如连通性和亏格计数），同时确保分割结构保留关键的宽度属性（如线厚度和长度）。

Conclusion: 所提出的框架能够有效解决拓扑结构缺乏宽度信息的问题，在图像分割中实现了拓扑保真度和宽度特征的统一保持。

Abstract: Existing research highlights the crucial role of topological priors in image segmentation, particularly in preserving essential structures such as connectivity and genus. Accurately capturing these topological features often requires incorporating width-related information, including the thickness and length inherent to the image structures. However, traditional mathematical definitions of topological structures lack this dimensional width information, limiting methods like persistent homology from fully addressing practical segmentation needs. To overcome this limitation, we propose a novel mathematical framework that explicitly integrates width information into the characterization of topological structures. This method leverages persistent homology, complemented by smoothing concepts from partial differential equations (PDEs), to modify local extrema of upper-level sets. This approach enables the resulting topological structures to inherently capture width properties. We incorporate this enhanced topological description into variational image segmentation models. Using some proper loss functions, we are also able to design neural networks that can segment images with the required topological and width properties. Through variational constraints on the relevant topological energies, our approach successfully preserves essential topological invariants such as connectivity and genus counts, simultaneously ensuring that segmented structures retain critical width attributes, including line thickness and length. Numerical experiments demonstrate the effectiveness of our method, showcasing its capability to maintain topological fidelity while explicitly embedding width characteristics into segmented image structures.

</details>


### [42] [PubMed-OCR: PMC Open Access OCR Annotations](https://arxiv.org/abs/2601.11425)
*Hunter Heidenreich,Yosheb Getachew,Olivia Dinica,Ben Elliott*

Main category: cs.CV

TL;DR: PubMed-OCR是一个基于PubMed Central开放获取PDF的科学文献OCR语料库，包含209.5K篇文章（150万页，约13亿词），提供单词、行和段落级别的边界框标注。


<details>
  <summary>Details</summary>
Motivation: 构建一个支持布局感知建模、坐标定位问答和OCR相关流程评估的大规模科学文献语料库。

Method: 使用Google Cloud Vision对PubMed Central开放获取PDF的页面图像进行OCR标注，采用紧凑的JSON格式发布数据，包含多层级边界框信息。

Result: 创建了一个包含209.5K篇文章的大规模语料库，分析了期刊覆盖范围和检测到的布局特征等语料特性。

Conclusion: 发布了数据和模式以促进下游研究，承认了依赖单一OCR引擎和启发式行重构等局限性，并欢迎扩展。

Abstract: PubMed-OCR is an OCR-centric corpus of scientific articles derived from PubMed Central Open Access PDFs. Each page image is annotated with Google Cloud Vision and released in a compact JSON schema with word-, line-, and paragraph-level bounding boxes. The corpus spans 209.5K articles (1.5M pages; ~1.3B words) and supports layout-aware modeling, coordinate-grounded QA, and evaluation of OCR-dependent pipelines. We analyze corpus characteristics (e.g., journal coverage and detected layout features) and discuss limitations, including reliance on a single OCR engine and heuristic line reconstruction. We release the data and schema to facilitate downstream research and invite extensions.

</details>


### [43] [Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps](https://arxiv.org/abs/2601.11442)
*Xiangjun Gao,Zhensong Zhang,Dave Zhenyu Chen,Songcen Xu,Long Quan,Eduardo Pérez-Pellitero,Youngkyoon Jang*

Main category: cs.CV

TL;DR: Map2Thought是一个用于3D视觉语言模型的空间推理框架，通过Metric-CogMap和Cog-CoT实现可解释的几何推理，在减少监督数据的情况下达到与全监督相当的性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D视觉语言模型中空间推理不透明和难以解释的问题，提供显式和可解释的空间推理能力。

Method: 提出Metric-CogMap（结合离散网格和连续度量表示）和Cog-CoT（基于确定性几何操作的可解释推理链），通过向量运算、边界框距离和遮挡感知等操作进行几何推理。

Result: 仅使用一半监督数据达到59.9%准确率，接近全监督基线60.9%；在10%、25%、50%训练子集上分别比现有最佳方法高出5.3%、4.8%、4.0%。

Conclusion: Map2Thought框架能够实现可解释的3D理解，在减少监督数据的情况下保持高性能，为3D视觉语言模型提供了有效的空间推理解决方案。

Abstract: We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations, including vector operations, bounding-box distances, and occlusion-aware appearance order cues, producing interpretable inference traces grounded in 3D structure. Experimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9% accuracy using only half the supervision, closely matching the 60.9% baseline trained with the full dataset. It consistently outperforms state-of-the-art methods by 5.3%, 4.8%, and 4.0% under 10%, 25%, and 50% training subsets, respectively, on the VSI-Bench.

</details>


### [44] [PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs](https://arxiv.org/abs/2601.11451)
*Oishee Bintey Hoque,Nibir Chandra Mandal,Kyle Luong,Amanda Wilson,Samarth Swarup,Madhav Marathe,Abhijin Adiga*

Main category: cs.CV

TL;DR: 提出了一种基于基础设施优先、可解释的流水线方法，用于从航空和卫星图像中识别和表征集中动物饲养操作（CAFOs），通过检测基础设施、提取结构化描述符和输出预测结果，在性能上优于现有基线方法15%。


<details>
  <summary>Details</summary>
Motivation: 大规模畜牧作业对人类健康和环境构成重大风险，且易受传染病和极端天气威胁。随着此类操作数量增长，准确且可扩展的映射变得日益重要。

Method: 采用三阶段流水线：(1) 使用领域调优的YOLOv8检测器检测候选基础设施，并利用SAM2生成掩码；(2) 提取结构化描述符并与深度视觉特征融合；(3) 输出CAFO类型预测和掩码级归因。

Result: 该方法在综合评估中达到最先进性能，Swin-B+PRISM-CAFO比最佳基线性能提升高达15%，在不同美国地区均表现优异。

Conclusion: 提出的基础设施优先方法在CAFO识别方面表现出色，通过系统梯度激活分析量化了领域先验的影响，展示了方法的有效性和可解释性。

Abstract: Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (1) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria, (2) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier, and (3) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show ho

</details>


### [45] [MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models](https://arxiv.org/abs/2601.11464)
*Xiaoran Fan,Zhichao Sun,Tao Ji,Lixing Shen,Tao Gui*

Main category: cs.CV

TL;DR: MHA2MLA-VLM是一个参数高效的多模态感知框架，可将现有多头注意力视觉语言模型转换为多头潜在注意力架构，显著减少KV缓存并加速推理。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型处理日益复杂的多模态任务，KV缓存快速增长导致内存和计算瓶颈，而现有方法缺乏将VLMs高效适配到MLA架构的解决方案。

Method: 采用两种核心技术：模态自适应部分RoPE策略（选择性屏蔽非必要维度）和模态解耦低秩近似方法（独立压缩视觉和文本KV空间），结合参数高效微调最小化输出激活误差。

Result: 在三个代表性VLM上的实验表明，该方法能恢复原始模型性能，显著减少KV缓存占用，并与KV量化无缝集成。

Conclusion: MHA2MLA-VLM提供了一种低成本、高性能的VLM转换方案，有效解决了KV缓存带来的内存和计算瓶颈问题。

Abstract: As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.

</details>


### [46] [Generative Scenario Rollouts for End-to-End Autonomous Driving](https://arxiv.org/abs/2601.11475)
*Rajeev Yasarla,Deepti Hegde,Shizhong Han,Hsin-Pai Cheng,Yunxiao Shi,Meysam Sadeghigooghari,Shweta Mahajan,Apratim Bhattacharyya,Litian Liu,Risheek Garrepalli,Thomas Svantesson,Fatih Porikli,Hong Cai*

Main category: cs.CV

TL;DR: GeRo是一个用于视觉-语言-动作模型的即插即用框架，通过自回归滚动生成语言基础化的未来交通场景，实现规划和场景生成联合进行，显著提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型主要依赖稀疏轨迹标注的模仿学习，未充分发挥其作为生成模型的潜力，需要开发能够进行语言基础化推理的生成式方法。

Method: 首先训练VLA模型将自车和智能体动态编码为潜在标记，然后在多视图图像、场景描述和自车动作问题条件下，进行语言条件化的自回归生成，通过滚动一致性损失稳定预测。

Result: 在Bench2Drive基准上，GeRo将驾驶分数和成功率分别提升了15.7和26.2个百分点，实现了最先进的闭环和开环性能。

Conclusion: 生成式、语言条件化的推理为更安全、可解释的端到端自动驾驶提供了有前景的基础。

Abstract: Vision-Language-Action (VLA) models are emerging as highly effective planning models for end-to-end autonomous driving systems. However, current works mostly rely on imitation learning from sparse trajectory annotations and under-utilize their potential as generative models. We propose Generative Scenario Rollouts (GeRo), a plug-and-play framework for VLA models that jointly performs planning and generation of language-grounded future traffic scenes through an autoregressive rollout strategy. First, a VLA model is trained to encode ego vehicle and agent dynamics into latent tokens under supervision from planning, motion, and language tasks, facilitating text-aligned generation. Next, GeRo performs language-conditioned autoregressive generation. Given multi-view images, a scenario description, and ego-action questions, it generates future latent tokens and textual responses to guide long-horizon rollouts. A rollout-consistency loss stabilizes predictions using ground truth or pseudo-labels, mitigating drift and preserving text-action alignment. This design enables GeRo to perform temporally consistent, language-grounded rollouts that support long-horizon reasoning and multi-agent planning. On Bench2Drive, GeRo improves driving score and success rate by +15.7 and +26.2, respectively. By integrating reinforcement learning with generative rollouts, GeRo achieves state-of-the-art closed-loop and open-loop performance, demonstrating strong zero-shot robustness. These results highlight the promise of generative, language-conditioned reasoning as a foundation for safer and more interpretable end-to-end autonomous driving.

</details>


### [47] [ReScene4D: Temporally Consistent Semantic Instance Segmentation of Evolving Indoor 3D Scenes](https://arxiv.org/abs/2601.11508)
*Emily Steiner,Jianhao Zheng,Henry Howard-Jenkins,Chris Xie,Iro Armeni*

Main category: cs.CV

TL;DR: 本文提出了ReScene4D方法，用于解决室内环境中稀疏时间采样的4D语义实例分割任务，能够在物体移动、出现或消失的情况下保持时间一致性识别。


<details>
  <summary>Details</summary>
Motivation: 室内环境是动态变化的，但现有3D语义实例分割方法缺乏时间推理能力，而4D LiDAR方法依赖高频时间测量，不适用于长期演化的室内场景。需要一种能够在稀疏时间观测下保持实例身份一致性的方法。

Method: 提出了ReScene4D方法，通过调整3DSIS架构来适应4DSIS任务，不需要密集观测。该方法探索了跨观测信息共享策略，利用共享上下文不仅实现一致的实例跟踪，还提高了标准3DSIS的质量。

Result: 在3RScan数据集上达到了最先进的性能，建立了理解演化室内场景的新基准。定义了新的评估指标t-mAP，扩展了mAP以奖励时间身份一致性。

Conclusion: ReScene4D成功解决了稀疏时间4D室内语义实例分割的挑战，证明了共享上下文信息对提高时间一致性和分割质量的重要性，为动态室内场景理解提供了有效解决方案。

Abstract: Indoor environments evolve as objects move, appear, or disappear. Capturing these dynamics requires maintaining temporally consistent instance identities across intermittently captured 3D scans, even when changes are unobserved. We introduce and formalize the task of temporally sparse 4D indoor semantic instance segmentation (SIS), which jointly segments, identifies, and temporally associates object instances. This setting poses a challenge for existing 3DSIS methods, which require a discrete matching step due to their lack of temporal reasoning, and for 4D LiDAR approaches, which perform poorly due to their reliance on high-frequency temporal measurements that are uncommon in the longer-horizon evolution of indoor environments. We propose ReScene4D, a novel method that adapts 3DSIS architectures for 4DSIS without needing dense observations. It explores strategies to share information across observations, demonstrating that this shared context not only enables consistent instance tracking but also improves standard 3DSIS quality. To evaluate this task, we define a new metric, t-mAP, that extends mAP to reward temporal identity consistency. ReScene4D achieves state-of-the-art performance on the 3RScan dataset, establishing a new benchmark for understanding evolving indoor scenes.

</details>


### [48] [ShapeR: Robust Conditional 3D Shape Generation from Casual Captures](https://arxiv.org/abs/2601.11514)
*Yawar Siddiqui,Duncan Frost,Samir Aroudj,Armen Avetisyan,Henry Howard-Jenkins,Daniel DeTone,Pierre Moulon,Qirui Wu,Zhengqin Li,Julian Straub,Richard Newcombe,Jakob Engel*

Main category: cs.CV

TL;DR: ShapeR是一种从随意拍摄的图像序列中生成3D物体形状的新方法，通过结合视觉-惯性SLAM、3D检测和视觉语言模型来提取稀疏点云、多视角图像和文本描述，然后使用整流流变压器生成高保真度的度量3D形状。


<details>
  <summary>Details</summary>
Motivation: 现有3D形状生成方法依赖干净、无遮挡且分割良好的输入，而真实世界场景很少满足这些条件。需要开发能够处理随意拍摄数据的鲁棒方法。

Method: 利用现成的视觉-惯性SLAM、3D检测算法和视觉语言模型提取物体特征，训练整流流变压器来融合多模态信息生成3D形状。采用组合增强、课程训练等技术提高鲁棒性。

Result: 在包含178个真实世界物体的新评估基准上，ShapeR显著优于现有方法，在Chamfer距离指标上比现有最佳方法提升了2.7倍。

Conclusion: ShapeR证明了从随意拍摄序列生成高质量3D形状的可行性，为真实世界应用提供了有效解决方案。

Abstract: Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.

</details>


### [49] [UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation](https://arxiv.org/abs/2601.11522)
*Ruiheng Zhang,Jingfeng Yao,Huangxuan Zhao,Hao Yan,Xiao He,Lei Chen,Zhou Wei,Yong Luo,Zengmao Wang,Lefei Zhang,Dacheng Tao,Bo Du*

Main category: cs.CV

TL;DR: UniX是一个统一的医学基础模型，通过解耦理解和生成任务，使用自回归分支进行理解，扩散分支进行生成，并通过跨模态自注意力机制实现任务协同，在参数减少75%的情况下显著提升了理解和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的医学基础模型难以同时实现视觉理解和生成，因为这两个任务存在目标冲突：语义抽象与像素级重建。参数共享的自回归架构通常导致一个或两个任务性能下降。

Method: UniX将理解和生成任务解耦为两个分支：自回归分支负责理解，扩散分支负责高保真生成。引入跨模态自注意力机制，用理解特征动态指导生成过程。结合严格的数据清洗流程和多阶段训练策略。

Result: 在两个代表性基准测试中，UniX在理解性能（Micro-F1）上提升了46.1%，在生成质量（FD-RadDino）上提升了24.2%，且参数仅为LLM-CXR的四分之一。

Conclusion: UniX实现了与任务专用模型相当的性能，为协同医学图像理解和生成建立了可扩展的范式。

Abstract: Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [50] [Verified Design of Robotic Autonomous Systems using Probabilistic Model Checking](https://arxiv.org/abs/2601.10720)
*Atef Azaiez,Alireza David Anisi*

Main category: cs.RO

TL;DR: 本文提出了一种基于概率模型检验（PMC）的方法论，用于系统性地评估和分析机器人自主系统（RAS）的设计概念，旨在在概念研究阶段早期考虑安全性和可靠性，最终生成经过验证的设计（VD）。


<details>
  <summary>Details</summary>
Motivation: 机器人自主系统（RAS）的复杂性和不确定性环境使得系统设计概念选择成为难题，传统方法从临时开发到基于模型的系统工程技术不一，缺乏系统性的验证手段。

Method: 采用概率模型检验（PMC）作为形式化方法，使用PRISM作为概率模型检查工具，结合领域特定的设计评估标准，对农业机器人用例进行系统设计概念的分析和验证。

Result: 通过农业机器人用例的应用，展示了该方法能够系统性地评估设计概念，并生成经过验证的设计集合，为RAS的安全性和可靠性提供早期保障。

Conclusion: 提出的PMC方法论能够有效支持RAS设计概念的系统性评估和验证，为复杂系统的安全可靠设计提供了可行的解决方案，尤其在农业机器人等动态不确定环境中具有应用价值。

Abstract: Safety and reliability play a crucial role when designing Robotic Autonomous Systems (RAS). Early consideration of hazards, risks and mitigation actions -- already in the concept study phase -- are important steps in building a solid foundations for the subsequent steps in the system engineering life cycle. The complex nature of RAS, as well as the uncertain and dynamic environments the robots operate within, do not merely effect fault management and operation robustness, but also makes the task of system design concept selection, a hard problem to address. Approaches to tackle the mentioned challenges and their implications on system design, range from ad-hoc concept development and design practices, to systematic, statistical and analytical techniques of Model Based Systems Engineering. In this paper, we propose a methodology to apply a formal method, namely Probabilistic Model Checking (PMC), to enable systematic evaluation and analysis of a given set of system design concepts, ultimately leading to a set of Verified Designs (VD). We illustrate the application of the suggested methodology -- using PRISM as probabilistic model checker -- to a practical RAS concept selection use-case from agriculture robotics. Along the way, we also develop and present a domain-specific Design Evaluation Criteria for agri-RAS.

</details>


### [51] [Collaborative Continuum Robots: A Survey](https://arxiv.org/abs/2601.10721)
*Xinyu Li,Qian Tang,Guoxin Yin,Gang Zheng,Jessica Burgner-Kahrs,Cesare Stefanini,Ke Wu*

Main category: cs.RO

TL;DR: 本文综述了协作连续体机器人（CCRs）的研究进展，包括三种协作模式的分类、结构设计、建模、运动规划与控制等方面的总结，并讨论了当前挑战和未来机遇。


<details>
  <summary>Details</summary>
Motivation: 连续体机器人（CRs）因其紧凑结构、固有柔顺性和灵活变形能力而广泛应用。通过协调多个CRs形成协作连续体机器人（CCRs），可以进一步提高任务适应性、工作空间、灵活性、负载能力和操作稳定性，具有显著优势。近年来该领域研究兴趣持续增长。

Method: 从不同系统架构层面提供全面综述，首先将CCRs分类为分离协作、辅助协作和并行协作三种模式并给出定义，然后系统总结每种模式在结构设计、建模、运动规划和控制方面的进展。

Result: 建立了清晰的研究框架，系统梳理了CCRs各协作模式的技术发展现状，为后续研究提供了理论基础和方法指导。

Conclusion: 讨论了CCRs当前面临的挑战和未来发展方向，为该新兴领域的进一步发展指明了路径。

Abstract: Continuum robots (CRs), owing to their compact structure, inherent compliance, and flexible deformation, have been widely applied in various fields. By coordinating multiple CRs to form collaborative continuum robots (CCRs), task adaptability, workspace, flexibility, load capacity, and operational stability can be further improved, thus offering significant advantages. In recent years, interest in this emerging field has grown steadily within the continuum-robotics community, accompanied by a consistent rise in related publications. By presenting a comprehensive overview of recent progress from different system-architecture levels, this survey provides a clear framework for research on CCRs. First, CCRs are classified into the three collaboration modes of separated collaboration, assistance collaboration, and parallel collaboration, with definitions provided. Next, advances in structural design, modeling, motion planning, and control for each mode are systematically summarized. Finally, current challenges and future opportunities for CCRs are discussed.

</details>


### [52] [A Survey of Real-Time Support, Analysis, and Advancements in ROS 2](https://arxiv.org/abs/2601.10722)
*Daniel Casini,Jian-Jia Chen,Jing Li,Federico Reghenzani,Harun Teper*

Main category: cs.RO

TL;DR: 本文对ROS~2在实时系统方面的研究进展进行了全面综述，包括其内部调度机制、通信架构、时序分析方法和社区增强功能，旨在帮助研究人员和从业者理解和改进ROS~2的实时能力。


<details>
  <summary>Details</summary>
Motivation: ROS~2作为机器人应用的重要中间件框架，近年来在实时系统领域受到广泛关注。本文旨在系统总结和分析相关研究成果，为实时ROS~2的发展提供指导。

Method: 采用文献综述方法，详细分析ROS~2的内部调度机制和分层架构，综述单线程和多线程执行器的时序分析、通信模式、社区驱动的运行时增强等技术。

Result: 建立了基于不同标准的分类法，系统化整理了相关研究成果，涵盖了执行器算法设计、实时GPU管理、微控制器支持等关键领域。

Conclusion: 本调查为研究人员和从业者提供了理解ROS~2实时能力的系统性框架，并指出了未来改进的方向。

Abstract: The Robot Operating System 2 (ROS~2) has emerged as a relevant middleware framework for robotic applications, offering modularity, distributed execution, and communication. In the last six years, ROS~2 has drawn increasing attention from the real-time systems community and industry. This survey presents a comprehensive overview of research efforts that analyze, enhance, and extend ROS~2 to support real-time execution. We first provide a detailed description of the internal scheduling mechanisms of ROS~2 and its layered architecture, including the interaction with DDS-based communication and other communication middleware. We then review key contributions from the literature, covering timing analysis for both single- and multi-threaded executors, metrics such as response time, reaction time, and data age, and different communication modes. The survey also discusses community-driven enhancements to the ROS~2 runtime, including new executor algorithm designs, real-time GPU management, and microcontroller support via micro-ROS. Furthermore, we summarize techniques for bounding DDS communication delays, message filters, and profiling tools that have been developed to support analysis and experimentation. To help systematize this growing body of work, we introduce taxonomies that classify the surveyed contributions based on different criteria. This survey aims to guide both researchers and practitioners in understanding and improving the real-time capabilities of ROS~2.

</details>


### [53] [Energy-Efficient Omnidirectional Locomotion for Wheeled Quadrupeds via Predictive Energy-Aware Nominal Gait Selection](https://arxiv.org/abs/2601.10723)
*Xu Yang,Wei Yang,Kaibo He,Bo Yang,Yanan Sui,Yilin Mo*

Main category: cs.RO

TL;DR: 提出了一种用于轮腿式机器人的分层控制框架，通过预测功率建模和残差强化学习优化全向运动能效，相比固定步态方法能耗降低35%


<details>
  <summary>Details</summary>
Motivation: 轮腿式机器人结合了轮子的效率和腿的灵活性，但在多样化环境中面临显著的能耗优化挑战

Method: 采用分层控制框架：1）新颖的功率预测网络预测1秒时间范围内的能耗，智能选择最节能的标称步态；2）强化学习策略生成残差调整，精细调节机器人动作以平衡能效和性能目标

Result: 比较分析显示该方法相比固定步态方法能耗降低高达35%，同时保持可比较的速度跟踪性能。在修改的Unitree Go1平台上通过仿真和真实实验验证了鲁棒性能

Conclusion: 该框架在轮腿式四足机器人上实现了显著的能量效率提升，即使在外部干扰下也能保持鲁棒性能

Abstract: Wheeled-legged robots combine the efficiency of wheels with the versatility of legs, but face significant energy optimization challenges when navigating diverse environments. In this work, we present a hierarchical control framework that integrates predictive power modeling with residual reinforcement learning to optimize omnidirectional locomotion efficiency for wheeled quadrupedal robots. Our approach employs a novel power prediction network that forecasts energy consumption across different gait patterns over a 1-second horizon, enabling intelligent selection of the most energy-efficient nominal gait. A reinforcement learning policy then generates residual adjustments to this nominal gait, fine-tuning the robot's actions to balance energy efficiency with performance objectives. Comparative analysis shows our method reduces energy consumption by up to 35\% compared to fixed-gait approaches while maintaining comparable velocity tracking performance. We validate our framework through extensive simulations and real-world experiments on a modified Unitree Go1 platform, demonstrating robust performance even under external disturbances. Videos and implementation details are available at \href{https://sites.google.com/view/switching-wpg}{https://sites.google.com/view/switching-wpg}.

</details>


### [54] [Adaptive Sliding Mode Control for Vehicle Platoons with State-Dependent Friction Uncertainty](https://arxiv.org/abs/2601.10724)
*Rishabh Dev Yadav*

Main category: cs.RO

TL;DR: 提出一种新的自适应滑模控制器，用于轮式移动机器人车辆编队，能够处理未知复杂的摩擦力和外部干扰，无需先验知识。


<details>
  <summary>Details</summary>
Motivation: 多机器人编队控制在车辆编队等领域有广泛应用，但现有自适应控制器难以准确建模和识别状态依赖的摩擦阻力，这些力随路面状况、轮胎磨损和车速变化而变化，无法先验界定。

Method: 采用两阶段控制方法：首先基于期望轨迹计算期望速度的动力学控制器，然后生成实现期望运动的动力学模型命令。通过分离机器人的运动学和动力学，简化控制问题。

Result: 该控制器能够有效调节编队速度并维持预定义的机器人间距，即使在存在外部干扰和不确定系统参数的情况下也能保持稳定性。

Conclusion: 所提出的自适应滑模控制器为轮式移动机器人编队提供了一种高效鲁棒的控制方案，特别适用于处理未知复杂摩擦力的挑战。

Abstract: Multi-robot formation control has various applications in domains such as vehicle troops, platoons, payload transportation, and surveillance. Maintaining formation in a vehicle platoon requires designing a suitable control scheme that can tackle external disturbances and uncertain system parameters while maintaining a predefined safe distance between the robots. A crucial challenge in this context is dealing with the unknown/uncertain friction forces between wheels and the ground, which vary with changes in road surface, wear in tires, and speed of the vehicle. Although state-of-the-art adaptive controllers can handle a priori bounded uncertainties, they struggle with accurately modeling and identifying frictional forces, which are often state-dependent and cannot be a priori bounded.
  This thesis proposes a new adaptive sliding mode controller for wheeled mobile robot-based vehicle platoons that can handle the unknown and complex behavior of frictional forces without prior knowledge of their parameters and structures. The controller uses the adaptive sliding mode control techniques to regulate the platoon's speed and maintain a predefined inter-robot distance, even in the presence of external disturbances and uncertain system parameters. This approach involves a two-stage process: first, the kinematic controller calculates the desired velocities based on the desired trajectory; and second, the dynamics model generates the commands to achieve the desired motion. By separating the kinematics and dynamics of the robot, this approach can simplify the control problem and allow for more efficient and robust control of the wheeled mobile robot.

</details>


### [55] [Multi-Agent Formation Navigation Using Diffusion-Based Trajectory Generation](https://arxiv.org/abs/2601.10725)
*Hieu Do Quang,Chien Truong-Quoc,Quoc Van Tran*

Main category: cs.RO

TL;DR: 本文提出了一种基于扩散策略的领导者-跟随者编队控制规划器，用于在复杂环境中生成平滑的运动轨迹，并通过距离约束的编队控制器实现跟随者对领导者的跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决在杂乱环境中多智能体编队控制的问题，特别是当环境包含狭窄空间或训练数据中未出现的障碍物配置时，传统方法可能无法生成可靠的轨迹。

Method: 使用扩散策略生成两个领导者中点作为刚性杆的轨迹，定义平面编队中的期望运动路径；跟随者仅基于局部坐标系中的相对位置，使用距离约束的编队控制器跟踪领导者并形成期望的编队几何形状。

Result: 该方法能够产生平滑的运动和较低的跟踪误差，但在狭窄的无障碍空间或训练数据中未出现的障碍物配置下可能出现失败。仿真结果表明扩散模型在多智能体编队规划中具有可靠性潜力。

Conclusion: 扩散模型在多智能体编队规划中展现出良好的应用前景，能够有效处理复杂环境中的编队控制问题，但需要进一步改进以应对训练数据未覆盖的障碍物配置情况。

Abstract: This paper introduces a diffusion-based planner for leader--follower formation control in cluttered environments. The diffusion policy is used to generate the trajectory of the midpoint of two leaders as a rigid bar in the plane, thereby defining their desired motion paths in a planar formation. While the followers track the leaders and form desired foramtion geometry using a distance-constrained formation controller based only on the relative positions in followers' local coordinates. The proposed approach produces smooth motions and low tracking errors, with most failures occurring in narrow obstacle-free space, or obstacle configurations that are not in the training data set. Simulation results demonstrate the potential of diffusion models for reliable multi-agent formation planning.

</details>


### [56] [Bidirectional Human-Robot Communication for Physical Human-Robot Interaction](https://arxiv.org/abs/2601.10796)
*Junxiang Wang,Cindy Wang,Rana Soltani Zarrin,Zackory Erickson*

Main category: cs.RO

TL;DR: BRIDGE系统通过自然语言实现人机双向实时交互，允许用户修改机器人轨迹参数，并利用LLM解释指令和提供语音反馈，显著提升了交互性和透明度。


<details>
  <summary>Details</summary>
Motivation: 实现有效的物理人机交互需要系统既能适应用户偏好，又能透明地展示其行为，但目前缺乏能够实时双向沟通的解决方案。

Method: 使用大型语言模型(LLM)解释用户指令在规划运动背景下的含义，支持实时修改轨迹的位置、速度和力参数，并提供语音反馈确认变化或澄清问题。

Result: 在18名老年人的用户研究中，参与者成功实时修改轨迹，双向反馈系统显著提高了交互性和透明度评分。

Conclusion: 机器人的语音反馈对于创造更直观的用户体验至关重要，BRIDGE系统证明了双向沟通在物理辅助任务中的有效性。

Abstract: Effective physical human-robot interaction requires systems that are not only adaptable to user preferences but also transparent about their actions. This paper introduces BRIDGE, a system for bidirectional human-robot communication in physical assistance. Our method allows users to modify a robot's planned trajectory -- position, velocity, and force -- in real time using natural language. We utilize a large language model (LLM) to interpret any trajectory modifications implied by user commands in the context of the planned motion and conversation history. Importantly, our system provides verbal feedback in response to the user, either assuring any resulting changes or posing a clarifying question. We evaluated our method in a user study with 18 older adults across three assistive tasks, comparing BRIDGE to an ablation without verbal feedback and a baseline. Results show that participants successfully used the system to modify trajectories in real time. Moreover, the bidirectional feedback led to significantly higher ratings of interactivity and transparency, demonstrating that the robot's verbal response is critical for a more intuitive user experience. Videos and code can be found on our project website: https://bidir-comm.github.io/

</details>


### [57] [SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM](https://arxiv.org/abs/2601.10814)
*Onur Bagoren,Seth Isaacson,Sacchin Sundar,Yung-Ching Sun,Anja Sheppard,Haoyu Ma,Abrar Shariff,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

TL;DR: 本文提出了一种用于水下立体视差估计的sim-to-real训练框架，结合自监督微调，并开发了融合多传感器的水下SLAM系统，在真实沉船数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 水下环境中立体深度估计面临挑战：图像质量受光线衰减、视觉伪影和动态光照影响，缺乏丰富纹理，且缺乏真实水下数据集。现有陆地训练的立体网络无法直接迁移到水下领域。

Method: 1. 使用模拟数据训练水下立体视差估计网络，结合自监督微调实现sim-to-real迁移；2. 开发融合立体相机、IMU、气压计和DVL测量的实时水下SLAM框架；3. 收集包含24,000多对立体图像的真实沉船数据集用于评估。

Result: 实验证明，所提出的训练方法在真实水下数据上显著改善了立体估计性能，能够准确估计复杂沉船遗址的轨迹并进行3D重建。

Conclusion: 该框架有效解决了水下立体深度估计的挑战，为水下机器人感知提供了可靠的定位和建图能力，特别是在缺乏纹理的真实水下环境中表现出色。

Abstract: Localization and mapping are core perceptual capabilities for underwater robots. Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks. However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging. In underwater environments, images are degraded by light attenuation, visual artifacts, and dynamic lighting conditions. Furthermore, real-world underwater scenes frequently lack rich texture useful for stereo depth estimation and 3D reconstruction. As a result, stereo estimation networks trained on in-air data cannot transfer directly to the underwater domain. In addition, there is a lack of real-world underwater stereo datasets for supervised training of neural networks. Poor underwater depth estimation is compounded in stereo-based Simultaneous Localization and Mapping (SLAM) algorithms, making it a fundamental challenge for underwater robot perception. To address these challenges, we propose a novel framework that enables sim-to-real training of underwater stereo disparity estimation networks using simulated data and self-supervised finetuning. We leverage our learned depth predictions to develop \algname, a novel framework for real-time underwater SLAM that fuses stereo cameras with IMU, barometric, and Doppler Velocity Log (DVL) measurements. Lastly, we collect a challenging real-world dataset of shipwreck surveys using an underwater robot. Our dataset features over 24,000 stereo pairs, along with high-quality, dense photogrammetry models and reference trajectories for evaluation. Through extensive experiments, we demonstrate the advantages of the proposed training approach on real-world data for improving stereo estimation in the underwater domain and for enabling accurate trajectory estimation and 3D reconstruction of complex shipwreck sites.

</details>


### [58] [Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets](https://arxiv.org/abs/2601.10827)
*Simin Liu,Tong Zhao,Bernhard Paus Graesdal,Peter Werner,Jiuguang Wang,John Dolan,Changliu Liu,Tao Pang*

Main category: cs.RO

TL;DR: 本文提出了一种新的接触丰富操作（CRM）规划方法，通过离线构建互可达集图和在线规划，实现了全局优化的操作运动，显著提升了任务效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的CRM规划器主要关注可行性而非最优性，限制了CRM优势的充分发挥。人类操作中利用整个操纵器表面接触物体比仅依赖末端执行器更高效自然。

Method: 采用两阶段方法：离线阶段构建互可达集图，每个集合包含从起始物体方向和抓握可达的所有物体方向；在线阶段在该图上规划，计算并排序局部计划以实现全局优化运动。

Result: 在具有挑战性的接触丰富任务中，该方法优于领先规划器，任务成本降低61%，在250次查询中达到91%的成功率，并保持亚分钟级的查询时间。

Conclusion: 全局优化的接触丰富操作现在已适用于现实世界任务，证明了该方法的实用性和高效性。

Abstract: If we consider human manipulation, it is clear that contact-rich manipulation (CRM)-the ability to use any surface of the manipulator to make contact with objects-can be far more efficient and natural than relying solely on end-effectors (i.e., fingertips). However, state-of-the-art model-based planners for CRM are still focused on feasibility rather than optimality, limiting their ability to fully exploit CRM's advantages. We introduce a new paradigm that computes approximately optimal manipulator plans. This approach has two phases. Offline, we construct a graph of mutual reachable sets, where each set contains all object orientations reachable from a starting object orientation and grasp. Online, we plan over this graph, effectively computing and sequencing local plans for globally optimized motion. On a challenging, representative contact-rich task, our approach outperforms a leading planner, reducing task cost by 61%. It also achieves a 91% success rate across 250 queries and maintains sub-minute query times, ultimately demonstrating that globally optimized contact-rich manipulation is now practical for real-world tasks.

</details>


### [59] [IMU-based Real-Time Crutch Gait Phase and Step Detections in Lower-Limb Exoskeletons](https://arxiv.org/abs/2601.10832)
*Anis R. Shakkour,David Hexner,Yehuda Bitton,Avishai Sintov*

Main category: cs.RO

TL;DR: 提出了一种基于单个低成本IMU的步态检测框架，用于下肢外骨骼和假肢的实时控制，无需机械改造，通过深度学习模型和有限状态机实现94%的步态检测成功率。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复杂的力传感硬件，导致控制延迟。需要一种低成本、实时的步态检测方案来确保外骨骼和假肢的同步运动与用户安全。

Method: 在拐杖手柄集成单个IMU，提出五阶段分类系统（包括标准步态阶段和非运动辅助状态）。比较三种深度学习架构，结合有限状态机（FSM）确保生物力学一致性。Temporal Convolutional Network（TCN）被选为最优架构。

Result: TCN模型在PC和嵌入式系统上均表现最佳，实现94%的拐杖步态检测成功率，延迟最低。模型在仅使用健康参与者数据训练的情况下，成功泛化至瘫痪用户。

Conclusion: 该框架提供了一种高性能、低成本的实时外骨骼控制解决方案，无需机械改造，具有优秀的泛化能力。

Abstract: Lower limb exoskeletons and prostheses require precise, real time gait phase and step detections to ensure synchronized motion and user safety. Conventional methods often rely on complex force sensing hardware that introduces control latency. This paper presents a minimalist framework utilizing a single, low cost Inertial-Measurement Unit (IMU) integrated into the crutch hand grip, eliminating the need for mechanical modifications. We propose a five phase classification system, including standard gait phases and a non locomotor auxiliary state, to prevent undesired motion. Three deep learning architectures were benchmarked on both a PC and an embedded system. To improve performance under data constrained conditions, models were augmented with a Finite State Machine (FSM) to enforce biomechanical consistency. The Temporal Convolutional Network (TCN) emerged as the superior architecture, yielding the highest success rates and lowest latency. Notably, the model generalized to a paralyzed user despite being trained exclusively on healthy participants. Achieving a 94% success rate in detecting crutch steps, this system provides a high performance, cost effective solution for real time exoskeleton control.

</details>


### [60] [Is open robotics innovation a threat to international peace and security?](https://arxiv.org/abs/2601.10877)
*Ludovic Righetti,Vincent Boulanin*

Main category: cs.RO

TL;DR: 本文讨论了机器人技术开放获取带来的双重用途风险，并提出了针对机器人领域的负责任研究路线图。


<details>
  <summary>Details</summary>
Motivation: 机器人技术的开放获取虽然有利于科学发展和系统开发，但也加剧了军事和有害用途的双重用途风险，而目前机器人领域缺乏针对性的监管和指导。

Method: 通过分析机器人技术与其他工程领域（如大规模杀伤性武器和人工智能）的双重用途风险对比，提出机器人社区需要制定专门的指导方针。

Result: 提出了包含四个实践方向的路线图：负责任机器人教育、激励风险评估、高风险材料扩散的调节以及红线的制定。

Conclusion: 机器人社区应制定适合自身特点的监管框架，通过教育、风险评估和适度限制来管理双重用途风险。

Abstract: Open access to publication, software and hardware is central to robotics: it lowers barriers to entry, supports reproducible science and accelerates reliable system development. However, openness also exacerbates the inherent dual-use risks associated with research and innovation in robotics. It lowers barriers for states and non-state actors to develop and deploy robotics systems for military use and harmful purposes. Compared to other fields of engineering where dual-use risks are present - e.g., those that underlie the development of weapons of mass destruction (chemical, biological, radiological, and nuclear weapons) and even the field of AI, robotics offers no specific regulation and little guidance as to how research and innovation may be conducted and disseminated responsibly. While other fields can be used for guidance, robotics has its own needs and specificities which have to be taken into account. The robotics community should therefore work toward its own set of sector-specific guidance and possibly regulation. To that end, we propose a roadmap focusing on four practices: a) education in responsible robotics; b) incentivizing risk assessment; c) moderating the diffusion of high-risk material; and d) developing red lines.

</details>


### [61] [Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation](https://arxiv.org/abs/2601.10930)
*Zhixian Xie,Yu Xiang,Michael Posa,Wanxin Jin*

Main category: cs.RO

TL;DR: 本文提出了一种分层强化学习-模型预测控制框架，通过将灵巧操作分解为高层接触意图规划和底层接触动力学控制，解决了接触密集型操作中的几何、运动学约束和复杂接触动力学问题。


<details>
  <summary>Details</summary>
Motivation: 端到端的视觉运动策略需要大量数据，从仿真到现实的迁移效果差，跨任务/体现的泛化能力弱。灵巧操作本质上是分层的，高层决定接触位置和物体运动，底层实现接触动力学控制。

Method: 高层RL策略预测接触意图（接触位置和物体子目标姿态），底层基于接触隐式MPC优化局部接触模式并重新规划接触动力学，生成驱动物体实现子目标的机器人动作。

Result: 在非抓取任务（包括几何泛化推动和物体3D重定向）中，该框架实现了接近100%的成功率，数据需求减少10倍，性能高度鲁棒，并实现零样本仿真到现实迁移。

Conclusion: 分层RL-MPC框架通过分离高层几何/运动学规划和底层接触动力学控制，有效解决了灵巧操作的关键挑战，显著提高了数据效率、鲁棒性和跨域迁移能力。

Abstract: A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.

</details>


### [62] [Crane Lowering Guidance Using a Attachable Camera Module for Driver Vision Support](https://arxiv.org/abs/2601.11026)
*HyoJae Kang,SunWoo Ahn,InGyu Choi,GeonYeong Go,KunWoo Son,Min-Sung Kang*

Main category: cs.RO

TL;DR: 提出一种可安装在起重机吊物上的摄像头系统，用于解决操作员在吊物下降阶段无法看到着陆点的问题，通过实时图像传输提供视觉引导。


<details>
  <summary>Details</summary>
Motivation: 起重机操作中，吊物下降阶段操作员视线被遮挡，传统依赖地面人员口头或手势指令存在安全隐患。

Method: 开发可吸附在吊物上的摄像头模块，包含单板计算机、电池和紧凑摄像头，实时采集和处理地面图像，生成安装引导并传输到主机计算机。

Result: 初步实验证实了实时图像采集和传输的可行性。

Conclusion: 该系统有潜力通过提供隐藏着陆区的即时视觉参考，显著提高施工现场安全性。

Abstract: Cranes have long been essential equipment for lifting and placing heavy loads in construction projects. This study focuses on the lowering phase of crane operation, the stage in which the load is moved to the desired location. During this phase, a constant challenge exists: the load obstructs the operator's view of the landing point. As a result, operators traditionally have to rely on verbal or gestural instructions from ground personnel, which significantly impacts site safety. To alleviate this constraint, the proposed system incorporates a attachable camera module designed to be attached directly to the load via a suction cup. This module houses a single-board computer, battery, and compact camera. After installation, it streams and processes images of the ground directly below the load in real time to generate installation guidance. Simultaneously, this guidance is transmitted to and monitored by a host computer. Preliminary experiments were conducted by attaching this module to a test object, confirming the feasibility of real-time image acquisition and transmission. This approach has the potential to significantly improve safety on construction sites by providing crane operators with an instant visual reference of hidden landing zones.

</details>


### [63] [H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning](https://arxiv.org/abs/2601.11063)
*Haishan Zeng,Peng Li*

Main category: cs.RO

TL;DR: H-AIM是一个新型的多机器人任务规划框架，通过三阶段级联架构解决异构机器人团队执行长期任务的挑战，将LLM的语义推理与经典规划器结合，显著提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 在具身人工智能中，异构机器人团队执行高级指令的长期任务仍面临挑战，LLM在长期推理和动态多机器人协调方面存在局限性。

Method: 提出H-AIM三阶段架构：1）使用LLM解析指令生成PDDL问题描述；2）结合LLM语义推理和经典规划器生成优化行动序列；3）将计划编译为行为树实现反应式控制。通过共享黑板机制支持动态异构机器人团队。

Result: 在MACE-THOR基准数据集（42个复杂任务，8种家庭布局）上实验，H-AIM将任务成功率从12%提升至55%，目标条件召回率从32%提升至72%，显著优于最强基线LaMMA-P。

Conclusion: H-AIM框架通过结合LLM和经典规划器的优势，有效解决了多机器人长期任务规划的挑战，为异构机器人团队协作提供了可行的解决方案。

Abstract: In embodied artificial intelligence, enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions remains a critical challenge. While large language models (LLMs) show promise in instruction parsing and preliminary planning, they exhibit limitations in long-term reasoning and dynamic multi-robot coordination. We propose Hierarchical Autonomous Intelligent Multi-Robot Planning(H-AIM), a novel embodied multi-robot task planning framework that addresses these issues through a three-stage cascaded architecture: 1) It leverages an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, thereby transforming commands into formal planning problems; 2) It combines the semantic reasoning of LLMs with the search capabilities of a classical planner to produce optimized action sequences; 3) It compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization. To validate our approach, we introduce the MACE-THOR benchmark dataset, comprising 42 complex tasks across 8 distinct household layouts. Experimental results demonstrate that H-AIM achieves a remarkable performance improvement, elevating the task success rate from 12% to 55% and boosting the goal condition recall from 32% to 72% against the strongest baseline, LaMMA-P.

</details>


### [64] [A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation](https://arxiv.org/abs/2601.11076)
*Jiaqi Liang,Yue Chen,Qize Yu,Yan Shen,Haipeng Zhang,Hao Dong,Ruihai Wu*

Main category: cs.RO

TL;DR: A3D框架通过点级几何表示学习自适应affordances，识别家具零件的最佳支撑位置，实现双臂协作的家具组装任务。


<details>
  <summary>Details</summary>
Motivation: 家具组装需要精确的双臂协调，其中一只手臂操作零件，另一只提供协作支撑和稳定。机器人需要在整个长周期组装过程中主动调整支撑策略，并泛化到不同的零件几何形状。

Method: 使用密集点级几何表示建模零件交互模式，引入自适应模块根据交互反馈动态调整支撑策略，在包含50个不同零件和8种家具类型的仿真环境中进行双臂协作评估。

Result: 实验表明该框架在仿真和真实环境中都能有效泛化到不同的零件几何形状和家具类别。

Conclusion: A3D框架通过学习自适应affordances和动态调整策略，成功解决了家具组装中的双臂协调和几何泛化问题。

Abstract: Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries. We propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions. We establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.

</details>


### [65] [Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments](https://arxiv.org/abs/2601.11078)
*Jiaohong Yao,Linfeng Liang,Yao Deng,Xi Zheng,Richard Han,Yuankai Qi*

Main category: cs.RO

TL;DR: 本文通过AirSim平台模拟不同城市环境、光照和天气条件，评估了基于标记的无人机自主着陆系统的鲁棒性，比较了两种启发式覆盖模式和一种强化学习智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于标记的无人机着陆方法大多假设理想的着陆点可见性和传感器性能，在复杂城市环境中的鲁棒性有限，需要在实际操作多样性条件下进行系统评估。

Method: 使用AirSim仿真平台创建系统变化的城市布局、光照和天气条件，利用机载摄像头传感器（RGB用于标记检测，深度用于避障），对比评估两种启发式覆盖模式和一种强化学习智能体的性能。

Result: 研究结果表明，探索策略和场景复杂度显著影响着陆成功率、路径效率和系统鲁棒性，强调了在多样化、传感器相关条件下评估标记着陆系统的重要性。

Conclusion: 需要在多样化的传感器相关条件下评估基于标记的自主着陆系统，以指导可靠空中导航系统的开发。

Abstract: Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. We present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity. Using onboard camera sensors (RGB for marker detection and depth for obstacle avoidance), we benchmark two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness. Results underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.

</details>


### [66] [Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model](https://arxiv.org/abs/2601.11143)
*Minho Lee,Hyeonseok Kim,Jin Tak Kim,Sangshin Park,Jeong Hyun Lee,Jungsan Cho,Jemin Hwangbo*

Main category: cs.RO

TL;DR: 本文提出了一种基于液压动力学的解析执行器模型，用于解决大型液压机器人从仿真到现实的迁移挑战，该模型能在1微秒内预测12个执行器的关节扭矩，并在数据有限的情况下优于神经网络模型。


<details>
  <summary>Details</summary>
Motivation: 大型液压机器人的仿真到现实迁移面临控制响应慢和复杂流体动力学的挑战，这些复杂性源于多缸结构和流体速率的差异，使得传统仿真方法不适用于强化学习应用。

Method: 提出了一种基于液压动力学的解析执行器模型，该模型能够快速预测所有12个执行器的关节扭矩（在1微秒内完成），适用于强化学习环境中的快速处理。

Result: 与基于神经网络的执行器模型相比，本文提出的模型在数据有限的情况下表现更优。通过在强化学习中训练的运动策略，成功部署在超过300公斤的重型液压四足机器人上，实现了稳定和鲁棒的命令跟踪运动。

Conclusion: 这是首次在重型液压四足机器人上成功实现稳定和鲁棒的命令跟踪运动的仿真到现实迁移，展示了先进的仿真到现实可迁移性。

Abstract: The simulation-to-reality (sim-to-real) transfer of large-scale hydraulic robots presents a significant challenge in robotics because of the inherent slow control response and complex fluid dynamics. The complex dynamics result from the multiple interconnected cylinder structure and the difference in fluid rates of the cylinders. These characteristics complicate detailed simulation for all joints, making it unsuitable for reinforcement learning (RL) applications. In this work, we propose an analytical actuator model driven by hydraulic dynamics to represent the complicated actuators. The model predicts joint torques for all 12 actuators in under 1 microsecond, allowing rapid processing in RL environments. We compare our model with neural network-based actuator models and demonstrate the advantages of our model in data-limited scenarios. The locomotion policy trained in RL with our model is deployed on a hydraulic quadruped robot, which is over 300 kg. This work is the first demonstration of a successful transfer of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot, demonstrating advanced sim-to-real transferability.

</details>


### [67] [Adaptive Monitoring of Stochastic Fire Front Processes via Information-seeking Predictive Control](https://arxiv.org/abs/2601.11231)
*Savvas Papaioannou,Panayiotis Kolios,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.RO

TL;DR: 本文提出了一种自适应监测野火前沿的方法，通过将监测任务建模为随机最优控制问题，整合了感知、估计和控制，并设计了具有渐近收敛性的信息寻求预测控制律。


<details>
  <summary>Details</summary>
Motivation: 现有方法在监测野火前沿时，通常将感知、估计和控制分开处理，且往往依赖线性高斯假设或启发式方法，缺乏明确的性能保证。野火演变的随机性要求这些环节的无缝集成。

Method: 首先为随机非线性椭圆增长野火前沿模型推导了最优递归贝叶斯估计器，然后将非线性随机控制问题转化为有限时域马尔可夫决策过程，并设计了基于置信下界自适应搜索算法的信息寻求预测控制律。

Result: 所提出的控制律具有渐近收敛到最优策略的性质，为野火前沿监测提供了理论性能保证。

Conclusion: 该方法通过集成感知、估计和控制，解决了野火前沿自适应监测的挑战，提供了具有收敛保证的优化控制策略。

Abstract: We consider the problem of adaptively monitoring a wildfire front using a mobile agent (e.g., a drone), whose trajectory determines where sensor data is collected and thus influences the accuracy of fire propagation estimation. This is a challenging problem, as the stochastic nature of wildfire evolution requires the seamless integration of sensing, estimation, and control, often treated separately in existing methods. State-of-the-art methods either impose linear-Gaussian assumptions to establish optimality or rely on approximations and heuristics, often without providing explicit performance guarantees. To address these limitations, we formulate the fire front monitoring task as a stochastic optimal control problem that integrates sensing, estimation, and control. We derive an optimal recursive Bayesian estimator for a class of stochastic nonlinear elliptical-growth fire front models. Subsequently, we transform the resulting nonlinear stochastic control problem into a finite-horizon Markov decision process and design an information-seeking predictive control law obtained via a lower confidence bound-based adaptive search algorithm with asymptotic convergence to the optimal policy.

</details>


### [68] [VLAgents: A Policy Server for Efficient VLA Inference](https://arxiv.org/abs/2601.11250)
*Tobias Jülg,Khaled Gamal,Nisarga Nilavadi,Pierre Krack,Seongjin Bien,Michael Krawez,Florian Walter,Wolfram Burgard*

Main category: cs.RO

TL;DR: VLAgents是一个模块化的策略服务器，通过统一的Gymnasium风格协议抽象VLA推理，支持零拷贝共享内存和压缩流式传输，在本地和远程通信基准测试中优于现有解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决Vision-Language-Action模型在机器人领域部署复杂、接口碎片化和分布式设置中通信延迟高的问题。

Method: 设计VLAgents架构，集成七种策略（包括OpenVLA和Pi Zero），通过透明适应上下文的通信层支持零拷贝共享内存和压缩流式传输。

Result: 在本地和远程通信基准测试中，VLAgents优于OpenVLA、OpenPi和LeRobot的默认策略服务器。

Conclusion: VLAgents提供了一个有效的解决方案，简化了VLA模型的部署，提高了通信效率，并在性能上超越了现有方法。

Abstract: The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents

</details>


### [69] [Skill-Aware Diffusion for Generalizable Robotic Manipulation](https://arxiv.org/abs/2601.11266)
*Aoshen Huang,Jiaming Chen,Jiyu Cheng,Ran Song,Wei Pan,Wei Zhang*

Main category: cs.RO

TL;DR: 提出了Skill-Aware Diffusion (SADiff)方法，通过显式融入技能级信息来提升机器人操作的泛化能力。该方法学习技能特定表示，并通过技能约束扩散模型生成物体中心运动流，在模拟和真实环境中均表现出良好性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过扩展数据和网络来改进泛化能力，但独立处理任务并忽视了技能级信息。观察到同一技能内的任务共享相似的运动模式，因此需要利用技能级信息来提升泛化能力。

Method: SADiff通过技能感知编码模块学习技能特定表示，使用可学习技能标记，并条件化技能约束扩散模型生成物体中心运动流。还采用技能检索转换策略利用技能特定轨迹先验来优化从2D运动流到可执行3D动作的映射。

Result: 在模拟和真实环境中的实验表明，SADiff在各种操作任务中实现了良好的性能和泛化能力。

Conclusion: SADiff通过显式融入技能级信息有效提升了机器人操作的泛化能力，并引入了IsaacSkill数据集进行综合评估和模拟到真实转移。

Abstract: Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.

</details>


### [70] [Distributed Control Barrier Functions for Safe Multi-Vehicle Navigation in Heterogeneous USV Fleets](https://arxiv.org/abs/2601.11335)
*Tyler Paine,Brendan Long,Jeremy Wenger,Michael DeFilippo,James Usevitch,Michael Benjamin*

Main category: cs.RO

TL;DR: 本文提出了一种分布式安全控制过滤器，使用控制屏障函数理论解决异构无人船队碰撞避免问题，结合CBF方法和COLREGS行为实现最佳安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 异构无人船队碰撞避免具有挑战性，因为不同平台的决策过程和控制器存在差异，且实时共享轨迹和控制值存在限制。

Method: 在每个自主车辆上添加控制过滤器，假设其他接触者（包括有人驾驶船只）采取最坏情况行为，基于控制屏障函数理论开发分布式安全控制过滤器。

Result: 模拟和真实世界实验表明，该方法在不同平台上有效，对人为操作的不合作行为具有鲁棒性。结合CBF方法和COLREGS行为可实现最佳安全性和效率。

Conclusion: 提出的分布式安全控制过滤器方法能够有效解决异构无人船队碰撞避免问题，特别是在存在不合作人为操作的情况下表现良好。

Abstract: Collision avoidance in heterogeneous fleets of uncrewed vessels is challenging because the decision-making processes and controllers often differ between platforms, and it is further complicated by the limitations on sharing trajectories and control values in real-time. This paper presents a pragmatic approach that addresses these issues by adding a control filter on each autonomous vehicle that assumes worst-case behavior from other contacts, including crewed vessels. This distributed safety control filter is developed using control barrier function (CBF) theory and the application is clearly described to ensure explainability of these safety-critical methods. This work compares the worst-case CBF approach with a Collision Regulations (COLREGS) behavior-based approach in simulated encounters. Real-world experiments with three different uncrewed vessels and a human operated vessel were performed to confirm the approach is effective across a range of platforms and is robust to uncooperative behavior from human operators. Results show that combining both CBF methods and COLREGS behaviors achieves the best safety and efficiency.

</details>


### [71] [The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning](https://arxiv.org/abs/2601.11394)
*Henrik Hose,Paul Brunzema,Devdutt Subhasish,Sebastian Trimpe*

Main category: cs.RO

TL;DR: 该论文介绍了Mini Wheelbot的全面动力学数据集，包含高频同步传感器数据、状态估计、运动捕捉真值等，用于支持不稳定系统的学习控制算法研究。


<details>
  <summary>Details</summary>
Motivation: 开发不稳定系统的学习控制算法需要高质量真实数据，但专业机器人硬件访问困难，因此需要提供开放数据集降低研究门槛。

Method: 收集多个硬件实例在不同表面上的实验数据，使用伪随机二进制激励、非线性模型预测控制和强化学习等多种控制范式，提供1kHz同步数据。

Result: 构建了包含所有机载传感器读数、状态估计、运动捕捉真值姿态和第三方视频日志的全面数据集。

Conclusion: 该数据集可用于动力学模型学习、状态估计和时间序列分类等机器人算法的基准测试，为研究社区提供重要资源。

Abstract: The development of robust learning-based control algorithms for unstable systems requires high-quality, real-world data, yet access to specialized robotic hardware remains a significant barrier for many researchers. This paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source, quasi-symmetric balancing reaction wheel unicycle. The dataset provides 1 kHz synchronized data encompassing all onboard sensor readings, state estimates, ground-truth poses from a motion capture system, and third-person video logs. To ensure data diversity, we include experiments across multiple hardware instances and surfaces using various control paradigms, including pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents. We include several example applications in dynamics model learning, state estimation, and time-series classification to illustrate common robotics algorithms that can be benchmarked on our dataset.

</details>


### [72] [ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models](https://arxiv.org/abs/2601.11404)
*Linqing Zhong,Yi Liu,Yifei Wei,Ziyu Xiong,Maoqing Yao,Si Liu,Guanghui Ren*

Main category: cs.RO

TL;DR: 该论文提出了Action Chain-of-Thought (ACoT)范式，通过直接在动作空间进行推理来改进机器人策略学习，并开发了ACoT-VLA架构。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型中的中间推理（如子任务预测或目标图像合成）通常是间接的，无法传递精确动作执行所需的完整信息。

Method: 提出ACoT-VLA架构，包含显式动作推理器(EAR)和隐式动作推理器(IAR)。EAR生成粗粒度参考轨迹作为显式推理步骤，IAR从多模态输入中提取潜在动作先验，共同形成ACoT来指导下游动作头。

Result: 在真实世界和仿真环境中进行大量实验，在LIBERO、LIBERO-Plus和VLABench数据集上分别达到98.5%、84.1%和47.4%的性能。

Conclusion: 直接在动作空间进行推理的ACoT范式能够有效提升机器人策略学习的性能，证明了动作级推理的优越性。

Abstract: Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.

</details>


### [73] [The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents](https://arxiv.org/abs/2601.11421)
*Ziyu Wang,Chenyuan Liu,Yushun Xiang,Runhao Zhang,Qingbo Hao,Hongliang Lu,Houyu Chen,Zhizhong Feng,Kaiyue Zheng,Dehao Ye,Xianchao Zeng,Xinyu Zhou,Boran Wen,Jiaxin Li,Mingyu Zhang,Kecheng Zheng,Qian Zhu,Ran Cheng,Yong-Lu Li*

Main category: cs.RO

TL;DR: GM-100是首个机器人学习奥运会项目，包含100个精心设计的任务，旨在系统评估机器人智能体能力并促进任务设计的多样性。


<details>
  <summary>Details</summary>
Motivation: 当前机器人学习和模仿学习的数据集及任务设计缺乏系统性考虑，无法准确评估不同方法的差异化性能。

Method: 通过系统分析现有任务设计，结合人机交互基元和物体可操作性概念，开发了涵盖广泛交互和长尾行为的100个任务。

Result: 实验结果表明GM-100任务既可行执行又具有足够挑战性，能有效区分当前VLA模型的性能差异。

Conclusion: GM-100为机器人学习提供了全面评估基准，推动了任务设计的多样性和复杂性发展。

Abstract: Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.

</details>


### [74] [Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations](https://arxiv.org/abs/2601.11460)
*Franziska Herbert,Vignesh Prasad,Han Liu,Dorothea Koert,Georgia Chalvatzaki*

Main category: cs.RO

TL;DR: 该论文提出了一种语义-几何任务图表示方法，结合MPNN编码器和Transformer解码器，用于从人类演示中学习结构化任务表示，特别适用于双手机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 从人类演示中学习结构化任务表示对于理解长时程操作行为至关重要，特别是在双手机器人操作中，动作顺序、对象参与和交互几何变化很大。需要一种能同时捕捉任务离散语义结构和对象几何关系时间演化的表示方法。

Method: 提出了语义-几何任务图表示，编码对象身份、对象间关系及其时间几何演化。使用MPNN编码器学习结构化表示，Transformer解码器基于动作上下文预测未来动作序列、相关对象和对象运动。

Result: 在人类演示数据集上的评估表明，语义-几何任务图表示特别适用于动作和对象变化大的任务，比简单序列模型能更好地捕捉任务进展。任务图表示可迁移到物理双手机器人并用于在线动作选择。

Conclusion: 语义-几何任务图表示作为可重用的任务抽象，具有在下游操作系统中进行决策的潜力，为双手机器人操作提供了有效的任务表示和推理框架。

Abstract: Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic-geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic-geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [75] [Analytic Bijections for Smooth and Interpretable Normalizing Flows](https://arxiv.org/abs/2601.10774)
*Mathis Gerdes,Miranda C. N. Cheng*

Main category: cs.LG

TL;DR: 本文提出了三种全局平滑、定义在全实数域且具有解析逆的双射函数族（立方有理、sinh和立方多项式），以及一种新颖的径向流架构，在保持训练稳定性的同时显著减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有归一化流方法存在表达力与可逆性之间的权衡：仿射变换平滑但表达力不足；单调样条有局部控制但仅分段平滑；残差流需要数值求逆。需要一种结合所有方法优点的解决方案。

Method: 1. 引入三种解析双射函数族：立方有理、sinh和立方多项式；2. 开发径向流架构，通过直接参数化变换径向坐标同时保持角度方向；3. 在耦合流中作为即插即用替换，并在高维物理问题上进行验证。

Result: 新双射函数在耦合流中匹配或超过样条性能；径向流在具有径向结构的目标上，仅用千分之一的参数即可达到与耦合流相当的质量；在φ⁴晶格场理论实验中优于仿射基线。

Conclusion: 提出的解析双射函数结合了现有方法的优点，径向流架构提供了训练稳定性和几何可解释性，为高维物理问题提供了有效的归一化流解决方案。

Abstract: A key challenge in designing normalizing flows is finding expressive scalar bijections that remain invertible with tractable Jacobians. Existing approaches face trade-offs: affine transformations are smooth and analytically invertible but lack expressivity; monotonic splines offer local control but are only piecewise smooth and act on bounded domains; residual flows achieve smoothness but need numerical inversion. We introduce three families of analytic bijections -- cubic rational, sinh, and cubic polynomial -- that are globally smooth ($C^\infty$), defined on all of $\mathbb{R}$, and analytically invertible in closed form, combining the favorable properties of all prior approaches. These bijections serve as drop-in replacements in coupling flows, matching or exceeding spline performance. Beyond coupling layers, we develop radial flows: a novel architecture using direct parametrization that transforms the radial coordinate while preserving angular direction. Radial flows exhibit exceptional training stability, produce geometrically interpretable transformations, and on targets with radial structure can achieve comparable quality to coupling flows with $1000\times$ fewer parameters. We provide comprehensive evaluation on 1D and 2D benchmarks, and demonstrate applicability to higher-dimensional physics problems through experiments on $φ^4$ lattice field theory, where our bijections outperform affine baselines and enable problem-specific designs that address mode collapse.

</details>


### [76] [Unified Optimization of Source Weights and Transfer Quantities in Multi-Source Transfer Learning: An Asymptotic Framework](https://arxiv.org/abs/2601.10779)
*Qingyue Zhang,Chang Chu,Haohao Fu,Tianren Peng,Yanru Wu,Guanbo Huang,Yang Li,Shao-Lun Huang*

Main category: cs.LG

TL;DR: 本文提出了UOWQ理论框架，统一优化多源迁移学习中的源任务权重和转移数量，解决了传统方法只关注单一维度的问题。


<details>
  <summary>Details</summary>
Motivation: 现有迁移学习方法通常只优化源权重或转移样本数量，忽略了二者的联合考虑，且多源均匀迁移可能导致负迁移问题。

Method: 基于Kullback-Leibler散度泛化误差度量的渐近分析，将多源迁移学习建模为参数估计问题，提供闭式解和凸优化数值方法。

Result: 理论证明在权重适当调整后使用全部可用源样本总是最优的，实验在DomainNet和Office-Home等基准数据集上优于强基线方法。

Conclusion: UOWQ框架在理论和实践上都表现出色，为多源迁移学习提供了统一优化的有效解决方案。

Abstract: Transfer learning plays a vital role in improving model performance in data-scarce scenarios. However, naive uniform transfer from multiple source tasks may result in negative transfer, highlighting the need to properly balance the contributions of heterogeneous sources. Moreover, existing transfer learning methods typically focus on optimizing either the source weights or the amount of transferred samples, while largely neglecting the joint consideration of the other. In this work, we propose a theoretical framework, Unified Optimization of Weights and Quantities (UOWQ), which formulates multi-source transfer learning as a parameter estimation problem grounded in an asymptotic analysis of a Kullback-Leibler divergence-based generalization error measure. The proposed framework jointly determines the optimal source weights and optimal transfer quantities for each source task. Firstly, we prove that using all available source samples is always optimal once the weights are properly adjusted, and we provide a theoretical explanation for this phenomenon. Moreover, to determine the optimal transfer weights, our analysis yields closed-form solutions in the single-source setting and develops a convex optimization-based numerical procedure for the multi-source case. Building on the theoretical results, we further propose practical algorithms for both multi-source transfer learning and multi-task learning settings. Extensive experiments on real-world benchmarks, including DomainNet and Office-Home, demonstrate that UOWQ consistently outperforms strong baselines. The results validate both the theoretical predictions and the practical effectiveness of our framework.

</details>


### [77] [Towards Tensor Network Models for Low-Latency Jet Tagging on FPGAs](https://arxiv.org/abs/2601.10801)
*Alberto Coppi,Ema Puljak,Lorenzo Borella,Daniel Jaschke,Enrique Rico,Maurizio Pierini,Jacopo Pazzini,Andrea Triossi,Simone Montangero*

Main category: cs.LG

TL;DR: 该研究系统评估了张量网络（TN）模型在FPGA上实现实时喷注标记的可行性，重点针对HL-LHC Level-1触发系统的低延迟需求。


<details>
  <summary>Details</summary>
Motivation: 受HL-LHC Level-1触发系统严格要求的驱动，探索张量网络作为深度神经网络的紧凑且可解释的替代方案。

Method: 使用矩阵乘积态（MPS）和树张量网络（TTN）模型处理低层级喷注成分特征，并研究后训练量化以实现硬件高效实现。

Result: 模型性能与最先进的深度学习分类器相当，FPGA合成显示亚微秒级延迟，支持在线部署可行性。

Conclusion: 该研究突显了基于张量网络的模型在低延迟环境中实现快速且资源高效推理的潜力。

Abstract: We present a systematic study of Tensor Network (TN) models $\unicode{x2013}$ Matrix Product States (MPS) and Tree Tensor Networks (TTN) $\unicode{x2013}$ for real-time jet tagging in high-energy physics, with a focus on low-latency deployment on Field Programmable Gate Arrays (FPGAs). Motivated by the strict requirements of the HL-LHC Level-1 trigger system, we explore TNs as compact and interpretable alternatives to deep neural networks. Using low-level jet constituent features, our models achieve competitive performance compared to state-of-the-art deep learning classifiers. We investigate post-training quantization to enable hardware-efficient implementations without degrading classification performance or latency. The best-performing models are synthesized to estimate FPGA resource usage, latency, and memory occupancy, demonstrating sub-microsecond latency and supporting the feasibility of online deployment in real-time trigger systems. Overall, this study highlights the potential of TN-based models for fast and resource-efficient inference in low-latency environments.

</details>


### [78] [Digital Metabolism: Decoupling Logic from Facts via Regenerative Unlearning -- Towards a Pure Neural Logic Core](https://arxiv.org/abs/2601.10810)
*Mengmeng Peng,Zhenyu Fang,He Sun*

Main category: cs.LG

TL;DR: 本文提出"数字代谢"假说，认为针对性遗忘对提炼纯神经逻辑核心至关重要，并引入RLCP双流训练框架实现事实依赖的线性不可解码，在Qwen2.5-0.5B模型上观察到结构结晶化现象和思维链推理的自发涌现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中参数纠缠问题，即通用推理能力与具体事实知识在共享权重中的叠加状态导致的"内存墙"和幻觉问题。

Method: 提出再生逻辑核心协议(RLCP)，采用双流训练框架，通过深层梯度反转使特定事实依赖关系线性不可解码。

Result: 在Qwen2.5-0.5B模型上，目标事实关联的准确率降至7%以下，同时观察到结构结晶化效应，模型在GSM8K上自发采用思维链推理。

Conclusion: 研究为模块化"神经CPU+符号RAM"架构提供了动态权重层面的对应方案，验证了针对性遗忘对提炼纯逻辑核心的必要性。

Abstract: Large language models (LLMs) currently suffer from parameter entanglement, where general reasoning capabilities (logic) and specific factual knowledge (facts) exist in a superposition state within shared weights. This coupling leads to the "memory wall," where computational capacity is squandered on simulating retrieval, often resulting in hallucinations. In this paper, we propose "digital metabolism," a thermodynamic hypothesis suggesting that targeted forgetting is necessary for distilling a pure neural logic core. To validate this hypothesis, we introduce the Regenerative Logic-Core Protocol (RLCP), a dual-stream training framework that renders specific factual dependencies linearly undecodable via deep-layer gradient reversal. Applying RLCP to Qwen2.5-0.5B, we observe a distinct phase transition: the model achieves near-zero retention of targeted factual associations (Accuracy < 7%) while exhibiting changes consistent with an emergent "structural crystallization" effect. Empirical analysis on GSM8K reveals that the "metabolized" model spontaneously adopts chain-of-thought (CoT) scaffolding, which we interpret as compensating for the loss of direct associative recall (shifting from $O(1)$ recall to $O(N)$ reasoning). While the causal mechanism underlying this behavioral shift requires further investigation, our findings provide a dynamic weight-level counterpart to architectural innovations like DeepSeek's Engram, paving the way for modular "Neural CPU + Symbolic RAM" architectures.

</details>


### [79] [Towards Reliable ML Feature Engineering via Planning in Constrained-Topology of LLM Agents](https://arxiv.org/abs/2601.10820)
*Himanshu Thakur,Anusha Kamath,Anurag Muthyala,Dhwani Sanmukhani,Smruthi Mukund,Jay Katukuri*

Main category: cs.LG

TL;DR: 提出一个规划器引导的多智能体框架，解决代码生成模型在特征工程中的三大挑战：数据集稀缺、工具集成不足、人机协作不佳，显著提升特征工程效率。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成模型在真实ML团队中应用受限，主要面临三个挑战：缺乏捕捉复杂特征工程过程的数据集、现有编码工具与团队工作流集成不足、人机协作时机不当导致反馈效果差。

Method: 采用规划器引导的约束拓扑多智能体框架，通过LLM驱动的规划器基于团队环境图协调智能体调用，生成上下文感知提示，并利用下游失败回溯修正上游产物，在关键步骤请求人工干预。

Result: 在内部数据集上，相比人工构建和无规划工作流，评估指标分别提升38%和150%；在实际推荐模型特征工程中，将开发周期从3周缩短至1天，服务超过1.2亿用户。

Conclusion: 该框架通过智能规划和人机协作机制，有效解决了特征工程自动化的关键瓶颈，实现了代码生成模型在真实生产环境中的实用化部署。

Abstract: Recent advances in code generation models have unlocked unprecedented opportunities for automating feature engineering, yet their adoption in real-world ML teams remains constrained by critical challenges: (i) the scarcity of datasets capturing the iterative and complex coding processes of production-level feature engineering, (ii) limited integration and personalization of widely used coding agents, such as CoPilot and Devin, with a team's unique tools, codebases, workflows, and practices, and (iii) suboptimal human-AI collaboration due to poorly timed or insufficient feedback. We address these challenges with a planner-guided, constrained-topology multi-agent framework that generates code for repositories in a multi-step fashion. The LLM-powered planner leverages a team's environment, represented as a graph, to orchestrate calls to available agents, generate context-aware prompts, and use downstream failures to retroactively correct upstream artifacts. It can request human intervention at critical steps, ensuring generated code is reliable, maintainable, and aligned with team expectations. On a novel in-house dataset, our approach achieves 38% and 150% improvement in the evaluation metric over manually crafted and unplanned workflows respectively. In practice, when building features for recommendation models serving over 120 million users, our approach has delivered real-world impact by reducing feature engineering cycles from three weeks to a single day.

</details>


### [80] [Mugi: Value Level Parallelism For Efficient LLMs](https://arxiv.org/abs/2601.10823)
*Daniel Price,Prabhu Vellaisamy,John Shen,Di Wu*

Main category: cs.LG

TL;DR: 本文提出了一种新的VLP架构Mugi，通过值级并行性优化LLM中的非线性操作和GEMM计算，显著提升了性能、能效和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有VLP技术主要针对大批次、低精度GEMM计算，但在LLM中存在更复杂的非线性操作和小批次GEMM需求，需要更通用的VLP优化方案。

Method: 1. 将VLP泛化用于非线性近似，采用值中心方法；2. 优化小批次非对称输入GEMM；3. 设计新VLP架构Mugi支持完整LLM工作负载。

Result: Mugi在非线性softmax操作上吞吐量提升45倍，能效提升668倍；LLM整体性能提升2.07倍，能效提升3.11倍；运营碳减少1.45倍，隐含碳减少1.48倍。

Conclusion: VLP技术可以有效优化LLM中的复杂操作，Mugi架构在性能、能效和可持续性方面均取得显著改进，为LLM优化提供了新方向。

Abstract: Value level parallelism (VLP) has been proposed to improve the efficiency of large-batch, low-precision general matrix multiply (GEMM) between symmetric activations and weights. In transformer based large language models (LLMs), there exist more sophisticated operations beyond activation-weight GEMM. In this paper, we explore how VLP benefits LLMs. First, we generalize VLP for nonlinear approximations, outperforming existing nonlinear approximations in end-to-end LLM accuracy, performance, and efficiency. Our VLP approximation follows a value-centric approach, where important values are assigned with greater accuracy. Second, we optimize VLP for small-batch GEMMs with asymmetric inputs efficiently, which leverages timely LLM optimizations, including weight-only quantization, key-value (KV) cache quantization, and group query attention. Finally, we design a new VLP architecture, Mugi, to encapsulate the innovations above and support full LLM workloads, while providing better performance, efficiency and sustainability. Our experimental results show that Mugi can offer significant improvements on throughput and energy efficiency, up to $45\times$ and $668\times$ for nonlinear softmax operations, and $2.07\times$ and $3.11\times$ for LLMs, and also decrease operational carbon for LLM operation by $1.45\times$ and embodied carbon by $1.48\times$.

</details>


### [81] [AI-Guided Human-In-the-Loop Inverse Design of High Performance Engineering Structures](https://arxiv.org/abs/2601.10859)
*Dat Quoc Ha,Md Ferdous Alam,Markus J. Buehler,Faez Ahmed,Josephine V. Carstensen*

Main category: cs.LG

TL;DR: 该论文提出了一种AI协同设计方法，通过机器学习预测用户在拓扑优化中的偏好区域，减少迭代次数，提高设计效率。


<details>
  <summary>Details</summary>
Motivation: 传统拓扑优化存在计算时间长、黑箱性质强的问题，而现有的人机协同方法依赖耗时的迭代区域选择，需要减少迭代试验次数。

Method: 使用U-Net架构的图像分割模型，在合成数据集上训练预测用户偏好区域（最长拓扑构件或最复杂结构连接），并将AI推荐呈现给用户。

Result: 模型成功预测了合理的修改区域，在非标准拓扑优化问题上表现出泛化能力，能够将线性屈曲载荷提高39%，总设计时间仅增加15秒。

Conclusion: 集成AI协同设计的人机协同拓扑优化方法显著提高了设计效率和性能，同时保持了用户交互性。

Abstract: Inverse design tools such as Topology Optimization (TO) can achieve new levels of improvement for high-performance engineered structures. However, widespread use is hindered by high computational times and a black-box nature that inhibits user interaction. Human-in-the-loop TO approaches are emerging that integrate human intuition into the design generation process. However, these rely on the time-consuming bottleneck of iterative region selection for design modifications. To reduce the number of iterative trials, this contribution presents an AI co-pilot that uses machine learning to predict the user's preferred regions. The prediction model is configured as an image segmentation task with a U-Net architecture. It is trained on synthetic datasets where human preferences either identify the longest topological member or the most complex structural connection. The model successfully predicts plausible regions for modification and presents them to the user as AI recommendations. The human preference model demonstrates generalization across diverse and non-standard TO problems and exhibits emergent behavior outside the single-region selection training data. Demonstration examples show that the new human-in-the-loop TO approach that integrates the AI co-pilot can improve manufacturability or improve the linear buckling load by 39% while only increasing the total design time by 15 sec compared to conventional simplistic TO.

</details>


### [82] [Beyond Accuracy: A Stability-Aware Metric for Multi-Horizon Forecasting](https://arxiv.org/abs/2601.10863)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.LG

TL;DR: 本文提出了一种新的概率多步预测质量评估指标——预测准确性与一致性评分（AC评分），该指标同时考虑多步预测的准确性和时间一致性，并允许用户自定义准确性与一致性的权重平衡。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法仅优化准确性，忽略了时间一致性（即模型在不同预测起点对同一未来事件预测的一致性）。这种单一目标无法全面评估预测质量。

Method: 开发了可微分的AC评分目标函数，用于训练季节性ARIMA模型，并在M4小时基准数据集上进行评估。该评分方法允许用户自定义准确性和一致性的权重。

Result: 与传统最大似然估计相比，AC优化模型在保持相当或改进的点预测准确性的同时，对相同时间戳的预测波动性降低了75%。

Conclusion: AC评分提供了一种更全面的预测质量评估框架，能够显著提升预测的时间一致性，同时保持预测准确性，为时间序列预测提供了新的优化方向。

Abstract: Traditional time series forecasting methods optimize for accuracy alone. This objective neglects temporal consistency, in other words, how consistently a model predicts the same future event as the forecast origin changes. We introduce the forecast accuracy and coherence score (forecast AC score for short) for measuring the quality of probabilistic multi-horizon forecasts in a way that accounts for both multi-horizon accuracy and stability. Our score additionally provides for user-specified weights to balance accuracy and consistency requirements. As an example application, we implement the score as a differentiable objective function for training seasonal ARIMA models and evaluate it on the M4 Hourly benchmark dataset. Results demonstrate substantial improvements over traditional maximum likelihood estimation. Our AC-optimized models achieve a 75\% reduction in forecast volatility for the same target timestamps while maintaining comparable or improved point forecast accuracy.

</details>


### [83] [Unit-Consistent (UC) Adjoint for GSD and Backprop in Deep Learning Applications](https://arxiv.org/abs/2601.10873)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: 该论文提出了一种在正齐次神经网络中实现尺度不变性的优化方法，通过引入单位一致（UC）伴随矩阵来替代欧几里得转置，从而确保优化过程对节点级对角重缩放具有不变性。


<details>
  <summary>Details</summary>
Motivation: 标准梯度下降方法对正齐次神经网络的尺度对称性不具有等变性，导致优化轨迹严重依赖于参数化方式。先前的研究提出了基于路径的优化方案，但本文旨在在反向伴随/优化几何层面制定不变性要求。

Method: 通过用单位一致（UC）伴随矩阵替换欧几里得转置，推导出UC尺度一致的梯度下降和反向传播算法。该方法在算子级别提供统一的处理方式，可应用于网络组件和优化器状态。

Result: 提出的UC伴随方法能够确保优化过程对节点级对角重缩放具有不变性，从而消除参数化方式对优化轨迹的任意影响。

Conclusion: 单位一致伴随矩阵为正齐次神经网络提供了一种简单且统一的尺度不变优化框架，解决了标准梯度下降中的尺度依赖性问题。

Abstract: Deep neural networks constructed from linear maps and positively homogeneous nonlinearities (e.g., ReLU) possess a fundamental gauge symmetry: the network function is invariant to node-wise diagonal rescalings. However, standard gradient descent is not equivariant to this symmetry, causing optimization trajectories to depend heavily on arbitrary parameterizations. Prior work has proposed rescaling-invariant optimization schemes for positively homogeneous networks (e.g., path-based or path-space updates). Our contribution is complementary: we formulate the invariance requirement at the level of the backward adjoint/optimization geometry, which provides a simple, operator-level recipe that can be applied uniformly across network components and optimizer state. By replacing the Euclidean transpose with a Unit-Consistent (UC) adjoint, we derive UC gauge-consistent steepest descent and backprogation.

</details>


### [84] [Action Shapley: A Training Data Selection Metric for World Model in Reinforcement Learning](https://arxiv.org/abs/2601.10905)
*Rajat Ghosh,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 本文提出了Action Shapley作为训练数据选择的公平度量指标，并设计了随机动态算法来降低计算复杂度，在五个真实世界案例中验证了其高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习和基于模型的方法中，世界模型的质量严重依赖于训练数据。当直接与环境交互成本高昂或不可行时，需要一种公平有效的数据选择方法。

Method: 引入Action Shapley作为数据选择的无偏度量，提出随机动态算法来避免传统Shapley值的指数级计算复杂度。

Result: 算法在五个数据受限的真实案例中验证，计算效率比传统指数时间计算提升80%以上，基于Action Shapley的数据选择策略优于临时选择方法。

Conclusion: Action Shapley为世界模型的训练数据选择提供了有效的解决方案，在保证公平性的同时显著提升了计算效率。

Abstract: Numerous offline and model-based reinforcement learning systems incorporate world models to emulate the inherent environments. A world model is particularly important in scenarios where direct interactions with the real environment is costly, dangerous, or impractical. The efficacy and interpretability of such world models are notably contingent upon the quality of the underlying training data. In this context, we introduce Action Shapley as an agnostic metric for the judicious and unbiased selection of training data. To facilitate the computation of Action Shapley, we present a randomized dynamic algorithm specifically designed to mitigate the exponential complexity inherent in traditional Shapley value computations. Through empirical validation across five data-constrained real-world case studies, the algorithm demonstrates a computational efficiency improvement exceeding 80\% in comparison to conventional exponential time computations. Furthermore, our Action Shapley-based training data selection policy consistently outperforms ad-hoc training data selection.

</details>


### [85] [Realistic Curriculum Reinforcement Learning for Autonomous and Sustainable Marine Vessel Navigation](https://arxiv.org/abs/2601.10911)
*Zhang Xiaocai,Xiao Zhe,Liang Maohan,Liu Tao,Li Haijiang,Zhang Wenbin*

Main category: cs.LG

TL;DR: 本文提出了一种课程强化学习框架，结合数据驱动的海洋仿真环境和机器学习燃料消耗预测模块，用于实现可持续和安全的船舶自主导航。


<details>
  <summary>Details</summary>
Motivation: 传统船舶导航依赖人工经验，缺乏自主性和排放意识，容易因人为错误影响安全性。海事运输的可持续性（包括温室气体排放和航行安全）日益重要。

Method: 使用真实船舶移动数据和扩散模型构建动态海洋仿真环境；基于历史操作数据和机器学习回归预测燃料消耗；设计轻量级策略型CRL智能体，采用综合考虑安全性、排放、时效性和目标完成度的奖励机制。

Result: 在印度洋海域验证表明，该框架能有效处理复杂任务，在连续动作空间中实现稳定高效的学习，支持可持续安全的船舶导航。

Conclusion: 提出的CRL框架能够渐进式处理复杂导航任务，为海事运输的可持续性和安全性提供了有效的技术解决方案。

Abstract: Sustainability is becoming increasingly critical in the maritime transport, encompassing both environmental and social impacts, such as Greenhouse Gas (GHG) emissions and navigational safety. Traditional vessel navigation heavily relies on human experience, often lacking autonomy and emission awareness, and is prone to human errors that may compromise safety. In this paper, we propose a Curriculum Reinforcement Learning (CRL) framework integrated with a realistic, data-driven marine simulation environment and a machine learning-based fuel consumption prediction module. The simulation environment is constructed using real-world vessel movement data and enhanced with a Diffusion Model to simulate dynamic maritime conditions. Vessel fuel consumption is estimated using historical operational data and learning-based regression. The surrounding environment is represented as image-based inputs to capture spatial complexity. We design a lightweight, policy-based CRL agent with a comprehensive reward mechanism that considers safety, emissions, timeliness, and goal completion. This framework effectively handles complex tasks progressively while ensuring stable and efficient learning in continuous action spaces. We validate the proposed approach in a sea area of the Indian Ocean, demonstrating its efficacy in enabling sustainable and safe vessel navigation.

</details>


### [86] [FAConvLSTM: Factorized-Attention ConvLSTM for Efficient Feature Extraction in Multivariate Climate Data](https://arxiv.org/abs/2601.10914)
*Francis Ndikum Nji,Jianwu Wang*

Main category: cs.LG

TL;DR: FAConvLSTM是一种改进的ConvLSTM模型，通过因子化注意力机制和轴向空间注意力，在保持循环动态的同时显著降低计算成本，并提高对多尺度物理过程和长程空间依赖的建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统ConvLSTM2D存在计算成本高、感受野局部性强的问题，难以有效建模地球观测数据中的长程遥相关、多尺度相互作用和非平稳性等复杂动态。

Method: 提出FAConvLSTM层，采用因子化门控计算（轻量级1×1瓶颈和共享深度空间混合）、多尺度扩张深度分支、挤压-激励重校准、窥视孔连接、轻量级轴向空间注意力和专用子空间头等技术。

Result: 在多变量时空气候数据上的实验表明，FAConvLSTM比标准ConvLSTM产生更稳定、可解释和鲁棒的潜在表示，同时显著降低计算开销。

Conclusion: FAConvLSTM作为ConvLSTM2D的直接替代方案，在效率、空间表达能力和物理可解释性方面均有显著提升，能够更好地建模复杂的地球系统动态。

Abstract: Learning physically meaningful spatiotemporal representations from high-resolution multivariate Earth observation data is challenging due to strong local dynamics, long-range teleconnections, multi-scale interactions, and nonstationarity. While ConvLSTM2D is a commonly used baseline, its dense convolutional gating incurs high computational cost and its strictly local receptive fields limit the modeling of long-range spatial structure and disentangled climate dynamics. To address these limitations, we propose FAConvLSTM, a Factorized-Attention ConvLSTM layer designed as a drop-in replacement for ConvLSTM2D that simultaneously improves efficiency, spatial expressiveness, and physical interpretability. FAConvLSTM factorizes recurrent gate computations using lightweight [1 times 1] bottlenecks and shared depthwise spatial mixing, substantially reducing channel complexity while preserving recurrent dynamics. Multi-scale dilated depthwise branches and squeeze-and-excitation recalibration enable efficient modeling of interacting physical processes across spatial scales, while peephole connections enhance temporal precision. To capture teleconnection-scale dependencies without incurring global attention cost, FAConvLSTM incorporates a lightweight axial spatial attention mechanism applied sparsely in time. A dedicated subspace head further produces compact per timestep embeddings refined through temporal self-attention with fixed seasonal positional encoding. Experiments on multivariate spatiotemporal climate data shows superiority demonstrating that FAConvLSTM yields more stable, interpretable, and robust latent representations than standard ConvLSTM, while significantly reducing computational overhead.

</details>


### [87] [HOSL: Hybrid-Order Split Learning for Memory-Constrained Edge Training](https://arxiv.org/abs/2601.10940)
*Aakriti,Zhe Li,Dandan Liang,Chao Huang,Rui Li,Haibo Yang*

Main category: cs.LG

TL;DR: HOSL是一种新型混合阶分割学习框架，通过在客户端使用零阶优化、服务器端使用一阶优化，解决了分割学习中内存效率与优化效果之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有分割学习系统主要依赖一阶优化，需要客户端存储中间激活值，导致内存开销大；而零阶优化虽然减少内存使用但收敛缓慢、性能下降。

Method: 提出HOSL框架，在客户端使用内存高效的零阶梯度估计消除反向传播和激活存储，在服务器端使用一阶优化确保快速收敛和竞争性能。

Result: 实验表明HOSL将客户端GPU内存减少高达3.7倍，同时准确率仅比一阶基线低0.20%-4.23%，比零阶基线性能提升高达15.55%。

Conclusion: HOSL通过混合阶优化策略有效解决了边缘设备内存受限问题，理论分析表明其收敛速度随更多计算卸载到服务器而改善。

Abstract: Split learning (SL) enables collaborative training of large language models (LLMs) between resource-constrained edge devices and compute-rich servers by partitioning model computation across the network boundary. However, existing SL systems predominantly rely on first-order (FO) optimization, which requires clients to store intermediate quantities such as activations for backpropagation. This results in substantial memory overhead, largely negating benefits of model partitioning. In contrast, zeroth-order (ZO) optimization eliminates backpropagation and significantly reduces memory usage, but often suffers from slow convergence and degraded performance. In this work, we propose HOSL, a novel Hybrid-Order Split Learning framework that addresses this fundamental trade-off between memory efficiency and optimization effectiveness by strategically integrating ZO optimization on the client side with FO optimization on the server side. By employing memory-efficient ZO gradient estimation at the client, HOSL eliminates backpropagation and activation storage, reducing client memory consumption. Meanwhile, server-side FO optimization ensures fast convergence and competitive performance. Theoretically, we show that HOSL achieves a $\mathcal{O}(\sqrt{d_c/TQ})$ rate, which depends on client-side model dimension $d_c$ rather than the full model dimension $d$, demonstrating that convergence improves as more computation is offloaded to the server. Extensive experiments on OPT models (125M and 1.3B parameters) across 6 tasks demonstrate that HOSL reduces client GPU memory by up to 3.7$\times$ compared to the FO method while achieving accuracy within 0.20%-4.23% of this baseline. Furthermore, HOSL outperforms the ZO baseline by up to 15.55%, validating the effectiveness of our hybrid strategy for memory-efficient training on edge devices.

</details>


### [88] [Multivariate LSTM-Based Forecasting for Renewable Energy: Enhancing Climate Change Mitigation](https://arxiv.org/abs/2601.10961)
*Farshid Kamrani,Kristen Schell*

Main category: cs.LG

TL;DR: 本文提出了一种基于多元LSTM网络的可再生能源发电预测方法，通过利用本地及邻近地区的历史数据，有效捕捉长期依赖关系和不同可再生能源之间的相互作用，从而提高预测准确性，降低碳排放并提高电力供应的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源在现代电力系统中的日益集成，其发电的固有变异性带来了显著挑战。准确的发电预测对于维持电力系统运行的可靠性、稳定性和经济效率至关重要。传统方法如确定性方法和随机规划往往依赖于聚类技术生成的代表性场景，但可能无法完全捕捉可再生能源数据中的复杂时间依赖性和非线性模式。

Method: 本文引入了一种基于多元长短期记忆（LSTM）网络的模型，利用实际历史数据预测可再生能源发电。该模型通过结合本地及邻近地区的历史数据，有效捕捉长期依赖关系和不同可再生能源之间的相互作用。

Result: 案例研究表明，所提出的预测方法能够实现更低的二氧化碳排放和更可靠的电力负荷供应。

Conclusion: 多元LSTM网络在可再生能源发电预测中表现出色，能够有效处理复杂的时间依赖性和非线性模式，为电力系统运行提供更准确、可靠的预测结果，有助于降低碳排放并提高供电可靠性。

Abstract: The increasing integration of renewable energy sources (RESs) into modern power systems presents significant opportunities but also notable challenges, primarily due to the inherent variability of RES generation. Accurate forecasting of RES generation is crucial for maintaining the reliability, stability, and economic efficiency of power system operations. Traditional approaches, such as deterministic methods and stochastic programming, frequently depend on representative scenarios generated through clustering techniques like K-means. However, these methods may fail to fully capture the complex temporal dependencies and non-linear patterns within RES data. This paper introduces a multivariate Long Short-Term Memory (LSTM)-based network designed to forecast RESs generation using their real-world historical data. The proposed model effectively captures long-term dependencies and interactions between different RESs, utilizing historical data from both local and neighboring areas to enhance predictive accuracy. In the case study, we showed that the proposed forecasting approach results in lower CO2 emissions, and a more reliable supply of electric loads.

</details>


### [89] [Transient learning dynamics drive escape from sharp valleys in Stochastic Gradient Descent](https://arxiv.org/abs/2601.10962)
*Ning Yang,Yikuan Zhang,Qi Ouyang,Chao Tang,Yuhai Tu*

Main category: cs.LG

TL;DR: 该论文通过分析SGD学习动力学，揭示了非平衡机制如何选择更平坦、泛化能力更强的解，发现SGD噪声将损失景观重塑为有利于平坦解的有效势，并提出瞬态冻结机制解释训练过程。


<details>
  <summary>Details</summary>
Motivation: 理解SGD为何偏好更平坦、泛化能力更强的解，以及其动力学起源仍然不清楚，需要揭示其背后的物理机制。

Method: 通过数值实验分析SGD学习动力学，使用可处理的物理模型展示SGD噪声如何重塑损失景观，并识别瞬态探索阶段和冻结机制。

Result: 发现SGD轨迹会反复逃离尖锐谷地并转向更平坦区域，SGD噪声将景观重塑为有利于平坦解的有效势，训练后期能量壁垒增长导致动力学被困在单个盆地中。

Conclusion: 研究提供了统一的物理框架，连接学习动力学、损失景观几何和泛化能力，为设计更有效的优化算法提供了原则。

Abstract: Stochastic gradient descent (SGD) is central to deep learning, yet the dynamical origin of its preference for flatter, more generalizable solutions remains unclear. Here, by analyzing SGD learning dynamics, we identify a nonequilibrium mechanism governing solution selection. Numerical experiments reveal a transient exploratory phase in which SGD trajectories repeatedly escape sharp valleys and transition toward flatter regions of the loss landscape. By using a tractable physical model, we show that the SGD noise reshapes the landscape into an effective potential that favors flat solutions. Crucially, we uncover a transient freezing mechanism: as training proceeds, growing energy barriers suppress inter-valley transitions and ultimately trap the dynamics within a single basin. Increasing the SGD noise strength delays this freezing, which enhances convergence to flatter minima. Together, these results provide a unified physical framework linking learning dynamics, loss-landscape geometry, and generalization, and suggest principles for the design of more effective optimization algorithms.

</details>


### [90] [Toward Adaptive Grid Resilience: A Gradient-Free Meta-RL Framework for Critical Load Restoration](https://arxiv.org/abs/2601.10973)
*Zain ul Abdeen,Waris Gill,Ming Jin*

Main category: cs.LG

TL;DR: 本文提出了一种元引导无梯度强化学习（MGF-RL）框架，用于在极端事件后快速恢复配电网的关键负荷，该框架通过元学习历史停电经验获得可迁移的初始化策略，并能以最少的任务特定调优快速适应新场景。


<details>
  <summary>Details</summary>
Motivation: 极端事件后恢复配电网关键负荷需要自适应控制来维持电网韧性，但可再生能源发电的不确定性、可调度资源有限以及非线性动态使得有效恢复变得困难。传统强化学习在泛化能力差且需要针对新停电配置或发电模式进行大量重新训练。

Method: MGF-RL将一阶元学习与进化策略相结合，实现无需梯度计算的可扩展策略搜索，同时适应非线性、受约束的配电系统动态。该框架从历史停电经验中学习可迁移的初始化策略。

Result: 在IEEE 13总线和IEEE 123总线测试系统上的实验表明，MGF-RL在可靠性、恢复速度和适应效率方面优于标准RL、基于MAML的元RL和模型预测控制，特别是在可再生能源预测误差下。MGF-RL能够泛化到未见过的停电和可再生能源模式，且比传统RL需要更少的微调回合。

Conclusion: MGF-RL提供了与任务相似性和环境变化相关的次线性遗憾边界，支持其经验优势，并激励将其用于可再生能源丰富的配电网实时负荷恢复。

Abstract: Restoring critical loads after extreme events demands adaptive control to maintain distribution-grid resilience, yet uncertainty in renewable generation, limited dispatchable resources, and nonlinear dynamics make effective restoration difficult. Reinforcement learning (RL) can optimize sequential decisions under uncertainty, but standard RL often generalizes poorly and requires extensive retraining for new outage configurations or generation patterns. We propose a meta-guided gradient-free RL (MGF-RL) framework that learns a transferable initialization from historical outage experiences and rapidly adapts to unseen scenarios with minimal task-specific tuning. MGF-RL couples first-order meta-learning with evolutionary strategies, enabling scalable policy search without gradient computation while accommodating nonlinear, constrained distribution-system dynamics. Experiments on IEEE 13-bus and IEEE 123-bus test systems show that MGF-RL outperforms standard RL, MAML-based meta-RL, and model predictive control across reliability, restoration speed, and adaptation efficiency under renewable forecast errors. MGF-RL generalizes to unseen outages and renewable patterns while requiring substantially fewer fine-tuning episodes than conventional RL. We also provide sublinear regret bounds that relate adaptation efficiency to task similarity and environmental variation, supporting the empirical gains and motivating MGF-RL for real-time load restoration in renewable-rich distribution grids.

</details>


### [91] [Reasoning Distillation for Lightweight Automated Program Repair](https://arxiv.org/abs/2601.10987)
*Aanand Balasubramanian,Sashank Silwal*

Main category: cs.LG

TL;DR: 本文研究如何通过轻量级符号推理监督来改进紧凑型自动程序修复模型中的修复类型分类。


<details>
  <summary>Details</summary>
Motivation: 小型代码模型在资源受限环境中具有吸引力，但它们通常只产生单一预测，难以确定是否学习了有意义的程序结构还是依赖浅层相关性。

Method: 提出推理蒸馏方法，使用大型教师模型提供结构化符号推理标签和修复类型标签，训练基于CodeT5的学生模型，比较仅标签和推理蒸馏两种设置。

Result: 在IntroClass基准测试中，推理监督持续提高了宏平均性能，特别是在较少出现的错误类别上，且不增加模型大小或复杂度。正确推理轨迹与正确预测强相关但不完全决定预测结果。

Conclusion: 符号推理蒸馏是提高轻量级程序修复模型可解释性和鲁棒性的实用方法。

Abstract: We study whether lightweight symbolic reasoning supervision can improve fix type classification in compact automated program repair models. Small code models are attractive for resource-constrained settings, but they typically produce only a single prediction, making it unclear whether they learn meaningful program structure or rely on shallow correlations. We propose a reasoning distillation approach in which a large teacher model provides structured symbolic reasoning tags alongside fix-type labels. These tags capture high-level causal properties of bugs without relying on free-form explanations. We train a CodeT5-based student model under label-only and reasoning-distilled settings on the IntroClass benchmark. Reasoning supervision consistently improves macro averaged performance, particularly on less frequent bug categories, without increasing model size or complexity. We further analyze the relationship between reasoning accuracy and fix-type prediction, showing that correct reasoning traces strongly correlate with correct predictions, while not fully determining them. Our results suggest that symbolic reasoning distillation is a practical way to improve interpretability and robustness in lightweight program repair models.

</details>


### [92] [Constant Metric Scaling in Riemannian Computation](https://arxiv.org/abs/2601.10992)
*Kisung You*

Main category: cs.LG

TL;DR: 本文讨论了黎曼流形上常数度量缩放的性质，区分了在缩放下变化的量（如范数、距离、体积元素、梯度大小）和保持不变的几何对象（如Levi-Civita联络、测地线、指数映射等），并探讨了其在黎曼优化中的意义。


<details>
  <summary>Details</summary>
Motivation: 在计算设置中经常出现对黎曼度量的常数缩放操作，但这种操作的影响在实践中并不总是清晰，可能被误认为是曲率、流形结构或坐标表示的变化。本文旨在澄清常数度量缩放的性质。

Method: 通过理论分析，系统地区分在常数度量缩放下变化的量和不变量，并讨论这些性质在黎曼优化算法中的具体含义。

Result: 明确了常数度量缩放仅影响度量相关的量（如距离、体积），而不改变流形的内在几何结构（如联络、测地线等）。在优化中，这种缩放可解释为步长的全局调整而非几何修改。

Conclusion: 常数度量缩放是一个纯技术操作，可以在不改变底层几何结构的情况下引入黎曼计算中，这有助于澄清实践中对尺度参数作用的误解。

Abstract: Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.

</details>


### [93] [Backdoor Attacks on Multi-modal Contrastive Learning](https://arxiv.org/abs/2601.11006)
*Simi D Kuniyilh,Rita Machacy*

Main category: cs.LG

TL;DR: 本文对对比学习中的后门攻击进行了系统综述，分析了威胁模型、攻击方法、目标领域和防御措施，强调了对比学习特有的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 对比学习已成为自监督表示学习的主流方法，但近期研究表明其容易受到后门攻击和数据投毒攻击的影响，攻击者可通过操纵预训练数据或模型更新来植入恶意行为。

Method: 采用系统性文献综述方法，对对比学习中的后门攻击进行全面的比较分析，包括威胁模型分类、攻击技术总结、目标领域识别和防御策略评估。

Result: 研究发现对比学习存在特定的安全漏洞，后门攻击在视觉、多模态、图学习和联邦学习等多个领域都具有实际威胁，现有防御措施仍面临挑战。

Conclusion: 该研究对工业级和分布式系统中对比学习的安全部署具有重要意义，指出了未来研究的关键方向和挑战。

Abstract: Contrastive learning has become a leading self- supervised approach to representation learning across domains, including vision, multimodal settings, graphs, and federated learning. However, recent studies have shown that contrastive learning is susceptible to backdoor and data poisoning attacks. In these attacks, adversaries can manipulate pretraining data or model updates to insert hidden malicious behavior. This paper offers a thorough and comparative review of backdoor attacks in contrastive learning. It analyzes threat models, attack methods, target domains, and available defenses. We summarize recent advancements in this area, underline the specific vulnerabilities inherent to contrastive learning, and discuss the challenges and future research directions. Our findings have significant implications for the secure deployment of systems in industrial and distributed environments.

</details>


### [94] [Combating Spurious Correlations in Graph Interpretability via Self-Reflection](https://arxiv.org/abs/2601.11021)
*Kecheng Cai,Chenyang Xu,Chao Peng*

Main category: cs.LG

TL;DR: 本文提出了一种自反思框架，通过将现有可解释图学习方法的重要性评分反馈给模型进行第二轮评估，从而提升在具有强伪相关性的Spurious-Motif基准数据集上的可解释性表现。


<details>
  <summary>Details</summary>
Motivation: Spurious-Motif基准数据集因包含伪相关性而极具挑战性，现有方法在该基准上表现显著较差。本文旨在提升模型在此类数据集上的可解释性，受大语言模型自反思技术的启发。

Method: 提出自反思框架，将现有方法生成的重要性评分反馈给原方法进行第二轮评估，模仿大语言模型的自反思提示机制。基于图表示学习分析改进原因，提出基于反馈机制的微调训练方法。

Result: 自反思技术能有效增强在具有强伪相关性数据集上的可解释性，通过迭代评估过程帮助模型区分真正相关结构与误导性模式。

Conclusion: 自反思框架可成功应用于图学习领域，提升模型在挑战性基准上的可解释性表现，为处理伪相关性提供了新思路。

Abstract: Interpretable graph learning has recently emerged as a popular research topic in machine learning. The goal is to identify the important nodes and edges of an input graph that are crucial for performing a specific graph reasoning task. A number of studies have been conducted in this area, and various benchmark datasets have been proposed to facilitate evaluation. Among them, one of the most challenging is the Spurious-Motif benchmark, introduced at ICLR 2022. The datasets in this synthetic benchmark are deliberately designed to include spurious correlations, making it particularly difficult for models to distinguish truly relevant structures from misleading patterns. As a result, existing methods exhibit significantly worse performance on this benchmark compared to others.
  In this paper, we focus on improving interpretability on the challenging Spurious-Motif datasets. We demonstrate that the self-reflection technique, commonly used in large language models to tackle complex tasks, can also be effectively adapted to enhance interpretability in datasets with strong spurious correlations. Specifically, we propose a self-reflection framework that can be integrated with existing interpretable graph learning methods. When such a method produces importance scores for each node and edge, our framework feeds these predictions back into the original method to perform a second round of evaluation. This iterative process mirrors how large language models employ self-reflective prompting to reassess their previous outputs. We further analyze the reasons behind this improvement from the perspective of graph representation learning, which motivates us to propose a fine-tuning training method based on this feedback mechanism.

</details>


### [95] [Matching High-Dimensional Geometric Quantiles for Test-Time Adaptation of Transformers and Convolutional Networks Alike](https://arxiv.org/abs/2601.11022)
*Sravan Danda,Aditya Challa,Shlok Mehendale,Snehanshu Saha*

Main category: cs.LG

TL;DR: 本文提出了一种与架构无关的测试时自适应方法，通过添加适配器网络预处理输入图像，并使用分位数损失来纠正分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应方法大多依赖修改分类器的权重，严重依赖于架构，难以扩展到通用架构。

Method: 提出通过添加适配器网络预处理输入图像，使用分位数损失来匹配高维几何分位数，从而纠正分布偏移。

Result: 在CIFAR10-C、CIFAR100-C和TinyImageNet-C数据集上验证了该方法对经典卷积网络和Transformer网络的有效性。

Conclusion: 该方法能够在不依赖特定架构的情况下有效实现测试时自适应，具有较好的通用性。

Abstract: Test-time adaptation (TTA) refers to adapting a classifier for the test data when the probability distribution of the test data slightly differs from that of the training data of the model. To the best of our knowledge, most of the existing TTA approaches modify the weights of the classifier relying heavily on the architecture. It is unclear as to how these approaches are extendable to generic architectures. In this article, we propose an architecture-agnostic approach to TTA by adding an adapter network pre-processing the input images suitable to the classifier. This adapter is trained using the proposed quantile loss. Unlike existing approaches, we correct for the distribution shift by matching high-dimensional geometric quantiles. We prove theoretically that under suitable conditions minimizing quantile loss can learn the optimal adapter. We validate our approach on CIFAR10-C, CIFAR100-C and TinyImageNet-C by training both classic convolutional and transformer networks on CIFAR10, CIFAR100 and TinyImageNet datasets.

</details>


### [96] [AVP-Pro: An Adaptive Multi-Modal Fusion and Contrastive Learning Approach for Comprehensive Two-Stage Antiviral Peptide Identification](https://arxiv.org/abs/2601.11028)
*Xinru Wen,Weizhong Lin,zi liu,Xuan Xiao*

Main category: cs.LG

TL;DR: AVP-Pro是一个新颖的两阶段预测框架，通过自适应特征融合和对比学习来准确识别抗病毒肽（AVPs），在抗病毒药物开发中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉复杂序列依赖性和区分高相似度样本方面存在局限，需要更有效的AVP识别工具来支持新型药物开发。

Method: 构建包含10种描述符的全景特征空间，采用分层融合架构整合自注意力和自适应门控机制，结合OHEM驱动的对比学习策略和BLOSUM62增强。

Result: 在通用AVP识别阶段达到0.9531准确率和0.9064 MCC值，优于现有SOTA方法；在功能亚型预测阶段实现了6种病毒家族和8种特定病毒的准确分类。

Conclusion: AVP-Pro为抗病毒药物高通量筛选提供了强大且可解释的新工具，并开发了用户友好的Web界面。

Abstract: The accurate identification of antiviral peptides (AVPs) is crucial for novel drug development. However, existing methods still have limitations in capturing complex sequence dependencies and distinguishing confusing samples with high similarity. To address these challenges, we propose AVP-Pro, a novel two-stage predictive framework that integrates adaptive feature fusion and contrastive learning. To comprehensively capture the physicochemical properties and deep-seated patterns of peptide sequences, we constructed a panoramic feature space encompassing 10 distinct descriptors and designed a hierarchical fusion architecture. This architecture integrates self-attention and adaptive gating mechanisms to dynamically modulate the weights of local motifs extracted by CNNs and global dependencies captured by BiLSTMs based on sequence context. Targeting the blurred decision boundary caused by the high similarity between positive and negative sample sequences, we adopted an Online Hard Example Mining (OHEM)-driven contrastive learning strategy enhanced by BLOSUM62. This approach significantly sharpened the model's discriminative power. Model evaluation results show that in the first stage of general AVP identification, the model achieved an accuracy of 0.9531 and an MCC of 0.9064, outperforming existing state-of-the-art (SOTA) methods. In the second stage of functional subtype prediction, combined with a transfer learning strategy, the model realized accurate classification of 6 viral families and 8 specific viruses under small-sample conditions. AVP-Pro provides a powerful and interpretable new tool for the high-throughput screening of antiviral drugs. To further enhance accessibility for users, we have developed a user-friendly web interface, which is available at https://wwwy1031-avp-pro.hf.space.

</details>


### [97] [Self-Augmented Mixture-of-Experts for QoS Prediction](https://arxiv.org/abs/2601.11036)
*Kecheng Cai,Chao Peng,Chenyang Xu,Xia Chen*

Main category: cs.LG

TL;DR: 本文提出了一种自增强的专家混合模型，用于解决服务质量预测中的稀疏性问题，通过迭代预测和专家间协作提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 服务质量预测面临用户-服务交互数据稀疏的挑战，传统方法难以有效利用有限的观测数据。

Method: 采用自增强策略，通过部分掩码预测值并反馈给模型进行迭代优化，设计专家混合架构实现多专家网络的协作预测。

Result: 在基准数据集上的实验表明，该方法优于现有基线方法，取得了有竞争力的结果。

Conclusion: 自增强的专家混合模型通过迭代优化和专家协作，有效提升了稀疏数据下的服务质量预测性能。

Abstract: Quality of Service (QoS) prediction is one of the most fundamental problems in service computing and personalized recommendation. In the problem, there is a set of users and services, each associated with a set of descriptive features. Interactions between users and services produce feedback values, typically represented as numerical QoS metrics such as response time or availability. Given the observed feedback for a subset of user-service pairs, the goal is to predict the QoS values for the remaining pairs.
  A key challenge in QoS prediction is the inherent sparsity of user-service interactions, as only a small subset of feedback values is typically observed. To address this, we propose a self-augmented strategy that leverages a model's own predictions for iterative refinement. In particular, we partially mask the predicted values and feed them back into the model to predict again. Building on this idea, we design a self-augmented mixture-of-experts model, where multiple expert networks iteratively and collaboratively estimate QoS values. We find that the iterative augmentation process naturally aligns with the MoE architecture by enabling inter-expert communication: in the second round, each expert receives the first-round predictions and refines its output accordingly. Experiments on benchmark datasets show that our method outperforms existing baselines and achieves competitive results.

</details>


### [98] [OpFML: Pipeline for ML-based Operational Forecasting](https://arxiv.org/abs/2601.11046)
*Shahbaz Alvi,Giusy Fedele,Gabriele Accarino,Italo Epicoco,Ilenia Manco,Pasquale Schiano*

Main category: cs.LG

TL;DR: OpFML是一个用于周期性预测的可配置机器学习管道，特别应用于野火危险指数预测，解决了传统方法高估风险的问题。


<details>
  <summary>Details</summary>
Motivation: 传统野火危险评估方法经常高估风险，而机器学习在气候和地球科学中的应用日益广泛，需要可操作的周期性预测系统。

Method: 开发了OpFML——一个可配置和适应性强的管道，用于部署机器学习模型进行周期性预测，并以每日火灾危险指数预测为例展示其功能。

Result: 成功创建了一个可操作的机器学习预测管道，能够有效应用于野火危险评估等周期性预测任务。

Conclusion: OpFML为基于机器学习的操作预测系统提供了一个灵活且可配置的解决方案，在野火危险评估等领域具有实际应用价值。

Abstract: Machine learning is finding its application in a multitude of areas in science and research, and Climate and Earth Sciences is no exception to this trend. Operational forecasting systems based on data-driven approaches and machine learning methods deploy models for periodic forecasting. Wildfire danger assessment using machine learning has garnered significant interest in the last decade, as conventional methods often overestimate the risk of wildfires. In this work, we present the code OpFML: Operational Forecasting with Machine Learning. OpFML is a configurable and adaptable pipeline that can be utilized to serve a machine learning model for periodic forecasting. We further demonstrate the capabilities of the pipeline through its application to daily Fire Danger Index forecasting and outline its various features.

</details>


### [99] [Spurious Rewards Paradox: Mechanistically Understanding How RLVR Activates Memorization Shortcuts in LLMs](https://arxiv.org/abs/2601.11061)
*Lecheng Yan,Ruizhe Li,Guanhua Chen,Qing Li,Jiahui Geng,Wenxi Li,Vincent Wang,Chris Lee*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is highly effective for enhancing LLM reasoning, yet recent evidence shows models like Qwen 2.5 achieve significant gains even with spurious or incorrect rewards. We investigate this phenomenon and identify a "Perplexity Paradox": spurious RLVR triggers a divergence where answer-token perplexity drops while prompt-side coherence degrades, suggesting the model is bypassing reasoning in favor of memorization. Using Path Patching, Logit Lens, JSD analysis, and Neural Differential Equations, we uncover a hidden Anchor-Adapter circuit that facilitates this shortcut. We localize a Functional Anchor in the middle layers (L18-20) that triggers the retrieval of memorized solutions, followed by Structural Adapters in later layers (L21+) that transform representations to accommodate the shortcut signal. Finally, we demonstrate that scaling specific MLP keys within this circuit allows for bidirectional causal steering-artificially amplifying or suppressing contamination-driven performance. Our results provide a mechanistic roadmap for identifying and mitigating data contamination in RLVR-tuned models. Code is available at https://github.com/idwts/How-RLVR-Activates-Memorization-Shortcuts.

</details>


### [100] [Bridging Cognitive Neuroscience and Graph Intelligence: Hippocampus-Inspired Multi-View Hypergraph Learning for Web Finance Fraud](https://arxiv.org/abs/2601.11073)
*Rongkun Cui,Nana Zhang,Kun Zhu,Qi Zhang*

Main category: cs.LG

TL;DR: HIMVH是一个受海马体启发的多视图超图学习模型，用于网络金融欺诈检测，通过跨视图不一致性感知和新颖性感知超图学习模块，有效解决欺诈伪装和长尾数据分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的欺诈检测方法面临欺诈伪装（恶意交易模仿良性行为）和长尾数据分布（稀有但关键的欺诈案例被掩盖）两大挑战，需要开发更有效的检测模型。

Method: 提出HIMVH模型，包含：1）受海马体场景冲突监测启发的跨视图不一致性感知模块，捕捉多交易视图间的细微差异；2）受CA1区匹配-不匹配新颖性检测机制启发的新颖性感知超图学习模块，测量特征与邻域期望的偏差并自适应重加权消息。

Result: 在6个基于网络的金融欺诈数据集上的实验表明，HIMVH相比15个最先进模型，平均AUC提升6.42%，F1提升9.74%，AP提升39.14%。

Conclusion: HIMVH通过模拟海马体的认知机制，有效解决了网络金融欺诈检测中的欺诈伪装和长尾分布问题，显著提升了检测性能。

Abstract: Online financial services constitute an essential component of contemporary web ecosystems, yet their openness introduces substantial exposure to fraud that harms vulnerable users and weakens trust in digital finance. Such threats have become a significant web harm that erodes societal fairness and affects the well being of online communities. However, existing detection methods based on graph neural networks (GNNs) struggle with two persistent challenges: (1) fraud camouflage, where malicious transactions mimic benign behaviors to evade detection, and (2) long-tailed data distributions, which obscure rare but critical fraudulent cases. To fill these gaps, we propose HIMVH, a Hippocampus-Inspired Multi-View Hypergraph learning model for web finance fraud detection. Specifically, drawing inspiration from the scene conflict monitoring role of the hippocampus, we design a cross-view inconsistency perception module that captures subtle discrepancies and behavioral heterogeneity across multiple transaction views. This module enables the model to identify subtle cross-view conflicts for detecting online camouflaged fraudulent behaviors. Furthermore, inspired by the match-mismatch novelty detection mechanism of the CA1 region, we introduce a novelty-aware hypergraph learning module that measures feature deviations from neighborhood expectations and adaptively reweights messages, thereby enhancing sensitivity to online rare fraud patterns in the long-tailed settings. Extensive experiments on six web-based financial fraud datasets demonstrate that HIMVH achieves 6.42\% improvement in AUC, 9.74\% in F1 and 39.14\% in AP on average over 15 SOTA models.

</details>


### [101] [Soft Bayesian Context Tree Models for Real-Valued Time Series](https://arxiv.org/abs/2601.11079)
*Shota Saito,Yuta Nakahara,Toshiyasu Matsushima*

Main category: cs.LG

TL;DR: 本文提出了Soft-BCT模型，这是一种用于实值时间序列的新型贝叶斯上下文树模型，采用软（概率性）分割而非硬分割。


<details>
  <summary>Details</summary>
Motivation: 改进传统BCT模型在实值时间序列处理中的硬分割限制，通过软分割提高模型的灵活性和性能。

Method: 基于变分推断的学习算法，实现上下文空间的概率性分割。

Result: 在多个真实数据集上，Soft-BCT表现出与传统BCT相当或更优的性能。

Conclusion: Soft-BCT通过软分割机制有效提升了实值时间序列建模能力，具有实际应用价值。

Abstract: This paper proposes the soft Bayesian context tree model (Soft-BCT), which is a novel BCT model for real-valued time series. The Soft-BCT considers soft (probabilistic) splits of the context space, instead of hard (deterministic) splits of the context space as in the previous BCT for real-valued time series. A learning algorithm of the Soft-BCT is proposed based on the variational inference. For some real-world datasets, the Soft-BCT demonstrates almost the same or superior performance to the previous BCT.

</details>


### [102] [Differentially Private Subspace Fine-Tuning for Large Language Models](https://arxiv.org/abs/2601.11113)
*Lele Zheng,Xiang Wang,Tao Zhang,Yang Cao,Ke Cheng,Yulong Shen*

Main category: cs.LG

TL;DR: DP-SFT是一种两阶段子空间微调方法，通过仅在任务相关子空间注入差分隐私噪声，显著降低噪声影响，在保持隐私保护的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统差分隐私微调在高维参数空间中注入噪声会导致大范数扰动，降低模型性能并破坏训练稳定性。参数更新主要发生在低维任务特定子空间中，其他方向变化很小

Method: 两阶段方法：第一阶段通过分析主梯度方向识别任务特定子空间；第二阶段将完整梯度投影到该子空间，添加DP噪声，然后将扰动后的梯度映射回原始参数空间进行模型更新

Result: 在多个数据集上的实验表明，DP-SFT在严格DP约束下提高了准确性和稳定性，加速了收敛速度，相比DP微调基线取得了显著提升

Conclusion: DP-SFT通过子空间限制噪声注入，有效解决了DP微调中的性能下降问题，为隐私保护下的语言模型微调提供了高效解决方案

Abstract: Fine-tuning large language models on downstream tasks is crucial for realizing their cross-domain potential but often relies on sensitive data, raising privacy concerns. Differential privacy (DP) offers rigorous privacy guarantees and has been widely adopted in fine-tuning; however, naively injecting noise across the high-dimensional parameter space creates perturbations with large norms, degrading performance and destabilizing training. To address this issue, we propose DP-SFT, a two-stage subspace fine-tuning method that substantially reduces noise magnitude while preserving formal DP guarantees. Our intuition is that, during fine-tuning, significant parameter updates lie within a low-dimensional, task-specific subspace, while other directions change minimally. Hence, we only inject DP noise into this subspace to protect privacy without perturbing irrelevant parameters. In phase one, we identify the subspace by analyzing principal gradient directions to capture task-specific update signals. In phase two, we project full gradients onto this subspace, add DP noise, and map the perturbed gradients back to the original parameter space for model updates, markedly lowering noise impact. Experiments on multiple datasets demonstrate that DP-SFT enhances accuracy and stability under rigorous DP constraints, accelerates convergence, and achieves substantial gains over DP fine-tuning baselines.

</details>


### [103] [Optimized Algorithms for Text Clustering with LLM-Generated Constraints](https://arxiv.org/abs/2601.11118)
*Chaoqi Jia,Weihong Wu,Longkun Guo,Zhigang Lu,Chao Chen,Kok-Leong Ong*

Main category: cs.LG

TL;DR: 本文提出了一种基于LLM的约束生成方法，通过生成约束集而非传统成对约束来降低资源消耗，并设计了针对LLM生成约束特点的聚类算法，在保持聚类精度的同时大幅减少LLM查询次数。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法使用人工标注的成对约束（must-link和cannot-link）来提升聚类精度，但标注成本高。随着大语言模型的发展，研究者开始探索利用LLM自动生成约束，但现有方法查询效率低且约束准确性不足。

Method: 1）提出新的约束生成方法，生成约束集而非成对约束，提高查询效率和约束准确性；2）设计专门的约束聚类算法，引入置信度阈值和惩罚机制处理可能不准确的约束；3）在五个文本数据集上进行评估。

Result: 实验结果表明，该方法在聚类精度上与最优算法相当，同时将LLM查询次数减少了20倍以上，显著降低了资源消耗。

Conclusion: 该方法有效解决了LLM约束生成中的资源消耗问题，在保持聚类质量的同时大幅提升了效率，为文本聚类任务提供了一种实用的解决方案。

Abstract: Clustering is a fundamental tool that has garnered significant interest across a wide range of applications including text analysis. To improve clustering accuracy, many researchers have incorporated background knowledge, typically in the form of must-link and cannot-link constraints, to guide the clustering process. With the recent advent of large language models (LLMs), there is growing interest in improving clustering quality through LLM-based automatic constraint generation. In this paper, we propose a novel constraint-generation approach that reduces resource consumption by generating constraint sets rather than using traditional pairwise constraints. This approach improves both query efficiency and constraint accuracy compared to state-of-the-art methods. We further introduce a constrained clustering algorithm tailored to the characteristics of LLM-generated constraints. Our method incorporates a confidence threshold and a penalty mechanism to address potentially inaccurate constraints. We evaluate our approach on five text datasets, considering both the cost of constraint generation and the overall clustering performance. The results show that our method achieves clustering accuracy comparable to the state-of-the-art algorithms while reducing the number of LLM queries by more than 20 times.

</details>


### [104] [Shape-morphing programming of soft materials on complex geometries via neural operator](https://arxiv.org/abs/2601.11126)
*Lu Chen,Gengxiang Chen,Xu Liu,Jingyan Su,Xuhao Lyu,Lihui Wang,Yingguang Li*

Main category: cs.LG

TL;DR: 提出了一种光谱和空间神经算子（S2NO），用于在复杂几何形状上实现高保真形变预测，结合进化算法实现体素级材料分布优化，显著提升复杂形状变形编程的效率和能力。


<details>
  <summary>Details</summary>
Motivation: 尽管在简单几何形状的基本形变设计方面取得了进展，但在复杂几何形状上实现精确多样的形变设计（如共形植入物部署或空气动力学形变）仍然具有挑战性。

Method: 开发了S2NO方法，通过集成拉普拉斯本征函数编码和空间卷积来有效捕捉不规则计算域上的全局和局部形变行为，并结合进化算法进行体素级材料分布优化。

Result: S2NO能够在各种复杂几何形状（包括不规则边界形状、多孔结构和薄壁结构）上实现高保真形变预测，其离散不变性特性支持超分辨率材料分布设计。

Conclusion: 该方法显著扩展了形变设计的多样性和复杂性，为复杂形状变形编程提供了高效且强大的解决方案。

Abstract: Shape-morphing soft materials can enable diverse target morphologies through voxel-level material distribution design, offering significant potential for various applications. Despite progress in basic shape-morphing design with simple geometries, achieving advanced applications such as conformal implant deployment or aerodynamic morphing requires accurate and diverse morphing designs on complex geometries, which remains challenging. Here, we present a Spectral and Spatial Neural Operator (S2NO), which enables high-fidelity morphing prediction on complex geometries. S2NO effectively captures global and local morphing behaviours on irregular computational domains by integrating Laplacian eigenfunction encoding and spatial convolutions. Combining S2NO with evolutionary algorithms enables voxel-level optimisation of material distributions for shape morphing programming on various complex geometries, including irregular-boundary shapes, porous structures, and thin-walled structures. Furthermore, the neural operator's discretisation-invariant property enables super-resolution material distribution design, further expanding the diversity and complexity of morphing design. These advancements significantly improve the efficiency and capability of programming complex shape morphing.

</details>


### [105] [FSL-BDP: Federated Survival Learning with Bayesian Differential Privacy for Credit Risk Modeling](https://arxiv.org/abs/2601.11134)
*Sultan Amed,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: 提出了一个联邦生存学习框架FSL-BDP，用于在数据隐私保护约束下进行跨机构信用风险建模，解决了传统违约预测的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统信用风险模型面临数据保护法规限制（如GDPR、CCPA）禁止跨境共享借款人数据，同时传统违约预测存在两个问题：二元分类忽略违约时间，集中式训练违反监管约束。

Method: FSL-BDP框架结合联邦学习和贝叶斯差分隐私，在不集中敏感数据的情况下建模违约时间轨迹，提供数据依赖的隐私保证。

Result: 在三个真实信用数据集上的实验表明，联邦学习改变了隐私机制的效果排序：贝叶斯DP在联邦环境中表现显著优于经典DP，接近非私有性能，在大多数参与客户中表现更好。

Conclusion: 隐私机制选择应在目标部署架构中评估，而非基于集中式基准测试，这为受监管多机构环境中的隐私保护决策支持系统设计提供了实用指导。

Abstract: Credit risk models are a critical decision-support tool for financial institutions, yet tightening data-protection rules (e.g., GDPR, CCPA) increasingly prohibit cross-border sharing of borrower data, even as these models benefit from cross-institution learning. Traditional default prediction suffers from two limitations: binary classification ignores default timing, treating early defaulters (high loss) equivalently to late defaulters (low loss), and centralized training violates emerging regulatory constraints. We propose a Federated Survival Learning framework with Bayesian Differential Privacy (FSL-BDP) that models time-to-default trajectories without centralizing sensitive data. The framework provides Bayesian (data-dependent) differential privacy (DP) guarantees while enabling institutions to jointly learn risk dynamics. Experiments on three real-world credit datasets (LendingClub, SBA, Bondora) show that federation fundamentally alters the relative effectiveness of privacy mechanisms. While classical DP performs better than Bayesian DP in centralized settings, the latter benefits substantially more from federation (+7.0\% vs +1.4\%), achieving near parity of non-private performance and outperforming classical DP in the majority of participating clients. This ranking reversal yields a key decision-support insight: privacy mechanism selection should be evaluated in the target deployment architecture, rather than centralized benchmarks. These findings provide actionable guidance for practitioners designing privacy-preserving decision support systems in regulated, multi-institutional environments.

</details>


### [106] [Context-aware Graph Causality Inference for Few-Shot Molecular Property Prediction](https://arxiv.org/abs/2601.11135)
*Van Thuy Hoang,O-Joun Lee*

Main category: cs.LG

TL;DR: CaMol是一个基于因果推理的上下文感知图学习框架，用于少样本分子属性预测，通过引入化学知识指导的上下文图、可学习原子掩码策略和分布干预器来发现因果子结构。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在少样本分子属性预测中无法有效利用功能基团先验知识和识别与属性直接相关的关键子结构的局限性。

Method: 1. 构建编码化学知识的上下文图，连接功能基团、分子和属性；2. 提出可学习原子掩码策略解耦因果子结构；3. 引入分布干预器通过后门调整分离因果效应。

Result: 在多个分子数据集上，CaMol在少样本任务中实现了优越的准确性和样本效率，并展现出对未见属性的良好泛化能力。

Conclusion: CaMol不仅提高了预测性能，而且发现的因果子结构与化学知识高度一致，增强了模型的可解释性。

Abstract: Molecular property prediction is becoming one of the major applications of graph learning in Web-based services, e.g., online protein structure prediction and drug discovery. A key challenge arises in few-shot scenarios, where only a few labeled molecules are available for predicting unseen properties. Recently, several studies have used in-context learning to capture relationships among molecules and properties, but they face two limitations in: (1) exploiting prior knowledge of functional groups that are causally linked to properties and (2) identifying key substructures directly correlated with properties. We propose CaMol, a context-aware graph causality inference framework, to address these challenges by using a causal inference perspective, assuming that each molecule consists of a latent causal structure that determines a specific property. First, we introduce a context graph that encodes chemical knowledge by linking functional groups, molecules, and properties to guide the discovery of causal substructures. Second, we propose a learnable atom masking strategy to disentangle causal substructures from confounding ones. Third, we introduce a distribution intervener that applies backdoor adjustment by combining causal substructures with chemically grounded confounders, disentangling causal effects from real-world chemical variations. Experiments on diverse molecular datasets showed that CaMol achieved superior accuracy and sample efficiency in few-shot tasks, showing its generalizability to unseen properties. Also, the discovered causal substructures were strongly aligned with chemical knowledge about functional groups, supporting the model interpretability.

</details>


### [107] [Assesing the Viability of Unsupervised Learning with Autoencoders for Predictive Maintenance in Helicopter Engines](https://arxiv.org/abs/2601.11154)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 本文比较了两种直升机发动机预测性维护策略：监督分类方法和基于自编码器的无监督异常检测方法，在真实数据集上评估了各自的性能表现。


<details>
  <summary>Details</summary>
Motivation: 直升机发动机的意外故障会导致严重的操作中断、安全隐患和昂贵的维修成本，因此需要有效的预测性维护策略来降低这些风险。

Method: 使用监督分类管道（需要正常和故障行为的标记数据）和无监督异常检测方法（基于自编码器，仅使用健康发动机数据学习正常操作模式，将偏差标记为潜在故障）。

Result: 监督模型在有标注故障数据时表现良好，而自编码器方法无需故障标签即可实现有效检测，特别适用于故障数据稀缺或不完整的场景。

Conclusion: 研究强调了在准确性、数据可用性和部署可行性之间的实际权衡，并指出无监督学习作为航空航天应用早期故障检测的可行解决方案的潜力。

Abstract: Unplanned engine failures in helicopters can lead to severe operational disruptions, safety hazards, and costly repairs. To mitigate these risks, this study compares two predictive maintenance strategies for helicopter engines: a supervised classification pipeline and an unsupervised anomaly detection approach based on autoencoders (AEs). The supervised method relies on labelled examples of both normal and faulty behaviour, while the unsupervised approach learns a model of normal operation using only healthy engine data, flagging deviations as potential faults. Both methods are evaluated on a real-world dataset comprising labelled snapshots of helicopter engine telemetry. While supervised models demonstrate strong performance when annotated failures are available, the AE achieves effective detection without requiring fault labels, making it particularly well suited for settings where failure data are scarce or incomplete. The comparison highlights the practical trade-offs between accuracy, data availability, and deployment feasibility, and underscores the potential of unsupervised learning as a viable solution for early fault detection in aerospace applications.

</details>


### [108] [Theoretically and Practically Efficient Resistance Distance Computation on Large Graphs](https://arxiv.org/abs/2601.11159)
*Yichun Yang,Longlong Lin,Rong-Hua Li,Meihao Liao,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了两种基于Lanczos方法的高效算法（Lanczos Iteration和Lanczos Push）来计算图上的电阻距离，相比现有方法在条件数κ较大的情况下具有更好的收敛性能和时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 电阻距离计算在图分析中至关重要，但现有方法在图的拉普拉斯矩阵条件数κ较大时收敛缓慢，需要更高效的算法。

Method: 提出了两种新算法：Lanczos Iteration（全局算法，时间复杂度Õ(√κ m)）和Lanczos Push（局部算法，时间复杂度Õ(κ^2.75)），均基于经典Lanczos方法设计以减少对κ的依赖。

Result: 在8个不同规模和统计特性的真实数据集上的实验表明，两种算法在效率和精度上都显著优于现有最优方法。

Conclusion: Lanczos-based算法在电阻距离计算中实现了重大突破，特别适用于条件数κ较大的图结构，为图分析应用提供了更高效的解决方案。

Abstract: The computation of resistance distance is pivotal in a wide range of graph analysis applications, including graph clustering, link prediction, and graph neural networks. Despite its foundational importance, efficient algorithms for computing resistance distances on large graphs are still lacking. Existing state-of-the-art (SOTA) methods, including power iteration-based algorithms and random walk-based local approaches, often struggle with slow convergence rates, particularly when the condition number of the graph Laplacian matrix, denoted by $κ$, is large. To tackle this challenge, we propose two novel and efficient algorithms inspired by the classic Lanczos method: Lanczos Iteration and Lanczos Push, both designed to reduce dependence on $κ$. Among them, Lanczos Iteration is a near-linear time global algorithm, whereas Lanczos Push is a local algorithm with a time complexity independent of the size of the graph. More specifically, we prove that the time complexity of Lanczos Iteration is $\tilde{O}(\sqrtκ m)$ ($m$ is the number of edges of the graph and $\tilde{O}$ means the complexity omitting the $\log$ terms) which achieves a speedup of $\sqrtκ$ compared to previous power iteration-based global methods. For Lanczos Push, we demonstrate that its time complexity is $\tilde{O}(κ^{2.75})$ under certain mild and frequently established assumptions, which represents a significant improvement of $κ^{0.25}$ over the SOTA random walk-based local algorithms. We validate our algorithms through extensive experiments on eight real-world datasets of varying sizes and statistical properties, demonstrating that Lanczos Iteration and Lanczos Push significantly outperform SOTA methods in terms of both efficiency and accuracy.

</details>


### [109] [Clustering High-dimensional Data: Balancing Abstraction and Representation Tutorial at AAAI 2026](https://arxiv.org/abs/2601.11160)
*Claudia Plant,Lena G. M. Bauer,Christian Böhm*

Main category: cs.LG

TL;DR: 该论文探讨聚类算法中抽象与表示之间的平衡问题，分析了传统K-means、子空间聚类和深度聚类方法在平衡这两者方面的不同策略，并展望了未来聚类研究的发展方向。


<details>
  <summary>Details</summary>
Motivation: 聚类算法需要在抽象和表示之间找到平衡点。过度的抽象会丢失重要细节，而过度的表示则可能导致算法无法有效识别聚类。本文旨在探讨如何在不同类型的聚类算法中实现这种平衡。

Method: 分析了传统K-means、子空间聚类和深度聚类方法。K-means采用高度抽象和简单表示；子空间聚类通过学习多个潜在空间来分离聚类相关信息；深度聚类通过聚类损失函数来强制实施抽象。

Result: 研究表明，随着表示能力的增强，需要在目标函数中显式地强制执行抽象以确保聚类效果。子空间聚类方法通过分离聚类相关信息和其它信息来帮助实现平衡。

Conclusion: 未来的聚类方法需要更自适应地平衡抽象和表示，以提高性能、能效和可解释性。人脑在聚类任务中的优异表现表明当前方法仍有很大改进空间。

Abstract: How to find a natural grouping of a large real data set? Clustering requires a balance between abstraction and representation. To identify clusters, we need to abstract from superfluous details of individual objects. But we also need a rich representation that emphasizes the key features shared by groups of objects that distinguish them from other groups of objects.
  Each clustering algorithm implements a different trade-off between abstraction and representation. Classical K-means implements a high level of abstraction - details are simply averaged out - combined with a very simple representation - all clusters are Gaussians in the original data space. We will see how approaches to subspace and deep clustering support high-dimensional and complex data by allowing richer representations. However, with increasing representational expressiveness comes the need to explicitly enforce abstraction in the objective function to ensure that the resulting method performs clustering and not just representation learning. We will see how current deep clustering methods define and enforce abstraction through centroid-based and density-based clustering losses. Balancing the conflicting goals of abstraction and representation is challenging. Ideas from subspace clustering help by learning one latent space for the information that is relevant to clustering and another latent space to capture all other information in the data.
  The tutorial ends with an outlook on future research in clustering. Future methods will more adaptively balance abstraction and representation to improve performance, energy efficiency and interpretability. By automatically finding the sweet spot between abstraction and representation, the human brain is very good at clustering and other related tasks such as single-shot learning. So, there is still much room for improvement.

</details>


### [110] [GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling](https://arxiv.org/abs/2601.11161)
*Pascal Schlachter,Bin Yang*

Main category: cs.LG

TL;DR: 该论文提出了首个持续源自由通用域自适应（continual SF-UniDA）研究，开发了GMM-COMET方法来解决模型在多个不同未标记目标域序列上的连续适应问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，源数据在适应过程中可能不再可用，且目标域标签空间可能与源域不同。现有SF-UniDA方法仅假设单一源到目标域偏移，而实际应用中模型需要连续适应多个不同目标域。

Method: 结合高斯混合模型伪标签和均值教师框架，引入一致性损失以提高长期适应稳定性。GMM-COMET方法整合了先前在线SF-UniDA方法的关键思想。

Result: GMM-COMET在所有评估场景中持续优于仅使用源数据的模型，为持续SF-UniDA提供了强大的首个基准。

Conclusion: 该研究填补了持续源自由通用域自适应领域的空白，提出的GMM-COMET方法在多个目标域序列适应中表现出稳定性和有效性。

Abstract: Unsupervised domain adaptation tackles the problem that domain shifts between training and test data impair the performance of neural networks in many real-world applications. Thereby, in realistic scenarios, the source data may no longer be available during adaptation, and the label space of the target domain may differ from the source label space. This setting, known as source-free universal domain adaptation (SF-UniDA), has recently gained attention, but all existing approaches only assume a single domain shift from source to target. In this work, we present the first study on continual SF-UniDA, where the model must adapt sequentially to a stream of multiple different unlabeled target domains. Building upon our previous methods for online SF-UniDA, we combine their key ideas by integrating Gaussian mixture model-based pseudo-labeling within a mean teacher framework for improved stability over long adaptation sequences. Additionally, we introduce consistency losses for further robustness. The resulting method GMM-COMET provides a strong first baseline for continual SF-UniDA and is the only approach in our experiments to consistently improve upon the source-only model across all evaluated scenarios. Our code is available at https://github.com/pascalschlachter/GMM-COMET.

</details>


### [111] [LSTM VS. Feed-Forward Autoencoders for Unsupervised Fault Detection in Hydraulic Pumps](https://arxiv.org/abs/2601.11163)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 本文探讨了两种无监督自编码器方案用于液压泵早期故障检测，仅使用健康数据进行训练，在包含故障的测试集上表现出高可靠性。


<details>
  <summary>Details</summary>
Motivation: 工业液压泵的意外故障会导致生产中断和巨大成本，需要有效的早期故障检测方法。

Method: 采用两种无监督自编码器：前馈模型分析单个传感器快照，LSTM模型捕捉短期时间窗口。两种网络仅使用52个传感器通道的健康数据进行训练。

Result: 尽管训练过程中未使用故障样本，模型在包含7个标注故障区间的测试集上实现了高可靠性。

Conclusion: 无监督自编码器方法在仅使用健康数据训练的情况下，能够有效检测液压泵的早期故障，具有实际应用价值。

Abstract: Unplanned failures in industrial hydraulic pumps can halt production and incur substantial costs. We explore two unsupervised autoencoder (AE) schemes for early fault detection: a feed-forward model that analyses individual sensor snapshots and a Long Short-Term Memory (LSTM) model that captures short temporal windows. Both networks are trained only on healthy data drawn from a minute-level log of 52 sensor channels; evaluation uses a separate set that contains seven annotated fault intervals. Despite the absence of fault samples during training, the models achieve high reliability.

</details>


### [112] [TimeMar: Multi-Scale Autoregressive Modeling for Unconditional Time Series Generation](https://arxiv.org/abs/2601.11184)
*Xiangyu Xu,Qingsong Zhong,Jilin Hu*

Main category: cs.LG

TL;DR: 本文提出了一个结构解耦的多尺度时间序列生成框架，通过多分辨率编码和粗到细的生成方式解决时间序列的结构复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 时间序列分析中存在数据稀缺和隐私问题，而现有方法未能充分处理时间序列的多尺度时间模式和异质性结构特征。

Method: 使用双路径VQ-VAE解耦趋势和季节性成分，在多时间分辨率下将序列编码为离散标记，采用粗到细的自回归生成方式，并引入基于引导的重建策略。

Result: 在六个数据集上的实验表明，该方法比现有方法生成更高质量的时间序列，显著减少参数数量的同时保持强性能，并能生成高质量的长序列。

Conclusion: 该框架有效解决了时间序列生成中的结构复杂性问题，在质量和效率方面均优于现有方法，为时间序列分析提供了有前景的解决方案。

Abstract: Generative modeling offers a promising solution to data scarcity and privacy challenges in time series analysis. However, the structural complexity of time series, characterized by multi-scale temporal patterns and heterogeneous components, remains insufficiently addressed. In this work, we propose a structure-disentangled multiscale generation framework for time series. Our approach encodes sequences into discrete tokens at multiple temporal resolutions and performs autoregressive generation in a coarse-to-fine manner, thereby preserving hierarchical dependencies. To tackle structural heterogeneity, we introduce a dual-path VQ-VAE that disentangles trend and seasonal components, enabling the learning of semantically consistent latent representations. Additionally, we present a guidance-based reconstruction strategy, where coarse seasonal signals are utilized as priors to guide the reconstruction of fine-grained seasonal patterns. Experiments on six datasets show that our approach produces higher-quality time series than existing methods. Notably, our model achieves strong performance with a significantly reduced parameter count and exhibits superior capability in generating high-quality long-term sequences. Our implementation is available at https://anonymous.4open.science/r/TimeMAR-BC5B.

</details>


### [113] [FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization](https://arxiv.org/abs/2601.11200)
*Haiyang Xiao,Weiqing Li,Jinyue Guo,Guochao Jiang,Guohua Liu,Yuewei Zhang*

Main category: cs.LG

TL;DR: FAQ是一种基于同家族LLM先验知识的校准数据再生框架，通过生成高保真校准样本来提升后训练量化的精度


<details>
  <summary>Details</summary>
Motivation: 传统PTQ方法依赖有限样本难以捕捉推理阶段的激活分布，导致量化参数偏差，需要解决校准数据的代表性和普适性问题

Method: 利用同家族更大LLM对原始校准样本进行再生，生成包含思维链推理的高保真数据，通过专家指导的组竞争选择最佳样本并进行重归一化

Result: 在Qwen3-8B等多个模型系列上的实验表明，FAQ相比基线将精度损失降低了28.5%

Conclusion: FAQ框架通过利用同家族模型知识生成高质量校准数据，显著提升了PTQ的量化效果，具有强大的应用潜力

Abstract: Although post-training quantization (PTQ) provides an efficient numerical compression scheme for deploying large language models (LLMs) on resource-constrained devices, the representativeness and universality of calibration data remain a core bottleneck in determining the accuracy of quantization parameters. Traditional PTQ methods typically rely on limited samples, making it difficult to capture the activation distribution during the inference phase, leading to biases in quantization parameters. To address this, we propose \textbf{FAQ} (Family-Aware Quantization), a calibration data regeneration framework that leverages prior knowledge from LLMs of the same family to generate high-fidelity calibration samples. Specifically, FAQ first inputs the original calibration samples into a larger LLM from the same family as the target model, regenerating a series of high-fidelity calibration data using a highly consistent knowledge system. Subsequently, this data, carrying Chain-of-Thought reasoning and conforming to the expected activation distribution, undergoes group competition under expert guidance to select the best samples, which are then re-normalized to enhance the effectiveness of standard PTQ. Experiments on multiple model series, including Qwen3-8B, show that FAQ reduces accuracy loss by up to 28.5\% compared to the baseline with original calibration data, demonstrating its powerful potential and contribution.

</details>


### [114] [SDFLoRA: Selective Dual-Module LoRA for Federated Fine-tuning with Heterogeneous Clients](https://arxiv.org/abs/2601.11219)
*Zhikang Shen,Jianrong Lu,Haiyuan Wan,Jianhai Chen*

Main category: cs.LG

TL;DR: 本文提出SDFLoRA方法，通过将客户端适配器分解为全局模块和本地模块，解决联邦学习中LoRA方法在秩异构情况下的聚合偏差和不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的LoRA方法在面临不同客户端使用不同低秩配置（秩异构）时，直接聚合更新会导致偏差和不稳定。现有解决方案要么强制统一秩，要么将异构更新对齐到共享子空间，这会过度约束客户端特定语义，限制个性化，并在差分隐私噪声下提供较弱的本地信息保护。

Method: SDFLoRA将每个客户端适配器分解为捕获可迁移知识的全局模块和保留客户端特定适应的本地模块。全局模块在客户端间选择性对齐和聚合，而本地模块保持私有。该设计支持在秩异构下的鲁棒学习，并通过仅在全局模块中注入差分隐私噪声来实现隐私感知优化。

Result: 在GLUE基准测试上的实验表明，SDFLoRA优于代表性的联邦LoRA基线方法，并实现了更好的效用-隐私权衡。

Conclusion: SDFLoRA通过选择性双模块设计有效解决了联邦LoRA中的秩异构问题，在保持个性化能力的同时提供了更好的隐私保护。

Abstract: Federated learning (FL) for large language models (LLMs) has attracted increasing attention as a way to enable privacy-preserving adaptation over distributed data. Parameter-efficient methods such as LoRA are widely adopted to reduce communication and memory costs. Despite these advances, practical FL deployments often exhibit rank heterogeneity, since different clients may use different low-rank configurations. This makes direct aggregation of LoRA updates biased and unstable. Existing solutions typically enforce unified ranks or align heterogeneous updates into a shared subspace, which over-constrains client-specific semantics, limits personalization, and provides weak protection of local client information under differential privacy noise. To address this issue, we propose Selective Dual-module Federated LoRA (SDFLoRA), which decomposes each client adapter into a global module that captures transferable knowledge and a local module that preserves client-specific adaptations. The global module is selectively aligned and aggregated across clients, while local modules remain private. This design enables robust learning under rank heterogeneity and supports privacy-aware optimization by injecting differential privacy noise exclusively into the global module. Experiments on GLUE benchmarks demonstrate that SDFLoRA outperforms representative federated LoRA baselines and achieves a better utility-privacy trade-off.

</details>


### [115] [Operator learning on domain boundary through combining fundamental solution-based artificial data and boundary integral techniques](https://arxiv.org/abs/2601.11222)
*Haochen Wu,Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: 提出MAD-BNO框架，仅使用边界数据学习线性偏微分方程的边界到边界映射，通过边界积分公式高效恢复内部解，无需全域采样或数值模拟


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法通常需要全域采样数据，计算成本高。本文旨在开发仅依赖边界数据的完全数据驱动方法，减少对数值模拟的依赖

Method: 结合MAD方法从基本解合成物理一致的数据，学习边界上的Dirichlet-Neumann数据对映射，通过边界积分公式恢复内部解

Result: 在二维Laplace、Poisson和Helmholtz方程上验证，达到或优于现有神经算子方法的精度，同时显著减少训练时间

Conclusion: MAD-BNO框架为线性偏微分方程提供高效的数据驱动求解方法，易于扩展到三维问题和复杂几何形状

Abstract: For linear partial differential equations with known fundamental solutions, this work introduces a novel operator learning framework that relies exclusively on domain boundary data, including solution values and normal derivatives, rather than full-domain sampling. By integrating the previously developed Mathematical Artificial Data (MAD) method, which enforces physical consistency, all training data are synthesized directly from the fundamental solutions of the target problems, resulting in a fully data-driven pipeline without the need for external measurements or numerical simulations. We refer to this approach as the Mathematical Artificial Data Boundary Neural Operator (MAD-BNO), which learns boundary-to-boundary mappings using MAD-generated Dirichlet-Neumann data pairs. Once trained, the interior solution at arbitrary locations can be efficiently recovered through boundary integral formulations, supporting Dirichlet, Neumann, and mixed boundary conditions as well as general source terms. The proposed method is validated on benchmark operator learning tasks for two-dimensional Laplace, Poisson, and Helmholtz equations, where it achieves accuracy comparable to or better than existing neural operator approaches while significantly reducing training time. The framework is naturally extensible to three-dimensional problems and complex geometries.

</details>


### [116] [Knowledge is Not Enough: Injecting RL Skills for Continual Adaptation](https://arxiv.org/abs/2601.11258)
*Pingzhi Tang,Yiding Wang,Muhan Zhang*

Main category: cs.LG

TL;DR: PaST框架通过提取领域无关的技能向量，实现高效的知识适应，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs的知识截止问题，传统SFT方法无法有效提升模型使用新知识进行问答和决策的能力，而RL方法计算成本过高。

Method: 提出参数化技能转移(PaST)框架，从源域提取领域无关的技能向量，在目标模型进行轻量级SFT后线性注入知识操作技能。

Result: 在SQuAD上比最先进的自编辑SFT基线提升9.9分；在LooGLE长文本问答上获得8.0分绝对准确率提升；在ToolBench工具使用基准上平均提升10.3分成功率。

Conclusion: PaST方法展示了强大的可扩展性和跨领域迁移能力，技能向量具有一致性的跨工具类别增益。

Abstract: Large Language Models (LLMs) face the "knowledge cutoff" challenge, where their frozen parametric memory prevents direct internalization of new information. While Supervised Fine-Tuning (SFT) is commonly used to update model knowledge, it often updates factual content without reliably improving the model's ability to use the newly incorporated information for question answering or decision-making. Reinforcement Learning (RL) is essential for acquiring reasoning skills; however, its high computational cost makes it impractical for efficient online adaptation. We empirically observe that the parameter updates induced by SFT and RL are nearly orthogonal. Based on this observation, we propose Parametric Skill Transfer (PaST), a framework that supports modular skill transfer for efficient and effective knowledge adaptation. By extracting a domain-agnostic Skill Vector from a source domain, we can linearly inject knowledge manipulation skills into a target model after it has undergone lightweight SFT on new data. Experiments on knowledge-incorporation QA (SQuAD, LooGLE) and agentic tool-use benchmarks (ToolBench) demonstrate the effectiveness of our method. On SQuAD, PaST outperforms the state-of-the-art self-editing SFT baseline by up to 9.9 points. PaST further scales to long-context QA on LooGLE with an 8.0-point absolute accuracy gain, and improves zero-shot ToolBench success rates by +10.3 points on average with consistent gains across tool categories, indicating strong scalability and cross-domain transferability of the Skill Vector.

</details>


### [117] [Latent Dynamics Graph Convolutional Networks for model order reduction of parameterized time-dependent PDEs](https://arxiv.org/abs/2601.11259)
*Lorenzo Tomada,Federico Pichi,Gianluigi Rozza*

Main category: cs.LG

TL;DR: 本文提出了一种名为LD-GCN的编码器自由图神经网络架构，用于参数化偏微分方程的非线性模型降阶，能够在保持几何归纳偏置的同时实现可解释的潜在动力学建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾几何归纳偏置和可解释的潜在行为，要么忽略动力学驱动特征，要么忽视空间信息。本文旨在填补这一空白。

Method: 采用编码器自由的纯数据驱动架构，在潜在空间中建模时间演化并通过时间步进推进，使用GNN将轨迹一致解码到几何参数化域上。

Result: 该方法支持时间外推和零样本预测，通过数学上的通用逼近定理验证，并在复杂计算力学问题上进行了数值测试，包括Navier-Stokes方程的分岔现象检测。

Conclusion: LD-GCN框架成功结合了几何归纳偏置和可解释的潜在动力学，为参数化PDE的非线性模型降阶提供了有效的解决方案。

Abstract: Graph Neural Networks (GNNs) are emerging as powerful tools for nonlinear Model Order Reduction (MOR) of time-dependent parameterized Partial Differential Equations (PDEs). However, existing methodologies struggle to combine geometric inductive biases with interpretable latent behavior, overlooking dynamics-driven features or disregarding spatial information. In this work, we address this gap by introducing Latent Dynamics Graph Convolutional Network (LD-GCN), a purely data-driven, encoder-free architecture that learns a global, low-dimensional representation of dynamical systems conditioned on external inputs and parameters. The temporal evolution is modeled in the latent space and advanced through time-stepping, allowing for time-extrapolation, and the trajectories are consistently decoded onto geometrically parameterized domains using a GNN. Our framework enhances interpretability by enabling the analysis of the reduced dynamics and supporting zero-shot prediction through latent interpolation. The methodology is mathematically validated via a universal approximation theorem for encoder-free architectures, and numerically tested on complex computational mechanics problems involving physical and geometric parameters, including the detection of bifurcating phenomena for Navier-Stokes equations. Code availability: https://github.com/lorenzotomada/ld-gcn-rom

</details>


### [118] [Sample-Near-Optimal Agnostic Boosting with Improved Running Time](https://arxiv.org/abs/2601.11265)
*Arthur da Cunha,Miakel Møller Høgsgaard,Andrea Paudice*

Main category: cs.LG

TL;DR: 本文提出了首个具有近似最优样本复杂度的不可知增强算法，该算法在固定其他参数时，运行时间与样本大小呈多项式关系。


<details>
  <summary>Details</summary>
Motivation: 增强方法能够将弱学习器转化为高精度的强学习器。虽然在经典设置下增强方法已被充分理解，但在不可知设置下（即不对数据做任何假设）的研究较少。现有算法虽然达到了近似最优的样本复杂度，但运行时间是指数级的。

Method: 设计了一种新的不可知增强算法，该算法在样本复杂度上达到近似最优，并且在固定其他参数的情况下，运行时间是样本大小的多项式函数。

Result: 该算法是首个在不可知设置下同时实现近似最优样本复杂度和多项式运行时间的增强方法。

Conclusion: 这项工作填补了不可知增强算法在高效运行时间方面的空白，为实际应用提供了可行的解决方案。

Abstract: Boosting is a powerful method that turns weak learners, which perform only slightly better than random guessing, into strong learners with high accuracy. While boosting is well understood in the classic setting, it is less so in the agnostic case, where no assumptions are made about the data. Indeed, only recently was the sample complexity of agnostic boosting nearly settled arXiv:2503.09384, but the known algorithm achieving this bound has exponential running time. In this work, we propose the first agnostic boosting algorithm with near-optimal sample complexity, running in time polynomial in the sample size when considering the other parameters of the problem fixed.

</details>


### [119] [Metabolomic Biomarker Discovery for ADHD Diagnosis Using Interpretable Machine Learning](https://arxiv.org/abs/2601.11283)
*Nabil Belacel,Mohamed Rachid Boulassel*

Main category: cs.LG

TL;DR: 该研究通过尿液代谢组学和可解释机器学习开发了一种基于生物标志物的ADHD客观诊断方法，使用14种代谢物实现了高达0.97的AUC值。


<details>
  <summary>Details</summary>
Motivation: ADHD作为一种常见的神经发育障碍，目前缺乏客观的诊断工具，迫切需要基于生物学的精准诊断框架。

Method: 整合尿液代谢组学和可解释机器学习框架，使用Closest Resemblance分类器分析52名ADHD患者和46名对照者的代谢组数据。

Result: CR模型优于其他分类器，基于14种代谢物实现AUC>0.97，这些代谢物涉及多巴胺能神经传递和氨基酸代谢通路。

Conclusion: 该研究展示了代谢组学与可解释机器学习相结合的可转化框架，为ADHD的客观、生物学知情诊断策略提供了支持。

Abstract: Attention Deficit Hyperactivity Disorder (ADHD) is a prevalent neurodevelopmental disorder with limited objective diagnostic tools, highlighting the urgent need for objective, biology-based diagnostic frameworks in precision psychiatry. We integrate urinary metabolomics with an interpretable machine learning framework to identify biochemical signatures associated with ADHD. Targeted metabolomic profiles from 52 ADHD and 46 control participants were analyzed using a Closest Resemblance (CR) classifier with embedded feature selection. The CR model outperformed Random Forest and K-Nearest Neighbor classifiers, achieving an AUC > 0.97 based on a reduced panel of 14 metabolites. These metabolites including dopamine 4-sulfate, N-acetylaspartylglutamic acid, and citrulline map to dopaminergic neurotransmission and amino acid metabolism pathways, offering mechanistic insight into ADHD pathophysiology. The CR classifier's transparent decision boundaries and low computational cost support integration into targeted metabolomic assays and future point of care diagnostic platforms. Overall, this work demonstrates a translational framework combining metabolomics and interpretable machine learning to advance objective, biologically informed diagnostic strategies for ADHD.

</details>


### [120] [FORESTLLM: Large Language Models Make Random Forest Great on Few-shot Tabular Learning](https://arxiv.org/abs/2601.11311)
*Zhihan Yang,Jiaqi Wei,Xiang Zhang,Haoyu Dong,Yiwen Wang,Xiaoke Guo,Pengkun Zhang,Yiwei Xu,Chenyu You*

Main category: cs.LG

TL;DR: FORESTLLM是一个结合决策树结构偏置和LLM语义推理能力的新框架，在少样本场景下实现高性能，训练时使用LLM作为离线模型设计器，测试时无需LLM推理。


<details>
  <summary>Details</summary>
Motivation: 解决传统树方法在少样本情况下因统计纯度指标不稳定而容易过拟合的问题，以及直接应用LLM忽略表格数据固有结构导致性能不佳的挑战。

Method: 1. 语义分割准则：LLM基于标记和未标记数据的连贯性评估候选分割；2. 一次性上下文推理机制：LLM将决策路径和支持样本提炼为确定性预测。

Result: 在多样化的少样本分类和回归基准测试中，FORESTLLM实现了最先进的性能。

Conclusion: FORESTLLM成功统一了决策森林的结构偏置和LLM的语义推理能力，在少样本场景下表现出色，且测试时无需LLM推理，具有轻量化和可解释性优势。

Abstract: Tabular data high-stakes critical decision-making in domains such as finance, healthcare, and scientific discovery. Yet, learning effectively from tabular data in few-shot settings, where labeled examples are scarce, remains a fundamental challenge. Traditional tree-based methods often falter in these regimes due to their reliance on statistical purity metrics, which become unstable and prone to overfitting with limited supervision. At the same time, direct applications of large language models (LLMs) often overlook its inherent structure, leading to suboptimal performance. To overcome these limitations, we propose FORESTLLM, a novel framework that unifies the structural inductive biases of decision forests with the semantic reasoning capabilities of LLMs. Crucially, FORESTLLM leverages the LLM only during training, treating it as an offline model designer that encodes rich, contextual knowledge into a lightweight, interpretable forest model, eliminating the need for LLM inference at test time. Our method is two-fold. First, we introduce a semantic splitting criterion in which the LLM evaluates candidate partitions based on their coherence over both labeled and unlabeled data, enabling the induction of more robust and generalizable tree structures under few-shot supervision. Second, we propose a one-time in-context inference mechanism for leaf node stabilization, where the LLM distills the decision path and its supporting examples into a concise, deterministic prediction, replacing noisy empirical estimates with semantically informed outputs. Across a diverse suite of few-shot classification and regression benchmarks, FORESTLLM achieves state-of-the-art performance.

</details>


### [121] [Unlocking the Potentials of Retrieval-Augmented Generation for Diffusion Language Models](https://arxiv.org/abs/2601.11342)
*Chuanyue Yu,Jiahui Wang,Yuhan Li,Heng Chang,Ge Lan,Qingyun Sun,Jia Li,Jianxin Li,Ziwei Zhang*

Main category: cs.LG

TL;DR: 本文提出SPREAD框架，通过查询相关性引导的去噪策略解决扩散语言模型在检索增强生成中的语义漂移问题，显著提高了生成精度。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在自然语言处理任务中表现出色，但其在检索增强生成框架中的潜力尚未被充分探索，主要由于LLM和DLM解码机制的根本差异。研究发现DLM+RAG组合存在响应语义漂移问题。

Method: 提出SPREAD框架，引入查询相关性引导的去噪策略，主动引导去噪轨迹，确保生成内容与查询语义保持一致，有效抑制语义漂移。

Result: 实验结果表明，SPREAD显著提高了RAG框架下生成答案的精度，有效缓解了响应语义漂移问题。

Conclusion: SPREAD框架成功解决了扩散语言模型在检索增强生成中的语义对齐问题，为DLM在RAG中的应用提供了有效解决方案。

Abstract: Diffusion Language Models (DLMs) have recently demonstrated remarkable capabilities in natural language processing tasks. However, the potential of Retrieval-Augmented Generation (RAG), which shows great successes for enhancing large language models (LLMs), has not been well explored, due to the fundamental difference between LLM and DLM decoding. To fill this critical gap, we systematically test the performance of DLMs within the RAG framework. Our findings reveal that DLMs coupled with RAG show promising potentials with stronger dependency on contextual information, but suffer from limited generation precision. We identify a key underlying issue: Response Semantic Drift (RSD), where the generated answer progressively deviates from the query's original semantics, leading to low precision content. We trace this problem to the denoising strategies in DLMs, which fail to maintain semantic alignment with the query throughout the iterative denoising process. To address this, we propose Semantic-Preserving REtrieval-Augmented Diffusion (SPREAD), a novel framework that introduces a query-relevance-guided denoising strategy. By actively guiding the denoising trajectory, SPREAD ensures the generation remains anchored to the query's semantics and effectively suppresses drift. Experimental results demonstrate that SPREAD significantly enhances the precision and effectively mitigates RSD of generated answers within the RAG framework.

</details>


### [122] [FEATHer: Fourier-Efficient Adaptive Temporal Hierarchy Forecaster for Time-Series Forecasting](https://arxiv.org/abs/2601.11350)
*Jaehoon Lee,Seungwoo Lee,Younghwi Kim,Dohee Kim,Sunghyun Sim*

Main category: cs.LG

TL;DR: FEATHer是一种用于边缘设备时间序列预测的超轻量级模型，在参数限制为几千个的情况下实现了准确的长时预测，在8个基准测试中取得60项第一，平均排名2.05。


<details>
  <summary>Details</summary>
Motivation: 工业领域边缘设备（如PLC、微控制器）存在严格的延迟和内存限制，传统深度架构无法适用，需要开发能在极少量参数下实现可靠长时预测的模型。

Method: 提出傅里叶高效自适应时间层次预测器（FEATHer），包含：超轻量级多尺度频率分解、共享密集时间核、频率感知分支门控和稀疏周期核，采用投影-深度卷积-投影结构，无需循环或注意力机制。

Result: 模型参数可低至400个，在8个基准测试中取得最佳排名，获得60项第一，平均排名2.05，显著优于基线方法。

Conclusion: FEATHer证明了在受限边缘硬件上实现可靠长时预测的可行性，为工业实时推理提供了实用方向。

Abstract: Time-series forecasting is fundamental in industrial domains like manufacturing and smart factories. As systems evolve toward automation, models must operate on edge devices (e.g., PLCs, microcontrollers) with strict constraints on latency and memory, limiting parameters to a few thousand. Conventional deep architectures are often impractical here. We propose the Fourier-Efficient Adaptive Temporal Hierarchy Forecaster (FEATHer) for accurate long-term forecasting under severe limits. FEATHer introduces: (i) ultra-lightweight multiscale decomposition into frequency pathways; (ii) a shared Dense Temporal Kernel using projection-depthwise convolution-projection without recurrence or attention; (iii) frequency-aware branch gating that adaptively fuses representations based on spectral characteristics; and (iv) a Sparse Period Kernel reconstructing outputs via period-wise downsampling to capture seasonality. FEATHer maintains a compact architecture (as few as 400 parameters) while outperforming baselines. Across eight benchmarks, it achieves the best ranking, recording 60 first-place results with an average rank of 2.05. These results demonstrate that reliable long-range forecasting is achievable on constrained edge hardware, offering a practical direction for industrial real-time inference.

</details>


### [123] [Offline Reinforcement-Learning-Based Power Control for Application-Agnostic Energy Efficiency](https://arxiv.org/abs/2601.11352)
*Akhilesh Raj,Swann Perarnau,Aniruddha Gokhale,Solomon Bekele Abera*

Main category: cs.LG

TL;DR: 本文探讨了使用离线强化学习设计自主CPU功耗控制器，以提高并行应用的能源效率，同时避免在线训练带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现代计算基础设施设计中，能源效率已成为关键因素。虽然强化学习看似适合设计能源效率控制系统，但在线训练存在模拟环境建模困难、噪声干扰和可靠性问题等挑战。

Method: 采用离线强化学习方法，利用预先收集的状态转换数据集进行训练，结合在线应用无关性能数据和硬件性能计数器，实现灰色盒能源效率控制。

Result: 在多种计算密集型和内存密集型基准测试中，通过Intel的RAPL控制实时系统功耗，证明离线训练的控制代理能显著降低能耗，同时性能下降在可接受范围内。

Conclusion: 离线强化学习是设计自主CPU功耗控制器的有效替代方案，能够在保证性能的前提下显著提升能源效率。

Abstract: Energy efficiency has become an integral aspect of modern computing infrastructure design, impacting the performance, cost, scalability, and durability of production systems. The incorporation of power actuation and sensing capabilities in CPU designs is indicative of this, enabling the deployment of system software that can actively monitor and adjust energy consumption and performance at runtime. While reinforcement learning (RL) would seem ideal for the design of such energy efficiency control systems, online training presents challenges ranging from the lack of proper models for setting up an adequate simulated environment, to perturbation (noise) and reliability issues, if training is deployed on a live system.
  In this paper we discuss the use of offline reinforcement learning as an alternative approach for the design of an autonomous CPU power controller, with the goal of improving the energy efficiency of parallel applications at runtime without unduly impacting their performance. Offline RL sidesteps the issues incurred by online RL training by leveraging a dataset of state transitions collected from arbitrary policies prior to training.
  Our methodology applies offline RL to a gray-box approach to energy efficiency, combining online application-agnostic performance data (e.g., heartbeats) and hardware performance counters to ensure that the scientific objectives are met with limited performance degradation. Evaluating our method on a variety of compute-bound and memory-bound benchmarks and controlling power on a live system through Intel's Running Average Power Limit, we demonstrate that such an offline-trained agent can substantially reduce energy consumption at a tolerable performance degradation cost.

</details>


### [124] [Latent Space Inference via Paired Autoencoders](https://arxiv.org/abs/2601.11397)
*Emma Hart,Bas Peters,Julianne Chung,Matthias Chung*

Main category: cs.LG

TL;DR: 本文提出了一种基于配对自动编码器的数据驱动潜在空间推理框架，用于处理反问题求解中的观测不一致性。该方法通过两个自动编码器（参数空间和观测空间）及其潜在空间之间的学习映射，实现低维信息潜在空间中的正则化反演和优化。


<details>
  <summary>Details</summary>
Motivation: 解决反问题中观测数据不一致（如部分、噪声或分布外数据）的挑战，同时保持与底层物理模型的一致性。

Method: 使用两个自动编码器分别处理参数空间和观测空间，通过学习两个潜在空间之间的映射关系，构建正则化反演的代理模型。框架能够重建损坏数据并用于参数估计。

Result: 与单独使用配对自动编码器和相同架构的端到端编码器-解码器相比，该方法在数据不一致场景下能产生更准确的重建结果。在医学层析成像和地球物理地震波形反演两个成像示例中验证了有效性。

Conclusion: 该框架能够灵活处理各种数据不一致情况，在科学和工程应用的各种反问题中具有广泛适用性。

Abstract: This work describes a novel data-driven latent space inference framework built on paired autoencoders to handle observational inconsistencies when solving inverse problems. Our approach uses two autoencoders, one for the parameter space and one for the observation space, connected by learned mappings between the autoencoders' latent spaces. These mappings enable a surrogate for regularized inversion and optimization in low-dimensional, informative latent spaces. Our flexible framework can work with partial, noisy, or out-of-distribution data, all while maintaining consistency with the underlying physical models. The paired autoencoders enable reconstruction of corrupted data, and then use the reconstructed data for parameter estimation, which produces more accurate reconstructions compared to paired autoencoders alone and end-to-end encoder-decoders of the same architecture, especially in scenarios with data inconsistencies. We demonstrate our approaches on two imaging examples in medical tomography and geophysical seismic-waveform inversion, but the described approaches are broadly applicable to a variety of inverse problems in scientific and engineering applications.

</details>


### [125] [Factored Value Functions for Graph-Based Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.11401)
*Ahmed Rashwan,Keith Briggs,Chris Budd,Lisa Kreusser*

Main category: cs.LG

TL;DR: 该论文提出扩散价值函数（DVF）来解决多智能体强化学习中图结构环境下的信用分配问题，通过在图结构上扩散奖励来构建分解的价值函数，并开发了相应的算法DA2C和LD-GNN。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，特别是具有结构化局部交互的大规模系统中，信用分配是一个核心挑战。现有的全局价值函数为每个智能体提供的学习信号较弱，而局部构造方法在无限时域设置中难以估计且表现不佳。

Method: 引入扩散价值函数（DVF），通过在图结构上结合时间折扣和空间衰减来扩散奖励，为每个智能体分配价值分量。基于DVF提出扩散A2C（DA2C）算法和稀疏消息传递actor（LD-GNN），用于在通信成本下学习去中心化算法。

Result: 在消防基准测试和三个分布式计算任务（向量图着色和两个传输功率优化问题）上，DA2C持续优于局部和全局critic基线，平均奖励提升高达11%。

Conclusion: DVF为图马尔可夫决策过程提供了一种分解的价值函数构造方法，具有良好的理论性质，并可以无缝集成到标准RL算法中，通过图神经网络进行可扩展估计。

Abstract: Credit assignment is a core challenge in multi-agent reinforcement learning (MARL), especially in large-scale systems with structured, local interactions. Graph-based Markov decision processes (GMDPs) capture such settings via an influence graph, but standard critics are poorly aligned with this structure: global value functions provide weak per-agent learning signals, while existing local constructions can be difficult to estimate and ill-behaved in infinite-horizon settings. We introduce the Diffusion Value Function (DVF), a factored value function for GMDPs that assigns to each agent a value component by diffusing rewards over the influence graph with temporal discounting and spatial attenuation. We show that DVF is well-defined, admits a Bellman fixed point, and decomposes the global discounted value via an averaging property. DVF can be used as a drop-in critic in standard RL algorithms and estimated scalably with graph neural networks. Building on DVF, we propose Diffusion A2C (DA2C) and a sparse message-passing actor, Learned DropEdge GNN (LD-GNN), for learning decentralised algorithms under communication costs. Across the firefighting benchmark and three distributed computation tasks (vector graph colouring and two transmit power optimisation problems), DA2C consistently outperforms local and global critic baselines, improving average reward by up to 11%.

</details>


### [126] [Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families](https://arxiv.org/abs/2601.11428)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 本文提出了一个系统性的压力测试框架，用于评估傅里叶神经算子（FNOs）在五种不同类型PDE家族中的鲁棒性，揭示了其在分布偏移、边界条件变化和分辨率外推等场景下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 傅里叶神经算子在求解偏微分方程方面表现出色，但其在分布偏移、长时程推演和结构扰动下的鲁棒性尚未得到充分研究。本文旨在系统性地探究FNOs的失效模式。

Method: 设计了控制性压力测试，包括参数偏移、边界条件变化、分辨率外推与谱分析、迭代推演等，对1,000个训练模型进行大规模评估，涵盖五种PDE家族（色散、椭圆、多尺度流体、金融和混沌系统）。

Result: 参数或边界条件的分布偏移可使误差增加一个数量级；分辨率变化主要导致高频模态误差集中；输入扰动通常不会放大误差，但局部泊松扰动等最坏情况仍具挑战性。

Conclusion: 研究结果提供了操作学习的失效模式图谱和可操作的改进见解，为提升算子学习的鲁棒性指明了方向。

Abstract: Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive, elliptic, multi-scale fluid, financial, and chaotic systems. Rather than optimizing in-distribution accuracy, we design controlled stress tests--including parameter shifts, boundary or terminal condition changes, resolution extrapolation with spectral analysis, and iterative rollouts--to expose vulnerabilities such as spectral bias, compounding integration errors, and overfitting to restricted boundary regimes. Our large-scale evaluation (1{,}000 trained models) reveals that distribution shifts in parameters or boundary conditions can inflate errors by more than an order of magnitude, while resolution changes primarily concentrate error in high-frequency modes. Input perturbations generally do not amplify error, though worst-case scenarios (e.g., localized Poisson perturbations) remain challenging. These findings provide a comparative failure-mode atlas and actionable insights for improving robustness in operator learning.

</details>


### [127] [When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models](https://arxiv.org/abs/2601.11444)
*Raphaël Razafindralambo,Rémy Sun,Frédéric Precioso,Damien Garreau,Pierre-Alexandre Mattei*

Main category: cs.LG

TL;DR: 本文研究了在无条件基于分数的扩散模型中应用集成方法的效果，发现虽然集成能改善分数匹配损失和模型似然，但并不能一致提升图像质量指标如FID。


<details>
  <summary>Details</summary>
Motivation: 尽管集成是改进监督模型的常用方法，但在无条件分数扩散模型中的应用尚未充分探索，本文旨在验证集成是否能为生成建模带来实际益处。

Method: 使用深度集成、蒙特卡洛dropout等方法，在CIFAR-10和FFHQ数据集上进行广泛实验，比较不同聚合规则的效果，并通过理论分析探讨分数模型求和的性质。

Result: 集成分数通常能改善分数匹配损失和模型似然，但无法一致提升FID等感知质量指标；在表格数据中，某些聚合策略表现优于其他方法。

Conclusion: 集成方法在扩散模型中的效果有限，研究为理解模型组合技术（如引导）提供了理论洞见，揭示了分数估计与图像质量之间的复杂关系。

Abstract: Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).

</details>


### [128] [Inter-patient ECG Arrhythmia Classification with LGNs and LUTNs](https://arxiv.org/abs/2601.11433)
*Wout Mommen,Lars Keuninckx,Paul Detterer,Achiel Colpaert,Piet Wambacq*

Main category: cs.LG

TL;DR: 本文提出使用深度可微分逻辑门网络(LGNs)和查找表网络(LUTNs)进行心电图自动分类，在MIT-BIH心律失常数据集上达到94.28%的准确率，计算量比现有方法低3-6个数量级，功耗仅5-7mW，适用于心脏植入设备或可穿戴设备。


<details>
  <summary>Details</summary>
Motivation: 开发能够在极低功耗下高效运行的心律失常检测方法，适用于心脏植入设备和可穿戴设备，特别是针对训练集未包含的患者进行准确分类。

Method: 使用深度可微分逻辑门网络和查找表网络，采用新型预处理方法，首次在LGNs和LUTNs中使用速率编码，并设计了基于多路复用器布尔方程的查找表训练方法。

Result: 在MIT-BIH心律失常数据集四分类问题上达到94.28%准确率和0.683的jκ指数，计算量仅2.89k-6.17k FLOPs，FPGA实现需要2000-2990个LUTs，功耗5-7mW（每推理50-70pJ）。

Conclusion: LGNs和LUTNs在心律失常检测方面表现出色，具有极低功耗和高速度的优势，适合用于心脏植入设备和可穿戴设备的实时监测应用。

Abstract: Deep Differentiable Logic Gate Networks (LGNs) and Lookup Table Networks (LUTNs) are demonstrated to be suitable for the automatic classification of electrocardiograms (ECGs) using the inter-patient paradigm. The methods are benchmarked using the MIT-BIH arrhythmia data set, achieving up to 94.28% accuracy and a $jκ$ index of 0.683 on a four-class classification problem. Our models use between 2.89k and 6.17k FLOPs, including preprocessing and readout, which is three to six orders of magnitude less compared to SOTA methods. A novel preprocessing method is utilized that attains superior performance compared to existing methods for both the mixed-patient and inter-patient paradigms. In addition, a novel method for training the Lookup Tables (LUTs) in LUTNs is devised that uses the Boolean equation of a multiplexer (MUX). Additionally, rate coding was utilized for the first time in these LGNs and LUTNs, enhancing the performance of LGNs. Furthermore, it is the first time that LGNs and LUTNs have been benchmarked on the MIT-BIH arrhythmia dataset using the inter-patient paradigm. Using an Artix 7 FPGA, between 2000 and 2990 LUTs were needed, and between 5 to 7 mW (i.e. 50 pJ to 70 pJ per inference) was estimated for running these models. The performance in terms of both accuracy and $jκ$-index is significantly higher compared to previous LGN results. These positive results suggest that one can utilize LGNs and LUTNs for the detection of arrhythmias at extremely low power and high speeds in heart implants or wearable devices, even for patients not included in the training set.

</details>


### [129] [GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2601.11440)
*Francisco Giral,Álvaro Manzano,Ignacio Gómez,Ricardo Vinuesa,Soledad Le Clainche*

Main category: cs.LG

TL;DR: GenDA是一个生成式数据同化框架，利用多尺度图扩散架构从稀疏传感器观测重建城市风场，在未见过的几何形状和风向下无需重新训练即可实现障碍物感知的重建。


<details>
  <summary>Details</summary>
Motivation: 城市风场重建对于评估空气质量、热量扩散和行人舒适度至关重要，但在只有稀疏传感器数据的情况下具有挑战性。现有方法难以处理复杂几何形状和稀疏观测。

Method: 采用基于多尺度图的扩散架构，在CFD模拟上训练。模型将无分类器引导解释为学习后验重建机制：无条件分支学习几何感知的流场先验，传感器条件分支在采样过程中注入观测约束。

Result: 与监督图神经网络基线和经典降阶数据同化方法相比，GenDA将相对均方根误差降低25-57%，结构相似性指数提高23-33%。

Conclusion: 该框架为复杂领域的环境监测提供了一条可扩展的生成式、几何感知数据同化路径。

Abstract: Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\mathrm{Re}\approx2\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.

</details>


### [130] [Low-Rank Key Value Attention](https://arxiv.org/abs/2601.11471)
*James O'Neill,Robert Clancy,Mariia Matskevichus,Fergal Reid*

Main category: cs.LG

TL;DR: LRKV是一种低秩KV适应方法，通过利用注意力头之间的冗余性来减少KV缓存内存，同时保持完整的token级分辨率，在训练和自回归解码中显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: Transformer预训练受到内存和计算资源的限制，其中KV缓存在训练和自回归解码过程中成为主要瓶颈，需要一种有效的方法来减少KV缓存内存而不损失模型性能。

Method: LRKV通过在每个层使用共享的全秩KV投影，并添加低秩、头特定的残差来实现。这种方法在完全共享和完全独立注意力之间提供连续权衡，是标准多头注意力的即插即用替代方案。

Result: 在大规模预训练实验中，LRKV相比标准注意力、MQA/GQA和MLA方法，实现了更快的损失减少、更低的验证困惑度和更强的下游任务性能。在2.5B规模下，LRKV使用约一半的KV缓存就能超越标准注意力性能，并以减少20-25%的训练计算达到同等模型质量。

Conclusion: LRKV是一种实用有效的注意力机制，能够在内存和计算受限的情况下扩展Transformer预训练，同时保持几乎所有的功能头多样性。

Abstract: Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution. Each layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.
  LRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to \textbf{20-25\% less training compute} when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.

</details>


### [131] [Extractive summarization on a CMOS Ising machine](https://arxiv.org/abs/2601.11491)
*Ziqing Zeng,Abhimanyu Kumar,Chris H. Kim,Ulya R. Karpuzcu,Sachin S. Sapatnekar*

Main category: cs.LG

TL;DR: 本文提出了一种在低功耗CMOS耦合振荡器伊辛机（COBI）上实现McDonald式抽取式摘要的方法，通过硬件感知的伊辛公式、随机舍入和分解策略，在保持摘要质量的同时实现了3-4.5倍的运行速度提升和2-3个数量级的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现有的抽取式摘要系统依赖CPU/GPU基础设施，能耗高且不适合资源受限环境中的实时推理。本文探索在低功耗CMOS伊辛机上实现高效摘要的可行性。

Method: 1）提出硬件感知的伊辛公式，解决局部场和耦合项之间的尺度不平衡问题；2）开发完整的ES流水线，包括随机舍入、迭代精炼和分解策略；3）将大规模ES问题分解为可在COBI上高效求解的较小伊辛子问题。

Result: 在CNN/DailyMail数据集上的实验表明，该方法仅使用有限精度的整数耦合伊辛硬件即可生成高质量摘要，相比暴力方法实现3-4.5倍速度提升，能耗降低2-3个数量级，摘要质量具有竞争力。

Conclusion: CMOS伊辛求解器在边缘设备上实现实时、低能耗文本摘要具有巨大潜力，为资源受限环境下的摘要应用提供了可行解决方案。

Abstract: Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constrained environments. In this work, we explore the feasibility of implementing McDonald-style extractive summarization on a low-power CMOS coupled oscillator-based Ising machine (COBI) that supports integer-valued, all-to-all spin couplings. We first propose a hardware-aware Ising formulation that reduces the scale imbalance between local fields and coupling terms, thereby improving robustness to coefficient quantization: this method can be applied to any problem formulation that requires k of n variables to be chosen. We then develop a complete ES pipeline including (i) stochastic rounding and iterative refinement to compensate for precision loss, and (ii) a decomposition strategy that partitions a large ES problem into smaller Ising subproblems that can be efficiently solved on COBI and later combined. Experimental results on the CNN/DailyMail dataset show that our pipeline can produce high-quality summaries using only integer-coupled Ising hardware with limited precision. COBI achieves 3-4.5x runtime speedups compared to a brute-force method, which is comparable to software Tabu search, and two to three orders of magnitude reductions in energy, while maintaining competitive summary quality. These results highlight the potential of deploying CMOS Ising solvers for real-time, low-energy text summarization on edge devices.

</details>


### [132] [QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid](https://arxiv.org/abs/2601.11500)
*Hoang M. Ngo,Tre' R. Jeter,Jung Taek Seo,My T. Thai*

Main category: cs.LG

TL;DR: 本文提出了QUPID和R-QUPID两种量子机器学习模型，用于智能电网异常检测，相比传统机器学习方法具有更好的性能和对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 智能电网需要强大的异常检测方法来应对网络物理威胁和系统故障，但传统机器学习模型难以处理智能电网系统的复杂性且易受对抗性攻击。

Method: 提出了基于分区量子神经网络(PQNN)的QUPID模型，并通过差分隐私增强的R-QUPID模型，采用量子增强特征表示来建模高维智能电网系统。

Result: QUPID和R-QUPID在各种场景下显著优于传统最先进的机器学习模型，在异常检测能力和鲁棒性方面都有显著提升。

Conclusion: 量子机器学习为智能电网异常检测提供了有效解决方案，分区框架解决了QML的可扩展性问题，使量子增强的异常检测在大规模智能电网环境中变得实用。

Abstract: Smart grid infrastructures have revolutionized energy distribution, but their day-to-day operations require robust anomaly detection methods to counter risks associated with cyber-physical threats and system faults potentially caused by natural disasters, equipment malfunctions, and cyber attacks. Conventional machine learning (ML) models are effective in several domains, yet they struggle to represent the complexities observed in smart grid systems. Furthermore, traditional ML models are highly susceptible to adversarial manipulations, making them increasingly unreliable for real-world deployment. Quantum ML (QML) provides a unique advantage, utilizing quantum-enhanced feature representations to model the intricacies of the high-dimensional nature of smart grid systems while demonstrating greater resilience to adversarial manipulation. In this work, we propose QUPID, a partitioned quantum neural network (PQNN) that outperforms traditional state-of-the-art ML models in anomaly detection. We extend our model to R-QUPID that even maintains its performance when including differential privacy (DP) for enhanced robustness. Moreover, our partitioning framework addresses a significant scalability problem in QML by efficiently distributing computational workloads, making quantum-enhanced anomaly detection practical in large-scale smart grid environments. Our experimental results across various scenarios exemplifies the efficacy of QUPID and R-QUPID to significantly improve anomaly detection capabilities and robustness compared to traditional ML approaches.

</details>


### [133] [MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management](https://arxiv.org/abs/2601.11505)
*Miriam K. Wolff,Peter Calhoun,Eleonora Maria Aiello,Yao Qin,Sam F. Royston*

Main category: cs.LG

TL;DR: 该研究整合了多个公开的1型糖尿病数据集，创建了标准化的MetaboNet数据集，包含3135名患者和1228患者年的连续血糖监测和胰岛素泵数据，旨在解决现有数据集碎片化和标准化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前1型糖尿病管理数据集存在碎片化、结构差异大、访问处理耗时等问题，限制了算法开发和数据整合，阻碍了算法性能的可比性和泛化性。

Method: 整合多个公开T1D数据集，要求同时包含连续血糖监测数据和胰岛素泵给药记录，保留碳水化合物摄入和体力活动等辅助信息，建立统一的MetaboNet数据集格式和自动处理流程。

Result: 创建了包含3135名患者、1228患者年数据的MetaboNet数据集，规模远超现有基准数据集，提供公开子集和受限子集两种访问方式，覆盖广泛的血糖特征和人口统计学特征。

Conclusion: MetaboNet数据集为T1D研究提供了统一可访问的数据资源，能够产生比单个数据集更具泛化性的算法性能，推动1型糖尿病算法开发进展。

Abstract: Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unified and accessible data resource for T1D algorithm development. Multiple publicly available T1D datasets were consolidated into a unified resource, termed the MetaboNet dataset. Inclusion required the availability of both continuous glucose monitoring (CGM) data and corresponding insulin pump dosing records. Additionally, auxiliary information such as reported carbohydrate intake and physical activity was retained when present. The MetaboNet dataset comprises 3135 subjects and 1228 patient-years of overlapping CGM and insulin data, making it substantially larger than existing standalone benchmark datasets. The resource is distributed as a fully public subset available for immediate download at https://metabo-net.org/ , and with a Data Use Agreement (DUA)-restricted subset accessible through their respective application processes. For the datasets in the latter subset, processing pipelines are provided to automatically convert the data into the standardized MetaboNet format. A consolidated public dataset for T1D research is presented, and the access pathways for both its unrestricted and DUA-governed components are described. The resulting dataset covers a broad range of glycemic profiles and demographics and thus can yield more generalizable algorithmic performance than individual datasets.

</details>


### [134] [Building Production-Ready Probes For Gemini](https://arxiv.org/abs/2601.11516)
*János Kramár,Joshua Engels,Zheng Wang,Bilal Chughtai,Rohin Shah,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 本文针对前沿语言模型滥用风险，提出改进的激活探针架构以应对长上下文分布偏移问题，并在网络安全领域验证了其鲁棒性，最终成功部署于Gemini模型。


<details>
  <summary>Details</summary>
Motivation: 随着前沿语言模型能力快速提升，需要更强的滥用缓解措施。现有激活探针技术在重要生产分布偏移下泛化能力不足，特别是从短上下文到长上下文的转换。

Method: 提出多种新探针架构处理长上下文分布偏移，采用多最大值方法结合多样化分布训练，并将探针与提示分类器配对使用以提高计算效率。

Result: 在网络安全领域测试表明，新架构在多种生产相关偏移（如多轮对话、静态越狱和自适应红队测试）中表现鲁棒，结合架构选择和多样化训练可实现广泛泛化。

Conclusion: 研究成功指导了Gemini模型中的滥用缓解探针部署，并发现AlphaEvolve在自动改进探针架构搜索和自适应红队测试方面具有潜力，表明AI安全研究的自动化已具备可行性。

Abstract: Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.
  We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.
  These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.

</details>
