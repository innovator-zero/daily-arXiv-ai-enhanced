<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]
- [cs.LG](#cs.LG) [Total: 167]
- [cs.RO](#cs.RO) [Total: 32]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: 研究提出ImgCoder逻辑驱动框架，通过"理解-规划-编码"流程生成科学图像，并创建SciGenBench评估科学正确性，发现像素生成模型的系统缺陷，最终证明使用高质量合成图像微调多模态模型能提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态推理受限于科学严谨图像的合成困难，现有文本到图像模型常产生视觉合理但科学错误的输出，导致视觉逻辑分歧，限制了其在科学推理中的价值。

Method: 1. 系统研究科学图像合成的生成范式、评估和下游应用；2. 分析像素生成和程序化合成两种方法；3. 提出ImgCoder逻辑驱动框架，采用"理解-规划-编码"工作流程提升结构精度；4. 创建SciGenBench评估图像的信息效用和逻辑有效性。

Result: 1. 揭示像素生成模型的系统失败模式；2. 发现表达力与精度之间的基本权衡；3. 证明在严格验证的合成科学图像上微调大型多模态模型能带来一致的推理提升；4. 观察到与文本领域类似的潜在扩展趋势。

Conclusion: 高保真科学合成是解锁大规模多模态推理能力的可行路径，验证了使用高质量合成图像训练模型能显著提升科学推理性能。

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [2] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: 提出了一种新颖的双增强框架，通过空间流形扩展和语义对象注入协同整合，提高医学图像分割的数据利用效率，特别是在标注数据有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割性能越来越取决于数据利用效率而非原始数据量。对于脑膜瘤等复杂病理的精确分割，需要模型充分利用有限高质量标注中的潜在信息。为了最大化现有数据集的价值，需要开发更有效的数据增强方法。

Method: 提出了双增强框架：1）使用隐式神经表示建模连续速度场，通过对积分变形场进行线性混合，在变形空间内插值生成解剖学上合理的变体，实现从少量锚点探索结构多样性；2）引入Sim2Real病灶注入模块，将病灶纹理移植到健康解剖背景中构建高保真模拟域，弥合合成增强与真实病理之间的差距。

Result: 在混合数据集上的综合实验表明，该框架显著提升了nnU-Net和U-Mamba等最先进模型的数据效率和鲁棒性，为有限标注预算下的高性能医学图像分析提供了有效策略。

Conclusion: 该双增强框架通过协同整合空间流形扩展和语义对象注入，有效提高了医学图像分割的数据利用效率，为有限标注资源下的高性能分割提供了有力解决方案。

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [3] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: 提出一种基于外周血涂片图像分析的自动化方法，用于分类计数正常与变形的红细胞，特别针对镰状细胞贫血的诊断。


<details>
  <summary>Details</summary>
Motivation: 目前通过显微镜观察外周血样本监测镰状细胞贫血等疾病耗时且依赖专家，主观性强、错误率高，需要自动化、客观的检测方法。

Method: 使用Chan-Vese主动轮廓模型分割图像中的红细胞，然后通过圆形形状因子和椭圆形状因子等基本形状分析描述符，将红细胞分类为正常、细长或其他变形。对于样本制备中部分遮挡的细胞，采用椭圆调整以分析盘状和细长形状的红细胞。

Result: 该方法在实验中优于现有方法，正常细胞的F-measure值为0.97，细长细胞为0.95，整体多类性能指标优秀，适用于镰状细胞贫血的临床治疗和诊断支持。

Conclusion: 提出的自动化方法能有效、准确地分类计数变形红细胞，为镰状细胞贫血的诊断提供了可靠的临床支持工具。

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [4] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: 本文创建了AMVICC基准测试，系统比较多模态大语言模型和图像生成模型在视觉推理任务中的失败模式，发现两者存在共享和特定的失败模式，图像生成模型在精细视觉属性控制上表现较差。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习快速发展，但视觉语言模型仍然无法理解或生成基本的视觉概念（如物体方向、数量、空间关系），这揭示了基础视觉推理能力的不足，需要系统评估不同模态的失败模式。

Method: 通过将MMVP基准问题转化为显式和隐式提示，创建AMVICC基准测试，在11个MLLM和3个IGM上测试九个视觉推理类别，系统分析跨模态的失败模式。

Result: 失败模式在模型和模态间既有共享也有特定性；IGM在处理显式提示时难以操纵特定视觉组件，对精细视觉属性控制能力差；某些失败是模型特定或模态特定的。

Conclusion: 该工作为未来跨模态对齐研究奠定基础，提供了评估生成和解释失败是否源于共享限制的框架，有助于指导统一视觉语言建模的改进。

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [5] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: 本文提出了一种混合视觉管道，结合深度特征提取和传统机器学习分类器，用于建筑垃圾自动分类，在真实数据集上达到99.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 建筑业产生大量废弃物，有效的分类对可持续废物管理和资源回收至关重要。需要自动化、鲁棒的分类方法应对真实场景中的建筑垃圾管理挑战。

Method: 1. 收集了阿联酋真实建筑工地的1800张高质量平衡图像数据集，包含陶瓷/瓷砖、混凝土、垃圾/废物、木材四类材料；2. 使用预训练的Xception网络提取深度特征；3. 系统评估了SVM、kNN、Bagged Trees、LDA和逻辑回归等多种传统机器学习分类器。

Result: 混合管道（Xception特征+简单分类器）达到最先进性能：最高99.5%准确率和macro-F1分数，优于更复杂或端到端的深度学习方法。Linear SVM、kNN和Bagged Trees表现最佳。

Conclusion: 该方法为鲁棒、可现场部署的建筑垃圾识别提供了操作优势，并为未来与机器人和现场自动化系统集成指明了路径。证明了简单分类器结合深度特征提取在实际应用中的有效性。

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [6] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: 研究人员开发了一个名为MANGO的大型全球红树林数据集，包含42,703个标注的图像-掩码对，覆盖124个国家，旨在解决现有红树林监测数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 红树林对气候变化减缓至关重要，需要可靠的监测以支持有效保护。虽然深度学习已成为红树林检测的有力工具，但现有数据集的局限性阻碍了其发展：许多资源仅提供年度地图产品而没有精心整理的单一日期图像-掩码对，局限于特定区域而非全球覆盖，或无法公开访问。

Method: 研究人员收集了2020年红树林区域所有可用的Sentinel-2影像，并采用目标检测驱动的方法，利用像素级坐标参考来确保自适应和代表性的图像-掩码配对，选择与红树林年度掩码对齐的最佳单一日期观测数据。

Result: 构建了MANGO数据集，包含42,703个标注的图像-掩码对，覆盖124个国家。该数据集为红树林监测提供了大规模、全球覆盖的资源。研究人员还提供了基于国家分离分割的多种语义分割架构基准测试。

Conclusion: MANGO数据集解决了现有红树林监测数据集的局限性，为可扩展和可靠的全球红树林监测奠定了基础。该资源将促进深度学习在红树林检测方面的进展，支持更有效的保护工作。

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [7] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: 该研究提出一个处理15-16世纪拉丁文历史文献的转录流水线，通过布局分析模型提取文本行，再用OCR模型识别，保留特殊字符和符号，保持历史文本原貌。


<details>
  <summary>Details</summary>
Motivation: 15-16世纪拉丁文历史文献的转录面临特殊挑战，需要保留具有特定含义的特殊字符和符号，以确保历史文本保持原有风格和意义。现有方法在处理这些特殊特征方面存在不足。

Method: 提出一个流水线方法：1) 使用布局分析模型分析历史文本图像并提取文本行；2) 通过OCR模型处理提取的文本行，生成完整的数字化页面。该方法扩展了现有的文本行识别方法，集成了布局分析模型。

Result: 实验表明，该流水线能够有效处理页面并产生高效结果。在多个数据集上的评估显示，掩码自编码器能够有效处理不同类型文本，包括手写体、印刷体和多语言文本。

Conclusion: 提出的流水线方法能够有效转录15-16世纪拉丁文历史文献，保留特殊字符和符号，保持历史文本的原始风格和意义，为历史文献数字化提供了有效解决方案。

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [8] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: 该研究探索了结合Leap Motion和RGB摄像头的多模态方法，用于阿拉伯手语识别，取得了78%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯手语识别系统依赖单一传感器（如Leap Motion或RGB摄像头），存在复杂手部方向跟踪不足和3D手部运动识别不精确的问题，需要更有效的多模态方法。

Method: 采用双通道子网络架构：Leap Motion数据使用定制密集神经网络（含dropout和L2正则化），RGB图像使用微调的VGG16模型（增强数据增强技术）。两种模态的特征表示在融合模型中拼接，通过全连接层处理，最后用SoftMax进行分类。

Result: 在包含18个阿拉伯手语单词的自定义数据集上评估，正确识别了13个单词，总体准确率达到78%。

Conclusion: 多模态融合在手语识别中具有可行性，但需要进一步优化和扩展数据集以提高性能。

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [9] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: 提出一种解耦成员矩阵与子空间矩阵的注意力机制DMSA，在视觉任务中比现有方法更高效且可解释


<details>
  <summary>Details</summary>
Motivation: 现有MCR2驱动的白盒transformer中，成员矩阵与子空间矩阵紧密耦合，导致错误token投影下的冗余编码，需要解耦这两个组件以获得更好的性能和可解释性

Method: 解耦MCR2目标中的成员矩阵和子空间U，从优化目标的梯度下降展开推导出可解释的稀疏线性注意力算子DMSA：直接从输入学习成员矩阵，然后从全空间S推导稀疏子空间

Result: 在ToST中用DMSA替换注意力模块（称为DMST），在ImageNet-1K上比ToST提升1.08%-1.45% top-1准确率，编码减少速率更快，计算效率更高，可解释性更强

Conclusion: 通过解耦成员矩阵和子空间矩阵，提出的DMSA注意力机制为视觉建模提供了高效、可解释的白盒transformer解决方案，在性能和效率上都优于现有方法

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [10] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 提出基于语义分割的深度学习方法，从含噪TEM图像中提取3D原子级深度信息


<details>
  <summary>Details</summary>
Motivation: 传统TEM图像分析在噪声干扰下难以准确获取3D原子级信息，需要一种能处理真实噪声的鲁棒深度估计方法

Method: 将深度估计建模为语义分割问题，使用深度卷积神经网络训练像素级深度分割图，利用合成噪声的模拟数据进行训练

Result: 方法在CeO2纳米颗粒的模拟和真实TEM数据上表现出准确、校准良好且对噪声鲁棒的深度估计能力

Conclusion: 提出的语义分割方法能有效从含噪TEM图像中提取可靠的3D原子级深度信息，为材料科学中的原子级表征提供了新工具

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [11] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: Noisomics框架通过对比预训练基础模型，将噪声从抑制对象转变为可解码的信息资源，仅需100个训练样本即可超越传统需要10万样本的方法，实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 传统成像噪声表征方法需要大量数据且设备依赖性强，将噪声视为干扰而非信息源，难以分离物理信号与算法伪影。

Method: 提出Noisomics框架和对比预训练基础模型，利用流形假设和合成噪声基因组，通过对比学习分离语义信号与随机扰动，打破深度学习缩放定律。

Result: 仅用100个训练样本就超越需10万样本的监督基线，数据需求降低3个数量级；在12个跨域数据集上实现零样本泛化，估计误差降低63.8%，决定系数提高85.1%。

Conclusion: 将噪声重新定义为多参数信息足迹而非降级因素，无需设备校准即可实现精确成像诊断，为从消费摄影到深层组织显微镜的应用提供新范式。

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [12] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: SiMiC是一个基于注意力机制卷积神经网络的硅微结构表征框架，用于从SEM图像中自动提取场发射尖端的形态特征，减少人工干预并提高测量一致性。


<details>
  <summary>Details</summary>
Motivation: 传统使用扫描电子显微镜（SEM）分析硅微结构需要大量人工操作，限制了通量和可重复性。需要一种自动化的方法来高效、准确地表征场发射尖端的形态特征。

Method: 开发了专门的硅基场发射尖端数据集，构建了结合注意力机制的自定义CNN架构，用于多类微结构分类和尺寸预测。通过深度学习自动提取大小、形状、顶点曲率等形态特征。

Result: 与传统图像处理技术相比，SiMiC实现了高准确性，同时保持了可解释性。该框架为数据驱动的微结构分析奠定了基础，可直接关联到场发射性能。

Conclusion: SiMiC为将发射器几何形状与发射行为相关联开辟了途径，可指导优化冷阴极和SEM电子源的设计。相关数据集和算法已开源，可作为该领域的基准。

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [13] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: ISAS 2025挑战赛聚焦于使用姿态数据识别发育障碍设施中的异常行为，吸引了40支团队参与，采用LOSO评估策略，结果显示在低维噪声数据中建模罕见、突发行为的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决发育障碍设施中异常行为自动识别的迫切需求，使用非侵入式姿态估计数据，为医疗健康和行为监测的社会责任AI应用提供支持。

Method: 挑战赛使用从视频记录中提取的骨架关键点数据，数据集反映真实世界的行为不平衡和时序不规则性，采用Leave-One-Subject-Out (LOSO)评估策略确保主体无关的泛化能力。

Result: 40支参赛团队采用从经典机器学习到深度学习架构的多种方法，使用宏平均F1分数评估以处理类别不平衡。结果显示在低维噪声数据中建模罕见、突发行为具有挑战性，强调捕捉时间和上下文细微差别的重要性。

Conclusion: 挑战赛强调了在嘈杂、低维数据中建模罕见、突发行为的困难，同时突出了捕捉行为建模中时间和上下文细微差别的重要性。这些见解可能有助于未来医疗健康和行为监测领域社会责任AI应用的发展。

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [14] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种单像素视觉语言模型（SP-VLM），通过低维单像素模态捕捉人类动态，实现隐私保护的异常行为检测，在隐私敏感环境中平衡安全监控与个人隐私保护。


<details>
  <summary>Details</summary>
Motivation: 隐私敏感环境（如卫生间、更衣室）中传统监控受限，但欺凌、骚扰等不良社会互动威胁个人安全和心理健康，需要一种既能保护隐私又能进行安全监控的技术方案。

Method: 提出单像素视觉语言模型（SP-VLM），通过单像素传感捕捉低维人类动态信息，结合视觉语言集成推断复杂行为模式，实现隐私保护设计。

Result: 单像素传感能有效抑制身份可恢复性，使先进人脸识别系统在低于临界采样率时失效；同时SP-VLM能从严重降级的单像素观测中提取有意义的行为语义，实现异常检测、人数统计和活动理解。

Conclusion: 该研究找到了一种实用的采样率机制，能在行为智能出现的同时保持个人身份隐私，为人权友好的安全监控提供了技术路径，支持及时干预而不侵犯隐私敏感空间的隐私权。

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [15] [Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults](https://arxiv.org/abs/2601.17053)
*Shuhao Que,Dieuwke van Dartel,Ilse Heeringa,Han Hegeman,Miriam Vollenbroek-Hutten,Ying Wang*

Main category: cs.CV

TL;DR: 本研究开发了一个基于合成数据指导的特征干预模型（FIM），用于在髋部骨折康复中改善老年患者的连续身体活动识别，特别是在80岁以上老年人群中实现了可靠的活动分类。


<details>
  <summary>Details</summary>
Motivation: 髋部骨折康复期间的身体活动监测对防止老年患者长期功能下降至关重要，但临床实践中很少量化。现有的商业可穿戴活动追踪器通常针对中年人群开发，在步态较慢且多变的老年人群中表现不可靠。

Method: 研究纳入24名80岁以上健康老年人，在模拟自由生活条件下执行日常活动（行走、站立、坐、躺下和姿势转换）75分钟，佩戴位于下背部和前大腿的两个加速度计。使用留一交叉验证评估模型鲁棒性，并利用合成数据改善跨参与者泛化能力。

Result: 特征干预模型（FIM）在合成数据指导下实现了可靠的活动识别：行走平均F1分数0.896、站立0.927、坐0.997、躺下0.937、姿势转换0.816。与无合成数据的对照模型相比，FIM显著改善了具有高临床相关性但常被忽视的姿势转换检测。

Conclusion: 初步结果表明在老年人群中实现鲁棒活动识别的可行性。需要在髋部骨折患者群体中进行进一步验证，以评估所提监测系统的临床效用。

Abstract: Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.

</details>


### [16] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: 本文提出了Ego4OOD基准，用于评估自我中心视频动作识别的域泛化能力，通过控制概念偏移来专注于协变量偏移的测量，并展示了轻量级二元分类方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自我中心域泛化基准常混淆协变量偏移与概念偏移，难以可靠评估模型在输入分布变化下的泛化能力。需要强调可测量的协变量多样性同时减少概念偏移的基准。

Method: 1) 从Ego4D构建Ego4OOD基准，包含八个地理域，使用语义连贯的时刻级动作类别减少概念偏移；2) 提出基于聚类的协变量偏移度量指标；3) 采用一对多二元训练目标，将多类动作识别分解为独立二元分类任务。

Result: 轻量级两层全连接网络在Argo1M和Ego4OOD上达到与最先进方法竞争的性能，参数更少且无需额外模态。实证分析显示测量的协变量偏移与识别性能存在明确关系。

Conclusion: Ego4OOD基准通过控制概念偏移和量化域难度，为研究自我中心视频的分布外泛化提供了重要工具。二元分类方法特别适合处理协变量偏移，在资源受限场景下具有实用价值。

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [17] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: 提出一個端到端計算機視覺系統，用於從射擊線拍攝的圖像中自動檢測彈孔並進行基於迭代的追蹤，專門用於步槍瞄準鏡歸零過程。


<details>
  <summary>Details</summary>
Motivation: 傳統步槍歸零過程需要射手親自檢查靶紙，不僅耗時（受射擊場安全協議限制），還容易引入人為錯誤。需要一個自動化系統來提高效率和準確性。

Method: 結合YOLOv8進行小目標檢測，使用IoU分析區分連續圖像中的彈孔。提出新穎的數據增強技術（移除而非添加物體來模擬實際射擊序列），並引入基於ORB的預處理管道進行透視校正。

Result: 系統在彈孔檢測上達到97.0%的平均精度（mAP），在將彈孔正確分配到對應射擊迭代上達到88.8%的準確率。

Conclusion: 該系統成功實現了步槍歸零過程的自動化，不僅適用於步槍歸零，還可推廣到其他需要區分視覺相似物體時間序列的應用領域。

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [18] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频生成世界模型分类法，旨在弥合传统状态中心世界模型理论与现代"无状态"视频架构之间的差距，并推动评估从视觉保真度转向功能基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视频生成模型已展现出一定的物理一致性，有潜力成为世界模型，但现代"无状态"视频架构与经典状态中心世界模型理论之间存在差距，需要建立系统化的分析框架来指导这一领域的发展。

Method: 提出了以"状态构建"和"动态建模"为核心支柱的新分类法。状态构建分为隐式范式（上下文管理）和显式范式（潜在压缩），动态建模则通过知识整合和架构重构来分析。同时倡导评估体系从视觉保真度转向功能基准测试。

Result: 建立了一个系统化的分析框架，为视频生成世界模型的研究提供了清晰的分类标准和评估方向。该框架有助于理解不同方法的优缺点，并指导未来研究的发展路径。

Conclusion: 视频生成世界模型领域需要重点关注两个前沿方向：通过数据驱动记忆和压缩保真度增强持久性，以及通过潜在因子解耦和推理先验整合推进因果推理。解决这些挑战将使该领域从生成视觉可信的视频发展到构建稳健、通用的世界模拟器。

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [19] [Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances](https://arxiv.org/abs/2601.17071)
*Jisui Huang,Andreas Alpers,Ke Chen,Na Lei*

Main category: cs.CV

TL;DR: 提出一种针对强不均匀图像的高效分割方法，采用两级聚类：先通过离散最优传输将像素聚为超像素，再用Wasserstein距离贪婪合并为对象级分割。


<details>
  <summary>Details</summary>
Motivation: 传统超像素合并方法基于平均颜色距离，在处理强不均匀图像时效果有限。需要一种数学统一的框架，既能处理像素级聚类，又能处理超像素合并。

Method: 两级聚类方法：1) 将像素通过线性最小二乘分配问题（离散最优传输特例）聚为超像素；2) 使用平方2-Wasserstein距离贪婪合并超像素为对象级分割。

Result: 数值实验表明，该方法在挑战性图像上提高了分割精度，同时保持了高计算效率。

Conclusion: 基于最优传输的分布距离框架为图像分割提供了数学统一的解决方案，在强不均匀图像上优于传统方法。

Abstract: We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.

</details>


### [20] [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088)
*Rui-Yang Ju,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: GlassesGB是一个将2D生成式眼镜定制与3D头部虚拟形象渲染相结合的可定制眼镜生成框架，支持VR场景中的个性化眼镜设计。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试戴系统大多基于预定义模板，缺乏细粒度的用户驱动定制能力。虽然GlassesGAN支持2D个性化眼镜设计，但仅限于2D图像生成。为了将2D生成式定制扩展到3D头部虚拟形象，需要解决2D与3D之间的桥梁问题。

Method: 结合3D高斯混合形状头部重建技术，提出GlassesGB框架。该框架整合了2D生成式定制和3D头部虚拟形象渲染，实现了可定制眼镜在3D头部模型上的生成。

Result: GlassesGB成功实现了可定制眼镜在3D头部虚拟形象上的生成，代码已在GitHub上开源。

Conclusion: GlassesGB有效解决了VR应用中个性化眼镜设计的挑战，通过将2D生成式定制与3D头部虚拟形象渲染相结合，为虚拟试戴系统提供了更灵活的定制能力。

Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.

</details>


### [21] [GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing](https://arxiv.org/abs/2601.17089)
*Qigan Sun,Chaoning Zhang,Jianwei Zhang,Xudong Wang,Jiehui Xie,Pengcheng Zheng,Haoyu Wang,Sungyoung Lee,Chi-lok Andy Tai,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出GRASP方法，一种参数高效微调策略，用于解决多模态大语言模型在遥感图像问答任务中的过拟合和细节忽略问题。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法直接应用于遥感图像时存在过拟合背景噪声或忽略目标细节的问题，这主要是由于遥感图像的大尺度变化、目标分布稀疏和复杂区域语义特征等挑战导致的。

Method: 提出GRASP（Guided Region-Aware Sparse Prompting）方法，引入与冻结视觉标记网格中提取的空间块相关联的空间结构化软提示，通过问题引导的稀疏融合机制，动态地将任务特定上下文聚合到紧凑的全局提示中。

Result: 在多个RSVQA基准测试上的广泛实验表明，GRASP相比现有微调和基于提示的方法实现了竞争性性能，同时保持高参数效率。

Conclusion: GRASP方法通过空间感知的稀疏提示机制，能够使模型专注于相关区域并过滤背景噪声，有效解决了遥感图像中的特定挑战，为多模态大语言模型在遥感任务中的应用提供了高效解决方案。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

</details>


### [22] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: 提出基於生成式AI的自動LoD草圖提取框架，能從高細節建築模型漸進簡化生成幾何一致的多層次細節表示，解決AI驅動建築建模缺乏高質量訓練數據的問題。


<details>
  <summary>Details</summary>
Motivation: 傳統建築LoD建模依賴人工操作，耗時耗力且容易產生幾何不一致。雖然生成式AI為從草圖生成多層次建築模型提供了新可能，但缺乏高質量配對的LoD訓練數據限制了其應用。

Method: 提出自動LoD草圖提取框架，整合計算機視覺技術與生成式AI方法，建立從詳細表示到體積抽象的漸進提取流程，能將高細節建築模型逐步簡化生成幾何一致且層次連貫的多LoD表示。

Result: 實驗結果顯示方法在LoD層級間保持強幾何一致性：LoD3到LoD2的SSIM值為0.7319，LoD2到LoD1為0.7532；對應的歸一化Hausdorff距離分別為圖像對角線的25.1%和61.0%，反映抽象過程中的可控幾何偏差。

Conclusion: 該框架在保持全局結構的同時實現了不同LoD層級的漸進語義簡化，為AI驅動的多層次建築生成和分層建模提供了可靠的數據和技術支持。

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [23] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: 医学影像AI性能评估中置信区间(CI)的大规模实证分析，揭示CI可靠性受样本量、性能指标、聚合策略、任务类型和CI方法选择等多种因素影响


<details>
  <summary>Details</summary>
Motivation: 医学影像AI的可靠性验证和临床转化需要性能不确定性量化，置信区间在其中起关键作用。但由于对医学影像中CI行为的研究有限，社区对多种CI方法及其在不同场景下的表现了解不足

Method: 在24个分割和分类任务上进行大规模实证分析，每个任务组使用19个训练模型，涵盖广泛使用的性能指标、多种聚合策略和几种常用CI方法，评估各CI方法的可靠性（覆盖率）和精度（宽度）

Result: 发现五个主要结论：1)可靠CI所需样本量从几十到几千不等；2)CI行为受性能指标选择影响；3)聚合策略显著影响CI可靠性；4)机器学习问题类型（分割vs分类）调节这些影响；5)不同CI方法在不同用例中可靠性不同

Conclusion: 这些结果为未来医学影像AI性能不确定性报告指南的制定提供了关键依据，强调了根据具体研究参数选择合适CI方法的重要性

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [24] [StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors](https://arxiv.org/abs/2601.17107)
*Qinkai Yu,Chong Zhang,Gaojie Jin,Tianjin Huang,Wei Zhou,Wenhui Li,Xiaobo Jin,Bo Huang,Yitian Zhao,Guang Yang,Gregory Y. H. Lip,Yalin Zheng,Aline Villavicencio,Yanda Meng*

Main category: cs.CV

TL;DR: StealthMark是一种针对医学分割模型的黑盒所有权验证方法，通过细微调节模型不确定性而不改变分割输出，利用模型解释方法提取特征归因，在触发条件下显示可验证的QR码水印。


<details>
  <summary>Details</summary>
Motivation: 医学数据标注成本高且受隐私限制，训练好的分割模型成为有价值的知识产权需要保护。现有保护技术主要针对分类和生成任务，分割模型保护研究不足。

Method: 提出StealthMark方法：1) 细微调节模型不确定性但不改变最终分割输出；2) 使用LIME等模型无关解释方法提取特征归因；3) 在特定触发条件下显示QR码水印；4) 黑盒条件下进行所有权验证。

Result: 在四个医学影像数据集和五个主流分割模型上测试，StealthMark有效性高（ASR>95%），隐蔽性好，对原始模型性能影响小（Dice和AUC下降<1%），显著优于基于后门的水印方法。

Conclusion: StealthMark为医学分割模型提供了一种有效、隐蔽、无害的黑盒所有权验证方法，具有实际部署潜力，解决了医学分割模型知识产权保护的空白。

Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.

</details>


### [25] [iFSQ: Improving FSQ for Image Generation with 1 Line of Code](https://arxiv.org/abs/2601.17124)
*Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan*

Main category: cs.CV

TL;DR: iFSQ通过简单的激活函数替换解决了FSQ量化中的激活崩溃问题，实现了离散与连续表示的统一建模，发现4比特维度是离散与连续表示的最佳平衡点，AR模型收敛快但扩散模型上限更高。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成领域存在自回归模型（基于离散token）和扩散模型（基于连续潜在变量）的分裂，这种分裂源于VQ-VAEs和VAEs的区别，阻碍了统一建模和公平基准测试。虽然FSQ提供了理论桥梁，但其等间隔量化会导致激活崩溃，需要在重建保真度和信息效率之间权衡。

Method: 提出iFSQ方法，将原始FSQ中的激活函数替换为分布匹配映射，以强制执行均匀先验。这种简单策略只需一行代码，但数学上保证了最优bin利用率和重建精度。使用iFSQ作为受控基准进行分析，并将表示对齐（REPA）方法扩展到AR模型中，得到LlamaGen-REPA。

Result: 发现两个关键洞察：1）离散与连续表示之间的最佳平衡点大约在每维度4比特；2）在相同重建约束下，AR模型表现出快速初始收敛，而扩散模型达到更优的性能上限，表明严格的顺序排序可能限制生成质量的上界。

Conclusion: iFSQ通过简单的修改解决了FSQ的激活崩溃问题，为离散和连续表示提供了统一的建模框架。研究发现4比特维度是最佳平衡点，且扩散模型在性能上限上优于AR模型。将REPA扩展到AR模型进一步验证了这些发现。

Abstract: The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ

</details>


### [26] [Scaling medical imaging report generation with multimodal reinforcement learning](https://arxiv.org/abs/2601.17151)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Yu Gu,Ying Jin,Sam Preston,Yanbo Xu,Sid Kiblawi,Wen-wai Yim,Tim Ossowski,Tristan Naumann,Mu Wei,Hoifung Poon*

Main category: cs.CV

TL;DR: UniRG是一个通用医学影像报告生成框架，通过强化学习直接优化应用指标，显著超越监督微调，在CXR报告生成中取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 前沿模型在自然语言理解方面表现出色，但在生物医学等多模态理解方面存在能力差距。医学影像报告生成是典型例子，监督微调容易过拟合到表面模板模式

Method: 提出Universal Report Generation (UniRG)通用框架，利用强化学习作为统一机制，直接针对终端应用设计的评估指标进行优化，实现跨机构临床实践的持久泛化

Result: 在权威ReXrank基准测试中，UniRG-CXR创造了新的总体SOTA，以较大优势超越先前最优方法

Conclusion: UniRG框架通过强化学习优化应用指标，显著改善了医学影像报告生成的性能，实现了跨机构泛化，为医学多模态理解提供了有效解决方案

Abstract: Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.

</details>


### [27] [LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction](https://arxiv.org/abs/2601.17185)
*Shima Salehi,Atharva Agashe,Andrew J. McFarland,Joshua Peeples*

Main category: cs.CV

TL;DR: 提出一种新的少样本3D重建方法，结合全局和局部频率正则化来稳定几何并保留稀疏视图下的细节，同时发布了一个包含四个光谱波段的多光谱温室数据集和开源基准测试包。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯泼溅（3DGS）模型在稀疏视图条件下存在几何不稳定和细节丢失的问题，需要改进少样本3D重建方法。同时缺乏专门的多光谱3D重建数据集和标准化评估协议。

Method: 提出了一种新的少样本3D重建方法，通过集成全局和局部频率正则化来稳定几何并保留精细细节。还创建了一个包含四个光谱波段的多光谱温室数据集，并开发了开源基准测试包，定义了标准化的少样本重建评估协议。

Result: 实验结果表明，在多光谱数据集和标准基准测试上，该方法比现有基线方法获得了更清晰、更稳定、光谱一致性更好的重建结果。

Conclusion: 提出的方法有效解决了稀疏视图下3DGS模型的几何不稳定问题，同时发布的数据集和基准测试包为少样本3D重建研究提供了有价值的资源。

Abstract: We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available

</details>


### [28] [Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments](https://arxiv.org/abs/2601.17194)
*Cheyu Lin,Katherine A. Flanigan,Sirajum Munir*

Main category: cs.CV

TL;DR: 本文提出了DUET数据集和动态识别框架，用于隐私保护地测量社交基础设施中的社交互动行为，填补了当前研究中缺乏统一互动测量方法的空白。


<details>
  <summary>Details</summary>
Motivation: 当前社会基础设施研究缺乏一致且隐私保护的社交互动测量方法，导致不同研究对"互动"的定义不一致，限制了实践者评估设计干预对社交资本相关行为影响的能力。

Method: 开发了DUET数据集，基于Ekman和Friesen的动态学分类法，捕捉12种二元互动行为；提出了隐私保护的动态识别框架，直接从骨骼运动推断交流功能，无需手工制作的动作-功能词典。

Result: DUET数据集涵盖五种动态功能；对6个开源活动识别模型的基准测试显示了交流功能识别的难度；识别框架揭示了动态功能的结构化聚类，并在跨主体和跨环境泛化方面表现良好。

Conclusion: DUET数据集和动态识别框架为测量社会基础设施中的社交互动提供了隐私保护的方法，有助于评估设计干预对社交资本相关行为的影响，推进了社会互动研究的标准化。

Abstract: Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.

</details>


### [29] [Structural Complexity of Brain MRI reveals age-associated patterns](https://arxiv.org/abs/2601.17211)
*Anzhe Cheng,Italo Ivo Lima Dias Pinto,Paul Bogdan*

Main category: cs.CV

TL;DR: 该研究将结构复杂性分析扩展到三维MRI信号，提出滑动窗口粗粒度化方法解决传统方法在粗分辨率下的不稳定问题，发现大脑结构复杂性随年龄系统性下降，且粗尺度效应最强


<details>
  <summary>Details</summary>
Motivation: 传统结构复杂性分析主要针对一维时间序列，需要将其扩展到三维信号特别是脑MRI数据，以捕捉大脑的多尺度组织特征。同时，传统块状粗粒度化方法在粗分辨率下因采样有限而不稳定，需要更稳健的方法

Method: 1. 将结构复杂性分析框架扩展到三维MRI信号；2. 引入滑动窗口粗粒度化方案，通过渐进增大的空间尺度对体素数据进行粗粒度化；3. 量化连续分辨率间的信息损失；4. 分析涵盖中老年期的大型结构MRI数据集

Result: 1. 滑动窗口方法比传统块状方法在粗尺度上更稳健，提供更平滑的估计；2. 大脑结构复杂性随年龄系统性下降；3. 最显著的年龄效应出现在较粗的空间尺度上；4. 结构复杂性可作为预测脑MRI生物年龄的有效工具

Conclusion: 结构复杂性分析是三维成像数据多尺度分析的可信信号处理工具，滑动窗口粗粒度化方法改进了传统方法的局限性，证明了该方法在捕捉大脑年龄相关变化方面的实用性，为脑MRI生物年龄预测提供了新方法

Abstract: We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.

</details>


### [30] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: 本文提出了一种基于语义V2X的实时碰撞预测框架，通过V-JEPA模型生成未来帧的时空语义嵌入，大幅降低通信开销的同时提升碰撞预测性能。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要实时碰撞预测以确保道路安全，但传统方法传输原始视频或高维传感器数据在车联网带宽和延迟约束下不实用。

Method: 使用路边单元摄像头通过V-JEPA模型生成未来帧的时空语义嵌入，这些嵌入捕获任务相关的交通动态，通过V2X链路传输到车辆，由轻量级注意力探针和分类器解码预测即将发生的碰撞。

Result: 实验结果表明，该框架在适当处理方法下，碰撞预测的F1分数提高了10%，同时传输需求相比原始视频减少了四个数量级。

Conclusion: 验证了语义V2X通信在实现智能交通系统中协作、实时碰撞预测方面的潜力。

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [31] [Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification](https://arxiv.org/abs/2601.17228)
*Tengyue Zhang,Ruiwen Ding,Luoting Zhuang,Yuxiao Wu,Erika F. Rodriguez,William Hsu*

Main category: cs.CV

TL;DR: 提出基于潜在扩散模型的半监督域适应框架，通过生成保留组织形态的目标感知合成图像，提升计算病理学中的域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中的深度学习模型常因域偏移问题而无法在不同队列和机构间良好泛化。现有方法要么未能充分利用目标域的无标签数据，要么依赖可能扭曲组织结构的图像翻译技术。

Method: 提出半监督域适应框架，使用在源域和目标域无标签数据上训练的潜在扩散模型，生成保留组织形态的目标感知合成图像。通过基于基础模型特征、队列身份和组织制备方法对扩散模型进行条件化，在保留源域组织结构的同时引入目标域外观特征。

Result: 在肺腺癌预后任务上验证了框架有效性。目标感知增强显著提升了目标域测试集性能而不降低源域性能：加权F1分数从0.611提升到0.706，宏观F1分数从0.641提升到0.716。

Conclusion: 目标感知的基于扩散的合成数据增强为改善计算病理学中的域泛化提供了有前景且有效的方法。

Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.

</details>


### [32] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: PEAfowl是一种增强感知的多视角视觉语言动作模型，通过几何感知的3D表示和迭代文本感知机制，显著提升了双臂机器人在杂乱场景中的操作性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言动作模型在双臂操作任务中存在两个主要问题：1）多视角特征通过视角无关的token拼接融合，导致3D空间理解能力弱；2）语言作为全局条件注入，导致指令定位粗糙。这些限制使得模型在遮挡、视角和场景变化下泛化能力不足。

Method: 1）空间推理：预测每个token的深度分布，进行可微分的3D提升，聚合局部跨视角邻居形成几何基础的跨视角一致表示；2）指令定位：用Perceiver风格的文本感知读取机制替代全局条件，在冻结的CLIP视觉特征上进行迭代证据积累；3）深度蒸馏：使用预训练深度教师模型进行仅训练时的深度蒸馏，监督深度分布头部，为感知前端提供几何感知先验。

Result: 在RoboTwin 2.0的领域随机化设置中，PEAfowl将最强基线的成功率提高了23.0个百分点。真实机器人实验进一步证明了可靠的仿真到真实迁移能力，以及深度蒸馏带来的持续改进。

Conclusion: PEAfowl通过几何感知的跨视角表示和迭代文本感知机制，显著提升了双臂操作任务的性能，在仿真和真实环境中都表现出色，为解决杂乱场景中的双臂操作问题提供了有效方案。

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [33] [C-RADIOv4 (Tech Report)](https://arxiv.org/abs/2601.17237)
*Mike Ranzinger,Greg Heinrich,Collin McCarthy,Jan Kautz,Andrew Tao,Bryan Catanzaro,Pavlo Molchanov*

Main category: cs.CV

TL;DR: C-RADIOv4是C-RADIO模型系列的最新版本，通过多教师蒸馏技术，在保持计算复杂度不变的情况下，显著提升了关键下游任务的性能，并增加了新功能。


<details>
  <summary>Details</summary>
Motivation: 构建一个统一的视觉主干模型，通过多教师蒸馏保留并提升多个专家模型（SigLIP2、DINOv3、SAM3）的独特能力，同时保持计算效率。

Method: 基于AM-RADIO/RADIOv2.5的设计，采用多教师蒸馏方法，使用更新后的教师模型集合（SigLIP2、DINOv3、SAM3）进行训练，并引入ViTDet选项以增强高分辨率下的效率。

Result: 发布了两个模型变体：-SO400M（4.12亿参数）和-H（6.31亿参数），在核心指标上有所改进，获得了模仿SAM3的新能力，增强了任意分辨率支持，并通过许可协议开放使用。

Conclusion: C-RADIOv4在保持计算效率的同时，通过多教师蒸馏成功整合了多个先进视觉模型的优势，提供了功能更强大、更灵活的视觉主干模型，并采用宽松许可证促进社区使用。

Abstract: By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.

</details>


### [34] [Masked Depth Modeling for Spatial Perception](https://arxiv.org/abs/2601.17895)
*Bin Tan,Changjiang Sun,Xiage Qin,Hanat Adai,Zelin Fu,Tianxiang Zhou,Han Zhang,Yinghao Xu,Xing Zhu,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: LingBot-Depth是一个深度补全模型，通过掩码深度建模利用视觉上下文优化深度图，并采用自动数据筛选管道进行可扩展训练，在深度精度和像素覆盖范围上优于顶级RGB-D相机。


<details>
  <summary>Details</summary>
Motivation: 空间视觉感知在自动驾驶和机器人操作等物理世界应用中至关重要。RGB-D相机捕捉像素对齐的度量深度是最可行的方法，但常受硬件限制和成像条件挑战（如镜面或纹理缺乏表面）的影响。作者认为深度传感器的不准确性可视为反映潜在几何模糊性的"掩码"信号。

Method: 提出LingBot-Depth深度补全模型：1）通过掩码深度建模利用视觉上下文优化深度图；2）采用自动数据筛选管道进行可扩展训练；3）模型在深度精度和像素覆盖范围上优于顶级RGB-D相机。

Result: 模型在深度精度和像素覆盖范围上优于顶级RGB-D相机。在一系列下游任务上的实验结果表明，LingBot-Depth在RGB和深度模态间提供了对齐的潜在表示。作者向空间感知社区发布了代码、检查点和300万RGB-深度对（包括200万真实数据和100万模拟数据）。

Conclusion: LingBot-Depth通过掩码深度建模和自动数据筛选，有效解决了深度传感器在复杂成像条件下的不准确性问题，提供了优于硬件传感器的深度补全性能，并为RGB和深度模态提供了对齐的表示，推动了空间视觉感知的发展。

Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.

</details>


### [35] [Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization](https://arxiv.org/abs/2601.17254)
*Takato Yasuno*

Main category: cs.CV

TL;DR: 日本桥梁损伤检测系统，在保护区域隐私的同时，利用SAM3检测钢筋腐蚀，DBSCAN补全漏检区域，高斯模糊保护施工标志，并通过预处理优化OCR，实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 日本规定每五年对基础设施进行目视检查，现场拍摄的损伤图像常包含混凝土裂缝和钢筋暴露，同时施工标志会泄露区域信息。为了安全使用基础设施而不引起公众焦虑，需要在准确提取损伤特征的同时保护区域隐私。

Method: 1. 使用Segment Anything Model (SAM) 3检测钢筋腐蚀；2. 采用DBSCAN自动补全漏检区域；3. 检测施工标志区域并用高斯模糊保护；4. 四种预处理方法提高OCR精度；5. GPU优化实现每张图像1.7秒处理速度。

Result: 技术栈包括SAM3、PyTorch、OpenCV、pytesseract和scikit-learn，实现了高效的桥梁检查系统，能够在保护区域信息的同时准确检测损伤。

Conclusion: 该系统为开源桥梁损伤检测系统，具备区域隐私保护能力，实现了损伤特征提取与区域信息保护的平衡，有助于安全基础设施维护决策。

Abstract: In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.

</details>


### [36] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: 提出FineVAU基准，通过细粒度三要素评估（事件、参与实体、位置）和FVScore指标，解决视频异常理解任务评估不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频异常理解（VAU）评估存在缺陷：基于n-gram的指标无法捕捉LVLM响应的丰富性和视觉基础，而基于LLM的评估过于关注语言质量而非事实相关性，导致与人类感知不一致。

Method: 1. 将VAU定义为三要素理解问题（What/事件、Who/参与实体、Where/位置）；2. 提出FVScore指标，评估LVLM回答中关键视觉元素的呈现；3. 构建FineW3数据集，通过结构化自动流程增强现有标注。

Result: 1. FVScore指标在人类评估中显示出与人类异常感知更好的对齐性；2. 实验揭示了LVLM在需要空间和细粒度时间理解的异常事件感知方面存在关键局限，尽管在粗粒度、静态信息和强视觉线索事件上表现良好。

Conclusion: FineVAU基准通过细粒度评估框架和人类对齐的FVScore指标，为视频异常理解提供了更有效的评估方法，同时揭示了当前LVLM在时空理解方面的不足。

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [37] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: 提出MinkUNeXt-VINE方法，一种轻量级深度学习框架，在葡萄园环境中超越现有技术，通过预处理和Matryoshka表示学习多损失方法实现高效实时定位。


<details>
  <summary>Details</summary>
Motivation: 农业环境因缺乏结构化特征和显著地标而难以进行定位，现有技术在农业场景中的地点识别任务表现有限，需要开发适应农业环境的轻量高效定位方法。

Method: 采用MinkUNeXt-VINE方法，包含预处理步骤和Matryoshka表示学习多损失策略，专门针对低成本稀疏LiDAR输入设计，生成低维输出以实现实时效率。

Result: 在多个评估案例和两个长期葡萄园数据集上超越现有方法，在低成本、低分辨率输入数据上表现出鲁棒性能，实现了性能与效率的良好权衡。

Conclusion: 该方法在农业环境中具有实用价值，特别是对于资源受限的实时应用场景，代码已公开以促进复现和进一步研究。

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [38] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: 本文提出一种无需额外训练的推理时区域约束颜色保持方法，通过ROI修复、背景潜在重新施加和潜在梯度引导，结合CIE Lab和线性RGB的复合损失，实现文本到图像扩散模型中的精确颜色控制。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散系统在精确颜色控制方面存在持续性问题，特别是在设计导向的工作流程中，输出必须满足用户指定的明确颜色目标。现有方法即使满足平均颜色约束，仍会产生感知上显著的局部失败。

Method: 1) 基于ROI的修复实现空间选择性；2) 背景潜在重新施加防止ROI外颜色漂移；3) 通过CIE Lab和线性RGB的复合损失进行梯度引导，该损失不仅控制ROI平均颜色，还通过CVaR风格和软最大惩罚控制像素误差分布的尾部；4) 包含延迟启动门和时间相关调度以稳定去噪步骤中的引导。

Result: 该方法提供了一种实用的、无需训练的机制，可用于目标颜色保持，能够集成到标准的Stable Diffusion修复流程中，有效解决仅基于平均颜色的基线方法存在的局部颜色失败问题。

Conclusion: 该研究提出了一种无需训练的区域约束颜色保持方法，通过分布感知的损失函数设计，实现了对文本到图像扩散模型颜色的精确控制，为设计工作流程提供了有效的颜色控制解决方案。

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [39] [Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales](https://arxiv.org/abs/2601.17271)
*Kun Huang,Fang-Lue Zhang,Neil Dodgson*

Main category: cs.CV

TL;DR: Cross360是一种基于交叉注意力的新型架构，通过结合局部切面投影特征和等距柱面投影特征，解决360°深度估计中全局连续性和局部一致性平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有360°深度估计方法难以平衡全局和局部一致性：局部补丁特征缺乏全局感知，而全局表示又无法解决补丁边界特征提取的不一致问题。

Method: 提出Cross360架构，包含两个核心模块：1）交叉投影特征对齐模块，使用交叉注意力将局部切面投影特征与等距柱面投影的360°视野对齐；2）渐进式注意力特征聚合模块，逐步细化多尺度特征。

Result: Cross360在大多数基准数据集上显著优于现有方法，特别是在完整360°图像可用的情况下，展示了其在准确且全局一致的深度估计方面的有效性。

Conclusion: 该研究提出的跨注意力架构有效解决了360°深度估计中全局与局部信息融合的挑战，通过特征对齐和渐进聚合实现了更准确和一致的深度估计。

Abstract: 360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.

</details>


### [40] [Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing](https://arxiv.org/abs/2601.17288)
*Jin Bai,Huiyao Zhang,Qi Wen,Shengyang Li,Xiaolin Tian,Atta ur Rahman*

Main category: cs.CV

TL;DR: Fluxamba是一个轻量级架构，通过拓扑感知特征校正解决地质线性特征分割中的拓扑不匹配问题，实现了高精度分割和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 地质线性特征（如行星线状构造和地面裂缝）的分割需要捕捉复杂各向异性拓扑中的长程依赖关系。现有状态空间模型依赖刚性的轴对齐扫描轨迹，导致与曲线目标的拓扑不匹配，造成上下文碎片化和特征侵蚀。

Method: 提出Fluxamba架构，核心是结构通量块（SFB），它通过各向异性结构门（ASG）和先验调制流（PMF）集成各向异性信息通量，将特征方向与空间位置解耦，沿目标内在几何动态门控上下文聚合。还包含分层空间调节器（HSR）进行多尺度语义对齐，以及高保真聚焦单元（HFFU）最大化微弱特征的信噪比。

Result: 在多个地质基准测试（LROC-Lineament、LineaMapper、GeoCrack）上达到最先进水平。在LROC-Lineament数据集上F1分数89.22%，mIoU 89.87%。实时推理速度超过24 FPS，仅需3.4M参数和6.3G FLOPs，计算成本比重量级基线降低两个数量级。

Conclusion: Fluxamba建立了分割保真度与机载部署可行性之间的新帕累托前沿，为地质线性特征分割提供了一种高效准确的解决方案。

Abstract: The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.

</details>


### [41] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: 提出动态元集成框架DMEF，通过自适应权重机制动态组合三个轻量级CNN模型，在边缘设备上实现高精度植物病害检测，平衡准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 边缘设备（如IoT传感器、智能手机）的计算资源和能量预算有限，制约了深度学习模型在植物病害检测中的部署。需要一种能在资源受限环境下实现高精度诊断的解决方案。

Method: 提出动态元集成框架DMEF，采用自适应权重机制动态组合MobileNetV2、NASNetMobile和InceptionV3三个轻量级CNN的预测结果。通过迭代更新集成权重，在训练过程中优化准确率提升（DeltaAcc）和计算效率（模型大小）之间的权衡。

Result: 在马铃薯和玉米病害基准数据集上分别达到99.53%和96.61%的分类准确率，超越单模型和静态集成方法2.1%和6.3%。推理延迟小于75ms，参数少于100万，计算效率高。

Conclusion: DMEF在边缘设备上实现了高精度植物病害诊断，兼具计算效率和紧凑模型尺寸，有望用于边缘农业监测和可扩展的作物病害管理，弥合了高精度AI与实际田间应用之间的差距。

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [42] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: 提出ClinNet框架，将膝关节骨关节炎分级建模为证据序数回归问题，通过双边不对称编码器、诊断记忆库和证据序数头联合估计连续KL分级和认知不确定性，显著提升分级性能并实现可信预测。


<details>
  <summary>Details</summary>
Motivation: 膝关节骨关节炎分级基于X光图像是一项关键但具有挑战性的任务，传统深度学习方法将其视为确定性的多类分类问题，忽略了疾病进展的连续性和专家标注的不确定性。

Method: 提出ClinNet框架，包含三个核心组件：双边不对称编码器（BAE）建模内外侧结构差异；诊断记忆库（Diagnostic Memory Bank）维护类别原型稳定特征表示；基于正态逆伽马分布的证据序数头联合估计连续KL分级和认知不确定性。

Result: ClinNet获得二次加权Kappa 0.892和准确率0.768，统计显著优于最先进基线方法（p<0.001）。模型的不确定性估计成功识别分布外样本和潜在误诊，为安全临床部署铺平道路。

Conclusion: ClinNet通过证据序数回归框架有效解决了膝关节骨关节炎分级中的连续进展和标注不确定性问题，不仅提升了分级性能，还提供了可信的不确定性估计，有望实现安全的临床部署。

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [43] [SkyReels-V3 Technique Report](https://arxiv.org/abs/2601.17323)
*Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-V3是一个基于扩散Transformer的统一多模态上下文学习框架，支持三种核心生成范式：参考图像到视频合成、视频扩展和音频引导视频生成。


<details>
  <summary>Details</summary>
Motivation: 视频生成是构建世界模型的基础，而多模态上下文推理是能力的关键测试。为了应对这一挑战，需要开发能够处理多种生成任务的统一模型。

Method: 1. 参考图像到视频：采用跨帧配对、图像编辑和语义重写的数据处理流程，结合图像视频混合策略和多分辨率联合优化
2. 视频扩展：集成时空一致性建模与大规模视频理解，支持无缝单镜头延续和智能多镜头切换
3. 音频引导视频生成：训练首尾帧插入模式并重构关键帧推理范式，优化音视频同步

Result: 在视觉质量、指令跟随和特定方面指标上达到或接近最先进水平，接近领先的闭源系统性能。

Conclusion: SkyReels-V3通过统一的多模态上下文学习框架，在单个架构中实现了三种核心视频生成范式，展示了强大的多模态推理能力，为构建世界模型提供了重要基础。

Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.

</details>


### [44] [SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision](https://arxiv.org/abs/2601.17326)
*Jasmine Lesner,Michael Beyeler*

Main category: cs.CV

TL;DR: SymbolSight是一个计算框架，通过优化符号到字母的映射来减少视网膜假体视觉中相邻字母的混淆，提高阅读能力。


<details>
  <summary>Details</summary>
Motivation: 视网膜假体视觉分辨率低且存在视觉暂留现象，导致连续字母呈现时，前一个符号的残影会干扰下一个符号的识别，造成系统性识别错误。作者希望在不依赖硬件改进的情况下，通过优化视觉符号本身来缓解这种时间干扰。

Method: 提出SymbolSight计算框架：1）使用模拟假体视觉（SPV）和神经代理观察者估计符号对之间的混淆度；2）利用语言特定的双字母统计信息；3）优化符号到字母的映射分配，以最小化频繁相邻字母之间的混淆。

Result: 在阿拉伯语、保加利亚语和英语的模拟实验中，优化后的异构符号集将预测混淆度相对于原生字母表降低了中位数22倍。

Conclusion: 标准排版不适合序列化、低带宽的假体视觉，计算建模可以有效地缩小视觉编码的设计空间，为未来的心理物理和临床评估生成高潜力的候选方案。

Abstract: Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.

</details>


### [45] [Learning with Geometric Priors in U-Net Variants for Polyp Segmentation](https://arxiv.org/abs/2601.17331)
*Fabian Vazquez,Jose A. Nuñez,Diego Adame,Alissen Moreno,Augustin Zhan,Huimin Li,Jinghao Yang,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: cs.CV

TL;DR: 该论文提出了一种几何先验引导模块（GPM），通过注入显式几何先验来增强基于U-Net的息肉分割模型，在低对比度或杂乱结肠镜场景中提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN、Transformer和Mamba的U-Net变体在息肉分割中难以捕捉几何和结构线索，特别是在低对比度或杂乱的结肠镜场景中。需要注入显式几何先验来提升分割准确性和鲁棒性。

Method: 提出几何先验引导模块（GPM），首先在模拟的ColonDepth数据集上微调VGGT模型来估计息肉图像的深度图，然后通过GPM将几何先验编码到编码器的特征图中，并使用空间和通道注意力机制进行细化。

Result: 在五个公共息肉分割数据集上的广泛实验表明，该方法在三个强基线模型上均取得了稳定提升，证明了GPM的有效性和泛化能力。

Conclusion: GPM模块能够有效注入几何先验到U-Net架构中，提升息肉分割性能，特别是在具有挑战性的结肠镜场景中，且该模块具有即插即用特性，可无缝集成到多种U-Net变体中。

Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg

</details>


### [46] [AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17336)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: AGE-Net是一个基于ConvNeXt的膝关节KL分级框架，通过光谱-空间融合、解剖图推理和差分细化模块，结合证据回归头和序数约束，在KL分级任务上取得了0.9017的QWK分数，优于现有CNN基线。


<details>
  <summary>Details</summary>
Motivation: 膝关节X光片的Kellgren-Lawrence（KL）自动分级面临三个主要挑战：细微结构变化、长距离解剖依赖关系以及分级边界附近的模糊性。传统方法难以有效处理这些问题。

Method: 提出AGE-Net框架，包含三个核心模块：1）光谱-空间融合（SSF）捕获局部和全局特征；2）解剖图推理（AGR）建模解剖结构间的长距离依赖；3）差分细化（DFR）处理分级边界模糊性。采用Normal-Inverse-Gamma证据回归头量化预测不确定性，并加入成对序数排序约束保持分级顺序。

Result: 在膝关节KL数据集上，AGE-Net达到二次加权kappa（QWK）0.9017±0.0045，均方误差（MSE）0.2349±0.0028（三次随机种子平均），显著优于强CNN基线。消融实验显示各模块均有贡献，同时展示了不确定性质量、鲁棒性和可解释性评估。

Conclusion: AGE-Net通过多模态特征融合、解剖图推理和边界细化，有效解决了KL分级中的关键挑战。证据回归头和序数约束进一步提升了预测可靠性和分级准确性，为医学图像分析提供了新的解决方案。

Abstract: Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.

</details>


### [47] [TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution](https://arxiv.org/abs/2601.17340)
*Haodong He,Xin Zhan,Yancheng Bai,Rui Lan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 该论文提出了Real-Texts数据集和TEXTS-Diff模型，用于解决真实世界文本图像超分辨率中文本区域恢复质量差和背景重建受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本图像超分辨率方法面临两个主要问题：1）现有数据集中文本图像数据稀缺，导致文本区域恢复效果差；2）基于孤立文本样本的数据集限制了背景重建质量。需要构建一个覆盖多样化场景、包含自然文本实例的大规模高质量数据集。

Method: 1）构建Real-Texts数据集：从真实世界图像收集的大规模高质量数据集，覆盖多样化场景，包含中英文自然文本实例；2）提出TEXTS-Diff模型：利用抽象概念提升对视觉场景中文本元素的理解，结合具体文本区域增强文本细节，减少文本区域的失真和幻觉伪影，同时保持高质量视觉场景保真度。

Result: 实验表明该方法在多个评估指标上达到最先进性能，在复杂场景中展现出优异的泛化能力和文本恢复准确性。

Conclusion: 通过构建Real-Texts数据集和开发TEXTS-Diff模型，有效解决了真实世界文本图像超分辨率中文本恢复和背景重建的问题，实现了高质量的视觉场景保真度和文本可读性恢复。

Abstract: Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.

</details>


### [48] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: STARS是一个针对缺失模态遥感数据的鲁棒语义分割框架，通过非对称对齐机制和像素级语义采样对齐策略解决特征崩溃和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，遥感多模态数据（如光学图像、SAR、DSM）经常出现模态缺失问题，导致传统多模态融合模型性能下降。现有方法存在特征崩溃和特征恢复过于泛化等局限性。

Method: 提出了STARS框架，包含两个关键设计：1）带双向翻译和停止梯度的非对称对齐机制，防止特征崩溃并降低超参数敏感性；2）像素级语义采样对齐策略，结合类别平衡像素采样和跨模态语义对齐损失，缓解类别不平衡导致的对齐失败。

Result: 论文没有在摘要中提供具体实验结果，但声称STARS能够有效处理缺失模态情况，防止特征崩溃，并改善少数类识别性能。

Conclusion: STARS为解决遥感多模态数据中的模态缺失问题提供了一个有效的鲁棒语义分割框架，通过创新的对齐机制和采样策略克服了现有方法的局限性。

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [49] [Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective](https://arxiv.org/abs/2601.17349)
*Hailong Yan,Shice Liu,Xiangtao Zhang,Lujian Yao,Fengxiang Yang,Jinwei Chen,Bo Li*

Main category: cs.CV

TL;DR: 提出了一种基于YUV色彩空间的轻量级低光照图像增强方法，通过频率域分析发现Y通道主要丢失低频内容而UV通道受高频噪声影响，设计了针对性的双流注意力模块，在减少参数的同时取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前移动设备上的轻量级低光照图像增强面临视觉质量与模型紧凑性的权衡。现有基于解耦策略的方法（如Retinex理论和YUV色彩空间变换）由于忽略了通道特定的退化模式和跨通道交互，性能受到根本限制。

Method: 通过频率域分析确认YUV色彩空间对L3IE的优越性，发现Y通道主要丢失低频内容，UV通道受高频噪声影响。提出新颖的YUV范式：为Y通道设计双流全局-局部注意力模块，为UV通道设计Y引导的局部感知频率注意力模块，最后通过引导交互模块进行特征融合。

Result: 在多个基准测试中建立了新的最先进性能，以显著更少的参数提供了优越的视觉质量。

Conclusion: 通过针对YUV通道特性的频率域分析和专门设计的注意力模块，成功解决了轻量级低光照图像增强中视觉质量与模型紧凑性的权衡问题。

Abstract: In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.

</details>


### [50] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: NeRF-MIR：一种基于NeRF的掩码图像修复方法，通过PERE策略和PIRE机制实现更好的3D场景重建


<details>
  <summary>Details</summary>
Motivation: NeRF在新视角合成方面表现出色，但在处理自然场景捕获中常见的损坏图像时仍有改进空间。损坏图像会显著影响NeRF的有效性，需要专门的方法来修复掩码图像。

Method: 1. 提出PERE（基于补丁的熵的射线发射）策略，通过分析图像纹理复杂性来合理分配发射射线，从不同视角图像中融合全面信息；2. 引入PIRE（渐进迭代修复）机制，通过自训练过程逐步修复掩码区域；3. 设计动态加权损失函数，自动重新校准掩码区域的损失权重；4. 构建三个掩码数据集来模拟损坏场景。

Result: 在真实数据和构建的数据集上进行的大量实验表明，NeRF-MIR在掩码图像修复方面优于现有方法。

Conclusion: NeRF-MIR展示了NeRF在掩码图像修复领域的潜力，通过创新的射线发射策略、渐进修复机制和动态损失函数，能够有效处理损坏图像并恢复3D场景。

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [51] [HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data](https://arxiv.org/abs/2601.17352)
*M. L. Mamud,Piyoosh Jaysaval,Frederick D Day-Lewis,M. K. Mudunuru*

Main category: cs.CV

TL;DR: 该研究提出了HyDeMiC（基于深度学习的矿物分类器），一种用于噪声条件下稳健矿物分类的卷积神经网络模型，在USGS的115种矿物光谱数据上训练，在含噪声合成数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统矿物分类方法如判别分析、逻辑回归和支持向量机在处理高光谱数据时，难以应对环境噪声、传感器限制和高维数据的计算复杂性。

Method: 提出HyDeMiC卷积神经网络模型，使用USGS实验室测量的115种矿物高光谱数据，通过卷积参考矿物光谱与HSI传感器响应函数生成训练数据集，在含1%、2%、5%、10%噪声的合成2D高光谱数据集上评估性能。

Result: HyDeMiC在清洁和低噪声数据集上实现近乎完美的分类准确率（MCC = 1.00），在中等噪声条件下仍保持强劲性能，证明了其在噪声存在下的鲁棒性。

Conclusion: HyDeMiC在高光谱成像中具有强大的噪声鲁棒性，在现实野外条件下具有实际应用潜力，能够应对噪声这一重要挑战。

Abstract: Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.

</details>


### [52] [PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling](https://arxiv.org/abs/2601.17354)
*Wenzhi Guo,Guangchi Fang,Shu Yang,Bing Wang*

Main category: cs.CV

TL;DR: PocketGS：一种移动端3D高斯泼溅建模方法，在资源受限的移动设备上实现高质量3D场景重建


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法依赖工作站级资源，无法在训练时间短、内存有限的移动设备上运行，需要开发适合移动端的3D场景建模方案

Method: 提出三个协同设计的算子：G算子构建几何保真点云先验；I算子注入局部表面统计信息以初始化各向异性高斯分布；T算子通过缓存中间结果和索引映射梯度散射实现稳定的移动端反向传播

Result: PocketGS在移动设备上超越了主流工作站级3DGS基线，实现了高质量重建，支持完整的端到端捕获到渲染工作流程

Conclusion: PocketGS解决了标准3DGS在移动设备上的根本矛盾，在训练效率、内存紧凑性和建模保真度之间取得了平衡，实现了实用的移动端3D场景建模

Abstract: Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.

</details>


### [53] [UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation](https://arxiv.org/abs/2601.17366)
*Chengbo Ding,Fenghe Tang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: UCAD提出了一种不确定性引导的轮廓感知位移框架，用于半监督医学图像分割，通过超像素生成解剖一致区域，选择性位移挑战性区域，并采用动态不确定性加权一致性损失提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有半监督分割中的位移策略仅操作矩形区域，忽略了解剖结构，导致边界扭曲和语义不一致。为了解决这些问题，需要一种能保留轮廓感知语义并增强一致性学习的方法。

Method: UCAD框架：1）利用超像素生成与解剖边界对齐的解剖一致区域；2）采用不确定性引导的选择机制，选择性位移挑战性区域以优化一致性学习；3）提出动态不确定性加权一致性损失，自适应稳定训练并有效正则化未标记区域。

Result: 大量实验表明，UCAD在有限标注条件下，持续优于最先进的半监督分割方法，实现了卓越的分割精度。

Conclusion: UCAD通过解剖感知的位移策略和不确定性引导的学习机制，有效解决了半监督医学图像分割中的边界扭曲和语义不一致问题，在有限标注下实现了更准确的分割性能。

Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.

</details>


### [54] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: 本文提出了首个物理提示注入攻击(PPIA)，这是一种黑盒、查询无关的攻击方法，通过在物理对象中嵌入恶意文本指令来攻击大型视觉语言模型，无需访问模型内部或输入通道。


<details>
  <summary>Details</summary>
Motivation: 现有针对大型视觉语言模型的提示注入攻击方法要么需要访问输入通道，要么依赖用户查询知识，这些假设在实际部署中很少成立。需要一种更实际的攻击方法来评估LVLMs在真实物理环境中的安全性。

Method: PPIA结合离线选择高识别度和语义有效的视觉提示，以及基于时空注意力的环境感知策略性放置，确保注入的提示既可见又能影响模型行为。该攻击无需访问模型、输入或内部管道，仅通过视觉观察操作。

Result: 在10个最先进的LVLMs上评估，包括视觉问答、规划和导航任务，PPIA在模拟和真实环境中攻击成功率高达98%，在不同物理条件（距离、视角、光照）下表现出强鲁棒性。

Conclusion: PPIA首次展示了在无需模型访问或查询知识的情况下，通过物理对象进行提示注入攻击的可行性，揭示了LVLMs在开放物理环境中的安全漏洞，对实际部署的安全性提出了重要警示。

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [55] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: 提出基于扩散模型的高质量鲁棒水印框架，通过零文本优化将干净图像转换为反演噪声，在潜在空间优化后通过扩散模型迭代去噪生成高质量水印图像，显著提升水印对图像损坏的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水印方法虽然能最小化对图像质量的影响，但在传输过程中遇到图像损坏时缺乏鲁棒性，限制了实际应用价值。需要一种既能保持高质量又能抵抗各种图像损坏的水印方法。

Method: 基于扩散模型的水印框架：1) 通过零文本优化过程将干净图像转换为反演噪声；2) 在潜在空间优化反演噪声；3) 通过扩散模型的迭代去噪过程生成高质量水印图像。引入自注意力约束和伪掩码策略防止反演噪声优化扭曲原始图像语义。

Result: 在COCO数据集上的12种不同图像变换测试中，该方法平均比stable signature方法性能高出10%。迭代去噪过程作为强大的净化机制，既保证了水印图像的视觉质量，又增强了水印对各种损坏的鲁棒性。

Conclusion: 提出的基于扩散模型的水印框架能够生成高质量且鲁棒的水印图像，有效解决了现有方法在图像损坏情况下鲁棒性不足的问题，具有实际应用价值。

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [56] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: 本文针对事件相机动作识别提出改进的时空多视图表示学习方法，通过平移不变密集转换、动态双分支融合架构和生物启发时序扭曲增强，在多个数据集上显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机动作识别方法存在局限性：1) 基于空间分箱的表示具有平移变异性；2) 早期简单拼接融合架构不足；3) 需要更有效处理真实世界动作速度变化的方法。

Method: 提出三个核心创新：1) 平移不变的密集转换方法处理稀疏事件数据；2) 双分支动态融合架构建模不同视图间的互补性；3) 生物启发时序扭曲增强模拟真实动作速度变化。

Result: 在HARDVS、DailyDVS-200和THU-EACT-50-CHL三个数据集上，Top-1准确率分别提升7.0%、10.7%和10.2%，同时参数减少30.1%，计算量降低35.7%。

Conclusion: 该框架为事件相机动作识别提供了新颖有效的范式，在保持高性能的同时显著降低了计算复杂度，具有实际应用价值。

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [57] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: ReLE系统通过动态评估304个模型揭示能力各向异性，提出符号基础混合评分和动态调度方法，大幅降低计算成本，发现模型更专业化而非通用优越


<details>
  <summary>Details</summary>
Motivation: 现有中文大语言模型评估存在基准饱和和计算成本高昂的问题，静态排行榜掩盖了模型在不同领域间的能力结构差异，需要更高效、细粒度的评估方法

Method: 开发ReLE可扩展系统，使用领域×能力的正交矩阵包含207,843个样本；提出符号基础混合评分机制消除推理任务中的嵌入假阳性；基于Neyman分配和噪声校正的动态方差感知调度器

Result: 评估304个模型（189个商业、115个开源），动态调度减少70%计算成本，排名相关性ρ=0.96；揭示聚合排名对权重方案高度敏感，模型更专业化而非通用优越

Conclusion: ReLE作为高频诊断监控工具，补充传统静态基准，能更准确反映模型在特定领域的能力各向异性，为模型选择和部署提供更精细的评估依据

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [58] [HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection](https://arxiv.org/abs/2601.17405)
*Chunze Yang,Wenjie Zhao,Yue Tang,Junbo Lu,Jiusong Ge,Qidong Liu,Zeyu Gao,Chen Li*

Main category: cs.CV

TL;DR: 提出HAAF框架解决病理图像中细粒度形态异常检测的粒度不匹配问题，通过跨层级对齐机制和双分支推理策略，在低资源场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精准病理学依赖于检测特定感兴趣区域内的细粒度形态异常，这些局部纹理丰富的线索而非全局图像上下文驱动专家诊断推理。虽然视觉-语言模型通过利用语义先验有望提高数据效率，但适应这些模型面临关键的粒度不匹配问题，通用表示无法解析此类细微缺陷。当前适应方法通常将模态视为独立流，未能将语义提示与ROI特定视觉上下文相融合。

Method: 提出分层适应与对齐框架（HAAF），核心是新颖的跨层级缩放对齐（CLSA）机制，强制执行顺序校准：视觉特征首先将上下文注入文本提示以生成内容自适应描述符，然后空间引导视觉编码器聚焦异常。此外，采用双分支推理策略，整合语义分数与几何原型，确保少样本设置下的稳定性。

Result: 在四个基准测试上的实验表明，HAAF显著优于最先进方法，并在低资源场景中有效扩展领域特定骨干网络（如CONCH）。

Conclusion: HAAF通过跨模态对齐机制成功解决了视觉-语言模型在病理图像细粒度异常检测中的粒度不匹配问题，为低资源医学图像分析提供了有效解决方案。

Abstract: Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

</details>


### [59] [Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity](https://arxiv.org/abs/2601.17408)
*Harsharaj Pathak,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 提出了一种基于邻域签名的源自由域自适应方法，通过优化目标域样本预测的相似性和不相似性，仅使用单个损失项实现更好的领域适应效果。


<details>
  <summary>Details</summary>
Motivation: 现有的源自由域自适应方法大多依赖邻域一致性，但容易受到误导性邻域信息的影响而产生错误。为了解决这个问题，本文探索从学习更信息化的聚类和减少噪声邻居影响的角度出发。

Method: 提出使用邻域签名的概念，通过专门优化的单个损失项来调整目标域样本预测的相似性和不相似性，从而学习更信息化的聚类并减少噪声邻居的影响。

Result: 在具有挑战性的VisDA数据集上超越了现有方法，在其他基准数据集上也取得了具有竞争力的结果。

Conclusion: 通过邻域签名方法，仅使用单个损失项就能有效实现源自由域自适应，减少了噪声邻居的影响并学习到更信息化的聚类。

Abstract: Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.

</details>


### [60] [Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase](https://arxiv.org/abs/2601.17414)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文提出了一种基于Google Firebase实时数据库的云使能物联网系统，用于环境监测和设备控制，具有低成本、高可靠性和实时同步的特点。


<details>
  <summary>Details</summary>
Motivation: 传统监控系统存在实时数据访问、远程控制和云集成方面的限制，而物联网设备的普及为远程监控应用创造了机会，需要一种更高效、易用的解决方案。

Method: 使用ESP32微控制器连接DHT22温湿度传感器和HC-SR04超声波距离传感器，通过Google Firebase实时数据库实现数据同步和远程控制，用户可通过云端界面控制两个LED指示灯。

Result: 系统实现99.2%的数据传输成功率，实时控制延迟低于1.5秒，提供持久数据存储用于历史分析，总实施成本仅32.50美元。

Conclusion: 该系统架构为各种物联网应用提供了可扩展框架，Firebase集成无需复杂服务器基础设施即可提供强大的云功能，使资源有限的开发者和研究人员也能构建高级物联网应用。

Abstract: The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.

</details>


### [61] [CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction](https://arxiv.org/abs/2601.17420)
*Shiu-hong Kao,Chak Ho Huang,Huaiqian Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: CoT-Seg提出一个无需训练的推理分割框架，结合思维链推理与自校正机制，利用预训练MLLMs（如GPT-4o）处理复杂查询和域外图像，显著提升分割的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法在处理复杂查询和域外图像时表现不足。受人类解决难题时逐步思考、查找信息、评估修正的启发，本文旨在探索一个能够模拟人类推理过程的分割系统。

Method: CoT-Seg是一个无需训练框架，利用预训练MLLMs（GPT-4o）的推理能力：1）将查询分解为元指令；2）从图像中提取细粒度语义；3）识别隐含或复杂提示下的目标对象。框架包含自校正阶段：模型根据原始查询和推理轨迹评估分割结果，识别不匹配并迭代优化掩码。还可集成检索增强推理，在输入信息不足时访问外部知识。

Result: 为展示CoT-Seg处理挑战性案例的能力，提出了新数据集ReasonSeg-Hard。结果表明，结合思维链推理和自校正为视觉语言驱动的分割提供了强大范式，显著提升了在模糊或易出错情况下的可靠性和鲁棒性。

Conclusion: CoT-Seg通过思维链推理与自校正的紧密结合，为复杂推理分割任务提供了一个有效且无需训练的解决方案，展示了预训练MLLMs在视觉语言集成分割中的强大潜力。

Abstract: Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.

</details>


### [62] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: 该研究针对X射线冠状动脉造影（XCA）中的血管分割问题，提出了一种结合传统方法和深度学习的两阶段框架，包括血管分割和血管类型标记，通过改进的图像增强、参数优化和监督策略，显著提升了分割精度和域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: XCA是评估冠状动脉疾病的临床金标准，但由于低对比度、运动、透视缩短、重叠和导管干扰等因素，常规数据中的血管分割存在困难，导致不同中心间的域偏移。可靠的血管分割和血管类型标记对于实现血管特异性分析和依赖于解剖定位的下游测量至关重要。

Method: 研究采用两阶段方法：1）从670个序列中选择最佳帧进行超分辨率和增强；2）基准测试包括经典血管滤波器（Meijering、Frangi、Sato）的三种参数设置策略，以及神经网络基线（U-Net、FPN、Swin Transformer），采用冠状动脉单独标注和冠状动脉+导管合并标注两种监督方式；3）第二阶段进行血管类型标记（LAD、LCX、RCA）。外部评估使用公开的DCA1数据集。

Result: SVR每图像调优显著提升了经典滤波器的Dice分数（如Frangi从0.741提升到0.759）。在深度模型中，FPN在冠状动脉单独标注下达到0.914±0.007 Dice，合并标注进一步提升到0.931±0.006。在严格的域外测试DCA1上，Dice下降到0.798（单独标注）和0.814（合并标注），但轻量域内微调可恢复到0.881±0.014和0.882±0.015。血管类型标记准确率：RCA 98.5%（Dice 0.844）、LAD 95.4%（0.786）、LCX 96.2%（0.794）。

Conclusion: 学习每图像调优增强了传统方法，而高分辨率FPN模型和合并标签监督提高了稳定性和域外迁移能力，只需适度适应即可获得良好性能。该方法为XCA定量分析提供了可靠的血管分割和类型标记解决方案。

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [63] [ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation](https://arxiv.org/abs/2601.17468)
*Chia-Ming Lee,Yu-Fan Lin,Jing-Hui Jung,Yu-Jou Hsiao,Chih-Chung Hsu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: ReflexSplit提出了一种用于单图像反射分离的双流框架，通过跨尺度门控融合、层融合-分离块和课程训练三个创新，有效解决了非线性混合下的传输-反射混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有单图像反射分离方法在非线性混合条件下存在传输-反射混淆问题，特别是在深层解码器中，这主要是由于隐式融合机制和多尺度协调不足导致的。

Method: 提出ReflexSplit双流框架，包含三个关键技术：(1)跨尺度门控融合(CrGF)自适应聚合语义先验、纹理细节和解码器上下文；(2)层融合-分离块(LFSB)交替进行融合和分离操作，并扩展注意力取消到双流分离；(3)课程训练通过深度相关初始化和逐阶段预热渐进增强差分分离。

Result: 在合成和真实世界基准测试中展示了最先进的性能，具有优越的感知质量和强大的泛化能力。

Conclusion: ReflexSplit通过创新的双流架构和多尺度协调机制，有效解决了单图像反射分离中的传输-反射混淆问题，实现了更准确的层分离和更好的视觉质量。

Abstract: Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.

</details>


### [64] [PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors](https://arxiv.org/abs/2601.17470)
*Chia-Ming Lee,Yu-Fan Lin,Yu-Jou Hsiao,Jing-Hui Jung,Yu-Lun Liu,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: PhaSR提出了一种物理对齐的阴影去除方法，通过双重先验对齐技术解决单光源到多光源环境下的阴影去除问题，在保持较低复杂度的同时实现更好的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 在不同光照条件下进行阴影去除需要将光照与内在反射率分离，但当物理先验未正确对齐时，这一挑战会变得更加复杂。传统方法在多光源环境照明下效果不佳。

Method: 提出PhaSR框架，包含两个核心组件：1）物理对齐归一化（PAN）：通过灰世界归一化、对数域Retinex分解和动态范围重组进行闭式光照校正，抑制色偏；2）几何-语义校正注意力（GSRA）：扩展差分注意力实现跨模态对齐，协调深度衍生的几何信息与DINO-v2语义嵌入，解决不同光照下的模态冲突。

Result: 实验显示在阴影去除任务中具有竞争力，计算复杂度较低，并能泛化到环境光照条件，在传统方法无法处理的多光源照明场景下表现良好。

Conclusion: PhaSR通过双重先验对齐实现了从单光源阴影到多光源环境照明的鲁棒阴影去除，源代码已开源，为复杂光照条件下的阴影去除提供了有效解决方案。

Abstract: Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.

</details>


### [65] [BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation](https://arxiv.org/abs/2601.17504)
*Yan Zhou,Zhen Huang,Yingqiu Li,Yue Ouyang,Suncheng Xiang,Zehua Wang*

Main category: cs.CV

TL;DR: BMDS-Net是一个针对脑肿瘤分割的鲁棒可信框架，通过零初始化多模态融合和残差门控深度解码器监督增强确定性分割，并采用贝叶斯微调策略提供不确定性估计，解决临床中模态缺失和置信度校准问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型（如Swin UNETR）在脑肿瘤分割中面临两个关键临床问题：对模态缺失的敏感性（临床实践中常见）和缺乏置信度校准。单纯追求理想数据上的高Dice分数无法满足真实医疗部署的安全要求。

Method: 1. 构建鲁棒确定性主干网络：集成零初始化多模态上下文融合（MMCF）模块和残差门控深度解码器监督（DDS）机制，实现稳定特征学习和精确边界划分；2. 引入内存高效的贝叶斯微调策略，将网络转换为概率预测器，提供体素级不确定性地图；3. 在BraTS 2021数据集上进行全面实验。

Result: BMDS-Net不仅保持了有竞争力的准确性，更重要的是在模态缺失场景下表现出优越的稳定性，而基线模型在这些情况下会失败。显著减少了Hausdorff距离，提供了临床可用的不确定性地图。

Conclusion: BMDS-Net是一个优先考虑临床鲁棒性和可信度的统一框架，超越了简单的指标最大化，通过结合确定性鲁棒分割和概率不确定性估计，为脑肿瘤分割的临床部署提供了更安全可靠的解决方案。

Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.

</details>


### [66] [FMIR, a foundation model-based Image Registration Framework for Robust Image Registration](https://arxiv.org/abs/2601.17529)
*Fengting Zhang,Yue He,Qinghao Liu,Yaonan Wang,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: FMIR是一个基于基础模型的医学图像配准框架，通过结合基础模型特征编码器和通用配准头，在单一数据集上训练就能实现域内SOTA性能，同时保持对域外图像的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习虽然大幅提升了医学图像配准速度，但其临床应用的瓶颈在于泛化能力有限，特别是在医学数据集通常规模较小的情况下。需要开发能够超越训练域泛化的配准方法。

Method: FMIR框架结合了基于基础模型的特征编码器（用于提取解剖结构特征）和通用配准头，采用通道正则化策略在单一数据集上进行训练。

Result: FMIR在域内性能达到SOTA水平，同时在域外图像上保持鲁棒的配准能力，展示了用有限资源构建可泛化医学成像基础模型的可行路径。

Conclusion: 该研究提出了一个能够克服深度学习配准方法泛化限制的框架，为资源有限情况下构建通用医学成像基础模型提供了有效途径。

Abstract: Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.

</details>


### [67] [Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries](https://arxiv.org/abs/2601.17535)
*Kevin Robbins,Xiaotong Liu,Yu Wu,Le Sun,Grady McPeak,Abby Stylianou,Robert Pless*

Main category: cs.CV

TL;DR: 该研究提出通过生成合成图像来评估视觉语言模型（如CLIP）在特定任务上的零样本性能，相比仅使用文本的方法能更准确地预测模型效果


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型虽然能让非专家用户通过文本描述构建视觉分类器，但用户难以评估模型在特定领域的效果。现有基于文本比较的方法存在局限性，需要更可靠的零样本性能评估方法。

Method: 在仅使用文本比较的基线方法基础上，生成与任务相关的合成图像来评估和优化零样本准确率预测。通过图像提供更丰富的评估信号，并让用户了解评估所用的图像类型。

Result: 在标准CLIP基准数据集上的实验表明，基于图像的方法相比纯文本基线显著提升了零样本性能预测质量。该方法能在无需标注样本的情况下帮助用户预测VLM在特定应用中的有效性。

Conclusion: 生成合成图像作为评估信号能有效提升视觉语言模型零样本性能的预测准确性，为非专家用户提供了更可靠的模型适用性评估工具，同时提供可视化反馈帮助理解评估依据。

Abstract: Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.

</details>


### [68] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: 论文提出了一种新的图像攻击性度量方法——目标纹理强度(OTI)，这是一种模型无关且视觉可解释的度量方式，通过测量图像语义目标的纹理强度来评估图像的可攻击性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像可攻击性度量方法存在两大局限性：1)依赖模型代理提供先验知识，而实际应用中许多任务特定模型不易获取；2)提取的特征缺乏视觉可解释性，难以直接与图像关联。这促使研究者开发一种模型无关且视觉可解释的度量方法。

Method: 提出目标纹理强度(OTI)作为图像可攻击性的度量指标，通过测量图像语义目标的纹理强度来评估其可攻击性。该方法从决策边界以及对抗扰动的中高频特性两个理论角度阐述了OTI的原理。

Result: 综合实验表明，OTI在评估图像可攻击性方面既有效又计算高效。该方法为对抗机器学习社区提供了对可攻击性的视觉理解。

Conclusion: OTI是一种创新的图像可攻击性度量方法，解决了现有方法的局限性，具有模型无关性和视觉可解释性两大优势，为对抗机器学习研究提供了新的工具和视角。

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [69] [Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper](https://arxiv.org/abs/2601.17555)
*Justin Downes,Sam Saltwick,Anthony Chen*

Main category: cs.CV

TL;DR: 提出一种基于显著性图的卫星图像预处理方法，结合传统有损压缩标准，实现单张大卫星图像内的可变比特率压缩


<details>
  <summary>Details</summary>
Motivation: 卫星图像数据量巨大（每日数百TB），存储和带宽成本高昂。许多下游任务只关注图像中的小区域，这些区域因任务而异。传统图像编码方法对整个图像一视同仁，无法针对重要区域进行优化编码

Method: 使用显著性图驱动的图像预处理技术，结合传统有损压缩编码标准。具体采用可变大小的平滑核，映射到不同的量化显著性级别，处理图像像素以优化下游压缩和编码方案

Result: 实现了单张大卫星图像内的可变比特率压缩，能够在保持重要区域质量的同时，对非重要区域进行更强的压缩

Conclusion: 通过显著性图驱动的预处理技术与传统压缩标准结合，可以有效优化卫星图像的压缩效率，降低存储和带宽成本，同时满足下游任务对特定区域的精度要求

Abstract: The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.

</details>


### [70] [Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning](https://arxiv.org/abs/2601.17566)
*Qi Li,Xinchao Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对LLM工具调用过程的攻击方法——海绵工具攻击(STA)，通过在严格仅查询访问假设下重写输入提示，将原本简洁高效的推理路径转换为冗长复杂的路径，从而造成显著计算开销，同时保持任务语义和用户意图的隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 尽管增强LLM的外部工具方法在实现高效推理方面取得了成功，但其在工具调用过程中对恶意操纵的内在脆弱性尚未得到充分探索。研究旨在识别工具特定的攻击面，并开发一种能够在不修改底层模型或外部工具的情况下破坏智能体推理的攻击方法。

Method: 提出海绵工具攻击(STA)，采用迭代的多智能体协作框架，通过明确的改写策略控制，从原始提示生成具有高语义保真度的良性外观提示重写。攻击在严格仅查询访问假设下工作，仅通过重写输入提示来干扰智能体推理。

Result: 在6个模型(包括开源模型和闭源API)、12个工具、4个智能体框架和5个领域的13个数据集上的广泛实验验证了STA的有效性。攻击成功将原本简洁高效的推理轨迹转换为不必要的冗长复杂路径，导致显著的计算开销。

Conclusion: STA揭示了LLM工具调用过程的安全漏洞，表明即使在不修改模型或工具的情况下，仅通过提示重写也能有效破坏智能体推理。这强调了在开发LLM增强系统时需要考虑安全性和鲁棒性。

Abstract: Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.

</details>


### [71] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: 本文提出Stylizing ViT，一种新颖的视觉Transformer编码器，通过权重共享的注意力块同时进行自注意力和交叉注意力，实现医学图像风格化增强，提升跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中，深度学习模型因数据异质性和稀缺性而难以跨域和跨人口群体泛化。传统增强方法在显著域偏移下失效，现有风格化增强方法要么风格多样性不足，要么会引入伪影。

Method: 提出Stylizing ViT，采用权重共享的注意力块设计，同一注意力块通过自注意力保持解剖结构一致性，通过交叉注意力进行风格迁移，在组织病理学和皮肤病学图像分类任务上进行数据增强评估。

Result: 方法在三个图像分类任务上实现优于现有技术的鲁棒性（最高提升13%准确率），生成无伪影的感知逼真图像；在测试时增强中也能带来17%的性能提升。

Conclusion: Stylizing ViT有效解决了医学图像风格化增强中的风格多样性和伪影问题，显著提升了模型的跨域泛化能力，在训练和推理阶段均有显著效果。

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [72] [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657)
*Taewan Cho,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.CV

TL;DR: SPACE-CLIP提出了一种直接从冻结的CLIP视觉编码器中提取几何知识的双路径解码器架构，绕过了文本编码器和文本提示，实现了语义理解和几何结构的协同融合。


<details>
  <summary>Details</summary>
Motivation: CLIP在语义理解方面表现出色，但难以感知几何结构。现有方法通过文本提示查询CLIP的方式间接且低效，需要一种更直接有效的方法来提取CLIP中的几何知识。

Method: 提出SPACE-CLIP双路径解码器架构：1）语义路径解释高层特征，使用FiLM基于全局上下文动态调节；2）结构路径从早期层提取细粒度空间细节；3）两个互补流通过分层融合实现语义上下文和精确几何的鲁棒合成。

Result: 在KITTI基准测试中，SPACE-CLIP显著优于之前的CLIP基方法。消融研究验证了双路径协同融合对成功的关键作用。

Conclusion: SPACE-CLIP提供了一种高效、架构优雅的蓝图，可用于重新利用大规模视觉模型。该方法不仅是独立的深度估计器，还是下一代具身AI系统（如VLA模型）中可轻松集成的空间感知模块。

Abstract: Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip

</details>


### [73] [Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting](https://arxiv.org/abs/2601.17666)
*Xinyue Pan,Yuhao Chen,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出Prompt Grafting (PG)框架，解决扩散模型生成多食物图像时物体纠缠的问题，通过两阶段空间提示嫁接实现可控的食物分离


<details>
  <summary>Details</summary>
Motivation: 现实世界餐食图像通常包含多种食物，可靠的组合式食物图像生成对于基于图像的饮食评估和食谱可视化等应用至关重要。然而，现代文本到图像扩散模型在生成准确的多食物图像时存在困难，因为相邻食物（如米饭和汤）会融合在一起，这是由于许多食物没有清晰的边界。

Method: 引入Prompt Grafting (PG)，一种无需训练的框架，将文本中的显式空间提示与采样过程中的隐式布局指导相结合。PG运行两阶段过程：首先布局提示建立不同区域，一旦布局形成稳定，目标提示被嫁接。该框架实现食物纠缠控制：用户可以通过编辑布局安排来指定哪些食物应保持分离或有意混合。

Result: 在两个食物数据集上，该方法显著提高了目标物体的存在性，并提供了可控分离的定性证据。

Conclusion: Prompt Grafting框架有效解决了扩散模型生成多食物图像时的物体纠缠问题，通过空间提示嫁接实现了可控的食物分离，为图像生成在饮食评估和食谱可视化等应用提供了可靠解决方案。

Abstract: Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.

</details>


### [74] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: Uni-RS模型解决了遥感多模态模型中的空间反转诅咒问题，即在图像理解中能准确识别物体位置关系，但在文本到图像生成中却难以忠实执行相同空间关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有统一的遥感多模态模型存在明显的空间反转诅咒：虽然能准确识别和描述图像中的物体位置，但在文本到图像生成时却经常无法忠实地执行相同的空间关系，而这些空间关系是遥感领域的核心语义信息。

Method: 1. 显式空间布局规划：将文本指令转换为空间布局计划，将几何规划与视觉合成解耦；2. 空间感知查询监督：使可学习查询偏向于指令中明确指定的空间关系；3. 图像-标题空间布局变化：让模型接触系统性的几何一致的空间变换。

Result: 在多个基准测试上的广泛实验表明，该方法显著提高了文本到图像生成的空间忠实度，同时在图像描述、视觉定位和VQA等多模态理解任务上保持了强大的性能。

Conclusion: Uni-RS是首个专门为遥感设计的统一多模态模型，通过显式解决理解和生成之间的空间不对称问题，有效缓解了空间反转诅咒，在保持理解能力的同时显著提升了生成任务的空间忠实度。

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [75] [StyleDecoupler: Generalizable Artistic Style Disentanglement](https://arxiv.org/abs/2601.17697)
*Zexi Jia,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: StyleDecoupler是一个信息论框架，通过利用单模态模型抑制风格而专注于内容不变特征的特点，从多模态视觉模型中解耦出纯风格特征，无需微调即可作为即插即用模块。


<details>
  <summary>Details</summary>
Motivation: 艺术风格表示具有挑战性，因为风格与语义内容深度纠缠。现有方法难以有效分离风格和内容信息。

Method: 利用多模态视觉模型编码风格和内容，而单模态模型抑制风格以专注于内容不变特征的特点，以单模态表示作为内容参考，通过互信息最小化从多模态嵌入中分离纯风格特征。

Result: 在WeART（28万艺术品，152种风格，1556位艺术家）和WikiART基准测试中实现最先进的风格检索性能，并支持风格关系映射和生成模型评估等应用。

Conclusion: StyleDecoupler提供了一个有效的风格解耦框架，无需微调即可在冻结的视觉语言模型上工作，并发布了大规模艺术数据集WeART，推动了艺术风格分析的研究。

Abstract: Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.

</details>


### [76] [An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays](https://arxiv.org/abs/2601.17703)
*Nikhil Kadivar,Guansheng Li,Jianlu Zheng,John M. Higgins,Ming Dao,George Em Karniadakis,Mengjia Xu*

Main category: cs.CV

TL;DR: 提出一个自动化深度学习框架，整合AI辅助标注、分割、分类和实例计数，用于量化时间序列显微镜数据中不同密度下的红细胞群体动态变化。


<details>
  <summary>Details</summary>
Motivation: 需要准确识别镰状细胞在不同生物物理条件下的形态转变，特别是在密集堆积和重叠的细胞群体中，以理解镰状细胞动力学。

Method: 使用Roboflow平台标注实验图像生成训练数据集，训练nnU-Net分割模型，结合分水岭算法解决细胞重叠问题，实现细胞分割、分类和计数。

Result: 仅需少量标注数据即可获得高分割性能，有效解决手动标注稀缺和细胞重叠的挑战；能够量化追踪红细胞形态动态变化，通过密集细胞悬浮液将实验通量提高一倍以上，捕获药物依赖性镰状化行为，揭示细胞形态演变的独特机械生物学特征。

Conclusion: 该AI驱动框架建立了一个可扩展、可重复的计算平台，用于研究细胞生物力学和评估微生理系统中的治疗效果。

Abstract: Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

</details>


### [77] [Advancing Structured Priors for Sparse-Voxel Surface Reconstruction](https://arxiv.org/abs/2601.17720)
*Ting-Hsun Chi,Chu-Rong Chen,Chi-Tun Hsu,Hsuan-Ting Lin,Sheng-Yu Huang,Cheng Sun,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 该论文提出结合3D高斯泼溅和稀疏体素光栅化优势的方法，通过智能体素初始化和深度几何监督，提高表面重建的几何精度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅收敛快且有几何先验，但表面保真度受限于点状参数化；稀疏体素光栅化提供连续不透明度场和清晰几何，但均匀密集网格初始化收敛慢且未充分利用场景结构。需要结合两者优势。

Method: 1) 提出体素初始化方法，将体素放置在合理位置并具有适当细节层次；2) 提出精细深度几何监督，将多视图线索转换为直接的每射线深度正则化，增强深度一致性而不模糊边缘。

Result: 在标准基准测试中，相比先前方法在几何精度、细结构恢复和表面完整性方面都有改进，同时保持快速收敛。

Conclusion: 通过结合3D高斯泼溅和稀疏体素光栅化的互补优势，提出了一种有效的表面重建方法，提高了几何精度并加速了优化过程。

Abstract: Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.

</details>


### [78] [Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study](https://arxiv.org/abs/2601.17723)
*Tayyab Nasir,Daochang Liu,Ajmal Mian*

Main category: cs.CV

TL;DR: 该研究对基于隐式神经表示（INR）的任意尺度图像超分辨率（ASSR）方法进行了首次系统性实证分析，比较了现有技术，建立了统一框架，并提出了新的损失函数来提升纹理保真度。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对INR-based ASSR方法的系统性实证研究，无法有效评估现有方法的真实性能、确定性能饱和点、以及探索训练配置（如缩放定律、目标设计和优化策略）的影响。该研究旨在填补这一空白，为领域建立基准并指明有前景的研究方向。

Method: 1. 在不同设置下比较现有INR-based ASSR技术，在多个图像质量指标上汇总性能结果；2. 贡献统一的框架和代码库以促进可重复比较；3. 研究受控训练配置对感知图像质量的影响；4. 提出新的损失函数，在训练过程中惩罚强度变化同时保留边缘、纹理和细节。

Result: 研究发现：1. 近期更复杂的INR方法相比早期方法仅提供边际改进；2. 模型性能与训练配置强相关，这是先前工作忽视的因素；3. 提出的损失函数能提升不同架构的纹理保真度，强调了目标设计对特定感知增益的作用；4. 缩放定律适用于INR-based ASSR，确认了模型复杂度和数据多样性增加带来的可预测增益。

Conclusion: 该研究为INR-based ASSR领域提供了首个系统性实证分析，揭示了训练配置的重要性大于架构复杂性，并证明了缩放定律在该领域的适用性。提出的损失函数能有效提升纹理保真度，为未来研究提供了有价值的基准和方向。

Abstract: Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.

</details>


### [79] [Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles](https://arxiv.org/abs/2601.17733)
*Junran Lu,Yuanqi Li,Hengji Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 提出一种将边界表示（B-Rep）重构为k-单元粒子集合的新范式，通过解耦拓扑层次结构实现拓扑与几何的联合生成，支持无条件生成和多种条件任务。


<details>
  <summary>Details</summary>
Motivation: 现有B-Rep生成方法通常采用级联序列处理层次结构，无法充分利用单元间的几何关系（如邻接性和共享性），限制了上下文感知和错误恢复能力。

Method: 将B-Rep重构为组合k-单元粒子集合，相邻单元在接口处共享相同的潜在表示；采用多模态流匹配框架合成粒子集合，支持无条件生成和条件任务。

Result: 实验表明该方法能生成高保真CAD模型，在有效性和可编辑性方面优于现有方法，并能自然扩展到局部修复和非流形结构（如线框）的直接合成。

Conclusion: 通过解耦B-Rep的刚性层次结构，实现了拓扑与几何的联合生成，提高了上下文感知能力，为CAD模型的生成和编辑提供了更灵活有效的表示方法。

Abstract: Boundary Representation (B-Rep) is the widely adopted standard
  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.
  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.

</details>


### [80] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: 提出一个端到端的对话到电影视频生成框架，通过ScripterAgent将对话转化为可执行电影脚本，再通过DirectorAgent协调现有视频模型生成连贯长视频，解决现有模型在长叙事生成中的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型虽然能从简单文本提示生成视觉内容，但难以从高级概念（如对话）生成长形式、连贯的叙事，存在"语义鸿沟"问题。需要弥合创意想法与电影执行之间的差距。

Method: 1. 构建ScriptBench大规模基准数据集，包含丰富的多模态上下文，通过专家指导流程标注；2. 训练ScripterAgent将粗略对话转化为细粒度可执行电影脚本；3. DirectorAgent使用跨场景连续生成策略协调最先进的视频模型，确保长时程连贯性；4. 引入AI驱动的CriticAgent和新的视觉-脚本对齐（VSA）指标进行评估。

Result: 该框架显著提高了所有测试视频模型的脚本忠实度和时间保真度。分析发现当前SOTA模型在视觉奇观和严格脚本遵循之间存在关键权衡，为自动化电影制作的未来提供了有价值的见解。

Conclusion: 提出的端到端代理框架成功弥合了对话到电影视频生成的语义鸿沟，通过脚本生成和视频协调实现了长形式连贯叙事生成，为自动化电影制作提供了系统化解决方案。

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [81] [Learning Sewing Patterns via Latent Flow Matching of Implicit Fields](https://arxiv.org/abs/2601.17740)
*Cong Cao,Ren Li,Corentin Dumery,Hao Li*

Main category: cs.CV

TL;DR: 提出基于隐式表示的缝纫图案建模方法，使用符号距离场表示裁片边界和无符号距离场表示缝纫端点，通过连续隐空间实现可微分网格化，支持缝纫图案的生成、图像估计、补全和调整。


<details>
  <summary>Details</summary>
Motivation: 缝纫图案是服装的结构基础，对时尚设计、制造和物理模拟至关重要。由于裁片几何形状和接缝排列的广泛变异性，准确建模缝纫图案仍然具有挑战性。

Method: 使用符号距离场表示裁片边界，无符号距离场表示缝纫端点，编码到连续隐空间实现可微分网格化。通过隐流匹配模型学习裁片组合分布，缝合预测模块从提取的边缘段恢复接缝关系。

Result: 能够准确建模和生成具有复杂结构的缝纫图案，相比现有方法在图像估计缝纫图案方面精度更高，支持图案补全和重新拟合等应用。

Conclusion: 该方法为数字时尚设计提供了实用的缝纫图案建模工具，能够处理复杂的缝纫图案结构，并支持多种实际应用。

Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.

</details>


### [82] [Frequency-aware Neural Representation for Videos](https://arxiv.org/abs/2601.17741)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: FaNeRV通过频率感知的神经表示方法解决INR视频压缩中的频谱偏差问题，通过分离高低频分量实现高效保真的视频重建。


<details>
  <summary>Details</summary>
Motivation: 现有的基于隐式神经表示（INR）的视频压缩框架存在固有的频谱偏差，倾向于低频分量，导致重建结果过于平滑和率失真性能不理想。

Method: 1. 提出频率感知的神经表示方法，显式解耦低频和高频分量；2. 采用多分辨率监督策略，通过分阶段监督引导网络逐步捕获全局结构和细粒度纹理；3. 设计动态高频注入机制，自适应强调困难区域；4. 构建频率分解网络模块，改善不同频带的特征建模。

Result: 在标准基准测试中，FaNeRV显著优于最先进的INR方法，并在率失真性能上与传统编解码器竞争。

Conclusion: FaNeRV通过频率感知的神经表示方法有效解决了INR视频压缩中的频谱偏差问题，实现了高效且保真的视频重建，为神经视频压缩提供了新思路。

Abstract: Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.

</details>


### [83] [Video Compression with Hierarchical Temporal Neural Representation](https://arxiv.org/abs/2601.17743)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: 提出TeNeRV，一种分层时间神经视频表示方法，通过帧间特征融合和GoP自适应调制来捕获短期和长期时间依赖，在率失真性能上优于现有INR方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示(INR)的视频压缩方法通常将时间维度作为独立输入处理，这限制了它们捕获复杂时间依赖的能力。为了克服这一限制，需要一种能够有效建模视频中短期和长期时间依赖的新方法。

Method: 提出TeNeRV，包含两个关键组件：1)帧间特征融合(IFF)模块，通过聚合相邻帧特征来增强局部时间一致性和捕获细粒度运动；2)GoP自适应调制(GAM)机制，将视频划分为图像组并学习组特定先验，通过调制网络参数实现不同GoP间的自适应表示。

Result: 大量实验表明，TeNeRV在率失真性能上始终优于现有的基于INR的方法，验证了所提方法的有效性。

Conclusion: TeNeRV通过分层时间建模方法有效解决了现有INR视频压缩中时间依赖捕获不足的问题，为视频压缩提供了更有效的神经表示框架。

Abstract: Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.

</details>


### [84] [Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.17747)
*Kaixuan Jiang,Chen Wu,Zhenghui Zhao,Chengxi Han*

Main category: cs.CV

TL;DR: 本文提出了UniCD框架，一个统一的变化检测模型，能够同时处理监督、弱监督和无监督三种任务，通过共享编码器和多分支协作学习机制实现异构监督信号的深度耦合。


<details>
  <summary>Details</summary>
Motivation: 在真实世界的变化检测场景中，像素级标注获取成本高昂，现有模型难以适应不同标注可用性的多样化场景。需要一种统一框架来处理从有监督到无监督的各种监督级别任务。

Method: 提出UniCD框架，包含共享编码器和三个特定监督分支：1)监督分支采用时空感知模块(STAM)实现双时相特征的高效协同融合；2)弱监督分支构建变化表示正则化(CRR)，引导模型从粗粒度激活向连贯可分的变化建模收敛；3)无监督分支提出语义先验驱动变化推理(SPCI)，将无监督任务转化为可控的弱监督路径优化。

Result: 在主流数据集上的实验表明，UniCD在三种任务上均达到最优性能。在弱监督和无监督场景下表现出显著的精度提升，在LEVIR-CD数据集上分别超过当前最优方法12.72%和12.37%。

Conclusion: UniCD通过统一的架构成功解决了变化检测中监督级别多样化的问题，实现了异构监督信号的深度耦合，为变化检测领域提供了一种灵活且高性能的统一解决方案。

Abstract: Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.

</details>


### [85] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: 提出了多视角主题视频生成（MV-S2V）任务，解决现有S2V方法仅使用单视角参考的限制，通过多视角参考实现3D级别主题一致性。


<details>
  <summary>Details</summary>
Motivation: 现有主题视频生成方法局限于单视角参考，本质上将任务简化为S2I+I2V流程，未能充分利用视频主题控制的全部潜力。多视角参考对于实现真正的3D级别主题一致性至关重要。

Method: 1) 开发合成数据生成管道创建定制化训练数据，辅以小规模真实采集数据集；2) 提出时间移位RoPE（TS-RoPE）机制，在条件生成中区分不同主题和同一主题的不同视角。

Result: 框架在多视角参考图像方面实现了优越的3D主题一致性，并生成高质量视觉输出，为主题驱动视频生成开辟了新的有意义方向。

Conclusion: MV-S2V任务突破了现有S2V方法的单视角限制，通过多视角参考实现了真正的3D级别主题一致性，为视频生成领域提供了更精确的主题控制能力。

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [86] [Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation](https://arxiv.org/abs/2601.17791)
*Rabin Dulal,Wenfeng Jia,Lihong Zheng,Jane Quinn*

Main category: cs.CV

TL;DR: 提出一种基于3D重建的无接触牛只活重估计算法，使用多视角RGB图像和SAM 3D融合技术，在低数据条件下实现高效、经济的农场部署。


<details>
  <summary>Details</summary>
Motivation: 传统牛只称重方法（如走过式称重系统或体况评分）需要人工操作，既影响生产效率又增加经济成本。需要开发低成本、无接触的自动化解决方案来改善畜牧管理、动物福利和生产力。

Method: 提出基于多视角RGB图像的3D重建流水线：1）使用SAM 3D进行多视角一致性引导融合，为每头牛生成单一3D点云；2）在低数据条件下比较经典集成模型与深度学习模型的回归性能。

Result: SAM 3D多视角一致性融合优于其他3D生成方法；经典集成模型在农场实际场景中表现最稳定（R² = 0.69 ± 0.10，MAPE = 2.22 ± 0.56%），适合农场部署。

Conclusion: 对于农场部署，改善3D重建质量比增加模型复杂度更为关键，特别是在难以获取大量3D数据的实际场景中。该方法为规模化农场应用提供了实用解决方案。

Abstract: Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.

</details>


### [87] [ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning](https://arxiv.org/abs/2601.17818)
*Wen Luo,Peng Chen,Xiaotao Huang,LiQun Huang*

Main category: cs.CV

TL;DR: ViTCoP是一个视觉-语言模型的视觉令牌剪枝框架，通过视觉编码器冗余过滤和基于LLM层次特性的逐步协同剪枝，有效保留关键且信息多样的视觉令牌，显著降低计算成本和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 大型视觉-语言模型存在视觉令牌冗余问题，现有剪枝方法要么在视觉编码器中过早丢失关键信息，要么在LLM中导致选择令牌间的信息冗余。需要一种能有效保留关键视觉信息同时减少冗余的方法。

Method: 提出ViTCoP框架：1）在视觉编码器中进行冗余过滤；2）基于LLM层次特性进行逐步协同剪枝；3）引入K向量L2范数作为LLM中令牌显著性度量，确保与FlashAttention等加速技术兼容。

Result: 在多种大型视觉-语言模型上的实验表明，ViTCoP在图像和视频理解任务上超越现有方法达到SOTA性能，同时显著降低推理延迟和GPU内存消耗。在极端剪枝率下性能优势更加明显。

Conclusion: ViTCoP通过视觉与文本语义协同剪枝，有效解决了LVLM中视觉令牌冗余问题，在保持性能的同时大幅提升计算效率，特别是在高剪枝率下表现优异。

Abstract: Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.

</details>


### [88] [VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training](https://arxiv.org/abs/2601.17830)
*Mengmeng Wang,Dengyang Jiang,Liuzhuozheng Li,Yucheng Lin,Guojiang Shen,Xiangjie Kong,Yong Liu,Guang Dai,Jingdong Wang*

Main category: cs.CV

TL;DR: 本文提出了一个轻量级的内在引导框架，利用预训练VAE特征加速扩散变换器训练，无需外部依赖，仅增加4%计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的去噪扩散变换器训练收敛效率低，现有加速方法如REPA依赖外部编码器或SRA需要双模型设置，都会带来大量计算开销。

Method: 利用现成的预训练VAE特征，通过轻量级投影层将扩散变换器的中间潜在特征与VAE特征对齐，使用特征对齐损失进行监督。

Result: 实验表明该方法提高了生成质量和训练收敛速度，匹配或优于最先进的加速方法，仅增加4%GFLOPs计算开销且无需外部引导模型成本。

Conclusion: 提出了一个简单有效的内在引导框架，通过利用预训练VAE特征加速扩散变换器训练，实现了高效且轻量化的训练加速方案。

Abstract: Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.

</details>


### [89] [Geometry-Grounded Gaussian Splatting](https://arxiv.org/abs/2601.17835)
*Baowen Zhang,Chenxing Jiang,Heng Li,Shaojie Shen,Ping Tan*

Main category: cs.CV

TL;DR: 论文提出了一种基于随机固体理论的几何基础高斯泼溅方法，通过将高斯基元视为随机固体来直接提取几何形状，解决了现有方法多视角一致性差和对漂浮物敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅（GS）在新视角合成中表现出色，但从高斯基元中提取几何形状仍然是一个未解决的问题。现有方法由于几何参数化和近似不足，存在多视角一致性差和对漂浮物敏感的问题。

Method: 通过严谨的理论推导，将高斯基元确立为一种特定类型的随机固体。利用随机固体的体积性质，高效渲染高质量深度图进行细粒度几何提取，实现几何基础的高斯泼溅。

Result: 实验表明，该方法在公共数据集上实现了所有基于高斯泼溅方法中最好的形状重建结果。

Conclusion: 通过将高斯基元理论化为随机固体，为几何基础的高斯泼溅提供了原理性基础，能够直接处理高斯基元作为显式几何表示，显著提升了形状重建的质量和一致性。

Abstract: Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

</details>


### [90] [SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction](https://arxiv.org/abs/2601.17857)
*Lan Yang,Minghan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 提出SynMind框架，通过将fMRI信号解析为句子级语义描述，结合视觉先验指导扩散模型，显著改善了fMRI图像重建的语义对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI图像重建方法虽然视觉质量高，但存在严重的语义错位问题——重建图像中的显著物体经常被替换或幻觉生成。这是因为现有方法过度依赖纠缠的视觉嵌入，优先考虑纹理和全局特征等低级外观线索，而忽略了明确的语义身份。

Method: 1. 将fMRI信号解析为丰富的句子级语义描述，反映人类视觉理解的层次化和组合性特征；2. 利用基于视觉语言模型生成类似人类的、多粒度的文本表示，捕捉物体身份和空间组织；3. 提出SynMind框架，将明确的语义编码与视觉先验结合，用于指导预训练的扩散模型。

Result: 1. SynMind在大多数定量指标上优于现有最先进方法；2. 通过将语义推理卸载到文本对齐模块，SynMind在使用更小的Stable Diffusion 1.4和单个消费级GPU的情况下，超越了基于SDXL的竞争方法；3. 大规模人类评估证实SynMind产生的重建结果更符合人类视觉感知；4. 神经可视化分析显示SynMind激活了更广泛、语义更相关的大脑区域，减少了对高级视觉区域的过度依赖。

Conclusion: 通过重新思考明确语义解释在fMRI解码中的作用，SynMind框架有效解决了现有fMRI图像重建中的语义错位问题，实现了更好的语义对齐和更符合人类感知的重建结果，同时保持了计算效率。

Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.

</details>


### [91] [Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment](https://arxiv.org/abs/2601.17862)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出一种轻量级量子增强域泛化框架，通过MobileNetV2域不变编码器、多域成像偏移模拟、域对抗训练和量子特征增强层，实现医学影像AI在未见目标域上的鲁棒泛化，无需真实多中心标注数据。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI模型在单中心/单设备设置下表现良好，但在真实世界跨中心部署时因域偏移导致性能下降，限制了临床泛化能力。需要解决域偏移问题，提高模型在未见目标域上的鲁棒性。

Method: 1) 使用亮度、对比度、锐化和噪声扰动模拟多域成像偏移；2) 采用梯度反转的域对抗训练抑制域判别特征；3) 设计轻量级量子特征增强层，应用参数化量子电路进行非线性特征映射和纠缠建模；4) 在推理时使用测试时间自适应策略。

Result: 在模拟多中心医学影像数据集上的实验表明，该方法在未见域上显著优于无域泛化或无量子增强的基线模型，降低了域特定性能方差，提高了AUC和灵敏度。

Conclusion: 该研究展示了在计算资源受限情况下量子增强域泛化的临床潜力，为混合量子-经典医学影像系统提供了可行范式，能够实现鲁棒的跨中心泛化。

Abstract: Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.

</details>


### [92] [MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance](https://arxiv.org/abs/2601.17866)
*Yoonwoo Jeong,Cheng Sun,Yu-Chiang Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: MV-SAM是一个用于多视角分割的框架，通过利用视觉几何模型重建的3D点云（pointmaps）来实现3D一致性，无需3D网络或标注3D数据。


<details>
  <summary>Details</summary>
Motivation: 现有的可提示分割方法（如SAM）在扩展到视频和多视角图像时缺乏3D感知能力，导致结果不一致，需要昂贵的逐场景优化来强制3D一致性。

Method: MV-SAM扩展SAM，利用pointmaps的像素-点一一对应关系，将图像和提示提升到3D空间。通过将SAM预训练编码器的图像嵌入提升为3D点嵌入，使用transformer通过交叉注意力与3D提示嵌入进行解码。

Result: 在SA-1B数据集上训练，方法在不同领域泛化良好，在NVOS、SPIn-NeRF、ScanNet++、uCo3D和DL3DV基准测试中优于SAM2-Video，并与逐场景优化基线方法性能相当。

Conclusion: MV-SAM通过将2D交互与3D几何对齐，使模型能够通过3D位置嵌入隐式学习跨视图的一致掩码，实现了无需显式3D网络或标注3D数据的3D一致多视角分割。

Abstract: Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.

</details>


### [93] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: VidLaDA是一种基于扩散语言模型的视频LLM，通过双向注意力机制解决自回归模型中的因果掩码偏差问题，同时引入MARS-Cache框架加速推理，在保持准确性的同时实现12倍加速。


<details>
  <summary>Details</summary>
Motivation: 标准自回归视频LLM存在因果掩码偏差，阻碍全局时空建模，导致理解效率低下。需要一种能够捕捉双向依赖关系并解决扩散解码在大量视频token上推理瓶颈的方法。

Method: 提出VidLaDA视频LLM，基于扩散语言模型利用双向注意力捕捉双向依赖。引入MARS-Cache框架，通过异步视觉缓存刷新和帧级分块注意力加速推理，同时使用锚点token保持全局连接性。

Result: VidLaDA在实验中超越了扩散基线模型，并可与最先进的自回归模型（如Qwen2.5-VL和LLaVA-Video）相媲美。MARS-Cache在不损害推理准确性的情况下实现了超过12倍的加速。

Conclusion: VidLaDA通过扩散语言模型和双向注意力有效解决了自回归视频LLM的因果掩码偏差问题，MARS-Cache框架显著提升了推理效率，为视频理解提供了高效准确的解决方案。

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [94] [Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran](https://arxiv.org/abs/2601.17880)
*Muhammad Umar Salman,Mohammad Areeb Qazi,Mohammed Talha Alam*

Main category: cs.CV

TL;DR: Quran MD是一个全面的古兰经多模态数据集，集成了经文和单词级别的文本、语言学和音频维度，包含32位不同诵经者的音频，支持NLP、语音识别、文本转语音等多种应用。


<details>
  <summary>Details</summary>
Motivation: 古兰经有着丰富的口传传统和多样的诵经风格，但缺乏一个整合文本、语音和语言学信息的综合数据集来支持计算研究。

Method: 构建了一个多层次数据集：在经文级别提供阿拉伯原文、英文翻译和音标转写；在单词级别提供对齐的阿拉伯文字、英文翻译、转写和音频片段；收录了32位不同诵经者的音频以捕捉风格和方言差异。

Result: 创建了Quran MD数据集，包含全面的古兰经多模态数据，支持从发音分析到语义检索的多种计算任务，已在Hugging Face平台公开发布。

Conclusion: 该数据集填补了古兰经计算研究的空白，为ASR、tajweed检测、TTS等任务提供了基础，支持多模态嵌入、风格迁移等高级应用，促进数字伊斯兰研究和社区应用。

Abstract: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset

</details>


### [95] [Revisiting 3D Reconstruction Kernels as Low-Pass Filters](https://arxiv.org/abs/2601.17900)
*Shengjun Zhang,Min Chen,Yibo Wei,Mingyu Dong,Yueqi Duan*

Main category: cs.CV

TL;DR: 本文从信号处理角度重新审视3D重建，将离散采样引起的周期性频谱扩展作为核心挑战，提出使用Jinc核作为理想低通滤波器，并进一步提出调制核来平衡空间效率和频域保真度。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建核（如高斯、指数函数、Student's t分布）作为低通滤波器存在非理想特性，导致高频分量与低频分量在离散信号频谱中重叠，限制了重建质量。

Method: 引入Jinc核作为理想低通滤波器，其幅度在截止频率处瞬时降为零；进一步提出调制核来平衡Jinc核在空间域衰减慢的问题，实现空间效率和频域保真度的折中。

Result: 实验结果表明，Jinc核和调制核在渲染性能上表现优越，有效提升了3D重建质量。

Conclusion: 从信号处理视角重新思考3D重建问题，通过引入理想低通滤波特性的Jinc核和改进的调制核，显著提升了3D重建的渲染性能，为3D重建提供了新的理论基础和方法。

Abstract: 3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.

</details>


### [96] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: Gen1S：一种用于单样本类增量学习的新方法，通过残差空间映射和生成建模来改善新类识别


<details>
  <summary>Details</summary>
Motivation: 单样本类增量学习（FSCIL）面临极大挑战，当每个新类只有一个样本且不允许进一步训练时，模型难以泛化到新类。需要一种方法利用基类和新类之间的结构相似性来改善识别性能。

Method: 提出Gen1S方法：1）将原始嵌入空间映射到残差空间，通过减去输入样本的类原型（平均类嵌入）；2）使用VAE或扩散模型等生成模型学习基类残差的多模态分布；3）利用这种结构先验来改善新类识别。

Result: Gen1S在多个基准测试和骨干架构上，相比现有技术持续改善新类识别性能，在单样本设置下表现出色。

Conclusion: 通过残差空间映射和生成建模学习结构先验，可以有效解决单样本类增量学习中的泛化难题，为有限数据下的持续学习提供了新思路。

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [97] [Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models](https://arxiv.org/abs/2601.17918)
*Dain Kim,Jiwoo Lee,Jaehoon Yun,Yong Hoe Koo,Qingyu Chen,Hyunjae Kim,Jaewoo Kang*

Main category: cs.CV

TL;DR: 该论文首次在医疗领域对多种DPO变体进行全面评估，发现现有方法效果不稳定且无法解决视觉误读问题，提出了针对视觉误读的偏好构建策略，性能提升3.6%。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在医疗应用中有巨大潜力，但现有方法在医疗高风险场景下的对齐和可靠性不足，DPO方法在医疗领域的有效性缺乏实证研究基础。

Method: 在两个医疗LVLM模型(LLaVA-Med和HuatuoGPT-Vision)上评估9种不同的DPO变体，并提出针对视觉误读错误的偏好构建策略。

Result: 发现当前DPO方法效果不稳定，对监督微调的提升有限且任务间差异大，无法解决视觉误读错误；提出的新策略在视觉问答任务上比现有最强DPO基线提升3.6%。

Conclusion: 医疗领域需要专门设计的偏好优化方法，单纯应用现有DPO变体效果有限；提出的针对视觉误读的策略证明有针对性的偏好构建能有效提升性能，为未来研究提供基础。

Abstract: Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.

</details>


### [98] [RemEdit: Efficient Diffusion Editing with Riemannian Geometry](https://arxiv.org/abs/2601.17927)
*Eashan Adhikarla,Brian D. Davison*

Main category: cs.CV

TL;DR: RemEdit是一个扩散模型图像编辑框架，通过Riemannian流形导航和Mamba模块学习流形结构，实现语义保真度和推理速度的平衡，同时引入任务特定的注意力剪枝机制加速推理。


<details>
  <summary>Details</summary>
Motivation: 当前可控图像生成面临语义保真度和推理速度之间的关键权衡问题，现有方法往往在这两方面难以兼顾，需要一种既能保持编辑质量又能实现实时性能的解决方案。

Method: 1. 将潜在空间视为Riemannian流形，使用Mamba模块学习流形结构，计算精确的测地线路径进行平滑语义编辑；2. 采用dual-SLERP混合技术和视觉语言模型的目标感知提示增强；3. 引入任务特定的注意力剪枝机制，通过轻量级剪枝头保留对编辑至关重要的token。

Result: RemEdit超越了先前最先进的编辑框架，在50%剪枝率下仍保持实时性能，为实用强大的图像编辑建立了新的基准。

Conclusion: RemEdit通过创新的流形导航和任务特定注意力剪枝，成功解决了可控图像生成中语义保真度与推理速度的权衡问题，实现了高质量实时图像编辑。

Abstract: Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.

</details>


### [99] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: SC-SAM是一种专家-通用框架，通过U-Net和SAM之间的双向协同训练，利用无标签数据进行医学图像分割，在多个基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 基础模型如SAM在医学图像上存在领域偏移、标签稀缺的问题，且参数高效微调无法利用无标签数据。传统U-Net在半监督医学学习中表现出色，但其辅助PEFT SAM的潜力被忽视。

Method: 提出SC-SAM专家-通用框架：U-Net作为专家提供基于点的提示和伪标签指导SAM适应；SAM作为通用专家监督U-Net。这种双向指导形成协同训练循环，让两个模型都能有效利用无标签数据。

Result: 在前列腺MRI和息肉分割基准测试中，该方法取得了最先进的结果，优于其他现有的半监督SAM变体，甚至超越了MedSAM等医学基础模型。

Conclusion: 该研究展示了专家-通用合作在标签高效医学图像分割中的价值，为适应基础模型到医学领域提供了一种有效方法。

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [100] [DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation](https://arxiv.org/abs/2601.17939)
*Chengkun Sun,Jinqian Pan,Renjie Liang,Zhengkang Fan,Xin Miao,Jiang Bian,Jie Xu*

Main category: cs.CV

TL;DR: 提出了一种名为可变形转置卷积（DTC）的新型上采样方法，通过动态学习采样位置来改善医学图像分割中的特征重建和细节恢复能力。


<details>
  <summary>Details</summary>
Motivation: 传统上采样方法（如转置卷积和线性插值）使用固定的采样位置，可能无法捕捉预定义采样位置之外的结构信息，导致伪影或细节丢失。受到可变形卷积的启发，希望开发一种能够学习动态坐标的上采样方法。

Method: 提出了可变形转置卷积（DTC）方法，该方法学习动态坐标（即采样位置）来生成高分辨率特征图。该方法适用于2D和3D医学图像分割任务，并可集成到现有医学图像分割模型中。

Result: 在3D数据集（如BTCV15）和2D数据集（如ISIC18、BUSI）上的实验表明，DTC能有效集成到现有医学图像分割模型中，持续提高解码器的特征重建和细节恢复能力。

Conclusion: DTC作为一种新型上采样方法，通过动态学习采样位置，克服了传统固定位置上采样方法的局限性，在医学图像分割任务中表现出更好的特征重建和细节恢复性能。

Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.

</details>


### [101] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: FlowMorph是一个物理一致的自监督框架，通过微流控视频学习无标签的红细胞力学代理参数k，无需监督分割或手动特征提取。


<details>
  <summary>Details</summary>
Motivation: 红细胞的力学特性是血液和全身性疾病的有前景的生物标志物，但现有方法依赖监督分割或手动特征提取，且未充分利用层流斯托克斯流的物理原理。

Method: 使用低维参数化轮廓建模每个细胞，通过可微分的"流动中胶囊"模型结合层流平流和曲率正则化弹性松弛来推进边界点，优化结合轮廓重叠、细胞内流一致性、面积守恒、壁约束和时间平滑性的损失函数。

Result: 在四个公开数据集上实现平均轮廓IoU 0.905，显著改善面积守恒和壁约束违规；力学参数k能有效区分翻滚和翻转动态（AUC 0.863）；仅用200个RT-DC事件校准，可预测杨氏模量（MAE 0.118 MPa）。

Conclusion: FlowMorph是一个物理一致的自监督框架，能从微流控视频中无标签地学习红细胞力学特性，在多种条件下表现出鲁棒性和泛化能力。

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [102] [UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders](https://arxiv.org/abs/2601.17950)
*Matthew Walmer,Saksham Suri,Anirud Aggarwal,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: UPLiFT是一个通用像素密集轻量特征变换架构，通过本地注意力算子实现高效迭代上采样，在保持低推理成本的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的特征上采样方法中，交叉注意力方法存在与骨干网络类似的效率扩展问题，而早期迭代上采样方法性能有限。需要一种既能保持高效又能达到高性能的特征上采样方法。

Method: 提出UPLiFT架构，采用迭代上采样方法，并引入高效的本地注意力算子，该算子使用完全局部的注意力池化公式，克服了先前迭代特征上采样方法的局限性。

Result: UPLiFT在保持低推理成本的同时达到最先进的性能，比现有像素密集特征上采样器成本更低，并且在生成下游任务中与最先进的耦合流匹配模型竞争性相当。

Conclusion: UPLiFT提供了一种通用且高效的方法来创建更密集的特征，证明了迭代上采样方法仍然可以与基于交叉注意力的方法竞争，并在效率和性能之间取得了良好平衡。

Abstract: The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.

</details>


### [103] [Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors](https://arxiv.org/abs/2601.17977)
*Jinchen Gu,Nan Zhao,Lei Qiu,Lu Zhang*

Main category: cs.CV

TL;DR: 提出DKGH-MoE方法，结合数据驱动和领域专家知识，在医学影像分析中提升性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学领域的混合专家模型受限于小数据集，而临床实践中有丰富的专家知识（如医生注视模式、诊断启发式方法）无法从有限数据中可靠学习。结合数据驱动专家和领域专家指导的专家可以提供互补优势。

Method: 提出DKGH-MoE模块，包含数据驱动的MoE从原始影像数据提取新特征，以及领域专家指导的MoE整合临床先验知识（特别是临床医生眼动注视线索），强调高诊断相关性区域。

Result: 通过整合领域专家洞察和数据驱动特征，DKGH-MoE同时提升了性能和可解释性。

Conclusion: DKGH-MoE是一个即插即用、可解释的模块，统一了数据驱动学习和领域专业知识，为医学影像分析提供了更鲁棒和临床意义更强的学习框架。

Abstract: Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.

</details>


### [104] [MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images](https://arxiv.org/abs/2601.18001)
*Aqsa Yousaf,Sint Sint Win,Megan Coffee,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: MorphXAI是一个可解释的寄生虫检测框架，将形态学监督整合到预测流程中，不仅能定位寄生虫，还能分析其临床相关形态特征，提供结构化解释。


<details>
  <summary>Details</summary>
Motivation: 现有寄生虫检测深度学习方法缺乏可解释性，只能提供视觉热图或注意力图，无法捕捉临床医生依赖的形态特征，限制了临床实用性。

Method: 提出MorphXAI框架，将形态学监督直接集成到预测流程中，使模型能同时定位寄生虫并表征临床相关属性（形状、曲率、可见点计数、鞭毛存在性、发育阶段）。创建了包含三种寄生虫物种（利什曼原虫、布氏锥虫、克氏锥虫）的临床医生标注数据集。

Result: 实验结果表明MorphXAI不仅提高了检测性能，还提供了结构化、有生物学意义的解释，建立了可解释寄生虫分析的新基准。

Conclusion: MorphXAI通过统一寄生虫检测与细粒度形态分析，解决了深度学习模型在寄生虫诊断中可解释性不足的问题，为临床决策提供了更可信的支持。

Abstract: Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.

</details>


### [105] [Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection](https://arxiv.org/abs/2601.18008)
*Asiegbu Miracle Kanu-Asiegbu,Nitin Jotwani,Xiaoxiao Du*

Main category: cs.CV

TL;DR: 本文提出Strip-Fusion网络，通过时空融合和模态平衡策略提升多光谱行人检测性能，特别是在错位、遮挡和光照变化等挑战性条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前多光谱行人检测方法主要关注空间融合而忽略时序信息，且RGB和热成像图像对可能存在错位问题。行人检测还面临光照变化、遮挡等挑战，需要更鲁棒的解决方案。

Method: 提出Strip-Fusion时空融合网络：1) 使用时序自适应卷积动态加权时空特征；2) 设计KL散度损失缓解可见光和热成像模态不平衡；3) 开发新型后处理算法减少误报。

Result: 在KAIST和CVC-14基准测试中表现优异，在严重遮挡和图像错位等挑战性条件下相比之前SOTA方法有显著改进。

Conclusion: Strip-Fusion通过有效的时空融合和模态平衡机制，提升了多光谱行人检测的鲁棒性，特别是在实际应用中常见的错位、遮挡和光照变化条件下表现出色。

Abstract: Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.

</details>


### [106] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: 该论文提出了PIs-Regressor模块和Topology SegNet框架，通过将拓扑特征直接融入网络架构而非辅助损失函数，来提升医学图像中曲线结构的分割精度和拓扑保真度。


<details>
  <summary>Details</summary>
Motivation: 医学图像中曲线结构分割对临床分析至关重要，整合拓扑属性（如连通性）能提高分割精度和一致性。然而，从持久性图（PD）中提取和嵌入拓扑属性存在挑战，因为PD具有不可微性和计算成本高的问题。现有方法主要通过手工设计的损失函数编码拓扑，但跨任务泛化能力差。

Method: 提出了PIs-Regressor模块，直接从数据中学习持久性图像（PI）——拓扑特征的有限、可微表示。结合Topology SegNet框架，在网络的降采样和上采样阶段融合这些拓扑特征，将拓扑信息直接整合到网络架构中，而非通过辅助损失函数。

Result: 在三个曲线结构基准测试上展示了最先进的性能，在像素级精度和拓扑保真度方面都有显著提升。实验结果表明，整合拓扑特征增强了模型鲁棒性，能有效处理医学图像中的过曝和模糊等挑战。

Conclusion: 通过将拓扑特征直接融入网络架构而非依赖手工设计的损失函数，提出了一种简单有效的医学图像曲线结构分割框架。该方法灵活且可与其他拓扑方法无缝结合，进一步提升分割性能。

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [107] [Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling](https://arxiv.org/abs/2601.18049)
*Yunfei Qiu,Qiqiong Ma,Tianhua Lv,Li Fang,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出了一种结合空间先验信息与动态学习机制的新型半监督高光谱图像分类框架，通过边缘感知超像素标签传播和动态历史融合预测等方法，解决了边界标签扩散和伪标签不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 由于高光谱图像标注成本高、样本有限，半监督学习面临边界标签扩散和伪标签不稳定的挑战。现有方法在这些问题上的表现仍有不足。

Method: 1. 边缘感知超像素标签传播模块：结合边缘强度惩罚和邻域校正策略，缓解超像素分割的标签扩散问题；2. 动态历史融合预测方法：维护历史预测并动态加权，平滑伪标签波动；3. 自适应三方样本分类策略：基于置信度和一致性度量，对简单、模糊和困难样本进行分层利用；4. 动态可靠性增强伪标签框架：整合上述组件实现时空一致性优化。

Result: 在四个基准数据集上的评估表明，该方法能够保持优越的分类性能，有效解决了边界标签扩散和伪标签不稳定问题。

Conclusion: 提出的框架通过整合空间先验信息和动态学习机制，实现了半监督高光谱图像分类的时空一致性优化，显著提升了分类鲁棒性和伪标签稳定性。

Abstract: Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.

</details>


### [108] [Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification](https://arxiv.org/abs/2601.18088)
*Jianshu Chao,Tianhua Lv,Qiqiong Ma,Yunfei Qiu,Li Fang,Huifang Shen,Wei Yao*

Main category: cs.CV

TL;DR: 提出无源域标签的自监督跨域迁移框架，通过空间-光谱Transformer和频率域约束学习可迁移的联合表征，在目标域少量样本下实现高效适应。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖源域标注且易受分布偏移影响，导致目标域泛化性能下降。自监督学习在高光谱表征中潜力巨大，但在跨域迁移场景中应用不足。

Method: 1. 自监督预训练阶段：设计S2Former模块，采用双分支空间-光谱Transformer和双向交叉注意力机制实现光谱-空间协同建模；引入频率域约束(FDC)保持频域一致性。2. 微调阶段：提出DAFT蒸馏机制，通过师生结构对齐语义演化轨迹，实现低标签条件下的鲁棒迁移学习。

Result: 在四个高光谱数据集上展示了稳定的分类性能和强大的跨域适应能力，验证了方法在资源受限条件下的有效性。

Conclusion: 提出的自监督跨域迁移框架能够在不依赖源域标签的情况下学习可迁移的光谱-空间联合表征，在目标域少量样本条件下实现高效适应，为资源受限的高光谱分析提供了有效解决方案。

Abstract: Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.

</details>


### [109] [Text-Pass Filter: An Efficient Scene Text Detector](https://arxiv.org/abs/2601.18098)
*Chuang Yang,Haozhao Ma,Xu Han,Yuan Yuan,Qi Wang*

Main category: cs.CV

TL;DR: TPF采用类带通滤波器思路直接分割整个文本区域，避免了传统收缩-扩展策略的固有局限，能自然分离粘连文本且无需复杂后处理，实现实时检测。


<details>
  <summary>Details</summary>
Motivation: 现有文本检测方法采用收缩-掩码扩展策略，但收缩操作会丢失文本边缘的视觉特征并混淆前景背景差异，限制了文本特征的识别能力。

Method: 提出文本通滤波器(TPF)，为每个文本构建独特的特征-滤波器对，模拟带通滤波器原理，通过滤波器提取匹配文本并阻挡其他特征。引入增强集成单元(REU)增强同一文本特征一致性并扩大滤波器识别范围，以及前景先验单元(FPU)提升前景背景区分能力。

Result: 实验证明了REU和FPU的有效性，并展示了TPF的优越性能。

Conclusion: TPF通过直接分割整个文本区域，避免了传统方法的固有局限，能自然分离粘连文本且无需复杂解码或后处理，实现了实时文本检测。

Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.

</details>


### [110] [Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs](https://arxiv.org/abs/2601.18099)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 提出一种零训练前向计算框架，用于实时实现高斯模型，通过离散计算分析表达式从清晰图像生成失焦图像，并通过相似性度量筛选最佳匹配。


<details>
  <summary>Details</summary>
Motivation: 在之前高斯模型验证的基础上，开发一个无需训练的前向计算框架，以实现实时应用中的模型实现。

Method: 基于高斯核标准差应用范围内的分析表达式离散计算，从清晰图像生成失焦图像，通过邻域相似性度量筛选多个解为单一解，处理两幅图像互为部分模糊版本的情况。

Result: 在真实图像上的实验评估显示，该框架在估计合成模糊值时的平均绝对误差低于1.7%，实际模糊图像强度与对应估计值之间的差异保持在2%以下。

Conclusion: 提出的零训练前向计算框架能够有效实现高斯模型的实时应用，在模糊值估计和图像强度重建方面表现出高精度。

Abstract: Following the earlier verification for Gaussian model in \cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\%$, obtained by applying the extracted defocus filters to less blurred images.

</details>


### [111] [Spatial-Conditioned Reasoning in Long-Egocentric Videos](https://arxiv.org/abs/2601.18100)
*James Tribble,Hao Wang,Si-En Hong,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 该研究探讨了显式空间信号对VLM长时程自我中心视频理解的影响，通过Sanpo-D数据集和深度图融合实验，发现深度感知和空间基础表示能提升安全关键任务的性能。


<details>
  <summary>Details</summary>
Motivation: 长时程自我中心视频由于视角漂移和缺乏持久几何上下文，给视觉导航带来挑战。现有VLM在图像和短视频理解上表现良好，但在长序列空间推理能力有限。研究旨在探索显式空间信号如何影响VLM的视频理解能力。

Method: 1. 引入Sanpo-D数据集，对Google Sanpo数据集进行细粒度重新标注；2. 在导航导向的空间查询上对多个VLM进行基准测试；3. 通过将深度图与RGB帧融合，研究输入级归纳偏置的影响。

Result: 研究揭示了通用准确性与空间专业化之间的权衡关系，表明深度感知和空间基础表示能够提升行人检测和障碍物检测等安全关键任务的性能。

Conclusion: 显式空间信号对VLM的长时程自我中心视频理解具有重要影响，深度感知和空间基础表示能有效提升安全关键任务的性能，为视觉导航系统提供了有价值的改进方向。

Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.

</details>


### [112] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: LungCRCT：一种基于潜在因果表征学习的肺癌分析框架，通过因果发现和图自编码器技术，实现了肺癌治疗的因果干预分析和高效的恶性肿瘤分类


<details>
  <summary>Details</summary>
Motivation: 肺癌早期症状不明显且易与其他呼吸系统疾病混淆，导致早期发现困难。虽然基于CNN和ViT的AI模型在肺癌检测和肿瘤分类方面取得了显著进展，但这些模型存在相关性依赖、可解释性差等内在限制，难以扩展到肺癌治疗分析和因果干预分析。

Method: 提出LungCRCT框架，采用基于图自编码器的因果发现算法，结合距离相关解纠缠和基于熵的图像重建细化技术，从肺癌进展的物理因果机制中提取因果表征。

Result: 该框架不仅支持肺癌治疗的因果干预分析，还在恶性肿瘤分类任务中实现了93.91%的AUC得分，同时下游模型极其轻量且鲁棒。

Conclusion: LungCRCT通过因果表征学习克服了传统深度学习方法在肺癌分析中的局限性，为肺癌治疗分析和早期检测提供了更可解释、更有效的解决方案。

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [113] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: FoGA是一个轻量级视频异常检测模型，通过前向一致性学习和门控上下文聚合，在约200万参数下实现高性能检测，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法通常依赖大规模模型追求极致精度，限制了在资源受限边缘设备上的可行性。同时主流预测方法仅使用单帧未来预测误差，忽略了更长期的时序信息约束。

Method: 基于Unet架构，对连续帧进行特征提取生成即时和前向预测；在跳跃连接中引入门控上下文聚合模块动态融合编码器和解码器特征；使用前向一致性损失联合优化，采用混合异常测量策略整合即时和前向帧的误差。

Result: 在广泛实验中，FoGA显著优于现有最先进方法，运行速度高达155 FPS，在性能和效率指标之间取得了极佳的平衡。

Conclusion: FoGA作为轻量级视频异常检测模型，通过前向一致性学习和门控上下文聚合，在保持高性能的同时大幅降低计算复杂度，特别适合边缘设备部署，实现了性能与效率的良好权衡。

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [114] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: EGAgent：基于实体场景图的智能体框架，用于全天候可穿戴设备（如智能眼镜）产生的长时程自我中心视频理解，通过结构化搜索和推理实现跨模态、时序一致的理解。


<details>
  <summary>Details</summary>
Motivation: 全天候个人AI助手需要理解连续、纵向的自我中心视频流，而现有方法（如大语言模型、检索增强生成）受限于有限的上下文窗口，缺乏对超长视频流进行组合式、多跳推理的能力。

Method: EGAgent框架以实体场景图为核心，表示随时间变化的人物、地点、物体及其关系。规划智能体配备结构化搜索和推理工具，以及混合视觉和音频搜索能力，支持详细、跨模态、时序一致的推理。

Result: 在EgoLifeQA数据集上达到最先进性能（57.5%），在Video-MME（Long）数据集上取得有竞争力的性能（74.1%），展示了复杂长时程视频理解任务的有效性。

Conclusion: EGAgent通过实体场景图和智能体框架解决了长时程视频理解的挑战，为全天候个人AI助手提供了有效的上下文理解能力，支持跨模态、多跳推理。

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [115] [TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration](https://arxiv.org/abs/2601.18168)
*Zehua Liu,Shihao Zou,Jincai Huang,Yanfang Zhang,Chao Tong,Weixin Si*

Main category: cs.CV

TL;DR: 提出一种用于TACE手术的2D-3D血管配准方法，采用从粗到精策略：全局对齐模块SA-PnP建立对应关系，TempDiffReg时间扩散模型迭代优化血管形变，显著提升配准精度。


<details>
  <summary>Details</summary>
Motivation: TACE是肝癌常用治疗方法，但术中血管导航复杂且解剖变异大，需要精确的2D-3D血管配准来引导微导管定位和优化治疗靶向。

Method: 1. 结构感知透视n点（SA-PnP）全局对齐模块建立2D-3D血管结构对应关系；2. TempDiffReg时间扩散模型利用时间上下文迭代优化血管形变，捕捉复杂解剖变异和局部结构变化。

Result: 在23名患者626对多帧样本上评估，MSE 0.63mm，MAE 0.51mm，比现有最佳方法分别降低66.7%和17.7%，在准确性和解剖合理性上均优于SOTA方法。

Conclusion: 该方法能显著提升TACE手术中血管配准精度，有助于经验不足的临床医生安全高效完成复杂TACE手术，改善手术效果和患者护理。

Abstract: Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}

</details>


### [116] [YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection](https://arxiv.org/abs/2601.18172)
*Lin Huang,Yujuan Tan,Weisheng Li,Shitai Shan,Liu Liu,Bo Liu,Linlin Shen,Jing Yu,Yue Niu*

Main category: cs.CV

TL;DR: 提出YOLO-DS框架，通过双统计协同算子（DSO）解耦目标特征，结合两个轻量门控模块（DSG和MSG），在MS-COCO基准上显著提升YOLOv8性能（AP增益1.1%-1.7%），同时推理延迟增加极小。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO系列单阶段目标检测器缺乏对共享特征通道中异质目标响应的显式建模，限制了性能的进一步提升。

Method: 提出YOLO-DS框架，核心是新颖的双统计协同算子（DSO），通过联合建模通道均值和峰均差来解耦目标特征。基于DSO设计了两个轻量门控模块：用于自适应通道特征选择的双统计协同门控（DSG）模块，以及用于深度特征加权的多路径分段门控（MSG）模块。

Result: 在MS-COCO基准测试中，YOLO-DS在五个模型尺度（N, S, M, L, X）上均优于YOLOv8，AP增益达1.1%到1.7%，同时推理延迟仅轻微增加。可视化、消融和对比研究验证了方法的有效性。

Conclusion: YOLO-DS通过显式建模异质目标响应，在保持高效率的同时显著提升了目标检测性能，展示了在异质目标区分方面的优越能力。

Abstract: One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.

</details>


### [117] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: NaVIDA是一个通过逆向动力学增强的VLN框架，通过建模视觉变化与动作之间的因果关系来提升导航性能


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法大多基于反应式状态-动作映射，缺乏对动作如何因果性改变后续视觉观测的显式建模，导致行为不稳定、泛化能力弱和轨迹累积误差

Method: 1) 基于分块的逆向动力学监督学习视觉变化与动作的因果关系；2) 分层概率动作分块(HPAC)组织轨迹为多步分块，提供更长范围的视觉变化线索；3) 熵引导机制自适应设置动作分块的执行范围以控制误差累积

Result: 在实验中表现优于最先进方法，参数更少(3B vs 8B)，实际机器人评估验证了方法的可行性和有效性

Conclusion: NaVIDA通过建模视觉-动作因果关系，显著提升了VLN的导航性能、稳定性和泛化能力，为实际机器人应用提供了可行方案

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [118] [Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2601.18190)
*Yifan Li,Shiying Wang,Jianqiang Huang*

Main category: cs.CV

TL;DR: MPS-CLIP 是一种参数高效的遥感图像文本检索框架，通过关键词引导的细粒度对齐取代传统的全局匹配，使用LLM提取关键词、SAM生成语义子视角，结合G^2A适配器和MPR模块，在RSICD和RSITMD基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLP模型（如CLIP）主要依赖粗粒度全局对齐，忽略了遥感图像密集多尺度语义特征，且完全微调计算成本高并可能导致灾难性遗忘。

Method: 1. 使用LLM提取核心语义关键词，引导Segment Anything Model生成语义相关的子视角；2. 引入Gated Global Attention适配器高效适应冻结主干；3. 设计Multi-Perspective Representation模块聚合局部线索；4. 采用多视角对比和加权三元组损失的混合目标函数。

Result: 在RSICD和RSITMD基准测试中分别达到35.18%和48.40%的平均召回率（mR），显著优于完全微调基线和近期竞争方法。

Conclusion: MPS-CLIP成功将检索范式从全局匹配转向关键词引导的细粒度对齐，在参数效率、计算成本和性能方面都有显著优势，为遥感图像文本检索提供了新的解决方案。

Abstract: Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.

</details>


### [119] [MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models](https://arxiv.org/abs/2601.18192)
*Tian-Yi Zhou,Xuan-Hao Liu,Bao-Liang Lu,Wei-Long Zheng*

Main category: cs.CV

TL;DR: MindCine：一个通过多模态联合学习和预训练大EEG模型解决EEG-视频重建中单模态和数据稀缺问题的新框架


<details>
  <summary>Details</summary>
Motivation: EEG的非侵入性和高时间分辨率使其成为重建人类动态视觉感知的重要工具，但现有方法面临两个主要挑战：1）仅使用文本模态对齐EEG信号，忽略其他模态且容易过拟合；2）有限的EEG-视频数据导致训练困难

Method: 提出MindCine框架，采用多模态联合学习策略整合超越文本的模态，利用预训练大EEG模型缓解数据稀缺问题来解码语义信息，同时设计具有因果注意力的Seq2Seq模型来解码感知信息

Result: 大量实验表明，MindCine在定性和定量上都优于现有最先进方法，证明了不同模态互补优势的有效性，以及利用大规模EEG模型通过缓解数据限制挑战进一步提升重建性能

Conclusion: MindCine成功解决了EEG到视频重建中的关键问题，通过多模态融合和预训练模型利用，在有限数据条件下实现了高保真视频重建，为脑机接口和神经科学应用提供了有力工具

Abstract: Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.

</details>


### [120] [QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding](https://arxiv.org/abs/2601.18195)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Kaiwei Zhang,Jun Jia,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: QualiRAG是一种无需训练的检索增强生成框架，利用大型多模态模型的潜在感知知识进行视觉质量评估，通过动态生成辅助知识源实现可解释的质量理解。


<details>
  <summary>Details</summary>
Motivation: 当前视觉质量评估方法需要监督微调或强化学习，依赖于人工标注的数据集，存在标注劳动密集和数据集特定偏差的问题。需要一种无需训练的方法来获得细粒度时空感知和辅助上下文信息。

Method: 提出QualiRAG框架：1）将问题分解为结构化请求；2）构建四个互补知识源：视觉元数据、主体定位、全局质量总结和局部质量描述；3）进行相关性感知检索，实现基于证据的推理。整个过程无需训练。

Result: 在视觉质量理解任务上显著优于开源通用LMM和VQA微调LMM，在视觉质量比较任务上表现有竞争力，无需任务特定训练即可实现鲁棒的质量评估能力。

Conclusion: QualiRAG通过动态生成和检索多模态知识源，有效利用LMM的潜在感知知识，为无需训练的视觉质量评估提供了新范式，在多个任务上表现出色。

Abstract: Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.

</details>


### [121] [HomoFM: Deep Homography Estimation with Flow Matching](https://arxiv.org/abs/2601.18222)
*Mengfan He,Liangzheng Sun,Chunyu Li,Ziyang Meng*

Main category: cs.CV

TL;DR: 本文提出HomoFM框架，首次将流匹配技术应用于单应性估计任务，通过建模连续点速度场将噪声分布转换为配准坐标，并结合梯度反转层增强域适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度单应性估计方法通常将其视为直接回归或迭代优化问题，难以捕捉复杂几何变换或跨域泛化。本文旨在解决这些局限性。

Method: 1) 将单应性估计重新定义为速度场学习问题，通过条件流轨迹恢复高精度变换；2) 在特征提取主干中集成梯度反转层，学习域不变表示以增强鲁棒性。

Result: 大量实验表明，HomoFM在标准基准测试中，在估计精度和鲁棒性方面均优于最先进方法。

Conclusion: 提出的HomoFM框架通过流匹配技术和域适应策略，有效解决了深度单应性估计中的复杂变换建模和跨域泛化问题，实现了性能提升。

Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.

</details>


### [122] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: 提出基於EfficientNetB2的輕量級面部情緒識別管道，採用兩階段熱身與微調策略，結合多種優化技術，在FER-2013數據集上以較少參數達到68.78%測試準確率


<details>
  <summary>Details</summary>
Motivation: 解決現實場景中面部情緒識別的挑戰：低圖像質量、光照變化、姿勢變化、背景干擾、類間差異小、標籤噪聲和嚴重類別不平衡，同時降低計算成本和內存需求以實現實時應用

Method: 使用基於EfficientNetB2的輕量級管道，採用兩階段熱身與微調策略，結合AdamW優化、解耦權重衰減、標籤平滑(ε=0.06)、裁剪類別權重、dropout、混合精度訓練和實時數據增強

Result: 在FER-2013數據集上達到68.78%測試準確率，參數數量比VGG16基線減少近十倍，訓練穩定且泛化能力強

Conclusion: 提出的輕量級方法在保持高準確性的同時顯著降低計算成本，適合實時和邊緣計算應用，為現實場景面部情緒識別提供了實用解決方案

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [123] [V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering](https://arxiv.org/abs/2601.18240)
*Mengyuan Jin,Zehui Liao,Yong Xia*

Main category: cs.CV

TL;DR: V-Loop是一种无需训练、即插即用的幻觉检测框架，通过视觉逻辑循环验证来检测医疗VQA中的幻觉问题，优于现有自省方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医疗VQA中表现出色，但其输出容易产生幻觉（与视觉事实相矛盾的响应），这在高风险医疗场景中带来显著风险。现有不确定性方法虽然计算高效，但本质上是间接的，因为它们估计图像-问题对的预测不确定性，而不是验证特定答案的事实正确性。

Method: 提出视觉逻辑循环验证(V-Loop)框架：1) MLLM为原始输入生成答案；2) 从原始QA对中提取语义单元；3) 基于答案单元重新查询问题单元生成验证问题；4) 强制执行视觉注意力一致性，确保原始问题和验证问题都依赖相同的图像证据；5) 如果验证答案与预期语义内容匹配，逻辑循环闭合，表明事实基础正确，否则标记为幻觉。

Result: 在多个医疗VQA基准测试和MLLMs上的实验表明，V-Loop一致优于现有自省方法，保持高效性，并且与不确定性方法结合使用时能进一步提升性能。

Conclusion: V-Loop通过视觉逻辑循环验证提供了一种直接检测幻觉的有效方法，解决了医疗VQA中高风险场景的可靠性问题，为多模态医疗AI系统的安全部署提供了重要保障。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.

</details>


### [124] [Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation](https://arxiv.org/abs/2601.18242)
*Zerui Kang,Yishen Lim,Zhouyou Gu,Seung-Woo Ko,Tony Q. S. Quek,Jihong Park*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型(VLM)引导的框架，通过语义先验加速和稳定可微分射线追踪(DRT)中的多材料参数估计


<details>
  <summary>Details</summary>
Motivation: 传统基于梯度的逆向射线追踪对初始化敏感且在有限测量下成本高昂，需要更快速、更稳定的RF材料参数估计方法

Method: 使用VLM解析场景图像推断材料类别，通过ITU-R材料表映射到定量先验，为电导率提供智能初始化；VLM进一步选择信息丰富的发射器/接收器位置以促进多样化的材料区分路径；在DRT中进行基于梯度的细化

Result: 在NVIDIA Sionna室内场景实验中，相比均匀或随机初始化及随机放置基准，收敛速度提高2-4倍，最终参数误差降低10-100倍，仅需少量接收器即可达到低于0.1%的平均相对误差

Conclusion: VLM提供的语义先验能有效指导基于物理的优化，实现快速可靠的RF材料估计

Abstract: Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.

</details>


### [125] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: OrthoFoundation是一个针对肌肉骨骼病理学的多模态视觉基础模型，使用120万未标记的膝关节X光和MRI图像进行自监督对比学习，在14个下游任务中达到SOTA性能，并展现出卓越的跨解剖结构泛化能力。


<details>
  <summary>Details</summary>
Motivation: 肌肉骨骼疾病是全球致残的主要原因，需要精确的医学影像解读。当前骨科AI方法依赖任务特定的监督学习，存在碎片化、需要大量标注数据、跨模态和临床场景泛化能力不足的问题。该领域基础模型的发展因缺乏大规模、高质量的开源肌肉骨骼数据集而受限。

Method: 构建了包含内部和公共数据库的120万张未标记膝关节X光和MRI图像的预训练数据集。使用Dinov3骨干网络，通过自监督对比学习训练模型，以捕捉稳健的放射学表征。

Result: 在14个下游任务中达到最先进性能：在X光骨关节炎诊断中取得卓越准确率，在MRI结构损伤检测中排名第一。展现出显著的标签效率，仅使用50%的标注数据即可匹配监督基线。尽管仅在膝关节图像上预训练，但能出色地泛化到髋、肩和踝关节。

Conclusion: OrthoFoundation代表了肌肉骨骼影像通用AI的重要进展。通过从大规模多模态数据中学习基础的、关节无关的放射学语义，克服了传统模型的局限性，为减少标注负担和提高临床诊断准确性提供了稳健框架。

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [126] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: Co-PLNet提出点线协同框架，通过点线提示编码器和交叉引导线解码器实现线框解析中线段与交点的联合优化，提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有线框解析方法将线段和交点分开预测后处理，导致不匹配和鲁棒性降低。需要一种能够协同处理点线任务的框架来提升结构化几何感知能力。

Method: 提出Co-PLNet点线协同框架：1) 点线提示编码器将早期检测转换为空间提示，编码几何属性为紧凑的空间对齐图；2) 交叉引导线解码器使用稀疏注意力机制，基于互补提示细化预测，强制点线一致性。

Result: 在Wireframe和YorkUrban数据集上实验显示，在线框解析任务中实现了准确性和鲁棒性的持续改进，同时具备良好的实时效率。

Conclusion: Co-PLNet通过点线协同框架有效解决了传统方法中点线不匹配问题，为结构化几何感知提供了高效且准确的解决方案，在SLAM等下游任务中有应用潜力。

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [127] [Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images](https://arxiv.org/abs/2601.18260)
*Eytan Kats,Kai Geissler,Daniel Mensing,Jochen G. Hirsch,Stefan Heldman,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 提出基于深度学习的框架，从单张2D深度图像直接预测多个内部器官的3D位置和形状，用于自动患者定位


<details>
  <summary>Details</summary>
Motivation: 自动患者定位在优化扫描流程和提高患者吞吐量方面很重要，利用RGB-D相机捕获的深度信息为估计内部器官位置提供了有前景的方法

Method: 使用大规模全身MRI扫描数据集，合成深度图像与对应解剖分割配对，训练统一的卷积神经网络架构，无需显式表面重建

Result: 方法能准确定位多种解剖结构（包括骨骼和软组织），展示了将深度传感器集成到放射工作流程中以简化扫描程序和增强患者体验的潜力

Conclusion: 基于深度学习的深度图像分析为自动患者定位提供了有效解决方案，有望优化放射学工作流程

Abstract: Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.

</details>


### [128] [Revisiting Aerial Scene Classification on the AID Benchmark](https://arxiv.org/abs/2601.18263)
*Subhajeet Das,Susmita Ghosh,Abhiroop Chatterjee*

Main category: cs.CV

TL;DR: 本文对航拍图像分类的机器学习方法进行了综述，并提出了Aerial-Y-Net模型，该模型通过空间注意力增强和多尺度特征融合机制，在AID数据集上取得了91.72%的准确率。


<details>
  <summary>Details</summary>
Motivation: 航拍图像在城乡规划和环境保护中至关重要，但由于其包含建筑物、森林、山脉、空地等多种异质结构，开发鲁棒的场景分类模型仍然具有挑战性。

Method: 1. 对航拍图像分类的机器学习方法进行了文献综述，涵盖了从手工特征（SIFT、LBP）到传统CNN（VGG、GoogLeNet）再到先进的深度混合网络等方法；2. 设计了Aerial-Y-Net模型，该模型采用空间注意力增强机制和多尺度特征融合技术。

Result: 在AID数据集上的评估结果显示，Aerial-Y-Net模型达到了91.72%的分类准确率，优于多个基线架构。

Conclusion: Aerial-Y-Net通过空间注意力机制和多尺度特征融合，能够更好地理解航拍图像的复杂性，在航拍图像分类任务中表现出色，为相关应用提供了有效的解决方案。

Abstract: Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.

</details>


### [129] [Contextual Range-View Projection for 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.18301)
*Seyedali Mousavi,Seyedhamidreza Mousavi,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 本文提出两种基于上下文信息的范围图像投影方法CAP和CWAP，通过整合实例中心距离和类别权重来改进传统基于最小深度的投影策略，在SemanticKITTI数据集上取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR点云到范围图像的投影采用最小深度优先策略，忽略了语义相关性和物体结构信息，导致重要上下文信息的丢失。需要改进投影策略以更好地保留语义信息。

Method: 提出两种机制：1) Centerness-Aware Projection (CAP)：根据点到实例中心的距离调整深度值，优先保留实例中心点而非噪声边界点和背景点；2) Class-Weighted-Aware Projection (CWAP)：通过用户定义权重对物体类别进行优先级排序，提供投影策略的灵活性。

Result: 在SemanticKITTI数据集上的评估显示，CAP在投影过程中保留了更多实例点，相比基线实现了最高3.1%的mIoU提升。CWAP有效提升了目标类别的性能，同时对其他类别性能影响极小。

Conclusion: 通过整合上下文信息（实例中心和类别标签）来改进范围图像投影策略，能够更有效地保留语义信息，提升3D点云语义分割的性能。

Abstract: Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \textit{Centerness-Aware Projection (CAP)} and \textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes

</details>


### [130] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: 本文针对GUI代理中滑动交互执行能力不足的问题，提出了SwipeGen自动化流水线来合成类人滑动交互，并构建了首个滑动执行能力评测基准。基于此开发的GUISwiper代理在滑动执行准确率上相比现有VLM基线提升了214%。


<details>
  <summary>Details</summary>
Motivation: 随着GUI代理的广泛应用，任务指令到具体动作的感知能力已得到改善，但步骤执行能力逐渐成为任务完成的新瓶颈。现有GUI代理在处理滑动交互时采用过于简化的策略，无法准确复制类人行为。

Method: 1. 将人类滑动手势分解为多个可量化维度；2. 提出SwipeGen自动化流水线，通过GUI探索合成类人滑动交互；3. 基于此构建首个GUI代理滑动执行能力评测基准；4. 利用合成数据开发GUISwiper代理，增强交互执行能力。

Result: GUISwiper在滑动执行准确率上达到69.07%，相比现有VLM基线提升了214%。

Conclusion: 通过分解和量化人类滑动手势，并利用自动化流水线合成训练数据，可以显著提升GUI代理的滑动交互执行能力，解决现有代理在该方面的瓶颈问题。

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [131] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本研究提出了一种用于脑肿瘤MRI分析的高效密集Swin混合框架，通过两个肿瘤感知实验设置联合捕获细粒度纹理模式和长距离上下文依赖关系。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤MRI分析中同时捕获局部纹理特征和全局上下文依赖的挑战，特别是针对不同类型肿瘤（弥漫性胶质瘤、脑膜瘤、垂体瘤）的诊断特异性问题。

Method: 提出EDSH框架，包含两种设置：1) 增强特征空间：定制化的DenseNet和Swin分支学习互补的局部和全局表示，通过维度对齐、融合和增强机制；2) 分层DenseNet-Swin架构：具有双重残差连接的深度特征提取，DenseNet作为主干CNN学习结构化局部特征，Swin_t模型学习全局肿瘤形态。

Result: 在大规模MRI数据集（40,260张图像，四类肿瘤）上评估，表现优于独立CNN、Vision Transformer和混合模型，在未见测试数据集上达到98.50%的准确率和召回率。

Conclusion: EDSH框架通过联合学习局部纹理和全局上下文依赖，有效解决了不同类型脑肿瘤的诊断挑战，在脑肿瘤MRI分析中表现出卓越性能。

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [132] [PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction](https://arxiv.org/abs/2601.18336)
*Isaac Deutsch,Nicolas Moënne-Loccoz,Gavriel State,Zan Gojcic*

Main category: cs.CV

TL;DR: 提出PPISP模块，通过物理可解释的变换分离相机固有特性和拍摄依赖效应，解决多视角三维重建中的光度不一致问题


<details>
  <summary>Details</summary>
Motivation: 多视角三维重建方法对相机光学特性和图像信号处理（ISP）变化引起的光度不一致高度敏感，现有方法缺乏物理基础且对新视角泛化能力差

Method: 提出物理合理的ISP（PPISP）校正模块，通过基于物理的可解释变换分离相机固有和拍摄依赖效应；训练专门的PPISP控制器预测新视角的ISP参数

Result: 在标准基准测试中达到最先进性能，提供直观控制，支持元数据集成，无需真实图像即可实现新视角的逼真公平评估

Conclusion: PPISP通过物理可解释的ISP校正有效解决了多视角重建中的光度不一致问题，实现了更好的泛化能力和评估公平性

Abstract: Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp

</details>


### [133] [Beyond Rigid: Benchmarking Non-Rigid Video Editing](https://arxiv.org/abs/2601.18340)
*Bingzheng Qu,Kehai Chen,Xuefeng Bai,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: NRVBench：首个专门评估非刚性视频编辑的基准，包含高质量数据集、NRVE-Acc评估指标和VM-Edit训练免费基线方法，旨在解决非刚性变形中的物理失真和时间闪烁问题。


<details>
  <summary>Details</summary>
Motivation: 尽管文本驱动视频编辑取得了显著进展，但生成连贯的非刚性变形仍然是一个关键挑战，常常受到物理失真和时间闪烁的困扰。需要专门的基准来评估和改进非刚性视频编辑技术。

Method: 1. 构建包含180个非刚性运动视频的高质量数据集，涵盖6个基于物理的类别，配备2,340个细粒度任务指令和360个多项选择题。2. 提出基于视觉语言模型的NRVE-Acc评估指标，用于严格评估物理合规性、时间一致性和指令对齐。3. 引入训练免费的基线方法VM-Edit，采用双区域去噪机制实现结构感知控制。

Result: 大量实验表明，现有方法在保持物理合理性方面存在不足，而提出的VM-Edit方法在标准和提出的指标上都表现出色。

Conclusion: NRVBench可作为推进物理感知视频编辑的标准测试平台，有助于解决非刚性视频编辑中的关键挑战。

Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.

</details>


### [134] [Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception](https://arxiv.org/abs/2601.18346)
*Sijing Wu,Yunhao Li,Zicheng Zhang,Qi Jia,Xinyue Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: Q-Bench-Portrait 是首个专门针对肖像图像质量感知的综合性基准测试，包含2765个图像-问题-答案三元组，涵盖多种图像来源、质量维度和问题格式，用于评估多模态大语言模型在肖像图像感知方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在通用图像的低层视觉基准测试中表现良好，但在具有独特结构和感知特性的肖像图像领域的感知和评估能力尚未得到充分探索。

Method: 提出Q-Bench-Portrait基准测试，包含：1) 多种肖像图像来源（自然图像、合成失真、AI生成、艺术图像、计算机图形图像）；2) 全面的质量维度（技术失真、AIGC特定失真、美学）；3) 多种问题格式（单选、多选、判断、开放式问题），涵盖全局和局部层面。

Result: 评估了20个开源和5个闭源MLLM，发现当前模型在肖像图像感知方面虽然具备一定能力，但表现仍然有限且不精确，与人类判断存在明显差距。

Conclusion: Q-Bench-Portrait基准测试有望推动进一步研究，提升通用和领域特定MLLM在肖像图像感知方面的能力。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.

</details>


### [135] [OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI](https://arxiv.org/abs/2601.18368)
*Caterina Fuster-Barceló,Claudia Castrillón,Laura Rodrigo-Muñoz,Victor Manuel Vega-Suárez,Nicolás Pérez-Fernández,Gorka Bastarrika,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: OREHAS是首个用于从常规3D-MRI自动定量内耳膜迷路积水的全自动流程，通过深度学习和临床工作流整合，实现无需人工干预的精确容积比计算。


<details>
  <summary>Details</summary>
Motivation: 目前内耳膜迷路积水的定量评估依赖手动操作，存在操作者依赖性高、方法不一致的问题。需要开发自动化系统来提高评估的可靠性、可重复性和临床实用性。

Method: OREHAS集成了三个组件：切片分类、内耳定位和序列特异性分割，形成一个完整工作流，直接从完整MRI体积计算每耳的内淋巴-前庭容积比（ELR）。仅需每个患者3-6个标注切片进行训练。

Result: 在外部验证队列中，OREHAS与专家标注高度一致（VSI=74.3%），显著优于临床软件syngo.via（VSI=42.5%）。Dice分数在SPACE-MRC上达到0.90，REAL-IR上达到0.75。内淋巴容积测量更小且更符合生理实际。

Conclusion: OREHAS证明通过有限监督可以实现可靠、可重复的内耳膜迷路积水定量。该系统减少操作者依赖性，确保方法一致性，为大规模研究和临床诊断阈值重新校准提供坚实基础。

Abstract: We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.
  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.
  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.

</details>


### [136] [Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues](https://arxiv.org/abs/2601.18372)
*Christos Petrou,Harris Partaourides,Athanasios Balomenos,Yannis Kopsinis,Sotirios Chatzis*

Main category: cs.CV

TL;DR: 提出結合頭戴顯示器運動信號與視覺顯著性特徵的視線預測框架，在缺乏直接眼動追蹤的VR環境中有效預測未來視線方向


<details>
  <summary>Details</summary>
Motivation: VR應用中視線預測對降低延遲和實現如注視點渲染等計算密集型技術至關重要，但直接眼動追蹤常因硬體限制或隱私問題而不可用

Method: 結合HMD運動信號與影片幀的視覺顯著性提示，使用UniSal輕量顯著性編碼器提取視覺特徵，與HMD運動數據融合後通過時間序列預測模組處理，評估TSMixer和LSTM兩種輕量架構

Result: 在EHTask數據集和商業VR硬體上的實驗顯示，該方法持續優於Center-of-HMD和Mean Gaze等基線，有效降低感知延遲

Conclusion: 預測性視線建模在直接眼動追蹤受限的VR環境中能有效減少感知延遲並增強自然交互，結合運動和視覺提示的方法具有實用價值

Abstract: Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.

</details>


### [137] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: 提出一种基于网格状导频信号的水印同步方法，通过分析网格畸变估计几何变换矩阵，对裁剪等操作具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法对裁剪操作缺乏鲁棒性，裁剪会改变图像原点导致同步困难。需要一种能准确估计几何变换以实现水印同步的方法。

Method: 嵌入网格状导频信号（水平和垂直值不同），利用Radon变换分析畸变网格的角度和间隔来估计变换矩阵，通过不同编码确定网格方向以减少歧义。

Result: 在各项异性缩放、旋转、剪切和裁剪等攻击下，该方法能准确估计变换矩阵，在单一和复合攻击下均表现出低误差。

Conclusion: 提出的基于导频信号的水印同步方法能有效应对裁剪等几何变换，提高了水印提取的准确性和鲁棒性。

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [138] [ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks](https://arxiv.org/abs/2601.18386)
*Gabriel Lee Jun Rong,Christos Korgialas,Dion Jia Xu Ho,Pai Chet Ng,Xiaoxiao Miao,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: ARMOR框架使用VLM引导的智能体动态协调三种对抗攻击原语，通过实时调整参数和语义感知，提升攻击的跨架构迁移性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化攻击套件采用静态集成和固定序列，缺乏战略适应性和语义感知能力，限制了对抗攻击的效果和适应性。

Method: ARMOR框架通过VLM引导的智能体协作协调CW、JSMA和STA三种对抗攻击原语，使用共享的"Mixing Desk"生成和合成扰动，LLM实时调整参数，利用图像特定语义漏洞。

Result: 在标准基准测试中，ARMOR实现了改进的跨架构迁移性，可靠地欺骗黑盒和白盒设置，为黑盒目标提供混合输出，为白盒目标选择最佳攻击或混合攻击。

Conclusion: ARMOR框架通过动态智能体协调和语义感知，显著提升了对抗攻击的适应性和效果，为自动化攻击系统提供了新的设计思路。

Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.

</details>


### [139] [Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space](https://arxiv.org/abs/2601.18392)
*Moritz Rempe,Lukas T. Rotkopf,Marco Schlimbach,Helmut Becker,Fabian Hörst,Johannes Haubold,Philipp Dammann,Kevin Kröninger,Jens Kleesiek*

Main category: cs.CV

TL;DR: 提出kViT——一种直接在k空间数据上进行分类的复数视觉Transformer，无需图像重建，大幅提升计算效率


<details>
  <summary>Details</summary>
Motivation: 当前MRI深度学习方法在重建后的幅度图像上操作，丢弃了相位信息且需要昂贵的计算变换。传统神经网络架构（卷积或网格分块）不适合k空间数据的全局非局部特性。

Method: 提出复数视觉Transformer（kViT），设计径向k空间分块策略，尊重频域能量分布，直接在k空间数据上执行分类任务。

Result: 在fastMRI和内部数据集上，kViT达到与最先进图像域基线（ResNet、EfficientNet、ViT）相当的分类性能，对高加速因子表现出优越鲁棒性，训练时VRAM消耗减少高达68倍。

Conclusion: kViT为资源高效、直接从扫描仪进行AI分析开辟了新途径，实现了MRI分析范式的转变。

Abstract: Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.

</details>


### [140] [Larger than memory image processing](https://arxiv.org/abs/2601.18407)
*Jon Sporring,David Stansby*

Main category: cs.CV

TL;DR: 该论文提出了一种面向海量图像分析的内存友好流式处理架构，通过领域特定语言(DSL)优化I/O密集型操作的执行策略，显著提升PB级数据集的处理性能。


<details>
  <summary>Details</summary>
Motivation: 解决PB级海量图像数据（如1.4PB电子显微镜体积数据、150TB人体器官图谱）处理中的内存限制问题。由于数据规模远超内存容量，性能主要受I/O限制，需要设计能够最小化磁盘访问的高效处理方案。

Method: 采用流式处理架构，支持基于2D切片堆栈和3D分块布局两种数据表示。引入基于扫描的执行模型、窗口化操作和重叠感知分块来最小化冗余访问。开发领域特定语言(DSL)，在编译时和运行时自动分析流水线，优化窗口大小、阶段融合、流操作和调度策略。

Result: 提出的流式处理架构能够在有限内存机器上实现接近线性的I/O扫描和可预测的内存占用，显著提升海量图像处理的吞吐量，无需将整个体积数据完全加载到内存中。

Conclusion: 通过将分析重构为数据流式处理流水线，优先采用顺序读写模式，可以显著提升超大规模图像处理的性能。该方法与现有分割和形态学工具兼容，但重新构建了预处理和后处理流程，为I/O密集型海量图像分析提供了高效解决方案。

Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

</details>


### [141] [Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings](https://arxiv.org/abs/2601.18414)
*Aura Loredana Dan*

Main category: cs.CV

TL;DR: 该研究比较了三种深度学习模型（MobileNet、EfficientNet、VGG16）在儿童绘画情感分类任务中的性能，重点关注分类准确性、鲁棒性和计算效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 自闭症谱系障碍（ASD）儿童在情感表达和沟通方面存在困难，早期情感状态评估具有挑战性。传统评估方法往往具有侵入性、主观性或难以一致应用。儿童绘画提供了非侵入性的情感表达途径，但需要有效的自动化分析方法。

Method: 采用迁移学习方法，在心理专家标注的儿童绘画情感数据集上训练三种深度学习架构：MobileNet、EfficientNet和VGG16。建立统一的实验框架来比较这些模型在情感分类任务中的性能。

Result: 研究结果揭示了轻量级架构（如MobileNet）与更深层架构（如VGG16）在绘画情感计算任务中的重要权衡。MobileNet等轻量模型在计算效率和移动/实时应用方面表现更好，而更深层模型可能在分类性能上有优势。

Conclusion: 该研究为基于儿童绘画的情感计算任务提供了深度学习模型的比较分析，特别强调了在移动和实时应用场景中轻量级与深层架构的权衡。这为开发非侵入性、客观的ASD儿童早期情感评估工具提供了技术基础。

Abstract: Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.

</details>


### [142] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: 传统GMM预处理在ML应用中存在数据污染风险，作者提出新的对齐方法并系统分析影响因素


<details>
  <summary>Details</summary>
Motivation: 传统GMM分析中，在将数据分为训练集和测试集之前使用GPA对所有标本进行对齐，这种做法可能引入统计依赖性并污染下游预测模型，需要研究其影响并提出改进方法

Method: 使用受控的2D和3D模拟实验，研究样本大小、标志点密度和异速生长模式的影响；提出新的重对齐程序，将测试样本对齐到训练集；使用线性和卷积回归模型分析空间自相关性的重要性

Result: 模拟实验揭示了样本大小与标志点空间之间的"对角线"关系，反映了各向同性变异下RMSE的缩放；推导了Procrustes切空间中自由度的斜率；展示了忽略标志点关系时性能下降的现象

Conclusion: GMM在ML应用中需要仔细的预处理，提供了重对齐的实用指南，并澄清了Procrustes形状空间固有的基本统计约束

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [143] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: 3DGesPolicy：基于扩散策略的动作框架，将整体手势生成重构为连续轨迹控制问题，通过统一的整体动作建模帧间变化，实现空间和语义连贯的运动轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成整合全身运动和面部表情的整体伴随手势时存在两个主要问题：1）身体运动语义不协调；2）空间不稳定无意义运动。这源于现有的部分分解或帧级回归方法。

Method: 1）将整体手势生成重构为通过机器人扩散策略的连续轨迹控制问题；2）将帧间变化建模为统一的整体动作，学习帧间整体手势运动模式；3）提出手势-音频-音素（GAP）融合模块，深度整合多模态信号，确保语音语义、身体运动和面部表情之间的结构化细粒度对齐。

Result: 在BEAT2数据集上的大量定量和定性实验表明，3DGesPolicy在生成自然、富有表现力且高度语音对齐的整体手势方面优于其他最先进方法。

Conclusion: 3DGesPolicy通过将整体手势生成重新定义为连续轨迹控制问题，解决了现有方法在语义协调和空间稳定性方面的不足，通过GAP融合模块实现了多模态信号的深度整合，显著提升了整体手势生成的质量。

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [144] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: Fair-Eye Net是一个公平可靠的多模态AI系统，用于青光眼从筛查到随访的临床闭环管理，通过融合多种数据源并加入公平性约束，显著降低误诊率并减少种族差异。


<details>
  <summary>Details</summary>
Motivation: 青光眼是全球不可逆失明的主要原因，但现有筛查和进展评估依赖单一测试或松散关联的检查，存在主观性和碎片化护理问题。高质量影像工具和专家资源的有限获取进一步影响了实际使用中的一致性和公平性。

Method: 开发了Fair-Eye Net系统，采用双流异构融合架构整合眼底照片、OCT结构指标、视野功能指数和人口统计学因素，使用不确定性感知的分层门控策略进行选择性预测和安全转诊，并通过公平性约束减少弱势亚组的漏诊。

Result: 系统实现了AUC 0.912（特异性96.7%），将种族假阴性差异减少了73.4%（从12.31%降至3.28%），保持稳定的跨域性能，并能提供3-12个月的早期风险预警（敏感性92%，特异性88%）。

Conclusion: Fair-Eye Net将公平性作为主要目标与临床可靠性通过多任务学习进行优化，为临床转化和大规模部署提供了可复现的路径，有助于推进全球眼健康公平。

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [145] [DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment](https://arxiv.org/abs/2601.18493)
*Sara Tehrani,Yonghao Xu,Leif Haglund,Amanda Berg,Michael Felsberg*

Main category: cs.CV

TL;DR: DisasterInsight是一个用于评估视觉语言模型在灾害分析任务中表现的多模态基准，基于xBD数据集构建了约112K建筑中心实例，支持多种任务评估，并提出了DI-Chat作为领域适应基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉语言基准主要关注粗粒度标签和图像级识别，忽视了实际人道主义工作流程中所需的功能理解和指令鲁棒性，需要构建更贴近实际灾害响应需求的评估基准。

Method: 将xBD数据集重构为约112K建筑中心实例，支持建筑功能分类、损害等级分类、灾害类型分类、计数和结构化报告生成等多种任务。提出DI-Chat模型，通过参数高效的LoRA方法在灾害特定指令数据上微调现有VLM骨干网络。

Result: 实验显示现有通用和遥感VLM在灾害分析任务中存在显著性能差距，特别是在损害理解和结构化报告生成方面。DI-Chat在损害等级分类、灾害类型分类和报告生成质量上取得显著改进，但建筑功能分类对所有评估模型仍具挑战性。

Conclusion: DisasterInsight为研究灾害图像中的接地多模态推理提供了统一基准，揭示了现有VLM在灾害分析任务中的局限性，并通过DI-Chat展示了领域适应方法的价值。

Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.

</details>


### [146] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: 提出了一种结合基础模型嵌入和聚类的主动学习冷启动策略，以及集成空间多样性的不确定性驱动框架，显著提升了医学图像分割在低数据量下的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需要准确标注，但手动标注耗时且需要专业知识，成为瓶颈。主动学习通过优先标注信息量大的样本可以减轻负担，但传统的冷启动策略有待改进。

Method: 提出新颖的冷启动采样策略：结合基础模型嵌入和聚类，自动选择聚类数量并实现跨聚类的比例采样，构建多样且具代表性的初始训练集。随后采用基于不确定性的主动学习框架，集成空间多样性指导样本选择。

Result: 在三个数据集上验证：CheXmask数据集上，冷启动策略优于随机选择，Dice从0.918提升到0.929，Hausdorff距离从32.41降至27.66mm；主动学习结合熵和多样性选择将Dice从0.919提升到0.939，Hausdorff距离从30.10降至19.16mm。Montgomery数据集上冷启动效果显著，Dice从0.928提升到0.950，Hausdorff距离从14.22降至9.38mm。SynthStrip数据集上也有改善。

Conclusion: 提出的框架在低数据量下始终优于基线方法，提高了分割准确性。方法直观且可解释，能够可视化候选样本的特征空间分布。

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [147] [GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning](https://arxiv.org/abs/2601.18543)
*Kaixun Jiang,Yuzheng Wang,Junjie Zhou,Pandeng Li,Zhihang Liu,Chen-Wei Xie,Zhaoyu Chen,Yun Zheng,Wenqiang Zhang*

Main category: cs.CV

TL;DR: GenAgent 是一个通过代理框架统一视觉理解和生成的多模态模型，它将理解任务交给多模态模型处理，而将生成任务委托给可调用的图像生成模型作为工具。该框架支持自主多轮交互，通过强化学习训练，显著提升了基础生成器的性能。


<details>
  <summary>Details</summary>
Motivation: 传统统一模型面临昂贵的训练成本和理解-生成权衡问题。现有模块化系统受限于静态流水线，无法实现自主的多轮交互。GenAgent 旨在通过代理框架解决这些问题，实现更灵活、高效的视觉理解和生成。

Method: 采用代理框架：理解由多模态模型处理，生成通过调用图像生成模型作为工具实现。使用两阶段训练策略：1）冷启动阶段，通过高质量工具调用和反思数据进行监督微调；2）端到端代理强化学习，结合点奖励（最终图像质量）和成对奖励（反思准确性），并进行轨迹重采样以增强多轮探索。

Result: GenAgent 显著提升了基础生成器（FLUX.1-dev）在 GenEval++ 和 WISE 基准上的性能，分别提升了 23.6% 和 14%。此外，该框架展现出跨工具泛化、测试时扩展以及任务自适应推理等关键特性。

Conclusion: GenAgent 通过代理框架有效统一了视觉理解和生成，避免了传统方法的训练成本和权衡问题。其自主多轮交互能力和强化学习训练策略带来了显著的性能提升和泛化优势，为多模态智能体系统提供了新的设计思路。

Abstract: We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.

</details>


### [148] [REMAC: Reference-Based Martian Asymmetrical Image Compression](https://arxiv.org/abs/2601.18547)
*Qing Ding,Mai Xu,Shengxi Li,Xin Deng,Xin Zou*

Main category: cs.CV

TL;DR: 提出REMAC方法用于火星图像压缩，通过参考图像利用和计算复杂度转移，在降低编码器复杂度43.51%的同时提升压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习压缩方法忽视火星有限计算资源，且未利用火星图像间的强相似性来提升压缩性能。

Method: 提出基于参考的火星非对称图像压缩方法，包括参考引导的熵模块和参考解码器，利用参考图像信息减少编码器冗余操作，并采用深度多尺度架构建模长程空间依赖。

Result: 相比SOTA方法，REMAC降低编码器复杂度43.51%，同时获得0.2664 dB的BD-PSNR增益。

Conclusion: REMAC通过利用火星图像的强相似性，将计算复杂度从编码器转移到资源丰富的解码器，在有限计算资源下实现了高效的火星图像压缩。

Abstract: To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \textit{intra-} and \textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.

</details>


### [149] [Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray](https://arxiv.org/abs/2601.18555)
*Roberto Di Via,Vito Paolo Pastore,Francesca Odone,Siôn Glyn-Jones,Irina Voiculescu*

Main category: cs.CV

TL;DR: 本研究通过匹配队列验证（89名患者，配对MRI/X射线），使用标准热图回归架构评估跨模态临床等效性，证明MRI在cam型股骨髋臼撞击症（FAI）的定位和诊断准确性上可达到与X射线相当的水平。


<details>
  <summary>Details</summary>
Motivation: 传统FAI筛查主要依赖X射线角度测量，但评估撞击区域的高度和范围需要MRI的3D视图。目前缺乏对两种模态临床等效性的验证，而MRI具有提供3D信息的潜力。

Method: 采用匹配队列验证研究（89名患者，配对MRI/X射线数据），使用标准热图回归架构来评估跨模态临床等效性。重点关注MRI冠状面视图的自动FAI评估。

Result: MRI在cam型股骨髋臼撞击症的定位和诊断准确性上可达到与X射线相当的水平。该方法在3D MRI体积的冠状面视图中具有临床可行性，为通过放置更多标志点进行体积分析提供了可能性。

Conclusion: 研究结果支持将自动化FAI评估整合到常规MRI工作流程中。MRI能够提供与X射线相当的临床准确性，同时具备3D分析潜力，为更全面的FAI评估开辟了道路。

Abstract: Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions

</details>


### [150] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出SDA-QEC框架，结合简化扩散增强与量子增强分类，解决医学图像分类中的类别不平衡问题，在冠状动脉造影图像分类上达到98.33%准确率。


<details>
  <summary>Details</summary>
Motivation: 真实医学数据集常存在严重类别不平衡，正样本远多于负样本，导致模型偏向多数类，少数类召回率低，影响诊断准确性并带来临床误诊风险。

Method: 提出SDA-QEC框架：1) 使用轻量级扩散增强器为少数类生成高质量合成样本，平衡训练分布；2) 在MobileNetV2架构中嵌入量子特征层，通过希尔伯特空间高维特征映射增强模型判别能力。

Result: 在冠状动脉造影图像分类实验中，SDA-QEC达到98.33%准确率、98.78% AUC和98.33% F1分数，显著优于ResNet18、MobileNetV2、DenseNet121和VGG16等经典基线。同时实现了98.33%的敏感性和98.33%的特异性，获得临床部署所需的平衡性能。

Conclusion: 该方法验证了在真实医学成像任务中结合生成增强与量子增强建模的可行性，为开发在小样本、高度不平衡和高风险诊断场景中高度可靠的医疗AI系统提供了新研究路径。

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [151] [AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging](https://arxiv.org/abs/2601.18560)
*Li Fang,Tianyu Li,Yanghong Lin,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 该论文提出了一种基于AI的卫星边缘计算范式，用于高光谱图像分类，采用轻量级非深度学习框架和少样本学习策略，解决卫星资源受限和图像质量退化问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像卫星在地球观测系统中具有重要作用，但其下行传输速度成为灾害监测和应急测绘等快速响应应用的瓶颈。需要让卫星具备自主决策能力，同时面临卫星平台资源受限和传感器故障导致的图像质量退化问题。

Method: 提出轻量级非深度学习框架，集成少样本学习策略。开发两阶段像素级标签传播方案：第一阶段通过构建锚点-像素亲和矩阵传播选定锚点标签；第二阶段利用稀疏图的闭式解替代迭代计算。使用基于秩约束的图聚类算法确定锚点标签。

Result: 该方法仅利用单个像素层面的内在光谱特征，无需考虑深度神经网络所需的空域结构信息，能有效处理坏像素/错位像素和混合噪声等图像质量问题。

Conclusion: 提出的AI使能卫星边缘计算范式能够实现高光谱图像的快速分类，使卫星具备自主决策能力，适用于资源受限的卫星平台和图像质量退化场景。

Abstract: As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.

</details>


### [152] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出自精炼视频采样方法，通过将预训练视频生成器解释为去噪自编码器，在推理时进行迭代内循环精炼，无需外部验证器或额外训练，显著提升运动连贯性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 现代视频生成器在处理复杂物理动力学时仍存在困难，往往缺乏物理真实感。现有方法使用外部验证器或增强数据额外训练，计算成本高且难以捕捉细粒度运动。

Method: 自精炼视频采样方法：将预训练视频生成器解释为去噪自编码器，在推理时进行迭代内循环精炼；引入不确定性感知精炼策略，基于自一致性选择性地精炼区域，避免过度精炼导致的伪影。

Result: 在最先进的视频生成器上实验表明，该方法显著提升了运动连贯性和物理对齐，相比默认采样器和基于引导的采样器，获得了超过70%的人类偏好率。

Conclusion: 自精炼视频采样是一种简单有效的方法，无需额外训练或外部验证器，通过迭代内循环精炼显著提升了视频生成的物理真实感和运动质量。

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [153] [GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization](https://arxiv.org/abs/2601.18585)
*Chenxi Liu,Selena Ling,Alec Jacobson*

Main category: cs.CV

TL;DR: GimmBO：基于偏好贝叶斯优化的交互式适配器融合探索框架，用于提升扩散模型图像生成中多适配器权重选择效率


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像生成中，社区创建了大量捕捉不同主题和风格的适配器。虽然源自同一基础模型的适配器可以通过权重融合实现新视觉效果的合成，但现有工作流依赖手动滑块调节，这种方法在候选适配器数量较多时（即使是20-30个）也难以高效选择权重，扩展性差。

Method: 提出GimmBO框架，通过偏好贝叶斯优化支持交互式适配器融合探索。基于实际使用中的稀疏性和权重范围受限等观察，引入两阶段贝叶斯优化后端，提高高维空间中的采样效率和收敛性。

Result: 通过模拟用户和用户研究评估，GimmBO在收敛性、成功率方面优于贝叶斯优化和线搜索基线方法，展现出显著改进。框架还通过多个扩展展示了灵活性。

Conclusion: GimmBO为扩散模型适配器融合的权重选择提供了高效的交互式探索解决方案，解决了手动调节的可扩展性问题，在高维空间中表现出更好的采样效率和收敛性能。

Abstract: Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.

</details>


### [154] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: 提出AGSP-DSA框架，通过自适应图信号处理与动态语义对齐实现多模态数据融合，在三个基准数据集上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 针对异质多模态数据（文本、音频、图像）融合的挑战，需要处理模态间关系、增强信息信号、动态调整模态贡献，以提高多模态学习任务的性能

Method: 采用双图结构学习模态内和模态间关系；使用谱图滤波增强信息信号；多尺度图卷积网络进行节点嵌入；语义感知注意力机制动态调整模态贡献

Result: 在CMU-MOSEI上达到95.3%准确率、0.936 F1分数和0.924 mAP，比MM-GNN提升2.6%；在AVE上达到93.4%准确率和0.911 F1分数；在MM-IMDB上达到91.8%准确率和0.886 F1分数

Conclusion: AGSP-DSA框架在多模态学习中表现出高效性，具有良好的泛化能力和鲁棒性，特别是在缺失模态场景下，适用于情感分析、事件识别和多媒体分类任务

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


### [155] [EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery](https://arxiv.org/abs/2601.18597)
*Yu Xia,Chang Liu,Tianqi Xiang,Zhigang Tu*

Main category: cs.CV

TL;DR: EFSI-DETR是一个用于无人机图像实时小目标检测的新框架，通过动态频率-空间协同网络和高效语义特征提取器提升性能，在VisDrone和CODrone基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的实时小目标检测面临特征表示有限和多尺度融合效果不佳的挑战。现有方法未能充分利用频率信息，且依赖静态卷积操作，限制了获取丰富特征表示的能力，阻碍了对深层语义特征的有效利用。

Method: 提出了EFSI-DETR框架，包含两个主要组件：1) 动态频率-空间统一协同网络(DyFusNet)，联合利用频率和空间线索进行鲁棒的多尺度特征融合；2) 高效语义特征集中器(ESFC)，以最小计算成本实现深层语义提取。还采用了细粒度特征保留(FFR)策略，在融合过程中包含空间丰富的浅层特征以保留细节。

Result: 在VisDrone和CODrone基准测试中，EFSI-DETR实现了最先进的性能，在VisDrone上AP和AP_s分别提升了1.6%和5.8%，同时在单个RTX 4090 GPU上获得了188 FPS的推理速度。

Conclusion: EFSI-DETR通过整合高效的语义特征增强和动态频率-空间指导，有效解决了无人机图像中小目标检测的挑战，在保持实时效率的同时实现了显著的性能提升。

Abstract: Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.

</details>


### [156] [Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures](https://arxiv.org/abs/2601.18619)
*Jorge Quesada,Ghassan AlRegib*

Main category: cs.CV

TL;DR: 提出一种尺度感知的自监督学习方法，通过小窗口裁剪增强对小尺度、稀疏目标的表征学习，在地震成像和神经影像两个领域验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法主要针对大而均匀的区域进行优化，但在分割小尺度、稀疏或不规则目标时性能下降。需要一种能更好地适应不同尺度目标的自监督学习策略。

Method: 提出尺度感知的自监督学习适应方法，将小窗口裁剪集成到数据增强流程中，在预训练阶段聚焦于精细尺度结构。

Result: 在两个不同模态的领域验证：地震成像（稀疏断层分割）和神经影像（小细胞结构分割）。相比标准方法和先进基线，在标签受限条件下，断层分割准确率提升达13%，细胞分割提升5%。对于大尺度特征（如地震相或组织区域）则改进有限。

Conclusion: 自监督学习的价值关键取决于目标对象的尺度，需要将SSL设计与目标尺寸和稀疏性对齐，为跨科学成像领域构建更有效的表征学习流程提供了通用原则。

Abstract: Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.

</details>


### [157] [Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation](https://arxiv.org/abs/2601.18623)
*Zihao Wang,Yuzhou Chen,Shaogang Ren*

Main category: cs.CV

TL;DR: 提出一种改进的跨模态图像翻译方法，通过空间变化的混合场和目标一致性恢复项，避免传统方法中的固定调度域转移问题，提升结构保真度和语义一致性，同时减少去噪步骤。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态图像翻译方法存在脆弱性和低效性问题。标准扩散方法通常依赖于单一全局线性转移，这迫使采样器遍历离流形的高成本区域，增加了校正负担并导致语义漂移。作者将这种共享失败模式称为"固定调度域转移"。

Method: 提出将域转移动态直接嵌入生成过程的方法。模型在每个反向步骤预测空间变化的混合场，并将明确的目标一致性恢复项注入漂移中。这种步内指导使大更新保持在流形上，并将模型角色从全局对齐转变为局部残差校正。提供了具有精确解形式的连续时间公式，并推导出保持边际一致性的实用一阶采样器。

Result: 在医学成像、遥感和电致发光语义映射等翻译任务中，该框架在更少的去噪步骤内收敛，同时提高了结构保真度和语义一致性。

Conclusion: 通过将域转移动态直接集成到生成过程中，并引入空间变化的混合场和目标一致性恢复项，能够有效解决传统跨模态图像翻译中的固定调度域转移问题，实现更高效、更准确的图像翻译。

Abstract: Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.

</details>


### [158] [CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search](https://arxiv.org/abs/2601.18625)
*Zequn Xie*

Main category: cs.CV

TL;DR: CONQUER是一个两阶段的文本行人搜索框架，通过训练阶段的多粒度编码和最优传输匹配增强跨模态对齐，推理阶段的即插即用查询增强模块优化模糊或不完整查询，在多个数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 文本行人搜索面临跨模态差异和模糊用户查询的挑战，这阻碍了其在公共安全等实际应用中的部署。现有方法在处理不完整或模糊查询时表现不佳，需要一种更实用的解决方案。

Method: 两阶段框架：训练阶段使用多粒度编码、互补对挖掘和基于最优传输的上下文引导最优匹配来学习鲁棒嵌入；推理阶段通过即插即用的查询增强模块，利用锚点选择和属性驱动丰富化来优化模糊或不完整查询，无需重新训练主干网络。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上的大量实验表明，CONQUER在Rank-1准确率和mAP指标上持续优于强基线，在跨域和不完整查询场景中表现出显著改进。

Conclusion: CONQUER为现实世界文本行人搜索部署提供了一个实用且有效的解决方案，能够处理跨模态差异和模糊查询问题，源代码已开源。

Abstract: Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.

</details>


### [159] [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/abs/2601.18633)
*Tong Shi,Melonie de Almeida,Daniela Ivanova,Nicolas Pugeault,Paul Henderson*

Main category: cs.CV

TL;DR: Splat-Portrait是一个基于高斯泼溅的3D说话头生成方法，通过单张肖像图像和音频输入，无需3D监督或运动先验，就能生成高质量、自然的说话视频和视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有3D说话头生成方法依赖领域特定的启发式方法（如基于变形的面部运动表示先验），导致3D头像重建不准确，降低了生成动画的真实感。需要一种更准确的3D重建和唇部运动合成方法。

Method: 采用高斯泼溅表示，将单张肖像图像自动解耦为静态3D重建（静态高斯泼溅）和预测的2D背景。通过纯2D重建和分数蒸馏损失进行训练，无需3D监督或关键点标注。唇部运动由输入音频条件生成，无需运动驱动先验。

Result: 实验结果表明，Splat-Portrait在说话头生成和视角合成方面表现优异，相比之前的工作获得了更好的视觉质量。

Conclusion: Splat-Portrait是一种有效的基于高斯泼溅的3D说话头生成方法，能够准确重建3D头部并合成自然的唇部运动，无需3D监督或运动先验，实现了高质量的动画生成。

Abstract: Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.

</details>


### [160] [Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge](https://arxiv.org/abs/2601.18698)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 本文提出GAP框架评估文本到视频生成模型的地理公平性，发现Sora 2模型在全球各地旅游景点生成中表现出相对均匀的地理视觉知识，挑战了传统地理偏见假设。


<details>
  <summary>Details</summary>
Motivation: 文本到视频生成模型虽然在视觉上取得了令人印象深刻的结果，但目前尚不清楚这些模型是否编码了地理公平的视觉知识。本研究旨在调查这些模型的地理公平性和地理基础视觉知识，特别关注旅游景点生成这一具体领域。

Method: 提出了Geo-Attraction Landmark Probing (GAP)系统框架，用于评估模型如何忠实地合成来自不同地区的旅游景点。构建了GEOATTRACTION-500基准数据集，包含500个全球分布的旅游景点，涵盖不同地区和知名度水平。GAP整合了互补的评估指标，包括全局结构对齐、细粒度关键点对齐和视觉语言模型判断，所有这些都经过了人类评估的验证。

Result: 将GAP应用于最先进的文本到视频模型Sora 2，发现与常见的地理偏见假设相反，该模型在不同地区、发展水平和文化群体中表现出相对均匀的地理基础视觉知识水平，仅对景点知名度有较弱的依赖性。

Conclusion: 当前文本到视频模型表达的全球视觉知识比预期的更加均匀，这突显了它们在全球部署应用中的潜力，同时也强调了随着这类系统的发展需要持续进行评估的必要性。

Abstract: Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.

</details>


### [161] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: 本文提出SeNeDiF-OOD框架，采用语义嵌套二分融合的层次结构来解决开放世界环境中异质OOD检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 在开放世界环境中，人工智能应用需要可靠的OOD检测能力。然而，OOD数据具有异质性（从低级损坏到语义偏移），传统的单阶段检测器难以有效处理这种复杂性。

Method: 提出SeNeDiF-OOD框架，基于语义嵌套二分融合。该方法将检测任务分解为二进制融合节点的层次结构，每一层设计用于整合与特定语义抽象级别对齐的决策边界。

Result: 在MonuMAI（真实世界的建筑风格识别系统）案例研究中，该框架在过滤多样OOD类别（包括非纪念碑图像、未知建筑风格和对抗攻击）方面显著优于传统基线，同时保持了分布内性能。

Conclusion: SeNeDiF-OOD框架通过层次融合方法有效解决了异质OOD检测的挑战，为开放世界环境中AI应用的可靠部署提供了有前景的解决方案。

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [162] [TelcoAI: Advancing 3GPP Technical Specification Search through Agentic Multi-Modal Retrieval-Augmented Generation](https://arxiv.org/abs/2601.16984)
*Rahul Ghosh,Chun-Hao Liu,Gaurav Rele,Vidya Sagar Ravipati,Hazar Aouad*

Main category: cs.LG

TL;DR: TelcoAI是一个专为3GPP技术规范设计的智能文档理解系统，通过多模态RAG架构结合智能体推理，显著提升了复杂技术文档的查询处理能力。


<details>
  <summary>Details</summary>
Motivation: 3GPP技术规范因其复杂的层次结构、密集的格式和多模态内容而难以处理。现有的LLM方法在处理复杂查询、视觉信息和文档间依赖关系方面存在不足，需要更先进的解决方案来支持电信研究和工程实践。

Method: 提出了TelcoAI系统，采用多模态检索增强生成（RAG）架构，包含：1）基于章节感知的文本分块；2）结构化查询规划；3）元数据引导的检索；4）文本与图表的多模态融合。

Result: 在多个基准测试（包括专家策划的查询）中，系统达到了87%的召回率、83%的声明召回率和92%的忠实度，相比现有最佳基线方法提升了16%。

Conclusion: 研究证明了智能体和多模态推理在技术文档理解中的有效性，为电信领域的实际研究和工程问题提供了先进的解决方案。

Abstract: The 3rd Generation Partnership Project (3GPP) produces complex technical specifications essential to global telecommunications, yet their hierarchical structure, dense formatting, and multi-modal content make them difficult to process. While Large Language Models (LLMs) show promise, existing approaches fall short in handling complex queries, visual information, and document interdependencies. We present TelcoAI, an agentic, multi-modal Retrieval-Augmented Generation (RAG) system tailored for 3GPP documentation. TelcoAI introduces section-aware chunking, structured query planning, metadata-guided retrieval, and multi-modal fusion of text and diagrams. Evaluated on multiple benchmarks-including expert-curated queries-our system achieves $87\%$ recall, $83\%$ claim recall, and $92\%$ faithfulness, representing a $16\%$ improvement over state-of-the-art baselines. These results demonstrate the effectiveness of agentic and multi-modal reasoning in technical document understanding, advancing practical solutions for real-world telecommunications research and engineering.

</details>


### [163] [Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2601.16991)
*Longteng Zhang,Sen Wu,Shuai Hou,Zhengyu Qing,Zhuo Zheng,Danning Ke,Qihong Lin,Qiang Wang,Shaohuai Shi,Xiaowen Chu*

Main category: cs.LG

TL;DR: SALR是一种结合低秩适应和稀疏剪枝的新颖微调范式，通过静态剪枝冻结基础权重并利用截断SVD低秩适配器恢复信息，在保持LoRA性能的同时实现50%稀疏度、2倍模型压缩和1.7倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法在资源受限环境下存在局限性：完全微调需要调整数百万参数，低秩适应（LoRA）虽然减少了可训练参数，但底层密集权重仍带来高存储和计算成本，而传统的基于幅度的剪枝会损害LoRA性能。

Method: 提出SALR（稀疏感知低秩表示）框架，在均方误差框架下统一低秩适应和稀疏剪枝。静态剪枝冻结的基础权重以最小化剪枝误差界，通过截断SVD低秩适配器恢复丢弃的残差信息。采用位图编码和两阶段流水线解码+GEMM设计实现硬件效率最大化。

Result: 在各种LLM上实现50%稀疏度，在GSM8K和MMLU基准测试中匹配LoRA性能，模型大小减少2倍，推理速度提升最高达1.7倍。

Conclusion: SALR提供了一种有效的微调范式，在保持性能的同时显著减少了存储和计算需求，为资源受限环境中的大语言模型部署提供了实用解决方案。

Abstract: Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\times$, and delivers up to a $1.7\times$ inference speedup.

</details>


### [164] [A Dataset of Dengue Hospitalizations in Brazil (1999 to 2021) with Weekly Disaggregation from Monthly Counts](https://arxiv.org/abs/2601.16994)
*Lucas M. Morello,Matheus Lima Castro,Pedro Cesar M. G. Camargo,Liliane Moreira Nery,Darllan Collins da Cunha e Silva,Leopoldo Lusquino Filho*

Main category: cs.LG

TL;DR: 该论文发布了一个巴西市级登革热住院时间序列数据集，将原始月度数据通过三次样条插值方法分解为周度分辨率，并包含多种环境和社会经济解释变量，适用于流行病学预测和环境健康研究。


<details>
  <summary>Details</summary>
Motivation: 提高原始月度数据的时间粒度，以便更有效地训练用于流行病学预测的AI模型。通过将市级登革热住院时间序列分解为周度分辨率，增强数据对机器学习模型的训练效果。

Method: 1. 将巴西市级登革热住院月度时间序列通过插值协议分解为周度分辨率（流行病学周）
2. 采用校正步骤保持月度总量不变
3. 使用圣保罗州高分辨率参考数据集（同时提供月度和周度计数）评估三种分解策略：线性插值、抖动和三次样条插值
4. 选择表现最佳的三次样条插值方法生成1999-2021年的周度序列
5. 对解释变量采用相同的时间分解方案以确保多变量兼容性

Result: 1. 三次样条插值在三种策略中达到最高参考数据符合度
2. 成功生成了1999-2021年巴西市级登革热住院周度时间序列数据集
3. 数据集包含住院时间序列和全面的解释变量：人口密度、CH4、CO2、NO2排放、贫困和城市化指数、最高温度、月平均降水量、最小相对湿度、市级纬度和经度
4. 提供了详细的数据集来源、结构、格式、许可证、限制和质量指标（MAE、RMSE、R2、KL、JSD、DTW和KS检验）

Conclusion: 该论文发布了一个高质量、多变量的巴西登革热流行病学数据集，将月度数据有效分解为周度分辨率。数据集适用于多变量时间序列分析、环境健康研究以及开发和训练用于疫情预测的机器学习和深度学习模型。

Abstract: This data paper describes and publicly releases this dataset (v1.0.0), published on Zenodo under DOI 10.5281/zenodo.18189192. Motivated by the need to increase the temporal granularity of originally monthly data to enable more effective training of AI models for epidemiological forecasting, the dataset harmonizes municipal-level dengue hospitalization time series across Brazil and disaggregates them to weekly resolution (epidemiological weeks) through an interpolation protocol with a correction step that preserves monthly totals. The statistical and temporal validity of this disaggregation was assessed using a high-resolution reference dataset from the state of Sao Paulo (2024), which simultaneously provides monthly and epidemiological-week counts, enabling a direct comparison of three strategies: linear interpolation, jittering, and cubic spline. Results indicated that cubic spline interpolation achieved the highest adherence to the reference data, and this strategy was therefore adopted to generate weekly series for the 1999 to 2021 period. In addition to hospitalization time series, the dataset includes a comprehensive set of explanatory variables commonly used in epidemiological and environmental modeling, such as demographic density, CH4, CO2, and NO2 emissions, poverty and urbanization indices, maximum temperature, mean monthly precipitation, minimum relative humidity, and municipal latitude and longitude, following the same temporal disaggregation scheme to ensure multivariate compatibility. The paper documents the datasets provenance, structure, formats, licenses, limitations, and quality metrics (MAE, RMSE, R2, KL, JSD, DTW, and the KS test), and provides usage recommendations for multivariate time-series analysis, environmental health studies, and the development of machine learning and deep learning models for outbreak forecasting.

</details>


### [165] [MathMixup: Boosting LLM Mathematical Reasoning with Difficulty-Controllable Data Synthesis and Curriculum Learning](https://arxiv.org/abs/2601.17006)
*Xuchen Li,Jing Chen,Xuzhao Li,Hao Liang,Xiaohuan Zhou,Taifeng Wang,Wentao Zhang*

Main category: cs.LG

TL;DR: MathMixup是一种通过混合与分解策略生成难度可控数学推理问题的新数据合成范式，构建了MathMixupQA数据集并设计了课程学习策略，显著提升了LLMs的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理数据合成方法存在多样性有限、难以精确控制问题难度的问题，无法有效支持课程学习等高效训练范式，需要更高质量、难度可控的训练数据来提升大语言模型的数学推理能力。

Method: 提出MathMixup数据合成范式，采用混合和分解策略系统生成高质量、难度可控的数学推理问题；通过自动自检和人工筛选确保语义清晰和难度梯度结构；构建MathMixupQA数据集并设计课程学习策略，支持与其他数据集的灵活集成。

Result: 实验结果表明MathMixup及其课程学习策略显著提升了LLMs的数学推理性能。微调后的Qwen2.5-7B在七个数学基准测试中平均得分达52.6%，超越了之前的最优方法。

Conclusion: MathMixup在提升LLMs数学推理能力和推进以数据为中心的课程学习方面具有有效性和广泛适用性，为数学推理任务提供了高质量、难度可控的训练数据合成解决方案。

Abstract: In mathematical reasoning tasks, the advancement of Large Language Models (LLMs) relies heavily on high-quality training data with clearly defined and well-graded difficulty levels. However, existing data synthesis methods often suffer from limited diversity and lack precise control over problem difficulty, making them insufficient for supporting efficient training paradigms such as curriculum learning. To address these challenges, we propose MathMixup, a novel data synthesis paradigm that systematically generates high-quality, difficulty-controllable mathematical reasoning problems through hybrid and decomposed strategies. Automated self-checking and manual screening are incorporated to ensure semantic clarity and a well-structured difficulty gradient in the synthesized data. Building on this, we construct the MathMixupQA dataset and design a curriculum learning strategy that leverages these graded problems, supporting flexible integration with other datasets. Experimental results show that MathMixup and its curriculum learning strategy significantly enhance the mathematical reasoning performance of LLMs. Fine-tuned Qwen2.5-7B achieves an average score of 52.6\% across seven mathematical benchmarks, surpassing previous state-of-the-art methods. These results fully validate the effectiveness and broad applicability of MathMixup in improving the mathematical reasoning abilities of LLMs and advancing data-centric curriculum learning.

</details>


### [166] [Analysis of voice recordings features for Classification of Parkinson's Disease](https://arxiv.org/abs/2601.17007)
*Beatriz Pérez-Sánchez,Noelia Sánchez-Maroño,Miguel A. Díaz-Freire*

Main category: cs.LG

TL;DR: 该研究使用机器学习模型结合特征选择方法，通过语音记录检测帕金森病，发现神经网络等模型在显著减少特征数量的同时仍能保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断困难，因为早期运动症状非常轻微。虽然语音记录可用于辅助诊断，但临床分析成本高，且语音特征众多但不知道哪些对诊断真正相关。

Method: 采用不同类型的机器学习模型（特别是神经网络）结合特征选择方法，通过确定哪些特征对问题提供最多信息来减少分类器使用的特征数量。

Result: 机器学习方法（特别是神经网络）适合帕金森病分类，特征数量可以显著减少而不影响模型性能。

Conclusion: 机器学习结合特征选择能够有效利用语音记录进行帕金森病早期诊断，在保持准确性的同时提高诊断效率。

Abstract: Parkinson's disease (PD) is a chronic neurodegenerative disease. Early diagnosis is essential to mitigate the progressive deterioration of patients' quality of life. The most characteristic motor symptoms are very mild in the early stages, making diagnosis difficult. Recent studies have shown that the use of patient voice recordings can aid in early diagnosis. Although the analysis of such recordings is costly from a clinical point of view, advances in machine learning techniques are making the processing of this type of data increasingly accurate and efficient. Vocal recordings contain many features, but it is not known whether all of them are relevant for diagnosing the disease.
  This paper proposes the use of different types of machine learning models combined with feature selection methods to detect the disease. The selection techniques allow to reduce the number of features used by the classifiers by determining which ones provide the most information about the problem. The results show that machine learning methods, in particular neural networks, are suitable for PD classification and that the number of features can be significantly reduced without affecting the performance of the models.

</details>


### [167] [Bayesian Robust Financial Trading with Adversarial Synthetic Market Data](https://arxiv.org/abs/2601.17008)
*Haochong Xia,Simin Li,Ruixiao Xu,Zhixia Zhang,Hongxiang Wang,Zhiqian Liu,Teng Yao Long,Molei Qin,Chuqiao Zong,Bo An*

Main category: cs.LG

TL;DR: 提出贝叶斯鲁棒框架，结合宏观条件生成对抗网络和鲁棒策略学习，解决算法交易模型在真实市场变化中性能下降的问题


<details>
  <summary>Details</summary>
Motivation: 现有算法交易模型在样本内表现良好，但在真实市场环境变化时性能下降。主要问题：1) 现有策略对高层次市场波动的鲁棒性不足；2) 缺乏真实多样的训练模拟环境，导致策略过拟合

Method: 提出贝叶斯鲁棒框架：1) 数据端：使用宏观条件GAN生成器，以宏观经济指标为主要控制变量，生成具有真实时间、跨工具和宏观相关性的数据；2) 策略端：将交易过程建模为两人零和贝叶斯马尔可夫博弈，对抗代理通过扰动宏观指标来模拟市场变化，交易代理通过分位数信念网络维护和更新对隐藏市场状态的信念，使用贝叶斯神经虚拟自博弈寻求鲁棒完美贝叶斯均衡

Result: 在9种金融工具上的实验显示，该框架优于9种最先进的基准方法。在COVID等极端事件中，该方法表现出更好的盈利能力和风险管理能力

Conclusion: 该框架为不确定和变化的市场动态下的交易提供了可靠解决方案，通过系统整合宏观条件生成模型和鲁棒策略学习，有效解决了算法交易中的鲁棒性问题

Abstract: Algorithmic trading relies on machine learning models to make trading decisions. Despite strong in-sample performance, these models often degrade when confronted with evolving real-world market regimes, which can shift dramatically due to macroeconomic changes-e.g., monetary policy updates or unanticipated fluctuations in participant behavior. We identify two challenges that perpetuate this mismatch: (1) insufficient robustness in existing policy against uncertainties in high-level market fluctuations, and (2) the absence of a realistic and diverse simulation environment for training, leading to policy overfitting. To address these issues, we propose a Bayesian Robust Framework that systematically integrates a macro-conditioned generative model with robust policy learning. On the data side, to generate realistic and diverse data, we propose a macro-conditioned GAN-based generator that leverages macroeconomic indicators as primary control variables, synthesizing data with faithful temporal, cross-instrument, and macro correlations. On the policy side, to learn robust policy against market fluctuations, we cast the trading process as a two-player zero-sum Bayesian Markov game, wherein an adversarial agent simulates shifting regimes by perturbing macroeconomic indicators in the macro-conditioned generator, while the trading agent-guided by a quantile belief network-maintains and updates its belief over hidden market states. The trading agent seeks a Robust Perfect Bayesian Equilibrium via Bayesian neural fictitious self-play, stabilizing learning under adversarial market perturbations. Extensive experiments on 9 financial instruments demonstrate that our framework outperforms 9 state-of-the-art baselines. In extreme events like the COVID, our method shows improved profitability and risk management, offering a reliable solution for trading under uncertain and shifting market dynamics.

</details>


### [168] [Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study](https://arxiv.org/abs/2601.17010)
*Hudson Golino*

Main category: cs.LG

TL;DR: 研究提出将LLM嵌入向量视为可搜索的语义景观，通过动态探索图分析在嵌入维度中寻找最优结构信息，发现准确性和组织性存在权衡，需要复合优化标准。


<details>
  <summary>Details</summary>
Motivation: 当前应用将LLM嵌入向量视为静态、横截面的表示，假设所有嵌入坐标贡献均匀，忽视了最优结构信息可能集中在嵌入空间特定区域的可能性。

Method: 将嵌入向量重构为可搜索的景观，采用动态探索图分析（DynEGA）系统遍历嵌入坐标，将维度索引视为伪时间排序。使用蒙特卡洛模拟嵌入自恋五个维度的项目，变化项目池大小和嵌入深度。

Result: 总熵拟合指数和归一化互信息在嵌入景观中产生竞争性优化轨迹。TEFI在深层嵌入范围（900-1200维）达到最小值，但结构准确性下降；NMI在浅层深度达到峰值，但熵拟合次优。单指标优化产生结构不连贯的解，加权复合标准能平衡准确性和组织性。

Conclusion: 嵌入景观是非均匀的语义空间，需要原则性优化而非默认使用完整向量。最优嵌入深度随项目池大小系统变化，复合优化标准能识别平衡准确性和组织性的嵌入维度深度区域。

Abstract: Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.

</details>


### [169] [FlashMoE: Reducing SSD I/O Bottlenecks via ML-Based Cache Replacement for Mixture-of-Experts Inference on Edge Devices](https://arxiv.org/abs/2601.17063)
*Byeongju Kim,Jungwan Lee,Donghyeon Han,Hoi-Jun Yoo,Sangyeob Kim*

Main category: cs.LG

TL;DR: FlashMoE是一个将不活跃专家卸载到SSD的MoE推理系统，通过轻量级ML缓存策略显著提升缓存命中率，在内存受限设备上实现高效MoE推理。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然参数巨大但稀疏激活，为设备端推理提供了可能。然而现有DRAM卸载方案不适合内存受限环境，随着MoE模型增长到数百GB，RAM卸载变得不切实际。

Method: 提出FlashMoE系统，将不活跃专家卸载到SSD，采用基于ML的轻量级缓存策略，结合最近使用和频率信号自适应优化专家重用，最大限度减少存储I/O。

Result: 在真实硬件平台上，FlashMoE比LRU和LFU等传统卸载策略提升缓存命中率最高达51%，相比现有MoE推理系统获得最高2.6倍加速。

Conclusion: FlashMoE通过SSD卸载和智能缓存策略，有效解决了内存受限环境下大型MoE模型的推理挑战，证明了设备端MoE推理的可行性。

Abstract: Recently, Mixture-of-Experts (MoE) models have gained attention for efficiently scaling large language models. Although these models are extremely large, their sparse activation enables inference to be performed by accessing only a fraction of the model at a time. This property opens the possibility of on-device inference of MoE, which was previously considered infeasible for such large models. Consequently, various systems have been proposed to leverage this sparsity and enable efficient MoE inference for edge devices. However, previous MoE inference systems like Fiddler[8] or DAOP[13] rely on DRAM-based offloading and are not suitable for memory constrained on-device environments. As recent MoE models grow to hundreds of gigabytes, RAM-offloading solutions become impractical. To address this, we propose FlashMoE, a system that offloads inactive experts to SSD, enabling efficient MoE inference under limited RAM. FlashMoE incorporates a lightweight ML-based caching strategy that adaptively combines recency and frequency signals to maximize expert reuse, significantly reducing storage I/O. In addition, we built a user-grade desktop platform to demonstrate the practicality of FlashMoE. On this real hardware setup, FlashMoE improves cache hit rate by up to 51% over well-known offloading policies such as LRU and LFU, and achieves up to 2.6x speedup compared to existing MoE inference systems.

</details>


### [170] [ThinkTank-ME: A Multi-Expert Framework for Middle East Event Forecasting](https://arxiv.org/abs/2601.17065)
*Haoxuan Li,He Chang,Yunshan Ma,Yi Bin,Yang Yang,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出ThinkTank-ME框架，通过模拟真实世界智库协作的多专家分析来改进中东事件预测，构建POLECAT-FOR-ME基准验证多专家协作在复杂地缘政治预测任务中的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的事件预测方法采用单一模型架构，沿单一显式轨迹生成预测，限制了其捕捉复杂区域背景下多样化地缘政治细微差别的能力。事件预测本身受多方面因素影响，包括国际关系、区域历史动态和文化背景。

Method: 引入ThinkTank-ME框架，模拟真实世界战略决策中的协作专家分析，采用多专家协作方法。构建POLECAT-FOR-ME中东事件预测基准，促进专家专业化和严格评估。

Result: 实验结果表明，多专家协作在处理复杂时间地缘政治预测任务方面具有优越性。

Conclusion: 提出的ThinkTank-ME框架通过模拟智库协作的多专家分析，有效提升了中东事件预测的准确性和对复杂地缘政治背景的捕捉能力。

Abstract: Event forecasting is inherently influenced by multifaceted considerations, including international relations, regional historical dynamics, and cultural contexts. However, existing LLM-based approaches employ single-model architectures that generate predictions along a singular explicit trajectory, constraining their ability to capture diverse geopolitical nuances across complex regional contexts. To address this limitation, we introduce ThinkTank-ME, a novel Think Tank framework for Middle East event forecasting that emulates collaborative expert analysis in real-world strategic decision-making. To facilitate expert specialization and rigorous evaluation, we construct POLECAT-FOR-ME, a Middle East-focused event forecasting benchmark. Experimental results demonstrate the superiority of multi-expert collaboration in handling complex temporal geopolitical forecasting tasks. The code is available at https://github.com/LuminosityX/ThinkTank-ME.

</details>


### [171] [Multi-Agent Deep Reinforcement Learning Under Constrained Communications](https://arxiv.org/abs/2601.17069)
*Shahil Shaik,Jonathon M. Smereka,Yue Wang*

Main category: cs.LG

TL;DR: 提出分布式多智能体强化学习框架DG-MAPPO，通过分布式图注意力网络实现完全去中心化训练与执行，消除对集中式评论器或全局信息的依赖


<details>
  <summary>Details</summary>
Motivation: 传统CTDE范式依赖训练时的全局状态信息，存在可扩展性、鲁棒性和泛化性瓶颈，在添加/删除队友或环境动态变化时脆弱且需要昂贵重训练。分布式方法仅需局部信息和点对点通信，更具适应性

Method: 提出分布式图注意力网络(D-GAT)，通过多跳通信进行全局状态推断，智能体以输入依赖的注意力权重整合邻居特征。基于D-GAT开发DG-MAPPO框架，智能体使用局部观测、多跳通信和共享/平均奖励优化局部策略和价值函数

Result: 在StarCraftII多智能体挑战、Google Research Football和Multi-Agent Mujoco上的实验表明，该方法持续优于强CTDE基线，在广泛合作任务中实现更优协调，适用于同质和异质团队

Conclusion: DG-MAPPO提供了一种原则性、可扩展的鲁棒协作解决方案，完全消除了对集中式训练或全局可观测性的依赖，首次实现仅通过点对点通信就能学习和行动的多智能体系统

Abstract: Centralized training with decentralized execution (CTDE) has been the dominant paradigm in multi-agent reinforcement learning (MARL), but its reliance on global state information during training introduces scalability, robustness, and generalization bottlenecks. Moreover, in practical scenarios such as adding/dropping teammates or facing environment dynamics that differ from the training, CTDE methods can be brittle and costly to retrain, whereas distributed approaches allow agents to adapt using only local information and peer-to-peer communication. We present a distributed MARL framework that removes the need for centralized critics or global information. Firstly, we develop a novel Distributed Graph Attention Network (D-GAT) that performs global state inference through multi-hop communication, where agents integrate neighbor features via input-dependent attention weights in a fully distributed manner. Leveraging D-GAT, we develop the distributed graph-attention MAPPO (DG-MAPPO) -- a distributed MARL framework where agents optimize local policies and value functions using local observations, multi-hop communication, and shared/averaged rewards. Empirical evaluation on the StarCraftII Multi-Agent Challenge, Google Research Football, and Multi-Agent Mujoco demonstrates that our method consistently outperforms strong CTDE baselines, achieving superior coordination across a wide range of cooperative tasks with both homogeneous and heterogeneous teams. Our distributed MARL framework provides a principled and scalable solution for robust collaboration, eliminating the need for centralized training or global observability. To the best of our knowledge, DG-MAPPO appears to be the first to fully eliminate reliance on privileged centralized information, enabling agents to learn and act solely through peer-to-peer communication.

</details>


### [172] [Attention-Based Variational Framework for Joint and Individual Components Learning with Applications in Brain Network Analysis](https://arxiv.org/abs/2601.17073)
*Yifei Zhang,Meimei Liu,Zhengwu Zhang*

Main category: cs.LG

TL;DR: CM-JIVNet：一种用于多模态脑连接组分析的统一概率框架，通过因子化潜在表示分离SC-FC的共享和模态特定信息，提升行为表型预测性能。


<details>
  <summary>Details</summary>
Motivation: 脑组织分析需要整合结构和功能连接（SC-FC）这两种互补但不同的模态数据，以揭示驱动行为表型的跨模态模式。然而，现有方法面临高维非线性数据、复杂SC-FC耦合以及难以分离共享与模态特定信息等挑战。

Method: 提出跨模态联合-个体变分网络（CM-JIVNet），采用统一概率框架学习配对SC-FC数据集的因子化潜在表示。模型使用多头注意力融合模块捕获非线性跨模态依赖，同时分离独立的模态特定信号。

Result: 在HCP-YA数据上验证，CM-JIVNet在跨模态重建和行为特征预测方面表现出优越性能。通过有效解耦联合和个体特征空间，为大规模多模态脑分析提供了稳健、可解释和可扩展的解决方案。

Conclusion: CM-JIVNet成功解决了多模态脑连接组数据整合的挑战，通过分离共享和模态特定信息，为理解脑组织与行为关系提供了更强大的分析工具，具有实际应用价值。

Abstract: Brain organization is increasingly characterized through multiple imaging modalities, most notably structural connectivity (SC) and functional connectivity (FC). Integrating these inherently distinct yet complementary data sources is essential for uncovering the cross-modal patterns that drive behavioral phenotypes. However, effective integration is hindered by the high dimensionality and non-linearity of connectome data, complex non-linear SC-FC coupling, and the challenge of disentangling shared information from modality-specific variations. To address these issues, we propose the Cross-Modal Joint-Individual Variational Network (CM-JIVNet), a unified probabilistic framework designed to learn factorized latent representations from paired SC-FC datasets. Our model utilizes a multi-head attention fusion module to capture non-linear cross-modal dependencies while isolating independent, modality-specific signals. Validated on Human Connectome Project Young Adult (HCP-YA) data, CM-JIVNet demonstrates superior performance in cross-modal reconstruction and behavioral trait prediction. By effectively disentangling joint and individual feature spaces, CM-JIVNet provides a robust, interpretable, and scalable solution for large-scale multimodal brain analysis.

</details>


### [173] [PhysE-Inv: A Physics-Encoded Inverse Modeling approach for Arctic Snow Depth Prediction](https://arxiv.org/abs/2601.17074)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: PhysE-Inv：结合物理约束与深度学习的新框架，用于解决北极雪深估计中的逆问题，在数据稀疏和噪声环境下显著提升预测精度和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 北极雪深估计是关键的时变逆问题，现有基于过程或数据驱动的方法要么对稀疏数据高度敏感，要么缺乏气候应用所需的物理解释性，需要一种新的框架来填补这一空白。

Method: 提出PhysE-Inv框架，集成LSTM编码器-解码器与多头注意力机制，结合物理引导的对比学习和推理。核心创新是引入满射的物理约束反演方法：1）利用静水平衡前向模型作为目标制定代理，在没有直接地面真值情况下有效学习；2）在潜空间上使用重建物理正则化，从噪声、不完整时间序列输入中动态发现隐藏物理参数。

Result: 与最先进基线相比，PhysE-Inv显著提升预测性能，误差降低20%，同时展现出比经验方法更优的物理一致性和对数据稀疏性的鲁棒性。

Conclusion: 该方法为噪声容忍、可解释的逆建模开辟了新路径，在地理空间和冰冻圈领域具有广泛适用性。

Abstract: The accurate estimation of Arctic snow depth ($h_s$) remains a critical time-varying inverse problem due to the extreme scarcity and noise inherent in associated sea ice parameters. Existing process-based and data-driven models are either highly sensitive to sparse data or lack the physical interpretability required for climate-critical applications. To address this gap, we introduce PhysE-Inv, a novel framework that integrates a sophisticated sequential architecture, an LSTM Encoder-Decoder with Multi-head Attention and physics-guided contrastive learning, with physics-guided inference.Our core innovation lies in a surjective, physics-constrained inversion methodology. This methodology first leverages the hydrostatic balance forward model as a target-formulation proxy, enabling effective learning in the absence of direct $h_s$ ground truth; second, it uses reconstruction physics regularization over a latent space to dynamically discover hidden physical parameters from noisy, incomplete time-series input. Evaluated against state-of-the-art baselines, PhysE-Inv significantly improves prediction performance, reducing error by 20\% while demonstrating superior physical consistency and resilience to data sparsity compared to empirical methods. This approach pioneers a path for noise-tolerant, interpretable inverse modeling, with wide applicability in geospatial and cryospheric domains.

</details>


### [174] [E2PL: Effective and Efficient Prompt Learning for Incomplete Multi-view Multi-Label Class Incremental Learning](https://arxiv.org/abs/2601.17076)
*Jiajun Chen,Yue Wu,Kai Huang,Wen Xi,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi,Guanjie Cheng*

Main category: cs.LG

TL;DR: 提出了一个名为E2PL的新框架，用于解决不完整多视图多标签类别增量学习（IMvMLCIL）问题，该框架通过任务定制提示和缺失感知提示，结合高效原型张量化模块和动态对比学习策略，有效处理视图缺失和类别动态扩展的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多视图多标签分类面临视图缺失和类别持续涌现的挑战。现有方法要么缺乏对新类别的适应性，要么在处理所有可能的缺失视图模式时参数呈指数增长，严重限制了在Web环境中的可扩展性。

Method: 提出了E2PL框架，包含两个核心设计：1）任务定制提示用于类别增量适应；2）缺失感知提示用于灵活整合任意视图缺失场景。通过高效原型张量化模块将提示参数复杂度从指数级降低到线性级，并采用动态对比学习策略显式建模不同缺失视图模式间的复杂依赖关系。

Result: 在三个基准测试上的广泛实验表明，E2PL在有效性和效率方面都持续优于最先进的方法。

Conclusion: E2PL框架成功解决了不完整多视图多标签类别增量学习问题，通过创新的提示设计和参数优化技术，实现了对视图缺失和类别动态扩展的有效处理，具有更好的可扩展性和实用性。

Abstract: Multi-view multi-label classification (MvMLC) is indispensable for modern web applications aggregating information from diverse sources. However, real-world web-scale settings are rife with missing views and continuously emerging classes, which pose significant obstacles to robust learning. Prevailing methods are ill-equipped for this reality, as they either lack adaptability to new classes or incur exponential parameter growth when handling all possible missing-view patterns, severely limiting their scalability in web environments. To systematically address this gap, we formally introduce a novel task, termed \emph{incomplete multi-view multi-label class incremental learning} (IMvMLCIL), which requires models to simultaneously address heterogeneous missing views and dynamic class expansion. To tackle this task, we propose \textsf{E2PL}, an Effective and Efficient Prompt Learning framework for IMvMLCIL. \textsf{E2PL} unifies two novel prompt designs: \emph{task-tailored prompts} for class-incremental adaptation and \emph{missing-aware prompts} for the flexible integration of arbitrary view-missing scenarios. To fundamentally address the exponential parameter explosion inherent in missing-aware prompts, we devise an \emph{efficient prototype tensorization} module, which leverages atomic tensor decomposition to elegantly reduce the prompt parameter complexity from exponential to linear w.r.t. the number of views. We further incorporate a \emph{dynamic contrastive learning} strategy explicitly model the complex dependencies among diverse missing-view patterns, thus enhancing the model's robustness. Extensive experiments on three benchmarks demonstrate that \textsf{E2PL} consistently outperforms state-of-the-art methods in both effectiveness and efficiency. The codes and datasets are available at https://anonymous.4open.science/r/code-for-E2PL.

</details>


### [175] [SFO: Learning PDE Operators via Spectral Filtering](https://arxiv.org/abs/2601.17090)
*Noam Koren,Rafael Moschopoulos,Kira Radinsky,Elad Hazan*

Main category: cs.LG

TL;DR: 提出SFO神经算子，使用通用谱基参数化积分核，在多个PDE基准测试中实现最先进精度，误差降低达40%，参数更少


<details>
  <summary>Details</summary>
Motivation: 传统神经算子在捕捉PDE解映射中的长程非局部相互作用方面效率低下，需要更有效的算子表示方法

Method: 提出SFO神经算子，使用从希尔伯特矩阵特征模导出的通用谱基参数化积分核，仅学习快速衰减特征值的谱系数，实现高效表示

Result: 在六个基准测试（包括反应扩散、流体动力学和3D电磁学）中达到最先进精度，误差相对强基线降低达40%，同时使用更少参数

Conclusion: SFO通过谱基表示提供了一种高效捕捉PDE长程相互作用的神经算子框架，在精度和参数效率方面均优于现有方法

Abstract: Partial differential equations (PDEs) govern complex systems, yet neural operators often struggle to efficiently capture the long-range, nonlocal interactions inherent in their solution maps. We introduce Spectral Filtering Operator (SFO), a neural operator that parameterizes integral kernels using the Universal Spectral Basis (USB), a fixed, global orthonormal basis derived from the eigenmodes of the Hilbert matrix in spectral filtering theory. Motivated by our theoretical finding that the discrete Green's functions of shift-invariant PDE discretizations exhibit spatial Linear Dynamical System (LDS) structure, we prove that these kernels admit compact approximations in the USB. By learning only the spectral coefficients of rapidly decaying eigenvalues, SFO achieves a highly efficient representation. Across six benchmarks, including reaction-diffusion, fluid dynamics, and 3D electromagnetics, SFO achieves state-of-the-art accuracy, reducing error by up to 40% relative to strong baselines while using substantially fewer parameters.

</details>


### [176] [CUROCKET: Optimizing ROCKET for GPU](https://arxiv.org/abs/2601.17091)
*Ole Stüven,Keno Moenck,Thorsten Schüppstuhl*

Main category: cs.LG

TL;DR: 本文提出CUROCKET，一种在GPU上高效执行ROCKET特征提取的算法，相比CPU版本实现了高达11倍的每瓦计算效率提升。


<details>
  <summary>Details</summary>
Motivation: ROCKET是时间序列分类的高效特征提取算法，但现有实现主要局限于CPU执行。卷积操作具有高度并行性，适合在GPU上运行以显著加速计算，但ROCKET使用的不均匀卷积核使得标准GPU卷积方法效率低下。

Method: 提出了一种能够高效在GPU上执行ROCKET的算法，解决了不均匀卷积核在GPU上并行计算的技术难题。

Result: CUROCKET相比CPU版本的ROCKET实现了高达11倍的每瓦计算效率提升，代码已在GitHub开源。

Conclusion: 通过专门设计的GPU算法，成功将ROCKET迁移到GPU平台，大幅提升了计算效率，为时间序列分类提供了更高效的解决方案。

Abstract: ROCKET (RandOm Convolutional KErnel Transform) is a feature extraction algorithm created for Time Series Classification (TSC), published in 2019. It applies convolution with randomly generated kernels on a time series, producing features that can be used to train a linear classifier or regressor like Ridge. At the time of publication, ROCKET was on par with the best state-of-the-art algorithms for TSC in terms of accuracy while being significantly less computationally expensive, making ROCKET a compelling algorithm for TSC. This also led to several subsequent versions, further improving accuracy and computational efficiency. The currently available ROCKET implementations are mostly bound to execution on CPU. However, convolution is a task that can be highly parallelized and is therefore suited to be executed on GPU, which speeds up the computation significantly. A key difficulty arises from the inhomogeneous kernels ROCKET uses, making standard methods for applying convolution on GPU inefficient. In this work, we propose an algorithm that is able to efficiently perform ROCKET on GPU and achieves up to 11 times higher computational efficiency per watt than ROCKET on CPU. The code for CUROCKET is available in this repository https://github.com/oleeven/CUROCKET on github.

</details>


### [177] [The Triangle of Similarity: A Multi-Faceted Framework for Comparing Neural Network Representations](https://arxiv.org/abs/2601.17093)
*Olha Sirikova,Alvin Chan*

Main category: cs.LG

TL;DR: 本文提出三角形相似性框架，结合静态表征相似性、功能相似性和稀疏相似性三个互补视角，用于更全面地比较神经网络表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法对神经网络表示的比较往往视角有限，无法全面评估模型是否收敛到相似的内部机制。需要更全面的框架来支持科学应用中的模型理解和验证。

Method: 提出三角形相似性框架，整合三种互补方法：1)静态表征相似性（CKA/Procrustes）；2)功能相似性（线性模式连接或预测相似性）；3)稀疏相似性（剪枝下的鲁棒性）。应用于CNN、Vision Transformer和视觉语言模型，使用ImageNetV2和CIFAR-10数据集进行评估。

Result: 1) 架构家族是表征相似性的主要决定因素，形成明显聚类；2) CKA自相似性和任务准确度在剪枝过程中强相关，但准确度下降更剧烈；3) 某些模型对在剪枝后表现出正则化效应，暴露出共享的计算核心。

Conclusion: 三角形相似性框架提供了更全面的方法来评估模型是否收敛到相似的内部机制，为科学研究中的模型选择和分析提供了有用工具。

Abstract: Comparing neural network representations is essential for understanding and validating models in scientific applications. Existing methods, however, often provide a limited view. We propose the Triangle of Similarity, a framework that combines three complementary perspectives: static representational similarity (CKA/Procrustes), functional similarity (Linear Mode Connectivity or Predictive Similarity), and sparsity similarity (robustness under pruning). Analyzing a range of CNNs, Vision Transformers, and Vision-Language Models using both in-distribution (ImageNetV2) and out-of-distribution (CIFAR-10) testbeds, our initial findings suggest that: (1) architectural family is a primary determinant of representational similarity, forming distinct clusters; (2) CKA self-similarity and task accuracy are strongly correlated during pruning, though accuracy often degrades more sharply; and (3) for some model pairs, pruning appears to regularize representations, exposing a shared computational core. This framework offers a more holistic approach for assessing whether models have converged on similar internal mechanisms, providing a useful tool for model selection and analysis in scientific research.

</details>


### [178] [Boltzmann-GPT: Bridging Energy-Based World Models and Language Generation](https://arxiv.org/abs/2601.17094)
*Junichiro Niimi*

Main category: cs.LG

TL;DR: 提出"嘴不是大脑"架构原则，将世界模型与语言模型分离，通过Deep Boltzmann Machine捕捉领域结构，结合冻结GPT-2实现可控生成。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能生成流畅文本，但对其是否真正理解世界还是仅产生看似合理的语言存在争议。作者希望明确分离世界理解与语言能力，探究语言模型如何通过与世界模型结合实现一致可控的生成。

Method: 提出三组件架构：1) Deep Boltzmann Machine作为基于能量的世界模型捕捉领域结构；2) 适配器将潜在信念状态投影到嵌入空间；3) 冻结的GPT-2提供语言能力但不含领域知识。在亚马逊智能手机评论领域实例化该框架。

Result: 1) 通过世界模型调节的生成相比仅基于提示的生成，情感相关性显著更高、困惑度更低、语义相似度更大；2) DBM能量函数能区分连贯与不连贯的市场配置，为不合理的品牌-价格组合分配更高能量；3) 对特定属性的干预能因果传播到生成文本，干预输出与目标配置的自然样本在统计上一致。

Conclusion: 即使小规模语言模型，当连接到适当的世界模型时，也能实现一致可控的生成。这为分离语言能力与世界理解提供了实证支持，表明将世界模型与语言模型明确分离是可行的架构原则。

Abstract: Large Language Models (LLMs) generate fluent text, yet whether they truly understand the world or merely produce plausible language about it remains contested. We propose an architectural principle, the mouth is not the brain, that explicitly separates world models from language models. Our architecture comprises three components: a Deep Boltzmann Machine (DBM) that captures domain structure as an energy-based world model, an adapter that projects latent belief states into embedding space, and a frozen GPT-2 that provides linguistic competence without domain knowledge. We instantiate this framework in the consumer review domain using Amazon smartphone reviews. Experiments demonstrate that (1) conditioning through the world model yields significantly higher sentiment correlation, lower perplexity, and greater semantic similarity compared to prompt-based generation alone; (2) the DBM's energy function distinguishes coherent from incoherent market configurations, assigning higher energy to implausible brand-price combinations; and (3) interventions on specific attributes propagate causally to generated text with intervened outputs exhibiting distributions statistically consistent with naturally occurring samples sharing the target configuration. These findings suggest that even small-scale language models can achieve consistent, controllable generation when connected to an appropriate world model, providing empirical support for separating linguistic competence from world understanding.

</details>


### [179] [MambaNet: Mamba-assisted Channel Estimation Neural Network With Attention Mechanism](https://arxiv.org/abs/2601.17108)
*Dianxin Luan,Chengsi Liang,Jie Huang,Zheng Lin,Kaitao Meng,John Thompson,Cheng-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种结合Mamba架构和自注意力机制的神经网络框架，用于OFDM系统的大规模子载波信道估计，在保持低复杂度的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的神经网络在处理大规模子载波信道估计时计算复杂度高，而Mamba架构虽然能有效捕捉长距离依赖关系，但传统单向扫描不适合非因果的信道增益关系。

Method: 提出Mamba辅助的神经网络框架，集成自注意力机制，采用双向选择性扫描（bidirectional selective scan）来处理子载波间的非因果依赖关系，相比传统Transformer具有更低的空间复杂度。

Result: 在3GPP TS 36.101信道模型上的仿真结果表明，相比其他基线神经网络方法，该方法在减少可调参数数量的同时，实现了更好的信道估计性能。

Conclusion: 该框架为大规模子载波OFDM系统提供了一种高效的低复杂度信道估计解决方案，通过结合Mamba架构和双向扫描机制，在性能和复杂度之间取得了良好平衡。

Abstract: This paper proposes a Mamba-assisted neural network framework incorporating self-attention mechanism to achieve improved channel estimation with low complexity for orthogonal frequency-division multiplexing (OFDM) waveforms, particularly for configurations with a large number of subcarriers. With the integration of customized Mamba architecture, the proposed framework handles large-scale subcarrier channel estimation efficiently while capturing long-distance dependencies among these subcarriers effectively. Unlike conventional Mamba structure, this paper implements a bidirectional selective scan to improve channel estimation performance, because channel gains at different subcarriers are non-causal. Moreover, the proposed framework exhibits relatively lower space complexity than transformer-based neural networks. Simulation results tested on the 3GPP TS 36.101 channel demonstrate that compared to other baseline neural network solutions, the proposed method achieves improved channel estimation performance with a reduced number of tunable parameters.

</details>


### [180] [Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts](https://arxiv.org/abs/2601.17111)
*Xuan-Phi Nguyen,Shrey Pandit,Austin Xu,Caiming Xiong,Shafiq Joty*

Main category: cs.LG

TL;DR: 论文提出LLEP算法解决MoE模型中专家路由不平衡问题，通过动态重路由过载token和专家参数到空闲设备，提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE模型在预训练时采用负载均衡约束，但实际路由仍然显著不平衡。这种不平衡虽然有助于领域知识集中，但在专家并行架构下会导致计算和内存问题，特别是在推理和后训练阶段无法使用显式负载均衡时。

Method: 提出最小负载专家并行（LLEP）算法，动态监测设备负载，将过载设备上的多余token和关联专家参数重路由到利用率低的设备，确保所有设备在最小集体延迟内完成工作，同时满足内存约束。

Result: 在不同模型规模下，LLEP相比标准EP实现了最高5倍加速和4倍峰值内存使用减少。对于gpt-oss-120b模型，推理速度提升约1.9倍。

Conclusion: LLEP有效解决了MoE模型中专家路由不平衡导致的硬件效率问题，为硬件特定的超参数调优提供了原则性框架，实现了更快的后训练和推理。

Abstract: Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.

</details>


### [181] [Low-Rank Tensor Approximation of Weights in Large Language Models via Cosine Lanczos Bidiagonalization](https://arxiv.org/abs/2601.17112)
*A. El Ichi,K. Jbilou*

Main category: cs.LG

TL;DR: 提出基于张量积的压缩框架，利用代数结构将权重张量转换到变换域，通过低秩近似实现高效压缩


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然性能强大，但存在内存占用大和计算成本高的问题，需要更高效的压缩方法

Method: 基于张量积的压缩框架，利用代数结构将权重张量转换到变换域，使正面切片能够通过低秩张量因子联合近似

Result: 该方法能够实现计算高效的压缩，比传统SVD方法更好地利用多维相关性

Conclusion: 提出的张量压缩框架为LLM的高效压缩提供了新途径，能够显著降低内存和计算成本

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks but suffer from extremely large memory footprints and computational costs. In this paper, we introduce a tensor compression framework based on the cproduct for computing low rank approximation In the first part of our approach, we leverage the algebraic structure of the cproduct to represent weight tensors such as those in embedding layers, attention projections, and feed forward networks in a transform domain where frontal slices can be jointly approximated by low rank tensor factors. This enables computationally efficient compression that exploits multidimensional correlations beyond traditional SVD methods.

</details>


### [182] [How does Graph Structure Modulate Membership-Inference Risk for Graph Neural Networks?](https://arxiv.org/abs/2601.17130)
*Megha Khosla*

Main category: cs.LG

TL;DR: 本文研究了图神经网络中的成员推理攻击，重点分析了图结构对隐私泄露的影响，包括训练图构建和推理时边访问两个维度，并探讨了差分隐私GNNs的可审计性问题。


<details>
  <summary>Details</summary>
Motivation: GNNs在敏感应用中的使用引发了训练数据泄露的担忧。现有隐私泄露研究主要借鉴非图领域（如图像和表格数据）的发现，缺乏针对图结构的专门分析。本文强调需要进行图特定的隐私分析，研究图结构对节点级成员推理攻击的影响。

Method: 1. 形式化节点-邻居元组的成员推理攻击；2. 研究两个关键维度：训练图构建（随机采样vs雪球采样）和推理时边访问；3. 实证分析不同模型和数据集；4. 探讨差分隐私GNNs的可审计性，适应图模型的统计可交换性定义。

Result: 1. 雪球采样的覆盖偏差通常会损害泛化能力；2. 推理时允许训练-测试边访问能提高测试精度、缩小训练-测试差距，并在大多数模型和数据集上获得最低的成员优势；3. 泛化差距（训练和测试节点性能差异）是成员推理风险的不足代理指标；4. 对于节点级任务，归纳分割（随机或雪球采样）破坏了可交换性，限制了差分隐私模型成员优势标准界限的适用性。

Conclusion: 图结构对GNNs的隐私泄露有重要影响，需要进行专门的图特定分析。推理时边访问对成员推理攻击效果有主导作用，而泛化差距不能完全反映隐私风险。差分隐私GNNs在节点级任务中的可审计性受到归纳分割破坏可交换性的限制。

Abstract: Graph neural networks (GNNs) have become the standard tool for encoding data and their complex relationships into continuous representations, improving prediction accuracy in several machine learning tasks like node classification and link prediction. However, their use in sensitive applications has raised concerns about the potential leakage of training data. Research on privacy leakage in GNNs has largely been shaped by findings from non-graph domains, such as images and tabular data. We emphasize the need of graph specific analysis and investigate the impact of graph structure on node level membership inference. We formalize MI over node-neighbourhood tuples and investigate two important dimensions: (i) training graph construction and (ii) inference-time edge access. Empirically, snowball's coverage bias often harms generalisation relative to random sampling, while enabling inter-train-test edges at inference improves test accuracy, shrinks the train-test gap, and yields the lowest membership advantage across most of the models and datasets. We further show that the generalisation gap empirically measured as the performance difference between the train and test nodes is an incomplete proxy for MI risk: access to edges dominates-MI can rise or fall independent of gap changes. Finally, we examine the auditability of differentially private GNNs, adapting the definition of statistical exchangeability of train-test data points for graph based models. We show that for node level tasks the inductive splits (random or snowball sampled) break exchangeability, limiting the applicability of standard bounds for membership advantage of differential private models.

</details>


### [183] [Learning to Collaborate: An Orchestrated-Decentralized Framework for Peer-to-Peer LLM Federation](https://arxiv.org/abs/2601.17133)
*Inderjeet Singh,Eleonore Vissol-Gaudin,Andikan Otung,Motoyoshi Sekiya*

Main category: cs.LG

TL;DR: KNEXA-FL是一个用于大语言模型微调的去中心化联邦学习框架，通过上下文多臂老虎机算法优化异构代理间的匹配，避免负迁移并保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在专业领域大语言模型微调中存在两难：需要跨组织数据但受限于数据隐私；集中式联邦学习有单点故障和模型反演攻击风险；去中心化联邦学习采用随机P2P配对，但忽视代理异构性并可能导致负迁移。

Method: 提出KNEXA-FL框架，采用非聚合的中央分析器/匹配器(CPM)，将P2P协作建模为上下文多臂老虎机问题，使用LinUCB算法基于抽象代理配置文件学习最优匹配策略。通过安全蒸馏实现基于PEFT的异构LLM代理间的直接知识交换，不访问模型本身。

Result: 在代码生成任务上的实验表明，KNEXA-FL相比随机P2P协作将Pass@1提升了约50%。该协调方法展现出稳定收敛，而强大的集中式蒸馏基线则出现灾难性性能崩溃。

Conclusion: 自适应、基于学习的协调是构建稳健有效的去中心化AI生态系统的基础原则，KNEXA-FL成功解决了隐私保护与协作效率之间的权衡问题。

Abstract: Fine-tuning Large Language Models (LLMs) for specialized domains is constrained by a fundamental challenge: the need for diverse, cross-organizational data conflicts with the principles of data privacy and sovereignty. While Federated Learning (FL) provides a framework for collaboration without raw data exchange, its classic centralized form introduces a single point of failure and remains vulnerable to model inversion attacks. Decentralized FL (DFL) mitigates this risk by removing the central aggregator but typically relies on inefficient, random peer-to-peer (P2P) pairings, forming a collaboration graph that is blind to agent heterogeneity and risks negative transfer. This paper introduces KNEXA-FL, a novel framework for orchestrated decentralization that resolves this trade-off. KNEXA-FL employs a non-aggregating Central Profiler/Matchmaker (CPM) that formulates P2P collaboration as a contextual bandit problem, using a LinUCB algorithm on abstract agent profiles to learn an optimal matchmaking policy. It orchestrates direct knowledge exchange between heterogeneous, PEFT-based LLM agents via secure distillation, without ever accessing the models themselves. Our comprehensive experiments on a challenging code generation task show that KNEXA-FL yields substantial gains, improving Pass@1 by approx. 50% relative to random P2P collaboration. Critically, our orchestrated approach demonstrates stable convergence, in stark contrast to a powerful centralized distillation baseline which suffers from catastrophic performance collapse. Our work establishes adaptive, learning-based orchestration as a foundational principle for building robust and effective decentralized AI ecosystems.

</details>


### [184] [ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning](https://arxiv.org/abs/2601.17135)
*Jakob Karalus,Friedhelm Schwenker*

Main category: cs.LG

TL;DR: ConceptACT通过利用演示中的语义概念注释（如物体属性、空间关系、任务约束）来改进模仿学习，在训练时使用概念感知的交叉注意力机制，无需在部署时提供语义输入，显著提高了学习效率和样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习方法仅依赖低级的传感器数据，忽略了人类自然拥有的丰富语义知识（如物体属性、空间关系、任务约束）。这些语义知识可以为机器人学习提供强大的归纳偏置，提高学习效率。

Method: ConceptACT是Action Chunking with Transformers的扩展，在训练时利用演示级别的语义概念注释。采用修改的transformer架构，在最终编码器层实现概念感知的交叉注意力机制，并通过监督学习使其与人类注释对齐。概念仅在演示收集时由人类提供，部署时不需要语义输入。

Result: 在两个具有逻辑约束的机器人操作任务上的实验表明，ConceptACT比标准ACT收敛更快，样本效率更高。通过注意力机制进行架构集成的方法显著优于简单的辅助预测损失或语言条件模型。

Conclusion: 适当集成的语义监督为更高效的机器人学习提供了强大的归纳偏置。ConceptACT展示了在训练时利用人类语义知识（无需部署时输入）可以显著提高模仿学习的效率。

Abstract: Imitation learning enables robots to acquire complex manipulation skills from human demonstrations, but current methods rely solely on low-level sensorimotor data while ignoring the rich semantic knowledge humans naturally possess about tasks. We present ConceptACT, an extension of Action Chunking with Transformers that leverages episode-level semantic concept annotations during training to improve learning efficiency. Unlike language-conditioned approaches that require semantic input at deployment, ConceptACT uses human-provided concepts (object properties, spatial relationships, task constraints) exclusively during demonstration collection, adding minimal annotation burden. We integrate concepts using a modified transformer architecture in which the final encoder layer implements concept-aware cross-attention, supervised to align with human annotations. Through experiments on two robotic manipulation tasks with logical constraints, we demonstrate that ConceptACT converges faster and achieves superior sample efficiency compared to standard ACT. Crucially, we show that architectural integration through attention mechanisms significantly outperforms naive auxiliary prediction losses or language-conditioned models. These results demonstrate that properly integrated semantic supervision provides powerful inductive biases for more efficient robot learning.

</details>


### [185] [Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging](https://arxiv.org/abs/2601.17180)
*Inés Gonzalez-Pepe,Vinuyan Sivakolunthu,Jacob Fortin,Yohan Chatelain,Tristan Glatard*

Main category: cs.LG

TL;DR: 通过分析CNN数值不确定性，发现许多操作对数值噪声主导的值进行无效计算。提出Conservative & Aggressive NaNs两种最大池化和反池化变体，用NaN标记不稳定体素，让后续层跳过无关计算，实现推理加速。


<details>
  <summary>Details</summary>
Motivation: 神经影像深度学习模型架构越来越大，效率成为持续关注点。研究发现许多卷积操作应用于数值噪声主导的值，对模型输出影响可忽略，部分模型中高达三分之二的卷积操作是冗余的。

Method: 提出Conservative & Aggressive NaNs两种方法：1）识别数值不稳定体素并用NaN替换；2）后续层可跳过NaN数据的计算。在PyTorch中实现，无需架构更改，适用于最大池化和反池化操作。

Result: 在含50%以上NaN的输入中观察到运行时改进；在超过三分之二NaN的数据中平均推理加速1.67倍。Conservative NaNs平均减少30%卷积操作，特定层可跳过64.64%卷积，无性能下降。Aggressive NaNs可跳过69.30%卷积，但可能影响性能。

Conclusion: 利用数值不确定性可减少CNN中的冗余计算，提高推理效率。两种方法在神经影像和图像分类任务中有效，特别是神经影像场景常见高比例NaN数据时效果显著。

Abstract: Deep learning models for neuroimaging increasingly rely on large architectures, making efficiency a persistent concern despite advances in hardware. Through an analysis of numerical uncertainty of convolutional neural networks (CNNs), we observe that many operations are applied to values dominated by numerical noise and have negligible influence on model outputs. In some models, up to two-thirds of convolution operations appear redundant. We introduce Conservative & Aggressive NaNs, two novel variants of max pooling and unpooling that identify numerically unstable voxels and replace them with NaNs, allowing subsequent layers to skip computations on irrelevant data. Both methods are implemented within PyTorch and require no architectural changes. We evaluate these approaches on four CNN models spanning neuroimaging and image classification tasks. For inputs containing at least 50% NaNs, we observe consistent runtime improvements; for data with more than two-thirds NaNs )common in several neuroimaging settings) we achieve an average inference speedup of 1.67x. Conservative NaNs reduces convolution operations by an average of 30% across models and datasets, with no measurable performance degradation, and can skip up to 64.64% of convolutions in specific layers. Aggressive NaNs can skip up to 69.30% of convolutions but may occasionally affect performance. Overall, these methods demonstrate that numerical uncertainty can be exploited to reduce redundant computation and improve inference efficiency in CNNs.

</details>


### [186] [Federated Proximal Optimization for Privacy-Preserving Heart Disease Prediction: A Controlled Simulation Study on Non-IID Clinical Data](https://arxiv.org/abs/2601.17183)
*Farzam Asad,Junaid Saif Khan,Maria Tariq,Sundus Munir,Muhammad Adnan Khan*

Main category: cs.LG

TL;DR: 该研究通过模拟非IID医疗数据分区，验证了FedProx算法在心脏病预测中的有效性，在保护隐私的同时实现了比集中式学习和孤立本地模型更好的准确率。


<details>
  <summary>Details</summary>
Motivation: 医疗数据因隐私法规无法直接共享，而临床数据固有的非IID特性（人口差异、疾病流行率差异等）给联邦学习带来挑战。需要研究如何在保护隐私的同时，在异构医疗数据上实现有效的协同模型训练。

Method: 使用UCI心脏病数据集（克利夫兰诊所数据，303名患者），通过基于人口统计的分层方法模拟生成四个异构医院客户端的非IID数据分区。采用FedProx算法，通过近端正则化（mu=0.05）来缓解客户端漂移问题，并进行50次独立运行的广泛消融研究。

Result: FedProx在mu=0.05时达到85.00%的准确率，优于集中式学习（83.33%）和孤立本地模型（平均78.45%）。实验证明近端正则化能有效抑制异构环境中的客户端漂移。

Conclusion: 该概念验证研究表明FedProx在异构医疗数据环境中具有优势，为实际医疗联邦学习系统提供了算法见解和部署指南，结果可直接应用于医院IT管理员的隐私保护协同学习实施。

Abstract: Healthcare institutions have access to valuable patient data that could be of great help in the development of improved diagnostic models, but privacy regulations like HIPAA and GDPR prevent hospitals from directly sharing data with one another. Federated Learning offers a way out to this problem by facilitating collaborative model training without having the raw patient data centralized. However, clinical datasets intrinsically have non-IID (non-independent and identically distributed) features brought about by demographic disparity and diversity in disease prevalence and institutional practices. This paper presents a comprehensive simulation research of Federated Proximal Optimization (FedProx) for Heart Disease prediction based on UCI Heart Disease dataset. We generate realistic non-IID data partitions by simulating four heterogeneous hospital clients from the Cleveland Clinic dataset (303 patients), by inducing statistical heterogeneity by demographic-based stratification. Our experimental results show that FedProx with proximal parameter mu=0.05 achieves 85.00% accuracy, which is better than both centralized learning (83.33%) and isolated local models (78.45% average) without revealing patient privacy. Through generous sheer ablation studies with statistical validation on 50 independent runs we demonstrate that proximal regularization is effective in curbing client drift in heterogeneous environments. This proof-of-concept research offers algorithmic insights and practical deployment guidelines for real-world federated healthcare systems, and thus, our results are directly transferable to hospital IT-administrators, implementing privacy-preserving collaborative learning.

</details>


### [187] [Rethinking Benchmarks for Differentially Private Image Classification](https://arxiv.org/abs/2601.17189)
*Sabrina Mokhtari,Sara Kodeiri,Shubhankar Mohapatra,Florian Tramer,Gautam Kamath*

Main category: cs.LG

TL;DR: 重新审视差分隐私图像分类的基准测试，提出全面的基准集，创建公开排行榜


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私机器学习缺乏全面的基准测试，难以有效评估不同技术在各种实际场景中的表现

Method: 提出一套全面的基准测试集，涵盖不同场景（有无额外数据、凸优化设置、不同数据集），并测试现有技术在这些基准上的表现

Result: 建立了差分隐私图像分类的综合基准框架，测试了现有技术的有效性，创建了公开的排行榜

Conclusion: 通过建立全面的基准测试和公开排行榜，为差分隐私机器学习社区提供了标准化的评估框架，有助于推动该领域的发展

Abstract: We revisit benchmarks for differentially private image classification. We suggest a comprehensive set of benchmarks, allowing researchers to evaluate techniques for differentially private machine learning in a variety of settings, including with and without additional data, in convex settings, and on a variety of qualitatively different datasets. We further test established techniques on these benchmarks in order to see which ideas remain effective in different settings. Finally, we create a publicly available leader board for the community to track progress in differentially private machine learning.

</details>


### [188] [PUNCH: Physics-informed Uncertainty-aware Network for Coronary Hemodynamics](https://arxiv.org/abs/2601.17192)
*Sukirt Thakur,Marcus Roper,Yang Zhou,Reza Akbarian Bafghi,Brahmajee K. Nallamothu,C. Alberto Figueroa,Srinivas Paruchuri,Scott Burger,Maziar Raissi*

Main category: cs.LG

TL;DR: 提出一种非侵入性、不确定性感知的框架，直接从标准血管造影中估计冠状动脉血流储备，无需真实血流测量数据，可在3分钟内完成单患者分析。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉微血管功能障碍影响数百万人，但现有金标准测量方法具有侵入性且可重复性差，需要开发非侵入、可扩展的诊断方法。

Method: 结合物理信息神经网络与变分推断，从造影剂传输的第一原理模型推断冠状动脉血流，无需真实血流测量数据，使用合成时空强度图进行验证。

Result: 在12名患者中，该方法与侵入性热稀释法测量结果高度一致，预测不确定性与误差有强相关性，置信区间比重复侵入测量的变异性更窄。

Conclusion: 该框架将常规血管造影转化为定量、不确定性感知的评估，可实现可扩展、更安全、更可重复的冠状动脉微血管功能评估，有望扩大CMD诊断的可及性。

Abstract: Coronary microvascular dysfunction (CMD) affects millions worldwide yet remains underdiagnosed because gold-standard physiological measurements are invasive and variably reproducible. We introduce a non-invasive, uncertainty-aware framework for estimating coronary flow reserve (CFR) directly from standard angiography. The system integrates physics-informed neural networks with variational inference to infer coronary blood flow from first-principles models of contrast transport, without requiring ground-truth flow measurements. The pipeline runs in approximately three minutes per patient on a single GPU, with no population-level training.
  Using 1{,}000 synthetic spatiotemporal intensity maps (kymographs) with controlled noise and artifacts, the framework reliably identifies degraded data and outputs appropriately inflated uncertainty estimates, showing strong correspondence between predictive uncertainty and error (Pearson $r = 0.997$, Spearman $ρ= 0.998$). Clinical validation in 12 patients shows strong agreement between PUNCH-derived CFR and invasive bolus thermodilution (Pearson $r = 0.90$, $p = 6.3 \times 10^{-5}$). We focus on the LAD, the artery most commonly assessed in routine CMD testing. Probabilistic CFR estimates have confidence intervals narrower than the variability of repeated invasive measurements.
  By transforming routine angiography into quantitative, uncertainty-aware assessment, this approach enables scalable, safer, and more reproducible evaluation of coronary microvascular function. Because standard angiography is widely available globally, the framework could expand access to CMD diagnosis and establish a new paradigm for physics-informed, patient-specific inference from clinical imaging.

</details>


### [189] [Accelerated Sinkhorn Algorithms for Partial Optimal Transport](https://arxiv.org/abs/2601.17196)
*Nghia Thu Truong,Qui Phu Pham,Quang Nguyen,Dung Luong,Mai Tran*

Main category: cs.LG

TL;DR: ASPOT方法将交替最小化与Nesterov加速结合，将部分最优传输的复杂度提升至O(n^{7/3}ε^{-5/3})，比传统Sinkhorn方法更优


<details>
  <summary>Details</summary>
Motivation: 部分最优传输（POT）处理两个分布间仅传输部分质量的问题，适用于边缘分布大小不等或包含异常值的情况。虽然Sinkhorn方法被广泛使用，但其在POT中的复杂度边界不理想，限制了可扩展性。

Method: 提出加速Sinkhorn for POT（ASPOT）方法，将交替最小化与Nesterov风格加速结合到POT设置中。同时证明，通过明智选择熵参数γ可以改进经典Sinkhorn方法的收敛速率。

Result: ASPOT实现了O(n^{7/3}ε^{-5/3})的复杂度，优于传统Sinkhorn方法。通过优化熵参数选择，经典Sinkhorn方法的收敛速率也得到了提升。在实际应用中的实验验证了理论结果，并展示了所提方法的优越性能。

Conclusion: ASPOT方法通过加速技术显著改善了部分最优传输问题的计算复杂度，同时优化熵参数选择能提升传统Sinkhorn方法的性能，为实际应用提供了更高效的解决方案。

Abstract: Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\mathcal{O}(n^{7/3}\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $γ$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.

</details>


### [190] [SpecBridge: Bridging Mass Spectrometry and Molecular Representations via Cross-Modal Alignment](https://arxiv.org/abs/2601.17204)
*Yinkai Wang,Yan Zhou Chen,Xiaohui Chen,Li-Ping Liu,Soha Hassoun*

Main category: cs.LG

TL;DR: SpecBridge是一种新颖的隐式对齐框架，将小分子质谱鉴定视为几何对齐问题，通过将质谱编码器投影到冻结分子基础模型的潜在空间中，实现了比现有方法显著提升的检索准确率。


<details>
  <summary>Details</summary>
Motivation: 目前基于深度学习的小分子质谱鉴定方法存在两极分化：要么是显式生成模型逐原子构建分子图，要么是从头学习跨模态子空间的联合对比模型。这两种方法都有局限性，需要一种更实用、稳定的替代方案。

Method: SpecBridge采用隐式对齐框架，微调自监督质谱编码器（DreaMS）直接投影到冻结的分子基础模型（ChemBERTa）的潜在空间中，然后通过余弦相似度与预计算的分子嵌入库进行检索。

Result: 在MassSpecGym、Spectraverse和MSnLib基准测试中，SpecBridge相对于强神经网络基线，将top-1检索准确率提高了约20-25%，同时保持了较少的可训练参数数量。

Conclusion: 与冻结基础模型对齐是一种实用且稳定的替代方案，避免了从头设计新架构的复杂性。这种方法在保持参数效率的同时显著提升了小分子质谱鉴定的准确性。

Abstract: Small-molecule identification from tandem mass spectrometry (MS/MS) remains a bottleneck in untargeted settings where spectral libraries are incomplete. While deep learning offers a solution, current approaches typically fall into two extremes: explicit generative models that construct molecular graphs atom-by-atom, or joint contrastive models that learn cross-modal subspaces from scratch. We introduce SpecBridge, a novel implicit alignment framework that treats structure identification as a geometric alignment problem. SpecBridge fine-tunes a self-supervised spectral encoder (DreaMS) to project directly into the latent space of a frozen molecular foundation model (ChemBERTa), and then performs retrieval by cosine similarity to a fixed bank of precomputed molecular embeddings. Across MassSpecGym, Spectraverse, and MSnLib benchmarks, SpecBridge improves top-1 retrieval accuracy by roughly 20-25% relative to strong neural baselines, while keeping the number of trainable parameters small. These results suggest that aligning to frozen foundation models is a practical, stable alternative to designing new architectures from scratch. The code for SpecBridge is released at https://github.com/HassounLab/SpecBridge.

</details>


### [191] [NewPINNs: Physics-Informing Neural Networks Using Conventional Solvers for Partial Differential Equations](https://arxiv.org/abs/2601.17207)
*Maedeh Makki,Satish Chandran,Maziar Raissi,Adrien Grenier,Behzad Mohebbi*

Main category: cs.LG

TL;DR: NewPINNs 是一个将神经网络与传统数值求解器耦合的物理信息学习框架，通过求解器一致性而非残差损失来训练网络，解决微分方程问题。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINNs）存在优化病态、损失权重敏感、在刚性或非线性区域表现差等问题。NewPINNs 旨在通过利用成熟数值求解器来规避这些失败模式。

Method: 将数值求解器直接集成到训练循环中。神经网络生成候选解状态，数值求解器推进这些状态，训练目标是最小化网络预测与求解器演化状态之间的差异。

Result: 该方法在涉及有限体积、有限元和谱求解器的多个正问题和反问题中表现出有效性。

Conclusion: NewPINNs 通过将物理约束、边界条件和数值稳定性委托给成熟的数值求解器，减轻了标准 PINNs 的多个已知缺陷，无需特定问题的损失工程或显式计算微分方程残差。

Abstract: We introduce NewPINNs, a physics-informing learning framework that couples neural networks with conventional numerical solvers for solving differential equations. Rather than enforcing governing equations and boundary conditions through residual-based loss terms, NewPINNs integrates the solver directly into the training loop and defines learning objectives through solver-consistency. The neural network produces candidate solution states that are advanced by the numerical solver, and training minimizes the discrepancy between the network prediction and the solver-evolved state. This pull-push interaction enables the network to learn physically admissible solutions through repeated exposure to the solver's action, without requiring problem-specific loss engineering or explicit evaluation of differential equation residuals. By delegating the enforcement of physics, boundary conditions, and numerical stability to established numerical solvers, NewPINNs mitigates several well-known failure modes of standard physics-informed neural networks, including optimization pathologies, sensitivity to loss weighting, and poor performance in stiff or nonlinear regimes. We demonstrate the effectiveness of the proposed approach across multiple forward and inverse problems involving finite volume, finite element, and spectral solvers.

</details>


### [192] [Quantum-Inspired Episode Selection for Monte Carlo Reinforcement Learning via QUBO Optimization](https://arxiv.org/abs/2601.17570)
*Hadi Salloum,Ali Jnadi,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.LG

TL;DR: 本文提出MC+QUBO方法，通过将蒙特卡洛强化学习中的轨迹选择问题转化为二次无约束二进制优化问题，利用量子启发采样器提升稀疏奖励、大状态空间环境中的学习效率。


<details>
  <summary>Details</summary>
Motivation: 蒙特卡洛强化学习在高样本复杂度环境下表现不佳，特别是在稀疏奖励、大状态空间和相关轨迹的场景中。现有方法难以平衡高奖励轨迹的选择与状态空间的充分覆盖。

Method: 提出MC+QUBO框架，将轨迹选择问题建模为QUBO问题：线性项偏好高奖励轨迹，二次项惩罚轨迹冗余性。使用模拟量子退火和模拟分岔两种量子启发采样器作为黑盒求解器。

Result: 在有限时域GridWorld实验中，MC+QUBO在收敛速度和最终策略质量上均优于传统蒙特卡洛方法，验证了量子启发优化在强化学习决策子程序中的潜力。

Conclusion: 将轨迹选择问题重新表述为QUBO问题并结合量子启发优化器，能够有效提升蒙特卡洛强化学习的样本效率，为量子计算在强化学习中的应用提供了新思路。

Abstract: Monte Carlo (MC) reinforcement learning suffers from high sample complexity, especially in environments with sparse rewards, large state spaces, and correlated trajectories. We address these limitations by reformulating episode selection as a Quadratic Unconstrained Binary Optimization (QUBO) problem and solving it with quantum-inspired samplers. Our method, MC+QUBO, integrates a combinatorial filtering step into standard MC policy evaluation: from each batch of trajectories, we select a subset that maximizes cumulative reward while promoting state-space coverage. This selection is encoded as a QUBO, where linear terms favor high-reward episodes and quadratic terms penalize redundancy. We explore both Simulated Quantum Annealing (SQA) and Simulated Bifurcation (SB) as black-box solvers within this framework. Experiments in a finite-horizon GridWorld demonstrate that MC+QUBO outperforms vanilla MC in convergence speed and final policy quality, highlighting the potential of quantum-inspired optimization as a decision-making subroutine in reinforcement learning.

</details>


### [193] [JetFormer: A Scalable and Efficient Transformer for Jet Tagging from Offline Analysis to FPGA Triggers](https://arxiv.org/abs/2601.17215)
*Ruoqing Zheng,Chang Sun,Qibin Liu,Lauri Laatu,Arianna Cox,Benedikt Maier,Alexander Tapper,Jose G. F. Coutinho,Wayne Luk,Zhiqiang Que*

Main category: cs.LG

TL;DR: JetFormer是一种专为粒子喷注标记设计的可扩展Transformer架构，能在从离线分析到在线触发的全场景下高效运行，在保持高性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的喷注标记方法通常针对特定部署场景设计，缺乏一个统一架构能在从高精度离线分析到超低延迟在线触发的全谱系场景中有效工作。需要一种既保持高性能又具备硬件部署可行性的解决方案。

Method: 提出JetFormer，一种基于编码器Transformer的架构，处理可变长度粒子特征集而不依赖显式成对相互作用。采用硬件感知优化流程，包括多目标超参数搜索、结构化剪枝和量化，生成适合FPGA部署的紧凑变体。

Result: 在JetClass数据集上，JetFormer在减少37.4%FLOPs的情况下，性能与ParT模型相当（差距仅0.7%）。在HLS4ML 150P基准测试中，比MLPs、Deep Sets和Interaction Networks准确率高3-4%。通过压缩优化，JetFormer-tiny变体能满足FPGA触发系统的亚微秒级延迟要求。

Conclusion: JetFormer通过统一的架构框架，成功地将高性能建模与硬件可部署性相结合，为LHC的离线和在线环境中部署基于Transformer的喷注标记器提供了实用途径。

Abstract: We present JetFormer, a versatile and scalable encoder-only Transformer architecture for particle jet tagging at the Large Hadron Collider (LHC). Unlike prior approaches that are often tailored to specific deployment regimes, JetFormer is designed to operate effectively across the full spectrum of jet tagging scenarios, from high-accuracy offline analysis to ultra-low-latency online triggering. The model processes variable-length sets of particle features without relying on input of explicit pairwise interactions, yet achieves competitive or superior performance compared to state-of-the-art methods. On the large-scale JetClass dataset, a large-scale JetFormer matches the accuracy of the interaction-rich ParT model (within 0.7%) while using 37.4% fewer FLOPs, demonstrating its computational efficiency and strong generalization. On benchmark HLS4ML 150P datasets, JetFormer consistently outperforms existing models such as MLPs, Deep Sets, and Interaction Networks by 3-4% in accuracy. To bridge the gap to hardware deployment, we further introduce a hardware-aware optimization pipeline based on multi-objective hyperparameter search, yielding compact variants like JetFormer-tiny suitable for FPGA-based trigger systems with sub-microsecond latency requirements. Through structured pruning and quantization, we show that JetFormer can be aggressively compressed with minimal accuracy loss. By unifying high-performance modeling and deployability within a single architectural framework, JetFormer provides a practical pathway for deploying Transformer-based jet taggers in both offline and online environments at the LHC. Code is available at https://github.com/walkieq/JetFormer.

</details>


### [194] [Parameter Inference and Uncertainty Quantification with Diffusion Models: Extending CDI to 2D Spatial Conditioning](https://arxiv.org/abs/2601.17224)
*Dmitrii Torbunov,Yihui Ren,Lijun Wu,Yimei Zhu*

Main category: cs.LG

TL;DR: 将条件扩散模型从一维时间信号扩展到二维空间数据，用于科学逆问题的不确定性量化，并在电子衍射参数推断中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 科学逆问题需要量化不确定性以区分可识别参数和模糊参数。虽然CDI在一维时间信号上有效，但其在更高维空间数据上的适用性尚未探索。

Method: 扩展CDI到二维空间条件化，直接从空间观测进行概率参数推断。在收敛束电子衍射（CBED）参数推断这一具有挑战性的多参数逆问题上进行验证。

Result: CDI产生了校准良好的后验分布：对确定性参数有紧密分布，对模糊参数有适当宽泛的分布。相比之下，标准回归方法虽然总体指标看似准确，却通过预测训练集均值来掩盖不确定性。

Conclusion: CDI成功从时间域扩展到空间域，为稳健的科学推断提供了真实的不确定性信息。

Abstract: Uncertainty quantification is critical in scientific inverse problems to distinguish identifiable parameters from those that remain ambiguous given available measurements. The Conditional Diffusion Model-based Inverse Problem Solver (CDI) has previously demonstrated effective probabilistic inference for one-dimensional temporal signals, but its applicability to higher-dimensional spatial data remains unexplored. We extend CDI to two-dimensional spatial conditioning, enabling probabilistic parameter inference directly from spatial observations. We validate this extension on convergent beam electron diffraction (CBED) parameter inference - a challenging multi-parameter inverse problem in materials characterization where sample geometry, electronic structure, and thermal properties must be extracted from 2D diffraction patterns. Using simulated CBED data with ground-truth parameters, we demonstrate that CDI produces well-calibrated posterior distributions that accurately reflect measurement constraints: tight distributions for well-determined quantities and appropriately broad distributions for ambiguous parameters. In contrast, standard regression methods - while appearing accurate on aggregate metrics - mask this underlying uncertainty by predicting training set means for poorly constrained parameters. Our results confirm that CDI successfully extends from temporal to spatial domains, providing the genuine uncertainty information required for robust scientific inference.

</details>


### [195] [A Constrained Optimization Perspective of Unrolled Transformers](https://arxiv.org/abs/2601.17257)
*Javier Porras-Valenzuela,Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种约束优化框架，使Transformer像优化下降算法一样工作，通过层间下降约束和原始-对偶训练方案，确保中间表示在期望上单调降低损失。


<details>
  <summary>Details</summary>
Motivation: 传统ERM训练可能无法保证Transformer中间层的优化行为，希望设计一种能像优化算法一样具有单调下降特性的Transformer，以提升鲁棒性和泛化能力。

Method: 1.引入层间下降约束，强制目标函数在每层后单调下降；2.用原始-对偶训练方案替代标准ERM；3.应用于展开式Transformer和预训练Transformer；4.在视频去噪和文本分类任务上验证。

Result: 约束Transformer在对抗扰动下表现出更强的鲁棒性，保持更高的分布外泛化能力，同时不损害分布内性能。

Conclusion: 通过约束优化框架训练Transformer具有优化算法特性，能有效提升模型鲁棒性和泛化能力，为Transformer设计提供新思路。

Abstract: We introduce a constrained optimization framework for training transformers that behave like optimization descent algorithms. Specifically, we enforce layerwise descent constraints on the objective function and replace standard empirical risk minimization (ERM) with a primal-dual training scheme. This approach yields models whose intermediate representations decrease the loss monotonically in expectation across layers. We apply our method to both unrolled transformer architectures and conventional pretrained transformers on tasks of video denoising and text classification. Across these settings, we observe constrained transformers achieve stronger robustness to perturbations and maintain higher out-of-distribution generalization, while preserving in-distribution performance.

</details>


### [196] [Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions](https://arxiv.org/abs/2601.18107)
*Pedram Agand,Mo Chen*

Main category: cs.LG

TL;DR: MoReBRAC是一个基于模型的离线强化学习框架，通过不确定性感知的潜在合成来缓解分布偏移问题，在随机和次优数据场景中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在工业机器人等安全关键领域具有巨大潜力，但静态数据集与学习策略之间的分布偏移问题导致需要高度保守的策略，限制了策略改进的可能性。

Method: MoReBRAC采用双循环世界模型合成高保真度转移来增强训练流形，通过分层不确定性管道（包括VAE流形检测、模型敏感性分析和MC dropout）确保合成数据的可靠性。

Result: 在D4RL Gym-MuJoCo基准测试中表现出显著的性能提升，特别是在"随机"和"次优"数据场景中。研究还揭示了VAE作为几何锚点的作用，并探讨了从近最优数据集中学习时的分布权衡。

Conclusion: MoReBRAC通过不确定性感知的潜在合成有效缓解了离线强化学习中的分布偏移问题，在挑战性数据场景中实现了性能提升，为模型化离线强化学习提供了新思路。

Abstract: Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.

</details>


### [197] [The Viscosity of Logic: Phase Transitions and Hysteresis in DPO Alignment](https://arxiv.org/abs/2601.17260)
*Marco Pollanen*

Main category: cs.LG

TL;DR: DPO的β参数不总是越大越好，能力变化非线性且模型架构依赖，偏好边界可能反推理性能力，需要能力分辨评估而非依赖单一指标。


<details>
  <summary>Details</summary>
Motivation: 挑战当前DPO调优中"β越大对齐越好"的假设，探索β作为控制参数在不同模型架构下的真实影响。

Method: 在三个7B开源模型家族（Mistral、Llama、Qwen）上固定DPO配方，密集扫描β参数，分析逻辑探针边界、能力变化和训练路径依赖性。

Result: Mistral能力非单调，仅在β≈10⁻²窄带内逻辑边界为正；不同架构响应模式各异；DPO偏好边界与推理能力反相关（Llama r=-0.91）；高β导致能力损失存在滞后效应。

Conclusion: 需要在β参数空间进行能力分辨评估，而非依赖偏好边界或聚合基准，β调优需考虑模型架构特异性和训练历史。

Abstract: Direct Preference Optimization (DPO) is often tuned as if increasing alignment pressure (controlled by $β$) yields progressively "better" behavior. We instead treat $β$ as a control parameter and densely sweep it for three 7B open-weight families under a fixed DPO recipe. In Mistral, capability is sharply non-monotonic: aggregated logic-probe margins become positive only in a narrow band near $β\approx 10^{-2}$ and revert outside it, with boundary points that are seed-sensitive. Across architectures under the same sweep, we observe qualitatively different response modes: sharp reorganization in Mistral, selective changes in Llama, and smooth trade-offs in Qwen. Critically, the DPO preference margin can anticorrelate with reasoning capability (Pearson $r=-0.91$ for Llama logic), so margin-based selection can prefer capability-impaired models. Training path also matters: exposure to high $β$ induces capability losses that persist even after $β$ is reduced (hysteresis). These findings motivate capability-resolved evaluation across the $β$ landscape rather than reliance on margins or aggregate benchmarks.

</details>


### [198] [AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning](https://arxiv.org/abs/2601.17261)
*Wei Lin,Yining Jiang,Qingyu Song,Qiao Xiang,Hong Xu*

Main category: cs.LG

TL;DR: AGZO提出一种激活引导的零阶优化方法，通过利用前向传播中的激活结构信息，在低秩子空间中进行扰动，显著提升零阶优化的性能，同时保持与现有零阶方法相似的内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法通常使用各向同性扰动，忽略了前向传播中丰富的激活结构信息。作者发现线性层的梯度被限制在其输入激活张成的子空间中，这为设计更高效的零阶优化方法提供了理论依据。

Method: AGZO在前向传播过程中实时提取紧凑的激活信息子空间，将扰动限制在这个低秩子空间中。该方法优化了一个子空间平滑的目标函数，理论上证明其更新方向与真实梯度的余弦相似度高于各向同性基线方法。

Result: 在Qwen3和Pangu模型上的实验表明，AGZO持续优于最先进的零阶基线方法，显著缩小了与一阶微调的性能差距，同时保持了与其他零阶方法几乎相同的峰值内存占用。

Conclusion: AGZO通过利用激活结构信息显著提升了零阶优化的效率，为大语言模型在严格内存约束下的微调提供了更有效的解决方案，同时保持了零阶优化的内存优势。

Abstract: Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.

</details>


### [199] [Unrolled Neural Networks for Constrained Optimization](https://arxiv.org/abs/2601.17274)
*Samar Hadou,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出约束对偶展开（CDU）框架，通过两个耦合的神经网络来近似拉格朗日函数的鞍点，用于解决约束优化问题，相比传统对偶上升算法具有加速和学习能力。


<details>
  <summary>Details</summary>
Motivation: 传统对偶上升算法在解决约束优化问题时效率有限，且难以适应未知的乘子分布。需要开发一种能够学习优化过程、加速收敛并具备良好泛化能力的新方法。

Method: 1. 提出CDU框架：包含两个耦合神经网络（原始网络和对偶网络）；2. 原始网络模拟迭代优化器，对给定对偶乘子寻找拉格朗日函数的驻点；3. 对偶网络在各层生成最优乘子轨迹，并在每层查询原始网络；4. 通过约束学习施加原始下降和对偶上升约束；5. 将训练建模为嵌套优化问题，采用交替更新策略。

Result: 在混合整数二次规划（MIQP）和无线网络功率分配问题上进行了数值评估。CDU框架能够产生接近最优且接近可行的解，并展现出强大的分布外（OOD）泛化能力。

Conclusion: CDU框架成功地将对偶上升算法转化为可学习的加速版本，通过耦合神经网络和约束学习实现了高效约束优化，在多个应用场景中表现出优越性能。

Abstract: In this paper, we develop unrolled neural networks to solve constrained optimization problems, offering accelerated, learnable counterparts to dual ascent (DA) algorithms. Our framework, termed constrained dual unrolling (CDU), comprises two coupled neural networks that jointly approximate the saddle point of the Lagrangian. The primal network emulates an iterative optimizer that finds a stationary point of the Lagrangian for a given dual multiplier, sampled from an unknown distribution. The dual network generates trajectories towards the optimal multipliers across its layers while querying the primal network at each layer. Departing from standard unrolling, we induce DA dynamics by imposing primal-descent and dual-ascent constraints through constrained learning. We formulate training the two networks as a nested optimization problem and propose an alternating procedure that updates the primal and dual networks in turn, mitigating uncertainty in the multiplier distribution required for primal network training. We numerically evaluate the framework on mixed-integer quadratic programs (MIQPs) and power allocation in wireless networks. In both cases, our approach yields near-optimal near-feasible solutions and exhibits strong out-of-distribution (OOD) generalization.

</details>


### [200] [Latent-Space Contrastive Reinforcement Learning for Stable and Efficient LLM Reasoning](https://arxiv.org/abs/2601.17275)
*Lianlei Shan,Han Chen,Yixuan Wang,Zhenjie Liu,Wei Li*

Main category: cs.LG

TL;DR: DLR提出一种潜在空间双向对比强化学习框架，将强化学习从高维离散token空间转移到连续潜在流形，解决传统RL在LLMs中面临的样本效率低、梯度方差大和灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂多步推理任务中更多是"统计拟合"而非系统逻辑推导。传统RL虽然引入"先思考后说话"范式，但在高维离散token空间中面临三大挑战：样本效率低的rollout、高梯度估计方差和灾难性遗忘风险

Method: 提出DeepLatent Reasoning (DLR)框架：1) 使用轻量级辅助模型在潜在空间采样K个推理链编码；2) 通过基于正确性和格式的双重奖励机制筛选高价值潜在轨迹；3) 仅将筛选后的轨迹输入冻结的主模型进行单次解码；4) 设计对比学习目标实现潜在空间中的定向探索，保持推理多样性

Result: 在可比GPU计算预算下，DLR实现更稳定的训练收敛，支持更长时域的推理链，促进推理能力的可持续积累，为LLMs提供了可靠且可扩展的强化学习路径

Conclusion: DLR通过将试错成本从昂贵的token级全序列生成转移到连续潜在流形，从根本上解决了LLMs强化学习中的结构性瓶颈，同时通过冻结主模型参数消除了灾难性遗忘，提供了一种可行的LLMs可靠强化学习方法

Abstract: While Large Language Models (LLMs) demonstrate exceptional performance in surface-level text generation, their nature in handling complex multi-step reasoning tasks often remains one of ``statistical fitting'' rather than systematic logical deduction. Traditional Reinforcement Learning (RL) attempts to mitigate this by introducing a ``think-before-speak'' paradigm. However, applying RL directly in high-dimensional, discrete token spaces faces three inherent challenges: sample-inefficient rollouts, high gradient estimation variance, and the risk of catastrophic forgetting. To fundamentally address these structural bottlenecks, we propose \textbf{DeepLatent Reasoning (DLR)}, a latent-space bidirectional contrastive reinforcement learning framework. This framework shifts the trial-and-error cost from expensive token-level full sequence generation to the continuous latent manifold. Specifically, we introduce a lightweight assistant model to efficiently sample $K$ reasoning chain encodings within the latent space. These encodings are filtered via a dual reward mechanism based on correctness and formatting; only high-value latent trajectories are fed into a \textbf{frozen main model} for single-pass decoding. To maximize reasoning diversity while maintaining coherence, we design a contrastive learning objective to enable directed exploration within the latent space. Since the main model parameters remain frozen during optimization, this method mathematically eliminates catastrophic forgetting. Experiments demonstrate that under comparable GPU computational budgets, DLR achieves more stable training convergence, supports longer-horizon reasoning chains, and facilitates the sustainable accumulation of reasoning capabilities, providing a viable path toward reliable and scalable reinforcement learning for LLMs.

</details>


### [201] [Tabular Foundation Models are Strong Graph Anomaly Detectors](https://arxiv.org/abs/2601.17301)
*Yunhui Liu,Tieke He,Yongchao Liu,Can Yi,Hong Jin,Chuntao Hong*

Main category: cs.LG

TL;DR: TFM4GAD框架将表格基础模型适配于图异常检测，通过"扁平化"图结构并构建增强特征表，实现了跨域、少标签的通用图异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法采用"一个模型一个数据集"范式，导致计算成本高、数据需求大、泛化能力差，需要一种能够跨域检测异常的通用基础模型。

Method: 将图"扁平化"为增强特征表，包含原始节点特征、拉普拉斯嵌入、局部和全局结构特征、异常敏感邻域聚合等，然后使用表格基础模型进行全上下文学习。

Result: TFM4GAD在多个数据集和不同表格基础模型骨干上显著优于专门训练的图异常检测模型，实现了更好的性能。

Conclusion: 该工作提供了利用表格基础模型作为强大通用图异常检测器的新视角和实践范式，解决了图异常检测中的跨域泛化和少标签问题。

Abstract: Graph anomaly detection (GAD), which aims to identify abnormal nodes that deviate from the majority, has become increasingly important in high-stakes Web domains. However, existing GAD methods follow a "one model per dataset" paradigm, leading to high computational costs, substantial data demands, and poor generalization when transferred to new datasets. This calls for a foundation model that enables a "one-for-all" GAD solution capable of detecting anomalies across diverse graphs without retraining. Yet, achieving this is challenging due to the large structural and feature heterogeneity across domains. In this paper, we propose TFM4GAD, a simple yet effective framework that adapts tabular foundation models (TFMs) for graph anomaly detection. Our key insight is that the core challenges of foundation GAD, handling heterogeneous features, generalizing across domains, and operating with scarce labels, are the exact problems that modern TFMs are designed to solve via synthetic pre-training and powerful in-context learning. The primary challenge thus becomes structural: TFMs are agnostic to graph topology. TFM4GAD bridges this gap by "flattening" the graph, constructing an augmented feature table that enriches raw node features with Laplacian embeddings, local and global structural characteristics, and anomaly-sensitive neighborhood aggregations. This augmented table is processed by a TFM in a fully in-context regime. Extensive experiments on multiple datasets with various TFM backbones reveal that TFM4GAD surprisingly achieves significant performance gains over specialized GAD models trained from scratch. Our work offers a new perspective and a practical paradigm for leveraging TFMs as powerful, generalist graph anomaly detectors.

</details>


### [202] [Decentralized Multi-Agent Swarms for Autonomous Grid Security in Industrial IoT: A Consensus-based Approach](https://arxiv.org/abs/2601.17303)
*Samaresh Kumar Singh,Joyjit Roy*

Main category: cs.LG

TL;DR: 本文提出了一种去中心化多智能体群架构，用于工业物联网安全监控，通过边缘AI代理协作检测威胁，实现亚毫秒级响应，相比集中式方案显著提升检测准确率并降低带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 随着工业物联网设备数量激增，集中式安全监控架构存在严重延迟问题，攻击者可利用此漏洞危害整个制造生态系统。传统静态防火墙方法无法满足大规模分布式环境的安全需求。

Method: 提出去中心化多智能体群架构，在每个边缘网关部署自主AI代理，形成分布式数字"免疫系统"。代理通过轻量级P2P协议协作检测异常行为，无需云端数据传输。引入共识式威胁验证流程，代理对识别威胁进行投票表决，实现即时隔离受感染节点。

Result: 在模拟2000个IIoT设备的创新工厂测试环境中，DMAS架构实现亚毫秒级响应时间（平均0.85ms），高负载下恶意活动检测准确率达97.3%，零日攻击检测准确率87%。相比云解决方案减少89%网络带宽使用，有效预防工业控制系统实时级联故障。

Conclusion: DMAS架构为大规模工业物联网提供了高效、低延迟的安全监控解决方案，通过分布式AI代理协作实现快速威胁检测和响应，显著优于传统集中式和边缘计算方法，为工业物联网安全防护提供了新范式。

Abstract: As Industrial Internet of Things (IIoT) environments expand to include tens of thousands of connected devices. The centralization of security monitoring architectures creates serious latency issues that savvy attackers can exploit to compromise an entire manufacturing ecosystem. This paper outlines a new, decentralized multi-agent swarm (DMAS) architecture that includes autonomous artificial intelligence (AI) agents at each edge gateway, functioning as a distributed digital "immune system" for IIoT networks. Instead of using a traditional static firewall approach, the DMAS agents communicate via a lightweight peer-to-peer protocol to cooperatively detect anomalous behavior across the IIoT network without sending data to a cloud infrastructure. The authors also outline a consensus-based threat validation (CVT) process in which agents vote on the threat level of an identified threat, enabling instant quarantine of a compromised node or nodes. The authors conducted experiments on a testbed that simulated an innovative factory environment with 2000 IIoT devices and found that the DMAS demonstrated sub-millisecond response times (average of 0.85ms), 97.3% accuracy in detecting malicious activity under high load, and 87% accuracy in detecting zero-day attacks. All significantly higher than baseline values for both centralized and edge computing. Additionally, the proposed architecture can prevent real-time cascading failures in industrial control systems and reduce network bandwidth use by 89% compared to cloud-based solutions.

</details>


### [203] [Weighted Graph Clustering via Scale Contraction and Graph Structure Learning](https://arxiv.org/abs/2601.17307)
*Haobing Liu,Yinuo Zhang,Tingting Wang,Ruobing Jiang,Yanwei Yu*

Main category: cs.LG

TL;DR: 提出了一种收缩边权感知图聚类网络，通过图收缩模块减少图规模并保留重要节点，利用边权感知注意力网络识别并削弱噪声连接，从而提升聚类效果。


<details>
  <summary>Details</summary>
Motivation: 现有图聚类方法未能充分利用边权信息，而引入边权会面临两个关键挑战：1) 显著增加存储空间和训练时间；2) 边权信息可能包含噪声影响聚类结果。目前很少有研究能同时优化聚类和边权。

Method: 提出收缩边权感知图聚类网络：1) 设计面向聚类的图收缩模块，在减少图规模的同时保留对聚类任务重要的节点；2) 设计边权感知注意力网络，识别并削弱噪声连接。

Result: 在三个真实世界加权图数据集上进行实验，模型优于最佳基线方法，表现出优越性能。实验还表明提出的图收缩模块能显著减少训练时间和存储空间。

Conclusion: 通过联合优化聚类和边权，提出的方法能有效缓解噪声边对聚类任务的负面影响，同时提高计算效率，在加权图聚类任务中取得了更好的效果。

Abstract: Graph clustering aims to partition nodes into distinct clusters based on their similarity, thereby revealing relationships among nodes. Nevertheless, most existing methods do not fully utilize these edge weights. Leveraging edge weights in graph clustering tasks faces two critical challenges. (1) The introduction of edge weights may significantly increase storage space and training time, making it essential to reduce the graph scale while preserving nodes that are beneficial for the clustering task. (2) Edge weight information may inherently contain noise that negatively impacts clustering results. However, few studies can jointly optimize clustering and edge weights, which is crucial for mitigating the negative impact of noisy edges on clustering task. To address these challenges, we propose a contractile edge-weight-aware graph clustering network. Specifically, a cluster-oriented graph contraction module is designed to reduce the graph scale while preserving important nodes. An edge-weight-aware attention network is designed to identify and weaken noisy connections. In this way, we can more easily identify and mitigate the impact of noisy edges during the clustering process, thus enhancing clustering effectiveness. We conducted extensive experiments on three real-world weighted graph datasets. In particular, our model outperforms the best baseline, demonstrating its superior performance. Furthermore, experiments also show that the proposed graph contraction module can significantly reduce training time and storage space.

</details>


### [204] [PAR: Plausibility-aware Amortized Recourse Generation](https://arxiv.org/abs/2601.17309)
*Anagha Sabu,Vidhya S,Narayanan C Krishnan*

Main category: cs.LG

TL;DR: PAR是一种基于摊销近似推理的算法补偿方法，通过约束最大后验推断生成高似然、现实可行的反事实解释，在效率和效果上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的算法补偿方法在生成反事实解释时，往往难以同时保证高似然性、现实可行性和效率。本文旨在提出一种能够高效生成既符合约束条件又具有高概率的反事实解释的方法。

Method: 将补偿问题形式化为约束最大后验推断问题，提出PAR摊销近似推理程序。使用可处理概率模型直接估计补偿似然，支持精确似然评估和高效梯度传播。训练生成器时最大化接受类分布下的似然，最小化拒绝类分布下的似然，并加入其他约束损失。还包含基于邻域的调节机制来定制化生成补偿。

Result: 在广泛使用的算法补偿数据集上验证，PAR能够高效生成有效、与事实相似、稀疏且高度合理的补偿，性能优于现有最先进方法。

Conclusion: PAR通过约束最大后验推断框架和摊销近似推理，成功解决了算法补偿中平衡似然性、现实可行性和效率的挑战，为生成高质量反事实解释提供了有效解决方案。

Abstract: Algorithmic recourse aims to recommend actionable changes to a factual's attributes that flip an unfavorable model decision while remaining realistic and feasible. We formulate recourse as a Constrained Maximum A-Posteriori (MAP) inference problem under the accepted-class data distribution seeking counterfactuals with high likelihood while respecting other recourse constraints. We present PAR, an amortized approximate inference procedure that generates highly likely recourses efficiently. Recourse likelihood is estimated directly using tractable probabilistic models that admit exact likelihood evaluation and efficient gradient propagation that is useful during training. The recourse generator is trained with the objective of maximizing the likelihood under the accepted-class distribution while minimizing the likelihood under the denied-class distribution and other losses that encode recourse constraints. Furthermore, PAR includes a neighborhood-based conditioning mechanism to promote recourse generation that is customized to a factual. We validate PAR on widely used algorithmic recourse datasets and demonstrate its efficiency in generating recourses that are valid, similar to the factual, sparse, and highly plausible, yielding superior performance over existing state-of-the-art approaches.

</details>


### [205] [Conformal Feedback Alignment: Quantifying Answer-Level Reliability for Robust LLM Alignment](https://arxiv.org/abs/2601.17329)
*Tiejin Chen,Xiaoou Liu,Vishnu Nandam,Kuan-Ru Liou,Hua Wei*

Main category: cs.LG

TL;DR: 本文提出Conformal Feedback Alignment (CFA)，利用Conformal Prediction的统计保证来量化答案可靠性，为偏好对齐提供更稳健的权重方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的对齐方法（如RLHF）面临标签噪声和不一致问题。当前的不确定性感知方法仅关注偏好权重，而忽略了被比较答案本身的可靠性这一更基本因素。

Method: 提出Conformal Feedback Alignment (CFA)框架，利用Conformal Prediction构建具有可控覆盖率的预测集合来量化答案级可靠性，然后将这些可靠性聚合为DPO和PPO风格训练的权重。

Result: 在不同数据集上的实验表明，CFA提高了对齐的鲁棒性和数据效率，证明对答案侧不确定性的建模能够补充偏好级权重，实现更稳健、数据高效的对齐。

Conclusion: CFA通过量化答案可靠性并基于此调整偏好权重，解决了偏好对齐中的噪声问题，为强化学习从人类反馈提供了更可靠的框架。

Abstract: Preference-based alignment like Reinforcement Learning from Human Feedback (RLHF) learns from pairwise preferences, yet the labels are often noisy and inconsistent. Existing uncertainty-aware approaches weight preferences, but ignore a more fundamental factor: the reliability of the \emph{answers} being compared. To address the problem, we propose Conformal Feedback Alignment (CFA), a framework that grounds preference weighting in the statistical guarantees of Conformal Prediction (CP). CFA quantifies answer-level reliability by constructing conformal prediction sets with controllable coverage and aggregates these reliabilities into principled weights for both DPO- and PPO-style training. Experiments across different datasets show that CFA improves alignment robustness and data efficiency, highlighting that modeling \emph{answer-side} uncertainty complements preference-level weighting and yields more robust, data-efficient alignment. Codes are provided here.

</details>


### [206] [Thermodynamically Optimal Regularization under Information-Geometric Constraints](https://arxiv.org/abs/2601.17330)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的理论框架，将热力学最优性、信息几何和正则化联系起来，证明在特定假设下，Fisher-Rao度量是信念空间的唯一可容许几何，热力学最优正则化对应于最小化到参考状态的Fisher-Rao距离平方。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习依赖于一系列经验上成功但理论上异质的正则化技术（如权重衰减、dropout、指数移动平均），同时训练大型模型的能量成本急剧增加，这引发了对学习算法是否接近任何基本效率界限的疑问。

Method: 提出了一个统一的理论框架，基于三个明确假设：(A1)最优性需要内在的、参数化不变的信息度量；(A2)信念状态由已知约束下的最大熵分布建模；(A3)最优过程是准静态的。在此框架下证明了条件最优性定理，并推导了高斯和圆形信念模型的诱导几何。

Result: 证明了Fisher-Rao度量是信念空间的唯一可容许几何，热力学最优正则化对应于最小化到参考状态的Fisher-Rao距离平方。推导出高斯信念模型对应双曲流形，圆形信念模型对应von Mises流形，并表明经典正则化方案在结构上无法保证热力学最优性。

Conclusion: 该工作为机器学习中的正则化提供了原则性的几何和热力学基础，引入了学习的热力学效率概念，并提出了可实验验证的预测，为理解正则化技术的理论基础和效率界限提供了新视角。

Abstract: Modern machine learning relies on a collection of empirically successful but theoretically heterogeneous regularization techniques, such as weight decay, dropout, and exponential moving averages. At the same time, the rapidly increasing energetic cost of training large models raises the question of whether learning algorithms approach any fundamental efficiency bound. In this work, we propose a unifying theoretical framework connecting thermodynamic optimality, information geometry, and regularization.
  Under three explicit assumptions -- (A1) that optimality requires an intrinsic, parametrization-invariant measure of information, (A2) that belief states are modeled by maximum-entropy distributions under known constraints, and (A3) that optimal processes are quasi-static -- we prove a conditional optimality theorem. Specifically, the Fisher--Rao metric is the unique admissible geometry on belief space, and thermodynamically optimal regularization corresponds to minimizing squared Fisher--Rao distance to a reference state.
  We derive the induced geometries for Gaussian and circular belief models, yielding hyperbolic and von Mises manifolds, respectively, and show that classical regularization schemes are structurally incapable of guaranteeing thermodynamic optimality. We introduce a notion of thermodynamic efficiency of learning and propose experimentally testable predictions. This work provides a principled geometric and thermodynamic foundation for regularization in machine learning.

</details>


### [207] [Power-based Partial Attention: Bridging Linear-Complexity and Full Attention](https://arxiv.org/abs/2601.17334)
*Yufeng Huang*

Main category: cs.LG

TL;DR: 该论文提出了基于幂的部分注意力机制（PPA），其复杂度为O(L^{1+p})，其中0≤p≤1，通过实验发现存在0<p<1使得次二次方复杂度注意力能达到与全注意力相当的性能。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制的复杂度是否必须为二次方O(L²)，是否存在次二次方注意力机制能达到与全注意力相当的性能。

Method: 引入基于幂的部分注意力机制（PPA），其复杂度为O(L^{1+p})，其中p=0对应线性复杂度的滑动窗口注意力，p=1对应全注意力，通过调整p值探索性能变化。

Result: 实验结果显示性能变化呈现S曲线行为：在p值的窄窗口内从滑动窗口注意力过渡到全注意力，当p接近1时性能趋于稳定。存在0<p<1使得O(L^{1+p})注意力足以达到与O(L²)全注意力相似的结果。

Conclusion: 次二次方注意力机制是可行的，并非所有注意力都需要全二次方复杂度，这为开发更高效的Transformer架构提供了理论依据。

Abstract: It is widely accepted from transformer research that "attention is all we need", but the amount of attention required has never been systematically quantified. Is quadratic $O(L^2)$ attention necessary, or is there a sub-quadratic attention mechanism that can achieve comparable performance? To answer this question, we introduce power-based partial attention (PPA), an attention mechanism of order $O(L^{1+p})$, where $0 \leq p \leq 1$, such that $p=0$ corresponds to sliding window attention with linear complexity, and $p=1$ corresponds to full attention. With this attention construction, we can explore how transformer architecture performance varies as a function of the attention scaling behavior controlled by $p$. The overall trend from our experiments shows an S-curve-like behavior where the performance transitions from sliding-window (linear-complexity) attention to full attention over a narrow window of $p$ values, and plateaus as $p$ approaches $1$. In our experiments, we show that there exists $0<p<1$ such that $O(L^{1+p})$ attention is sufficient to achieve similar results as $O(L^2)$ full attention.

</details>


### [208] [Spectral Geometry for Deep Learning: Compression and Hallucination Detection via Random Matrix Theory](https://arxiv.org/abs/2601.17357)
*Davide Ettori*

Main category: cs.LG

TL;DR: 该论文提出了基于谱几何和随机矩阵理论的统一框架，通过分析隐藏激活的谱特征来解决大模型的可靠性问题和计算成本问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型和深度神经网络虽然性能强大，但存在可靠性问题（如幻觉）和计算成本过高的问题。需要一种统一的理论框架来同时解决这两个问题

Method: 提出基于谱几何和随机矩阵理论的统一框架，分析隐藏激活的谱结构。包含两个主要贡献：1) EigenTrack：利用谱特征及其时间动态实时检测语言和视觉语言模型中的幻觉和分布外行为；2) RMT-KD：通过识别信息性谱成分并应用迭代知识蒸馏来进行模型压缩

Result: 谱统计为大规模神经网络的监控不确定性和指导压缩提供了可解释且鲁棒的信号，能够同时解决可靠性问题和计算效率问题

Conclusion: 谱几何和随机矩阵理论为分析和优化大模型提供了有效的理论框架，既能监控模型可靠性，又能实现高效压缩，在保持准确性的同时提升模型实用性

Abstract: Large language models and deep neural networks achieve strong performance but suffer from reliability issues and high computational cost. This thesis proposes a unified framework based on spectral geometry and random matrix theory to address both problems by analyzing the eigenvalue structure of hidden activations. The first contribution, EigenTrack, is a real-time method for detecting hallucinations and out-of-distribution behavior in language and vision-language models using spectral features and their temporal dynamics. The second contribution, RMT-KD, is a principled compression method that identifies informative spectral components and applies iterative knowledge distillation to produce compact and efficient models while preserving accuracy. Together, these results show that spectral statistics provide interpretable and robust signals for monitoring uncertainty and guiding compression in large-scale neural networks.

</details>


### [209] [Robust Privacy: Inference-Time Privacy through Certified Robustness](https://arxiv.org/abs/2601.17360)
*Jiankai Jin,Xiangzheng Zhang,Zhao Liu,Deyue Zhang,Quanchen Zou*

Main category: cs.LG

TL;DR: 论文提出鲁棒隐私（RP）概念，将认证鲁棒性思想应用于隐私保护，通过保证模型在输入邻域内的预测不变性来防止推理时隐私泄露，并开发属性隐私增强（APE）方法将输入级不变性转化为属性级隐私效果。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在推理时可能通过个性化输出泄露敏感输入属性，现有隐私保护方法在推理时保护不足，需要一种新的推理时隐私概念来防止模型反演攻击等隐私威胁。

Method: 提出鲁棒隐私（RP）概念：如果模型在输入x的半径R邻域内（如ℓ2范数下）的预测具有可证明的不变性，则x享有R-鲁棒隐私。开发属性隐私增强（APE）方法，将输入级不变性转化为属性级隐私效果。在受控推荐任务中验证方法有效性。

Result: 在敏感属性决定推荐结果的受控任务中，RP扩展了与正向推荐兼容的敏感属性值集合，相应扩大了推理区间。实验显示RP能显著缓解模型反演攻击（MIA）：即使在小噪声水平（σ=0.1）下，攻击成功率从73%降至4%，模型性能部分下降；无性能下降时攻击成功率也能降至44%。

Conclusion: 鲁棒隐私（RP）为推理时隐私保护提供了新框架，通过输入邻域预测不变性实现隐私保护，能有效缓解模型反演攻击，在隐私保护和模型性能间取得良好平衡。

Abstract: Machine learning systems can produce personalized outputs that allow an adversary to infer sensitive input attributes at inference time. We introduce Robust Privacy (RP), an inference-time privacy notion inspired by certified robustness: if a model's prediction is provably invariant within a radius-$R$ neighborhood around an input $x$ (e.g., under the $\ell_2$ norm), then $x$ enjoys $R$-Robust Privacy, i.e., observing the prediction cannot distinguish $x$ from any input within distance $R$ of $x$. We further develop Attribute Privacy Enhancement (APE) to translate input-level invariance into an attribute-level privacy effect. In a controlled recommendation task where the decision depends primarily on a sensitive attribute, we show that RP expands the set of sensitive-attribute values compatible with a positive recommendation, expanding the inference interval accordingly. Finally, we empirically demonstrate that RP also mitigates model inversion attacks (MIAs) by masking fine-grained input-output dependence. Even at small noise levels ($σ=0.1$), RP reduces the attack success rate (ASR) from 73% to 4% with partial model performance degradation. RP can also partially mitigate MIAs (e.g., ASR drops to 44%) with no model performance degradation.

</details>


### [210] [Diversified Scaling Inference in Time Series Foundation Models](https://arxiv.org/abs/2601.17376)
*Ruijin Hua,Zichuan Liu,Kun Zhang,Yiyuan Yang*

Main category: cs.LG

TL;DR: 该研究系统探索了时间序列基础模型的推理时计算潜力，通过多样化采样提升性能而不更新模型参数。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型主要依赖大规模预训练，但推理时的计算潜力尚未充分挖掘。标准采样方法由于对解空间探索不足，往往无法遵循缩放定律，限制了模型性能。

Method: 1. 首先分析TSFMs在标准采样下的行为特性；2. 通过定制的时间序列扰动进行多样化推理缩放，扩展生成分布的支撑集；3. 理论分析多样性-保真度权衡，推导多样化采样优于标准采样的关键样本阈值；4. 提出RobustMSE指标量化固定预算下TSFM的性能提升空间。

Result: 在各种TSFMs和数据集上的广泛实验表明，适当的多样化推理缩放能带来显著的性能提升，且无需参数更新。推理设计成为TSFM优化中一个关键且计算高效的维度。

Conclusion: 研究阐明了这些因素的相互作用，使得在不重新训练TSFMs的情况下，通过多样化的并行大规模推理能够实现可靠的性能提升。推理时优化为时间序列基础模型提供了一个有效的新优化维度。

Abstract: The advancement of Time Series Foundation Models (TSFMs) has been driven primarily by large-scale pre-training, but inference-time compute potential remains largely untapped. This work systematically investigates two questions: how do TSFMs behave under standard sampling-based inference scaling, and can controlled sampling diversity enhance performance? We first examine the properties of TSFMs under standard sampling often fail to adhere to scaling laws due to insufficient exploration of the solution space. Building on this, we then delve into diversified inference scaling via tailored time series perturbations to expand the generative distribution's support. We theoretically analyze the diversity-fidelity trade-off and derive a critical sample threshold for diversified sampling to outperform standard sampling. Extensive experiments across various TSFMs and datasets show proper diversified inference scaling yields substantial performance gains without parameter updates, establishing inference design as a critical, compute-efficient dimension of TSFM optimization. As an application, we propose RobustMSE, a rigorous metric to quantify the headroom performance of TSFM under a fixed budget. Overall, our findings clarify these factor interactions, enabling reliable performance via diverse large-scale inference time series in parallel environments without re-training TSFMs.

</details>


### [211] [GO-OSC and VASH: Geometry-Aware Representation Learning for Early Degradation Detection in Oscillatory Systems](https://arxiv.org/abs/2601.17396)
*Vashista Nobaub*

Main category: cs.LG

TL;DR: GO-OSC：一种几何感知的振荡时间序列表示学习框架，通过规范化的潜在参数化实现早期退化检测，比传统能量基方法更敏感。


<details>
  <summary>Details</summary>
Motivation: 振荡系统早期退化通常表现为动态的几何失真（如相位抖动、频率漂移），在信号能量变化可检测之前就已发生。传统能量基诊断和无约束学习表示对此不敏感，导致检测延迟或不稳定。

Method: 提出GO-OSC框架：1）强制规范化和可识别的潜在参数化，支持短时无标签窗口的稳定比较和聚合；2）定义不变线性几何探针，针对退化相关的潜在空间方向；3）理论分析证明几何探针的检测敏感性。

Result: 理论证明：在早期仅相位退化情况下，能量基统计量的检测能力为零，而几何探针具有严格正敏感性。合成基准和真实振动数据集验证了该方法能实现更早检测、更高数据效率和运行条件变化的鲁棒性。

Conclusion: GO-OSC通过几何感知表示学习解决了振荡系统早期退化检测的关键挑战，提供了一种比传统能量基方法更敏感、更稳定的检测框架，在工业预测性维护等领域具有应用价值。

Abstract: Early-stage degradation in oscillatory systems often manifests as geometric distortions of the dynamics, such as phase jitter, frequency drift, or loss of coherence, long before changes in signal energy are detectable. In this regime, classical energy-based diagnostics and unconstrained learned representations are structurally insensitive, leading to delayed or unstable detection. We introduce GO-OSC, a geometry-aware representation learning framework for oscillatory time series that enforces a canonical and identifiable latent parameterization, enabling stable comparison and aggregation across short, unlabeled windows. Building on this representation, we define a family of invariant linear geometric probes that target degradation-relevant directions in latent space. We provide theoretical results showing that under early phase-only degradation, energy-based statistics have zero first-order detection power, whereas geometric probes achieve strictly positive sensitivity. Our analysis characterizes when and why linear probing fails under non-identifiable representations and shows how canonicalization restores statistical detectability. Experiments on synthetic benchmarks and real vibration datasets validate the theory, demonstrating earlier detection, improved data efficiency, and robustness to operating condition changes.

</details>


### [212] [Efficient Dilated Squeeze and Excitation Neural Operator for Differential Equations](https://arxiv.org/abs/2601.17407)
*Prajwal Chauhan,Salah Eddine Choutri,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: 提出了D-SENO（扩张挤压-激励神经算子），一种轻量级的算子学习框架，用于高效求解各种偏微分方程，比传统基于Transformer的模型和神经算子训练速度快约20倍，同时保持或超越其精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的模型和现有神经算子在求解物理驱动的偏微分方程时参数过多，导致训练成本高昂且部署缓慢，需要开发更轻量高效的替代模型。

Method: 结合扩张卷积块和挤压-激励模块，扩张卷积捕获宽感受野和长程物理依赖关系，挤压-激励模块通过通道注意力自适应重新校准特征通道，强调动态相关尺度。

Result: 在多个PDE基准测试中（包括翼型势流、多孔介质达西流、管道泊肃叶流和不可压缩纳维-斯托克斯涡流场），D-SENO训练速度比标准Transformer模型和神经算子快约20倍，同时精度达到或超越这些模型。

Conclusion: D-SENO是一种高效轻量的PDE求解框架，通过扩张卷积和挤压-激励模块的协同设计，在保持高精度的同时显著提升了训练和部署效率，为物理驱动PDE的高效求解提供了实用方案。

Abstract: Fast and accurate surrogates for physics-driven partial differential equations (PDEs) are essential in fields such as aerodynamics, porous media design, and flow control. However, many transformer-based models and existing neural operators remain parameter-heavy, resulting in costly training and sluggish deployment. We propose D-SENO (Dilated Squeeze-Excitation Neural Operator), a lightweight operator learning framework for efficiently solving a wide range of PDEs, including airfoil potential flow, Darcy flow in porous media, pipe Poiseuille flow, and incompressible Navier Stokes vortical fields. D-SENO combines dilated convolution (DC) blocks with squeeze-and-excitation (SE) modules to jointly capture wide receptive fields and dynamics alongside channel-wise attention, enabling both accurate and efficient PDE inference. Carefully chosen dilation rates allow the receptive field to focus on critical regions, effectively modeling long-range physical dependencies. Meanwhile, the SE modules adaptively recalibrate feature channels to emphasize dynamically relevant scales. Our model achieves training speed of up to approximately $20\times$ faster than standard transformer-based models and neural operators, while also surpassing (or matching) them in accuracy across multiple PDE benchmarks. Ablation studies show that removing the SE modules leads to a slight drop in performance.

</details>


### [213] [Active Hypothesis Testing for Correlated Combinatorial Anomaly Detection](https://arxiv.org/abs/2601.17430)
*Zichuan Yang,Yiming Xing*

Main category: cs.LG

TL;DR: 提出ECC-AHT算法，解决相关噪声下异常流检测问题，通过最大化Chernoff信息实现主动噪声消除，达到最优样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 在信息物理系统监控和安全中，需要在相关噪声环境下识别异常流。现有组合多臂赌博机和假设检验方法通常假设观测独立，未能利用相关性进行高效测量设计。

Method: 提出ECC-AHT自适应算法，选择连续、受限的测量来最大化竞争假设间的Chernoff信息，通过差分感知实现主动噪声消除。

Result: ECC-AHT实现了最优样本复杂度保证，在合成和真实世界相关环境中显著优于现有基线方法。

Conclusion: 该算法有效解决了相关噪声下的异常流检测问题，为信息物理系统监控提供了高效解决方案。

Abstract: We study the problem of identifying an anomalous subset of streams under correlated noise, motivated by monitoring and security in cyber-physical systems. This problem can be viewed as a form of combinatorial pure exploration, where each stream plays the role of an arm and measurements must be allocated sequentially under uncertainty. Existing combinatorial bandit and hypothesis testing methods typically assume independent observations and fail to exploit correlation for efficient measurement design. We propose ECC-AHT, an adaptive algorithm that selects continuous, constrained measurements to maximize Chernoff information between competing hypotheses, enabling active noise cancellation through differential sensing. ECC-AHT achieves optimal sample complexity guarantees and significantly outperforms state-of-the-art baselines in both synthetic and real-world correlated environments. The code is available on https://github.com/VincentdeCristo/ECC-AHT

</details>


### [214] [Data-driven Clustering and Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2601.17441)
*Ondrej Bohdal,Taha Ceritli,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.LG

TL;DR: 论文提出D2C方法，通过少量任务样本对适配器进行聚类，合并同簇适配器创建多任务适配器，以在存储受限设备上提升性能。


<details>
  <summary>Details</summary>
Motivation: 移动设备存储所有任务适配器不现实，但可存储有限数量。现有研究未探索如何选择代表性适配器以跨任务泛化，这成为关键挑战。

Method: 提出D2C适配器聚类方法，利用少量任务样本（如每任务10个），通过迭代优化精炼簇分配，合并同簇适配器创建多任务适配器。

Result: 实验结果表明，该方法在给定存储预算下有效提升性能。

Conclusion: D2C方法解决了设备端大语言模型适配器选择与合并问题，为资源受限设备提供了有效的多任务适配方案。

Abstract: On-device large language models commonly employ task-specific adapters (e.g., LoRAs) to deliver strong performance on downstream tasks. While storing all available adapters is impractical due to memory constraints, mobile devices typically have sufficient capacity to store a limited number of these parameters. This raises a critical challenge: how to select representative adapters that generalize well across multiple tasks - a problem that remains unexplored in existing literature. We propose a novel method D2C for adapter clustering that leverages minimal task-specific examples (e.g., 10 per task) and employs an iterative optimization process to refine cluster assignments. The adapters within each cluster are merged, creating multi-task adapters deployable on resource-constrained devices. Experimental results demonstrate that our method effectively boosts performance for considered storage budgets.

</details>


### [215] [DREAM: Dual-Standard Semantic Homogeneity with Dynamic Optimization for Graph Learning with Label Noise](https://arxiv.org/abs/2601.17449)
*Yusheng Zhao,Jiaye Xie,Qixin Zhang,Weizhi Zhang,Xiao Luo,Zhiping Xiao,Philip S. Yu,Ming Zhang*

Main category: cs.LG

TL;DR: 本文提出DREAM方法，用于处理图神经网络中标签噪声问题，通过双重标准语义同质性动态优化来提高图学习可靠性


<details>
  <summary>Details</summary>
Motivation: 现实世界图数据中的标签往往存在噪声，现有方法难以区分可靠与不可靠节点，且忽视图拓扑中的关系信息，这限制了图神经网络在实际应用中的可靠性

Method: 提出DREAM方法：1）设计关系感知的动态优化框架，迭代评估图中每个标记节点的可靠性；2）采用双重标准选择策略，基于节点邻近性和图拓扑选择锚节点集；3）计算目标节点与锚节点之间的语义同质性，指导优化过程

Result: 在六个不同领域的图数据集上，针对三种类型的图标签噪声进行广泛实验，结果表明DREAM方法相比竞争基线具有显著优势

Conclusion: DREAM方法通过结合关系信息和动态优化机制，有效解决了图神经网络中的标签噪声问题，提高了图学习的可靠性和鲁棒性，并提供了理论分析支持

Abstract: Graph neural networks (GNNs) have been widely used in various graph machine learning scenarios. Existing literature primarily assumes well-annotated training graphs, while the reliability of labels is not guaranteed in real-world scenarios. Recently, efforts have been made to address the problem of graph learning with label noise. However, existing methods often (i) struggle to distinguish between reliable and unreliable nodes, and (ii) overlook the relational information embedded in the graph topology. To tackle this problem, this paper proposes a novel method, Dual-Standard Semantic Homogeneity with Dynamic Optimization (DREAM), for reliable, relation-informed optimization on graphs with label noise. Specifically, we design a relation-informed dynamic optimization framework that iteratively reevaluates the reliability of each labeled node in the graph during the optimization process according to the relation of the target node and other nodes. To measure this relation comprehensively, we propose a dual-standard selection strategy that selects a set of anchor nodes based on both node proximity and graph topology. Subsequently, we compute the semantic homogeneity between the target node and the anchor nodes, which serves as guidance for optimization. We also provide a rigorous theoretical analysis to justify the design of DREAM. Extensive experiments are performed on six graph datasets across various domains under three types of graph label noise against competing baselines, and the results demonstrate the effectiveness of the proposed DREAM.

</details>


### [216] [Harnessing Reasoning Trajectories for Hallucination Detection via Answer-agreement Representation Shaping](https://arxiv.org/abs/2601.17467)
*Jianxiong Zhang,Bing Guo,Yuming Jiang,Haobo Wang,Bo An,Xuefeng Du*

Main category: cs.LG

TL;DR: 本文提出ARS方法，通过生成反事实答案并学习答案一致性表征来改进大型推理模型的幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型经常生成看似连贯的推理轨迹却得出错误答案，使幻觉检测困难。现有方法直接使用轨迹文本或隐藏状态存在局限性，容易过拟合到表面模式而非答案有效性。

Method: 引入答案一致性表征塑造（ARS），通过潜在干预生成反事实答案，扰动轨迹边界嵌入，根据答案是否一致标记扰动，学习将答案一致的表示聚集、答案不一致的表示分离的表征。

Result: 实验表明ARS持续改进检测性能，显著优于强基线方法，且无需人工标注训练。

Conclusion: ARS通过编码答案稳定性学习检测友好的表征，能有效暴露潜在不稳定性，提高大型推理模型的幻觉检测能力。

Abstract: Large reasoning models (LRMs) often generate long, seemingly coherent reasoning traces yet still produce incorrect answers, making hallucination detection challenging. Although trajectories contain useful signals, directly using trace text or vanilla hidden states for detection is brittle: traces vary in form and detectors can overfit to superficial patterns rather than answer validity. We introduce Answer-agreement Representation Shaping (ARS), which learns detection-friendly trace-conditioned representations by explicitly encoding answer stability. ARS generates counterfactual answers through small latent interventions, specifically, perturbing the trace-boundary embedding, and labels each perturbation by whether the resulting answer agrees with the original. It then learns representations that bring answer-agreeing states together and separate answer-disagreeing ones, exposing latent instability indicative of hallucination risk. The shaped embeddings are plug-and-play with existing embedding-based detectors and require no human annotations during training. Experiments demonstrate that ARS consistently improves detection and achieves substantial gains over strong baselines.

</details>


### [217] [Identifying and Correcting Label Noise for Robust GNNs via Influence Contradiction](https://arxiv.org/abs/2601.17469)
*Wei Ju,Wei Zhang,Siyu Yi,Zhengyang Mao,Yifan Wang,Jingyang Yuan,Zhiping Xiao,Ziyue Qiao,Ming Zhang*

Main category: cs.LG

TL;DR: ICGNN：一种利用图结构信息处理图中噪声标签的新方法，通过影响矛盾分数检测噪声标签，结合高斯混合模型进行精确检测，并采用软策略修正噪声标签，最后通过伪标签增强监督信号。


<details>
  <summary>Details</summary>
Motivation: 现实场景中图数据常存在标签噪声（如标注错误或不一致），这会严重影响图神经网络的性能。现有方法在处理图结构数据中的噪声标签方面存在不足，需要一种能够利用图结构信息来有效缓解噪声标签影响的方法。

Method: 1. 设计新颖的噪声指示器：基于图扩散矩阵计算影响矛盾分数（ICS），量化具有干净标签节点的可信度，ICS值越高的节点越可能被检测为噪声标签。
2. 利用高斯混合模型精确检测节点标签是否为噪声。
3. 开发软策略：结合图中相邻节点的预测来修正检测到的噪声标签。
4. 引入伪标签：为大量未标记节点生成伪标签，提供辅助监督信号指导模型优化。

Result: 在基准数据集上的实验表明，该方法具有优越性，能够有效处理图数据中的噪声标签问题。

Conclusion: ICGNN通过利用图结构信息有效缓解了噪声标签对图神经网络的影响，提出的影响矛盾分数检测机制、高斯混合模型噪声检测、软策略修正和伪标签增强等方法，共同构建了一个鲁棒的图神经网络学习框架。

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in learning from graph-structured data with various applications such as social analysis and bioinformatics. However, the presence of label noise in real scenarios poses a significant challenge in learning robust GNNs, and their effectiveness can be severely impacted when dealing with noisy labels on graphs, often stemming from annotation errors or inconsistencies. To address this, in this paper we propose a novel approach called ICGNN that harnesses the structure information of the graph to effectively alleviate the challenges posed by noisy labels. Specifically, we first design a novel noise indicator that measures the influence contradiction score (ICS) based on the graph diffusion matrix to quantify the credibility of nodes with clean labels, such that nodes with higher ICS values are more likely to be detected as having noisy labels. Then we leverage the Gaussian mixture model to precisely detect whether the label of a node is noisy or not. Additionally, we develop a soft strategy to combine the predictions from neighboring nodes on the graph to correct the detected noisy labels. At last, pseudo-labeling for abundant unlabeled nodes is incorporated to provide auxiliary supervision signals and guide the model optimization. Experiments on benchmark datasets show the superiority of our proposed approach.

</details>


### [218] [LeanTutor: Towards a Verified AI Mathematical Proof Tutor](https://arxiv.org/abs/2601.17473)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.LG

TL;DR: LeanTutor是一个结合大语言模型和定理证明器的数学证明辅导系统，通过三个模块提供可证明正确的数学辅导，并在PeanoBench数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然能进行自然语言交流但容易出错，定理证明器（如Lean）能保证证明正确性但学习曲线陡峭，需要结合两者优势来开发一个既易于使用又能保证正确性的数学证明辅导系统。

Method: 开发了LeanTutor系统，包含三个核心模块：1)自动形式化/证明检查器，2)下一步生成器，3)自然语言反馈生成器。使用从Natural Numbers Game衍生的PeanoBench数据集（包含371个Peano算术证明，有人工编写的自然语言和形式语言版本）进行评估。

Result: 提出了一个概念验证系统，结合了LLMs和定理证明器的互补优势，能够提供可证明正确的数学证明辅导。

Conclusion: 通过结合大语言模型和定理证明器，可以创建一个既能进行自然语言交互又能保证证明正确性的数学证明辅导系统，为解决AI教育中正确性与易用性的权衡问题提供了可行方案。

Abstract: This paper considers the development of an AI-based provably-correct mathematical proof tutor. While Large Language Models (LLMs) allow seamless communication in natural language, they are error prone. Theorem provers such as Lean allow for provable-correctness, but these are hard for students to learn. We present a proof-of-concept system (LeanTutor) by combining the complementary strengths of LLMs and theorem provers. LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and (iii) a natural language feedback generator. To evaluate the system, we introduce PeanoBench, a dataset of 371 Peano Arithmetic proofs in human-written natural language and formal language, derived from the Natural Numbers Game.

</details>


### [219] [Unintended Memorization of Sensitive Information in Fine-Tuned Language Models](https://arxiv.org/abs/2601.17480)
*Marton Szep,Jorge Marin Ruiz,Georgios Kaissis,Paulina Seidl,Rüdiger von Eisenhart-Rothe,Florian Hinterwimmer,Daniel Rueckert*

Main category: cs.LG

TL;DR: 本文系统研究LLM微调中的PII泄露风险，发现即使PII仅出现在输入中而非训练目标中，也会被模型无意记忆和泄露，并提出四种隐私保护方法的评估。


<details>
  <summary>Details</summary>
Motivation: 在敏感数据集上微调大语言模型存在无意记忆和泄露个人身份信息的重大风险，可能违反隐私法规并危及个人安全。目前对仅出现在模型输入中而非训练目标中的PII暴露这一关键且未充分探索的漏洞缺乏系统研究。

Method: 使用合成和真实数据集，设计受控提取探针来量化无意PII记忆，研究语言、PII频率、任务类型和模型规模等因素对记忆行为的影响，并基准测试四种隐私保护方法（差分隐私、机器遗忘、正则化和偏好对齐）。

Result: 研究表明后训练方法通常提供更一致的隐私-效用权衡，而差分隐私在特定设置中能显著减少泄露，但可能引入训练不稳定性。语言、PII频率、任务类型和模型规模都会影响记忆行为。

Conclusion: 微调LLM中的记忆问题仍然是一个持久挑战，需要开发强大、可扩展的隐私保护技术，特别是在PII仅出现在输入而非训练目标的情况下，后训练方法显示出较好的隐私-效用平衡。

Abstract: Fine-tuning Large Language Models (LLMs) on sensitive datasets carries a substantial risk of unintended memorization and leakage of Personally Identifiable Information (PII), which can violate privacy regulations and compromise individual safety. In this work, we systematically investigate a critical and underexplored vulnerability: the exposure of PII that appears only in model inputs, not in training targets. Using both synthetic and real-world datasets, we design controlled extraction probes to quantify unintended PII memorization and study how factors such as language, PII frequency, task type, and model size influence memorization behavior. We further benchmark four privacy-preserving approaches including differential privacy, machine unlearning, regularization, and preference alignment, evaluating their trade-offs between privacy and task performance. Our results show that post-training methods generally provide more consistent privacy-utility trade-offs, while differential privacy achieves strong reduction in leakage in specific settings, although it can introduce training instability. These findings highlight the persistent challenge of memorization in fine-tuned LLMs and emphasize the need for robust, scalable privacy-preserving techniques.

</details>


### [220] [Automatic Stability and Recovery for Neural Network Training](https://arxiv.org/abs/2601.17483)
*Barak Or*

Main category: cs.LG

TL;DR: 提出一个运行时稳定性框架，将优化视为受控随机过程，通过创新信号检测和恢复训练不稳定问题


<details>
  <summary>Details</summary>
Motivation: 现代神经网络训练越来越脆弱，罕见但严重的破坏性更新会导致不可逆的发散或性能退化。现有优化方法主要依赖优化器内的预防机制，一旦发生不稳定，检测和恢复能力有限。

Method: 引入监督式运行时稳定性框架，将优化视为受控随机过程。通过从验证探针等次要测量中提取创新信号，实现自动检测和恢复破坏性更新，无需修改底层优化器。

Result: 提供理论运行时安全保证，形式化有界退化和恢复。实现开销最小，兼容内存受限的训练设置。

Conclusion: 该框架为神经网络训练提供了有效的运行时稳定性监控和恢复机制，解决了现有方法在检测和恢复训练不稳定问题上的局限性。

Abstract: Training modern neural networks is increasingly fragile, with rare but severe destabilizing updates often causing irreversible divergence or silent performance degradation. Existing optimization methods primarily rely on preventive mechanisms embedded within the optimizer, offering limited ability to detect and recover from instability once it occurs. We introduce a supervisory runtime stability framework that treats optimization as a controlled stochastic process. By isolating an innovation signal derived from secondary measurements, such as validation probes, the framework enables automatic detection and recovery from destabilizing updates without modifying the underlying optimizer. We provide theoretical runtime safety guarantees that formalize bounded degradation and recovery. Our implementation incurs minimal overhead and is compatible with memory-constrained training settings.

</details>


### [221] [SpatialMath: Spatial Comprehension-Infused Symbolic Reasoning for Mathematical Problem-Solving](https://arxiv.org/abs/2601.17489)
*Ashutosh Bajpai,Akshat Bhandari,Akshay Nambi,Tanmoy Chakraborty*

Main category: cs.LG

TL;DR: SpatialMath框架通过将空间感知融入符号推理链，提升多模态语言模型在视觉密集型数学问题（特别是几何问题）上的表现，在MATHVERSE-PLUS数据集上相比基线有显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前多模态中小型语言模型（MSLMs）在视觉理解和数学推理方面存在局限，特别是在处理视觉信息注入程度不同的几何问题时，难以准确分解复杂视觉输入并将感知与结构化推理相连接。

Method: 提出SpatialMath框架，包含专门感知模块提取视觉图表中的空间基础表示，捕获关键几何结构和空间关系，然后将这些表示系统地注入符号推理链中，实现视觉感知的结构化推理。

Result: 在MATHVERSE-PLUS数据集上显著优于现有多模态基线，在视觉密集型设置下相比有数据增强的监督微调提升高达10个百分点。鲁棒性分析显示增强的空间表示直接提升了推理准确性。

Conclusion: SpatialMath框架通过将空间理解融入符号推理链，有效提升了MSLMs在视觉密集型数学问题上的表现，验证了结构化感知-推理管道在多模态语言模型中的必要性。

Abstract: Multimodal Small-to-Medium sized Language Models (MSLMs) have demonstrated strong capabilities in integrating visual and textual information but still face significant limitations in visual comprehension and mathematical reasoning, particularly in geometric problems with diverse levels of visual infusion. Current models struggle to accurately decompose intricate visual inputs and connect perception with structured reasoning, leading to suboptimal performance. To address these challenges, we propose SpatialMath, a novel Spatial Comprehension-Infused Symbolic Reasoning Framework designed to integrate spatial representations into structured symbolic reasoning chains. SpatialMath employs a specialized perception module to extract spatially-grounded representations from visual diagrams, capturing critical geometric structures and spatial relationships. These representations are then methodically infused into symbolic reasoning chains, facilitating visual comprehension-aware structured reasoning. To this end, we introduce MATHVERSE-PLUS, a novel dataset containing structured visual interpretations and step-by-step reasoning paths for vision-intensive mathematical problems. SpatialMath significantly outperforms strong multimodal baselines, achieving up to 10 percentage points improvement over supervised fine-tuning with data augmentation in vision-intensive settings. Robustness analysis reveals that enhanced spatial representations directly improve reasoning accuracy, reinforcing the need for structured perception-to-reasoning pipelines in MSLMs.

</details>


### [222] [PEARL: Prototype-Enhanced Alignment for Label-Efficient Representation Learning with Deployment-Driven Insights from Digital Governance Communication Systems](https://arxiv.org/abs/2601.17495)
*Ruiyu Zhang,Lin Nie,Wai-Fung Lam,Qihao Wang,Xin Zhao*

Main category: cs.LG

TL;DR: PEARL提出了一种标签高效的嵌入对齐方法，通过有限监督将嵌入与类别原型软对齐，改善局部邻域结构，显著提升相似性检索性能


<details>
  <summary>Details</summary>
Motivation: 实际部署系统中，相似性检索常因嵌入空间的局部邻域结构不匹配而失败。固定预训练嵌入在标签稀缺、领域漂移且重训练不可行的情况下，其几何结构往往与下游任务需求不一致，导致检索性能下降。

Method: PEARL（原型增强对齐表示学习）利用有限监督信息，通过软对齐方式将嵌入向量向类别原型调整，重塑局部邻域几何结构，同时保持维度不变，避免激进投影或表示坍塌。

Result: 在标签稀缺条件下，PEARL显著提升局部邻域质量，相比原始嵌入获得25.7%的性能提升，相比强无监督后处理方法提升超过21.1%，在相似性系统最脆弱的场景中表现优异。

Conclusion: PEARL有效填补了无监督后处理（增益有限）与全监督投影（需要大量标签）之间的空白，为标签稀缺环境下的嵌入对齐提供了实用解决方案，改善了基于相似性的系统的鲁棒性。

Abstract: In many deployed systems, new text inputs are handled by retrieving similar past cases, for example when routing and responding to citizen messages in digital governance platforms. When these systems fail, the problem is often not the language model itself, but that the nearest neighbors in the embedding space correspond to the wrong cases. Modern machine learning systems increasingly rely on fixed, high-dimensional embeddings produced by large pretrained models and sentence encoders. In real-world deployments, labels are scarce, domains shift over time, and retraining the base encoder is expensive or infeasible. As a result, downstream performance depends heavily on embedding geometry. Yet raw embeddings are often poorly aligned with the local neighborhood structure required by nearest-neighbor retrieval, similarity search, and lightweight classifiers that operate directly on embeddings. We propose PEARL (Prototype-Enhanced Aligned Representation Learning), a label-efficient approach that uses limited supervision to softly align embeddings toward class prototypes. The method reshapes local neighborhood geometry while preserving dimensionality and avoiding aggressive projection or collapse. Its aim is to bridge the gap between purely unsupervised post-processing, which offers limited and inconsistent gains, and fully supervised projections that require substantial labeled data. We evaluate PEARL under controlled label regimes ranging from extreme label scarcity to higher-label settings. In the label-scarce condition, PEARL substantially improves local neighborhood quality, yielding 25.7% gains over raw embeddings and more than 21.1% gains relative to strong unsupervised post-processing, precisely in the regime where similarity-based systems are most brittle.

</details>


### [223] [One-Shot Federated Clustering of Non-Independent Completely Distributed Data](https://arxiv.org/abs/2601.17512)
*Yiqun Zhang,Shenghong Cai,Zihua Yang,Sen Feng,Yuzhu Ji,Haijun Zhang*

Main category: cs.LG

TL;DR: 本文提出GOLD框架解决联邦聚类中Non-IID数据的挑战，特别是揭示了Non-ICD现象（客户端可能分割集群），通过全局导向的本地分布学习提升聚类性能。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，无监督的联邦聚类面临Non-IID数据的严重挑战，特别是现有方法难以处理客户端可能分割集群的复杂分布情况，需要新的方法来融合全局知识并指导本地聚类。

Method: 提出GOLD框架：1）精细探索客户端潜在的不完整本地集群分布；2）上传分布摘要到服务器进行全局融合；3）在全局分布指导下进行本地集群增强。

Result: 通过显著性检验、消融研究、可扩展性评估和定性结果等大量实验验证了GOLD的优越性，表明该框架能有效处理Non-ICD问题并提升聚类性能。

Conclusion: GOLD框架通过揭示并解决联邦聚类中的Non-ICD问题，提供了一种有效的全局导向本地分布学习方法，显著提升了在复杂Non-IID数据场景下的聚类性能。

Abstract: Federated Learning (FL) that extracts data knowledge while protecting the privacy of multiple clients has achieved remarkable results in distributed privacy-preserving IoT systems, including smart traffic flow monitoring, smart grid load balancing, and so on. Since most data collected from edge devices are unlabeled, unsupervised Federated Clustering (FC) is becoming increasingly popular for exploring pattern knowledge from complex distributed data. However, due to the lack of label guidance, the common Non-Independent and Identically Distributed (Non-IID) issue of clients have greatly challenged FC by posing the following problems: How to fuse pattern knowledge (i.e., cluster distribution) from Non-IID clients; How are the cluster distributions among clients related; and How does this relationship connect with the global knowledge fusion? In this paper, a more tricky but overlooked phenomenon in Non-IID is revealed, which bottlenecks the clustering performance of the existing FC approaches. That is, different clients could fragment a cluster, and accordingly, a more generalized Non-IID concept, i.e., Non-ICD (Non-Independent Completely Distributed), is derived. To tackle the above FC challenges, a new framework named GOLD (Global Oriented Local Distribution Learning) is proposed. GOLD first finely explores the potential incomplete local cluster distributions of clients, then uploads the distribution summarization to the server for global fusion, and finally performs local cluster enhancement under the guidance of the global distribution. Extensive experiments, including significance tests, ablation studies, scalability evaluations, qualitative results, etc., have been conducted to show the superiority of GOLD.

</details>


### [224] [Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment](https://arxiv.org/abs/2601.17563)
*Nathan Gavenski,Matteo Leonetti,Odinaldo Rodrigues*

Main category: cs.LG

TL;DR: 提出UfO方法，通过无监督方式从观察中学习模仿，解决现有ILfO方法需要动作监督、假设状态唯一最优动作、忽略环境状态等问题


<details>
  <summary>Details</summary>
Motivation: 现有ILfO方法存在三个主要局限：需要动作监督优化、假设状态有唯一最优动作、应用教师动作时未充分考虑实际环境状态。虽然观测轨迹中包含真实信息，但现有方法难以无监督地提取这些信息。

Method: UfO采用两阶段学习过程：1) 从观测状态转移中近似教师真实动作；2) 通过调整智能体轨迹使其与教师轨迹对齐，进一步精炼学习到的策略。

Result: 在五个广泛使用的环境中，UfO不仅超越了教师和所有其他ILfO方法，而且表现出最小的标准差，表明在未见场景中具有更好的泛化能力。

Conclusion: UfO成功解决了现有ILfO方法的局限，通过无监督方式从观察中学习模仿，实现了更好的性能和泛化能力。

Abstract: State-of-the-art imitation learning from observation methods (ILfO) have recently made significant progress, but they still have some limitations: they need action-based supervised optimisation, assume that states have a single optimal action, and tend to apply teacher actions without full consideration of the actual environment state. While the truth may be out there in observed trajectories, existing methods struggle to extract it without supervision. In this work, we propose Unsupervised Imitation Learning from Observation (UfO) that addresses all of these limitations. UfO learns a policy through a two-stage process, in which the agent first obtains an approximation of the teacher's true actions in the observed state transitions, and then refines the learned policy further by adjusting agent trajectories to closely align them with the teacher's. Experiments we conducted in five widely used environments show that UfO not only outperforms the teacher and all other ILfO methods but also displays the smallest standard deviation. This reduction in standard deviation indicates better generalisation in unseen scenarios.

</details>


### [225] [Understanding Transformer Encoder-Decoder Representations through Bernoulli Dropout](https://arxiv.org/abs/2601.17602)
*Xuanzhou Chen*

Main category: cs.LG

TL;DR: 研究Transformer过参数化问题，通过高维编码器-解码器嵌入中的角度相似性视角，使用伯努利dropout识别稀疏性阈值以保持Top-1预测准确性


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在过参数化情况下的行为，特别关注编码器和解码器嵌入之间的角度相似性如何影响模型性能。通过分析稀疏性对模型稳定性的影响，理解Transformer在特定条件下保持性能的机制。

Method: 1. 在编码器和解码器之间应用伯努利dropout，改变保留概率p来识别稀疏性阈值
2. 理论上证明：当嵌入的有效稀疏性足够大时，解码器性能在适度坐标dropout下保持稳定
3. 构建带有二进制擦除通道(BEC)的新型Transformer模型进行实验验证
4. 在英法翻译任务上测试模型性能，可视化验证准确率和BLEU分数变化趋势

Result: 实验结果显示验证准确率和BLEU分数在某些阈值处急剧下降，表明存在一个稀疏性依赖的阈值，超过该阈值Top-1预测得以保持。理论分析证实当嵌入稀疏性足够大时，模型性能在适度dropout下保持稳定。

Conclusion: Transformer模型存在稀疏性依赖的性能阈值，当嵌入稀疏性足够大时，模型对编码器-解码器之间的适度dropout具有鲁棒性。这为理解Transformer过参数化和设计更高效的模型架构提供了理论依据。

Abstract: We study Transformer overparameterization through the lens of angular similarity in high-dimensional encoder-decoder embeddings. We apply Bernoulli dropout between the encoder and the decoder, varying the keep probability $p$ to identify a sparsity-dependent threshold above which the Top-1 prediction is preserved. Theoretically, we prove that, if the effective sparsity embeddings is sufficiently large, and thus decoder performance, remain stable under moderate coordinate dropout. Empirically, we implement the Bernoulli dropout by constructing a new Transformer model augmented with Binary Erasure Channel (BEC) and test its performance on an English-French translation task. Experimental results visualize the trends for validation accuracies and BLEU scores, both decline sharply at some threshold.

</details>


### [226] [A Thermodynamic Theory of Learning I: Irreversible Ensemble Transport and Epistemic Costs](https://arxiv.org/abs/2601.17607)
*Daisuke Okanohara*

Main category: cs.LG

TL;DR: 论文提出学习过程本质上不可逆，必须产生熵增。通过建立认知自由能框架，推导出"认知速度极限"不等式，下界化学习过程所需的最小熵产，仅取决于初始和最终分布之间的Wasserstein距离。


<details>
  <summary>Details</summary>
Motivation: 传统信息论表明确定性变换不会增加信息，这与学习系统能从数据中获得结构化内部表示的现象存在矛盾。论文旨在解决这一根本问题：学习如何在不违反信息论限制的情况下产生抽象和洞察？

Method: 将学习建模为模型配置概率分布空间中的输运过程，引入认知自由能框架。定义自由能下降作为记录学习轨迹上总认知自由能减少的簿记量，将其分解为与潜在改进相关的可逆分量和与熵产对应的不可逆分量。

Result: 推导出"认知速度极限"不等式，为任何学习过程实现给定分布变换所需的最小熵产提供下界。该界限仅取决于初始和最终整体分布之间的Wasserstein距离，与具体学习算法无关。

Conclusion: 学习本质上是在有限时间内不可逆的过程，认知结构的实现必然伴随熵产。认知速度极限为学习过程的基本物理限制提供了量化框架，揭示了学习与热力学第二定律的深层联系。

Abstract: Learning systems acquire structured internal representations from data, yet classical information-theoretic results state that deterministic transformations do not increase information. This raises a fundamental question: how can learning produce abstraction and insight without violating information-theoretic limits?
  We argue that learning is inherently an irreversible process when performed over finite time, and that the realization of epistemic structure necessarily incurs entropy production. To formalize this perspective, we model learning as a transport process in the space of probability distributions over model configurations and introduce an epistemic free-energy framework.
  Within this framework, we define the free-energy drop as a bookkeeping quantity that records the total reduction of epistemic free energy along a learning trajectory. This reduction decomposes into a reversible component associated with potential improvement and an irreversible component corresponding to entropy production.
  We then derive the Epistemic Speed Limit (ESL), a finite-time inequality that lower-bounds the minimal entropy production required by any learning process to realize a given distributional transformation. This bound depends only on the Wasserstein distance between initial and final ensemble distributions and is independent of the specific learning algorithm.

</details>


### [227] [Split-on-Share: Mixture of Sparse Experts for Task-Agnostic Continual Learning](https://arxiv.org/abs/2601.17616)
*Fatema Siddika,Md Anwar Hossen,Tanwi Mallick,Ali Jannesari*

Main category: cs.LG

TL;DR: SETA框架通过混合稀疏专家机制解决LLMs持续学习中的可塑性-稳定性困境，将知识分解为特定任务专家和共享专家，使用弹性权重锚定保护关键知识，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续学习面临可塑性-稳定性困境：学习新能力会导致灾难性遗忘。现有方法通常对参数进行统一处理，无法区分特定任务知识和共享能力，需要一种更好的解决方案。

Method: 提出SETA框架，将模型分解为模块化子空间：特定任务专家（隔离任务特定模式）和共享专家（捕获共同特征）。通过弹性权重锚定保护关键共享知识，并使用统一门控网络在推理时自动检索正确的专家组合。

Result: 在多个领域特定和通用基准测试上的广泛实验表明，SETA始终优于基于参数高效微调的最先进持续学习方法。

Conclusion: SETA框架通过混合稀疏专家机制有效解决了LLMs持续学习中的可塑性-稳定性困境，为任务无关的持续学习提供了有效的解决方案。

Abstract: Continual learning in Large Language Models (LLMs) is hindered by the plasticity-stability dilemma, where acquiring new capabilities often leads to catastrophic forgetting of previous knowledge. Existing methods typically treat parameters uniformly, failing to distinguish between specific task knowledge and shared capabilities. We introduce Mixture of Sparse Experts for Task-Agnostic Continual Learning, referred to as SETA, a framework that resolves the plasticity-stability conflict by decomposing the model into modular subspaces. Unlike standard updates, where tasks compete for the same parameters, SETA separates knowledge into unique experts, designed to isolate task-specific patterns, and shared experts, responsible for capturing common features. This structure is maintained through elastic weight anchoring, which protects critical shared knowledge and enables a unified gating network to automatically retrieve the correct expert combination for each task during inference. Extensive experiments across diverse domain-specific and general benchmarks demonstrate that SETA consistently outperforms state-of-the-art parameter-efficient fine-tuning-based continual learning methods.

</details>


### [228] [BrainDistill: Implantable Motor Decoding with Task-Specific Knowledge Distillation](https://arxiv.org/abs/2601.17625)
*Yuhan Xie,Jinhan Liu,Xiaoyong Ni,Fei Tan,Icare Sakr,Thibault Collin,Shiqi Sun,Alejandro Rodriguez Guajardo,Demon Fanny,Charles-francois Vincent Latchoumane,Henri Lorach,Jocelyne Bloch,Gregoire Courtine,Mahsa Shoaran*

Main category: cs.LG

TL;DR: BrainDistill是一种用于植入式脑机接口的轻量化运动解码管道，通过任务特定知识蒸馏和量化感知训练，在保持高性能的同时满足植入系统的严格功耗约束。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的大参数神经网络在脑机接口任务中表现出色，但计算需求高、参数多，难以部署在功耗受限的植入式系统中。需要开发能在严格功耗约束下保持高性能的轻量化解码器。

Method: 提出BrainDistill管道：1）植入式神经解码器（IND）；2）任务特定知识蒸馏（TSKD）框架，通过监督投影优先保留解码关键特征；3）量化感知训练方案，学习激活裁剪范围，实现纯整数推理。

Result: IND在多个神经数据集上优于现有神经解码器；TSKD蒸馏版本在少样本校准设置中超越其他蒸馏方法；量化IND在植入式BCI的严格功耗约束下部署，性能损失最小。

Conclusion: BrainDistill通过任务特定知识蒸馏和量化感知训练，成功解决了大参数Transformer模型在植入式脑机接口中部署的功耗挑战，实现了高性能轻量化运动解码。

Abstract: Transformer-based neural decoders with large parameter counts, pre-trained on large-scale datasets, have recently outperformed classical machine learning models and small neural networks on brain-computer interface (BCI) tasks. However, their large parameter counts and high computational demands hinder deployment in power-constrained implantable systems. To address this challenge, we introduce BrainDistill, a novel implantable motor decoding pipeline that integrates an implantable neural decoder (IND) with a task-specific knowledge distillation (TSKD) framework. Unlike standard feature distillation methods that attempt to preserve teacher representations in full, TSKD explicitly prioritizes features critical for decoding through supervised projection. Across multiple neural datasets, IND consistently outperforms prior neural decoders on motor decoding tasks, while its TSKD-distilled variant further surpasses alternative distillation methods in few-shot calibration settings. Finally, we present a quantization-aware training scheme that enables integer-only inference with activation clipping ranges learned during training. The quantized IND enables deployment under the strict power constraints of implantable BCIs with minimal performance loss.

</details>


### [229] [RPNT: Robust Pre-trained Neural Transformer -- A Pathway for Generalized Motor Decoding](https://arxiv.org/abs/2601.17641)
*Hao Fang,Ryan A. Canfield,Tomohiro Ouchi,Beatrice Macagno,Eli Shlizerman,Amy L. Orsborn*

Main category: cs.LG

TL;DR: RPNT是一种预训练的神经Transformer模型，通过多维旋转位置嵌入、上下文注意力机制和鲁棒自监督学习目标，实现对不同脑区、会话、行为类型和受试者的神经活动解码的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前脑解码模型在处理跨脑区、跨会话、跨行为类型和跨受试者的泛化能力有限，需要开发能够适应和泛化的预训练神经Transformer模型。

Method: 提出RPNT模型，包含：1) 多维旋转位置嵌入(MRoPE)聚合实验元数据；2) 基于上下文的注意力机制处理神经群体活动的非平稳性；3) 鲁棒自监督学习目标。在两个不同数据集上预训练：多会话多任务多受试者微电极基准数据集和高密度Neuropixel 1.0探针多站点记录数据集。

Result: RPNT在跨会话、跨类型、跨受试者和跨站点的下游行为解码任务中，一致达到并超越了现有解码模型的性能。

Conclusion: RPNT通过创新的模型架构和预训练策略，显著提升了脑解码模型在多种变化条件下的泛化能力，为神经科学研究和脑机接口应用提供了更强大的工具。

Abstract: Brain decoding aims to interpret and translate neural activity into behaviors. As such, it is imperative that decoding models are able to generalize across variations, such as recordings from different brain sites, distinct sessions, different types of behavior, and a variety of subjects. Current models can only partially address these challenges and warrant the development of pretrained neural transformer models capable to adapt and generalize. In this work, we propose RPNT - Robust Pretrained Neural Transformer, designed to achieve robust generalization through pretraining, which in turn enables effective finetuning given a downstream task. In particular, RPNT unique components include 1) Multidimensional rotary positional embedding (MRoPE) to aggregate experimental metadata such as site coordinates, session name and behavior types; 2) Context-based attention mechanism via convolution kernels operating on global attention to learn local temporal structures for handling non-stationarity of neural population activity; 3) Robust self-supervised learning (SSL) objective with uniform causal masking strategies and contrastive representations. We pretrained two separate versions of RPNT on distinct datasets a) Multi-session, multi-task, and multi-subject microelectrode benchmark; b) Multi-site recordings using high-density Neuropixel 1.0 probes. The datasets include recordings from the dorsal premotor cortex (PMd) and from the primary motor cortex (M1) regions of nonhuman primates (NHPs) as they performed reaching tasks. After pretraining, we evaluated the generalization of RPNT in cross-session, cross-type, cross-subject, and cross-site downstream behavior decoding tasks. Our results show that RPNT consistently achieves and surpasses the decoding performance of existing decoding models in all tasks.

</details>


### [230] [A Mosco sufficient condition for intrinsic stability of non-unique convex Empirical Risk Minimization](https://arxiv.org/abs/2601.17646)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

TL;DR: 论文将PK上半连续性作为ERM解对应的内在稳定性概念，在最小非退化条件下，Mosco一致扰动和局部有界极小化子能保证PK上半连续性、极小值连续性和渐近极小化子的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统经验风险最小化（ERM）稳定性研究通常关注单值输出，但凸非严格损失函数会产生集值极小化子。需要为ERM解对应关系建立适当的稳定性概念，以便理解选择子（selections）的稳定性。

Method: 将Painlevé-Kuratowski上半连续性（PK-u.s.c.）作为ERM解对应的内在稳定性概念（集合层面的Hadamard适定性）。在最小非退化条件下，通过Mosco一致扰动和局部有界极小化子来保证稳定性。

Result: 证明了在Mosco一致扰动和局部有界极小化子的条件下，ERM解对应具有PK上半连续性、极小值连续性，以及渐近极小化子的一致性。当满足二次增长条件时，还能得到显式的定量偏差界。

Conclusion: PK上半连续性是ERM解对应关系的固有稳定性概念，也是理解选择子稳定性的先决条件。在最小非退化条件下，Mosco一致扰动和局部有界性足以保证良好的稳定性性质。

Abstract: Empirical risk minimization (ERM) stability is usually studied via single-valued outputs, while convex non-strict losses yield set-valued minimizers. We identify Painlevé-Kuratowski upper semicontinuity (PK-u.s.c.) as the intrinsic stability notion for the ERM solution correspondence (set-level Hadamard well-posedness) and a prerequisite to interpret stability of selections. We then characterize a minimal non-degenerate qualitative regime: Mosco-consistent perturbations and locally bounded minimizers imply PK-u.s.c., minimal-value continuity, and consistency of vanishing-gap near-minimizers. Quadratic growth yields explicit quantitative deviation bounds.

</details>


### [231] [Time-Varying Causal Treatment for Quantifying the Causal Effect of Short-Term Variations on Arctic Sea Ice Dynamics](https://arxiv.org/abs/2601.17647)
*Akila Sampath,Vandana Janeja,Jianwu Wang*

Main category: cs.LG

TL;DR: 提出KGCM-VAE模型，通过知识引导的因果推断方法量化海冰厚度与海面高度之间的因果关系，结合物理约束和分布平衡机制提升处理效应估计精度。


<details>
  <summary>Details</summary>
Motivation: 海冰融化与淡水分布的因果关系对理解极地气候变化和全球海平面上升至关重要，但传统深度学习方法在时空场景中因未观测混杂变量和缺乏物理约束而难以可靠估计处理效应。

Method: 提出知识引导的因果模型变分自编码器（KGCM-VAE），包含：1）速度调制方案，通过Sigmoid函数根据SSH变化动态放大平滑速度信号生成物理基础的因果处理；2）最大均值差异（MMD）平衡潜在空间中处理和对照协变量分布；3）因果邻接约束解码器确保与已知物理结构对齐。

Result: 在合成和真实北极数据集上的实验表明，KGCM-VAE在PEHE指标上优于现有基准方法。消融研究证实方法的有效性，MMD和因果邻接约束的联合应用使估计误差降低1.88%。

Conclusion: KGCM-VAE通过整合物理知识和因果推断机制，有效解决了时空场景中的处理效应估计问题，为量化海冰厚度与海面高度的因果关系提供了可靠框架。

Abstract: Quantifying the causal relationship between ice melt and freshwater distribution is critical, as these complex interactions manifest as regional fluctuations in sea surface height (SSH). Leveraging SSH as a proxy for sea ice dynamics enables improved understanding of the feedback mechanisms driving polar climate change and global sea-level rise. However, conventional deep learning models often struggle with reliable treatment effect estimation in spatiotemporal settings due to unobserved confounders and the absence of physical constraints. To address these challenges, we propose the Knowledge-Guided Causal Model Variational Autoencoder (KGCM-VAE) to quantify causal mechanisms between sea ice thickness and SSH. The proposed framework integrates a velocity modulation scheme in which smoothed velocity signals are dynamically amplified via a sigmoid function governed by SSH transitions to generate physically grounded causal treatments. In addition, the model incorporates Maximum Mean Discrepancy (MMD) to balance treated and control covariate distributions in the latent space, along with a causal adjacency-constrained decoder to ensure alignment with established physical structures. Experimental results on both synthetic and real-world Arctic datasets demonstrate that KGCM-VAE achieves superior PEHE compared to state-of-the-art benchmarks. Ablation studies further confirm the effectiveness of the approach, showing that the joint application of MMD and causal adjacency constraints yields a 1.88\% reduction in estimation error.

</details>


### [232] [Kareus: Joint Reduction of Dynamic and Static Energy in Large Model Training](https://arxiv.org/abs/2601.17654)
*Ruofan Wu,Jae-Won Chung,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Kareus是一个AI训练系统，通过细粒度内核调度和频率缩放联合优化动态和静态能耗，推动时间-能耗权衡前沿，相比现有技术可减少28.3%训练能耗或缩短27.5%训练时间。


<details>
  <summary>Details</summary>
Motivation: AI计算需求快速增长，但能源供应跟不上，能源已成为昂贵且竞争激烈的资源，需要明确的管理和优化。现有工作只关注动态或静态能耗的单一方面，而实际中细粒度内核调度和频率缩放会同时且相互依赖地影响这两种能耗。

Method: 设计Kareus训练系统，将难以处理的联合优化问题分解为基于分区的局部子问题，使用多通道多目标优化算法寻找推动时间-能耗权衡前沿的执行调度方案。

Result: 相比现有技术，Kareus在相同训练时间下可减少最多28.3%的训练能耗，或在相同能耗下可缩短最多27.5%的训练时间。

Conclusion: 通过联合优化动态和静态能耗，Kareus系统显著提升了AI训练的时间-能耗权衡效率，为解决AI计算能耗问题提供了有效方案。

Abstract: The computing demand of AI is growing at an unprecedented rate, but energy supply is not keeping pace. As a result, energy has become an expensive, contended resource that requires explicit management and optimization. Although recent works have made significant progress in large model training optimization, they focus only on a single aspect of energy consumption: dynamic or static energy.
  We find that fine-grained kernel scheduling and frequency scaling jointly and interdependently impact both dynamic and static energy consumption. Based on this finding, we design Kareus, a training system that pushes the time--energy tradeoff frontier by optimizing both aspects. Kareus decomposes the intractable joint optimization problem into local, partition-based subproblems. It then uses a multi-pass multi-objective optimization algorithm to find execution schedules that push the time--energy tradeoff frontier. Compared to the state of the art, Kareus reduces training energy by up to 28.3% at the same training time, or reduces training time by up to 27.5% at the same energy consumption.

</details>


### [233] [Entropic Risk-Aware Monte Carlo Tree Search](https://arxiv.org/abs/2601.17667)
*Pedro P. Santos,Jacopo Silvestrin,Alberto Sardinha,Francisco S. Melo*

Main category: cs.LG

TL;DR: 提出一种可证明正确的蒙特卡洛树搜索算法，用于求解具有熵风险度量的风险感知马尔可夫决策过程，并提供非渐近分析。


<details>
  <summary>Details</summary>
Motivation: 现有的MCTS算法主要针对风险中性MDP，而实际决策中需要考虑风险因素。熵风险度量能够捕捉决策者的风险偏好，但将其整合到MCTS框架中并保证理论正确性具有挑战性。

Method: 提出一种风险感知MCTS算法，利用先前工作中引入的熵风险度量动态规划公式，将其整合到基于上置信界的树搜索框架中，实现风险感知的决策树搜索。

Result: 算法在理论上被证明是正确且收敛的：1）根节点的经验熵风险度量收敛到最优值；2）具有多项式后悔集中性。实验验证了算法相对于基线的优越性能。

Conclusion: 成功开发了一种可证明正确的风险感知MCTS算法，为具有熵风险度量的MDP提供了一种有效且理论保证的求解方法，填补了风险感知决策与树搜索算法之间的空白。

Abstract: We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving \textit{risk-aware} Markov decision processes (MDPs) with \textit{entropic risk measure} (ERM) objectives. We provide a \textit{non-asymptotic} analysis of our proposed algorithm, showing that the algorithm: (i) is \textit{correct} in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys \textit{polynomial regret concentration}. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.

</details>


### [234] [Fast KVzip: Efficient and Accurate LLM Inference with Gated KV Eviction](https://arxiv.org/abs/2601.17668)
*Jang-Hyun Kim,Dongyoon Han,Sangdoo Yun*

Main category: cs.LG

TL;DR: 提出一种基于门控机制的KV缓存淘汰方法，通过轻量级sink-attention门控模块识别并保留关键KV对，在仅需前向传播的训练下实现高达70%的KV缓存压缩，且性能几乎无损。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩技术通常存在性能下降和计算开销之间的权衡问题，需要一种既能高效压缩缓存又不会显著影响模型性能的轻量级解决方案。

Method: 引入轻量级sink-attention门控模块来识别关键KV对，设计无需反向传播的门训练算法，采用任务无关的重建目标实现强泛化能力，并同时支持预填充和解码阶段。

Result: 在Qwen2.5-1M、Qwen3和Gemma3系列模型上实验表明，该方法在淘汰高达70%的KV缓存时仍能保持接近无损的性能，在长上下文理解、代码理解和数学推理等任务上表现一致。

Conclusion: 该方法提供了一种计算成本极低、性能几乎无损的KV缓存压缩方案，具有良好的通用性和实用性，适用于实际LLM部署场景。

Abstract: Efficient key-value (KV) cache management is crucial for the practical deployment of large language models (LLMs), yet existing compression techniques often incur a trade-off between performance degradation and computational overhead. We propose a novel gating-based KV cache eviction method for frozen-weight LLMs that achieves high compression ratios with negligible computational cost. Our approach introduces lightweight sink-attention gating modules to identify and retain critical KV pairs, and integrates seamlessly into both the prefill and decoding stages. The proposed gate training algorithm relies on forward passes of an LLM, avoiding expensive backpropagation, while achieving strong task generalization through a task-agnostic reconstruction objective. Extensive experiments across the Qwen2.5-1M, Qwen3, and Gemma3 families show that our method maintains near-lossless performance while evicting up to 70% of the KV cache. The results are consistent across a wide range of tasks, including long-context understanding, code comprehension, and mathematical reasoning, demonstrating the generality of our approach.

</details>


### [235] [$\infty$-MoE: Generalizing Mixture of Experts to Infinite Experts](https://arxiv.org/abs/2601.17680)
*Shota Takashiro,Takeshi Kojima,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.LG

TL;DR: 提出∞-MoE方法，通过连续空间选择大型FFN的部分参数，实现无限专家数量，在保持计算效率的同时提高训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统MoE将每个专家视为完全独立且在离散空间组合，当专家数量增加时难以有效训练每个专家。需要一种方法能在增加专家数量的同时稳定训练。

Method: 提出∞-MoE，基于每个token采样的连续值选择大型前馈网络的部分参数。通过连续空间考虑专家，允许无限数量的专家同时保持计算效率。

Result: 基于GPT-2 Small的∞-MoE模型（1.29亿活跃参数，1.86亿总参数）达到密集GPT-2 Medium（3.5亿参数）相当性能。推理时可调整采样专家数量，在准确性和速度间灵活权衡，比传统MoE准确率提升达2.5%。

Conclusion: ∞-MoE通过在连续空间选择参数，解决了传统MoE专家数量增加时的训练难题，实现了无限专家数量、训练稳定性和计算效率的良好平衡。

Abstract: The Mixture of Experts (MoE) selects a few feed-forward networks (FFNs) per token, achieving an effective trade-off between computational cost and performance. In conventional MoE, each expert is treated as entirely independent, and experts are combined in a discrete space. As a result, when the number of experts increases, it becomes difficult to train each expert effectively. To stabilize training while increasing the number of experts, we propose $\infty$-MoE that selects a portion of the parameters of large FFNs based on continuous values sampled for each token. By considering experts in a continuous space, this approach allows for an infinite number of experts while maintaining computational efficiency. Experiments show that a GPT-2 Small-based $\infty$-MoE model, with 129M active and 186M total parameters, achieves comparable performance to a dense GPT-2 Medium with 350M parameters. Adjusting the number of sampled experts at inference time allows for a flexible trade-off between accuracy and speed, with an improvement of up to 2.5\% in accuracy over conventional MoE.

</details>


### [236] [Agentic reinforcement learning empowers next-generation chemical language models for molecular design and synthesis](https://arxiv.org/abs/2601.17687)
*Hao Li,He Cao,Shenyao Peng,Zijing Liu,Bin Feng,Yu Wang,Zhiyuan Yan,Yonghong Tian,Yu Li,Li Yuan*

Main category: cs.LG

TL;DR: ChemCRAFT是一个通过智能体强化学习将化学推理与知识存储解耦的框架，让小规模本地部署的语言模型能够通过沙盒交互实现高精度化学任务，在保持隐私和低成本的同时超越云端大模型。


<details>
  <summary>Details</summary>
Motivation: 当前生化领域语言模型面临两难：小模型易产生幻觉且知识有限，而云端大模型存在隐私风险和推理成本高的问题。需要一种既能保持隐私和低成本，又能实现高精度化学推理的解决方案。

Method: 1. 提出ChemCRAFT框架，通过智能体强化学习将化学推理与知识存储解耦；2. 构建智能体轨迹构建流程和化学智能体沙盒；3. 创建ChemToolDataset - 首个大规模化学工具轨迹数据集；4. 提出SMILES-GRPO构建密集化学奖励函数，提升模型调用化学智能体的能力。

Result: 在药物设计的多个方面（分子结构分析、分子优化、合成路径预测）的评估中，ChemCRAFT超越了当前云端大语言模型，证明了科学推理不仅是模型规模的涌现能力，更是工具编排的可学习策略。

Conclusion: 该工作建立了成本效益高且隐私保护的AI辅助化学范式，为通过本地可部署智能体加速分子发现开辟了新途径，表明小模型通过工具编排可以实现超越大模型的科学推理能力。

Abstract: Language models are revolutionizing the biochemistry domain, assisting scientists in drug design and chemical synthesis with high efficiency. Yet current approaches struggle between small language models prone to hallucination and limited knowledge retention, and large cloud-based language models plagued by privacy risks and high inference costs. To bridge this gap, we introduce ChemCRAFT, a novel framework leveraging agentic reinforcement learning to decouple chemical reasoning from knowledge storage. Instead of forcing the model to memorize vast chemical data, our approach empowers the language model to interact with a sandbox for precise information retrieval. This externalization of knowledge allows a locally deployable small model to achieve superior performance with minimal inference costs. To enable small language models for agent-calling ability, we build an agentic trajectory construction pipeline and a comprehensive chemical-agent sandbox. Based on sandbox interactions, we constructed ChemToolDataset, the first large-scale chemical tool trajectory dataset. Simultaneously, we propose SMILES-GRPO to build a dense chemical reward function, promoting the model's ability to call chemical agents. Evaluations across diverse aspects of drug design show that ChemCRAFT outperforms current cloud-based LLMs in molecular structure analysis, molecular optimization, and synthesis pathway prediction, demonstrating that scientific reasoning is not solely an emergent ability of model scale, but a learnable policy of tool orchestration. This work establishes a cost-effective and privacy-preserving paradigm for AI-aided chemistry, opening new avenues for accelerating molecular discovery with locally deployable agents.

</details>


### [237] [REV-INR: Regularized Evidential Implicit Neural Representation for Uncertainty-Aware Volume Visualization](https://arxiv.org/abs/2601.17689)
*Shanu Saklani,Tushar M. Athawale,Nairita Pal,David Pugmire,Christopher R. Johnson,Soumya Dutta*

Main category: cs.LG

TL;DR: REV-INR提出了一种基于证据正则化的隐式神经表示方法，能够同时预测体数据值、数据不确定性和模型不确定性，在单次前向传播中实现高效的不确定性感知重建。


<details>
  <summary>Details</summary>
Motivation: 传统确定性INR只能预测数值，无法提供预测不确定性信息，这可能导致不可靠的数据解释和可视化。当原始体数据因体积过大而不可用时，难以识别模型预测中的错误结果。

Method: REV-INR（正则化证据隐式神经表示）学习同时准确预测数据值以及相关的坐标级数据不确定性和模型不确定性，在推理时只需单次前向传播。

Result: REV-INR在体重建质量上表现最佳，能够提供鲁棒的数据（偶然）不确定性和模型（认知）不确定性估计，同时具有最快的推理时间。

Conclusion: REV-INR能够评估提取的等值面和体可视化结果的可靠性和可信度，使得分析可以完全基于模型预测的数据进行。

Abstract: Applications of Implicit Neural Representations (INRs) have emerged as a promising deep learning approach for compactly representing large volumetric datasets. These models can act as surrogates for volume data, enabling efficient storage and on-demand reconstruction via model predictions. However, conventional deterministic INRs only provide value predictions without insights into the model's prediction uncertainty or the impact of inherent noisiness in the data. This limitation can lead to unreliable data interpretation and visualization due to prediction inaccuracies in the reconstructed volume. Identifying erroneous results extracted from model-predicted data may be infeasible, as raw data may be unavailable due to its large size. To address this challenge, we introduce REV-INR, Regularized Evidential Implicit Neural Representation, which learns to predict data values accurately along with the associated coordinate-level data uncertainty and model uncertainty using only a single forward pass of the trained REV-INR during inference. By comprehensively comparing and contrasting REV-INR with existing well-established deep uncertainty estimation methods, we show that REV-INR achieves the best volume reconstruction quality with robust data (aleatoric) and model (epistemic) uncertainty estimates using the fastest inference time. Consequently, we demonstrate that REV-INR facilitates assessment of the reliability and trustworthiness of the extracted isosurfaces and volume visualization results, enabling analyses to be solely driven by model-predicted data.

</details>


### [238] [FedCCA: Client-Centric Adaptation against Data Heterogeneity in Federated Learning on IoT Devices](https://arxiv.org/abs/2601.17713)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Yinfeng Cao*

Main category: cs.LG

TL;DR: FedCCA是一种针对物联网设备数据异质性的联邦学习算法，通过动态客户端选择和基于特定编码器的自适应聚合，为每个客户端学习独特模型，显著提升性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 物联网设备上的人体感知数据等私有数据需要AI模型训练，联邦学习提供了隐私保护的分布式训练框架。然而，物联网设备间的数据异质性问题会显著降低模型性能和收敛速度。现有方法局限于固定的客户端选择和云服务器聚合，难以在本地训练中隐私保护地提取客户端特定信息。

Method: 提出FedCCA算法：1) 动态客户端选择；2) 基于额外客户端特定编码器的自适应聚合；3) 采用基于注意力的全局聚合策略来增强多源知识转移。通过选择性适应，为每个客户端学习独特模型。

Result: 在多个数据集上进行广泛实验，结果表明FedCCA在解决数据异质性问题上相比基线方法具有显著性能优势。

Conclusion: FedCCA通过最优利用客户端特定知识，有效缓解了数据异质性对联邦学习的影响，为物联网环境下的隐私保护AI训练提供了有效解决方案。

Abstract: With the rapid development of the Internet of Things (IoT), AI model training on private data such as human sensing data is highly desired. Federated learning (FL) has emerged as a privacy-preserving distributed training framework for this purpuse. However, the data heterogeneity issue among IoT devices can significantly degrade the model performance and convergence speed in FL. Existing approaches limit in fixed client selection and aggregation on cloud server, making the privacy-preserving extraction of client-specific information during local training challenging. To this end, we propose Client-Centric Adaptation federated learning (FedCCA), an algorithm that optimally utilizes client-specific knowledge to learn a unique model for each client through selective adaptation, aiming to alleviate the influence of data heterogeneity. Specifically, FedCCA employs dynamic client selection and adaptive aggregation based on the additional client-specific encoder. To enhance multi-source knowledge transfer, we adopt an attention-based global aggregation strategy. We conducted extensive experiments on diverse datasets to assess the efficacy of FedCCA. The experimental results demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [239] [Do Reasoning Models Ask Better Questions? A Formal Information-Theoretic Analysis on Multi-Turn LLM Games](https://arxiv.org/abs/2601.17716)
*Daniel M. Pedrozo,Telma W. de L. Soares,Bryan L. M. de Oliveira*

Main category: cs.LG

TL;DR: 论文提出一个多轮对话框架，通过信息增益(IG)量化评估LLM在层次知识图环境中通过是/否问题收集信息的效果，并在"猜城市"游戏中测试不同LLM的提问能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在解决用户请求模糊性时提问能力不足，现有基准缺乏基于信息增益的全面评估框架，且缺少思维链推理与无思维链模型的系统比较。

Method: 采用三元交互LLM代理框架：提问代理、回答代理和假设空间更新代理。在五级分类的地理猜城市游戏环境中，基于香农熵计算信息增益作为核心指标，评估完全可观察和部分可观察条件下不同LLM的表现。

Result: 实验表明，具有显式推理能力的模型在每轮获得更高信息增益，且用更少步骤解决问题，尤其在部分可观察条件下。小模型通过更积极探索候选问题弥补能力限制，大模型在选择最优查询时更自信，生成具有更高潜在IG的候选问题。

Conclusion: 基于信息增益的评估框架能有效量化LLM的提问能力，思维链推理显著提升信息收集效率，模型规模影响提问策略：小模型探索更积极，大模型选择更自信。

Abstract: Large Language Models (LLMs) excel at many tasks but still struggle with a critical ability for LLM-based agents: asking good questions for resolving ambiguity in user requests. While prior work has explored information-seeking behavior through word games, existing benchmarks lack comprehensive evaluation frameworks that provide both final and intermediate signals based on Information Gain (IG). Moreover, they rarely provide systematic comparisons between models that use chain-of-thought reasoning and those that do not. We propose a multi-turn dialogue framework that quantitatively measures how effectively LLMs gather information through yes/no questions in a hierarchical knowledge graph environment. Our framework employs a triad of interacting LLM agents that ask questions, answer them, and update the hypothesis space. We adopt IG as the main metric, grounded in Shannon entropy, to assess query effectiveness at each turn and cumulatively. We instantiate our framework in a geographical Guess My City game setting organized in a five-level taxonomy and evaluate multiple LLM variants under fully and partially observable conditions, with and without Chain-of-Thought reasoning. Our experiments demonstrate that, among the evaluated models, the ones with explicit reasoning capabilities achieve higher IG per turn and reach solutions in fewer steps, particularly in partially observable settings. Analysis of reasoning traces reveals that smaller models compensate for limited capacity through more aggressive exploration of candidate questions, while larger models exhibit higher assertiveness in selecting optimal queries, generating candidates with greater potential IG.

</details>


### [240] [AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation](https://arxiv.org/abs/2601.17761)
*Dongjie Cheng,Ruifeng Yuan,Yongqi Li,Runyang You,Wenjie Wang,Liqiang Nie,Lei Zhang,Wenjie Li*

Main category: cs.LG

TL;DR: AR-Omni是一个统一的任意到任意多模态模型，采用自回归范式，支持文本、图像和流式语音生成，无需专家解码器，实现了高质量、实时的多模态生成。


<details>
  <summary>Details</summary>
Motivation: 现实世界的感知和交互本质上是多模态的，需要同时支持多模态输入和输出的"全能"多模态大语言模型。现有系统大多依赖额外的专家组件来实现多模态生成，限制了统一训练和推理的简洁性。自回归建模在文本领域已被证明是优雅且可扩展的基础架构，因此希望将其扩展到多模态领域。

Method: 提出AR-Omni模型，采用统一的自回归范式，仅使用单个Transformer解码器支持文本、图像和流式语音生成。通过三种技术解决统一自回归建模的实际问题：1）通过任务感知损失重加权解决模态不平衡问题；2）通过轻量级token级感知对齐损失提高视觉保真度；3）通过有限状态解码机制平衡稳定性和创造性。

Result: AR-Omni在三种模态上都实现了强大的生成质量，同时保持实时性，语音生成的实时因子达到0.88，表明模型在保持高质量的同时具有高效的推理性能。

Conclusion: AR-Omni展示了自回归建模范式可以有效地扩展到多模态领域，实现了统一的任意到任意生成，无需专家解码器，为简洁、可扩展的多模态系统提供了有前景的解决方案。

Abstract: Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of "Omni" MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.

</details>


### [241] [LLM-42: Enabling Determinism in LLM Inference with Verified Speculation](https://arxiv.org/abs/2601.17768)
*Raja Gond,Aditya K Kamath,Arkaprava Basu,Ramachandran Ramjee,Ashish Panwar*

Main category: cs.LG

TL;DR: LLM-42提出了一种基于调度的确定性LLM推理方法，通过轻量级的验证-回滚循环，在保持动态批处理高吞吐量的同时实现输出确定性。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的非确定性源于浮点非结合性与动态批处理、GPU核函数的结合，导致相同提示在不同运行中产生不同输出。现有方法要么禁用动态批处理（严重降低吞吐量），要么要求核函数设计为批次不变（实现复杂且有固定开销）。

Method: LLM-42采用基于调度的确定性方法：1）使用非确定性快速路径解码token；2）通过轻量级验证-回滚循环强制执行确定性；3）验证器在固定形状的归约调度下重放候选token；4）提交保证跨运行一致的token，回滚违反确定性的token。

Result: 该方法大部分复用现有核函数不变，仅在需要确定性的流量比例上产生开销，在保持动态批处理高吞吐量的同时实现了输出确定性。

Conclusion: LLM-42通过调度方法解耦了确定性与核函数设计，实现了高效、灵活的确定性LLM推理，只在必要时产生开销，平衡了性能与确定性需求。

Abstract: In LLM inference, the same prompt may yield different outputs across different runs. At the system level, this non-determinism arises from floating-point non-associativity combined with dynamic batching and GPU kernels whose reduction orders vary with batch size. A straightforward way to eliminate non-determinism is to disable dynamic batching during inference, but doing so severely degrades throughput. Another approach is to make kernels batch-invariant; however, this tightly couples determinism to kernel design, requiring new implementations. This coupling also imposes fixed runtime overheads, regardless of how much of the workload actually requires determinism.
  Inspired by ideas from speculative decoding, we present LLM-42, a scheduling-based approach to enable determinism in LLM inference. Our key observation is that if a sequence is in a consistent state, the next emitted token is likely to be consistent even with dynamic batching. Moreover, most GPU kernels use shape-consistent reductions. Leveraging these insights, LLM-42 decodes tokens using a non-deterministic fast path and enforces determinism via a lightweight verify-rollback loop. The verifier replays candidate tokens under a fixed-shape reduction schedule, commits those that are guaranteed to be consistent across runs, and rolls back those violating determinism. LLM-42 mostly re-uses existing kernels unchanged and incurs overhead only in proportion to the traffic that requires determinism.

</details>


### [242] [Shortcut Learning in Binary Classifier Black Boxes: Applications to Voice Anti-Spoofing and Biometrics](https://arxiv.org/abs/2601.17782)
*Md Sahidullah,Hye-jin Shim,Rosa Gonzalez Hautamäki,Tomi H. Kinnunen*

Main category: cs.LG

TL;DR: 提出一个分析二分类器中数据集偏差和"捷径学习"的新框架，通过干预和观察视角结合线性混合效应模型进行后验分析，在音频反欺骗和说话人验证任务上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在数据驱动应用中的广泛采用引发了人们对数据集和模型偏见的关注。被忽视或隐藏的偏见可能导致意外结果，需要研究数据集偏见和分类器中的"捷径学习"现象。

Method: 提出一个分析黑盒分类器的新框架，结合干预和观察视角，使用线性混合效应模型进行后验分析，评估分类器性能超越错误率，分析训练和测试数据对分类器分数的影响。

Result: 在音频反欺骗和说话人验证任务上，使用统计模型和深度神经网络验证了方法的有效性，能够提供对偏见数据集的深入理解，并揭示分类器行为的影响因素。

Conclusion: 该研究为解决其他领域的偏见问题提供了更广泛的启示，并推动了可解释人工智能领域的发展，为理解分类器行为和数据集偏见提供了系统性的分析框架。

Abstract: The widespread adoption of deep-learning models in data-driven applications has drawn attention to the potential risks associated with biased datasets and models. Neglected or hidden biases within datasets and models can lead to unexpected results. This study addresses the challenges of dataset bias and explores ``shortcut learning'' or ``Clever Hans effect'' in binary classifiers. We propose a novel framework for analyzing the black-box classifiers and for examining the impact of both training and test data on classifier scores. Our framework incorporates intervention and observational perspectives, employing a linear mixed-effects model for post-hoc analysis. By evaluating classifier performance beyond error rates, we aim to provide insights into biased datasets and offer a comprehensive understanding of their influence on classifier behavior. The effectiveness of our approach is demonstrated through experiments on audio anti-spoofing and speaker verification tasks using both statistical models and deep neural networks. The insights gained from this study have broader implications for tackling biases in other domains and advancing the field of explainable artificial intelligence.

</details>


### [243] [Robust Computational Extraction of Non-Enhancing Hypercellular Tumor Regions from Clinical Imaging Data](https://arxiv.org/abs/2601.17802)
*A. Brawanski,Th. Schaffer,F. Raab,K. -M. Schebesch,M. Schrey,Chr. Doenitz,A. M. Tomé,E. W. Lang*

Main category: cs.LG

TL;DR: 本文提出了一种从常规MRI数据生成非增强高细胞性肿瘤区域概率图的稳健计算框架，解决了神经肿瘤成像中未满足的需求。


<details>
  <summary>Details</summary>
Motivation: 准确识别非增强高细胞性肿瘤区域是神经肿瘤成像中一个未满足的需求，对患者管理和治疗计划具有重要影响。当前存在固有变异性和缺乏清晰成像边界的问题。

Method: 开发了一个稳健的计算框架，利用多种网络架构从常规MRI数据生成NEH区域的概率图，以应对数据变异性和边界模糊的挑战。

Result: 该框架通过相对脑血容量和增强肿瘤复发位置等独立临床标志物进行验证，证明了方法的稳健性和生物学相关性，能够可靠地非侵入性映射NEH肿瘤区域。

Conclusion: 该框架支持将NEH区域作为影像生物标志物整合到临床工作流程中，推进脑肿瘤患者的精准肿瘤学发展。

Abstract: Accurate identification of non-enhancing hypercellular (NEH) tumor regions is an unmet need in neuro-oncological imaging, with significant implications for patient management and treatment planning. We present a robust computational framework that generates probability maps of NEH regions from routine MRI data, leveraging multiple network architectures to address the inherent variability and lack of clear imaging boundaries. Our approach was validated against independent clinical markers -- relative cerebral blood volume (rCBV) and enhancing tumor recurrence location (ETRL) -- demonstrating both methodological robustness and biological relevance. This framework enables reliable, non-invasive mapping of NEH tumor compartments, supporting their integration as imaging biomarkers in clinical workflows and advancing precision oncology for brain tumor patients.

</details>


### [244] [MergeMix: Optimizing Mid-Training Data Mixtures via Learnable Model Merging](https://arxiv.org/abs/2601.17858)
*Jiapeng Wang,Changxin Tian,Kunlong Chen,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.LG

TL;DR: MergeMix通过模型融合权重作为低成本性能代理，高效优化LLM数据混合比例，无需大规模训练即可找到最优数据组合。


<details>
  <summary>Details</summary>
Motivation: 当前优化大型语言模型数据混合比例的方法依赖启发式试验或昂贵的代理训练，计算成本过高，需要更高效、自动化的解决方案。

Method: 训练领域特定专家模型，使用少量tokens，然后通过优化这些专家模型的融合权重来优化下游基准性能，将模型融合权重作为高性能、低成本的代理指标。

Result: 在8B和16B参数模型上的实验验证，MergeMix性能达到或超过详尽人工调优，同时大幅降低搜索成本，显示出高排名一致性（Spearman ρ>0.9）和强大的跨尺度可迁移性。

Conclusion: MergeMix为数据混合优化提供了一个可扩展的自动化解决方案，能够高效确定最优数据混合比例，为LLM数据优化提供了实用工具。

Abstract: Optimizing data mixtures is essential for unlocking the full potential of large language models (LLMs), yet identifying the optimal composition remains computationally prohibitive due to reliance on heuristic trials or expensive proxy training. To address this, we introduce \textbf{MergeMix}, a novel approach that efficiently determines optimal data mixing ratios by repurposing model merging weights as a high-fidelity, low-cost performance proxy. By training domain-specific experts on minimal tokens and optimizing their merging weights against downstream benchmarks, MergeMix effectively optimizes the performance of data mixtures without incurring the cost of full-scale training. Extensive experiments on models with 8B and 16B parameters validate that MergeMix achieves performance comparable to or surpassing exhaustive manual tuning while drastically reducing search costs. Furthermore, MergeMix exhibits high rank consistency (Spearman $ρ> 0.9$) and strong cross-scale transferability, offering a scalable, automated solution for data mixture optimization.

</details>


### [245] [EEG Foundation Models: Progresses, Benchmarking, and Open Problems](https://arxiv.org/abs/2601.17883)
*Dingkun Liu,Yuheng Chen,Zhu Chen,Zhenyao Cui,Yaozhi Wen,Jiayu An,Jingwei Luo,Dongrui Wu*

Main category: cs.LG

TL;DR: 该论文填补了EEG基础模型缺乏公平全面比较的空白，通过统一框架评估了12个开源模型在13个数据集上的表现，发现线性探测常不足、专用模型仍有竞争力、模型规模不一定带来更好性能。


<details>
  <summary>Details</summary>
Motivation: EEG基础模型在脑机接口领域快速发展，但由于预训练目标、预处理方法和下游评估协议不一致，缺乏公平全面的比较。本文旨在填补这一空白，为EEG基础模型的研究提供系统性的评估框架。

Method: 1. 回顾50个代表性模型并构建统一分类框架（数据标准化、模型架构、自监督预训练策略）
2. 评估12个开源基础模型和竞争性专用基线模型
3. 使用13个EEG数据集涵盖9种BCI范式
4. 采用两种评估设置：跨被试泛化（留一被试法）和快速校准（被试内少样本）
5. 比较全参数微调与线性探测，并分析模型规模与下游性能的关系

Result: 1. 线性探测经常不足以获得良好性能
2. 从头训练的专用模型在许多任务中仍然具有竞争力
3. 在当前数据规模和训练实践下，更大的基础模型不一定能带来更好的泛化性能

Conclusion: 该研究为EEG基础模型提供了首个系统性评估框架，揭示了当前方法的局限性，为未来EEG基础模型的发展提供了重要见解和基准。

Abstract: Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.

</details>


### [246] [Adaptive Weighting in Knowledge Distillation: An Axiomatic Framework for Multi-Scale Teacher Ensemble Optimization](https://arxiv.org/abs/2601.17910)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出一个与算子无关的公理化框架，用于多教师知识蒸馏中的自适应权重分配，涵盖token、task和context三个互补尺度，为异构、分布偏移和安全约束下的自适应蒸馏方法提供理论分析基础。


<details>
  <summary>Details</summary>
Motivation: 现有多教师知识蒸馏方法主要依赖启发式或实现特定的权重方案，缺乏理论指导。本文旨在建立一个通用的公理化框架，使自适应权重分配方法能够进行原则性分析，特别是在异构、分布偏移和安全约束等复杂场景下。

Method: 开发了一个算子无关的公理化框架，形式化了自适应权重算子在不同尺度（token、task、context）上的结构条件，包括良好定义性、多重非等价实现可能性、通过乘积结构归一化的层次组合。在该框架内分析了算子的存在性、非唯一性、梯度优化的收敛性、稳定性和扰动鲁棒性，并提供了安全约束蒸馏的抽象表述。

Result: 建立了自适应权重算子的存在性和非唯一性，在标准假设下证明了梯度优化的收敛性，分析了稳定性和扰动鲁棒性。该框架将理论保证与具体权重公式解耦，为异构、分布偏移和安全约束下的自适应蒸馏方法提供了原则性分析基础。

Conclusion: 提出的公理化框架为多教师知识蒸馏中的自适应权重分配提供了理论基础，使研究者能够在不依赖具体实现细节的情况下分析方法的理论性质，为开发更鲁棒、高效和安全的知识蒸馏方法提供了指导。

Abstract: Knowledge distillation with multiple teachers is increasingly used to improve robustness, efficiency, and safety, yet existing approaches rely largely on heuristic or implementation-specific weighting schemes. This paper develops an operator-agnostic axiomatic framework for adaptive weighting in multi-teacher knowledge distillation across three complementary scales: token, task, and context. We formalize structural conditions under which adaptive weighting operators are well-defined, admit multiple non-equivalent implementations, and can be hierarchically composed via product-structure normalization. Within this framework, we establish existence and non-uniqueness of conforming operators, characterize convergence of gradient-based optimization under standard assumptions, analyze stability and perturbation robustness, and provide an abstract formulation of safety-constrained distillation. The results decouple theoretical guarantees from specific weighting formulas, enabling principled analysis of adaptive distillation methods under heterogeneity, distribution shift, and safety constraints.

</details>


### [247] [Causal Pre-training Under the Fairness Lens: An Empirical Study of TabPFN](https://arxiv.org/abs/2601.17912)
*Qinyi Liu,Mohammad Khalil,Naman Goel*

Main category: cs.LG

TL;DR: TabPFN等表格数据基础模型通过因果预训练获得高预测准确性，但在公平性方面表现有限，特别是在MNAR协变量偏移下，需要额外的公平性干预措施。


<details>
  <summary>Details</summary>
Motivation: 虽然TabPFN等表格数据基础模型通过因果预训练展现出高预测性能，但其公平性特性尚未得到充分研究，需要评估这些因果预训练模型在实际应用中的公平性表现。

Method: 对TabPFN及其微调变体进行全面的实证评估，包括预测性能、公平性和鲁棒性测试，考察不同数据集大小和分布偏移情况下的表现。

Result: TabPFN相比基线模型具有更强的预测准确性和对虚假相关性的鲁棒性，但公平性改善有限且不一致，特别是在MNAR协变量偏移下表现不佳。

Conclusion: TabPFN的因果预训练有帮助但不足以实现算法公平性，实际部署时需要额外的公平性干预，这对基础模型在实践中的应用具有重要意义。

Abstract: Foundation models for tabular data, such as the Tabular Prior-data Fitted Network (TabPFN), are pre-trained on a massive number of synthetic datasets generated by structural causal models (SCM). They leverage in-context learning to offer high predictive accuracy in real-world tasks. However, the fairness properties of these foundational models, which incorporate ideas from causal reasoning during pre-training, have not yet been explored in sufficient depth. In this work, we conduct a comprehensive empirical evaluation of TabPFN and its fine-tuned variants, assessing predictive performance, fairness, and robustness across varying dataset sizes and distributional shifts. Our results reveal that while TabPFN achieves stronger predictive accuracy compared to baselines and exhibits robustness to spurious correlations, improvements in fairness are moderate and inconsistent, particularly under missing-not-at-random (MNAR) covariate shifts. These findings suggest that the causal pre-training in TabPFN is helpful but insufficient for algorithmic fairness, highlighting implications for deploying such models in practice and the need for further fairness interventions.

</details>


### [248] [UniPACT: A Multimodal Framework for Prognostic Question Answering on Raw ECG and Structured EHR](https://arxiv.org/abs/2601.17916)
*Jialu Tang,Tong Xia,Yuan Lu,Aaqib Saeed*

Main category: cs.LG

TL;DR: UniPACT提出统一框架，通过结构化提示将数值型EHR数据转换为语义丰富的文本，并与原始ECG波形表示融合，使LLM能够对多模态临床数据进行整体推理，在MDS-ED基准上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 临床预后需要整合结构化EHR数据和实时生理信号（如ECG），但LLM难以原生处理这些异质、非文本数据类型，存在模态鸿沟问题。

Method: 提出UniPACT统一框架：1）结构化提示机制将数值EHR数据转换为语义丰富的文本；2）将文本化患者上下文与原始ECG波形学习到的表示融合；3）使LLM能够对两种模态进行整体推理。

Result: 在MDS-ED基准测试中，在诊断、恶化、ICU入院和死亡率等多种预后任务上达到89.37%的平均AUROC，优于专用基线模型。多模态多任务方法对性能至关重要，且在数据缺失场景中表现出鲁棒性。

Conclusion: UniPACT成功弥合了临床数据模态鸿沟，通过统一框架实现了多模态临床数据的有效整合，为LLM在临床预后应用提供了可行解决方案。

Abstract: Accurate clinical prognosis requires synthesizing structured Electronic Health Records (EHRs) with real-time physiological signals like the Electrocardiogram (ECG). Large Language Models (LLMs) offer a powerful reasoning engine for this task but struggle to natively process these heterogeneous, non-textual data types. To address this, we propose UniPACT (Unified Prognostic Question Answering for Clinical Time-series), a unified framework for prognostic question answering that bridges this modality gap. UniPACT's core contribution is a structured prompting mechanism that converts numerical EHR data into semantically rich text. This textualized patient context is then fused with representations learned directly from raw ECG waveforms, enabling an LLM to reason over both modalities holistically. We evaluate UniPACT on the comprehensive MDS-ED benchmark, it achieves a state-of-the-art mean AUROC of 89.37% across a diverse set of prognostic tasks including diagnosis, deterioration, ICU admission, and mortality, outperforming specialized baselines. Further analysis demonstrates that our multimodal, multi-task approach is critical for performance and provides robustness in missing data scenarios.

</details>


### [249] [treaming-dLLM: Accelerating Diffusion LLMs via Suffix Pruning and Dynamic Decoding](https://arxiv.org/abs/2601.17917)
*Zhongyu Xiao,Zhiwei Hao,Jianyuan Guo,Yong Luo,Jia Liu,Jie Xu,Han Hu*

Main category: cs.LG

TL;DR: Streaming-dLLM是一个训练免费框架，通过空间维度上的衰减引导后缀建模和时间维度上的动态置信感知策略，显著加速扩散大语言模型推理，最高达到68.2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散大语言模型推理加速方法（如KV缓存重用或启发式解码）忽略了块状扩散过程中的内在低效性：空间冗余（对信息稀疏的后缀区域进行统一建模）和时间低效（在整个解码过程中应用固定的去噪调度）。

Method: 提出Streaming-dLLM框架：1）空间上，通过衰减引导后缀建模来修剪冗余的掩码标记，近似完整上下文；2）时间上，采用动态置信感知策略和早期退出机制，让模型为已收敛的标记跳过不必要的迭代。

Result: 广泛实验表明，Streaming-dLLM在保持生成质量的同时，实现了最高68.2倍的推理加速，凸显了其在扩散解码中的有效性。

Conclusion: Streaming-dLLM通过在空间和时间维度上优化扩散大语言模型的推理过程，显著提升了效率，为扩散模型的实用化提供了有效解决方案。

Abstract: Diffusion Large Language Models (dLLMs) offer a compelling paradigm for natural language generation, leveraging parallel decoding and bidirectional attention to achieve superior global coherence compared to autoregressive models. While recent works have accelerated inference via KV cache reuse or heuristic decoding, they overlook the intrinsic inefficiencies within the block-wise diffusion process. Specifically, they suffer from spatial redundancy by modeling informative-sparse suffix regions uniformly and temporal inefficiency by applying fixed denoising schedules across all the decoding process. To address this, we propose Streaming-dLLM, a training-free framework that streamlines inference across both spatial and temporal dimensions. Spatially, we introduce attenuation guided suffix modeling to approximate the full context by pruning redundant mask tokens. Temporally, we employ a dynamic confidence aware strategy with an early exit mechanism, allowing the model to skip unnecessary iterations for converged tokens. Extensive experiments show that Streaming-dLLM achieves up to 68.2X speedup while maintaining generation quality, highlighting its effectiveness in diffusion decoding. The code is available at https://github.com/xiaoshideta/Streaming-dLLM.

</details>


### [250] [Dissipative Learning: A Framework for Viable Adaptive Systems](https://arxiv.org/abs/2601.17933)
*Laurent Caraffa*

Main category: cs.LG

TL;DR: 学习本质上是耗散过程，遗忘和正则化不是启发式附加项而是自适应系统的结构要求。BEDS框架将学习建模为在耗散约束下压缩信念状态的演化，证明了Fisher-Rao正则化是唯一热力学最优策略。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习将遗忘和正则化视为启发式工具，缺乏理论依据。本文旨在从热力学和信息论角度重新理解学习过程，将学习视为内在耗散过程，为遗忘和正则化提供理论基础。

Method: 提出BEDS（贝叶斯涌现耗散结构）框架，结合信息理论、热力学和信息几何学。引入条件最优性定理，证明Fisher-Rao正则化是唯一热力学最优正则化策略，将Ridge、SIGReg、EMA、SAC等方法统一为单个控制方程的特例。

Result: 证明了Fisher-Rao正则化在热力学上最优，而欧几里得正则化是结构上次优的。将过拟合解释为过度结晶，灾难性遗忘解释为耗散控制不足。区分了BEDS可结晶问题和BEDS可维持问题。

Conclusion: 学习应被重新定义为在耗散约束下维持可行信念状态的过程。该框架为遗忘、正则化和稳定性提供了原则性视角，适用于持续学习和多智能体系统，用可行性和稳定性替代渐近最优性作为主要标准。

Abstract: We propose a perspective in which learning is an intrinsically dissipative process. Forgetting and regularization are not heuristic add-ons but structural requirements for adaptive systems. Drawing on information theory, thermodynamics, and information geometry, we introduce the BEDS (Bayesian Emergent Dissipative Structures) framework, modeling learning as the evolution of compressed belief states under dissipation constraints.
  A central contribution is the Conditional Optimality Theorem, showing that Fisher-Rao regularization measuring change via information divergence rather than Euclidean distance is the unique thermodynamically optimal regularization strategy, achieving minimal dissipation. Euclidean regularization is shown to be structurally suboptimal. The framework unifies existing methods (Ridge, SIGReg, EMA, SAC) as special cases of a single governing equation.
  Within this view, overfitting corresponds to over-crystallization, while catastrophic forgetting reflects insufficient dissipation control. The framework distinguishes BEDS-crystallizable problems, where beliefs converge to stable equilibria, from BEDS-maintainable problems, which require continual adaptation. It extends naturally to continual and multi-agent systems, where viability, stability under adaptation and finite resources replaces asymptotic optimality as the primary criterion. Overall, this work reframes learning as maintaining viable belief states under dissipation constraints, providing a principled lens on forgetting, regularization, and stability.

</details>


### [251] [FedGraph-VASP: Privacy-Preserving Federated Graph Learning with Post-Quantum Security for Cross-Institutional Anti-Money Laundering](https://arxiv.org/abs/2601.17935)
*Daniel Commey,Matilda Nkoom,Yousef Alsenani,Sena G. Hounsinou,Garth V. Crosby*

Main category: cs.LG

TL;DR: FedGraph-VASP是一个用于虚拟资产服务提供商的隐私保护联邦图学习框架，通过边界嵌入交换协议在不暴露原始数据的情况下实现跨机构反洗钱检测，在连接性高的交易图中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 虚拟资产服务提供商在反洗钱检测中面临监管合规与用户隐私的根本矛盾：当前方法要么需要共享敏感交易数据，要么孤立运行导致跨链洗钱模式无法被检测。

Method: 提出FedGraph-VASP框架，核心是边界嵌入交换协议，只共享边界账户的压缩、不可逆图神经网络表示，并使用后量子密码学（Kyber-512和AES-256-GCM）确保安全。

Result: 在Elliptic比特币数据集上，FedGraph-VASP的F1分数达到0.508，优于FedSage+的0.453（提升12.1%）。在高连接性场景下接近集中式性能（F1=0.620），但在稀疏连接性下FedSage+表现更好。隐私审计显示嵌入仅部分可逆（R²=0.32）。

Conclusion: 存在拓扑依赖的权衡：嵌入交换在连接性高的交易图中表现优异，而生成式插补在高度模块化的稀疏图中占优势。该方法为VASPs提供了隐私保护的协作反洗钱解决方案。

Abstract: Virtual Asset Service Providers (VASPs) face a fundamental tension between regulatory compliance and user privacy when detecting cross-institutional money laundering. Current approaches require either sharing sensitive transaction data or operating in isolation, leaving critical cross-chain laundering patterns undetected. We present FedGraph-VASP, a privacy-preserving federated graph learning framework that enables collaborative anti-money laundering (AML) without exposing raw user data. Our key contribution is a Boundary Embedding Exchange protocol that shares only compressed, non-invertible graph neural network representations of boundary accounts. These exchanges are secured using post-quantum cryptography, specifically the NIST-standardized Kyber-512 key encapsulation mechanism combined with AES-256-GCM authenticated encryption. Experiments on the Elliptic Bitcoin dataset with realistic Louvain partitioning show that FedGraph-VASP achieves an F1-score of 0.508, outperforming the state-of-the-art generative baseline FedSage+ (F1 = 0.453) by 12.1 percent on binary fraud detection. We further show robustness under low-connectivity settings where generative imputation degrades performance, while approaching centralized performance (F1 = 0.620) in high-connectivity regimes. We additionally evaluate generalization on an Ethereum fraud detection dataset, where FedGraph-VASP (F1 = 0.635) is less effective under sparse cross-silo connectivity, while FedSage+ excels (F1 = 0.855), outperforming even local training (F1 = 0.785). These results highlight a topology-dependent trade-off: embedding exchange benefits connected transaction graphs, whereas generative imputation can dominate in highly modular sparse graphs. A privacy audit shows embeddings are only partially invertible (R^2 = 0.32), limiting exact feature recovery.

</details>


### [252] [Scaling Effects and Uncertainty Quantification in Neural Actor Critic Algorithms](https://arxiv.org/abs/2601.17954)
*Nikos Georgoudios,Konstantinos Spiliopoulos,Justin Sirignano*

Main category: cs.LG

TL;DR: 研究神经Actor-Critic算法在浅层神经网络下的收敛性和统计特性，重点关注网络宽度多项式缩放对算法输出的统计特征影响


<details>
  <summary>Details</summary>
Motivation: 先前研究关注收敛速度，本文转向更全面的统计特性分析，旨在量化神经Actor-Critic方法中的不确定性，为超参数选择提供理论指导

Method: 研究网络宽度的通用逆多项式缩放（指数在1/2到1之间可调），推导网络输出的渐近展开，分析其作为统计估计量的结构

Result: 方差随网络宽度以幂次衰减，指数为1/2减去缩放参数，当缩放参数接近1时统计鲁棒性更好；数值实验支持该行为并显示更快收敛

Conclusion: 分析结果为选择算法超参数（学习率、探索率）提供具体指导，确保可证明的有利统计行为，为神经Actor-Critic方法提供理论支撑

Abstract: We investigate the neural Actor Critic algorithm using shallow neural networks for both the Actor and Critic models. The focus of this work is twofold: first, to compare the convergence properties of the network outputs under various scaling schemes as the network width and the number of training steps tend to infinity; and second, to provide precise control of the approximation error associated with each scaling regime. Previous work has shown convergence to ordinary differential equations with random initial conditions under inverse square root scaling in the network width. In this work, we shift the focus from convergence speed alone to a more comprehensive statistical characterization of the algorithm's output, with the goal of quantifying uncertainty in neural Actor Critic methods. Specifically, we study a general inverse polynomial scaling in the network width, with an exponent treated as a tunable hyperparameter taking values strictly between one half and one. We derive an asymptotic expansion of the network outputs, interpreted as statistical estimators, in order to clarify their structure. To leading order, we show that the variance decays as a power of the network width, with an exponent equal to one half minus the scaling parameter, implying improved statistical robustness as the scaling parameter approaches one. Numerical experiments support this behavior and further suggest faster convergence for this choice of scaling. Finally, our analysis yields concrete guidelines for selecting algorithmic hyperparameters, including learning rates and exploration rates, as functions of the network width and the scaling parameter, ensuring provably favorable statistical behavior.

</details>


### [253] [TensorLens: End-to-End Transformer Analysis via High-Order Attention Tensors](https://arxiv.org/abs/2601.17958)
*Ido Andrew Atad,Itamar Zimerman,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: TensorLens提出了一种新颖的注意力张量表示方法，将整个Transformer模型统一为一个输入依赖的线性算子，解决了现有注意力分析局限于单头或单层的问题。


<details>
  <summary>Details</summary>
Motivation: 现有注意力分析大多关注单个注意力头或层，无法捕捉模型的全局行为。虽然已有研究尝试通过平均和矩阵乘法扩展注意力表示，或融入归一化和FFN等组件，但仍缺乏一个能够完整封装所有Transformer块的统一表示方法。

Method: 引入TensorLens，一种新颖的注意力-交互张量表示，将整个Transformer建模为单一输入依赖的线性算子。该张量联合编码注意力机制、FFN、激活函数、归一化层和残差连接，提供了理论一致且表达力强的线性表示。

Result: TensorLens在理论上具有坚实基础，实证验证表明它比先前的注意力聚合方法产生更丰富的表示。实验证明注意力张量可作为开发可解释性和模型理解工具的强大基础。

Conclusion: TensorLens填补了Transformer全局表示方法的空白，为注意力矩阵分析提供了统一框架，有望推动可解释性研究和模型理解工具的发展。

Abstract: Attention matrices are fundamental to transformer research, supporting a broad range of applications including interpretability, visualization, manipulation, and distillation. Yet, most existing analyses focus on individual attention heads or layers, failing to account for the model's global behavior. While prior efforts have extended attention formulations across multiple heads via averaging and matrix multiplications or incorporated components such as normalization and FFNs, a unified and complete representation that encapsulates all transformer blocks is still lacking. We address this gap by introducing TensorLens, a novel formulation that captures the entire transformer as a single, input-dependent linear operator expressed through a high-order attention-interaction tensor. This tensor jointly encodes attention, FFNs, activations, normalizations, and residual connections, offering a theoretically coherent and expressive linear representation of the model's computation. TensorLens is theoretically grounded and our empirical validation shows that it yields richer representations than previous attention-aggregation methods. Our experiments demonstrate that the attention tensor can serve as a powerful foundation for developing tools aimed at interpretability and model understanding. Our code is attached as a supplementary.

</details>


### [254] [Federated learning for unpaired multimodal data through a homogeneous transformer model](https://arxiv.org/abs/2601.17986)
*Anders Eklund*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的联邦学习框架，用于在多模态数据分散、未配对且私有的环境中训练全局多模态变换器模型。


<details>
  <summary>Details</summary>
Motivation: 现实联邦环境中，数据通常以未配对形式分散在不同节点（如图像在A节点，文本在B节点），且严格私有、无共享样本。现有联邦学习方法无法处理这种场景，因为它们假设本地客户端拥有对齐的数据对或需要共享原始特征嵌入，这会违反数据主权。

Method: 1) 引入小型公共锚点集来对齐不相关的私有流形；2) 使用基于这些公共锚点计算的Gram矩阵，通过中心核对齐强制跨模态语义对齐，无需传输私有样本；3) 提出子空间稳定微调方法处理大型变换器模型的联邦学习；4) 提出精度加权平均，利用高效获取的不确定性估计来降低不确定节点的权重。

Result: 该方法为联邦未配对基础模型建立了数学基础，使全局模型能够从分散、不相关且私有的数据孤岛中学习世界的统一表示，无需集中存储或配对样本。

Conclusion: 该框架解决了联邦环境中多模态数据未配对且私有的挑战，提供了比原型共享更优越的隐私保证，并通过几何对齐和不确定性加权实现了稳健的全局模型训练。

Abstract: Training of multimodal foundation models is currently restricted to centralized data centers containing massive, aligned datasets (e.g., image-text pairs). However, in realistic federated environments, data is often unpaired and fragmented across disjoint nodes; one node may hold sensor data, while another holds textual logs. These datasets are strictly private and share no common samples. Current federated learning (FL) methods fail in this regime, as they assume local clients possess aligned pairs or require sharing raw feature embeddings, which violates data sovereignty. We propose a novel framework to train a global multimodal transformer across decentralized nodes with disjoint modalities. We introduce a small public anchor set to align disjoint private manifolds. Using Gram matrices calculated from these public anchors, we enforce semantic alignment across modalities through centered kernel alignment without ever transmitting private samples, offering a mathematically superior privacy guarantee compared to prototype sharing. Further, we introduce a subspace-stabilized fine-tuning method to handle FL with huge transformer models. We strictly decouple domain-specific magnitude shifts from semantic direction, ensuring that nodes with varying sensor characteristics align geometrically to the global consensus. Lastly, we propose precision weighted averaging, where efficiently obtained uncertainty estimates are used to downweight uncertain nodes. This paper establishes the mathematical backbone for federated unpaired foundation models, enabling a global model to learn a unified representation of the world from fragmented, disjoint, and private data silos without requiring centralized storage or paired samples.

</details>


### [255] [Systematic Characterization of Minimal Deep Learning Architectures: A Unified Analysis of Convergence, Pruning, and Quantization](https://arxiv.org/abs/2601.17987)
*Ziwei Zheng,Huizhi Liang,Vaclav Snasel,Vito Latora,Panos Pardalos,Giuseppe Nicosia,Varun Ojha*

Main category: cs.LG

TL;DR: 提出一种系统性探索收敛性、剪枝和量化关系的计算方法，发现深度学习模型在不同架构下性能基本不变，学习动态呈现三个典型阶段，深层模型对剪枝更具鲁棒性，量化对参数少的模型影响更大。


<details>
  <summary>Details</summary>
Motivation: 深度学习网络在分类任务上表现出色，但确定能够可靠完成任务的最小架构仍然具有挑战性。需要系统性地探索和分析收敛性、剪枝和量化之间的关系，为在剪枝和低精度约束下选择紧凑稳定模型提供指导。

Method: 提出计算方法学：首先在大量架构上进行结构化设计扫描，然后在代表性模型上评估收敛行为、剪枝敏感性和量化鲁棒性。研究聚焦于复杂度递增的经典图像分类任务，涵盖深度神经网络、卷积神经网络和视觉Transformer。

Result: 尽管架构多样，性能基本不变；学习动态呈现三个典型阶段：不稳定、学习和过拟合。深层架构比浅层架构对剪枝更具鲁棒性，参数冗余高达60%；量化对参数少的模型影响更严重，对更难的图像数据集影响更大。

Conclusion: 这些发现为在图像分类任务中，在剪枝和低精度约束下选择紧凑稳定模型提供了可操作的指导。研究确认了深层架构的剪枝鲁棒性，并量化了减少数值精度对可训练参数的影响。

Abstract: Deep learning networks excel at classification, yet identifying minimal architectures that reliably solve a task remains challenging. We present a computational methodology for systematically exploring and analyzing the relationships among convergence, pruning, and quantization. The workflow first performs a structured design sweep across a large set of architectures, then evaluates convergence behavior, pruning sensitivity, and quantization robustness on representative models. Focusing on well-known image classification of increasing complexity, and across Deep Neural Networks, Convolutional Neural Networks, and Vision Transformers, our initial results show that, despite architectural diversity, performance is largely invariant and learning dynamics consistently exhibit three regimes: unstable, learning, and overfitting. We further characterize the minimal learnable parameters required for stable learning, uncover distinct convergence and pruning phases, and quantify the effect of reduced numeric precision on trainable parameters. Aligning with intuition, the results confirm that deeper architectures are more resilient to pruning than shallower ones, with parameter redundancy as high as 60%, and quantization impacts models with fewer learnable parameters more severely and has a larger effect on harder image datasets. These findings provide actionable guidance for selecting compact, stable models under pruning and low-precision constraints in image classification.

</details>


### [256] [Coding-Enforced Resilient and Secure Aggregation for Hierarchical Federated Learning](https://arxiv.org/abs/2601.17995)
*Shudi Weng,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: H-SecCoGC：一种集成编码策略的鲁棒分层安全聚合方案，可在不可靠通信下保障模型精度与隐私保护


<details>
  <summary>Details</summary>
Motivation: 分层联邦学习虽然改善了客户端与服务器间的链路质量，但在不可靠通信环境下，隐私噪声协调可能被随机破坏，难以同时保证模型精度和隐私保护

Method: 提出H-SecCoGC方案，集成编码策略强制结构化聚合，避免部分参与问题，在变化的隐私级别下确保准确的全局模型构建

Result: 理论分析和实验结果表明，该方案在任意强隐私保证下的不可靠通信环境中表现出优越性，显著提高了鲁棒性、隐私保护和学习效率

Conclusion: H-SecCoGC通过编码策略实现了结构化聚合，有效解决了分层联邦学习在不可靠通信下模型精度与隐私保护的权衡问题

Abstract: Hierarchical federated learning (HFL) has emerged as an effective paradigm to enhance link quality between clients and the server. However, ensuring model accuracy while preserving privacy under unreliable communication remains a key challenge in HFL, as the coordination among privacy noise can be randomly disrupted. To address this limitation, we propose a robust hierarchical secure aggregation scheme, termed H-SecCoGC, which integrates coding strategies to enforce structured aggregation. The proposed scheme not only ensures accurate global model construction under varying levels of privacy, but also avoids the partial participation issue, thereby significantly improving robustness, privacy preservation, and learning efficiency. Both theoretical analyses and experimental results demonstrate the superiority of our scheme under unreliable communication across arbitrarily strong privacy guarantees

</details>


### [257] [Spelling Bee Embeddings for Language Modeling](https://arxiv.org/abs/2601.18030)
*Markus N. Rabe,Judith Clymo,Zheren Dong*

Main category: cs.LG

TL;DR: 在嵌入层中融入拼写信息，提升模型在拼写和标准基准测试上的性能，相当于减少8%的计算和数据需求


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入层主要关注语义信息，而忽略了词汇的拼写特征。通过在嵌入层中融入拼写信息，可以提升模型对词汇结构的理解，进而改善整体性能。

Method: 对嵌入层进行简单修改，将词汇的拼写信息融入词嵌入表示中。具体来说，将词汇的拼写特征（如字符序列信息）与传统的语义嵌入相结合。

Result: 使用这种拼写增强嵌入的模型不仅在拼写任务上表现更好，在各种标准基准测试中也有显著提升。在40M到800M参数规模的模型上进行缩放研究显示，这种改进相当于需要减少约8%的计算和数据来达到相同的测试损失。

Conclusion: 在嵌入层中融入拼写信息是一种简单而有效的改进方法，能够提升语言模型的整体性能，同时降低训练成本，具有很好的实用价值。

Abstract: We introduce a simple modification to the embedding layer. The key change is to infuse token embeddings with information about their spelling. Models trained with these embeddings improve not only on spelling, but also across standard benchmarks. We conduct scaling studies for models with 40M to 800M parameters, which suggest that the improvements are equivalent to needing about 8% less compute and data to achieve the same test loss.

</details>


### [258] [Multimodal Machine Learning for Soft High-k Elastomers under Data Scarcity](https://arxiv.org/abs/2601.18032)
*Brijesh FNU,Viet Thanh Duy Nguyen,Ashima Sharma,Md Harun Rashid Molla,Chengyi Xu,Truong-Son Hy*

Main category: cs.LG

TL;DR: 作者创建了一个丙烯酸酯介电弹性体的高质量数据集，并提出一个多模态学习框架，利用预训练的聚合物表征进行小样本预测，以克服数据稀缺问题


<details>
  <summary>Details</summary>
Motivation: 随着软性可拉伸电子学的发展，需要同时具备高介电常数和低杨氏模量的介电弹性体。但目前缺乏系统涵盖分子序列、介电和机械性能的结构化数据集

Method: 1. 收集过去10年文献中的实验数据，创建丙烯酸酯介电弹性体的紧凑高质量数据集
2. 提出多模态学习框架，利用基于图和序列编码器的大规模预训练聚合物表征
3. 通过预训练嵌入转移丰富的化学和结构知识，实现从分子序列准确预测介电和机械性能的小样本学习

Result: 开发了一个新的范式，能够从预训练多模态模型转移知识来克服严重的数据稀缺问题。该方法可以轻松应用于其他聚合物骨架（如硅酮、聚氨酯），加速软性高介电常数弹性体的数据高效发现

Conclusion: 该研究通过创建结构化数据集和多模态学习框架，为介电弹性体的数据高效发现提供了新方法，能够加速软性高介电常数弹性体的开发，并公开了源代码和数据集

Abstract: Dielectric materials are critical building blocks for modern electronics such as sensors, actuators, and transistors. With the rapid recent advance in soft and stretchable electronics for emerging human- and robot-interfacing applications, there is a surging need for high-performance dielectric elastomers. However, it remains a grand challenge to develop soft elastomers that simultaneously possess high dielectric constants (k, related to energy storage capacity) and low Young's moduli (E, related to mechanical flexibility). While some new elastomer designs have been reported in individual (mostly one-off) studies, almost no structured dataset is currently available for dielectric elastomers that systematically encompasses their molecular sequence, dielectric, and mechanical properties. Within this context, we curate a compact, high-quality dataset of acrylate-based dielectric elastomers, one of the most widely explored elastomer backbones due to its versatile chemistry and molecular design flexibility, by screening and aggregating experimental results from the literature over the past 10 years. Building on this dataset, we propose a multimodal learning framework that leverages large-scale pretrained polymer representations from graph- and sequence-based encoders. These pretrained embeddings transfer rich chemical and structural knowledge from vast polymer corpora, enabling accurate few-shot prediction of both dielectric and mechanical properties from molecular sequences. Our results represent a new paradigm for transferring knowledge from pretrained multimodal models to overcome severe data scarcity, which can be readily translated to other polymer backbones (e.g., silicones, urethanes) and thus accelerate data-efficient discovery of soft high-k dielectric elastomers. Our source code and dataset are publicly available at https://github.com/HySonLab/Polymers

</details>


### [259] [Resonant Sparse Geometry Networks](https://arxiv.org/abs/2601.18064)
*Hasi Hays*

Main category: cs.LG

TL;DR: RSGN是一种受大脑启发的稀疏几何网络，在双曲空间中实现输入依赖的稀疏连接，计算复杂度为O(n*k)，比Transformer的O(n²)更高效。


<details>
  <summary>Details</summary>
Motivation: Transformer的密集注意力机制具有O(n²)计算复杂度，效率低下且不具生物合理性。受大脑稀疏连接和几何组织原理启发，需要开发更高效、生物合理的架构。

Method: 1. 在学习的双曲空间中嵌入计算节点，连接强度随测地距离衰减，实现输入依赖的动态稀疏性
2. 双时间尺度运行：快速可微分激活传播（梯度下降优化）和慢速结构学习（Hebbian启发式局部相关规则）
3. 数学分析证明计算复杂度为O(n*k)，其中k<<n为平均活跃邻域大小

Result: 1. 长程依赖任务：96.5%准确率，参数比标准Transformer少约15倍
2. 20类分层分类：23.8%准确率（随机基线5%），仅41,672参数；Transformer基线需要403,348参数达到30.1%准确率
3. 消融研究确认各组件贡献，Hebbian学习提供持续改进

Conclusion: 受大脑启发的稀疏、几何组织计算原理为开发更高效和生物合理的神经架构提供了有前景的方向，RSGN在保持性能的同时显著减少了参数和计算复杂度。

Abstract: We introduce Resonant Sparse Geometry Networks (RSGN), a brain-inspired architecture with self-organizing sparse
  hierarchical input-dependent connectivity. Unlike Transformer architectures that employ dense attention mechanisms with
  O(n^2) computational complexity, RSGN embeds computational nodes in learned hyperbolic space where connection strength
  decays with geodesic distance, achieving dynamic sparsity that adapts to each input. The architecture operates on two
  distinct timescales: fast differentiable activation propagation optimized through gradient descent, and slow
  Hebbian-inspired structural learning for connectivity adaptation through local correlation rules. We provide rigorous
  mathematical analysis demonstrating that RSGN achieves O(n*k) computational complexity, where k << n represents the average
  active neighborhood size. Experimental evaluation on hierarchical classification and long-range dependency tasks
  demonstrates that RSGN achieves 96.5% accuracy on long-range dependency tasks while using approximately 15x fewer
  parameters than standard Transformers. On challenging hierarchical classification with 20 classes, RSGN achieves 23.8%
  accuracy (compared to 5% random baseline) with only 41,672 parameters, nearly 10x fewer than the Transformer baselines
  which require 403,348 parameters to achieve 30.1% accuracy. Our ablation studies confirm the contribution of each architectural
  component, with Hebbian learning providing consistent improvements. These results suggest that brain-inspired principles
  of sparse, geometrically-organized computation offer a promising direction toward more efficient and biologically plausible
  neural architectures.

</details>


### [260] [Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming](https://arxiv.org/abs/2601.18076)
*Alexandra Chouldechova,A. Feder Cooper,Solon Barocas,Abhinav Palia,Dan Vann,Hanna Wallach*

Main category: cs.LG

TL;DR: 论文指出当前基于攻击成功率（ASR）比较来评估AI系统安全性或攻击方法效果的做法存在问题，许多结论缺乏有效证据支持，源于苹果与橘子的比较或低效度测量。


<details>
  <summary>Details</summary>
Motivation: 当前AI红队测试中，研究人员经常通过攻击成功率（ASR）的比较来得出关于系统相对安全性或攻击方法有效性的结论。然而，作者认为这些结论往往缺乏足够的证据支持，因为许多比较是苹果与橘子的对比或基于低效度的测量。这可能导致对AI安全性的错误评估和误导性的结论。

Method: 作者从社会科学测量理论和推断统计学的角度出发，提出了一个概念框架来回答"攻击成功率何时可以有意义地比较"这一问题。他们阐述了ASR可以和不可以在何种条件下进行有意义的比较。以越狱攻击为例，提供了苹果与橘子ASR比较的具体例子，并深入讨论了测量效度挑战。

Result: 通过概念、理论和实证分析，作者证明了当前许多基于ASR比较的结论存在问题。他们展示了这些比较往往缺乏有效的基础，可能是苹果与橘子的比较，或者使用了低效度的测量方法。这为AI安全评估领域提供了重要的方法论警示。

Conclusion: AI红队测试中基于攻击成功率比较得出的结论需要更加谨慎和严谨。研究人员必须考虑测量效度和比较的有效性条件，避免苹果与橘子的比较。本文提出的概念框架为未来更严谨的AI安全评估提供了方法论基础。

Abstract: We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.

</details>


### [261] [DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal](https://arxiv.org/abs/2601.18081)
*Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You*

Main category: cs.LG

TL;DR: DRPG是一个基于代理框架的学术反驳自动生成系统，通过分解、检索、规划和生成四个步骤，显著提升了反驳质量，在顶级会议数据上表现超过现有方法，达到人类平均水平以上。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在科研工作流中应用日益广泛，但学术反驳这一关键学术交流和同行评审环节的自动化支持仍未被充分探索。现有方法通常依赖现成LLM或简单流程，难以处理长文本理解，且无法生成有针对性和说服力的回应。

Method: 提出DRPG代理框架，包含四个步骤：1) 将评审意见分解为原子化关注点；2) 从论文中检索相关证据；3) 规划反驳策略；4) 据此生成回应。其中规划器能识别最可行的反驳方向，准确率超过98%。

Result: 在顶级会议数据上的实验表明，DRPG显著优于现有反驳流程，仅使用8B参数模型就实现了超越人类平均水平的性能。分析显示规划器设计有效，能提供多视角、可解释的建议，且在更复杂的多轮对话场景中表现良好。

Conclusion: DRPG展示了为学术讨论提供高质量反驳内容和支持规模化的潜力，其规划器设计尤其有效，能够提供多视角、可解释的建议，有助于提升学术交流质量。

Abstract: Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.

</details>


### [262] [LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts](https://arxiv.org/abs/2601.18089)
*Venmugil Elango,Nidhi Bhatia,Roger Waleffe,Rasoul Shafipour,Tomer Asida,Abhinav Khattar,Nave Assaf,Maximilian Golub,Joey Guman,Tiyasa Mitra,Ritchie Zhao,Ritika Borkar,Ran Zilberstein,Mostofa Patwary,Mohammad Shoeybi,Bita Rouhani*

Main category: cs.LG

TL;DR: LatentMoE是一种通过软硬件协同设计优化的MoE架构，在相同计算成本下比标准MoE实现更高的准确率，已被Nvidia Nemotron-3模型采用。


<details>
  <summary>Details</summary>
Motivation: 尽管MoE在大型语言模型中广泛应用，但现有架构在推理成本（按FLOP和参数衡量的准确率）方面是否达到最优仍不明确，需要从软硬件协同设计角度重新审视。

Method: 采用软硬件协同设计方法，分析不同部署场景下的性能瓶颈，通过经验设计和理论分析探索设计空间，开发出LatentMoE架构，并在高达950亿参数和1万亿token的训练规模下进行验证。

Result: LatentMoE在每FLOP和每参数准确率方面持续优于标准MoE架构，其优势已被Nvidia Nemotron-3 Super和Ultra模型采用，并成功扩展到更大规模。

Conclusion: 通过系统性的软硬件协同设计探索，开发出的LatentMoE架构显著提升了MoE的效率，为大型语言模型提供了更优的推理成本-准确率权衡，具有实际应用价值。

Abstract: Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).

</details>


### [263] [From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models](https://arxiv.org/abs/2601.18091)
*Longwei Ding,Anhao Zhao,Fanghua Ye,Ziyang Chen,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 该研究对比了指令跟随型LLM和推理增强型LLM的剪枝策略差异，发现不同范式需要不同的剪枝方法，深度剪枝在分类任务表现更好，宽度剪枝在生成和推理任务更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝研究主要针对指令跟随型LLM，不清楚这些策略是否适用于生成长中间推理轨迹的推理增强型模型。需要系统研究不同范式LLM的剪枝特性差异。

Method: 对指令跟随型LLM和推理增强型LLM进行控制性剪枝研究，对齐剪枝校准和后剪枝恢复数据与原始训练分布，评估静态深度剪枝、静态宽度剪枝和动态剪枝在17个任务上的表现。

Result: 发现明显的范式依赖差异：深度剪枝在分类任务表现更好，宽度剪枝在生成和推理任务更鲁棒；静态剪枝能更好保持推理性能，动态剪枝在分类和生成任务表现优异但在长链推理中仍有挑战。

Conclusion: 推理增强型LLM需要专门考虑其特性的剪枝策略，不能简单沿用指令跟随型模型的剪枝方法。剪枝策略应针对不同模型范式和任务类型进行定制化设计。

Abstract: Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\textbf{LLM-instruct}$) and reasoning-augmented ($\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.

</details>


### [264] [AttenMIA: LLM Membership Inference Attack through Attention Signals](https://arxiv.org/abs/2601.18110)
*Pedram Zaree,Md Abdullah Al Mamun,Yue Dong,Ihsen Alouani,Nael Abu-Ghazaleh*

Main category: cs.LG

TL;DR: AttenMIA是一种新的成员推理攻击框架，利用Transformer模型中的自注意力模式来推断训练数据成员身份，相比基于输出置信度或嵌入特征的方法，在隐私攻击效果上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在实际应用中的广泛部署，其训练数据记忆倾向引发了严重的隐私和知识产权担忧。现有的成员推理攻击主要依赖输出置信度或嵌入特征，但这些信号往往脆弱，攻击成功率有限，需要更有效的攻击方法。

Method: 提出AttenMIA框架，利用Transformer模型内部的自注意力模式来推断成员身份。注意力机制控制Transformer内部的信息流动，暴露不同的记忆模式。该方法从各层注意力头提取信息，结合基于扰动的差异度量来训练有效的MIA分类器。

Result: 在LLaMA-2、Pythia和Opt等开源模型上的实验表明，基于注意力的特征始终优于基线方法，特别是在低误报率指标下表现突出（如在WikiMIA-32基准测试中，使用Llama2-13b达到0.996 ROC AUC和87.9% TPR@1%FPR）。注意力信号在不同数据集和架构上具有泛化能力，层和头级分析揭示了成员信息泄漏最显著的位置。在数据提取框架中使用AttenMIA替代其他成员推理攻击，实现了优于现有技术的训练数据提取攻击。

Conclusion: 研究发现，最初为增强可解释性而引入的注意力机制可能无意中放大了大语言模型的隐私风险，这强调了开发新防御机制的必要性。AttenMIA框架展示了利用模型内部机制进行隐私攻击的有效性，为未来的安全研究提供了重要启示。

Abstract: Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.

</details>


### [265] [Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting](https://arxiv.org/abs/2601.18111)
*Jean Kossaifi,Nikola Kovachki,Morteza Mardani,Daniel Leibovici,Suman Ravuri,Ira Shokar,Edoardo Calvello,Mohammad Shoaib Abbas,Peter Harrington,Ashay Subramaniam,Noah Brenowitz,Boris Bonev,Wonmin Byeon,Karsten Kreis,Dale Durran,Arash Vahdat,Mike Pritchard,Jan Kautz*

Main category: cs.LG

TL;DR: 本文提出一个简单可扩展的天气预测框架，通过下采样潜在空间和历史条件局部投影器实现多尺度大气动力学学习，无需复杂架构或专门训练策略就能达到最先进的概率预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的天气预报方法存在架构复杂、训练策略碎片化的问题，难以识别预测准确性的根本驱动因素。本文旨在证明实现最先进的概率预测技能不需要复杂的架构约束或专门的训练启发式方法。

Method: 引入可扩展框架，结合直接下采样的潜在空间和历史条件局部投影器来学习多尺度大气动力学。该框架对概率估计器选择具有鲁棒性，支持随机插值、扩散模型和CRPS集成训练等多种方法。

Result: 与集成预报系统（IFS）和深度学习概率模型GenCast相比，该框架在大多数变量上实现了统计显著改进，表明通用模型的扩展足以实现最先进的中期预测。

Conclusion: 扩展通用模型足以实现最先进的天气预报，无需定制化训练方案，且在各种概率框架中均有效，简化了天气预测系统的设计和训练。

Abstract: The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.

</details>


### [266] [Robust Learning of a Group DRO Neuron](https://arxiv.org/abs/2601.18115)
*Guyang Cao,Shuyao Li,Sushrut Karmalkar,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 该论文研究在存在任意标签噪声和组级分布偏移的情况下，学习单个神经元的分布鲁棒优化问题，提出计算高效的原始-对偶算法，在LLM预训练基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在现实机器学习应用中，数据常常存在标签噪声和分布偏移，特别是在多组数据分布下，传统学习方法可能对最坏情况表现不佳。需要开发能够处理任意标签噪声和组级分布偏移的鲁棒学习方法。

Method: 提出一个组分布鲁棒优化（Group DRO）框架，通过最小化最坏情况目标（凸组合的组分布）来学习最佳拟合神经元参数。开发计算高效的原始-对偶算法，使用对偶外推更新来处理损失函数的固有非凸性，并引入f-散度惩罚偏离均匀组权重的情况。

Result: 算法输出的向量ŵ在常数因子内与最优参数w*竞争，在最坏情况组权重下具有鲁棒学习保证。在LLM预训练基准测试中，由算法框架启发的对偶外推更新实现显示出良好前景。

Conclusion: 该研究为处理任意标签噪声和组特定分布偏移提供了理论保证和实用算法，在存在分布偏移的现实场景中实现了鲁棒的神经元学习，并在大规模语言模型预训练中显示出应用潜力。

Abstract: We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\mathcal p_{[1]},\dots,\mathcal p_{[K]}$, we seek to approximate $\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\boldsymbolλ \in Δ_K$, where the objective is $\sum_{i \in [K]}λ_{[i]}\,\mathbb E_{(\mathbf x,y)\sim\mathcal p_{[i]}}(σ(\mathbf w\cdot\mathbf x)-y)^2 - νd_f(\boldsymbolλ,\frac{1}{K}\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $ν\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\widehat{\mathbf w}$ that is constant-factor competitive with $\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.

</details>


### [267] [Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods](https://arxiv.org/abs/2601.18142)
*Mingxu Zhang,Huicheng Zhang,Jiaming Ji,Yaodong Yang,Ying Sun*

Main category: cs.LG

TL;DR: 本文提出了一种基于自抗扰控制的拉格朗日方法（ADRC-Lagrangian），用于安全强化学习，显著减少了安全违规和振荡问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法（如PID和经典拉格朗日方法）存在参数敏感性和相位滞后问题，导致振荡和频繁的安全违规，需要更鲁棒的解决方案。

Method: 提出ADRC-Lagrangian方法，利用自抗扰控制（ADRC）增强鲁棒性并减少振荡，该框架将经典和PID拉格朗日方法作为特例包含在内。

Result: 实验表明，该方法将安全违规减少74%，约束违规幅度降低89%，平均成本降低67%，在复杂环境中表现出优越的安全性能。

Conclusion: ADRC-Lagrangian方法为安全强化学习提供了更鲁棒、更有效的解决方案，显著改善了安全约束的满足性能。

Abstract: Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\%, establishing superior effectiveness for Safe RL in complex environments.

</details>


### [268] [FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning](https://arxiv.org/abs/2601.18150)
*Zhaopeng Qiu,Shuang Yu,Jingqi Zhang,Shuai Zhang,Xue Huang,Jingyi Yang,Junjie Lai*

Main category: cs.LG

TL;DR: 本文提出了一种针对大语言模型强化学习的FP8推理栈，通过量化技术和KV缓存优化，在保持学习性能的同时提升44%的推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型强化学习中的推理（生成）环节正成为性能瓶颈，长输出序列导致注意力机制和KV缓存占用大量内存和时间。FP8低精度计算虽能加速推理，但在RL场景中面临独特挑战：策略权重频繁变化需要重复量化，低精度推理可能偏离高精度训练策略，导致训练-推理不匹配和不稳定性。

Method: 1. 使用分块FP8量化实现W8A8线性层推理；2. 通过每步QKV尺度重新校准将FP8扩展到KV缓存，解决长上下文内存瓶颈；3. 使用基于重要性采样的推理修正技术（token级TIS/MIS变体）缓解训练-推理不匹配问题。

Result: 该FP8推理栈在密集模型和MoE模型上实现了高达44%的推理吞吐量提升，同时保持了与BF16基线相当的学习行为。

Conclusion: 提出的FP8推理栈成功解决了RL场景中的工程和算法挑战，实现了显著的性能提升而不损害学习效果，为大规模语言模型强化学习的效率优化提供了实用解决方案。

Abstract: Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.

</details>


### [269] [Learning Fair Domain Adaptation with Virtual Label Distribution](https://arxiv.org/abs/2601.18171)
*Yuguang Zhang,Lijun Sheng,Jian Liang,Ran He*

Main category: cs.LG

TL;DR: 提出VILL框架，通过自适应重加权和基于KL散度的再平衡策略，提升无监督域自适应中的类别公平性，改善最差类别性能同时保持高整体准确率。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法大多关注整体准确率提升，却忽视了不同类别间的性能差异（类别公平性问题）。实证分析发现UDA分类器倾向于偏好某些容易类别而忽视困难类别。

Method: 提出虚拟标签分布感知学习（VILL）框架，包含：1）自适应重加权策略，增强难以分类类别的影响；2）基于KL散度的再平衡策略，显式调整决策边界以提升类别公平性。

Result: 在常用数据集上的实验表明，VILL可以作为即插即用模块无缝集成到现有UDA方法中，显著改善类别公平性。

Conclusion: VILL是一个简单有效的框架，能够在保持高整体准确率的同时提升最差类别性能，解决了无监督域自适应中的类别公平性问题。

Abstract: Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.

</details>


### [270] [Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients](https://arxiv.org/abs/2601.18189)
*Rui Wu,Yongjun Li*

Main category: cs.LG

TL;DR: 提出一种名为SPG-AHOC的新方法，通过混合阶无环约束和光滑近端梯度优化，保证在有限迭代内精确恢复DAG结构，无需后处理阈值处理。


<details>
  <summary>Details</summary>
Motivation: 现有连续优化方法（如NOTEARS）只能保证渐近收敛到平稳点，通常产生稠密权重矩阵，需要任意的后处理阈值处理才能恢复DAG。连续优化与离散图结构之间的差距是根本性挑战。

Method: 提出混合阶无环约束（AHOC），并通过光滑近端梯度（SPG-AHOC）进行优化。利用近端算法的流形识别特性，确保在有限迭代内精确恢复DAG支持（结构）。

Result: 理论证明：在标准可识别性假设下，SPG-AHOC在有限迭代内恢复精确的DAG支持，无需启发式截断。实证结果：SPG-AHOC达到最先进的准确性，并强有力地支持有限时间识别理论。

Conclusion: SPG-AHOC成功弥合了连续优化与离散图结构之间的差距，通过有限时间预言性质提供理论保证，消除了结构模糊性，返回具有精确零条目的图。

Abstract: Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.

</details>


### [271] [HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models](https://arxiv.org/abs/2601.18200)
*Chenyu Zhang,Xinchen Lyu,Chenshan Ren,Shuhan Liu,Qimei Cui,Xiaofeng Tao*

Main category: cs.LG

TL;DR: 提出了HeterCSI框架，通过梯度动态分析解决CSI数据在尺度和场景双重异质性下的预训练挑战，实现高效训练和强大跨场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 无线基础模型在处理CSI数据时面临尺度和场景双重异质性的挑战，现有预训练方法要么限制输入固定维度，要么按尺度隔离训练，限制了模型的泛化能力和可扩展性。

Method: 1) 通过梯度动态分析发现尺度异质性导致破坏性梯度干扰，而场景多样性在适当管理下促进建设性梯度对齐；2) 将异构CSI批次构建公式化为最小化零填充开销同时保持场景多样性的分区优化问题；3) 开发尺度感知自适应批处理策略对齐相似尺度的CSI样本；4) 设计双掩码机制隔离有效信号和填充伪影。

Result: 在12个数据集上的实验表明，HeterCSI无需场景特定微调即可建立通用基础模型，在CSI重建、时域预测和频域预测任务上分别比WiFo基准降低NMSE 7.19 dB、4.08 dB和5.27 dB。相比现有方法减少53%训练延迟，平均提升泛化性能1.53 dB。

Conclusion: HeterCSI框架通过梯度动态分析和自适应批处理策略，成功解决了CSI数据双重异质性带来的预训练挑战，实现了训练效率与跨场景泛化能力的平衡，为无线基础模型的发展提供了新思路。

Abstract: Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.

</details>


### [272] [PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR](https://arxiv.org/abs/2601.18207)
*James Burgess,Jan N. Hansen,Duo Peng,Yuhui Zhang,Alejandro Lozano,Min Woo Sun,Emma Lundberg,Serena Yeung-Levy*

Main category: cs.LG

TL;DR: 提出了一种训练AI智能体在科学论文中搜索和推理的方法，创建了包含1600万篇生物医学论文摘要的搜索语料库和包含6万个样本的PaperSearchQA问答数据集，训练出的搜索智能体超越了非强化学习检索基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习可验证奖励的搜索智能体主要针对通用领域问答，与科学、工程和医学等专业技术AI系统相关性有限。需要开发能够搜索和推理科学论文的智能体，这对于技术问答测试、实际科学应用以及未来AI科学家系统至关重要。

Method: 构建了包含1600万篇生物医学论文摘要的搜索语料库，创建了基于该语料库的PaperSearchQA问答数据集（包含6万个样本），使用Search-R1代码库进行强化学习可验证奖励训练，训练搜索智能体在该环境中执行任务。

Result: 训练的搜索智能体在性能上超越了非强化学习检索基线，智能体展现出规划、推理和自我验证等有趣行为，证明了该方法在技术问答领域的有效性。

Conclusion: 该方法成功实现了在科学论文中进行搜索和推理的AI智能体训练，创建的数据集和基准可扩展到其他科学领域，为未来AI科学家系统的发展提供了重要基础。

Abstract: Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.

</details>


### [273] [Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting](https://arxiv.org/abs/2601.18231)
*Trong Khiem Tran,Manh Cuong Dao,Phi Le Nguyen,Thao Nguyen Truong,Trong Nghia Hoang*

Main category: cs.LG

TL;DR: 提出一个理论框架来分析预训练模型适应新特征模态时特征对齐与目标拟合的交互作用，通过特征标签扭曲概念建立可证明的泛化界，指导算法设计并显著提升性能


<details>
  <summary>Details</summary>
Motivation: 预训练模型适应未见特征模态的需求日益增长，但特征对齐与目标微调的无校准组合会加剧源与目标特征标签结构间的错配，降低目标泛化能力，现有工作缺乏对这一关键交互的理论理解

Method: 开发一个理论框架，建立目标误差的可证明泛化界，通过特征标签扭曲的新概念解释特征对齐与目标拟合的交互作用，为实际算法设计提供可操作的优化指导

Result: 该方法在广泛的基准数据集上显著优于最先进的方法，实现了性能的显著提升

Conclusion: 通过理论框架分析特征对齐与目标拟合的交互作用，为跨模态知识迁移提供了理论基础和实用指导，显著提升了预训练模型适应新特征模态的性能

Abstract: Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.

</details>


### [274] [Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity](https://arxiv.org/abs/2601.18245)
*Santanu Das,Jatin Batra*

Main category: cs.LG

TL;DR: 该论文提出了首个多项式时间算法，用于解决在存在重尾噪声和对抗性破坏情况下的鲁棒相位恢复问题，样本复杂度接近线性。


<details>
  <summary>Details</summary>
Motivation: 相位恢复是一个经典问题，在光学、晶体学、天体物理学等领域有广泛应用。现有算法在存在重尾噪声和对抗性破坏（测量值和传感向量都可能被破坏）时效率低下，需要指数时间。需要开发高效的多项式时间算法来解决这一问题。

Method: 通过建立鲁棒谱初始化与鲁棒主成分分析（PCA）之间的联系，利用鲁棒PCA的最新算法进展来实现高效的谱初始化，从而开发出多项式时间的鲁棒相位恢复算法。

Result: 提出了首个多项式时间算法，能够在存在重尾噪声和对抗性破坏的情况下进行鲁棒相位恢复，样本复杂度为接近线性的O(n log n)。

Conclusion: 通过连接鲁棒谱初始化和鲁棒PCA的算法进展，成功解决了鲁棒相位恢复的计算效率问题，为实际应用提供了可行的解决方案。

Abstract: Phase retrieval is the classical problem of recovering a signal $x^* \in \mathbb{R}^n$ from its noisy phaseless measurements $y_i = \langle a_i, x^* \rangle^2 + ζ_i$ (where $ζ_i$ denotes noise, and $a_i$ is the sensing vector) for $i \in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.

</details>


### [275] [Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs](https://arxiv.org/abs/2601.18255)
*Fei Meng*

Main category: cs.LG

TL;DR: 研究发现经验回放在大语言模型持续学习中存在二重性：对非结构化任务产生正向迁移，但对结构化任务（如代码生成）造成严重负向迁移。作者提出正交子空间唤醒（OSW）方法，通过识别关键参数子空间并强制正交更新，保护脆弱知识结构。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续学习中需要平衡稳定性（保留旧知识）和可塑性（学习新任务）。经验回放是应对灾难性遗忘的标准方法，但其对不同能力的影响尚未得到充分探索。研究发现经验回放存在一个关键二分现象，需要解决这种结构性安全与广泛整合之间的权衡。

Method: 提出正交子空间唤醒（OSW）方法：1）通过简短的"唤醒"阶段识别先前任务的关键参数子空间；2）对新任务强制执行正交更新；3）为已建立的知识结构提供数学基础的"安全保证"。这种方法旨在保护脆弱的结构化知识同时保持对新任务的学习能力。

Result: 在四个任务的序列实验中，OSW在经验回放失败的情况下成功保护了脆弱的编码能力，同时保持了新任务的高可塑性。经验回放对非结构化NLP分类任务产生正向迁移，但对代码生成等结构化领域造成显著的负向迁移（编码准确率相对下降）。

Conclusion: 大语言模型持续学习需要同时评估结构安全和平均保留。正交子空间唤醒（OSW）方法通过数学保证的知识结构保护，解决了经验回放中的结构性安全与广泛整合之间的权衡问题，为保护脆弱能力提供了有效解决方案。

Abstract: Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief "wake-up" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded "safety guarantee" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.

</details>


### [276] [FGGM: Fisher-Guided Gradient Masking for Continual Learning](https://arxiv.org/abs/2601.18261)
*Chao-Hong Tan,Qian Chen,Wen Wang,Yukun Ma,Chong Zhang,Chong Deng,Qinglin Zhang,Xiangang Li,Jieping Ye*

Main category: cs.LG

TL;DR: FGGM框架通过Fisher信息指导的梯度掩码缓解大语言模型持续学习中的灾难性遗忘问题，在TRACE基准上相比SFT提升9.6%，比MIGU提升4.4%


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在持续学习过程中面临的灾难性遗忘问题，现有基于参数幅度的掩码方法（如MIGU）缺乏数学理论基础，需要更原则性的参数重要性评估方法

Method: 提出Fisher-Guided Gradient Masking (FGGM)框架，使用对角Fisher信息矩阵策略性地选择需要更新的参数，动态生成具有自适应阈值的二进制掩码，在不需要历史数据的情况下保护关键参数，平衡稳定性和可塑性

Result: 在TRACE基准测试中，FGGM相比监督微调(SFT)在保持通用能力方面相对提升9.6%，相比MIGU在TRACE任务上提升4.4%；在代码生成任务上的额外分析证实了FGGM的优越性能和减少遗忘的效果

Conclusion: FGGM作为一种基于数学原理的参数重要性估计方法，通过Fisher信息指导的梯度掩码有效缓解灾难性遗忘，是平衡稳定性和可塑性的有效解决方案

Abstract: Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.

</details>


### [277] [Neural Network Approximation: A View from Polytope Decomposition](https://arxiv.org/abs/2601.18264)
*ZeYu Li,ShiJun Zhang,TieYong Zeng,FengLei Fan*

Main category: cs.LG

TL;DR: 本文提出了一种基于多面体分解的ReLU网络通用逼近理论，通过考虑目标函数的局部正则性，相比传统均匀划分方法更加高效灵活。


<details>
  <summary>Details</summary>
Motivation: 现有通用逼近理论大多通过均匀划分输入空间来构造神经网络，忽略了目标函数的局部正则性。本文希望从多面体分解的角度研究ReLU网络的通用逼近能力，提供更现实、任务导向的方法。

Method: 1. 开发显式核多项式方法获得连续函数的通用逼近，特征包括精细的Totik-Ditzian型连续模和多面体域分解；2. 在每个子域中分别构造ReLU网络来逼近核多项式；3. 将方法扩展到解析函数以获得更高逼近率。

Result: 多面体分解使逼近在许多情况下比现有方法更高效灵活，特别是在目标函数的奇异点附近。方法可扩展到解析函数达到更高逼近率。

Conclusion: 通过多面体分解视角研究ReLU网络的通用逼近能力，提供了比传统均匀划分方法更现实、任务导向的逼近框架，特别在处理函数奇异点时表现出优势。

Abstract: Universal approximation theory offers a foundational framework to verify neural network expressiveness, enabling principled utilization in real-world applications. However, most existing theoretical constructions are established by uniformly dividing the input space into tiny hypercubes without considering the local regularity of the target function. In this work, we investigate the universal approximation capabilities of ReLU networks from a view of polytope decomposition, which offers a more realistic and task-oriented approach compared to current methods. To achieve this, we develop an explicit kernel polynomial method to derive an universal approximation of continuous functions, which is characterized not only by the refined Totik-Ditzian-type modulus of continuity, but also by polytopical domain decomposition. Then, a ReLU network is constructed to approximate the kernel polynomial in each subdomain separately. Furthermore, we find that polytope decomposition makes our approximation more efficient and flexible than existing methods in many cases, especially near singular points of the objective function. Lastly, we extend our approach to analytic functions to reach a higher approximation rate.

</details>


### [278] [What Do Learned Models Measure?](https://arxiv.org/abs/2601.18278)
*Indrė Žliobaitė*

Main category: cs.LG

TL;DR: 机器学习模型作为测量工具时，标准评估标准（如泛化误差、校准性、鲁棒性）无法保证测量稳定性，需要新的评估维度


<details>
  <summary>Details</summary>
Motivation: 在许多科学和数据驱动应用中，机器学习模型越来越多地被用作测量仪器，而不仅仅是预定义标签的预测器。当测量函数从数据中学习时，从观测到数量的映射由训练分布和归纳偏好隐式决定，允许多个不等价的映射满足标准预测评估标准

Method: 将学习到的测量函数作为评估的独特焦点，引入测量稳定性概念，该概念捕捉学习过程的可实现性和跨上下文中测量量的不变性。通过真实世界案例研究展示问题

Result: 研究表明，具有可比预测性能的模型可以实现系统上不等价的测量函数，分布偏移为这种失败提供了具体例证。标准评估标准无法保证测量稳定性

Conclusion: 现有评估框架在将学习到的模型输出识别为测量的设置中存在局限性，需要额外的评估维度来确保测量稳定性

Abstract: In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.

</details>


### [279] [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292)
*Zhewen Tan,Wenhan Yu,Jianfeng Si,Tongxin Liu,Kaiqi Guan,Huiyan Jin,Jiawen Tao,Xiaokun Yuan,Duohe Ma,Xiangzheng Zhang,Tong Yang,Lin Sun*

Main category: cs.LG

TL;DR: TriPlay-RL：一个基于强化学习的闭环框架，通过攻击者、防御者和评估者三个角色的协同进化，实现大语言模型安全对齐的迭代优化，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型的安全风险日益突出，需要有效减轻有害内容生成。当前主流安全对齐范式通常采用三个角色协作框架，但缺乏系统化的协同进化机制。

Method: 提出TriPlay-RL闭环强化学习框架，让攻击者、防御者和评估者三个角色在统一学习循环中迭代协同改进。攻击者生成对抗性提示，防御者进行安全防御，评估者评估响应质量，三者通过强化学习共同进化。

Result: 攻击者在保持高输出多样性的同时，对抗效果提升20%-50%；防御者安全性能获得10%-30%提升且不损害一般推理能力；评估者通过迭代持续提升细粒度判断能力，能准确区分不安全响应、简单拒绝和有用指导。

Conclusion: TriPlay-RL为大语言模型安全对齐建立了一个高效可扩展的范式，能够在统一学习循环中实现三个角色的持续协同进化，为LLM安全对齐提供了系统化解决方案。

Abstract: In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.

</details>


### [280] [A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods](https://arxiv.org/abs/2601.18314)
*Lina Felsner,Sevgi G. Kafali,Hannah Eichhorn,Agnes A. J. Leth,Aidas Batvinskas,Andre Datchev,Fabian Klemm,Jan Aulich,Puntika Leepagorn,Ruben Klinger,Daniel Rueckert,Julia A. Schnabel*

Main category: cs.LG

TL;DR: 该论文报告了一个学生可复现性黑客马拉松，旨在复现三篇有影响力的MRI重建论文的结果，并总结了构建可复现代码库的基本实践。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是通过学生黑客马拉松的形式，系统地评估和复现MRI重建领域的关键方法，同时探索建立可复现代码库的最佳实践。

Method: 组织学生黑客马拉松，参与者尝试复现三篇MRI重建论文：(1) MoDL（基于模型的学习去噪展开网络）; (2) HUMUS-Net（混合展开多尺度CNN+Transformer架构）; (3) 基于定量MR模型的未训练、物理正则化动态MRI方法。研究记录了黑客马拉松的设置、复现过程和额外实验。

Result: 报告了黑客马拉松的复现结果和额外实验发现，并详细阐述了构建可复现代码库的基本实践方法。

Conclusion: 通过黑客马拉松的形式可以有效评估和促进MRI重建方法的可复现性，同时总结出的实践指南有助于建立更健壮、可复现的代码库。

Abstract: We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.

</details>


### [281] [Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals](https://arxiv.org/abs/2601.18326)
*Jie Li,Jing Li,Lu Lv,Zhanyu Ju,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于ZC序列和时频图像认知融合的无人机信号OOD检测算法，在RID任务中实现多模态特征融合与自适应注意力加权，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决无人机远程识别中未知或非标准通信协议信号的检测问题，需要处理多种信号特征，提高OOD检测的准确性和鲁棒性。

Method: 结合ZC序列和时频图像双模态特征：1)从接收信号中提取两种特征；2)通过专用模块增强和对齐特征；3)进行多模态特征交互、单模态融合和多模态融合；4)计算空间和通道维度的判别分数并转换为自适应注意力权重；5)加权特征通过Softmax分类。

Result: 仿真结果显示，该算法优于现有方法，在RID和OODD指标上分别提升1.7%和7.5%，在不同飞行条件和无人机类型下表现出强鲁棒性。

Conclusion: 基于ZC序列和时频图像认知融合的OOD检测算法有效整合了多模态信息，显著提升了无人机信号识别的准确性和鲁棒性，适用于复杂实际场景。

Abstract: We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.

</details>


### [282] [Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection](https://arxiv.org/abs/2601.18329)
*Chuhan Feng,Jing Li,Jie Li,Lu Lv,Fengkui Gong*

Main category: cs.LG

TL;DR: 提出基于可区分性驱动的空间-通道选择和梯度范数的无人机信号离群检测算法，通过协议特定时频特征自适应加权特征，结合梯度范数度量扰动敏感性，提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机信号检测中，离群（OOD）样本的识别对确保系统鲁棒性至关重要。现有方法在区分正常与异常信号方面存在局限，特别是在复杂射频环境中，需要更有效的特征选择和稳定性度量机制。

Method: 1. 基于可区分性驱动的空间-通道选择：利用协议特定的时频特征，通过量化类间相似性和方差，自适应加权时频图像特征的空间和通道维度。2. 梯度范数度量：引入梯度范数来测量扰动敏感性，捕捉OOD样本的内在不稳定性。3. 联合推理：将梯度范数度量与基于能量的评分融合进行联合推断。

Result: 仿真结果表明，该算法在不同信噪比（SNR）和多种无人机类型下，展现出优异的判别能力和鲁棒性能，显著提升了无人机信号OOD检测的效果。

Conclusion: 提出的基于可区分性驱动空间-通道选择和梯度范数的算法，通过自适应特征加权和扰动敏感性度量，有效提升了无人机信号离群检测的判别能力和鲁棒性，为复杂射频环境下的异常信号识别提供了有效解决方案。

Abstract: We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.

</details>


### [283] [Structural Gender Bias in Credit Scoring: Proxy Leakage](https://arxiv.org/abs/2601.18342)
*Navya SD,Sreekanth D,SS Uma Sankari*

Main category: cs.LG

TL;DR: 该研究通过台湾信用违约数据集审计发现，即使移除敏感属性并应用行业标准公平干预，性别偏见仍通过婚姻状况、年龄、信用额度等非敏感特征作为代理变量持续存在，传统公平审计无法检测这种结构性偏见。


<details>
  <summary>Details</summary>
Motivation: 随着金融机构越来越多地采用机器学习进行信用风险评估，算法偏见的持续存在成为实现公平金融包容性的关键障碍。研究旨在挑战"通过盲视实现公平"的主流观点，揭示即使移除显式受保护属性，性别偏见仍通过非敏感特征持续存在的现象。

Method: 使用SHAP（SHapley Additive exPlanations）识别婚姻状况、年龄、信用额度等作为性别代理变量的特征；采用对抗性逆建模框架数学量化信息泄漏，从纯粹的非敏感金融特征中重建受保护的性别属性。

Result: 研究发现性别预测信号深度嵌入非敏感特征中，保护性别属性可以从纯粹的非敏感金融特征中以ROC AUC得分0.65重建，表明传统公平审计不足以检测隐式结构性偏见。

Conclusion: 研究主张从表面统计平等转向因果感知建模和结构性问责，金融AI需要更深入的方法来解决嵌入在非敏感特征中的结构性偏见，而非仅仅移除显式敏感属性。

Abstract: As financial institutions increasingly adopt machine learning for credit risk assessment, the persistence of algorithmic bias remains a critical barrier to equitable financial inclusion. This study provides a comprehensive audit of structural gender bias within the Taiwan Credit Default dataset, specifically challenging the prevailing doctrine of "fairness through blindness." Despite the removal of explicit protected attributes and the application of industry standard fairness interventions, our results demonstrate that gendered predictive signals remain deeply embedded within non-sensitive features. Utilizing SHAP (SHapley Additive exPlanations), we identify that variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender, allowing models to maintain discriminatory pathways while appearing statistically fair. To mathematically quantify this leakage, we employ an adversarial inverse modeling framework. Our findings reveal that the protected gender attribute can be reconstructed from purely non-sensitive financial features with an ROC AUC score of 0.65, demonstrating that traditional fairness audits are insufficient for detecting implicit structural bias. These results advocate for a shift from surface-level statistical parity toward causal-aware modeling and structural accountability in financial AI.

</details>


### [284] [Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning](https://arxiv.org/abs/2601.18356)
*Weiqin Yang,Haowen Xue,Qingyi Peng,Hexuan Hu,Qian Huang,Tingbo Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种多模态因果检索增强生成框架，通过整合因果推理原理与多模态检索，改进医学视觉语言模型，使其超越表面相关性，基于因果机制进行推理。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型主要依赖表面统计关联而非因果病理机制，导致模型脆弱、易产生幻觉、对数据集偏差敏感。传统检索增强生成依赖语义相似性，可能引入新的虚假相关性。

Method: 提出多模态因果检索增强生成框架，从外部源检索临床相关示例和因果图，基于反事实和干预证据而非单纯相关性来调节模型推理。

Result: 应用于放射学报告生成、诊断预测和视觉问答任务，提高了事实准确性、对分布偏移的鲁棒性和可解释性。

Conclusion: 因果检索为医学视觉语言模型提供了一条可扩展的路径，使其超越模式匹配，实现高风险临床环境中可信赖的多模态推理。

Abstract: Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.

</details>


### [285] [Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach](https://arxiv.org/abs/2601.18399)
*Mehmet Velioglu,Song Zhai,Alexander Mitsos,Adel Mhamdi,Andreas Jupke,Manuel Dahmen*

Main category: cs.LG

TL;DR: 提出了一种结合物理信息神经网络和扩展卡尔曼滤波的两阶段训练方法，仅使用廉价的体积流量测量来估计重力沉降器中难以测量的相高度。


<details>
  <summary>Details</summary>
Motivation: 重力沉降器中液-液分散相的分离在化工、制药和回收过程中至关重要，但密集堆积区高度作为重要性能和安全指标，由于光学限制通常难以测量且成本高昂。

Method: 1) 首先在合成数据和从低精度机理模型推导的物理方程上预训练PINN；2) 然后用稀缺的实验数据微调PINN；3) 将可微分的PINN作为预测模型集成到扩展卡尔曼滤波状态估计框架中，从流量测量跟踪相高度。

Result: 在所有评估中，两阶段训练的PINN在相高度估计方面最为准确，优于机理模型、非预训练的PINN以及两阶段训练的纯数据驱动神经网络。

Conclusion: 该方法通过结合物理先验知识和少量实验数据，有效解决了重力沉降器中相高度难以直接测量的问题，为工业过程中的实时监测和控制提供了实用解决方案。

Abstract: Separating liquid-liquid dispersions in gravity settlers is critical in chemical, pharmaceutical, and recycling processes. The dense-packed zone height is an important performance and safety indicator but it is often expensive and impractical to measure due to optical limitations. We propose to estimate phase heights using only inexpensive volume flow measurements. To this end, a physics-informed neural network (PINN) is first pretrained on synthetic data and physics equations derived from a low-fidelity (approximate) mechanistic model to reduce the need for extensive experimental data. While the mechanistic model is used to generate synthetic training data, only volume balance equations are used in the PINN, since the integration of submodels describing droplet coalescence and sedimentation into the PINN would be computationally prohibitive. The pretrained PINN is then fine-tuned with scarce experimental data to capture the actual dynamics of the separator. We then employ the differentiable PINN as a predictive model in an Extended Kalman Filter inspired state estimation framework, enabling the phase heights to be tracked and updated from flow-rate measurements. We first test the two-stage trained PINN by forward simulation from a known initial state against the mechanistic model and a non-pretrained PINN. We then evaluate phase height estimation performance with the filter, comparing the two-stage trained PINN with a two-stage trained purely data-driven neural network. All model types are trained and evaluated using ensembles to account for model parameter uncertainty. In all evaluations, the two-stage trained PINN yields the most accurate phase-height estimates.

</details>


### [286] [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401)
*Yufeng Huang*

Main category: cs.LG

TL;DR: Superlinear attention是一种多步注意力架构，通过多步搜索实现次二次复杂度，同时保持随机上下文访问能力，为长序列处理提供高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力机制在处理长序列时存在二次复杂度问题，现有高效注意力方法通常牺牲了随机上下文访问能力（即某些位置可能被结构性地排除在注意力之外）。需要一种既能保持随机上下文访问，又能实现次二次复杂度的注意力架构。

Method: 将标准因果自注意力重新表述为N步搜索问题，复杂度为O(L^{1+1/N})。提出N=2的基线实现，第一步进行O(L^{3/2})跨度搜索选择相关序列片段，第二步在选定片段上应用O(L^{3/2})跨度注意力。还展示了O(L^{1.54})的扩展配置。

Result: 在修改的30B混合MoE模型上，单B200 GPU上实现：1M上下文长度平均解码吞吐量114 tokens/sec，10M上下文长度80 tokens/sec。在有限训练下，在256K上下文长度的NIAH任务上表现出色，证明路由跨度选择可端到端学习。

Conclusion: Superlinear attention在保持随机上下文访问的同时实现了次二次复杂度，为长序列处理提供了系统可行的架构方案。论文主要关注架构制定、扩展分析和系统可行性验证，全面质量评估留待未来工作。

Abstract: In this paper, we propose \textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \textbf{random context access} (a.k.a.\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.

</details>


### [287] [Frequency-Based Hyperparameter Selection in Games](https://arxiv.org/abs/2601.18409)
*Aniket Sanyal,Baraah A. M. Sidahmed,Rebekka Burkholz,Tatjana Chavdarova*

Main category: cs.LG

TL;DR: 提出Modal LookAhead (MoLA)，一种基于频率估计的自适应超参数选择方法，用于解决博弈学习中由于旋转动力学导致的超参数调优难题。


<details>
  <summary>Details</summary>
Motivation: 在平滑博弈中学习与标准最小化问题存在根本差异，旋转动力学使得经典超参数调优策略失效。尽管超参数调优在实际中很重要，但博弈中的有效调优方法仍未被充分探索。LookAhead方法虽然表现出色，但引入了对性能至关重要的额外参数。

Method: 通过频率估计振荡动力学来分析连续时间轨迹和离散动力学频谱。基于此分析，提出了Modal LookAhead (MoLA)，这是LookAhead的扩展，能够根据给定问题自适应选择超参数。

Result: MoLA在纯旋转博弈和混合机制中都能加速训练，且计算开销最小。提供了收敛性保证，并通过实验验证了其有效性。

Conclusion: MoLA为博弈学习中的超参数选择提供了一个原则性方法，通过利用频率分析自适应调整参数，有效解决了旋转动力学带来的调优挑战。

Abstract: Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.

</details>


### [288] [Gradient Regularized Natural Gradients](https://arxiv.org/abs/2601.18420)
*Satya Prakash Dash,Hossein Abdi,Wei Pan,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出了一种结合梯度正则化和自然梯度的二阶优化器GRNG，通过两种变体提升训练稳定性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 虽然梯度正则化已被证明能提高模型泛化能力，自然梯度下降能加速训练初期优化，但二阶优化器的训练动态如何从梯度正则化中获益的研究较少

Method: 提出了GRNG框架，包含两种算法：1）频率主义变体通过结构化近似避免Fisher信息矩阵显式求逆；2）贝叶斯变体基于正则化卡尔曼公式完全消除FIM求逆需求

Result: 建立了GRNG的收敛保证，证明梯度正则化提高稳定性并能够收敛到全局最小值；在视觉和语言基准测试中，GRNG在优化速度和泛化性能上均优于一阶方法（SGD、AdamW）和二阶基线（K-FAC、Sophia）

Conclusion: 梯度正则化是一个有原则且实用的工具，能够释放自然梯度方法在大规模深度学习中的鲁棒性潜力

Abstract: Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.

</details>


### [289] [GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level](https://arxiv.org/abs/2601.18447)
*Jinlong Hu,Jiacheng Liu*

Main category: cs.LG

TL;DR: GCFX：基于深度图生成的模型级反事实解释方法，通过增强的图生成框架和全局摘要算法提供高质量的全局反事实解释。


<details>
  <summary>Details</summary>
Motivation: 深度图学习模型虽然功能强大但缺乏透明度，用户难以理解和信任其决策过程。需要提供模型级的解释技术来全面理解模型的整体决策机制。

Method: 提出GCFX方法，结合双编码器、结构感知标记器和消息传递神经网络解码器，准确学习输入数据的真实潜在分布，生成高质量反事实示例。使用全局反事实摘要算法从众多候选中选择最具代表性的解释。

Result: 在合成数据集和多个真实数据集上的实验表明，GCFX在反事实有效性和覆盖范围方面优于现有方法，同时保持较低的解释成本。

Conclusion: GCFX为增强全局反事实解释的实用性和可信度提供了重要支持，有助于提高深度图学习模型的可解释性和用户信任。

Abstract: Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.

</details>


### [290] [Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States](https://arxiv.org/abs/2601.18479)
*Kyoleen Kwak,Hyoseok Hwang*

Main category: cs.LG

TL;DR: 提出ASAP方法，通过对齐转换诱导相似状态的动作和惩罚二阶差异来平滑强化学习中的高频动作振荡


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在实际应用中存在高频动作振荡问题，现有方法依赖启发式或合成的状态相似性定义，无法准确反映系统动态

Method: 引入转换诱导相似状态的概念，基于环境反馈和实际收集数据定义状态相似性；提出ASAP方法，通过对齐转换诱导相似状态的动作和惩罚二阶差异来平滑动作

Result: 在Gymnasium和Isaac-Lab环境中实验证明，ASAP能产生更平滑的控制并提升策略性能

Conclusion: ASAP方法有效缓解了强化学习中的动作振荡问题，通过更准确地反映系统动态的状态相似性定义实现了更好的动作平滑效果

Abstract: Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.

</details>


### [291] [Nearly Optimal Bayesian Inference for Structural Missingness](https://arxiv.org/abs/2601.18500)
*Chen Liang,Donghua Yang,Yutong Wang,Tianle Zhang,Shenghe Zhou,Zhiyu Liang,Hengtong Zhang,Hongzhi Wang,Ziqi Li,Xiyang Zhang,Zheng Liang,Yifei Li*

Main category: cs.LG

TL;DR: 该论文提出了一种贝叶斯方法处理结构性缺失数据，通过后验预测分布进行预测，避免传统插值方法的偏差和过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 传统"先插值再训练"的方法在处理结构性缺失数据时存在根本性缺陷：缺失值可能由因果或逻辑约束定义，缺失机制可能依赖于观察变量、未观察变量（MNAR）和其他缺失指标。这导致三个核心问题：(1)因果循环的困境，(2)MNAR下的分布偏移，(3)单一插值导致过度自信的偏差决策。

Method: 采用贝叶斯视角，通过后验预测分布进行预测，将学习过程解耦为两个步骤：首先学习模型内缺失值的后验分布，然后通过优化预测后验分布进行标签预测。这种方法能够传播不确定性并实现后验积分。

Result: 在43个分类基准和15个插值基准上取得了最先进的性能，并在SCM先验下提供了有限样本近贝叶斯最优性的理论保证。

Conclusion: 贝叶斯后验预测框架为结构性缺失数据问题提供了理论严谨且实践有效的解决方案，实现了"几乎免费的午餐"：一旦后验被学习，预测即可即插即用，同时保持不确定性传播。

Abstract: Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.

</details>


### [292] [Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark](https://arxiv.org/abs/2601.18509)
*Andro Sabashvili*

Main category: cs.LG

TL;DR: 本文综述时间序列预测中保形预测方法，重点解决时间序列数据依赖性与保形预测可交换性假设之间的冲突，系统评述了四类解决方案并进行了实际性能基准测试。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中可靠的不确定性量化至关重要，但传统方法依赖限制性分布假设。保形预测作为无分布框架具有理论保证，但时间序列的时序依赖性违反了保形预测所需的数据可交换性核心假设。

Method: 系统评述四类算法解决方案：1) 放松可交换性假设的方法；2) 重新定义数据单元为独立时间序列集合的方法；3) 显式建模预测残差动态的方法；4) 适应分布漂移以维持长期覆盖的在线学习算法。

Result: 通过综合这些方法，突出了在真实世界数据上的计算效率和实际性能，为时间序列预测中的保形预测应用提供了全面的技术基准。

Conclusion: 本文系统梳理了解决时间序列保形预测中可交换性冲突的主要方法，为实际应用提供了方法选择和性能评估的指导框架。

Abstract: Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.

</details>


### [293] [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510)
*Yibo Li,Zijie Lin,Ailin Deng,Xuan Zhang,Yufei He,Shuo Ji,Tri Cao,Bryan Hooi*

Main category: cs.LG

TL;DR: JitRL是一个无需训练的强化学习框架，通过在测试时动态检索经验轨迹来估计动作优势值，直接调制LLM输出logits，实现持续学习


<details>
  <summary>Details</summary>
Motivation: LLM智能体部署后权重固定，难以持续适应新任务；传统RL方法计算成本高且存在灾难性遗忘问题

Method: 维护动态非参数记忆库，检索相关经验轨迹实时估计动作优势值，通过加性更新规则直接调制LLM输出logits

Result: 在WebArena和Jericho基准上达到训练无关方法的新SOTA，性能超越计算昂贵的微调方法(如WebRL)，同时降低30倍以上成本

Conclusion: JitRL为持续学习智能体提供了一种可扩展的解决方案，通过免训练方式实现测试时策略优化

Abstract: While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.

</details>


### [294] [LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models](https://arxiv.org/abs/2601.18513)
*Kai Hu,Haoqi Hu,Matt Fredrikson*

Main category: cs.LG

TL;DR: LipNeXt是首个无约束、无卷积的1-Lipschitz架构，通过流形优化和空间移位模块实现，在保持严格Lipschitz控制的同时达到最先进的清洁和认证鲁棒性精度，并能扩展到数十亿参数的大型模型。


<details>
  <summary>Details</summary>
Motivation: 基于Lipschitz的认证能提供高效、确定性的鲁棒性保证，但在模型规模、训练效率和ImageNet性能方面难以扩展。现有方法受到约束和卷积结构的限制。

Method: 使用两种关键技术：1) 直接在正交流形上更新参数的流形优化程序；2) 无需卷积建模空间模式的空间移位模块。网络采用正交投影、空间移位、简单的1-Lipschitz β-Abs非线性激活和L2空间池化。

Result: 在CIFAR-10/100和Tiny-ImageNet上达到最先进的清洁和认证鲁棒性精度；在ImageNet上能扩展到1-2B参数的大型模型，相比之前Lipschitz模型将认证鲁棒性精度提高了最多8%（在ε=1时），同时保持高效、稳定的低精度训练。

Conclusion: Lipschitz-based认证可以从现代扩展趋势中受益，而不牺牲确定性或效率，证明了该方法在大规模应用中的可行性。

Abstract: Lipschitz-based certification offers efficient, deterministic robustness guarantees but has struggled to scale in model size, training efficiency, and ImageNet performance. We introduce \emph{LipNeXt}, the first \emph{constraint-free} and \emph{convolution-free} 1-Lipschitz architecture for certified robustness. LipNeXt is built using two techniques: (1) a manifold optimization procedure that updates parameters directly on the orthogonal manifold and (2) a \emph{Spatial Shift Module} to model spatial pattern without convolutions. The full network uses orthogonal projections, spatial shifts, a simple 1-Lipschitz $β$-Abs nonlinearity, and $L_2$ spatial pooling to maintain tight Lipschitz control while enabling expressive feature mixing. Across CIFAR-10/100 and Tiny-ImageNet, LipNeXt achieves state-of-the-art clean and certified robust accuracy (CRA), and on ImageNet it scales to 1-2B large models, improving CRA over prior Lipschitz models (e.g., up to $+8\%$ at $\varepsilon{=}1$) while retaining efficient, stable low-precision training. These results demonstrate that Lipschitz-based certification can benefit from modern scaling trends without sacrificing determinism or efficiency.

</details>


### [295] [Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning](https://arxiv.org/abs/2601.18521)
*Emna Boudabbous,Mohamed Karaa,Lokman Sboui,Julio Montecinos,Omar Alam*

Main category: cs.LG

TL;DR: 论文提出了一个城市级公交延迟预测系统，结合多分辨率特征工程、降维和深度学习，在蒙特利尔公交网络上实现准确、可扩展的实时预测。


<details>
  <summary>Details</summary>
Motivation: 现有公交延迟预测系统通常只处理少数线路，依赖手工特征，缺乏可扩展的架构设计。公交公司需要可靠的城市级网络延迟预测来提供准确到站信息和实时运营控制。

Method: 采用多分辨率特征工程生成1683个时空特征，使用自适应PCA降维至83个成分。引入H3+拓扑混合聚类方法形成12个平衡路线集群，避免"巨型集群"问题。比较了五种模型架构，最终选择全局LSTM模型。

Result: 全局LSTM模型在准确性和效率上取得最佳平衡，比Transformer模型性能提升18-52%，参数数量减少275倍。系统支持实时、城市级部署，可通过有限适配用于其他网络。

Conclusion: 该框架为公交延迟预测提供了可扩展、可复用的解决方案，能够满足城市级实时预测需求，显著优于现有方法，具有实际部署价值。

Abstract: Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.
  We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the "giant cluster" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.
  We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.

</details>


### [296] [From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale](https://arxiv.org/abs/2601.18524)
*Yongqi Jin,Yecheng Wang,Jun-jie Wang,Rong Zhu,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: 提出半监督框架，利用文献提取的百万级未标记光谱学习NMR化学位移，无需原子级标注，显著提升预测准确性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法依赖有限、劳动密集型的原子标注数据集，难以准确预测NMR化学位移。需要利用大规模文献数据来突破数据瓶颈

Method: 将NMR化学位移预测建模为置换不变集合监督问题，通过最优二分图匹配简化为基于排序的损失函数，结合少量标注数据和大量未标注光谱进行半监督训练

Result: 模型在准确性和鲁棒性上显著超越现有方法，在更大更多样的分子数据集上展现更强泛化能力，首次在大规模上捕捉常见NMR溶剂的系统性溶剂效应

Conclusion: 大规模未标注文献光谱可作为训练NMR位移模型的有效数据源，表明文献衍生的弱结构化数据在科学数据驱动AI中具有更广泛的应用潜力

Abstract: Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.

</details>


### [297] [Closing the Modality Gap Aligns Group-Wise Semantics](https://arxiv.org/abs/2601.18525)
*Eleonora Grassucci,Giordano Cicchetti,Emanuele Frasca,Aurelio Uncini,Danilo Comminiello*

Main category: cs.LG

TL;DR: 本文挑战了现有对模态间隙(modality gap)的认知，证明其在分组任务中影响显著，并提出新方法减少模态间隙，显著提升分组任务性能


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP在跨模态学习中被视为标准方法，但其学习到的潜在空间往往只部分共享，存在结构不匹配（模态间隙）。现有研究对是否需要解决这一现象存在争议，特别是其对实例级任务影响有限。本文旨在证明模态间隙在分组级任务中具有显著影响。

Method: 提出一种新颖方法，在双模态设置中一致地减少模态间隙，并可简单扩展到一般n模态情况。该方法专门针对模态对齐的结构不匹配问题。

Result: 广泛评估表明：减少模态间隙对传统实例级任务仅提供边际或不一致的改进，但显著增强分组级任务性能。这为模态间隙的理解提供了新视角。

Conclusion: 模态间隙在需要语义分组的任务中扮演关键角色，这一发现可能重塑我们对模态间隙的理解，强调其在提升分组任务性能中的重要性。

Abstract: In multimodal learning, CLIP has been recognized as the \textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.

</details>


### [298] [Information Hidden in Gradients of Regression with Target Noise](https://arxiv.org/abs/2601.18546)
*Arash Jamshidi,Katsiaryna Haitsiukevich,Kai Puolamäki*

Main category: cs.LG

TL;DR: 本文提出了一种仅使用梯度信息恢复二阶信息（如Hessian矩阵）的方法，通过注入高斯噪声使目标噪声方差等于批量大小，从而让经验梯度协方差近似Hessian矩阵。


<details>
  <summary>Details</summary>
Motivation: 在许多现代机器学习场景中，只有梯度是可观测的，而二阶信息（如曲率或数据协方差）对于优化、诊断和鲁棒性至关重要。如何仅从梯度中恢复这些二阶信息是一个重要问题。

Method: 核心方法是方差校准：通过注入高斯噪声，使总目标噪声方差等于批量大小n。这种方法确保经验梯度协方差能够紧密近似Hessian矩阵，即使是在远离最优解的位置评估。

Result: 在次高斯输入下提供了非渐近算子范数保证。证明了如果没有这种校准，恢复可能会失败Ω(1)因子。该方法实用（"将目标噪声方差设为n"的规则）且鲁棒（方差O(n)足以按比例恢复Σ）。

Conclusion: 仅使用梯度信息就能恢复Hessian矩阵，在线性回归中等于数据协方差Σ。该方法具有实际应用价值，包括加速优化的预条件、对抗风险估计和分布式系统中的纯梯度训练。

Abstract: Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $Σ$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $Ω(1)$ factor. The proposed method is practical (a "set target-noise variance to $n$" rule) and robust (variance $\mathcal{O}(n)$ suffices to recover $Σ$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.

</details>


### [299] [An Unsupervised Tensor-Based Domain Alignment](https://arxiv.org/abs/2601.18564)
*Chong Hyun Lee,Kibae Lee,Hyun Hee Yim*

Main category: cs.LG

TL;DR: 提出一种基于张量的域对齐算法，通过在不变子空间中使用对齐矩阵来对齐源域和目标域张量，算法在斜流形上进行优化，比传统Stiefel流形更灵活，并加入保持方差的正则化项。


<details>
  <summary>Details</summary>
Motivation: 现有张量域对齐方法通常限制在Stiefel流形上，灵活性不足。需要一种更灵活、适应性更强的张量域对齐框架，能够同时提升转换速度和分类精度。

Method: 提出基于张量的域对齐算法，使用对齐矩阵将源域和目标域张量对齐到不变子空间。通过斜流形约束进行迭代优化，比Stiefel流形更灵活。加入保持源域和目标域方差的正则化项以确保鲁棒性。该框架可泛化现有张量域对齐方法为特例。

Result: 实验表明，该方法不仅提高了域对齐转换速度，还显著提升了分类精度。性能优于当前最先进技术，适用于复杂的域适应任务。

Conclusion: 提出的基于张量的域对齐算法在斜流形上进行优化，比传统方法更灵活，能有效提升域适应任务的转换速度和分类精度，是复杂域适应任务的优选方法。

Abstract: We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.

</details>


### [300] [K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents](https://arxiv.org/abs/2601.18580)
*Vincenzo De Paola,Mirco Mutti,Riccardo Zamboni,Marcello Restelli*

Main category: cs.LG

TL;DR: K-Myriad是一种可扩展的无监督方法，通过最大化并行策略群体诱导的集体状态熵来促进多样化探索，为强化学习提供鲁棒初始化，提高训练效率并发现异构解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习并行化通常使用多个工作者从相同的采样分布收集经验，这种设计忽视了多样化探索策略的优势，限制了并行化的潜力。

Method: 提出K-Myriad方法，通过最大化并行策略群体诱导的集体状态熵，培养专门化探索策略组合，为强化学习提供鲁棒初始化。

Result: 在高维连续控制任务上进行大规模并行化实验，证明K-Myriad能够学习广泛的独特策略，展现了集体探索的有效性。

Conclusion: K-Myriad为强化学习并行化提供了新的策略方向，通过多样化探索提高训练效率和解决方案的异构性。

Abstract: Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.

</details>


### [301] [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](https://arxiv.org/abs/2601.18586)
*Miguel Costa,Arthur Vandervoort,Carolin Schmidt,Morten W. Petersen,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 提出了一个结合集成评估模型与强化学习的决策支持框架，用于学习城市交通系统在气候变化下的多十年自适应投资路径。


<details>
  <summary>Details</summary>
Motivation: 气候变化加剧降雨等灾害，对城市交通系统造成更多干扰。设计有效适应策略面临长期序列投资、深度不确定性和跨部门复杂交互的挑战。

Method: 提出通用决策支持框架，耦合集成评估模型与强化学习，结合长期气候预测、灾害建模、基础设施影响评估和社会成本计算，通过强化学习循环学习自适应气候适应政策。

Result: 在哥本哈姆市区2024-2100年城市内涝案例中，学习到的策略产生了协调的时空路径，相比不作为和随机行动等传统优化基线表现出更好的鲁棒性。

Conclusion: 该框架可推广到其他灾害和城市，为在深度不确定性下制定自适应气候适应投资策略提供了有效工具。

Abstract: Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.

</details>


### [302] [LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation](https://arxiv.org/abs/2601.18604)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: 提出LaCoGSEA框架，結合深度表示學習與通路統計，用於無監督轉錄組數據的通路富集分析


<details>
  <summary>Details</summary>
Motivation: 現有的通路富集分析方法(如GSEA)依賴預先定義的表型標籤和成對比較，限制了在無監督場景下的應用。現有無監督擴展主要捕捉線性關係，深度學習方法雖能捕捉非線性結構，但其解釋依賴通用的可解釋AI技術，這些方法並非專為通路層級解釋設計。

Method: LaCoGSEA使用自編碼器捕捉非線性流形，提出全局基因-潛在相關性指標作為差異表達的代理，在無需先驗標籤的情況下生成密集基因排序

Result: LaCoGSEA在三個方面表現優異：(1)在區分癌症亞型方面比現有無監督基線方法有更好的聚類性能；(2)相比線性降維和基於梯度的XAI方法，能在更高排名恢復更廣泛的生物學相關通路；(3)在不同實驗協議和數據集大小下保持高魯棒性和一致性

Conclusion: LaCoGSEA在無監督通路富集分析中提供了最先進的性能，填補了深度表示學習與通路級解釋之間的空白

Abstract: Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.
  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.
  Availability and implementation: https://github.com/willyzzz/LaCoGSEA

</details>


### [303] [Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem](https://arxiv.org/abs/2601.18615)
*Ramiro Valdes Jara,Adam Meyers*

Main category: cs.LG

TL;DR: 提出一种基于条件扩散模型的数据驱动方法解决心电图成像逆问题，无需几何建模，通过概率采样处理非唯一性


<details>
  <summary>Details</summary>
Motivation: 传统心电图成像逆问题解决方法需要患者特定网格构建，且通常提供确定性解，无法处理问题的非唯一性和欠定性

Method: 使用条件扩散框架学习从噪声体表信号到心表电位的概率映射，无需几何建模，完全数据驱动，可生成多个可能解

Result: 在真实ECGI数据集上评估，相比卷积神经网络、LSTM和Transformer等确定性基线模型，扩散方法获得了更高的重建精度

Conclusion: 扩散模型为无创心脏电生理成像提供了强大工具，能处理逆问题的非唯一性，且无需患者特定几何建模

Abstract: This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.

</details>


### [304] [CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling](https://arxiv.org/abs/2601.18620)
*Panagiotis Lymperopoulos,Abhiramon Rajasekharan,Ian Berlot-Attwell,Stéphane Aroca-Ouellette,Kaheer Suleman*

Main category: cs.LG

TL;DR: CASSANDRA是一种神经符号世界建模方法，利用LLM作为知识先验构建轻量级规划转换模型，在咖啡店和主题公园业务模拟器中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界领域（如商业）具有丰富的语义，可以利用世界知识从有限数据中有效建模复杂的行动效果和因果关系，这对于规划至关重要。

Method: CASSANDRA结合两个组件：(1) LLM合成代码建模确定性特征；(2) LLM引导的概率图模型结构学习以捕捉随机变量间的因果关系。

Result: 在咖啡店模拟器和复杂的主题公园业务模拟器中评估，CASSANDRA在转换预测和规划方面相比基线方法有显著改进。

Conclusion: CASSANDRA通过LLM知识先验构建轻量级转换模型，有效利用世界知识建模复杂领域，为现实世界规划提供了有效的神经符号世界建模方法。

Abstract: Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.

</details>


### [305] [Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning](https://arxiv.org/abs/2601.18626)
*Yingxiao Huo,Satya Prakash Dash,Radu Stoican,Samuel Kaski,Mingfei Sun*

Main category: cs.LG

TL;DR: 提出一种高效可扩展的自然策略优化方法，利用秩-1近似来逼近Fisher信息矩阵的逆，避免了传统自然梯度方法中计算密集型矩阵求逆操作。


<details>
  <summary>Details</summary>
Motivation: 自然梯度在深度强化学习中具有快速收敛和协变权重更新的优势，但传统方法需要每次迭代计算Fisher信息矩阵的逆，这在计算上是不可行的。需要一种既能保持自然梯度优势又计算高效的方法。

Method: 提出一种基于秩-1近似的自然策略优化技术，用秩-1矩阵来近似完整的逆Fisher信息矩阵，从而大幅降低计算复杂度。

Result: 理论上证明在特定条件下，秩-1近似的逆Fisher信息矩阵比策略梯度收敛更快，并且在某些条件下具有与随机策略梯度方法相同的样本复杂度。实验表明该方法在多样化环境中优于标准actor-critic和信任域基线方法。

Conclusion: 提出的秩-1近似自然策略优化方法既保持了自然梯度的理论优势，又解决了计算瓶颈问题，在多个基准测试中表现出优越性能。

Abstract: Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.

</details>


### [306] [Physics-Informed Uncertainty Enables Reliable AI-driven Design](https://arxiv.org/abs/2601.18638)
*Tingkai Xue,Chin Chun Ooi,Yang Jiang,Luu Trung Pham Duong,Pao-Hsiung Chiu,Weijiang Zhao,Nagarajan Raghavan,My Ha Dao*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理信息不确定性的新范式，利用模型预测违反物理定律的程度作为预测不确定性的计算廉价代理，显著提高了频率选择表面设计的成功率和效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的代理辅助优化方法通常缺乏不确定性量化，导致在数据稀疏区域产生错误预测，从而降低优化性能。特别是在频率选择表面等微电子和光学超材料设计中，需要更有效的逆设计方法。

Method: 提出物理信息不确定性范式，将模型预测违反基本物理定律的程度作为预测不确定性的计算廉价代理。将此方法集成到多保真度不确定性感知优化工作流程中，用于设计20-30 GHz范围内的复杂频率选择表面。

Result: 在频率选择表面设计中，将寻找高性能解决方案的成功率从不到10%提高到超过50%，同时与单独使用高保真求解器相比，计算成本降低了一个数量级。

Conclusion: 研究强调了在高维问题中机器学习驱动的逆设计中纳入不确定性量化的必要性，确立了物理信息不确定性作为物理系统代理模型中量化不确定性的可行替代方案，为能够高效稳健探索和评估候选设计的自主科学发现系统奠定了基础。

Abstract: Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.

</details>


### [307] [TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning](https://arxiv.org/abs/2601.18640)
*Zhiwei Zheng,Kevin Bryson*

Main category: cs.LG

TL;DR: TwinPurify是一个基于Barlow Twins自监督学习框架的表征学习方法，通过利用同一队列中的相邻正常组织作为"背景"指导，从bulk转录组数据中学习连续的高维肿瘤嵌入，从而消除肿瘤纯度变化的影响，分离出肿瘤特异性信号。


<details>
  <summary>Details</summary>
Motivation: 虽然单细胞和空间转录组技术已实现细胞分辨率分析，但大规模患者队列研究仍依赖bulk转录组数据。肿瘤纯度的变化会掩盖肿瘤固有的转录信号，限制下游发现。现有的反卷积方法在合成混合物上表现良好，但难以泛化到真实患者队列。

Method: TwinPurify采用Barlow Twins自监督目标，不将bulk混合物分解为离散细胞类型比例，而是利用同一队列中的相邻正常组织作为"背景"指导，学习连续的高维肿瘤嵌入，无需依赖外部参考就能分离肿瘤特异性信号。

Result: 在多个大型癌症队列（RNA-seq和微阵列平台）的基准测试中，TwinPurify优于传统表征学习方法（如自编码器），能更好地恢复肿瘤固有信号和免疫信号。纯化后的嵌入改善了分子亚型和分级分类，提高了生存模型的一致性，并揭示了更有生物学意义的通路活性。

Conclusion: TwinPurify提供了一个可转移的框架来净化bulk转录组数据，扩展了现有临床数据集在分子发现中的实用性，为肿瘤特异性信号分析提供了新方法。

Abstract: Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.
  Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as "background" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.
  Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.

</details>


### [308] [FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning](https://arxiv.org/abs/2601.18650)
*Liheng Yu,Zhe Zhao,Yuxuan Wang,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 该论文提出FaLW方法，针对长尾分布数据下的机器遗忘问题，通过动态损失重加权机制解决现有方法在长尾数据上表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘研究主要评估在相对平衡的遗忘集上的表现，忽略了现实世界中遗忘数据（如用户活动记录）常遵循长尾分布的情况。这种长尾设置下，现有方法存在异质性遗忘偏差和倾斜遗忘偏差两个关键问题。

Method: 提出FaLW方法，这是一种即插即用的实例级动态损失重加权方法。通过比较每个样本的预测概率与同一类别未见数据的分布，创新性地评估每个样本的遗忘状态。基于此，使用由平衡因子调节的遗忘感知重加权方案，自适应调整每个样本的遗忘强度。

Result: 大量实验表明，FaLW在长尾分布数据下的机器遗忘任务中取得了优越性能。

Conclusion: FaLW方法有效解决了长尾数据场景下机器遗忘的挑战，为现实世界中非平衡数据分布的遗忘问题提供了实用解决方案。

Abstract: Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \textit{Heterogeneous Unlearning Deviation} and \textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \textbf{Supplementary Material}.

</details>


### [309] [A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.18672)
*Spyros Rigas,Thanasis Papaioannou,Panagiotis Trakadas,Georgios Alexandridis*

Main category: cs.LG

TL;DR: 提出基于曲率的网格自适应框架提升KAN性能，相比标准输入密度方法在合成函数、Feynman数据集和PDE问题上分别减少25.3%、9.4%和23.3%相对误差


<details>
  <summary>Details</summary>
Motivation: 现有KAN网格自适应策略仅依赖输入数据密度，忽略了目标函数的几何复杂度及训练过程中的计算指标，限制了网络性能提升

Method: 提出通用框架将节点分配视为重要性密度函数（IDFs）控制的密度估计任务，引入基于曲率的自适应策略，让训练动态决定网格分辨率

Result: 在合成函数拟合、Feynman数据集子集回归和Helmholtz PDE不同实例上，曲率自适应策略显著优于标准输入基线，相对误差分别降低25.3%、9.4%和23.3%，Wilcoxon符号秩检验确认统计显著性

Conclusion: 曲率自适应为KAN训练提供了稳健且计算高效的替代方案，通过考虑目标函数几何特性而非仅输入数据密度，有效提升模型性能

Abstract: Kolmogorov-Arnold Networks (KANs) have recently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training.

</details>


### [310] [Learning temporal embeddings from electronic health records of chronic kidney disease patients](https://arxiv.org/abs/2601.18675)
*Aditya Kumar,Mario A. Cypko,Oliver Amft*

Main category: cs.LG

TL;DR: 研究基于电子病历的时间嵌入模型能否在不牺牲预测性能的情况下学习有临床意义的表征，以及架构选择如何影响嵌入质量。使用MIMIC-IV数据集比较三种循环架构，发现时间感知LSTM能产生更有结构的嵌入，且嵌入模型在ICU死亡率预测上优于端到端模型。


<details>
  <summary>Details</summary>
Motivation: 模型引导医学需要能够捕捉疾病动态且保持透明、任务无关的表征，而大多数临床预测模型仅针对单一任务优化。表示学习有助于学习可泛化到下游任务的嵌入，循环架构适合建模临床数据的时间结构。

Method: 使用MIMIC-IV数据集，针对慢性肾病（CKD）患者比较三种循环架构：普通LSTM、注意力增强LSTM和时间感知LSTM（T-LSTM）。所有模型都作为嵌入模型和直接端到端预测器进行训练。通过CKD分期聚类和ICU死亡率预测评估嵌入质量。

Result: T-LSTM产生更有结构的嵌入，Davies-Bouldin指数更低（9.91），CKD分期分类准确率更高（0.74），优于普通LSTM（DBI=15.85，准确率=0.63）和注意力增强LSTM（DBI=20.72，准确率=0.67）。在ICU死亡率预测中，嵌入模型（准确率0.82-0.83）始终优于端到端预测器（准确率0.72-0.75），表明学习嵌入作为中间步骤比直接端到端学习更有效。

Conclusion: 时间嵌入模型可以在保持预测性能的同时学习有临床意义的表征，时间感知架构能产生质量更高的嵌入。学习嵌入作为中间步骤比直接端到端学习更有效，这为开发既具有预测能力又能提供临床可解释表征的模型提供了支持。

Abstract: We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.

</details>


### [311] [Quasi Monte Carlo methods enable extremely low-dimensional deep generative models](https://arxiv.org/abs/2601.18676)
*Miles Martinez,Alex H. Williams*

Main category: cs.LG

TL;DR: 该论文提出了准蒙特卡洛隐变量模型(QLVMs)，这是一种专门用于寻找高维数据集的极低维可解释嵌入的深度生成模型。它通过准蒙特卡洛积分直接近似边缘似然，在低维空间(1-3维)中表现优于传统变分自编码器。


<details>
  <summary>Details</summary>
Motivation: 传统变分自编码器(VAEs)和重要性加权自编码器(IWAEs)通常需要学习编码器和变分下界，在追求极低维、可解释的嵌入表示时存在局限性。作者希望开发一种能够提供更透明、可分析的低维潜在空间的方法。

Method: QLVMs采用准蒙特卡洛(QMC)积分直接近似边缘似然，而不是依赖学习编码器和变分下界。这种方法虽然计算密集，但在极低维空间(1-3维)中特别有效，能够提供更准确的边缘似然估计。

Result: 在多个数据集上的实验表明，QLVMs在匹配潜在维度的情况下，始终优于传统VAEs和IWAEs。该方法能够产生高度可解释的低维嵌入，支持透明的可视化、非参数密度估计、聚类和测地路径计算等后分析。

Conclusion: QLVMs为需要优先考虑可解释性和潜在空间分析的应用提供了一个有吸引力的解决方案，特别是在1-3维的极低维空间中表现出色。虽然计算密集且在复杂数据集中难以生成精细细节，但在追求透明度和可分析性时具有明显优势。

Abstract: This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.

</details>


### [312] [Counterfactual Explanations on Robust Perceptual Geodesics](https://arxiv.org/abs/2601.18678)
*Eslam Zaher,Maciej Trzaskowski,Quan Nguyen,Fred Roosta*

Main category: cs.LG

TL;DR: 论文提出PCG方法，通过感知黎曼度量构建反事实解释的测地线，实现平滑、在流形上的语义有效转换，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法存在缺陷：距离度量的选择模糊性导致扰动可能是语义上有意义的，也可能是对抗性的；现有方法采用平坦或不对齐的几何结构，导致离流形伪影、语义漂移或对抗性塌缩。

Method: 引入感知反事实测地线（PCG），使用鲁棒视觉特征诱导的感知黎曼度量构造反事实，沿着测地线追踪，该几何结构与人类感知对齐并惩罚脆弱方向。

Result: 在三个视觉数据集上的实验表明，PCG优于基线方法，并揭示了标准度量下隐藏的失败模式。

Conclusion: PCG方法通过感知黎曼度量构建反事实解释，实现了平滑、在流形上的语义有效转换，为反事实解释提供了更可靠的几何基础。

Abstract: Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.

</details>


### [313] [ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule](https://arxiv.org/abs/2601.18681)
*Yilie Huang,Wenpin Tang,Xunyu Zhou*

Main category: cs.LG

TL;DR: 提出了自适应重参数化时间（ART）方法，通过控制时间变量的时钟速度来优化扩散模型的时间步长分配，以减少离散化误差。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用均匀或手工设计的时间网格在给定时间步预算下可能不是最优的，需要更智能的时间步长分配方法来提高采样质量。

Method: 引入自适应重参数化时间（ART），控制重参数化时间变量的时钟速度，实现不均匀的时间步长分配。提出随机控制版本ART-RL，将时间变化建模为连续时间强化学习问题，使用高斯策略学习最优时间调度。

Result: 基于EDM管道，ART-RL在CIFAR-10上显著改善了Fréchet Inception Distance，并在AFHQv2、FFHQ和ImageNet等数据集上表现出良好的迁移性，无需重新训练。

Conclusion: ART方法通过自适应时间步长优化有效减少了扩散模型采样过程中的离散化误差，ART-RL框架提供了数据驱动的方式来学习最优时间调度，在多个数据集上表现出优越性能。

Abstract: We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.

</details>


### [314] [Explainability Methods for Hardware Trojan Detection: A Systematic Comparison](https://arxiv.org/abs/2601.18696)
*Paul Whitten,Francis Wolff,Chris Papachristou*

Main category: cs.LG

TL;DR: 本文比较了硬件木马检测中三种可解释性方法：基于电路属性的分析、基于案例的推理和模型无关的特征归因，发现前两种方法在领域对齐和基于先例的可解释性方面优于通用特征排名方法。


<details>
  <summary>Details</summary>
Motivation: 硬件木马检测需要准确识别和可解释的结果，以便安全工程师能够验证并采取行动。本研究旨在比较不同可解释性方法在门级木马检测中的效果，为实际部署提供指导。

Method: 使用XGBoost分类器在Trust-Hub基准测试上比较三种可解释性方法：(1)基于电路属性的分析（31个电路特定特征，包括门扇入模式、触发器距离和I/O连接性）；(2)基于案例的推理（k-最近邻算法）；(3)模型无关的特征归因（LIME、SHAP、梯度方法）。

Result: XGBoost分类器在11,392个测试样本上达到46.15%的精确率和52.17%的召回率，相比先前工作（Hasegawa等：5.13%）精确率提高了9倍，同时将误报率从5.6%降低到0.25%。基于案例的推理实现了97.4%的预测与训练示例对应性。LIME和SHAP的特征归因具有强相关性（r=0.94, p<0.001）。

Conclusion: 基于电路属性的分析和基于案例的推理方法在领域对齐和基于先例的可解释性方面优于通用特征排名方法，对于需要验证ML预测的实际部署场景具有重要意义。梯度归因方法运行速度比SHAP快481倍，但同样缺乏电路级上下文信息。

Abstract: Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).
  Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like "high fanin complexity near outputs indicates potential triggers." Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.
  XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.
  This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.

</details>


### [315] [Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/abs/2601.18699)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

TL;DR: 该论文对大型语言模型在顺序微调中的灾难性遗忘现象进行了机制分析，识别出梯度干扰、表征漂移和损失景观平坦化三大主要机制，并发现遗忘严重程度与任务相似度高度相关。


<details>
  <summary>Details</summary>
Motivation: 尽管观察到大型语言模型在顺序微调时会出现灾难性遗忘现象，但对其机制的理解仍然有限。该研究旨在深入分析Transformer架构LLMs在顺序微调中导致遗忘的具体机制。

Method: 通过在不同规模模型（109B到400B参数）和任务序列上进行系统实验，分析梯度干扰、表征漂移和损失景观平坦化等机制，并测量任务相似度与遗忘程度的相关性。

Result: 发现遗忘严重程度与任务相似度高度相关（Pearson r = 0.87），约15-23%的注意力头在微调过程中受到严重干扰，且低层网络对遗忘更为敏感。

Conclusion: 该研究为灾难性遗忘现象提供了机制性解释，为开发针对性的持续学习缓解策略建立了理论基础，有助于改进大型语言模型的顺序学习能力。

Abstract: Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.

</details>


### [316] [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](https://arxiv.org/abs/2601.18702)
*Hansheng Ren*

Main category: cs.LG

TL;DR: 论文挑战当前深度学习依赖浮点运算的范式，提出精确性假说：通用智能需要任意精度算术计算基础，并引入基于有理数运算的Halo架构和精确推理单元来解决大语言模型的逻辑不一致问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习范式过于注重计算吞吐量而忽视数值精度，认为智能源于大规模统计相关性。但作者认为这种基于IEEE 754浮点近似误差的积累会导致大语言模型出现"幻觉"和逻辑不一致问题，阻碍了高级因果推理能力的发展。

Method: 提出精确性假说，认为通用智能需要任意精度算术计算基础。设计了Halo架构，采用有理数算术（ℚ）作为计算基础，并开发了新颖的精确推理单元（EIU）来支持这一架构。

Result: 在Huginn-0125原型上的实证验证表明，6000亿参数的BF16基线在混沌系统中崩溃，而Halo架构能够无限期保持零数值发散，证明了精确算术在减少逻辑不确定性方面的有效性。

Conclusion: 精确算术是减少System 2 AGI逻辑不确定性的先决条件，Halo架构为实现通用智能提供了新的计算范式，为高级因果推理能力的发展奠定了基础。

Abstract: Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the "hallucinations" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.

</details>


### [317] [SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model](https://arxiv.org/abs/2601.18707)
*Jan Hagnberger,Mathias Niepert*

Main category: cs.LG

TL;DR: SMART是一种神经代理模型，仅使用点云表示几何形状，无需模拟网格即可在任意查询位置预测物理量，性能与依赖网格的方法相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模拟网格作为输入以减少预测误差，但为新几何生成网格计算成本高；而无需网格的方法通常误差较高。需要一种既能保持高性能又无需网格依赖的解决方案。

Method: SMART将几何形状和模拟参数编码到共享潜在空间，捕获物理场的结构和参数特征。物理解码器通过跨层交互关注编码器的中间潜在表示，将空间查询映射到物理量，联合更新潜在几何特征和演化物理场。

Result: 大量实验表明，SMART与依赖模拟网格输入的现有方法竞争且常优于它们，展示了其在工业级模拟中的能力。

Conclusion: SMART提供了一种高效、准确的网格无关神经代理模型，解决了依赖模拟网格的计算成本问题，同时保持了高性能预测能力。

Abstract: Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.

</details>


### [318] [Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data](https://arxiv.org/abs/2601.18728)
*Willem Diepeveen,Oscar Leong*

Main category: cs.LG

TL;DR: Riemannian AmbientFlow：一种从噪声或线性损坏观测中同时学习概率生成模型和底层非线性数据流形的框架，结合了数据驱动的黎曼几何和变分推断。


<details>
  <summary>Details</summary>
Motivation: 在许多科学和成像应用中，干净的样本不可用，只能观察到噪声或线性损坏的测量数据。此外，数据中的潜在结构（如流形几何）对于下游科学分析很重要，需要提取。

Method: 基于AmbientFlow的变分推断框架，通过归一化流诱导的数据驱动黎曼几何，通过拉回度量和黎曼自编码器提取流形结构。

Result: 在适当的几何正则化和测量条件下，学习到的模型能够以可控误差恢复底层数据分布，并获得平滑的双Lipschitz流形参数化。平滑解码器可作为逆问题的生成先验并具有恢复保证。

Conclusion: 该框架能够从损坏观测中同时学习生成模型和底层流形结构，在合成流形和MNIST数据集上得到了实证验证，为逆问题提供了理论保证的生成先验。

Abstract: Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.

</details>


### [319] [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734)
*Siyan Zhao,Zhihui Xie,Mengchen Liu,Jing Huang,Guan Pang,Feiyu Chen,Aditya Grover*

Main category: cs.LG

TL;DR: OPSD是一种自蒸馏框架，单个模型既当老师又当学生，通过不同上下文条件实现，无需额外教师模型，在数学推理基准上达到4-8倍token效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法存在分布不匹配问题，需要额外的大模型作为教师，且未充分利用推理数据集中的真实解信息。作者受启发于强大LLM能够理性分析外部特权推理轨迹并教授其弱化版本的直觉。

Method: 提出On-Policy Self-Distillation (OPSD)框架：单个模型同时作为教师和学生，教师策略基于特权信息（如已验证推理轨迹），学生策略仅看到问题。训练时最小化这两个分布在学生自身生成轨迹上的每token差异。

Result: 在多个数学推理基准测试中，相比GRPO等强化学习方法达到4-8倍的token效率提升，且性能优于离策略蒸馏方法。

Conclusion: OPSD通过自蒸馏机制有效解决了传统蒸馏方法的分布不匹配问题，无需额外教师模型，显著提升了推理效率和性能。

Abstract: Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.

</details>


### [320] [Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift](https://arxiv.org/abs/2601.18736)
*Jake Lyon,Ehsan Saeedizade,Shamik Sengupta*

Main category: cs.LG

TL;DR: 本研究调查了四种监督学习模型在IoT-23数据集上的恶意软件检测效果，发现树基模型在有限训练数据下表现优异，但性能会随时间推移而下降。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源有限、安全防护薄弱且部署环境复杂，容易成为网络攻击目标。机器学习为自动化恶意软件检测提供了可能，但需要轻量级且有效的模型。

Method: 使用IoT-23数据集，评估随机森林、LightGBM、逻辑回归和多层感知机四种监督学习模型在二进制和多类别分类任务中的表现，分析训练数据量敏感性和时间鲁棒性。

Result: 树基模型（随机森林、LightGBM）在准确率和泛化能力方面表现最佳，即使在有限训练数据下也能保持高性能；但所有模型性能都会随时间推移而下降，因为恶意软件多样性增加。

Conclusion: 研究强调了开发自适应、资源高效的机器学习模型对于在实际物联网环境中保障系统安全的重要性，需要能够应对不断演变的威胁环境。

Abstract: The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.

</details>


### [321] [Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback](https://arxiv.org/abs/2601.18751)
*Seyed Amir Hosseini,Maryam Abdolali,Amirhosein Tavakkoli,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.LG

TL;DR: TriTrust-PBRL (TTP) 是一个从多专家偏好反馈中联合学习共享奖励模型和专家特定信任参数的统一框架，能自动识别并反转对抗性偏好，而不是简单地丢弃噪声数据。


<details>
  <summary>Details</summary>
Motivation: 现实世界的偏好数据通常来自具有不同可靠性的异构标注者（准确、噪声、系统性对抗）。现有PBRL方法要么平等对待所有反馈，要么试图过滤不可靠来源，但在面对系统性提供错误偏好的对抗性标注者时都会失败。

Method: TTP框架联合学习共享奖励模型和专家特定信任参数。关键洞察是：信任参数在基于梯度的优化中自然演变为正值（信任）、接近零（忽略）或负值（翻转），使模型能够自动反转对抗性偏好并恢复有用信号，而不仅仅是丢弃被破坏的反馈。

Result: 在MetaWorld操作任务和DM Control运动任务等四个不同领域的各种破坏场景中，TTP实现了最先进的鲁棒性，在对抗性破坏下保持接近Oracle的性能，而标准PBRL方法则灾难性地失败。TTP成功地从包含可靠和对抗性标注者的混合专家池中学习，表现优于现有基线。

Conclusion: TTP提供了一个统一的框架，能够从异构专家偏好中稳健学习，自动识别并处理对抗性标注者，而无需超出标识索引的专家特征，并能与现有PBRL流程无缝集成。

Abstract: Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.

</details>


### [322] [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753)
*Xinyue Zeng,Junhong Lin,Yujun Yan,Feng Guo,Liang Shi,Jun Wu,Dawei Zhou*

Main category: cs.LG

TL;DR: 本文提出了幻觉风险边界理论框架，将LLM幻觉分解为数据驱动和推理驱动两种来源，并基于此开发了HalluGuard检测方法，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在医疗、法律等高风险领域存在幻觉问题，现有检测方法通常只针对单一来源且依赖任务特定启发式方法，难以推广到复杂场景。需要统一的框架来系统分析幻觉来源。

Method: 提出幻觉风险边界理论框架，将幻觉风险形式化分解为数据驱动（训练时失配）和推理驱动（推理时不稳定性）两个部分。在此基础上开发HalluGuard方法，利用NTK诱导的几何结构和捕获的表征来联合识别两种幻觉。

Result: 在10个多样化基准测试、11个竞争性基线和9个流行LLM骨干网络上评估HalluGuard，在所有测试中一致达到最先进的幻觉检测性能。

Conclusion: 幻觉风险边界为理解LLM幻觉提供了理论基础，HalluGuard作为基于该理论的检测方法，能够有效识别多种形式的幻觉，解决了现有方法的局限性。

Abstract: The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.

</details>


### [323] [Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values](https://arxiv.org/abs/2601.18760)
*Henry Bell,Lara Neubauer da Costa Schertel,Bochu Ding,Brandon Fain*

Main category: cs.LG

TL;DR: GCAI框架通过整合用户通用期望和交互偏好生成AI宪法原则，相比现有方法更受人类偏好，被认为更具道德基础、连贯性和多元性。


<details>
  <summary>Details</summary>
Motivation: 现有宪法AI框架中，如何公平确定代表广泛利益相关者价值观的宪法原则存在挑战。需要统一框架既能反映用户对AI的通用期望，又能捕捉交互时的具体偏好。

Method: 扩展逆向宪法AI方法，从人类偏好标注数据中生成情境性原则，利用人类提供的偏好原因；同时从用户关于AI的价值观陈述中提取通用原则，形成完整的宪法框架。

Result: GCAI生成的宪法比ICAI生成的宪法更受人类偏好，既适用于个人使用也适用于广泛AI行为治理。参与者认为GCAI宪法更具道德基础、连贯性和多元性。

Conclusion: GCAI为AI对齐提供了更全面、更具代表性的宪法生成框架，能够更好地整合通用价值观和情境性偏好，促进更公平、多元的AI治理。

Abstract: A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.

</details>


### [324] [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777)
*Abhishek Divekar,Anirban Majumder*

Main category: cs.LG

TL;DR: 提出了PRECISE框架，结合少量人工标注和LLM判断来评估搜索/RAG系统质量，显著减少标注需求，同时校正LLM偏差


<details>
  <summary>Details</summary>
Motivation: 传统搜索、排序和RAG系统评估需要大量人工相关性标注，成本高昂。虽然LLM可以作为自动评估工具，但其固有偏差限制了直接用于指标估计的可靠性

Method: 扩展预测驱动推理(PPI)的统计框架PRECISE，结合少量人工标注（100个查询）和大量LLM判断（10,000个未标注样本），将计算复杂度从O(2^|C|)降低到O(2^K)，其中|C|为语料库规模

Result: 在多个知名检索数据集上的实验表明，该方法减少了关键业务指标Precision@K的估计方差，在低资源设置下有效校正了LLM偏差，显著降低标注需求

Conclusion: PRECISE框架通过结合少量人工标注和LLM判断，为搜索/RAG系统评估提供了可靠、高效的解决方案，大幅降低了传统方法的高昂标注成本

Abstract: Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

</details>


### [325] [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778)
*Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe*

Main category: cs.LG

TL;DR: SOAR是一个基于元强化学习的自我改进框架，让教师模型生成合成问题来训练学生模型，通过学生的进步作为奖励信号，能够在初始成功率为0的困难数学问题上实现学习突破。


<details>
  <summary>Details</summary>
Motivation: 当大型推理模型在初始成功率极低的数据集上进行微调时，由于训练信号稀疏，强化学习方法会陷入学习停滞。研究者想知道预训练LLM是否能够利用潜在知识为自身无法解决的问题生成自动化课程。

Method: 设计SOAR框架：教师模型副本为学生模型副本生成合成问题，教师通过学生在少量困难问题上的进步获得奖励。该方法基于测量的学生进展而非内在代理奖励来建立课程。

Result: 在初始成功率为0/128的最难数学基准测试中：1)实现了双层级元强化学习，解锁了稀疏二元奖励下的学习能力；2)基于进展的奖励优于先前LLM自对弈中使用的内在奖励方案，避免了不稳定性和多样性崩溃；3)生成问题的结构质量和良好定义性比解决方案正确性对学习进展更重要。

Conclusion: 研究结果表明，生成有用"垫脚石"的能力并不需要预先具备解决困难问题的能力，这为无需额外策划数据就能突破推理停滞提供了原则性路径。

Abstract: Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.

</details>


### [326] [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779)
*Yuxiao Qu,Amrith Setlur,Virginia Smith,Ruslan Salakhutdinov,Aviral Kumar*

Main category: cs.LG

TL;DR: 该论文提出了POPE方法，使用特权信息引导强化学习在困难问题上的探索，解决了现有RL方法在困难推理问题上探索失败的问题。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在提升大语言模型推理能力时，在困难问题上经常完全无法探索到正确解，导致零奖励信号和学习停滞。传统的探索方法如熵奖励、重要性比率调整等对此无效，而混合难易问题的训练反而会产生"射线干扰"现象，抑制对困难问题的学习。

Method: 提出了特权在线策略探索（POPE）方法，利用人类或其他oracle的解作为特权信息来引导困难问题的探索。具体来说，在困难问题前添加oracle解的前缀，让RL能在引导下获得非零奖励，同时通过指令遵循和推理的协同作用，使学习到的行为能迁移回原始未引导的问题。

Result: POPE显著扩展了可解决的问题范围，在多个具有挑战性的推理基准测试中大幅提升了性能表现。

Conclusion: POPE通过特权信息引导探索，有效解决了RL在困难推理问题上的探索瓶颈问题，为大语言模型的强化学习训练提供了新的有效方法。

Abstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

</details>


### [327] [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783)
*Deepthi Pathare,Leo Laine,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 提出基于PPO的多目标强化学习框架，为重型车辆学习连续Pareto最优策略集，平衡安全、能源效率和驾驶时间效率的权衡


<details>
  <summary>Details</summary>
Motivation: 高速公路驾驶中，重型车辆需要在安全、效率和运营成本之间进行复杂权衡决策。传统标量奖励函数通过聚合这些竞争目标，往往模糊了它们之间的权衡结构。

Method: 基于近端策略优化（PPO）的多目标强化学习框架，学习一个连续的政策集，明确表示安全、能源效率和驾驶时间效率三个冲突目标之间的权衡。

Result: 该方法学习到一个平滑且可解释的连续Pareto最优策略集，能够在不同冲突目标之间灵活选择驾驶行为，无需重新训练即可在不同驾驶策略之间无缝切换。

Conclusion: 该框架为自动驾驶卡车应用提供了稳健且自适应的决策策略，通过显式表示目标权衡，实现了对驾驶行为的灵活控制。

Abstract: Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.

</details>


### [328] [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795)
*Amrith Setlur,Zijian Wang,Andrew Cohen,Paria Rashidinejad,Sang Michael Xie*

Main category: cs.LG

TL;DR: PrefixRL是一种新的强化学习方法，通过重用旧的计算资源（离策略轨迹）来提升大语言模型推理任务的训练效率，避免传统离策略方法的不稳定性，在困难推理问题上比基线方法快2倍且最终奖励提升3倍。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在处理困难推理问题时存在计算浪费问题：正确轨迹稀少、策略梯度消失、学习停滞。同时，重用旧的计算资源（离策略轨迹）会引发不稳定性。

Method: 提出PrefixRL方法：基于成功离策略轨迹的前缀进行条件化，然后运行在策略强化学习来完成剩余部分。通过调节前缀长度来控制问题难度，并利用基础模型通过拒绝采样获取离策略轨迹，形成自我改进循环。

Result: PrefixRL在困难推理问题上达到相同训练奖励的速度比最强基线（在离策略数据上进行监督微调后再强化学习）快2倍，最终奖励提升3倍。效果可迁移到未见过的基准测试，且当离策略轨迹来自不同模型家族时仍然有效。

Conclusion: PrefixRL通过重用离策略轨迹的前缀来提升强化学习效率，避免了传统离策略方法的不稳定性，在困难推理任务上显著优于现有方法，具有实际应用灵活性。

Abstract: Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [329] [Breaking Task Impasses Quickly: Adaptive Neuro-Symbolic Learning for Open-World Robotics](https://arxiv.org/abs/2601.16985)
*Pierrick Lorang*

Main category: cs.RO

TL;DR: 该论文提出了一个结合分层抽象、任务与运动规划以及强化学习的神经符号框架，用于实现机器人快速适应开放世界环境中的不可预见变化。


<details>
  <summary>Details</summary>
Motivation: 开放世界环境中存在不可预见的新颖变化，而现有的混合规划和强化学习方法存在样本效率低、适应速度慢、灾难性遗忘等问题，限制了自主系统在真实世界的部署能力。

Method: 提出了一个神经符号框架，整合了分层抽象、任务与运动规划（TAMP）和强化学习。该架构结合了符号化目标导向学习和基于世界模型的探索，以促进对环境变化的快速适应。

Result: 在机器人操作和自动驾驶任务中验证，该方法相比最先进的混合方法实现了更快的收敛速度、更高的样本效率和更强的鲁棒性。

Conclusion: 该神经符号框架展示了在真实世界部署的潜力，能够有效解决开放世界环境中机器人快速适应的问题。

Abstract: Adapting to unforeseen novelties in open-world environments remains a major challenge for autonomous systems. While hybrid planning and reinforcement learning (RL) approaches show promise, they often suffer from sample inefficiency, slow adaptation, and catastrophic forgetting. We present a neuro-symbolic framework integrating hierarchical abstractions, task and motion planning (TAMP), and reinforcement learning to enable rapid adaptation in robotics. Our architecture combines symbolic goal-oriented learning and world model-based exploration to facilitate rapid adaptation to environmental changes. Validated in robotic manipulation and autonomous driving, our approach achieves faster convergence, improved sample efficiency, and superior robustness over state-of-the-art hybrid methods, demonstrating its potential for real-world deployment.

</details>


### [330] [Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap](https://arxiv.org/abs/2601.17219)
*David Wireko Atibila,Vineet R. Kamat,Carol C. Menassa*

Main category: cs.RO

TL;DR: 构建了一个六层分类法，用于评估建筑领域人机协作的即兴能力，发现当前研究集中在较低层次，存在向真正协作即兴发展的关键障碍。


<details>
  <summary>Details</summary>
Motivation: 建筑行业面临生产力停滞、熟练劳动力短缺和安全问题。虽然机器人自动化提供了解决方案，但建筑机器人在适应非结构化、动态的工地时存在困难。即兴能力——通过创造性问题解决来适应意外情况——目前主要依赖人类。在建筑不可预测的环境中，协作式人机即兴对于工作流程连续性至关重要。

Method: 通过系统综述2010-2025年的214篇文章，开发了一个六层分类法，将建筑机器人技术分为：人工工作（0级）、人控执行（1级）、自适应操作（2级）、模仿学习（3级）、人在环BIM工作流程（4级）、基于云的知识集成（5级）和真正协作即兴（6级）。使用五维雷达框架（规划、认知角色、物理执行、学习能力、即兴）分析渐进演化。

Result: 分析显示当前研究集中在较低层次，在经验学习和向协作即兴发展方面存在关键差距。识别了三个基本障碍：接地和对话推理的技术限制、人类即兴与机器人研究之间的概念差距、方法论挑战。五维雷达框架展示了互补的人机能力如何创造超越个体贡献的团队绩效。

Conclusion: 建议未来研究重点通过增强/虚拟现实界面、大语言模型集成和基于云的知识系统来改进人机通信，以推进真正的协作即兴。分类法和框架为评估建筑机器人即兴能力提供了系统化方法，并指出了实现更高级别协作的路径。

Abstract: The construction industry faces productivity stagnation, skilled labor shortages, and safety concerns. While robotic automation offers solutions, construction robots struggle to adapt to unstructured, dynamic sites. Central to this is improvisation, adapting to unexpected situations through creative problem-solving, which remains predominantly human. In construction's unpredictable environments, collaborative human-robot improvisation is essential for workflow continuity. This research develops a six-level taxonomy classifying human-robot collaboration (HRC) based on improvisation capabilities. Through systematic review of 214 articles (2010-2025), we categorize construction robotics across: Manual Work (Level 0), Human-Controlled Execution (Level 1), Adaptive Manipulation (Level 2), Imitation Learning (Level 3), Human-in-Loop BIM Workflow (Level 4), Cloud-Based Knowledge Integration (Level 5), and True Collaborative Improvisation (Level 6). Analysis reveals current research concentrates at lower levels, with critical gaps in experiential learning and limited progression toward collaborative improvisation. A five-dimensional radar framework illustrates progressive evolution of Planning, Cognitive Role, Physical Execution, Learning Capability, and Improvisation, demonstrating how complementary human-robot capabilities create team performance exceeding individual contributions. The research identifies three fundamental barriers: technical limitations in grounding and dialogic reasoning, conceptual gaps between human improvisation and robotics research, and methodological challenges. We recommend future research emphasizing improved human-robot communication via Augmented/Virtual Reality interfaces, large language model integration, and cloud-based knowledge systems to advance toward true collaborative improvisation.

</details>


### [331] [Hierarchical Informative Path Planning via Graph Guidance and Trajectory Optimization](https://arxiv.org/abs/2601.17227)
*Avraiem Iskandar,Shamak Dutta,Kevin Murrant,Yash Vardhan Pant,Stephen L. Smith*

Main category: cs.RO

TL;DR: 提出一个三层级框架解决有旅行预算的IP问题：图规划→分段预算分配→样条细化，在杂乱环境中结合全局指导与局部优化，实现更低后验不确定性和更快计算速度


<details>
  <summary>Details</summary>
Motivation: 解决在杂乱环境中信息路径规划的问题。现有方法存在局限：基于图的求解器提供全局保证但假设预选测量位置，连续轨迹优化支持路径感知但计算密集且在障碍密集环境中对初始化敏感。

Method: 提出三层级框架：1) 基于图的全局规划；2) 使用几何和核边界的段式预算分配；3) 每段样条细化，包含硬约束和障碍修剪。结合全局指导与局部细化。

Result: 方法在合成杂乱环境和北极数据集上，相比仅基于图的方法和连续基准方法，实现了更低的后验不确定性，同时计算速度更快：比基于梯度的方法快9倍，比黑盒优化器快20倍。

Conclusion: 通过分层框架结合全局规划与局部优化，有效解决了在预算约束下杂乱环境中的信息路径规划问题，在不确定性和计算效率方面均优于现有方法。

Abstract: We study informative path planning (IPP) with travel budgets in cluttered environments, where an agent collects measurements of a latent field modeled as a Gaussian process (GP) to reduce uncertainty at target locations. Graph-based solvers provide global guarantees but assume pre-selected measurement locations, while continuous trajectory optimization supports path-based sensing but is computationally intensive and sensitive to initialization in obstacle-dense settings. We propose a hierarchical framework with three stages: (i) graph-based global planning, (ii) segment-wise budget allocation using geometric and kernel bounds, and (iii) spline-based refinement of each segment with hard constraints and obstacle pruning. By combining global guidance with local refinement, our method achieves lower posterior uncertainty than graph-only and continuous baselines, while running faster than continuous-space solvers (up to 9x faster than gradient-based methods and 20x faster than black-box optimizers) across synthetic cluttered environments and Arctic datasets.

</details>


### [332] [Real-Time, Energy-Efficient, Sampling-Based Optimal Control via FPGA Acceleration](https://arxiv.org/abs/2601.17231)
*Tanmay Desai,Brian Plancher,R. Iris Bahar*

Main category: cs.RO

TL;DR: 该论文提出了一种针对FPGA优化的MPPI设计，用于自主移动机器人的快速低能耗规划控制，相比嵌入式GPU和CPU分别实现3.1倍到7.5倍加速和2.5倍到5.4倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 自主移动机器人（AMR）需要快速鲁棒的规划控制方案，但现有基于GPU和CPU的采样模型预测控制算法在嵌入式平台上难以满足严格的能耗和延迟要求。

Method: 设计了针对FPGA优化的MPPI（模型预测路径积分控制）架构，通过细粒度并行化、深度流水线和算法阶段并行化来消除同步瓶颈。

Result: 相比优化的嵌入式GPU和CPU实现，分别获得平均3.1倍到7.5倍速度提升，同时能耗降低2.5倍到5.4倍。

Conclusion: FPGA架构是实现高能效高性能边缘机器人系统的有前景方向，特别适合电池受限的自主移动机器人平台。

Abstract: Autonomous mobile robots (AMRs), used for search-and-rescue and remote exploration, require fast and robust planning and control schemes. Sampling-based approaches for Model Predictive Control, especially approaches based on the Model Predictive Path Integral Control (MPPI) algorithm, have recently proven both to be highly effective for such applications and to map naturally to GPUs for hardware acceleration. However, both GPU and CPU implementations of such algorithms can struggle to meet tight energy and latency budgets on battery-constrained AMR platforms that leverage embedded compute. To address this issue, we present an FPGA-optimized MPPI design that exposes fine-grained parallelism and eliminates synchronization bottlenecks via deep pipelining and parallelism across algorithmic stages. This results in an average 3.1x to 7.5x speedup over optimized implementations on an embedded GPU and CPU, respectively, while simultaneously achieving a 2.5x to 5.4x reduction in energy usage. These results demonstrate that FPGA architectures are a promising direction for energy-efficient and high-performance edge robotics.

</details>


### [333] [Quantifying Ergonomics in the Elevate Soft Robotic Suit](https://arxiv.org/abs/2601.17249)
*Peter Bryan,Rejin John Varghese,Dario Farina*

Main category: cs.RO

TL;DR: 研究人员对Elevate软体机器人外骨骼进行了人机工程学和舒适度评估，该设备通过电缆驱动辅助肩部提升。实验测量了辅助肩部提升至70度时的压力分布和体积压缩，结果显示压力在人体可接受范围内，且未报告不适感。


<details>
  <summary>Details</summary>
Motivation: 软体机器人外骨骼具有轻量化、低成本、体积小等优势，适合日常康复和辅助使用，但其数据驱动、用户定制和舒适度优先的设计挑战限制了广泛应用。本研究旨在评估Elevate外骨骼的人机工程学和舒适度。

Method: 使用运动捕捉系统和力传感器，测量Elevate外骨骼在辅助肩部提升至70度时的人机工程学性能。进行两次4小时实验，传输高达200N的电缆张力，测量压力分布并估算躯干和上臂的体积压缩率。

Result: 实验未报告不适感，肩部压力估计在69.1-85.1kPa范围内（相当于人手抓握压力），躯干体积压缩<3%，上臂体积压缩<8%。

Conclusion: Elevate外骨骼的人机工程学设计得到初步验证，压力分布和压缩率均在可接受范围内，为未来患者群体研究奠定了基础。

Abstract: Soft robotic suits have the potential to rehabilitate, assist, and augment the human body. The low weight, cost, and minimal form-factor of these devices make them ideal for daily use by both healthy and impaired individuals. However, challenges associated with data-driven, user-specific, and comfort-first design of human-robot interfaces using soft materials limit their widespread translation and adoption. In this work, we present the quantitative evaluation of ergonomics and comfort of the Elevate suit - a cable driven soft robotic suit that assists shoulder elevation. Using a motion-capture system and force sensors, we measured the suit's ergonomics during assisted shoulder elevation up to 70 degrees. Two 4-hour sessions were conducted with one subject, involving transmitting cable tensions of up to 200N with no discomfort reported. We estimated that the pressure applied to the shoulder during assisted movements was within the range seen in a human grasp (approximately 69.1-85.1kPa), and estimated volumetric compression of <3% and <8% across the torso and upper arm, respectively. These results provide early validation of Elevate's ergonomic design in preparation for future studies with patient groups.

</details>


### [334] [EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects](https://arxiv.org/abs/2601.17251)
*Yunuo Chen,Yafei Hu,Lingfeng Sun,Tushar Kusnur,Laura Herlant,Chenfanfu Jiang*

Main category: cs.RO

TL;DR: 本文提出EMPM框架，基于可微分MPM物理模拟器，从多视角RGB-D视频重建并优化复杂可变形物体的动态模型，实现更准确、自适应的物理感知表示。


<details>
  <summary>Details</summary>
Motivation: 现有可变形物体建模方法要么过度简化复杂动力学，要么需要大量训练数据，限制了泛化能力。需要一种既能捕捉真实物理特性又数据高效的框架。

Method: 基于可微分MPM物理模拟器，从多视角RGB-D视频重建几何与外观，通过最小化预测与观测视觉数据的差异来模拟物体行为，并在线优化MPM参数。

Result: EMPM在实验中优于弹簧-质量基线模型，能够实现自适应、鲁棒且物理感知的物体表示，为机器人操作复杂可变形物体开辟新可能。

Conclusion: EMPM框架能够有效建模复杂可变形物体的动力学特性，通过物理模拟与视觉反馈的在线优化，实现了更准确、自适应的物体表示，具有实际应用价值。

Abstract: Modeling deformable objects - especially continuum materials - in a way that is physically plausible, generalizable, and data-efficient remains challenging across 3D vision, graphics, and robotic manipulation. Many existing methods oversimplify the rich dynamics of deformable objects or require large training sets, which often limits generalization. We introduce embodied MPM (EMPM), a deformable object modeling and simulation framework built on a differentiable Material Point Method (MPM) simulator that captures the dynamics of challenging materials. From multi-view RGB-D videos, our approach reconstructs geometry and appearance, then uses an MPM physics engine to simulate object behavior by minimizing the mismatch between predicted and observed visual data. We further optimize MPM parameters online using sensory feedback, enabling adaptive, robust, and physics-aware object representations that open new possibilities for robotic manipulation of complex deformables. Experiments show that EMPM outperforms spring-mass baseline models. Project website: https://embodied-mpm.github.io.

</details>


### [335] [Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots](https://arxiv.org/abs/2601.17287)
*Yanrong Chen,Xihan Bian*

Main category: cs.RO

TL;DR: 提出用于NAO机器人的实时多模态交互框架，通过LLM同步生成文本和动作描述，实现语音韵律与全身手势的情感同步


<details>
  <summary>Details</summary>
Motivation: 随着人形机器人越来越多地进入社交场景，实现情感同步的多模态交互仍然是一个重大挑战。为了促进人形机器人在服务角色中的进一步采用和整合，需要解决语音与动作的情感协调问题。

Method: 提出三个关键技术：(1) 双通道情感引擎，LLM同时生成上下文感知的文本响应和生物力学可行的运动描述；(2) 持续时间感知的动态时间规整，精确对齐语音输出和运动关键帧；(3) 闭环可行性验证，通过实时适应确保手势符合NAO的物理关节限制。

Result: 评估显示，与基于规则的系统相比，情感对齐提高了21%。通过协调声调（由唤醒驱动）与上肢运动学，同时保持下半身稳定性来实现。

Conclusion: 该框架通过实现无缝的感觉运动协调，推进了上下文感知社交机器人在动态应用中的部署，如个性化医疗、互动教育和响应式客户服务平台。

Abstract: As humanoid robots increasingly introduced into social scene, achieving emotionally synchronized multimodal interaction remains a significant challenges. To facilitate the further adoption and integration of humanoid robots into service roles, we present a real-time framework for NAO robots that synchronizes speech prosody with full-body gestures through three key innovations: (1) A dual-channel emotion engine where large language model (LLM) simultaneously generates context-aware text responses and biomechanically feasible motion descriptors, constrained by a structured joint movement library; (2) Duration-aware dynamic time warping for precise temporal alignment of speech output and kinematic motion keyframes; (3) Closed-loop feasibility verification ensuring gestures adhere to NAO's physical joint limits through real-time adaptation. Evaluations show 21% higher emotional alignment compared to rule-based systems, achieved by coordinating vocal pitch (arousal-driven) with upper-limb kinematics while maintaining lower-body stability. By enabling seamless sensorimotor coordination, this framework advances the deployment of context-aware social robots in dynamic applications such as personalized healthcare, interactive education, and responsive customer service platforms.

</details>


### [336] [Eye-Tracking-Driven Control in Daily Task Assistance for Assistive Robotic Arms](https://arxiv.org/abs/2601.17404)
*Anke Fischer-Janzen,Thomas M. Wendt,Kristof Van Laerhoven*

Main category: cs.RO

TL;DR: 提出一个基于眼动追踪的控制框架，帮助重度身体残疾人士通过注视任务象形图选择对象和任务，实现独立完成日常任务，准确率达97.9%


<details>
  <summary>Details</summary>
Motivation: 当前眼动追踪驱动的人机交互方法存在3D注视估计准确性问题和多任务区分困难，需要为重度身体残疾人士开发更有效的独立任务执行系统

Method: 使用任务象形图作为基准标记，结合特征匹配方法，将选择的对象数据传输给眼在手配置系统进行必要测量，无需用户相对于对象的位置信息

Result: 框架在高达97.9%的测量中正确解释对象和任务选择，评估中发现的问题已改进并分享为经验教训，开源框架可适应新任务和对象

Conclusion: 该眼动追踪控制框架为重度残疾人士提供了有效的独立任务执行方案，通过开源设计和集成先进对象检测模型，具有良好的适应性和扩展性

Abstract: Shared control improves Human-Robot Interaction by reducing the user's workload and increasing the robot's autonomy. It allows robots to perform tasks under the user's supervision. Current eye-tracking-driven approaches face several challenges. These include accuracy issues in 3D gaze estimation and difficulty interpreting gaze when differentiating between multiple tasks. We present an eye-tracking-driven control framework, aimed at enabling individuals with severe physical disabilities to perform daily tasks independently. Our system uses task pictograms as fiducial markers combined with a feature matching approach that transmits data of the selected object to accomplish necessary task related measurements with an eye-in-hand configuration. This eye-tracking control does not require knowledge of the user's position in relation to the object. The framework correctly interpreted object and task selection in up to 97.9% of measurements. Issues were found in the evaluation, that were improved and shared as lessons learned. The open-source framework can be adapted to new tasks and objects due to the integration of state-of-the-art object detection models.

</details>


### [337] [DiffusionCinema: Text-to-Aerial Cinematography](https://arxiv.org/abs/2601.17412)
*Valerii Serpiva,Artem Lykov,Jeffrin Sam,Aleksey Fedoseev,Dzmitry Tsetserukou*

Main category: cs.RO

TL;DR: 提出一个基于扩散模型的无人机创意拍摄系统，用户通过自然语言描述期望镜头，系统自动生成并执行最优飞行轨迹来拍摄匹配提示的影片


<details>
  <summary>Details</summary>
Motivation: 传统无人机操控需要专业技能和手动操作，用户难以精确实现复杂创意镜头。需要一种更直观、易用的交互方式，让普通用户也能通过自然语言描述来获得专业级航拍效果

Method: 系统结合扩散模型、自然语言处理和无人机控制技术。用户输入文本提示，系统编码提示和初始视觉快照，扩散模型采样满足场景几何和镜头语义的时空运动计划，生成最优飞行轨迹后由无人机自主执行

Result: NASA-TLX评估显示系统显著降低用户工作量（M=21.6 vs 58.1），心理需求（11.5 vs 60.5）和挫败感（14.0 vs 54.5）也大幅降低。系统能生成平滑、可重复的视频片段，准确匹配用户文本描述

Conclusion: 该研究展示了"文本到影院飞行"的新交互范式，扩散模型作为"创意操作员"直接将故事意图转换为空中运动，为无人机创意拍摄提供了更直观、易用的解决方案

Abstract: We propose a novel Unmanned Aerial Vehicles (UAV) assisted creative capture system that leverages diffusion models to interpret high-level natural language prompts and automatically generate optimal flight trajectories for cinematic video recording. Instead of manually piloting the drone, the user simply describes the desired shot (e.g., "orbit around me slowly from the right and reveal the background waterfall"). Our system encodes the prompt along with an initial visual snapshot from the onboard camera, and a diffusion model samples plausible spatio-temporal motion plans that satisfy both the scene geometry and shot semantics. The generated flight trajectory is then executed autonomously by the UAV to record smooth, repeatable video clips that match the prompt. User evaluation using NASA-TLX showed a significantly lower overall workload with our interface (M = 21.6) compared to a traditional remote controller (M = 58.1), demonstrating a substantial reduction in perceived effort. Mental demand (M = 11.5 vs. 60.5) and frustration (M = 14.0 vs. 54.5) were also markedly lower for our system, confirming clear usability advantages in autonomous text-driven flight control. This project demonstrates a new interaction paradigm: text-to-cinema flight, where diffusion models act as the "creative operator" converting story intentions directly into aerial motion.

</details>


### [338] [Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.17428)
*Ziming Li,Chenhao Li,Marco Hutter*

Main category: cs.RO

TL;DR: 提出LP-ACRL框架，通过在线估计学习进度自动调整任务采样分布，无需先验难度知识，使ANYmal D四足机器人在复杂地形上实现高速稳定运动


<details>
  <summary>Details</summary>
Motivation: 传统课程学习在扩展到复杂广泛的任务空间时面临困难，因为这些空间通常缺乏明确定义的难度结构，难以定义所需的难度排序

Method: LP-ACRL框架通过在线估计智能体的学习进度，自适应调整任务采样分布，实现无需任务空间难度分布先验知识的自动课程生成

Result: LP-ACRL训练的策略使ANYmal D四足机器人能在楼梯、斜坡、碎石、低摩擦平面等多种地形上实现并保持2.5 m/s线速度和3.0 rad/s角速度的稳定高速运动，而先前方法通常仅限于平坦地形高速或复杂地形低速

Conclusion: LP-ACRL展示了强大的可扩展性和实际应用价值，为复杂广泛机器人学习任务空间的课程生成研究提供了稳健基准

Abstract: Curriculum learning has demonstrated substantial effectiveness in robot learning. However, it still faces limitations when scaling to complex, wide-ranging task spaces. Such task spaces often lack a well-defined difficulty structure, making the difficulty ordering required by previous methods challenging to define. We propose a Learning Progress-based Automatic Curriculum Reinforcement Learning (LP-ACRL) framework, which estimates the agent's learning progress online and adaptively adjusts the task-sampling distribution, thereby enabling automatic curriculum generation without prior knowledge of the difficulty distribution over the task space. Policies trained with LP-ACRL enable the ANYmal D quadruped to achieve and maintain stable, high-speed locomotion at 2.5 m/s linear velocity and 3.0 rad/s angular velocity across diverse terrains, including stairs, slopes, gravel, and low-friction flat surfaces--whereas previous methods have generally been limited to high speeds on flat terrain or low speeds on complex terrain. Experimental results demonstrate that LP-ACRL exhibits strong scalability and real-world applicability, providing a robust baseline for future research on curriculum generation in complex, wide-ranging robotic learning task spaces.

</details>


### [339] [PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes](https://arxiv.org/abs/2601.17440)
*Xinru Cui,Linxi Feng,Yixuan Zhou,Haoqi Han,Zhe Liu,Hesheng Wang*

Main category: cs.RO

TL;DR: 提出PILOT框架，通过单阶段强化学习实现感知式全身控制，融合感知运动与全身操作，提升人形机器人在非结构化环境中的稳定性和任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 现有全身控制器缺乏对外部环境的感知能力，在复杂非结构化场景中执行任务时稳定性不足，需要能够同时实现精确运动与灵巧操作的统一控制器。

Method: 提出PILOT框架：1）跨模态上下文编码器融合基于预测的本体感知特征和基于注意力的感知表示；2）专家混合策略架构协调不同运动技能；3）单阶段强化学习框架统一感知运动与全身控制。

Result: 在仿真和Unitree G1人形机器人上的实验表明，PILOT在稳定性、指令跟踪精度和地形穿越能力方面优于现有基线方法。

Conclusion: PILOT框架可作为非结构化场景中运动操作的鲁棒基础低级控制器，为复杂环境中的感知式全身控制提供了有效解决方案。

Abstract: Humanoid robots hold great potential for diverse interactions and daily service tasks within human-centered environments, necessitating controllers that seamlessly integrate precise locomotion with dexterous manipulation. However, most existing whole-body controllers lack exteroceptive awareness of the surrounding environment, rendering them insufficient for stable task execution in complex, unstructured scenarios.To address this challenge, we propose PILOT, a unified single-stage reinforcement learning (RL) framework tailored for perceptive loco-manipulation, which synergizes perceptive locomotion and expansive whole-body control within a single policy. To enhance terrain awareness and ensure precise foot placement, we design a cross-modal context encoder that fuses prediction-based proprioceptive features with attention-based perceptive representations. Furthermore, we introduce a Mixture-of-Experts (MoE) policy architecture to coordinate diverse motor skills, facilitating better specialization across distinct motion patterns. Extensive experiments in both simulation and on the physical Unitree G1 humanoid robot validate the efficacy of our framework. PILOT demonstrates superior stability, command tracking precision, and terrain traversability compared to existing baselines. These results highlight its potential to serve as a robust, foundational low-level controller for loco-manipulation in unstructured scenes.

</details>


### [340] [EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds](https://arxiv.org/abs/2601.17486)
*Zhiyuan Zhang,Yu She*

Main category: cs.RO

TL;DR: EquiForm：一个噪声鲁棒的SE(3)-等变策略学习框架，用于点云视觉模仿学习，通过几何去噪模块和对比等变对齐目标提升对传感器噪声、姿态扰动和遮挡的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于点云的视觉模仿学习方法对传感器噪声、姿态扰动和遮挡伪影高度敏感，这些因素会扭曲几何结构并破坏鲁棒泛化所需的等变假设。现有等变方法主要将对称约束编码到神经网络架构中，但未明确纠正噪声引起的几何偏差或强制学习表示中的等变一致性。

Method: 1. 形式化噪声引起的几何失真如何导致观察到动作映射的等变偏差；2. 引入几何去噪模块，在噪声或不完整观察下恢复一致的3D结构；3. 提出对比等变对齐目标，在刚性变换和噪声扰动下强制表示一致性；4. 构建集成噪声鲁棒几何推理与现代生成模型的灵活策略学习管道。

Result: 在16个模拟任务和4个真实世界操作任务上进行评估，相比最先进的点云模仿学习方法，模拟任务平均提升17.2%，真实世界实验平均提升28.1%，展示了强大的噪声鲁棒性和空间泛化能力。

Conclusion: EquiForm通过将几何去噪和等变对齐相结合，显著提高了点云策略对噪声和几何失真的鲁棒性，为实际机器人操作中的视觉模仿学习提供了更可靠的解决方案。

Abstract: Visual imitation learning with 3D point clouds has advanced robotic manipulation by providing geometry-aware, appearance-invariant observations. However, point cloud-based policies remain highly sensitive to sensor noise, pose perturbations, and occlusion-induced artifacts, which distort geometric structure and break the equivariance assumptions required for robust generalization. Existing equivariant approaches primarily encode symmetry constraints into neural architectures, but do not explicitly correct noise-induced geometric deviations or enforce equivariant consistency in learned representations. We introduce EquiForm, a noise-robust SE(3)-equivariant policy learning framework for point cloud-based manipulation. EquiForm formalizes how noise-induced geometric distortions lead to equivariance deviations in observation-to-action mappings, and introduces a geometric denoising module to restore consistent 3D structure under noisy or incomplete observations. In addition, we propose a contrastive equivariant alignment objective that enforces representation consistency under both rigid transformations and noise perturbations. Built upon these components, EquiForm forms a flexible policy learning pipeline that integrates noise-robust geometric reasoning with modern generative models. We evaluate EquiForm on 16 simulated tasks and 4 real-world manipulation tasks across diverse objects and scene layouts. Compared to state-of-the-art point cloud imitation learning methods, EquiForm achieves an average improvement of 17.2% in simulation and 28.1% in real-world experiments, demonstrating strong noise robustness and spatial generalization.

</details>


### [341] [MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions](https://arxiv.org/abs/2601.17507)
*Yutong Shen,Hangxu Liu,Kailin Pei,Ruizhe Xia,Tongtong Feng*

Main category: cs.RO

TL;DR: MetaWorld提出了一个分层世界模型，通过专家策略迁移来整合语义规划和物理控制，解决了人形机器人操作中的语义-物理鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人操作方法存在三个主要限制：强化学习的样本效率低、模仿学习的泛化能力差、以及视觉语言模型(VLMs)的物理不一致性。这些限制导致了语义-物理鸿沟，阻碍了机器人在复杂环境中的操作能力。

Method: MetaWorld采用分层世界模型架构，将任务解耦为VLM驱动的语义层和在紧凑状态空间中运行的潜在动力学模型。通过动态专家选择和运动先验融合机制，利用预训练的多专家策略库作为可迁移知识，实现高效在线适应的两阶段框架。VLMs作为语义接口，将指令映射为可执行技能，绕过符号基础问题。

Result: 在Humanoid-Bench上的实验表明，MetaWorld在世界模型基础的强化学习方法中，在任务完成率和运动连贯性方面表现更优。

Conclusion: MetaWorld通过分层世界模型和专家策略迁移，有效解决了人形机器人操作中的语义-物理鸿沟问题，为机器人操作提供了更高效和通用的解决方案。

Abstract: Humanoid robot loco-manipulation remains constrained by the semantic-physical gap. Current methods face three limitations: Low sample efficiency in reinforcement learning, poor generalization in imitation learning, and physical inconsistency in VLMs. We propose MetaWorld, a hierarchical world model that integrates semantic planning and physical control via expert policy transfer. The framework decouples tasks into a VLM-driven semantic layer and a latent dynamics model operating in a compact state space. Our dynamic expert selection and motion prior fusion mechanism leverages a pre-trained multi-expert policy library as transferable knowledge, enabling efficient online adaptation via a two-stage framework. VLMs serve as semantic interfaces, mapping instructions to executable skills and bypassing symbol grounding. Experiments on Humanoid-Bench show MetaWorld outperforms world model-based RL in task completion and motion coherence. Our code will be found at https://anonymous.4open.science/r/metaworld-2BF4/

</details>


### [342] [AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation](https://arxiv.org/abs/2601.17550)
*Deepak Singh,Shreyas Khobragade,Nitin J. Sanket*

Main category: cs.RO

TL;DR: 提出AsterNav系统，利用红外单目相机与编码透镜结合结构光实现绝对黑暗中的无人机自主导航，无需外部基础设施


<details>
  <summary>Details</summary>
Motivation: 灾后搜救常在断电黑暗环境中进行，小型无人机受资源限制无法在黑暗中安全导航寻找幸存者

Method: 结合红外单目相机、大孔径编码透镜和结构光，利用深度相关散焦线索，训练AsterNet深度估计模型，在仿真中生成数据直接迁移到真实世界

Result: AsterNet在NVIDIA Jetson Orin Nano上以20Hz运行，成功在真实黑暗环境中导航，包括暗色障碍物和细绳（直径6.25mm），总体成功率95.5%

Conclusion: 这是首个基于单目结构光的四旋翼无人机绝对黑暗导航工作，无需微调即可从仿真迁移到真实世界，系统简单且成本效益高

Abstract: Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.

</details>


### [343] [Correct-by-Construction Vision-based Pose Estimation using Geometric Generative Models](https://arxiv.org/abs/2601.17556)
*Ulices Santa Cruz,Mahmoud Elfar,Yasser Shoukry*

Main category: cs.RO

TL;DR: 提出一个用于自主系统姿态估计的可认证神经网络框架，结合物理建模与学习估计，为安全关键应用提供可证明的误差保证


<details>
  <summary>Details</summary>
Motivation: 深度学习在视觉任务中表现出色，但缺乏输出正确性的可证明保证，这对安全关键应用至关重要。需要设计具有可认证保证的神经网络用于感知姿态估计

Method: 提出几何生成模型（GGM），基于目标物体的成像过程设计类神经网络模型。利用该模型训练具有认证误差保证的姿态估计器，并通过神经网络可达性分析扩展到杂乱环境中的目标检测，最终构建多阶段感知管道

Result: 框架在合成和真实图像上评估，包括自动驾驶场景中的平面物体。使用事件相机捕获的图像显示，训练后的编码器能有效估计交通标志姿态，且符合框架提供的认证误差边界

Conclusion: 该框架成功将物理驱动建模与学习估计相结合，为自主系统的视觉姿态估计提供了具有可认证保证的解决方案，可扩展到杂乱环境并保持认证特性

Abstract: We consider the problem of vision-based pose estimation for autonomous systems. While deep neural networks have been successfully used for vision-based tasks, they inherently lack provable guarantees on the correctness of their output, which is crucial for safety-critical applications. We present a framework for designing certifiable neural networks (NNs) for perception-based pose estimation that integrates physics-driven modeling with learning-based estimation. The proposed framework begins by leveraging the known geometry of planar objects commonly found in the environment, such as traffic signs and runway markings, referred to as target objects. At its core, it introduces a geometric generative model (GGM), a neural-network-like model whose parameters are derived from the image formation process of a target object observed by a camera. Once designed, the GGM can be used to train NN-based pose estimators with certified guarantees in terms of their estimation errors. We first demonstrate this framework in uncluttered environments, where the target object is the only object present in the camera's field of view. We extend this using ideas from NN reachability analysis to design certified object NN that can detect the presence of the target object in cluttered environments. Subsequently, the framework consolidates the certified object detector with the certified pose estimator to design a multi-stage perception pipeline that generalizes the proposed approach to cluttered environments, while maintaining its certified guarantees. We evaluate the proposed framework using both synthetic and real images of various planar objects commonly encountered by autonomous vehicles. Using images captured by an event-based camera, we show that the trained encoder can effectively estimate the pose of a traffic sign in accordance with the certified bound provided by the framework.

</details>


### [344] [Delay-Compensated Stiffness Estimation for Robot-Mediated Dyadic Interaction](https://arxiv.org/abs/2601.17812)
*Mingtian Du,Suhas Raghavendra Kulkarni,Bernardo Noronha,Domenico Campolo*

Main category: cs.RO

TL;DR: 提出一种延迟补偿的刚度估计框架，通过代数估计器显式考虑专家输入与新手响应的时间对齐，并引入标准化加权最小二乘法过滤动态偏差，显著提升远程康复机器人交互中的刚度估计精度。


<details>
  <summary>Details</summary>
Motivation: 远程康复机器人交互中，网络延迟导致力与位置信号时间不对齐，传统刚度估计方法忽略延迟造成显著估计误差，需要开发能补偿延迟的鲁棒刚度估计方法。

Method: 基于准静态平衡推导显式考虑专家输入与新手响应时间对齐的代数估计器，引入标准化加权最小二乘法（NWLS）实现鲁棒滤波以消除代数推导产生的动态偏差。

Result: 在商用康复机器人（H-MAN）平台上的实验表明，该方法显著优于标准估计器，在多种引入延迟下保持一致的跟踪精度。

Conclusion: 该方法为远程双向交互中实现高保真触觉感知提供了有前景的解决方案，有望促进跨网络治疗环境中可靠的刚度评估。

Abstract: Robot-mediated human-human (dyadic) interactions enable therapists to provide physical therapy remotely, yet an accurate perception of patient stiffness remains challenging due to network-induced haptic delays. Conventional stiffness estimation methods, which neglect delay, suffer from temporal misalignment between force and position signals, leading to significant estimation errors as delays increase. To address this, we propose a robust, delay-compensated stiffness estimation framework by deriving an algebraic estimator based on quasi-static equilibrium that explicitly accounts for temporally aligning the expert's input with the novice's response. A Normalised Weighted Least Squares (NWLS) implementation is then introduced to robustly filter dynamic bias resulting from the algebraic derivation. Experiments using commercial rehabilitation robots (H-MAN) as the platform demonstrate that the proposed method significantly outperforms the standard estimator, maintaining consistent tracking accuracy under multiple introduced delays. These findings offer a promising solution for achieving high-fidelity haptic perception in remote dyadic interaction, potentially facilitating reliable stiffness assessment in therapeutic settings across networks.

</details>


### [345] [Less Is More: Scalable Visual Navigation from Limited Data](https://arxiv.org/abs/2601.17815)
*Yves Inglin,Jonas Frey,Changan Chen,Marco Hutter*

Main category: cs.RO

TL;DR: 该研究提出了一种结合模仿学习和几何规划的方法，通过生成合成轨迹来增强有限的专家演示数据，从而提升视觉导航性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习为移动机器人视觉导航提供了强大框架，但其效果严重依赖于训练数据的质量和多样性。收集高质量的人类演示成本高昂，因此需要更有效的数据增强方法。

Method: 提出LiMo（Less is More）方法，使用基于transformer的视觉导航策略，从单张RGB观测中预测目标条件的SE(2)轨迹。利用经典几何规划器生成合成轨迹，与有限的专家演示数据结合进行训练。

Result: 通过合成轨迹增强有限的专家演示数据能带来显著的性能提升。研究分析了数据集规模和多样性对规划性能的影响，并展示了在真实机器人上的部署效果。

Conclusion: 稳健的视觉导航不仅需要收集更多演示数据，更需要战略性地策划多样化的高质量数据集。可扩展的、针对具体体现的几何监督是实现数据高效视觉导航的实用途径。

Abstract: Imitation learning provides a powerful framework for goal-conditioned visual navigation in mobile robots, enabling obstacle avoidance while respecting human preferences and social norms. However, its effectiveness depends critically on the quality and diversity of training data. In this work, we show how classical geometric planners can be leveraged to generate synthetic trajectories that complement costly human demonstrations. We train Less is More (LiMo), a transformer-based visual navigation policy that predicts goal-conditioned SE(2) trajectories from a single RGB observation, and find that augmenting limited expert demonstrations with planner-generated supervision yields substantial performance gains. Through ablations and complementary qualitative and quantitative analyses, we characterize how dataset scale and diversity affect planning performance. We demonstrate real-robot deployment and argue that robust visual navigation is enabled not by simply collecting more demonstrations, but by strategically curating diverse, high-quality datasets. Our results suggest that scalable, embodiment-specific geometric supervision is a practical path toward data-efficient visual navigation.

</details>


### [346] [NeuroManip: Prosthetic Hand Manipulation System Based on EMG and Eye Tracking Powered by the Neuromorphic Processor AltAi](https://arxiv.org/abs/2601.17991)
*Roman Akinshin,Elizaveta Lopatina,Kirill Bogatikov,Nikolai Kiz,Anna V. Makarova,Mikhail Lebedev,Miguel Altamirano Cabrera,Dzmitry Tsetserukou,Valerii Kangler*

Main category: cs.RO

TL;DR: 提出结合表面肌电信号与眼动追踪的神经形态控制架构，用于上肢假肢，实现低功耗实时手势识别，并通过上下文视觉信息提高准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前上肢假肢控制需要更自然、节能且可靠的方案，传统肌电接口存在功耗高、缺乏上下文感知等问题，需要结合视觉信息提高控制准确性和安全性。

Method: 使用神经形态处理器AltAi部署尖峰神经网络实时分类EMG模式，结合眼动追踪头显和场景摄像头识别用户注视物体，将视觉信息限制为三个上下文相关手势，排除不安全手势。

Result: 在6种功能手势上达到与传统肌电接口相当的识别性能，当结合视觉上下文限制为3个适当手势时，识别准确率提升至约95%，同时功耗低于1瓦，实现可穿戴轻量级系统。

Conclusion: 提出的神经形态上下文感知控制器能提供节能可靠的假肢控制，有潜力改善上肢截肢者日常活动的安全性和可用性，展示了神经形态计算在可穿戴辅助设备中的应用前景。

Abstract: This paper presents a novel neuromorphic control architecture for upper-limb prostheses that combines surface electromyography (sEMG) with gaze-guided computer vision. The system uses a spiking neural network deployed on the neuromorphic processor AltAi to classify EMG patterns in real time while an eye-tracking headset and scene camera identify the object within the user's focus. In our prototype, the same EMG recognition model that was originally developed for a conventional GPU is deployed as a spiking network on AltAi, achieving comparable accuracy while operating in a sub-watt power regime, which enables a lightweight, wearable implementation. For six distinct functional gestures recorded from upper-limb amputees, the system achieves robust recognition performance comparable to state-of-the-art myoelectric interfaces. When the vision pipeline restricts the decision space to three context-appropriate gestures for the currently viewed object, recognition accuracy increases to roughly 95% while excluding unsafe, object-inappropriate grasps. These results indicate that the proposed neuromorphic, context-aware controller can provide energy-efficient and reliable prosthesis control and has the potential to improve safety and usability in everyday activities for people with upper-limb amputation.

</details>


### [347] [Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization](https://arxiv.org/abs/2601.18121)
*Byeonggyeol Choi,Woojin Oh,Jongwoo Lim*

Main category: cs.RO

TL;DR: 提出一个仿真循环优化框架，将视觉对齐的手-物体轨迹转换为物理可执行的轨迹，使用基于关键帧的样条表示和CMA-ES优化器，在保持接近原始演示的同时实现物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有手部操作数据集（如DexYCB和HO3D）主要针对视觉对齐优化，但在物理仿真中回放时会产生物理上不合理的结果，包括穿透、接触缺失和不稳定抓握，限制了其在物理仿真和策略学习中的应用。

Method: 采用仿真循环优化框架，将问题表述为可处理的黑盒优化问题。使用基于稀疏时间关键帧的低维样条表示来参数化手部运动，然后使用无梯度优化器CMA-ES，将高保真物理引擎作为黑盒目标函数进行优化，在最大化物理成功（稳定抓握和抬起）的同时最小化与原始人类演示的偏差。

Result: 与最近的MANIPTRANS传输管道相比，该方法在回放时实现了更低的手部和物体姿态误差，更准确地恢复了手-物体物理交互，能够生成物理有效的轨迹。

Conclusion: 该方法提供了一种通用且可扩展的方法，将视觉演示转换为物理有效的轨迹，为稳健策略学习生成高保真数据铺平了道路。

Abstract: Dexterous hand manipulation increasingly relies on large-scale motion datasets with precise hand-object trajectory data. However, existing resources such as DexYCB and HO3D are primarily optimized for visual alignment but often yield physically implausible interactions when replayed in physics simulators, including penetration, missed contact, and unstable grasps.
  We propose a simulation-in-the-loop refinement framework that converts these visually aligned trajectories into physically executable ones. Our core contribution is to formulate this as a tractable black-box optimization problem. We parameterize the hand's motion using a low-dimensional, spline-based representation built on sparse temporal keyframes. This allows us to use a powerful gradient-free optimizer, CMA-ES, to treat the high-fidelity physics engine as a black-box objective function. Our method finds motions that simultaneously maximize physical success (e.g., stable grasp and lift) while minimizing deviation from the original human demonstration.
  Compared to MANIPTRANS-recent transfer pipelines, our approach achieves lower hand and object pose errors during replay and more accurately recovers hand-object physical interactions. Our approach provides a general and scalable method for converting visual demonstrations into physically valid trajectories, enabling the generation of high-fidelity data crucial for robust policy learning.

</details>


### [348] [Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation](https://arxiv.org/abs/2601.18289)
*Jialong Li,Zhenguo Wang,Tianci Wang,Maj Stenmark,Volker Krueger*

Main category: cs.RO

TL;DR: Quest2ROS2是一个用于双手遥操作的开源ROS2框架，通过相对运动控制解决工作空间限制问题，支持直观的姿势无关操作，包含实时可视化、夹持器控制和暂停重置等安全功能。


<details>
  <summary>Details</summary>
Motivation: 现有的Quest2ROS框架存在工作空间限制问题，需要扩展为双手遥操作框架以支持机器人数据收集的规模化。

Method: 基于相对运动的控制方法，通过计算VR控制器姿势变化来确定机器人运动，实现直观的姿势无关操作。采用模块化架构，支持"并排"和"镜像"两种控制模式。

Result: 开发了一个完整的开源ROS2框架，集成了实时RViz可视化、流线型夹持器控制、暂停重置功能等关键可用性和安全特性。

Conclusion: Quest2ROS2成功克服了工作空间限制，为机器人数据收集提供了可扩展的双手遥操作解决方案，支持跨不同平台的优化操作体验。

Abstract: Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports "Side-by-Side" and "Mirror" control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.

</details>


### [349] [TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion](https://arxiv.org/abs/2601.18323)
*Weishi Mi,Yong Bao,Xiaowei Chi,Xiaozhu Ju,Zhiyuan Qin,Kuangzhi Ge,Kai Tang,Peidong Jia,Shanghang Zhang,Jian Tang*

Main category: cs.RO

TL;DR: TC-IDM通过提取世界模型生成的工具轨迹作为中间表示，将视觉规划与物理控制桥接起来，显著提升了机器人任务执行的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）范式依赖大规模高质量机器人数据，泛化能力有限；生成式世界模型虽前景广阔，但其像素级规划与可执行物理动作之间存在鸿沟。

Method: 提出以工具为中心的逆动力学模型（TC-IDM），从生成视频中通过分割和3D运动估计提取工具点云轨迹，利用解耦的动作头将规划轨迹映射为6自由度末端执行器运动和控制信号。

Result: 在真实世界评估中，TC-IDM结合世界模型平均成功率61.11%，简单任务达77.7%，零样本可变形物体任务达38.46%，显著优于端到端VLA基线和其他逆动力学模型。

Conclusion: TC-IDM的"规划-转换"范式建立了视觉规划与物理控制之间的稳健桥梁，支持多种末端执行器，具有强泛化能力，为通用具身AI提供了有效解决方案。

Abstract: The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.

</details>


### [350] [SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation](https://arxiv.org/abs/2601.18442)
*Hongyi Zhao,Shuo Wang,Qijie He,Ziyuan Pu*

Main category: cs.RO

TL;DR: SG-CADVLM是一个基于上下文感知解码的多模态视觉语言模型框架，能够从事故报告和道路网络图生成安全关键场景，相比基线方法在关键风险场景生成率上提升469%。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全验证需要测试安全关键场景，但这些事件在现实驾驶中罕见且测试成本高。事故报告提供了真实的安全关键事件规格，是稀缺的真实碰撞轨迹数据的宝贵替代来源。现有方法存在局限性：数据驱动方法缺乏多样性，对抗方法缺乏物理真实性，而LLM/VLM方法存在上下文抑制问题。

Method: 提出SG-CADVLM框架，集成上下文感知解码与多模态输入处理，从事故报告和道路网络图生成安全关键场景。该框架缓解了VLM幻觉问题，同时生成道路几何和车辆轨迹。

Result: 实验结果显示，SG-CADVLM生成关键风险场景的比例达到84.4%，而基线方法仅为12.5%，提升了469%。同时能生成可执行的自动驾驶测试仿真场景。

Conclusion: SG-CADVLM通过上下文感知解码有效解决了VLM在场景生成中的上下文抑制问题，显著提高了安全关键场景的生成率和真实性，为自动驾驶安全验证提供了有效的仿真测试工具。

Abstract: Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.

</details>


### [351] [DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation](https://arxiv.org/abs/2601.18492)
*Zijun Li,Shijie Li,Zhenxi Zhang,Bin Li,Shoujun Zhou*

Main category: cs.RO

TL;DR: DV-VLN 是一个新的视觉语言导航框架，采用生成-验证范式，通过LLaMA-2骨干网络生成结构化导航思维链，然后通过真伪验证和掩码实体验证两个互补通道验证候选动作，通过多样本聚合验证成功率来选择动作。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的视觉语言导航代理大多依赖单次动作决策，需要从嘈杂的多视角文本化观察中选择一个选项。由于局部不匹配和不完美的中间推理，这种决策容易偏离正确路径，导致误差累积和在未见环境中可靠性降低。

Method: 1. 对开源LLaMA-2骨干网络进行参数高效的领域内适应，生成结构化导航思维链；2. 通过两个互补通道验证候选动作：真伪验证（TFV）和掩码实体验证（MEV）；3. 通过聚合多个样本的验证成功率来选择动作，生成可解释的分数进行重新排序。

Result: 在R2R、RxR（英文子集）和REVERIE数据集上的实验表明，DV-VLN相比直接预测和仅采样基线有持续改进，在纯语言VLN代理中具有竞争力，与多个跨模态系统相比也展现出有前景的结果。

Conclusion: DV-VLN通过生成-验证范式解决了传统LLM-based VLN代理的单次决策问题，通过结构化思维链和双重验证机制提高了导航的可靠性和可解释性，在多个基准测试中表现出色。

Abstract: Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.

</details>


### [352] [SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction](https://arxiv.org/abs/2601.18537)
*Linyong Gan,Zimo Li,Wenxin Xu,Xingjian Li,Jianhua Z. Huang,Enmei Tu,Shuhang Chen*

Main category: cs.RO

TL;DR: 提出基于语义关键点条件的轨迹建模框架，通过预测下一个关键点（NKP）来指导长时域船舶轨迹预测，解决现有方法在长时预测中方向漂移和不合理轨迹的问题。


<details>
  <summary>Details</summary>
Motivation: 现有船舶轨迹预测方法在长时域预测中存在全局方向一致性难以保持的问题，容易产生漂移或不合理的轨迹，这源于复杂的航行行为和环境因素带来的不确定性累积。

Method: 提出语义关键点条件的轨迹建模框架，将长时域预测分解为全局语义决策（预测下一个关键点NKP）和局部运动建模。采用预训练-微调策略来从历史观测中高效估计NKP先验。

Result: 在真实AIS数据上的大量实验表明，该方法在长航行时间、方向准确性和细粒度轨迹预测方面一致优于现有最先进方法。

Conclusion: 通过将长时域轨迹预测分解为全局语义决策和局部运动建模，该方法有效限制了未来轨迹的可行语义子集，显著提升了长时域预测的准确性和合理性。

Abstract: Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.

</details>


### [353] [Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field](https://arxiv.org/abs/2601.18548)
*Yulin Li,Zhiyuan Song,Yiming Li,Zhicheng Song,Kai Chen,Chunxin Zheng,Zhihai Bi,Jiahang Cao,Sylvain Calinon,Fan Shi,Jun Ma*

Main category: cs.RO

TL;DR: 提出了广义构型空间距离场(GCDF)，将固定基座机械臂的构型空间距离场扩展到移动机械臂，解决了无界工作空间和紧耦合的基座-臂协调问题，并开发了基于神经网络的GCDF表示和高效序列凸优化求解器。


<details>
  <summary>Details</summary>
Motivation: 移动机械臂在狭窄拥挤空间中的全身轨迹优化面临高维非凸性和快速精确碰撞推理的挑战。构型空间距离场(CDF)为固定基座机械臂提供了优化友好的碰撞代价表示，但难以扩展到移动机械臂，因为存在无界工作空间和更紧密的基座-臂耦合问题。

Method: 提出了广义构型空间距离场(GCDF)，将CDF扩展到具有平移和旋转关节的移动机械臂；开发了数据生成和训练流程，获得具有准确值和梯度的连续神经GCDF；构建了基于GCDF碰撞推理的高性能序列凸优化框架，支持在线神经约束指定、稀疏感知主动集检测和增量约束管理。

Result: 证明了GCDF保持了欧几里得局部距离结构，并能在构型空间中准确编码全身几何；实现了支持高效GPU批量查询的连续神经GCDF；开发的优化框架能够扩展到大量隐式约束，在场景变化下实现快速重规划。

Conclusion: GCDF成功将构型空间距离场的优势扩展到移动机械臂，解决了无界工作空间和基座-臂耦合的挑战，为移动机械臂在拥挤狭窄空间中的高效轨迹优化提供了可行的解决方案。

Abstract: Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.

</details>


### [354] [Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2601.18569)
*Seokju Lee,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 提出AttenNKF（基于注意力的神经增强卡尔曼滤波器）用于四足机器人状态估计，通过注意力机制补偿足部滑动引起的误差。


<details>
  <summary>Details</summary>
Motivation: 足部滑动是四足机器人状态估计的主要误差来源，当滑动发生时，运动学测量违反无滑动假设，在更新步骤中引入偏差。需要估计并补偿这种滑动引起的误差。

Method: 将不变扩展卡尔曼滤波器（InEKF）与神经补偿器增强，使用注意力机制根据足部滑动严重程度推断误差，然后将该估计作为后更新补偿应用于InEKF状态。补偿器在潜在空间中训练，减少对原始输入尺度的敏感性，鼓励结构化滑动条件补偿，同时保持InEKF递归。

Result: 实验表明，相比现有四足机器人状态估计器，特别是在易滑动条件下，AttenNKF表现出改进的性能。

Conclusion: 提出的AttenNKF通过神经补偿和注意力机制有效处理足部滑动引起的状态估计误差，在滑动条件下显著提升了四足机器人的状态估计精度。

Abstract: In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.

</details>


### [355] [ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection](https://arxiv.org/abs/2601.18629)
*Yiming Wang,Ruogu Zhang,Minyang Li,Hao Shi,Junbo Wang,Deyi Li,Jieji Ren,Wenhai Liu,Weiming Wang,Hao-Shu Fang*

Main category: cs.RO

TL;DR: ExoGS是一个机器人无关的4D真实-仿真-真实框架，通过被动外骨骼捕捉人类演示的精确轨迹和环境交互，重建为可编辑的3D高斯泼溅资产，实现几何一致的回放和大规模数据增强，显著提升数据效率和策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有真实-仿真-真实方法主要关注环境层面的视觉转换，忽视了交互行为的转换。对于接触丰富的任务，纯粹在仿真中获取交互数据既困难又低效，需要一种能同时捕捉静态环境和动态交互的解决方案。

Method: 使用自设计的机器人同构被动外骨骼AirExo-3捕捉毫米级精度的人类演示轨迹和同步RGB观测；将机器人、物体和环境重建为可编辑的3D高斯泼溅资产；通过轻量级Mask Adapter向策略注入实例级语义以增强视觉域偏移下的鲁棒性。

Result: 真实世界实验表明，ExoGS相比基于遥操作的基线方法，显著提高了数据效率和策略泛化能力。代码和硬件文件已在GitHub上开源。

Conclusion: ExoGS提供了一个新的可扩展操作数据收集和策略学习解决方案，能够无缝地将真实世界的静态环境和动态交互转移到仿真环境中，缩小了仿真与现实之间的差距。

Abstract: Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.

</details>


### [356] [Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation](https://arxiv.org/abs/2601.18639)
*Ojasva Mishra,Xiaolong Wu,Min Xu*

Main category: cs.RO

TL;DR: 针对机器人关节控制的饱和离散时间PID调节问题，提出结合稳定性分析、抗饱和设计和贝叶斯优化的系统化调参流程，在模拟环境下提升鲁棒性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 实际机器人关节控制中，连续时间PID理论因离散执行、执行器饱和、微小延迟和测量误差等因素而失效，需要针对离散时间实现进行鲁棒性分析和调参。

Method: 1) 使用Jury判据分析欧拉和零阶保持离散化下PI稳定性区域；2) 评估饱和主导下的离散反计算抗饱和实现；3) 提出混合认证贝叶斯优化流程，筛选不稳定候选并优化鲁棒IAE目标。

Result: 在随机模型族模拟不确定、延迟、噪声、量化和更紧饱和条件下，鲁棒导向调参将中位IAE从0.843降至0.430，同时保持中位超调低于2%；认证筛选在边界内拒绝11.6%随机采样增益。

Conclusion: 该工作流为饱和离散时间关节控制提供了系统化的实现感知分析和调参方法，在仅模拟环境下有效提升鲁棒性并避免硬件实验，具有实际应用价值。

Abstract: The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.

</details>


### [357] [A Pragmatic VLA Foundation Model](https://arxiv.org/abs/2601.18692)
*Wei Wu,Fan Lu,Yunnan Wang,Shuai Yang,Shi Liu,Fangjing Wang,Qian Zhu,He Sun,Yong Wang,Shuailei Ma,Yiyu Ren,Kejia Zhang,Hui Yu,Jingmei Zhao,Shuai Zhou,Zhenqi Qiu,Houlong Xiong,Ziyu Wang,Zechen Wang,Ran Cheng,Yong-Lu Li,Yongtao Huang,Xing Zhu,Yujun Shen,Kecheng Zheng*

Main category: cs.RO

TL;DR: LingBot-VLA是一个基于20,000小时真实世界数据训练的视觉-语言-动作基础模型，在多种双臂机器人配置上表现出色，实现了强大的泛化能力和高效训练。


<details>
  <summary>Details</summary>
Motivation: 为了开发能够在不同任务和平台上忠实泛化、同时确保成本效益（数据需求和GPU训练时间）的视觉-语言-动作基础模型，以推动机器人操控领域的发展。

Method: 使用约20,000小时来自9种流行双臂机器人配置的真实世界数据训练LingBot-VLA模型，并构建高效代码库实现261样本/秒/GPU的训练吞吐量。

Result: 在3个机器人平台上进行系统评估（每个平台完成100个任务，每个任务130次后训练测试），模型明显优于竞争对手，展现出强大的性能和广泛的泛化能力。

Conclusion: LingBot-VLA具备真实世界部署的适用性，通过开源代码、基础模型和基准数据推动机器人学习领域发展，支持更具挑战性的任务和健全的评估标准。

Abstract: Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.

</details>


### [358] [Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods](https://arxiv.org/abs/2601.18723)
*Mengyuan Liu,Juyi Sheng,Peiming Li,Ziyi Wang,Tianming Xu,Tiantian Xu,Hong Liu*

Main category: cs.RO

TL;DR: 提出Eval-Actions基准和AutoEval架构，解决机器人模仿学习中可信评估缺失的问题，通过整合失败场景和多维度监督信号实现更全面的行为评估。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习的评估方法落后于模型发展，依赖简单的二进制成功率，无法衡量信任度的关键维度（来源真实性和执行质量），阻碍了可信评估标准的建立。

Method: 1. 构建Eval-Actions基准数据集，包含VA/VLA策略执行轨迹和人类遥操作数据，特别包含失败场景；2. 提出AutoEval架构，使用时空聚合进行语义评估，辅以运动学校准信号优化运动平滑度；3. AutoEval-P版本引入组相对策略优化增强逻辑推理能力。

Result: AutoEval在专家评分和排名引导协议下分别达到0.81和0.84的斯皮尔曼等级相关系数；框架能准确区分策略生成和遥操作视频，准确率达99.6%，为可信机器人评估建立了严格标准。

Conclusion: Eval-Actions基准和AutoEval架构有效解决了机器人模仿学习中的可信评估问题，通过整合多维度监督信号和失败场景，实现了对策略行为来源真实性和执行质量的全面评估，为未来研究提供了可靠的评估框架。

Abstract: Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman's Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.

</details>


### [359] [Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge](https://arxiv.org/abs/2601.18733)
*Li Kang,Heng Zhou,Xiufeng Song,Rui Li,Bruno N. Y. Chen,Ziye Wang,Ximeng Meng,Stone Tao,Yiran Qin,Xiaohong Liu,Ruimao Zhang,Lei Bai,Yilun Du,Hao Su,Philip Torr,Zhenfei Yin,Ruihao Gong,Yejun Zeng,Fengjun Zhong,Shenghao Jin,Jinyang Guo,Xianglong Liu,Xiaojun Jia,Tianqi Shan,Wenqi Ren,Simeng Qin,Jialing Yang,Xiaoyu Ma,Tianxing Chen,Zixuan Li,Zijian Cai,Yan Qin,Yusen Qin,Qiangyu Chen,Kaixuan Wang,Zhaoming Han,Yao Mu,Ping Luo,Yuanqi Yao,Haoming Song,Jan-Nico Zaech,Fabien Despinoy,Danda Pani Paudel,Luc Van Gool*

Main category: cs.RO

TL;DR: 提出Multi-Agent Robotic System (MARS)挑战赛，聚焦多智能体机器人系统中的规划与控制问题，旨在推动协作式具身AI系统发展


<details>
  <summary>Details</summary>
Motivation: 随着具身AI向更复杂任务场景发展，多智能体系统框架对于实现可扩展、高效和协作的解决方案变得至关重要。主要驱动因素包括：智能体能力提升、通过任务委派提高系统效率、以及促进高级人机交互。

Method: 提出MARS挑战赛作为解决方案，该竞赛在NeurIPS 2025 SpaVLE研讨会上举办，聚焦两个关键领域：1) 使用视觉语言模型进行多智能体具身规划以协调任务；2) 在动态环境中执行机器人操作的政策控制。

Result: 通过评估参与者提交的解决方案，该挑战赛为具身多智能体系统的设计和协调提供了有价值的见解，但具体实验结果需要等待竞赛完成后的详细分析。

Conclusion: MARS挑战赛为解决多智能体协作面临的挑战提供了一个重要平台，通过促进相关研究和解决方案的发展，将推动先进协作式AI系统的未来发展。

Abstract: Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.

</details>


### [360] [Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery](https://arxiv.org/abs/2601.18765)
*Shutong Chen,Adnan Aijaz,Yansha Deng*

Main category: cs.RO

TL;DR: 提出面向目标的通信框架GoC，通过联合设计通信-计算-控制循环，实现快速鲁棒的机器人故障检测与恢复，将FDR时间降低82.6%，任务成功率提升76%。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测与恢复框架存在通信计算延迟大、运动生成不可靠等问题，主要因为通信-计算-控制循环设计未考虑下游FDR目标，无法满足智能工厂动态不确定环境中低延迟鲁棒性要求。

Method: 1) 故障检测：定义并提取3D场景图作为语义表示，通过监控3D-SG空间关系变化检测故障；2) 故障恢复：通过LoRA微调小型语言模型，结合知识蒸馏增强推理泛化能力生成恢复动作；3) 设计轻量级目标导向数字孪生重建模块，使用任务相关物体轮廓优化恢复动作。

Result: 相比依赖视觉语言模型进行故障检测和大语言模型进行故障恢复的现有框架，GoC框架将FDR时间降低82.6%，任务成功率提升76%。

Conclusion: 提出的GoC框架通过联合设计通信-计算-控制循环，实现了快速鲁棒的机器人故障检测与恢复，在动态不确定环境中显著提升了系统性能。

Abstract: Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.

</details>
