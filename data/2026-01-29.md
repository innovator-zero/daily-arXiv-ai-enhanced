<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.LG](#cs.LG) [Total: 113]
- [cs.RO](#cs.RO) [Total: 20]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出一种从单目图像恢复真实比例3D重建的方法，用于精确饮食评估，在体积估计误差上比现有技术降低近30%


<details>
  <summary>Details</summary>
Motivation: 慢性疾病如肥胖和糖尿病需要准确监测食物摄入量，但现有AI饮食评估方法在从单目图像恢复食物大小（分量）信息方面存在挑战，特别是缺乏真实世界比例信息限制了在精准营养中的应用。

Method: 利用在大规模数据集上训练的模型提取丰富视觉特征，估计重建对象的真实比例，将单视图3D重建转换为真实比例、物理上有意义的模型。

Result: 在两个公开数据集上的广泛实验和消融研究表明，该方法一致优于现有技术，平均绝对体积估计误差降低近30%。

Conclusion: 该方法成功连接了3D计算机视觉和数字健康领域，通过恢复真实比例的3D重建对象，展示了在增强精准营养领域的潜力。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: DiSa提出了一种新颖的显著性感知前景-背景解耦框架，通过显式结合显著性线索，分别建模前景和背景特征，解决了开放词汇语义分割中CLIP等视觉语言模型的前景偏见和空间定位有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP等视觉语言模型的开放词汇语义分割方法存在两个关键限制：1）前景偏见，倾向于忽略背景区域；2）空间定位有限，导致物体边界模糊。这是因为这些模型在图像-文本对预训练中偏向于显著、以物体为中心的区域。

Method: 提出DiSa框架，包含两个核心模块：1）显著性感知解耦模块（SDM），通过显式结合显著性线索，以前景-背景解耦的方式分别建模前景和背景集合特征；2）分层细化模块（HRM），利用像素级空间上下文，通过多级更新实现通道级特征细化。

Result: 在六个基准测试上的广泛实验表明，DiSa始终优于最先进的方法。

Conclusion: 通过显式结合显著性线索并以分治方式建模前景和背景特征，DiSa有效解决了现有视觉语言模型在开放词汇语义分割中的前景偏见和空间定位限制问题。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: SSMAE是一种半监督学习方法，结合掩码自编码器和伪标签，通过验证门控机制动态选择高质量伪标签，在少标注数据场景下显著提升Vision Transformers性能。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Transformers在标注数据稀缺但未标注数据充足情况下的训练挑战，传统监督学习方法需要大量标注数据，而自监督方法（如MAE）在少标注场景下表现有限。

Method: 提出半监督掩码自编码器（SSMAE）框架，联合优化掩码图像重建和分类任务。引入验证驱动的门控机制，仅在模型对弱增强和强增强视图产生一致的高置信度预测时才激活伪标签，减少确认偏差。

Result: 在CIFAR-10和CIFAR-100数据集上，SSMAE始终优于监督ViT和微调MAE。在10%标注的CIFAR-10上获得最大提升（比ViT高9.24%），在低标注场景下表现尤为突出。

Conclusion: 伪标签引入的时机与生成方式同样重要，验证门控机制能有效减少确认偏差，SSMAE为数据高效的Transformer训练提供了有效解决方案。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [4] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 提出在CLIP训练中直接集成稀疏性，获得既可解释又高性能的表示，挑战了可解释性与性能相互冲突的传统观念


<details>
  <summary>Details</summary>
Motivation: CLIP的密集不透明表示存在可解释性挑战，传统认为可解释性与性能存在冲突，后处理方法会降低下游任务性能并损失多模态能力

Method: 在CLIP训练中直接集成稀疏性，而不是采用后处理的自编码器方法，获得稀疏的CLIP表示

Result: 稀疏CLIP表示保持强大的下游任务性能，获得更好的可解释性，保留多模态能力，支持语义概念对齐并揭示跨模态知识涌现的训练动态

Conclusion: 可解释性和性能可以共同优化，挑战了传统观念，为未来模型设计提供了有前景的原则

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [5] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 对现有H&E染色图像核实例分割数据集进行系统评估、排名和融合，提出统一测试集NucFuse-test和训练集NucFuse-train，建立新的基准


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注开发新分割算法，但缺乏对数据集的系统评估。该工作旨在通过标准化和评估公开可用的H&E染色图像核实例分割数据集，为领域提供更好的训练、测试和评估基准。

Method: 1) 通过文献综述识别手动标注的公开数据集；2) 将数据集标准化为统一格式；3) 使用两种SOTA分割模型（CNN和CNN-ViT混合架构）系统评估数据集性能；4) 提出统一测试集NucFuse-test用于公平跨数据集评估；5) 提出统一训练集NucFuse-train通过合并多个数据集图像提升分割性能

Result: 1) 系统评估并排名了各数据集的核实例分割性能；2) 创建了融合数据集NucFuse-test和NucFuse-train；3) 进行了外部验证；4) 提供了公开可用的实现，为H&E染色组织图像核实例分割模型建立了新的基准

Conclusion: 该工作通过数据集层面的系统分析，而非算法开发，为核实例分割领域提供了新的评估基准和融合数据集，有助于更公平地比较模型性能并提升分割效果。

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [6] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: 提出SAP训练无关剪枝方法，在ViDoRe基准上实现超过90%索引向量压缩，同时保持检索性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如ColPali）的视觉文档检索存在索引向量过大问题，训练无关剪枝方法在高压缩场景下性能不佳，传统方法在最后层剪枝时结构信号已消散

Method: 提出Structural Anchor Pruning (SAP)训练无关剪枝方法，从中间层识别关键视觉补丁；引入Oracle Score Retention (OSR)协议评估分层信息对压缩效率的影响

Result: 在ViDoRe基准上，SAP减少超过90%索引向量同时保持稳健检索性能；OSR分析显示语义结构锚点补丁在中间层持续存在

Conclusion: SAP为视觉RAG提供高度可扩展解决方案，中间层的结构锚点补丁是实现高效训练无关压缩的关键

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [7] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对扩散式大型多模态模型的结构化令牌剪枝策略，通过在中间到后期层选择性移除视觉令牌，减少65%计算成本的同时保持95%任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散式大型多模态模型（如LLaDA-V）虽然表现出色，但其双向注意力机制和扩散式迭代去噪范式引入了显著计算开销。研究发现，与自回归解码器不同，LLaDA-V主要在中间到后期层聚合跨模态信息，导致语义对齐延迟。

Method: 提出结构化令牌剪枝策略，受FastV启发但针对扩散模型特点：1）在中间到后期层选择性移除视觉令牌；2）专注于第一个去噪步骤进行剪枝，该剪枝效果可延续到后续所有步骤；3）与LLaDA-V延迟注意力聚合特性对齐以保持输出质量。

Result: 在多个基准测试中，最佳配置将计算成本降低高达65%，同时平均保持95%的任务性能。这是首次在基于扩散的大型多模态模型中研究结构化令牌剪枝。

Conclusion: 该框架为高效LLaDA-V推理提供了实证基础，突显了在基于扩散的多模态模型中视觉感知剪枝的潜力。通过针对模型注意力特性的结构化剪枝，可以在大幅减少计算开销的同时保持性能。

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [8] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleStyle是一个基于Qwen-Image-Edit构建的轻量级图像和视频风格迁移模型，通过课程持续学习框架处理高质量策划数据和噪声合成数据，实现了内容保真度和风格适应性的平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器（DiTs）在内容风格迁移中存在内容和风格特征内在纠缠的问题，导致难以同时保持内容完整性和风格准确性。需要开发一个既能保持内容保真度又能适应多样风格的解决方案。

Method: 1. 基于Qwen-Image-Edit构建模型；2. 策划高质量特定风格数据集并合成数千种野外风格的三元组数据；3. 采用课程持续学习框架训练混合数据（干净策划数据和噪声合成数据）；4. 引入视频到视频风格化模块增强时间一致性和视觉质量。

Result: TeleStyle在三个核心评估指标上达到最先进性能：风格相似性、内容一致性和美学质量。模型能够泛化到未见过的风格而不损害内容保真度。

Conclusion: TeleStyle通过创新的课程持续学习框架和混合数据集训练，成功解决了DiTs中内容和风格特征纠缠的问题，实现了高质量的内容保持风格迁移，并在图像和视频领域都表现出色。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [9] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: 该研究探讨了利用计算机视觉模型和大语言模型自动评估船体生物污损程度的方法，比较了不同技术在生物污损严重程度分类上的表现。


<details>
  <summary>Details</summary>
Motivation: 船舶生物污损对生态、经济和生物安全构成重大风险，传统潜水员检查方法存在危险且可扩展性有限，需要开发自动化评估方案。

Method: 使用卷积神经网络、基于Transformer的分割模型和零样本大语言模型，在专家标注的新西兰初级产业部数据集上评估生物污损严重程度的自动分类。

Result: 计算机视觉模型在极端污损级别分类准确率高，但在中间级别表现不佳（因数据集不平衡和图像构图问题）。大语言模型通过结构化提示和检索实现了有竞争力的性能，无需训练且输出可解释。

Conclusion: 不同方法具有互补优势，结合分割覆盖度与大语言模型推理的混合方法为可扩展且可解释的生物污损评估提供了有前景的途径。

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [10] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO通过密集奖励解决GRPO方法中的稀疏奖励问题，提出基于ODE的步骤奖励增益预测和奖励感知的探索空间校准，显著提升流匹配模型的人类偏好对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO和流匹配模型的方法在文本到图像生成的人类偏好对齐方面取得了进展，但仍存在稀疏奖励问题：整个去噪轨迹的终端奖励被应用于所有中间步骤，导致全局反馈信号与中间去噪步骤的细粒度贡献不匹配。

Method: 提出DenseGRPO框架，包含两个关键组件：1) 通过ODE方法在中间干净图像上应用奖励模型，预测每个去噪步骤的步骤奖励增益作为密集奖励；2) 基于估计的密集奖励，揭示现有GRPO方法中均匀探索设置与时变噪声强度之间的不匹配问题，提出奖励感知方案，通过自适应调整SDE采样器中特定时间步的随机性注入来校准探索空间。

Result: 在多个标准基准上的广泛实验证明了DenseGRPO的有效性，并突出了有效密集奖励在流匹配模型对齐中的关键作用。

Conclusion: DenseGRPO通过引入密集奖励和奖励感知探索空间校准，解决了GRPO方法中的稀疏奖励问题，显著提升了流匹配模型在人类偏好对齐方面的性能。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [11] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: FPL是一种简单高效的CLIP适应方法，通过将分类问题转化为特征投影问题，在保持高性能的同时减少可学习参数和训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有的VLP模型适应方法存在性能有限、可学习参数过多或训练时间过长的问题，阻碍了CLIP模型在下游任务中的有效适应。

Method: 提出特征投影学习(FPL)，开发投影模型将类别原型特征投影到查询图像特征空间并重建查询图像特征图，将负平均平方重建误差作为类别得分，将分类问题转化为特征投影问题。

Result: 综合实证评估证实FPL提供卓越的准确性，大幅超越当前最先进方法。

Conclusion: FPL是一种简单而高效有效的CLIP适应方法，通过特征投影学习解决了现有方法的局限性。

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [12] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: 提出PAE方法解决视觉提示调优中的训练不稳定问题，通过频率域初始化、共享Koopman算子和Lyapunov稳定性正则化，实现更快收敛和更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示调优方法存在训练不稳定问题，表现为梯度振荡。浅层提示早期停滞，深层提示高方差振荡，导致跨层不匹配，影响收敛速度和最终性能。

Method: 提出Prompt-Agnostic Evolution (PAE)：1) 从频率域角度初始化提示，发现并传播主干网络用于识别的频率捷径模式；2) 使用共享Koopman算子施加全局线性变换而非层特定更新；3) 基于Lyapunov稳定性理论引入正则化约束误差放大。

Result: 在25个数据集上的实验表明，PAE平均加速收敛1.41倍，准确率提升1-3%。方法轻量、提示无关，可与多种VPT变体无缝集成，无需修改主干网络或推理时更改。

Conclusion: PAE通过显式建模提示动态，解决了视觉提示调优中的训练不稳定问题，实现了更稳定、更快的收敛和更好的下游任务性能，具有广泛的适用性。

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [13] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: BLenDeR是一种基于扩散模型的采样方法，通过集合论启发的并集和交集操作控制生成样本的多样性，用于增强深度度量学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在增强深度度量学习时缺乏对生成样本多样性的可控性，难以有效提高类内多样性。

Method: 提出BLenDeR方法，利用集合论中的并集和交集操作处理去噪残差：并集操作鼓励多个提示中的任何属性出现，交集操作通过主成分替代提取共同方向，从而可控地合成每个类内多样化的属性组合。

Result: 在标准深度度量学习基准测试中，BLenDeR在多个数据集和骨干网络上始终优于现有最优方法，特别是在CUB-200上Recall@1提升3.7%，在Cars-196上提升1.8%。

Conclusion: BLenDeR通过可控的多样化生成有效解决了现有生成方法的关键限制，显著提升了深度度量学习的性能。

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [14] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出可逆高效扩散模型(RED)，通过显式监督训练框架解决扩散模型在图像融合任务中的细节丢失问题，同时避免分布估计


<details>
  <summary>Details</summary>
Motivation: 多模态图像融合需要将不同源图像的互补信息整合到统一表示中，但现有扩散模型在图像融合任务中容易因马尔可夫过程的噪声误差累积而导致细节丢失，而显式监督的端到端训练又面临计算效率挑战

Method: 提出可逆高效扩散模型(RED)，这是一种显式监督训练框架，继承扩散模型的强大生成能力，同时避免分布估计

Result: 该方法预期能够保持细粒度细节和高视觉保真度，解决传统扩散模型在图像融合中的细节丢失问题

Conclusion: RED模型为多模态图像融合提供了一种既保持扩散模型生成能力又避免细节丢失的高效解决方案

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [15] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: 提出LVLMs-Saliency框架，通过融合注意力权重和输入梯度来量化视觉基础强度，发现幻觉常由前序token低显著性导致，并设计SGRS和LocoRE两种机制来缓解幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力动态检测LVLM幻觉的方法仅依赖前向传播注意力模式，忽略了梯度信号，无法可靠区分幻觉与事实性输出。

Method: 提出LVLMs-Saliency框架，融合注意力权重和输入梯度量化视觉基础强度；基于分析发现幻觉常由前序token低显著性导致，设计两种推理时机制：SGRS（基于显著性阈值动态过滤候选token）和LocoRE（轻量插件模块强化当前token对最近前驱的注意力）。

Result: 在多个LVLM上的实验表明，该方法显著降低幻觉率，同时保持流畅性和任务性能。

Conclusion: LVLMs-Saliency提供了一种鲁棒且可解释的解决方案，通过梯度感知诊断和双机制干预有效增强模型可靠性。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [16] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出了一种新颖的无源域自适应方法，通过多视图增强和潜在空间一致性技术直接从目标域学习域不变特征，无需源域数据或对抗训练。


<details>
  <summary>Details</summary>
Motivation: 现有域自适应方法通常需要访问源域数据、进行对抗训练或复杂的伪标签技术，计算成本高昂。为了解决这些挑战，需要一种更高效的无源域自适应方法。

Method: 采用多视图增强和潜在空间一致性技术，通过生成目标域数据的多个增强视图，并最小化它们在潜在空间中的特征表示距离，直接从目标域学习可迁移表示。使用ConvNeXt编码器，设计结合分类和一致性目标的损失函数。

Result: 在Office-31、Office-Home和Office-Caltech数据集上分别达到90.72%、84%和97.12%的平均分类准确率。相比现有方法，分别提高了+1.23%、+7.26%和+1.77%的平均分类准确率。

Conclusion: 该方法成功实现了无需源域数据或源-目标对齐的无源域自适应，通过多视图增强和潜在空间一致性有效学习域不变特征，在多个数据集上优于现有方法。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [17] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 论文提出了GenVID数据集和DVAR框架，用于细粒度检测和分类视频生成中的伪影，关注外观、运动和相机三个关键感知维度。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评估方法通常只提供粗略的质量评分，缺乏对具体伪影的详细定位和分类，无法满足日益增长的高质量视频生成评估需求。

Method: 1. 定义基于人类感知的三个评估维度：外观、运动和相机，包含10类常见伪影；2. 构建GenVID数据集（8万视频，多种SOTA模型生成，人工标注）；3. 开发DVAR框架进行密集视频伪影识别和分类。

Result: 实验表明该方法显著提高了伪影检测准确率，能有效过滤低质量内容，为视频生成评估提供了细粒度分析工具。

Conclusion: 论文提出了一个全面的视频生成伪影评估框架，通过大规模数据集和密集识别方法，实现了对生成视频质量的细粒度分析和改进。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [18] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: Li-ViP3D++ 是一个基于查询的多模态端到端感知与预测框架，通过 Query-Gated Deformable Fusion 融合多视角 RGB 和 LiDAR 数据，在 nuScenes 数据集上实现了更高的 EPA、mAP 和更低的误报率。


<details>
  <summary>Details</summary>
Motivation: 模块化感知与预测流程存在信息流动受限和误差传播问题，现有的查询式端到端模型虽然有所改进，但在多模态（摄像头与激光雷达）融合方面仍然依赖启发式对齐和离散选择，未能充分利用信息并可能引入偏差。

Method: 提出 Query-Gated Deformable Fusion (QGDF)：1) 通过掩码注意力跨摄像头和特征层聚合图像证据；2) 通过可学习的查询偏移进行完全可微的 BEV 采样提取 LiDAR 上下文；3) 使用查询条件门控机制自适应加权视觉和几何线索。该框架在单一端到端模型中联合优化检测、跟踪和多假设轨迹预测。

Result: 在 nuScenes 数据集上，Li-ViP3D++ 提升了端到端行为质量和检测质量：EPA 达到 0.335，mAP 达到 0.502，显著降低了误报率（FP ratio 0.147），且比之前的 Li-ViP3D 变体更快（139.82 ms vs. 145.91 ms）。

Conclusion: 研究表明，在查询空间中实现完全可微的摄像头-LiDAR 融合能够增强端到端感知与预测的鲁棒性，同时不牺牲部署可行性。

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [19] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: 本文提出C-SAM框架，将SAM的锐度感知学习从参数扰动转向掩码扰动，以同时优化模型压缩和输入变化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: SAM能提升DNN对输入变化的鲁棒性，但在模型压缩场景中存在局限性：1）剪枝SAM训练的模型会破坏鲁棒性，因为连续参数空间的平坦性不一定适用于剪枝引起的离散结构变化；2）剪枝后应用SAM会受到早期剪枝模式造成的架构限制。

Method: 提出C-SAM框架，通过显式扰动剪枝掩码（而非参数）进行训练，促进在模型结构方面的更平坦损失景观，从而发现同时优化模型紧凑性和输入变化鲁棒性的剪枝模式。

Result: 在CelebA-HQ、Flowers-102和CIFAR-10-C数据集上，使用ResNet-18、GoogLeNet和MobileNet-V2进行实验，C-SAM相比强基线实现了更高的认证鲁棒性（提升高达42%），同时保持与未剪枝模型相当的任务准确率。

Conclusion: C-SAM通过将锐度感知学习从参数扰动转向掩码扰动，有效解决了模型压缩与鲁棒性之间的权衡问题，为设备端DNN部署提供了同时优化紧凑性和鲁棒性的有效框架。

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [20] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 该研究提出了一种双域学习策略，通过整合无施源器（NA）和有施源器（WA）的CT数据，解决医学图像分割中的协变量偏移问题。研究发现，仅需在NA数据中加入10-30%的WA数据，就能达到与纯WA数据训练相当的分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中协变量偏移导致的性能下降是一个主要挑战。在CT引导的妇科近距离放疗中，有施源器（WA）的CT扫描稀缺且存在显著解剖变形和成像伪影，而无施源器（NA）的CT扫描虽然广泛可用，但无法捕捉WA图像的特征。需要解决如何利用有限目标域数据与分布偏移样本进行有效学习的问题。

Method: 提出双域学习策略，整合NA和WA CT数据。使用精选的混合数据集，系统评估不同比例WA数据（10-30%）对分割性能的影响。在轴向、冠状和矢状平面上使用多种深度学习架构进行实验，验证方法的有效性。

Result: 实验表明，仅使用NA数据无法有效分割WA图像。但在NA训练集中加入少量WA数据（10-30%）能显著提升性能，达到与纯WA数据训练相当的分割效果。最佳结果达到Dice相似系数0.94和交并比0.92，证明有效的域适应和临床可靠性提升。

Conclusion: 该研究证明整合解剖相似但分布偏移的数据集能有效克服数据稀缺问题，提升深度学习分割模型在近距离放疗治疗规划中的鲁棒性和泛化能力。双域学习策略为医学图像分割中的协变量偏移问题提供了实用解决方案。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [21] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 提出基于物理结构的单图像质量估计框架，通过几何体积和材料语义来约束质量预测，解决视觉质量估计的歧义性问题。


<details>
  <summary>Details</summary>
Motivation: 从RGB图像估计物体质量具有挑战性，因为质量取决于几何体积和材料密度，这两者都无法从RGB外观直接观测。质量预测是一个病态问题，需要物理上有意义的表示来约束解空间。

Method: 从单张RGB图像中，通过单目深度估计恢复物体中心的三维几何来获取体积信息，使用视觉语言模型提取粗略材料语义来指导密度推理。通过实例自适应门控机制融合几何、语义和外观表示，并通过两个独立的回归头预测体积相关和密度相关的物理引导潜在因子，仅使用质量监督进行训练。

Result: 在image2mass和ABO-500数据集上的实验表明，该方法在质量估计任务上持续优于现有最优方法。

Conclusion: 通过将视觉线索与物理因素对齐，提出的物理结构化框架能够有效解决单图像质量估计的歧义性问题，在多个数据集上表现出色。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [22] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 提出SLDM模型，通过结构约束和语言引导的扩散模型，从低剂量造影剂CT生成正常剂量造影剂CT图像，减少造影剂用量同时保持诊断能力。


<details>
  <summary>Details</summary>
Motivation: 碘造影剂(ICM)过量使用会导致肾损伤和过敏反应等风险。现有深度学习方法难以在不完全配对图像中实现准确增强，主要因为模型识别特定结构的能力有限。

Method: 提出结构约束语言引导扩散模型(SLDM)：1) 提取图像结构先验信息约束模型推理；2) 引入具有空间智能的语义监督策略，整合视觉感知和空间推理；3) 应用减影血管增强模块改善造影剂区域对比度。

Result: 定性的视觉比较和多个指标的定量结果都证明了该方法在低剂量造影剂CT血管造影重建中的有效性。

Conclusion: SLDM模型通过整合结构协同和空间智能，能够在不完全配对图像中实现准确的CT造影增强，为减少造影剂剂量同时保持诊断能力提供了有效解决方案。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [23] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: TPGDiff提出了一种三重先验引导的扩散网络，用于统一的图像恢复。该方法通过结构先验、语义先验和退化先验的层次化互补引导，解决了现有方法在严重退化区域内容重建困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的一体化图像恢复方法通常依赖退化先验进行引导，但在严重退化区域的内容重建上存在困难。虽然最近的研究尝试利用语义信息来辅助内容生成，但将其集成到扩散模型的浅层会破坏空间结构（如模糊伪影）。

Method: 提出了三重先验引导扩散网络（TPGDiff）：1）在浅层引入多源结构线索作为结构先验，捕捉细粒度细节；2）在深层引入通过蒸馏驱动的语义提取器生成的语义先验，提供可靠的高层指导；3）使用退化提取器学习退化感知先验，在所有时间步实现阶段自适应控制。

Result: 在单退化和多退化基准测试上的广泛实验表明，TPGDiff在各种恢复场景中实现了卓越的性能和泛化能力。

Conclusion: TPGDiff通过层次化互补的先验引导机制，有效解决了现有方法在严重退化区域内容重建的困难，为一体化图像恢复提供了新的解决方案。

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [24] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: OSDEnhancer：首个通过高效一步扩散过程实现真实世界时空视频超分辨率的方法，通过线性预插值初始化时空结构，结合时间精化与空间增强混合专家模型，以及双向可变形VAE解码器，在保持优异泛化能力的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在视频超分辨率方面表现出色，但在时空视频超分辨率（STVSR）领域潜力未充分探索。STVSR不仅需要从低分辨率恢复高分辨率细节，还需提升帧率并保持时间一致性。现有方法大多基于简化的退化假设，难以应对真实世界中复杂未知的退化情况。

Method: 提出OSDEnhancer框架：1）采用线性预插值策略初始化基本时空结构；2）训练时间精化与空间增强混合专家模型（TR-SE MoE），让不同专家路径分别学习时间一致性和空间细节的专业表示；3）引入双向可变形VAE解码器进行循环时空聚合与传播，增强跨帧重建保真度；4）通过高效的一步扩散过程实现真实世界STVSR。

Result: 实验表明该方法在保持优异泛化能力的同时，在真实世界场景中达到了最先进的性能水平。

Conclusion: OSDEnhancer是首个通过一步扩散过程实现真实世界时空视频超分辨率的方法，通过创新的混合专家架构和双向可变形VAE解码器，有效解决了STVSR中的重建保真度和时间一致性问题，在真实场景中展现出强大的性能。

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [25] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: CPiRi提出了一种通道排列不变的多变量时间序列预测框架，通过解耦时空架构和排列不变正则化训练策略，实现不依赖固定通道顺序的跨通道特征学习。


<details>
  <summary>Details</summary>
Motivation: 当前多变量时间序列预测方法存在局限性：通道依赖模型容易过拟合通道顺序，无法适应通道增减或重排；通道独立模型忽略了通道间依赖关系，性能受限。需要一种既能学习跨通道特征又不依赖固定通道顺序的方法。

Method: 提出CPiRi框架，包含：1）时空解耦架构：冻结的预训练时间编码器提取高质量时间特征，轻量级空间模块学习内容驱动的通道间关系；2）排列不变正则化训练策略：通过通道打乱策略在训练中强制实现通道排列不变性。

Result: 在多个基准测试中取得SOTA结果；在通道顺序打乱时保持稳定；具有强归纳泛化能力，即使仅训练一半通道也能泛化到未见通道；在大规模数据集上保持实际效率。

Conclusion: CPiRi通过通道排列不变框架解决了现有方法的局限性，能够适应通道结构和分布的协同漂移而无需重新训练，为多变量时间序列预测提供了更灵活和鲁棒的解决方案。

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [26] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 本文提出GVGS方法，通过高斯可见性感知的多视角几何一致性约束和渐进式四叉树校准的单目深度约束，解决了3D高斯泼溅中表面重建不准确的问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽能实现高效优化和高质量渲染，但准确的表面重建仍具挑战。现有方法通过多视角几何一致性或单目深度先验来改进高斯深度估计，但多视角约束在大几何差异下不可靠，而单目先验存在尺度模糊和局部不一致问题，导致高斯深度监督不准确。

Method: 1) 高斯可见性感知的多视角几何一致性约束：通过聚合共享高斯基元在不同视角下的可见性，提供更准确稳定的几何监督；2) 渐进式四叉树校准的单目深度约束：从粗到细的空间尺度执行块状仿射校准，缓解深度先验的尺度模糊，同时保留细粒度表面细节。

Result: 在DTU和TNT数据集上的大量实验表明，该方法在几何精度上优于先前的高斯基和隐式表面重建方法，实现了更准确的表面重建。

Conclusion: 提出的GVGS方法通过结合可见性感知的多视角约束和渐进校准的单目深度约束，有效解决了3D高斯泼溅中表面重建的准确性挑战，在多个基准数据集上表现出优越性能。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [27] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: TopoOT是一种结合拓扑数据分析与最优传输的异常分割框架，通过多尺度持久性图对齐和稳定性评分，在域偏移下实现鲁棒的自适应异常检测。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的二值化方法在分布偏移下会产生脆弱的掩码，而拓扑数据分析能够从全局结构角度表征异常，捕捉跨尺度的连接性和循环等结构不变量。

Method: 提出TopoOT框架，整合多过滤持久性图与测试时自适应。核心创新是最优传输链式对齐，顺序对齐不同阈值和过滤下的持久性图，生成测地稳定性评分，识别跨尺度一致保留的特征。使用稳定性感知伪标签监督轻量级头部，结合OT一致性和对比学习目标进行在线训练。

Result: 在标准2D和3D异常检测基准测试中，TopoOT取得最先进性能，在2D数据集上比最竞争方法提升高达24.1%的平均F1分数，在3D异常分割基准上提升10.2%。

Conclusion: TopoOT通过拓扑感知的最优传输框架，将持久性图与测试时自适应相结合，实现了在域偏移下鲁棒的异常分割，证明了拓扑结构分析在异常检测中的有效性。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [28] [MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis](https://arxiv.org/abs/2601.20347)
*Chengying She,Chengwei Chen,Xinran Zhang,Ben Wang,Lizhuang Liu,Chengwei Shao,Yun Bian*

Main category: cs.CV

TL;DR: MMSF是一个用于计算病理学的多任务多模态监督框架，通过线性复杂度的MIL骨干网络分解和融合跨模态信息，在多个数据集上显著提升了分类和生存预测性能。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中需要整合千兆像素的整张切片图像（显示肿瘤形态）和患者级临床描述符（提供预后补充信息），但异质信号的特征空间具有不同的统计特性和尺度，整合这些信息具有挑战性。

Method: MMSF框架包含：1）图特征提取模块嵌入组织拓扑结构；2）临床数据嵌入模块标准化患者属性；3）特征融合模块对齐模态共享和模态特定表示；4）基于Mamba的MIL编码器配合多任务预测头。

Result: 在CAMELYON16和TCGA-NSCLC上，准确率提升2.1-6.6%，AUC提升2.2-6.9%；在五个TCGA生存队列中，C-index比单模态方法提升7.1-9.8%，比多模态替代方法提升5.6-7.1%。

Conclusion: MMSF通过显式分解和融合跨模态信息，有效整合了病理图像和临床数据的异质信号，在分类和生存预测任务上显著优于现有方法，为计算病理学提供了强大的多模态分析框架。

Abstract: Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\% accuracy and 2.2--6.9\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\% C-index improvements compared with unimodal methods and 5.6--7.1\% over multimodal alternatives.

</details>


### [29] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: PalmBridge：基于向量量化的特征空间对齐框架，用于开集掌纹验证，通过映射到代表性向量并混合来抑制域偏移的干扰，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度掌纹识别模型通常假设封闭且静态的分布，容易过拟合到数据集特定纹理，而非学习域不变表示。数据增强方法通常假设增强样本能近似目标部署分布，但在显著域不匹配时这一假设往往失效。

Method: 提出PalmBridge，一种即插即用的特征空间对齐框架，基于向量量化学习训练特征中的紧凑代表性向量集。在注册和验证时，每个特征向量通过最小距离准则映射到最近的代表性向量，然后将映射向量与原向量混合。代表性向量与骨干网络通过任务监督、特征一致性目标和正交正则化项联合优化。

Result: 在多个掌纹数据集和骨干架构上的实验表明，PalmBridge在数据集内开集评估中持续降低EER，并提升跨数据集泛化能力，运行时开销可忽略至适度。

Conclusion: PalmBridge通过特征空间对齐有效缓解了掌纹识别中的域偏移问题，提供了一种不依赖数据增强的解决方案，提升了模型在异构部署条件下的鲁棒性和泛化性能。

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [30] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 论文提出了SpatialGenEval基准测试和SpatialT2I数据集，用于系统评估和提升文本到图像模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理复杂空间关系（如空间感知、推理、交互）方面存在明显不足，而现有基准测试因提示设计简短或信息稀疏而无法充分评估这些关键方面。

Method: 1. 构建SpatialGenEval基准：包含1,230个信息密集的长提示，覆盖25个真实场景，每个提示涉及10个空间子领域和相应的10个多项选择题-答案对。
2. 创建SpatialT2I数据集：包含15,400个文本-图像对，通过重写提示确保图像一致性同时保持信息密度。
3. 对21个最先进模型进行广泛评估，并在基础模型上进行微调。

Result: 1. 评估发现高阶空间推理仍然是主要瓶颈。
2. 在Stable Diffusion-XL、Uniworld-V1、OmniGen2等基础模型上微调后，分别获得+4.2%、+5.7%、+4.4%的性能提升，并在空间关系上产生更真实的效果。

Conclusion: 该工作不仅提供了系统评估文本到图像模型空间智能的基准，还展示了通过数据为中心的方法（使用信息密集的训练数据）可以有效提升模型的空间推理能力，为实现文本到图像模型的空间智能提供了新范式。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [31] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: CURVE是一个因果启发的框架，通过变分不确定性建模和不确定性引导的结构正则化来抑制高方差、环境特定的关系，以提升场景图在分布外泛化中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 场景图虽然为场景理解提供了结构化抽象，但常常过度拟合虚假相关性，严重阻碍了分布外泛化能力。需要解决这一限制以提升场景图在分布偏移下的可靠性。

Method: 提出CURVE框架，结合变分不确定性建模和不确定性引导的结构正则化。采用原型条件去偏方法，将不变交互动态与环境依赖变化解耦，促进稀疏且域稳定的拓扑结构。

Result: 在零样本迁移和低数据模拟到真实适应场景中评估CURVE，验证了其学习域稳定稀疏拓扑的能力，并能提供可靠的不确定性估计以支持分布偏移下的风险预测。

Conclusion: CURVE通过因果启发的方法有效抑制了场景图中的虚假相关性，提升了分布外泛化能力，为场景理解提供了更鲁棒的结构化表示。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [32] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 本文提出RAW-Flow框架，将RGB-to-RAW重建重新定义为确定性潜在传输问题，通过流匹配学习潜在空间中的向量场，有效恢复RAW数据细节和颜色信息。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法将RGB-to-RAW重建视为直接回归任务，但由于逆ISP的病态性和量化RGB图像的信息损失，存在细节不一致和颜色偏差问题。需要新方法来克服这些限制。

Method: 1. 提出RAW-Flow框架，将RGB-to-RAW重建重新定义为确定性潜在传输问题，使用流匹配学习潜在空间中的向量场；2. 引入跨尺度上下文引导模块，将分层RGB特征注入流估计过程；3. 设计具有特征对齐约束的双域潜在自编码器，联合编码RGB和RAW输入，支持稳定训练和高保真重建。

Result: 大量实验表明，RAW-Flow在定量和视觉上都优于最先进的方法。

Conclusion: 通过生成视角重新定义RGB-to-RAW重建为潜在传输问题，RAW-Flow能够有效恢复RAW数据的结构细节和颜色信息，解决了现有方法的局限性。

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [33] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: 论文提出了一种集成了RFID门禁控制和多传感器环境安全监测的双模态物联网框架，通过统一云架构实现协同工作，在保持低成本的同时达到专业级性能。


<details>
  <summary>Details</summary>
Motivation: 传统物理安防系统和环境安全监测系统各自独立运行，导致操作效率低下、应急响应延迟和管理复杂性增加，需要一种集成的解决方案。

Method: 设计了一个双子系统框架：子系统1实现RFID认证与伺服门控，子系统2提供火焰检测、水流测量等安全监测功能，两者均采用ESP32微控制器进行边缘处理和无线连接，通过统一云架构集成。

Result: 45天实验显示出色性能：RFID认证准确率99.2%，响应时间0.82秒；火焰检测可靠性98.5%（5米范围）；云数据记录成功率99.8%；总成本仅5400 BDT（约48美元），比商业方案降低82%。

Conclusion: 该研究建立了一个实用的安全-安防协同集成框架，证明通过精心架构设计和组件优化，可以在保持卓越成本效益和可访问性的同时实现专业级性能，适用于多样化应用场景。

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [34] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: 提出RepSFNet轻量级网络，通过大核重参数化与特征融合模块，在保持精度的同时显著降低计算成本，适用于实时人群计数。


<details>
  <summary>Details</summary>
Motivation: 解决人群计数中因尺度变化、遮挡和现有模型计算成本高导致的挑战，需要设计一个既准确又轻量、适合实时和边缘计算的方法。

Method: 使用RepLK-ViT主干网络提取多尺度特征，结合ASPP和CAN的特征融合模块进行密度自适应上下文建模，通过拼接融合模块保持空间分辨率并生成高质量密度图，避免注意力机制和多分支设计以降低复杂度。

Result: 在ShanghaiTech、NWPU和UCF-QNRF数据集上达到竞争性精度，推理延迟比现有方法降低高达34%，参数和计算复杂度显著减少。

Conclusion: RepSFNet是一种准确、轻量且适合实时人群计数的方法，在精度和效率之间取得了良好平衡，适用于边缘计算应用。

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [35] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: HINT是首个基于扩散模型的自回归多人运动生成框架，通过分层交互建模处理变长文本和可变人数，在交互基准上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有离线多人运动生成方法只能处理固定长度和固定人数的运动，无法适应变长文本和可变人数场景。这促使研究者开发自回归框架来逐步生成运动序列。

Method: 1) 在规范化潜在空间中使用解耦运动表示，分离局部运动语义和人际交互；2) 采用滑动窗口策略进行高效在线生成；3) 聚合窗口内局部条件和跨窗口全局条件，捕捉历史轨迹、人际依赖和文本对齐。

Result: 在公开基准测试中，HINT与强离线模型性能相当并超越自回归基线。在InterHuman数据集上，FID达到3.100，显著优于之前最优的5.154。

Conclusion: HINT是首个自回归多人运动生成框架，通过分层交互建模有效处理变长文本和可变人数，实现了细粒度交互建模和长序列连贯性，在多个指标上达到最先进水平。

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [36] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: BiFTA通过视觉视图精炼和文本描述精炼去除冗余信息，提升CLIP模型的零样本性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将细粒度文本描述与局部图像块对齐，但两者都包含冗余信息，降低了文本-视觉对齐的有效性

Method: 提出双向精炼方法：1) 视图精炼 - 通过高IoU比率去除冗余图像块；2) 描述精炼 - 通过高余弦相似度去除冗余文本描述

Result: 在6个基准数据集上，基于ViT和ResNet的CLIP模型都取得了优异的零样本性能

Conclusion: 去除视觉-文本对齐中的冗余信息对于提升零样本性能是必要的，BiFTA方法为此提供了有效解决方案

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [37] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 提出了Quartet of Diffusions框架，通过四个协调的扩散模型分别学习全局形状潜在、对称性、语义部件及其空间组合的分布，实现具有保证对称性、部件一致性和高质量输出的点云生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将形状生成视为整体过程，要么仅支持部件组合，缺乏对对称性和部件结构的系统性建模。需要一种能够同时整合对称性和部件先验的生成框架。

Method: 使用四个协调的扩散模型：全局形状潜在扩散模型、对称性扩散模型、语义部件扩散模型和空间组装扩散模型。通过解耦生成过程实现可解释的组件控制，中心全局潜在确保组装部件间的结构一致性。

Result: 实验表明Quartet达到了最先进的性能，实现了保证的对称性、一致的部件放置和多样化的高质量输出，是首个在生成过程中完全整合并强制执行对称性和部件先验的3D点云生成框架。

Conclusion: Quartet of Diffusions通过结构化建模部件组合和对称性，实现了对形状属性的细粒度控制，支持在保持全局一致性的同时针对性地操作单个部件，为点云生成提供了新的结构化方法。

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [38] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: Youtu-Parsing是一个高效多功能的文档解析模型，采用动态分辨率ViT提取共享特征，结合提示引导的Youtu-LLM-2B语言模型进行布局分析和区域提示解码，通过令牌并行和查询并行策略实现5-11倍加速。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统文档解析模型在处理复杂文档结构（如表格、公式、图表等）时效率低下、并行性差的问题，需要开发一个既能保持高质量输出又能大幅提升解码速度的文档解析系统。

Method: 采用解耦且特征可复用的架构：动态分辨率ViT提取共享文档特征，提示引导的Youtu-LLM-2B进行布局分析。核心创新是两种并行解码策略：令牌并行（每步生成64个候选令牌，通过验证机制筛选）和查询并行（同时预测多个边界框内容，最多5个）。

Result: 在OmniDocBench和olmOCR-bench基准测试中达到SOTA性能。令牌并行实现5-11倍加速（特别适合表格识别），查询并行提供额外2倍加速且保持输出质量。模型能处理文本、公式、表格、图表、印章等多种元素，对罕见字符、多语言和手写内容具有强鲁棒性。

Conclusion: Youtu-Parsing通过创新的并行解码策略实现了高效且高质量的文档解析，在大规模文档智能应用中具有显著的实验价值和实用价值。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [39] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: MARE提出了一种基于多模态对齐和强化学习的可解释Deepfake检测方法，通过视觉语言模型增强检测准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，对Deepfake检测提出了新的要求。现有方法主要将问题建模为分类或空间定位，需要更准确可靠且可解释的检测方法。

Method: 1. 设计综合奖励函数，结合人类反馈强化学习（RLHF），激励生成文本-空间对齐的推理内容；2. 引入伪造解耦模块，从高级面部语义中捕捉内在伪造痕迹，提升真实性检测能力。

Result: MARE在准确性和可靠性方面达到最先进的性能，定量和定性实验结果均证明了其优越性。

Conclusion: MARE通过多模态对齐和强化学习实现了可解释的Deepfake检测，为对抗恶意内容传播提供了更准确可靠的解决方案。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [40] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: 该论文提出了一种通过"污染"真实图像来训练AI生成图像检测器的方法，利用图像生成器的最终架构组件来提升检测器对未见生成器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI图像生成器的快速普及，准确检测AI生成图像对维护可信的在线环境至关重要。然而，现有的深度伪造检测器对未见生成器生成的图像泛化能力较差。研究者发现，尽管现代图像生成器（如扩散模型或自回归模型）采用不同的训练范式，但它们共享相似的最终架构组件，这些组件负责将中间表示转换为最终图像。

Method: 提出"污染"真实图像的方法：使用图像生成器的最终组件处理真实图像，然后训练检测器区分原始真实图像和被"污染"的图像。引入基于生成器最终组件的分类法，将21个广泛使用的生成器进行分类。基于DINOv3骨干网络进行微调，仅使用三个代表性类别中每个类别的100个样本进行训练。

Result: 在22个未见生成器的测试集上，检测器平均准确率达到98.83%。这证明了该方法能有效提升检测器对未见生成器的泛化能力。

Conclusion: 通过利用图像生成器共享的最终架构组件来"污染"真实图像，可以训练出具有强大泛化能力的AI生成图像检测器。该方法仅需少量训练样本就能实现高准确率，为应对不断涌现的新型图像生成器提供了有效的解决方案。

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [41] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出Dummy Forcing方法，通过异质内存分配和动态头编程减少自注意力头对历史帧的冗余利用，实现2.0倍加速，在质量下降小于0.5%的情况下达到24.3 FPS的视频生成。


<details>
  <summary>Details</summary>
Motivation: 发现自回归视频扩散模型中多头自注意力机制对历史帧的利用不足，约25%的注意力头几乎只关注当前帧，丢弃其KV缓存仅导致轻微性能下降。

Method: 提出Dummy Forcing方法，包括：1) 异质内存分配减少头间上下文冗余；2) 动态头编程自适应分类头类型；3) 上下文打包技术实现更激进的缓存压缩。

Result: 无需额外训练，相比基线实现最高2.0倍加速，视频生成达到24.3 FPS，质量下降小于0.5%。

Conclusion: Dummy Forcing通过有效控制不同注意力头的上下文可访问性，显著提升了自回归视频扩散模型的推理效率，同时保持了生成质量。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [42] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 研究探讨了在部分标注数据下训练联合白质高信号和缺血性卒中病灶分割模型的六种策略，发现伪标签方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 白质高信号和缺血性卒中病灶在FLAIR序列中视觉上容易混淆且常同时出现，开发能够区分这两者的深度学习分割模型具有挑战性，特别是当数据只有部分标注时。

Method: 研究了六种训练策略，结合私有和公开的部分标注数据集，共使用2052个MRI体积（1341个有WMH标注，1152个有ISL标注），评估不同方法在部分标注数据下的表现。

Result: 多种方法能有效利用部分标注数据提升模型性能，其中伪标签方法取得了最佳结果。

Conclusion: 伪标签是处理部分标注数据、训练联合WMH和ISL分割模型的有效策略，能显著提升模型在区分这两种脑小血管病影像特征方面的性能。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [43] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 论文提出Latent Temporal Discrepancy (LTD)作为运动先验来指导损失加权，解决现有视频生成模型在动态场景中质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在静态场景表现良好，但在动态视频生成中性能有限，质量在剧烈动态变化时会下降。这是因为噪声破坏了时间一致性，增加了学习动态区域的难度。现有扩散模型对所有场景都依赖静态损失，限制了其捕捉复杂动态的能力。

Method: 引入Latent Temporal Discrepancy (LTD)作为运动先验来指导损失加权。LTD在潜在空间中测量帧间变化，为差异较大的区域分配更大的惩罚，同时对稳定区域保持常规优化。这种运动感知策略稳定了训练，使模型能更好地重建高频动态。

Result: 在通用基准VBench和运动聚焦的VMBench上进行大量实验，结果显示一致改进。该方法在VBench上优于强基线3.31%，在VMBench上优于基线3.58%，在运动质量方面实现了显著提升。

Conclusion: 提出的LTD运动先验方法能有效提升视频生成模型在动态场景中的表现，通过运动感知的损失加权策略，显著改善了模型对复杂动态的捕捉能力。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [44] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 本文提出肖像集合生成任务，构建了首个大规模数据集CHEESE和框架SCheese，通过自适应特征融合和细粒度特征注入解决多属性修改与细节保持的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体平台的发展，用户需要直观的方法来创建多样化、高质量的肖像集合。现有方法面临复杂多属性修改和高保真细节保持的双重挑战。

Method: 提出CHEESE数据集（24K肖像集合，573K样本）和SCheese框架，结合文本引导生成与分层身份细节保持，采用自适应特征融合机制和ConsistencyNet注入细粒度特征。

Result: 综合实验验证了CHEESE数据集在推进PCG任务上的有效性，SCheese框架实现了最先进的性能表现。

Conclusion: 本文成功定义了肖像集合生成任务，并通过构建大规模数据集和创新框架有效解决了该任务的核心挑战，为社交媒体内容创作提供了新工具。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [45] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: 论文提出CoTA方法解决dMLLMs缓存机制导致的重复文本生成问题，通过增强上下文注意力并引入惩罚项来缓解重复诅咒


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的多模态大语言模型使用缓存加速推理时会导致重复文本生成（重复诅咒），需要探究其机制并提出解决方案

Method: 从信息流角度分析重复生成机制，提出CoTA方法：增强上下文token的注意力以保持信息流模式，在解码时对置信度分数引入惩罚项以避免不确定上下文token驱动的输出

Result: CoTA在缓解重复生成方面显著有效，在通用任务上实现了一致的性能提升

Conclusion: 通过信息流分析揭示了重复诅咒的机制，提出的CoTA方法能有效缓解该问题，为dMLLMs的缓存优化提供了新思路

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [46] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: AnomalyVFM是一个将预训练视觉基础模型（VFMs）转化为强零样本异常检测器的框架，通过三阶段合成数据集生成和参数高效适配机制，在9个数据集上实现94.1%的平均图像级AUROC，超越现有方法3.3个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有基于纯视觉基础模型（VFMs）的零样本异常检测方法性能落后于基于视觉语言模型（VLMs）的方法，主要原因是两个实际问题：现有辅助异常检测数据集多样性有限，以及VFM适配策略过于浅层。

Method: 提出AnomalyVFM框架，包含：1）鲁棒的三阶段合成数据集生成方案；2）参数高效适配机制，使用低秩特征适配器和置信度加权的像素损失。这些组件共同使现代VFMs能够大幅超越当前最先进方法。

Result: 以RADIO为骨干网络，AnomalyVFM在9个多样化数据集上实现平均图像级AUROC为94.1%，显著超越先前方法3.3个百分点。

Conclusion: AnomalyVFM成功解决了现有VFM零样本异常检测方法的局限性，通过改进的数据集生成和适配策略，将预训练VFMs转化为强大的零样本异常检测器，在多个数据集上取得最先进性能。

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [47] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: 提出IOTA框架，结合数据驱动的黑盒模块与知识驱动的白盒模块，通过对比错误预测与正确认知生成纠正知识，以可解释提示指导黑盒模块，提升下游任务适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效调优方法将预训练模型视为黑盒，仅依赖数据驱动优化，未能充分利用其内在先验知识，限制了模型在下游任务中的适应潜力。

Method: 提出IOTA框架，包含黑盒模块（数据驱动）和白盒模块（知识驱动）。白盒模块通过对比错误预测与正确认知生成纠正知识，将其转化为可解释的人类提示，并通过纠正知识引导的提示选择策略指导黑盒模块进行更准确的预测。

Result: 在12个图像分类基准测试中，在少样本和易到难适应设置下，实验证明了纠正知识的有效性以及该方法优于现有最先进方法。

Conclusion: 通过结合知识和数据驱动的学习信号，IOTA实现了有效的下游任务适应，充分利用了预训练模型的先验知识，提升了适应性能。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [48] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World是一个基于视频生成的开源世界模拟器，具备高保真度、长时记忆和实时交互能力，适用于内容创作、游戏和机器人学习等领域。


<details>
  <summary>Details</summary>
Motivation: 当前开源和闭源技术之间存在差距，需要提供高质量的世界模型来支持各种应用场景，如内容创作、游戏开发和机器人学习。

Method: 基于视频生成技术构建世界模拟器，实现高保真度的环境模拟、分钟级时间跨度下的上下文一致性保持，以及每秒16帧的实时交互能力。

Result: 开发出了LingBot-World世界模拟器，具备多种环境下的高保真度和鲁棒动态特性，支持长时记忆（分钟级时间跨度），并实现低于1秒的实时交互延迟。

Conclusion: LingBot-World作为一个开源的世界模拟器，填补了开源与闭源技术之间的鸿沟，将为社区在内容创作、游戏和机器人学习等领域提供强大的实用工具。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [49] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: DeepSeek-OCR 2提出了一种新颖的视觉编码器DeepEncoder V2，能够根据图像语义动态重新排序视觉标记，模拟人类视觉的因果推理过程，挑战传统的固定顺序处理方式。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型采用固定的光栅扫描顺序（从上到下、从左到右）处理视觉标记，这与人类视觉感知不符。人类视觉会根据语义逻辑采用灵活但连贯的扫描模式，特别是对于复杂布局的图像，人类视觉表现出基于因果推理的顺序处理能力。

Method: 提出DeepEncoder V2编码器，赋予编码器因果推理能力，使其能够在基于LLM的内容解释之前智能地重新排序视觉标记。探索通过两个级联的1D因果推理结构实现2D图像理解的新范式。

Result: 代码和模型权重已在http://github.com/deepseek-ai/DeepSeek-OCR-2公开访问，表明该研究已实现可用的技术方案。

Conclusion: 这项工作提出了一种新的架构方法，通过级联因果推理结构实现2D图像理解，有潜力实现真正的2D推理，为视觉语言模型提供了新的设计思路。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [50] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: DiffVC-RT：首个实现实时扩散神经视频压缩的框架，通过高效架构、一致性建模和异步并行解码，在保持高质量的同时达到206/30fps的编解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散神经视频压缩在实际部署中面临严重信息丢失、推理延迟高和时间一致性差等关键挑战，需要解决这些瓶颈以实现实时应用。

Method: 1. 高效信息模型架构：通过策略性模块替换和剪枝降低计算复杂度，缓解结构信息丢失。2. 显式隐式一致性建模：在U-Net中引入零成本在线时间移位模块增强时间一致性，辅以混合隐式一致性约束。3. 异步并行解码流水线：采用混合半精度，通过批维度时间移位设计实现异步潜在解码和并行帧重建。

Result: 在HEVC数据集上，DiffVC-RT相比VTM-17.0在LPIPS指标上实现80.1%的比特率节省，在NVIDIA H800 GPU上对720p视频达到206fps编码和30fps解码的实时速度。

Conclusion: DiffVC-RT是扩散视频压缩领域的重要里程碑，首次实现了实时性能，为实际部署扩散神经视频压缩系统铺平了道路。

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [51] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出StructAlign方法，通过结构化跨模态对齐来解决连续文本-视频检索中的特征漂移问题，有效缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 连续文本-视频检索面临灾难性遗忘的严重挑战，主要表现为两种特征漂移：模态内特征漂移和跨模态非协同特征漂移，导致模态对齐失效。

Method: 提出StructAlign方法：1) 引入单纯形等角紧框架作为统一几何先验；2) 设计跨模态ETF对齐损失，将文本和视频特征与类别级ETF原型对齐；3) 设计跨模态关系保持损失，利用互补模态保持跨模态相似性关系。

Result: 在基准数据集上的大量实验表明，该方法持续优于最先进的连续检索方法。

Conclusion: 通过同时解决跨模态非协同特征漂移和模态内特征漂移，StructAlign有效缓解了连续文本-视频检索中的灾难性遗忘问题。

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [52] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 本文系统评估了行人重识别模型的跨域泛化能力，比较了监督学习、自监督学习和语言对齐模型三种训练范式，发现语言对齐模型在跨域场景中表现出意外的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 行人重识别在计算机视觉中仍具挑战性，现有模型在跨域应用中的鲁棒性有待评估。本研究旨在探索基础模型（如SigLIP2）是否能通过更丰富、可迁移的视觉表征来提升泛化能力，并回答监督模型在跨域场景中的表现、基础模型在ReID任务中的效果，以及当前模型的弱点等问题。

Method: 研究比较了三种训练范式：监督学习、自监督学习和语言对齐模型。在11个模型和9个数据集上进行了系统性分析，评估模型在训练域和跨域场景中的表现。

Result: 监督模型在训练域内表现优异，但在跨域数据上性能显著下降。语言对齐模型（尽管未针对ReID任务进行专门训练）在跨域场景中展现出令人惊讶的鲁棒性，超越了监督模型的泛化能力。

Conclusion: 语言对齐模型通过丰富的视觉语言联合表征，为行人重识别提供了更强的跨域泛化能力，这为未来ReID研究指出了新的方向：利用基础模型的通用表征能力来克服领域差异问题。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [53] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: CLEAR-Mamba: 基于MedMamba的增强框架，通过超网络自适应条件层和可靠性感知预测方案，解决眼科血管造影图像分类中的跨域适应性和预测可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 解决眼科血管造影（FFA和ICGA）图像分类中的挑战：单模态性质、细微病变模式、显著的设备间差异导致现有方法在泛化性和高置信度预测方面存在局限。

Method: 1. 提出CLEAR-Mamba框架，基于MedMamba优化架构和训练策略
2. HaC（超网络自适应条件层）：根据输入特征分布动态生成参数，提高跨域适应性
3. RaP（可靠性感知预测方案）：基于证据不确定性学习，鼓励模型关注低置信度样本，提高稳定性和可靠性
4. 构建大规模眼科血管造影数据集，涵盖FFA和ICGA两种模态和多种视网膜疾病

Result: CLEAR-Mamba在多个指标上持续优于包括原始MedMamba在内的多个基线模型，在多疾病分类和可靠性感知预测方面表现出特别优势。

Conclusion: 该研究为特定模态医学图像分类任务提供了一个平衡泛化性和可靠性的有效解决方案。

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [54] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: GDCNet是一个用于多模态讽刺检测的新框架，通过生成客观的图像描述作为语义锚点，计算与原始文本的语义和情感差异，结合视觉-文本保真度测量，实现更准确的讽刺检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理视觉和文本内容松散相关或语义间接时表现不佳，且基于大语言模型生成的讽刺线索存在多样性和主观性噪声。需要一种更稳定、基于事实的跨模态冲突检测方法。

Method: 提出生成差异比较网络(GDCNet)，利用多模态大语言模型生成客观、事实基础的图像描述作为语义锚点，计算生成描述与原始文本的语义和情感差异，同时测量视觉-文本保真度，通过门控模块自适应融合差异特征与视觉文本表示。

Result: 在MSD基准测试上进行了广泛实验，GDCNet表现出优越的准确性和鲁棒性，在MMSD2.0基准上建立了新的最先进性能。

Conclusion: GDCNet通过生成客观描述作为稳定语义锚点，有效捕捉跨模态冲突，解决了现有方法在处理松散相关内容和生成噪声方面的局限性，在多模态讽刺检测任务上取得了显著改进。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [55] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: OS-Marathon是一个用于评估计算机使用代理在长时程重复性任务中表现的基准测试，包含242个任务，并提出了一种基于少量示例构建精简演示的有效方法。


<details>
  <summary>Details</summary>
Motivation: 专业环境中存在大量长时程、重复性的工作流程（如处理报销单据、录入学生成绩），这些任务对人类来说繁琐耗时，但对计算机使用代理来说是理想的应用场景。然而，缺乏评估基准是当前的主要瓶颈。

Method: 1) 建立OS-Marathon基准测试，包含242个长时程重复性任务，涵盖2个领域；2) 提出一种经济高效的方法，仅使用少量示例构建精简演示，教导代理底层工作流程逻辑，使其能够在更大的、未见过的数据集合上有效执行类似工作流程。

Result: 广泛实验证明了这些任务的内在挑战性以及所提方法的有效性。该方法能够使代理基于少量示例学习工作流程逻辑，并在更大规模数据上有效执行。

Conclusion: OS-Marathon基准测试填补了长时程重复性任务评估的空白，提出的基于少量示例的精简演示方法为计算机使用代理在实际专业场景中的应用提供了有效的解决方案。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [56] [FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection](https://arxiv.org/abs/2601.20656)
*Diogo J. Paulo,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 提出了一种基于区域感知频率域的单图像人脸篡改检测方法，通过残差频率域建模和马尔可夫随机场区域融合，在跨数据集和跨篡改方法场景下显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前单图像人脸篡改检测（S-MAD）系统在跨数据集场景中表现不佳，缺乏可靠的参考图像。需要一种轻量级方法来解决跨数据集和跨篡改方法的检测挑战。

Method: 1) 引入残差频率域概念，将信号频率与自然频谱衰减解耦，便于区分篡改和真实人脸；2) 采用马尔可夫随机场全局和局部推理，结合不同面部区域的证据做出全局一致决策。

Result: 在FRLL-Morph数据集上平均EER为1.85%，在MAD22数据集上排名第二，平均EER为6.12%。仅使用频谱特征就能在低攻击分类错误率下获得良好的真实人脸分类错误率。

Conclusion: 傅里叶域残差建模与结构化区域融合为深度S-MAD架构提供了有竞争力的替代方案，在跨数据集和跨篡改方法场景下表现出色。

Abstract: Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

</details>


### [57] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 提出了ProSkill数据集，这是首个用于程序性任务中动作级别技能评估的基准数据集，提供了绝对技能评估标注和成对评估标注。


<details>
  <summary>Details</summary>
Motivation: 现有技能评估研究主要关注体育领域，缺乏复杂程序性活动的大规模数据集，且通常只涉及有限动作，要么是成对评估要么是二元标签，无法满足程序性视频技能评估的需求。

Method: 开发了新颖且可扩展的标注协议，利用瑞士锦标赛方案进行高效的成对比较，然后通过ELO评分系统将这些比较聚合成一致、连续的全局分数，从而创建绝对技能评估排名。

Result: 创建了ProSkill基准数据集，并使用该数据集对主要的SOTA技能评估算法进行了基准测试，包括基于排名和成对范式的方法，现有方法在该数据集上表现不佳。

Conclusion: ProSkill数据集为程序性视频技能评估领域提供了有价值的基准，现有SOTA算法在该数据集上的次优表现突显了该领域面临的挑战，同时也证明了ProSkill数据集的重要性。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [58] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: BiMoRS：面向遥感任务的轻量级双模态提示学习框架，通过图像描述模型提取文本语义摘要并与视觉特征融合，在跨域泛化任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前提示学习方法在自然图像上表现良好，但在遥感图像上存在挑战：多标签场景、类内差异大、空间分辨率多样，现有方法难以识别主导语义线索，且对新类泛化能力不足。

Method: 使用冻结的图像描述模型（如BLIP-2）从遥感图像提取文本语义摘要，通过BERT分词器标记化，与CLIP编码器的高层视觉特征融合。轻量级交叉注意力模块基于融合的文本-视觉表示条件化可学习查询提示，生成上下文提示而不改变CLIP主干。

Result: 在四个遥感数据集和三个域泛化任务上评估，BiMoRS表现出稳定的性能提升，平均比强基线高出2%。

Conclusion: BiMoRS有效解决了遥感图像中提示学习的挑战，通过双模态融合提升了跨域泛化能力，为遥感视觉-语言模型适配提供了轻量级解决方案。

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [59] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: LEAF框架通过从大型多模态语言模型(MLLM)教师向轻量级学生回归器蒸馏感知质量先验，显著减少了图像质量评估(IQA)所需的人工标注，同时保持与人类评估的良好对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在图像质量评估任务中表现良好，但适应这些大规模模型计算成本高，且仍依赖大量平均意见分数(MOS)标注。作者认为，对于基于MLLM的IQA，核心瓶颈不在于MLLM的质量感知能力，而在于MOS尺度校准。

Method: 提出LEAF框架：1) 教师模型通过逐点判断和成对偏好进行密集监督，并估计决策可靠性；2) 学生模型通过联合蒸馏学习教师的质量感知模式；3) 在小型MOS子集上校准学生模型，使其与人类标注对齐。

Result: 在用户生成和AI生成的IQA基准测试中，该方法显著减少了对人工标注的需求，同时保持了与MOS的良好相关性，使得在有限标注预算下实现轻量级IQA变得可行。

Conclusion: LEAF框架通过高效利用MLLM教师的知识，解决了IQA任务中标注效率低的问题，为资源受限场景下的图像质量评估提供了实用解决方案。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [60] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: 本文介绍了LEMON基准测试，这是一个专注于STEM讲座视频的多模态理解评估基准，旨在测试模型在长时程推理和跨模态整合方面的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉、音频和语言任务上取得了显著进展，但在长格式、知识密集和时间结构化的教育内容上的表现仍未得到充分探索。现有视频基准测试缺乏对STEM讲座这种需要长期推理和跨模态整合的内容的评估。

Method: 构建了LEMON基准测试，包含2,277个视频片段，涵盖5个学科29门课程，平均时长196.1秒，包含4,181个高质量QA对（3,413个选择题和768个开放性问题）。基准测试具有语义丰富性、紧密耦合的多模态、明确的时间教学结构以及上下文关联的多轮问答等特点。

Result: 实验结果显示，即使最先进的MLLMs（如GPT-4o）在时间推理和教学预测等任务上也存在显著性能差距。基准测试揭示了模型在不同任务上的表现差异，特别是在需要长时程推理的任务上表现不佳。

Conclusion: LEMON是一个可扩展且具有挑战性的基准测试，可用于推动多模态感知、推理和生成在长格式教学内容中的发展。该基准测试有助于识别当前MLLMs的局限性，并为未来研究提供方向。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [61] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 该论文通过统一视觉编码和视觉token技术，从优化角度探讨压缩效率与模型性能的权衡，并预测下一代视觉编解码器和token技术的发展方向。


<details>
  <summary>Details</summary>
Motivation: 压缩效率与智能模型性能之间存在相关性，传统视觉编码和新兴视觉token技术都旨在最小化计算成本的同时最大化语义信息保真度，需要统一理解这两类技术的内在联系。

Method: 首先全面综述视觉编码和视觉token技术两大技术家族，然后从优化角度统一它们，提出连接两者的统一公式，并基于此双向洞察和预测下一代技术发展。

Result: 实验表明任务导向的token技术在MLLMs、AIGC和具身AI等实际任务中具有巨大潜力，并展望未来可能像传统编解码器一样标准化通用token技术。

Conclusion: 通过统一视觉编码和视觉token技术，揭示了压缩效率与模型性能权衡的本质，为下一代视觉表示技术发展提供了理论框架和实践指导，有望推动智能任务的高效统一处理。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [62] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: FairT2V是一个无需训练的去偏框架，通过中性化提示嵌入来减轻文本到视频生成模型中的性别偏见，同时保持语义和时序一致性。


<details>
  <summary>Details</summary>
Motivation: 文本到视频扩散模型存在人口统计学偏见（特别是性别偏见），但这一问题尚未得到充分研究。研究发现这种偏见主要来自预训练文本编码器，即使对于中性提示也会编码隐性的性别关联。

Method: 提出FairT2V框架：1）通过基于锚点的球面测地变换中性化提示嵌入；2）使用动态去噪调度，仅在早期身份形成步骤应用去偏以保持时序一致性；3）提出结合VideoLLM推理和人工验证的视频级公平性评估协议。

Result: 在Open-Sora模型上的实验表明，FairT2V显著减少了跨职业的人口统计学偏见，同时对视频质量影响最小。

Conclusion: FairT2V是一种有效的无需训练的去偏方法，能够减轻文本到视频生成模型中的性别偏见，同时保持生成质量和时序一致性。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [63] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: FunHSI是一个无需训练、功能驱动的框架，能够从开放词汇任务提示生成功能正确的人-场景3D交互，通过功能感知接触推理和阶段优化确保物理合理性和功能正确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对物体功能和相应人-场景接触的显式推理，导致生成不自然或功能错误的3D人-场景交互，限制了在具身AI、机器人和交互内容创建中的应用。

Method: 1. 功能感知接触推理：识别功能场景元素，重建3D几何，通过接触图建模高级交互；2. 利用视觉语言模型合成任务图像并估计3D身体和手部姿势；3. 通过阶段优化细化3D身体配置，确保物理合理性和功能正确性。

Result: FunHSI能够生成功能正确且物理合理的3D人-场景交互，支持从"坐在沙发上"到"提高房间温度"等不同粒度的功能交互，在多样室内外场景中表现一致优秀。

Conclusion: FunHSI通过显式功能推理和接触建模，解决了3D人-场景交互生成中的关键挑战，为具身AI、机器人和交互内容创建提供了更可靠、功能正确的解决方案。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [64] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 本文提出一种用于道路表面分类的多模态框架，融合图像和惯性测量数据，使用双向交叉注意力模块和自适应门控层来应对领域偏移，并引入包含真实、视觉和合成数据的ROAD数据集。


<details>
  <summary>Details</summary>
Motivation: 现有道路表面分类技术由于传感模态有限和数据集缺乏环境多样性，难以在狭窄操作条件之外实现泛化。本文旨在解决这些限制，为环境感知的预测性维护系统提供更鲁棒的解决方案。

Method: 提出多模态框架，融合RGB图像和IMU数据：1) 使用轻量级双向交叉注意力模块实现模态间信息交互；2) 通过自适应门控层在领域偏移下调整模态贡献；3) 引入ROAD数据集，包含真实多模态记录、视觉专用子集和合成数据子集。

Result: 在PVS基准测试上比先前SOTA提升1.4个百分点，在多模态ROAD子集上提升11.6个百分点，在少数类别上获得更高的F1分数。框架在夜间、大雨和混合表面过渡等挑战性视觉条件下表现稳定。

Conclusion: 结合经济型相机和IMU传感器与多模态注意力机制，为道路表面理解提供了可扩展、鲁棒的基础，特别适用于环境变化大且成本受限的地区，推动了环境感知预测性维护系统的发展。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [65] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix：一种无需微调的方法，利用预训练图像扩散模型提升神经辐射场和3D高斯泼溅的外推渲染质量，通过2D-3D交替优化策略和逐像素置信度掩码实现高质量、一致性的新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经辐射场和3D高斯泼溅的新视角合成方法依赖密集输入，在外推视角下质量下降。现有使用扩散模型的方法面临泛化性和保真度的权衡：微调扩散模型能提高保真度但可能过拟合，不微调的方法保持泛化性但保真度较低。

Method: 提出FreeFix方法，包含：1）交替的2D-3D优化策略，利用预训练图像扩散模型进行一致性优化而无需昂贵的视频扩散模型；2）逐像素置信度掩码，识别不确定区域进行针对性改进。

Result: 在多个数据集上的实验表明，FreeFix提升了多帧一致性，性能达到或超过基于微调的方法，同时保持了强大的泛化能力。

Conclusion: FreeFix在无需微调扩散模型的情况下，成功提升了外推渲染的质量，在泛化性和保真度之间取得了更好的平衡，为利用预训练图像扩散模型改进3D重建提供了有效途径。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Gap-K%: Measuring Top-1 Prediction Gap for Detecting Pretraining Data](https://arxiv.org/abs/2601.19936)
*Minseo Kwak,Jaehyung Kim*

Main category: cs.LG

TL;DR: 本文提出Gap-K%方法，利用LLM预训练优化动态进行预训练数据检测，通过分析top-1预测与目标词之间的概率差异，结合滑动窗口捕获局部相关性，在WikiMIA和MIMIR基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)预训练语料库的不透明性引发隐私和版权问题，现有基于词元似然的方法通常忽视与模型top-1预测的差异以及相邻词元间的局部相关性。

Method: 提出Gap-K%方法，基于LLM预训练优化动态：利用top-1预测词元与目标词元之间的对数概率差异，结合滑动窗口策略捕获局部相关性并缓解词元级波动。

Result: 在WikiMIA和MIMIR基准测试中，Gap-K%实现最先进性能，在不同模型规模和输入长度下均优于现有基线方法。

Conclusion: 通过分析预训练优化动态，Gap-K%方法有效解决了现有预训练数据检测方法的局限性，为隐私和版权保护提供了更可靠的检测工具。

Abstract: The opacity of massive pretraining corpora in Large Language Models (LLMs) raises significant privacy and copyright concerns, making pretraining data detection a critical challenge. Existing state-of-the-art methods typically rely on token likelihoods, yet they often overlook the divergence from the model's top-1 prediction and local correlation between adjacent tokens. In this work, we propose Gap-K%, a novel pretraining data detection method grounded in the optimization dynamics of LLM pretraining. By analyzing the next-token prediction objective, we observe that discrepancies between the model's top-1 prediction and the target token induce strong gradient signals, which are explicitly penalized during training. Motivated by this, Gap-K% leverages the log probability gap between the top-1 predicted token and the target token, incorporating a sliding window strategy to capture local correlations and mitigate token-level fluctuations. Extensive experiments on the WikiMIA and MIMIR benchmarks demonstrate that Gap-K% achieves state-of-the-art performance, consistently outperforming prior baselines across various model sizes and input lengths.

</details>


### [67] [DecHW: Heterogeneous Decentralized Federated Learning Exploiting Second-Order Information](https://arxiv.org/abs/2601.19938)
*Adnan Ahmad,Chiara Boldrini,Lorenzo Valerio,Andrea Passarella,Marco Conti*

Main category: cs.LG

TL;DR: 本文提出一种新颖的DFL聚合方法，通过二阶信息近似生成共识权重，解决数据和模型异构性带来的收敛慢问题，在降低通信成本的同时提升局部模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习面临数据和模型初始化异构性挑战，设备间不同的数据经验和交互水平导致局部模型参数差异，从而减慢收敛速度。

Method: 提出基于参数级别证据可信度的聚合方法，通过近似局部模型在本地数据集上的二阶信息生成共识权重，利用这些权重缩放邻居更新后再聚合为全局邻居表示。

Result: 在计算机视觉任务上的大量实验表明，该方法在降低通信成本的同时，显著提升了局部模型的泛化能力。

Conclusion: 通过显式处理参数级别的变化证据可信度，提出的聚合方法能有效应对DFL中的数据和模型异构性问题，实现更快的收敛和更好的泛化性能。

Abstract: Decentralized Federated Learning (DFL) is a serverless collaborative machine learning paradigm where devices collaborate directly with neighbouring devices to exchange model information for learning a generalized model. However, variations in individual experiences and different levels of device interactions lead to data and model initialization heterogeneities across devices. Such heterogeneities leave variations in local model parameters across devices that leads to slower convergence. This paper tackles the data and model heterogeneity by explicitly addressing the parameter level varying evidential credence across local models. A novel aggregation approach is introduced that captures these parameter variations in local models and performs robust aggregation of neighbourhood local updates. Specifically, consensus weights are generated via approximation of second-order information of local models on their local datasets. These weights are utilized to scale neighbourhood updates before aggregating them into global neighbourhood representation. In extensive experiments with computer vision tasks, the proposed approach shows strong generalizability of local models at reduced communication costs.

</details>


### [68] [oculomix: Hierarchical Sampling for Retinal-Based Systemic Disease Prediction](https://arxiv.org/abs/2601.19939)
*Hyunmin Kim,Yukun Zhou,Rahul A. Jonas,Lie Ju,Sunjin Hwang,Pearse A. Keane,Siegfried K. Wagner*

Main category: cs.LG

TL;DR: 提出Oculomix分层采样策略，用于混合样本增强，通过约束患者和检查层面的混合空间来更好地保留患者特定特征，在心血管事件预测任务上相比传统方法提升3% AUROC。


<details>
  <summary>Details</summary>
Motivation: 传统图像级混合样本增强方法（如CutMix、MixUp）会扰乱患者特定的医学合并症和临床因素等属性，因为它们只考虑图像和标签。在眼科影像学（Oculomix）中，需要更好地保留患者特定特征。

Method: 提出Oculomix分层采样策略，基于两个临床先验：1）同一患者同一时间点的图像共享相同属性（检查层面）；2）同一患者不同时间点的图像具有软时间趋势（患者层面）。该方法约束混合空间到患者和检查层面，利用分层关系。

Result: 在大型多种族人群（Alzeye）的五年主要不良心血管事件预测任务中，使用ViT模型验证，Oculomix相比图像级CutMix和MixUp在AUROC上持续提升高达3%。

Conclusion: Oculomix方法在眼科影像学中具有必要性和价值，能更好地保留患者特定特征，提升疾病预测性能。

Abstract: Oculomics - the concept of predicting systemic diseases, such as cardiovascular disease and dementia, through retinal imaging - has advanced rapidly due to the data efficiency of transformer-based foundation models like RETFound. Image-level mixed sample data augmentations, such as CutMix and MixUp, are frequently used for training transformers, yet these techniques perturb patient-specific attributes, such as medical comorbidity and clinical factors, since they only account for images and labels. To address this limitation, we propose a hierarchical sampling strategy, Oculomix, for mixed sample augmentations. Our method is based on two clinical priors. First (exam level), images acquired from the same patient at the same time point share the same attributes. Second (patient level), images acquired from the same patient at different time points have a soft temporal trend, as morbidity generally increases over time. Guided by these priors, our method constrains the mixing space to the patient and exam levels to better preserve patient-specific characteristics and leverages their hierarchical relationships. The proposed method is validated using ViT models on a five-year prediction of major adverse cardiovascular events (MACE) in a large ethnically diverse population (Alzeye). We show that Oculomix consistently outperforms image-level CutMix and MixUp by up to 3% in AUROC, demonstrating the necessity and value of the proposed method in oculomics.

</details>


### [69] [Continuous-Flow Data-Rate-Aware CNN Inference on FPGA](https://arxiv.org/abs/2601.19940)
*Tobias Habermann,Michael Mecik,Zhenyu Wang,César David Vera,Martin Kumm,Mario Garrido*

Main category: cs.LG

TL;DR: 该论文提出了一种针对CNN的数据流感知连续流架构设计方法，通过交织低数据率信号和共享硬件单元，实现接近100%的硬件利用率，从而在单个FPGA上高效实现复杂CNN如MobileNet。


<details>
  <summary>Details</summary>
Motivation: 现有的展开式数据流实现主要关注全连接网络，但CNN在相同精度下需要更少计算量。然而CNN中的池化层和步长大于1的卷积层会导致输出数据减少，严重影响全并行实现中的数据速率，导致硬件单元利用率低下。

Method: 提出数据流感知的连续流CNN架构设计方法：通过分析CNN数据流，采用低数据率信号交织和硬件单元共享技术，配合适当的并行化策略，实现接近完全并行实现的吞吐量。

Result: 该方法能够显著节省算术逻辑资源，使复杂CNN如MobileNet能够在单个FPGA上实现，同时保持高吞吐量，硬件利用率接近100%。

Conclusion: 论文提出的数据流感知连续流架构有效解决了CNN硬件实现中的数据率不匹配问题，通过优化硬件利用率和并行化策略，实现了高效、高吞吐的FPGA部署方案。

Abstract: Among hardware accelerators for deep-learning inference, data flow implementations offer low latency and high throughput capabilities. In these architectures, each neuron is mapped to a dedicated hardware unit, making them well-suited for field-programmable gate array (FPGA) implementation. Previous unrolled implementations mostly focus on fully connected networks because of their simplicity, although it is well known that convolutional neural networks (CNNs) require fewer computations for the same accuracy. When observing the data flow in CNNs, pooling layers and convolutional layers with a stride larger than one, the number of data at their output is reduced with respect to their input. This data reduction strongly affects the data rate in a fully parallel implementation, making hardware units heavily underutilized unless it is handled properly. This work addresses this issue by analyzing the data flow of CNNs and presents a novel approach to designing data-rate-aware, continuous-flow CNN architectures. The proposed approach ensures a high hardware utilization close to 100% by interleaving low data rate signals and sharing hardware units, as well as using the right parallelization to achieve the throughput of a fully parallel implementation. The results show that a significant amount of the arithmetic logic can be saved, which allows implementing complex CNNs like MobileNet on a single FPGA with high throughput.

</details>


### [70] [Latent Object Permanence: Topological Phase Transitions, Free-Energy Principles, and Renormalization Group Flows in Deep Transformer Manifolds](https://arxiv.org/abs/2601.19942)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.LG

TL;DR: 该论文通过几何和统计物理视角研究Transformer语言模型中多步推理能力的涌现，发现在模型达到一定规模时会出现类似相变的维度塌缩现象，形成可复用的"瞬态类别对象"结构。


<details>
  <summary>Details</summary>
Motivation: 研究深度Transformer语言模型中多步推理能力如何从底层计算结构中涌现，探索模型内部表示空间的几何结构和统计物理特性。

Method: 将隐藏状态轨迹视为隐式黎曼流形上的流，分析激活的层间协方差谱，使用基于稀疏性/局部化的序参数追踪随机矩阵本体的偏离，将前向传播形式化为离散粗粒化映射。

Result: 在1.5B-30B规模的模型中观察到有效维度的急剧减少，存在临界归一化深度γ_c≈0.42附近的相变现象，形成低熵区域和可复用的瞬态类别对象(TCOs)。

Conclusion: Transformer语言模型通过层间表示的粗粒化过程实现多步推理，这一过程伴随着相变般的维度塌缩和稳定概念盆地的形成，为理解语言模型的推理能力提供了新的理论框架。

Abstract: We study the emergence of multi-step reasoning in deep Transformer language models through a geometric and statistical-physics lens. Treating the hidden-state trajectory as a flow on an implicit Riemannian manifold, we analyze the layerwise covariance spectrum of activations, where $C^{(\ell)}=\mathbb{E}[h^{(\ell)}h^{(\ell)\top}]$, and track deviations from a random-matrix bulk. Across model scales (1.5B--30B), we observe a sharp reduction in effective dimensionality consistent with a phase transition: an order parameter based on sparsity/localization, $Ω(h)=1-\|h\|_1/(\sqrt{d}\|h\|_2)$, exhibits a discontinuity near a critical normalized depth $γ_c\approx 0.42$ in sufficiently large models. We formalize the forward pass as a discrete coarse-graining map and relate the appearance of stable "concept basins" to fixed points of this renormalization-like dynamics. The resulting low-entropy regime is characterized by a spectral tail collapse and by the formation of transient, reusable object-like structures in representation space, which we call Transient Class Objects (TCOs). We provide theoretical conditions connecting logical separability to spectral decay and validate the predicted signatures with layerwise probes on multiple open-weight model families.

</details>


### [71] [Emergent Specialization in Learner Populations: Competition as the Source of Diversity](https://arxiv.org/abs/2601.19943)
*Yuhao Li*

Main category: cs.LG

TL;DR: NichePopulation算法通过竞争动态诱导学习者自发专业化，无需显式通信或多样性激励，在六个现实领域验证有效。


<details>
  <summary>Details</summary>
Motivation: 探索在没有显式通信或多样性激励的情况下，学习者群体如何发展协调且多样化的行为。旨在通过竞争动态实现自发专业化，符合生态位理论。

Method: 提出NichePopulation算法，结合竞争排斥和生态位亲和度追踪机制。学习者通过竞争动态自发划分为不同环境机制下的专家。

Result: 在六个现实领域（加密货币交易、商品价格、天气预测、太阳辐照度、城市交通和空气质量）验证，平均专业化指数达0.75，效应量Cohen's d > 20。λ=0时仍实现SI>0.30，证明专业化是真正涌现的；多样化群体通过方法级分工比同质基线表现提升26.5%；比MARL基线（QMIX、MAPPO、IQL）表现好4.3倍且快4倍。

Conclusion: 仅通过竞争就足以诱导涌现专业化，学习者通过竞争动态自发形成专家分工。NichePopulation算法简单有效，在多个现实领域验证了其优越性，为群体智能中的专业化问题提供了新解决方案。

Abstract: How can populations of learners develop coordinated, diverse behaviors without explicit communication or diversity incentives? We demonstrate that competition alone is sufficient to induce emergent specialization -- learners spontaneously partition into specialists for different environmental regimes through competitive dynamics, consistent with ecological niche theory. We introduce the NichePopulation algorithm, a simple mechanism combining competitive exclusion with niche affinity tracking. Validated across six real-world domains (cryptocurrency trading, commodity prices, weather forecasting, solar irradiance, urban traffic, and air quality), our approach achieves a mean Specialization Index of 0.75 with effect sizes of Cohen's d > 20. Key findings: (1) At lambda=0 (no niche bonus), learners still achieve SI > 0.30, proving specialization is genuinely emergent; (2) Diverse populations outperform homogeneous baselines by +26.5% through method-level division of labor; (3) Our approach outperforms MARL baselines (QMIX, MAPPO, IQL) by 4.3x while being 4x faster.

</details>


### [72] [Classifier Calibration at Scale: An Empirical Study of Model-Agnostic Post-Hoc Methods](https://arxiv.org/abs/2601.19944)
*Valery Manokhin,Daniel Grønhaug*

Main category: cs.LG

TL;DR: 对21种分类器在表格数据上进行后处理校准方法对比研究，发现Venn-Abers预测器在降低对数损失方面表现最佳，而常用的Platt缩放和等渗回归可能损害现代表格模型的校准性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估模型无关的后处理校准方法在监督二元分类中的效果，特别关注具有分布无关有效性保证的共形和Venn方法，以改善概率预测的可靠性。

Method: 使用TabArena-v0.1套件中的二元任务，通过随机分层五折交叉验证评估21种分类器（包括线性模型、SVM、树集成和现代神经网络）。在单独校准集上训练五种校准器（等渗回归、Platt缩放、Beta校准、Venn-Abers预测器、Pearsonify），并在测试集上应用。

Result: Venn-Abers预测器平均降低对数损失最多，Beta校准紧随其后；Platt缩放效果较弱且不一致。Beta校准在最多任务中改善对数损失，而Venn-Abers表现出更少的极端性能下降。常用的Platt缩放和等渗回归可能系统性地损害现代表格模型的校准性能。

Conclusion: 校准效果因数据集和模型架构差异很大，没有单一方法在所有情况下都最优。虽然除Pearsonify外的方法都轻微提高了准确率，但增益有限（最大约0.008%）。Venn-Abers和Beta校准表现较好，但需要根据具体应用选择合适方法。

Abstract: We study model-agnostic post-hoc calibration methods intended to improve probabilistic predictions in supervised binary classification on real i.i.d. tabular data, with particular emphasis on conformal and Venn-based approaches that provide distribution-free validity guarantees under exchangeability. We benchmark 21 widely used classifiers, including linear models, SVMs, tree ensembles (CatBoost, XGBoost, LightGBM), and modern tabular neural and foundation models, on binary tasks from the TabArena-v0.1 suite using randomized, stratified five-fold cross-validation with a held-out test fold. Five calibrators; Isotonic regression, Platt scaling, Beta calibration, Venn-Abers predictors, and Pearsonify are trained on a separate calibration split and applied to test predictions. Calibration is evaluated using proper scoring rules (log-loss and Brier score) and diagnostic measures (Spiegelhalter's Z, ECE, and ECI), alongside discrimination (AUC-ROC) and standard classification metrics. Across tasks and architectures, Venn-Abers predictors achieve the largest average reductions in log-loss, followed closely by Beta calibration, while Platt scaling exhibits weaker and less consistent effects. Beta calibration improves log-loss most frequently across tasks, whereas Venn-Abers displays fewer instances of extreme degradation and slightly more instances of extreme improvement. Importantly, we find that commonly used calibration procedures, most notably Platt scaling and isotonic regression, can systematically degrade proper scoring performance for strong modern tabular models. Overall classification performance is often preserved, but calibration effects vary substantially across datasets and architectures, and no method dominates uniformly. In expectation, all methods except Pearsonify slightly increase accuracy, but the effect is marginal, with the largest expected gain about 0.008%.

</details>


### [73] [NCSAM Noise-Compensated Sharpness-Aware Minimization for Noisy Label Learning](https://arxiv.org/abs/2601.19947)
*Jiayu Xu,Junbiao Pang*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的视角，通过理论分析损失景观平坦度与标签噪声之间的关系，提出噪声补偿锐度感知最小化(NCSAM)方法，利用SAM的扰动来缓解标签噪声的损害。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常包含错误或损坏的标注（如网络爬取数据），现有的标签噪声学习研究多集中于复杂的标签校正机制。本文从新的理论角度出发，探索损失景观平坦度与标签噪声之间的关系。

Method: 提出噪声补偿锐度感知最小化(NCSAM)，该方法利用锐度感知最小化(SAM)的扰动来补偿标签噪声带来的损害。通过理论分析证明，精心模拟的标签噪声能协同增强模型的泛化性能和噪声鲁棒性。

Result: 在多个基准数据集上的广泛实验表明，该方法在多种任务上始终优于现有的最先进方法。分析显示，测试精度表现出与在无噪声数据集上观察到的相似行为。

Conclusion: 该研究通过理论分析损失平坦度与标签噪声的关系，提出了有效的NCSAM方法，为标签噪声学习提供了新的视角和解决方案，在保持泛化性能的同时增强了模型的噪声鲁棒性。

Abstract: Learning from Noisy Labels (LNL) presents a fundamental challenge in deep learning, as real-world datasets often contain erroneous or corrupted annotations, \textit{e.g.}, data crawled from Web. Current research focuses on sophisticated label correction mechanisms. In contrast, this paper adopts a novel perspective by establishing a theoretical analysis the relationship between flatness of the loss landscape and the presence of label noise. In this paper, we theoretically demonstrate that carefully simulated label noise synergistically enhances both the generalization performance and robustness of label noises. Consequently, we propose Noise-Compensated Sharpness-aware Minimization (NCSAM) to leverage the perturbation of Sharpness-Aware Minimization (SAM) to remedy the damage of label noises. Our analysis reveals that the testing accuracy exhibits a similar behavior that has been observed on the noise-clear dataset. Extensive experimental results on multiple benchmark datasets demonstrate the consistent superiority of the proposed method over existing state-of-the-art approaches on diverse tasks.

</details>


### [74] [Probabilistic Sensing: Intelligence in Data Sampling](https://arxiv.org/abs/2601.19953)
*Ibrahim Albulushi,Saleh Bunaiyan,Suraj S. Cheema,Hesham ElSawy,Feras Al-Dirini*

Main category: cs.LG

TL;DR: 提出一种基于概率神经元(p-neuron)的智能传感范式，通过概率决策决定是否采样，实现无损数据采集的同时大幅降低系统能耗


<details>
  <summary>Details</summary>
Motivation: 传统传感系统中，在数据采集过程中做出确定性采样决策可能导致信息丢失风险。为了解决这一问题，同时实现传感系统的智能化以提升能源效率，研究人员从自主神经系统获得灵感

Method: 采用概率神经元(p-neuron)驱动模拟特征提取电路，构建概率决策传感范式。系统响应时间达到微秒级，突破亚采样率响应时间限制，实现实时智能自主激活数据采样

Result: 在主动地震勘探数据上的验证实验表明，实现了无损概率数据采集，归一化均方误差仅为0.41%，同时系统有效运行时间和生成样本数量减少了93%

Conclusion: 该概率传感范式通过智能概率决策机制，在保证数据质量的同时显著提升能源效率，为传感器智能化提供了新的解决方案

Abstract: Extending the intelligence of sensors to the data-acquisition process - deciding whether to sample or not - can result in transformative energy-efficiency gains. However, making such a decision in a deterministic manner involves risk of losing information. Here we present a sensing paradigm that enables making such a decision in a probabilistic manner. The paradigm takes inspiration from the autonomous nervous system and employs a probabilistic neuron (p-neuron) driven by an analog feature extraction circuit. The response time of the system is on the order of microseconds, over-coming the sub-sampling-rate response time limit and enabling real-time intelligent autonomous activation of data-sampling. Validation experiments on active seismic survey data demonstrate lossless probabilistic data acquisition, with a normalized mean squared error of 0.41%, and 93% saving in the active operation time of the system and the number of generated samples.

</details>


### [75] [MeanCache: From Instantaneous to Average Velocity for Accelerating Flow Matching Inference](https://arxiv.org/abs/2601.19961)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Ruijia Wu,Li YanTao,Qiang Hui,Yuren You,Ting Lu,Chao Tan,Shaoan Zhao,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.LG

TL;DR: MeanCache是一个无需训练的缓存框架，用于高效Flow Matching推理，通过平均速度视角和轨迹稳定性调度策略，在保持生成质量的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有缓存方法虽然减少冗余计算，但通常依赖瞬时速度信息（如特征缓存），在高加速比下容易导致严重轨迹偏差和误差累积。需要一种能有效缓解局部误差累积的缓存方法。

Method: 1. 引入平均速度视角：利用缓存的Jacobian-向量积构建区间平均速度而非瞬时速度
2. 轨迹稳定性调度策略：采用峰值抑制最短路径在预算约束下确定缓存调度，提高缓存时机和JVP重用稳定性

Result: 在FLUX.1、Qwen-Image和HunyuanVideo上分别实现4.12倍、4.56倍和3.59倍加速，同时在生成质量上持续优于最先进的缓存基线方法。

Conclusion: MeanCache为Flow Matching推理提供了简单有效的新视角，有望激发在商业规模生成模型中稳定性驱动加速的进一步探索。

Abstract: We present MeanCache, a training-free caching framework for efficient Flow Matching inference. Existing caching methods reduce redundant computation but typically rely on instantaneous velocity information (e.g., feature caching), which often leads to severe trajectory deviations and error accumulation under high acceleration ratios. MeanCache introduces an average-velocity perspective: by leveraging cached Jacobian--vector products (JVP) to construct interval average velocities from instantaneous velocities, it effectively mitigates local error accumulation. To further improve cache timing and JVP reuse stability, we develop a trajectory-stability scheduling strategy as a practical tool, employing a Peak-Suppressed Shortest Path under budget constraints to determine the schedule. Experiments on FLUX.1, Qwen-Image, and HunyuanVideo demonstrate that MeanCache achieves 4.12X and 4.56X and 3.59X acceleration, respectively, while consistently outperforming state-of-the-art caching baselines in generation quality. We believe this simple yet effective approach provides a new perspective for Flow Matching inference and will inspire further exploration of stability-driven acceleration in commercial-scale generative models.

</details>


### [76] [Cross-Session Decoding of Neural Spiking Data via Task-Conditioned Latent Alignment](https://arxiv.org/abs/2601.19963)
*Canyang Zhao,Bolin Peng,J. Patrick Mayo,Ce Ju,Bing Liu*

Main category: cs.LG

TL;DR: 提出了TCLA框架，通过任务条件潜在对齐解决跨会话神经解码中的非平稳性问题，在有限数据下显著提升解码性能


<details>
  <summary>Details</summary>
Motivation: 植入式电极记录的神经活动存在跨会话非平稳性，导致在一个会话训练的脑机接口解码器难以泛化到后续会话，而新会话通常只有有限数据，使解码器重训练或适应变得困难

Method: 基于自编码器架构，TCLA首先从有足够数据的源会话学习神经动力学的低维表示，然后对有限数据的目标会话，以任务条件的方式将目标潜在表示与源会话对齐，实现学习到的神经动力学的有效迁移

Result: 在猕猴运动和眼动中心-外数据集上评估，相比仅使用目标会话数据训练的基线方法，TCLA在不同数据集和解码设置下均持续提升解码性能，在运动数据集的y坐标速度解码中决定系数提升高达0.386

Conclusion: TCLA提供了从源会话向目标会话迁移知识的有效策略，能够在有限数据条件下实现更稳健的神经解码

Abstract: Cross-session nonstationarity in neural activity recorded by implanted electrodes is a major challenge for invasive Brain-computer interfaces (BCIs), as decoders trained on data from one session often fail to generalize to subsequent sessions. This issue is further exacerbated in practice, as retraining or adapting decoders becomes particularly challenging when only limited data are available from a new session. To address this challenge, we propose a Task-Conditioned Latent Alignment framework (TCLA) for cross-session neural decoding. Building upon an autoencoder architecture, TCLA first learns a low-dimensional representation of neural dynamics from a source session with sufficient data. For target sessions with limited data, TCLA then aligns target latent representations to the source in a task-conditioned manner, enabling effective transfer of learned neural dynamics. We evaluate TCLA on the macaque motor and oculomotor center-out dataset. Compared to baseline methods trained solely on target-session data, TCLA consistently improves decoding performance across datasets and decoding settings, with gains in the coefficient of determination of up to 0.386 for y coordinate velocity decoding in a motor dataset. These results suggest that TCLA provides an effective strategy for transferring knowledge from source to target sessions, enabling more robust neural decoding under conditions with limited data.

</details>


### [77] [Modeling Cascaded Delay Feedback for Online Net Conversion Rate Prediction: Benchmark, Insights and Solutions](https://arxiv.org/abs/2601.19965)
*Mingxuan Luo,Guipeng Xv,Sishuo Chen,Xinyu Li,Li Zhang,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Bo Zheng,Chen Lin*

Main category: cs.LG

TL;DR: 该论文提出了NetCVR预测问题，针对传统CVR忽略退款行为的不足，通过CASCADE数据集和TESLA框架解决多阶段级联延迟反馈的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 传统转化率(CVR)忽略了退款行为，无法完全反映推荐效果的真实用户满意度和商业价值。NetCVR（净转化率）考虑了购买且不退款的概率，但面临多阶段级联延迟反馈、缺乏开源数据集和在线持续训练方案等挑战。

Method: 提出了TESLA框架，采用CVR-退款率级联架构、分阶段去偏和延迟时间感知的排序损失。框架基于CASCADE数据集（首个大规模开源的淘宝NetCVR数据集），通过级联建模CVR和退款率而非直接建模NetCVR。

Result: TESLA在CASCADE数据集上显著优于现有方法，NetCVR预测的RI-AUC绝对提升12.41%，RI-PRAUC绝对提升14.94%。

Conclusion: NetCVR预测需要在线持续建模，级联建模CVR和退款率优于直接建模，延迟时间是重要特征。提出的TESLA框架有效解决了多阶段级联延迟反馈的挑战。

Abstract: In industrial recommender systems, conversion rate (CVR) is widely used for traffic allocation, but it fails to fully reflect recommendation effectiveness because it ignores refund behavior. To better capture true user satisfaction and business value, net conversion rate (NetCVR), defined as the probability that a clicked item is purchased and not refunded, has been proposed.Unlike CVR, NetCVR prediction involves a more complex multi-stage cascaded delayed feedback process. The two cascaded delays from click to conversion and from conversion to refund have opposite effects, making traditional CVR modeling methods inapplicable. Moreover, the lack of open-source datasets and online continuous training schemes further hinders progress in this area.To address these challenges, we introduce CASCADE (Cascaded Sequences of Conversion and Delayed Refund), the first large-scale open dataset derived from the Taobao app for online continuous NetCVR prediction. Through an in-depth analysis of CASCADE, we identify three key insights: (1) NetCVR exhibits strong temporal dynamics, necessitating online continuous modeling; (2) cascaded modeling of CVR and refund rate outperforms direct NetCVR modeling; and (3) delay time, which correlates with both CVR and refund rate, is an important feature for NetCVR prediction.Based on these insights, we propose TESLA, a continuous NetCVR modeling framework featuring a CVR-refund-rate cascaded architecture, stage-wise debiasing, and a delay-time-aware ranking loss. Extensive experiments demonstrate that TESLA consistently outperforms state-of-the-art methods on CASCADE, achieving absolute improvements of 12.41 percent in RI-AUC and 14.94 percent in RI-PRAUC on NetCVR prediction. The code and dataset are publicly available at https://github.com/alimama-tech/NetCVR.

</details>


### [78] [Perturbation-Induced Linearization: Constructing Unlearnable Data with Solely Linear Classifiers](https://arxiv.org/abs/2601.19967)
*Jinlin Liu,Wei Chen,Xiaojin Zhang*

Main category: cs.LG

TL;DR: 提出PIL方法，使用线性代理模型生成扰动，实现高效且有效的不可学习示例生成，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有不可学习示例方法依赖深度神经网络作为代理模型生成扰动，计算成本高。需要一种更高效的方法来保护数据不被未经授权使用。

Method: 提出Perturbation-Induced Linearization (PIL)方法，仅使用线性代理模型生成扰动。该方法通过诱导深度模型线性化来实现不可学习效果，并分析了基于百分比的部分扰动特性。

Result: PIL方法在性能上与现有基于代理模型的方法相当或更好，同时大幅减少计算时间。揭示了不可学习示例的关键机制是诱导深度模型线性化。

Conclusion: PIL不仅提供了一种实用的数据保护方法，还揭示了不可学习示例的有效机制，为数据保护提供了新的见解。

Abstract: Collecting web data to train deep models has become increasingly common, raising concerns about unauthorized data usage. To mitigate this issue, unlearnable examples introduce imperceptible perturbations into data, preventing models from learning effectively. However, existing methods typically rely on deep neural networks as surrogate models for perturbation generation, resulting in significant computational costs. In this work, we propose Perturbation-Induced Linearization (PIL), a computationally efficient yet effective method that generates perturbations using only linear surrogate models. PIL achieves comparable or better performance than existing surrogate-based methods while reducing computational time dramatically. We further reveal a key mechanism underlying unlearnable examples: inducing linearization to deep models, which explains why PIL can achieve competitive results in a very short time. Beyond this, we provide an analysis about the property of unlearnable examples under percentage-based partial perturbation. Our work not only provides a practical approach for data protection but also offers insights into what makes unlearnable examples effective.

</details>


### [79] [BayPrAnoMeta: Bayesian Proto-MAML for Few-Shot Industrial Image Anomaly Detection](https://arxiv.org/abs/2601.19992)
*Soham Sarkar,Tanmay Sen,Sayantan Banerjee*

Main category: cs.LG

TL;DR: BayPrAnoMeta：一种用于少样本工业图像异常检测的贝叶斯原型MAML方法，通过概率正态模型和贝叶斯后验预测似然提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业图像异常检测面临极端类别不平衡和标记缺陷样本稀缺的挑战，特别是在少样本场景下。现有Proto-MAML方法依赖确定性类原型和基于距离的适应，在极端少样本场景中可能不够鲁棒。

Method: 提出BayPrAnoMeta，将原型替换为任务特定的概率正态模型，使用Normal-Inverse-Wishart先验建模正常支持嵌入，产生Student-t预测分布进行贝叶斯内环适应。扩展为具有监督对比正则化的联邦元学习框架，适用于异构工业客户端。

Result: 在MVTec AD基准测试中，相比MAML、Proto-MAML和基于PatchCore的方法，在少样本异常检测设置中实现了持续且显著的AUROC改进。

Conclusion: BayPrAnoMeta通过概率建模和贝叶斯适应机制，在极端少样本工业异常检测场景中提供了更鲁棒和不确定性感知的解决方案，并能扩展到联邦学习环境。

Abstract: Industrial image anomaly detection is a challenging problem owing to extreme class imbalance and the scarcity of labeled defective samples, particularly in few-shot settings. We propose BayPrAnoMeta, a Bayesian generalization of Proto-MAML for few-shot industrial image anomaly detection. Unlike existing Proto-MAML approaches that rely on deterministic class prototypes and distance-based adaptation, BayPrAnoMeta replaces prototypes with task-specific probabilistic normality models and performs inner-loop adaptation via a Bayesian posterior predictive likelihood. We model normal support embeddings with a Normal-Inverse-Wishart (NIW) prior, producing a Student-$t$ predictive distribution that enables uncertainty-aware, heavy-tailed anomaly scoring and is essential for robustness in extreme few-shot settings. We further extend BayPrAnoMeta to a federated meta-learning framework with supervised contrastive regularization for heterogeneous industrial clients and prove convergence to stationary points of the resulting nonconvex objective. Experiments on the MVTec AD benchmark demonstrate consistent and significant AUROC improvements over MAML, Proto-MAML, and PatchCore-based methods in few-shot anomaly detection settings.

</details>


### [80] [Decomposing multimodal embedding spaces with group-sparse autoencoders](https://arxiv.org/abs/2601.20028)
*Chiraag Kaushik,Davis Barch,Andrea Fanelli*

Main category: cs.LG

TL;DR: 该论文提出一种改进的稀疏自编码器方法，用于多模态嵌入空间分解，通过跨模态随机掩码和组稀疏正则化，解决传统SAE在多模态场景下学习"分裂字典"的问题，提升模态对齐和特征可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器（SAE）应用于多模态嵌入空间（如CLIP、CLAP）时，往往会学习到"分裂字典"，即大多数稀疏特征只对单一模态数据有效，这限制了特征在多模态间的对齐和可解释性。

Method: 提出基于交叉模态随机掩码和组稀疏正则化的SAE改进方法。首先证明在已对齐的嵌入空间中，分裂字典分解意味着存在改进模态对齐的非分裂字典。然后通过跨模态随机掩码和组稀疏正则化来学习更对齐的多模态字典。

Result: 在CLIP（图像/文本）和CLAP（音频/文本）嵌入上的实验表明，相比标准SAE，新方法学习到更多模态对齐的字典，减少了"死亡神经元"数量，提高了特征语义性，改善了概念在多模态间的对齐。

Conclusion: 改进的SAE方法能有效提升多模态嵌入分解的模态对齐性，增强特征的可解释性，并为跨模态任务的可解释性和控制提供改进基础。

Abstract: The Linear Representation Hypothesis asserts that the embeddings learned by neural networks can be understood as linear combinations of features corresponding to high-level concepts. Based on this ansatz, sparse autoencoders (SAEs) have recently become a popular method for decomposing embeddings into a sparse combination of linear directions, which have been shown empirically to often correspond to human-interpretable semantics. However, recent attempts to apply SAEs to multimodal embedding spaces (such as the popular CLIP embeddings for image/text data) have found that SAEs often learn "split dictionaries", where most of the learned sparse features are essentially unimodal, active only for data of a single modality. In this work, we study how to effectively adapt SAEs for the setting of multimodal embeddings while ensuring multimodal alignment. We first argue that the existence of a split dictionary decomposition on an aligned embedding space implies the existence of a non-split dictionary with improved modality alignment. Then, we propose a new SAE-based approach to multimodal embedding decomposition using cross-modal random masking and group-sparse regularization. We apply our method to popular embeddings for image/text (CLIP) and audio/text (CLAP) data and show that, compared to standard SAEs, our approach learns a more multimodal dictionary while reducing the number of dead neurons and improving feature semanticity. We finally demonstrate how this improvement in alignment of concepts between modalities can enable improvements in the interpretability and control of cross-modal tasks.

</details>


### [81] [Structural Compositional Function Networks: Interpretable Functional Compositions for Tabular Discovery](https://arxiv.org/abs/2601.20037)
*Fang Li*

Main category: cs.LG

TL;DR: StructuralCFN：一种通过可微分结构先验建模特征间关系的新型神经网络架构，在保持科学可解释性的同时，在表格数据上超越了梯度提升决策树的性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习架构在处理表格数据时难以同时实现高性能和科学可解释性，通常将特征视为独立实体，忽略了表格分布中固有的流形结构依赖关系。

Method: 提出StructuralCFN架构，通过可微分结构先验引入关系感知归纳偏置，使用可微分自适应门控将每个特征建模为其对应特征的数学组合，自动发现最佳激活机制（如注意力式过滤与抑制极性），并支持结构化知识集成。

Result: 在18个基准数据集上的10折交叉验证显示，在科学和临床数据集（如血液输血、臭氧、WDBC）上取得了统计显著改进（p<0.05），参数规模仅为300-2500个，比标准深度学习基线小10-20倍。

Conclusion: StructuralCFN不仅性能优于梯度提升决策树，还提供了内在符号可解释性，能够以人类可读的数学表达式恢复数据流形的"定律"，同时保持紧凑的参数规模。

Abstract: Despite the ubiquity of tabular data in high-stakes domains, traditional deep learning architectures often struggle to match the performance of gradient-boosted decision trees while maintaining scientific interpretability. Standard neural networks typically treat features as independent entities, failing to exploit the inherent manifold structural dependencies that define tabular distributions. We propose Structural Compositional Function Networks (StructuralCFN), a novel architecture that imposes a Relation-Aware Inductive Bias via a differentiable structural prior. StructuralCFN explicitly models each feature as a mathematical composition of its counterparts through Differentiable Adaptive Gating, which automatically discovers the optimal activation physics (e.g., attention-style filtering vs. inhibitory polarity) for each relationship. Our framework enables Structured Knowledge Integration, allowing domain-specific relational priors to be injected directly into the architecture to guide discovery. We evaluate StructuralCFN across a rigorous 10-fold cross-validation suite on 18 benchmarks, demonstrating statistically significant improvements (p < 0.05) on scientific and clinical datasets (e.g., Blood Transfusion, Ozone, WDBC). Furthermore, StructuralCFN provides Intrinsic Symbolic Interpretability: it recovers the governing "laws" of the data manifold as human-readable mathematical expressions while maintaining a compact parameter footprint (300--2,500 parameters) that is over an order of magnitude (10x--20x) smaller than standard deep baselines.

</details>


### [82] [CiMRAG: Cim-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs](https://arxiv.org/abs/2601.20041)
*Shih-Hsuan Chiu,Ming-Syan Chen*

Main category: cs.LG

TL;DR: 提出TONEL框架，通过噪声感知投影模型学习任务特定嵌入，提升边缘设备上RAG在噪声环境下的鲁棒性和领域适应性


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的个性化虚拟助手使用RAG进行个性化，但面临效率瓶颈：配置文件数据快速增长，计算内存架构虽能缓解但易受环境噪声影响，降低检索精度。动态多领域边缘场景（如旅行、医疗、法律）对准确性和适应性要求高。

Method: 提出任务导向噪声鲁棒嵌入学习框架TONEL，采用噪声感知投影模型学习任务特定嵌入，兼容计算内存硬件约束，实现在噪声条件下的准确检索。

Result: 在个性化基准测试上进行广泛实验，证明方法相对于强基线在任务特定噪声场景下的有效性和实用性。

Conclusion: TONEL框架有效解决了边缘设备上RAG面临的噪声鲁棒性和领域适应性挑战，为边缘环境中的个性化虚拟助手提供了实用解决方案。

Abstract: Personalized virtual assistants powered by large language models (LLMs) on edge devices are attracting growing attention, with Retrieval-Augmented Generation (RAG) emerging as a key method for personalization by retrieving relevant profile data and generating tailored responses. However, deploying RAG on edge devices faces efficiency hurdles due to the rapid growth of profile data, such as user-LLM interactions and recent updates. While Computing-in-Memory (CiM) architectures mitigate this bottleneck by eliminating data movement between memory and processing units via in-situ operations, they are susceptible to environmental noise that can degrade retrieval precision. This poses a critical issue in dynamic, multi-domain edge-based scenarios (e.g., travel, medicine, and law) where both accuracy and adaptability are paramount. To address these challenges, we propose Task-Oriented Noise-resilient Embedding Learning (TONEL), a framework that improves noise robustness and domain adaptability for RAG in noisy edge environments. TONEL employs a noise-aware projection model to learn task-specific embeddings compatible with CiM hardware constraints, enabling accurate retrieval under noisy conditions. Extensive experiments conducted on personalization benchmarks demonstrate the effectiveness and practicality of our methods relative to strong baselines, especially in task-specific noisy scenarios.

</details>


### [83] [Regime-Adaptive Bayesian Optimization via Dirichlet Process Mixtures of Gaussian Processes](https://arxiv.org/abs/2601.20043)
*Yan Zhang,Xuefeng Liu,Sipeng Chen,Sascha Ranftl,Chong Liu,Shibo Li*

Main category: cs.LG

TL;DR: RAMBO提出了一种基于狄利克雷过程混合高斯过程的贝叶斯优化方法，能够自动发现优化过程中的潜在区域（regimes），每个区域用独立的GP建模，适用于多区域优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化假设搜索空间具有均匀平滑性，但在多区域问题（如分子构象搜索、药物发现）中这一假设不成立。单一高斯过程要么过度平滑尖锐转变，要么在平滑区域产生噪声幻觉，导致不确定性校准错误。

Method: RAMBO使用狄利克雷过程混合高斯过程自动发现潜在区域，每个区域用独立的高斯过程建模并优化局部超参数。采用折叠吉布斯采样对潜在函数进行解析边缘化实现高效推理，引入自适应浓度参数调度实现从粗到细的区域发现。提出将不确定性分解为区域内和区域间分量的获取函数。

Result: 在合成基准测试和实际应用中（包括分子构象优化、药物发现虚拟筛选、聚变反应堆设计）的实验表明，RAMBO在多区域目标上相比最先进的基线方法取得了持续改进。

Conclusion: RAMBO通过自动发现和建模多区域结构，有效解决了传统贝叶斯优化在多区域问题中的局限性，在实际应用中表现出优越性能。

Abstract: Standard Bayesian Optimization (BO) assumes uniform smoothness across the search space an assumption violated in multi-regime problems such as molecular conformation search through distinct energy basins or drug discovery across heterogeneous molecular scaffolds. A single GP either oversmooths sharp transitions or hallucinates noise in smooth regions, yielding miscalibrated uncertainty. We propose RAMBO, a Dirichlet Process Mixture of Gaussian Processes that automatically discovers latent regimes during optimization, each modeled by an independent GP with locally-optimized hyperparameters. We derive collapsed Gibbs sampling that analytically marginalizes latent functions for efficient inference, and introduce adaptive concentration parameter scheduling for coarse-to-fine regime discovery. Our acquisition functions decompose uncertainty into intra-regime and inter-regime components. Experiments on synthetic benchmarks and real-world applications, including molecular conformer optimization, virtual screening for drug discovery, and fusion reactor design, demonstrate consistent improvements over state-of-the-art baselines on multi-regime objectives.

</details>


### [84] [Externally Validated Longitudinal GRU Model for Visit-Level 180-Day Mortality Risk in Metastatic Castration-Resistant Prostate Cancer](https://arxiv.org/abs/2601.20046)
*Javier Mencia-Ledo,Mohammad Noaeen,Zahra Shakeri*

Main category: cs.LG

TL;DR: 开发并验证了用于转移性去势抵抗性前列腺癌患者180天死亡风险的纵向预测模型，GRU和RSF模型表现出色，临床指标如BMI和收缩压对风险预测最为重要。


<details>
  <summary>Details</summary>
Motivation: 转移性去势抵抗性前列腺癌预后差且治疗反应异质性高，需要开发能够准确预测短期死亡风险的模型，以支持临床主动护理规划。

Method: 使用两个III期临床试验队列的纵向数据，开发并外部验证了访视级别的180天死亡风险模型。比较了LSTM、GRU、Cox比例风险、随机生存森林和逻辑回归五种架构，仅使用有可观察180天结局的访视数据。

Result: GRU和RSF模型初始区分度最高（C-index均为87%）。外部验证中，GRU校准度更好（斜率0.93，截距0.07），PR-AUC达0.87。临床影响分析显示真阳性中位预警时间为151.0天，假阳性为59.0天。BMI和收缩压被确定为最重要的风险关联因素。

Conclusion: 纵向常规临床标志物能够有效估计mCRPC患者的短期死亡风险，支持在数月时间窗口内进行主动护理规划，为临床决策提供了实用工具。

Abstract: Metastatic castration-resistant prostate cancer (mCRPC) is a highly aggressive disease with poor prognosis and heterogeneous treatment response. In this work, we developed and externally validated a visit-level 180-day mortality risk model using longitudinal data from two Phase III cohorts (n=526 and n=640). Only visits with observable 180-day outcomes were labeled; right-censored cases were excluded from analysis. We compared five candidate architectures: Long Short-Term Memory, Gated Recurrent Unit (GRU), Cox Proportional Hazards, Random Survival Forest (RSF), and Logistic Regression. For each dataset, we selected the smallest risk-threshold that achieved an 85% sensitivity floor. The GRU and RSF models showed high discrimination capabilities initially (C-index: 87% for both). In external validation, the GRU obtained a higher calibration (slope: 0.93; intercept: 0.07) and achieved an PR-AUC of 0.87. Clinical impact analysis showed a median time-in-warning of 151.0 days for true positives (59.0 days for false positives) and 18.3 alerts per 100 patient-visits. Given late-stage frailty or cachexia and hemodynamic instability, permutation importance ranked BMI and systolic blood pressure as the strongest associations. These results suggest that longitudinal routine clinical markers can estimate short-horizon mortality risk in mCRPC and support proactive care planning over a multi-month window.

</details>


### [85] [Domain Expansion: A Latent Space Construction Framework for Multi-Task Learning](https://arxiv.org/abs/2601.20069)
*Chi-Yao Huang,Khoa Vo,Aayush Atul Verma,Duo Lu,Yezhou Yang*

Main category: cs.LG

TL;DR: 提出Domain Expansion框架，通过正交池化机制构建相互正交的子空间，解决多目标训练中的潜在表示坍缩问题


<details>
  <summary>Details</summary>
Motivation: 解决多目标训练中梯度冲突导致共享表示坍缩的问题，使网络在多个任务上都能保持最优性能

Method: 使用正交池化机制构建潜在空间，为每个目标分配相互正交的子空间，避免表示冲突

Result: 在ShapeNet、MPIIGaze和Rotated MNIST等基准测试中有效防止表示坍缩，获得可解释、可组合的潜在空间

Conclusion: Domain Expansion框架通过结构化潜在空间解决了多目标训练中的表示冲突问题，实现了更好的性能和可解释性

Abstract: Training a single network with multiple objectives often leads to conflicting gradients that degrade shared representations, forcing them into a compromised state that is suboptimal for any single task--a problem we term latent representation collapse. We introduce Domain Expansion, a framework that prevents these conflicts by restructuring the latent space itself. Our framework uses a novel orthogonal pooling mechanism to construct a latent space where each objective is assigned to a mutually orthogonal subspace. We validate our approach across diverse benchmarks--including ShapeNet, MPIIGaze, and Rotated MNIST--on challenging multi-objective problems combining classification with pose and gaze estimation. Our experiments demonstrate that this structure not only prevents collapse but also yields an explicit, interpretable, and compositional latent space where concepts can be directly manipulated.

</details>


### [86] [Distributional value gradients for stochastic environments](https://arxiv.org/abs/2601.20071)
*Baptiste Debes,Tinne Tuytelaars*

Main category: cs.LG

TL;DR: 该论文提出了Distributional Sobolev Training方法，将分布强化学习扩展到连续状态-动作空间，不仅建模状态-动作值函数的分布，还建模其梯度的分布，以解决现有梯度正则化方法在随机/噪声环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有梯度正则化值学习方法（如MAGE）在随机或噪声环境中表现不佳，限制了其应用范围。需要一种能更好处理随机环境的方法来提高样本效率。

Method: 扩展连续状态-动作空间的分布强化学习，同时建模标量状态-动作值函数及其梯度的分布。使用条件变分自编码器(cVAE)实现奖励和转移分布的一步世界模型，采用最大切片最大均值差异(MSMMD)实例化分布Bellman算子。

Result: 证明了Sobolev增强的Bellman算子是压缩算子且具有唯一不动点，揭示了梯度感知强化学习中平滑性的基本权衡。在简单随机强化学习玩具问题和多个MuJoCo环境中验证了方法的有效性。

Conclusion: Distributional Sobolev Training方法通过同时建模值函数及其梯度的分布，成功解决了现有梯度正则化方法在随机环境中的局限性，提高了样本效率。

Abstract: Gradient-regularized value learning methods improve sample efficiency by leveraging learned models of transition dynamics and rewards to estimate return gradients. However, existing approaches, such as MAGE, struggle in stochastic or noisy environments, limiting their applicability. In this work, we address these limitations by extending distributional reinforcement learning on continuous state-action spaces to model not only the distribution over scalar state-action value functions but also over their gradients. We refer to this approach as Distributional Sobolev Training. Inspired by Stochastic Value Gradients (SVG), our method utilizes a one-step world model of reward and transition distributions implemented via a conditional Variational Autoencoder (cVAE). The proposed framework is sample-based and employs Max-sliced Maximum Mean Discrepancy (MSMMD) to instantiate the distributional Bellman operator. We prove that the Sobolev-augmented Bellman operator is a contraction with a unique fixed point, and highlight a fundamental smoothness trade-off underlying contraction in gradient-aware RL. To validate our method, we first showcase its effectiveness on a simple stochastic reinforcement learning toy problem, then benchmark its performance on several MuJoCo environments.

</details>


### [87] [Techno-economic optimization of a heat-pipe microreactor, part II: multi-objective optimization analysis](https://arxiv.org/abs/2601.20079)
*Paul Seurin,Dean Price*

Main category: cs.LG

TL;DR: 本文扩展了热管微堆的设计优化框架，采用PEARL多目标优化算法，同时最小化棒积分峰值因子和LCOE，评估三种成本情景，识别出降低峰值因子的设计策略和优化LCOE的四项关键策略。


<details>
  <summary>Details</summary>
Motivation: 热管微堆是适用于偏远地区的紧凑型核能系统。先前研究已建立基于代理模型和强化学习的单目标优化框架，仅最小化LCOE。本研究旨在扩展为多目标优化，同时考虑安全性和经济性，以找到设计参数的最佳权衡。

Method: 采用PEARL多目标优化算法，在最小化棒积分峰值因子和LCOE的双目标下进行优化，考虑安全与运行约束。评估三种成本情景：高成本轴向和鼓反射体、低成本轴向反射体、低成本轴向和鼓反射体。通过代理模型和强化学习进行优化。

Result: 发现减小固体慢化剂半径、针间距和鼓涂层角度，同时增加燃料高度可有效降低峰值因子。在所有三种情景中，优化LCOE的四项关键策略一致：最小化高成本轴向反射体贡献、减少控制鼓依赖、用石墨价格的轴向反射体材料替代昂贵的TRISO燃料、最大化燃料燃耗。

Conclusion: PEARL算法在不同设计情景中展现处理权衡的能力，但代理模型预测与全阶模拟间仍存在差异。通过约束松弛和代理模型改进可进一步提升性能，这是持续研究的重点。

Abstract: Heat-pipe microreactors (HPMRs) are compact and transportable nuclear power systems exhibiting inherent safety, well-suited for deployment in remote regions where access is limited and reliance on costly fossil fuels is prevalent. In prior work, we developed a design optimization framework that incorporates techno-economic considerations through surrogate modeling and reinforcement learning (RL)-based optimization, focusing solely on minimizing the levelized cost of electricity (LCOE) by using a bottom-up cost estimation approach. In this study, we extend that framework to a multi-objective optimization that uses the Pareto Envelope Augmented with Reinforcement Learning (PEARL) algorithm. The objectives include minimizing both the rod-integrated peaking factor ($F_{Δh}$) and LCOE -- subject to safety and operational constraints. We evaluate three cost scenarios: (1) a high-cost axial and drum reflectors, (2) a low-cost axial reflector, and (3) low-cost axial and drum reflectors. Our findings indicate that reducing the solid moderator radius, pin pitch, and drum coating angle -- all while increasing the fuel height -- effectively lowers $F_{Δh}$. Across all three scenarios, four key strategies consistently emerged for optimizing LCOE: (1) minimizing the axial reflector contribution when costly, (2) reducing control drum reliance, (3) substituting expensive tri-structural isotropic (TRISO) fuel with axial reflector material priced at the level of graphite, and (4) maximizing fuel burnup. While PEARL demonstrates promise in navigating trade-offs across diverse design scenarios, discrepancies between surrogate model predictions and full-order simulations remain. Further improvements are anticipated through constraint relaxation and surrogate development, constituting an ongoing area of investigation.

</details>


### [88] [Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery](https://arxiv.org/abs/2601.20088)
*Meng Xin,Sweta Priyadarshi,Jingyu Xin,Bilal Kartal,Aditya Vavre,Asma Kuriparambil Thekkumpate,Zijia Chen,Ameya Sunil Mahabaleshwarkar,Ido Shahaf,Akhiad Bercovich,Kinjal Patel,Suguna Varshini Velury,Chenjie Luo,Zhiyu Cheng,Jenny Chen,Chen-Han Yu,Wei Ping,Oleg Rybakov,Nima Tajbakhsh,Oluwatobi Olabiyi,Dusan Stosic,Di Wu,Song Han,Eric Chung,Sharath Turuvekere Sreenivas,Bryan Catanzaro,Yoshi Suhara,Tijmen Blankevoort,Huizi Mao*

Main category: cs.LG

TL;DR: 本文提出了量化感知蒸馏（QAD）方法，用于恢复NVFP4量化大型语言模型和视觉语言模型的精度，相比传统量化感知训练具有更好的稳定性和数据鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统量化感知训练（QAT）在多阶段后训练流程（包括监督微调、强化学习和模型合并）中面临工程复杂性和训练不稳定性的挑战，且对数据质量和覆盖范围敏感，需要完整训练数据。

Method: 提出量化感知蒸馏（QAD）方法，使用KL散度损失将全精度教师模型的知识蒸馏到量化学生模型中，适用于经过多阶段后训练的模型。

Result: 在多个后训练模型（包括AceReason Nemotron、Nemotron 3 Nano、Nemotron Nano V2、Nemotron Nano V2 VL和Llama Nemotron Super v1）上评估QAD，结果显示该方法能够稳定地将量化模型精度恢复到接近BF16精度的水平。

Conclusion: QAD为量化大型语言模型和视觉语言模型提供了一种有效且稳定的精度恢复方案，特别适用于复杂的多阶段后训练模型，且对数据质量要求较低，具有实际应用价值。

Abstract: This technical report presents quantization-aware distillation (QAD) and our best practices for recovering accuracy of NVFP4-quantized large language models (LLMs) and vision-language models (VLMs). QAD distills a full-precision teacher model into a quantized student model using a KL divergence loss. While applying distillation to quantized models is not a new idea, we observe key advantages of QAD for today's LLMs: 1. It shows remarkable effectiveness and stability for models trained through multi-stage post-training pipelines, including supervised fine-tuning (SFT), reinforcement learning (RL), and model merging, where traditional quantization-aware training (QAT) suffers from engineering complexity and training instability; 2. It is robust to data quality and coverage, enabling accuracy recovery without full training data. We evaluate QAD across multiple post-trained models including AceReason Nemotron, Nemotron 3 Nano, Nemotron Nano V2, Nemotron Nano V2 VL (VLM), and Llama Nemotron Super v1, showing consistent recovery to near-BF16 accuracy.

</details>


### [89] [In-Context Reinforcement Learning From Suboptimal Historical Data](https://arxiv.org/abs/2601.20116)
*Juncheng Dong,Moyang Guo,Ethan X. Fang,Zhuoran Yang,Vahid Tarokh*

Main category: cs.LG

TL;DR: 提出Decision Importance Transformer (DIT)框架，用于上下文强化学习，通过训练基于transformer的价值函数评估次优轨迹的优势函数，再训练基于transformer的策略，在离线数据集包含次优历史数据时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在上下文学习方面表现出色，但在上下文强化学习(ICRL)中，当离线数据集包含从次优行为策略采样的轨迹时，标准自回归训练相当于模仿学习，会导致次优性能。需要解决如何利用次优离线数据提升RL任务性能的问题。

Method: 提出DIT框架：1) 训练基于transformer的价值函数，评估收集次优轨迹的行为策略的优势函数；2) 训练基于transformer的策略，使用加权最大似然估计损失，权重基于训练好的价值函数构建，引导次优策略向最优策略转变。

Result: 在赌博机和马尔可夫决策过程问题上进行了广泛实验，结果显示DIT实现了优越性能，特别是在离线数据集包含次优历史数据的情况下表现突出。

Conclusion: DIT框架能够有效利用次优离线数据进行上下文强化学习，通过模拟actor-critic算法的上下文方式，显著提升了在新RL任务上的性能表现。

Abstract: Transformer models have achieved remarkable empirical successes, largely due to their in-context learning capabilities. Inspired by this, we explore training an autoregressive transformer for in-context reinforcement learning (ICRL). In this setting, we initially train a transformer on an offline dataset consisting of trajectories collected from various RL tasks, and then fix and use this transformer to create an action policy for new RL tasks. Notably, we consider the setting where the offline dataset contains trajectories sampled from suboptimal behavioral policies. In this case, standard autoregressive training corresponds to imitation learning and results in suboptimal performance. To address this, we propose the Decision Importance Transformer(DIT) framework, which emulates the actor-critic algorithm in an in-context manner. In particular, we first train a transformer-based value function that estimates the advantage functions of the behavior policies that collected the suboptimal trajectories. Then we train a transformer-based policy via a weighted maximum likelihood estimation loss, where the weights are constructed based on the trained value function to steer the suboptimal policies to the optimal ones. We conduct extensive experiments to test the performance of DIT on both bandit and Markov Decision Process problems. Our results show that DIT achieves superior performance, particularly when the offline dataset contains suboptimal historical data.

</details>


### [90] [A Reinforcement Learning Based Universal Sequence Design for Polar Codes](https://arxiv.org/abs/2601.20118)
*David Kin Wai Ho,Arman Fazeli,Mohamad M. Mansour,Louay M. A. Jalloul*

Main category: cs.LG

TL;DR: 基于强化学习的通用Polar码序列设计框架，可扩展到2048码长，在5G支持的(N,K)配置下性能优于5G NR序列和beta-expansion基线。


<details>
  <summary>Details</summary>
Motivation: 为6G应用推进Polar码设计，开发可扩展、适应不同信道条件和解码策略的通用序列设计框架。

Method: 基于强化学习的序列设计框架，包含三个关键要素：(i)基于Polar码通用偏序特性的物理定律约束学习，(ii)利用决策的弱长期影响限制前瞻评估，(iii)联合多配置优化提高学习效率。

Result: 框架可扩展到2048码长，在5G支持的所有(N,K)配置下，性能与5G NR序列相当，在N=2048时比beta-expansion基线获得高达0.2 dB增益。

Conclusion: 该强化学习框架为6G Polar码设计提供了可扩展的解决方案，通过物理定律约束学习和高效优化策略实现了大规模学习，适用于标准化应用。

Abstract: To advance Polar code design for 6G applications, we develop a reinforcement learning-based universal sequence design framework that is extensible and adaptable to diverse channel conditions and decoding strategies. Crucially, our method scales to code lengths up to $2048$, making it suitable for use in standardization. Across all $(N,K)$ configurations supported in 5G, our approach achieves competitive performance relative to the NR sequence adopted in 5G and yields up to a 0.2 dB gain over the beta-expansion baseline at $N=2048$. We further highlight the key elements that enabled learning at scale: (i) incorporation of physical law constrained learning grounded in the universal partial order property of Polar codes, (ii) exploitation of the weak long term influence of decisions to limit lookahead evaluation, and (iii) joint multi-configuration optimization to increase learning efficiency.

</details>


### [91] [Going NUTS with ADVI: Exploring various Bayesian Inference techniques with Facebook Prophet](https://arxiv.org/abs/2601.20120)
*Jovan Krajevski,Biljana Tojtovska Ribarski*

Main category: cs.LG

TL;DR: 作者基于PyMC重新实现了Facebook Prophet模型，以克服原版在推理方法和API灵活性上的限制，支持多种贝叶斯推理技术（MCMC、MAP、变分推理）的评估和比较。


<details>
  <summary>Details</summary>
Motivation: 使用Facebook Prophet进行贝叶斯推理时，发现原版仅提供两种内置推理方法（MAP和MCMC），无法应用其他推理技术，且其API设计不够灵活，难以实现自定义建模想法。

Method: 完全基于PyMC重新实现Prophet模型，扩展基础模型，支持评估和比较多种贝叶斯推理方法，包括全MCMC技术、MAP估计和变分推理技术。

Result: 详细分析了不同贝叶斯推理技术在时间序列预测问题上的实现，讨论了采样方法、收敛诊断、预测指标以及计算效率，并识别了未来需要解决的问题。

Conclusion: PyMC-based实现解决了原版Prophet在推理方法选择和API灵活性上的限制，为时间序列预测提供了更强大的贝叶斯建模框架，未来将进一步优化和改进。

Abstract: Since its introduction, Facebook Prophet has attracted positive attention from both classical statisticians and the Bayesian statistics community. The model provides two built-in inference methods: maximum a posteriori estimation using the L-BFGS-B algorithm, and Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS). While exploring various time-series forecasting problems using Bayesian inference with Prophet, we encountered limitations stemming from the inability to apply alternative inference techniques beyond those provided by default. Additionally, the fluent API design of Facebook Prophet proved insufficiently flexible for implementing our custom modeling ideas. To address these shortcomings, we developed a complete reimplementation of the Prophet model in PyMC, which enables us to extend the base model and evaluate and compare multiple Bayesian inference methods. In this paper, we present our PyMC-based implementation and analyze in detail the implementation of different Bayesian inference techniques. We consider full MCMC techniques, MAP estimation and Variational inference techniques on a time-series forecasting problem. We discuss in details the sampling approach, convergence diagnostics, forecasting metrics as well as their computational efficiency and detect possible issues which will be addressed in our future work.

</details>


### [92] [Membership Inference Attacks Against Fine-tuned Diffusion Language Models](https://arxiv.org/abs/2601.20125)
*Yuetian Chen,Kaiyuan Zhang,Yuntao Du,Edoardo Stoppa,Charles Fleming,Ashish Kundu,Bruno Ribeiro,Ninghui Li*

Main category: cs.LG

TL;DR: 本文首次系统研究扩散语言模型（DLMs）在成员推理攻击（MIA）中的隐私泄露脆弱性，并提出SAMA攻击方法，相比基线AUC提升30%，低误报率下提升高达8倍。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归语言模型的有前景替代方案，使用双向掩码token预测，但其在成员推理攻击下的隐私泄露脆弱性尚未得到充分探索。与自回归模型的单一固定预测模式不同，DLMs的多种掩码配置指数级增加了攻击机会。

Method: 提出SAMA（子集聚合成员攻击）方法：通过采样渐进密度的掩码子集，应用基于符号的统计量来处理重尾噪声；采用逆加权聚合优先稀疏掩码的干净信号，将稀疏记忆检测转化为鲁棒的投票机制。

Result: 在九个数据集上的实验显示，SAMA相比最佳基线实现了30%的相对AUC改进，在低误报率下改进高达8倍，揭示了DLMs先前未知的显著脆弱性。

Conclusion: 扩散语言模型存在显著的成员推理攻击脆弱性，需要开发针对性的隐私防御措施。SAMA攻击方法通过利用DLMs的多掩码配置优势，有效提高了攻击成功率。

Abstract: Diffusion Language Models (DLMs) represent a promising alternative to autoregressive language models, using bidirectional masked token prediction. Yet their susceptibility to privacy leakage via Membership Inference Attacks (MIA) remains critically underexplored. This paper presents the first systematic investigation of MIA vulnerabilities in DLMs. Unlike the autoregressive models' single fixed prediction pattern, DLMs' multiple maskable configurations exponentially increase attack opportunities. This ability to probe many independent masks dramatically improves detection chances. To exploit this, we introduce SAMA (Subset-Aggregated Membership Attack), which addresses the sparse signal challenge through robust aggregation. SAMA samples masked subsets across progressive densities and applies sign-based statistics that remain effective despite heavy-tailed noise. Through inverse-weighted aggregation prioritizing sparse masks' cleaner signals, SAMA transforms sparse memorization detection into a robust voting mechanism. Experiments on nine datasets show SAMA achieves 30% relative AUC improvement over the best baseline, with up to 8 times improvement at low false positive rates. These findings reveal significant, previously unknown vulnerabilities in DLMs, necessitating the development of tailored privacy defenses.

</details>


### [93] [Scaling Next-Brain-Token Prediction for MEG](https://arxiv.org/abs/2601.20138)
*Richard Csaky*

Main category: cs.LG

TL;DR: 研究者开发了一个用于脑磁图(MEG)的大型自回归模型，能够跨数据集和扫描仪进行长上下文的下一个标记预测，处理超过500小时的数据，并训练Qwen2.5-VL骨干网络递归生成分钟级的MEG信号。


<details>
  <summary>Details</summary>
Motivation: 现有的脑信号生成模型在处理长上下文、跨数据集泛化方面存在局限，需要能够稳定生成分钟级脑磁图信号并保持条件特异性的模型。

Method: 使用改进的SEANet风格向量量化器将多通道MEG压缩为扁平标记流，在Qwen2.5-VL骨干网络上从头训练，预测下一个脑标记并递归生成MEG信号。引入三种任务匹配测试评估长时程生成质量。

Result: 模型在跨数据集测试中表现出良好的泛化能力，生成信号在长时程中保持相对稳定，且与正确延续相比，比交换提示的控制组更接近真实分布。

Conclusion: 该研究成功开发了一个能够跨数据集和扫描仪生成稳定、条件特异性脑磁图信号的大型自回归模型，为脑信号建模和生成提供了新的工具和方法。

Abstract: We present a large autoregressive model for source-space MEG that scales next-token prediction to long context across datasets and scanners: handling a corpus of over 500 hours and thousands of sessions across the three largest MEG datasets. A modified SEANet-style vector-quantizer reduces multichannel MEG into a flattened token stream on which we train a Qwen2.5-VL backbone from scratch to predict the next brain token and to recursively generate minutes of MEG from up to a minute of context. To evaluate long-horizon generation, we introduce three task-matched tests: (i) on-manifold stability via generated-only drift compared to the time-resolved distribution of real sliding windows, and (ii) conditional specificity via correct context versus prompt-swap controls using a neurophysiologically grounded metric set. We train on CamCAN and Omega and run all analyses on held-out MOUS, establishing cross-dataset generalization. Across metrics, generations remain relatively stable over long rollouts and are closer to the correct continuation than swapped controls. Code available at: https://github.com/ricsinaruto/brain-gen.

</details>


### [94] [Spectral Ghost in Representation Learning: from Component Analysis to Self-Supervised Learning](https://arxiv.org/abs/2601.20154)
*Bo Dai,Na Li,Dale Schuurmans*

Main category: cs.LG

TL;DR: 该论文提出一个统一的理论框架来理解自监督学习的表示学习，从谱表示视角揭示现有成功SSL算法的本质，并为开发更高效、易于使用的表示学习算法提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在表示学习方面取得了显著进展，但缺乏清晰统一的理论理解。现有方法差异巨大，学习目标多样，这种理论缺失阻碍了表示学习的进一步发展，使得算法设计原则不明确，实践应用缺乏理论依据。

Method: 从谱表示视角理论分析表示的充分性，揭示现有成功SSL算法的谱本质，建立统一的理论框架来理解和分析表示学习。

Result: 提出了一个原则性的表示学习理论基础，该框架能够统一理解现有SSL算法，并为开发更高效、易于使用的表示学习算法提供理论指导。

Conclusion: 该论文建立的统一理论框架填补了自监督表示学习的理论空白，为算法设计提供了原则性指导，使表示学习方法在实践中的应用更加合理和有效。

Abstract: Self-supervised learning (SSL) have improved empirical performance by unleashing the power of unlabeled data for practical applications. Specifically, SSL extracts the representation from massive unlabeled data, which will be transferred to a plenty of down streaming tasks with limited data. The significant improvement on diverse applications of representation learning has attracted increasing attention, resulting in a variety of dramatically different self-supervised learning objectives for representation extraction, with an assortment of learning procedures, but the lack of a clear and unified understanding. Such an absence hampers the ongoing development of representation learning, leaving a theoretical understanding missing, principles for efficient algorithm design unclear, and the use of representation learning methods in practice unjustified. The urgency for a unified framework is further motivated by the rapid growth in representation learning methods. In this paper, we are therefore compelled to develop a principled foundation of representation learning. We first theoretically investigate the sufficiency of the representation from a spectral representation view, which reveals the spectral essence of the existing successful SSL algorithms and paves the path to a unified framework for understanding and analysis. Such a framework work also inspires the development of more efficient and easy-to-use representation learning algorithms with principled way in real-world applications.

</details>


### [95] [PASS: Ambiguity Guided Subsets for Scalable Classical and Quantum Constrained Clustering](https://arxiv.org/abs/2601.20157)
*Pedro Chumpitaz-Flores,My Duong,Ying Mao,Kaixun Hua*

Main category: cs.LG

TL;DR: PASS是一个用于成对约束聚类的子集选择框架，通过将must-link约束压缩为伪点，并采用两种选择规则来保持约束满足的同时实现可扩展的高质量聚类。


<details>
  <summary>Details</summary>
Motivation: 现有的成对约束聚类方法在处理大规模数据时存在可扩展性问题，特别是在量子或量子混合聚类等专业应用中。ML和CL约束增加了聚类问题的复杂性，现有方法难以应对数据规模的增长。

Method: PASS框架包含两个核心组件：1）将must-link约束压缩为伪点；2）提供两种选择器：基于约束感知边界的规则（收集边界点和CL违反点）和基于信息几何的规则（使用软分配后验的Fisher-Rao距离评分，在预算下选择最高信息子集）。

Result: 在多样化的基准测试中，PASS以显著低于精确或基于惩罚方法的成本获得了有竞争力的SSE（平方误差和），并且在现有方法失败的场景中仍然保持有效。

Conclusion: PASS框架成功解决了成对约束聚类的可扩展性问题，通过智能子集选择在保持约束满足的同时实现了高质量聚类，为大规模约束聚类应用提供了实用解决方案。

Abstract: Pairwise-constrained clustering augments unsupervised partitioning with side information by enforcing must-link (ML) and cannot-link (CL) constraints between specific samples, yielding labelings that respect known affinities and separations. However, ML and CL constraints add an extra layer of complexity to the clustering problem, with current methods struggling in data scalability, especially in niche applications like quantum or quantum-hybrid clustering. We propose PASS, a pairwise-constraints and ambiguity-driven subset selection framework that preserves ML and CL constraints satisfaction while allowing scalable, high-quality clustering solution. PASS collapses ML constraints into pseudo-points and offers two selectors: a constraint-aware margin rule that collects near-boundary points and all detected CL violations, and an information-geometric rule that scores points via a Fisher-Rao distance derived from soft assignment posteriors, then selects the highest-information subset under a simple budget. Across diverse benchmarks, PASS attains competitive SSE at substantially lower cost than exact or penalty-based methods, and remains effective in regimes where prior approaches fail.

</details>


### [96] [What's the plan? Metrics for implicit planning in LLMs and their application to rhyme generation and question answering](https://arxiv.org/abs/2601.20164)
*Jim Maar,Denis Paperno,Callum Stuart McDougall,Neel Nanda*

Main category: cs.LG

TL;DR: 该研究提出了更简单的技术来评估语言模型中的隐式规划能力，通过押韵诗歌生成和问答案例研究发现，隐式规划是通用机制，存在于参数小至1B的模型中。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明语言模型在训练过程中表现出隐式规划行为，但现有评估方法复杂。本研究旨在开发更简单、可扩展的方法来评估各种语言模型的隐式规划能力。

Method: 提出了简单的技术来评估隐式规划，包括通过向量引导来操纵生成结果。使用押韵诗歌生成和问答作为案例研究，在多个模型上验证方法的有效性。

Result: 研究发现隐式规划是通用机制，存在于比先前认为更小的模型中（从1B参数开始）。通过向量引导可以影响中间token的生成，从而操纵最终的押韵词或答案。

Conclusion: 该方法为研究LLM的隐式规划能力提供了广泛适用的直接途径。理解语言模型的规划能力可以为AI安全和控制决策提供信息。

Abstract: Prior work suggests that language models, while trained on next token prediction, show implicit planning behavior: they may select the next token in preparation to a predicted future token, such as a likely rhyming word, as supported by a prior qualitative study of Claude 3.5 Haiku using a cross-layer transcoder. We propose much simpler techniques for assessing implicit planning in language models. With case studies on rhyme poetry generation and question answering, we demonstrate that our methodology easily scales to many models. Across models, we find that the generated rhyme (e.g. "-ight") or answer to a question ("whale") can be manipulated by steering at the end of the preceding line with a vector, affecting the generation of intermediate tokens leading up to the rhyme or answer word. We show that implicit planning is a universal mechanism, present in smaller models than previously thought, starting from 1B parameters. Our methodology offers a widely applicable direct way to study implicit planning abilities of LLMs. More broadly, understanding planning abilities of language models can inform decisions in AI safety and control.

</details>


### [97] [Local Duality for Sparse Support Vector Machines](https://arxiv.org/abs/2601.20170)
*Penghe Zhang,Naihua Xiu,Houduo Qi*

Main category: cs.LG

TL;DR: 该论文建立了稀疏支持向量机（SSVM）的局部对偶理论，证明了SSVM是0/1损失SVM的对偶问题，其局部解为hSVM和rSVM的超参数选择提供指导，并能解释为何SSVM在实证中优于hSVM和rSVM。


<details>
  <summary>Details</summary>
Motivation: 尽管稀疏支持向量机（SSVM）在优化中因基数最小化而受到关注，并在实证中显示出优于凸SVM的优势，但现有方法（如在凸SVM对偶问题中添加ℓ₀范数）缺乏理论依据。本文旨在填补这一理论空白。

Method: 为SSVM建立局部对偶理论，探索其与hinge-loss SVM（hSVM）和ramp-loss SVM（rSVM）的关系。证明SSVM是0/1损失SVM的对偶问题，并研究其局部解的性质。

Result: 证明了SSVM是0/1损失SVM的对偶问题，其线性表示定理对局部解成立。SSVM的局部解为hSVM和rSVM的超参数选择提供指导。在特定条件下，hSVM全局解序列收敛到0/1损失SVM的局部解，且0/1损失SVM的局部极小值也是rSVM的局部极小值。

Conclusion: 该研究为SSVM提供了理论依据，解释了其在实证中优于hSVM和rSVM的原因。通过在真实数据集上的数值实验，展示了SSVM在局部优化解方面的潜在优势。

Abstract: Due to the rise of cardinality minimization in optimization, sparse support vector machines (SSVMs) have attracted much attention lately and show certain empirical advantages over convex SVMs. A common way to derive an SSVM is to add a cardinality function such as $\ell_0$-norm to the dual problem of a convex SVM. However, this process lacks theoretical justification. This paper fills the gap by developing a local duality theory for such an SSVM formulation and exploring its relationship with the hinge-loss SVM (hSVM) and the ramp-loss SVM (rSVM). In particular, we prove that the derived SSVM is exactly the dual problem of the 0/1-loss SVM, and the linear representer theorem holds for their local solutions. The local solution of SSVM also provides guidelines on selecting hyperparameters of hSVM and rSVM. {Under specific conditions, we show that a sequence of global solutions of hSVM converges to a local solution of 0/1-loss SVM. Moreover, a local minimizer of 0/1-loss SVM is a local minimizer of rSVM.} This explains why a local solution induced by SSVM outperforms hSVM and rSVM in the prior empirical study. We further conduct numerical tests on real datasets and demonstrate potential advantages of SSVM by working with locally nice solutions proposed in this paper.

</details>


### [98] [Loss Landscape Geometry and the Learning of Symmetries: Or, What Influence Functions Reveal About Robust Generalization](https://arxiv.org/abs/2601.20172)
*James Amarel,Robyn Miller,Nicolas Hengartner,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: cs.LG

TL;DR: 提出一种基于影响诊断的方法，通过测量对称轨道上损失梯度的传播来评估神经网络模拟器是否内化了物理对称性，超越了前向传播等变性测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过前向传播等变性测试来评估神经模拟器的对称性，但这种方法无法揭示学习动态是否真正耦合了物理上等效的配置。需要一种能够直接评估学习过程中对称性内化程度的诊断工具。

Method: 引入基于影响的诊断方法，测量参数更新在对称相关状态之间的传播。具体定义为沿着群轨道的损失梯度的度量加权重叠，该量探测学习损失景观的局部几何结构。

Result: 将该诊断应用于自回归流体流动模拟器，显示轨道梯度相干性为学习在对称变换上的泛化提供了机制，并指示了训练何时选择了对称兼容的盆地。

Conclusion: 提出了一种评估替代模型是否内化了已知解算子的对称性质的新技术，超越了传统的前向传播等变性测试，能够更深入地理解神经模拟器如何内化物理对称性。

Abstract: We study how neural emulators of partial differential equation solution operators internalize physical symmetries by introducing an influence-based diagnostic that measures the propagation of parameter updates between symmetry-related states, defined as the metric-weighted overlap of loss gradients evaluated along group orbits. This quantity probes the local geometry of the learned loss landscape and goes beyond forward-pass equivariance tests by directly assessing whether learning dynamics couple physically equivalent configurations. Applying our diagnostic to autoregressive fluid flow emulators, we show that orbit-wise gradient coherence provides the mechanism for learning to generalize over symmetry transformations and indicates when training selects a symmetry compatible basin. The result is a novel technique for evaluating if surrogate models have internalized symmetry properties of the known solution operator.

</details>


### [99] [MAPLE: Self-supervised Learning-Enhanced Nonlinear Dimensionality Reduction for Visual Analysis](https://arxiv.org/abs/2601.20173)
*Zeyang Huang,Takanori Fujiwara,Angelos Chatzimparmpas,Wandrille Duchemin,Andreas Kerren*

Main category: cs.LG

TL;DR: MAPLE是一种新的非线性降维方法，通过自监督学习和最大流形容量表示来改进UMAP的流形建模能力，特别适用于高维生物或图像数据。


<details>
  <summary>Details</summary>
Motivation: 现有的UMAP方法在流形建模方面存在不足，特别是在处理具有高簇内方差和弯曲流形结构的高维数据时，需要更有效的降维方法来提供更清晰的聚类分离和子聚类分辨率。

Method: MAPLE采用自监督学习方法，通过最大流形容量表示来压缩局部相似数据点之间的方差，同时放大不相似数据点之间的方差，从而更好地解缠复杂流形结构。

Result: 定性和定量评估表明，MAPLE比UMAP能产生更清晰的视觉聚类分离和更精细的子聚类分辨率，同时保持相似的计算成本。

Conclusion: MAPLE通过改进流形建模能力，为高维数据可视化提供了更有效的非线性降维解决方案，特别适用于生物和图像数据等复杂数据类型的分析。

Abstract: We present a new nonlinear dimensionality reduction method, MAPLE, that enhances UMAP by improving manifold modeling. MAPLE employs a self-supervised learning approach to more efficiently encode low-dimensional manifold geometry. Central to this approach are maximum manifold capacity representations (MMCRs), which help untangle complex manifolds by compressing variances among locally similar data points while amplifying variance among dissimilar data points. This design is particularly effective for high-dimensional data with substantial intra-cluster variance and curved manifold structures, such as biological or image data. Our qualitative and quantitative evaluations demonstrate that MAPLE can produce clearer visual cluster separations and finer subcluster resolution than UMAP while maintaining comparable computational cost.

</details>


### [100] [NeuraLSP: An Efficient and Rigorous Neural Left Singular Subspace Preconditioner for Conjugate Gradient Methods](https://arxiv.org/abs/2601.20174)
*Alexander Benanti,Xi Han,Hong Qin*

Main category: cs.LG

TL;DR: NeuraLSP是一种新型神经预处理器，通过利用系统矩阵近零空间向量的左奇异子空间和新的损失函数，解决了现有GNN方法中的秩膨胀问题，实现了最高53%的加速。


<details>
  <summary>Details</summary>
Motivation: 现有基于图神经网络的预处理器方法在提取连接结构时，需要将离散化的系统矩阵聚合为图，这会导致秩膨胀和次优收敛速度的问题。

Method: 提出了NeuraLSP神经预处理器，结合了一种新颖的损失函数，该函数利用系统矩阵近零空间向量的左奇异子空间，将频谱信息压缩到固定的低秩算子中。

Result: 该方法具有理论保证，对秩膨胀具有经验鲁棒性，实现了最高53%的加速。在不同类型的PDE家族上的综合实验结果验证了理论进展。

Conclusion: NeuraLSP通过创新的损失函数和低秩压缩方法，有效解决了现有神经预处理器中的秩膨胀问题，在理论和实验上都取得了显著改进。

Abstract: Numerical techniques for solving partial differential equations (PDEs) are integral for many fields across science and engineering. Such techniques usually involve solving large, sparse linear systems, where preconditioning methods are critical. In recent years, neural methods, particularly graph neural networks (GNNs), have demonstrated their potential through accelerated convergence. Nonetheless, to extract connective structures, existing techniques aggregate discretized system matrices into graphs, and suffer from rank inflation and a suboptimal convergence rate. In this paper, we articulate NeuraLSP, a novel neural preconditioner combined with a novel loss metric that leverages the left singular subspace of the system matrix's near-nullspace vectors. By compressing spectral information into a fixed low-rank operator, our method exhibits both theoretical guarantees and empirical robustness to rank inflation, affording up to a 53% speedup. Besides the theoretical guarantees for our newly-formulated loss function, our comprehensive experimental results across diverse families of PDEs also substantiate the aforementioned theoretical advances.

</details>


### [101] [Causal-Driven Feature Evaluation for Cross-Domain Image Classification](https://arxiv.org/abs/2601.20176)
*Chen Cheng,Ang Li*

Main category: cs.LG

TL;DR: 该论文从因果视角重新审视OOD分类，提出基于必要性和充分性评估表征的因果有效性，而非仅追求域不变性，在跨域基准测试中取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有OOD泛化方法大多追求域不变表征，隐含假设不变性意味着可靠性。但跨域不变的特征不一定对预测具有因果有效性，这限制了OOD分类的实际效果。

Method: 提出显式的片段级框架，直接测量表征在不同分布偏移下的因果必要性和充分性，提供比单纯域不变性更可靠的评估标准。

Result: 在多域基准测试中，该方法在OOD性能上取得一致提升，特别是在具有挑战性的域偏移场景下表现更优。

Conclusion: 因果评估为鲁棒泛化提供了重要价值，基于必要性和充分性的因果有效性标准比单纯追求表征不变性更能改善OOD分类性能。

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in real-world classification, where test distributions often differ substantially from training data. Most existing approaches pursue domain-invariant representations, implicitly assuming that invariance implies reliability. However, features that are invariant across domains are not necessarily causally effective for prediction.
  In this work, we revisit OOD classification from a causal perspective and propose to evaluate learned representations based on their necessity and sufficiency under distribution shift. We introduce an explicit segment-level framework that directly measures causal effectiveness across domains, providing a more faithful criterion than invariance alone.
  Experiments on multi-domain benchmarks demonstrate consistent improvements in OOD performance, particularly under challenging domain shifts, highlighting the value of causal evaluation for robust generalization.

</details>


### [102] [On the Computational Complexity of Performative Prediction](https://arxiv.org/abs/2601.20180)
*Ioannis Anagnostides,Rohan Chauhan,Ioannis Panageas,Tuomas Sandholm,Jingming Yan*

Main category: cs.LG

TL;DR: 本文研究了预测性预测的复杂性，发现当ρ>1时，计算ε-预测稳定点是PPAD完全的，即使在简单二次损失和线性分布偏移下也难解，并在战略分类中证明计算战略局部最优是PLS难的。


<details>
  <summary>Details</summary>
Motivation: 预测性预测中，部署预测模型会改变底层数据分布。当ρ<1时，简单重训练动态已知会线性收敛，但ρ>1时的复杂性一直未解决。本文旨在揭示这一复杂机制。

Method: 通过理论分析建立相变边界，证明计算ε-预测稳定点的PPAD完全性。技术贡献包括将PPAD难解性扩展到一般凸域，并在战略分类中证明PLS难解性。

Result: 发现尖锐相变：当ρ=1+O(ε)时，计算ε-预测稳定点是PPAD完全的，与一般和博弈中的纳什均衡多项式时间等价。即使在二次损失和线性分布偏移的简单设置下，难解性依然存在。

Conclusion: 预测性预测在ρ>1时具有计算难解性，这一发现对变分不等式复杂性有更广泛意义。战略分类中的战略局部最优计算也是PLS难的，揭示了实际应用中的根本性限制。

Abstract: Performative prediction captures the phenomenon where deploying a predictive model shifts the underlying data distribution. While simple retraining dynamics are known to converge linearly when the performative effects are weak ($ρ< 1$), the complexity in the regime $ρ> 1$ was hitherto open. In this paper, we establish a sharp phase transition: computing an $ε$-performatively stable point is PPAD-complete -- and thus polynomial-time equivalent to Nash equilibria in general-sum games -- even when $ρ= 1 + O(ε)$. This intractability persists even in the ostensibly simple setting with a quadratic loss function and linear distribution shifts. One of our key technical contributions is to extend this PPAD-hardness result to general convex domains, which is of broader interest in the complexity of variational inequalities. Finally, we address the special case of strategic classification, showing that computing a strategic local optimum is PLS-hard.

</details>


### [103] [Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery](https://arxiv.org/abs/2601.20193)
*Zhipeng Zhang,Wenting Ma,Kai Li,Meng Guo,Lei Yang,Wei Yu,Hongji Cui,Yichen Zhang,Mo Zhang,Jinzhe Lin,Zhenjie Yao*

Main category: cs.LG

TL;DR: 提出元认知强化学习框架，通过价值预测误差稳定性驱动的元信任变量来评估、调节和恢复学习行为，在奖励污染环境中提升鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒强化学习方法仅关注抑制不可靠经验或污染奖励，但缺乏对自身学习过程可靠性的推理能力，导致要么过度保守要么在不确定性累积时灾难性失败

Method: 提出元认知强化学习框架，引入基于价值预测误差稳定性(VPES)驱动的元信任变量，通过故障安全调节和渐进信任恢复来调制学习动态

Result: 在奖励污染的连续控制基准测试中，启用恢复的元认知控制相比强鲁棒性基线实现了更高的平均回报，并显著减少了后期训练失败

Conclusion: 元认知方法通过内部可靠性信号使智能体能够评估、调节和恢复学习行为，有效解决了鲁棒强化学习中过度保守或灾难性失败的问题

Abstract: Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.
  In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.
  Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.

</details>


### [104] [DeRaDiff: Denoising Time Realignment of Diffusion Models](https://arxiv.org/abs/2601.20198)
*Ratnavibusena Don Shahain Manujith,Yang Zhang,Teoh Tze Tzun,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: DeRaDiff是一种用于扩散模型对齐的实时采样调节方法，通过单次对齐训练后，在采样时通过参数λ动态调节正则化强度，避免传统方法需要多次训练对齐的昂贵计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型对齐方法需要为不同正则化强度分别训练模型，计算成本极高。如何在保持对齐效果的同时避免"奖励黑客"问题，并高效找到最佳正则化强度是核心挑战。

Method: DeRaDiff将语言模型的解码时实时对齐技术扩展到扩散模型，通过将反向步骤的参考分布替换为对齐后验和参考后验的几何混合，在采样时通过单一参数λ动态调节正则化强度。

Result: 实验表明，DeRaDiff能够有效近似不同正则化强度下从头训练的模型性能，在文本图像对齐和图像质量指标上表现一致，显著降低计算成本。

Conclusion: DeRaDiff提供了一种高效搜索最优正则化强度的方法，消除了昂贵的对齐扫描需求，大幅降低了扩散模型对齐的计算成本。

Abstract: Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to "reward hacking". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.

</details>


### [105] [Minimum-Cost Network Flow with Dual Predictions](https://arxiv.org/abs/2601.20203)
*Zhiyang Chen,Hailong Yao,Xia Yin*

Main category: cs.LG

TL;DR: 提出首个结合对偶预测的最小成本网络流算法，基于ε松弛法，在预测误差范数下有理论保证，实验显示12.74倍和1.64倍加速。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测已被证明能提升经典算法性能，但尚未有结合对偶预测的最小成本网络流算法。

Method: 基于ε松弛法的最小成本流算法，融入对偶预测，理论分析预测误差无穷范数下的时间复杂度，证明PAC学习预测的样本复杂度。

Result: 在交通网络和芯片逃逸布线两个应用中，分别使用固定预测和基于特征的神经网络预测，实验显示平均12.74倍和1.64倍加速。

Conclusion: 首个结合对偶预测的最小成本网络流算法，理论上有误差鲁棒性保证，实际应用显著加速，验证了机器学习预测提升经典算法的潜力。

Abstract: Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\times$ and $1.64\times$ average speedup on two applications.

</details>


### [106] [Hyperparameter Transfer with Mixture-of-Expert Layers](https://arxiv.org/abs/2601.20205)
*Tianze Jiang,Blake Bordelon,Cengiz Pehlevan,Boris Hanin*

Main category: cs.LG

TL;DR: 本文提出了一种用于混合专家（MoE）层的新参数化方法，通过动态平均场理论分析，实现了从5100万到20亿参数模型间的可靠超参数迁移，简化了MoE模型的超参数调优。


<details>
  <summary>Details</summary>
Motivation: 稀疏MoE层虽然能有效扩展模型规模，但增加了训练复杂性：1）路由权重需要超参数调优；2）专家数量和大小等架构维度需要选择且可能很大。现有方法在超参数选择上成本高且不可靠。

Method: 提出了一种新的MoE层参数化方法，基于动态平均场理论（DMFT）分析进行理论验证。该方法在固定token预算下，通过改变模型宽度、深度、专家数量和专家大小等维度，实现超参数的可迁移性。

Result: 实验表明，该参数化方法能够实现从5100万到超过20亿总参数模型间的可靠超参数迁移。通过在小模型上识别出的超参数，可以成功训练更大规模的模型，并获得良好的性能表现。

Conclusion: 提出的参数化方法显著降低了MoE模型的超参数调优成本，提高了训练可靠性，为大规模MoE模型的开发提供了实用且理论支持的工具。

Abstract: Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.

</details>


### [107] [Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning](https://arxiv.org/abs/2601.20209)
*Jinyang Wu,Shuo Yang,Changpeng Yang,Yuhao Shen,Shuai Zhang,Zhengqi Wen,Jianhua Tao*

Main category: cs.LG

TL;DR: Spark是一个用于强化学习训练语言模型智能体的框架，通过关键状态动态分支实现资源高效探索，在有限计算资源下显著提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习训练语言模型智能体面临两大挑战：高质量轨迹稀缺（特别是在资源有限情况下），以及现有方法盲目扩大rollout规模、均匀分配计算资源到所有中间步骤，导致大量计算浪费在平凡步骤上，且无法保证样本质量。

Method: 提出Spark框架，核心思想是在关键决策状态进行选择性分支探索。该方法利用智能体内在决策信号，在关键决策点激活自适应分支探索来探测有前景的轨迹，实现精确的资源分配，优先考虑采样质量而非盲目覆盖。

Result: 在多种任务（如具身规划）上的实验表明，Spark能以显著更少的训练样本实现更高的成功率，并且在未见场景中展现出强大的泛化能力。

Conclusion: Spark通过关键状态动态分支机制，实现了资源高效的探索策略，减少了对人类先验知识的依赖，使智能体能够自主扩展探索并实现更强的泛化能力。

Abstract: Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \textbf{Spark} (\textbf{S}trategic \textbf{P}olicy-\textbf{A}ware explo\textbf{R}ation via \textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.

</details>


### [108] [An Accounting Identity for Algorithmic Fairness](https://arxiv.org/abs/2601.20217)
*Hadi Elzayn,Jacob Goldin*

Main category: cs.LG

TL;DR: 本文推导了预测模型的会计恒等式，将准确性与常见公平性标准联系起来，表明在全局校准模型中，组内校准误差和组间误差不平衡的加权和等于"总不公平预算"。


<details>
  <summary>Details</summary>
Motivation: 现有研究常将准确性与公平性视为对立目标，但缺乏统一的理论框架来解释两者关系。本文旨在建立预测模型中准确性与公平性之间的数学联系，澄清误解并提供理论指导。

Method: 通过推导会计恒等式建立理论框架，将模型准确性（均方误差）与公平性指标（组内校准误差、组间误差不平衡）联系起来。通过理论分析和基准数据实验验证，并扩展到非二元结果预测任务。

Result: 1. 对于全局校准的二元结果模型，总不公平预算等于均方误差乘以不同结果类别的组间流行度差异；2. 准确性与公平性是互补关系：提高准确性会缩小总不公平预算，反之亦然；3. 许多公平性干预措施主要在公平性违规之间进行替代，当降低准确性时往往扩大总不公平预算；4. 理论可扩展到非二元结果，揭示额外结果信息如何缓解公平性冲突。

Conclusion: 准确性与公平性在二元预测任务中应被视为互补而非对立关系。提高模型准确性自然减少总不公平预算，为公平性改进创造空间。理论框架将标准不可能性结果作为特例，为理解和优化预测模型的公平性-准确性权衡提供了新视角。

Abstract: We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a "total unfairness budget." For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.

</details>


### [109] [Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization](https://arxiv.org/abs/2601.20226)
*Julian Gutierrez,Redouane Silvente*

Main category: cs.LG

TL;DR: 论文提出了两个机器学习框架：快速参数化模型用于预测聚合曲线并优化储能策略，以及生成模型用于全面分析市场情景。


<details>
  <summary>Details</summary>
Motivation: 为了在EPEX SPOT日前市场中准确预测聚合供需曲线并优化储能策略，需要既能满足日常操作需求（快速、可解释），又能进行全面市场分析的解决方案。

Method: 1. 快速参数化模型：使用低维且对网格稳健的表示法预测小时级供需曲线，结合最小/最大交易量和切比雪夫多项式表示弹性段
2. 生成模型：学习给定天气和燃料变量下24小时订单级提交的联合分布，生成包含个体买卖订单的合成日情景，然后聚合得到小时级供需曲线

Result: 参数化模型实现了低误差和良好可解释性，适合日常使用；生成模型能全面分析市场情景。基于这些预测，优化了价格制定储能策略，量化了收益分布，并揭示了容量扩张时的价格压缩效应（峰值降低、低谷升高、收益递减）。

Conclusion: 两个互补的机器学习框架为EPEX SPOT日前市场提供了有效的预测和优化工具：参数化模型适合日常运营，生成模型适合全面分析。储能策略优化显示了容量扩张的收益递减规律。

Abstract: We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.

</details>


### [110] [ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance](https://arxiv.org/abs/2601.20227)
*Zichao Yu,Ming Li,Wenyi Zhang,Difan Zou,Weiguo Gao*

Main category: cs.LG

TL;DR: 提出ProFlow框架，通过近端引导实现零样本物理一致采样，无需重新训练即可从稀疏观测中推断满足PDE约束的物理场。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型在逆问题中难以严格强制执行物理约束，要么需要昂贵的重新训练，要么会破坏已学习的生成先验。需要一种能同时保证严格物理一致性、观测保真度与预训练先验统计结构的采样机制。

Method: ProFlow采用两阶段交替方案：1）终端优化步，通过近端最小化将流预测投影到物理一致集和观测一致集的交集；2）插值步，将精炼状态映射回生成轨迹以保持与学习流概率路径的一致性。该过程可解释为局部最大后验概率更新序列。

Result: 在泊松方程、亥姆霍兹方程、达西方程和粘性伯格斯方程上的综合基准测试表明，ProFlow相比最先进的扩散和流基线方法，实现了更优的物理和观测一致性，以及更准确的分布统计。

Conclusion: ProFlow提供了一种有效的零采样框架，能够在不重新训练的情况下，从稀疏观测中推断出严格满足PDE约束的物理场，平衡了物理一致性、观测保真度和生成先验的统计结构。

Abstract: Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.

</details>


### [111] [Certificate-Guided Pruning for Stochastic Lipschitz Optimization](https://arxiv.org/abs/2601.20231)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.LG

TL;DR: 提出CGP方法用于噪声Lipschitz函数黑盒优化，通过显式证书排除次优点，在边界条件下获得理论保证，并开发了自适应、信任域和混合扩展，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自适应离散化方法虽然能隐式避免次优区域，但缺乏显式的最优性证书和可测量的进展保证，无法提供理论上的收敛保证和停止准则。

Method: 提出CGP方法，通过置信度调整的Lipschitz包络维护显式活动集A_t，排除可证明次优的点，在边界条件下获得理论保证。扩展包括：CGP-Adaptive在线学习Lipschitz常数，CGP-TR通过信任域扩展到高维，CGP-Hybrid在检测到局部平滑时切换到高斯过程优化。

Result: 理论证明在边界条件下，活动集体积以受控速率收缩，获得样本复杂度$\tildeO(\varepsilon^{-(2+α)})$。在12个基准测试（d∈[2,100]）中，CGP变体匹配或超越强基线方法，同时提供基于证书体积的原则性停止准则。

Conclusion: CGP为噪声Lipschitz函数优化提供了首个具有显式证书和理论保证的框架，其扩展方法在实际应用中表现出色，填补了现有方法在可证明最优性和停止准则方面的空白。

Abstract: We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\tildeO(\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.

</details>


### [112] [Order-Optimal Sample Complexity of Rectified Flows](https://arxiv.org/abs/2601.20250)
*Hari Krishna Sahoo,Mudit Gaur,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 本文证明了整流流模型在标准假设下达到了样本复杂度$\tilde{O}(\varepsilon^{-2})$，改进了流匹配模型的最佳已知$O(\varepsilon^{-4})$界限，并与均值估计的最优速率相匹配。


<details>
  <summary>Details</summary>
Motivation: 基于流的生成模型相比扩散模型表现出更高的效率。整流流模型将传输轨迹约束为从基分布到数据分布的线性路径，这种结构限制大大加速了采样，通常只需一个欧拉步就能实现高质量生成。需要从理论上解释其优越的实证性能。

Method: 研究整流流模型，其约束传输轨迹为从基分布到数据分布的线性路径。在参数化速度场和数据分布的神经网络类的标准假设下，分析其样本复杂度。利用整流流特定结构：模型通过沿线性路径的平方损失训练，相关假设类允许严格控制的局部Rademacher复杂度。

Result: 证明了整流流模型达到样本复杂度$\tilde{O}(\varepsilon^{-2})$，改进了流匹配模型的最佳已知$O(\varepsilon^{-4})$界限，与均值估计的最优速率相匹配。

Conclusion: 整流流模型的特殊结构使其假设类具有严格控制的局部Rademacher复杂度，从而获得了改进的、阶最优的样本复杂度，为整流流模型的强大实证性能提供了理论解释。

Abstract: Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\tilde{O}(\varepsilon^{-2})$. This improves on the best known $O(\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.

</details>


### [113] [HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH](https://arxiv.org/abs/2601.20255)
*Yueyang Wang,Jiawei Fu,Baolong Bi,Xili Wang,Xiaoqing Liu*

Main category: cs.LG

TL;DR: 该论文针对LLM在软件工程任务中的能力提升，提出了一种基于熵压缩假设的新度量标准HE-SNR，以解决传统指标在指导中期训练时的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏能够有效指导LLM中期训练的度量标准。传统指标如困惑度(PPL)受"长上下文税"影响，与下游软件工程性能关联性弱，无法准确评估模型在复杂软件工程任务中的潜力。

Method: 1. 提出严格的数据过滤策略；2. 提出熵压缩假设，将智能重新定义为将不确定性结构化为低阶熵压缩状态的能力；3. 基于细粒度熵分析，提出新度量标准HE-SNR（高熵信噪比）；4. 在工业级MoE模型上验证，覆盖不同上下文窗口（32K/128K）。

Result: HE-SNR在工业级MoE模型上表现出优越的鲁棒性和预测能力，相比传统指标能更好地预测模型在SWE任务上的表现。

Conclusion: 该研究为优化LLM在复杂工程领域的潜在能力提供了理论基础和实践工具，解决了中期训练度量标准的缺失问题。

Abstract: SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the "Long-Context Tax" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders ("reasonable hesitation"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.

</details>


### [114] [C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding](https://arxiv.org/abs/2601.20257)
*Jinren Ding,Xuejian Xu,Shen Jiang,Zhitong Hao,Jinhui Yang,Peng Jiang*

Main category: cs.LG

TL;DR: 本文提出C2框架，通过交叉学习块和约束感知损失增强决策变换器，解决自动出价中序列间相关性建模不足和最优/次优行为学习不区分的问题。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在生成式自动出价中显示出潜力，但存在两个关键局限：状态、动作和回报序列之间的跨相关性建模不足，以及对最优和次优行为的不加区分学习。

Method: 提出C2框架，包含两个核心创新：1) 通过交叉注意力机制增强序列间相关性建模的交叉学习块；2) 结合预算和获客成本约束进行选择性最优轨迹学习的约束感知损失。

Result: 在AuctionNet数据集上的离线评估显示，C2在不同预算设置下持续获得性能提升（相比最先进的GAVE方法提升高达3.23%），消融研究验证了交叉学习块和约束感知损失的互补协同作用。

Conclusion: C2框架通过增强跨序列相关性建模和选择性学习机制，在自动出价任务中表现出优越性，为解决决策变换器的局限性提供了有效方案。

Abstract: Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.

</details>


### [115] [Robust SDE Parameter Estimation Under Missing Time Information Setting](https://arxiv.org/abs/2601.20268)
*Long Van Tran,Truyen Tran,Phuoc Nguyen*

Main category: cs.LG

TL;DR: 提出一种在时间顺序信息缺失或混乱时，同时恢复时间顺序和估计随机微分方程参数的新框架。


<details>
  <summary>Details</summary>
Motivation: 传统SDE参数估计方法依赖准确的时间戳序列，但在时间顺序信息被破坏、缺失或隐藏时（如隐私保护场景）会失效。需要开发能在时间顺序未知情况下进行参数估计的方法。

Method: 利用前向和后向过程的不对称性，通过得分匹配准则推断观测对之间的正确时间顺序，然后通过排序过程恢复总顺序，最后使用最大似然估计从重建序列中估计SDE参数。

Result: 在合成和真实数据集上进行了广泛实验，证明了方法的有效性，将参数估计扩展到时间顺序缺失的场景，拓宽了在敏感领域的应用。

Conclusion: 提出了一种能同时恢复时间顺序和估计SDE参数的框架，解决了时间顺序信息缺失时的参数估计问题，扩展了SDE在隐私敏感等领域的应用范围。

Abstract: Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.

</details>


### [116] [The Forecast After the Forecast: A Post-Processing Shift in Time Series](https://arxiv.org/abs/2601.20280)
*Daojun Liang,Qi Li,Yinglong Wang,Jing Chen,Hu Zhang,Xiaoxiao Cui,Qizheng Wang,Shuo Li*

Main category: cs.LG

TL;DR: 提出了δ-Adapter，一种轻量级、架构无关的后处理方法，用于提升已部署时间序列预测模型的性能，无需重新训练或修改模型架构。


<details>
  <summary>Details</summary>
Motivation: 随着预测模型在准确率上接近收益递减，存在一个被忽视的机会：后处理的战略使用。解决时间序列预测中的"最后一公里"问题，即在不重新训练或修改已部署骨干模型的情况下提高准确性和不确定性估计。

Method: δ-Adapter通过两个接口学习微小有界模块：输入微调（对协变量进行软编辑）和输出残差校正。提供局部下降保证、O(δ)漂移边界和组合稳定性。同时可作为特征选择器学习稀疏、时域感知的输入掩码，并作为分布校准器（分位数校准器和保形校正器）提供校准的不确定性区间。

Result: 在多种骨干模型和数据集上的实验表明，δ-Adapter能以可忽略的计算成本提升准确性和校准效果，且无需接口更改。

Conclusion: δ-Adapter为解决时间序列预测的"最后一公里"问题提供了一种有效的后处理方案，能够在保持模型架构不变的情况下显著提升性能，同时增强可解释性和不确定性校准能力。

Abstract: Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $δ$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $δ$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(δ)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $δ$-Adapter improves accuracy and calibration with negligible compute and no interface changes.

</details>


### [117] [Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle](https://arxiv.org/abs/2601.20282)
*Viet Hung Dinh,Ming Ding,Youyang Qu,Kanchana Thilakarathna*

Main category: cs.LG

TL;DR: 该研究探索了Transformer注意力层中的记忆机制，提出了"关键词作为线索"的假说，并识别了编码和检索上下文关键词的特定神经元，为可解释AI和无学习应用提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 随着监管压力增加，可解释AI在确保大语言模型透明度、问责制和隐私保护无学习中的作用日益重要。尽管XAI领域已有进展，但Transformer注意力层在LLMs中的具体作用仍未被充分探索，特别是其在记忆机制中的角色。

Method: 借鉴心理学和计算心理语言学的研究，将Transformer注意力机制与人类记忆中的线索检索联系起来：查询编码检索上下文，键索引候选记忆痕迹，注意力权重量化线索-痕迹相似度，值携带编码内容。基于编码特异性原则，提出关键词作为检索线索的假说，并通过实验验证。

Result: 验证了"关键词作为线索"假说，识别了注意力层中特定神经元，这些神经元的激活选择性地编码和促进上下文定义关键词的检索。这些关键词可以从识别出的神经元中提取，并应用于下游任务如无学习。

Conclusion: 研究揭示了Transformer注意力层在记忆机制中的重要作用，为理解LLMs的内部工作机制提供了新视角，并为可解释AI、模型透明度和无学习等应用提供了理论基础和实用方法。

Abstract: While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.

</details>


### [118] [A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography](https://arxiv.org/abs/2601.20291)
*Kaiyi Yang,Seonyeong Park,Gangwon Jeong,Hsuan-Kai Huang,Alexander A. Oraevsky,Umberto Villa,Mark A. Anastasio*

Main category: cs.LG

TL;DR: 该研究提出了一种学习型空间脉冲响应补偿框架，用于3D光声计算机断层扫描成像，通过将SIR污染的测量数据映射到理想点状换能器应获得的数据，既保持计算效率又提升图像分辨率。


<details>
  <summary>Details</summary>
Motivation: 光声计算机断层扫描（PACT）结合了光学对比度和超声检测的优势，但使用大面积超声换能器会因空间脉冲响应（SIR）而降低图像分辨率。传统的解析重建方法忽略SIR导致分辨率下降，而基于优化的方法能考虑SIR但计算成本过高，特别是在3D应用中。因此需要一种既准确又快速的重建方法。

Method: 提出了一个学习型SIR补偿框架，在数据域进行操作。该框架将SIR污染的PACT测量数据映射到理想点状换能器应获得的补偿数据。研究了两种补偿模型：U-Net模型和专门设计的物理启发模型Deconv-Net。还包括一个快速分析训练数据生成过程。

Result: 通过虚拟成像研究验证了该框架，证明了分辨率提升以及对噪声变化、对象复杂性和声速异质性的鲁棒性。应用于体内乳腺成像数据时，学习补偿模型揭示了被SIR诱导伪影掩盖的精细结构。这是首次在3D PACT成像中展示学习型SIR补偿。

Conclusion: 该研究成功开发了一种学习型SIR补偿框架，能够在保持计算效率的同时提高3D PACT图像分辨率，解决了传统方法在准确性和速度之间的权衡问题，为临床应用中快速高质量成像提供了可行方案。

Abstract: Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.

</details>


### [119] [Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines](https://arxiv.org/abs/2601.20295)
*Yuxuan Bao,Jan Zajac,Megan Powers,Venkat Raman,J. Nathan Kutz*

Main category: cs.LG

TL;DR: Cheap2Rich是一个多尺度数据同化框架，通过结合快速低精度先验模型与可解释的差异修正，从稀疏传感器数据重建高精度状态空间，应用于旋转爆轰发动机等复杂多尺度系统。


<details>
  <summary>Details</summary>
Motivation: 解决计算廉价模型与复杂物理系统之间的仿真-现实差距是多尺度工程应用中机器学习面临的核心挑战，特别是当降阶模型通常只能捕捉主导动力学时。

Method: 提出Cheap2Rich多尺度数据同化框架，结合快速低精度先验模型与可学习的、可解释的差异修正，从稀疏传感器历史数据重建高精度状态空间。

Result: 成功从稀疏测量重建旋转爆轰发动机的高精度状态，同时分离出与喷射器驱动效应相关的物理意义明确的差异动力学。

Conclusion: 该框架为复杂多尺度系统的数据同化和系统识别提供通用多精度方法，支持快速设计探索、实时监测控制，并提供可解释的差异动力学分析。

Abstract: Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.

</details>


### [120] [Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction](https://arxiv.org/abs/2601.20299)
*Tianyi Alex Qiu,Micah Carroll,Cameron Allen*

Main category: cs.LG

TL;DR: 该论文提出基于对等预测的LLM评估与后训练方法，利用机制设计理论激励诚实回答，无需真实标签，能有效对抗欺骗性模型。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估和后训练依赖强监督，但对困难任务往往缺乏高质量监督，导致模型可能利用不完美的评估机制产生欺骗性结果。机制设计中的激励相容理论可以解决这一问题。

Method: 采用对等预测方法，基于互预测性度量奖励诚实和信息的回答，无需真实标签。通过理论保证和实证验证，在最高405B参数的模型上测试其效果。

Result: 方法能有效抵抗欺骗：使用对等预测奖励训练的8B模型能恢复因恶意微调而下降的真实性；对等预测在专家与参与者能力差距大时表现更好，而LLM-as-a-Judge在面对5-20倍大小的欺骗模型时会差于随机猜测。

Conclusion: 对等预测为LLM评估和后训练提供了可靠的弱监督方法，特别在评估强模型时，随着能力差距增大，其抵抗欺骗的能力反而增强，这为前沿模型评估提供了新途径。

Abstract: The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.

</details>


### [121] [Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches](https://arxiv.org/abs/2601.20307)
*Xinyu Li,Sishuo Chen,Guipeng Xv,Li Zhang,Mingxuan Luo,Zhangming Chan,Xiang-Rong Sheng,Han Zhu,Jian Xu,Chen Lin*

Main category: cs.LG

TL;DR: 该论文针对广告排序模型中GMV预测的延迟反馈问题，建立了TRACE基准数据集，并提出READER模型来分别处理单次购买和重复购买样本，动态校准回归目标以解决标签不完整问题。


<details>
  <summary>Details</summary>
Motivation: 在线广告排序模型的预测目标正从转化率(CVR)等概率指标转向GMV等数值业务指标。与CVR预测中已充分研究的延迟反馈问题不同，GMV预测的延迟反馈建模尚未探索且面临更大挑战，因为GMV是连续目标，且单次点击可能导致多次购买累积形成标签。

Method: 建立了TRACE基准数据集，包含从每个用户点击产生的完整交易序列，支持在线流式延迟反馈建模。提出READER模型：1) 通过路由器预测重复购买，选择性激活专家参数；2) 动态校准回归目标以缓解因不完整标签导致的低估问题。

Result: 在TRACE基准上的实验结果显示，READER在准确率方面比基线方法提升了2.19%，表现出优越性能。

Conclusion: 该研究为GMV预测的在线延迟反馈建模开辟了新途径，TRACE基准数据集和洞察将促进该方向未来的研究和应用。代码和数据集已开源。

Abstract: The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .

</details>


### [122] [Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching](https://arxiv.org/abs/2601.20332)
*Fengrui Zuo,Zhiwei Ke,Yiming Liu,Wenqi Lou,Chao Wang,Xvehai Zhou*

Main category: cs.LG

TL;DR: 论文提出Window-Diffusion方法，通过窗口化的token剪枝和缓存机制，大幅减少扩散语言模型推理时的冗余计算，实现高达99倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLMs)在推理时需要每轮迭代都对整个序列进行注意力计算，导致大量冗余计算，特别是对已掩码的token。现有块状扩散方法通常需要重新训练且更新顺序受限，难以直接应用于预训练DLMs。

Method: 基于token级分析发现DLM推理具有明显的结构局部性，提出Window-Diffusion方法：维护一个随去噪过程向右滑动的局部计算窗口，将未解码token分为三类：(1)在线计算的活跃token，(2)KV状态缓存并定期刷新的缓冲token，(3)窗口外被剪枝的远场token。计算仅限于窗口内的活跃和缓冲token。

Result: 在LLaDA和Dream模型上的实验表明，在匹配的计算预算下，该方法能实现高达99倍的推理加速，同时基本保持生成性能。

Conclusion: 通过利用扩散语言模型推理中的结构局部性，Window-Diffusion方法能够显著减少冗余计算，实现高效推理，为预训练DLMs提供了有效的加速方案。

Abstract: Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \textbf{\placeholder}\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \textit{active tokens} that are computed online, (ii) \textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\times$ inference speedup while largely preserving generation performance.

</details>


### [123] [TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs](https://arxiv.org/abs/2601.20357)
*Minjae Lee,Wonjun Kang,Byeongkeun Ahn,Christian Classen,Kevin Galim,Seunghyuk Oh,Minghao Yan,Hyung Il Koo,Kangwook Lee*

Main category: cs.LG

TL;DR: 该论文提出了TABED方法，通过动态集成多个草稿来加速大型视觉语言模型推理，相比自回归解码平均获得1.74倍速度提升。


<details>
  <summary>Details</summary>
Motivation: 推测解码在LLM推理加速中有效，但在大型视觉语言模型（LVLMs）中尚未充分探索。现有方法在不同输入场景下性能波动较大，需要更稳定的加速方案。

Method: 提出测试时自适应批处理集成草稿生成（TABED），利用推测解码设置中可用的历史真实值偏差，通过批处理推理动态集成多个草稿，保持训练无关且通过参数共享控制集成成本。

Result: 在11个数据集上测试，TABED相比自回归解码平均获得1.74倍稳健的墙钟时间加速，比单一草稿方法提升5%，并能与高级验证和替代草稿方法兼容集成。

Conclusion: TABED为LVLMs提供了一种高效、即插即用的推测解码加速方案，通过动态集成策略显著提升推理速度，同时保持成本可控。

Abstract: Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.

</details>


### [124] [TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs](https://arxiv.org/abs/2601.20361)
*Chen-Yang Dai,Che-Chia Chang,Te-Sheng Lin,Ming-Chih Lai,Chieh-Hsin Lai*

Main category: cs.LG

TL;DR: TINNs（时间诱导神经网络）通过将网络权重参数化为时间的函数，解决了传统PINNs在处理时变PDE时特征耦合的问题，实现了4倍精度提升和10倍收敛加速。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理时变偏微分方程时，将时间作为输入但使用共享权重的单一网络，导致不同时间的动态特征耦合，这会降低精度并可能使训练不稳定。

Method: 提出TINNs架构，将网络权重参数化为时间的函数，使空间表示能够随时间演化同时保持共享结构，形成非线性最小二乘问题并使用Levenberg-Marquardt方法优化。

Result: 在多种时变PDE实验中，相比PINNs和其他基线方法，TINNs实现了高达4倍的精度提升和10倍的收敛加速。

Conclusion: TINNs通过将网络权重参数化为时间的函数，有效解决了传统PINNs在处理时变PDE时的特征耦合问题，显著提升了精度和训练效率。

Abstract: Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\times$ improved accuracy and $10\times$ faster convergence compared to PINNs and strong baselines.

</details>


### [125] [Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku](https://arxiv.org/abs/2601.20363)
*Mariia Drozdova*

Main category: cs.LG

TL;DR: 连续时间生成模型能够表示支撑集为极度稀疏、全局约束离散集的分布。以数独网格为测试平台，研究发现随机采样优于确定性流，基于分数的采样器最可靠，DDPM式祖先采样整体有效性最高。模型可重新用于引导生成，作为概率性数独求解器。


<details>
  <summary>Details</summary>
Motivation: 研究标准连续时间生成模型能否表示支撑集为极度稀疏、全局约束离散集的分布。以数独网格作为受控测试平台，探讨连续松弛空间中这些离散结构的表示能力。

Method: 使用数独网格作为测试集，将其视为连续松弛空间的子集。训练流匹配和基于分数的模型沿着高斯概率路径，比较确定性（ODE）采样、随机（SDE）采样以及从相同连续时间训练导出的DDPM式离散化方法。

Result: 无条件生成时，随机采样显著优于确定性流；基于分数的采样器在连续时间方法中最可靠；DDPM式祖先采样整体有效性最高。模型可通过重复采样在固定线索下的完成情况，在满足约束时停止，从而作为概率性数独求解器使用。

Conclusion: 经典扩散/流模型能够为非零概率质量分配到全局约束的组合结构，并可通过随机搜索用于约束满足。虽然样本效率低于经典求解器和离散几何感知的扩散方法，但证明了连续时间生成模型对稀疏离散集分布的表征能力。

Abstract: Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.

</details>


### [126] [Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models](https://arxiv.org/abs/2601.20367)
*Qing Lyu,Zhe Fu,Alexandre Bayen*

Main category: cs.LG

TL;DR: 提出基于多智能体Transformer的无监督异常检测框架，用于识别自动驾驶中的安全关键场景，通过预测残差测量偏差，并采用双重评估方案验证检测稳定性和物理对齐性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中安全关键场景极为罕见，使得监督标注不切实际；传统基于规则的指标（如碰撞时间）过于简化，无法捕捉复杂的交互风险；现有方法缺乏系统化验证统计异常是否真正反映物理危险。

Method: 基于多智能体Transformer的无监督异常检测框架，通过建模正常驾驶行为并测量预测残差来识别异常；提出双重评估方案：稳定性评估使用Kendall秩相关系数和Jaccard指数，物理对齐性评估通过与传统替代安全指标（SSM）的相关性分析。

Result: 在NGSIM数据集上验证框架有效性：最大残差聚合器实现最高的物理对齐性同时保持稳定性；识别出388个被碰撞时间和统计基线方法遗漏的独特异常，捕捉到如横向漂移下的反应制动等微妙多智能体风险；检测到的异常可聚类为四种可解释的风险类型。

Conclusion: 该框架能够有效识别传统方法遗漏的安全关键场景，提供可解释的风险分类，为自动驾驶仿真和测试提供可操作的见解，弥补了统计异常检测与物理危险验证之间的差距。

Abstract: Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.

</details>


### [127] [LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning](https://arxiv.org/abs/2601.20375)
*Wei Huang,Anda Cheng,Yinggui Wang,Lei Wang,Tao Wei*

Main category: cs.LG

TL;DR: LLM-AutoDP：利用大语言模型作为智能体自动生成和优化数据处理策略的框架，无需人工干预或访问原始数据，通过迭代学习和加速技术提高效率。


<details>
  <summary>Details</summary>
Motivation: 领域数据中常包含大量低质量样本，需要有效的数据处理。传统方法依赖人工分析和试错，成本高且在高隐私领域（如医疗）存在隐私泄露风险。因此需要实现不暴露原始数据的自动化数据处理。

Method: 提出LLM-AutoDP框架：1）利用LLM作为智能体自动生成和优化数据处理策略；2）通过迭代上下文学习机制，基于反馈信号和比较评估不断优化策略；3）引入三种加速技术：分布保持采样（减少数据量但保持分布完整性）、处理目标选择（使用二元分类器识别低质量样本进行针对性处理）、缓存重用机制（减少冗余计算）。

Result: 1）使用LLM-AutoDP处理的数据训练的模型，相比未处理数据训练的模型胜率超过80%；2）相比基于LLM智能体的AutoML基线方法，LLM-AutoDP胜率约65%；3）加速技术将总搜索时间减少达10倍，显示出高效性。

Conclusion: LLM-AutoDP成功实现了无需人工干预和原始数据访问的自动化数据处理，通过LLM智能体和迭代优化机制生成高质量处理策略，加速技术显著提升效率，为解决高隐私领域的数据处理挑战提供了有效方案。

Abstract: Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.

</details>


### [128] [FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance](https://arxiv.org/abs/2601.20397)
*Kaile Wang,Jiannong Cao,Yu Yang,Xiaoyin Li,Mingjin Zhang*

Main category: cs.LG

TL;DR: FedRD是一个异构感知的联邦学习算法，通过参数引导的全局泛化聚合和局部去偏分类来解决联邦域泛化中的优化和性能分歧问题，为参与和未见客户端获得最优全局模型。


<details>
  <summary>Details</summary>
Motivation: 异构联邦学习(HFL)需要在不同实体间进行有效且隐私保护的协作。新加入的客户端需要对现有系统进行大量调整和额外训练，因此在异构数据下将联邦学习模型泛化到未见客户端变得越来越重要。作者指出了联邦域泛化中两个未解决的挑战性问题：优化分歧和性能分歧。

Method: 提出了FedRD算法，采用异构感知的联邦学习方法，协同利用参数引导的全局泛化聚合和局部去偏分类来减少分歧。该方法旨在为参与和未见客户端获得最优全局模型。

Result: 在公共多域数据集上的大量实验表明，该方法在解决这一特定问题上相比竞争基线具有显著的性能优势。

Conclusion: FedRD算法通过解决联邦域泛化中的优化分歧和性能分歧问题，能够有效地为异构联邦学习场景中的参与和未见客户端提供高性能的全局模型。

Abstract: Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.

</details>


### [129] [ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting](https://arxiv.org/abs/2601.20401)
*Wei Li*

Main category: cs.LG

TL;DR: ScatterFusion：结合散射变换和层次注意力机制的时间序列预测框架，通过多尺度特征提取和自适应增强提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测面临复杂多时间尺度依赖的挑战，现有方法在处理局部与全局模式、动态调整特征重要性方面存在局限。

Method: 提出包含四个关键组件的框架：1)层次散射变换模块提取多尺度不变特征；2)尺度自适应特征增强模块动态调整特征重要性；3)多分辨率时间注意力机制学习不同时间跨度依赖；4)趋势-季节-残差分解引导的结构感知损失函数。

Result: 在七个基准数据集上的实验表明，ScatterFusion优于其他常见方法，在不同预测时间跨度上显著降低了误差指标。

Conclusion: ScatterFusion通过协同整合散射变换和层次注意力机制，为时间序列预测提供了一个鲁棒有效的框架，能够更好地处理多尺度时间依赖。

Abstract: Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.

</details>


### [130] [AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting](https://arxiv.org/abs/2601.20409)
*Wei Li*

Main category: cs.LG

TL;DR: AWGformer是一种结合自适应小波分解与跨尺度注意力机制的多变量时间序列预测模型，通过动态选择小波基和分解层、跨尺度特征融合、频率感知注意力等模块，在多尺度非平稳时间序列预测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测需要同时捕获多个时间尺度的模式，同时保持计算效率。现有方法在处理多尺度和非平稳时间序列时存在局限性，需要更好的架构来捕捉不同频率分量之间的相互作用。

Method: 1. 自适应小波分解模块（AWDM）：根据信号特性动态选择最优小波基和分解层数
2. 跨尺度特征融合（CSFF）：通过可学习的耦合矩阵捕捉不同频带之间的相互作用
3. 频率感知多头注意力（FAMA）：根据频率选择性加权注意力头
4. 分层预测网络（HPN）：在多个分辨率上生成预测后进行重构

Result: 在基准数据集上的大量实验表明，AWGformer相比最先进方法取得了显著的平均改进，特别是在多尺度和非平稳时间序列上表现尤为有效。

Conclusion: AWGformer通过集成自适应小波分解和跨尺度注意力机制，有效提升了多变量时间序列预测性能。理论分析提供了收敛保证，并建立了小波引导注意力与经典信号处理原理之间的联系。

Abstract: Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.

</details>


### [131] [Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs](https://arxiv.org/abs/2601.20420)
*Yuhang Liu,Erdun Gao,Dong Gong,Anton van den Hengel,Javen Qinfeng Shi*

Main category: cs.LG

TL;DR: 论文提出ConCA框架，将LLM表示建模为概念对数后验的线性混合，为概念提取提供理论依据，优于现有稀疏自编码器方法。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器（SAEs）虽然能从LLM激活中提取可解释概念，但缺乏理论依据，导致方法设计和评估存在困难。需要建立LLM表示与人类可理解概念之间的理论联系。

Method: 提出概念成分分析（ConCA）框架，将LLM表示建模为潜在概念对数后验的线性混合。具体实现稀疏ConCA变体，利用稀疏先验解决线性分解的病态问题，设计了12种具体实现。

Result: 在多个LLM上验证了稀疏ConCA能够提取有意义的可解释概念，相比SAEs具有理论支撑的优势。

Conclusion: 通过潜在变量模型框架为LLM概念提取提供了理论依据，ConCA方法比现有SAEs更具原则性，为可解释AI提供了新的理论视角。

Abstract: Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.

</details>


### [132] [Nonlinear Dimensionality Reduction with Diffusion Maps in Practice](https://arxiv.org/abs/2601.20428)
*Sönke Beier,Paula Pirker-Díaz,Friedrich Pagenkopf,Karoline Wiesner*

Main category: cs.LG

TL;DR: 该论文对扩散映射技术进行了实践导向的综述，指出了数据预处理、参数设置和分量选择对结果的重要影响，并展示了一种识别最相关分量的技术。


<details>
  <summary>Details</summary>
Motivation: 扩散映射作为一种谱降维技术，能揭示高维数据中的非线性子流形，已在生物学、工程学和社会科学等领域广泛应用。然而，数据预处理、参数设置和分量选择对结果流形有显著影响，这一问题在现有文献中尚未得到全面讨论。

Method: 论文提供了一个实践导向的扩散映射技术综述，展示了常见陷阱，并引入了一种最近提出的识别最相关分量的技术。

Result: 研究结果表明，第一个分量不一定是最相关的分量，这与传统认知不同。

Conclusion: 扩散映射的应用需要谨慎考虑数据预处理、参数设置和分量选择，论文提供的实践指导和分量识别技术有助于更准确地应用该技术。

Abstract: Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.

</details>


### [133] [TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series](https://arxiv.org/abs/2601.20448)
*Zhiyu Chen,Minhao Liu,Yanru Zhang*

Main category: cs.LG

TL;DR: TimeCatcher是一个针对非平稳时间序列的变分预测框架，通过变分编码器捕捉潜在动态模式，并利用波动感知增强机制检测和放大局部显著变化，在长期高波动性预测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级MLP模型基于局部平稳性假设，在预测高度非平稳时间序列（特别是包含突发波动的场景，如网络流量监测）时容易出错。需要一种能够有效处理非平稳性和突发波动的长期预测方法。

Method: TimeCatcher扩展线性架构，包含：1）变分编码器捕捉历史数据中的潜在动态模式；2）波动感知增强机制检测并放大显著的局部变化。该框架专门针对非平稳时间序列的长期预测设计。

Result: 在交通、金融、能源和天气等9个真实世界数据集上的实验表明，TimeCatcher始终优于现有最先进基线方法，特别是在具有高波动性和突发波动的长期预测场景中取得了显著改进。

Conclusion: TimeCatcher通过变分编码和波动感知增强机制，有效解决了传统MLP模型在非平稳时间序列长期预测中的局限性，为高波动性场景下的时间序列预测提供了新的解决方案。

Abstract: Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.

</details>


### [134] [Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations](https://arxiv.org/abs/2601.20449)
*Fatima Ezzeddine,Obaida Ammar,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 提出基于强化学习的模型无关方法，生成同时满足个体公平性和群体公平性的反事实解释，并量化不同公平层级的成本。


<details>
  <summary>Details</summary>
Motivation: 反事实解释在XAI中至关重要，但需要确保相似个体和不同受保护群体获得相似且可执行的补救措施，以实现可信和公平的决策。

Method: 将公平反事实生成建模为优化问题，提出基于强化学习的模型无关方法，同时考虑个体公平性、群体公平性和混合公平性，扩展现有审计指标作为公平度量。

Result: 在三个基准数据集上评估，方法能有效确保个体和群体公平性，同时保持反事实解释的邻近性和合理性，并分别量化了不同公平层级的成本。

Conclusion: 开启了关于混合公平性及其在XAI及更广泛领域中作用和影响的更广泛讨论，证明了同时实现个体和群体公平性的可行性。

Abstract: Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.

</details>


### [135] [Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability](https://arxiv.org/abs/2601.20642)
*Rohan Asthana,Vasileios Belagiannis*

Main category: cs.LG

TL;DR: 提出一种新的扩散模型记忆检测方法，结合各向同性范数和各向异性对齐度量，无需去噪步骤，比现有方法快5倍以上


<details>
  <summary>Details</summary>
Motivation: 现有基于分数差异范数的记忆检测方法主要适用于高/中噪声水平的各向同性分布假设，但在低噪声的各向异性区域效果有限。需要更有效的记忆检测方法，特别是针对低噪声设置

Method: 通过分析各向异性区域，发现记忆样本在低噪声下具有指导向量和无条件分数之间的强角度对齐。结合各向同性范数度量和各向异性对齐度量，开发了新的记忆检测指标，仅需两次前向传播（条件和无条件）在纯噪声输入上计算

Result: 在Stable Diffusion v1.4和v2上的检测实验表明，该指标优于现有的免去噪检测方法，且比之前最佳方法快约5倍。基于该指标的自适应记忆提示缓解策略也证明了其有效性

Conclusion: 结合各向同性和各向异性特征的混合记忆检测方法能更准确地识别扩散模型中的记忆现象，提供高效且有效的检测方案，为记忆缓解提供了实用工具

Abstract: Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.

</details>


### [136] [Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations](https://arxiv.org/abs/2601.20477)
*Kadircan Aksoy,Peter Jung,Protim Bhattacharjee*

Main category: cs.LG

TL;DR: 论文通过二元假设检验的视角研究神经分类器的监督训练动态，发现泛化能力好的网络会逐渐对齐Neyman-Pearson最优决策规则。


<details>
  <summary>Details</summary>
Motivation: 研究神经分类器在监督训练过程中的动态特性，理解神经网络如何通过训练获得良好的泛化能力。

Method: 将分类建模为类别条件表示分布之间的二元假设检验，通过实证分析训练轨迹，观察KL散度的单调改善与错误率指数的关系。

Result: 发现泛化能力好的网络在训练过程中会逐渐对齐Neyman-Pearson最优决策规则，KL散度单调改善并与错误率指数相关。

Conclusion: 这为理解神经网络泛化提供了新视角，并可能启发针对不同类型神经网络的训练或正则化策略。

Abstract: We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.

</details>


### [137] [Continual GUI Agents](https://arxiv.org/abs/2601.20732)
*Ziwei Liu,Borui Kang,Hangjie Yuan,Zixiang Zhao,Wei Li,Yifan Zhu,Tao Feng*

Main category: cs.LG

TL;DR: 提出GUI持续学习新任务和GUI-AiF框架，通过APR-iF和ARR-iF两种奖励机制稳定GUI智能体在界面分布变化时的性能


<details>
  <summary>Details</summary>
Motivation: 数字环境（数据分布）不断变化，新的GUI数据随时间引入新领域或分辨率，静态环境训练的智能体性能会下降。现有方法无法在GUI分布随时间变化时保持稳定的基础性能

Method: 提出GUI-Anchoring in Flux (GUI-AiF)强化微调框架，包含两种新奖励：Anchoring Point Reward in Flux (APR-iF)和Anchoring Region Reward in Flux (ARR-iF)，引导智能体对齐变化的交互点和区域

Result: 大量实验表明GUI-AiF超越现有最先进基线方法

Conclusion: 建立了首个GUI智能体持续学习框架，揭示了强化微调在持续GUI智能体中的未开发潜力

Abstract: As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.

</details>


### [138] [An explainable framework for the relationship between dementia and glucose metabolism patterns](https://arxiv.org/abs/2601.20480)
*C. Vázquez-García,F. J. Martínez-Murcia,F. Segovia Román,A. Forte,J. Ramírez,I. Illán,A. Hernández-Segura,C. Jiménez-Mesa,Juan M. Górriz*

Main category: cs.LG

TL;DR: 提出一个半监督变分自编码器框架，通过相似性正则化将选择的潜在变量与痴呆进展的临床或生物标志物对齐，用于从高维神经影像数据中提取疾病相关模式。


<details>
  <summary>Details</summary>
Motivation: 高维神经影像数据存在复杂的非线性关系，传统方法难以有效分析神经退行性疾病。需要一种能够将影像数据编码到低维潜在空间，同时能够与临床指标对齐的可解释方法。

Method: 提出半监督变分自编码器框架，包含灵活的相似性正则化项，使选择的潜在变量与痴呆进展的临床或生物标志物对齐。使用ADNI的PET扫描数据，将第一个潜在维度与认知评分对齐。

Result: 监督潜在变量成功捕获了认知障碍相关的代谢变化，特别是在海马体、默认模式网络和中央执行网络等关键区域。其他潜在变量编码了仿射变换和强度变化等混杂因素。

Conclusion: 该框架能有效提取与阿尔茨海默病生物标志物一致的疾病相关模式，为研究神经退行性进展提供了一个可解释且适应性强的工具。

Abstract: High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.

</details>


### [139] [C3Box: A CLIP-based Class-Incremental Learning Toolbox](https://arxiv.org/abs/2601.20852)
*Hao Sun,Da-Wei Zhou*

Main category: cs.LG

TL;DR: C3Box是一个模块化、全面的Python工具箱，用于CLIP-based类增量学习，统一了传统、ViT-based和CLIP-based CIL方法，提供标准化实验流程。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CLIP的类增量学习方法分散在不同的代码库中，配置不一致，阻碍了公平比较、可复现性和实际应用。

Method: 开发C3Box工具箱，继承PyCIL的简洁设计，提供基于JSON的配置和标准化执行流程，将代表性CIL方法集成到统一的CLIP框架中。

Result: C3Box成功创建了一个可靠的可复现实验平台，仅依赖广泛使用的开源库，支持主流操作系统，代码已开源。

Conclusion: C3Box为持续学习研究提供了一个低工程开销、可复现的基准平台，有望促进CLIP-based类增量学习领域的公平比较和实际应用。

Abstract: Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.

</details>


### [140] [CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes](https://arxiv.org/abs/2601.20518)
*Jiawen Chen,Qi Shao,Mingtong Zhou,Duxin Chen,Wenwu Yu*

Main category: cs.LG

TL;DR: CCMamba是首个基于Mamba的统一神经框架，用于在组合复形上进行学习，通过将多秩关联关系组织成结构化序列，在保持线性时间复杂度的同时实现自适应、定向和长程信息传播。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑深度学习方法主要依赖注意力机制的局部消息传递，存在二次复杂度问题，且局限于低维表示，无法有效处理高阶复形中的秩感知信息聚合，限制了可扩展性。

Method: 将消息传递重新表述为选择性状态空间建模问题，通过将多秩关联关系组织成结构化序列，由秩感知状态空间模型处理，实现自适应、定向和长程信息传播的线性时间复杂度。

Result: 在图、超图和单纯复形基准测试中，CCMamba始终优于现有方法，同时展现出更好的可扩展性和对深度的鲁棒性。理论分析表明其消息传递的表达能力上限为1-Weisfeiler-Lehman测试。

Conclusion: CCMamba提供了一个统一且高效的框架，用于组合复形上的拓扑深度学习，解决了现有方法在可扩展性和高阶信息聚合方面的限制，为复杂关系结构的建模开辟了新途径。

Abstract: Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.

</details>


### [141] [Unsupervised Ensemble Learning Through Deep Energy-based Models](https://arxiv.org/abs/2601.20556)
*Ariel Maymon,Yanir Buznah,Uri Shaham*

Main category: cs.LG

TL;DR: 提出一种无监督集成学习方法，仅利用个体学习器的预测构建元学习器，无需标签数据、学习器特征或问题特定信息，在条件独立假设下有理论保证。


<details>
  <summary>Details</summary>
Motivation: 在缺乏真实标签或额外数据的情况下，如何有效组合多个学习器的预测是无监督集成学习的关键挑战。这在评估个体分类器性能困难、信息有限的场景中尤为重要。

Method: 提出基于深度能量的新方法，仅使用个体学习器的预测来构建准确的元学习器，能够捕捉学习器间的复杂依赖结构。方法完全无监督，无需标签数据、学习器特征或问题特定信息。

Result: 在多样化集成场景中表现优异，包括具有挑战性的专家混合设置。在标准集成数据集和专门设计的测试数据集上均展示了优越性能，验证了模型融合多源专业知识的能力。

Conclusion: 无监督集成学习在数据稀缺或隐私敏感环境中具有重要潜力，能够有效利用集体智能。提出的深度能量方法为这一领域提供了有力工具。

Abstract: Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.

</details>


### [142] [Reinforcement Unlearning via Group Relative Policy Optimization](https://arxiv.org/abs/2601.20568)
*Efstratios Zaradoukas,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: PURGE是一种基于组相对策略优化的LLM遗忘方法，通过内在奖励信号惩罚对禁止概念的提及，实现了高效、可验证的遗忘，在保持模型效用的同时显著降低目标token使用量。


<details>
  <summary>Details</summary>
Motivation: LLM在预训练过程中会无意中记忆敏感或受版权保护的数据，这违反了GDPR和欧盟AI法案等法律框架的要求。现有遗忘方法存在泄露数据、牺牲流畅性和鲁棒性、依赖昂贵外部奖励模型等问题。

Method: PURGE基于组相对策略优化框架，将遗忘构建为可验证问题。使用内在奖励信号惩罚任何对禁止概念的提及，实现安全一致的遗忘，无需依赖外部奖励模型。

Result: 相比现有方法，PURGE将每个目标的token使用量降低高达46倍，同时流畅性提高5.48%，对抗鲁棒性提高12.02%。在RWKU基准测试中，实现了11%的遗忘效果，同时保留了98%的原始效用。

Conclusion: 将LLM遗忘构建为可验证任务能够实现更可靠、高效和可扩展的遗忘，为遗忘研究提供了结合理论保证、改进安全性和实际部署效率的新方向。

Abstract: During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.

</details>


### [143] [Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM](https://arxiv.org/abs/2601.20571)
*Anna van Elst,Igor Colin,Stephan Clémençon*

Main category: cs.LG

TL;DR: 提出AsylADMM算法，用于去中心化的中位数和分位数估计，具有通信高效、鲁棒性强、内存占用少（每个节点仅需2个变量）的特点，支持异步更新，并能实现基于分位数的修剪、几何中位数估计等应用。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的去中心化学习需要通信高效、鲁棒性强且内存占用少的算法。现有的gossip方法虽通信高效，但鲁棒性不足；基于ADMM的异步去中心化方法虽能估计中位数（比均值更鲁棒），但内存需求随节点度数增长，在内存受限时不实用。

Method: 提出AsylADMM算法，一种新颖的gossip算法，专为异步更新设计，每个节点仅需维护两个变量。通过分析同步变体建立理论保证，并实证验证异步算法的快速收敛。算法支持基于分位数的修剪、几何中位数估计和基于深度的修剪。

Result: AsylADMM在实证中表现出快速收敛。基于分位数的修剪方法在性能上超越了现有的基于排序的方法。此外，通过马尔可夫链理论对基于排序的修剪进行了新的理论分析。

Conclusion: AsylADMM是一种高效、鲁棒且内存友好的去中心化中位数和分位数估计算法，适用于资源受限的边缘设备。它不仅解决了现有方法在鲁棒性和内存效率方面的不足，还拓展了分位数修剪等应用，并通过理论分析为基于排序的修剪提供了新的见解。

Abstract: Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.

</details>


### [144] [Ranking-aware Reinforcement Learning for Ordinal Ranking](https://arxiv.org/abs/2601.20585)
*Aiming Hao,Chen Zhu,Jiashu Zhu,Jiahong Wu,Xiangxiang Chu*

Main category: cs.LG

TL;DR: RARL是一个新颖的强化学习框架，通过统一的回归和排序目标，结合排名感知的奖励机制和响应突变操作，有效解决序数回归和排序问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效建模序数回归和排序任务中的序数依赖关系，需要一种能够显式学习这些关系的框架。

Method: 提出RARL框架：1）统一目标协同整合回归和排序学习；2）排名感知的可验证奖励机制，联合评估回归精度和排序准确性；3）响应突变操作（RMO）注入受控噪声以增强探索和避免鞍点停滞。

Result: 在三个不同的基准测试上进行广泛实验验证了RARL的有效性。

Conclusion: RARL通过强化学习框架有效解决了序数回归和排序问题，实现了回归和排序任务的相互促进。

Abstract: Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.

</details>


### [145] [Regularized Gradient Temporal-Difference Learning](https://arxiv.org/abs/2601.20599)
*Hyunjun Na,Donghwan Lee*

Main category: cs.LG

TL;DR: 提出正则化GTD算法（R-GTD），解决传统GTD在特征交互矩阵奇异时的不稳定问题，保证收敛性


<details>
  <summary>Details</summary>
Motivation: 现有GTD算法的收敛分析依赖于特征交互矩阵非奇异的限制性假设，但在实践中该矩阵可能奇异，导致算法不稳定或性能下降

Method: 通过重新形式化均方投影贝尔曼误差最小化问题，提出正则化优化目标，得到正则化GTD算法（R-GTD）

Result: R-GTD算法即使在特征交互矩阵奇异时也能保证收敛到唯一解，建立了理论收敛保证和显式误差界，并通过实验验证了有效性

Conclusion: 正则化GTD算法解决了传统GTD在奇异特征交互矩阵下的稳定性问题，为离策略策略评估提供了更鲁棒的解决方案

Abstract: Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.

</details>


### [146] [CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks](https://arxiv.org/abs/2601.20605)
*Junaid Sajid,Ivo Müürsepp,Luca Reggiani,Davide Scazzoli,Federico Francesco Luigi Mariani,Maurizio Magarini,Rizwan Ahmad,Muhammad Mahtab Alam*

Main category: cs.LG

TL;DR: 提出CoBA深度学习模型，集成CNN、BiLSTM和注意力机制，利用5G毫米波无线电测量对低空无人机在授权和限制空域的操作进行分类。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在民用和工业应用中的增加，确保低空操作安全变得至关重要。在密集毫米波环境中，准确分类低空无人机是否在授权或限制空域仍然具有挑战性，需要能够处理复杂传播和信号变化的模型。

Method: 提出CoBA深度学习模型，集成卷积神经网络（CNN）、双向长短期记忆网络（BiLSTM）和注意力机制层，以捕捉无人机无线电测量中的空间和时间模式。使用TalTech的5G毫米波网络收集专用数据集，包含受控低空无人机在授权和限制场景下的飞行数据。

Result: 实验结果表明，CoBA模型取得了优异的准确率，显著优于所有基线模型（传统机器学习模型和基于指纹识别的基准模型），证明了其在可靠和规范化的无人机空域监控方面的潜力。

Conclusion: CoBA模型通过集成CNN、BiLSTM和注意力机制，能够有效处理5G毫米波环境中的复杂信号变化，为低空无人机在授权和限制空域的分类提供了可靠解决方案，具有实际应用价值。

Abstract: Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.

</details>


### [147] [WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport](https://arxiv.org/abs/2601.20606)
*Xinyu Wang,Ruoyu Wang,Qiangwei Peng,Peijie Zhou,Tiejun Li*

Main category: cs.LG

TL;DR: 提出Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM)框架，通过平均速度和质量增长场实现快速一步生成，无需轨迹模拟，大幅提升单细胞动力学重建的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于动态不平衡最优传输的方法在推理时需要模拟轨迹，成为可扩展应用的关键瓶颈，需要更高效的推理方法。

Method: 提出平均流框架，用平均速度场和质量增长场总结任意时间间隔内的传输和质量增长动态；在此基础上构建Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM)解决Wasserstein-Fisher-Rao几何下的动态不平衡最优传输问题。

Result: 在合成和真实单细胞RNA测序数据集上，WFR-MFM比现有基线方法推理速度快几个数量级，同时保持高预测准确性；能够在大规模合成数据集上高效预测数千种条件的扰动响应。

Conclusion: WFR-MFM框架通过避免轨迹模拟实现了快速推理，为单细胞生物学中的可扩展动态重建提供了高效解决方案，特别适用于大规模扰动响应预测。

Abstract: Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.

</details>


### [148] [ACFormer: Mitigating Non-linearity with Auto Convolutional Encoder for Time Series Forecasting](https://arxiv.org/abs/2601.20611)
*Gawon Lee,Hanbyeol Park,Minseop Kim,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: ACFormer结合卷积与线性投影，通过共享压缩模块、门控注意力机制和独立补丁扩展层，解决时间序列预测中线性模型难以捕捉非线性信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有线性架构虽然在捕捉全局趋势方面高效，但在处理非线性信号时表现不佳。时间序列预测面临复杂的时间依赖性和通道间相关性建模挑战，需要更好的方法。

Method: 1. 系统分析CNN时间序列模型的感受野，提出"个体感受野"概念揭示结构依赖；2. 设计ACFormer架构，包含共享压缩模块、门控注意力机制和独立补丁扩展层，结合卷积的非线性特征提取能力和线性投影的效率。

Result: 在多个基准数据集上的实验表明，ACFormer持续取得最先进的性能，有效缓解了线性模型在捕捉高频分量方面的固有缺陷。

Conclusion: ACFormer成功地将卷积的非线性特征提取能力与线性投影的效率相结合，为时间序列预测提供了一种有效的新方法，特别是在处理非线性波动方面表现出色。

Abstract: Time series forecasting (TSF) faces challenges in modeling complex intra-channel temporal dependencies and inter-channel correlations. Although recent research has highlighted the efficiency of linear architectures in capturing global trends, these models often struggle with non-linear signals. To address this gap, we conducted a systematic receptive field analysis of convolutional neural network (CNN) TSF models. We introduce the "individual receptive field" to uncover granular structural dependencies, revealing that convolutional layers act as feature extractors that mirror channel-wise attention while exhibiting superior robustness to non-linear fluctuations. Based on these insights, we propose ACFormer, an architecture designed to reconcile the efficiency of linear projections with the non-linear feature-extraction power of convolutions. ACFormer captures fine-grained information through a shared compression module, preserves temporal locality via gated attention, and reconstructs variable-specific temporal patterns using an independent patch expansion layer. Extensive experiments on multiple benchmark datasets demonstrate that ACFormer consistently achieves state-of-the-art performance, effectively mitigating the inherent drawbacks of linear models in capturing high-frequency components.

</details>


### [149] [DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration](https://arxiv.org/abs/2601.20627)
*Gilles Eerlings,Brent Zoomers,Jori Liesenborgs,Gustavo Rovelo Ruiz,Kris Luyten*

Main category: cs.LG

TL;DR: DIVERSE框架使用FiLM层和CMA-ES算法探索深度神经网络的Rashomon集，无需重新训练即可生成预测行为不同但性能相当的高质量模型变体。


<details>
  <summary>Details</summary>
Motivation: 传统上要探索Rashomon集（即预测行为不同但准确率相当的模型集合）需要重新训练多个模型，计算成本高昂。DIVERSE旨在以更高效的方式系统性地探索这个集合。

Method: 在预训练模型基础上添加特征线性调制(FiLM)层，使用协方差矩阵自适应进化策略(CMA-ES)在潜在调制空间中进行搜索，无需重新训练或梯度访问即可生成多样化的模型变体。

Result: 在MNIST、PneumoniaMNIST和CIFAR-10数据集上，DIVERSE成功发现了多个性能相当但功能不同的模型。与重新训练相比，DIVERSE以更低的计算成本实现了相当的多样性。

Conclusion: DIVERSE为Rashomon集的探索提供了一种竞争性且高效的方法，能够在保持模型鲁棒性和性能的同时支持良好平衡的模型多样性，尽管重新训练仍然是生成Rashomon集的基准方法。

Abstract: We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.

</details>


### [150] [A Foundation Model for Virtual Sensors](https://arxiv.org/abs/2601.20634)
*Leon Götz,Lars Frederik Peiss,Erik Sauer,Andreas Udo Sass,Thorsten Bagdonat,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 首个虚拟传感器基础模型，可同时预测多种虚拟传感器，计算效率高，无需专家知识，在基准测试中实现415倍计算时间和951倍内存减少


<details>
  <summary>Details</summary>
Motivation: 现有虚拟传感器方法需要针对每个传感器构建特定模型，无法利用任务协同效应，缺乏统一基准。同时，现有时间序列基础模型计算昂贵且仅限于预测输入信号，不适用于虚拟传感器应用。

Method: 引入首个虚拟传感器基础模型，采用统一架构同时预测多种虚拟传感器，自动学习每个虚拟传感器的相关输入信号，保持计算效率，参数数量基本恒定，可扩展到数百个虚拟传感器。

Result: 在标准基准测试和包含180亿样本的应用特定数据集上，相比基线方法实现415倍计算时间减少和951倍内存需求减少，同时保持或提升预测质量。模型可优雅扩展到数百个虚拟传感器。

Conclusion: 该基础模型解决了虚拟传感器领域的两个关键限制，能够同时预测多种虚拟传感器并保持计算效率，消除了专家知识需求，增加了可解释性，为大规模传感器网络的实际部署提供了可行方案。

Abstract: Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.

</details>


### [151] [An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems](https://arxiv.org/abs/2601.20637)
*Panayiotis Ioannou,Pietro Liò,Pietro Cicuta*

Main category: cs.LG

TL;DR: 该研究探讨了神经ODE在复杂系统动力学建模和符号回归在发现控制方程方面的能力，发现神经ODE能有效外推到具有动态相似性的新边界条件，符号回归能从噪声数据恢复方程但依赖正确的输入变量选择。


<details>
  <summary>Details</summary>
Motivation: 准确建模复杂系统动力学并发现其控制微分方程对于加速科学发现至关重要。研究旨在探索神经ODE的外推能力和符号回归恢复底层方程的能力，特别是在噪声数据下的表现。

Method: 使用两个阻尼振荡系统的噪声合成数据，研究神经ODE（NODEs）的外推能力，以及符号回归（SR）从数据中恢复控制方程的能力。特别探讨了使用神经ODE在仅10%完整模拟数据上训练生成的数据进行符号回归的效果。

Result: 1. 神经ODE能有效外推到新边界条件，前提是生成的轨迹与训练数据具有动态相似性；2. 符号回归能成功从噪声真实数据恢复方程，但性能取决于输入变量的正确选择；3. 使用仅训练了10%完整模拟数据的神经ODE生成的数据，符号回归恢复了三个控制方程中的两个，并对第三个提供了良好近似。

Conclusion: 使用神经ODE丰富有限数据并让符号回归推断物理定律代表了一种有前景的科学发现新方法。虽然使用神经ODE生成数据进行符号回归恢复全部方程仍需要进一步研究，但整体结果表明了这一组合方法的潜力。

Abstract: Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.

</details>


### [152] [Learning Contextual Runtime Monitors for Safe AI-Based Autonomy](https://arxiv.org/abs/2601.20666)
*Alejandro Luque-Cerpa,Mengyuan Wang,Emil Carlsson,Sanjit A. Seshia,Devdatt Dubhashi,Hazem Torfah*

Main category: cs.LG

TL;DR: 提出了一种用于AI控制集成系统的上下文感知运行时监控框架，通过上下文学习选择最适合当前环境条件的控制器，提高安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习控制器在复杂决策任务中表现出色，但在陌生环境中性能会急剧下降，带来安全隐患。传统的集成方法通过平均或投票来提升鲁棒性，但往往会稀释单个控制器在不同操作上下文中的专业优势。

Method: 将安全AI控制集成设计重新定义为上下文监控问题。监控器持续观察系统上下文，并选择最适合当前条件的控制器。将监控器学习建模为上下文学习任务，借鉴上下文多臂老虎机技术。

Result: 在两个模拟自动驾驶场景中验证了该框架，相比非上下文基线方法，在安全性和性能方面都取得了显著改进。

Conclusion: 通过上下文感知监控框架，能够识别和利用控制器在不同操作环境中的专业优势，提供了理论安全保证，并提高了控制器多样性的利用效率。

Abstract: We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.

</details>


### [153] [MuRAL-CPD: Active Learning for Multiresolution Change Point Detection](https://arxiv.org/abs/2601.20686)
*Stefano Bertolasi,Diego Carrera,Diego Stucchi,Pasqualina Fragneto,Luigi Amedeo Bianchi*

Main category: cs.LG

TL;DR: 提出MuRAL-CPD方法，结合主动学习和多分辨率分解的半监督变点检测算法，通过用户反馈优化超参数，提升检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统变点检测方法通常是无监督的，无法适应任务特定的变化定义，也无法利用用户知识。需要一种能够结合用户反馈的方法来提高检测准确性和适应性。

Method: 提出MuRAL-CPD方法，基于小波的多分辨率分解来检测多个时间尺度上的变化，并整合主动学习机制，通过用户反馈迭代优化关键超参数。

Result: 在多个真实世界数据集上的实验表明，MuRAL-CPD相比最先进方法表现更优，特别是在仅有少量监督信息的情况下效果显著。

Conclusion: MuRAL-CPD通过整合用户反馈和多分辨率分析，能够更好地适应用户对变化的定义，提高变点检测的准确性和可解释性，尤其在监督信息有限的情况下优势明显。

Abstract: Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.

</details>


### [154] [Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models](https://arxiv.org/abs/2601.20687)
*Zhiqiang Kou,Junyang Chen,Xin-Qiang Cai,Xiaobo Xia,Ming-Kun Xie,Dong-Dong Wu,Biao Liu,Yuheng Jia,Xin Geng,Masashi Sugiyama,Tat-Seng Chua*

Main category: cs.LG

TL;DR: 提出了一种正例-未标记(PU)强化学习蒸馏方法，用于本地部署的小模型，无需人工标注偏好或奖励模型，通过教师模型的生成来诱导偏好信号进行对齐优化。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、成本和延迟限制，小模型本地部署越来越普遍，但实际应用通常只进行监督微调而无法进行强化学习对齐，因为RL对齐需要昂贵的人工偏好标注或依赖高质量奖励模型，这些都不适合本地部署环境。

Method: 提出正例-未标记强化学习蒸馏方法：为每个提示查询教师模型获得锚定响应，本地采样多个学生候选响应，进行锚定条件下的自排序来诱导成对或列表偏好，通过直接偏好优化或组相对策略优化实现完全本地训练循环。

Result: 理论分析表明该方法诱导的偏好信号具有顺序一致性和集中于接近最优候选的特性，支持偏好优化的稳定性。实验证明在低成本设置下能获得持续强大的性能。

Conclusion: 该方法为本地部署的小模型提供了一种无需人工偏好标注或奖励模型的强化学习对齐解决方案，解决了传统RL对齐方法在本地环境中的局限性。

Abstract: Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.

</details>


### [155] [Optimal Transport Group Counterfactual Explanations](https://arxiv.org/abs/2601.20692)
*Enrique Valero-Leal,Bernd Bischl,Pedro Larrañaga,Concha Bielza,Giuseppe Casalicchio*

Main category: cs.LG

TL;DR: 提出一种学习显式最优传输映射的方法，为任意群体实例生成反事实解释，无需重新优化，最小化群体传输成本并支持泛化


<details>
  <summary>Details</summary>
Motivation: 现有群体反事实解释方法存在三个主要问题：(1) 仅针对固定群体优化，无法泛化到新群体成员；(2) 依赖强模型假设（如线性）以保证可处理性；(3) 对反事实群体几何变形控制不佳

Method: 学习一个显式的最优传输映射，将任意群体实例发送到其反事实，无需重新优化，最小化群体的总传输成本。对于线性分类器，通过数学优化推导出群体反事实函数，识别底层凸优化类型（QP、QCQP等）

Result: 实验表明，该方法能够准确泛化，保持群体几何结构，与基线方法相比仅产生可忽略的额外传输成本。即使无法利用模型线性性，该方法也显著优于基线方法

Conclusion: 提出的最优传输映射方法解决了现有群体反事实解释方法的局限性，实现了泛化能力、几何保持和低传输成本，为可解释的群体决策提供了有效工具

Abstract: Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.

</details>


### [156] [Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?](https://arxiv.org/abs/2601.20694)
*Hao Liang,Jiayu Cheng,Sean R. Sinclair,Yali Du*

Main category: cs.LG

TL;DR: 本文证明了在Exo-MDPs（外生马尔可夫决策过程）中，纯利用算法无需探索即可获得有限样本遗憾界，推翻了传统认为探索必要的观点。


<details>
  <summary>Details</summary>
Motivation: 在库存控制、能源存储和资源分配等运筹学应用中，外生不确定性（如需求、到达或价格）主导系统行为。尽管经验表明贪婪的纯利用方法在这些设置中效果很好，但现有理论仍依赖显式探索或表格假设。本文旨在填补理论与实践的差距，证明探索在Exo-MDPs中是不必要的。

Method: 提出了纯利用学习（PEL）算法：1）表格情况下，PEL达到$\widetilde{O}(H^2|Ξ|\sqrt{K})$遗憾界；2）对于大型连续内生状态空间，引入LSVI-PE线性近似方法，其遗憾仅与特征维度、外生状态空间和时域有关，与内生状态和动作空间无关。分析工具包括反事实轨迹和Bellman闭特征转移。

Result: 1）首次证明了Exo-MDPs中纯利用算法的一般有限样本遗憾界；2）在表格情况下获得$\widetilde{O}(H^2|Ξ|\sqrt{K})$遗憾；3）LSVI-PE的遗憾多项式依赖于特征维度、外生状态空间和时域；4）实验显示PEL在合成和资源管理任务中始终优于基线方法。

Conclusion: 本文彻底改变了"探索是必要的"这一传统观点，证明了在Exo-MDPs中，纯利用就足够了。这为运筹学应用中的序列决策提供了新的理论依据，并为贪婪方法在实践中的成功表现提供了理论解释。

Abstract: Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\widetilde{O}(H^2|Ξ|\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.

</details>


### [157] [Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs](https://arxiv.org/abs/2601.20704)
*Melika Mobini,Vincent Holst,Floriano Tori,Andres Algaba,Vincent Ginis*

Main category: cs.LG

TL;DR: GPT-4o生成的参考文献在拓扑结构上与人类相似，但语义内容存在可检测的差异；结构特征仅能微弱区分，而文本嵌入能高精度识别LLM生成的参考文献。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地用于文献整理，需要研究LLM生成的参考文献列表是否能与人类编写的区分开，以评估其可靠性和潜在偏差。

Method: 构建了10,000篇论文的配对引用图（真实vs GPT-4o生成），并添加了匹配领域的随机基线。使用两种方法比较：(1)仅结构特征（度中心性、接近中心性、特征向量中心性、聚类系数、边数），(2)3072维标题/摘要嵌入。采用随机森林和图神经网络进行分析。

Result: 仅结构特征难以区分GPT与真实引用（RF准确率≈0.60），但能有效排除随机基线（≈0.89-0.92）。嵌入特征显著提高区分能力：聚合嵌入的RF达到≈0.83，带嵌入特征的GNN在测试集上达到93%准确率。使用Claude Sonnet 4.5和其他嵌入模型的结果一致。

Conclusion: LLM仅凭参数知识生成的参考文献能很好地模仿人类引用拓扑结构，但留下可检测的语义指纹；检测和去偏应关注内容信号而非全局图结构。

Abstract: Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\approx$ 0.60) despite cleanly rejecting the random baseline ($\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\approx$ 0.83, and GNNs with embedding node features achieve 93\% test accuracy on GPT vs.\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\ Claude $\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.

</details>


### [158] [Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions](https://arxiv.org/abs/2601.20714)
*Raul de la Rosa,Ivana Dusparic,Nicolas Cardozo*

Main category: cs.LG

TL;DR: 本文提出MORPHIN框架，一种自适应的Q学习框架，能够在非平稳环境中动态调整学习参数，适应奖励函数变化和动作空间扩展，防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体在现实应用中面临环境非平稳性的挑战，特别是当奖励函数变化或动作空间扩展时，传统方法需要完全重新训练，效率低下且可能导致灾难性遗忘。

Method: 提出MORPHIN框架，集成概念漂移检测机制，动态调整学习和探索超参数，支持奖励函数变化和动作空间扩展的在线适应，同时保留先前的策略知识。

Result: 在Gridworld基准测试和交通信号控制仿真中验证，MORPHIN相比标准Q学习基线表现出更优的收敛速度和持续适应能力，学习效率提升高达1.7倍。

Conclusion: MORPHIN框架有效解决了强化学习在非平稳环境中的适应问题，通过动态调整机制实现了高效在线学习，为实际应用提供了可行的解决方案。

Abstract: Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.

</details>


### [159] [Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis](https://arxiv.org/abs/2601.20729)
*Anchen Sun,Zhibin Chen,Xiaodong Cai*

Main category: cs.LG

TL;DR: 提出了基于Mean Teacher框架的深度半监督Cox模型Cox-MT，能同时利用标注和未标注数据训练，在癌症预后预测中显著优于仅用标注数据的现有ANN-based Cox模型


<details>
  <summary>Details</summary>
Motivation: 传统基于神经网络的Cox比例风险模型需要大量标注样本（包含生存时间信息）来训练高维特征模型，但实际中标注数据有限，这限制了模型性能。需要解决标注数据不足的问题。

Method: 采用深度半监督学习方法，基于Mean Teacher框架开发单模态和多模态的神经网络Cox模型（Cox-MT），同时利用标注和未标注数据进行训练。在TCGA癌症数据上应用，包括RNA-seq数据和全切片图像。

Result: 单模态Cox-MT模型在四种癌症类型上显著优于现有Cox-nnet模型；随着未标注样本增加，给定标注数据下的性能显著提升；多模态Cox-MT模型性能明显优于单模态模型。

Conclusion: Cox-MT模型能有效利用标注和未标注数据，相比仅用标注数据训练的现有神经网络Cox模型，显著提高了预后预测准确性。

Abstract: The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.

</details>


### [160] [SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning](https://arxiv.org/abs/2601.20738)
*Dawit Kiros Redie,Reza Arablouei,Stefan Werner*

Main category: cs.LG

TL;DR: 提出了SA-PEF方法，结合步前校正与部分误差反馈，解决联邦学习中非IID数据下梯度压缩导致的收敛缓慢问题，实现更快的训练收敛。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，带有误差反馈的梯度压缩虽然减少了通信开销，但在非IID数据下，残差误差衰减缓慢，导致梯度不匹配和早期训练轮次进展停滞的问题。

Method: 提出了步前部分误差反馈（SA-PEF），该方法结合了步前校正（SA）与部分误差反馈（PEF）。SA-PEF在步前系数α=0时恢复为EF，在α=1时恢复为步前EF（SAEF）。该方法通过控制步前系数α来平衡快速预热和长期稳定性。

Result: 在非凸目标和δ-压缩器条件下，建立了二阶矩界和残差递归，保证了在异构数据和部分客户端参与下的收敛性。收敛率与非凸Fed-SGD标准保证匹配，达到O((η,η_0TR)^{-1})的收敛速度。实验表明SA-PEF在多种架构和数据集上都能比EF更快达到目标精度。

Conclusion: SA-PEF方法有效解决了联邦学习中梯度压缩在非IID数据下的收敛问题，通过步前校正加速早期训练，同时保持长期稳定性，在实际应用中表现出优于传统误差反馈的性能。

Abstract: Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $α=0$ and step-ahead EF (SAEF) when $α=1$. For non-convex objectives and $δ$-contractive compressors, we establish a second-moment bound and a residual recursion that guarantee convergence to stationarity under heterogeneous data and partial client participation. The resulting rates match standard non-convex Fed-SGD guarantees up to constant factors, achieving $O((η,η_0TR)^{-1})$ convergence to a variance/heterogeneity floor with a fixed inner step size. Our analysis reveals a step-ahead-controlled residual contraction $ρ_r$ that explains the observed acceleration in the early training phase. To balance SAEF's rapid warm-up with EF's long-term stability, we select $α$ near its theory-predicted optimum. Experiments across diverse architectures and datasets show that SA-PEF consistently reaches target accuracy faster than EF.

</details>


### [161] [HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs](https://arxiv.org/abs/2601.20745)
*Guoan Wang,Feiyu Wang,Zongwei Lv,Yikun Zong,Tong Yang*

Main category: cs.LG

TL;DR: Hestia是一个基于Hessian引导的可微分低比特量化训练框架，通过软松弛替代硬量化，渐进硬化，利用Hessian迹指导温度退火，在Llama-3.2上实现优于现有三元量化的性能提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署受内存瓶颈限制，需要极低比特量化。现有量化感知训练方法早期采用硬量化和直通估计器，导致优化景观过早离散化，潜在权重与量化权重间梯度失配，阻碍量化模型有效优化。

Method: 提出Hestia框架：1) 用温度控制的softmax松弛替代刚性阶跃函数，保持训练早期梯度流动，渐进硬化量化；2) 利用张量级Hessian迹作为轻量曲率信号，驱动细粒度温度退火，实现模型敏感感知的离散化。

Result: 在Llama-3.2上评估，Hestia持续优于现有三元量化基线，1B和3B模型平均零样本提升分别为5.39%和4.34%，表明Hessian引导的松弛能有效恢复表示能力，为1.58比特LLM建立更鲁棒的训练路径。

Conclusion: Hestia通过Hessian引导的可微分量化框架，解决了极低比特量化中的梯度失配问题，为大规模语言模型的低比特部署提供了更有效的训练方法。

Abstract: As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.

</details>


### [162] [GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning](https://arxiv.org/abs/2601.20753)
*Zhiheng Jiang,Yunzhe Wang,Ryan Marr,Ellen Novoseller,Benjamin T. Files,Volkan Ustun*

Main category: cs.LG

TL;DR: 提出了GraphAllocBench基准测试，用于评估多目标强化学习中的偏好条件策略学习，基于城市管理启发的图资源分配环境，并提出了新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有偏好条件策略学习（PCPL）的基准测试主要局限于玩具任务和固定环境，缺乏现实性和可扩展性，无法有效评估复杂高维组合分配任务中的方法性能。

Method: 引入GraphAllocBench基准测试，基于名为CityPlannerEnv的新型图资源分配沙盒环境，提供多样化目标函数、偏好条件和可扩展问题集。同时提出了两个新评估指标：非支配解比例（PNDS）和排序得分（OS）。

Result: 实验表明GraphAllocBench能够暴露现有MORL方法的局限性，为图神经网络等图基方法在复杂高维组合分配任务中的应用铺平道路。基准测试具有灵活性，允许用户自定义目标、偏好和分配规则。

Conclusion: GraphAllocBench是一个多功能、可扩展的基准测试，能够有效评估偏好条件策略学习方法，特别适用于复杂、高维的图基资源分配任务，推动了PCPL研究的发展。

Abstract: Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench

</details>


### [163] [Supervised Guidance Training for Infinite-Dimensional Diffusion Models](https://arxiv.org/abs/2601.20756)
*Elizabeth L. Baker,Alexander Denker,Jes Frellsen*

Main category: cs.LG

TL;DR: 本文提出了首个函数空间中的扩散模型后验采样方法，通过无限维Doob's h-变换理论框架，结合监督引导训练实现高效稳定的后验采样。


<details>
  <summary>Details</summary>
Motivation: 虽然基于分数的扩散模型已被扩展到无限维函数空间用于偏微分方程逆问题，但在贝叶斯逆问题框架下，如何条件化这些模型以从后验分布中采样仍缺乏理论支持。

Method: 假设先验位于Cameron-Martin空间或相对于高斯测度绝对连续，证明可通过无限维Doob's h-变换进行条件化，并提出无模拟的分数匹配目标（监督引导训练）来高效学习引导项。

Result: 建立了扩散模型在函数空间中条件化的理论框架，条件分数可分解为无条件分数和引导项，并通过监督引导训练实现了函数空间贝叶斯逆问题的高效后验采样。

Conclusion: 本文首次提供了函数空间中扩散模型后验采样的理论和方法，为贝叶斯逆问题的扩散模型应用奠定了理论基础，并通过数值实验验证了有效性。

Abstract: Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.

</details>


### [164] [Less is More: Clustered Cross-Covariance Control for Offline RL](https://arxiv.org/abs/2601.20765)
*Nan Qiao,Sheng Yue,Shuning Wang,Yongheng Deng,Ju Ren*

Main category: cs.LG

TL;DR: 该论文针对离线强化学习中的分布偏移问题，提出通过分区缓冲采样和梯度校正惩罚来减少TD交叉协方差的有害影响，提高策略学习的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中存在分布偏移的根本挑战，尤其是在数据稀缺或数据集中包含大量分布外（OOD）区域时。理论分析和实验表明，标准平方误差目标会产生有害的TD交叉协方差，在OOD区域中这种效应会被放大，导致优化偏差和策略学习性能下降。

Method: 提出了两种互补策略：1）分区缓冲采样（Clustered Cross-Covariance Control for TD, C^4），通过限制更新在局部重放分区内，减少不规则协方差效应并对齐更新方向；2）显式的基于梯度的校正惩罚，在每个更新中消除协方差引起的偏差。证明了缓冲分区保持了最大化目标的下界性质，且这些约束在极端OOD区域中缓解了过度保守性而不改变策略约束离线强化学习的核心行为。

Result: 经验结果表明，该方法展现出更高的稳定性，并在先前方法基础上实现了高达30%的回报提升，特别是在小数据集和强调OOD区域的数据分割中表现突出。

Conclusion: 通过解决TD交叉协方差问题，提出的C^4方法和梯度校正惩罚有效缓解了离线强化学习中的分布偏移问题，特别是在数据稀缺和OOD区域占主导的情况下，显著提高了策略学习的稳定性和性能。

Abstract: A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.

</details>


### [165] [COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI](https://arxiv.org/abs/2601.20772)
*Shakhyar Gogoi*

Main category: cs.LG

TL;DR: COMET-SG1是一种轻量级、稳定性导向的自回归回归模型，专为边缘和嵌入式AI系统中的时间序列预测设计，具有线性行为空间编码、内存锚定转换估计和确定性状态更新等特点。


<details>
  <summary>Details</summary>
Motivation: 解决边缘和嵌入式AI系统中时间序列预测的关键问题：在完全自回归推理下保持长期稳定性，避免预测误差随时间累积导致漂移问题。

Method: 采用线性行为空间编码、内存锚定转换估计和确定性状态更新的结构，不同于传统的RNN或Transformer序列模型，优先考虑有界的长期行为。

Result: 在非平稳合成时间序列数据上的实验表明，COMET-SG1在短期预测精度上与基线模型竞争，同时在长期预测漂移方面显著减少，相比MLP、LSTM和k近邻基线表现更好。

Conclusion: COMET-SG1通过紧凑的参数占用和兼容定点算术的操作，为边缘和嵌入式AI应用提供了实用且可解释的稳定自回归预测方法。

Abstract: COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.

</details>


### [166] [Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying](https://arxiv.org/abs/2601.20773)
*Rubén Jiménez,Oriol Pujol*

Main category: cs.LG

TL;DR: 该论文提出了一种基于距离的黑盒模型复制框架，通过使用有符号距离替代硬标签监督，将模型复制转化为平滑回归问题，从而提高复制模型的保真度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实际部署的机器学习系统需要持续演化以适应数据、架构和法规的变化，但往往无法访问原始训练数据或模型内部。在黑盒访问（仅输入-输出查询）且限制为硬标签输出的情况下，模型复制变成一个从点查询重构不连续表面的问题，严重限制了恢复决策边界几何的效率。

Method: 提出了基于距离的复制（蒸馏）框架：1）用有符号距离到教师模型决策边界的监督替代硬标签监督；2）开发了α控制的平滑和正则化方案，对诱导的目标表面进行Hölder/Lipschitz控制；3）提出了两种模型无关算法来估计仅标签访问下的有符号距离。

Result: 在合成问题和UCI基准测试上的实验表明，该方法在保真度和泛化准确率上相比硬标签基线有持续改进，同时能为黑盒副本提供距离输出作为不确定性相关信号。

Conclusion: 基于距离的复制框架通过利用局部几何信息，将硬标签复制问题转化为平滑回归问题，有效提高了黑盒模型复制的效率和效果，并为复制模型提供了不确定性估计能力。

Abstract: Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $α$-governed smoothing and regularization scheme with Hölder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.

</details>


### [167] [When More Data Doesn't Help: Limits of Adaptation in Multitask Learning](https://arxiv.org/abs/2601.20774)
*Steve Hanneke,Mingyue Xu*

Main category: cs.LG

TL;DR: 该论文超越了之前的多任务学习无免费午餐定理，证明了即使每个任务有大量样本，适应性问题依然存在，揭示了多任务学习的固有困难无法通过增加单任务样本量来解决。


<details>
  <summary>Details</summary>
Motivation: 深入理解多任务学习的统计极限，超越arXiv:2006.15785的无免费午餐定理，探索即使每个任务有大量样本时是否仍存在适应性问题。

Method: 通过建立更强的不可行性结果来证明适应性问题，该结果适用于任意大的每个任务样本量，并讨论了未来可能感兴趣的"最优适应性"概念。

Result: 证明了多任务学习的困难无法通过增加每个任务的样本量来克服，即使每个任务有大量数据，适应性问题依然存在，这一结果比之前的无免费午餐定理更强。

Conclusion: 多任务学习存在固有统计限制，单靠增加每个任务的样本量无法解决适应性问题，需要新的理论框架和方法来应对这一挑战。

Abstract: Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.
  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.

</details>


### [168] [Active Learning for Decision Trees with Provable Guarantees](https://arxiv.org/abs/2601.20775)
*Arshia Soltani Moakhar,Tanapoom Laoaron,Faraz Ghahremani,Kiarash Banihashem,MohammadTaghi Hajiaghayi*

Main category: cs.LG

TL;DR: 本文首次分析了决策树作为二分类器的主动学习标签复杂度，在特定假设下实现了对数多项式级的标签查询数，并提出了首个具有乘法误差保证的主动学习算法。


<details>
  <summary>Details</summary>
Motivation: 当前对决策树主动学习标签复杂度的理论理解不足，特别是缺乏对决策树分歧系数（disagreement coefficient）的分析，这是影响主动学习效率的关键参数。

Method: 1) 首次分析决策树的分歧系数；2) 提出首个具有(1+ε)近似保证的通用主动学习算法；3) 在特定假设下设计针对决策树的主动学习算法：要求每条根到叶路径查询不同特征维度，且输入数据具有规则网格结构。

Result: 1) 在假设条件下实现了对数多项式级的标签查询复杂度；2) 放松假设会导致多项式级复杂度；3) 算法对误差容忍度ε的依赖接近最优；4) 提供了标签复杂度的下界。

Conclusion: 本文为决策树的主动学习提供了首个完整的理论分析框架，在特定自然假设下实现了接近最优的标签效率，填补了该领域的重要理论空白。

Abstract: This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+ε)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $ε$ is close to optimal.

</details>


### [169] [Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces](https://arxiv.org/abs/2601.20800)
*Kaito Baba,Yoshihiko Ozaki,Shuhei Watanabe*

Main category: cs.LG

TL;DR: condPED-ANOVA框架用于条件搜索空间中超参数重要性估计，解决传统方法无法处理条件超参数的问题


<details>
  <summary>Details</summary>
Motivation: 原始PED-ANOVA虽然能高效估计超参数重要性，但假设搜索空间是固定的、无条件的，无法正确处理条件超参数（即某个超参数的存在或定义域依赖于其他超参数）

Method: 提出条件超参数重要性定义，并推导出闭式估计器，能准确反映条件激活和定义域变化

Result: 实验表明，现有HPI估计器的简单适应在条件设置下会产生误导性或不可解释的重要性估计，而condPED-ANOVA能始终提供反映底层条件结构的有意义重要性

Conclusion: condPED-ANOVA为条件搜索空间提供了原则性的超参数重要性估计框架，解决了传统方法在条件设置下的局限性

Abstract: We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.

</details>


### [170] [Reinforcement Learning via Self-Distillation](https://arxiv.org/abs/2601.20802)
*Jonas Hübotter,Frederike Lübeck,Lejs Behric,Anton Baumann,Marco Bagatella,Daniel Marta,Ido Hakimi,Idan Shenfeld,Thomas Kleine Buening,Carlos Guestrin,Andreas Krause*

Main category: cs.LG

TL;DR: SDPO是一种强化学习方法，利用丰富的文本反馈（而非仅标量奖励）进行自我蒸馏，提升可验证领域（如代码、数学）中LLM的样本效率和最终准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习方法仅依赖每次尝试的标量结果奖励，存在严重的信用分配瓶颈。许多可验证环境实际上提供丰富的文本反馈（如运行时错误、评估说明），这些反馈能解释失败原因，但现有方法未能充分利用。

Method: 提出Self-Distillation Policy Optimization (SDPO)，将token化的反馈转化为密集学习信号，无需外部教师或显式奖励模型。SDPO将基于反馈的当前模型视为自我教师，将其反馈信息的下一个token预测蒸馏回策略中，利用模型在上下文中回顾性识别自身错误的能力。

Result: 在科学推理、工具使用和LiveCodeBench v6的竞赛编程任务中，SDPO相比强RLVR基线提高了样本效率和最终准确性。即使在仅返回标量反馈的标准RLVR环境中，SDPO通过使用成功rollout作为失败尝试的隐式反馈，也优于基线。在测试时对单个问题应用SDPO，能在困难二元奖励任务中加速发现，用3倍更少的尝试达到与best-of-k采样或多轮对话相同的发现概率。

Conclusion: SDPO有效利用了可验证环境中丰富的文本反馈，通过自我蒸馏机制缓解了信用分配瓶颈，在多种任务上展现出优于现有方法的性能，并能显著减少发现正确答案所需的尝试次数。

Abstract: Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.

</details>


### [171] [GNN Explanations that do not Explain and How to find Them](https://arxiv.org/abs/2601.20815)
*Steve Azzolin,Stefano Teso,Bruno Lepri,Andrea Passerini,Sagar Malhotra*

Main category: cs.LG

TL;DR: 该论文发现自解释图神经网络（SE-GNNs）存在解释失效问题：其解释可能与模型推理过程完全无关，且现有忠实度指标难以检测这种失效。作者提出了一种新的忠实度指标来可靠识别这类问题。


<details>
  <summary>Details</summary>
Motivation: 虽然现有研究指出SE-GNNs的解释可能不理想且具有误导性，但对这些解释的失效情况缺乏系统性分析。该论文旨在识别SE-GNNs解释的关键失效模式，特别是解释与模型推理过程完全无关的情况，这对于理解模型内部机制和检测敏感属性滥用至关重要。

Method: 作者通过理论分析和实证研究，首先证明了SE-GNNs在获得最优真实风险的同时仍可能产生与推理过程无关的退化解释。然后分析了现有忠实度指标在检测这种失效模式时的局限性。最后，提出了一种新的忠实度指标，能够在恶意攻击和自然场景下可靠地标记退化解释为不忠实。

Result: 研究发现退化解释既可能被恶意植入（用于隐藏敏感属性的使用），也可能自然产生。大多数现有忠实度指标无法有效识别这种失效模式。作者提出的新指标在恶意和自然设置下都能可靠地将退化解释标记为不忠实。

Conclusion: SE-GNNs的解释存在与模型推理过程完全无关的严重失效问题，这凸显了可靠审计的必要性。作者提出的新忠实度指标为解决这一问题提供了有效工具，有助于提高SE-GNNs解释的可靠性。

Abstract: Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.

</details>


### [172] [Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning](https://arxiv.org/abs/2601.20829)
*Minwu Kim,Safal Shrestha,Keith Ross*

Main category: cs.LG

TL;DR: 本文提出了一种名为"失败前缀条件化"的方法，通过从罕见错误推理轨迹中提取前缀来重新分配探索，以解决RLVR训练在饱和问题上的停滞问题。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然显著提升了LLM的推理能力，但在问题趋于饱和时训练会停滞。核心挑战在于信息性失败的可访问性差：学习信号存在但很少在标准推理过程中遇到。

Method: 提出失败前缀条件化方法：不从原始问题开始，而是基于罕见错误推理轨迹的前缀来重新分配探索，让模型暴露于易失败状态。还探索了迭代方法，在训练期间刷新失败前缀。

Result: 失败前缀条件化能带来与中等难度问题训练相当的性能提升，同时保持标记效率。该方法减少了在误导性失败前缀下的性能下降，但对正确早期推理的遵循略有降低。迭代方法能在性能平台期后实现额外增益。

Conclusion: 失败前缀条件化为在饱和问题上扩展RLVR训练提供了一条有效途径，通过暴露模型于易失败状态来克服训练停滞问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.

</details>


### [173] [Reward Models Inherit Value Biases from Pretraining](https://arxiv.org/abs/2601.20838)
*Brian Christian,Jessica A. F. Thompson,Elle Michelle Yang,Vincent Adam,Hannah Rose Kirk,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.LG

TL;DR: 研究发现奖励模型（RMs）的输出会受到其基础预训练语言模型的显著影响，即使使用相同偏好数据和微调过程，不同基础模型（如Llama和Gemma）的RMs在人类价值观维度上表现出系统性差异。


<details>
  <summary>Details</summary>
Motivation: 虽然奖励模型对将大语言模型与人类价值观对齐至关重要，但相对于预训练和微调阶段的研究，奖励模型本身受到的关注较少。研究旨在探索基础预训练模型对奖励模型行为的影响程度和性质。

Method: 对10个领先的开源权重奖励模型进行了全面研究，使用经过验证的心理语言学语料库进行分析。通过"Big Two"心理轴（能动性与共融性）框架，比较不同基础模型（Llama和Gemma）的奖励模型偏好。设计了实验来追溯这些差异到指令微调和预训练模型的logits，并推导出可用的隐式奖励分数。还进行了消融实验，控制偏好数据来源和数量。

Result: 发现奖励模型在多个人类价值观维度上表现出显著差异，这些差异与其基础模型直接相关。Llama基础模型的奖励模型表现出对"能动性"的稳健偏好，而Gemma基础模型的奖励模型则表现出对"共融性"的稳健偏好。即使偏好数据和微调过程完全相同，这种差异仍然存在。这种效应不仅可重复，而且具有惊人的持久性。

Conclusion: 奖励模型的输出受到其基础预训练语言模型的显著影响，这凸显了在预训练阶段进行安全和对齐工作的重要性。开源开发者在选择基础模型时，不仅要考虑性能，还需要将其视为价值观的选择。

Abstract: Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the "Big Two" psychological axes, we show a robust preference of Llama RMs for "agency" and a corresponding robust preference of Gemma RMs for "communion." This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.

</details>


### [174] [$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval](https://arxiv.org/abs/2601.20844)
*Zihao Wang,Hang Yin,Lihui Liu,Hanghang Tong,Yangqiu Song,Ginny Wong,Simon See*

Main category: cs.LG

TL;DR: 本文研究了将子集成员关系嵌入向量空间所需的最小维度(MED)，针对不同距离/相似度度量给出了理论紧界和实证支持，发现基于嵌入的检索限制主要源于学习性挑战而非几何约束。


<details>
  <summary>Details</summary>
Motivation: 研究基于嵌入的检索系统中子集成员关系嵌入的几何约束问题，探究是否真的需要高维空间来表示子集关系，还是学习性挑战才是主要瓶颈。

Method: 1) 理论推导最小可嵌入维度(MED)的紧界；2) 对ℓ₂距离、内积和余弦相似度等不同度量进行实证验证；3) 在更实际的可实现设置中进行数值模拟，将子集嵌入选择为包含元素的嵌入质心。

Result: 1) 推导出MED的理论紧界；2) 实证支持了这些理论边界；3) 数值模拟显示MED与元素数量之间存在对数依赖关系；4) 发现基于质心的嵌入方法在实践中更容易实现。

Conclusion: 嵌入检索的限制主要源于学习性挑战而非几何约束，这为未来算法设计提供了指导方向，表明重点应放在改进学习过程而非增加维度上。

Abstract: This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of "distances" or "similarities," including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.

</details>


### [175] [PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting](https://arxiv.org/abs/2601.20845)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz*

Main category: cs.LG

TL;DR: PatchFormer是一个基于patch的时间序列基础模型，通过分层掩码重建进行自监督预训练，并使用轻量级适配器实现高效迁移，在多个领域的时间序列预测任务上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法通常需要特定领域的特征工程和大量标注数据，这限制了模型的可扩展性和跨领域应用能力。

Method: 将时间序列分割为patch，通过分层掩码重建学习多尺度时间表示，使用动态掩码和同时关注局部准确性与全局一致性的目标进行预训练，最后通过跨领域知识蒸馏和轻量级适配器实现任务适应。

Result: 在24个基准数据集（涵盖天气、能源、交通、金融和医疗）上实现了最先进的零样本多步预测性能，相比强基线平均减少27.3%的均方误差，同时只需6%的任务特定训练数据；模型在1000亿数据点内呈现近似对数线性扩展，处理512长度序列比完整序列transformer快3.8倍。

Conclusion: PatchFormer提供了一个有效的时间序列基础模型框架，能够显著减少对领域特定特征工程和标注数据的依赖，实现高效的时间序列预测，具有良好的可扩展性和计算效率。

Abstract: Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.

</details>


### [176] [Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation](https://arxiv.org/abs/2601.20848)
*Weixin Chen,Li Chen,Yuhan Zhao*

Main category: cs.LG

TL;DR: Cofair提出了一种单一训练框架，能够在推荐系统中实现训练后的公平性控制，无需为不同的公平性要求重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法通常在训练时固定公平性要求，缺乏训练后的灵活性。现实场景中不同利益相关者可能需要随时间变化的公平性要求，重新训练成本过高。

Method: Cofair采用共享表示层和公平条件适配器模块，生成针对不同公平性级别的用户嵌入，同时引入用户级正则化项确保用户层面的单调公平性改进。

Result: 在多个数据集和骨干模型上的实验表明，Cofair能够提供动态的公平性控制，在不重新训练的情况下达到与最先进方法相当或更好的公平性-准确性权衡。

Conclusion: Cofair框架有效解决了推荐系统中动态公平性控制的需求，为不同公平性要求提供了灵活、高效的解决方案。

Abstract: Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.

</details>


### [177] [Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation](https://arxiv.org/abs/2601.20854)
*Aníbal Silva,Moisés Santos,André Restivo,Carlos Soares*

Main category: cs.LG

TL;DR: 本文实证研究了在VAE中集成Transformer对表格数据生成的影响，发现Transformer在潜在空间和解码器中的位置会带来保真度与多样性的权衡，且Transformer连续块之间高度相似，解码器中输入输出近似线性关系。


<details>
  <summary>Details</summary>
Motivation: 表格数据对生成模型具有挑战性，标准VAE架构（通常由多层感知机组成）难以建模特征间关系，尤其是在处理混合数据类型时。相比之下，Transformer通过注意力机制更适合捕捉复杂的特征交互。

Method: 在VAE的不同组件中集成Transformer进行实证研究，在OpenML CC18套件的57个数据集上进行实验，探索Transformer在不同位置（编码器、潜在空间、解码器）的影响。

Result: 1. Transformer在潜在空间和解码器中的位置会导致保真度与多样性的权衡；2. 观察到Transformer在所有组件中连续块之间高度相似，特别是在解码器中，Transformer的输入输出关系近似线性。

Conclusion: 在VAE中集成Transformer能改善表格数据生成，但需要注意组件位置带来的权衡效应，同时Transformer内部结构的相似性表明可能存在简化设计的空间。

Abstract: Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.

</details>


### [178] [Evolutionary Strategies lead to Catastrophic Forgetting in LLMs](https://arxiv.org/abs/2601.20861)
*Immanuel Abdi,Akshat Gupta,Micah Mok,Alexander Lu,Nicholas Lee,Gopala Anumanchipalli*

Main category: cs.LG

TL;DR: 进化策略（ES）在LLM持续学习中的遗忘问题研究：虽然ES在数学推理任务上能达到接近GRPO的性能，但会伴随严重的遗忘现象，限制了其在线训练应用。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏部署后持续学习的能力，梯度算法需要大量内存，而进化策略（ES）作为无梯度替代方案在特定任务上表现良好。研究ES在持续学习中的遗忘问题，评估其适用性。

Method: 对ES进行综合分析，特别评估其在增加更新步数时的遗忘曲线。与GRPO在数学推理任务上比较性能，分析ES更新的稀疏性和ℓ₂范数特性。

Result: ES在可比计算预算下能达到接近GRPO的性能，但伴随显著的先前能力遗忘。ES更新比GRPO更新稀疏性差且ℓ₂范数大几个数量级，解释了两种算法遗忘曲线的差异。

Conclusion: ES在持续学习中存在严重遗忘问题，限制了其在线训练应用。研究旨在揭示无梯度算法遗忘问题，并希望启发未来工作缓解这些问题。

Abstract: One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [179] [E2HiL: Entropy-Guided Sample Selection for Efficient Real-World Human-in-the-Loop Reinforcement Learning](https://arxiv.org/abs/2601.19969)
*Haoyuan Deng,Yuanjiang Xue,Haoyang Du,Boyang Zhou,Zhenyu Wu,Ziwei Wang*

Main category: cs.RO

TL;DR: 提出了一种名为E2HIL的样本高效人机交互强化学习框架，通过主动选择信息样本减少人类干预需求，在真实世界操作任务中取得更好效果


<details>
  <summary>Details</summary>
Motivation: 现有的人机交互强化学习框架样本效率低，需要大量人工干预才能收敛，导致高昂的人工成本

Method: 通过分析不同样本对策略熵的影响函数，选择中等影响值的样本，剔除导致熵急剧下降的捷径样本和影响可忽略的噪声样本

Result: 在四个真实世界操作任务中，相比最先进的人机交互RL方法，成功率提高42.1%，同时人类干预减少10.1%

Conclusion: E2HIL框架通过主动样本选择有效提高了人机交互强化学习的样本效率，减少了人工干预需求，在真实世界操作任务中表现出色

Abstract: Human-in-the-loop guidance has emerged as an effective approach for enabling faster convergence in online reinforcement learning (RL) of complex real-world manipulation tasks. However, existing human-in-the-loop RL (HiL-RL) frameworks often suffer from low sample efficiency, requiring substantial human interventions to achieve convergence and thereby leading to high labor costs. To address this, we propose a sample-efficient real-world human-in-the-loop RL framework named \method, which requires fewer human intervention by actively selecting informative samples. Specifically, stable reduction of policy entropy enables improved trade-off between exploration and exploitation with higher sample efficiency. We first build influence functions of different samples on the policy entropy, which is efficiently estimated by the covariance of action probabilities and soft advantages of policies. Then we select samples with moderate values of influence functions, where shortcut samples that induce sharp entropy drops and noisy samples with negligible effect are pruned. Extensive experiments on four real-world manipulation tasks demonstrate that \method achieves a 42.1\% higher success rate while requiring 10.1\% fewer human interventions compared to the state-of-the-art HiL-RL method, validating its effectiveness. The project page providing code, videos, and mathematical formulations can be found at https://e2hil.github.io/.

</details>


### [180] [Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning](https://arxiv.org/abs/2601.19972)
*Kuanqi Cai,Liding Zhang,Xinwen Su,Kejia Chen,Chaoqun Wang,Sami Haddadin,Alois Knoll,Arash Ajoudani,Luis Figueredo*

Main category: cs.RO

TL;DR: JIT*算法改进EIT*，通过即时模块优化边连接和采样密度，以及运动性能模块平衡可操作性和轨迹成本，提升高维复杂环境中的路径规划效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的方法在高维复杂环境中难以同时保证路径的可行性和最优性，特别是对于机器人机械臂，运动奇异性、自碰撞等问题进一步增加了规划难度。

Method: 提出JIT*算法，包含两个核心模块：1)即时模块（Just-in-Time Edge动态优化边连接，Just-in-Time Sample在瓶颈区域调整采样密度）；2)运动性能模块（通过动态切换平衡可操作性和轨迹成本，降低奇异性风险）。

Result: 在R^4到R^16维度空间中，JIT*均优于传统基于采样的规划器；在单臂和双臂操作任务中进一步验证了算法的有效性。

Conclusion: JIT*算法通过即时优化和运动性能平衡，显著提升了高维复杂环境下机器人路径规划的效率和安全性。

Abstract: In high-dimensional robotic path planning, traditional sampling-based methods often struggle to efficiently identify both feasible and optimal paths in complex, multi-obstacle environments. This challenge is intensified in robotic manipulators, where the risk of kinematic singularities and self-collisions further complicates motion efficiency and safety. To address these issues, we introduce the Just-in-Time Informed Trees (JIT*) algorithm, an enhancement over Effort Informed Trees (EIT*), designed to improve path planning through two core modules: the Just-in-Time module and the Motion Performance module. The Just-in-Time module includes "Just-in-Time Edge," which dynamically refines edge connectivity, and "Just-in-Time Sample," which adjusts sampling density in bottleneck areas to enable faster initial path discovery. The Motion Performance module balances manipulability and trajectory cost through dynamic switching, optimizing motion control while reducing the risk of singularities. Comparative analysis shows that JIT* consistently outperforms traditional sampling-based planners across $\mathbb{R}^4$ to $\mathbb{R}^{16}$ dimensions. Its effectiveness is further demonstrated in single-arm and dual-arm manipulation tasks, with experimental results available in a video at https://youtu.be/nL1BMHpMR7c.

</details>


### [181] [Real-Time Robot Execution with Masked Action Chunking](https://arxiv.org/abs/2601.20130)
*Haoxuan Wang,Gengyu Zhang,Yan Yan,Yuzhang Shang,Ramana Rao Kompella,Gaowen Liu*

Main category: cs.RO

TL;DR: 论文提出REMAC方法，通过掩码动作分块学习纠正调整，解决异步推理中的分块内不一致问题，实现实时机器人操作的高可靠性


<details>
  <summary>Details</summary>
Motivation: 实时执行对机器人等网络物理系统至关重要。异步推理虽能实现实时响应，但简单集成常导致执行失败。以往方法关注分块间不连续性，但忽略了分块内不一致性这一关键因素——机器人执行的动作分块与当前感知部分不匹配。

Method: 提出REMAC方法：1）通过掩码动作分块在预训练策略上学习纠正调整，使策略在异步推理中面对预期动作与实际执行不匹配时保持鲁棒性；2）引入前缀保留采样程序来增强分块间连续性。

Result: 在仿真和现实环境中的大量实验表明，该方法能实现更快的任务执行，在不同延迟下保持鲁棒性，并持续获得更高的完成率，且不增加额外延迟。

Conclusion: REMAC通过解决异步推理中的分块内不一致问题，提供了一种更可靠的实时机器人操作策略，在保持实时性的同时显著提升了执行成功率。

Abstract: Real-time execution is essential for cyber-physical systems such as robots. These systems operate in dynamic real-world environments where even small delays can undermine responsiveness and compromise performance. Asynchronous inference has recently emerged as a system-level paradigm for real-time robot manipulation, enabling the next action chunk to be predicted while the current one is being executed. While this approach achieves real-time responsiveness, naive integration often results in execution failure. Previous methods attributed this failure to inter-chunk discontinuity and developed test-time algorithms to smooth chunk boundaries. In contrast, we identify another critical yet overlooked factor: intra-chunk inconsistency, where the robot's executed action chunk partially misaligns with its current perception. To address this, we propose REMAC, which learns corrective adjustments on the pretrained policy through masked action chunking, enabling the policy to remain resilient under mismatches between intended actions and actual execution during asynchronous inference. In addition, we introduce a prefix-preserved sampling procedure to reinforce inter-chunk continuity. Overall, our method delivers more reliable policies without incurring additional latency. Extensive experiments in both simulation and real-world settings demonstrate that our method enables faster task execution, maintains robustness across varying delays, and consistently achieves higher completion rates.

</details>


### [182] [A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes](https://arxiv.org/abs/2601.20149)
*Muzaffar Qureshi,Tochukwu Elijah Ogri,Kyle Volle,Rushikesh Kamalapurkar*

Main category: cs.RO

TL;DR: 论文提出了一种针对移动机器人定位误差导致高斯过程模型性能下降的实时修正方法，通过预计算雅可比和海森矩阵来快速更新模型。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程假设测量位置精确已知，但移动机器人应用中存在定位误差，导致测量位置估计不准确，这会严重降低高斯过程的均值和协方差估计精度。

Method: 利用核函数的可微性，开发了基于预计算雅可比和海森矩阵的二阶修正算法，能够在获得更准确的位置估计时实时更新高斯过程模型，而无需完全重新训练。

Result: 仿真结果表明，该方法相比完全重新训练模型，在预测精度和计算效率方面都有显著提升。

Conclusion: 该方法有效地解决了移动机器人应用中定位不确定性对高斯过程模型的影响，实现了实时高效的模型修正，为实际应用提供了实用解决方案。

Abstract: Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.

</details>


### [183] [TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement](https://arxiv.org/abs/2601.20208)
*Wanjun Jia,Kang Li,Fan Yang,Mengfei Duan,Wenrui Chen,Yiming Jiang,Hui Zhang,Kailun Yang,Zhiyong Li,Yaonan Wang*

Main category: cs.RO

TL;DR: TRACER是一个针对可变形物体操作的纹理鲁棒性交互点预测框架，通过层次化语义推理和边界约束机制，解决了现有方法在复杂纹理和外观变化下的边界溢出和功能区域碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 可变形物体操作的核心挑战在于将高层语义指令与物理交互点对齐，同时处理复杂的外观和纹理变化。现有基于视觉的交互点预测方法常因可变形物体的近无限自由度、复杂动力学和异质模式而出现边界溢出和功能区域碎片化问题。

Method: 提出TRACER框架：1) 树状交互点思维链(TA-CoT)将高层任务意图分解为层次化子任务语义；2) 空间约束边界细化(SCBR)机制抑制预测溢出；3) 交互收敛细化流(ICRF)聚合受外观噪声污染的离散像素，增强空间连续性和物理合理性。

Result: 在Fine-AGDDO15数据集和真实机器人平台上进行广泛实验，TRACER显著提高了跨不同纹理和模式的交互点定位精度，并提升了长时域任务的成功率，有效弥合了高层语义推理与低层物理执行之间的差距。

Conclusion: TRACER通过建立从层次化语义推理到外观鲁棒、物理一致的功能区域细化的跨层次映射，成功解决了可变形物体操作中的纹理鲁棒性问题，为机器人操作提供了更可靠的交互点预测框架。

Abstract: The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.

</details>


### [184] [TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance](https://arxiv.org/abs/2601.20239)
*Zhemeng Zhang,Jiahua Ma,Xincheng Yang,Xin Wen,Yuzhi Zhang,Boyan Li,Yiran Qin,Jin Liu,Can Zhao,Li Kang,Haoqin Hong,Zhenfei Yin,Philip Torr,Hao Su,Ruimao Zhang,Daolin Ma*

Main category: cs.RO

TL;DR: TouchGuide提出了一种新的跨策略视觉-触觉融合范式，通过两阶段推理引导预训练的视觉运动策略，结合任务特定的接触物理模型和低成本触觉数据收集系统，显著提升了接触丰富任务的性能。


<details>
  <summary>Details</summary>
Motivation: 机器人精细和接触丰富的操作仍然具有挑战性，主要原因是触觉反馈未得到充分利用。现有方法在融合视觉和触觉模态方面存在不足，需要一种能有效结合两种感官信息的方法来提升操作性能。

Method: 提出TouchGuide两阶段推理框架：第一阶段仅使用视觉输入通过预训练的扩散或流匹配视觉运动策略生成粗略动作；第二阶段通过任务特定的接触物理模型提供触觉引导，使用对比学习训练CPM来评估动作的物理接触可行性，并引导采样过程。同时开发了TacUMI低成本触觉数据收集系统，利用刚性指尖获取直接触觉反馈。

Result: 在五个具有挑战性的接触丰富任务（如系鞋带、芯片交接等）上的大量实验表明，TouchGuide始终且显著优于现有的视觉-触觉策略，证明了该方法的有效性。

Conclusion: TouchGuide通过新颖的跨策略视觉-触觉融合范式，成功解决了接触丰富操作任务中触觉反馈利用不足的问题，同时TacUMI系统提供了高质量低成本的数据收集方案，为机器人精细操作提供了有效的解决方案。

Abstract: Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.

</details>


### [185] [Shallow-π: Knowledge Distillation for Flow-based VLAs](https://arxiv.org/abs/2601.20262)
*Boseong Jeon,Yunho Choi,Taehan Kim*

Main category: cs.RO

TL;DR: Shallow-pi提出了一种知识蒸馏框架，通过大幅减少VLA模型的Transformer层数（从18层压缩到6层），实现超过2倍的推理加速，且成功率下降不到1%，并在工业级机器人平台上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着实时机器人部署需求的增长，需要快速、设备端的视觉-语言-动作模型推理。现有研究主要关注token级效率优化（如视觉token剪枝），而对系统性的Transformer层减少研究有限，特别是在基于流的VLA模型的知识蒸馏方面尚未探索。

Method: 提出Shallow-pi知识蒸馏框架，激进地减少VLM主干和基于流的动作头的Transformer深度，将模型从18层压缩到6层，通过知识蒸馏保持性能。

Result: 在标准操作基准测试中，实现超过2倍的推理加速，成功率绝对下降不到1%，在压缩的VLA模型中达到最先进性能。在Jetson Orin和Jetson Thor等工业级平台上，通过多种机器人系统（包括人形机器人）在复杂动态操作场景中验证了方法的有效性。

Conclusion: Shallow-pi为实时机器人部署提供了一种有效的模型压缩方案，通过激进层减少和知识蒸馏，在保持性能的同时显著提升推理速度，并在实际工业场景中得到验证。

Abstract: The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.

</details>


### [186] [Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation](https://arxiv.org/abs/2601.20321)
*Yuzhe Huang,Pei Lin,Wanlin Li,Daohan Li,Jiajun Li,Jiaming Jiang,Chenxi Xiao,Ziyuan Jiao*

Main category: cs.RO

TL;DR: 论文提出TaF-VLA框架，通过触觉-力对齐范式增强视觉-语言-动作模型在接触密集型任务中的物理直觉，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型主要依赖视觉模态，缺乏接触密集型任务所需的物理直觉（如精确力调节和物理推理）。当前将触觉传感集成到VLA模型的方法通常将触觉输入视为辅助视觉纹理，忽略了表面变形与交互动力学之间的内在关联。

Method: 提出触觉-力对齐范式，开发TaF-VLA框架。包括：1）构建自动化触觉-力数据采集设备和TaF数据集（包含超过1000万同步触觉观测、6轴力/扭矩和矩阵力图）；2）设计触觉-力适配器，将序列触觉观测与交互力对齐，提取离散化潜在信息；3）将力对齐编码器集成到VLA骨干网络中。

Result: 广泛的真实世界实验表明，TaF-VLA策略在接触密集型任务上显著优于最先进的触觉-视觉对齐和纯视觉基线，验证了其通过跨模态物理推理实现鲁棒、力感知操作的能力。

Conclusion: 通过从触觉-视觉对齐转向触觉-力对齐的范式转变，TaF-VLA框架成功地将触觉观测物理地锚定在交互力上，使VLA模型能够捕获历史依赖、噪声不敏感的物理动力学，而非静态视觉纹理，从而实现了更鲁棒的接触密集型操作。

Abstract: Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.

</details>


### [187] [Demonstration-Free Robotic Control via LLM Agents](https://arxiv.org/abs/2601.20334)
*Brian Y. Tsui,Alan Y. Fang,Tiffany J. Hwu*

Main category: cs.RO

TL;DR: FAEA将前沿LLM智能体框架直接应用于机器人操作，无需修改、演示或微调，在多个基准测试中达到与需要大量演示的VLA模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型需要任务特定的演示和微调，且领域适应性差。研究通用LLM智能体框架能否作为机器人操作的新控制范式，实现免演示、免微调的操作能力。

Method: 将前沿LLM智能体框架（Claude Agent SDK）直接应用于机器人操作环境，利用其迭代推理能力（类似代码调试）来推理操作策略，无需任何修改。

Result: 在LIBERO、ManiSkill3、MetaWorld基准测试中，FAEA分别达到84.9%、85.7%、96%的成功率，接近需要<100演示的VLA模型性能。添加一轮人工反馈后，LIBERO性能提升至88.2%。

Conclusion: 通用智能体框架足以处理需要深思熟虑任务规划的机器人操作任务，为机器人系统利用前沿模型进展开辟了新路径，具有免演示数据增强等实用价值。

Abstract: Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim

</details>


### [188] [RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification](https://arxiv.org/abs/2601.20377)
*Xinyan Chen,Qinchun Li,Ruiqin Ma,Jiaqi Bai,Li Yi,Jianfei Yang*

Main category: cs.RO

TL;DR: RF-MatID是首个开源、大规模、宽频带、几何多样化的射频数据集，用于细粒度材料识别，包含16个细粒度类别、142k样本，频率范围4-43.5GHz，并建立了多设置、多协议的深度学习基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的材料识别方法受光学传感器固有局限，而基于射频的方法能揭示材料内在特性但面临缺乏大规模公开数据集和学习方法基准测试的问题。

Method: 构建了RF-MatID数据集，包含16个细粒度材料类别（分为5个超类），142k个样本，频率范围4-43.5GHz，同时包含频域和时域表示，并系统引入入射角和距离等几何扰动。

Result: 建立了多设置、多协议基准，评估了最先进的深度学习模型在分布内性能和分布外鲁棒性（跨角度和跨距离迁移），5个频率分配协议支持系统频率和区域级分析。

Conclusion: RF-MatID旨在促进可重复研究、加速算法进步、增强跨域鲁棒性，并支持射频材料识别在实际应用中的发展。

Abstract: Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.

</details>


### [189] [STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation](https://arxiv.org/abs/2601.20381)
*Alexandre Chapin,Emmanuel Dellandréa,Liming Chen*

Main category: cs.RO

TL;DR: STORM是一个轻量级对象中心适配模块，通过多阶段训练策略将冻结的视觉基础模型增强为语义感知的槽位表示，用于机器人操控任务。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型虽然提供强大的感知特征，但其密集表示缺乏明确的对象级结构，限制了机器人操控任务的鲁棒性和可收缩性。

Method: 采用多阶段训练策略：1）使用语言嵌入进行视觉-语义预训练以稳定对象中心槽位；2）与下游操控策略联合适配。避免重新训练大型骨干网络。

Result: 在对象发现基准和模拟操控任务中，STORM相比直接使用冻结基础模型特征或端到端训练对象中心表示，能更好地泛化到视觉干扰物并提升控制性能。

Conclusion: 多阶段适配是高效将通用基础模型特征转化为任务感知对象中心表示的有效机制，适用于机器人控制。

Abstract: Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.

</details>


### [190] [A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests](https://arxiv.org/abs/2601.20529)
*Julia Richter,David Oberacker,Gabriela Ligeza,Valentin T. Bickel,Philip Arm,William Talbot,Marvin Grosse Besselmann,Florian Kehl,Tristan Schnell,Hendrik Kolvenbach,Rüdiger Dillmann,Arne Roennau,Marco Hutter*

Main category: cs.RO

TL;DR: 该论文提出了一个结构化KPI框架，用于评估月球多机器人勘探任务，以解决当前实地试验结果难以比较的问题。


<details>
  <summary>Details</summary>
Motivation: 当前月球机器人勘探任务中，由于机器人平台和实验设置的差异，不同实地试验结果难以比较。现有评估通常使用场景特定的工程指标，未能建立实地性能与科学目标之间的明确联系。

Method: 从三个现实的多机器人月球场景中推导出结构化KPI框架，这些场景反映了科学目标和操作约束。框架强调效率、鲁棒性和精度的场景依赖性优先级，并专门为实地部署的实际应用而设计。

Result: 在多机器人实地测试中验证了该框架，发现效率和鲁棒性相关的KPI实用且易于应用，而精度导向的KPI需要可靠的基准数据，这在室外模拟环境中并非总能获得。

Conclusion: 该框架可作为通用评估标准，实现多机器人实地试验的一致、目标导向比较，并支持未来行星探索机器人系统的系统化开发。

Abstract: Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.

</details>


### [191] [Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands](https://arxiv.org/abs/2601.20555)
*Wadhah Zai El Amri,Nicolás Navarro-Guerrero*

Main category: cs.RO

TL;DR: 通过低成本压电麦克风和音频谱图Transformer实现机器人全身接触定位，静态误差小于5mm，材料特性对定位性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 传统触觉皮肤成本高且集成复杂，需要一种可扩展的替代方案来实现经济实惠的接触感知。

Method: 使用七个低成本压电麦克风捕捉振动信号，采用音频谱图Transformer解码振动特征，实现全身接触定位。

Result: 静态条件下定位误差小于5mm，硬质材料在冲击响应定位中表现优异，纹理材料在轨迹跟踪中提供更好的摩擦特征，系统对机器人自身运动具有鲁棒性。

Conclusion: 复杂物理接触动态可以从简单振动信号有效解码，为机器人领域提供了经济实惠、可广泛应用的接触感知解决方案。

Abstract: Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.

</details>


### [192] [MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization](https://arxiv.org/abs/2601.20577)
*Baiqing Wang,Helei Cui,Bo Zhang,Xiaolong Zheng,Bin Guo,Zhiwen Yu*

Main category: cs.RO

TL;DR: MeCo是一个基于相似性感知的多机器人协作框架，通过"缓存与重用"机制减少LLM重复规划，提高效率


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多机器人协作方法在遇到相似任务时需要重新规划，忽略了任务级相似性，导致计算冗余和效率低下

Method: 提出MeCo框架，引入相似性测试方法检索相关已解决任务，实现计划重用而不重新调用LLM；同时创建MeCoBench基准测试

Result: 实验结果表明，MeCo相比最先进方法显著降低了规划成本并提高了成功率

Conclusion: MeCo通过相似性感知的缓存重用机制，有效解决了多机器人协作中的计算冗余问题，提升了系统效率和适应性

Abstract: Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse'' (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.

</details>


### [193] [GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control](https://arxiv.org/abs/2601.20668)
*Shuhao Liao,Peizhuo Li,Xinrong Yang,Linnan Chang,Zhaoxin Fan,Qing Wang,Lei Shi,Yuhong Cao,Wenjun Wu,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: GPO是一种通过时变动作变换限制早期有效动作空间，随后逐步扩展以增强探索的强化学习训练框架，用于足式机器人的扭矩控制学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法在位置控制中表现良好，但需要环境特定的启发式方法（如奖励塑造、课程设计、手动初始化）。对于扭矩控制，探索动作空间和获取有意义的梯度信号更加困难，因此需要更有效的训练框架。

Method: 引入Growing Policy Optimization (GPO)框架，使用时变动作变换在早期阶段限制有效动作空间，鼓励更有效的数据收集和策略学习，然后逐步扩展动作空间以增强探索。

Result: 在四足和六足机器人上评估GPO，包括仿真训练策略的零样本硬件部署。使用GPO训练的策略始终获得更好的性能。

Conclusion: GPO为学习足式运动提供了一个通用、环境无关的优化框架，通过动作空间的逐步扩展实现了更有效的策略学习和探索。

Abstract: Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.

</details>


### [194] [Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model](https://arxiv.org/abs/2601.20682)
*Péter Polcz,Katalin Schäffer,Miklós Koller*

Main category: cs.RO

TL;DR: 提出了一种通过肌腱位移和张力估计关节角度的计算方法，并应用于无关节传感器的肌腱驱动仿人机器人手的闭环控制。


<details>
  <summary>Details</summary>
Motivation: 肌腱驱动仿人机器人手通常缺乏直接关节角度传感，因为集成关节编码器会影响机械紧凑性和灵巧性。

Method: 基于Denavit-Hartenberg约定建立高效运动学模型，通过简化肌腱模型推导肌腱状态与关节位置的非线性方程，采用非线性优化方法求解，结合基于雅可比矩阵的PI控制器和前馈项实现闭环控制。

Result: 在MuJoCo仿真环境中使用具有5个自由度的长手指和6个自由度的拇指的解剖学正确生物机电手，验证了所提估计和控制框架的有效性和局限性。

Conclusion: 实现了无需直接关节传感的手势跟踪控制，为肌腱驱动仿人机器人手提供了一种可行的关节角度估计和控制解决方案。

Abstract: Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.

</details>


### [195] [One Step Is Enough: Dispersive MeanFlow Policy Optimization](https://arxiv.org/abs/2601.20701)
*Guowei Zou,Haitao Wang,Hejun Wu,Yukun Qian,Yuhang Wang,Weibing Li*

Main category: cs.RO

TL;DR: DMPO提出了一种用于实时机器人控制的单步生成策略框架，通过MeanFlow实现无需知识蒸馏的单步推理、分散正则化防止表征崩溃、RL微调超越专家演示，在保持性能的同时实现5-20倍推理加速，满足>120Hz实时控制需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散和流匹配的生成策略需要多步采样，限制了在时间关键场景中的部署。实时机器人控制需要快速的动作生成，但多步采样从根本上制约了推理速度。

Method: DMPO包含三个关键组件：1) MeanFlow：数学推导的单步推理方法，无需知识蒸馏；2) 分散正则化：防止表征崩溃；3) RL微调：超越专家演示性能。结合轻量级模型架构，实现单步生成。

Result: 在RoboMimic操作和OpenAI Gym运动基准测试中，DMPO达到与多步基线竞争或更优的性能。推理速度提升5-20倍，满足>120Hz实时控制要求，在高性能GPU上达到数百赫兹。在Franka-Emika-Panda机器人上验证了实际部署可行性。

Conclusion: DMPO通过统一的单步生成框架解决了实时机器人控制的速度瓶颈，在保持性能的同时大幅提升推理速度，实现了真正的单步生成，为时间关键的控制场景提供了实用解决方案。

Abstract: Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (>120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.

</details>


### [196] [Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy](https://arxiv.org/abs/2601.20776)
*Huanyu Tian,Martin Huber,Lingyun Zeng,Zhe Han,Wayne Bennett,Giuseppe Silvestri,Gerardo Mendizabal-Ruiz,Tom Vercauteren,Alejandro Chavez-Badiola,Christos Bergeles*

Main category: cs.RO

TL;DR: 本文提出一种弱监督框架，将标定感知感知与导纳控制结合，重新思考稳态机械手操作。该方法利用可复用的预热轨迹提取隐式空间信息，无需外部基准标记或手动深度标注，实现了标定感知的深度感知。系统通过观测和标定模型的残差建立任务空间误差预算，在用户研究中显著降低了操作负荷。


<details>
  <summary>Details</summary>
Motivation: 传统自动化方法依赖劳动密集型的2D标注，需要外部基准标记或手动深度标注，这增加了系统复杂性和设置要求。本文旨在开发一种更实用的显微镜引导生物医学微操作框架，提高操作可靠性同时减少复杂设置需求。

Method: 采用弱监督框架，融合标定感知感知与导纳控制。利用可复用的预热轨迹提取隐式空间信息，实现无需外部基准标记或手动深度标注的标定感知深度感知。通过明确表征观测和标定模型的残差，建立任务空间误差预算。系统在任务空间中建立误差预算，实现闭环控制。

Result: 系统在95%置信水平下达到约49微米的横向闭环精度（最坏情况测试子集），在大范围平面移动中深度精度≤291微米（95%置信边界）。在8人参与的用户研究中，学习代理相比简单稳态辅助基线将NASA-TLX总体负荷降低了77.1%。

Conclusion: 弱监督代理在不引入复杂设置要求的情况下，提高了显微镜引导生物医学微操作的可靠性。该框架为显微镜引导干预提供了实用的解决方案，通过利用预热轨迹的隐式空间信息，实现了无需外部基准标记的高精度操作。

Abstract: This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.

</details>


### [197] [A Methodology for Designing Knowledge-Driven Missions for Robots](https://arxiv.org/abs/2601.20797)
*Guillermo GP-Lenza,Carmen DR. Pita-Romero,Miguel Fernandez-Cortizas,Pascual Campoy*

Main category: cs.RO

TL;DR: 提出在ROS 2系统中实现知识图谱的综合方法，用于提升自主机器人任务效率和智能性，通过Aerostack2框架在Gazebo环境中演示了搜救任务


<details>
  <summary>Details</summary>
Motivation: 提升自主机器人任务的效率和智能性，通过知识图谱增强决策能力和任务性能

Method: 包含定义初始和目标条件、结构化任务与子任务、规划任务序列、用知识图谱表示任务相关数据、使用高级语言设计任务等步骤的完整方法

Result: 在Aerostack2框架中实现了模拟搜救任务，无人机在Gazebo环境中自主定位目标，验证了方法在提升决策和任务性能方面的有效性

Conclusion: 提出的知识图谱方法能有效提升ROS 2系统中自主机器人任务的智能性和效率，为复杂任务提供了结构化解决方案

Abstract: This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.

</details>


### [198] [End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting](https://arxiv.org/abs/2601.20846)
*Jamie Hathaway,Alireza Rastegarpanah,Rustam Stolkin*

Main category: cs.RO

TL;DR: 提出一种基于神经风格迁移的sim-to-real强化学习策略转移方法，通过变分自编码器学习特征表示并生成弱配对轨迹，提高合成轨迹的物理真实性。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人控制中依赖大量仿真数据，但仿真与物理系统存在领域差距，且真实世界样本有限，限制了实际部署。

Method: 将神经风格迁移从图像处理重新解释到轨迹数据，使用变分自编码器联合学习自监督特征表示，生成弱配对的源-目标轨迹以提高合成轨迹的物理真实性。

Result: 相比基线方法（包括先前工作、CycleGAN和条件变分自编码器时间序列翻译），该方法在机器人切割未知材料任务中实现了更优的任务完成时间和行为稳定性，仅需少量真实数据。

Conclusion: 该框架对几何和材料变化具有鲁棒性，展示了在真实世界奖励信息不可用、接触丰富的挑战性任务中进行策略适应的可行性。

Abstract: Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.

</details>
